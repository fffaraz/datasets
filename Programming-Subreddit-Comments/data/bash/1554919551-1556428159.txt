I just edited my original reply to add another possibility that might work for you.
Ah! I didn't see your reply here until after editing mine to include something similar as another option. Your reply is clearer than what I wrote.
You‚Äôve specified two conflicting requirements here. You‚Äôve said your input is JavaScript. Therefore you will need to run it through a JavaScript interpreter. This conflicts with your other stated requirement of not having external dependencies, because Bash doesn‚Äôt understand JavaScript. You could probably mash something together with grep, sed, awk etc but it‚Äôll be brittle as it won‚Äôt understand JavaScript syntax, so a change to the JS that‚Äôs still syntactically correct could break your script. Given that you have data that needs to be read from both Bash and from JS, in your position I would try to look at how the project could be refactored such that the values come from a common source (say a config file) which could be processed from either language.
Your question got answered pretty well when someone asked something similar on stackexchage, regarding parsing HTML with regex. &amp;#x200B; You are parsing JS with bash, so it's sort of the same answer. &amp;#x200B; [https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags)
[https://stackoverflow.com/a/1402389](https://stackoverflow.com/a/1402389)
Okay, but I like you and appreciate your input.
Thank you. This was what I needed. I searched awk and was overwhelmed. I have no clue what all these characters man but it works.
You can also use `mogrify` from Imagemagick (should be installed if you already have `convert`). As mentioned in this [answer](https://unix.stackexchange.com/a/29875), when supplied with -format option it will write a new file and just changes the extension. You do not have to loop through explicitly. mogrify -format jpg -- "$1"/*.png The -- ensures that it doesn't interpret any filenames with strange characters differently.
What about grepping out the JSON from the JavaScript source and then running that through jq?
I suggest you read this. http://www.gnu.org/software/gawk/manual/gawk.html
Three conflicting requirements. He said JSON but gave javascript.
I edited my response with comments.
Then the title is incorrect
All I did was remove unnecessary whitespace and it stopped working! const credentials={"secret": "value"} export default credentials; I put in the secret for my test environment, but it's using the commented-out one! const credentials = { "secret": "hunter2" } // const credentials = { // "secret": "value" // } export default credentials;
You probably want to just make a systemd service for it.
Do you want the naming to start from the beginning for each directory or do you want the alphabetical thing to go across directories?
Start of each directory.
 while true; do java ... sleep 10 echo resuming server ... done
Here is what I could come up with. #!/bin/bash (( $# &lt; 1 )) &amp;&amp; { echo "At least one argument required"; exit 1; } src=${1%/} target=${2:-${src}_copy} shopt -s globstar for dir in "$src"/**/; do t="$target/${dir#$src\/}" mkdir -p -- "$t" n=0 for file in "$dir"/*.jpg; do printf -v i '%x' "$n" cp -- "$file" "$t/$i.jpg" ((n++)) done done
Remind me! 1.5 hours
ty.
Its a script that literally takes 10 seconds for someone who knows bash to write for me, why would I bother with that?
Because it sounds like you want this to run on startup and to respawn automatically when it crashes. Proper way to do this is either an init script or a systemd unit (on modern systems).
K
A
This should work. I'm using `pushd` `popd` to make sure we work with the files in their respective directories. If a file exists without an extension, this will rename it to `xxx.original_name`. If you want to leave those untouched, we can add a check in the `[[` test. Change `%04x` to `%0nx` where `n` is the number of hex digits you want to be zero-padded. shopt -s globstar for d in **/; do pushd "$d" &gt; /dev/null i=0 for f in *; do if [[ -f "$f" ]]; then mv -n "$f" "$(printf '%04x' "$i").${f##*.}" (( i++ )) fi done popd &gt; /dev/null done I'm using `-n` (no-clobber) on `mv` to be safe.
They create a separate tree and copy the files over, mine is pretty similar, but `mv`s them in-place.
Indeed. Personally I feel safer with copying than I do with in-place.
N
Worked perfectly, now I just have to figure out why GNOME is like: Alphabetical order, oh yeah its: 01abcdef23456789.
https://mywiki.wooledge.org/BashPitfalls#read_num.3B_echo_.24.28.28num.2B-1.29.29
It looks like the sort order of `ls` is loosely defined by `LC_COLLATE`. Can you please try the following: LC_COLLATE="cs_CZ.ISO8859-2" ls as well as ls -f &gt; locale: Cannot set LC_ALL to default locale: No such file or directory It looks like perhaps KDE is donking things up for you. I haven't used KDE in years so I have limited familiarity here, but it seems that you will need to either familiarise yourself with `~/.kde/env/setlocale.sh` and adjust it until it is correct, or you will have to empty it and make it immutable.
As pointed out, any solution like this is pretty brittle. If you really want JavaScript, use Node. Otherwise, filter out commented lines with grep/awk.
Dude, I just fell in love right now
What do you want to happen? Wireshark is meant for interactive analysis
Nautilus splits filenames into fully-alphabetic and fully-numeric parts, so that `foo2` is ordered before `foo10`.
**LC_COLLATE="cs_CZ.ISO8859-2" ls** exhibits the same behavior as before. **ls -f** simply turns off the color and doesn't sort. I find no path below **.kde** named **/env**, so perhaps things have changed since you used KDE - or, as you suggested, perhaps my setup is hosed. Either way you have given me some ideas of places I can poke my nose into. Thank-you for your help.
It was the quotes around wireshark -r
Oh
 export APP_HOME=/path/to/somewhere echo cding to $APP_HOME pushd . 1&gt;/dev/null 2&gt;/dev/null cd $WEBAPP_HOME # do some stuff # do some more stuff echo "Returning to where we started" popd 1&gt;/dev/null 2&gt;/dev/null
sas
cron exists too
The script will be more resilient towards user error/environments/etc if you don't use relative paths. It should be fairly trivial, if you don't want to put the files in `/usr/local/bin` or similar, to get the current directory and use that to call `secondFile.sh`. Check out this: [https://stackoverflow.com/questions/59895/get-the-source-directory-of-a-bash-script-from-within-the-script-itself](https://stackoverflow.com/questions/59895/get-the-source-directory-of-a-bash-script-from-within-the-script-itself)
Interesting. Not sure how this would differ from what I've done. You do a cd $APP\_HOME where $APP\_HOME=/path/to/file. So we can temporarily just skip the variable and write it out plain. You are doing a "cd /path/to/file" then you are running stuff. Isn't that what I am doing with a "cd /path/to/file &amp;&amp; /path/to/file/run.sh"? Looks like the pushd and popd are just to get back to where we started, so we can temporarily ignore that from your example for the moment, right?
Cron isn't really the right solution for this.
Look into writing an init script.
Thanks thats what I was looking for
Or go back to the app owner with "Finish the job".
I'm going to ignore your 'need to use sudo for uname' comment as I need more coffee before even touching that one and just leave the below for you to ponder... cat /etc/*-release
You can run \`uname\` without root, also sourcing /etc/os-release can work too.
I am sorry, I didn't explain very clearly - the thing is that I am checking not for linux version, but if OS is Windows, Linux or Mac. So I am using bash script. Now for Windows(using bash), there was no need for 'run as admin', but for mac it is(and for linux probably too). I just want to avoid sudo, because I am not the only one that will use this file. My friend owns a mac book and he just can't run 'uname' without sudo... Is sourcing /etc/os-release for linux only?
`cat /etc/os-release` That file is the standard.
Running `source /etc/os-release` a Unix/Linux thing, it may be possible on a mac since it's based on Unix.
On Linux and Mac you can `file` on some binary on that host and look for either `elf` or `darwin` in the output.
 - `uname -s` will tell you the "operating system" name (according to POSIX), or the "kernel name" (on GNU-ish systems) - `uname -o` will tell you the "operating system" name on systems were `uname -s` tells you the kernel name (`-o` is not specified in POSIX) - On most modern GNU/Linux systems, looking at `/etc/os-release` (and falling back to `/usr/lib/os-release`) will tell you the specific GNU/Linux OS and version. - On most older GNU/Linux systems, look for files matching `/etc/*-release`; what the `*` expands to is usually the OS name. The contents of the file will usually tell you the version. - On Darwin-based systems (macOS), `sw_vers -productName` will tell you the product name of the operating system (usually "Mac OS X", but it could be something else on Darwin systems other than macOS). `sw_vers -productVersion` will tell you the OS version number. None of the above requires root.
I didn't explain very clearly - the thing is that I am checking not for linux version, but if OS is Windows, Linux or Mac. So I am using bash script. Now for Windows(using bash), there was no need for 'run as admin', but for mac it is(and for linux probably too). I just want to avoid sudo, because I am not the only one that will use this file.
 - GNU/Linux: `uname -s` ‚Üí `Linux` - macOS: `uname -s` ‚Üí `Darwin` - Windows: `uname -s` ‚Üí I think `Windows NT`, but I don't have a Windows computer to verify that.
It works on windows, what does that "-s" do?
Use -eu in your bash scripts to exit on undefined variables and unhandled errors. Would have helped you here, and is good for sanity in general.
What do some sample lines of your .csv file look like?
According to POSIX: -s Write the name of the implementation of the operating system. However, on some systems `-s` documented to write the name of the kernel, and `-o` prints the operating system name (many systems don't make the distinction between the OS and the kernel).
God the lack of formatting... for net in virbr{0..20} vnet{0..20}; do ifconfig "$net" promisc ifconfig "$net" down ifconfig "$net" up done ifconfig vnet56 promisc;ifconfig vnet56 down;ifconfig vnet56 up
Depends, if it's always the same amount, this'll work: \`\`\` for i in $(seq 0 56); do; ifconfig "virbr$i" promisc; ifconfig "virbr$i" down; ifconfig "virbr$i" up; done; \`\`\`
SCRIPT\_DIR="$(cd "$(dirname "${BASH\_SOURCE\[0\]}")" &amp;&amp; pwd)"
Another way to do that (encapsulating the silencing of `pushd` and `popd`) would be like this: silent() { "$@" &gt;/dev/null 2&gt;&amp;1; } export APP_HOME="/path/to/somewhere" echo cding to "$APP_HOME" silent pushd "$APP_HOME" # do some stuff # do some more stuff echo "Returning to where we started" silent popd
ugly but effective: cat /etc/\*release and then grep what you need to know
That is very strange. `uname` shouldn't require `sudo`; I'm on a MacBook and I can run `uname` with `sudo` fine.
so the data set I'll be receiving is First name Last name Company employee id course title course number Registration date course completed Does that help?
Ooh. I did not know about `-u`. Thanks for mentioning this! Love your username, btw. *Office Space* fan?
Not really. How exactly is your date formatted? That would help. Can you cut &amp; paste here a couple lines of sample data?
Are you doing this in terminal, or did you create a bash file?
^ ditto
These both worked great! If you wouldn't mind, how should I interpret the script? I can see the net is the variable, but how does it know to translate that to "ifconfig &lt;name&gt;&lt;#&gt;"? Trying to understand the logic.
It works fine in both.
Sorry had to mock up some data. First Name,Last Name,Company,Employee ID,Course Title,Course Number,Registration Date,Course Completed Linette, Stacey,ABC,132569,Audience and Purpose in Business Writing,87988985,15-MAY-2018,04-DEC-2018 Sammie ,Hsu,ABC,785946,Becoming More Professional through Business Etiquette,56986523,15-MAY-2018,14-SEP-2018 Bobbye ,Gerner,ABC,785946,Becoming Your Own Best Boss,12584578,23-APR-2018,13-SEP-2018 Ethelyn, Bradeen,ABC,458575,Becoming an Accountable Professional,12569856,01-JUN-2018,03-DEC-2018 Marget ,Sideris,ABC,965874,Clarity and Conciseness in Business Writing,12536589,22-MAY-2018,04-DEC-2018 Marin ,Raso,ABC,56981,Managing Meetings and Notes in Outlook,12585489,25-JUN-2018,12-DEC-2018 Audria, Purifoy,ABC,123658,Creating Work/Life Balance,12584785,13-SEP-2018,13-SEP-2018 Wonda ,Bucklew,ABC,123658,Forging Ahead with Perseverance and Resilience,25687895,13-SEP-2018,12-DEC-2018 Reggie ,Jenks,ABC,123548,Make The Time You Need: Get Organized,78985698,04-DEC-2018,07-JUN-2018 Cornell ,Klutts,ABC,125685,Productivity by Managing Time and Tasks,78985874,08-JUN-2018,01-JUN-2018 Christel, Rester,ABC,125894,Setting and Managing Priorities,78958758,12-DEC-2018,22-MAY-2018 Ilse, Sarabia,ABC,487595,Staying Balanced in a Shifting World,,,
OK. That definitely helps! Hmm, so... parsing CSV files is actually not very well suited to bash. But you could do it in awk or Perl or Python or something else \*from\* a bash script pretty easily. The trick is that CSV fields can contain commas if the field is quoted, so your best bet is probably to start with a library that understands CSV. But let's say that your data doesn't have any embedded commas. Then you could do the following: CSV_FILE="/path/to/somefile.csv" awk -v FS="," -v OFS="," ' function date_to_eoq(date) { month = substr(date, 4, 3); year = substr(date, 8, 4); return sprintf("%s-%s", eoq[month], year); } BEGIN { eoq["JAN"] = "31-MAR"; eoq["FEB"] = "31-MAR"; eoq["MAR"] = "31-MAR"; eoq["APR"] = "30-JUN"; eoq["MAY"] = "30-JUN"; eoq["JUN"] = "30-JUN"; eoq["JUL"] = "30-SEP"; eoq["AUG"] = "30-SEP"; eoq["SEP"] = "30-SEP"; eoq["OCT"] = "31-DEC"; eoq["NOV"] = "31-DEC"; eoq["DEC"] = "31-DEC"; } { print $0, date_to_eoq($8); }' \ "$CSV_FILE" Note: I'm assuming here that field #8 (as opposed to #7) is the one that you want to round up to the nearst end-of-quarter boundary.
I knew there would be a better answer than mine, nice!
hi, there is a good chance that the people who responded to your post will not respond to this message, as it is a reply to yourself, and not to either of their messages. They could check back, but that should not be expected. You have to reply to their replies. *** What the commands are basically saying are to do things in a predefined loop. `$(seq 0 20)` accomplishes the same as `{0..20}` in this case. `0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20` one example substitutes just the number, `$i` , into the loop and the other substitutes the in the whole variable `virbr{0..20}`
The Advanced Bash Scripting Guide may help you out here. The chapter on loops: [https://www.tldp.org/LDP/abs/html/loops1.html](https://www.tldp.org/LDP/abs/html/loops1.html)
Oh I hadn't realized you could define the sequence within the same string. Thanks!
This worked, it's so much smaller than my wall of text too lol. Thanks!
The files I'll be receiving is piped delimited, but I see your point. I was going to attempt to use python if this wasn't possible. Thanks so much! I'll have to try this :)
Everything looks good to me, the API is using HTTPS and the keys are stored in the scripts itself. I don‚Äôt see any problems here.
Interesting, what's the difference between `{0..20}` and `(seq 0 22)` ? Are they just different ways of saying the same thing? Does the bracket type matter here?
No problem. What is "piped delimited"? ASCII vertical bar (character code 124)?
I apologize, I felt a bit like trump putting up that wall lol.
Yeah, `{}` and `$()` are two different things. `{0..20}` is brace expansion. It tells bash to expand this to all the between 0 and 20: https://wiki.bash-hackers.org/syntax/expansion/brace#ranges `$(seq 0 22)` is a way to run commands (think `eval()` from other languages) inside the `$()`. So this says "run `seq 0 22`" and then get the output from that and plug it in. This is called "command substitution": https://wiki.bash-hackers.org/syntax/expansion/cmdsubst
R or python+pandas will do this easily and realiably
Literally nothing that has to do with bash in this post...
Rebooting on a single 200 seems a bit aggressive, but if a simple scheme fits your use-case, go for it. What exactly do you mean by ‚Äúsafe‚Äù by the way, do you mean from a security standpoint?
Great - thanks! Mainly wasn't sure about storing the keys in the script.
Yep!!
the problem is that when you refer to "$1" in your user\_input\_function, that means the first argument supplied to the function, not the first argument supplied to the script. it would work if you changed two lines.. call main\_function using... main\_function "$@" and call user\_input\_test using... user\_input\_test "$@" to pass the arguments supplied to the script onwards...
Awesome, now it works as intended. Thanks a lot! I can't believe I did not notice that detail earlier. I'm quite new to bash. If I'm understanding correctly. Any arguments passed to the script are picked up by the main\_function script and then also picked up by the user\_input\_test function.
cmd | nc &lt;address&gt; &lt;port&gt; /dev/tcp is not a real file, it's just a bashism. It should work through that interface just by redirecting stout cmd &gt; /dev/tcp/&lt;address&gt;/&lt;port&gt;
I am using a Raspberry PI and there is no "cmd" and I tried ./program &gt; /dev/tcp/192.168.0.2/80 but the output is not being redirected/piped and I've various ways but it confusing.
`shuf` would do better if you have it.
By cmd , I meant whatever the command is you're running. prinrtf is probably writing to stdout, but if you're using stderr also, you can redirect both. ./program 2&gt;&amp;1 &gt; /dev/tcp/192.168.0.2/80 You can also try writing to a regular file. Since the dev/tcp is just a bashism, it's possible your bash isn't compiled with this feature. I think it's not compiled in on debian, but I'm not entirely sure about that. I would say just connect it with netcat. To connect stdout and stderr, you could do this: ./program 2&gt;&amp;1 | nc 192.168.0.2 80
&gt;./program 2&gt;&amp;1 &gt; /dev/tcp/192.168.0.2/80 Both examples aren't working which I've tried that before.
Oh, of course I should point out this is for writing to the port, not reading from it. And it's a raw socket writing, not http. That might not actually be what you want to do, of course, if you're talking to a web server. The program you might want for http calls is curl, or wget, or w3m maybe. You can get a bidirectional socket connection too, if you want both reading and writing. If you want to get confused (and idk, maybe you do) you could do something like this: exec 5&lt;&gt;/dev/tcp/192.168.0.2/80 ./program &lt;&amp;5 |&amp;5 Not tested, writing from my phone, but that would be open up a new file descriptor with an id of 5, such that input and output are connected to the tcp socket. Then, execute ./program with stdin reading from file 5, and stdout writing to file 5.
 ``` echo "$string" | sed -n "$(shuf -i 1-3 -n 1)p" ```
 Are there error messages? nc is a tool for checking whether a socket is accessible. It could be a firewall or network issue.
Could you explain?
Example feedback: bash: 5: command not found Oh port 80 is not what I should be using as the aim is to port over network to an application that will read the command line output data (virtually button press events for now) from the program. The receiving application is in c#, so tcp or udp and not really anything complex.
Somebody told me to go like: sed . . . &lt;&lt; END Hey, how are you Hello, how's is going Hola, como estas . . . END But I don't get it.
actually that was a really dumb idea lol. this is a simpler way to do it ``` echo "$string" | shuf -n 1 ```
$string that's my variable right?
yes, u need to define it before
 #!/bin/bash variable=["Hey, how are you","Hello, how's is going","Hola, como estas"] echo "$variable" | shuf -n 1 &amp;#x200B; // doesn't work
what doesn't work? paste this in your terminal ``` string="Hey, how are you Hello, how's is going Hola, como estas"; echo "$string" | shuf -n 1```
Yes it works... I had square brackets, and my sentences where separated by strings also. Now I put everything in one string and it works. Thank you very much.
You could use sed to print a single line of text of a file with the "&lt;line-number&gt;p" (print command), and the -n (silent) option to suppress printing all the other lines. For example, assume the strings are in the file `messages`, one string per line, and no blanks or other text: $ cat messages Hey, how are you Hello, how's is going Hola, como estas Then individual lines can be printed with sed by line number: $ sed -n 1p messages Hey, how are you $ sed -n 2p messages Hello, how's is going $ sed -n 3p messages Hola, como estas We can get the number of lines with the `wc` command, and we can use the `bash` RANDOM function to get a random line number. $ MAX=$(wc -l &lt; messages) $ x=$(( 1 + RANDOM % $MAX )) $ sed -n "${x}p" messages Hola, como estas
Using that method, how would you do it without creating a file, and using /dev/urandom, to change your "1p", "2p", etc. ?
That's not the correct syntax for Bash arrays. Try this: Array=( "Hey, how are you?" "Hello, how's it going?" "¬øHola, como estas?" ); echo "${Array[RANDOM%3]}"
You could use &lt;&lt; operator and EOF delimiter to configure the lines of text in the current script, as an input to your command.
You're doing two different things: if [[ $( ps -ef | grep [s]treamlink) ]] $( ) # capture output of contained command into a string [[ ]] # if non-empty, return 0 (true) if # execute what is after "then" if the statement returns "true" On the other hand: if ps -ef | grep [s]treamlink # if grep returns zero (true), execute what's after the "then" The second is more beneficial, since it doesn't involve a subshell, or reading stdout of `grep So if `foo --bar baz` returns zero, the block will be executed.
What you're trying to do here is recreate [ansible](https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html). Using ansible will give you a much more granular approach to what you're trying to do.
Beware the grep will also show up in the ps output.
awk '{ if ( $1 &gt;= 100 || $2 &gt;= 100 || $3 &gt;= 100 ) print $0;}' From memory, syntax may not be ok ..... but it should get you started.
Thanks for the answer. Thing is, my file has 23 columns. So I was wondering if there is a way to avoid writing if ( $1 &gt;= 100 || $2 &gt;= 100 || .... || $23 &gt;= 100 )?
You can take the solution from [here on Stackexchange](https://unix.stackexchange.com/questions/362338/awk-how-to-loop-through-a-file-to-get-every-line) and modify it to your needs.
You can use for loops in awk. Loop through all your fields and print then break if a field is over the threshold.
So this seems to work: `awk '{ FS = "," } ; {for(i=1;i&lt;=NF;i++) if(sqrt($i&gt;100)^2) print $0}' f1.txt &gt;f2.txt` **BUT** it keeps some lines at the start of the file that do not satisfy the condition. So it's not completely right.
 awk '{ for (i=1; i&lt;=NF; i++) { if ($i &gt; 100) { print $0; break; }; }; }'
You probably need to check if the field actually is a number. I string can also be greater than 100.
You can do what /u/Schreq suggested or if you know how many non-number lines there will be you could skip them like so (in this example skipping 10 lines): awk 'NR &gt; 10 {for(i=1;i&lt;=NF;i++) if($i&gt;100) print $0}' f1.txt &gt;f2.txt
Here is my quick and #!/usr/bin/gawk -E { split($0,a,/"/,s); for (i=1; i&lt;=length(a); i++) { if (i%2 == 1) gsub(/ /, "_", a[i]) printf a[i] printf s[i] } print "" } If you change `if (i%2 == 1)` to `if (i%2 == 0)` it changes the spaces inside the quotes instead. If you really want to use cut though you should just use the tab or some other uncommon character as the column delimiter so the spaces would work just fine inside and outside quotes.
You probably shouldn't be making CGI pages in bash if you're this bad at it.
Thanks for your reply ! Can I use it in a piped command line ? I tried with gawk -f without success &gt; If you really want to use cut though you should just use the tab or some other uncommon character as the column delimiter so the spaces would work just fine inside and outside quotes. Using space as the delimiter when it's common in your data is just kind of retarded. I have no choice, I'm given huge log files generated by our hardware.
you could put it in a file. If you say named it `replace_spaces.awk` then you could use it in a pipe like so: somecommand | ./replace_spaces.awk | someothercommand ....
perl is also a good fit for this: perl -MList::Util=any -ane 'print if any { $_ &gt; 100 } @F'
What happens for `"` text characters that are part of the contents of one of the columns? What rules are used to escape `"` characters? Can you add this to your example? I would use a Perl one-liner: $ perl -nlE '@f = /("[^"]*"|\S+)/g; say "this is the third column: ", $f[2]' &lt; testfile this is the third column: "column3 with a lot of informations" About what's going on there, the following is a regex match that creates a list of the columns: /("[^"]*"|\S+)/g The list is saved in an array variable named "f": @f = ... In Perl, an array starts at zero so this here is the third entry of the array: $f[2] About other features of 'cut', like for example doing `cut -f1,3-5`, that would translate into this: say join " ", @f[0,2-4]
Yes, right now any arguments passed would be taken as command after you add "$@" so you can add a case statement alongside shift that would allow only your functions to be taken as an argument. If you want I can show you how.
what is $PS1 set to?
Lots of info missing here. I assume you wanted to customize your PS1 but ran into this? Other assumption is that this is only happening when specific output is generated? The later is easier to answer, in some cases output just doesn't include a newline. If it's the former, please do `echo "$PS1"` and post the output.
Because there's no newline char after the output?
LOLü§£
I assume you just "cat"-ed a file that ends without a newline? If the previously invoked command exits without printing a newline (as cat does), bash doesn't insert one for you before printing its prompt. That would get annoying - you'd have an extra newline most of the time, because most commands print a newline following their output. &amp;#x200B; Bash prints a newline when you provide it a newline char as input - that's it. &amp;#x200B; Of course, if this is happening with all commands, take a look at de\_argh's comment. You might have a misconfig with your prompt.
you are right! I edited the OP
There‚Äôs no newline at the end of the file. Run these commands printf ‚Äúthis has no newline‚Äù | cat -A It‚Äôll have the same behavior as your file/other command printf ‚Äúthis has a newline\n‚Äù | cat -A This will show the prompt on the correct line. At the end of the command you‚Äôll see a \n. The cat -A shows special characters. The dollar sign you‚Äôll see means it‚Äôs the end of the line
Duly noted, thank you.
So I copied the commands you posted into the terminal, but still having the same issue:
That's because bash doesn't check by default if the previous command had a newline at the end (like zsh). I use the following modification of `PS1` in my bashrc (from [this question](https://serverfault.com/q/97503)). $ echo -n Hello Hello$ $ PS1='$(printf "%$((`tput cols`-1))s\r")'$PS1 $ echo -n Hello Hello
&gt;PS1='${debian\_chroot:+($debian\_chroot)}\\\[\\033\[01;32m\\\]\\u@\\h\\\[\\033\[0m\\\]:\\\[\\033\[01;34m\\\]\\w\\\[\\033\[00m\\\]$(parse\_git\_branch)\\\[\\033\[0m\\\] $ '
The \[s\] in "ps -ef | grep \[s\]treamlink" removes the grep part of the output but you're right that pgrep is still better so I'll start using that instead, thanks!
Yikes! OP: http://www.cgi101.com/book/ch6/text.html
I love this sub.
I think you forgot something
Wait... What? I had to think this for 5 minutes, until the penny dropped. Thanks back to you. That's ingenious, and dangerous! What if there is a matching file in the current directory.
That's because their browser/reddit app inserted `‚Äú` and `‚Äù` instead of `"`. printf 'has no newline' | cat -A printf 'has newline\n' | cat -A
Does List::Util have to be installed separately.
Nope, `man perlmodlib`
Ok nice. Then I might have use for this `if any` thing in the future.
You need to use `if`, `elif`, ..., `elif`, `fi` instead of many separate `if`, `fi`.
for anything greater than nine you set `folder` variable multiple times. reversing the checks (from greater to lesser) would do the trick and replace fi if with just `elif`, well, maybe next time you script something.
Aren't the checks running from greater to lesser now?
I might try something like this. case "$((mdur / 5))" in (0) folder=filler5 ;; (1) folder=filler10 ;; (2) folder=filler15 ;; (4) folder=filler20 ;; (5) folder=filler25 ;; (6) folder=filler30 ;; (7) folder=filler35 ;; (8) folder=filler40 ;; (*) folder=superlong ;; esac
sorry, I fixed that, typo
But we don't have a filler25, filler35, or filler40.
You can combine cases like so. (4|5) folder=filler20 ;;
I'll explain a bit: for `$mdur` greater 4 the last check is true, so no matter how high the value of `$mdur` the last thing you do to `$folder` is set it to 'Filler10', so you always end up with this value in it.
This almost works, except for 2 things: superlong - 00:17:17 - Burlesque - A Bedroom Fantasy - Lili St Cyr Shouldn't that go into filler20? And then the script crashes out shortly after with this: util/sortcontent.sh: line 20: 08: value too great for base (error token is "08") Line 20 is: case "$((mdur / 5))" in
The way you have it written, you will go through every single if statement and reset folder each time it matches. If you really want to keep it inefficient then just add exit 0 every time you set the folder. &amp;#x200B; But since you mentioned that Filler10 is being selected most of the time, I have a feeling your mdur variable isn't being set correctly. &amp;#x200B; Once you figure out the mdur issue, I'd write the code something like this. I only added the less than block to the last elif statement as an example of how to do all of them. if [ "${mdur}" -gt 44 ] ; then folder="Filler60" elif [ "${mdur}" -gt 29 ] ; then folder="Filler45" elif [ "${mdur}" -gt 19 ] ; then folder="Filler30" elif [ "${mdur}" -gt 14 ] ; then folder="Filler20" elif [ "${mdur}" -gt 9 ] ; then folder="Filler15" elif [ "${mdur}" -gt 4 ] &amp;&amp; [ "${mdur}" -le 9 ] ; then folder="Filler10" fi
Here's how I'm getting $mdur: tdur=`ffmpeg -i "$file" 2&gt;&amp;1 | grep "Duration"| cut -d ' ' -f 4 | sed s/,//` hdur=`echo "$tdur" | cut -f1 -d:` mdur=`echo "$tdur" | cut -f2 -d:` Yes, I'm sure there's more efficient ways to do this, but I need the other variables later on.
Add a set -x on the 2nd line of the script and see what the output shows.
I can't do anything with a single line. your mdur is probably not the duration in minutes so that's probably why it doesn't work.
Better yet, just use sshguard or fail2ban.
Nothing is simple ;) Instead check "fail2ban", it does exactly what you want.
In principle, you'd need to a) know which logs to read for failed ssh logins, b) know which keywords to search in those logs, c) know how to extract its address from those logs for failed logins, d) know how to add an iptables rule to block that ip address when the threshold is hit and e) eventually remove that iptables rule after the lockout period is complete. In practise, fail2ban does all of this very well and is very mature, but trying to implement your own version of that in any language might be a good way to learn about various OS subsystems.
That fixed it! Here's the final code block: case "$(( ${mdur#0} / 5 ))" in (0) folder=Filler05 ;; (1) folder=Filler10 ;; (2) folder=Filler15 ;; (3) folder=Filler20 ;; (4|5) folder=Filler30 ;; (6|7|8) folder=Filler45 ;; (*) folder=Filler60 ;; esac Tested against 1,258 files, and no failures!
A simple solution to a problem is the "reset" command, but I am now curious too why terminals break sometimes.
You could so do a "simpler" method with but IPTABLES. Reduce your max she attempts per connection to 1 or 2. Then use IPTABLES to track the number of new connections. After x new connections in x secs. Block the IP for a mins/hrs, now escalate and repeat, each time increasing the block time. Now the fun part is setting up the rule to not count established connections.
The command lastb shows the relevant information, I just don't know how to organize it unfortunately.
How would I go about this?
If my assignment wasn't to do the bash script, I'd do just that,
It does! Unfortunately I have to do a bash script for this.
\#!/bin/bash \## \## What Network card is connected to your WAN / Internet \## DEVICE\_WAN=eth0 \## \## Put in here remote ipaddress that you trust and never want to be locked out \## This can be a string of ips sperated by spaces OR a path to a file with 1 ip per line TRUSTED\_REMOTE\_SERVER\_IP="111.111.111.111 222.222.222.222" \## \## Location of iptables \## IPTABLES=/sbin/iptables \## \## In your log files how do you want to defind a iptables log \## This will make grep'ing alot easy'er later LOG\_TXT="IPT" \## \## Iptables logging options \## I dought you really need to change these. LOG\_IT="LOG --log-ip-options --log-prefix" \## \## Short Hand Vars STATE\_NEW="-m state --state NEW" RECENT\_SET\_NAME="-m recent --set --name" RECENT\_UPD\_NAME="-m recent --update --name" RECENT\_CHK\_NAME="-m recent --rcheck --name" LOG="-j $LOG\_IT" &amp;#x200B; &amp;#x200B; echo "SSH lockdown &amp; port scan hider" $IPTABLES -N Ssh\_block\_5min $IPTABLES -F Ssh\_block\_5min $IPTABLES -A Ssh\_block\_5min $LOG " :${LOG\_TXT}:Ssh:Blocked 5 min: " $IPTABLES -A Ssh\_block\_5min -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_block\_1hr $IPTABLES -F Ssh\_block\_1hr $IPTABLES -A Ssh\_block\_1hr $LOG " :${LOG\_TXT}:Ssh:Blocked 1 hour: " $IPTABLES -A Ssh\_block\_1hr -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_block\_1day $IPTABLES -F Ssh\_block\_1day $IPTABLES -A Ssh\_block\_1day $LOG " :${LOG\_TXT}:Ssh:Blocked 1 day: " $IPTABLES -A Ssh\_block\_1day -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_block\_1wk $IPTABLES -F Ssh\_block\_1wk $IPTABLES -A Ssh\_block\_1wk $LOG " :${LOG\_TXT}:Ssh:Blocked 1 week: " $IPTABLES -A Ssh\_block\_1wk -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_block\_1mo $IPTABLES -F Ssh\_block\_1mo $IPTABLES -A Ssh\_block\_1mo $LOG " :${LOG\_TXT}:Ssh:Blocked 1 month: " $IPTABLES -A Ssh\_block\_1mo -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_block\_lifer $IPTABLES -F Ssh\_block\_lifer $IPTABLES -A Ssh\_block\_lifer $LOG " :${LOG\_TXT}:Ssh:Blocked 4 life: " $IPTABLES -A Ssh\_block\_lifer -p tcp -j REJECT --reject-with tcp-reset &amp;#x200B; $IPTABLES -N Ssh\_blacklist $IPTABLES -F Ssh\_blacklist $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_5min $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_1hr $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_1day $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_1wk $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_1mo $IPTABLES -A Ssh\_blacklist $RECENT\_SET\_NAME ssh\_blacklist\_lifer &amp;#x200B; $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_lifer --hitcount 12 -j Ssh\_block\_lifer $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_1mo --seconds 2419200 --hitcount 11 -j Ssh\_block\_1mo $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_1wk --seconds 604800 --hitcount 9 -j Ssh\_block\_1wk $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_1day --seconds 86400 --hitcount 7 -j Ssh\_block\_1day $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_1hr --seconds 3600 --hitcount 5 -j Ssh\_block\_1hr $IPTABLES -A Ssh\_blacklist $RECENT\_UPD\_NAME ssh\_blacklist\_5min --seconds 300 --hitcount 3 -j Ssh\_block\_5min &amp;#x200B; $IPTABLES -N Ssh\_block $IPTABLES -F Ssh\_block $IPTABLES -A Ssh\_block $LOG " :${LOG\_TXT}:Ssh:Blocked 2 fast: " $IPTABLES -A Ssh\_block -p tcp -j Ssh\_blacklist &amp;#x200B; $IPTABLES -N Ssh\_allow $IPTABLES -F Ssh\_allow $IPTABLES -A Ssh\_allow $LOG " :${LOG\_TXT}:Ssh:Allowed: " $IPTABLES -A Ssh\_allow -j ACCEPT &amp;#x200B; $IPTABLES -N Ssh\_almosted\_allow $IPTABLES -F Ssh\_almosted\_allow $IPTABLES -A Ssh\_almosted\_allow $STATE\_NEW $RECENT\_SET\_NAME Ssh\_almosted\_allow $IPTABLES -A Ssh\_almosted\_allow $STATE\_NEW $RECENT\_UPD\_NAME Ssh\_almosted\_allow --seconds 60 --hitcount 3 -j Ssh\_blacklist $IPTABLES -A Ssh\_almosted\_allow -j Ssh\_allow &amp;#x200B; $IPTABLES -N Ssh $IPTABLES -F Ssh $IPTABLES -A Ssh -m state --state ESTABLISHED,RELATED -j ACCEPT $IPTABLES -A Ssh -m state --state NEW $LOG "${LOG\_TXT}:Ssh:Attemp: " &amp;#x200B; if \[ -e "$TRUSTED\_REMOTE\_SERVER\_IP" \]; then echo " =&gt; Loading From File : ${TRUSTED\_REMOTE\_SERVER\_IP}" TRUSTED\_REMOTE\_SERVER\_IP=\\cat ${TRUSTED\_REMOTE\_SERVER\_IP} | sort -u | tr -d "\\r" | tr "\\n" " "\`\` fi &amp;#x200B; \# Allow from trusted Sites (avoids any ssh lock outs) for IP in $TRUSTED\_REMOTE\_SERVER\_IP; do echo " =&gt; Trusting : ${IP}" $IPTABLES -A Ssh -s $IP -j Ssh\_allow done &amp;#x200B; $IPTABLES -A Ssh $STATE\_NEW $RECENT\_SET\_NAME Ssh $IPTABLES -A Ssh $STATE\_NEW $RECENT\_UPD\_NAME Ssh --seconds 1 --hitcount 2 -j Ssh\_block $IPTABLES -A Ssh $STATE\_NEW $RECENT\_UPD\_NAME Ssh --seconds 2 --hitcount 2 -j Ssh\_block $IPTABLES -A Ssh $STATE\_NEW $RECENT\_CHK\_NAME Ssh --seconds 4 --hitcount 2 -j Ssh\_almosted\_allow $IPTABLES -A Ssh -j DROP $IPTABLES -A INPUT -i $DEVICE\_WAN -p tcp --dport 22 -j Ssh
Oh wow that is the full script, I am going to need a few hours to study it before I can give any type of response. Thank you so much for your help.
Sorry if I missed anything. I had to RDP(ish) from phone to my desktop so I could copy paste from my script to the here. As trying to C&amp;P all that from by phone was not working so well ;)
Did you just write that??
Good to hear.
CSVTOOL ( [https://github.com/Chris00/ocaml-csv](https://github.com/Chris00/ocaml-csv) ). Why reinvent the wheel ?
Also depending on your distro it might already be installed :)
Look up awk examples
I've known it was possible to have the leading `(` for the cases even though it's optional, but I've never seen anyone actually use it. What's the benefit of having it there other than symmetry?
Clever trick. Unfortunately, this does mean that you don't know if output has a newline or not just by looking at the prompt, but it's not like you need to find that out very often. I just have a newline at the beginning of my [prompt](https://github.com/tpenguinltg/dotfiles/blob/01df48e4afe64f8c7231341e3c3323eca553e227/.bashrc#L73), which means I have a blank line for most commands. This works for me, though, because I have a two-line prompt and the first one is ridiculously long, so it's nice to have that separation.
I just think it looks better. I don't think there is any actual benefit to it.
Is there an option to set line breaks before each prompt? Say what if I want 5 line breaks before each prompt no matter what the output to previous command was!
Make it a table... declare -a durations=( 44 29 19 14 9 4 ) declare -a folders=( 60 45 30 20 15 10 ) folder= for ((i = 0; i &lt; ${#durations[@]}; i++)); do if (( $mdur &gt; ${durations[$i]} )); then folder="Filler${folders[$i]}" break; fi done echo mdur=$mdur folder=$folder
I looked at your script for the past few hours but I am concerned it is past the scope of this assignment, it's just an introduction to Linux class. The assignment suggests to utilize the command lastb and lockout ip addresses with 3 or more failed attempts in 60 seconds. However, I don't even know how to take the information from lastb and utilize it properly.
Sorry, just seeing this now under a pile in my inbox. Can you please fill in some blanks where I am missing the interpretation: ipmi-sensors |awk -F\| ' execute ipmi-sensors, pipe into awk and we're looking at fields?...I don't understand the `|` and `'` $2~/Fan|Ma?rgi?n|Temp/ &amp;&amp; $4+=($2~/Ma?rgi?n/?90:0) { Second column, look for Fan, Margin, Temp, if you find those, take the 4th column, and tack 90 onto anything matching `Mrgn`with a continuation bracket....so, if we pause here...can you help me with all the formatting on that line...the slashes, tildas, ?, |, :, etc...that whole thing blows my mind here. (MATLAB is my jam) sub(/Ma?rgi?n/, "Temp", $2) Take anywhere there is the word "margin" or variation of it...change it to "Temp" in the second column? again...help on formatting there... printf("%-48s%4d%s\n", $1$3$2, $4, $5) Fuck me....okay...print 48 characters in a string, 4 in a decimal and an unassigned string length with a new line, column 1, 3, 2, 4, 5 }' |sort -k2,2 -k1,1 .....I'm out. Really, you've been a huge help, me sticking this into a script and hitting it with `watch` is phenomenal. If you have time and can correct me and explain a bit more, that would be awesome. Again, I already thing you guys in /r/bash go way above and beyond any other 'language' subreddit. bash &gt;&gt;&gt;&gt;&gt; python help
I'd start with `man lastb`
Okay, I did that. There isn't a whole lot the command can do to be more or useful.
what did you see as options that might be useful ?
I didn't find anything that would be useful when. Is there something that you saw that I may have missed?
there are a lot of options with time. maybe create your new user in another terminal window first and then put them in loop trying to login. This way you can see how the fails will show in the logs.
Lastb by itself shows time and ip address which is enough. Already had another vm fail a few Ssh attempts. The issue is, how does one utilize this information in a meaningful way?
* the time options in lastb can help you limit your logs to a minute time span. * you can count the occurrences of an address in a limited span * If the occurrence meets the threshold, * append the address to deny.hosts * restart the daemon for it to take affect *(there are probably nicer ways to do this now, but that's how I think it had to be done once upon a time)*
I understand, that is an excellent way of doing it. How can I count the occurrences of an ip address? I think the rest of it, I can do.
create a regex pattern to recognize ip addresses create a list with unique values and a list number of occurrences
&gt; guys in r/bash go way above and beyond Fuck yeah we do! It's because of love. I'll break this down by line. --- * `ipmi-sensors |awk -F\| '` `'strong quotes'` prevent Bash from interpreting special characters within them. It's the same as `\e\s\c\a\p\i\n\g` every character. We can't use `awk -F|`, because the pipe will be interpreted by the shell. The unmatched `'` is the beginning of a script we want to be passed literally to awk. * `$2~/Fan|Ma?rgi?n|Temp/ &amp;&amp; $4+=($2~/Ma?rgi?n/?90:0) {` This is our conditional, implying `IF condition AND condition THEN`. First condition matches regex against second column, like you said. The second condition contains its own compact if statement, called a [Ternary Operator](https://en.wikipedia.org/wiki/%3F:#AWK). In English: &gt; "Add a value to Column 4 that is either 90 or 0 depending on whether Column 2 matches this regular expression. Return true if Column 4 exists and the added value is an integer." `{` designates the beginning of an action block. All code within this block will be executed against every line that matches our above conditions. * `sub(/Ma?rgi?n/, "Temp", $2)` `sub(regex, string, target)`: Within Column 2, replace the first occurrence of `/Ma?rgi?n/` with the string "Temp". * `printf("%-48s%4d%s\n", $1$3$2, $4, $5)` Print a string padded (with spaces) to 48 characters, followed by a number *right-justified* to 4 digits, followed by a raw string, followed by a newline. `%-48s` will be the concatenation of Columns 1, 3, and 2, `%4d` will be Column 4, and `%s` will be Column 5. * `}' |sort -k2,2 -k1,1` `}'` designates the end of the action block, following by the end of the Awk script. At this point, the output is unsorted. Sorting things in Awk is more complex than piping everything to `sort`. The `-k` options mean "sort by the second column, then sort by the first". By default, `sort` will use whitespace as a separator, which is why we rearranged our columns with `printf`.
I dont know what a regex pattern is, or how to create one. How would I learn this, or could you show a sample that explains how this works?
[https://stackoverflow.com/questions/14928573/sed-how-to-extract-ip-address-using-sed](https://stackoverflow.com/questions/14928573/sed-how-to-extract-ip-address-using-sed)
Perhaps do the following which uses the length of the array to check: if (( ${#files_preview[@]} == 0 )); then ... About why this might be a better method, in theory your current method can go wrong. When you use `$files_preview`, you are looking at the content at position zero in the array. In bash an array can have "holes". You can for example have an array with three elements that are saved in spot 1 and 2 and 5, with spots 0 and 3 and 4 being unused and empty. Here's an example at the command line showing how your current method fails: $ x=(a b c) $ unset x[0] $ echo "${x[@]}" b c $ echo ${#x[@]} 2 $ echo "$x" $ : "${x:="empty"}" $ echo $x empty $ echo ${x[@]} empty b c
So lastb displays the fallowing columns (on Debian) `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` Which breaks down to `Username service Remote IP Date` So now you need to find a way to limit your output to just "now" or this minute (if I'm understanding your assignment correctly). So lastb has the following options. `-&lt;number&gt; how many lines to show` `-a, --hostlast display hostnames in the last column` `-d, --dns translate the IP number back into a hostname` `-f, --file &lt;file&gt; use a specific file instead of /var/log/btmp` `-F, --fulltimes print full login and logout times and dates` `-i, --ip display IP numbers in numbers-and-dots notation` `-n, --limit &lt;number&gt; how many lines to show` `-R, --nohostname don't display the hostname field` `-s, --since &lt;time&gt; display the lines since the specified time` `-t, --until &lt;time&gt; display the lines until the specified time` `-p, --present &lt;time&gt; display who were present at the specified time` `-w, --fullnames display full user and domain names` `-x, --system display system shutdown entries and run level changes` `--time-format &lt;format&gt; show timestamps in the specified &lt;format&gt;:` `notime|short|full|iso` `-h, --help display this help and exit` Out of these options you need to see if any of them "meet" your current goal OR at least help you get closer. In this case we "could" get away with using either `-s, --since &lt;time&gt; display the lines since the specified time` `-p, --present &lt;time&gt; display who were present at the specified time` Both can be used to limit your output to just the time you want to filter thru. But both require a time, man lastb tells us the time formats can be `TIME FORMATS` `The options that take the time argument understand the following formats:` `YYYYMMDDhhmmss` `YYYY-MM-DD hh:mm:ss` `YYYY-MM-DD hh:mm (seconds will be set to 00)` `YYYY-MM-DD (time will be set to 00:00:00)` `hh:mm:ss (date will be set to today)` `hh:mm (date will be set to today, seconds to 00)` `now` `yesterday (time is set to 00:00:00)` `today (time is set to 00:00:00)` `tomorrow (time is set to 00:00:00)` `+5min` `-5days` Now the last to options tell us we can have lastb "figure out the time" for us. Which means we could use `-1min` which would tell lastb to give us all the failed login attemps within the last min from now (this min). &amp;#x200B; So something like `lastb -s -1min` will show us all the logins within the last min from "now". &amp;#x200B; Now we want to also make sure that lastb only give us the IP of the remote device which means we'll also want to the option `-i, --ip display IP numbers in numbers-and-dots notation` Which gives us the full command of `lastb --ip -s -1min` (personally I'd use -2min instead of -1min would only give you logins from this last min from this min where -2min would be "closer" a minute from now) or in other words if my logins looked like this `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` and the min now is 07:25 -1min would only show us `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` `ib ssh:notty` [`10.0.0.99`](https://10.0.0.99)`Sat Apr 13 07:43 - 07:43 (00:00)` That's because lastb looks at the min mark and not at 60 secs from now. It all comes down how close to with in a min your looking for. &amp;#x200B; Now you need to "select" out just the needed info.
Permissions on the file. Make sure the execute bit is set for the owner (at a minimum).
in the first instance, 'bash' is being executed with a parameter of 'script.sh'.. the script does not need exec permission because bash is executing the commands inside the script file.. in the second, 'script.sh' is being executed.. if it has exec permission..
this is one script with permission: \-rw-r--r-- if i add chmod + x ./ works also but bash works without +x permission...
when i use bash it is being executed inside like read only mode like cat? what is safer?
Just some notes after i read your post: a) /var/log/wtmp is being read by "last" command b) lastb parses /var/log/btmp c) /etc/hosts.allow|deny have meaning only if you use inetd/xinetd or in case you don't use any of the super-servers, sshd must have been compiled with libwrap.a
as far as i know, both methods are exactly the same..
Right because "bash" is a shell interpreter your telling "bash" to run the script in its shell the same as doing "ksh" and the script. When the script is executable and the first line is #!/bin/bash Your telling the script on execute run in the bash shell.
Right because "bash" is a shell interpreter your telling "bash" to run the script in its shell the same as doing "ksh" and the script. When the script is executable and the first line is #!/bin/bash Your telling the script on execute run in the bash shell.
You can have a multiline bash prompt, and any of those lines can be empty
I would use `set -- *.$suffix` instead of the loop (with bash you can probably also field split directly into an actual array). You only need to make sure the file exists if there is only 1 element, because the positional parameters get set to "*.MTS" if there are 0 files with that extension in a folder. if [ $# -eq 1 ] &amp;&amp; [ ! -e "$1" ]; then echo "There are no files..." exit 1 fi
Ok i understand but why this happen even on 0 permission ? [https://i.imgur.com/lzKd3x8.png](https://i.imgur.com/lzKd3x8.png)
ok i understand but why this is happen even on 0 permission [https://i.imgur.com/lzKd3x8.png](https://i.imgur.com/lzKd3x8.png)
Cygwin is decent if there‚Äôs nothing else, but it uses windows pipe behaviors and you need to select a lot of additional packages if you want a more complete experience. You can also use something like virtualbox to run a small Linux vm, which will a much better learning experience.
There's a lot of Windows binaries for additional commands available, but there's not really an easy way to add them to Git's bash implementation *without cygwin* anyway--I'm currently trying to get ``rsync`` and all of its' dependencies in a portable GitBash for a project at work... At work I use GitBash, and I'm always wanting to go grab a package or two (or 1000) to solve some problem I'm working on--but its not something that's easy or straightforward within that application. On my personal machines, I use Cygwin, as it allows me to install additional packages--albeit, with the GUI installer. Depending on what you're doing, cygwin may be the better tool for the job.
If you happen to run Windows 10, I would use WSL. Otherwise MSYS2, which I think is better than cygwin. I would assume Git BASH actually is only the bare minimum of commands needed for git (pager, diff, etc.).
 (( ${#files_preview[@]} )) &amp;&amp; echo "Depth is greater than zero"
Root can read/write a file even without permissions. You should also be able to do "cat scripts.sh" as root and it should work with no permissions.
ls -al /bin | grep bash U have executable permission for bash
/r/BashOnUbuntuOnWindows
well this is unsecure others can use bash to execute scripts...
ok but bin/bash have +x for others also [https://i.imgur.com/TWmr9Zu.png](https://i.imgur.com/TWmr9Zu.png)
&gt; if (( ${#files_preview[@]} )); then This should be enough.
ok but +x is for others also [https://i.imgur.com/TWmr9Zu.png](https://i.imgur.com/TWmr9Zu.png)
`/bin/bash` has execute permissions. That's why you can execute it with `script.sh` as a parameter. `script.sh` doesn't have execute permissions, so you can't execute it.
thanks
 (( ${#files_preview[@]} )) || echo "array is empty" --- `(( foo ))` returns true if 'foo' is an integer greater than zero. `||` performs the command on the right if the left is false.
mayby this is stupid but if we create one group for example admins add add root and admin user account to it and we remove -x from other in /bin/bash ? This can increase level of security?
A shell needs to be set for users to interact with the machine. If your operating system is up to date with security patches, giving a user a shell should not be an issue (look at the read/write permissions of /). However, once you start modifying the system, for example adding a cron job, you introduce attack vectors. Basically it is up to the sys admin to keep the system secure after the initial installation. If you are really concerned, you can give users a ‚Äúrestricted‚Äù shell, a custom program that only exposes certain commands. However, restricted shell can be just as insecure.
No no no my dude! You just did they're homework for them! No one else posted a script on here for OP because everyone realized OP is just lazy
What are you trying to do?
Not at all, this script uses nothing the OP is so-post to use. It's a complete example thinking outside the box to accomplish a similar goal.
just want to understand permission in linux. &amp;#x200B; Like i said earlier What do you think about this: 1. change group of /bin/bash to admins 2. user (admin account) and root are in group admin 3. remove -x on bin/bash to prevent use bash for anyone (except root and user)
What you write is the same as `!= 0`, not `== 0`. I chose `(( ${#x[@]} == 0 ))` because I think it's easier to see what's happening than when writing `(( !${#x[@]} ))`. I was worried the `!` would be easy to overlook because of all of those other weird characters that show up around the variable name (I mean the `${#[@]}`.
Do not remove +x on `/bin/bash`. Bash is used by everything. You're likely to end up with a system that can't boot. Removing +x doesn't really do much for security. The file is still readable. Nothing prevents a user from copying it somewhere else and adding +x again. You'd also have to make it unreadable and mount everything the user has write access to as `noexec`. And even then, there's nothing stopping the user from just typing whatever was in the script in the shell. I think you're going down a rabbit hole with the "what if I do XYZ?" type of question. There's an infinite number of things you could do, but most of them are things that nobody would do intentionally. Instead, come up with what you want to achieve and figure out the ways to do it.
You're correct. My mistake.
All I want is to learn some of the basics. All of the file management commands work on Git BASH but id like to learn more.
ops thanks for advice! + for you!
"Others" could include system processes, install scripts or other things.
You can also remove the execute but of others and group chmod -x
I thought all strings had a numeric value of zero. What's an example of a string that's greater than 100?
Git SCM is awesome. We'd like to see actual error though.
For learning purposes, I‚Äôd recommend setting up a VM with Linux on it, as there‚Äôll be less of an ‚Äúoops‚Äù possibility. The thing with Cygwin is that it doesn‚Äôt come with a lot of the tools Git does‚Äîlike, they‚Äôre available to add to it; but they don‚Äôt come OOTB like the do in Git...like ``ssh`` iirc...though, they‚Äôre all available via the installer (which is really a GUI package manager of sorts). What all are you trying to do that isn‚Äôt working in the vanilla Git Client?
Cygwin is pretty good if you‚Äôre trying to learn bash, and the POSIX ecosystem (grep, sed, awk, cut, tee, etc). Just be sure to add the git package and you can toss git bash. One thing to note as well - Cygwin is slower than what you‚Äôll experience on an equivalent Mac or Linux system. You might also want to check out the Windows Subsystem for Linux. I‚Äôve not used it, but it seems pretty solid: https://docs.microsoft.com/en-us/windows/wsl/install-win10 Bash can be challenging at times, but don‚Äôt give up - it‚Äôs a skill that will still be paying off 20 years from now.
The ones that I remember trying yesterday were the man, tree, nedit, and password commands.
So I have a similar question as OP. It's for my compture security class. I only have to crack a two character long pw encrypted in ECB. It's using {A..Z}{a..z}{0..9}. Instead of generating a pw dictionary I decided to try to create pw as I went. I modified your code, but after it runs for a little while all I've successfully done is open openssl in the console. For reference: for c1 in {A..Z}{a..z}{0..9}{A..Z}{a..z}{0..9};do ((counter++)) if openssl enc -d -DES -ECB -base64 \-pass pass:$c1 \-in file11.cipher -out fil11cracked.txt then break else echo $counter $c1 fi done &amp;#x200B; I'm an amateur with bash. Any help appreciated.+
We use `\` before the end of a line to extend that line. You've removed those escapes, so Bash thinks `openssl`, `end -d -Des -ECB -base64`, etc. are different commands. Your script is running openssel with no options, then waiting for input.
Okay makes sense. Does my logic check out for ECB? Should I use -K instead of -pass?
'bash script.sh' is essentially the same as 'cat script.sh | bash' As long as you can read the scripts you can try and execute the commands in the script. Regarding your concerns about security of bash having x bit set for others, just because a user can execute a script, does not necesserly mean they have permision to run all pf the commands in the script... ie: If there are executables called in the script that the user does not have access to execute, that will still fail. Similarly, if the script tries to modify files that the user running the script doesn't have access to otherwise read/write, that will fail as well.
Yeah, a lot of those things aren‚Äôt built in, because they aren‚Äôt needed for git‚Äîthe whole point of that application. You can try dealing with Cygwin MYSYS (or whatever); but having a VM with bash a core utilities installed is gonna provide a much better learning environment; vs trying to find and add Windows ports of all those tools and docs.
As a Win10 Admin I agree with this. WSL is the easiest way to experience Linux and it doesn‚Äôt require any VM knowledge. Not to say that VM‚Äôs are hard but WSL is ridiculously easy and really low level. Get stuck in the app? Just reset the app. I use Arch BTW.
Yes but that isn't the point. Bash has +x bit so anyone can run bash. But, script.sh doesn't have +x so when you try to execute the file it is denied.
Yes, you can do that but you're making the system incredibly more complicated for very little return on security.
dziƒôki dobrze wiedzieƒá.
[...again](https://i.imgur.com/pIj2FdH.png)
WSL is fantastic.
yes, fail2ban is quite easy to set
OP: I saw this question and your [other post](https://www.reddit.com/r/bash/comments/bcjjha/trying_to_make_a_simple_script_that_locks_out_an/). WTF course is this 'introduction to Linux class' where you have exercises like this? Coming from a real-world standpoint this exercise breaks two golden rules: * Do not re-invent the wheel * Do not implement your own security software All software has bugs. Generally the more it is used, the more those bugs get worked out. Battle tested public code is ideal. Your own code is likely to fail somewhere and you really don't know how, when or why until that happens (you may even not know _when_ it happens). Operationally it is wasted effort to reimplement something that already exists- in this case fail2ban or sshguard - without some sensible rationale. Both of the above apply in multiple to security software. Trying to implement this yourself is a **really** bad idea. There is a lot of really subtle stuff that you won't even be aware of to consider. You can very easily backdoor yourself or similar. IMO this sort of exercise (unless it is has been significantly misrepresented) is: 1. Way, Way beyond an 'introduction to Linux'. It's certainly beyond, for example RHCSA (a recognised course which covers linux, ssh and security and is a bit beyond 'introductory') 1. An example of bad practice in duplicating prior art and especially with security software. I'm genuinely curious who is offering it
MobaXterm
Not nomad, but lastb isn't working for me here. The output is garbled hard core. It also thinks the auth.log file was created in 2028. There's nothing wrong with my clock. Another possibility is to locate the file that SSH logs and use grep, cut, and sed; or awk to parse it.
Regex means "regular expression". You don't need to worry about learning regex right now when you're trying to build your bash script, so I'm going to provide an already created one for you and explain the syntax. If you like this pattern recognition you can learn more about regex later. Also note this only works for IPv4, not IPv6: (\^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$) &amp;#x200B; `()` means "match everything inside. `^` means "from the start of the string". `$` means "at the end of the string". `\d` means "match with any digit (0-9). `{1,3}` means "match for any amount of characters between 1 character and 3 characters". `\.` In Regex, the "." character means "match for a *single* character". The backslash `\` overloads this and allows us to match for a dot. So in total this regular expression is saying "match exactly from the beginning of the string for any text that contains between 1-3 digits and is between three periods". &amp;#x200B; So examples like "[1.1.1.1](https://1.1.1.1)" and "[8.8.8.8](https://8.8.8.8)" will work but examples like "192.1680.0.1" won't work nor will "A.B.C.D". &amp;#x200B; You can play around with test examples and edge cases using [Regex101](https://regex101.com/) . &amp;#x200B; Also my example will break for cases like 123.456.789.0 (456 and 789 are not valid ranges but they are digits between 1 and 3 characters \[in this case exactly three characters\]). But for your purposes from the values you're getting back, btmp would never log an invalid IP address like the 123 example I gave.
Of course this example breaks those golden rules. This is for an educational exercise. How is a sysadmin supposed to better understand *how* tools like sshguard work if the sysadmin doesn't know about /var/log/btmp or /var/log/wtmp? I will agree with you that this exercise is **way** beyond an introduction to Linux exercise. But there is absolutely nothing wrong with reinventing the wheel for educational purposes. In fact it's greatly encouraged because we can rely on documentation and similar examples/exercises to better understand how something works.
checkout the following: $ strace bash script.sh $ strace ./script.sh $ chmod +x script.sh $ strace ./script.sh $ chmod -x script.sh $ strace bash -c './test.sh' The command `strace` shows the system kernel calls the process is making. Check out the man pages for more details on strace or the kernel calls being made. The last command contains the permission denied error when invoking execve: execve("./test.sh", ["./test.sh"], [/* 42 vars */]) = -1 EACCES (Permission denied)
./script uses the shebang'ed interpreter whereas bash script forces bash interpreter. This matters when using non-POSIX features such as set -e, where a script doesn't continue processing instructions in the event of an error. Dot slash is not available on COMSPEC (defsult) Windows environments. bash may not be available on all environments; many use a more pared down or performant shell like mksh, ash, or even a non-POSIX shell like csh or PowerShell. Dot slash requires executable permissions to be set on the file. Note that executable permissions tend to be dropped when sharing files across network connections, unless you are using particularly UNIX suited mechanisms like scp, rsync, ftp, git, tgz AND the destination file system supports UNIX file permissions. Whereas bash script just runs the script regardless of file permissions. Both styles have prons and cons, it's unfortunately very situational dependent which is better to use. In some ways, using a pure `#!/bin/sh` shebang with executable bits and placing onto $PATH, is the most portable setup. Even when executable bits and shebangs are included, I tend to invoke scripts as bash script, from Makefile contexts, in order to have my Makefiles run more reliably on Windows environments. Again, this sacrifices the ability to perform basic checks on your scripts, so no set -E, set -u, set -o pipefail. You can, but expect your script to fail on other machines, because sh does not always expand to a bash interpreter. Or, you could shebang `#!/bin/bash` and limit your scripts to using only features available in bash v3 and below. Then your scripts are more likely to ron on any given machine, as long as bash is available. Some may prefer mksh, posh, dash, or ash as more performant alternatives. ... shell just sucks, write a dedicated application in Go/Ruby/Python/whatever instead. You'll end up with far fewer latent logic bombs and corner cases.
`cron` doesn't natively support seconds level resolution. You probably need to use something like `/bin/sleep 50` in the `cron` job, or within the script you're calling.
``` * * * * * ( sleep 50 ; /path/to/executable param1 param2 ) ```
So, [from the Arch wiki](https://wiki.archlinux.org/index.php/Sshguard) &gt; sshguard is a daemon that protects SSH and other services against brute-force attacks, similar to fail2ban. &gt; sshguard is different from the latter in that it is written in C, is lighter and simpler to use with fewer features while performing its core function equally well. &gt; sshguard is not vulnerable to most (or maybe any) of the log analysis vulnerabilities that have caused problems for similar tools. [Fail2ban is written in python](https://github.com/fail2ban/fail2ban) Both are run as daemons, e.g. from systemd, which is non trivial for a novice to set up I disagree that this is a sensible exercise. Teaching people to do something badly does not teach them to do it well. Something like having the student's 'script' scan and log to a file should be more than enough (and still far beyond introductory level). Having it actually manage connections attempts is ridiculous from any perspective.
Ooops! My typo :D What is param1 and param2?
Yeah
Theoretical command line arguments for the fictional /path/to/executable program.
I see. Should the first star not be a 1 so it knows to run every minute and then wait 50 seconds each time?
If the first number was a 1 it would run on the first minute of every hour, once per hour. Star means every minute. By the same token a 1 in the second column (hours) would mean only run during 01:00-01:59. So `18 1 * * *` means ‚Äòrun once, at 01:18, every day‚Äô. Whereas `* 1 * * *` means ‚Äòrun once per minute, but only from 01:00-01:59, every day‚Äô. Quite why you‚Äôd want to do that I don‚Äôt know, but the `cron` syntax allows it.
I think you're misunderstanding the crontab format. A star in the first slot means "every minute". A 1 in the first slot instead means "on minute 1", i.e. 00:01, 01:01, 02:01,.
Or use a systemd timer with `OnCalendar=*:*:50` (and `AccuracySec=1us`).
Could you use shopt nullglob or failglob ?
&gt;(( ${#files\_preview\[@\]} )) || echo "array is empty" Cheers! I went with this. ``` ((${#files_preview[@]})) || (echo "${red}There are no files with the container $suffix${normal}" &amp;&amp; exit 1) ```
Start your script with ‚Äúsleep 50‚Äù?
Start at the beginning.... What does ${newPort} -le 0 and ${#newPort} -ge 6 test for? Don't just answer what you think... read the [manual](https://www.gnu.org/software/bash/manual/html_node/Bash-Conditional-Expressions.html) or a [guide](https://www.tldp.org/LDP/abs/html/tests.html) and answer
&gt;${newPort} -le 0 Checks if the value I insert as new Port is less or equal to 0 (?) &gt;${#newPort} -ge 6 Checks if the number of digits of $newPort is greater or equal to 6. &gt;Also, your definition of a port needing to be &gt; &gt;needs to be less or equal to 5 digits &gt; &gt;is wrong... I wasn't trying to give a definition, just to verify them on the technical side... I make confusion between: (|| and -o) and (&amp;&amp; and -a). I'm used to code with C++ and I think here those operands just work differently.
If you don't want to make a script (file) executable but still be able to run it when you want then you can do &amp;#x200B; bash [script.sh](https://script.sh) &amp;#x200B; This will call bash to run the script without it having +x
You should use -o instead of ||. Or better yet, use `[[ ... ]] || [[ ... ]] || ...`. The lsof call should also be outside of the test. I'd do: until [[ $newPort -ge 0 ]] &amp;&amp; [[ $newPort -le 65535 ]] &amp;&amp; lsof -i :$newPort &gt;/dev/null; do ... done
 perl -e 'use POSIX; tcflush(0, TCIFLUSH);' Better options must exist
Ty veeery much! Is perl a standard package for linux distros?
I don't know for sure. *while read -t 0 foo; do done* might be an option too, but it doesn't feel right either.
"until" is kind of the reverse of "while", didn't know this iteration. However, thank you very much! At first I couldn't figure out why this didn't work, but then I realized I had to put a '!' in front of lsof, because it's supposed not to recognize the port as in use :)
I think you need `bind -m vi-command '"J": history-search-forward'`. But when i try that there seems to be a bug(?) that only searches for the length-1 of characters (since the cursor goes backward one space when entering vi command mode).
Do you want to write the script with some guidance or get something handed to you on a plate for whatever reason (can't google / too lazy / other)?
Guidance so i can learn about bash
Well, to backup and compress, use `tar` tar -czvf &lt;FILENAME-TO-CREATE&gt; &lt;PATH-TO-FOLDER-TO-BACKUP&gt; replacing `&lt;THESE&gt;` with suitable values. You can create dynamic filenames by using variables or by using the output of commands.. e.g. tar -czvf $(date +%F) would (today) create a file called $ls 2019-04-14 and mix them... tar -czvf $(date +%F).gz would create a file called $ls 2019-04-14.gz As for the second part, `find` which is both more versatile and recommended over the simpler but discouraged `ls` used in day-to-day terminal use find -mtime +14 will list all files older than 2 weeks
Nice idea, unfortunately the 2nd doesn't work... The first works fine, but I have to put it before the input. Not exactly what I was thinking to, but hey it works!
So the syntax above runs it every minute and with the 50 second sleep, it runs it 50 seconds after every minute?
within cron or on its own?
That would work also :D
Yes
Independent of cron.
And then rm them? How tho
You can rm $(find ...) or use [find](https://www.gnu.org/software/findutils/manual/html_mono/find.html)'s `execdir` option (preferable over `exec`) to do it
Note that that won't actually exit the script since you're running it in a subshell. Also, `exit 1` won't run at all if `echo` somehow fails (e.g. if `stdout` is closed) since you're using `&amp;&amp;`, and you might want to output it to `stderr` instead as it's an error. Here's the improved snippet: ((${#files_preview[@]})) || { echo "${red}There are no files with the container $suffix${normal}" &gt;&amp;2; exit 1; }
If you want a "reference script", I've been working on my own backup script for quite some time. I'm making a Borg version too (under the branch `borg`). https://gitlab.com/krathalan/bash-backup-script
While its not history completion as you want, you can enable vi mode for your command line (add 'set -o vi' to your .bashrc or .bash\_profile) and then you can just use ctrl-r and search your history on the cli. I use this all the time and its insanely handy.
This can be done in pure Bash (version 4+) with `read -t 0.01`. If that doesn't get it all, try `while read -t 0.01; do :; done`.
I think I made some cool hacks here: https://github.com/nemoload/backup
The second does the job! Thank you.
Create a list of all files you want to work on and then use that list to do the wiping. For a /bin/sh script, you would use 'find': find /var/log -type f | while IFS= read -r filename; do cat /dev/null &gt; "$filename" done For a bash script, there's a "globstar" feature you could use. It enables a `**` wildcard that does recursion: shopt -s globstar for filename in /var/log/**; do if [[ -f $filename ]]; then cat /dev/null &gt; "$filename" fi done
Would the first work in a bash script as well? Idk much about the difference between sh and bash honestly... Thx for the answer \^\^
Make a function that calls itself for each successive directory, like ```bash clean_logs () { for i in $1; do if [[ -d ${i} ]]; then clean_logs "${i}/*" continue fi cat /dev/null/ &gt; ${i} done } clean_logs "/var/log/*" ``` This way, the original loop doesn't leave the current directory. I don't know if this is best practice; I'm kind of self-taught.
Looks like somebody who knows better got here before I finished typing.
No worries! Your one is more understandable, I'll probably go for that. Thank you for the answer!
`sh` originally referred to the Bourne shell, which Bash is based on. Nowadays, `/bin/sh` is just a symlink to the system's default shell, a portable lowest common denominator so your script can be run no matter what (POSIX) system it's put on, whether that has bash, dash, or what have you for an editor. However, some of bash's powerful features aren't present in other UNIX shells, so you have to write it for `/bin/bash` and sacrifice some portability (admittedly, not much.) In many situations, therefore, there are multiple ways to accomplish things, depending on whether you care about portability or just want it to run with bash.
1. never change directories in scripts if you can suffice without it, work with absolute paths. 2. use `find` instead of globs, more flexibility, less mess with recursion. OLDIFS="$IFS" # store original field separator IFS=$'\n' # find outputs newline separated list, so temporary change field separator to it. # good thing about find is that it prepends search path to found files (by default) # so you work with absolute paths: for log in $(find /var/log -type f -name '*.log'); do [[ -f "${log}" ]] &amp;&amp; cat /dev/null &gt; "${log}" || echo "failed cleaning $log" &gt;&amp;2 done IFS="$OLDIFS" # restore field separator to it's original The field separator magic is not required if you're 100% sure you won't have spaces in file paths.
&gt; (( ${#files_preview[@]} )) || { echo "${red}There are no files with the container $suffix${normal}" &gt;&amp;2 exit 1 } Thanks! I have updated my code, I forgot about `&gt;&amp;2`.
I think you can skip cat and simply do `&gt; "$filename"`.
Useful and simple, will help in my lab's
Thanks, if you notice any bug, problem or have any suggestion to improve the script please let me know
"and then purge backups older than two weeks." Beware, it has happened, that automatic backups have failed, but the automatic purge hasn't.
Do you know *newsyslog*, particularly -F flag for forced trimming, any chance it would do what you want ?
Why not just use regular shell functions? Why is it written in python? Really don't get the point of this.
Unfortunatly it is not and I have a read-only profile on the system so I can't make it work.
I wrote a little python script that I can call for this particular need : `import sys` `for line in sys.stdin:` `l = line.split('"')` `ct = 0` `buff = ""` `for i in l:` `if ct%2 == 0:` `i=i.replace(' ','\t')` `buff=buff+i` `ct+=1` `sys.stdout.write(buff)` The spaces characters outside quotes are replaced by tabulations.
Does *lsof -i :port* see all connections, with the port either at local or remote end ? I mean, say you wanted to check there is no sshd running. lsof -i :22 would however show ssh clients.
Wicked handy script, thanks for sharing this with us!
Thank you üí™üèªüôèüèª
 pkill terminal
nope ¬∞\_¬∞
Maybe this is more accurate: sudo lsof -i -P -n | grep LISTEN | grep ${port} You seem to understand more than me... do you think this is better?
What do you mean nope?
I don't have Linux to doublecheck, hoping not to just add confusion... Going by an online manpage, lsof -nP -iTCP:$port -sTCP:LISTEN
Doesn't work with that. I think it searches just for a process called 'terminal' and nothing more. By using regular expressions I could kill all the running terminals in the system. In that way the script would be able to manage more terminals than just the one I specify.
No... The pattern is a regex. Did you even try it?
Ops, I didn't see the 'p' in front of kill. What can I say... I'm sorry and thank you!
I don't know for sure where the p comes from but this is my theory. In pgrep it stands for something like process grep. In pkill the p is there because of pgrep.
Careful with that axe. Casual use of -9 can leave temp files, shared memory regions and all sorts of litter behind. Maybe not so much with terminal emulators, but...
Ty for the info. Do you know any better way to get this?
I would just leave out the -9 option. Default, SIGTERM, allows process to do any required cleanup it needs to do.
*Half* the script worked great, boss....
Can we as tech people and professionals not do this? It's patently unhelpful, discourages people new to bash/nix, and discourages newer professionals from reaching out for help. Every time something like this comes up may feel repetitive, but each one helps someone, and then gets indexed on search engines and Reddit's own search to help still more. Also, it's just not nice.
Can we as tech people and professionals not do this? It's patently unhelpful, discourages people new to bash/nix, and discourages newer professionals from reaching out for help. Every time something like this comes up may feel repetitive, but each one helps someone, and then gets indexed on search engines and Reddit's own search to help still more. Also, it's just not nice. Good on you for helping after, though!
IIRC this may not work on all shell/nix versions. If you test and it does on yours, of course, good to go.
That sloppy use of `$*` and `${array[*]}` tho :|
&gt; Can we as tech people and professionals not do this? What? cutting through the crap? if the poster is just flubbing and just wants the answer on a plate, they'll probably not stick around.... If despite what they say (and shock - yes, people do tell fibs on Reddit!!!!!) they just want an answer on a plate then asking might save someone providing an answer that is totally ignored &gt; Every time something like this comes up may feel repetitive, but each one helps Your letters are in the wrong order I think... &gt; Good on you for helping after, though! Ahh, thank you for your blessings - I feel that I have somehow redeemed myself from my most loathsome questioning of OP's wishes... now go and wallow in the warm glow of your self-righteousness.
&gt; I found fail2ban far too complicated for the needs of the project I was working on. Right there, the entire article loses all credibility. So fail2ban is "too complicated" yet this is exactly its use case and it pretty much works out of the box. No, instead let's use this convoluted set of bash scripts that are completely ad-hoc, aren't extensible, have no hooks to the system services management, don't work in any standard way that a sysadmin would expect... great idea.
cool, thx!
`iptables` does this without requiring any bash scripting. https://debian-administration.org/article/187/Using_iptables_to_rate-limit_incoming_connections I've been using variations of the above solution for years now. Works great.
For the `sh` script, wouldn't it be better to include the action in the `find` command using the `-exec` option? I mean: find /var/log -type f -exec dd if=/dev/null of={} \;
It works in dash, ash, ksh and bash, but you are right, it's not portable. After some research, a portable way would be using noop as in: `: &gt; /dev/null`.
Cheers bro, I'll check it out
Thanks for your sour comment üôÇ I mean the thanking, every piece of feedback makes both side better. You can‚Äôt possibly know the requirements of my project and why fail2ban didn‚Äôt fit and it‚Äôs not a subject of discussion anyway. I‚Äôm writing about bash. The point of this article is to give an example of simple and effective bash scripts, examples that aren‚Äôt fictional, that I can vouch for being applied in the real world. Whether the whole solution is feasible is very subjective to each and every project.
The solutions are similar in some ways, but it‚Äôs subject to the needs of the project where it‚Äôs applied. While your iptables solution is purely a rate limiting one, mine is more of a filtering one, closer to what fail2ban does, but different in its own way. Thanks for jumping in with feedback!
The script is iptables wrapper. Not saying good or bad. But the author is using iptables.
Yes, that's my mistake. I'm always forgetting about `-exec`.
Is this GNU find? You could use find / -regextype posix-extended -regex "/(proc|dev|run|sys|srv)" -prune -o and then the rest of your options.
`-type` is inclusive. From `man find`: To search for more than one type at once, you can supply the combined list of type letters separated by a comma `,' (GNU extension). Try `-type f,l`, but be aware of portability.
Maybe I'm being thick, but I fail to see the benefit of your solution over a pure `iptables` grey listing.
I do not want links hence the multiple.. but i also wants dirs i got the format from find man. furthermore : &amp;#x200B; \# find . -type f,d find: Arguments to -type should contain only one letter
this works but it still displaying the dirs i want to completely ignore them and do not display them on the results. i have to run in a large amount of hosts remotely not sure a redhat 5 or other will support the regextype. option.
I was pointing out that he didn't need to use both to accomplish the task.
After the very first attempt, the second try is already denied with my solution. Greylisting this would be too strict to do this, I think. If I take what you say as the absolute truth, the why would the world need fail2ban over greylisting?
After the first attempt, the second try is already denied with my solution. Greylisting this would be too strict to do this, I think. If I take what you say as the absolute truth, the why would the world need fail2ban over greylisting?
Dunno I've never used fail2ban. I think you might be thinking I'm someone else. I'm pretty sure fail2ban is way more flexible and easy to use though. I think banning someone after the first failed attempt is overzealous. I ran a corporate environment for awhile, which required a bastion ssh server. If I greylisted after every wrong attempt, not only would I have been locked out, my phone would have been ringing off the hook. Of course, your project requirements haven't been laid out, so I don't understand the scope of your project.
What version do you have? `find . -type f,d` here finds every file and dir as it should. Non-GNU? But you have the regex support, really strange.
Add -print to the end, otherwise the default print sees the pruned directories, and they get printed too. Like this: (( stuff to ignore ) -prune -o stuff to match ) -print When you explicitly add -print to the end, it goes like this (brackets not to be typed, only for emphasis). ( stuff to ignore ) -prune -o [ stuff to match ] -print
\]# find --version find (GNU findutils) 4.5.11 Copyright (C) 2012 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;[http://gnu.org/licenses/gpl.html](http://gnu.org/licenses/gpl.html)\&gt;. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. &amp;#x200B; Written by Eric B. Decker, James Youngman, and Kevin Dalley. Features enabled: D\_TYPE O\_NOFOLLOW(enabled) LEAF\_OPTIMISATION SELINUX FTS(FTS\_CWDFD) CBO(level=2)
yes I had found out using `-exec ls -l {} \;` that they do not get printed.
My `find` version gives find (GNU findutils) 4.6.0.225-235f Copyright (C) 2019 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;https://gnu.org/licenses/gpl.html&gt;. This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Written by Eric B. Decker, James Youngman, and Kevin Dalley. Features enabled: D_TYPE O_NOFOLLOW(enabled) LEAF_OPTIMISATION FTS(FTS_CWDFD) CBO(level=2) but the listing of several type options should work with your version as well. If you really want to suppress the output, I think you need to do postprocessing of the output with something else.
The first rule of Expect is that you almost never need to use it. What's your end goal?
What would happen if more than one `.txt` file is present in `/home/username/path/`?
Read information from a text file and SSH into an old network device and execute commands. The traditional ways of doing this don't work.
Good question, but there never will be. The last line of script will move the text file to an archive folder.
What's the network device, out of curiousity? I once wrote Python script that was basically a wrapper to an expect script for logging into and making/verifying changes to about 1400 Adtran devices, so I had probably gone down a similar path.
Fortigate. I had it working with Posh-SSH on Powershell on Windows but I'd like to move this script to a nix box. (I know Powershell is on nix now but I'd like to accomplish this with `expect`)
I am guessing that expect isn‚Äôt able to expand the glob (or that it has to be done differently) ‚Äî is it possible to test by using the explicit filename for now?
The script works with the explicit filename.
Maybe what you can try is: -Have the actual expect script work off of argv -- e.g. `set MyVar [lindex $argv 0]` -Use a wrapper .sh script that sets the file contents to a var and pass that into the expect script as the first arg, e.g. #!/bin/bash FILE=$(cat /home/username/*.txt) /path/to/expect/script $FILE
This works. Cheers man.
Niiice
Might want to quote that.
You need to use Tcl's [glob](https://www.tcl.tk/man/tcl/TclCmd/glob.htm) instead to expand the `*`.
Try wrapping it into an actual if statement‚ÄîI‚Äôve personally had some *weird* behavior just using ``test &amp;&amp; action``. So: if [ -n ‚Äú$1‚Äù ]; then cp ‚Äú$1‚Äù ‚Äú$HOME/.config/wall.png‚Äù &lt;notify-send command&gt; &lt;wallpaper change command&gt; if ~on mobile, so I can‚Äôt actually see your code while responding; but it should still be relevant. Also, why not use ``feh``?
When I started using arch xwallpaper is just what I used and have been. Should I use feh over x?
yea, i would use feh over x. This will give you a new wallpaper everytime this command is run: feh --randomize --bg-fill \~/Pictures/Wallpapers/\* HTH :)
Hm I like that I might write a script now to run that command then pywal. How is multi monitor support with feh?
Even when I run feh --bg-fill path it opens in a window
Thanks for giving the right answer, I was looking for this but the docs I found pertaining to globs were regarding expected strings rather than setting vars.
Excuse the shameless plug but if you don't mind the overkill you could give [this](https://github.com/michaeltd/dots/blob/master/dot.files/bin/wallpaper-rotate.sh) a try. I'd love some input.
I will check it out.
I used this to fix it. &amp;#x200B; `#!/bin/sh` `# Sets the background. If given an argument, will set file as background.` `if [ -n ‚Äú$1‚Äù ]; then` `cp ‚Äú$1‚Äù ‚Äú$HOME/.config/wall.png‚Äù` `notify-send -i ‚Äú$HOME/.config/wall.png‚Äù ‚ÄúWallpaper Changed‚Äù` `feh --bg-scale --no-xinerama ~/.config/wall.png` `fi`
Use find to do the recursion.
Nice but i dont like the -ex, why would you instant exit on error? Even cleanup functions wont run then. Better catch the error and work with it (send a useful error message, run clean ups etc)
I really like the idea about time. I can't really get it to work though... cat test.sh #!/bin/env time bash sleep 2 exit 0 &amp;#x200B; RedHat 7.6: ./test.sh /bin/env: time bash: No such file or directory Arch: ./test.sh /bin/env: 'time bash': No such file or directory /bin/env: use -[v]S to pass options in shebang lines
I don't always want to use -e or especially -x; I tend not to use those when a script has gone through testing and (seems to be) working properly. If there's any error handling to be done, I try to build it into the script as much as possible. I can also see how `time` would be beneficial, but how much extra resources does it take up? If it slows (long) scripts down noticably, I'd rather not use it. &amp;#x200B; I do, however, agree with using `/usr/bin/env bash` instead of always invoking `/bin/bash` or `/bin/sh` As a mac user, I've installed bash through homebrew, and I invoke `/usr/local/bin/bash`, because as per standard, OSx ships with bash 3.2. Always good to call on the right shell ;)
env is in /usr/bin not /bin
Especially the -x is not something I want, except when I do testing. It will screw up my output to the screen or my output in a crontab. The -e is also not always wanted. e.g. if I do a loop where it treis to do a backup of several servers, if I get an error on the first server, I still want to backup the rest. What I do instead is something similar to this: #!/bin/bash TEST=1 if [ "$TEST" == "1" ] then set -ex &lt;Some more settings&gt; fi I will also set a trap and an error handling part, so I can just do something like #!/bin/bash .... ERROR () { echo "ERROR on line $1" exit 1 } mount /this/wont/work || ERROR $LINENO In the trap I will include a calculation of time using $SECONDS All this can vary from script to script. e.g. I might add a second parameter at the ERROR to determine if I want to exit or not.
go on, catch all errors you can think of. set -e , it will catch all others :)
Instead of `time` use `$SECONDS` inside the script if you only want to know the time. hrs=$(( SECONDS/3600 )); mins=$(( (SECONDS-hrs*3600)/60)); secs=$(( SECONDS-hrs*3600-mins*60 )) printf 'Time spent: %02d:%02d:%02d\n' $hrs $mins $secs
See https://blog.twentytwotabs.com/the-smallest-bash-program-in-the-universe/. After it's asked to execute a file, and when it sees that the file begins with a shebang, the kernel will execute the interpreter (first word after #!) with up to three arguments: - the interpreter name (argv0) - all **words** following the interpreter name as a single argument...this is omitted of there aren't any words - the path of the file with the shebang line Using env precludes the ability to pass command line args to the interpreter. For much more detail, see the post.
I've been told `#!/bin/sh` is mandated by POSIX, so there really isn't a need to use `env(1)` for plain sh. For other shells it's a good idea though. I doubt `time` would have a significant performance impact. However, depending on the type of work your script does and the speed of your terminal, `-x` could have a performance impact: $ time sh -c 'set -x; for i in $(seq 100000); do :; done' + : + : ... 0m02.76s real 0m00.68s user 0m00.99s system $ time { sh -c 'set -x; for i in $(seq 100000); do :; done' 2&gt;/dev/null; } 0m00.77s real 0m00.55s user 0m00.25s system $ time sh -c 'set +x; for i in $(seq 100000); do :; done' 0m00.44s real 0m00.40s user 0m00.07s system The nice thing with `-e` is, that you don't necessarily have to handle everything manually, and even if you do, it catches the cases you forgot. It really does only good without any harm. I still wouldn't use a shebang like suggested though, and simply use `set -efu` at the start of my scripts.
&gt; The -e is also not always wanted. e.g. if I do a loop where it treis to do a backup of several servers, if I get an error on the first server, I still want to backup the rest. backup-server "$ip" || : This will keep going if one fails.
&gt; I've been told `#!/bin/sh` is mandated by POSIX, so there really isn't a need to use `env(1)` for plain sh. POSIX does not specify the location of any program at all. However, this includes `env`, so `#!/bin/sh` should be fine. &gt; The nice thing with `-e` is, that you don't necessarily have to handle everything manually, and even if you do, it catches the cases you forgot. It really does only good without any harm. `errexit` can do a lot of harm. For example, it makes the following snippet exit: `i=0; ((i++))`. Why? Because Bash an arithmetic expression returns 1 when it evaluates to 0. Obviously there shouldn't be anything wrong with that, though; `errexit` is flawed. See https://mywiki.wooledge.org/BashFAQ/105 for more information.
You can clean up using exit traps: [http://redsymbol.net/articles/bash-exit-traps/](http://redsymbol.net/articles/bash-exit-traps/)
This is bad advice. First, it's not portable, as the behavior of passing multiple words in a shebang is undefined by POSIX. Some kernels, like Linux, interpret each word as a separate argument, but some, like Darwin, passes the whole thing as a single argument, so it tells `env` to execute a binary named `'time bash -ex'`. Second, `-x` is not really something that you should set everywhere. It can output a ton of junk, filling the terminal, slowing the execution significantly, and pushing actual output out of the screen. Only use it for debugging or scripts where its output is manageable. Third, `errexit` is quite flawed. For example, a simple `i=0; ((i++))` will trigger it, as arithmetic expressions return 1 when they evaluate to 0; it obviously isn't an actual error, though, just a nicety that allows you to evaluate variables as booleans and such. You can find more info at https://mywiki.wooledge.org/BashFAQ/105.
Hmm, it looks like it works only only for BSD env, but not GNU env.
Shouldn't you be trapping EXIT for cleanup functions? There are other problems with `errexit`, though; see [my comment](https://www.reddit.com/r/bash/comments/be57l8/a_better_shehashbang_for_your_shell_scripts/el3ea1l/).
Also, recommend taking a look at ``wpgtk`` too; uses feh and pywal on the backend; but will do all the things in one command...as well as other handy things.
Remember, there‚Äôs the maximum length supported by the system. In fact, due to this limitation, even xargs has a built in feature to execute max possible arguments
When I get off work I will check it out thanks.
This 'kinda' works on Red Hat, but I get the following output: ./test.sh 0.00user 0.00system 0:02.01elapsed 0%CPU (0avgtext+0avgdata 1392maxresident)k 0inputs+0outputs (0major+676minor)pagefaults 0swaps on Arch though, there is no /usr/bin/time as time is a shell builtin there.
TIL, and I'm always glad I don't use bash (I know this is the bash sub). I mean it boils down to knowing your shell. Sure, I agree, if you don't know about all those corner cases, using `errexit` can be harmful, but so can not using it.
Thank you.
Shebangs are unusable in COMSPEC environments. Shebangs are ignored when scripts are run with an explicit interpreter prefix. POSIX sh lacks basic sanity checks while bash is slow and unavailable on some distributions. I‚Äôd go with a general purpose scripting language, or better yet, a compiled application.
First step is I wouldn't do something like this on your live system, one faulty argument and you could quickly FUBAR your system. Second, [read](http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-8.html)
Oh yeah I don't. I run the script in a virtual box for testing. And thanks for the reply, but I don't see anything about EOF statements in functions, sorry if i'm not getting it.
This is a step above writing heart pacemaker software in bash. I'll not tell you not to do it, but...
Don't use it if you don't understand it. Use something like echo "g n p 1" | fdisk /dev/${DRIVE}
Should I use another language? I have written one in bash before, but it was SUPER rudimentary. Here it is if you want to look at it for giggles, it technically worked, but I wanted to re-write it and make it more readable. But i'm down to use another language if I can. &amp;#x200B; [https://github.com/CodeCanna/super-duper-archey-installer/blob/master/arch\_installer/enterarchway](https://github.com/CodeCanna/super-duper-archey-installer/blob/master/arch_installer/enterarchway)
I have seen this method of doing it as well. I understand how EOF works, but I don't know why it won't go into a function. But this method looks more readable anyway.
Why not using LVM? It will by way easier to manage the volumes from bash.
ps -ef | grep python3 | grep -v grep | wc -l
ps -ef | grep \[p\]ython3 | wc -l To avoid an extra pipe
jobs
Or pgrep python3 if you only want id numbers. Although personally I'd do as the other said and use jobs
I know, i just wanted to do a "showcase", for that "grep -v grep"
Nothing wrong with that my friend. As always, there are many ways to do a thing and no one way is the "right" way :-)
 $ shellcheck lol In lol line 35: EOF ^-- SC1039: Remove indentation before end token (or use &lt;&lt;- and indent with tabs).
Cgdisk has this sort of thing built in and can be scripted from
thx, this is what I was looing for happy to see the other answers in this post too : )
`sgdisk` was made to be scripted: function_partition_drives() { PART_DRIVE="/dev/$DRIVE" sgdisk -o \ -n 1:0:+1G -t 1:EFI \ -n 2:0:+20G -t 2:8300 \ -n 3:0:+16G -t 3:8200 \ -n 4:0:0 -t 2:8300 \ "/dev/$DRIVE" format_parts "$DRIVE" }
What is LVM? I'll look into it :)
Hm never heard of sgdisk, I'll check it out. Thanks :)
Or even fdisk /dev/${DRIVE} &lt;&lt;&lt;'g n p 1' To avoid the inefficient pipe.
Sure how much are you paying.
Thanks so much for the reply I will check out cgdisk.
Hey also is there a good place to find documentation for sfdisk? I tried the obvious, with man but it's a little confusing. Anything more detailed?
Also pgrep -l includes process names
What are you trying to do? *sfdisk* is made for [MBR](https://en.wikipedia.org/wiki/Master_boot_record) partitioning. *sgdisk* handles both MBR and [GPT](https://en.wikipedia.org/wiki/GUID_Partition_Table) (what you most likely want). Also, I personally find *sgdisk* much easier to use and understand than *sfdisk*.
**Master boot record** A master boot record (MBR) is a special type of boot sector at the very beginning of partitioned computer mass storage devices like fixed disks or removable drives intended for use with IBM PC-compatible systems and beyond. The concept of MBRs was publicly introduced in 1983 with PC DOS 2.0. The MBR holds the information on how the logical partitions, containing file systems, are organized on that medium. The MBR also contains executable code to function as a loader for the installed operating system‚Äîusually by passing control over to the loader's second stage, or in conjunction with each partition's volume boot record (VBR). *** **GUID Partition Table** The GUID Partition Table (GPT) is a standard for the layout of partition tables of a physical computer storage device, such as a hard disk drive or solid-state drive, using globally unique identifiers (GUIDs). Forming a part of the Unified Extensible Firmware Interface (UEFI) standard (Unified EFI Forum-proposed replacement for the PC BIOS), it is nevertheless also used for some BIOS systems, because of the limitations of master boot record (MBR) partition tables, which use 32 bits for logical block addressing (LBA) of traditional 512-byte disk sectors. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
This is an Archlinux install script, so I want to have automatic partitioning. I want it to be as automated as possible, which is proving to be a real challenge.
Maybe just run `wait` until that finishes. Heck, even `wait &amp;` might work. lul
Here's my 100% automated [ArchLinux USB install script](https://pastebin.com/ER0jrLw2) to get you thinking/started.
Oh wow, that'a a lot. That's awesome, thank you!
Does it have to be in bash? It sounds to me like something that would be easier in Python/Ruby/etc.
steven -p
To get the parameter, you would use string comparison in bash. I think you can look up string comparison and figure it out from there. To get the value for each field, you can use `cut` or `awk` to get the value, whether string, float, or integer. An example using `cut` where the file "deleteme" contains the text from input.voltage.nominal to ups.load. cdickbag@uss-dickship:~$ cut -d: -f2 deleteme 230 265.0 enabled 20 30 35 An example using `awk` would read as follows: cdickbag@uss-dickship:~$ awk -F: '{print $2}' deleteme 230 265.0 enabled 20 30 35 For the rest of it, you can use comparison operators, regular expressions, and `if` statements to determine what type the value is. You can choose to package this information in a series of functions, or a series of evaluation statements. However, if you're trying to write a plugin for something like Nagios, it's likely that one already exists, and you may be better off using said plugin from the Nagios community.
pipe to `ps` for further detiail: `pgrep python3 | xargs ps -up`
Top
It can be in bash or Powershell. I dont mind either.
&gt; However, if you're trying to write a plugin for something like Nagios, it's likely that one already exists, and you may be better off using said plugin from the Nagios community. It sucks lol....I have to write my own :(
Been there, dude. What equipment are you writing this plugin for, and what information do you lack?
runtime, battery condition, etc....A couple of things are missing.
&gt; First you talk about a file, not the output of a command. I tried replacing "deleteme" file with the literal command I posted but it tries to look for a file. `cut` and `awk` accept a filename in place of a redirection or pipe from STDOUT. For example, the following are roughly equivalent: echo "output.voltage: 265.0" | cut -d: -f2 And the following have the same result, where "deleteme" is a file name. cut -d: -f2 deleteme &gt; I was expecting a grep instead of a cut/awk `grep` is solely used for pattern matching. `cut` is a limited tool which is used cutting strings. `awk` is for lack of better words, a full text processing language of its own. `awk` and `sed` do pattern matching, substitutions, and a bunch of other stuff. &gt; Next, what you posted there, give back ALL the values.....I want each value, one at a time. Yes, it gives all the values because when `cut` or `awk` process a file, they read the entire file, for the most part. In my case, I input your entire UPS output example into a text file, then ran `cut` and `awk` on that text file. That means they process they entire file, line by line. When you're reading output from a command or program, you will usually be reading no more than one line at a time. That's when you want to do something like, `upsc nameofups ups.load`. The command will only return one value, like the following. `ups.load: 35` You save that string as a variable, then process its component parts, and run the rest of your script based on the output.
....oh shit. upsc nameofups ups.load That returns 35 lol....Its a lot more simple then.... upsc nameofusp ups.load &gt; /dev/stdout 2&gt; /dev/null Gets me the data ; There is a stupid garbage Init SSL without certificate database message that gets spit out as well and it seems with those redirects it silences it.... OK well....this changes a lot of things lol.... How do I then compare that value to a int and/or string???
....oh shit. upsc nameofups ups.load That returns 35 lol....Its a lot more simple then.... Also: upsc nameofusp ups.load &gt; /dev/stdout 2&gt; /dev/null Gets me the data ; There is a stupid garbage Init SSL without certificate database message that gets spit out as well and it seems with those redirects it silences it.... OK well....this changes a lot of things lol.... How do I then compare that value to a int and/or string???
Frankly, do you need to compare the value to an integer or a string? You already have the positional parameter being fed to the script. You know whether to expect an integer, string, or whatever else, because the positional parameter is always going to return the same type of value. Evaluate based on the positional parameter.
I think so.... my script would be something like ./checksometing {required parameter} {optional parameter} {optional parameter} Those last two are warning and critical levels....I can different combinations on if I want a crit or a warn or both...more so with strings
Ok, we have a known number of positional parameters we know to expect. Those values will be integers, some of them will be strings, and some of them will be floats. The logic required in order to process all of those values will be much greater than to process the number of known positional parameters starting at $1, and return OK, WARN, or CRIT based on the script logic. Instead, I would write a function per initial positional parameter, and define OK, WARN, and CRIT within the function. So basically, determine what $1 is, then execute a function with the appropriate logic to determine how bash should process the command, and return the appropriate code.
$1 is the paramter i want ups.load for example $2 and $3 are optional and would be the different warning and critical values I think with a example script I can go on from there.
You overlooked that `upsc` can already take an optional second parameter, see here: $ upsc yunto ups.load 10
Yeah, i figured that out file testing......I also saw that the stupid ssl message can be fed to null So now I just need to script to create the logic upon arguments and define if it is a int or string
Maybe you can just skip testing if something is an integer. Bash will not complain when you try to use a variable that's not containing a number inside its "math" mode. It will treat the value as a zero. See here: $ x=hello $ echo $(( x )) 0 $ if (( x &gt;= 80 )); then echo true; else echo false; fi false $ if (( x &lt; 20 )); then echo true; else echo false; fi true The same happens inside `[[` when you use the number tests like `-ge`, `-lt`. It then depends on what comparisons you want to do exactly. This 'zero' behavior might already be good enough. If you do want to actually test if something is an integer, you can use a regex pattern or enable the 'extglob' feature that enables a bunch of extra wildcard patterns. Using regex looks like this: $ x=234 $ if [[ $x =~ ^[0-9]+$ ]]; then echo good; else echo bad; fi good $ x=hello $ if [[ $x =~ ^[0-9]+$ ]]; then echo good; else echo bad; fi bad The 'extglob' method first needs this command because it's off by default: shopt -s extglob And then using it looks like this: $ if [[ $x == +([0-9]) ]]; then echo good; else echo bad; fi bad $ x=234 $ if [[ $x == +([0-9]) ]]; then echo good; else echo bad; fi good Thinking about this some more, a problem that might show up is extra space characters in the output of the `upsc` command. I mean a content for the variable like `x=" 234"` for example. To fix that, you could change the regex test to this: [[ $x =~ ^\ *[0-9]+\ *$ ]] And I have no idea how to do the same with extglob.
 while read n v do n=${n%:} # remove : at end n=${n//\./_} # each . -&gt; _ eval $n=\$v done &lt;&lt;&lt; `upsc` That would give you variables $ups\_load etc, maybe easier to handle, and cheaper, less program launches. The method doesn't feel right though... Does anyone know a better way?
Why not use ssh-agent for this? Start the agent, add your key, and then you can source the two environment vars from the agent in your script. It's been a long time since I've used expect, but you may need to quote the expected string and password.
Been a while since I used expect but I still love seeing `expect assword:` My guess as to one issue is that the send string isn't including some kind of line terminator like `\r` or `\n`, so the password is typed in but then still awaiting a simulated "enter" key. As far as log output, maybe you could try the `log_file` option in the expect script.
I wrote my first working expect script last week (yay!) and this is what worked for me: 1) quote everything 2) always append a space to your `expect` lines- e.g.: `expect "Username: "` 3) append a carriage return to your `send` lines- e.g.: `send "$user\r"` Hope that helps!
\- add \\r after password \- add expect eof `expect -c "spawn /home/username/grabconifg.sh` `expect assword:` `send !qazwsx!\r` `expect eof` `"`
why the epect EOF? interesting. I'm getting super close with this, it's not working because i think it's taking the password wrong. &amp;#x200B; so the password is !qazwsx! how do I type that in so it's read correctly?
**THIS WORKED!** &amp;#x200B; Final Code &amp;#x200B; \*edit\* THIS WORKED this was the finally command expect -c " spawn /home/jferreira/grabconfig.sh expect assword: send \\!qazwsx\\!\\r expect EOF "
That was in an old script I made years ago, I don't remember why the `expect eof` is needed, sorry
&gt;so the password is !qazwsx! how do I type that in so it's read correctly? You can also use single quote `'` instead of double `"` to use exclamation mark without escape
Is this on a hobby stack? Hard coding a password is bad mkay.
Can be okay if you're using an account with the rssh shell.
The eof is necessary so Expect waits for more output. Otherwise, it's done everything you asked of it (sent the password to the spawned procedure). Try send "!qazwsx!\r"
Ansible?
People will tell you to use Ansible. Don't do this. You will regret your life. Easiest thing would be to just write your own script and use GNU parallel. Some options that you might find useful if you decide to go the parallel route: --slf --onall --trc --pipe --joblog
Distribution is not my issue - and Ansible, while great/wonderful for what it does, only provides a portion of what I am requesting. I'll play with Ansible again where I can get the results in JSON - I would then have to parse/curl the response in a separate process. What I am thinking is building an ssh/cron where I would say \`df -h | parse\_info\_into\_json | iterate over results do curl -X post $hostname disk\_free, $key, $value | echo hostname 'done' I could then query the time/series db and build a table based on the hostname with the most recent result.
Instead of using `df -h` you can use `findmnt` directly. For example. $ findmnt --type ext4 --json { "filesystems": [ {"target": "/", "source": "/dev/mapper/fedora-root", "fstype": "ext4", "options": "rw,relatime,seclabel", "children": [ {"target": "/boot", "source": "/dev/nvme0n1p2", "fstype": "ext4", "options": "rw,relatime,seclabel"}, {"target": "/home", "source": "/dev/mapper/fedora-home", "fstype": "ext4", "options": "rw,relatime,seclabel"} ] } ] } Obviously you can run this directly over ssh, or via some orchestration software like Ansible.
Next level partitioning....Worth to learn about it!
&gt; and Ansible, while great/wonderful for what it does No. Ansible is utter garbage at what it does. It's one of the worst pieces of software I've ever had to use. &gt; What I am thinking is building an ssh/cron where I would say `df -h | parse_info_into_json | iterate over results do curl -X post $hostname disk_free, $key, $value | echo hostname 'done' So do you want to run it automatically or do you want to invoke the run from your machine? &gt;I could then query the time/series db and build a table based on the hostname with the most recent result. So you want the results stored somewhere other than your own machine? I feel like you're might be over complicating things.
Now you're amazing and how would I know of this wonderful utility? My background is core/posix compliant.
I may be over complicating it and I appreciate your responses. I maintain many diverse systems (mostly linux and every flavor) where volumes, databases, and files are added on an ongoing basis. The infrastructure is not within my responsibility/authority. I would like to be able, at a glace, figure out if Server X has enough space to drop 22TB of data or see the row count of a database on Server Y. My issue at the moment is getting the information from the output and parsing it into json. &amp;#x200B; I suppose I could write some python but have not due to the diverse ecosystems as there may be version conflicts.
Yeah, understood; I was a Unix person for many years myself, but Linux has taken over. Many of the modern core utilities work this way in Linux, and the legacy tools are actually just invocations of the modern tools. They maintain comparability, like when `mount` is run... It's just `findmnt` under the hood running in legacy mode. Same for `df` and `du` and heaps of others. Don't get me started on ` ifconfig` versus `ip`, and `netstat` versus `ss`. Anyways, with so many people doing exactly what you are doing we put a ` --json` cmdline switch to make it easier. By we, I mean Red Hat employees and the upstream Linux community at large. Fyi, remarks are my own, not any employer. Anyways, just read the manual pages, and scope out the various core utilities. Good luck!
Since I started in the \*nix world the solution appears to be "RTFM". Your response was kind and helpful and appreciated.
I must agree with the above comment, I absolutely despised every second of trying to work with sensible. However, I was recently told about Puppet Bolt and I love it so far. It‚Äôs free, agentless, and super configurable. You can run ssh or winrm commands on many servers all from one machine. Regarding your comment about being able to see disk space, why not just use some free / open source monitoring software like nagios or icings to visualize disk space? It also has a ton of plugins, not sure but most likely sql capability as well. Just a thought as opposed to writing your own monitoring solution.
Add ```set -x``` to the start of the script and you'll get some good debug info.
If autovert-tv.sh reads from stdin it will consume all of the remaining lines. Try adding ` &lt;/dev/null` at the end of that line. You might want to add that behind all of the script calls.
You shouldn't use `ls -1 | while read`.. Tried to rewrite it for you quickly. #!/bin/bash cd /mnt/media/vert/input/ \ || { echo "error changing director!" &gt;&amp;2; exit 1; } ls -1 | while read line; do for d in */; do t= case "$d" in (Movie *) printf 'Movie: %s\n' "$d" /mnt/media/util/autovert-movie.sh "$d" ;; (TV *) printf 'TV: %s\n' "$d" /mnt/media/util/autovert-tv.sh "$d" ;; (TVl *) printf 'TVl: %s\n' "$d" /mnt/media/util/autovert-tvl.sh "$d" ;; (Premium *Kevin Ryan*) printf 'Kevin: %s\n' "$d" /mnt/media/util/autovert-kevin.sh "$d" ;; (*) printf 'Not processed: %s\n' "$d";; esac done
 util/autovert.sh: line 7: syntax error near unexpected token `*' util/autovert.sh: line 7: ` (./Movie *) printf 'Movie: %s\n' "$f"' The golfer in you would tag a bunch of other premium videos not made by Kevin with Kevin's overlay. That would cause mayhem, which I enjoy, but would also confuse the Lurkers... And a confused Lurker doesn't watch the channel anymore. We don't want that.
It gave very minor info, and nothing at all useful. Just showed line by line what was running.
No change, the loop still exits after the first file.
Sorry, forgot to escape those spaces. `(./Movies\ *)` This is what you want.
No joy. It just drops through to "Not processed" for all the files now.
Last message had Movies instead of Movie, did you correct that?
Can you show the line you modified? Maybe with a couple of lines before and after for context?
 #!/bin/bash cd /mnt/media/vert/input/ || { echo "error changing director!" &gt;&amp;2; exit 1; } for f in ./*; do case "$f" in (./Movie\ *) printf 'Movie: %s\n' "$f" /mnt/media/util/autovert-movie.sh "$f" ;; (./TV\ *) printf 'TV: %s\n' "$f" /mnt/media/util/autovert-tv.sh "$f" ;; (./TVl\ *) printf 'TVl: %s\n' "$f" /mnt/media/util/autovert-tvl.sh "$f" ;; (./Premium\ *Kevin\ Ryan*) printf 'Kevin: %s\n' "$f" /mnt/media/util/autovert-kevin.sh "$f" ;; (*) printf 'Not processed: %s\n' "$f";; esac done echo "All done!" exit Here's the whole thing.
Yes, same result.
You didn't add the ` &lt;/dev/null` to the autovert lines? This is what I mean: #!/bin/bash cd /mnt/media/vert/input/ || { echo "error changing director!" &gt;&amp;2; exit 1; } for f in ./*; do case "$f" in (./Movie\ *) printf 'Movie: %s\n' "$f" /mnt/media/util/autovert-movie.sh "$f" &lt;/dev/null;; (./TV\ *) printf 'TV: %s\n' "$f" /mnt/media/util/autovert-tv.sh "$f" &lt;/dev/null ;; (./TVl\ *) printf 'TVl: %s\n' "$f" /mnt/media/util/autovert-tvl.sh "$f" &lt;/dev/null ;; (./Premium\ *Kevin\ Ryan*) printf 'Kevin: %s\n' "$f" /mnt/media/util/autovert-kevin.sh "$f" &lt;/dev/null ;; (*) printf 'Not processed: %s\n' "$f";; esac done echo "All done!" exit Are you by chance using mplayer (aka mencoder) inside the autovert-*.sh scripts?
type aa and... here you go. I've been using it with screen and i3 for a while. If it fits anybody needs, here you are: [Github](https://github.com/LionyxML/aa)
Sorry, I tried it, and nothing happened at all. My reply with the updated script was for u/StallmanTheLeft. And no, ffmpeg to add titles in an overlay.
Wait.. I wasn't reading correctly. My edit was to make you original script work. It won't affect the new loop which doesn't use a pipe. You are probably running into a different problem now. The `./` is the for loop is not necessary. Also, in order to isolate if the problem is this script or the autovert-*.sh scripts add `echo ` before each of those just as a test. #!/bin/bash cd /mnt/media/vert/input/ || { echo "error changing director!" &gt;&amp;2; exit 1; } for f in *; do case "$f" in (Movie\ *) printf 'Movie: %s\n' "$f" echo /mnt/media/util/autovert-movie.sh "$f" ;; (TV\ *) printf 'TV: %s\n' "$f" echo /mnt/media/util/autovert-tv.sh "$f" ;; (TVl\ *) printf 'TVl: %s\n' "$f" echo /mnt/media/util/autovert-tvl.sh "$f" ;; (Premium\ *Kevin\ Ryan*) printf 'Kevin: %s\n' "$f" echo /mnt/media/util/autovert-kevin.sh "$f" ;; (*) printf 'Not processed: %s\n' "$f";; esac done echo "All done!" exit What is the exact output if you do this?
I ran it on the original script, and adding &lt;/dev/null did absolutely nothing. u/StallmanTheLeft's rewrite of the filtering didn't work at all either. The only problem I'm having is getting it to loop through files. Redirecting output won't fix that. Changing how it sorts the files won't either. I'm not interested in efficiency, as long as the code actually runs.
What filtering? &gt;I ran it on the original script, and adding &lt;/dev/null did absolutely nothing. Well, his theory was that if your autovert scripts read from stdin then they're taking the rest of the output from the ls. This would mean that there isn't anything loop over after the first line. Can you upload one of the autovert scripts so we can see if those are causing your problems?
&gt; The ./ is the for loop is not necessary. Sure, it isn't. Often times it's still desirable so you don't get problems when files start with dashes.
That's very useful information. If you walk through it you can figure out why it isn't doing what you want.
Did you add the echos like /u/ralfwolf suggested?
I never suggested any filtering or sorting and everything I've suggested is an attempt to help you solve your problem. Good luck to you then.
Something like: case "$1" in output.voltage ) check_output_voltage ;; ups.beeper.status ) check_beeper_status ;; "" ) echo -e "You need to supply at parameter." ;; esac # Functions go below this line check_output_voltage () { blah blah blah blah } When the script is executed, it looks for $1. Bash reads through the case statement until it finds a match. If the match is output.voltage, for example, it will then execute the function check_output_voltage. If $1 is empty, bash complains. Hopefully, this gives you an idea of how to handle writing the script. You're going to have to use three different types of comparisons, and bash sucks at float comparison. The only true bash solutions I've seen are stupid ugly. Instead, most people use the calculator `bc` for [float comparison](https://stackoverflow.com/questions/8654051/how-to-compare-two-floating-point-numbers-in-bash). For integers, you can use bash's `-lt` and `-gt` comparison operators. For string comparison, bash's `==` comparison operator.
What you suggested was redirect everything to /dev/null. I accidentally replied to you with the updated script from Stallman.
I have solved the problem. I changed the following: ls-1 | while read line; do to: for i in *; do and it works. Thank you for your suggestions. They didn't work, but thank you anyway.
No, if you read my post and the post from /u/StallmanTheLeft the ` &lt;/dev/null` (note the direction of the redirect) is to prevent your downstream script from reading stdin thus consuming lines from the pipe in your original script. The behavior you described is exactly what would happen if you had called a program or script that read stdin from within a piped `while read...` loop.
My mistake. Either way, it's now working.
Here is a quick demo of it to accompany your comment. printf "%s\n" line{,,,,,,} | while read -r l; do echo "$l"; cat &gt;/dev/null; done
You didn't close the `case` statement with `esac`.
Ah, of course. Thanks for the help. Also, I /love/ the way bash does that. How to end an if statement? fi How to end a case, esac. I love it. Thanks again!
&gt; Also, I /love/ the way bash does that. How to end an if statement? fi How to end a case, esac. I'm not sure if you're being sarcastic, but a lot of programmers actually *hate* that. I do kind of like it in a way, though. By the way, I edited my comment with some more feedback. Another thing is that instead of checking a variable in the while loop condition and setting it to break out of the loop, you can simply use `true` or `:` as the condition to make an infinite loop and `break` to, obviously, break out of it.
Not being sarcastic at all. I think it's actually delightful. I've edited my other reply to reply to your edit, lol. Break works? When I was looking up how to do case statements in bash, everyone did ;; instead of break. I assumed it couldn't. I prefer break, thanks!
No, `break` is for loops. `;;` is to end a case in a case statement.
Ah, I see. In that case, I won't be able to do it that way. There would be nowhere that I could safely put the break where it would break the loop. Also, with regard to using read -p prompt, where do I store the user input? Is it read -p prompt variable?
&gt; EDIT: Why menu() over function menu? I read online somewhere that they are the same. I chose function menu over menu() because the parentheses aren't used in bash (from what I read). Whoops, seems that I didn't edit my comment fast enough. As the current version now says, it's compatible with older versions of Bash. There's another reason listed [here](https://wiki.bash-hackers.org/scripting/obsolete) (scroll down a bit).
&gt; In that case, I won't be able to do it that way. There would be nowhere that I could safely put the break where it would break the loop. Aren't you trying to emulate this with the x variable (`x=1` instead of `break`)? Or are you going to add some more stuff after the case statement or something? &gt; Also, with regard to using read -p prompt, where do I store the user input? &gt; Is it read -p prompt variable? Yep, that's how command-line options work.
Ah, excellent. Thanks for that link! I'm gonna be keeping it up to read over everything else when I have a little time.
There is nothing after the case statement. But the case statement is in a while loop. I want it loop. If I break after the case statement, and they gave the wrong input, the program will end. It needs to not end until they give valid input.
I wonder if there is a date format that everyone would be okay with. %d.%m just doesn't work well in the USA. It's not a problem to customize, but if there's a solution everyone would be happy with that would be better.
Looking at the code, it would print out what your machine's data format is default too.
In all fairness, the majority of people in the world would be happy with %d.%m. I think Americans are the odd ones out :D But cool that he says it defaults to locale.
&gt; but a lot of programmers actually hate that They just pretend to hate it so they look cool. In reality the difference is just a couple extra characters.
`%Y.%m.%d`
No, they hate it because it's not very consistent/intuitive/sane; I mean, the constructs are `if then else fi`, `loop do done`, `case in ) ;; esac`, and `function() { ;}`. I get their point, even if I like it in a way myself.
A lot of older languages have whole words for their loop constructs. if condition { if condition; then {while,for} condition { {while,for, until} condition; do These differences are purely visual.
Yeah, use jq for this if there is any possibility of those input strings needing JSON escaping.
You could do like `printf -v MongoQuery '%q' "$MongoQuery"` to have it quoted.
My first thought would be to just use `printf '%q'` to escape the strings and then using a heredoc to make the json so we don't have to deal with quoting and unquoting stuff. printf -v MongoQuery '%q' "$MongoQuery" printf -v CollectionName '%q' "$CollectionName" printf -v DataBaseName '%q' "$DataBaseName" JsonArgs=$(cat &lt;&lt; EOF { "query":"$MongoQuery", "mongoCollection":"$CollectionName" "mongoDB":"$DataBaseName" } EOF ) This would also work but I think the first one looks better. JsonArgs=$(cat &lt;&lt; EOF { "query":"$(printf '%q' "$MongoQuery")", "mongoCollection":"$(printf '%q' "$CollectionName")", "mongoDB":"$(printf '%q' "$DataBaseName")" } EOF ) The printf might quote things that aren't necessary to quote for json.
&gt; Just because they aren't what people using modern languages aren't used to doesn't mean they aren't sane. The thing is that they aren't consistent; with braces, once you know it, you know how to start and end any block. In some POSIX-incompatible shells, they do away with keywords like `then` and `do` and end all blocks with the same word, like `end`. &gt; What is intuitive depends on the person. How is Bourne shell intuitive? There is no way someone would guess the syntax of all its constructs, even if they already know one and/or they're a programmer.
Why not `read -d "" JsonArgs &lt;&lt; EOF`? It avoids using a subshell and an external command. You can also use the `${var@Q}` parameter expansion to shell-quote strings.
 &gt; The thing is that they aren't consistent; with braces, once you know it, you know how to start and end any block Oh no, I need to match three pairs of two words. Woe is me! A 3 year old would have no problem with this, how come an adult programmer does? &gt;How is Bourne shell intuitive? There is no way someone would guess the syntax of all its constructs, even if they already know one and/or they're a programmer. You can't really guess the syntax for any programming language though.
&gt;Why not read -d "" JsonArgs &lt;&lt; EOF? Out of habit. &gt;You can also use the ${var@Q} `${var@Q}` doesn't escape quotes, so it's not really usable for this case.
`%F`?
&gt; Oh no, I need to match three pairs of two words. Woe is me! A 3 year old would have no problem with this, how come an adult programmer does? Yes, it's simple and I personally find the syntax quite memorable. However, a lot of programmers who just use it occasionally on the side struggle with it and frequently have to look it up. &gt; You can't really guess the syntax for any programming language though. You can to some extent, with the knowledge I specified. As I said, in shells like `fish`, you simply end blocks with `end` and there are no keywords like `then`. So it's just `if command; commands; end`, `while command; commands; end`, `function name; commands; end`, etc. I actually "guessed" (in quotes because they're basically the same) the latter two writing this and checked it after. And in Elvish (another POSIX-incompatible shell), it's `if boolean { commands }`, `while boolean { commands }`, `fn name { commands }`, etc. All three of those have different syntax in Bourne shell.
&gt;However, a lot of programmers who just use it occasionally on the side struggle with it and frequently have to look it up. They should go back to kindergarten. &gt;You can to some extent, with the knowledge I specified. What you said you have to already know. It's not a guess if you had already learned it anyways. In both cases you have to learn these things no matter what. It makes no difference that the rules aren't the same. &gt; And in Elvish (another POSIX-incompatible shell) You're making my case of having to learn the syntax regardless for me. &gt;All three of those have different syntax in Bourne shell. Yet you have to learn them just the same.
&gt; They should go back to kindergarten. ... &gt; What you said you have to already know. It's not a guess if you had already learned it anyways. In both cases you have to learn these things no matter what. It makes no difference that the rules aren't the same. But I only have to know the syntax for one command, which is the rule I stated earlier, because they're consistent. I didn't know `fish`'s while loop and forgot the function when I wrote that comment, but I got them on the first try since it's so intuitive. I'm starting to think that you're trolling...
&gt;But I only have to know the syntax for one command You also have to know the rule.
What do you mean by that?
Most of the syntax for programming languages has to be learned. Guessing won't get you far. So getting hung up on having to learn literally 6 words is pathetic. That's what I mean.
%x works for me, incidentally just that 19.04.2019
&gt;Should I use a module like.... The answer is almost always yes. Doesn't matter what you're doing. You may code alone, but we all work together. Use the tools that are available.
If it is files in a dir I'd do the same as you, but if you would change it to a file I'd do this https://mywiki.wooledge.org/BashFAQ/001 If your gonna dive into Bash I'd suggest you read abit in there :)
What is the grep [p]... for?
The program "jo" is made to do exactly what you want. It's a little different from jq, and much less general purpose -- it's really just for building up JSON strings from different inputs. https://github.com/jpmens/jo One downside: unlike jq, as far as I know jo isn't in any package managers. If you need this script to be relatively portable (e.g. you want to run it on fresh EC2 boxes without too much pain), that may be a hassle. If you only need to run this on a few machines, though, building jo from source a couple times won't be a big deal.
I wouldn‚Äôt use bash for this.
About the date, dd.mm.yyyy sequence is standard where I leave. I only hand formated the date there because I don't like dd/mm/yyyy format (with slashes) that would be date program default format defined by the locale. To be fair, people most surelly will have to customize things like the battery sys folder or if they want, a cabled connection. I am happy we can discuss this here, thank you guys! Fork it at will.
Please read my note bellow. :D
&gt; Ah, of course. Thanks for the help. Also, I /love/ the way bash does that. How to end an if statement? fi How to end a case, esac. That's a Bourne shell thing, not a bash thing (but bash inherited it of course). Also fun to know: do...done was originally do...od, but the od was changed to done to avoid a conflict with the od utility.
Well damn, they should have fixed the utility to avoid conflict with the awesomeness.
[Objectively correct](https://xkcd.com/1179/), everyone else is wrong forever
Probally the easiest way using sed doing a find and replace Echo "This is a line of text" | sed s/is/newword/g
`sed` has a replace inline feature... e.g. `sed -i ‚Äòs/foo/bar/‚Äò filename.txt`. That‚Äôs a contrived example... you may also want to check the manpage about either doing the search/replace on a certain line only, and/or doing it for only one instance instead of globally.
&gt; search/replace on a certain line only, and/or doing it for only one instance instead of globally. &lt;line number&gt; { &lt;expression&gt; } or /&lt;pattern&gt;/ { expression } These are the forms I'm aware of, but these might be convoluted, from my limited knowledge.
A couple ideas: VOLU=$( amixer get Speaker | awk -F'[][]' '/dB/ { if NF&gt;3 &amp;&amp; $4=="[off]" print "Mute; else print $2; exit }' ) USAG=$( df --output=pcent / ) MEMO -&gt; awk '/Foo/ { print $2 }' /proc/meminfo read LOAD x &lt;/proc/loadavg
 sed -i -e 's|is|might|' \ -e 's|of|blah|' &lt;&lt;&lt;this is a line of text
That's roughly how you'd do it using`awk`, not `sed`, but either too lol do the job!
Thanks I‚Äôll try this.
oops. though i omitted the sed command itself, it does work in the version of sed in the archlinux repo, anyway.
 echo This is a line of text | sed '/^This/ s/\bis/is not/g' echo This is a line of text | sed '1 s/\bis/is not/g'
You might want to change the match for "is" to "\\bis". "\\b" matches the word boundary. Otherwise you would also match the "is" in "this".
I wish it wasn‚Äôt standard for you to leave. üíîü§î
Hahahaha... thanks, fixed the typo.
Dude, based on the stuff your requesting... it really sounds like you want stats from your servers you can query? Have you looked into graphite or collectd and graphana ? Maybe I‚Äôm way off, otherwise good luck!
 line="This is a line of text" line2=${line/ is / of } line3=${line2/ of / over } echo $line2 This was a line of text echo $line3 This was a line over text
Move all your `&lt;function prototype&gt;`s into your header file.
You ought to be able to do: &lt;main&gt; &lt;function 1...n&gt; main() Bash will be mad if you call a function before it's defined, but by the time `main` is actually called all your functions will be defined.
Wow O\_O &amp;#x200B; I really didn't think about it
An additional benefit to this approach is that it forces the entire file to be parsed before any significant code is executed. It's a lot easier to catch syntax errors that way.
No worries, bash is a bit weird for mostly legacy reasons.
Though that is pretty common in interpreted language (thinking of python, which always disappoint me on that point, particularly for decorators in short scripts)
how is the above approach better than: &amp;#x200B; `function1() {}` `function2() {}` &amp;#x200B; `main_code` &amp;#x200B; In C, main() is usually in an independent file but with bash it's usually part of the same file.. why use a main(){} - unless your main function is somehow super complicated?
Lots of room for improvement here, especially if the linux kernel you're running is compiled with procfs and sysfs support (which yours is, since `acpi` and other stuff is working). I know it's pretty tempting and easy to exec a bunch of text processing utilities on some high level output, but with something that needs to update so often such as a status line shell script, it's best to do less. For example, your `BATT=$( acpi | cut -d ',' -f 2 | tr -d ',' | tr -d " " )` can be reduced to `BATT=$(&lt; /sys/class/power_supply/BAT0/capacity)`. This costs 1 subshell and a file read, compared to your 1 subshell and 4 execs. It also depends on nothing but linux sysfs. If you're totally crazy, you can go one step further and set up a `cat` `coproc` to be reused, just remember to set up a killer in a trap. `coproc` is bash specific though.
Yeah, python's a bit weird because Guido does what he want. :)
`cat &gt;&gt; journal_$(date +%H_%M_%d_%m_%y).txt`
is there more to it than that? #!/bin/sh LOGDIR=~/log mkdir -p "$LOGDIR" $EDITOR "$LOGDIR/$(date +%y-%m-%d_%T)"
\*logger\* ?
jrnl should do it, i think it's in the repos.. http://jrnl.sh/ is the link
if it were me, I'd go without the `_%T` in the format string to be able to edit the same file over the course of a day. but that's totally up to OP's taste
What do you mean by soft quota exactly?
I don't know anything about the kind of setup you're talking about, but you mentioned encryption and email, so I googled "TLS SMTP" and SMTPS seems to be a thing, so maybe that would be a good starting point? Or is PGP specifically important here?
PGP is specifically important within this context. On other scripts we've encrypted everything client side before a network connection is even initialed. Then it's sent to a secure inbox with the only version of the Private Key. Whilst i appreciate the security of TLS SMTP or SMTPS, we are mind set on removing the risk of in transit interception to the best of our abilities.
You could just have rsyslog use tls encryption to the central log host. Plenty of documentation on this topic online.
This is a little further off track that I had initially intended but upon reading it its something that i now want to make a part of my project. Thank you for your insight, best wishes!
Hehe, yeah understood. Don't go to far off into the weeds. Start with small achievable steps. My main thing is protecting the audit logs from tampering, and encryption does that. In the past I've protected the central location his about the same as my primary Kerberos/ldap domain host. Anyways good luck!
I can feel this one being an all nighter. Thanks for your help. Take it easy
 Disk quotas for user test (uid 1006): Filesystem blocks soft hard inodes soft hard /dev/sdb1 9252 8192 10240 6 0 0
I mean the disk quota you configure with edquota -u $user
There are a lot of things that need to be addressed here: * "data that exceeds quota" - How is this determined? How do you tell which part of the data to be compressed? * What if the compressed data doesn't fit in the quota either? A script that checks the quota status and performs some actions is easy enough to write. But the actions you specify are vague and can have unintended side effects.
Learn to format your posts properly.
I think `read` is taking a word, not a line. Looks like you want to then word split the lines? Would be helpful if we had some data to look at.
tempField and tempMatrix both have matched matrices in them so, they would both look like: 1 2 3 4 5 6 7 8 I would want the for loop to read each number, then multiply, so the formula works out (1x 1) + ( 2 x 2)+(3x3)+(4x4), then continue on with each line.
Indeed, "it compresses the data that exceeds the quota" jumps out to me as a very bad idea. Don't mess with a user's data unless you know exactly what that data is and you have their permission. If you're talking about filesystem-level compression (like what btrfs has), it's probably a little less bad. In any case, I would go with the approach of notifying the user and giving them pointers on how to bring down their usage, leaving the responsibility of resolving the issue to them.
If I'm not mistaken, `read` takes a line in this case. It will split on words if there is more than one variable to assign to, but it will assign the rest to the last one; if there is just one variable, then it assigns the whole line to it.
And for that I am really thankfull!!! I am glad I shared this. Thanks, I'll look for improvement related to your observations!
Thank you! I will sure look for this! :D
`for` loops can only assign to one variable. You'll probably want to look at using a `while read &amp;&amp; read` loop similar to your outermost loop. `read` can split on `IFS` characters and assign to multiple variables at once. This looks like a homework question (I wish I got some as interesting as this), so I'll err on the caution and say that the rest is up to you to figure out. It should be enough to get you started. Also, your `product` line looks wrong.
not sure how you are set up, but assume your users are on a partition /home, then something like; /usr/sbin/repquota /home | grep [user] |awk - F " " '{print $3 " " $4 " " $5 " " $6} will give you the blocks, quota, limit, and files You will have to take it from there, like check if $4 gt X and if so compress the home directory.
[removed]
Maybe silly here. Why not scripting encryption of a file container with luks+pgp and place those logs into it before sending ? Also script on client side something to easily decrypt it ?
 set -- $lineB use $1 and shift to iterate.
 man openssl
&gt; 1 2 3 4 &gt; 5 6 7 8 &gt; (1x 1) + ( 2 x 2)+(3x3)+(4x4) Do you want this, or `(1*5)+(2*6)+(3*7)+(5*8)`?
That‚Äôs a cracking idea. As files are produced I can have a script PGP encrypt them then email them out. That‚Äôs great for the /var/log directory I‚Äôm thinking about trying to PGP encrypt my Pam.d/ssh notifications
You can read the rows into arrays, then iterate those with a for loop: while read -ru3 -a lineA &amp;&amp; read -ru4 -a lineB; do for i in "${!lineA[@]}"; do # iterating the indices printf 'Do something with %s from fileA and %s from fileB\n' "${lineA[i]}" "${lineB[i]}" done done 3&lt; "$tempField" 4&lt; "$tempMatrix"
Ok, now this is epic. Parallel is one of my absolute favorite utilities. I'll definitely consider organizing a parallel party.
What about using the `read` built-in instead of process substitution?
Maybe you should have pinged /u/TitusDevOps or replied directly to him so he gets notified.
Nice. Please give a feddback when accomplished. I'm curious about what pam can do.
I have answered my own question. `grep -Fxvf dbserver.list dbbackup.list`
[removed]
You could try setting it in /etc/.profile
diff
This is really cool. For those who don't know, you can use `CTRL+Z` to put a running task in the background; then use `jobs` to list the running background tasks. Using this feature, with timec you could create Chicken, Rice, and Egg timers, for example, and all put them in the background. To check on them simply use `fg X` with X being its job number (which you use `jobs` to find). `kill %X` will stop the specified job.
 awk '!s[$0]++ &amp;&amp; NR &gt; FNR' dbserver.list dbbackup.list
Great, hahaha. You only have to remember it's not a timer... but... compare its value to something and... :D
Ha - that's even better and probably the most optimal way, nice one!
Good point. I'll give that a try. However, I should have clarified, it seems to forget even within the same session. Maybe 30 minutes later, for example. But whether your suggestion fixes it or not, it's a good idea to put it in my login script. I'm wondering if it could be related to tmux or iterm2? I'll have to do some more experiments. And this is a tough thing to search for online, because the word "tabs" has other meaning WRT to bash.
`tabs` is *not* a Bash command; you can see the list of Bash built-ins by running the `help` command. It's an external utility, from `ncurses`. This is not supposed to happen; there is no timer or anything. The problem is probably with your terminal or something you did. Perhaps you reset your terminal or ran something that used `tabs`, too, and reverted it back to the default after it was done?
Okay so doing `grep --line-buffered` and removing the `...` after the command has solved the issue. Although I cant say i fully understand why the `--line-buffered` is required.
Noted, I worded my title incorrectly. It is of course an external utility, on the man page of my system it says it's from BSD. `ncurses` is also installed, so that may be where it got picked up. I'm not aware of any changes to the `tabs` settings from within any scripts that I run, but maybe it's the terminal.
Thank you this is exactly what I was after!
`comm` is also handy
I just did a test. Set `tabs -24` and tested it, it worked fine. I didn't do anything for two hours, tried it again, and tabs had been reset. It doesn't appear to be an option to view the tab settings, or else I could run a script to check and see how long it takes to reset.
You could use the [`SECONDS`](https://www.gnu.org/software/bash/manual/bash.html#index-SECONDS) Bash variable to get a second counter.
This is what awk was made for! Check this out -- here's the last entry of the EXAMPLES section for `man awk` on my machine: &gt;Run an external command for particular lines of data: &gt; tail -f access_log | &gt; awk '/myhome.html/ { system("nmap " $1 "&gt;&gt; logdir/myhome.html") }' So yours would be: tail -f -n 1 /home/clu/logs/mpris.log | awk ' /Paused/ { system("kill -STOP...") } /Playing/ { system("kill -CONT....") }'
grep, like most programs, uses full buffering when output is not to a terminal. You don't need that grep, btw, in that pipeline. Just ignore the \*) case.
Please format code as code to preserve indentation.
Table of Contents for the lazy &amp;#x200B; &amp;#x200B; &amp;#x200B; `1. Introduction` `1. Bash Scripting Fundamentals: Introduction 00:01:13` `2. Lesson 1: Creating Your First Shell Script` `1. Learning objectives 00:00:36` `2. 1.1 Why Scripting in Bash Makes Sense 00:02:39` `3. 1.2 Choosing an Editor 00:02:45` `4. 1.3 Core Bash Script Ingredients 00:07:01` `5. 1.4 Storing and Running the Script 00:06:52` `6. 1.5 Using Bash Internal Commands versus External Commands 00:03:27` `7. 1.6 Finding Help About Scripting Components 00:03:30` `8. Exercise 1 00:00:47` `9. Exercise 1 Solution 00:03:52` `3. Lesson 2: Working with Variables and Parameters` `1. Learning objectives 00:00:39` `2. 2.1 About Terminology 00:01:57` `3. 2.2 Using and Defining Variables 00:04:57` `4. 2.3 Defining Variables with the read Command 00:09:14` `5. 2.4 Understanding Variables and Subshells 00:06:23` `6. 2.5 Sourcing 00:06:24` `7. 2.6 Quoting 00:07:32` `8. 2.7 Handling Script Arguments 00:13:57` `9. 2.8 Understanding the Need to Use Shift 00:04:21` `10. 2.9 Using Command Substitution 00:02:14` `11. 2.10 String Verification 00:04:16` `12. 2.11 Using Here Documents 00:05:02` `13. Exercise 2 00:00:44` `14. Exercise 2 Solution 00:01:49` `4. Lesson 3: Transforming Input` `1. Learning objectives 00:00:34` `2. 3.1 Working with Substitution Operators 00:05:34` `3. 3.2 Using Pattern Matching Operators 00:06:41` `4. 3.3 Understanding Regular Expressions 00:03:22` `5. 3.4 Calculating 00:04:43` `6. Exercise 3 00:00:50` `7. Exercise 3 Solution 00:01:47` `5. Lesson 4: Using Essential External Tools` `1. Learning objectives 00:00:29` `2. 4.1 Using grep 00:02:19` `3. 4.2 Using test 00:05:25` `4. 4.3 Using cut and sort 00:04:40` `5. 4.4 Using tail and head 00:01:21` `6. 4.5 Using sed 00:03:40` `7. 4.6 Using awk 00:04:37` `8. 4.7 Using tr 00:01:33` `9. Exercise 4 00:00:52` `10` `1. Learning objectives 00:00:28` `2. 6.1 Working with Options 00:05:30` `3. 6.2 Using Functions 00:03:45` `4. 6.3 Working with Arrays 00:04:30` `5. 6.4 Defining Menu Interfaces 00:06:29` `6. 6.5 Using trap 00:02:30` `7. Exercise 6 00:01:13` `8. Exercise 6 Solution 00:07:29` `8. Lesson 7: Script Debugging and Analyzing` `1. Learning objectives 00:00:36` `2. 7.1 Design Considerations 00:03:32` `3. 7.2 Common Analyzing Tools 00:04:00` `4. 7.3 Using bash -x 00:02:00` `5. Exercise 7 00:00:33` `6. Exercise 7 Solution 00:05:57` `9. Lesson 8: Scripting by Example` `1. Learning objectives 00:00:35` `2. 8.1 Monitoring CPU Utilization Part 1 00:10:38` `3. 8.2 Monitoring CPU Utilization Part 2 00:09:03` `4. 8.3 Practicing Calculation 00:07:22` `5. 8.4 Reading an init Script 00:06:28` `6. 8.5 Using a Countdown Script 00:03:34` `7. 8.6 Efficient if then fi 00:04:53` `8. 8.7 Simple Process Monitoring 00:06:38` `10. Summary. Exercise 4 Solution 00:01:42` `6. Lesson 5: Using Conditional Statements` `1. Learning objectives 00:00:25` `2. 5.1 Using if then fi 00:03:36` `3. 5.2 Using &amp;&amp; and || 00:05:45` `4. 5.3 Using for 00:07:35` `5. 5.4 Using case 00:04:54` `6. 5.5 Using while and until 00:05:33` `7. Exercise 5 00:01:38` `8. Exercise 5 Solution 00:02:38` `7. Lesson 6: Using Advanced Scripting Options` `1. Learning objectives 00:00:28` `2. 6.1 Working with Options 00:05:30` `3. 6.2 Using Functions 00:03:45` `4. 6.3 Working with Arrays 00:04:30` `5. 6.4 Defining Menu Interfaces 00:06:29` `6. 6.5 Using trap 00:02:30` `7. Exercise 6 00:01:13` `8. Exercise 6 Solution 00:07:29` `8. Lesson 7: Script Debugging and Analyzing` `1. Learning objectives 00:00:36` `2. 7.1 Design Considerations 00:03:32` `3. 7.2 Common Analyzing Tools 00:04:00` `4. 7.3 Using bash -x 00:02:00` `5. Exercise 7 00:00:33` `6. Exercise 7 Solution 00:05:57` `9. Lesson 8: Scripting by Example` `1. Learning objectives 00:00:35` `2. 8.1 Monitoring CPU Utilization Part 1 00:10:38` `3. 8.2 Monitoring CPU Utilization Part 2 00:09:03` `4. 8.3 Practicing Calculation 00:07:22` `5. 8.4 Reading an init Script 00:06:28` `6. 8.5 Using a Countdown Script 00:03:34` `7. 8.6 Efficient if then fi 00:04:53` `8. 8.7 Simple Process Monitoring 00:06:38` `10. Summary` `1. Bash Scripting Fundamentals: Summary 00:00:25`
thanks, did as you told and provided some additional informaiton :)
Thanks, now it's more readable.
Any what will you do when the file will be "rewritten"? There are some applications that deletes log file and creates a new one, thile tail will just simply hang and won't get any data after...
 #!/bin/bash err() { printf 'ERROR: %s\n' "$@" &gt;&amp;2 exit 1 } restore() { echo "Implement restore here" } backup_db() { echo "Implement db backup here" } backup_plugins() { echo "implement plugin backup here" } case "$1" in (backup) case "$2" in (sql) backup_db ;; (plugin) backup_plugins ;; ("") backup_db; backup_plugins ;; (*) err "unknown option $2" ;; esac ;; (restore) restore ;; (*) err "unknown option $2" ;; esac
oh thanks, thats exactly what i was hoping to gain &amp;#x200B; thanks for your effort in even typing some code out really appreciate it, does look cleaner than my version
No problem.
 backup_sql(){ #sql specific part } backup_plugin(){ #plugin specific part } backup_full(){ #full backup specific part } backup(){ case $1 in sql) backup_sql;; plugin) backup_plugin;; full) backup_full; #common backup procedure part } restore(){ #same as in backup but for restore } while [ ! -z $1 ] do if [ $1 == backup ] then procedure='backup' elif [ $1 == restore ] then procedure='restore' elif [ $1 == sql ] then type='sql' elif [ $1 == plugin ] then type='plugin' else exit 1 shift done [[ -z type ]] &amp;&amp; type='full' ${procedure} $type
Interesting will check out when I get home in a few days.
I recommend using Ryan‚Äôs tutorials personally. Gave me a good grasp of the fundamentals was also very fun!
If you want files with whitespace before smartctl matched then change the `$0` to `$1`. find yourdirectory -type f -exec awk '$0 ~ /^smartctl/ {print FILENAME} {nextfile}' {} +
although not exactly what i was looking for because i wanted to have 2 different parameters input, this is still an interesting idea dynamiccaly calling specific functions &amp;#x200B; Thanks for your opinion!
it accepts two parameters, e.g `script backup sql` or `script plugin restore`
ah thank you very much now i get it the `shift`
He offers samples of his youtube channel if you want a taste. https://www.youtube.com/channel/UComgXoI6pysmetOzuNH_TDQ
This does not output any files for me. I also tried echo instead of printf, but I get no output: list_files "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/" list_files "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/var/log/smart_result/" for file in "${files[@]}"; do echo "$file " #printf '%s\n' "$file" done
Did you include the function in your script? Is there whitespace before the "smartctl"?
Yes, I included the function in my script. No, theres no whitespace before smartctl, the files directly start with "smartctl".
Have you tried putting `set -x` before the list_files uses and checked that the directories are correct?
Try this ```sh #!/usr/bin/env bash # example: ./script.sh . # define array declare -a files list() { counter=0 for f in $(find $1 -type f -name 'smartctl*'); do files[$counter]="$f" counter=$(( $counter + 1 )) done } # make a file list list $1 for file in "${files[@]}"; do echo "$file" done ```
Yes, the directories are correct, but still no files are found. + list_files /home/me/Downloads/neu/debug_125000/dsm/result/ + mapfile -O 0 -d '' files ++ find /home/me/Downloads/neu/debug_125000/dsm/result/ -type f -exec awk '$0 ~ /^smartctl/ {printf "%s\0", FILENAME} {nextfile}' '{}' + + list_files /home/me/Downloads/neu/debug_125000/dsm/var/log/smart_result/ + mapfile -O 0 -d '' files ++ find /home/me/Downloads/neu/debug_125000/dsm/var/log/smart_result/ -type f -exec awk '$0 ~ /^smartctl/ {printf "%s\0", FILENAME} {nextfile}' '{}' + Do I maybe need to add * to the path for file expansion or does find do that already?
This find runs the awk command for each file in the path. It also recurses subdirectories. You could try running the command directly to the path to test it. find /home/me/Downloads/neu/debug_125000/dsm/result -type f -exec awk '$0 ~ /^smartctl/ {printf "%s\0", FILENAME} {nextfile}' {} +
\`\`\` code blocks aren't part of markdown used by reddit. You need to indent your code with four spaces to make it code block.
Thank you. Comment edited.
You don't need -z in [[ ]] to test if variable is set. Also I think the pile of if/elif would look a bit nicer as case. while (( $# )); do case "$1" in (backup|restore) procedure=$1;; (sql|plugin) type=$1;; (*) exit 1;; esac shift done [[ $type ]] || type='full' $procedure $type
Looking at your answer I realize OP meant the first word in the filename and not the first word in the file.
Your for loop will break for files with whitespace in the filename. Also the "$1" probably should be quoted.
Here is your basic loop for I in smartctl* do files+=($I) done Of course you probably don't need a loop for that, especially if you say there are multiple directory. find . -type f -name smartctl* | readarray files But that uses a pipe, which make the shell slower. To each her own preference. Good luck.
Fyi printf '%s/n' ${files[@]} Does the same thing as a loop, the over flow prints on new lines. Printf is a poor man's loop construct.
fixed
Sure, here the print was just used as example. OP would probably replace it with something else.
You can eliminate the counter if you just use `${#files[@]}` though I'm not sure how this would compare performance wise. Also instead of while read etc. you could just pipe the find straight to mapfile.
yeah, `(( $# ))` is a cleaner solution to check for available parameters, gonna keep that in mind, ty. as for `[[ -z $var ]]` vs `[[ $var ]]` (empty string vs trying to evaluate whatever's in there to true|false), it's just personal taste, mostly. `if` is definitely easier to read and understand for novices. why open parentheses in `case` conditionals? never seen anyone do this before.
&gt;why open parentheses in case conditionals? never seen anyone do this before. I just think it looks better :p
&gt; With out copying/pasting In the first 10 seconds he copy/pastes into his no copy paste solution of Vim deletions....
It covers the basics. If this something that you are looking for, go ahead. I like Sanders methods
I cannot see the video atm, but this may be helpful related information: a simple plugin which makes replacing a selection with text from the default register easier in fewer keystrokes: [https://github.com/vim-scripts/ReplaceWithRegister]
You mean the entries within the file would have a datestamp of when you're running the script?
Correct. For example: &amp;#x200B; \`Tue Apr 23 12:42:16 EDT 2019 one\_scoop.csv -&gt; two\_scoops.csv \`
I have this function at the top of most of my bash scripts: function log { echo -n "$(date -Iseconds) ${1}" &gt;&gt; "${LOGDIR}/${LOGNAME}${LOGDATE}.log" } Which lets me do this: log "This message is logged" Which looks like this in the log file: 2019-04-22T20:48:40-04:00 This message is logged Does this help?
If you have moreutils loaded, you could try this: mv -v file2 file12 2&gt;&amp;1 |tee -a &gt;(ts "%m-%d-%y %H_%M_%S" &gt; log.log)
Interesting. I enjoyed his RHCSA/RHCE book.
Just watching the work flow was worth the video
You have a couple of options. Since you are already looping through each file and moving one at a time you can simply do this: mv -v "$file" "${file/$find_string/$replace_string}" 2&gt;&amp;1 | (echo -n "`date` ";cat) |tee -a $LOG_FILE The only problem with this is that if `mv` outputs multiple lines then the lines following the first line won't have a date stamp in front. You can concatenate all the lines together into one line like so: mv -v "$file" "${file/$find_string/$replace_string}" 2&gt;&amp;1 | (echo -n "`date` "; echo `cat`) |tee -a $LOG_FILE If you want to handle multiple line output but keep them separate then: mv -v "$file" "${file/$find_string/$replace_string}" 2&gt;&amp;1 | while read line; do echo "`date` $line";done |tee -a $LOG_FILE Now for the funnest (is that a word?) bit of mctrickery IMHO: exec 2&gt;&amp;1 exec &gt; &gt;(while read line; do echo "`date` $line";done | tee $LOG_FILE) string_rename() { printf "%s\n\n" "Renaming files..." for file in *; do mv -v "$file" "${file/$find_string/$replace_string}" done printf "\n%s\n\n" "Done." } This will log anything your script outputs to the file with a datestamp.
+1 for Ryan's tutorials. Cleanly laid out, cuts straight to the point, everything you need and nothing you don't.
Thank you for the info :) And i should have known grep was not needed in the pipe :/
&gt; this is what awk was made for! Thank you for the comment, and I could not agree more. I initially wanted to use awk because of what you stated above and as an exercise to get more familiar with it, but I can not figure out how to properly concatenate the system call lines... my brain just glitches out, uhg. This is the full command... `sleep 1 &amp;&amp; kill -STOP $(pgrep -f 'conky -q -c /home/clu/.config/conky/conky2.conf')` I totally prefer the awk method, maybe I just need to look at it with fresh eyes.
This is a case where I think it might be good to ask why you need to know the location of the red square... Doesn't really sound like a problem well suited for bash. I would probably start with converting the image to black and white with ImageMagick. There should be a way to get it to set only the specific red you want to detect to black. Next I would convert the image to a text representation with something like [farbfeld](https://tools.suckless.org/farbfeld/) and then try to process it.
Why would you do such a task in bash? Better use a langiage like python or ruby
Basically what I'm trying to do is find a button and get the coordinates of it. The button looks the same every time but changes position based on how much text precedes it
Yeah, those would probably be better, but I don't know either.
A hacky solution would be to take a screenshot and have another image which is just the button and then do ImageMagick subimage search. You can get the coordinates of the subimage with this and then use those with xdotool or something similar to click on it.
I've actually attempted basically this exact setup. Doing a subimage search takes about a minute to a minute and a half, so that didn't work for my application. Nevertheless, thanks for the help.
You need to tell us way more to get proper help IMO. Why do you need to hit the button? Is it a web app, or what? What happens when you hit it? What have you tried so far? Why bash?
You need to tell us way more to get proper help IMO. Why do you need to hit the button? Is it a web app, or what? What happens when you hit it? What have you tried so far? Why bash?
Did you try with the different metrics that are available?
If you can convert the image to XPM - using Imagemagick for example - it is *possible* that you could solve it using bash. [https://en.wikipedia.org/wiki/X\_PixMap](https://en.wikipedia.org/wiki/X_PixMap)
**X PixMap** X PixMap (XPM) is an image file format used by the X Window System, created in 1989 by Daniel Dardailler and Colas Nahaboo working at Bull Research Center at Sophia Antipolis, France, and later enhanced by Arnaud Le Hors.It is intended primarily for creating icon pixmaps, and supports transparent pixels. Derived from the earlier XBM syntax, it is a plain text file in the XPM2 format or of a C programming language syntax, which can be included in a C program file. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Bash is not the proper tool. Try an image processing toolkit.
1: the button advances to the next screen in a web app I'm trying to automate 2: Yep 3: see 1 4: I know bash but I don't know much else
What do you mean?
OpenCV in python will do this nicely and easily. Not a job you should do in bash.
No, loops do not introduce a new variable scope: $ unset x; for x in a b c; do :; done; echo "$x" c
You've exposed me as a fraud.
I *think* you can lose the single quotes around 'conky...' and use backslashes instead: tail -f -n 1 /home/clu/logs/mpris.log | awk ' /Paused/ { system("sleep 1 &amp;&amp; kill -STOP $(pgrep -f conky\\ -q\\ -c\\ /home/clu/.config/conky/conky2.conf)") } /Playing/ { system("kill -CONT...") }'
That indeed did it. Thank you. The main factor involved in this is an awk program that uses `mediainfo` and `mpris2` to parse metadata. So it made sense to use awk here as well I suppose. I true wizard with awk wrote the majority of the program, I still cant understand the more involved parts of it, but I intend to get there someday! Thanks for your help :)
Sure! I barely know how to use it, but I love awk. It's so weird. I bet that program is a real treat to read -- maybe it holds the secret to becoming an awk wizard yourself.
&gt; but I love awk. It's so weird Haha truth. Perhaps it does hold a secret, and I am just unable to see it yet :) Here is a link to it in base64, for any curious eyes that may pass through. https://www.pastiebin.com/5cbfdf6db9f76 Cheers.
You could also try to send keystrokes to navigate to the button. "Click" on a fixed part of the app and use the tab key to get to the button.
I love Kai's youtube channel. Top notch confusing content.
execs should be swapped, or 2 is left at original 1 ?
Thanks for adding to the noise.
Oh my. The man himself! Keep it up!
[removed]
Yes, bash (and most other languages) are imperative: the script controls the flow of the program, which runs essentially in a straight line barring any process backgrounding, which is done explicitly and forks an entire separate process, or threading, which bash can‚Äôt really do. You can test this yourself by littering your code with ‚Äúecho‚Äù messages if you like.
Thanks very much for the response! I'll definitely add some echo commands to see how it plays out. I just want to make certain really about one line in this script: &amp;#x200B; greeting\_occasion=0 &amp;#x200B; I'm assuming that when this script runs, the interpreter will parse through the entire script, then return to the beginning of the while statement, b/c setting greeting\_occasion=0 would simply reset this variable back to a value of zero after each pass, thus rendering greeting\_occasion=$((greeting\_occasion + 1)) ineffective. Am I correct in this assumption?
Bash is an interpreted language, meaning the script is actually parsed statement by statement. For example, if a statement has a syntax error but doesn't need to be interpreted yet, the script will run until that statement is interpreted, then crash and burn with a cryptic error. The while statement is a statement and therefore needs to be parsed entirely, but only once it's encountered (after the initialization of greeting_ocassion). And unlike in non-interpreted languages, some processing of each statement in the while loop will be done every time each line is run and re-run. As far as control flow, as the other commenter mentioned, the lines in the script will be executed from the first line of the script to the last, including the while. So, greeting_occassion will be set to 0, then the loop will begin, and start evaluating expressions involving $greeting_occasion or executing statements involving greeting_occasion. Since things are executed from top down, and since the initialization doesn't appear in a loop, the initialization will only be run once, and the increment will be very effective. Btw, an easy way to trace the execution of the script is to turn on tracing by running: `set -x` ...where you want tracing to be turned on in your script. Run `set +x` in your script where you want to turn tracing off. This could be handy if your script runs a lot of statements, but you only need to debug a small portion of them.
Thank you for attempting to clarify this for me! Just so I'm crystal-clear on this, the initial three variables are interpreted, the while statement then runs, greeting\_occasion is iterated by one, then the while loop runs again, etc. In other words, the only time that greeting\_occasion=0 is interpreted is on the first through the script, correct?
Correct. To confirm, just below the #! line, run `set -x` and run the script to see what happens. I'm curious. Are you coming from a language with hoisting or shadowing? Maybe JavaScript, by chance? Your particular question reveals at least a little baggage üíºüòÄ
Thank you for helping me understand this concept. :) &amp;#x200B; Actually, I have no relevant experience w/ coding. I'm in the process of truly learning Linux from the CLI, and shell scripting is one of the covered topics. I've worked in IT in the past, so I'm familiar w/ reading code, but I've never written it myself.
It's a truly insightful question. You'd have fooled me üòâ. Enjoy your learnings.
Good video. BUT. I wold like more explanation on what you typed for commands and why you typed those commands.
Well, thank you very much! That's kind of you to say. I'm looking forward to gaining a greater understanding of technologies I'd always wanted to work w/. Linux is just the first step in a long journey. :)
 I'd just like to interject for a moment. What you're referring to as Linux, is in fact, GNU/Linux, or as I've recently taken to calling it, GNU plus Linux. Linux is not an operating system unto itself, but rather another free component of a fully functioning GNU system made useful by the GNU corelibs, shell utilities and vital system components comprising a full OS as defined by POSIX. Many computer users run a modified version of the GNU system every day, without realizing it. Through a peculiar turn of events, the version of GNU which is widely used today is often called "Linux", and many of its users are not aware that it is basically the GNU system, developed by the GNU Project. There really is a Linux, and these people are using it, but it is just a part of the system they use. Linux is the kernel: the program in the system that allocates the machine's resources to the other programs that you run. The kernel is an essential part of an operating system, but useless by itself; it can only function in the context of a complete operating system. Linux is normally used in combination with the GNU operating system: the whole system is basically GNU with Linux added, or GNU/Linux. All the so-called "Linux" distributions are really distributions of GNU/Linux.
yes - once you enter the while body, no variables outside the while body are re-initialized whilst you are in the while body
Thank you for pointing this out. For sure, it's interesting to note. I've always found GNU and POSIX to be rather esoteric topics. I just began reading a bit more about POSIX to the point that I'm now somewhat comfortable w/ its basic definition, but I must say that I still don't really understand the principle of GNU. Frankly, much of my misunderstanding comes from the naming convention they chose, but I suppose that's really neither here nor there. I'll do some more reading on GNU to gain a better understanding of what it truly is. But, thank you for elucidating this concept. It's probably the best explanation I've seen on the topic.
This is exactly what I was looking for! lol I thought that this was how it worked, but I just wanted to be certain, since we're talking about an exacting science. :)
Imagemagick has different metrics for comparison. Look for -metric on this page: https://imagemagick.org/script/command-line-options.php
If it's a webapp then why not just use selenium or some other tool made for this job specifically?
Or he could use selenium
He wants to automate webapp, possibly for testing. This is why I mentioned the "xyproblem" in my comment. He is asking for how to do X when he really needs to do Y. Selenium (or something similar) is what he should use.
Did you read this: http://mywiki.wooledge.org/XyProblem
Please format the code in your posts as code.
&gt; barring any process backgrounding, which is done explicitly and forks an entire separate process, or threading, which bash can‚Äôt really do. See bash coprocesses. https://www.gnu.org/software/bash/manual/html_node/Coprocesses.html And yeah, bash still cannot do them very well
Are there particular use cases where reusing variables is preferred?
Why use different variable names when you don't have to? If you've got two independent loops iterating over lists of files, say, it makes sense to iterate both of them using the same `file` variable.
I tried a variation on this but couldn't get the output of the \`mv\` command to write to the log. \`mv\` interpreted it as part of the argument.
You want to log the output of mv, so do this: function log { echo "$(date -Iseconds) $*" &gt;&gt; "${LOGDIR}/${LOGNAME}${LOGDATE}.log" } for file in *; do log "$(mv -v "$file" "${file/$find_string/$replace_string}")" done Without seeing what you tried, I'm not sure why mv would be interpreting anything except the arguments you give it.
&gt;tee -a &gt;(ts "%m-%d-%y %H\_%M\_%S" &gt; log.log) I don't have \*\*moreutils\*\* installed, but was wondering if you could just substitute some variation of \`date\` for the \`ts\` command?
Numbers starting with 0 are treated as octal.
Cheers found it pretty quick RTFMing... Slowly learning :)
‚Äã #!/bin/bash for n in {0..300}; do printf '%0s\n' "$n" sleep 0.2 printf '%0s\n' "$((n+3))" done
I put the \`log\` function at the end of the \`mv\` command, rather than before it. Derp. Building off of your suggestion, I did the following: log () { printf "%s\\n" "$(date) $\*" |tee -a "$LOG\_FILE" } Thanks for the suggestion.
Thank you, /r/ralfwolf. I'll take a look at your suggestions.
To force bash to interpret the number as decimal use the form $base#number. In your example replace echo $(( n + 3)) with echo $(( 10#$n + 3 )) and it should work.
Your variables stay set until they are changed. The first three lines of your script are run only once: at the start of execution. Your while loop, from `while` to `done`, is counted as a single statement that runs again and again until `$greeting_occasion` equals 3. Only the while loop contents gets repeated, not your entire script. `$greeting_occasion` is the only variable that gets modified later in the script. It acts as the while loop control variable and as the condition for your if statement.
Why don't you try with: &amp;#x200B; `for n in $(seq -w 0 300)`
Don't be dumb, that's how. Leave this place Windows weakling.
Reading some of your post history... You got some anger issues man. How about you stop some being a keyboard warrior &amp; asshole and seek some help. You need it.
Not with bash though, because I'm not a fucking dolt like you.
So you want to run the same commands on two different sets of data? Sounds like function time to me.
I genuinely hope you understand that that guy is a rare exception and aren't scared off from the sub by him. Stick around and learn stuff (I'm going to make another post with an idea for your actual question).
That is correct. I need both of those variables to get filled into the $DEVICE\_FILE variable.
I'm super new to bash myself, so I couldn't help with a lot of code, but here is the logical approach that I would make. I would have a function that steps through each file and pulls out the variable you want, and then I'd act on those variables. After acting on them, cycle back to the beginning and get the next line in the file. Repeat. Unless I am misunderstanding your question.
I didn't think of doing in that way... Which got me thinking, I would not use two separate lists but an array variable...
Not at all scared off. BASH is something I've really been needing to learn how to do.
__This person is a pedophile. Judging by his post history his mental illness is untreated__
__This person is a pedophile. Judging by his post history his mental illness is untreated__
Same. I've started a chat program I'm writing in BASH for that very reason.
As someone else said to me in a C++ thread: "once we start looking at something one way, it's hard to see it another way." Coming from PS, as you used in your example, it's a lot higher level and does a lot of the dirty work for you. As simple as the solution may be, once you've got that particular solution in your head, it can be hard to see things another way. Good luck with your work!
I think he's a troll. Still has issues probably, but I don't think it's anger related.
If the source file has dname dip space separated and one pair per line in a file, I have used awk to do similar. In awk you can use \x22 to print a " and \x27 to print a ' Another way would be to set the IFS to the newline character and use read to turn each line into an array IFS='\n' for line in $(cat input); do read -r -a array &lt;&lt;&lt; "$line"
Outside of square brakets (shown here), it means the start of a line, inside square brackets, it negates the selection.
In this context, ^ means 'beginning of line' and $ means 'end of line' So in this case "\^[0-9]{8}$" will match 12345678 but not a12345678 NOTE: if the \^ is inside brackets like [\^0-9] it means 'not' whatever's in the brackets.
**Anchoring** The caret \^ and the dollar sign $ are meta-characters that respectively match the empty string at the beginning and end of a line.
See [this](https://www.tldp.org/LDP/abs/html/x17129.html). In short when not used inside brackets `^` and `$` are anchors and denote position in a line. For example `"^$"` indicates blank line. Brackets enclose a set of characters and when `^` is used there it is used to negate the pattern for example `[^0-9]` matches anything other than a digit.
&gt;Outside of square brakets (shown here), it means the start of a line, inside square brackets, it negates the selection.edit: he even says a few seconds later at "31:26" Thank you very much. What is the meaning of that ''8'' inside that braket?
Linuxacademy has some nice stuff.
The 8 is to indicate that he want's 8 of the things; and there is no negation here. Please read up on regular expressions; learning about the general syntax (as it differs slightly between regex types), will answer all of these questions.
Hey /u/luchins I am starting a study group in a week or so, that will be dedicated to learning linux and bash. I really think, you would fit in. I will message you the details tomorrow.
This sounds like an ideally case for the use of jobs. Creating a job, well keep the terminal open until you release the job.
The difference between sleep and read is that read is a built-in, sleep handles ctrl-c and co itself. If you really need to stop any further execution after the signal handler is done, just exec another shell maybe?
Your code seems to be incomplete. Show a minimum working example, the input data and the output format. As for the specific problem of iterating over two lists it is not possible to do it like you showed in powershell, but you can use associative arrays to loop over key and values. `jq` is a tool that can convert a json into a parseable format. IMO this is more easily solved using Python.
I think ctrl-c is delivered to sleep and read and your trap. Also you might try this: ``` function __ctrl_c() { echo "bye" } trap '__ctrl_c; return 1' SIGINT ```
`sleep` isn't a builtin, so the signal gets sent to `sleep`, not your script. `read`, on the other hand, *is* a builtin, so your script gets it. `sleep` is a loadable builtin, so you can test this by enabling it with `enable -f /path/to/lib/bash/sleep sleep` (usually the path is `/usr/lib`). Notice that your script exits then.
the SIGINT will be sent to all processes in the process-group of the foreground process. when you source your script, then (in your interactive shell) invoke bar, then bash will run sleep (likely from /bin/sleep) in a separate process-group. you can see this by invoking bar, then running "pstree -g" in another terminal. when i do this, i get... |-gnome-terminal-(3429)-+-bash(6119)---sleep(6412) &amp;#x200B; this is different from what bash does when doing the same in a non-interactive shell, eg with the following... neil@tvpc$ cat neil-test #!/bin/bash trap "echo got sigint" SIGINT sleep 3600 then when executing "neil-test" and again using pstree -g, i get... |-gnome-terminal-(3429)-+-bash(6298)---neil-test(6474)---sleep(6474) so NOW both the sleep and my bash script are in the same process-group and... behold... when i control-c... neil@tvpc$ ./neil-test ^Cgot sigint neil@tvpc$ &amp;#x200B; it works as you expect when calling "foo" for the "read", as read is a shell-builtin ... your interactive bash shell remains the foreground process so will receive the SIGINT itself.
This works \#!/bin/bash for n in {0..300} do printf "%0\*d\\n" 3 $n sleep 0.2 echo $((n+3)) done
Thank you for showing the desired output. We need to understand the input data structure. Where do you get these variables, `$dnamelist` and `$diplist`? Do you always have the same number of names and IPs? Are they always in matching order?
Missing variable call `$` on `{OUTDIR}`, line 4. Do:While loops are not a thing in Bash. The While:Do syntax is `while TEST; do COMMAND; done`
Hmm, Ok, thanks for the feedback.
Do you create diplist and dnamelist in the script or is it from somewhere else? More precisely, are you able/willing to change how you initialize the lists? You can do a for loop through two lists like you describe in bash but you can play some tricks with bash arrays to accomplish what you want. I would do it one of two ways. Probably the easiest way is to use two parallel arrays. This is all assuming that the lists are coordinated have the same elements and ordered to match. #!/bin/bash diplist=("1.1.1.1" "1.1.1.2" "1.1.1.3") dnamelist=("one" "two" "three") i=0 while [ $i -lt ${#diplist[@]} ]; do echo "ip=${diplist[$i]} name=${dnamelist[$i]}" ((i++)) done Second way is to combine the two into a 2d array effectively. #!/bin/bash mylist="one:1.1.1.1 two:2.2.2.2 three:3.3.3.3" for element in $mylist; do IFS=: read name ip &lt;&lt;&lt;"$element" echo "ip=$ip name=$name" done
This does not work for me, it ouputs no files, where it should. i called it with list "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/" which should be correct, right?
When I run the second script manually, it finds the files. But it outputs all files, also the ones, that don't match. How do I make it show only the matching files and also only show me the filename? No, it is correct, that I want to find the files with smartctl as the first word in the file, not as a filename!
Maybe you can do this with nullglob, but I would recommend you to look into `find` for better control over the files you match.
I second what /u/hielkew said: find "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result" -type f -regextype sed -regex '\(smart_.*.result\|sd*\|sas.*\|nv.*\)' | while read -r myfile; do echo "found [${myfile}]" done
I ran this as a one-liner, but it does not find any files, althoug hthe fiels are there via ll: find "/home/me/Downloads/neu/debug_105320/dsm/result/" -type f -regextype sed -regex '\(smart_*.result\|sd*\|sas*\|nv*\)' | while read -r myfile; do echo "found [${myfile}]";done;
Did you write: `-regex '\(smart_*.result\|sd*\|sas*\|nv*\)'` instead of: `-regex '\(smart_.*.result\|sd.*\|sas.*\|nv.*\)'` on purpose?
yes, the files are named smart_something.result, sd*something etc. not smart_.*something*.result or sd.*something.
Not to hijack the thread, but in trying to test some of these answers to help out, I came across a new behavior. If I'm in a folder with files named failed-XXX and I do: find / -iname failed-* I get &gt;find: paths must precede expression: failed-aerial-messages-2016.tar.gz (which is one of the file names) but if I change to a completely different folder, the command runs fine and gives me many results. Why would that be?
The 'find' command uses the same wildcards as the shell if you use the `-name ...` parameter, so if you know normal shell wildcards best then use `-name` instead of `-regex`. In regex patterns, a `.` means "any character", and `*` means "repeat previous", so writing `.*` means "a sequence of any characters". About the problem in your first post, you should experiment with all of this directly at the command line, not in a script. Try to make bash print a list of your file names with `echo` or with `printf "%s\n"`. First set your variables at the prompt and then run the echo and see what bash prints: $ DOWNLOAD_DIR=... DATE=... DSM=... $ echo "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/"{smart_*.result,sd*,sas*,nv*} Also, perhaps run `shopt -s nullglob` to disable bash printing `*` characters if there are no files matching the pattern.
yeah, as written just before (`-regex`), it's a regex
I already did that. The problem with that line is, if there is no file named smart_*.result, it will stop searching and not look for the other four possible names.
It also does not find any fiels with the dots: find "/home/thomas/Downloads/neu/debug_105320/dsm/result/" -type f -regextype sed -regex '\(smart_.*.result\|sd.*\|sas.*\|nv.*\)' | while read -r myfile; do echo "found [${myfile}]";done;
&gt;But when I do it like this, it ouputs no files, if there are no files namend "smart_*.result" and it does not find the other files. Looks to me like it should work just fine. Are you testing it as part of your script or in a regular terminal? Do you get any results if you try it with all the variables substituted? However if any of the globs don't match then they will end up in the array as is unless you do `shopt -s nullglob`. Remember to also do `shopt -u nullglob` afterwards. I obviously don't have the same files you do but when I tested the following on my machine it worked just as expected. declare -a f shopt -s nullglob f=( "$HOME/bin/"{nonexistent*.sh,a*,c*.sh} ) printf '%q\n' "${f[@]}" shopt -u nullglob Can you run... `$SHELL --version`
How to pose support questions, post: 1. Your code, in runnable state 2. Any input 3. Expected results 4. Actual results.
Oh, sorry ! will edit my post ;)
$arg1 should be just $1, and the same with $arg2.
&gt;[pgcd.sh](https://pgcd.sh): line 7: \[27 : command not found &gt; &gt;42
There is probably something wrong the regex. I generally think the regex solution will get complicated really quick and it is easier to just use the built-in globbing of find if you only need to globbing anyway. find is one of the only programs to do globbing by itself and next to that find provides full query power in itself with the -o (or) option. find "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result" -type f \( -name 'smart_*.result' -o -name 'sd*' -o -name 'sas*' -o -name 'nv*' \) (The escaped parenthesis are not required, but are, if you for example want to end the find command with a -printf statement (take a look at the man page for the power of -printf, it is again very powerful.) Making an array from this can than be done with: FILES=($(find ...))
That's not how bash behaves here for me. If I for example do this here, I still get a list of .txt files: echo {*.skajghkjgh,*.txt}
I think the &gt; 0 should not be in the $(()): [ $(( $A % $B )) &gt; 0 ]
Niet :(
Did you change the &gt; to -gt too and got the same error?
Seems like the mod is the problem :/
Don't let shell see and expand that \*. Put the argument in quotes.
Thank you!
Can you post what that line looks like again? The mod should work, so Im a little lost on what the error is now
#!/bin/bash #pgcd.sh script A=$1 B=$2 C=0 echo $A #dont mind this echo $B #dont mind this while [$(($A % $B)) &gt; 0]; do let C = $(($A%$B)) let A = $B let B = $C done echo $B hahah thanks for ur help fam, we are lost here too
[change &gt; to -gt](https://www.tldp.org/LDP/abs/html/comparison-ops.html#ICOMPARISON1)
Same result even with gt :/
for god sake finally it was cancer omg \#!/bin/bash \#[pgcd.sh](https://pgcd.sh) script A=$1 B=$2 C=0 let "C = $A % $B" &amp;#x200B; until \[ "$C" -eq 0 \]; do let "C = $A % $B" let "A = $B" let "B = $C" done echo $A &amp;#x200B; exit 0
The problem was that you need a space after the `[`. When you rewrote it to break out the C variable, you added the space. If you go back to your previous version and add a space there, it will work too.
I'd use find this way in declaration of the variable: find ${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/ -type f \( -iname \*.xlsx -o -iname smart_\*.result -o -iname sd\* -o -iname sas\* -o -iname nv\* \)
I can't replicate this issue. Is it possible one of your variables (`DOWNLOAD_DIR`, `DATE`, `DSM`) are unset or are not matching the file tree? henry@desk:~$ touch smart_{hands,feet}.result sdwan sdlan sassy sassless nvone nvtwo henry@desk:~$ Files=( "./"{smart_*.result,sd*,sas*,nv*} ) henry@desk:~$ echo ${#Files[@]} 8
Some people like to sound superior by spouting what they read somewhere. I doubt that all those people that say printf is slower have done actual experiments themselves. Ultimately, in typical usage, it doesn't really matter what you use. I personally use echo 99% of the time and only resort to printf if I need formatted numbers. That said, if I were teaching someone bash right now, I might just start with printf because it is more versatile.
Often you see people doing `echo $var`. This can sometimes cause problems.
The wrong way is using printf when theres no use of the formatting capability. Thats confusing to read. Use echo and echo -n over printf.
[ ] is not part of the while or if syntax. Just do while (( A % B )); do echo foo done
I couldn't replicate it either. Your guess about the vars is probably correct.
Another option would be to just.... while :; do lsof -n "$INPUT_FILE" &amp;&gt;/dev/null FILE_STATUS=$? (( FILE_STATUS )) &amp;&amp; break done
Or just... while read -r dname dip _; do cat &lt;&lt; EOF ... EOF done
Can you give an example?
Echo accepts arguments. So variable starting with -e or -n might cause priblems.
[This](https://www.in-ulm.de/~mascheck/various/echo+printf/) is quite a nice read (the rest of the site also btw :)
Could even add it to the shebang
If people are complaining if you do `echo "Welcome"` then they are just being jerks. Just be aware of the cases when `echo` may behave weirdly: leading dashes and backslashes. Consider this simple `cat` implementation: #!/bin/sh while IFS="" read -r line do echo "$line" done &lt; "$1" With this input.txt: You have guessed the letters "etnr" and your clue is now: -nee For more information, see \\mywindowsshare\hangman\readme.txt On CentOS (where `sh` is bash) you get this: You have guessed the letters "etnr" and your clue is now: For more information, see \\mywindowsshare\hangman\readme.txt On Debian (where `sh` is dash) you get this: You have guessed the letters "etnr" and your clue is now: -nee eadme.txtinformation, see \mywindowsshare\hangman
That is definitely cleaner, forgot you could do that with read. If this wasn't r/bash I would have suggested doing it in python.
Oh great. So even using "" echo would read -n or R as an argument. That's a nice example! Thank you very much!
I've never seen any credible reports of `echo` going faster than built-in `printf`, but that might just be blissful ignorance of recent shell drama. Both are very fast, but `printf` is better in many objective ways. The most prominent being better compatibility across the Unix ecosystems. Or put another way, in Unix one could observe non-deterministic results using `echo` on different flavors of Unix. Solaris would do different from HP-UX, etc.. but printf was always the same result. But this is Bash we are talking about, so we can ignore the technical debt of other shells and other platforms. In Bash the built-in `printf` has to process format strings, which involves bit of work, not much.... but probably measurable over incredibly long loops.
Right, quoting doesn't and can't stop something from being considered a flag. This is also why you can't delete a file starting with a dash using \`rm "-f"\`
You forgot to declare IFS="" right before read so it does not break on files containing consecutive spaces
Don't use capitalized variables as they are reserved to the shell (like COLUMNS) or exported config variables for programs (like LESS). You WILL run into hard to debug collisions. A script of mine failed on a partner's computer because JOB was used on his distro but not in mine. Use camel case or lower case with underscores.
&amp;#x200B; ...
If you have been programming for a while you'll be aware that there are a lot of bias among programmers which often have no sound logical basis. This is almost like one of those cases except here you have some justifiable reasons for preferring one over the other. If you're just doing routine tasks on terminal typing one off commands or scripts that do not deal with external input, special characters etc and *you have complete control over what you're printing*, the argument is so pointless it is not even worth thinking about. Your examples are like that. It is only when you're writing a script that will use input you do not have complete control over and can be used in different \*nix implementations, you should be using `printf` (to be really portable you should only use core POSIX features of `printf`).
The only argument I could hear about `echo` being faster is that it's typically a built-in in bash, so there's no need for it to go hunt for it in the `PATH` and launch another process. So there's some performance benefit there, although I don't think enough to matter.
&gt; if I were teaching someone bash right now, I might just start with printf because it is more versatile. I agree on that. I mostly use `printf` in a lot of cases where I need to format data or construct some sort of string from existing data since it can be a little easier to read. But day-to-day, I use `echo` since it's less characters to type. heh.
Printf is a bash built-in also, and also exists as a separate command.
Oh! I didn‚Äôt realize that! Thanks for pointing it out.
Wait, you retards don't use the sys\_write syscall?
so your script had mainly syntax errors. such as spaces where their shouldn't be any. first, as others have stated, $arg1 and $arg2 should be just $1 and $2 second, your while loop had the right logic just the syntax was wrong, Importent thing to remember is that \[ is its only command so it needs a space on both sides of it, same goes for \] &amp;#x200B; finally the let commands can't have spaces on either side of the = &amp;#x200B; here is the fixed script, #!/bin/bash #pgcd.sh script A=$2 B=$1 C=0 while [ $(( $A % $B )) -gt 0 ] do let C=$(( $A % $B )) let A=$B let B=$C done echo $B &amp;#x200B; and here is the output I got from it $ ./pgcd.sh 69 42 3 let me know if you have any questions
It is important to take the habit of using echo "$var" instead of echo "$var"
This can be prevented by quoting the variable `echo "$var"`
Still causes problem if $var has just -e or -n or -E. Sure it's a corner case, but if you're echoing strings coming from a third party you should still keep it in mind.
It can still bite you in the ass even if you don't quote it.
I quite often use an associative array for this. instead of having to match up keys in two arrays I just do something like this: declare -A array array[foo]=bar for key in "${!array[@]}"; do printf '%s: %s\n' "$key" "${array[$key]}" done
What do you mean? I tried the following and it worked: $ touch "a b" $ touch "c d" $ find | while read line; do echo "found $line"; done found . found ./a b found ./c d $
good Catch, so I guess it would best if you don't want echo to interpret any more options you should do `echo -- "$var"` so even if its just a -e or -n it just seen as text.
Sadly, `echo` doesn't support '--' as you would like (eg in bash-4): echo -- foo -- foo
echo doesn't have --
Just tweak your benchmark until you get the result you want. If you prefer `echo`, make up a scenario in which it's faster: var="hello"; time for ((i=1;i&lt;=1000000;i++)); do echo "$var"; done &gt; /dev/null real 0m10.732s var="hello"; time for ((i=1;i&lt;=1000000;i++)); do printf '%s\n' "$var"; done &gt; /dev/null real 0m12.037s If you prefer `printf`, you can do the same: hex='\xF0\x9F\x92\xA9'; time for ((i=1;i&lt;=10000;i++)); do printf -v text "$hex"; done real 0m0.046s hex='\xF0\x9F\x92\xA9'; time for ((i=1;i&lt;=10000;i++)); do text=$(echo -e "$hex"); done real 0m7.928s
Format your code as code for fucks sakes.
This is probably why none of the suggested stuff his previous thread worked. https://old.reddit.com/r/bash/comments/bge1uf/looping_through_files_and_adding_special_files_to/
Yeah, I see what I missed from my test $ echo -- $TEST -- -n I ran this and didn't really look at the output, just say that it didn't interpret the `-n` so I just assumed it worked. The documentation stats that the options need to come first so a preceding space, or any other character would prevent this issue one case $ echo "" $TEST -n Of course then you might want to remove that space so, still not perfect.
Trying to add a bit of what others have already contributed. &amp;#x200B; This is part of what is called "Regex", short for Regular Expressions. If you search for this keyword, you'll find a lot in videos, articles and even dedicated books! &amp;#x200B; Good luck!
Note that this tests for a *non-zero* value, though, so the loop will run on negative values. If this is undesirable or you want better clarity, use `(( A%B &gt; 0 ))` instead. Otherwise, this is the best solution.
`--` is not supported by `echo`; the Bash maintainer refused to add it as they said that new code should use `printf`, which does support it. POSIX also recommends `printf` and has changed code examples to use it; this ends the debate IMO.
I can type "echo" in 400 ms. "Printf" takes me 700 ms. So echo is actually 75% more efficient.
I wasn't arguing which is better, I was more responding to this comment as a thought experiment, and simple seeing if this was a possibility. I agree that printf is what will take over In the future and echo is simply still use for legacy purposes and just out of habit for many.
`echo` is not portable when printing variable values, as it accepts options in them. Users have asked the Bash maintainer to add support for `--`, and he [refused to](http://lists.gnu.org/archive/html/bug-bash/2013-04/msg00008.html): &gt; It's not; echo has never behaved this way. Any code that would use `echo --' is, by definition, new code and new code should use printf. The POSIX standard expresses a [similar sentiment](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/echo.html#tag_20_37_16): &gt; It is not possible to use echo portably across all POSIX systems unless both -n (as the first argument) and escape sequences are omitted. &gt; The printf utility can be used portably to emulate any of the traditional behaviors of the echo utility &gt; New applications are encouraged to use printf instead of echo. While a simple `echo "Welcome"` does not have problems, and I'm actually not aware of any speed differences (yes, those people are jerks), I would encourage you to get used to using `printf` in scripts; the equivalent of your command is actually `printf '%s\n' "Welcome!"`, which is a bit of a hassle to type interactively. I also discourage using `echo`'s options.
&gt;How can I create the same results using Solaris sed Does the command you provided not work in Solaris? &gt;or another utility As an awk man I'd... awk '{print} /The Fellowship/ {print "\n# Foo\nBar\n"}' books.txt
You sir, are God! Thank you RMS!!! Never again will I attempt the Solaris implementation of sed...
Not really a one-liner anymore, but the following is how things are really supposed to work I guess: sed -e '/The Fellowship/a\ \ # Add comment here\ This is my comment\ ' books.txt
&gt;find | while read line; do echo "found $line"; done I got it slightly wrong. It's so it does not break on files containing \*leading and trailing whitespaces\*. Thanks for noticing.
That's as far as I got, and the Solaris implementation doesn't have gsed (which works the way I want - it's a hardened system), but it has to be a one-liner. \u\StallmanTheLeft nailed it. Thx
I didn't know that! TIL!
Just out of curiosity, could you try this, please? sed -e '/The Fellowship/a\' -e '\' -e '# Add comment here\' -e 'This is my comment\' -e '\' books.txt
I think I found the bug. When there are not fiels named smart_*.result, the Array is set to lieteral PATH/smart_*.result. But I need the Array do be empty, if it finds no files. Is there a way to make the array empty, if there are no files named like that?
The Vars are set correct. I think the problem is, for example for this: SMART_FILES=( "${DOWNLOAD_DIR}/debug_${DATE}/${DSM}/result/smart"*.result ) The Array is set to literal $paths/smart*.result I need the Array to be empty, if no files are found, how do I do this?
The nullglob solves that. Have you tried maybe running the command from the OP with all of the variables expanded?
Nullglob has been mentioned in this thread multiple times already.
 FILES=($(find ...)) This will cause problems with filenames that have spaces/tabs/newlines. I would much rather... mapfile -d $'\0' files &lt; &lt;(find ... -print0)
About that "nullglob" thing, what you need to do exactly is, you need to put the following line somewhere at the start of your script: shopt -s nullglob
Sounds like fun. I decided to write a simple chat server as well. Wasn't too difficult to be honest, I might just try to write a basic IRCd with bash...
Excellent! Thank you! $ sed -e '/The Fellowship/a\' -e '\' -e '# Add comment here\' -e 'This is my comment\' -e '\' books.txtA Storm of Swords, George R. R. Martin The Two Towers, J. R. R. Tolkien The Alchemist, Paulo Coelho The Fellowship of the Ring, J. R. R. Tolkien # Add comment here This is my comment The Pilgrimage, Paulo Coelho A Game of Thrones, George R. R. Martin
It's a matter of portability. \`printf\` is portable, \`echo\` is not. &amp;#x200B; Use printf. Always.
Also, quoteless does not preserve whitespace, but turns each to one space.
Bummer. I started reading "OReilly Sed &amp; Awk 2nd Edition" a while back and I remember it specifically mentioning this being the most portable way. I guess my memory failed me :)
__This person is a pedophile. Judging by his post history his mental illness is untreated and he has no place here__
__This person is a pedophile. Judging by his post history his mental illness is untreated and he has no place here__
__This person is a pedophile. Judging by his post history his mental illness is untreated and he has no place here__
__This person is a pedophile. Judging by his post history his mental illness is untreated and he has no place here__
__This person is a pedophile. Judging by his post history his mental illness is untreated and he has no place here__
What do you mean by portability? Between sh, bash, tcsh, zsh?
echo doesn't always work the same. Printf is supposed to be the same everywhere, but isn't, but not as badly as echo.
Some Nixes will include the quotes in the output. It is infuriating when it happens.
This kills the Solaris.
If you want to use a variable from bash as the regex then do awk -v r="$var" '{print} r {print "\nfoo\nbar\n"}'
^((If I see any other useful changes, I'll let you know)^) ---- Your `$type` variable is restricted to the one pipeline, you won't be able to access it outside of the function because of the pipeline. `read` only reads a single line, you need a `while read foo` loop to parse every line of input. Also, a `case` construct may be useful here to save you some pipelines and grep calls: while read line; do # for each line in stdin case $line in *'AAAA'*) type='(IPv6)' ;; *'A'*) type='(IPv6)' ;; # match other information? esac # other parsing of $line, extracting other information # ... echo "$type $foo" # print type and something else done I don't know what kind of output tcpdump is giving you, perhaps you could provide a sample?
You're having trouble with performance because every time you use `$()` or `|`, bash needs to open a whole new shell. It's going to be magnitudes faster to use a text processor like awk (my favorite) or perl. I really enjoy these kinds of problems. Could you please share a sample of your input data and the exact expected output?
I appreciate your help but I don't think you understood it well... but however: Input example: IP 192.168.1.23.40128 &gt; 192.168.1.35.53: 38426+ [1au] A? improving.duckduckgo.com. (53) IP 192.168.1.23.38318 &gt; 192.168.1.35.53: 18718+ [1au] AAAA? improving.duckduckgo.com. (53) IP 192.168.1.35.53 &gt; 192.168.1.23.40128: 38426* 1/0/1 A 0.0.0.0 (69) IP 192.168.1.35.53 &gt; 192.168.1.23.38318: 18718* 1/0/1 AAAA :: (81) IP 192.168.1.23.33379 &gt; 192.168.1.35.53: 65198+ [1au] AAAA? icons.duckduckgo.com. (49) IP 192.168.1.35.53 &gt; 192.168.1.23.33379: 65198 0/0/1 (49) IP 192.168.1.23.49859 &gt; 192.168.1.35.53: 16314+ [1au] AAAA? icons.duckduckgo.com. (49) IP 192.168.1.35.53 &gt; 192.168.1.23.49859: 16314 0/0/1 (49) Output: [improving.duckduckgo.com](https://improving.duckduckgo.com) (IPv4) [improving.duckduckgo.com](https://improving.duckduckgo.com) (IPv6) [icons.duckduckgo.com](https://icons.duckduckgo.com) (IPv4) [icons.duckduckgo.com](https://icons.duckduckgo.com) (IPv6) Explanation: The first part matches a domain given this expression: (\[a-z0-9\]\[\\.-\]?)+\\.\[a-z\]+ The second part is (IPv4) when there is an A or A? in the line, while it's (IPv6) when there is an AAAA or AAAA? in the line. (Sorry for my english and thx again)
I actually think I understood it pretty well. Try: tcpdump -t -n -N 'port 53' 2&gt;/dev/null |awk '$8~/\.[a-z]/{ print $8, "(IPv" ($7~/AA+/?6:4) ")" }' If you ask nicely I'll show you how to remove the trailing period from the domain.
Try this: tcpdump -l -t -n -N 'port 53' 2&gt;/dev/null | awk ' { sub(/\.$/, "", $8) } $7 ~ /AAAA/ { print $7, $8, "(IPv6)"; next } $7 ~ /A/ { print $7, $8, "(IPv4)"; next } '
This is just magic to me, thank you very much! These are some cases I just noticed: IP 192.168.1.35.53 &gt; 192.168.1.23.51669: 12406 1/1/1 CNAME quora.map.fastly.net. (141) IP 192.168.1.23.47691 &gt; 192.168.1.35.53: 36544+ [1au] AAAA? quora.map.fastly.net. (49) IP 192.168.1.35.53 &gt; 192.168.1.23.57353: 20243 2/4/1 CNAME quora.map.fastly.net., A 151.101.241.2 (167) IP 192.168.1.35.53 &gt; 192.168.1.23.58770: 29631 2/4/1 CNAME quora.map.fastly.net., A 151.101.241.2 (167) I'd like to manage these myself... just a thing: Could you briefly explain your cmd? If you can't do it briefly, these are some points: 1. Why $8 2. Which part is the regex? 3. Why { } Thanks a lot again.
It seems you have two problems here a) Get the output based on IPV6 or IPV4 b) Get it realtime. The following sed worked for me. You can use `-l` option to `tcpdump` to get the output line buffered. The `-u` option to sed or `unbuffer` command in linux also works. $ tcpdump -t -n -N 'port 53' 2&gt; /dev/null | sed -n 's/.* A?* \([.a-z]\+\)\..*/\1 (IPV4)/p ; &gt; s/.* AA\+?* \([.a-z]\+\)\..*/\1 (IPV6)/p' It is a very simplistic regex but it does the job based on the number of separate A it observes on a line (separated by spaces) and then replaces by appropriate string (IPV4 or IPV6).
Ty very much for the answer! &gt;You can use -l option to tcpdump to get the output line buffered Could you explain which advantages does it bring? I saw it works even without that flag. I'm not used to use sed at all, so forgive me for the question - could you briefly explain it?
Are you married to writing your own? If you search this subreddit or other Linux subreddits for ‚Äúdotfiles‚Äù you‚Äôll find hundreds of solutions for the very thing you want to do. Not saying it isn‚Äôt admirable or a great way to teach yourself, but another great learning experience is knowing when something is a solved problem and picking a pre-existing solution. Others will undoubtedly offer suggestions, so I‚Äôll just say I went with GNU Stow and have been very happy with its simplicity and feature set.
Hi, thanks for tipp. It's more a way of learning and teaching myself in the art of bash fu. I never heard about stow and I will take a look at it. &amp;#x200B; But suggestions to solve this problem are highly appreciated, because apparently I missed something obvious about the way symlinks .
&gt; Could you explain which advantages does it bring? Normally when a command sends its output to stdout it is line buffered meaning that the buffer is flushed at every newline. This allows you to view the output as and when a new line is received. But when a command writes to a pipe like the one here it flushes when it reaches the set buffer size (in the order of kB) so there is a lag. Using `-l` forces tcpdump to flush after every newline which means sed gets each line quicker and displays it after filtering. Note that the `tcpdump` manual says on Windows line-buffered means unbuffered so `-l` will dump each character individually. &gt; I'm not used to use sed at all, so forgive me for the question - could you briefly explain it? The sed command is very simple: - The `-n` flag suppresses all output (we have to print what we want explicitly) - Each expression with single quotes separated by semicolon is a `sed` command - The command `s/.* A?* \([.a-z]\+\)\..*/\1 (IPV4)/p` is a substitution command. It searches for a line that contains a single `A` followed by an optional `?` followed by the domain name followed by a period and then other characters. It captures the domain name using `\(\)` expression and replaces the whole line with the capture (indicated by `\1` followed by (IPV4). - Similar explanation for the second command which searches for at least two or more As followed by domain name. - The `p` after each substitution command prints the matched line.
In awk: `$COLUMN`, `condition{action}`, `/regex/`, `(if?then:else)` $ awk '$8~/\.[a-z]/{ print $8, "(IPv" ($7~/AA+/?6:4) ")" }' &lt;&lt;EOF &gt; IP 192.168.1.35.53 &gt; 192.168.1.23.51669: 12406 1/1/1 CNAME quora.map.fastly.net. (141) &gt; IP 192.168.1.23.47691 &gt; 192.168.1.35.53: 36544+ [1au] AAAA? quora.map.fastly.net. (49) &gt; IP 192.168.1.35.53 &gt; 192.168.1.23.57353: 20243 2/4/1 CNAME quora.map.fastly.net., A 151.101.241.2 (167) &gt; IP 192.168.1.35.53 &gt; 192.168.1.23.58770: 29631 2/4/1 CNAME quora.map.fastly.net., A 151.101.241.2 (167) &gt; EOF quora.map.fastly.net. (IPv4) quora.map.fastly.net. (IPv6) quora.map.fastly.net., (IPv4) quora.map.fastly.net., (IPv4) What's the problem with this output? What protocol should be listed if column 7 is "CNAME"?
I‚Äôm brain farting on exactly what happened but I used to run into a similar problem doing very large ‚Äúfind $variables -delete‚Äù and getting a similar ‚Äútoo many objects‚Äù or ‚Äútoo many arguments‚Äù . Was half a decade ago so I apologize for vagueness and I wish I knew exactly what‚Äôs limiting it... maybe sysctl open file limit in your case or something Anywho... I always found a quick and easy solution was to wrap it in a simple for loop or pipe | everything to xargs both take a little longer but much more painless
Your description is vague so I'm not certain but I think you may have it backwards. Often times you might do something like &lt;cmd&gt; * and get a too many arguments error. This is due to the * shell wild card expansion trying to resolve them all, THEN invoke &lt;cmd&gt; with that whole list passed in. using find in these cases can solve it, either with a specific use case built in flag like delete or a more generic --exec &lt;cmd&gt; {} argument to find. When doing this you'd need to quote the wildcarded pattern you were trying to use so it gets passed in as a literal single argument itself, else you're back where you started with shell expansion blowing up trying to build the list to pass into find itself. Some people use the pattern of combining find with a pipe into xargs versus using finds direct --exec argument, and there are plenty that will go off on why one is superior to the other, but I'm not getting into all that.
 ~/ echo 'This file has' "$( sed 's/[[:alpha:]]//g' temp | tr -d '\n' | wc -c )" 'alphanumeric characters in it.'
 echo 'This file has' "$( sed 's/\W//g' temp | tr -d '\n' | wc -m )" 'alphanumeric characters in it.'
I can't have three pipes. But my hacked solution was to just subtract one. Because it's only counting one newline. Thanks!
Awk solution! Because Awk! awk '{foo+=length} END{print foo}' file_one file_two ...
`*` is a [Glob](https://mywiki.wooledge.org/glob). Try quoting the name of the directory when you try to access it. ls "directory*with*stars"
This week I wrote up a script to create symlinks from `~/.dotfiles`/ to `~/.` . In my case, I overwrite the existing symlinks using `--remove-destination` before copying the new symlinks over to `~/.` I'm pretty sure this is the flag you're looking for? &amp;#x200B; My script: cp -v -r --remove-destination -s ~/.dotfiles/.{backup,bash,brew,duti,gitconfig,macos,mysql,plistbuddy,python,rEFInd,setup,zsh}/. ~/
In other words `cd "&lt;directory&gt;" if so that doesn't work.
Is it just an executable file being marked by `ls`? This is ridiculous. Please post the commands you entered, the information you currently see, and the expected result. Thank you.
 count=$(tr -d [[:allnun:]] -- "$1" | wc -c)
This is about word for word what I ended up doing.
Sry I forgot the -c flag from the tr.
^W ^H ^A ^T How do you expect to use a prompt and a pager at the same time? Maybe you should look into GNU Screen or Tmux (my preference).
Yes, this is *technically* possible with something like: exec &gt; &gt;(more) 2&gt;&amp;1 However, it's not going to work correctly. `more` wants to read from the terminal itself (directly, not via standard input) in order to provide it's paging functionality. This is going to conflict with Bash's own use of the terminal for line editing (i.e. readline). Essentially what you want us for each command *independently* to be automatically sent to a new pager instance. There's no simple way to do this.
I use rcm for managing my dotfiles. Really handy and you get some helpers for when you add new ones (mkrc) as well as tags if you don't want all configuration on all machines. https://github.com/thoughtbot/rcm
It really just a comfort thing for reading. I have a script i also to delay the character stream sometimes [https://pastebin.com/bkMsH6xx](https://pastebin.com/bkMsH6xx) I feel it easier to digest information... like a dict.org query or investigating curl requests. &amp;#x200B; I think i might be better off trying to hack the virtual terminal some how and leaving bash be. I'll have a look in the lilyterm source :)
Have a peek at the manpage of your terminal emulator. Maybe you could make custom key-bindings that mimic more with the scroll-back buffer.
Maybe not the most efficient way but I would just use git to keep track of the changes. And for listing the files if you are ok with `tree` is alright, but maybe `find` could be a bit faster.
So do you want to execute a command, or use a text file as arguments for a command? Looking at dmenu, I see you get a result after using dmenu: `list=("1\n2\n3\n4\n5")` `choice=$(echo -e "$list" | dmenu)` Now I think you should be able to run the command in your script: `openvpn --config $choice`
I'd use rsync.
 $8~/\.[a-z]/{action} What does this condition mean? &gt;What's the problem with this output? What protocol should be listed if column 7 is "CNAME"? It should display nothing in all the other cases, so just (IPv4) when A or A? and (IPv6) when AAAA or AAAA?
Isn't there a way to just use events? Not really familiar with this, but might be interesting: [https://github.com/rvoicilas/inotify-tools/wiki](https://github.com/rvoicilas/inotify-tools/wiki)
Oh man. I looked through your script and I think it‚Äôs pretty neat... ... and I hope rsync doesn‚Äôt break your heart. It supports file patterns, resume, intelligent diffing etc and it works really well.
Never worked with rsync before, will it improve the speed that much in this scenario?
You will save in dev speed and you will save in interrupted transfers, esp with your workload. But take an hour to read an internet tutorial and scan the manpage at least once. It‚Äôs a powerful, focused tool. 11/10
Even if all files I have to copy, don't exist yet on the other file? It is a pure copy, not a backup or anything. And it's not planned to be an backup/sync script? I am looking it up atm.
I‚Äôm not claiming it will push bits and bytes around faster, that‚Äôs up to the wires, I‚Äôm claiming the total time it will take you to get your work done will improve. üôÇüëçüèº
Git would store a copy of every file committed. So you would never really delete any files. And for large media files that would quickly be problematic.
Why are you putting it into an array when you're going to use it as a variable? It would be better to use `list=(1 2 3 4 5` and `choice=$(printf '%s\n' "${list[@]}" | dmenu)`.
I just copied it from dmenu's page without really thinking about it.
Alright thanks, I'll give it a try
GNU screen has a log to file mode, you interact with your session in the normal way, and Screen logs it for you. From the manual page: ‚Äò-L‚Äô Tell screen to turn on automatic output logging for the windows. ‚Äò-Logfile "file"‚Äô By default logfile name is "screenlog.0". You can set new logfile name with the -Logfile option.
**Nowadays,** business companies are using IP geolocation API services to customize their advertisements and messages. They also change the language of the websites according to the visitors. It has become an important tool to increase sales by locating visitors. Media companies use IP geolocation to block specific media in specific regions. So due to its large demand, there is a great competition among the IP geolocation API service providers. There worth depends upon a number of features. Some prominent IP geolocation API service providers are listed below: [**IP Geolocation**](https://ipgeolocation.io/) [**GeoIP Nekudo**](http://geoip.nekudo.com/) [**IP**](https://db-ip.com/api/)[**stack**](https://ipstack.com/) [**IP2Location**](https://www.ip2location.com/) [**MaxMind**](https://dev.maxmind.com/) Among these IP geolocation API providers, I would recommend [**ipgeolocaion.io**](https://ipgeolocaion.io) due to some prominent features which it has. [ipgeolocation.io](https://ipgeolocation.io) has the following features: * A vast and weekly updated database * Reasonably high accuracy * Minimum latency * Price economical * Easy to use **How to find the location of the IP Address by using** [Free IP Geolocation API](http://ipgeolocation.io/) **:** * Simply sign up to [Free IP Geolocation API](http://ipgeolocation.io/) and select a plan. * After signing in, copy the API key and use it to locate the website user in a quite simple way. 1. \# Get geolocation for an IPv4 IP Address = 8.8.8.8' 2. $ curl 'https://api.ipgeolocation.io/ipgeo?apiKey=API\_KEY&amp;ip=8.8.8.8' **Its JSON response will be:** 1. { 2. "ip": "8.8.8.8", 3. "hostname": "google-public-dns-a.google.com", 4. "continent\_code": "NA", 5. "continent\_name": "North America", 6. "country\_code2": "US", 7. "country\_code3": "USA", 8. "country\_name": "United States", 9. "country\_capital": "Washington", 10. "state\_prov": "California", 11. "district": "", 12. "city": "Mountain View", 13. "zipcode": "94043", 14. "latitude": "37.4229", 15. "longitude": "-122.085", 16. "is\_eu": **false**, 17. "calling\_code": "+1", 18. "country\_tld": ".us", 19. "languages": "en-US,es-US,haw,fr", 20. "country\_flag": "https://ipgeolocation.io/static/flags/us\_64.png", 21. "isp": "Level 3 Communications", 22. "connection\_type": "", 23. "organization": "Google Inc.", 24. "geoname\_id": "5375480", 25. "currency": { 26. "code": "USD", 27. "name": "US Dollar", 28. "symbol": "$" 29. }, 30. "time\_zone": { 31. "name": "America/Los\_Angeles", 32. "offset": -8, 33. "current\_time": "2019-01-14 03:30:00.135-0800", 34. "current\_time\_unix": 1547465400.135, 35. "is\_dst": **false**, 36. "dst\_savings": 1 37. } 38. }
Does it have to be oneliner, and using find ? If not, shopt -s globstar for i in **/*.HEIC do tifig -p "$i" "${i%.HEIC}.jpg" done
[Here you go](https://github.com/elken/bin/blob/master/vpn.sh)
&gt; I would like command2 not to start until command1 is finished. You don't have to do anything special to do that.
thank you kindly
Other than what aioeu said, &amp;&amp; executes the next command if the previous one exits successfully (0).
Don't use the `p` flag on the substitution. That tells `sed` to print the line if the substitution succeeded, but because you didn't use `sed -n`, it's already going to print the line anyway.
Agreed, need the I/O and then the expected output you are looking for. If you have trouble finding the command just run the 'history' command as long as your history is long enough and you haven't pushed out to many commands since you should be able to find it. Once you find it use a bang plus the entry number, For example: 89 tmux 90 exit 91 man wget 92 history user@pop-os:\~$ !89 tmux \[exited\] user@pop-os:\~$
Doesn't have to be a one-liner at all, thank you! I'd never heard of shopt, so much to learn. This [link](https://www.linuxjournal.com/content/globstar-new-bash-globbing-option) suggests I could use \*\* instead of \*\*/\*.HEIC. What's the difference here? &amp;#x200B; Thanks so much again!
&gt; This link suggests I could use ** instead of **/*.HEIC. What's the difference here? You can think of `**` like `find .` and `**/.HEIC` like `find . -name "*.HEIC"`. &gt; ... want the script to see if there is a jpg file in the dir that's associated w/ the HEIC file, and move to the next file. You mean like prefixing the command with `[[ -e "${i/%HEIC/jpg}" ]] ||`?
I presume you know about shift-pgup already?
Thx a lot for your answer! Could you analyze better this part? \([.a-z]\+\)\..*/\1 More specifically: 1. I don't understand the slashes / backslashes 2. Is '\\1' the matched domain?
Awesome! Can you explain how the `[[ -e "${i/%HEIC/jpg}" ]] ||` works? It's hard to google when I don't know what I'm googling.
As it happens, I also learned the globstar thing from here just the other day. Thanks, everyone!
There's a typo, either ${i/HEIC/jpg} (substitute) or ${i%.HEIC}.jpg (traditional, snip .HEIC from end)
 sed '7s/\(21-04-19:\)50/\134/'
Search for `PARAMETER EXPANSION` in `bash(1)` (bash man-page). Searching for `##` also works. Try to remember those two search terms and you will parameter expansion in no time.
It is not a typo. If the pattern begins with `%`, it matches it at the end of the parameter.
`[[ -e file ]]` tests if the file exists. `${i/%HEIC/jpg}` matches `HEIC` at the end of the parameter (due to the `%` being the first character of the pattern) and replaces it with `jpg`. `||` runs the command after it if the previous one fails, which is if the file does not exist in this case.
You are correct. I try to doublecheck the manpage next time :I
It is a directory; what I'm trying to do is `cd &lt;directory&gt;` although it says, `not a directory` since it has a asterisks on the directory ?
I went down this path and then decided to use `rcm` instead. It just wasn't worth the time spent.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
git-annex. You can force drop the file once done. All will be left there would be a broken shortcut.
I would use the `find` approach. Something like this, sheduled with cron: `find . 2&gt; /dev/null &gt; "files_log $(date).txt"` ...and then you can use `diff` to search for the differences
Basically GNU sed supports two regular expression syntax - Basic Regular Expression (BRE) and Extended Regular Expressions (ERE). This is described in detail in the [sed manual](https://www.gnu.org/software/sed/manual/html_node/sed-regular-expressions.html#sed-regular-expressions). The only difference is how the characters like `?`, `{}`, `+` and `|` are treated. In BRE (which is the syntax used above, ERE can be enabled by using `-E` flag) they are treated literally unless preceded by `\`. Now to your questions &gt;1. I don't understand where the regexp starts and where it ends As I said a sed substituion command has the following syntax `&lt;addr&gt;s/search_expr/replace_expr/&lt;optional command&gt;`. So the regexp is everything between `s/` and the next unescaped `/`. So here `/\1` is the replacement expression. The actual regexp is between `\(` and `\)` which is the capture group used to capture whatever was matched by the regexp. &gt;2. What does "\\+\\" do? With regexps it is an escaped +, so it corresponds exactly to '+', but this seems not to be the case. `\+` matches *one or more* of the preceding character or character class (here `[.a-z]`. Because we are using BRE so you have to escape `+` so that it is treated specially. In ERE you wouldn't need to do this (but you have to escape the `?` that appears before). The `\` following `\+` is part of the capture group `\(...\)`. &gt;3. Is '\\1' the matched domain? Yes, more specifically the part of the match between the capture groups `\(...\)`. In this case there is only one such capture group in the regexp. If there were more than one they would be numbered as \\1, \\2. ...etc based on their order of appearance.
What does this do that `rename` doesn't? rename tale tail * rename tale tail *.txt
I've managed it! Thank you again!! (Your one just missed some domains, was perfect for the rest) tcpdump -t -l -n -N 'port 53' 2&gt; /dev/null | sed -n 's/.* A?\? \(\([a-z0-9][\.-]\?\)\+\.[a-z]\+\)\..*/\1 (IPv4)/p ; s/.* AA\+?\? \(\([a-z0-9][\.-]\?\)\+\.[a-z]\+\)\..*/\1 (IPv6)/p'
Other than the confirmation check, logging, and user interface, they are functionally the same.
Other option than rename since it's not installed everywhere: *Example #1* find . -name '*tale*' -exec bash -c 'mv "$1" "${1/tale/tail}"' -- {} \; *Example #2* find . -name '*tale*' -name '*.txt' -exec bash -c 'mv "$1" "${1/tale/tail}"' -- {} \; I know that you have to get used to the syntax, but it is a lot more flexible with what it finds and what can be replaced.
Yeah, as I said the regex I used was fairly simplistic. You should use more robust ones that are designed for currently accepted domain names. See [this](https://stackoverflow.com/a/26850032/2116081) and [this](https://stackoverflow.com/a/26987741/2116081) for example. You may have to adapt those to conform to BRE/ERE syntax. Also, you can save a complicated regex in a shell variable (use single quotes). To reuse that you can use sed with double quotes (Regex='&lt;regexp&gt;') so that the shell variables are expanded sed -n "s/$Regex/Replacement/p".
Perhaps your bash version is too old? The &amp;&gt;&gt; syntax was introduced in 4.0 or so, but that should be obvious from the error message. Anyway, you can simply use the portable syntax like you did above it. #!/bin/bash exec &gt;&gt; script.log 2&gt;&amp;1
Good bot! At last bot....
So you have reimplemented `rename` as a shell script?
&gt;Other option than rename since it's not installed everywhere The `rename` utility is from coreutils, so this begs the question where is coreutils not installed? Maybe BusyBox or something?
I have Ubuntu on several systems, 19.04, 18.04 and a minimal 19.04 system. `rename` is installed on none of them. `apt show rename` shows it is in optional, universe/perl.
Thanks for reminding me why I never used Ubuntu. Wow! I'm not sure what to say, I guess Ubuntu team decided to exclude parts of util-linux and coreutils. That is a regrettable pity. ‚òπÔ∏è
Thank you for replying. I was looking into a solution using "case", however when I read the man page for it, it claims to be outdated being phased out. Is there a similar program that is uptodate?
thank you both! The last line is especially what I needed! I'm not sure of the differences between the way you each pipped info to dmenu, but I'll check it out.
[It's not Ubuntu's fault, but Debian's](https://unix.stackexchange.com/a/275278/140914).
Honestly, that is really sad, and pathetic. I'm actually adding this to a list I maintain of significant cross distro issues. Thanks I guess, as in thanks for the bad news. Facts are facts.
After a bit of research, I found out that on debian-like distros like Ubuntu, Debian and Mint, `rename` from util-linux is called `rename.ul`. So it is installed, but somewhat hidden.
You are right. This solved my problem entirely. Thank you very much!
I looked at it and rcm does everything I want and more. However I will debug, improve and extend my script as a learning exercise.
Share your dotfiles repo when you‚Äôve got it configured!
You are me a few months ago. I manage a few PBs of data spread out across many volumes and it was my goal to 1) gain eyes on what is online and 2) keep track of file lifespans. My solution is a multi-part and probably not the best nor straightforward. 1. the scan - my problem requires a VERY custom solution for identifying sets of files online. it's not as easy as doing a find -type f for me but it may be for you. i am also gathering file size data. these scans run nightly and run on various machine nodes for speed but you may not need/want that data. this scan is currently a perl script but if you don't need to worry about sets of files then you can most certainly do this in bash. once the data is gathered i use mysql to collect the data. this includes the date that the scan occurred, the file size, and full file path. 2. file lifespan - as i only care about files that are online, i run a query on files with today's date. if i want to calculate lifespan i run another query to find the min(date) and then date math. other calculations are possible but then you're getting into learning mysql queries (which why not!). things that didn't work for me - i tried doing a reverse search for files that were online the day before, checking to see if they were still alive and then updating the "last active" date in mysql. here's why that didn't work for me: i would still have to scan the active files for today AGAIN. i was doing double the work to merely find the few files that weren't online. you may be able to do this effectively but again you're still going to have to process that online files for the day. fyi, i used bash to send the mysql commands as that is what i am most comfortable with. let me know if you're interested in setting this up and i can walk you through both the scan and mysql part.
 I mean git of only a file, that is your list of files
This worked. Thanks!
You can also do a `set -e` at the beginning of the script and it'll do the same üòÅ. I mostly use `&amp;&amp;` when inlining in a shell and it's nice for that!
Case isn't a program, it's a bash statement. It'll be outdated when bash is outdated.
I was thinking along these same lines, but instead of creating a lot of files each day, just create one masterlist.txt file and then check it into a git repo. Then you can just schedule a commit/push and see the changes. At a minimum, do $(date +%Y%m%d) to only get the date 20090421 instead of "Mon 04/21/2019 12:45:00 PM PST.txt" or something.
You haven't had an answer from /u/oweiler... so I'll respond: the answer is "yes". Also across various *nix systems e.g. AIX, HP-UX, Solaris, Linux, FreeBSD (and, by extension, OSX) etc etc all use `echo` in different ways. `printf` is almost exactly the same across all of them. FWIW the Solaris `man` page for `echo` has [given encouragement](https://docs.oracle.com/cd/E19683-01/816-0210/6m6nb7m84/index.html) to not use `echo` for probably decades now. If you're writing for portability, `echo` is a major no-no. -Source: I, sadly, have years of experience at having to write for portability.
What? $ apt list rename Listing... Done rename/stable,now 0.20-4 all [installed,automatic]
Huh, I was going by what I read on "man case" &amp;#x200B; \&gt; Note: the **case** command is obsolete and is supported only for backward compatibility. At some point in the future it may be removed entirely. You should use the **switch** command instead.
I have another issue, maybe some of you anticipated it. I need to run "openvpn --config /blah/blah.ovpn" with sudo in order to connect. I have tried to look into configuring openvpn without the use of root but it seems like it is challenging because it needs root to function. &amp;#x200B; I'm trying to find a way around this, maybe with a prompt that lets me enter my password? The script runs in the shell and asks for my password but when I execute it with a global keybind, there is no shell, and thus nowhere to enter my pw.
Thanks for the waking nightmare. Please find whoever wrote that script and damage their fingers. #!/bin/bash unzip -P ${file%.*} $file
grep "\\--" is a way (not a very good way IMHO) of getting grep to output lines that contain two sequential hyphens -- remember, the usage of grep is basically... grep \[options...\] PATTERN \[FILE...\] and options always take the form of -a, -b... or --foo, --bar. basically grep traverses everything passed to it, thinking that anything starting with - or -- must be an option. as soon as it finds the first thing that doesn't begin with - or -- then that must be the PATTERN. So, how do you supply a PATTERN starting with a hyphen? &amp;#x200B; the author of the script has discovered that they can escape the first hyphen with a backslash (they have to put it in double quotes so that the shell doesn't interpret the backslash) and that grep doesn't treat this as an option. fine, but it's reliant on then grep understanding escape codes for the pattern and reducing \\-- to just -- &amp;#x200B; a much better way of doing this (and this is common for many gnu/linux commands) is to use a special option "--". most gnu/linux commands will interpret this as "this marks the end of the command line options". so, for the usage in your script, a better approach would be to do... grep -- -- (the first -- tells grep that there are no more options) (the second -- is then taken to be the search pattern)
Thanks for a very much shortened version, though this seems quite experienced? As I barley managed to understand the code above. ${1%.*} makes no sense to me right now but I‚Äôll have a look around and see what I can find. Thanks again!
This is brilliant! Explains everything as to what I needed to know I have been looking at MAN pages as that‚Äôs how I got my epiphany. To summarise what you wrote using ‚Äú\‚Äî‚Äú is a way to escape the interpretation from the shell but to also let grep know it‚Äôs not an option but a pattern. But a better approach is to write grep ‚Äî ‚Äî which tells grep there are no more options and then the actual search pattern. So if I wanted a different search pattern I‚Äôd use ‚Äògrep ‚Äî -‚Äò if I wanted to grep just ‚Äò-‚Äò?
i think your formatting is a little off, but if you asking how to grep for a single hyphen, then you'd do grep -- - (the -- would tell grep there are no more command line arguments) (the single hyphen on the end would be the search pattern) &amp;#x200B; the same applies to many command, eg. echo or rm to output text beginning with hyphens or removing files beginning with hyphens. it's good practice *always* to use -- to delimit where options end and non-options start.
Yeah that‚Äôs what I meant, mobile reddit didn‚Äôt help me here... that‚Äôs great info to know. I‚Äôll make a mental note to always use -- to delimit. Once again thanks for the help and the follow up!
https://en.wikipedia.org/wiki/Barley `${1%.*}` is [Parameter Expansion](https://wiki.bash-hackers.org/syntax/pe). It means "Remove everything after and including the last period", which is, in this case, the file extension.
&gt;I'm currently learning bash scripting and would like to ask for some help on a few parts of the code below; #!/bin/bash INPUT=$1 while true; do file=$(zipinfo $INPUT | grep "\--" | awk '{print $9}') echo "file :" $file pass=$(echo $file | awk -F"." '{print $1}') echo "pass: " $pass unzip -P $pass $INPUT INPUT=$file done The only thing you'll learn from that is how NOT to do things. For example: INPUT=$1 To start: your variables should never be UPPERCASE until you understand why you want them to be. UPPERCASE is for environment variables. Use alllowercase, TitleCase/PascalCase, camelCase or snake_case instead. Next: you should try to use meaningful variable names. Next: you should almost always quote your variables. That's the basic stuff out of the way. If you want to take this up a level: Next: When using positional parameters, you might like to consider using parameter substitution as a form of built-in error checking. So straight off the bat, the first line could be rewritten as something like: zipFile="${1:?No file defined}" So in this example, if `$1` isn't set, `bash` will fail with a message saying "No file defined". Moving on... while true; do This is a personal preference thing, but I prefer having my `then`'s and `do`'s on the same line. This is also recommended in the Google style guide which is linked in the sidebar. I started getting a sense of deja-vu, so I went and [dug up this comment](https://www.reddit.com/r/shell/comments/b7gxc8/hoping_to_get_constructive_criticisms_on_my/ejsgd3e/) where you can see that I'm on-message. The other comments in this thread more than cover the rest, so I'll add two more things: grep "\--" | awk '{print $9}' This is a Useless Use of Grep. Most instances of `grep | awk` can simply be done with one call to `awk`. In this case: awk '/--/{print $9}' Finally, pass=$(echo $file | awk -F"." '{print $1}') In `bash`, this is a Useless Use of Echo. `bash` provides here strings, so this can be written as: pass=$(awk -F"." '{print $1}' &lt;&lt;&lt; "${file}") The argument against this approach is that for newbies, the 'echo | awk' example is more readable as it's a left-to-right pipeline.
quotes around the directory should work, if not just escape the asterisk. $ mkdir "xy*z" $ ls 'xy*z' $ cd xy\*z/
I highly appreciate the detail and explanation of things in this such as the Uppercase variables etc. The error checking is something I haven‚Äôt really looked into yet but in your example I understand it. Especially if the script was wrong I wouldn‚Äôt know where to it went wrong. Not having to use grep and only use awk clears a lot of confusion I had before I managed to figure out when reading the MAN pages. As for the use of echo, I understand that too especially since I would consider myself as a newbie, but after reading the explanation I remember seeing &lt;&lt;&lt; around in other examples. This has helped a lot, thanks for your time!
Do you mean `cd "&lt;directoryName&gt;"` or `cd &lt;directoryName&gt;*` ?
So fyi awk does grep. awk '/--/ {print $9}' No need to `echo` into awk, just use shell stdio. awk -F. '{print $1}' &lt;&lt;&lt;$file That should help make it more succinct.
that's literal terminal output, not an thought experiment. try it yourself.
`awk -F"." '{print $1}'` is equivalent to `${1%%.*}`, not `${1%.*}`. The first removes the longest suffix, whereas the second removes the shortest prefix. This matters when there is more than one `.` in the filename: `ab.cd.zip`. With `${1%%.*}`, it becomes `ab`, and with `${1%.*}`, it becomes `ab.cd`.
There are multiple graphical programs for this, but since you're already using dmenu, you can just use it. Place `dmenu -p "$1" -nf '#222222' &lt;/dev/null` in a script, then use `SUDO_ASKPASS=&lt;the script&gt; sudo -A &lt;your command&gt;` in your script. Note that the `-nf #222222` sets the foreground to the default background to hide the password; if you want to see it you can omit that, and if your default background color is different, change it to that.
md5sum tends to be more reliable as in you can modify a file without adding anything to it. md5sum will catch it lots of other tools won't.
I'm actually looking to check if something was modified in the file. I'm not looking to check for file integrity, I'm interested in the contents of the file. Essentially it's a list which gets updated and I feed it onto a function for processing. I only want to feed it to the function when there's an actual change to the contents of the file.
Then IMO you can just grep the contents of whichever tool you prefer to verify the contents are correct. md5sum would be overkill.
Which is more common, a diff or equivalence? A byte-by-byte diff can stop checking when it finds the first delta. A md5sum must read the entire file before it knows if the files differ. &amp;#x200B; If you can cache the md5sum of the variable-populated data then you'll only have to read the remote file to determine difference. The remote file may also return a last-modified or etag header that you could use to determine if the files are different as well without having to process the full file.
MD5sum will only tell you \*if\* the file has changed -- it won't tell you what changed inside it. However, if you are running this every 5 seconds, doing a 'diff' on a 100M file might start to lag your process if the previous job didn't finish before you kicked off another one. &amp;#x200B; Something you might want to do is run an md5sum on the file. IF the file has a different thumbprint, THEN pull the new one aside and diff it against the previous one and report the results. If it hasn't changed, assume the pedigree is the same.
&gt; those errors What errors?