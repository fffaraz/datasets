&gt; Is the condition and print necessary? Yes. In this statement, awk sees 'time=75' as a single "word", so some parsing of the word is necessary before you can test for the condition. Then you need to tell awk what words to output unless you want awk to return the entire line as {print} or {print $0} is implied if nothing is specified.
Yeah if it's all on one line you'd need that semi colon but I was lazy and just did multiple lines, where it's still probably better practice to include it but apparently not required- I did test it on my machine first just in case. I keep telling myself that I'm going to just take a week and go all awk to relearn it but... well... life doesn't seem to want me to right now. I remember when I first started learning it I was sort of amazed that it's essentially a programming language within a scripting environment and was blown away at how much it can actually do. It can essentially replace grep, sed, tr, cut, etc...
&gt; In this statement, awk sees 'time=75' as a single "word" Actually, thanks to tr it's just "75" and awk can directly eval this integer. It's definitely not necessary on my system, and I was only asking in case awk on his system behaves differently. [root@box ~]# cat ping_results.txt PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 64 bytes from 8.8.8.8: icmp_seq=1 ttl=49 time=29.7 ms 64 bytes from 8.8.8.8: icmp_seq=2 ttl=49 time=30.7 ms 64 bytes from 8.8.8.8: icmp_seq=3 ttl=49 time=30.8 ms 64 bytes from 8.8.8.8: icmp_seq=4 ttl=49 time=31.4 ms 64 bytes from 8.8.8.8: icmp_seq=5 ttl=49 time=34.1 ms --- 8.8.8.8 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4033ms rtt min/avg/max/mdev = 29.759/31.386/34.182/1.495 ms [root@box ~]# cat ping_results.txt | sed -u -e 's/.*time=\([0-9.]\+\).*/\1/g' | awk '$1 &gt; 30' PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data. 30.7 30.8 31.4 34.1 rtt min/avg/max/mdev = 29.759/31.386/34.182/1.495 ms [root@box ~]# cat ping_results.txt | sed -u -e 's/.*time=\([0-9.]\+\).*/\1/g' | awk '{ if ($1 &gt; 30) print $1 }' PING 30.7 30.8 31.4 34.1 rtt [root@box ~]#
egrep -f &lt;file full of search patterns&gt; filename | sort -nu
Easy fix? Use single quotes. $ ARGS="--name='John Smith'" $ echo $ARGS --name='John Smith' However, don't seem to be able to replicate your results... $ ARGS="--name=\"John Smith\"" $ echo $ARGS --name="John Smith" $ echo ${ARGS} --name="John Smith" 
You want this: ARGS="--name=John Smith" cm "$ARGS" Variable expansion happens before tokenization in the shell. It's kind of ridiculous, but it is true. 
Don't try to mash multiple arguments into a single string. Use an array or a function, depending on the case. See [BashFAQ 50](http://mywiki.wooledge.org/BashFAQ/050)
This is kind of a trivial use of inotify. There's not much downside to a cron job that runs rsync 1x/hr and immediately exits because there are no changes. A better example would be using inotify to take a particular action when a file is uploaded to a location by ftp or by another app. For example, I use inotify to trigger some bash housekeeping scripts when Magento uploads user files to a particular location -- the bash scripts do things that Magento doesn't do. Far easier to use inotify to trigger the script than to write a Magento extension to do so.
This is when you want a server which can be used as a failover. 
Maybe put brackets around everything before the 2&gt;&amp;1? 
Try /usr/bin/time -v &lt;program&gt; 2&gt;details
I think this doesn't ensure file integrity. If after a file is modified you sync all the files, what happens with files being modified in that same time? I think that dropbox-like software checks each file on their own and sync them individually with a queue daemon. I'd do this: * queue daemon (manages a queue of recently modified files for syncing) * inotify daemon (send messages to queue daemon on "change" events for each file) What do you think? (sorry for my english)
A more formal example of inotify+rysnc is [lsyncd](https://code.google.com/p/lsyncd/).
the book is amazing, one of the books I'd recommend everyone to read.
Some comments on the script: 1. Avoid using uppercase variable names. You risk overriding special shell variables and environment variables. 1. Why not use a more human readable timestamp for the filename? like `%Y-%m-%d %H:%M:%S` 1. `stat` is not a standard command, so some systems doesn't have `stat`, and some systems have a `stat` command, but with a completely different syntax than GNU `stat` (which you are using). A portable way to get the size of a file is to use `wc -c`. 1. Be more consistent with syntax. On line 17 you use `let` command to do math, while on line 19 you use `((...))`. Use one and stick with it. In this case though, since you only need the calculated value once, you might as well skip storing it in a variable in the first place, e.g. `if (( pngsize &lt; jpgsize * 3 )); then` 1. `~/bin` is an odd place to put an audio file. I'd put it in `~/.local/share/sounds` or something like that. So here's my suggested rewrite: #!/usr/bin/env bash # No point in continuing if either of these commands don't exist type scrot convert play &gt; /dev/null || exit dir=~/Pictures/screenshots file=$dir/$(date "+%Y-%m-%d %H:%M:%S") scrot -s "$file.png" || exit convert "$file.png" "$file.jpg" || exit read -r pngsize _ &lt; &lt;(wc -c "$file.png") read -r jpgsize _ &lt; &lt;(wc -c "$file.jpg") if (( pngsize &lt; 3 * jpgsize )); then rm "$file.jpg" else rm "$file.png" fi play ~/.local/share/sounds/shutter.jpg
I have a `newpy` and a `newbash` command that I made. They create the files from templates , `chmod +x`, and then open them in my favorite editor (which is configurable for some reason). I had a bit of boilerplate to write every time I was making a new command-line tool. For Python it was importing `docopt`, writing a basic usage string, and then writing a basic `main()` and `if __main__`. Now I just do `newpy myscript` and start working on actual code. If I need to add command-line args I just edit the existing usage string. `docopt` is amazing, and uses the usage string to parse arguments automatically. The `newbash` command doesn't do very much, but it's still set up so I can just start writing code. I also made a command called `cedit` (short for cjwelborn-edit :P), that will open a file with my favorite editor, and use an elevation command on files that I don't have write permissions on. It also has configurable 'favorite directories', so typing `cedit myscript.py` will look in the current directory, but also my 'favorite directories' until it finds one to use. If it can't find it, it will ask you if you want to create it. I got tired of misspelling file names and then having my editor think I wanted to create a new one. I also got tired of forgetting to run `sudo myeditor`, making changes, and then having my editor complain about not having write permissions. I would have to re-open the file using `sudo`. So this `cedit` will handle all of that for me, and it never saves a password or anything stupid like that. It just runs `sudo`, `gksudo`, or `kdesudo` so you can enter it. I'm glad to see I'm not the only one.
&gt; # Makes an array of all the users in the home directoy. If I'm reading this correctly, what you have makes an array of all the users who have bash as their default shell. To get the users who have a sub-directory of /home/ then you'll need to change the file path you grep for. 
&gt; # This is where you want the backups to go, I would suggest putting them on a different &gt; # partition then the / and /boot s/then/than/
Now I did think of that. I mean "ls /home/" is easier than what I did, but I was concerned about what if there are other directories in /home, and instead of excluding all of the random ones (especially seeing that there are other users and can't exclude all) and I didn't want to manually put in the users, in case someone adds a new user. I thought that this would provide the information I need without giving me other directories that are not some user's root directory. 
&gt; user=$( cat /etc/passwd | grep "/bin/bash" | awk -F\: '{ print $1 }' | sed 's/root//g' | sed '/^$/d' ) So many unnecessary commands in this statement, it makes my head hurt. All you need is this: user=$(awk -F: '$7 ~ /bash/ &amp;&amp; $1 != /root/ {print $1}' /etc/passwd)
 $ echo lol &gt;&amp;2 | cat 2&gt;cat lol $ cat cat $ As you can see, redirecting only second command will not store error message from the first one. I'd recommend either $ mysqldump -u${USERNAME} -p${PASSWORD} ${DATABASE} 2&gt;database.err| gzip &gt; backup.sql.gz 2&gt;&gt;database.err Here &gt;&gt; is used to append errors, so gzip will not truncate this file on start and will append its errors to it or a little clearer $ ( mysqldump -u${USERNAME} -p${PASSWORD} ${DATABASE} | gzip &gt;backup.sql.gz ) 2&gt;&gt;database.err Here a subshell will be created and all its output will go to the log-file or even an unnamed function $ { mysqldump -u${USERNAME} -p${PASSWORD} ${DATABASE} | gzip &gt;backup.sql.gz; } 2&gt;&gt;database.err The same as previous but without a subshell (Note a semicolon before closing bracket). or named function, e.g. $ create_db_dump() { mysqldump -u${USERNAME} -p${PASSWORD} ${DATABASE} | gzip &gt;backup.sql.gz } $ create_db_dump 2&gt;&gt;database.err The last one is my personal favorite even in the case of little amount of commands. Helps readability.
Don't know, why someone gave you a downvote. Here is an upvote.
Right, I agree that you wouldn't want to just pull a list from /home. Instead of grepping for the shell, you could grep for '/home' in /etc/passwd, which will pull in any users who have a home directory in /home (or you could egrep for multiple matches, like `egrep '/home|/root' /etc/passwd` to return those users who have homes in /home or /root.) There are other ways to further simplify this with bash string manipulation, or by using just awk as /u/lalligood said, which are fine options. There are lots of ways to accomplish this, and even if you don't have the most clean / simple code, it sounds like this was a tool to help you learn scripting and it seems like it is doing that job. At my first linux job, my manager gave me a task and asked me to accomplish the same task using 4 different bash scripts, which used 4 fundamentally different ways to accomplish the task. If you're learning bash scripting, I would encourage you to try this sometime. I always found the first two (and sometimes 3) were not too difficult, but after for sure by the 4th I was pushed into areas outside of my immediate knowledge base, and really learned some new things. 
 user=$( cat /etc/passwd | grep "/bin/bash" | awk -F\: '{ print $1 }' | sed 's/root//g' | sed '/^$/d' ) for u in "${user[@]}" nope. user is not an array. Let's see what happens if you get more than 1 user $ user=$( cat /etc/passwd | grep "/bin/bash" | awk -F\: '{ print $1 }' ) $ for u in "${user[@]}"; do echo "[$u]"; done [root vaphell] If you want to make an array out of newline delimited output, you should use readarray $ readarray -t user &lt; &lt;( cat /etc/passwd | grep "/bin/bash" | awk -F\: '{ print $1 }' ) $ for u in "${user[@]}"; do echo "[$u]"; done [root] [vaphell] alternatively you could wrap $() in () user=( $(...) ) but it's IFS dependent and i don't like implicit splits. Imo they are a blight of shell scripting. readarray -t arrname &lt; ... IFS=... read -ra arrname &lt; ... while read -r ln; do arrname+=( "$ln" ); done &lt; .... are the way to go, depending on required delimiter and corner cases. also don't use allcaps for your var names like you did with DATE. You could cause a collision with special/env variables which are all caps by convention and shell won't complain, things just will go south. At least one lowercase would be nice. You should "" every $var/string created by gluing $vars together. sure you can make assumptions given your inputs here, but being lazy with 2 keypresses will bite you in the ass every time you have whitespace ridden values of paths or whatever. Allowing word splitting is a source of untold number of bugs. 
Thanks for the input. As soon as I have some free time I will test your suggestions out.
I'm not very good with awk. Still learning I'll have to test your suggestion thinks for the input. 
Thanks for the input. I'll have to do some research and see how to implement your suggestion. 
the problem is that the users are not guaranteed to live in /home. It's just a default that can be overriden. People can also change their shell so looking for /bin/bash is kinda weak too looking at my ubuntu cfg, ordinary users - have uid 1000+ (iirc in fedora its 500+) - have a shell that is not ( /usr/sbin/nologin, /bin/false, /bin/sync ) 
True, but OP did not say that all users were to be backed up: &gt; Makes an array of all the users in the home directoy. If the criteria were different (such as to backup all users who are allowed to log in with a valid shell), the recommendation would've been different. My point was just that if OP wants to back up all users who login to their user directory in the `/home` directory that it would make more sense to look at their login dir, not the default shell. As I mentioned before, there are loads of ways you can do this. Heck, you could even create a `backup` group, and add to that group all users who should be backed up, then you don't have to rely on any potentially automated / default setup aspect to dictate who gets backed up. Then you get your comma separated array of backup users by running `awk -F':' '/backup/{print $4}' /etc/group`. It all just depends on what OP is shooting for...
I'm not a programmer but I guess that it is because the contents of the environment variables themselves are meant to be processed by another application which does not have the intention to execute their values as arbitrary bash commands. For example, the content of environment variables could be information taken from an http-request directed to an apache webserver running on the box. Apache won't just process any functions you provide it with. It wasn't intended to do that. However, It is whatever comes **after** the function, as that part still **gets executed by bash**. It is outside the function and I'm guessing by that also outside the environment variable. Yet Bash does not stop executing the provided statement. In other words, By submitting an empty function with no data and then appending arbitrary code after the ";" the rest of the line still gets run by bash and not the application that the information was directed to.
Good point. It all depends on what OP wants, but I think you're on the right track. Surely OP wants to catch errors from both processes.
I was just shooting for a simple and easy backup script that backs up all user data, as well as / and /boot. In my environment / is on one partition /boot on another, and /home on another (and largest) partition. Because I have dd (I chose dd for ease of use and how easy it is to restore) making images of / and /boot, and then those are stored on /home, because I don't have another partition for backups I can't make a dd image of /home, also I wanted to be able to restore a single file from the users' directories without having to restore everything. But I agree there are other more simple and direct ways of finding all of the users. I will be making that change as soon as I have some free time.
That kind of expects you define functions with specific names. Generally speaking, when you get userdata you pass it to a fixed environment variable (something like UNSANITIZED_USER_DATA) before doing anything to it. you would never define keys based on userdata. That's the kind of thing that fucked up oldschool php guys who sucked in all their get/post data into their global variables.
That's what I was thinking, what if I want an env var that starts with '() {' I would find it surprising that bash would then try to make a function out of it. It's just my data, it should be opaque to bash.
If patching bash is the best solution then does that not make it a bug in bash? &gt; The implementation detail of using an environment variable whose value starts with "() {" and which may contain further commands after the function definition is not documented, but could still be considered a feature. Some might call that a bug too.
Filtering variables is never a good option... Already shows how much the blogger understand about security... If Apache, DHCP and whoever should do something is ESCAPING... But that is very very very hard for a turing complete language like BASH.... Definitely where this problem has to be solved is in BASH. Not in 100 other software... Either thru flags to pass insecure variables or maybe better, just get rid of function importing "feature"?
imo undocumented/forgotten feature that arbitrarily treats passive data as executable code smells really bad no matter how you slice it. Exporting functions, assuming that should be allowed, should be made separate from normal env vars syntactically to require conscious effort, no automagic. And even if we accept that's it's a honest to god feature and apache shouldn't send garbage, that doesn't mean that the failsafes against exploits should not be there on the other side too, especially when the happy combo is X+bash and it's the bash feature that is attacked. Lack of security can fly in case of internal communication between modules of the project you control, but standalone programs that are black boxes for all intents and purposes should be made secure to provide defense in depth. 
I love that FAQing site. Can spend hours just reading:)
I'm up voting, but only to make this more visible, not because I agree with it. I strongly disagree. There are two levels to this. First is assuming that passing functions via any env var is okay. Even if that's the case this was definitely still a bug. If you mentioned to a bash developer around the time this feature was implemented, "hey, in addition to setting the function, bash is also executing arbitrary code after the end of the function definition." they'd be surprised and would see it as a bug (even though they might not appreciate the security implications). Second level is to ask if it's a good idea to implement passing of functions in this manner. I'd argue that it's not and that the value of arbitrary env vars is my concern, not bash's. If bash wants to send functions via the environment, then reserve certain env var names. Maybe BASH_FUNC_foobar would set the foobar function. It just strikes me as weird that bash is looking at the beginning of every env var to see if it starts with some magic bytes and evaluating it if so.
 $ mkdir ~/shell_learn Then put in .bashrc: export SHELL_LEARN=${HOME}/shell_learn export PATH=$PATH:${SHELL_LEARN} And call your scripts by names (cool_script.sh, for example).
Create an account on github.com . Then on your local machine, create a projects directory: mkdir -p ~/projects/shellscripts Follow u/Spirit_of_Stallman's advice below and add the project path to .bashrc . When you add a script, or make modifications, 'git add' it, commit it, then push it up to github. git add newscript.sh git commit -m "made a new script" git push origin master This will act as a free backup for your scripts, and can help others help you out if you're having problems -- just point them to the script on github. It also helps you to start developing good coding practices, including experience with git, code commits, and remote repositories.
Thanks for the reply, that's a really good way to go, but my question is different. I'll try to explain myself better... My concern is about why I would prefer to put my code in functions inside .bashrc instead of scripts in my (already in $PATH) ~/bin dir (or bice-versa). I just dont see the difference, since calling them from the command line would be the same, right? Thanks!
The difference between `.bashrc` (or `.bash_profile` for that matter) is that `.bashrc` is loaded by default every time you log on, where as scripts you drop in to `/bin` are manually called only when you need them. (or better than putting them in `/bin`, add them to `/usr/local/bin` or something similar... or if you use git, you can save your personal scripts to a specific directory sync'd with git, and add that directory to `$PATH` in .bashrc so that bash will look for scripts in there when you run a command) So, if you want to have certain aliases and maybe a rsync script to update some directory in your home folder every time you log in, you'd want this to happen every time you log on -- this would go into `.bashrc`. By contrast, if you have something that you want to do at specific times, you'd drop it in to the `bin` directory. 
I would suggest only putting stuff into bashrc if it's really simple. For anything that's more than a couple of lines, you're better off making a separate script for maintainability and reading purposes. (Reading through one hugeee bashrc will quickly become annoying) You also mention that you are using these to learn. That sounds like it would be better suited to having them in their own files. As to where specifically to put them, I would follow /u/Spirit_of_Stallman's suggestion. Personally, I have a `~/Bin` folder where I throw everything. I usually just call the scripts by dragging them into the terminal to get their full paths, If it's something that I call often, I remove the `.sh` extension and use it like a normal executable. (Make sure your first line is a proper shebang)
I have quite a few simple functions that are exported by `.bashrc` (probably too many), except they're not in `.bashrc`. They're in a separate file called `bash.extras.sh` which gets sourced by `.bashrc`. This helps me keep my `.bashrc` clean, and helps me distinguish between what is 'original' and what is 'mine'. I have another one that gets sourced for non-interactive sessions called `bash.always.sh`. That one really only modifies `$PATH`. But like everyone else said, if it's more than a few lines it probably needs it's own script.
In my system there is a generous pseudo user, 'common', who provides all other users with software, data and services. Common's $HOME is in '/home/common' with many [files/subdirectories](http://i.imgur.com/kd7PIoG.jpg). Users inherit many environment variables after login: echo $PATH /home/glesialo/bin:/home/common/bin:/usr/local/bin:/usr/local/Bins/D.Java/bin:/usr/local/Bins/D.Java/jre/bin:/usr/local/Bins/D.Java/db/bin:/bin:/usr/bin:/usr/bin/X11:/usr/games Bash scripts (and other local programs: java, python...) usually go into /home/common/bin or /home/common/sbin (only available to root). Sometimes I use scripts in 'cascade'. Example: I have written a shell wrapper for video player 'mplayer': inpath mplayer Directory '/home/common/bin/': mplayer Directory '/usr/bin/': mplayer Directory '/usr/bin/X11/ -&gt; /usr/bin/': mplayer Shell wrapper '/home/common/bin/mplayer' locates the next 'mplayer' in the $PATH chain ('/usr/bin/mplayer') and invokes it after doing 'shell wrapper things'. '/home/common/bin/mplayer' is the system wide 'mplayer' (what runs when you type 'mplayer' in a terminal) but user glesialo can have his own personalized version: '/home/glesialo/bin/mplayer' (which will run when typing 'mplayer' in a terminal). glesialo's 'mplayer' locates the next 'mplayer' in the $PATH chain ('/home/common/bin/mplayer') and invokes it after doing 'shell wrapper things'. 
Baller username!
This is more or less the stuff I can do. But the script is examining two specific directories I don't want it to find anything outside of those. What I'm having the most trouble with is the variables line the - a and others I want to have in it 
Instead of answering this question, (which I probably can't anyway) I'd like to ask you another one. Why do you need a script that will do this? What is the larger problem that you are trying to solve, here? The reason why I ask, is because this sounds excessively complex.
What about using something like this to find duplicates? [phil@greenmachine]$ ll dir* dir1: total 0 -rw-rw-r-- 1 phil phil 36 Sep 30 03:37 duplicatefile -rw-rw-r-- 1 phil phil 21 Sep 30 03:38 file1 dir2: total 0 -rw-rw-r-- 1 phil phil 36 Sep 30 03:37 duplicatefile -rw-rw-r-- 1 phil phil 13 Sep 30 03:38 file2 [phil@greenmachine]$ diff -srq dir1/ dir2/ Files dir1/duplicatefile and dir2/duplicatefile are identical Only in dir1/: file1 Only in dir2/: file2 As for the rest of your script, I would structure it to function as follows ./dirdeduper dir1 dir2 -a In your code you can have a bash function called usage which will display a helpful usage message to users if they don't enter the correct set of commands. # Set bold text and return text BT='\033[1m' RT='\033[0m' usage () { echo -e "Usage: ${BT}./$(basename $0) ${RT}[${BT}-a${RT}] [${BT}-h${RT}] [${BT}-n${RT}] [${BT}-t${RT}]\n" ${BT}-a${RT} : Do some stuff ${BT}-h${RT} : Show this usage message ${BT}-n${RT} : Do different stuff ${BT}-t${RT} : Specify temp dir } TEMP="$(mktemp)" Now, this is some stuff I had written in the past for myself when I was interested in getopts. You'll need to fit it to your needs. I have a double case statement below because I had wondered how to do that too. This should be what you wanted. I urge you to read the [following documentation](http://wiki.bash-hackers.org/howto/getopts_tutorial) before you implement this as it was confusing to me while trying to grasp it. On the ?) option, you can see a call to the usage function I had written above. while getopts "hwa:i:n:" opt; do case $opt in a) case $OPTARG in ''|*[!1-9]+$) echo "-d argument must be &gt;0"; exit;; *) DATE=$(date +%Y-%m-%d -d "$OPTARG days ago");; esac;; h) usage;; n) case $OPTARG in ''|*[!2-9]+$) echo "-i argument must be &gt;1"; exit;; *) RANGE=$OPTARG;; esac;; t) DATE=$(date +%Y-%m-%d -d "1 week ago"); RANGE=8;; ?) usage;; esac done I would use [these tests](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_07_01.html) as to determine if the two directories you specified exist, to see if they're both empty because then why even search, to see if the first two arguements passed into the script are not getopts args and are directories. If they're directories you would whatever XYZ. 
there's a program on most distros available called `fdupes` that compares file size and hashes of files.. something along the lines of `vimdiff &lt;(find dir1 -exec md5sum {} +) &lt;(find dir2 -exec md5sum {} +)` might work for you though
using diff sure is a lot simpler
Better start learning, the whole thing might sound complex but it's not hard learning one piece at a time.
There's probably a real command to do it, but something like this should approximate it: egrep ^/ /etc/shells | grep -f - /etc/passwd | cut -f1 -d:
Keep playing with linux and you'll get there.
I don't know why; ^ isn't a special character for bash.
As far as I can determine from the mess of half-information given, Giant\_IT\_Burrito has a directory of files named according to the pattern IRSTR\_\*\_YYYYMMD_EOD.csv where YYYYMMD represents a numerical date, and the problem they're trying to solve is how to rename each file in the directory so that one business day is subtracted from the date part of its filename (i.e. skipping weekends). Is that correct, /u/Giant_IT_Burrito ? 
ah, forgot I was trying out fish. Works fine as is in bash, works in fish with the single quotes.
I was doing the same with zsh. Keep forgetting it isn't bash because it usually doesn't matter. 
My guess at the general thought process, might help: * I need a list of users, I know /etc/passwd has it * Which of these user can login? Users that can login have a shell * I need to identify lines in /etc/passwd with a shell, but I need to know what shells are available to check for them * /etc/shells has a shell per line. Some lines are comments so I'll grep for only entries that start with "/" ( grep ^/ ) * I'll grep through /etc/passwd, passing the list of shells as matching patterns to grep ( "-f -" says use the piped in list as match patterns ) * I don't need all this other shit, I'll just get the first column, using ":" as a field delimiter.
Oh nice, yeah my first thought was scraping /etc/passwd for shells and having an array of "usable" shells but that's fucked up too.
Going with the same strategy, here's an extended version that cross-references with `/etc/shadow` to see if the accounts with valid shells are enabled, and limits the searches to complete field values in the correct positions: grep ^/ /etc/shells | xargs -n1 -i echo ':{}$' | grep -f - /etc/passwd | cut -d: -f1 | xargs -n1 -i echo '^{}:\$' | sudo grep -f - /etc/shadow | cut -d: -f1 The `grep -f` strategy is `O(n/2)`-ish per line, so a server with a ton of users will have to do more work than is really necessary in the shadow cross-reference. Here's something akin to a set intersection that's much faster with a large data set: sort -m \ &lt;(grep ^/ /etc/shells | xargs -n1 -i echo ':{}$' | grep -f - /etc/passwd | cut -d: -f1 | sort) \ &lt;(sudo awk -F: '$2 ~ /^\$/ { print $1 }' /etc/shadow | sort) \ | uniq -d
On monday, the date is -3 because it needs the one from friday. It can either create a copy and name it or rename the latest file to fit the date format.
Pretty sure this does what you want: find -name 'IRSTR_*_EOD.csv' -perm -g=w | while read -r filename; do IFS=_ read _ _ filedate _ &lt;&lt;&lt;"$filename" case "$(date -d "$filedate" +%u)" in 7) sub=2 ;; 1) sub=3 ;; *) sub=1 ;; esac lastbiz=$(date -d "$filedate - $sub days" +%Y%m%d) newfile="${filename/$filedate/$lastbiz}" mv -v "$filename" "$newfile" chmod g-w "$newfile" done The default umask generally creates files with g+w which likely isn't needed in this case, so I use that bit as a pending/done flag. A test: $ for i in $(seq 0 9); do touch IRSTR_foo${i}_2014092${i}_EOD.csv; done $ ls -l *.csv | awk '{print $1,$9}' -rw-rw-r-- IRSTR_foo0_20140920_EOD.csv -rw-rw-r-- IRSTR_foo1_20140921_EOD.csv -rw-rw-r-- IRSTR_foo2_20140922_EOD.csv -rw-rw-r-- IRSTR_foo3_20140923_EOD.csv -rw-rw-r-- IRSTR_foo4_20140924_EOD.csv -rw-rw-r-- IRSTR_foo5_20140925_EOD.csv -rw-rw-r-- IRSTR_foo6_20140926_EOD.csv -rw-rw-r-- IRSTR_foo7_20140927_EOD.csv -rw-rw-r-- IRSTR_foo8_20140928_EOD.csv -rw-rw-r-- IRSTR_foo9_20140929_EOD.csv $ ./irstr_rename | sort `./IRSTR_foo0_20140920_EOD.csv' -&gt; `./IRSTR_foo0_20140919_EOD.csv' `./IRSTR_foo1_20140921_EOD.csv' -&gt; `./IRSTR_foo1_20140919_EOD.csv' `./IRSTR_foo2_20140922_EOD.csv' -&gt; `./IRSTR_foo2_20140919_EOD.csv' `./IRSTR_foo3_20140923_EOD.csv' -&gt; `./IRSTR_foo3_20140922_EOD.csv' `./IRSTR_foo4_20140924_EOD.csv' -&gt; `./IRSTR_foo4_20140923_EOD.csv' `./IRSTR_foo5_20140925_EOD.csv' -&gt; `./IRSTR_foo5_20140924_EOD.csv' `./IRSTR_foo6_20140926_EOD.csv' -&gt; `./IRSTR_foo6_20140925_EOD.csv' `./IRSTR_foo7_20140927_EOD.csv' -&gt; `./IRSTR_foo7_20140926_EOD.csv' `./IRSTR_foo8_20140928_EOD.csv' -&gt; `./IRSTR_foo8_20140926_EOD.csv' `./IRSTR_foo9_20140929_EOD.csv' -&gt; `./IRSTR_foo9_20140926_EOD.csv' $ ./irstr_rename # Nothing should happen since g+w has been removed from the files $ $ touch IRSTR_foo10_20140930_EOD.csv $ ./irstr_rename # Should only process the new file `./IRSTR_foo10_20140930_EOD.csv' -&gt; `./IRSTR_foo10_20140929_EOD.csv' If you look at the dates in the filenames the days of the week change accordingly: Orig | New ---|--- Sunday|Friday Monday|Friday Tuesday|Monday Wednesday|Tuesday Thursday|Wednesday Friday|Thursday Saturday|Friday
I have a folder ~/bash for my coding projects. Silly stuff and notes go in that folder and tracked/git projects have their own subfolders.
yeah, i get that. My concern is that if the file format stays the same and you apply the procedure to all of them that means the algo can't figure out which file is renamed already and which is not. That would mean that every time the cron runs each file gets rolled back in time by one business day (=files endlessly drifting back in time which i am not sure you want), not to mention possibility of collision, depending on the order of operations, where newer file overwrites older one before it gets to move out of the way. Your description imo lacks sufficient background to suggest a truly solid solution. I get that you came here to look for the date rollback algo, but the algo is not going to work in vacuum. Some more detailed information how this interacts with other parts of the workflow pipe would be nice because there might be caveats that you'd like to avoid. ---------- It would be nice if you could give an abstract overview of the whole pipeline including possible corner cases, eg: i need a script to put in a workflow pipeline prog A -&gt; script -&gt; prog B prog A spits out files named file_YYYYMMDD.txt on every business day i have files file_20141009.txt // already renamed file_20141010.txt // already renamed file_20141014.txt // new file prog B expects these files to be stamped with $(DATE - 1 business day) instead of DATE because of that I need a script to have [SOME/ALL] files renamed [ONCE/X TIMES] to file_$(DATE-1 business day).txt expected state is eg: file_20141009.txt // already renamed file_20141010.txt // already renamed file_20141013.txt // new file renamed, preserved format the script is going to run every day. -------------- Description plus tangible example is better than a wordy prose and selling your problem accurately to others is definitely a skill. All in all it sounds a bit like a very frequent X-Y problem, where a person really wants Y but asks about X because s/he envisioned it as a part of solution towards Y, when in reality a different, easier, cleaner path toward Y exists. Here X is going to be used in Y, but X alone might not be sufficient if there are caveats that one needs to be aware of, that need to be covered by the complete solution. So given my abstract example as a template you want to preserve format and roll back by 1 business day but only on the newest file in the directory? What should happen if the machine is down and the cron job doesn't run one time? 
`if [ "$HOME" = "/home/local.mydomain.$USER" ]`? That seems brittle, and I'm not sure that is the best way to accomplish your goal. Windows Active Directory login can push startup scripts to the user, can't Samba4? And hijacking the login startup scripts to show (what I assume is) a restricted operations menu via shell seems a bit easy to bypass. Especially if the users can write their own `.bashrc`.
BTW, your suggestion worked perfectly as far as the .bash_profile. So the only issue now, I think, would be fool-proofing the menu.
Disable Ctrl-C with: trap '' 2 (see 'help trap') 
I suppose that if they can just abort the menu program then foolproofing the launch chain isn't a priority.... Just curious as to why you can't push the menu script to the client on login.
It is Samba4 but it is primarily an ADS system for Windows clients. I'm not really sure how to push out Linux scripts but I'm sure it is possible... The trap suggestion below seems to have solved the Ctrl + C issue anyway
Test based on a group the users belong to? Get user information with `id` or `getent`.
&gt; In this case I am not synchronizing users and groups with the DC He never said you were...?
How I've done this in the past is to set their shell to be the script Before: -&gt; login -&gt; interpreter (e.g. /bin/bash) -&gt; script After: -&gt; login -&gt; script Less opportunity for them to break out to something if their immediate breakout point is from the script... to the script. chsh -s /domain/menu.sh [account] should do what you want. If not, you will need to either manually nurse /etc/passwd, or add /domain/menu.sh to /etc/shells and try chsh again. Then in your menu script you need to lock them inside a while or until loop and use traps to capture Ctrl-C at least (which you've already done), and you may like to trap Ctrl-Z and Ctrl-D as well. Then obviously in your menu you have a Q) option for quit, that sets the exit condition for the loop. Then at the end of the script you kill the session, so if they do escape from the loop - either intentionally or not - they're kicked out. Have a google for "linux unix captive login menu" or something along those lines. Also, for the test you're looking at, I'd use a contains test e.g. if [[ "${HOME}" =~ .*mydomain.* ]]; then... ... something like that. But if all these users are going to have the script as their shell, as I've suggested, you'd want to look to automate that so that their shell is set on creation. For existing users you could do something like: for u in $(ls -1 /home/*mydomain* | cut -d "/" -f3 | cut -d '.' -f1); do chsh -s /domain/menu.sh "${u}"; done Though double check that... This post was brought to you at 4am my time, because I'm on-call and waiting for a massive backup file to gzip to resolve a priority 2 call out. I'm quite tired, so take sleep deprivation into account when reading the above.
OK let mew try this. We are in the process of automating our trading platform. We have a program that reads in a formatted csv formatted in the way i stated before. This file has yet to have its contents changed. The program looks for the file with the most recent date(as i was told. I did not create the program) The script should find the file from the previous biz day and rename it with the most recent biz date. The cron job would be set to only run monday -friday and if the machine goes down, worst case is i manually run the script or change the file names myself. so file a&gt; script &gt; file b the files are all in a format that contains *YYYYMMDD*.csv The folder will only contain one set of files, the folder is called latest. When a program starts, it imports the files in this folder that fit this convention. At the end of the day, the program closes and at that point the files can be renamed for the next biz day. Is that better?
How can I test the users for a group they belong to, which would be different from any other local user, when their bash_profile is getting created?
I think the point he is making is that the conditions in which the feature was secure changed drastically 5 years later, making the developers implementing Bash outside its scope vulnerable to critical mistakes. In this perspective, the bug is not a bug, but a pothole dangerous to vehicles that didn't exist on the road at the time the road was paved. Consequently, the drivers should know what they're doing and avoid the potholes, while the road builders update the road or reiterate a vehicle restriction.
i still consider it a bug because allowing hybrids with side effects simply doesn't make sense here. There is no upside to it and given that shells expose pretty much all functionality to users it is asking for trouble. There is only one namespace and no variable is truly internal. Failing to terminate function def correctly is somewhat similar to failing to terminate a C-string with null. The latter is certainly considered a bug/security flaw even if that doesn't happen in the normal use case. 
Never heard of shflags. It's true that there are some things you can't do with getopts, but if you need to do something reasonably complex I would recommend a different language anyway. This is how basically every bash script I write starts out: #!/bin/bash readonly ME=`basename $0` Show_help() { cat &lt;&lt; EOF $ME - Do something. -a :: A thing, accepts an argument that tells you which thing. -z :: Zomething. -v :: Verbose. EOF exit $? } verbose=false z_set=false a_thing="" while getopts "a:zvh" opt do case $opt in a) a_thing="$OPTARG" ;; z) z_set=true ;; v) verbose=true ;; h) Show_help 0 ;; *) Show_help 1 ;; esac done I don't think that's too complex. Once you give up on long options and accept the small amount of overhead that this documentation style requires, it isn't that bad. If you really need long options, I'd suggest an `if grep "--long-opt" &lt;&lt;&lt; $*; then ... ; elif grep "--another" &lt;&lt;&lt; $*` style block after the getopts loop. You could also use equivalent bashisms, and you'll probably need to bring in sed if your options have arguments. 
If you absolutely NEED long options you can use getopt(3). It's close to getopts and supports short and long options.
docopt looks great
Why do you need sudo? Can't you just change permissions on your own file? If the script works on its own but not via Transmission, it sounds like a Transmission problem, not a bash problem.
I don’t know about `rc`, but to get your result you can use: $ echo path/{a,b,c}.c This is called [*brace expansion*](https://www.mankier.com/1/bash#Expansion-Brace_Expansion). For more complicated commands than `echo`, I don’t think there’s a way to distribute them over argument lists… in that case, afaik, yes, you need a loop. EDIT: Oh, and brace expansion is a bash feature. If you need POSIX `sh`, that’s not available.
&gt; echo path/{$(printf '%s,' a b c)}.c But couldn't you just use printf directly? printf 'path/%s.c ' a b c
This doesn't completely work, and, is not POSIX shell.
I haven't tried making it change file permissions but I've found that script-torrent-done runs when the torrent finishes seeding, not when it finishes downloading.
what about the permissions and ownership on the actual torrent/torrent location?
Sadly, /r/shell is not as active as /r/bash. :(
They're the only shell you can rely on on POSIX systems. And yes, I mean POSIX, not Linux, not GNU. Some features from various shells are really useful, like concatenation and brace expansion talked about here, or named pipes or arrays, but I can't rely on them.
&gt; Do you need a loop? Yes.
Most likely sudo is refusing to run because there's no terminal. Consider explaining what you are **really** trying to achieve.
Love it. Reposted to hackernews https://news.ycombinator.com/item?id=8443635
I have never used zsh, and I'm actually pretty new to bash. I must investigate this zsh shell...
`setopt autocd`
On OSX, you will need to route the 'source .commacd.bash' into .bash_profile, rather than .bashrc. In other words: curl https://raw.githubusercontent.com/shyiko/commacd/master/commacd.bash -o ~/.commacd.bash &amp;&amp; echo "source ~/.commacd.bash" &gt;&gt; ~/.bash_profile
Seems really useful, btw. Thanks for posting.
Transmission downloads files under the transmission-daemon user. I haven't had time to test it out yet, but I believe setting umask to 0 will take care of most of my problems. I was unaware of that setting. Thanks for the help on that one.
What I am really trying to do is to have all the torrent files/folders set to my user account, not debian-transmission, as well as have permssions set to 777 so a variety of other users and applications can access them. /u/splatter72 recommended setting umask to 0 in settings.json. I think this will get my most of the way.
I will add them.
Functions are loaded into memory every time you start the shell. These should be reserved for often-used routines that are relatively short. Anything that is relatively long should be placed in a separate script. If it is a command that does not need to manipulate the values of variables or command line parameters, you should create an alias. If you have an alias that you are trying to figure out how to make behave conditionally or that you want to operate iteratively, it’s probably time to convert it to a function. If you have a function that is beginning to take up a large swath of space in your ~/.bashrc, it is probably time to turn it into a separate script. This is a judgment call. You can also separate out parts of your .bashrc into separate files and use: source /path/to/separate/file-for-functions or something in your ~/.bashrc. This can get ridiculous, but it also makes your ~/.bashrc more manageable. You can also do this for useful shell script functions that you use over and over in many scripts. 
No problem. if you Do need to have transmission do something via sudo you may want to do something like this: https://www.reddit.com/r/linuxadmin/comments/2afn2b/sususudo/ umask was only part of my solution since I wanted transmission to also shut itself down, change the routes &amp; kill the VPN.
&gt; Oh come on, there's never any excuse to set mode 777 I hear you, but for me there is only one user (me) &amp; we are talking about videos so I'm a little lax &amp; just want to be able to manipulate my files without always using sudo.
I know this is an old post, but I'm quite fond of parameter expansion (or substitution): INTERFACE=eth0 TABLE=$(( ${INTERFACE//[a-z]/} + 3 )) This will work with any interface type (eth, en, bond, etc.): $ INTERFACE=en0 $ echo "$(( ${INTERFACE//[a-z]/} + 3 ))" 3 $ INTEFACE=bond6 $ echo "$(( ${INTERFACE//[a-z]/} + 3 ))" 9 Good read on parameter expansion: http://www.cyberciti.biz/tips/bash-shell-parameter-substitution-2.html 
You can use the `fdupes' utility, but it discovers the duplicates in any place in the given directories
&gt; I hear you, but for me there is only one user (me) &amp; we are talking about videos so I'm a little lax &amp; You're wrong about that. Maybe there's only one "human" user, but your system will still contain a lot of "system" users. Do you know why services, such as web servers and databases and transmission etc... run as separate, unprivileged users? because if they happen to have an exploitable bug that lets attackers run some code remotely, or otherwise do unintended things, then at least it's limited what damage they can do to the rest of the system. Most such system users only have write access to `/tmp`, so... oh but wait, look at all those video files with mode 777 (-rwxrwxrwx) let's add malicious code to some of them and wait for an unsuspecting user to double click the videos thinking they will be played as videos... &gt; just want to be able to manipulate my files without always using sudo. The method I explained allows you to do just that. And given that only your user needs the access, you can use your primary group instead of creating a new group.
I am putting it into my i3bar with i3blocks. i was just reading 'awk'. Would that be better?
No, awk is heavier than sed. But for completeness, it could be done via df -h | awk "NR == 2"
 df -h Filesystem Size Used Avail Use% Mounted on /dev/sda2 19G 8.7G 8.7G 50% / none 4.0K 0 4.0K 0% /sys/fs/cgroup udev 2.0G 12K 2.0G 1% /dev tmpfs 396M 1.4M 394M 1% /run none 5.0M 0 5.0M 0% /run/lock none 2.0G 4.6M 2.0G 1% /run/shm none 100M 32K 100M 1% /run/user /dev/sda3 19G 7.6G 9.8G 44% /home /dev/sda4 1.8T 1.8T 15G 100% /home/common/Store ===================================== df -h | grep "/dev/sda2" /dev/sda2 19G 8.7G 8.7G 50% / 
Use df -Ph to get it in one line to avoid line breaks 
What's with the '-' as the second argument to grep? OSX Mavericks isn't happy with that. Quick web search doesn't reveal anything. Is that a typo?
Not a typo; that indicates that the file given to grep's -f should be stdin. There are quite a few OS X command-line tools that work slightly different than Linux's (they're mostly BSD). Perhaps they would like it if it were rewritten as grep -f &lt;(egrep ^/ /etc/shells) /etc/passwd | cut -f1 -d:
Even though OP said he wanted the 2nd line, I like grepping for the specific device. If the order changes for some reason, you'll still get the info you want on that device, rather than whatever fills in the 2nd line.
I am trying to put the total size and the used size for my / partition in my i3blocks. and then I plan to make it clickable to show all the partitions. I got it to show up but it was the first line and not the one I wanted. Why is awk too heavy, it seems perfect for reading the output of df?
I would like to have this line in my i3bar but how do I not get the /dev/sda/ ?
 df -h | grep /dev/sda1 | awk '{print $2,$3,$4,$5}' This will output the following (your numbers will be different of course) 213G 10G 192G 5% Another way with a whole millisecond difference on my system. df -h /dev/sda1 | tail -n1 | awk '{print $2,$3,$4,$5}' 
I'm seeing lots of comments about greping for the device. Why not pass the device as an argument to df? df -h /
If you're not trying to make some generalized tool that works everywhere who cares if different systems are different; just make it fit your own.
I tried that but then it shows the first line only in the i3bar. I need the second line. If I can use to show just the info that would also be fine.
thanks, I think it doesn't matter so much on the speed since it is just in my i3bar. I will have it repeat like conky every 10sec or 30sec or something.
in a single-line application (like a statusbar in i3) this will only show the first line. He'll still need to get the correct line from that output
Thanks, that pretty much what I did in the end. I am going to post a gif when it is finished.
 df -h / | tail -1 Example: df -h / Filesystem Size Used Avail Use% Mounted on /dev/sda6 19G 7.2G 12G 38% / df -h / | tail -1 /dev/sda6 19G 7.2G 12G 38% /
One of the best ways to debug such problems is to issue: set -x before you run your script. Then issue: set +x after you run your script. Carefully examine the resulting output. This will usually reveal the problem quickly. You can place set -x at the top of your script if you want to make it easier and remove it after you figure out the problem. 
That is flipping aweomse! The last line you gave worked. I'll be reading up on using pax. 
Even though I was able to get a more simplified solution from the other reply, being the geek I am am and love for learning, i gave this a try to see what it does, however, i saw no difference in the output. Can you provide a link (or give a good term to search for) for finding information on this? Trying to google it, well set is too used to find good results. Thanks.
 df -h / | tail -1
Just so you know, it tried to look specificially for the file * in the folder "/home/greg/sample_html/wordpress" (at least, that's what I could gather from the given information) So when you put the glob in the quotes like that, it errored because it couldn't find the * file. Good luck to you!
hm, TIL. Thanks
Basically, as you can see from all the answers here, there are a lot of different ways to do this. Some are better than others, some are just different approaches and aren't better/worse. Just take what you need, and if it works for you, move on! (I'd also recommend putting your source out too, even if it's in your own blog or something)
That's a good idea, I have put some up on git hub. I will put this up when it is finished.
Haha, I don't care much about performance on this either. Just added that second command to try and make the "awk is to heavy for this" crowd happy. 
 #!/bin/bash exec java [... java args ...] It should pretty much be the same, I think. EDIT: except use : for the paths in your classpath instead of the semicolon.
Hmmm I wonder if there's another way of doing it... Maybe changing my java version? Mmmm, well either way, thanks for all the help!
Running Ubuntu? Install `openjdk-7-jre` and then https://help.ubuntu.com/community/Java#Choosing_the_default_Java_to_use
&gt; javac -d bin -cp "lib/*:" -sourcepath src "src/com/ew/*.java" In DOS/Windows, the program is supposed to parse wildcards, but in Unix/Linux, the shell parses wildcards before passing the result off to the program. Quoting strings prevents the shell from parsing wildcards. So remove the quotes. 
Interesting, didn't know that, thanks for the tip!
Check this out: Why zsh is cooler than your shell: http://www.slideshare.net/jaguardesignstudio/why-zsh-is-cooler-than-your-shell-16194692 zmv -- been needing this utility forever and have been too lazy to look it up: zmv '(*).lis' '$1.txt' EDIT: And here's the official "What is [zsh] good at?" list: 1.3: What is it good at? Here are some things that zsh is particularly good at. No claim of exclusivity is made, especially as shells copy one another, though in the areas of command line editing and globbing zsh is well ahead of the competition. I am not aware of a major interactive feature in any other freely-available shell which zsh does not also have (except smallness). Command line editing: programmable completion: incorporates the ability to use the full power of zsh's globbing and shell programming features, multi-line commands editable as a single buffer (even files!), variable editing (vared), command buffer stack, print text straight into the buffer for immediate editing (print -z), execution of unbound commands, menu completion in two flavours, variable, editing function and option name completion, inline expansion of variables and history commands. Globbing --- extremely powerful, including: recursive globbing (cf. find), file attribute qualifiers (size, type, etc. also cf. find), full alternation and negation of patterns. Handling of multiple redirections (simpler than tee). Large number of options for tailoring. Path expansion (=foo -&gt; /usr/bin/foo). Adaptable messages for spelling, watch, time as well as prompt (including conditional expressions). Named directories. Comprehensive integer and floating point arithmetic. Manipulation of arrays (including reverse subscripting). Associative arrays (key-to-value hashes) Spelling correction. From here: http://zsh.sourceforge.net/FAQ/zshfaq01.html#l4
That’s also neat… I like how it will work even if it’s a background process.
[slp](https://sites.google.com/site/dannychouinard/Home/unix-linux-trinkets/little-utilities/slp) (a more feature-full alternative to /bin/sleep) can sleep until a process terminates. Useful in the case where the process isn't a controllable job. Does a bunch of other neat things too.
If you have the terminal bell activated it's even more useful when the bell sequence is aliased. fg; bell This way you'll get notified when the command is finished. Works pretty darn good with tmux and i3 for instance. 
Btw we are working in bash
What script? Have you even attempted to write one? I'd work on the details of what you need, then research what is needed before posting here.
what's wrong with just 'grep urgent $1 &gt; urgent_messages'?
Nah, there's no point in invoking a new `rm` for every single file.
That error message is not from bash.
minicom maybe? I have no idea.
With \; rm will be called for each file found. With + it will call rm once with every file found as an argument. Runs a whole lot faster if there's lots of files. 
Give this a shot then make the needed changes to meet your assignment. while read -r line do name=$line echo "Read from file - $name" done &lt; FILE
That program is falling into disuse it would seem. It is no longer part of most default installs. 
Better to learn good habits than to unlearn bad ones. 
Didn't know about the -d parameter. That could become handy sometimes.
Quotation is also useful eg: Watch -d " ps aux | grep command | awk ..."
There's a limit to how many files you can pass that way. I had a folder with over 100k files in it that it took me days to figure out how to delete files over a certain age (denoted by filename). e: just did some research and the total command line can be no more than 2 MB long.
Unless you have too many files which extend beyond 2Mb in the one command. 'I generally use \; or throw it to xargs, but that's personal preference. I really wasn't aware of the + use.
It knows the ARG_MAX limit (which is system dependant, typically 2MiB on linuces) and runs more than one `rm` if necessary, just like xargs would. Why use three commands when two is enough, and shorter.
If that's the case then the question to be researched now is what better way is there to automate scripts that require input?
I don't know of an alternative to expect, at least, if you want to keep it interactive. But it would certainly be preferable to get rid of the dependency on user input if at all possible. There are other tools available that I found after looking into my package manager’s database: perl-Expect perl-Expect-Simple perl-Net-SSH-Expect perl-Test-Expect python-pexpect python3-pexpect are all modules for Perl and Python with expect-like funcionality. 
[BashFAQ #1](http://mywiki.wooledge.org/BashFAQ/001) covers this type of parsing (even has an example of parsing /etc/passwd). while IFS=: read -r add nam numA numB; do ...command "$add" "$nam" ...command "$numA" "$numB" done &lt; input.file Don't omit the quotes.
What is PS1 set to in your .bashrc? 
Typing this on a phone. Please excuse my orangutan. #First save your IFS OLDIFS="${IFS}" while read $line ; do # if you have a line like # abc:ttt:8910:def # change your internal field separator, # then "set" the line IFS=':' set $line # Assign each field to a var var1=$1 var2=$2 var3=$3 var4=$4 # do stuff with vars grep "$var2" "$otherfile" &gt; "$file4" "$var4" "$file4" # fix your IFS or you will hate life IFS="${OLDIFS}" done &amp;lt; $inputfile 
The following line should identify where the odd line is: PS4='+ $BASH_SOURCE:$LINENO:' bash -xlic ''
Why try to use an interactive installer non-interactively? Doesn't your operating system have a package management system where you can install and configure mysql non-interactively?
What are the benefits of setting the line and saving the IFS over just using /u/geirha's suggestion?
None for a short script, but for something longer it's easier to read. Also, you have more control over the IFS, if you want to do other things inside your while loop. 
Well that's not true. There's quite a lot of issues with that code. Here's the list of the issues I see currently, I might be missing some: 1. `$line` is not a variable name, it's a variable expansion expanding the value of the variable named `line`, so `read $line` is wrong. 1. If you don't include the `-r` option on the `read` command, it will eat backslashes in the input. 1. Since `IFS` has the default value when the `read` command runs, leading and trailing whitespace will be trimmed away. 1. If the first field of the line starts with a dash (`-`), `set $line` will consider it an option. 1. If any of the fields in the line contains any glob characters (`*`, `?`, `[...]`), bash will try to replace that field with matching filenames (this is called pathname expansion) 1. Missing quotes around `$inputfile` will make word splitting and pathname expansion be attempted on the filename, possibly leading to an `ambiguous redirect` error message. `&amp;lt;` is probably your phone's fault, so I'm ignoring that one.
It was this kind of use-case that caused me to learn Perl many moons ago... I would suggest Python or Ruby nowadays but this is probably the wrong sub for that kind of talk! 
Would you please describe what you mean by "heavier"? Do you mean the awk executable is a larger file size? Takes up more CPU?
search &lt;type&gt; &lt;query&gt;: searchtype="$1" shift searchquery="$*" Not sure what exactly you're trying to pass in as arguments, tho.
So the only relevant part is &lt;query&gt;. I know I want to be searching for an album so &lt;type&gt; is covered. I want to pass into query the album name, which I cut from its path. The problem I'm having is that the &lt;query&gt; needs to be surrounded by quotes, but to preserve the unity of my command substituted variable it also needs to be quoted. So my thinking would be that I need double quotes, but this is not working and is instead returning an empty argument.
This is what's driving me crazy. It feels like the logic is there and I'm just having problem with the syntax. For whatever reason my code above returns an empty argument. Why it's doing that is beyond me, which is why I came to the experts.
That's awesome! I hadn't thought of that before, but that is very helpful. As far as running the command you only need to one set of quotes mpc search album "A Love Supreme" &gt; quoting variable preserves its unity, not literal "s at the ends. So if I want to preserve the unity of my command-substituted variable do I need an additional set of quotes to place the output in quotes or does quoting the variable place the command output in quotes automatically? i.e. does "${ALBUM}" output A Love Supreme or "A Love Supreme"?
well, can you give me an idea of what you're trying to do, specifically? What arguments do you envision needs to come from the command line and what information can you pull from those arguments in the script? I can show you a couple things if I knew what we're looking at. 
&gt; does "${ALBUM}" output A Love Supreme or "A Love Supreme"? the former, as a single string which is forced by quotes. you just need to quote the variable itself and do no further nonsense. The outermost quotes (unescaped) are always for the shell and act a bit like parentheses, telling the shell to group shit together. quotes around "this is string" tell the shell 'take *this is string* as a 1 continuous param'. The recipient (invoked command) won't ever see the quotes, it will just get a single blob of characters to be treated as a single entity. Remember that in case of shell scripts you should always quote your vars otherwise they will undergo word splitting. Unquoted $var will split to separate words, "$var" won't. consider this example (print each seen param in brackets on a separate line) $ x='A Love Supreme' $ printf '[%s]\n' $x [A] [Love] [Supreme] $ printf '[%s]\n' "$x" [A Love Supreme] Note the difference. That's how vars and params work. When in doubt you can always plug your prepared params into `printf '%s\n'` and it will tell you exactly what params are going to be passed. 
This comes up regularly at the mailing list. The behavior changed in bash 4.3. You are seeing the behavior of an older bash version. http://lists.gnu.org/archive/html/bug-bash/2014-03/msg00036.html The easy way out is to put the ~ inside a variable. Also, avoid using echo, especially with options. Use printf instead. tilde='~' PROMPT_COMMAND='printf "\e]0;%s\a" "${PWD/#"$HOME"/$tilde}"' 
Who would have guessed! Thanks for the workaround. 
Okay that's what I thought. When I run `printf '%s/n'` I get the desired output, but I'm still getting nothing when running the script in its entirety. Any ideas? Could there be some way to circumvent this problem using other commands?
show some example of running by hand vs the same via script. Echo every variable you got if you must.
usually i just set up a constant that mimicks repeatable input and then flow from there, checking if results of following steps are as predicted and sometimes i copypaste crucial lines into terminal one by one. There is also -x in bash that enables debug mode (+x disables) which should tell you what happens just put `set -x` near the top 
When consuming the STDIN with the read command, don't use the dollar sign; also I would use the built in prompt vs an echo line. Bash also sees any line in direct single quotes \' as literals so your find statement with "-name '$filename' " will not interpret $filename, use double quotes instead. read -p "Enter the file name: " filename if [ -n "$filename" ];then find /home/dir -iname "$filename" else echo -e "ERROR: no filename specified" fi
I see your 'if' statement was meant to check if the given file was actually valid, not if they actually typed anything. You would need to full path 'find' returns for each match. So let's make it a little more interesting... read -p "Enter the file name: " filename if [ -n "$filename" ];then while read line;do if [ -e "$line" ];then echo "[+] Valid: $line" else echo "[-] Not valid: $line" fi done&lt; &lt;(find /home/dir -iname "*$filename*") else echo -e "ERROR: no filename specified" fi Now it will check for actual input, AND check for a valid file.
Would help to know what it is actually supposed to achieve.
`read` reads a line of input. Without options, `read foo` will take everything up to the first newline from `stdin` and place it into the variable `foo`. For example: $ read foo &lt;&lt;&lt; "I am Steve" $ echo $foo I am Steve If, however, you specify more than one identifier after `read`, the consumed line will be broken up using the value of the variable `IFS` into as many variables as you've specified, e.g.: $ read foo bar &lt;&lt;&lt; "I am Steve" $ echo $foo I $ echo $bar am Steve $ read foo bar baz &lt;&lt;&lt; "I am Steve" $ echo $foo I $ echo $bar am $ echo $baz Steve The information from `help read` may also be helpful to you. To answer your more specific question about echoing the contents of mail messages, I'd have to see more of what you're trying to do.
Thanks! It makes a lot more sense now :) 
I myself created a scripts folder inside /home/user Although I am considering writing a script to sync it with dropbox and/or github. I like the idea of backups.
Something like: result="$( find . -iname $1 )" [ -z "$result" ] &amp;&amp; echo "$1 not found" 
Sad, but true.
It's a conceptually larger language (though they're both Turing-complete). But awk has more machinery, basically. It takes longer to start up and run the same command: $ time for i in {1..1000}; do df -h | sed -n 2p; done [snip 1000 lines] real 0m2.936s user 0m0.123s sys 0m0.210s $ time for i in {1..1000}; do df -h | awk "NR == 2"; done [snip 1000 lines] real 0m3.696s user 0m1.987s sys 0m0.597s 
Thanks for the explanation!
Something about this feels as though you are trying to solve a different problem than you are presenting. 
That's interesting, but why?
Seconded, that it is possible is really cool - but I don't get why either. 
Why does it work, or why bother? Both interesting questions, imo.
You can replace several of those steps with ssh-copy-id.
Why would you be doing that? Don't you normally just have your python scripts... run python?
&gt; The classic example being: I want to use the python to find a directory, then cd to that when the script is finished. The "cd " command has to happen in bash. What's wrong with `cd "$(python /path/to/python/script)"` and have that script return the path to cd into? It would use two files with no mixed languages. Although polyglots are great, they often serve no particular purpose. [This one](http://ideology.com.au/polyglot/polyglot.txt) compiles in 8 languages.
You can [cd just fine in Python](http://stackoverflow.com/q/431684/120999). If you're referring to changing the outside shell's directory, then, well, that's [a much more complicated process](http://stackoverflow.com/q/2375003/120999) no matter your implementation language. Also, I'm obliged to mention that I wrote [a tool](https://github.com/xiongchiamiov/waypoint) that does exactly what you mention (determine a directory to `cd` to, in python) without resorting to this sort of trick (just a *different* awful one ;) ).
Not sure if you're doing this for the sake of learning scripting, and I'm not completely sure what end result you're going for, but based on what you've said I think you may want to take a look at `netcat` or one of its alternatives (`ncat`, `nc`, etc.)
well ive been using C# and socketAPI to make an echo server. i want to try doing it using BASH. i dont even know if thats possible, but it would be weird if it wasnt. yeah i kinda wanna try to learn bash.
Bash can connect to a tcp or udp socket, but it can't bind one for listening itself. So you can write the client in pure bash, but not the server. To write a server you'll have to rely on an external command, `netcat` being a common and simple one.
Ok, ill try that, i dont quite understand what youre doing there, but thats the point, to learn :) thanks
Well, bash doesn't exactly do a lot itself*. It's strength is in tying together other programs that do stuff. When you 'program in bash' what you're really doing is using other programs for 99% of the actual heavy lifting. So when /u/jnux suggests looking into `nc` (or even `telnet`) it's like saying, "I'm going to write a bash script that downloads files off a website, by using `curl` or `wget`." I guess one way to think about bash is that it's just the syntax part of a programming language with the ability to call other libraries. Most programming languages are the syntax plus a standard library of functionality. Usually networking is part of that standard library. Imagine you had a python `interpreter`, but no `socket` library builtin, no `urllib`/`urllib2`. You had to access all that functionality by making calls to other programs. That's what shell programming is like. \*This is getting less true as time goes on, bash actually has a lot of functionality built in at this point. And I believe fairly recently bash gained the ability to do any sort of network communication using the special `/dev/tcp` 'filesystem', but it's very rudimentary. Still though, the vast majority of what bash does is done through calling other programs.
do your own homework..
On how to do math (with integers) in bash, see [Arithmetic expression](http://mywiki.wooledge.org/ArithmeticExpression). Bash doesn't do floating point, but see [BashFAQ 22](http://mywiki.wooledge.org/BashFAQ/022) on how to use commands that do.
It's not homework
Then what's the extra credit for?
It was for an exam, which I done. I did the calculations wrong and tried to fix it when I got home because I was annoyed I didn't do it right, so I tried to make it right and couldn't. But it's ok I got it figured out 
Nice solution, thanks. another question Say i want to create my own simple terminal web browser. so i connect via exec 3&lt;&gt;/dev/tcp/hostname/port send a request echo -e "GET / HTTP/1.0\n\n"&gt;&amp;3 then i can cat &lt;&amp;3 to see the contents. but i only want the lines between the &lt;body&gt; and dont want lines that are in parenthesis (&lt;,&gt;). so how do i do &lt;&amp;3 to a string so i can modify it (maybe using regex) or do you have a better solution ? 
if COMPREPLY is a variable: for word in ${COMPREPLY}; do echo ${word} done or if COMPREPLY is something executable: for word in $(COMPREPLY); do echo ${word} done
You can set the [readline variable](http://www.gnu.org/software/bash/manual/bashref.html#index-variables_002c-readline) `completion-display-width` to `0`. bind 'set completion-display-width 0'
I would quit thinking in terms of Bash if your goal is to do web scraping. While it is possible to do what you want via regex, it is unreliable. A large part of programming is choosing the right tool for the job. While Bash is powerful, it is not the best solution for every job. Use a language that is better suited to the task. Python, Groovy, Ruby, etc.. all have libraries made specifically for web scraping.
It's not really something i want to do because i need it. just for fun. I will figure out how to extract the text, but can you tell me if its possible to put something from a file descriptor to a string ? this is the code exec 3&lt;&gt;/dev/tcp/$imehosta/80 echo -e "GET / HTTP/1.0\n\n"&gt;&amp;3 html=html echo $html html=&lt;&amp;3 echo $html so i want &lt;&amp;3 to go to a string variable. is it possible ? (the code here doesnt work) edit: i think this worked, thanks for the help html=$(cat &lt;&amp;3)
&gt; And I believe fairly recently bash gained the ability to do any sort of network communication using the special /dev/tcp 'filesystem', but it's very rudimentary. The /dev/tcp and /dev/udp redirections were introduced in bash 2.04, which was released around 2000. Not sure if I'd call 14 years *fairly recent*. :)
Using `&gt; file` is the way to go, but you will probably have to change your C program to separate the outputs. Print the prompts on stderr (fd 2), and the results (the ones you want to capture in the end) on stdout (fd 1). fprintf(stderr, "Input your choice: "); ... printf("Result: %s\n", resultvar);
Nice
Yeah decided that &gt; is the best way. I just wrote another c program that parses through the output and grabs the necessary data Thanks 
 if [ "$x" -gt 10000 ] &amp;&amp; [ "$x" -lt 12500 ]; then echo "$x is more than 10000 but less than 12500" fi
Thought thats what it was alright, thank you.
 if [[ $x -gt 10000 ]] &amp;&amp; [[ $x -lt 12500 ]]; then taxRate=0.24 elif [[ $x -ge 12500 ]]; then taxRate=0.40 fi Note that 12500 is an edge case that must be dealt with: either by doing what I did and checking for greater or equal (-ge) or by replacing -lt with -le in the first test. 
&gt; In many cases, this is perfectly safe In value comparisons like we're dealing with here it should work just fine, no?
Thanks for that! So this would work: (( x &gt; 10000 )) &amp;&amp; (( x &lt; 12500)) &amp;&amp; echo "$x is greater than 10K but less than 12,5K" || echo "$x is out of range" The echo returns 0.
If you look in the man-page of `sudo` you'll see this: -p prompt The -p (prompt) option allows you to override the default password prompt and use a custom one. The following percent ('%') escapes are supported by the sudoers policy: %H expanded to the host name including the domain name (on if the machine's host name is fully qualified or the fqdn option is set in sudoers(5)) %h expanded to the local host name without the domain name %p expanded to the name of the user whose password is being requested (respects the rootpw, targetpw, and runaspw flags in sudoers(5)) %U expanded to the login name of the user the command will be run as (defaults to root unless the -u option is also specified) %u expanded to the invoking user's login name %% two consecutive '%' characters are collapsed into a single '%' character The prompt specified by the -p option will override the system password prompt on systems that support PAM unless the passprompt_override flag is disabled in sudoers. So, `-p ""` will clear that. However keep in mind to set the correct permissions for the file holding the password (700).
As a side note, if you want to do it this way, you should probably chuck your password into a dotfile and read it in, something like this: echo $(&lt;.somesecretfile) | sudo -S yum check-update This prevents your password from showing up in a ps listing. chmod 600 it and make sure to delete it when you're done (both scriptable jobs), and you've easily bumped your security up a little bit. /edit: This is useful in environments where you have sudo rights but you can't readily fix sudoers access across lots of systems, and/or if you're doing a quick one-liner. In short: it makes the best of a bad situation. Obviously if this is going to be a routine task, do it properly and setup sudoers right.
You should instead change your sudoers.conf to allow whatever user to run yum check-update with nopasswd, rather than having your password in plain text anywhere.
This is the correct answer. Storing passwords in the clear is really bad practice.
I'll probably do that. The password/script is on my personal laptop and filesystem is encrypted but ya...still a bad practice.
What's the point of the script? Is it a cronjob to update? I ask because I run yum update or apt-get update or a number of servers using cron root jobs, no passwords needed as they're already elevated.
Variables in functions are global unless you declare them local (with `local` or `declare`). Your problem is not with the variables being local; because they *are* global. Your problem is that you are running the function in a subshell (created by the command substitution `$(music)`. Change: echo -e "$(music)" to: music
Thank you. It was a brace expansion, and all files are alive and well in the ultimate folder.
mv -v is your friend -v, --verbose explain what is being done I have an alias for mv cp and chmod to have -v by default. 
So is rm --preserve-root
Will add alias. Thank you.
Mine is alias mv='mv -i -v' The -i asks for confirmation before overwriting.
Here are some links worth looking at: http://cfenollosa.com/misc/tricks.txt http://mywiki.wooledge.org/BashGuide http://www.dwheeler.com/essays/filenames-in-shell.html Bash guide will be a good start.
Like any code but scripts in particular need to be omnipresent. Meaning they need to have logic that will not clobber or break existing configurations. It needs to be executed over and over without errors. As your scripts grow in complexity you'll need to look into exception handling in BASH. Especially if your script accept user input. 
Hey, thanks for the feeds. Coming from PHP world, I understand that the script need to have more abstraction to extend it later easily as it grows, and better error handling but before I move to that stage, I think it is important to know the basics of [separation of concerns](http://en.wikipedia.org/wiki/Separation_of_concerns) which is what I am struggling to find in bash. If you look at the github page, you can see the process on how to install nginx, but my question is: How can it be made better? 
Hey, thanks for the links. As far as bash guide goes, I actually have some links and a video tutorial: [Lynda up and running with bash scripting](http://www.lynda.com/Bash-tutorials/Up-Running-Bash-Scripting/142989-2.html) which explains the basics. But my problem is not with bash programming at all, as I know I can improve the codes with time, but what I have no clue of is, the guide to how file structure should be handled, how arguments should be handled. Like in PHP terms: Design Patterns, OOP best practices, SoP... If you look at [the github page](https://github.com/samayo/centomatic) you can see, than I can install nginx with these commands. $ wget -qO ~/centomatic.sh https://raw.githubusercontent.com/samayo/centOmatic/master/centomatic.sh $ cp ~/centomatic.sh /usr/bin/centomatic $ chmod ug+x /usr/bin/centomatic $ bash centomatic $ nginx If you run the 4th command: bash centomatic the script waits for an input, then according to your choice: nginx, php, varnish ... it will build, configure and run the entered name. ex: If you give it only "nginx" It would run [the nginx](https://github.com/samayo/centomatic/blob/master/nginx/install.sh) script. So, set aside the newbie code, but what would you change about the process? that is what I wanted to know. 
Hey, thanks. But I didn't require help with the actual coding, and counting arguments or that kind of stuff, I needed help with organizing the files, and best practices... you can check out [my explanation more](http://www.reddit.com/r/bash/comments/2mi4jb/new_to_bash_made_this_script_to_learn_the_basics/cm5b5ei) about why I have asked this question. 
[BashGuide](http://mywiki.wooledge.org/BashGuide) has already been mentioned, but I'll link to it again, just in case. Anything to help save your from learning bash by the guides at tldp.org or google. Anyway, looking at the scripts at your github project, the first thing that strikes me is that you put `.sh` extension on a bash script. Bash is not sh, so don't confuse your users by suggesting that your bash script is actually an sh script. Commands don't need extensions anyway, so best to omit it entirely. Next, `centomatic.sh` does one of the worst cardinal sins in unix and unix-like systems; running `chmod 777` on a file. `chmod 777` is always, always, always wrong. Even if you're the only user on the system, you're not the only user on the system. Don't give system users, that has already been specifically given low privileges, the ability to have arbitrary commands run as higher privileged users on your system. Instead, learn how [permissions](http://mywiki.wooledge.org/Permissions) work. Next, `nginx/install.sh` does the same sin. It also uses `set -e`, which should be avoided unless you know of all the edge cases. And once you know of all those edge cases, you'll know it's a pointless feature that doesn't really help you at all. See [BashFAQ #105](http://mywiki.wooledge.org/BashFAQ/105) for more on that.
New link http://www.dwheeler.com/essays/filenames-in-shell.html
solved in other thread: alias copy='echo $(history -p !!) | xclip -selection clipboard' 
See [BashFAQ #100](http://mywiki.wooledge.org/BashFAQ/100) for more.
Bingo: This is the kind of response I was hoping for. I was not aware of `.sh` extension. Thanks for that. Spot on the permission settings too. the `set -e` command though is something I have problem getting around to. I asked [this question on SO](http://stackoverflow.com/questions/26934153/can-i-make-bash-report-errors-only-errors-at-the-end-of-a-script) maybe you can shed some light depending on the answers I got. thanks again 
from mobile so not tested curl -s http://fetchip.com &gt; /path/to/output
That's not too bad! With some grep: curl -s http://fetchip.com | grep -Eo '[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}' &gt; myipfile Grepping from: [What regular expression can I use to match an IP address?](http://superuser.com/questions/202818/what-regular-expression-can-i-use-to-match-an-ip-address)
Isn't --preserve-root a default option?
Should be as simple as this: curl -s icanhazip.com &gt; ip.txt This site adds in no extra HTML / CSS / etc. coding at all. Great for using is scripts etc.
 `dig +short myip.opendns.com @resolver1.opendns.com &gt; /path/file`
Exactly what I was going for, and downvote turns into upvote :)
&gt; chmod 777 is always, always, always wrong. But it's so fun to type! :P
Just whipped this up recently. It's more than you ask but the concept is the same. Works but all the things I wanted aren't finished yet. Cut/paste/rob/plagarize as you see fit. http://pastebin.com/XHgYnPAc
Maybe you can scan through /etc/mtab. That will tell if a network share is mounted. [here is a thread](http://ubuntuforums.org/showthread.php?t=1171012) that talks about it.
I added this code from that link. if mount|grep $volume; then echo "mounted" else echo "not mounted" fi It's not mounted. Directory does not exist! Directory does not exist! not mounted What am I missing, lol? 
 &gt; #!/bin/bash &gt; &gt; DIR="/media/wdmybook/folder" &gt; &gt; if [ ! -d "$DIR" ]; then &gt; &gt; echo "Directory does not exist!" That code does not test for the existence of a directory. It tests whether a file is a directory. * -e tests for the existence of a file, and a directory is a file * -f tests if a file is a regular file, such as a text file or an image file * -d tests if a file is a directory rather than a regular file &gt; elif [ "$DIR" != -d ]; then &gt; &gt; echo "Not a Directory" That code tests whether the string contained in $DIR does not match the string ‘-d’. It should run the corresponding echo command and display the string “Not a Directory” every time you run it while DIR contains /media/wdmybook/folder. But it still does not test whether $DIR is or is not a directory. If you want to test whether /media/wdmybook/folder ($DIR) exists and is a directory, this code should do it: if [[ -d $DIR ]]; then echo "$DIR is a directory" fi Or if you want to do something when it does not exist as a directory: if ! [[ -d $DIR ]]; then echo "$DIR is not a directory" if ! [[ -e $DIR ]]; then echo "and it does not exist" elif [[ -f $DIR ]]; then echo "because it is a regular file" fi fi 
&gt; if ! [[ -d $DIR ]]; then echo "$DIR is not a directory" if ! [[ -e $DIR ]]; then echo "and it does not exist" elif [[ -f $DIR ]]; then echo "because it is a regular file" fi fi I just ran that script, and it gave no output whatsoever. Nothing. My goal is to be able to detect if a paramter is a folder/directory/subdirectory, and if so use that to do some work. If it fails to detect it as above, then it just fails. Thank you.
This is working for me: #!/bin/bash if grep '/dev/sdb1' /etc/mtab &gt; /dev/null 2&gt;&amp;1 ; then echo "mounted" else echo "not mounted" fi It is correctly saying mounted or not depending on if my thumbdrive is plugged in.
&gt; if grep '/dev/sdb1' /etc/mtab &gt; /dev/null 2&gt;&amp;1 ; then echo "mounted" else echo "not mounted" fi Good work, but I think, I dumbly may have side-tracked this conversation. I don't need to check if it is mounted, just if it is a directory/folder, etc. But I wonder, would bash be totally unable to check if a mounted subdirectory is a subdirectory?
I did. I set it to /home/user and it outputs nothing. Only when I set it to a file does it output anything. Regardless of whether it is a real file or not. 
Which is what I wrote it to do, but as I said in another comment: if [[ -d $DIR ]]; then echo "$DIR exists and is a directory" fi will detect your directory and will allow you to then do something based on that. Test it. 
I tested it, and it works. But it wasn't working last week. I am just befuddled to be honest. 
I suggest you spend some time at [Bash Hackers Wiki](http://wiki.bash-hackers.org/doku.php) and perhaps with the book [The Linux Command Line](http://iweb.dl.sourceforge.net/project/linuxcommand/TLCL/09.12/TLCL-09.12.pdf). 
Thank you for your time and effort.
great. I understood your first script, but second needs some time to grasp. Although, with the first script, I would have to do `&gt;&amp;2` for every message I want to echo, but your solution is much more efficient than mine, so I will go with that for now.
Not `&gt;$2`, `&gt;&amp;2`.
Oh, sorry. Yeah, got it.
&gt; I would have to do &gt;&amp;2 for every message I want to echo Not necessarily. You could define a function to encapsulate echo and use that instead. Example: #!/bin/sh exec 3&gt;&amp;1 &gt;$TMPDIR/stdout 2&gt;$TMPDIR/stderr msg() { echo "`date` $*" &gt;&amp;3 } command1 msg "Command1 done." command2 msg "Command2 exit code: $?" Example Output: $ sh toto.sh Wed Nov 19 03:37:08 EST 2014 Command1 done. Wed Nov 19 03:37:08 EST 2014 Command2 exit code: 127
Wow, this is mind blowing. Thanks. 
According to your help, I remade an old scrip to this [pastebin](http://pastebin.com/Q0WtWyh3) what do you think? Is it "find" or "horrible"? It's just for a pet project, to get used to / learn bash.
My use of "command1" and "command2" were just pseudo examples of commands, they're not to be used verbatim. Your calls to the msg function should be peppered in sequence and in-between your real commands; yum, wget, tar, make, etc, in the positions where you have `# comments` right now, so that they indicate the progress of your script in real time. 4:42 AM, I'm off to bed. G'night and have fun!
Hi! I really like this solution but I can't get mail-command to work. I don't know if its due to a lack of configuration or some other problem. When I try 'mail my@email.adress' I get the Cc-field for a few seconds but then Segmentation fault: 11 Could one use alpine for the same thing? 
**Thanx all** I'm not done yet but I like [/u/2cats2hats](https://www.reddit.com/user/2cats2hats) solution very much - must just get the mail-part to work...
In my comments it describes what packages you need to use this script.
I've solved this in the past with something along the lines of: MYFOLDER="/media/wdmybook/folder" DIR=$MYFOLDER if [[ -e $MYFOLDER ]]; then until mountpoint -q $DIR; do dir=`dirname $DIR` done echo $MYFOLDER is mounted on $DIR else echo "$MYFOLDER doesn't exist" fi
I cannot reproduce this issue in bash 4.3.30(1). I made a test file (hidden) containing only a cd command, and I sourced it: success. Is there anything else in your file? Edit: Never mind, stupid question. I see the output of your cat call. Is this sequence of commands exactly what you are executing?
If I execute "source ./.venv", it works. If I execute it without "./", i.e. "source .venv", error. Yes, I execute them in this order. However, I have some other .venv files in different folders. I think it tries to find it somewhere else first, not immediately in the current directory. Edit: I found something [here](http://www.gnu.org/software/bash/manual/html_node/The-Shopt-Builtin.html): sourcepath If set, the source builtin uses the value of PATH to find the directory containing the file supplied as an argument. This option is enabled by default. Update: I have the solution: "shopt | grep sourcepath" says it's on. How to switch it off: "shopt -u sourcepath". Interesting. I always thought that "source" is looking for its parameter in the current directory. Thanks for the help!
I did that before, which works, but I wanted to make sure it was a numeric input, hence the regex.
Missing a `?` to make the fractional part optional. number='^0?[0-9]?[0-9]([.][0-9]+)?$|^100$' # that one ^ prompt="Enter a grade between 0 and 100: " while read -ep "$prompt" -i "$input" input; [[ ! $input =~ $number ]]; do printf &gt;&amp;2 "That is not a number between 0 and 100.\n" done 
I saw ssmtp but can't figure out how to install that utility 
sudo apt-get install ssmtp
 read input case "$input" in *[!0-9.]*) # check to ensure nothing but numbers are input echo "$input: input must be numeric" exit 1 ;; *) # case for proper input do something ;; esac This should do what you want. 
regex is an overkill `[[ $input = [^0-9] ]]` should get rid of anything with non-digit in it and once you get past that `(( 0 &lt;= 10#$input &amp;&amp; 10#$input &lt;= 100 ))` will check if the number is in required range, regardless of 0 padding (implies octal) 
Mac OS X doesn't support apt-get/Debian packages :(
There might be an equivalent with homebrew. I only own PPC macs so I don't use it. http://brew.sh/
I got it to work with *mutt* but I have yet to get it to run as a launchd service..., last hurdle.
And now we're up and running! The problem with running the script as a launch daemon was that one has to use the **full** path to every invoked app (*wget* in this particular case). Thanx! Now I just have to see if I get a mail if/when my IP changes.
 man scp
+1. Because you (/u/taglay) are over-engineering. What does scp do? It does: 1. Connect to remote host 2. Locate file 3. If file does not exists, abort 4. Copy file What do you want to do? 1. Connect to remote host 2. Locate file 3. If file does not exist, abort 4. Connect to remote host 5. Locate file 6. If file does not exist, abort 7. Copy file
You can do ssh login@host ls /dir/ No idea about the returned value.
You guys are making this too hard; here's a simple one liner. export FILETEST=$(ssh user@remote.host.com "if [[ -e /path/to/file ]]; then echo "EXISTS"; fi"); if [[ $FILETEST = "EXISTS" ]]; then scp user@remote.host.com:/path/to/file .; fi
&gt; ... also, don't ever use this. it has issues with the responses and it probably can't handle a whole lot of usage. Request granted. 
 find sourcedir -type f | cpio -vmdp destdir
Copy the directory structure with `rsync` or `cp`, and then run `find &lt;directory&gt; -type d -exec rmdir --ignore-fail-on-non-empty {} \;` to remove any empty directories? Or `rsync -am &lt;sourcedir&gt; &lt;destdir&gt;`
grep -c &lt;pattern&gt; &lt;filename&gt; which will filter for the word that you prompted the user for (plus the word "restarted" or the date or whatever) and count the occurances. 
Another command that will be helpful for you is read -p ["Prompt Text: "] &lt;var&gt; which gets a string from the user (with a prompt) and stores it as a variable. You should look at this link for more tips: http://stackoverflow.com/questions/4651437/how-to-set-a-bash-variable-equal-to-the-output-from-a-command You've got it from here. The real bitch is probably going to be getting the syntax of the pattern you want to match correct for your grep command.
The tools I'd you would need to complete this exercise would be: bash arrays, while loop, something to column the data (awk, cut, read, etc) The simplest way I could think of doing it would be to parse the log for the search term that matches a service restart, awk out the date's for each - store the sort/uniq'd list in an array. For each value in the array, do a grep with the date on the logfile, pipe into grep -c for the restart searchterm, echo the date value in the array and then run another grep and awk/cut out the date/time field. There are tidier methods, etc - but the above should be enough to point you on the right direction whilst leading you to actually write the code and without having to go too deep into any one program. You should find simple examples you can modify when googling for how to do any of the above.
This is a fairly easy task to do with `awk`. It's pretty efficient at this type of parsing, and you won't really need to involve any other tools. http://www.gnu.org/software/gawk/manual/gawk.html
Thanks for the help guys! 
`openssl` has several. $ printf hello | openssl md5 (stdin)= 5d41402abc4b2a76b9719d911017c592 $ printf hello | openssl sha1 (stdin)= aaf4c61ddcc5e8a2dabede0f3b482cd9aea9434d See the manual for a full list. While we're at it, encoding and decoding base64 is occasionally useful $ printf 'hello\n' | openssl base64 aGVsbG8K $ printf 'aGVsbG8K\n' | openssl base64 -d hello
Thanks for all your help but I realized that I was SSHed into another Unix server that has what I was looking for. I feel like such a fool. Thanks!
Just as curiosity, I would definitely need to use a While loop? Or is there another way without anything too crazy. 
sha256 is the current standard for cryptographically secure hashing. sha3 is good too, but if your machine cannot calculate the hashes fast enough, it is not good. 
echo logs | grep -E 'restarted [0-9]+' | cut -d -f1 " " { | while read x; do (( total = total + x ));done; echo "$total" } Edit: sorry, i just realized you didnt provide sample output of your syslogd command now and just gave us some random strings... feel free to modify the command above or give us more information.
I've actually got a similar project that seems to be a bit further if you want to take a look at my codes for my strategies https://github.com/IDSninja/CenchOS/blob/master/install.sh
the only time bash variables aren't remembered anymore is when they are sent to a new sub shell (I found this happens with while loops). to make them use the same subshell you literally just enclose the statements with {}
[ "$x" -gt 10000 -a "$x" -lt 12500 ] &amp;&amp; echo it is 
They're in the [POSIX shell standard](http://pubs.opengroup.org/onlinepubs/009695399/utilities/xcu_chap02.html). It would be a really bad decision for the bash developers to break over 40 years of backwards compatibility.
OK thanks, damn shellcheck... I really prefer the backticks 
there is no difference functionally in simple cases, but $() can be stacked with ease with no escaping, while \` \` require picket fencing \\\\\\\\\ to do the same which makes them nigh unreadable and look like utter shit. Backticks suck
ya man I'm not saying it's a bad tool, sorry if I gave that impression. it found some legit errors for me. 
it's not only easier to read but also easier to nest, I'd recommend to switch over to $() remember you don't only code for yourself, but for everyone else who might interact with your code.
I didn't know shellcheck! Thanks for the address!
I think this is what you need... you can always use $ sudo su this will be pretty much the same thing as $ su except you will type your user's password instead of root's 
I'd also recommend you switch over to $(). Backticks are only really relevant if you're using the original bourne shell, so it's circa late 70's behaviour. It doesn't take long to switch either. In no time you'll be coming across scripts with backticks and thinking "What year is it?!" Deprecated is probably too strong a word, "obsolete" perhaps is more appropriate.
You might also want to read the [POSIX rationale](http://pubs.opengroup.org/onlinepubs/9699919799/xrat/V4_xcu_chap02.html#tag_23_02_06_03) for including the `$()` syntax.
When using backticks i typically enclose them with double quotes, should I continue to do that for the same affect with $()?
OK so I got this but im really tired. Let me know if you have any questions: logfile='/var/log/messages' grep -E 'syslogd:\s+restart' "$logfile" | grep -Eo '^[a-zA-Z]{3}\s+[0-9]{1,2}' | uniq | while read day do dayCount="$(grep -co "$day" "$logfile")" echo "The syslogd service restarted $dayCount times on $day, the times are:" grep -F "$day" "$logfile" | grep -Eo '[0-9]{2}:[0-9]{2}:[0-9]{2}' done Example output The syslogd service restarted 1 times on Nov 30, the times are: 22:00:03 The syslogd service restarted 3 times on Dec 1, the times are: 07:00:05 21:00:02 22:00:05 note: This puts each time on a new line which could make a lot of scrolling if there's a lot of restarts 
I prefer while loops but a for loop could probably do it too. With bash theres' usually a few options to chose from. I found that while loops preserve crazy special characters in file names better than for loops and never looked back, but the thing is while loops are piped so that means a new subshell which means variables inside of it will not be declared outside of it unless you do some special syntax like enclosing with {} I am unsure of how to do this one without some kind of loop, but i would think maybe with awk? But id say awk is more complicated than a loop
Yes, the result of and unquoted command substitution is subjected to word splitting and pathname expansion with both of the syntaxes, so do quote it.
Don't put extensions on scripts. Especially not `.sh` when the script isn't even an sh script.
&gt;Don't put extensions on scripts. Why?
&gt; The shebang command must be the first line of the file and can contain any valid path for the interpreter, followed by the arguments that will be sent with the invocation. Some shells won't interpret multiple arguments after the shebang, however.
For loops are for arguments; while loops for reading lines. http://mywiki.wooledge.org/DontReadLinesWithFor http://mywiki.wooledge.org/BashFAQ/001
&gt; &gt; Don't put extensions on scripts. &gt; Why? Same reason you don't put extensions on other commands. On linux systems, the command to list files is usually an ELF binary; but it's still named `ls`, not `ls.elf`, because the user running doesn't care or doesn't need to know what language was used to implement the command. And what if you wrote a command, using sh, and named it `foo.sh`, but later realized it should rather be written in perl, then you have a problem. Do you keep the name as is, having now a perl script with a misleading `.sh` extension? or do you rename it to `foo.pl` and break everything that were relying on `foo.sh`? See also http://www.talisman.org/~erlkonig/documents/commandname-extensions-considered-harmful
&gt; Don't put extensions on scripts. Bullshit. If there is a suffix or not doesn't matter for the environment, it only matters for you if you need to find every *zsh or *bash-script on your system or in a specific folder. If you're working with multiple shells you'll need that. If you only use one it doesn't matter. What if you have bash and python scripts? If you want to search and list only your python-pieces you'd need to go into every file and grep the first line to see if it uses `#!/usr/bin/env python` or `#!/usr/bin/env bash`. So, instead of `find -type f \*py` you'll need to use grep (slooow) or silver searcher/ack (and even these will be slower since you need to access the file every time - not only the filename). &gt; Especially not `.sh` when the script isn't even an sh script. That, on the other hand, is correct.
&gt; And what if you wrote a command, using sh, and named it foo.sh, but later realized it should rather be written in perl, then you have a problem. Do you keep the name as is, having now a perl script with a misleading .sh extension? or do you rename it to foo.pl and break everything that were relying on foo.sh? That is a lack of planning and not a reason for bad practice.
What part of «No i do not want it done for me.» was unclear? Though it's a very inefficient way of doing it and not very robust.
I don't think that every decision to rewrite a script can be called "lack of planning."
Thanks! I did actually remove .sh from my first public script but added it on later ones to avoid confusion. I'm interested in adhering to good coding practices/standards so I'll read the link you provided.
If you just started and then got to the point that it should be written in another language (e.g. because it is simpler or provides packages that do something for you) it doesn't matter, since the script shouldn't be used already (since it is not finished). If you need to add something new much later, that needs a rewrite of the script in another language then it is a lack of planning that you can't easily change the calls to the script.
Googles reasoning is, that the system doesn't need the suffix - which is correct (as I said [here](https://www.reddit.com/r/bash/comments/2o19g4/using_in_your_scripts/cmitqz3)). But imho the searchability and the information of what exactly the file is, is more important than not having to change the calls for this particular script. If it is a particular system-dependent script being called by numerous programs you should consider using symlinks anyway. Edit: Additionally, Google's shell scripting standard only allows bash-scripts, which makes the suffixes useless for google.
Good point, that makes sense for searchability. I think there's enough discussion here to help future readers make an informed decision for what will work best in their environment. 
&gt; If you need to add something new much later, that needs a rewrite of the script in another language then it is a lack of planning that you can't easily change the calls to the script. That may be the case in whatever utopian world you live in, but in the real world needs change over time and a script may evolve in ways that could never have been foreseen and that may require reimplementation in a suitable language.
And thats why its a lack of planning when you can't easily change the callings of said script. Or - as said earlier, its a lack of planning because such a critical script isn't referenced with symlinks. It happens, yes, no doubt - but that doesn't mean that its good practice.
On a side note: Check out the `file` command on Linux. You don't have to do all the madness grepping for shebang lines.
I keep a list of links without extensions that point to the files (which have them). Whenever I create a new script or rename an old one, I simply run a script which updates the list of links. Yes, it's another step, but it means I can type (and execute) either "script" or "script.sh" and all other scripts call the extension-less script name so that I can change from script.pl to script.sh to script.py without any difficulties.
it seemed like you weren't getting the answers and I figured I'd throw something together that you can learn off of. if you really don't want it done for you, rewrite it. my code isn't perfect and there are more than one way to do it EDIT: Now realized you aren't OP; please see OP's thank you
ya ya I guess I'll work on converting the scripts out of backticks
Quick notes: You can use `read -p` to print a prompt instead of echoing it separately. You might consider seeing if the file already exists, and reporting an error if so. You don't really have to do touch and echo in two steps. And, in fact, for style points, I'd rather use a heredoc: cat &gt;"$SCRIPT" &lt;&lt;__END_HEADER #!/bin/bash __END_HEADER Use [[ instead of [. Finally, probably use "$EDITOR", the standard variable for the user's preferred editor, instead of calling vi directly.
Thanks for the response! Just played around with read and understand what you mean... read -p "What's your name? &gt; " NAME Still getting the hang of heredocs. I knew this was an alternate way to do this just wanted to try getting it running with something basic.... In terms of using the $EDITOR variable... is there a way to test for a variable directly, or would I have to test for its contents and if $EDITOR == "" (none set) then use vi instead? Also... why '[[' instead of '[' ? Haven't gotten that far in the guide I'm currently using, assuming it has to do with expansion? 
Came here to make the same recommendation about using $EDITOR. But it looks good, good work! This is something I could use :) EDIT I'm not the original commentor, but you're right on with testing for $EDITOR. There are quite a few difference between the '[' and '[[' test. Most notably is that '[[' has different (better) parsing rules but is a little less portable. e.g. $ [ a &lt; b ] -bash: b: No such file or directory $ [[ a &lt; b ]] The first example returns an error because Bash tries to redirect the file b to the command [ a ] (see File Redirection for additional explanation). The second example actually does what we expect it to, alphabetical comparision. The character &lt; no longer has its typical meaning of a File Redirection operator when it's used with the extended test [[ keyword. [source](http://mywiki.wooledge.org/BashGuide/CommandsAndArguments) Here's a really short and sweet bashFAQ on the topic: [What is the difference between test, \[ and \[\[ ?](http://mywiki.wooledge.org/BashFAQ/031)
Awesome, thanks for that link on the differences, will be reading through that tonight!
BashFAQ is the best site for gotchas and good practices! In addition, it sprinkles cool trivia and historical developments on top;)
That was exactly what i was looking for! All i was missing essentially was proper while implementation THANK YOU SO MUCH! 
Very nice.
&gt; In terms of using the $EDITOR variable... is there a way to test for a variable directly, or would I have to test for its contents and if $EDITOR == "" (none set) then use vi instead? You can use `${EDITOR:-vi}`. It will use the value of `$EDITOR` if it is set and not null, otherwise it will use `vi`. [Bash Parameter Expansion](http://wiki.bash-hackers.org/syntax/pe) 
no problem, I've been there before
I agree with the other comments... Why are you using this: $(touch "$SCRIPT"; chmod +x "$SCRIPT") Instead of, for example, this? touch "$SCRIPT" &amp;&amp; chmod +x "$SCRIPT" 
Because that was the only way I had learned it from the guides to far... I did know about &amp;&amp; though but probably wanted to try something I had just learned. Is there a reason you would use &amp;&amp; over $() in this case?
[Great site to help you check your script for possible issues. IT also offers suggestions](http://www.shellcheck.net/)
THE POWER IS IN YOUR HANDS! It feels awesome, eh? :)
ya sorry I was tired and just threw it together. I don't think he copy and pasted it anyways.
I've no idea. Why not test it? Also if you're allowed to use bash, maybe consider scp?
$() is equivalent to backticks which are used for command substituion: So, this: echo "10" Gives the same output as: echo $(echo 5 + 5 | bc) Stuff inside the $() is evaluated first and replaces the evaluated, bracketed command string. These are exactly the same: $(echo 5 + 5 | bc) = $((5 + 5)) = $(seq 1 10 | wc -l) = `echo 10` = 10 In your case, you weren't interested in the output of the touch and chmod commands so the program worked as you expected in that it created and chmodded the file you wanted it to... /u/thestoicattack is absolutely correct when he/she says that the &amp;&amp; is extraneous. In this case it's unlikely that the touch command will fail. I always tend to use boolean &amp;&amp; (logical AND) or || (logical OR) as they're embedded in my fingertips... 
If they are going for a speed only contest, I would recommend against using encrypted protocols... I do like the idea of testing it though! There is a cool python script called speedometer https://excess.org/speedometer/ https://github.com/wardi/speedometer You just start the data transfer (in the background?) and then run speedometer and it will tell you how fast data is being transferred (with graphs). It works on a headless server over ssh no probz. Make large data files with dd/fallocate http://www.cyberciti.biz/faq/howto-create-lage-files-with-dd-command/
Before you dive too deep into your own implementation, you may want to read [this](http://intermediatesql.com/linux/scrap-the-scp-how-to-copy-data-fast-using-pigz-and-nc/).
You need a semicolon after the return, before the curly brace. EDIT: or a newline. In fact, that's exactly what the error message says. Definitely the form with the braces is better than the &amp;&amp; since there's not really a logical connection between the result of the `echo` and the fact that you want to return. EDIT2: unless you need strict POSIX-compatibility for some reason, prefer [[ to [.
Since you mentioned protocols, could SPDY be of help? I realize that since it's just 1 file the difference may not be that great.
Wget all the things
I can't find anything specific about how %b should handle precision in the docs, but I'm pretty sure the intention is that %b should behave the same as %s, with the only exception being the handling of backslash escapes. Chet will very likely want to fix this, so I suggest you report it as a bug (in launchpad or directly to the upstream bug-bash list). Oh and I've confirmed that the bug is still present in the devel branch.
[Bug #1399087](https://bugs.launchpad.net/ubuntu/+source/bash/+bug/1399087)
[RHash](http://rhash.anz.ru/) supports many. It's in the Ubuntu repos and probably others.
The description of the assignment is very vague, so there is not much to go on, but here is a loop that [reads a file line by line](http://mywiki.wooledge.org/BashFAQ/001) and does something different when a line starts with a #: while IFS= read -r line; do if [[ $line = '#'* ]]; then printf 'Line is a comment: %s\n' "$line" else printf 'Line is not a comment: %s\n' "$line" fi done &lt; file
Sorry about that. It's something along the lines of: #comment1 #comment2 #comment3 cat &gt; script1 # create file if [[ $1 = [a-z] ]] # see if first paramter is a-z then echo foo # if it is a-z, echo foo elif [[ $1 = [0-9] ]] # if it is 0-9 then echo bar #echo bar else echo baz #otherwise echo baz fi Where i need to write a script that will only format lines above where if the line starts with #, don't do anything to that line. Any other line, format the comments to be aligned (so the # are all aligned on every line that doesn't start with #). I'm confused even where to start on how to make it so nothing happens on the lines that start with #. Any general hints would be great, not expecting or wanting answers since I figure it's best to learn that way.
I find the scp result quite surprising, since it runs over TCP which is designed to "fill the pipe". Perhaps something is limiting the amount of network resource a single application can take, or other network processes were in use when running the experiment, but nothing in the protocols themselves would limit the network capacity used? Would be interesting to see how `nc domain.com port_number &lt; my_file` without compression compares to the other methods used. What upper layer protocol does it use? What improved the transfer speed the most in his experiments was compression, however this might not be an option for me since we might be using a video file.
Hey guys, thanks for all the input! I've made some changes to the script and am working on getting the script to automatically rename with a .sh extension if you haven't already (which still aren't working correctly). Here's my updates so far, it's a work in progress! #!/bin/bash #Promp user for name and store as a variable read -p "What is your name? &gt; " AUTHOR # Promp user for name of script and store as a variable read -p "What would you like your script to be named? &gt; " name # Prompt user for description and store as a variable read -p "Enter a short description of what your script does &gt; " DESCRIPTION # Create the file, make it executable touch "$name" &amp;&amp; chmod +x "$name" # Search to see if you named the file with the .sh extension, and if not rename your title to include .sh -- need to get this working correctly extension=$(echo $name | cut -d "." -f2) if [ $extension == "sh" ]; then break else mv "$name" "$name".sh fi # Add #/bin/bash to the beginning of the script, csript name, author, date created, and description cat &gt; "$name".sh &lt;&lt; _EOF_ #!/bin/bash # Script name: $name # Author: $AUTHOR # Created on: $(date +"%D") # Description: $DESCRIPTION _EOF_ # Use $EDITOR if it is set, if not set use vi if [[ $EDITOR != "" ]]; then $EDITOR "$name".sh else vi "$name".sh fi EDIT: a typo 
I like how you use it for output! +1 thanks for the info. in this case I was just trying to conserve lines. I guess I'll go ahead and convert to double brackets then. I thought single brackets would have been more portable to other shells 
Oh wow thanks man. I think I've ran into the escapes issue and if I switch to $() i might have to remove some \'s But anyways, here it goes! 
If you want to use bash then it sounds like you'd want to use a loop with mysql -u -p -e 'select username from user.database' This command will need credentials though. You could make a read only user and store them in plain text or I was thinking you could obfuscate and compile with shc edit: or just type the password every time the script runs
You'll definitely want to validate the entered data before passing it to bash in case there are any nasty commands. To be honest, I think Perl or Python are more suited to this type of thing but here is my (untested) code. #!/usr/bin/bash ## Database vars dbhost="" dbpass="" dbname="" dbuser="" dbquery="SELECT user, pass FROM newusers WHERE setup = 0;" mysql -h $dbhost -u$dbuser -p$dbpass $dbname -e "$dbquery" &gt; ~/list.txt IFS=$'\n' while IFS=$'\t' read user pass do useradd -m -g users -s /bin/bash -p $pass $user done &lt; ~/list.txt # optional rm ~/list.txt
You might want to look in to PAM (Pluggable Authentication Modules) and more specifically [pam mysql](http://pam-mysql.sourceforge.net/), you can set it up to read from you user data table and effectively create accounts on the fly when users login. This is a much more robust solution as it will make password changes and account removal much easier to manage if you have a database that will have significant uptime. If db uptime is a concern look in to puppet or chef for managing users and you can automate the creation and removal of users better than a shell script.
[Conditional Blocks (if, test and \[\[)](http://mywiki.wooledge.org/BashGuide/TestsAndConditionals#Conditional_Blocks_.28if.2C_test_and_.5B.5B.29) has more information on the differences between [ and [[. The differences are not simply a matter of style. BashGuide starts out by saying that "[[ is much like [, but it's special (a shell keyword), and it offers far more versatility" and proceeds to go into detail about how much better it is and demonstrates its points with examples. It is well worth reading. 
Thanks. I checked out the scrip, but as of now I can't understand much of it, because I am newbie, but it sure will be helpful in the future.
From a n00b perspective, it can be difficult to distinguish different '` marks. Especially on non-US keyboard layouts where ` is a combo. Not a real issue, but $() avoids all that.
The argument against using .sh or .pl or .py, etc. is that if you have scripts that are used by other programs and you later decide to rewrite them in a different language, like changing from sh to perl, you must either change all references to the script you named with .sh to .pl, or you must leave it alone. Either way, you have a problem. If you make the change, you risk missing one or two references in out-of-the-way places. And if you don't make the change to its file name, your file's name becomes a big fat lie. But if you leave off the suffix altogether, you avoid both problems and your other scripts will never know the difference. 
Exactly. for f in *; do if ! [[ $(file -b --mime-type "$f") =~ perl ]]; then continue fi ... done 
You could use: rsync -r --prune-empty-dirs src/ dst/ For brevity you can use `-m` instead of `--prune-empty-dirs`.
[If it is a particular system-dependent script being called by numerous programs you should consider using symlinks anyway.](http://www.reddit.com/r/bash/comments/2o19g4/using_in_your_scripts/cmiwjum)
How does using symlinks change a file's searchability or help identify what a file is (that's what the file command is all about, by the way)? What is your reasoning for using symlinks when a script is system-dependent, and what do you mean by system-dependent? Even if you use symlinks to the original script, if you change that script's implementation language and have it named with a suffix indicating that language, renaming it will break all of the symlinks, and not renaming it will mislead users about its content. Symlinks do not solve these issues. 
&gt; How does using symlinks change a file's searchability or help identify what a file is (that's what the file command is all about, by the way)? I.e. we have the script `do_serious_stuff.py`, which will be referenced by many other programs - this will be symlinked to `do_serious_stuff`. This way, we can add a new version `do_serious_stuff.0.1.py` (or even a version in another language `do_serious_stuff.0.1.pl`) without a problem and then force-symlink `do_serious_stuff` to the new file. Easy. And also - thats how its done with libraries and thats how its done in production systems and thats how its done with webservers and .... &gt; What is your reasoning for using symlinks when a script is system-dependent, and what do you mean by system-dependent? System-dependent is a script of which your particular system is dependent. &gt; Even if you use symlinks to the original script, if you change that script's implementation language and have it named with a suffix indicating that language, renaming it will break all of the symlinks, and not renaming it will mislead users about its content. Symlinks do not solve these issues. Wrong. As I said in the example above. Thats why we have symlinking in the first place. We have symlinking because the BSD guys were fed up with exchanging the real files all the time, so instead they created symlinks that can point to the correct version of the file.
lol, I thought the article was mocking me with that image before I realized I could scroll down.
I wrote a mock progress bar for CMD on Windows XP, bored at work one day. Meh.
http://lmgtfy.com/?q=Insert+a+literal+character+ 
Do you have a textbook resource or something? You have to be specific with your question. Is there a portion of the assignment you're having trouble with? Just break it down into steps and work your way from there. What commands can you run to list the logged in users? What command can you run to show disk usage? etc. Homework questions in the form of "do this assignment for me" are usually frowned upon.
Sorry if I gave the impression I wanted someone to do it for me. That'd be pointless since I still wouldn't understand it. I know the commands for displaying disk usage and who's logged in and importing them to a text file. What I don't understand is how to write the script in the shell. 
Go into your editor, and start with the shebang line: #!/bin/bash Then write your script after that. Once you're ready to run your script, give it execute permissions: chmod +x &lt;name of your file here&gt; Remember not to run this as root. Execute the script: ./&lt;name of your file here&gt; You can try it with a quick script if you'd like: #!/bin/bash echo "hi" If you can get that to run, erase the echo part and start working on your assignment. 
So you're going to need basic linux skills but all your script really needs is * a few print statements * $tool &gt;&gt; $log so for instance if I wanted to print the users logged on it'd be #!/bin/bash echo "--- USERS LOGGED IN ---" &gt;&gt; log.txt w &gt;&gt; log.txt all this does is adds the text users logged in and then sends the command output to a file, be careful you dont use "&gt;" and not "&gt;&gt;" as "&gt;" means replace "&gt;&gt;" means append as for the user id it's as simple as taking the users logged in text and changing it up, possibly doing something like echo "--- CURRENT USER ---" &gt;&gt; log.txt whoami &gt;&gt; log.txt there's shorter ways to do it in one line but if you're just starting out this will probably work best. user input is just as simple 
This is ruby not bash 
You want to insert a character code for a literal control character, but when you type it, it is intercepted and acted upon by your terminal. You can get around this by pressing C-q before entering the control character. * press and hold the key marked **control**, **Ctrl**, or something similar * press and release the **Q** key * press and release the **V** key * release the **control** key Depending on your terminal, you may have to use **C-v** instead of **C-q**. * press and hold the **control** key * press and release the **V** key * press and release the **V** key * release the **control** key If neither of these works, then read the manual for your terminal. If I have misunderstood your problem, then I apologize for that. I'm moving on now, so best of luck to you. :)
Well, if you don't know where to begin with, start at the beginning! http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO.html And once you're done with the basics, if you're interested, you can go on to the advanced stuff: http://www.tldp.org/LDP/abs/html/ Tip: what you're looking for is redirecting (of the command output to a file). Also, look for what commands can give you what information.
I would personally use Autokey for this: https://code.google.com/p/autokey/
When I'm starting a new script from scratch, especially one that does a lot, It normally helps to just start writing. Just pick a task, any task, and write it down. It doesn't necessarily have to be the first thing you need it to do. Then do another and another and before you know it, you have the whole script done. Perhaps start with what you already know how to do, and then figure out how to do what you don't know how to do. Google a lot. Use the command line to test your commands one line at a time if you have to and add it to the script when it works. Test as you go so you understand what works and what doesn't. You'll be done in no time and you will learn along the way.
-
Does it have to be bash? I could whip up a Perl script that would be better.
Update - Perl script below. Modules required are DBI and DBD-MySQL. Looked into this a bit further and useradd expects the password to be ran through the crypt function. Should be run with sudo permissions. #!/usr/bin/perl use warnings; use strict; use DBI; my $dbhost = ''; my $dbuser = ''; my $dbpass = ''; my $dbname = ''; my $dbh = DBI-&gt;connect("DBI:mysql:$dbname:$dbhost", $dbuser, $dbpass) or die "$DBI::errstr\n"; my $sql = "SELECT username, pwd FROM users"; my $sth = $dbh-&gt;prepare( $sql ) or die $dbh-&gt;errstr . "\n"; $sth-&gt;execute(); # Insert database rows into array and iterate through rows. while( my @data = $sth-&gt;fetchrow_array() ) { my $uid = $data[0]; my $pwd = crypt($data[1], "password"); unless (system "useradd -m -g users -s /bin/bash -p $pwd $uid") { # Return code was 0, meaning success. Print success message. print "$uid has been added to the system successfully\n"; } }
Neat! I try to use echo this way as much as possible, but there are some new ones here for me. How compatible are these? Are they POSIX?
Thanks everyone. Managed to get it to work
You should really look into rsync for incremental backups, both local and remote. It's a tool that does exactly what you're trying to do with many awesome extra features. Also, have you looked into Python as an alternative to shell script? Either language is capable of the task but I personally find reading and debugging Python scripts to be far less taxing, plus the language lends itself to many more platforms and uses. I'm sure there will be people who disagree on this point which is fair enough. 
It seems like this could be accomplished in a simpler fashion with cron. Also, I don't really know what that tar command is doing so I can't say this for sure that this applies to you, but rsync provides a nice and commonly used solution for incremental backups.
you could use bash =~ inside [[ ]] to test with regex, eg $ x=11 $ [[ $x =~ ^[0-9]+$ ]] &amp;&amp; echo digits digits $ [[ $x =~ ^[0-9]{2}$ ]] &amp;&amp; echo exactly 2 digits exactly 2 digits $ x=Ab $ [[ ${x,,} =~ ^[a-z]{2}$ ]] &amp;&amp; echo exactly 2 letters exactly 2 letters 
xdotool can emit keypresses. `xdotool key ctrl+v`
There are a number of things wrong in there, but the primary issue is your testing of $zip (that's the error you're receiving). Your tests are expecting numbers as input, even though you know that they could be numbers or letters. Instead of using greater than / less than (which will only work on numbers), you would be better off either looking into regular expressions and matching patterns that way instead, or using `case`to process matches (at least, I don't think `case` will throw an error when it gets an integer while looking for a letter). Anyhow, I re-wrote the script to test this, but I'm not going to give it to you -- I think you should work it out given that this is for a class. But, I will say that I tested the zip variable using: `[[ "$zip" =~ ^[0-9]+$ ]]` Good luck.
Keep at it -- we all started there. I learned best by grinding my teeth on stuff like this, but I learned "on the job" as a Tier 1 in a datacenter -- it was shit work for shit pay, but I had lots of resources available to me, and lots of little tasks that I could automate with bash scripts. Even though it gets frustrating, if you keep grinding through it on little projects like this, and before long you'll look back and realize that you can actually do some pretty amazing things.
Maybe look [here](http://www.tldp.org/LDP/abs/html/string-manipulation.html) under "Substring Replacement"?
What about tr ' ' '\ '
`tr`'s replacement only works with single characters
In the sed command, escape the backslash with another backslash. $ echo 'one two three .' | sed 's/ /\\ /g' one\ two\ \ three\ \ \ . 
`echo "test test" | sed s/' '/'\\ '/g` The `s` means we're doing substitution of the first expression by the second. The `g` tells it to replace all spaces, not just the first one. The backslash always needs to be escaped because it's used as an escape character itself.
There's really no problem with spaces or any other special character in filenames, as long as you're careful with your quotes.
I'm trying to write a script to cd into specific directories on my server, and download them. Only thing is, some files have spaces in them, and the server only recognizes them as "File\ Name". 
So this is a case of "[I want to do X, but I'm asking how to do Y](http://mywiki.wooledge.org/XyProblem)". You haven't yet specified the **X** sufficiently to answer it, but it's clear that prepending backslashes in front of spaces is not a solution. Explain **X** more detailed.
You just double quote the var...? "$DIR" can be "/home/vortextraz/My Puppy Pictures"..
I'm trying to write a script that will connect to a remote server, and then download any new files. To do so, I'm trying to user lftp for the segmented downloads. I current have the first part working, connecting to the server, however when I try to cd into a directory: $ cd files/something/open this folder it returns Usage: cd remote-dir From what I'm able to understand, when trying to cd into a directory that has a space in it's name, it requires a backslash just before the space, else it won't work. The part I'm having issues with is passing in the directory name, with the backslashes, as a string to the lftp script. Unfortunately, I'm unable to use the lftp mirror feature, as I'll be reorganizing all of the files once downloaded. Doing so will create duplicates.
Shaka, when the walls fell.
I'm not familiar with `vmctrl`, but I'm guessing it might take a PID as an argument. If you were to launch `mate-terminal` in the background you could then refer to its PID in the call to `vmctrl`, e.g. mate-terminal &amp; wmctrl -r :ACTIVE: -b toggle,above -p ${!} ... presuming `wmctrl` does take that type of input in a manner similar to my guess. 
I just finished configuring wmctrl for a script I use. In my case I run mplayer and want its window to be on a particular desktop. The problem I had was similar to yours. wmctrl was not targeting the right window. I discovered it was a timing issue. My script runs much faster than the window manager. It starts mplayer in the background and can then run wmctrl before the window manager has a chance to create mplayer's window, which causes wmctrl to fail to find the right window. Here's how I worked around the problem, using your code as an example: mate-terminal &amp; while ! wmctrl -l | grep -q mate-terminal; do continue done wmctrl -r :ACTIVE: -b toggle,above -p ${!} Essentially, the script enters a loop and stays there doing nothing until it detects the mate-terminal window. Then it exits the loop and wmctrl can properly detect the mate-terminal window. If you want this to work with multiple mate-terminal windows you can assign a window ID number to it when starting the terminal. This should be a command-line option for mate-terminal. Then when you run wmctrl include the -i switch. That will cause the argument to -r to be interpreted as a numeric window ID number, which will target the specific mate-terminal window you want to affect. 
Thank you for this. I tried it but with no luck. I will look further into this. Thank you for your help and time.
**EDIT:** I just had to do this to the code: mate-terminal --title="terminal" &amp; wmctrl -a terminal -b toggle,above But now for some reason every other terminal will be "always on top". I have to figure this out, but since I added the `--tab` option in the `mate-terminal` line I guess I can live with it for now. I just need to open up two to get the "always on top" if I need three I will then need to open a fourth because that third will disable the "always on top". If that makes any sense. Thank you fro your time. _____________________________ Thank you for this. I tried this and I get the same results I was getting before. I played with the code some and got it to work some what. If nothing else is opened I get my result, but with a flashing window. If other windows are opened and my mouse is over them then it is selected as the "always on top" option and they then proceed to flash wildly, ha ha ha. Thank you for your help and time.
The more proper way to do this though would be to use /tmp to store the path file. That's what it's there for. 
That references all the items in the array, as described here: http://www.tldp.org/LDP/Bash-Beginners-Guide/html/sect_10_02.html &gt; If the index number is @ or *, all members of an array are referenced. So the line ``echo "${_TIME_DIFFERENCE[@]}"`` would print out all the items in the array.
Is there any practical difference between @ and *?
Yes. `"${arr[@]}"` expands each element as a separate argument, while `"${arr[*]}"` expands to a single string containing all elements with the first character of IFS between the elements. $ arr=( "first one" "second one" ) $ printf '«%s»\n' "${arr[@]}" «first one» «second one» $ IFS=, $ printf '«%s»\n' "${arr[*]}" «first one,second one» 
Thanks! That's pretty useful, actually:)
I use extensions on the actual scripts because I have a wide variety of languages that I use (bash, python, js, etc.). Most are in their own directory under `~/scripts/[insert language]/`, but some are not (they are older than my "filing system" itself). Anything that I use a lot has a symlink in my `~/bin` folder without the extension. So if I change languages for the command (which I have done before) I only need to change what the symlink points to. I also have one that will check all of my symlinks to make sure they are not dead links (they never are). 
There is a lot of neat procedures on that page. As to running backgrounded jobs, why don't more people use screen? 
Screen, tmux or whatever seems better then the posted solution.
I updated my script, it's better now IMHO.
You're correct, I added better user management.
Oh good, you found the link in our sidebar! I was worried no one paid attention to the sidebar.
Thanks for looking. -- I use screen all the time, all day long, but I do not over do it, I used to have 30 screen windows open, now many less, and mainly when a given window is for a different host. It's nice to not tie up any interactive shell, even in screen. Also nice to get a 'beep' which can alert you in screen, when your bg job finishes.
The `make &amp;&amp; make install` was actually unintended, but feels good to know. About your other point, I have thought about it before, but assuming in the future that I have 100 lines of takes, it will be cumbersome to have 100 if/elseif statements. I was thinking about an approach that involves, where bash would trigger an error and halt exit if it encounters an error, this may be done by reading the `stderr` from `$TMPDIR/stderr` or wherever the error is kept, or show "sucess" if it can't find any errors, while it has completed that last task from the script. 
/u/dk45365 gives some good pointers, but in my opinion, running a command and explicitly checking its return status is a bit of an antipattern, unless you need the value itself for some reason. If you just want to check success/failure, I'd rewrite his example as echo "Updating YUM Packages" if yum -y update; then echo "yum update succeeded" else echo "yum update encountered an error" fi
The problem with this, as I just mentioned to /u/dk45365 is when the size of the script gets bigger, It becomes inevitable to use something reusable or efficient as opposed to endless control flow statements. I should have made this obvious in my questions though. sorry.
Someone will invariably suggest `set -e` for automatic error detection, but it has some problems: http://mywiki.wooledge.org/BashFAQ/105
Why not write yourself a little function? run_or_die() { if "$@"; then printf "%s suceeded\n" "$1" else printf "%s failed; exiting.\n" "$1" exit 1 fi } then run_or_die yum -y update run_or_die yum -y install [...] [...]
How hard is it to create a function that detects `stderr` for any value and exists the bash script? I could then my commands and this function line after line to check if there was an error with previous command, I could exit. In my noob mind, this somehow makes sense.
Thanks. This is too sweet specially to add other functions like progress bar. 
Ending an -exec predicate of find with + instead of \\; will cause the exec'ed command to use as many files as possible as arguments, instead of one invocation per file. So efficiency should not be a concern. You *could* do this, but don't: find . -type f -size +1000 -exec ls -lh {} + Overall I think that is a bad idea, since you *never* want to parse ls output. Never, ever. Maybe -exec stat -f "%N %z" {} + which would give you a standard format you can parse, but then you're right back at the same problem of converting a byte count to human readable. I did a quick google and there doesn't seem to be a standard utility, esp. not on OS X.
Try this: echo "Updating YUM Packages" if ! yum -y update; then echo "yum update failed" &gt;&amp;2 exit 1 fi echo "Installing required CentOS 7 components for Nginx." if ! yum -y install bzip2 gcc-c++ pcre-devel zlib-devel openssl-devel libxslt-devel; then echo "installing Nginx components failed" &gt;&amp;2 exit 1 fi echo "Downloading and extracting Nginx source file." cd /usr/local/src &amp;&amp; if ! wget -q0 nginx-1.6.2.tar.gz http://nginx.org/download/nginx-1.6.2.tar.gz &amp;&amp; tar -xzf nginx-1.6.2.tar.gz; then echo "Downloading and extracting Nginx source failed" &gt;&amp;2 exit 1 fi || { echo "Could not access /usr/local/src" &gt;&amp;2 exit 1 } echo "Configuring the Nginx build" if cd nginx-1.6.2; then ./configure --user=nginx --group=nginx --prefix=/usr/share/nginx \ --sbin-path=/usr/sbin/nginx --conf-path=/etc/nginx/nginx.conf \ --error-log-path=/var/log/nginx/error.log \ --http-log-path=/var/log/nginx/access.log \ --pid-path=/var/run/nginx.pid --lock-path=/var/run/nginx.lock \ --with-http_ssl_module --with-http_gzip_static_module --with-pcre else echo "Configuring Nginx failed" &gt;&amp;2 exit 1 fi echo "Running Make and Make install." if ! make &amp;&amp; make install; then echo "make &amp;&amp; make install failed" &gt;&amp;2 exit 1 fi echo "Adding user nginx." if ! useradd -r nginx; then echo "Adding user nginx failed" &gt;&amp;2 exit 1 fi 
I actually forgot that this does not show the custom message as in extracting folder ... doing yum update ... failed to create /var/foo dir ... it can only throw `suceeded` or `failed; exiting.` right? Because, I don't see anywhere there costume messages can be passed. 
Thanks. I will try this. although a bit verbose, it is very important to show each success/fail status for each task. 
Right, at the moment it doesn't do that. That would be a good thing for you to add!
:) fair enough.
How about using ansible (or puppet, or chef) for this?
I have a function called fail that basically echoes out a message and either exits or returns with error. I'm on my phone so I don't have the exact function, but this is close: fail() { echo $* 1&gt;&amp;2 exit 1 } And I use it as: Yum install nginx || fail "error installing nginx " Also included in fail is the name of the root script being called and the current script and line number, when sourced in, by using $BASH_SOURCE if I remember correctly. It is invaluable when you get to complex scripts. I also use set -u to fail on using uninitiated variables, and use ${var:-} to test if something is already set. Hope this helps and good luck!
Thanks! I just switched from MacPorts to HomeBrew because I felt it was time to learn a new one, not because I was unhappy with MacPorts. Actually, so far, in comparison, they are essentially the same, though I might go as far as to say MacPorts handles a lot of things better, though through added complexity. I would rather not use a different find that is part of some utility. I would be okay with using POSIX version that is the same as all the other Linux's out there. The differences in BSD's find to Linux's find is significant. Even moreso if you compare it to the Mac due to some of the file meta data such as resource forks/data forks. I will give the du -h a try just to learn and test a little. Thank you again.
Thanks! Before I comment any further, of all the mini commands and pipes and redirections I have used in the last 15 years or so, I have never understood, let alone knew where to search for an answer, in regards to what find is up to syntactically. `find` is simple, it's the parts after it, specifically after an -exec that I don't understand, and more specially the {} \; syntax, which I now see you can do totally different as you have, with a trailing wildcard. Can you break down your one liner and explain what is going on, and perhaps a reference to some docs on what is going on in general with post -exec and post xargs syntax.
I have a feeling I can learn and cobble together what I need from this. Thank you very much, I will report back when I have it all working just to let everyone know which syntax worked best under my use case and why. Thanks again.
You should check out _return_ for functions like the above.
This is a good answer but got downvoted for some reason. To add to this, assuming the reason you wrote a script for this is that you will need to do it multiple times or on multiple machines, maybe it makes sense to use config management (ansible, salt, etc..). But... why would you do this multiple times? Why not compile once and make an *rpm package*. Be lazy, make you life less complicated.
Looks like it's just [adware](http://www.thesafemac.com/arg-buca-apps/). Not sure why you're posing this in /r/bash though...
&gt; some docs `man find` is the obvious place to start, but like everything else, there's an excellent reference at the [bash wiki](http://mywiki.wooledge.org/UsingFind). Let's not worry about the + for a moment, and stick with the \\; that you know. For an -exec predicate, everything between -exec and \\; is turned into a command, and the {} is replaced with a single filename from the find and then run. So consider the current directory that has 3 files, foo bar and baz. $ find . ./foo ./bar ./baz Now if you run an -exec predicate, like `find . -exec stat -f "%N" {} \;`, it is exactly as if the following three commands are run: stat -f "%N" ./foo stat -f "%N" ./bar stat -f "%N" ./baz As you can see, it's a direct substitution of the filename as the argument where the {} was. Note that it's not a *textual* substitution; you don't have to be careful to quote or anything; files with spaces work correctly. `find` just subs in the entire filename when running the command and there's no weird shell stuff involved. Now, the reason you might want to use + is that many standard commands can work with a list of files, as you well know. Since find could generate a lot of files, someone had the exact same idea you have and realized it would be inefficient to start a new process for every single file, especially when multiple files can be used at once. So when you use +, you're saying "replace {} with a list of arguments, one for each filename, as many as possible. So `find . -exec stat -f "%N" {} +` will be exactly the same as running stat -f "%N" ./foo ./bar ./baz where the {} was replaced with as many found files as possible. xargs is a whole different kettle of fish. Basically, wherever you would use xargs, try to use find -exec instead to be safe. Normally xargs expects one filename per line, so if you have a file with a newline in its name, you're hosed. As mentioned, find -exec always does the right thing. If you *must* use xargs, be sure to do find -print0 | xargs -0 That will cause find to output filenames with a `'\0'` null character between them instead of a newline, and xargs to read its files using the null character as delimiter. This is the only safe way to handle all filenames, because null is the only character guaranteed not to be in a filename.
Yeah I found the differences between Macports and Homebrew to be pretty trivial. I moved to Homebrew a while ago because it feels like a much more active project and like that it *tries* to integrate more with OS X rather than set up its own isolated environment. Also you should try out [cask](http://caskroom.io/) which I started using recently, it's pretty awesome. As far as `find` goes, I had a similar experience with `sed`: the builtin one for OS X is so different and quirky that I finally got fed up with it and started using the one from Homebrew's coreutils and it has been loads better. YMMV, but I'd at least try installing `findutils` and try some commands with `gfind` (when Homebrew installs stuff like coreutils that have comparable executables in OS X, it'll precede the installed program with `g*` to differentiate the two.) 
I always source [stringent.sh](https://github.com/goozbach/stringent.sh) (think `use strict` for bash) on any script of importance. It's got a built-in error checking. Also I'd try to do an [on_exit handler](http://redsymbol.net/articles/bash-exit-traps/) .
Maybe try /r/techsupport
Just use mv -i
I like to approach, it seems good. But what does `$*` represent? 
You almost had me. But, then I realized that this is not OP's desired result. &gt; -i, --interactive &gt; prompt before overwrite I think he wants a prompt to confirm all moves.
It expands to all the parameters, that is everything after 'fail'. 
&gt; what I actually want to do is have the script target ANY file in there and refer to it in the script but I don't know the syntax to do that. How about this? Use file globbing in a loop. for file in "$1"/*; do stat "$file" read -p "move file: ${file}? (y/n) " resp if [[ "$resp" = "y" || "$resp" = "Y" ]]; then chmod -w "$file" mv "${file}" "$2" echo "file has been moved to $2 and is now read only" else echo "File (${file}) has been skipped" 1&gt;&amp;2 fi done * Quote your variables. * Make it read-only prior to moving it so we don't need to determine the new filename. * Take a single character as input. (Y/y) * I'm not sure what that `stat` is for, but I left it. * You will probably want to test $1 and make sure it's not null. That could be a problem, especially if you ran this as root, and ended up moving /bin/* instead of /myinput/bin/* or something.
If you use -exec, make sure to wrap {} with "" otherwise bad things could happen with your exec command. Good luck!
 for f in "$1"/*; do stat "$f" read -n 1 -p "move $f [y/n] " case $REPLY in [yY]) mv "$f" "$2" chmod -w "$2/${f##*/}" ;; [nN]) echo "${f##*/} has been skipped" ;; *) echo "invalid response: $REPLY. Skipping ${f##*/}" ;; esac done 
Thanks for the help your code works great, exactly what I wanted. I will heed your advice also. could you explain what &gt;1&gt;&amp;2 is? 
1 is stout and 2 is sterr. You are redirecting stout from the command to sterr. Not sure why.. normally it is 2&gt;&amp;1 which would redirect sterr to stout so it prints.
&gt; Quote your variables. This is very important, but you don't need the braces unless you're doing PE. &gt; Make it read-only prior to moving it so we don't need to determine the new filename. can you explain this? it doesn't seem to affect it when overwriting or anything. &gt; Take a single character as input. (Y/y) I think you need `-n1` for that. &gt; I'm not sure what that stat is for, but I left it. It seems a way to get some idea of what file you're moving but seems overkill, `ls -l` would suffice I guess. &gt; You will probably want to test $1 and make sure it's not null. That could be a problem, especially if you ran this as root, and ended up moving /bin/* instead of /myinput/bin/* or something. indeed, there's many ways to do that my favorite is: : "${1:?'Missing "source" argument'}" "${2:?'Missing "dest" argument'}" 
Woah, thanks. This is definitely the kind of thing I needed to know. Could you help me out with understanding your sed usage? First off, is there any difference between delimiting with underscores vs backslashes? In other words is sed -e 's_\\_\\\\_g' any different from sed -e 's/\\/\\\\/g' I assume they are different but how so? Also what does $ signify in sed? And what do the backslashes after each sed call do? String them together? Sorry for all the questions. And thanks so much for your help. Hadn't thought about these things before and they're super cool to learn! P.S. your gist uploader seems great! This took me a good amount of time to do to be honest, so it's really cool that you were able to do it so fast!
Can you give an example? Braces don't generally need to be escaped in bash, and I've never seen a command messed up by them.
I sometimes use this as a template (to remind me how the trap command works). I can't take credit, shamelessly stolen from the internets some time ago. #!/bin/bash script_dir="$( cd "$(dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd )" function error_handler() { echo "" echo "**************************************" echo "Error occured in script at line: ${1}." echo "Line exited with status: ${2}" echo "**************************************" echo "" } trap 'error_handler ${LINENO} $?' ERR set -o errexit set -o errtrace set -o nounset echo "I'm good" test 1 -eq 0 echo "Won't reach here"
Personally, I am always a fan of playful (See: vulgar) aliases. My personal favorite is: alias fuck='sudo !!' 
Did anybody else freak out because of the "snow"? I had no idea what it was at first. It is very annoying.
I definitely thought people stopped using those kinds of effects when we entered the 21st century.
The process of tarring/gzipping and then using rsync is not a very good one. Instead of only backing up and transmitting deltas you will instead backup and transmit the entire contents of your home directory, every time. IMO it would make more sense to skip the tar/gzip and just rsync your home directory to the remote server, or look into using the -g/-G flags of tar to create incremental tar files. Or better yet, just use tarsnap :p
Small points: Use [[ instead of [. The whole thing is wrapped in a subshell for no obvious reason; if you only want grouping and don't need a separate subshell, just use { and } instead. For more philosophical objections, see /u/er0k's comment.
Add: cd / then have ./home. That way can restore in any directory
&gt; echo "Subject: Backup Complete" | sendmail somedude@gmail.com &lt; backup.log Here you give `sendmail` two separate inputs (once via pipe and once via redirection). Only one will win and it's the `&lt; backup.log`; the subject line is lost. Put both inputs on the left hand side of the pipe: { printf 'Subject: Backup Complete\n' cat backup.log } | sendmail somdude@gmail.com That said, I'd recommend using `mailx` or `mutt` or similar instead of `sendmail`.
I would send an email to admin only in case of error. I start to ignore daily emails after some time. You dont want to automatically mark email as read when backup fails. 
Good catch. I didn't even pay attention to that. In addition, I'd recommend attaching the log to the message. At least that's how I prefer it. That way the client can't fool around with formatting. uuencode backup.log backup.log | mailx -s 'Backup Complete' somedude@gmail.com
&gt; This is very important, but you don't need the braces unless you're doing PE. Mostly, I did braces where the var wasn't separated by whitespace. There was a reason I thought of that I did it in the mv command... Now I can't remember. Maybe I did it in case the file name was like "-option something" so it wouldn't try to add "-option" to the mv command? &gt; can you explain this? it doesn't seem to affect it when overwriting or anything. His script had the chmod after the mv. I didn't want to have to determine the new destination filename so I just did it before. &gt; I think you need -n1 for that. Good catch! How embarrassing! I can't believe I left that out. Cheers! Merry Christmas, if you observe.
You could use a named pipe. Something like: test: fifo tee -a logfile &lt; "$&lt;" &amp; ./testcmd &gt; "$&lt;" 2&gt;&amp;1 fifo: mkfifo "$@" clean: rm -f fifo Or tell make to use bash (or ksh), so you can use a [process substitution](http://mywiki.wooledge.org/ProcessSubstitution) instead of a pipe.
I think that what you are looking for is the bash -o pipefail builtin/option. I believe if you start your script with: set -o pipefail non-zero exit codes are passed all the way through the pipe. You could set it then unset it before and after the command if you only want it there. edit: man bash /Pipelines
That may also suffice, but requires that you tell make to use bash instead of sh as the shell to parse the commands with.
It's in r/bash so I thought that was a given.
Would it work with: Make &amp; tail -f loggfile ?
Could you make use of the $PIPESTATUS array variable? http://tldp.org/LDP/abs/html/internalvariables.html
To expand on what /u/WhiteLab commented, see the man page: PIPESTATUS An array variable (see Arrays below) containing a list of exit status values from the processes in the most-recently-executed foreground pipeline (which may contain only a single command). Therefore, add a test like this in your Makefile: bla: cmd | tee file; test ${PIPESTATUS[0]} = 0 and make will sense cmd's exit status instead of tee's.
What is the error? Edit: why is your shebang using sh if you want to use bash features like array variables? Edit edit: the line you reference is not in your script. This is a very confusing question.
You can't possibly expect the same outcome. The way your script reads (apt-get, putting things in ~/), it seems that you're running this on the Pi.
Hi CaptPikel, That actually didn't work. But I think I kind of understand your logic there. Would the (([^,] portion be to take everything after the first ( excluding the , ? Just not sure about the \+\, portion. I do understand that the ( ) is a grouping and you're accessing that by using \1/ I'm getting the entire string back using your command, include the comma after dell.
Sorry to keep bothering, but do you know where I can get more information about groupings? I was trying to modify the above line of code to instead work with parenthesis (I assume it would work the same exact way) I tried: sed 's/\(\(\)\(.*\)\(,\)\(.*\)\(\)\)/\2\4/' Essentially replacing the first " with ( and the last " with ). Instead all that is returned is a comma. If I cat out each portion \1/ \2/ etc. etc. it clearly isn't breaking apart the string pieces correctly. My assumption is because sed may get confused when you use a parenthesis within a grouping (\(\) - I also tried just \(()\. Thanks for any info! 
Here is my solution using awk, with simple substitution First put that string into memory: $ echo $string John Doe, model (dell, 24-inch), 847483992, 1 infinity loop bash-3.2$ echo $string | awk '{ gsub(/dell,/, "dell"); print }' John Doe, model (dell 24-inch), 847483992, 1 infinity loop Here is my sed example, also using simple substitution of the second comma: bash-3.2$ echo $string | sed 's/,//2' John Doe, model (dell 24-inch), 847483992, 1 infinity loop On OS X 10.10.1 here. So, what are you up to in Cupertino?
I would probably write some logic to organize your data sets first. Evaluate each line, and when you find something out of place, like the commas, then edit it. Lines that have no issue just redirect output the new csv file as well. However, there are many ways to do this. I would first build some sort of logic in code that evaluated each line and depending on the evaluation it would take some sort action and then output it to the new CSV file. Especially if you are dealing with all different sorts of data sets. 
That is pretty much what I am suggesting. Loop through each line of the CSV, if the data looks good output it to a clean file, if not have a logic check to look for specific things you want to change. If it is always data that contains a comma inside parenthesis, you can build logic in the code to do so. Then change the string and output it as a new line to the clean csv file.
You could look at Python or Perl which would probably have more options than bash. The sed wizardry is mainly regex I think, I can look into it later when I get more time, but I am about to hop off the computer for a while.
I'm on my tablet or I'd put a solution together for you but you have a couple ways of approaching this using sed. You can either use pattern matching as a substitute for grep , for example sed '/pattern/s/old/new/g’ which will only make replacements on lines that match the pattern or just use [pattern matching] (http://www.grymoire.com/Unix/Sed.html#uh-4). More or less you'd search for a pattern that would be 1,2,3 and just replace that with 1,3. I've used both approaches with success but be warned that sed and awk are amazing tools and you run the risk of spending far too much time learning 'cool' tricks with them. My wife didn't see the point of getting excited about sed buffers... I tried to explain that it makes lspci -vv so much easier to get the particular thing you want and it's relevant paragraph but she was walking away by the time I got to 's'. Good luck and let me know if you need further guidance.
Terribly sorry, forgot I made a new commit. Edited original post with correct link on earlier commit.
What error?
Replacing the "+" quantifier with "*" will work on Mac &amp; Linux: echo "John Doe, model (dell, 24-inch), 847483992, 1 infinity loop" | sed 's/\(([^,]*\),/\1/' Or you can use the "-E" option to use extended regular expressions: echo "John Doe, model (dell, 24-inch), 847483992, 1 infinity loop" | sed -E 's/(\([^,]+),/\1/' 
I quickyl set up a VM with Debian and tested it out: this is what I get when using the bash shebang instead of the sh shebang: http://inft.ly/Gxu3eeH
Hey js3kgt, Thanks for the explanation! I just realized that in some cases, the first value that comes in COULD have parenthesis as well, so John (Doe), model (dell, 24-inch). I end up with John (Doe)model (dell 24-inch) in those cases. Is there an easy way to specify only the second set of parenthesis? I tried a /2 at the end but that's not working
You could put some type of matching on the regular expression for the capture group. echo "John (Doe), model (dell, 24-inch), 847483992, 1 infinity loop" | sed 's:\((dell\)\(,\)\(.*)\):\1\3:g' This will force sed to only act on the **(dell**, 24-inch) parenthesis. It sounds like this could be getting complicated. You have to ask yourself, how many records are there and how many commas are in the wrong place? If it's a small dataset, ( 10,000 or less ) and your only doing the data cleanup once on 50 lines, I would just use sed to remove the data that's invalid but the same. No sense going the algorithmic route if it's only needed a few times. sed 's/model dell,/model dell/g' You could also remove all parenthesis after that is done to clean up the data. sed -e 's/(//g' -e 's/)//g'
Awesome! Thanks. I used a slightly modified version of your echo "John (Doe), model (dell, 24-inch), 847483992, 1 infinity loop" | sed 's:\((dell\)\(,\)\(.*)\):\1\3:g' I had two or three different variations of formatting so I accounted for those as well. 
Or you could just get in the habit of typing find . -name "*.ext" which works on both and conforms to the POSIX standard, unlike the GNU find shorthand. 
`/usr/bin/tput`
I second /u/McDutchie. Just add the bloody dot. Case closed. On a side note, don't use an alias that runs a function; that's a bit redundant. You only need a function. Use the `command` builtin to avoid calling the function recursively. find() { ... command find . "$@" ... }
I'm glad this works for you, but I don't really see the value. This seems like a lot of cruft to avoid having to type, essentially, `history | grep foo`, and creates a lot of clumsy ambiguous functions. It's good that you are learning basic scripting though, but in time you'll probably figure out it is simply easier to remember / learn the "core" utilities on the command line, and avoid the need for hundreds of functions. `&lt;esc&gt;` will break you from "history review mode", if `C-g` is too difficult, at least in vi mode. `C-r` will do a reverse search, if you get used to it, and pressing it again will repeat your search.
My preferred method is to add these to /etc/inputrc or ~/.inputrc "\e[5~": history-search-backward "\e[6~": history-search-forward Type a couple of characters from the start of the command you're looking for, hit pgup to search upwards through your history. Use \^r if you need to search the middle of a command.
This is because bash does not enable aliases in scripts (while sh does). There's no point in having aliases in scripts anyway, since functions work, and functions are superior to aliases. reset() { tput sgr0; } In this case though, it makes more sense to store the output of tput in a variable and use that variable throughout instead. reset=$(tput sgr0) black=$(tput setaf 0) red=$(tput setaf 1) etc...
Got that to work but I think the color variables are wrong now: http://inft.ly/rUANB3N
Yes, this is because `echo` behaves differently between different (posix compliant) shells. In bash, `echo` does not replace backslash escapes (without an option). Some shells' `echo` does, some don't. Both Chet (the maintainer of bash), and POSIX recommend using `printf` rather than `echo` because of this. However, if you use tput instead of hard-coding the escapes, that won't be a problem. Anyway, here's an example using `tput` and `printf`: reset=$(tput sgr0) black=$(tput setaf 0) red=$(tput setaf 1) green=$(tput setaf 2) printf '%s%s%s %s%s%s\n' "$red" "some red text here" "$reset" \ "$green" "and some green here" "$reset"
The script needs this before the alias shopt -s expand_aliases And it might be worth using the full path to the command. Or you could create a function, something like function reset { /usr/bin/tput sgr0 }
I could probably get away with it, but it seems useless to write a script like that. Is there a way around this problem?
Thanks for the .inputrc suggestion! I frequently search back through the history for very recent commands with up and down arrow. However I found myself sometimes going back through the history, and scrolling through a lot of repeats, and it taking me too much time to find just the right command. For whatever reason, ctrl+r never felt natural to me. I've tried but just couldn't integrate it into my workflow. 
You may want to read over [this discussion of lock files in bash at Bash Hackers Wiki](http://wiki.bash-hackers.org/howto/mutex). It reveals some problems with your implementation of lock files and provides a cleaner solution. 
Nvm forgot fi in the end
Thanks for pointing this out. I'll update my library and blog post to deal with this issue. 
I've updated the library and blog if you'd like to take a look. Thanks for your feedback!
Or just use seq's formatting option to pad the number with zeros &amp; do it all in one loop with no if statement. for i in $(seq -f "%02g" 0 21) do wget "www.doesntmatterwhatwebsiteis.com/0$i.jpg" done
Yeah that would clearly be the best solution. 
That looks better; though I'm not sure if noclobber is guaranteed to be atomic on all systems. I generally prefer using mkdir to create locks. if mkdir "$lock_dir/$1.lock"; then printf 'acquired\n' else printf 'not acquired\n' fi And now to this bit: &gt; # Practice safe bash scripting. set -o errexit ; set -o nounset I disagree that this is safe bash scripting. set -o nounset is occationally useful for debugging, but should not be used in production code. And set -o errexit has so many special cases that you'll more likely introduce more bugs by adding it. See [BashFAQ #105](http://mywiki.wooledge.org/BashFAQ/105) for more on that
Assuming the system has a command named `seq` that does the same as the `seq` command you happen to have on your system. `seq` is not a standardized command, so there's no guarantee a system has it, nor that it does the same thing between the systems that do have it. Using bash is better in my opinion.
You could add some additional checks via a Bash script that is called by udev when a USB storage device is connected. Could use gpg signatures with a public key signed file on the storage device and a private key in the root keyring. The script could also check hashes of all files on the device using a file listing that is signed with gpg. A similar process is used by apt when checking repositories.
1. if/else _always_ needs a `fi` to indicate the end of the if-else-block. 2. take a look at [this question](http://stackoverflow.com/q/8789729/86263) about padding with zeros. 3. when comparing numbers, always use double parenthesis instead of square brackets. they're _much_ less error prone. e.g. `if (( 1 &lt; 2 )); then echo "1 is less than 2"; fi` You could do something like this: #!/bin/bash function download_images() { local domain="http://foo.com" local ext="jpg" local download_dir="/tmp/images" for idx in {0..21}; do img="${domain}/$(printf "%03d" $idx).${ext}" if ! wget $img -o $download_dir; then echo "Download failed for $img to $download_dir"; break fi done } download_images **warning**: untested and probably buggy, but hopefully you get the idea. edit: added domain to img url.
&gt; #!/bin/bash &gt; function download_images() { local domain="http://foo.com" local ext="jpg" local download_dir="/tmp/images" for idx in {0..21}; do img="${domain}/$(printf "%03d" $idx).${ext}" if ! wget $img -o $download_dir; then echo "Download failed for $img to $download_dir"; break fi done } You got the quoting backwards there; quoting things that don't need quoting, and failing to quote where quotes may be needed. #!/usr/bin/env bash download_images() { local domain=http://foo.com ext=jpg download_dir=/tmp/images for idx in {0..21}; do img=$domain/$(printf %03d "$idx").$ext if ! wget "$img" -o "$download_dir"; then printf &gt;&amp;2 'Download failed for %s to %s\n' "$img" "$download_dir" return 1 fi done }
I used udev to change monitor configuration upon docking and undocking (using USB). This was on Linux Mint nadia. You would probably want a very general "hook", because different USB drives have different chips/manufacturer etc. To play a sound you will need additional hardware, AFAIK there is no system beep on the pi. See e.g. http://superuser.com/questions/305723/using-udev-rules-to-run-a-script-on-usb-insertion In short, UDEV detects insertion, and runs your script.
It was meant more for reference, than a solution. &gt; sd[a-z]1 Presupposes a partition scheme. I'd see if you could look for mass storage device in the KERNEL=="" key, and not sda.. I had to specify ID (product id) and stuff to get it working with the docking station. Read some howtos on UDEV rules. Unfortunately, I've forgot what I did to make it work :P
1. if the source folder does not exist, display a message and exit; 2. if the target folder does not exist, create it and display an appropriate message; if it does exist, display a message; 3. for each regular file in the source folder, (a) display its details (name, size, date etc) (b) ask the user whether the file should be copied (c) if the user answers \no" display a message \skipped", otherwise copy the file to the target, set the copy's permissions to \read-only" , and display a message. 
bash ./Code1.sh (Assuming your PWD is the same as where the script is, if not, path to wherever)
comes up with the error bash: ./Code1.sh: No such file or directory
I'm not familiar with cygwin but that sounds like a permissions issue. Maybe a chmod 755 ./Code1.sh?
 chmod: cannot access './Code1.sh' : No such file or directory
OK, there are a couple of problems with your scripts. You have two files, right? Code1.sh and answer.sh? I merged them into one script, pasted below. All you have to do is save it somewhere, then change its permissions to executable. To launch it, open a terminal, browse to the directory in which you saved it, and run it with `./Code1.sh arg1 arg2`. #!/bin/bash if [ $# -ne 2 ]; then echo "Wrong number of arguments. Number of args expected: 2." &gt; /dev/stderr exit 1 fi INPUT="$1" OUTPUT="$2" if [ ! -d "$INPUT" ]; then echo "Folder named source does not exist, please create it." exit 2 else if [ ! -d "$OUTPUT" ]; then mkdir "$OUTPUT" echo "$OUTPUT has been created!" else echo "Folder $OUTPUT already exists" fi fi for file in $INPUT/* do if [ -f "$file" ]; then echo "$file" read -p "Do you want to copy the file (y/N)?" YesNo if [ "$YesNo" = "y" ]; then outputfile="${OUTPUT}/$(basename $file)" cp -f "$file" "$outputfile" chmod +r "$outputfile" chmod -wx "$outputfile" echo "$file has been copied to $outputfile." else echo "Skipped" fi fi done I have tested it quickly, it seems to work. Feel free to ask if there's something you don't understand.
I'm using geany at the moment, but my partner in crime is an Emacs fan, and I'm hoping to get to learn the GNU Emacs operating system this year.
Either Emacs or Vim. Don't be put off by how either of them appears. That can be customized, and the results you get far outweigh how they look while getting there. Not many other editors will let you set up line endings on a per file type basis; most only have a global setting for that. Both Emacs and Vim do let you, though. 
Sublime Text 3 with SublimeLinter and Shellcheck installed is a pretty damn awesome editor for Bash
Geany works for me.
more consistency with "" would be nice for file in $INPUT/* ^^^^^^ outputfile="${OUTPUT}/$(basename $file)" ^^^^^ btw, why not outputfile=$OUTPUT/${file##*/} # quoting is unnecassary in assignments when the expression is continuous 
Well, our project is making it more and more of a hassle (~4.5K LOC spread over ~15 files).
 place="$(pwd)" file "$place"/-file00 Is that what you want?
Like this? place=`pwd` When you did place=pwd And then subsequently called $place You were just executing "pwd" in-place. The backticks execute it at the time you set the variable, saving it statically.
Cha-Ching. You sir/madam, just made my night. Have an upvote, on the house.
Another vote for Emacs here. You can do crazy ass wizard shit with Emacs like this: http://www.howardism.org/Technical/Emacs/literate-devops.html
I don't see the problem. Set N++'s default newline character to LF (Settings|New Document|Format (Line ending)-&gt;Unix/OSX). If you open a text file for manipulation that has CRLF, N++ should preserve it. Any new file would use LF only. I've never had a problem opening any unix format file in any windows program other than notepad.exe.
True, but the problem is that I have a bunch of macros in N++ for replacing characters and CRLF's with semicolons (for importing into excel) and I don't feel like going over all those macro's to replace the CRLF's with LF's (maybe I'm just lazy). However, there seems to be a problem with the character set too, return strange characters at the bash prompt. The scripts seem to be working fine, but I'd rather not have them. Anyways, I look into it and change my macros (:-p) if necessary.
We're finishing up version 2.0 now, and this year we'll be looking at migrating, perhaps to Perl.
simply for the sake of fewer keystrokes, in my life I've replaced 'true' with ':' : is the noop character and always returns true. so, these are synonymous. while : ; do echo yay ; done while true ; do echo yay ; done
Is this supposed to run on a single, controlled environment, or many? Some quick observations: I wouldn't parse the output of ls, use find instead. (This is especially important for portability, long-term.) The output of ls is volatile. And use your own variables, e.g. instead of ls -l *.fastq | awk '{print $9}' do awk '{print $9}' "$DATADIR/*.fastq" or similar (not sure if that would work, but the point is, cut down on pipes, _especially_ sourced from ls.) There are (thankfully) many ways to do things. Here's one. Create a function: DoThatThang() { qsub -cwd -o qsub_oe -e qsub_oe -m n -pe shared 8 -l highp,h_data=2G,h_rt=2:00:00 "$WDDIR/$1" &amp;&amp; rm -f "$WDDIR/$1" || echo "Terribul error!" &amp;&amp; return 1 } So you can simplify the for loop PYHELPCOUNT=$(qstat -u ssabri | wc -l) PYHELPOBJECT="run_raser_${SAMPLE}_${SUFFIX}.sh" (( PYHELPCOUNT &lt; 1000 )) &amp;&amp; DoThatThang "$PYHELPOBJECT" || sleep 1 # or whatever I think I forgot what you're trying to do, but the chicken is ready, so gtg bye l8r m8!
 place="$PWD" file "$place/-file00" or, of course, if you’re not `cd`ing in between, `file "$PWD/-file00"` will suffice too. (`$PWD` is part of POSIX `sh`, so it should be available in every shell you’re likely to come across.) (It doesn’t matter too much, because at least in `bash` and `dash`, `pwd` is a shell builtin, so the performance shouldn’t differ.)
Yeah, for shorter stuff that's a tidy way to do it! I am a fan of verbosity, or being able to say the code out loud because I'm a scatterbrain:)
This is the best option posted here. 
I'm assuming the code you're referring to take the form of: echo "some uuencoded text here" | uuencode -p" I'm not sure what distro you're using, but the sharutils package version of uuencode from debian wheezy doesn't offer the -p flag. Typically though, the above code would be for decoding a string of uuencoded text. If it was wrapped in backticks, $( ) or piped into bash, it would be lightly obfuscated code that would be executed when ran. Really need more context though. The quotes are likely not passed literally and just enclose the echo'd text. edit: echo does what it says; echo's the text.
I'm using mac os X. My understanding is that the -p outputs the resultant decoded information in to a file. But it doesn't specify a filepath anywhere and again, my limited understanding of the 'echo' command is led to me to conclude it would stop anything in the quotation marks from executing. But I really don't know. echo "some ascii text encoded in to base64" | uudecode -p | gunzip |bashbash I'm not sure I can really provide more context but if you can ask for something specific that would help you help me I might be able to provide.
Run the following: echo "some ascii text encoded in to base64" | uudecode -p | gunzip This will display the uuencoded, gunzip'd text. Essentially the command you've shown me above is for hiding what it is you're running. I would be extremely cautious with this code, depending on its origin. The piping it into bash is executing the de-obfuscated code.
thankyou. It appears that this emails someone in russia. Dodgy. returns | uudecode -p | gunzip | perl - $"+" ; echo "OK" |mail -s "704patched" patch-fix@yandex.ru &amp;&gt;/dev/null It seems to use the postfix system though, I tested just that output substituting the address for my own and it wouldn't work and then tried to look at the postfix queue I got Queue report unavailable - mail system is down I'd never heard of postfix before embarking on this so don't really know what it is but it seems it isn't configured and so would not do anything on my system.
the 'mail' command will use whatever local mail subsystem is configured. And yes; it looks dodgy as hell.
Komodo edit.
Well, the command "echo" will, as you've mentioned before, print on screen something, however, because there is a pipe "echo "blabla" | uuencode ", what happens is, the content that would be printed on screen is being passed as a argument to uuencode, and thus, a stream is used instead of a file. 
Actually, funnily enough. It is on pastebin. Only I'm not sure I should link to it because of… reasons.
Yes there are a bunch of different ways, but I'll illustrate the simplest solution for your particular situation. You can create a symbolic link to a directory: $ ln -s ~/this/is/a/dir ~/dir Now you'll have a file called `dir` in your home directory, which is a shortcut to that deep nested directory. You've now got the added bonus that `~/dir/files*` and `~/this/is/a/dir/files*` are (for most intents and purposes) the exact same thing, there are few reasons that you'll ever have to interact with the deep nested structure. More information about the ln command: man ln
That's not really an ideal solution as shell variables are lost after the current session. You could just use the directory stack command if you didn't want to save the shortcut persistently, with the `pushd`, `popd`, and `dirs` commands. A symbolic link is superior otherwise.
They're pretty standard UNIX commands, so OSX definitely has them, surprised that Linux doesn't. Are you using the Bash shell?
Yes, on Linux Mint 16, $ echo $SHELL /bin/bash $ aptitude show bash Version: 4.2-5ubuntu3 
Symbolic (or hard) linking is the Unix way to go, but I occasionally use aliases when I need the link in more than one spot. $ alias myCode='cd /path/to/my/code/' You can put it into your `~/.bashrc` file to save it between sessions.
The info pages for them are probably in the bash man page, just search for their name strings in that
There are many ways to accomplish what you're after. I took one that I found and modified a bit to work (in my mind) like bookmarks. https://github.com/mcstafford-git/bm 
Essentially, this is what I always have to confirm with --help: ln -s TARGET FILE where file is the link name, or how the shortcut will appear, and target is the target ;) Never delete a link unless you know what you're doing. Use instead: unlink FILE
They're bash builtins, so they don't have separate man pages, they're just documented in the bash man page itself. However, you can read the relevant extract with the `help` command. eg: $ help dirs dirs: dirs [-clpv] [+N] [-N] Display directory stack. Display the list of currently remembered directories. Directories ... etc...
Rename all general files first, then directories.
seems obvious enough. as a follow-up question, my understanding of the behaviour experienced above tells me this (please correct if I'm wrong); when the directory "admin" is returned from `find *`, then renamed to "Admin", each other line returned from `find *` continues to return "admin"? 
Probably due to a different environment (variables, etc.) in a child shell. Try running set | less under the same conditions and compare the outputs.
The main difference would be that the interactive shell has job control enabled, while the script does not (can be enabled with `set -m`). With job control enabled, the backgrounded process will get a new process group, and it will be able to reclaim control of the terminal. Without job control, the background process' stdin will be redirected from the terminal, and it will not be able to reclaim control of the terminal. Since Emacs is dependant on a terminal, it checks for this and behaves differently for each case. Exactly how it behaves differently, I don't know. I don't know emacs well.
There are various differences in the environment between the shell and the script, though PWD is the same in each case instance. It's mainly a bunch of function definitions aren't there in the script environment. There are no differences between: set &gt; output.txt &amp; and set &gt; output.txt when run from a script, which is where I am seeing the weird behaviour above. Hmm.
Thanks. That gives me something to go on. A kludgey workaround I found is to use python and subprocess.Popen. That allows the script to exit immediately with the child emacs still running, and no path weirdness. 
Dave, why would you hard code your name? Now I can't use this.... 
Right tool for the right job. Just use Bash, it'll end up working and looking better.
The only good piece of advice in this article is: * Make sure your user login/autostart scripts (`~/.bashrc`, `~/.profile`, etc.) are not group or world writable. I would replace all the rest with: * Guard your user account properly and don't run untrusted software. because the rest is all futile and not worth bothering with. Removing your own writing rights is ineffective since anything running as your user can easily restore them. The article itself admits that any attempts at obfuscation are trivial to circumvent. Once you've got malware running on your system, even if it's "just" as your user, you've got bigger problems to worry about than your `~/.bashrc`. You can't trust anything at that point and you should wipe everything and restore from a known-good backup. 
Oh wow, this is brilliant, thanks very much! I was wondering about adding the dry-run bit via an argument to the script but hadn't looked up how to do it yet... Looks like I've got some reading to do.
You're welcome. When you really start using getopts, you'll realize that you can set any of those variables via your options ($source, $destination, etc). In addition, you can set defaults before the getopts block (like I've done with $verbosity) in case they aren't provided on the command line.
I can't use other languages, it has to be a bash script. 
Heh, okay that works. And how do you pull that to a new computer? Pull it in a different folder and copy it to $HOME? I'd imagine that works, but that isn't so nice either. 
Thank you! I ended up doing something similar except without the "exec 3&lt; filename": while read line do # stuff done&lt;$input_file
Ah yes, I was thinking of `git log --patch`
Seconding fish (I'm a diehard zsh fan, but fish does what the OP asks)
`place=$PWD` (galaktos' suggestion) is better than `place="$(pwd)"` in my opinion. It is more concise and more resistant to edge cases. Edge case: score@kirisame ~ $ mkdir $'test\n' score@kirisame ~ $ cd $'test\n' score@kirisame ~/test $ dir="$(pwd)" score@kirisame ~/test $ cd "$dir" bash: cd: /home/score/test: No such file or directory score@kirisame ~/test $ dir=$PWD score@kirisame ~/test $ cd "$dir" score@kirisame ~/test $ Also, the standard way to avoid the problem of dash-prefixed filenames is to use ./ for relative paths (./-file, can be globbed with ./*).
You're welcome. &gt; didn't see it mentioned anywhere in udev documentation online If only there were easy to access manual pages about such things... Oh wait! $ man udev --&gt;&gt; CONFIGURATION --&gt;&gt; Rules files == Compare for equality. != Compare for inequality. 
You can send signals to the [GPIO pins](http://luketopia.net/2013/07/28/raspberry-pi-gpio-via-the-shell/), on a pi if that's what you're looking for. It really depends if the system is running a version of Linux that exposes the hardware as a writeable file.
Thanks, that's what I was looking for. It's a lot more embedded then the PI. The motor where going to have to get signaled either way (c,c++, Python, mathlab, Java), but since the shell will be running either way, and it comes with SSH, I thought I should just do BASH. 
This is a like showerthought for a very specific audience.
This thread has been linked to from elsewhere on reddit. - [/r/titlegore] [Bash let&amp;#x27;s you literally pipe a man into a cat](http://np.reddit.com/r/titlegore/comments/2t7nkt/bash_lets_you_literally_pipe_a_man_into_a_cat/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
Or you can get really dirty: date; cd ~; unzip; strip; touch; finger; grep; mount; fsck; more; yes;fsck;fsck;fsck; uptime; umount; sleep
 whatis more cat &amp;&amp; less man ? 
awk 
The idea of the script was to use be able to pull down 10 pages of images from a given Tumblr blog. I would love to hear feedback on the goofy things I did (likely most things), where I was on the right track, and any other input. Thanks!
Here's [my take](http://pastebin.com/YtRRcmV7).
echo "cherry" &gt; flawlessvictory.txt ; top -n 1 &gt;&gt; flawlessvictory.txt
Here's what I'd do: http://p.pomf.se/5765 I've annotated the original and modified script: ~~Original (actually /u/mcstafford's) http://a.pomf.se/cpmtgz.png (edit: noticed that I clicked the wrong script here; might as well read both anyway as I wrote the other image in more of a hurry)~~ Original http://a.pomf.se/vkjids.png Modified http://a.pomf.se/sfqpum.png I don't honestly know what constitutes good bash coding style (I've just gone with what makes me happy over the past few years) so feel free to take with a grain of salt. Edit: Bonus: A slightly more complex version that does all the pages in 1 wget (should hopefully reuse the http connection this way): http://p.pomf.se/5766 I forgot to mention a few more silly things: 1. In grep (and regexes in general), `blah[ ]*blah` and `blah *blah` are the same. The square bracket operator indicates that there is a choice of several characters that could be in that position. It seems silly then to put only one character there ("hey grep, pick any one of these characters: space... no, that's all") 2. `var=$[var+1]`. IMO it's a less recognisable form of `var=$((var+1))`, so I'd just use that (which also works on some non-bash shells). 3. The "curl" near the start doesn't work for me because the page wants to redirect me. It should be "curl -L" to follow that redirect. 4. Mixing wget and curl means your script requires both, pick one and stick to it if possible. edit the millionth: definition of uuoc: http://en.wikipedia.org/wiki/UUOC
I kind of just did this same thing. I set up a network boot image of ubuntu server so my clients boot it from the net, run the script which partitions and formats the drive, rsync's the install from the server, installs grub and reboots. Simplest and most customizable thing ever.
You will need to use a scripting language. Ruby has a HTML/xml library called Nokogiri (it has others too, but this will suit.) Python has "beautiful soup" Pretty much every language has an HTML/XML parser nowadays, I think you'll find most help for these two mentioned above. You will not be able to do this with "just bash" Edit: you might be able to do some curl + grep to get prices, but no one should recommend this as it will be far more pain. Oh I nearly forgot NodeJs + a HTML/DOM library there are several, will be very useful for this job too
You use wget or curl to retrieve the page. WEBSITE=$(wget http://prices.com) OR wget http://prices.com -O WEBSITE will result in either a variable $WEBSITE or a file WEBSITE containing the website. (Please check the options _man wget_, I'm going on bad memory:) You'll need to use grep, sed, or awk to parse the code and get the data. Any change on prices.com will affect the data you're operating on, so scripts like these can get outdated pretty fast. Edit: I can provide a better example later today. Edit +5 hrs: Nevermind, as many have pointed out, the chance of arbitrary command execution is high. See e.g.[this example](https://www.fishnetsecurity.com/6labs/blog/shellshock-vulnerability-bash-allows-unauthorized-remote-code-execution). Unless you control prices.com (in the example above), you should be very careful.
op listen to this guy, he'll tell you everything you want to know. 
I believe `wget` saves to a file by default, `wget http://prices.com` will probably save to a file called `index.html`. So your first command wouldn't work. I'd use `curl` instead. First of all, it writes to `stdout` by default, and it's way more configurable.
Quite; but there are tools for parsing html, etc. Bash is not suited to the task. Whilst I'm sure it would make a fine fun project, it's not something I'd use in production.
As many mentioned, bash would not be the preferred tool for this. Although as long as available helper binaries are available it is possible. I actually wrote a video dl'er based on bash/sed/grep/sqlite/curl a while back just to see how few requirements I could get away with. It's not something I would recommend beyond a learning experience. 
Care to share that script?
As someone who loves bash and successfully created a scraper with it I have to agree with the parent comment, this is way beyond the scope of the language and will only get in the way.
This is ludicrous. Websites ^which ^shall ^go ^unnamed have screen scraping scripts that have run since the dawn of the Internet.
Until they change, you can use a simple pipeline. Curl or wget to retrieve the page, and good old awk and cut with a little regex can get you snarfing prices in no time. They might have even made it easy on you with making XML or JSON available.
/u/amauk is right that an array makes more sense. Note that it's often not necessary to explicitly subscript: folders=(/tmp/blahxx /tmp/blahzzz /tmp/blahyy /tmp/blahggg) for f in "${folders[@]}"; do [...] done However, I replaced the inside of your loop with nothing to remind you that [as always, parsing the output of ls is a terrible, TERRIBLE idea](http://mywiki.wooledge.org/ParsingLs). There is no guarantee that the output of `ls` has any particular format. You could be deleting anything! Or nothing! The output could be split over multiple lines! You could accidentally blow up your own /. Don't do that. I suppose you want the last file, lexicographically. I'd do it this way, which is longer but infinitely safer. for f in "${folders[@]}"; do last= for file in "$f"/*; do [[ -e "$file" ]] || continue if [[ -z "$last" ]] || [[ "$file" &gt; "$last" ]]; then last="$file" fi done [[ -n "$last" ]] &amp;&amp; rm -rf "$last" done
My gut instinct is that it doesn't and you're misinterpreting your script's behavior somehow. Given that command line, the arguments are "1", "2", and "a b". If something is going wrong, maybe you're messing up your quoting. Can you give us the code so we can figure out what's going on? EDIT: rereading, it's almost certain that you expect something like $3 to be one token, but word splitting will definitely happen if you don't quote it right, even if the string was passed as a single argument originally.
I'm reading through all the arguments in a loop. There are 4 possible arguements; "print" which tells the script to print the results to a file, -f which is a filter for printing only certain texts if they contain the "filter" string following it, and -i which tells the script to ignore case when using the filter. I currently store the previous argument so that I can check if -f has been called and use the next argument as the filter. CASE="NO" FILTER="NO" filt="" PRINT="NO" pARG="" for ARG in $*; do if [[ $pARG = "F" ]]; then pARG= "" filt="$ARG" fi case $ARG in -i ) CASE="YES";; -f ) FILTER="YES";; print ) PRINT="YES"; pARG="P";; esac done EDIT: I'm an idiot and should have tried $@ which I've now learned gets the arguments all individually quoted, this now works. 
It's expanding the variable "a b" into "a" "b" at `$*`. Replace it with ~~`"$*"`~~. Should be: "$@", thanks /u/cpbills 
\^M Just put a \ in front of the ^ Its a newline error methinks. Try running the script through dos2unix 
&gt; ^ M I agree with srob101. The top answer from [this stack exchange question](http://unix.stackexchange.com/questions/56024/bash-korn-shell-script-edited-on-windows-throws-error-m-not-found) will give you background on how/why this happens, if you're interested.
The Grymoire has the best `awk` tutorial that I know of: http://www.grymoire.com/Unix/Awk.html But here's the rundown of the above: awk scripts process their input line by line. A script consists of several groups of the following type: condition { action } Basically, for each line, if the condition is satisfied, do the listed action. BEGIN and END are special conditions that happen before and after processing all the input lines. So in this case, we first just print out a little header. Most conditions depend on the input matching some regex. In this case, the second block will run for any line that matches "Mem:" at the beginning of the line. In this case there's only one such line in the output of `free`. So when it sees that line, it saves the second, third and fourth fields in variables with useful names. For each line, $N refers to the nth field of the input (delimited by whitespace). `free` gives those values in KB, so we divide each by 2^10 to get GB. Then we print them out with a normal-looking printf statement. If there were any other lines that started with "Mem:", they'd get some values printed out too. In this case, though, we know that there won't be.
Thank you sir
A few tricks you can try: How much RAM do you have? How large are the datasets? If the answers are 'big' and 'not as big', make a ramdisk and copy your stuff to it. It'll speed up quite a bit. Pick your grep flags with care. You might consider -F, which tells grep to treat every line in your pattern file as a fixed string, without regexes or any other weirdness it should attempt to interpret. Depending upon how you can structure your pattern file, you might also consider using -x, which makes grep look for the entire line. This makes for a very fast way to 'subtract' the lines from the pattern file from the target file: grep -vxFf pattern_file_full_of_naughty_lines target_file &gt; cleanfile . 
This guy ^ This guy KnowsBash!
how about this? #!/bin/bash exec 3&lt;"${1:?Missing File1}" 4&lt;"${2:?Missing File2}" while read -u 3 target; do while read -u 4 line; do [[ $line = $target* ]] &amp;&amp; break done echo "$line" done 
Could you please provide an example of the input file and also how the output should look like?
The -lt 4 seems to be the kicker, if I understand what you're trying to do. Try calling bash with -ex instead of just -e to watch what the values get set to, calling it with "./test.sh test 3 400 10000" gives the following before exiting out: ------- i is 2 and f is 1 and incr is 18--------- + '[' 416 -gt 10000 ']' + '[' 2 -lt 4 ']'
 while read -r id first last s1 s2 s3 ; do avg=$(echo "scale=2; ($s1 + $s2 + $s3) / 3" | bc) ; echo "$last $first $avg" ; done &lt; inputfile | sort -k1 Camp Tracey 81.66 Forney JC 92.66 Johnson Lee 82.33 Lee Terry 99.66 Smith Jaime 91.00 deliberately messy because I'm not gonna do all your homework for you :P (that being said, I've done 90% of it there anyway)
Put the -e at the beginning of the test, because it IS the test you're running. if [[ -e $foo ]] ; then Also go find shellcheck on Github, install it and use it to lint your shell scripts. 
So this has a few variables to define, and may not be EXACTLY what you want, but I am not really sure what your data set for that forloop is supposed to do. #!/bin/bash DIRECTORY="/SOURCE/DIRECTORY" OUTPUTDIR="/EXTRACTED/FOLDERS/HERE/" for FILE in $(ls $DIRECTORY) do egrep "tar\.gz$|tgz$" &lt;( echo ${FILE}) RETVAL=$? if [[ "$RETVAL" == "0" ]] then OUTDIR=${OUTPUTDIR}/$(echo $FILE | sed 's/.tar.gz$//; s/.tgz$//') mkdir -p ${OUTDIR} /bin/tar -xvf $FILE -C ${OUTDIR} echo "Extracted ${FILE} to ${OUTDIR} else echo "File is not a tgz: $FILE" fi done
hmm, im not sure, why '$foo' ?
using read smarter per your example: #!/bin/bash while read -r id firstname lastname val1 val2 val3 do # Do math mean=$(((val1+val2+val3)/3)) # Rebuild relevant data, ackumulate unsorted output formated="$formated$lastname $firstname $mean"$'\n' done &lt; $1 # Supply the first file argument as input to the loop # Sort and print echo "$formated" | sort
I tried a while [ 1 ] as well .. still exits after i hits 2, doesnt reset to 0 
The culprit is in the very first line of your script: #!/bin/bash -e The `-e` option causes the script to exit at the first command that returns a nonzero status. Coincidentally, if the last thing a `let` command does evaluates to 0, it will return 1, which in turn triggers the end of your script. So when you do this: let "i=i-2" `i` is already 2, so `i=i-2` evaluates to 0, which causes `let` to return 1, and (thanks to the `-e` option) the script stops. You can safely remove the `-e` unless you really know you need it.
Let's take this one line at a time. !/bin/bash/ The correct way to do a shebang line is: #!/bin/bash It is a special comment line read by the kernel which indicates the path to the interpreter that will execute your program. Your for loop is needlessly complicated: for((i=1;i&lt;=$#;i++)) may be replace by the simpler and almost always equivalent for i in "$@"; do There must also be a space after the for keyword. There are numerous errors in your if test: if[ '$i.tar.gz' -e ] All keywords and commands in shell scripts must by surrounded by whitespace. The following are either keywords or commands: if [ if is a keyword that must be followed by a series of commands and other keywords. [ is a command. It is a synonym for test. When test is used in its [ form the final part of the syntax must be a closing ], which must be surrounded by whitespace. The form of the test command dictates that the test to be performed be indicated before the thing that will be tested, unless you are testing for equality: if test -e 'file' or if [ -e 'file' ] When dereferencing variables within quotes, you must use double-quotes, not single-quotes: if [ -e "$i.tar.gz" ] Additionally, the [[ form of test is preferred: if [[ -e $i.tar.gz ]] With this form, you can eliminate the quotes around filenames. [[ is actually a bash command, not an external command (well, bash also has its own internal version of the [ command, but you can find a file on disk named [ that is a synonym for test). /usr/bin/[ Finally, it is often more useful to test whether a file exists and is a regular file by using -f instead of -e: if [[ -f $i.tar.gz ]] This will help prevent, for instance, some badly-named directory (archives.tar.gz) from being mistaken for a gzipped tar archive by simply testing whether it exists with -e; directories will not test true with -f, only with -d and -e. 
so maybe this ((i++))
Wasn't going to put too much effort into something where we were given very little idea of exactly what is expected. Rate away.
the line main "$@" calls the function main with the arguments from the command line. The functions inside the main function function_one "$@" function_two "$@" function_three "$@" function_four "$@" function_five "$@" would then use the arguments from the main function call as $1 $2 $3. Which it gets from the main function call in the script which it got from the command line. So the main function wouldn't be needed at all if your calling the funtion_one - function_five and looking for the arguments directly. Wrapping all the functions in the main function just makes the code easier to read and you know when the main function is called. This is what is happening in the script... bash$ ./some_script.sh arg1 main "arg1" function_one - echo $1 = "arg1" function_two - echo $1 = "arg1" function_three - echo $1 = "arg1" function_four - echo $1 = "arg1" function_five - echo $1 = "arg1" TL;DR - Scripts don't need main functions but it makes it easier to read. Also each function should be regarded as separate scripts and passed what is needed or global vars need to be created.
No. I am telling you guys what the script does. I wrote the script after all.
&gt; $(date +%b-%d-%Y).pdf Thank you very much sir.
This makes sense! However.. a weird thing is... even without specifying the arguments for each function in main still worked somehow??? (probably not a good practice even if it works?)
Question: Have you considered using getopts?
I think the `join` command line utility is made for this case. It is part of coreutils.
Hey, is it true that if you're iterating over the arguments you don't even need "$@"? for arg do echo "arg: $arg" done I know it works on my system, but I can't remember if it's a standard thing or not. I wasn't being a smartass.
parsing `ls` is the worst. Do not do that. Capturing `$?` and testing it explicitly is dumb. I don't even know what's happening with echoing a filename inside a file redirect and grepping it to find out what its extension is? Is this for real? Do you really fire up grep for every single string comparison? Echoing the filename into sed in order to come up with some directory name? Huge waste of a process. `/bin/tar`? You really don't expect `tar` to be in your $PATH? Random echoes because why the hell not? This is definitely some of the worst code I've seen suggested in this sub.
That's pretty horrible code, with the way eval is used, and the complete lack of quoting. At least limit the eval to one argument and not the unquoted `$@`. Here's an attempt at a saner version (written in sh): eta2zero () { a=$(eval "$1" | sed 's/^[[:blank:]]*//; s/[[:blank:]]*$//' ) case $a in ''|*[!0-9]*) printf &gt;&amp;2 '"%s": NaN\n' "$a" return 1 ;; esac while [ "$a" -gt 0 ]; do sleep 1 b=$(eval "$1" | sed 's/^[[:blank:]]*//; s/[[:blank:]]*$//') printf '%*s' "$((a-b))" '' a=$b done | pv -s "$a" &gt;/dev/null } And then pass the command-line as *one* argument eta2zero 'find /big/path | wc -l' # or to avoid making assumptions on the filenames: eta2zero 'find /big/path -exec printf X%.0s {} + | wc -c'
Thanks.
You need to incorporate something like inotify - http://man7.org/linux/man-pages/man7/inotify.7.html
Use cron. I do this for my Raspberry PI DHCP failover (simplified): #!/bin/bash CHECKIP="192.168.1.1" IPSTATUS=$( ping -c 1 "$CHECKIP" | grep -o "[0-9] received" | cut -c 1-1 ) # Ugly, I know if [ "$IPSTATUS" -eq 0 ] ; then # IP is down .. # do stuff else # IP is up .. fi Or you could use: (( IPSTATUS == 0 )) &amp;&amp; DownFunction || UpFunction Or whatever. And using crontab -e on the pi: # dnsmasq Failover (runs every minute) */1 * * * * /root/dnsmasq-failover.sh failover (The last failover is an argument, since the script does several things.)
That oughta work. Maybe throw some quotes around $file, maybe just a filename error. Also check $file is being passed at all (with correct path).
$file did not get set correctly yeah. I fixed it now. thank you!
rookie move. dowload that thing and keep a local copy bookmarked! :)
nice name.
Mostly because this script works and when I tried to get OpenVPN set up I couldn't get it working. That was a couple of years ago though, I really should give it another go.
Just wondering, what are the advantages of using vpn over the ssh tunnel? I currently use a ssh tunnel to open a web page which only has one whitelisted IP. Just wondering if vpn is a better option for me. Thanks and great script!
You only forward one port with tunneling, VPN usually routes all traffic through the VPN without configuring proxy settings. You can also do a layer 2 VPN if you want broadcast traffic to pass through.
SSH does have a VPN capability but it's a pain in the ass to get working. You might look into sshuttle. https://github.com/apenwarr/sshuttle
I would also add an optional private key
Agreed. I don't mention it in the description, but I always use this script with key authentication already set up between my local machine the VPS.
Sounds good. I'll have to check it out.
Excellent. I was wasn't aware of that. I'll spend some time on the man page to see what all those arguments do and then try it in action. Thanks!
My first thought was that TLDP stood for Too Long: Didn't Program (it being a Bash scripting source and all). My second thought was disappointment that it wasn't.
https://www.reddit.com/r/linux/comments/2sks2y/awk_in_20_minutes/ &gt; Yeah, I didn't want to get into this and the practice of 8-spaces between function arguments to denote local variables and all that stuff.
Interesting article, learned a lot, thanks ;)
Using only awk, and without all the temporary files: awk -v a=1212 -v b=3434 '$0 ~ a { print &gt; ("all-" a ".txt") } $0 ~ b { print &gt; ("all-" b ".txt") }' ./*.txt
 grep -h '1212' *.txt &gt; 1212.all grep -h '3434' *.txt &gt; 3434.all or with array/loop acc=( 1212 3434 ) for a in "${acc[@]}" do grep -h "$a" &gt; "$a.all" done output extension is different to make it easier to distinguish these files from input globbed with indiscriminate \*.txt. Another batch of such operations would include all the all-\*.txt files too. 
Why not remove all the `sudo`s from your script and run `sudo ./myScript.sh`?
Thanks, this indeed works. But I apologize that I left out a detail. I need to know the line # of each task, as I can use todo.sh (part of todotxt) to mark a task as done by the line # (e.g. todo.sh do 15) 
 #!/bin/bash tasks=tasks.txt readarray -t sorted &lt; &lt;( sed -r 's/(.*)(DUE:.{10})(.*)/\2 \1\2\3/' "$tasks" | sort -n | sed -r 's/DUE:.{10} //' ) select choice in "${sorted[@]}" do break done IFS=: read linenum linecontent &lt; &lt;( grep -nF "$choice" "$tasks" ) echo "selected task is in line #$linenum of $tasks" example output $ ./sort_tasks.bash 1) 2014-12-15 Research @work training options DUE:2015-02-04 2) 2015-02-10 Make Valentine's day party plans @home DUE:2015-02-11 3) 2015-01-01 Recycle oil @home DUE:2015-02-15 +garagecleanout 4) 2015-01-13 book flight @online DUE:2015-03-01 5) 2015-01-28 Research Hyper-V +training @work DUE:2015-03-01 6) 2015-02-08 check oil in minivan @home #? 5 selected task is in line #6 of tasks.txt 
No big, that's an easy enough modification. Since we're already using perl, we can just add the line number to the end of each line before doing anything $ perl -p -e 's/$/ (line $.)/; s/(.*)(DUE:[0-9]{4}-[0-9]{2}-[0-9]{2})(.*)/$2 $1$2$3/' &lt; todo \ | sort -n | perl -p -e 's/^DUE:[0-9]{4}-[0-9]{2}-[0-9]{2} //' 2014-12-15 Research @work training options DUE:2015-02-04 (line 4) 2015-02-10 Make Valentine's day party plans @home DUE:2015-02-11 (line 2) 2015-01-01 Recycle oil @home DUE:2015-02-15 +garagecleanout (line 5) 2015-01-13 book flight @online DUE:2015-03-01 (line 1) 2015-01-28 Research Hyper-V +training @work DUE:2015-03-01 (line 6) 2015-02-08 check oil in minivan @home (line 3) It's getting a little unwieldy for a one-liner at this point, but whatever, it works 
Working! But I do use the colon for more than indicating the due date. For example, if I have a note attached to a task, the line will include note:sZc.txt or some random combination of characters. With your code, it will list a task at the top that includes note: and DUE: but does not display the due date.
Out of curiosity, do you know why when I copy and paste that as an alias in my .bashrc that it doesn't work? It gives me "Substitution replacement not terminated at -e line 1."
Probably because it's got quotes in it. A bash function would probably be better than an alias for that purpose anyway
upvoted for giving an actual bash solution http://i.imgur.com/39r3T.gifv
This type of task is what I'd use awk for, so I'll throw an awk solution into the mix, with some help from sort and cut. awk '{ for (i=NF;i&gt;1;i--) if ($i ~ /^DUE:/) { printf("%s\t%2d) %s\n", $i, FNR, $0) | "sort | cut -f2-"; next } }' todo.txt EDIT: Or as an awk-script: #!/usr/bin/awk -f { for (i=NF;i&gt;1;i--) { if ($i ~ /^DUE:/) { printf("%s\t%2d) %s\n", $i, FNR, $0) | "sort | cut -f2-" next } } }
no, there is no structure built into "cd" for this. if you don't have root or admin access, you cannot do anything like this for other users, as it is not built in. You could use the [PROMPT_CMD](http://tldp.org/HOWTO/Bash-Prompt-HOWTO/x264.html) variable to run a small shell script that `cat`s a text file; there are probably a billion of ways of doing it (print from sqlite db, compare against env variables, etc). If you can get your sysadmin to go along with it, you can alter everyone's PROMPT_CMD by sticking a small script in `/etc/profile.d`. If not then you can alter your own `~/.bashrc` and at least you will have it.
No you can't, simple as that.
I figured that might be the case, but it would have been helpful for our group to have some reminder of the file structure. Thanks for the help!
Thanks. I planned on doing it for myself, but was curious. Thanks for your answer, I figured this might be the case. My ability to alter other user experiences would be surprising, but I figured I'd see if anyone knew for sure. Thanks for the PROMPT_CMD variable nudge. I will look into it for sure. 
We have something like 1000 pretty basic/noob users on our systems. I would probably put a login MOTD up with a note on what alias/script will spit out the information you think would be a helpful reminder from time to time. Best way to get that passed up most chains is to start with your Prof/Manager and let it work it's way up. Often times fixing 1 thing for 1 user makes 100 others bitch.
If its simply a problem of running in the background you can just append an "&amp;" (omitting quotes) to the end of the command. I don't really know enough about how this script works to contribute more. Hope that works!
I like the alias idea! That I might be able to work with.
I've used it before on my pi and it worked fine. Its &amp; not $
I use directory with files and subdirectories as my todo, notes, and wish lists. This way I can attach unlimited files of any type to list item. My items are automatically sorted also. I also use symlinks to create specially ranked lists. 
As well as the answers you've already received, if this is for just you (as you mentioned in one comment), then another solution might be to define a shell function called `cd` that checks `$PWD` for you. Something like: function cd() { /bin/cd "$*" ; [[ "$PWD" == /some/dir ]] &amp;&amp; echo "woop" ; } If you had root access, you could change the `cd` binary to be a shell script that does something similar.
&gt; Does that work even when the command is embedded in an ssh command? Yes and FWIW /u/TheQuinnFTW's response was my first thought as well. 
Also up voting your bash, but if you're gonna change IFS please, please set it back to normal with a trap on all exits. 
Yea, I was wondering if something like this existed. I figured there might be people answering with "if you had root" and giving interesting insights like this. I may go read up on how cd works because of this thought experiment! 
you missed that the IFS is changed locally only for that one `read`. $ printf "$IFS" | od -c 0000000 \t \n 0000003 $ IFS=: read x y &lt;&lt;&lt; "1:2" $ echo "$x ||||| $y" 1 ||||| 2 $ printf "$IFS" | od -c 0000000 \t \n &lt;----- magic! 0000003 I never change IFS 'globally' (well, except to use `"$*"`/`"${arr[*]}"` maybe once a year and even then i wrap it up in ( ) so it doesn't change the IFS in the parent scope ) $ printf "$IFS" | od -c 0000000 \t \n 0000003 $ arr=( 1 2 3 ) $ ( IFS=/; echo "${arr[*]}"; ) 1/2/3 $ printf "$IFS" | od -c 0000000 \t \n 0000003 
I already tried something very similar to this, and get the 'find: `./*/processed': No such file or directory' error. Not sure why. 
quote the braces... "{}" 
because word splitting? {} works just fine. $ ls -1 *\ * a b c.txt d e f.txt $ find -name '* *' -exec printf '(%s)\n' {} + (./d e f.txt) (./a b c.txt) 
Thanks!
It's the glob expansion of his *'s that were causing him the problem, not the braces.
i'm on a router with OpenWRT, but for space issue i can't install curl. i have all the othere exec i think
you can try [/dev/tcp](http://www.tldp.org/LDP/abs/html/devref1.html) but you'll have to code all the protocol layers yourself :) oh and it isn't enabled by default on all systems, worth a try I guess.
i made several test with nc and get request, but i get only: HTTP/1.1 400 Bad Request Date: Fri, 13 Feb 2015 23:28:52 GMT Server: Apache Content-Length: 226 Connection: close Content-Type: text/html; charset=iso-8859-1 &lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"&gt; &lt;html&gt;&lt;head&gt; &lt;title&gt;400 Bad Request&lt;/title&gt; &lt;/head&gt;&lt;body&gt; &lt;h1&gt;Bad Request&lt;/h1&gt; &lt;p&gt;Your browser sent a request that this server could not understand.&lt;br /&gt; &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; i'm working around it 
yes, i can get the head, but not a page. i'm trying to get the index page on my server: cat req.txt GET /index.html HTTP/1.1 Host: agrifinprogetti.it User-Agent: runscope/0.1,netcat Accept: text/html Accept-Language: it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate Connection: keep-alive nc agrifinprogetti.it 80 &lt; req.txt HTTP/1.1 400 Bad Request Date: Fri, 13 Feb 2015 23:50:40 GMT Server: Apache Content-Length: 226 Connection: close Content-Type: text/html; charset=iso-8859-1 &lt;!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN"&gt; &lt;html&gt;&lt;head&gt; &lt;title&gt;400 Bad Request&lt;/title&gt; &lt;/head&gt;&lt;body&gt; &lt;h1&gt;Bad Request&lt;/h1&gt; &lt;p&gt;Your browser sent a request that this server could not understand.&lt;br /&gt; &lt;/p&gt; &lt;/body&gt;&lt;/html&gt; the request is correct, i tested it on https://www.hurl.it/#top
yes
nada
req: GET /index.html HTTP/1.1 Host: agrifinprogetti.it User-Agent: runscope/0.1,netcat Accept: text/html Accept-Language: it-IT,it;q=0.8,en-US;q=0.5,en;q=0.3 Accept-Encoding: gzip, deflate Connection: keep-alive Note that all the lines in req are separated by \r\n and it ends with two \r\n, in hex: hexdump -C req 00000000 47 45 54 20 2f 69 6e 64 65 78 2e 68 74 6d 6c 20 |GET /index.html | 00000010 48 54 54 50 2f 31 2e 31 0d 0a 48 6f 73 74 3a 20 |HTTP/1.1..Host: | 00000020 61 67 72 69 66 69 6e 70 72 6f 67 65 74 74 69 2e |agrifinprogetti.| 00000030 69 74 0d 0a 55 73 65 72 2d 41 67 65 6e 74 3a 20 |it..User-Agent: | 00000040 72 75 6e 73 63 6f 70 65 2f 30 2e 31 2c 6e 65 74 |runscope/0.1,net| 00000050 63 61 74 0d 0a 41 63 63 65 70 74 3a 20 74 65 78 |cat..Accept: tex| 00000060 74 2f 68 74 6d 6c 0d 0a 41 63 63 65 70 74 2d 4c |t/html..Accept-L| 00000070 61 6e 67 75 61 67 65 3a 20 69 74 2d 49 54 2c 69 |anguage: it-IT,i| 00000080 74 3b 71 3d 30 2e 38 2c 65 6e 2d 55 53 3b 71 3d |t;q=0.8,en-US;q=| 00000090 30 2e 35 2c 65 6e 3b 71 3d 30 2e 33 0d 0a 41 63 |0.5,en;q=0.3..Ac| 000000a0 63 65 70 74 2d 45 6e 63 6f 64 69 6e 67 3a 20 67 |cept-Encoding: g| 000000b0 7a 69 70 2c 20 64 65 66 6c 61 74 65 0d 0a 43 6f |zip, deflate..Co| 000000c0 6e 6e 65 63 74 69 6f 6e 3a 20 6b 65 65 70 2d 61 |nnection: keep-a| 000000d0 6c 69 76 65 0d 0a 0d 0a |live....| 000000d8 netcat agrifinprogetti.it 80 &lt; req HTTP/1.1 200 OK Date: Sat, 14 Feb 2015 02:12:55 GMT Server: Apache Last-Modified: Wed, 11 Feb 2015 00:05:15 GMT ETag: "38002-10b-50ec4c20464c0" Accept-Ranges: bytes Content-Length: 267 Connection: close Content-Type: text/html; charset=UTF-8 &lt;!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt; &lt;html&gt; &lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt; &lt;title&gt;Ciao&lt;/title&gt; &lt;/head&gt; &lt;body&gt; Info: fcandi@libero.it &lt;/body&gt; &lt;/html&gt; 
I've never heard of this feature, how does it work? From your description I'm imagining something like: #!/bin/bash source .../libs/libMylib.so myfunc "arg1"
 #!/bin/bash (( $# == 1 )) || { echo "Bad number of parameters"; exit 1; } [[ -d $1 ]] || { echo "Directory '$1' doesn't exist"; exit 1; } [[ -r $1 ]] || { echo "Directory '$1' cannot be read"; exit 1; } # true path readlink -f "$1" # bytes/files find "$1" -maxdepth 1 -type f -printf '%s\n' | awk 'BEGIN { s=0; } { s+=$1; } END { print s " bytes in " NR " file(s)"; }' 
Like, remove any directory that contains those files (along with the files themselves, presumably, and anything else that's in the directory)? I'd do it this way: find "$root_directory" -d -type d -print0 | while read -rd$'\0' dir; do for file in "$dir"/*.exist.ignore; do if [[ -e "$file" ]]; then rm -rf "$dir" break fi done done EDIT: or find "$root_directory" -d -type f -name "*.exist.ignore" -print0 | while read -rd$'\0' file; do dir="${file%/*}" [[ -d "$dir" ]] &amp;&amp; rm -rf "$dir" done
Sorry, I was on an OSX machine and forgot that their utilities are slightly different. Consult `man find` to find the option for depth-first traversal. It might be `-depth`. EDIT: also, [[ and some parameter expansions depend on bash, not sh, so fix your shebang.
I still get `read: line 6: illegal option -d`. Sorry, I'm really new to bash scripting. Do I need to modify `-rd$'\0'` ? If so, what to? Changing the other two to `-depth` solved my other issue. #!/bin/sh root_directory=/volume1/Stuff/Downloads/; find "$root_directory" -depth -type f -name "*.exist.ignore" -print0 | while read -rd$'\0' file; do dir="${file%/*}" [[ -depth "$dir" ]] &amp;&amp; rm -rf "$dir" done
That could be a problem. :) Seriously, just fix your shebang and it shouldn't be a problem, unless bash isn't installed at all, which would be surprising.
Thanks for all the help though, I'm going to see if there's not a wash to install bash (which I'm not sure is possible).
I did get it sorted by installing bash on my machine, but I do appreciate the effort!
In the basis set file, none will start in the first line, and each entry for each atomic symbol is unique and only appears once. If you don't mind, can you break down the commands for me? 
The subscript of an (indexed) array has arithmetic context, and in bash's arithmetic context, a number is considered base 8 if it starts with 0, base 16 if it starts with 0x, and base 10 otherwise. Since 8 and 9 are not valid digits in octal, you get an error. To fix, use 8 instead of 08, and with the command substitution, prepend `10#` to force it to be treated as decimal. files=( [8]=dawn.png [9]=dawn.png [10]=... ) ... "${files[10#$(date +%H)]}" Another alternative is to use an associative array instead. In which case, the subscript is treated as a string (key) instead of a number. declare -A files files=( [08]=dawn.png ...
Thanks for a very informative response. I will have to look more into the way bash handles data types.
Simple example: find . -type f -name '*.exist.ignore' -printf "%h\0" | xargs -0 echo rm -rf --
That is a cool idea, but my backgrounds are of a sunrise. I want them to change depending on the time of the day.
Oh, ok!
Why not create images like `08-dawn.png`, `10-morning.png`, etc? Then dump the following in `$HOME/bin/update-wallpaper`: #!/bin/bash set_wallpaper='/usr/bin/fbsetbg' image_dir="$HOME/wallpapers/" hour=$(date +"%H") for file in "$image_dir/$hour-"*; do if [[ -f "$file" ]]; then $set_wallpaper "$file" fi done Then add the following line to your crontab (`crontab -e`): */10 * * * * $HOME/bin/update-wallpaper
You should use http://xplanet.sourceforge.net/ with a cron that sets the wall paper to the rotation of the earth at that time.
You need to mount the URI to use it, no? Then you can use `-d`.
You're right I could mount it manually. But this is how network shares are referenced in Nautilus, so I was hoping to find a way to test this URI type. Everything else in the rest of the script (calls to youtube-dl, subliminal, etc) works fine with these URIs so it seems like a limitation of the test command. Is there an alternative method?
&gt; you accidentally put the `*` inside quotes. Gracias, fixed. &gt; binaries Scripts aren't binaries, and don't need to be compiled, so I don't see a problem with 'hard coding' an external binary's path. Probably overkill, but the reason it is a variable and 'easily' updated in this case is to allow for the user to use whatever wallpaper changing utility they want to use.
You're asking the wrong person, and perhaps the wrong subreddit. It's possible /r/linuxquestions would have someone who knows more about testing URIs, I'm pretty sure it isn't something bash can do, natively (though I'm often wrong). It's good to know that some tools can handle those types of URIs natively, I generally just add common network shares to `/etc/fstab` and use the unified filesystem.
Sorry, I meant commands, not binaries. Avoid hard coding the path to commands. If you insist on storing the command, use a function. Use variables to store data, functions to store code. (EDIT: I'm referring to `set_wallpaper`here) &gt; &gt; you accidentally put the * inside quotes. &gt; &gt; Gracias, fixed. Actually no, you removed the quotes instead of moving them. for file in "$image_dir/$hour-"*; do
Can you point to one of such discussions, please? I work with bash tightly for years, but I cannot understand why this code works. It looks like double expansion at the right side of expression, but it works even when there is no variables right side: $ num='a[$(echo injection &gt;&amp;2)]' ; let num++ injection $ num='a[$(echo injection &gt;&amp;2)]' ; let num+=1 injection $ num='a[$(echo injection &gt;&amp;2)]' ; let num=num+1 injection 
So you would have to edit the entire script, and it cannot be used on the old system any more. While with the function, you could make it work on both by just changing the function at the start of the script.
While I take your point, for a larger script, the script in question is 8 lines. In order to change it would mean changing a variable and adding a line, and changing the variable used by that line.
Apparently, that's [just how it works]( http://lists.gnu.org/archive/html/bug-bash/2015-02/msg00083.html). He mentions backwards compatibility as a concern. (I followed KnowsBash's link.)
The tool you need to use is `sed`. To test: sed 's/&lt;CCNUMBER VALUE="[0-9]*"\/&gt;/&lt;CCNUMBER VALUE="DELETED"\/&gt;/g' /path/to/my/file.xml | grep CCNUMBER To actually make the changes: sed -i 's/&lt;CCNUMBER VALUE="[0-9]*"\/&gt;/&lt;CCNUMBER VALUE="DELETED"\/&gt;/g' /path/to/my/file.xml EDIT: More info: sed [-i] 'instructions for sed' /file/to/process `-i` tells `sed` to execute the instructions inside the file. When you do not put `-i`, sed doesn't modify the file, and outputs the result of the instructions to your terminal instead. Instruction I used: s/oldPattern/newPattern/g * `s` means "substitute" * `oldPattern` selects what needs to be modified * `newPattern` describes with what it should be replaced * `g` means you have to do it for all occurrences in the file EDIT2: If you want to do this to all XML files located immediately inside a given folder, `cd` to this folder and execute this: for xmlFile in *.xml; do sed -i 's/&lt;CCNUMBER VALUE="[0-9]*"\/&gt;/&lt;CCNUMBER VALUE="DELETED"\/&gt;/g' "$xmlFile"; done
Thank you so, so much for the quick response. I was learning about sed last night (still pretty new to shell scripting) but I don't quite grok it yet. This is almost perfect, the only problem I have is that I am moving a whole bunch of files at once using the mv command. How can I change all the files as they are moved, or replace string across all documents in the Archive folder at once? Here's the commands I'm using (essentially): rsync -vzh --no-p --no-g --chmod=ugo-rwX /Folder1/* /Folder2/ &amp;&amp; mv /Folder1/* /Folder1/Archive/
Weird. Almost works. When I do the above without the -i it seems like it's outputting correctly, (I removed the -name because each document has a different name) but when I try to run it for real by adding -i it says "Invalid command code F" on each file. When I tried to do for xmlFile in *.xml; do sed -i 's/&lt;CCNUMBER VALUE="[0-9]*"\/&gt;/&lt;CCNUMBER VALUE="DELETED"\/&gt;/g' "$xmlFile"; done It also outputted correctly without the i, but with: I get ": invalid command code U" **Edit: Got it, in OS X you need to add a -e after -i, and put in '' after -i. 
You can use awk to get the penultimate element. You may have to tweak, but if the 3rd line prints what you want, you can uncomment the 4th. find * -type f -iname '*.exist.ignore' | while read -r line do echo $line | awk -F\/ 'sub(FS $NF,x)' #do rm -rf `echo $line | awk -F\/ 'sub(FS $NF,x)'` done 
Thanks for the echo tip, I had no idea... I usually test if certain commands fail, however, my scripts are not on a multi user environment, so when I cd to a directory, the directory is present and the user is able to access it.
Sure, there's always an excuse for doing it wrong.
I find two concerning things about that one. First is that it names the command `wifi.sh`, rather than `wifi` which you'd expect. Also since it's not an sh script, `.sh` is just plain misleading. And the script is very much OS specific, it'll probably only work with a handful of linux distributions, yet this does not appear to be mentioned anywhere.
a set -e would work there too, and is a good habit to use I think. So is using traps where you can.
* you could use a trap to send an email if the script fails * you could do something more sophisticated with rdiff-backup with incrementals
I generally recommend against using `set -e`. I has so many "surprising" side-effects; you really have to know the shell in and out to know all the "exceptions to the rule", not to mention that the behaviour changes between bash versions, making it hard to write a script that will work with bash 3.2 for instance. See [FAQ 105](http://mywiki.wooledge.org/BashFAQ/105) for some examples.
three lines ? $ git clone https://github.com/bpkg/bpkg.git $ cd bpkg $ PREFIX=$HOME/opt make install
Something like this? FILE="/path/to/file" TS_TODAY=$(date --date="$(date "+%Y-%m-%d") 00:00:00" +%s) if [ -f "$FILE" ]; then TS_FILE=$(stat --format="%Y" "$FILE") if [ "$TS_FILE" -ge "$TS_TODAY" ]; then # file has been modified today # exit script exit 0 fi fi touch "$FILE" # Rest of script
Thanks. The challenge is I want to run the script iff it is older than today. If it was run at 11:59 pm yesterday, then running it at 12:01 am today is fine. If it was run at 12:01 am today, then I still do not want to run it at 11:59 pm today. The earlier logic provided to set the time to 12:00am today and then check if the file is older does the trick much better. The code does not work for me as-is because the script needs to run on Mac OS X but it did give me something to work on. Edit - The iff is not a typo. In our college days, it was a shortcut for "if and only if"
This too is a very neat approach. Thanks. 
 $ help builtin builtin: builtin [shell-builtin [arg ...]] Execute shell builtins. Execute SHELL-BUILTIN with arguments ARGs without performing command lookup. This is useful when you wish to reimplement a shell builtin as a shell function, but need to execute the builtin within the function. Exit Status: Returns the exit status of SHELL-BUILTIN, or false if SHELL-BUILTIN is not a shell builtin.. 
Use Perl and run the stat() system call. That's what I've always had to do in Solaris, and then I giggled merrily when I discovered the stat command in Linux. 
Oh, that looks great, but what about the line there makes the command relative to the file? I'm not sure I'm wrapping my head around it correctly.
Better solution: I just checked the `find` manpage and saw it has `-execdir`, which runs the command “from the subdirectory containing the matched file”. find main_folder/ -name '*.wav' -execdir mv {} .. \;
Not sure I understand the reason you would print that 'the file is missing from the directory if the log file exists' but here is a skeleton of the beginning. d=$1 log="$d/.logfile" [[ -n $d ]] || { echo "Usage:"; exit 1; } [[ -d $d ]] || { echo "dirlog $d"; exit 1; } if [[ -f $log ]]; then echo "exists" else echo "creating for $d" fi After that, you can check diffs between the file and directory contents a number of ways. Here are a few to go on. Please keep in mind you might need to sort/uniq these items * issue a diff command (or comm); report if no differences diff &lt;(cat .logfile) &lt;(ls) * iterate through the file (using a while read loop) checking existence (-e) for each file. after, you can recreate the file by redirecting an ls ls &gt; .logfile Hope this helps a little bit. EDIT: formatting, english
&gt; these Ok so i got the first part a couple of hours ago but now i am stuck on creating the logfile if it already exists. do you think you could help me with that I know that It may seem that i no nothing at this point but my professor really didnt teach me and i cant really understand from just googling it.
alright so i found the diff command but i am not sure how to incorporate it. so far i have this: [[ -n $1 ]] || { echo "usage: dirlog_name \n";exit 1; } [[ -d $d ]] || { echo "dirlog $d; exit 1; } if [[ -f $log ]];then echo "exists" cp $1 $1.logfile2 if diff $3.logfile $1.logfile2 echo "match found" else echo "unable to find match" 
&gt; first part a couple of hours ago but now i am stuck on creating the logfile if it already exists. do you think you co i meant to say how to compare the argument that the user put in with the file that already exists
numbers that start with 0 are interpreted as octal, you can force base 10 by prefixing them with `10#` like `echo $((10#09))`
Just a quick suggestion. Type four or more spaces before you line to get the code block. Trying to read your post is very difficult due to formatting. This is a code block
 find . -type f -printf "%Td/%Tm/%TY %f\n"
If you’re using Firefox, you can open the Developer Tools (F12), then go to the Network tab, load the page when you clock in/out, select the right request, and “Copy as cURL” from the context menu. Perhaps that can help you to get going? (Though it’s probably not enough – you’ll probably need some kind of token.)
This is a good place to start: http://curl.haxx.se/docs/httpscripting.html#Forms_explained Your form looks a bit more complex than just a straight form submission... there are a few hidden fields, some javascript, and a session cookie, so you will probably need to handle these things as well. If you have some programming experience you may want to look into scraping libraries like [Scrapy](https://github.com/FriendsOfPHP/Goutte) (Python) or [Goutte](https://github.com/FriendsOfPHP/Goutte) (PHP), or possibly even browser automation framework like [Selenium](http://www.seleniumhq.org/).
Oh wow thanks. Yeah it is using a session cookie. This is a solid place to start. Cheers.
I did this at my job using curl with `--data` and extracted the post fields using the network tab in firefox developer tools or chrome developer tools (basically the same functionality). The tricky part about doing this is that in order to see what fields are needed, you need to actually clock in, and to test it I had to wait until the next day. (I didn't want to run a bunch of successful clock-ins in a row or they would wonder what I was doing). One thing you might have to do is hit the clock in page first and store the cookies, then when you POST to the actual clock-in action url, use the cookies: curl -c ${HOME}/tmp/cookies.txt "https://timeandpay.payrollservers.us/WebClock/" \ --data "txtUserName=your_usernametxtPassword=yourpass&amp;action=login" Somthing like that (make adjustments based on the POST parameters you see when you login looking at the network tab in developer tools) Then to actually punch in: curl -b ${HOME}/tmp/cookies.txt "https://timeandpay.payrollservers.us/WebClock/" \ --data "clockIn=true" Again, that's an example, you'll have to check the actual post parameters. Also note that `--data` expects url-encoded data. If you want to pass the plain text values, check out curl (greater than version 7) `--data-urlencode`. **EDIT:** I found the script I made way back for this when we used something called Insperity Timestar for our timeclock and payroll stuff. Here it is in case it helps. Apparently the window_id param didn't really matter and I don't actually know what it did in the application but I left them in just in case as you can see below. You can also see that in this case, the params could be sent in the url itself as GET params. Security be damned. At least it was over https. #!/bin/bash username="my_username" password="p4ssw0rd" base_url="https://www.insperitytimestar.com/redacted_company_name" punch() { category="$1" rm -f /tmp/cookies.txt curl "${base_url}/pages/php/login.php?window_id=382594075&amp;process_id=1&amp;=&amp;action=login&amp;password=${password}&amp;username=${username}&amp;window_id=382594075" \ -c /tmp/cookies.txt curl -b /tmp/cookies.txt \ "${base_url}/pages/php/clock.php?window_id=382594075&amp;process_id=8&amp;=&amp;action=process&amp;category=${category}&amp;comment=&amp;default_org=1&amp;dst_flag=1&amp;stay_logged_in=true" \ -c /tmp/cookies.txt } case "$1" in 'in') category="IND";; 'out') category="OUT";; *) echo -ne "\n\nUsage $0 &lt;in|out&gt;\n\n"; exit 1;; esac punch $category I kept this in my ${PATH} so when I got to work in the morning, from a command line I could just type: punch in Or when I left: punch out 
I like to start with wireshark. Sniff the connection and you can see exactly what's getting sent to the site.
This worked perfectly, thank you!
That is a pretty cool way of doing it. Thanks.
How about in_mysql_section= in_client_section= while read line; do case "$line" in "[mysql]" )) in_mysql_section=yes ;; "[client]" )) in_client_section=yes ;; "["* )) in_mysql_section= in_client_section= ;; socket=* )) sock="${line#socket=}" [[ "$in_mysql_section" ]] &amp;&amp; sql_sock="${sock}" if [[ "$in_client_section" ]]; then printf "socket=%s\n" "$mysql_sock" continue fi ;; esac printf "%s" "$line" done &lt;config
A few style things, nothing high-level: 1. Put a space between the pound sign and your comments if they're prose. It helps to separate out "this is something I'm saying" from "this is code we're not using". 2. Use `-e` in your hashbang to automatically exit on failures and you'll save yourself a lot of trouble. `-x` is also helpful when debugging, and `set -o pipefail` extends `-e` in useful ways. 3. [Don't use `function foo()`]( http://mywiki.wooledge.org/BashPitfalls#function_foo.28.29). 4. [Quote everything]( http://mywiki.wooledge.org/Quotes). Now, do you have anything you were looking at and were unhappy about, or just felt like there was probably something somewhere that could be better? Edit: also `rmdir`.
The sidebar of /r/bash has a link to shellcheck, which is a web site where you can run the shellcheck program against your script. Shellcheck is also a program you can install on your system. Fedora, at least, has it in the standard repositories. Its output will guide you to improving your script and help you learn with your own code as examples. Your code is mostly well-formed. I would replace the [ test ] with [[ test ]] and be sure to quote your positional parameters: "$1" "$2" instead of $1 and $2. Pretty much any variable with values that may contain spaces should be quoted. 
Yeah, I get what you're saying. But I literally spent several days trying to get a python script to do everything I wanted this script to do and I never got python to play well with yum and installing these several programs or forcing updates. I'd love to be able to do it since that was my first instinct, but bash was so much simpler when it came to this sort of thing. The script was working except I tried to do mysqldump on one of the systems and it acted like it wasn't configured correctly. So I'm having to add this configuration feature to my install script since it is a one time change and not something I'll be running every week.
The double `))` should just be a single `)`.
it's a pretty good damn script for a beginner, and I don't say that often congrats! one idea which may help, consider having a function `echo` the actual ratio and capture it with $() instead of checking it against each possibility with booleans, that might simplify a few things. oh and be aware that bash arithmetic doesn't handle floating point values, only integers so you division might not work as you expect.
If you have any external dependencies, like identify, make a function to test for their existence. Your functions should return 0, and your 'main' function (that you need) should check for any non-zero returns and handle them. The whole script could sum all the return vals and return that number (should be zero). You can install shellcheck locally. I highly recommend it. 
EDIT: I've updated the original post. Oh apologies. The syntax is while read -r line do .... done &lt; filename So you may want to use a variable to store the filename. config=path/to/config ... while read -r line do ... done &lt; "$config" Regarding your client query: [[ $in_mysql &amp;&amp; $line = socket\=* ]] &amp;&amp; socket=${line#*=} [[ $in_client &amp;&amp; $line = socket\=* ]] &amp;&amp; line="socket=$socket" The first line tests if we're inside the mysqld (should probably rename the in_mysql variable to in_mysqld) section and if the line begins with 'socket='. `socket=${line#*=}` (called Parameter Expansion) then strips off 'socket=' from the beginning of `$line` and stores the value in the variable `socket`. The second line tests if we're inside the client block and if the line starts with socket= and then it overwrites the line with 'socket=$socket' i.e. the previously saved socket variable from the mysqld block. You could test if the values are different if you wanted to but I found this method simpler.
Lots of problems here. I think it's better to just create a directory as needed rather than deleting them after the fact. If you must keep deleteEmptyDir, for God's sake, don't use the output of ls. That's just begging for trouble. You may as well just use rmdir -i. Keeping the widths and heights in separate arrays is just asking for them to get out of sync. Plus you have to explicitly iterate, which is just asking to forget the correct bounds of the array. Better, I think: sizes=(16x9 16x10 5x4 4x3) for file in *.whatever; do for size in sizes; do width="${size%x*}" height="${size#*x}" [...] done done If all you care is whether something succeeded, it's redundant to explicitly check the return code. Just do if AspectRatioDetector "$width" "$height"; then Oh! Another nice advantage of keeping the sizes in an array that way is just doing mv "$file" "$size"/, which I think is a little nicer. EDIT: `identify` can easily return both sizes in one call; just use -format "%h %w" or something like that. Also, it's bash, not sh, so use [[ instead of [ because it is so much safer.
 mkfifo fifo cat fifo | A | B &gt; fifo
On Solaris I've seen `bash` in any of half a dozen places, so I've been using the `env` way since my junior sysadmin days. Apart from a couple of ancient, never used unices, `env` is pretty much guaranteed to be in `/usr/bin`, so using `#!/usr/bin/env bash` is a reasonably balanced portability move. Some caveats: AFAI-Remember you can't feed it flags, so this won't work: #!/usr/bin/env bash -x But you could immediately follow the shebang line with `set -x` or launch the script with `bash -x scriptname` The other caveat is that `env` will go through your `$PATH` and use the first instance of `bash` it finds. So if you have, say, bash 2.4 in `~/bin`, and bash 4.3 in `/usr/local/bin` (I'll just throw that Solarisism out there), `env` may wind up loading 2.4 when you want it to load 4.3. Realistic odds of this happening in the `bash` world is next to nil, this is more a concern for the use of `env` for launching `perl` or `python` where having multiple versions is not uncommon. There is another caveat - a potential race condition, but this pops up with the use of `setuid`. Don't use `setuid` (as you shouldn't), and you won't have a problem.
You should replace `which touch` $1 with either command touch "$1" or simply &gt; "$1" 
certainly interesting reads! will take them into account. thanks again. 
Yes making a backup, processing the backup and using redirection to overwrite the original file is how I would use it. Although perhaps you want `&gt;` instead of `&gt;&gt;` as `&gt;&gt;` appends?
Yeah, I figured out that &gt;&gt; appends when I wound up with a 6,000 line long confit file. I've been practicing this on a copy of the file in my home dir, and everything is working properly. But when I replace the file names with the actual path to the /etc/my.cnf, I get permission denied. I haven't been able to get it to work around that either. Is there a directory other than my home folder that my script can copy the file to, run like it should, and mv the end result back to /etc?
really nice the print format :) for those who search the documentation about it, it's worth to note that it's from [ImageMagick](http://www.imagemagick.org/)'s escape code [format](http://www.imagemagick.org/script/escape.php) and not [GraphicsMagick](http://www.graphicsmagick.org/)'s [format](http://www.graphicsmagick.org/GraphicsMagick.html#details-format), which is the default man page for identify. For ubuntu, there's a change from [precise](http://manpages.ubuntu.com/manpages/precise/man1/identify.1.html) to [trusty](http://manpages.ubuntu.com/manpages/trusty/man1/identify.1.html), and cygwin default's to GraphicsMagick, your distribution may vary. I read a [few articles](https://www.google.com/search?num=100&amp;newwindow=1&amp;q=imagemagick+vs+graphicsmagick+2014) about the differences between the two, and to sum it up quickly, although they provides essentially the same functions, they're complementary tools, ImageMagick seems the more complete for the options while GraphicsMagick is faster. But make your own opinion.
 while read -r name do cp -r "$name" /other/harddrive/ done &lt; list_of_names
 find . -type d -name $name -exec cp -rv {} /cygdrive/d \; ;)
I think your cat just walked across the keyboard there. 
:D I made a mistake, i misunderstood the way the directories were ordered. nothing bad, but it may fail to find correctly directories like 5000 in "./5001-6000/5001-5100/5000" instead of "4001-5000/4901-5000/5000". Use this version instead if it is the case: perl -nle '$t=int(($_-1)/1000); $h=int(($_-1)/100); printf qq!cp -rv "%04d-%04d/%04d-%04d/%04d" "/cygdrive/d/"\n!, $t*1000+1, ($t+1)*1000, $h*100+1, ($h+1)*100, $_;' input_file If you want to have a cleaner version of a oneliner, you can always ask perl to make it a little better by adding `-MO=Deparse` or `-MO=Deparse,-p,-q,-sCT`. $ perl -MO=Deparse -nle '$t=int(($_-1)/1000); $h=int(($_-1)/100); printf qq!cp -rv "%04d-%04d/%04d-%04d/%04d" "/cygdrive/d/"\n!, $t*1000+1, ($t+1)*1000, $h*100+1, ($h+1)*100, $_;' input_file BEGIN { $/ = "\n"; $\ = "\n"; } LINE: while (defined($_ = &lt;ARGV&gt;)) { chomp $_; $t = int(($_ - 1) / 1000); $h = int(($_ - 1) / 100); printf qq[cp -rv "%04d-%04d/%04d-%04d/%04d" "/cygdrive/d/"\n], $t * 1000 + 1, ($t + 1) * 1000, $h * 100 + 1, ($h + 1) * 100, $_; } -e syntax OK
You can get rid of the `ls` by using `for file in +([[:digit:]])-+([[:alnum:]]).txt; do`. You can use `ticketNum=${file%%-*}` instead of the `echo | cut`. That syntax is called **Parameter Expansion**. http://mywiki.wooledge.org/BashFAQ/100 has some more examples/explanations. I'm a bit confused; you run `grep -i TODO` on each file initially, if `TODO` is present, you then search for `DONE`, if `DONE` is not found you add it to the `notdonefiles` variable. Then you do: for f in $notdonefiles ; do ... for d in `grep -i TODO $f` ; do you're running `grep -i TODO` on the files again even though you already know they have `TODO` in them?
The second grep -i TODO is to print out all the TODO items from the file. I'll look at your replacement for echo | cut, that would be a full call out to remove. Thanks.
It's in the main for loop, the whole script takes ~3.5 seconds, the main for loop takes just over 3 of those seconds..
Thanks for the link, some good stuff there.
No script one-liner, using sox: bpm=60; play -n -c1 synth 0.001 sine 1000 pad `awk "BEGIN { print 60/$bpm -.001 }"` repeat 9999999 
I'd just like to interject for moment. What you're refering to as Linux, is in fact, GNU/Linux, or as I've recently taken to calling it, GNU plus Linux. Linux is not an operating system unto itself, but rather another free component of a fully functioning GNU system made useful by the GNU corelibs, shell utilities and vital system components comprising a full OS as defined by POSIX. Many computer users run a modified version of the GNU system every day, without realizing it. Through a peculiar turn of events, the version of GNU which is widely used today is often called Linux, and many of its users are not aware that it is basically the GNU system, developed by the GNU Project. There really is a Linux, and these people are using it, but it is just a part of the system they use. Linux is the kernel: the program in the system that allocates the machine's resources to the other programs that you run. The kernel is an essential part of an operating system, but useless by itself; it can only function in the context of a complete operating system. Linux is normally used in combination with the GNU operating system: the whole system is basically GNU with Linux added, or GNU/Linux. All the so-called Linux distributions are really distributions of GNU/Linux!
Don't do this: DIR=$( cd "$( dirname "${BASH_SOURCE[0]}" )" &amp;&amp; pwd ) It is always a bad idea: http://mywiki.wooledge.org/BashFAQ/028 Why bother with the function Play? Also, I thought I read somewhere that `sleep` doesn't guarantee any exact timing, but I can't find my source for that, so you may be okay with sleep. On the other hand, it may be slightly variable.
You could try using `printf %s "$PWD"` instead of the `echo $()`. `sed` reads its input a line at a time and the newline is stripped, so you cannot replace newlines like that. (You'd need to use a loop and append each line to the hold space, `tr` is simpler `tr '\n' ' '`)
Adding -n solved the issue... Thanks a ton. Here is the final command... bind '"\e[24~":"echo -n $(pwd) | xclip -selection clipboard \n"'
Using printf also worked. Here is the command. bind '"\e[24~":"printf $(pwd) | xclip -selection clipboard \n"'
The console method is likely to be even simpler:- cp -a $(pwd) mynewdirectory
What am trying to do is copy the current working directory path (pwd) to clipboard. Not to copy the current directory to another location.
That's great, now let me explain a bit of what's going on there so you can improve your shell-fu as well. echo -n $(pwd) | xclip -selection clipboard Here we have three commands, `echo`, `pwd` and `xclip`; the `$()` is known as "Command Substitution" and when executed it replaces itself with the output (stdout) of the command it contains, in this case `pwd` which will reply back with the current directory and that is what `echo` receives and prints back. As a general rule the less commands you have the better it is, most of the time it doesn't matter but if you ever do anything that requires some performance you'll see huge gains from removing commands so it's a good thing to try at least. In this case you have `echo` which prints to stdout the parameters given and `pwd` that prints the current directory, since you're passing the output of `pwd` to `echo` you could've discarded `echo` entirely since it's not doing anything by itself (but you would have the newline back). I mentioned the `$PWD` variable before, this is a special variable set by the shell and it contains the current directory same as the output of `pwd` so another alternative would be to discard the `pwd` command and use this variable instead, in that case it'll look like: echo -n "$PWD" | xclip -selection clipboard This is better because now we're executing just two commands instead of three and we've also removed the command substitution which has some overhead of its own. The `|` is known as "Pipe" and it connects the stdout of the command on the left to the stdin of the command on the right, there's other ways to interact and connect those streams that the programs use in this case the one that could help is known as a "Here String" and it looks like this: xclip -selection clipboard &lt;&lt;&lt;$PWD What the here string does is to take a variable/parameter and pass it to the stdin of the command it's attached to, with this we got rid of `echo` and the pipe as well, now the command is the most efficient. As mentioned before, in this case there's absolutely no reason to optimize I'm just showing you some alternatives but your solution is perfectly ok. We didn't mentioned `bind` yet, this command is part of the `readline` toolkit and in the form you're using it what it does is to insert a given string to the commandline when you press a certain keystroke, I've noticed that you added `\n` so the string inserted is immediately executed but you should know that there's a way for `bind` to do this for you, it's with the -x modifier which tells it to execute the given string instead of inserting it on the current shell. The final version then could be something like this: bind -x '"\e[24~":"xclip -selection clipboard &lt;&lt;&lt;$PWD"'
Thank you so much for taking the time to explain. You truly are a master! Although I just hacked it together and got it working, what you said absolutely make sense and is the perfect solution and its clear now. Cheers.
I just tried out the command bind -x '"\e[24~":"xclip -selection clipboard &lt;&lt;&lt;$PWD"' in my bash shell and it gave me this error... bash: bash_execute_unix_command: cannot find keymap for command. Am i missing anything..?
What happens if you remove the `sleep 30` and `exit`?
You might want to try formatting your post properly. Lines starting with 4 spaces will be treated as code: An example Of Formatting If you want to store the output of a command in a variable you can use `variable=$(command)` e.g. in your post you have `id="echo "$x" | jq -r ._id"` which would be rewritten as `id=$(echo "$x" | jq -r ._id)`. You cannot have whitespace around the `=`. To get the value of a variable you prepend it with `$` so `$scroll_id = http ...` is incorrect as `$scroll_id` is expanded to the value of `scroll_id` and there is also the problem with the whitespace around `=`. The correct form would be `scroll_id=$(your command here)`.
Change "sleep 30" to "expect eof". Increase timeout before this as needed.
That looks like the right direction, it seems like timeout sets the duration before it actually starts a transfer and then also specifies the duration of the transfer? Do you know of anyway to force the transfer to start once the connection is made and continue until it's finished?
Ah, that indicates that an expect command is not matching, but timing out. Do you just have one yes/no prompt? If so, then lose the exp_continue. That is causing it the expect command to start over and look for another yes/no. And if you don't want a timeout when waiting for the transfer to complete, you can use exp_wait instead of expect eof. 
After the first run against a specific server I won't actually have the yes/no prompt at all, so that's only in there just in case. After removing the expect yes/no entirely it seems that the transfer is starting instantly, though swapping expect eof to exp_wait looks like it's freezing the whole thing-- won't actually let me exit out of the script once it starts either. Will look into exp_wait and see if there's anything else I need to do to get it going. 
I tried unsetting and even tried on a different machine but got the same error. I am on CentOS release 6.5 (Final) and GNU bash, version 4.1.2(1)-release (x86_64-redhat-linux-gnu) if that helps understand the issue any better. 
Meh... I think you're getting ahead of yourself.
It's only ridiculous when you consider that it's not ubiquitous and you're forced to copy/paste into an HTML email. 
i am asking for a sample because i think the parsing can be made tidier, not to mention that i'd probably move removing the garbage outside the loop so it's not spawning 2 processes per line. Without a sample i cannot give it a spin. Imo it could look like this while read -a array do .... done &lt; &lt;( sed ... file | tr .... ) `sed | tr` possibly replaced with something better btw while ...; do ....; done &lt; file `&lt; &lt;(cat file)` is a very roundabout way of doing a simple thing.
Windy, for Harry Lucy Beale snog Barbecue Big quid my bollocks.
Eval will work, alternatively you could try indirect expansion: var1="this/is/a/path/first" var2="this/is/a/path/second" ref=var1 echo "${!ref}" ref=var2 echo "${!ref}" STDOUT&gt;&gt; this/is/a/path/first STDOUT&gt;&gt; this/is/a/path/second
Thanks for the optimizations, I found the actual problem though. It seems that "IFS=', '" was inhibiting each number from being read into its own array element. I removed it completely and it worked on both systems. Thanks for the help anyway.
Yeah, I didn't want disclose some of that code otherwise I would show the purpose of the script which I didn't want to do. But I had debugged the crap out of that script and narrowed it down to that line which was giving me problems. Since that was literally the beginning of the loop i didn't think i needed to show more than that since the array wasn't building right in the first place. However, I was able to fix the problem and you did give me good pointers on optimizing the script quite a bit so thanks for the help and in the future i will provide more information on my problem to more properly aid others in diagnosing my problem. Thanks
you can have your secrets but there is nothing that says you can't produce a few lines of your inputs with sensitive data replaced with abcdef xyz 111111, eg. john doe,, 21 jump street|130.00|aaaa|bbbb arnold governator,, castle in narnia|150.15| xoxoxoxo|ffffuuuu- If you are fighting delimiters, saying "well there are some `\t`'s, maybe `|`'s and some `,`'s" is not enough. Providing a tangible example that can be used to confirm that "i expect X but get Y" happens indeed is.
There are plenty of ways, here are two -- there are others. Use whichever one you like most. That's the nice thing about it :) $ echo "blah1.blah2.blah3.com" | cut -d '.' -f 1-2 $ echo "blah1.blah2.blah3.com" | awk 'BEGIN{FS="."} {print $1"."$2}'
I'm looking for a bash built-in solution.
It'll be part of login scripts and is a pattern that will be repeated a few times, so its important it's as fast as possible and doesn't spawn sub shells. It'll only run the hostname command if the built-in HOSTNAME variable hasn't already been set.
 myhost=$(hostname -f) myhost=${myhost%.*.*} http://mywiki.wooledge.org/BashFAQ/100
Very interesting! 
This works only if you're sure the hostname has exactly four dotted parts.
With awk you can just use -F. instead of setting the Field Separator inside of bash. e.g. awk -F. '{print $1"."$2}'
Where's the rest of your script? Pseudocode isn't helpful in this case. 
Bash is just showing you where his arguments are after parsing the line. Try: echo """" """" and you'll get `echo '' ''` -- so it didn't replace double quotes with single quotes, it just eval'ed the line, so the double quotes are really going out, and it then printed single quotes around his args, for debugging purposes, so that you know how where the args were.
Ah great explanation. Thank You.
I don't know how the frame comparison works. Is it stepping through frames? Use bc to add and subtract from the times. Maybe use awk ot grep to get the specific column or the floating point number. When you find the forst frame,save the previous 15. Make a loop where you step through the remaining frames. When you don't find anymore, save another 15.
`` &lt;-- are used to execute a command within another syntax (back ticks ) for example... NumLine=`cat temp.srv | wc -l` NumLine now equals the amount of lines in temp.srv Now... looking at quotations... We start the sed with a single ' quotation, now we want to use other quotations within these quotations to signify variables. Quotations are just one of those things that you have to play around with to understand how they are going to work in your syntax. All in all, it doesn't matter unless you are using variables within quotations and such, and its to allow bash to differentiate between quotation segments. sed -i 's/'"$VARINIIP1"'/'"$VARIP"'/' /home/ts/$VARFOLDER/ts3server.ini 
I think I might have it. If someone can still direct me to where I probably saw this, please do. Also, can I do this without a subshell ( ) around the evaluation of the variables? FewerNotLessDotCom@localhost:~$ ./foo One or more of the following variables was not set: PWD='/home/FewerNotLessDotCom' USER='FewerNotLessDotCom' MYVARIABLE='' FewerNotLessDotCom@localhost:~$ MYVARIABLE=oiehg ./foo Variables are all set; woo! FewerNotLessDotCom@localhost:~$ cat foo #!/bin/bash ( : ${HOSTNAME?:} ${USER?:} ${MYVARIABLE?:} ) 2&gt;/dev/null || { cat &lt;&lt;EOT One or more of the following variables was not set: PWD='$PWD' USER='$USER' MYVARIABLE='$MYVARIABLE' EOT exit 1 } echo "Variables are all set; woo!" FewerNotLessDotCom@localhost:~$ 
 #!/bin/bash for var in HOSTNAME USER MYVARIABLE; do if ! [[ -v $var ]]; then printf &gt;&amp;2 '%s is not set\n' "$var" exit 1 fi done printf 'All good!\n' No point in actually testing HOSTNAME though, since you're guaranteed that bash will set HOSTNAME for you.
Ok, I prefer to concentrate on what the goal is. Here's an even shorter one: declare -p HOSTNAME USER MYVARIABLE &gt;/dev/null || exit
 :~$ cat blah #!/bin/bash su -c 'echo $HOME' :~$ bash blah Password: /root If you use double-quotes, your variable will be interpreted before it's ran. i.e: :~$ cat blah #!/bin/bash su -c "echo $HOME" :~$ bash blah Password: /home/turnipsoup Hard to say past that; please provide more info if the above doesn't answer your question.
Are you just trying to get the last/next 15 numbers or are you correlating them with image data from the frames?
That's no Bourne script. It's a (mostly) POSIX sh script using GNU utilities.
Thanks! I reallt appreciate your help!
With the speed of today's computers, this is almost never a practical solution. 
This should work, though I haven't tested it: pushd source for file in */data/file.xml ; do mkdir -p ../temp/$(dirname ${file}) cp ${file} ../temp/$(dirname ${file}) done popd It assumes that `source` is directly underneath your current directory, and that you want `temp` to be created next to `source`. 
I would just use the :? syntax honestly. : ${HOSTNAME:?Please set the HOSTNAME variable} ${USER:?Please set the USER variable} ${MYVARIABLE:?Something went wrong oh no} The closest (visually) I can think of to your script there is: [ "${HOSTNAME+x}${USER+x}${MYVARIABLE+x}" = xxx ] || { cat &lt;&lt;EOT One or more of the following variables was not set: HOSTNAME='$HOSTNAME' USER='$USER' MYVARIABLE='$MYVARIABLE' EOT exit 1 } If you know what ${var+text} does it should be relatively simple to grok: add an x to a string for each variable that's set, ensure that there are 3 "x"s, i.e. 3 variables, which ensures that everything's set. ---- (edit: you probably don't need to read this part judging by your other comment) By the way, "exit" doesn't exit the script when used in a subshell. Test these two snippets: false || (echo whoa; exit 1) false || { echo whoa; exit 1; } Notice that the first one doesn't actually exit. Also notice that there's some extra mandatory space around the braces in the second one, and that you need either a newline or a semicolon after the last command. What (...) is doing differently is that it's running the code in a subshell, i.e. another instance of bash is running that code, and that other instance is what is exiting. Subshells when you don't expect them can be a cause of several subtle bugs, especially when dealing with while/for loops that need data piped into them.
you misused the `.`, which is the current directory. Unless you are in /, `./some/path` (relative path equal to &lt;current dir&gt;/some/path) is not the same as `/some/path` (absolute path) you could modify your $PATH to include the dir with the executable or put a symlink in some location that is already in $PATH . That way calling `blender` will be enough. 
Now that makes sense... Here is what am going to do... Have different versions of blender in /opt/blender/ 2..73; 2.74; 2.75 and so on. I will create /opt/blender/launcher and create symlinks to the blender executables of different versions over there as bl2.73; bl2.74. And add /opt/blender/launcher to the $PATH. So, when ever there is a new version available I just need to copy it to /opt/blender and add the symlink in the launcher folder. And I should be able to launch by executing different versions using a standard syntax. Instead of creating multiple aliases. Looks like a much cleaner approach. Any better way... ? Thanks for the help.
You could just grab the values from /proc. Easier to parse. awk '/^MemFree:|^SwapFree:/{print $2/1024}' /proc/meminfo
I know that's what you meant, but I decided to show you a better way to extract those values. Anyway, to delete lines 1 and 3 with sed, it's `sed '1d;3d'`, but it's silly to do that only to pass it on to awk which can easily do the exact same operation.
sed accepts a range so the simplest thing to do will be to combine them and we get: `free -m | sed '1,2d' | awk '{print $3}'` But awk handles lines as well. So, why not do it in awk itself? And we get `free -m | awk 'NR&gt;2 {print $3}'`
Thanks for the help. My understanding of awk and sed are limited. I am sure you are right but this way is clearer in my thought process.
Or: free -m | awk '/^Mem:/{print $3}'
It's ok as long as you are learning, but chaining the sed calls is like opening a document with MS Word, editing a paragraph, saving and closing, then opening it again, editing a second paragraph, saving and closing etc. Very inefficient, of course. sed is not as long to start as MS Word, but there's still a cost, and splitting the processing between several chained calls can dramatically increase the total execution time. So, the general *NIX logic is: once you have started a process (sed, awk, whatever), do as much work as you can in that process before piping to another one.
Ok, thanks for all these options. I will need tidy up the code now that I understand better. I wrote a blocklet for my I3blocks to show how much memory I am using.
Thanks for noticing. I've missed the fact that the first line is removed first and decided the guy wanted Swap (though I wondered why everyone uses Mem)
No, I wanted the 2 line. Men used and total memory. The original statement worked correctly, I just was inefficient.
Besides I would rather have Linux use all available ram to increase performance as much as possible. Unused resources are just wasted right.
i actually had read this before but thanks for the reminder.
Presumably so that you can do path building easily.
Whoops, yes. Fixed - thanks! (Too much copy-paste.) 
one thing i find weird: normally 'ls -d *' outputs in the form 'dir1 dir2 ...' (without '/' at the end), one can use 'ls -F' instead to display directories that way - like in your last lines. but when i now do 'ls -dF \*/' it outputs 'dir1// dir2// ...'. is it really supposed to work that way? that the -F option just blindly appends a '/' to directories, regardless of it already being displayed? one might think that because of the '*/' expansion ls thinks that '/' is actually part of the name of the directories. which is weird because this cannot happen.
&gt; For simplicity, let's say I want to print the previous index value.... I realize you've solved this already, but here's an answer anyway since it's quite simple: prev= for file in ./*; do echo "$prev → $file" prev=$file done
You can just remove the dot from your command and it will work... Bash will look for a command in the PATH, unless it has a slash in it in which case it will look for that file. That's where the "./" idiom comes from in this case, it's just a way of getting a slash into there.
Whenever I'm manipulating numbers in bash, I use the 'let' command c=0 while $keepGoing; do let c=$c+1 echo "c = $c" done 
Try turning off compression. I'm far from certain that this will help, but some versions of rsync will hang arbitrarily if you don't disable compression. 
I too recently thought about OOP in bash, and came up with this [project](https://github.com/kjkuan/bashoo). It certainly isn't the first attempt at OOP in bash, but I couldn't find one approach that's to my liking..., so, there it is~
Thanks, I didn't know about that command. Neatly does away with my `ps` parsing/formatting. I'll edit it into my OP as well.
Great, glad you figured it out :) Sometimes all it takes is messing with the switches. The version of smbtree on debian wheezy (a few years ago, it may be better now) was like that - half the switches didn't seem to work at all. 
Oddly enough, that only exists after the second error is generated, though it clearly prints a question mark! It still only prints a question mark followed by a new line, which is expected behavior, except for the fact that it doesn't return until it sees another new line, I suppose.
Which is then aliased back to `/bin/bash` in a lot of systems. They are going for more portability than anything I guess.
And there are systems where it's not linked, and bashisms will break sh. Been there, seen that.
It's trivial to change that behavior. Just remove the line: tput rc;tput el;printf "\r" # Delete the last printed line It's useful to delete the last line when you're using this inside a large script that has many steps, where you only care if the command succeeded (exit code) or not
ahh thank you. i was trying a cp with no success. 
&gt; df -h &gt; usage.txt worked perfectly 
Ironically enough... score@kirisame ~ % diff -U5 &lt;(curl https://gist.github.com/lucaswerkmeister/169b0163f623692c0154) &lt;(curl https://gist.github.com/lucaswerkmeister/169b0163f623692c0154) % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0 % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 18412 0 18412 0 0 30776 0 --:--:-- --:--:-- --:--:-- 30789 100 18412 0 18412 0 0 30833 0 --:--:-- --:--:-- --:--:-- 30892 --- /proc/self/fd/11 2015-03-21 06:42:31.626419897 +0000 +++ /proc/self/fd/12 2015-03-21 06:42:31.626419897 +0000 @@ -5,16 +5,16 @@ &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"&gt; &lt;title&gt;Watch a website and get emailed on changes&lt;/title&gt; &lt;meta content="authenticity_token" name="csrf-param" /&gt; -&lt;meta content="SdhJiVUPyn8SWqpe2LhT5RW287J5Fh1ObfFG+lqy8JZqSQyegmLlqATBDz+zmxd/DMEV7NO/36z5Ni1LfwFD5A==" name="csrf-token" /&gt; +&lt;meta content="1K5pUzEmfMED7Fc0K/5P5SmQks/wHhKYMnB5drs6zW+vbXHRNzzsUK0U+5XJka6gi8H9CNpYUqPJGb6IBCy4/A==" name="csrf-token" /&gt; &lt;meta name="viewport" content="width=960"&gt; &lt;link type="text/plain" rel="author" href="https://github.com/humans.txt" /&gt; - &lt;meta content="gist" name="octolytics-app-id" /&gt;&lt;meta content="collector.githubapp.com" name="octolytics-host" /&gt;&lt;meta content="collector-cdn.github.com" name="octolytics-script-host" /&gt;&lt;meta content="2598F977:4531:E5215B:550D12D7" name="octolytics-dimension-request_id" /&gt; + &lt;meta content="gist" name="octolytics-app-id" /&gt;&lt;meta content="collector.githubapp.com" name="octolytics-host" /&gt;&lt;meta content="collector-cdn.github.com" name="octolytics-script-host" /&gt;&lt;meta content="2598F977:4531:E5215C:550D12D7" name="octolytics-dimension-request_id" /&gt; &lt;link rel="assets" href="https://gist-assets.github.com/"&gt; &lt;link rel="zeroclipboard-assets" href="https://gist-assets.github.com/assets/zeroclipboard/dist/ZeroClipboard-aeb54c213f0d09883fed5a0947132da9.swf"&gt; &lt;link rel="editor-assets" href="https://gist-assets.github.com/assets/editor-cac2d04c8f22b5ffe38f6bc97dc54f54.js"&gt; &lt;link rel="mount-point" href="/"&gt; @@ -315,11 +315,11 @@ &lt;a href="/"&gt; &lt;span class="mega-octicon octicon-mark-github" title="GitHub "&gt;&lt;/span&gt; &lt;/a&gt; &lt;ul class="site-footer-links"&gt; - &lt;li&gt;&amp;copy; 2015 &lt;span title="0.01727s from github-fe103-cp1-prd.iad.github.net"&gt;GitHub&lt;/span&gt;, Inc.&lt;/li&gt; + &lt;li&gt;&amp;copy; 2015 &lt;span title="0.01642s from github-fe102-cp1-prd.iad.github.net"&gt;GitHub&lt;/span&gt;, Inc.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/site/terms"&gt;Terms&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/site/privacy"&gt;Privacy&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/security"&gt;Security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/contact"&gt;Contact&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; As someone who used to write automated tests for sites for a living, I know this all too well! A lot of sites seem to do this just to make scraping a pain, I swear.
While you've had your immediate question answered, can I suggest the use of -P as well? i.e. df -hP &gt; usage.txt It's a minor thing, but down the track if you want to parse the output, the -P formatting will make your life easier, not to mention portable.
thank you all for the responses, I will take them all into practice! 
Yea good stuff. Tee is awesome as well as tmux. I use it all the time.
That takes the output stream of a command and puts it into a file, I usually say "pipes to a file", where | is pipes (to another process). 
I think you can use [Xvfb](http://www.x.org/releases/X11R7.6/doc/man/man1/Xvfb.1.xhtml) for that purpose. [Here](http://stackoverflow.com/questions/14321636/fake-x-server-for-testing)'s a not-so-unrelated stackoverflow thread on the subject.
A good option is xpra. It is very simple.
I have this set up by running vncserver and setting up ~/.vnc/xstartup to launch twm and whatever else I want to launch. Here is my invocation of vncserver: vncserver :2 -geometry 1440x900 -nolisten tcp -localhost I'm on my phone so I can't get full details on how this is set up but I can try to get info later if you're interested. 
ah right, upvote!
With Vim you'll have the possibility to set up some macros and map them to shortcuts rapidly. For example, I have a setting that highlight white spaces with a shortcut and convert 'serialized' data into a bloc of text with new lines... Easy mappings and fun to fiddle with. It should be doable in emacs as well, but I found e-lisp heavy to learn compared to vim settings (no even need of scripts). In any case, Emacs and Vim have a steep learning curve but as stated already will be rewarding in the long run.
`next` is superfluous nslookup google.com | awk 'NR&gt;3 &amp;&amp; /^(Name|Address)/ {print $2}' 
You're absolutely right about that - I originally had it as `NR&gt;3` and then changed it because I was looking into doing something else and then forgot to remove it before posting. Thanks for pointing it out ;)
also | grep -v google.com should work.
Thanks for your answer. I must admit I don't understand your solution fully as you might be able to tell from my code I'm quite a beginner with this :). Do you maybe have time to explain it a bit?
This did work.
&gt; superfluous Can you explain this? 
What /u/vaphell was saying is that in my awk script I used the directive `NR&lt;3 {next}; #do something` which means "while line number is less than 3 go to the next line, then do something". The same thing can be achieved however by saying `NR&gt;3 #do something` which just means "while the line is greater than 3, do something". So, accomplishes the same end goal, but with less code and uses fewer CPU cycles.
Thanks! learned something new today.
The unescaped extra space before the final forward slash is what caused it. You ran rm -r on two directories: /Volumes/Salvador/AFLBS\ Images and then /. 
face palm* .... I believe you are right sir.
This. This is why I quote paths when they have spaces. It's way easier to see the error when the command looks like this: rm -r "/Volumes/Salvador/AFLBS Images" /
that is a good idea. Ill remember to try that next time!
FWIW find can do the empty removal thing internally: find &lt;dir&gt; -type d -empty -delete -delete implies -depth so it correctly does it from the bottom up.
Sorry, i might be noob, but what's going on here? What's wrong with the output?
Yum!
I just quit one at halliburton. Because of this stat, entry level is around $75-80k in houston...
That's written in Python, not Bash `;)`.
not familiar with LaunchAgent, but just a wild guess, maybe your script finishes running before your dialog has a chance to be shown? Maybe LaunchAgent would terminate the dialog child process as soon as its parent process(your bash script) is terminated? Try adding a `wait` command at the end of the script and see if that helps.
:)
I am just curious why this question would be down voted? Was this not the right sub to post in?
According to the man page for pwpolicy requiresMixedCase is no longer a legacy user policy for the pwpolicy command. What version of OSX are you running?
10.10.2
Just do "mv /foo/bar/* /voo/var/".
That's what I did (see above) but it although it works it also says "Invalid Argument."
You need to close your directories. mv /foo/bar/* /foo/bar is not the same as mv /foo/bar/* /foo/bar/
Yes, this is a script that will run on a schedule set by a LaunchDaemon plist... handling business-critical data, so I want to it be pretty solid. This step is basically to a file once data is imported into a database and move it into an archive so that when I receive the new data each the next day the directory contains only the current file for import. I did see some examples out there that did as you described but I was having trouble. &gt; cd /foo/bar/ &amp;&amp; find -f -maxdepth 1 . -exec mv /foo/bar/dir/ This isn't working. I don't understand where I'm going wrong. It's just listing all the files already in the .../dir/ but it isn't moving the file in /bar/ into /dir/. 
Thanks so much! I might go with your bottom solution, but can you explain a little bit how I would use exec on the results of the find?
Easiest way: shopt -s extglob mv /foo/bar/!(dir) /foo/bar/dir/ To turn extended globbing off: shopt -u extglob
Sure, it's imperfect, but conceptually it's easy to communicate. 
Fail2Ban should be standard on every sever. 
Not much, but I found it ironic that the site the script is hosted on always returns a different page (so if the script were to monitor it, it would always say it had changed).
You are correct, sir.
Cool script and all - but this seems like a backwards way to do it. Wouldn't you just whitelist IPs to TCP80 via iptables?
Thanks! I'm working on it. :) I'll keep you guys posted. One of these days I'm going to be helping out on this subreddit instead of spamming you with the basic of the basics.
Oh I was unaware your phpmyadmin was for hosting where customer(s) connect from IPs you don't know about. In that case configure your webserver to only allow access to the pages your customers need and whitelist access to specific directories/pages that only you would need. Fail2Ban is pretty awesome too, check it out
What are you trying to do exactly? Just read a file line by line and echo it to the screen?
At this point, I am going to be adding functionality right now I'm trying to set up my environment to test. By the way when I run this from a virtual machine running linux it does echo the "Done" but no data their either. I ran dos2unix on both the script and the file.txt to get rid of windows formatting, but still not getting what I'd expect. I found that it doesn't work with only one record in the file. But I don't know what I would want to do if the file only has one line in it.
Post your test data? This works fine for me: cat text.txt | while read line; do echo $line; done
Why is this so complex? Why isn't it just mv /foo/bar/*.* /foo/bar/
The first while might need the -r for read to treat back slashes as back slashes and not escape characters. while read -r p; do Also try running cat in a subshell on the stdin last line done &lt; &lt;(cat file.txt) The second while, where are you getting the $file variable? Why aren't you just echoing $line? If you are looking to run something on each line of a curl output, why not just run the while loop on the curl? done &lt; &lt;(curl http://url.to.json/file.json)
Nicely written :)
It's not, he's asking why he got the error, which is because he told bash to move /this/dir into /this/dir/here (destination is in source directory). Windows also has an error like this.
thanks i think that did it, the curl is expensive, i was using a file as a cache of data from the service while building the script then ill refactor.
Bash does substitution with variables. So the line you're having problems with: ! $FINDALL Bash will first replace $FINDALL with its value and then execute it. Not sure what it will do with the `!` since I don't normally see them outside of a conditional (although use them in readline all the time). Since your script doesn't set a value for `$FINDALL` it is depending on it being already set in the environment before the script is executed. If it isn't, it is replaced with an empty string. The line would then be: ! Which would result in a syntax error.
Ah, that makes a lot of sense. Thank you. I am still having problems, but at this point I'm not sure what they are since there aren't any error messages to be seen. I've been updating the link if you're interested in seeing what I've done to it since you last saw it. I don't know if I'm getting closer or not. Thanks again for your help.
So from what I gather, you just want to append those lines to the file in question? Why don't you just try something like this: printf "%s\n" \ "Package: *" \ "Pin: release l=Debian" \ "Pin-Priority: 110" \ &gt;&gt; "${prefer}" From what I gather, your problem is that your heredoc is flipping out about the fact that you've got tab-indented lines. Either way, if you're just appending lines to a file, cat is the wrong tool for that, and heredoc'ing is a convoluted way to do this. This is also an example of using one hit of `printf` vs multiple lines of `echo` as another reason why `printf` should be preferred where possible. Another quick tip, in your script you check that it's being run as root with `"$(id -u)"`, you can skip the need to go to an external by using a bash internal variable, i.e. `if [[ "${UID}" -eq "0" ]]; then...` (you may also like to use `"${EUID}"`, more reading [here](http://tldp.org/LDP/abs/html/internalvariables.html))
 PATHS=$(echo $PATH | sed 's/:/ /g') for path in $PATHS; do ... First of all, if your paths have space (perfectly possible as space is a perfectly cromulent char), sed is going to make them indistinguishable from separators. Second, unquoted vars mangle whitespace in them so leaving them unquoted is almost always a bad idea. Preserving data integrity in your shell scripts requires discipline and attention to detail. Second, `for var in $unquoted_var` is a wrong way to do it. Avoid that idiom and the implicit word splitting if possible bececause the bug spawning devil lives there. As a rule of thumb for loop is to be used with params and arrays. bash has arrays, use them. Also you can use `read` to split directly on `:` (though there would be a problem if you had to differenciate between `:` as delimiter and `\:` as part of path name, but methinks `:` in paths is very rare. If you had that problem you'd have to go outside of bash builtin functionality). IFS=: read -ra paths &lt;&lt;&lt; "$PATH" printf '%s\n' "paths:" "${paths[@]}" for path in "${paths[@]}"
Thanks i *am* compiling hints, things are much different for me scripting bash vs vba or javascript.
&gt; Why doesn't bash offer some parallel solution superior to the posix kludge, eg array based that would clean this shit up? Too late, everybody uses $PATH and relies on it; it would be quite confusing (and a source of bugs) to have something unexpectedly override it. In practice, the current colon-separated list works reasonably well, too (in 20 years of *NIX, it has never betrayed me).
everybody relies on PATH because there is nothing else, if there was a known feature in bash, then people would know how to check for it, use it and/or know how to safeguard against it. The point is the array based solution would be hands down superior with no cutting by hand required ever. Shell scripting is not for the faint of heart already and there is a shitton of utterly broken shit in the ancient posix people have to deal every day writing scripts. For example the implicit word spliting is an unmitigated disaster and the main source of bugs. Sure, it was necessary when your "array" was a string but if you have legit arrays and explicit ways to populate them it's a completely retarded feature. colon separated list works reasonably well, unless you have a dumb idea of using :-ridden directory name like i had. Why can't i use it when everything but / and \0 are legal chars? Such violations of transparency imo stink.
When writing shell scripts, you will want to use tput to interface with the terminfo database instead of using the escape codes. I implore[ you to read over this website](http://mywiki.wooledge.org/BashFAQ/037) when you have time. It's even linked on the sidebar.
What system are you running these commands on and what is the version of Bash you're using? phil@PhilsLaptop :~$ bash --version bash --version GNU bash, version 4.3.11(1)-release (x86_64-pc-linux-gnu) Copyright (C) 2013 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt; On my Ubuntu 14.04 system I get the following which are correct. phil@PhilsLaptop :~$ stringx=abCf12FG phil@PhilsLaptop :~$ echo ${stringx^} AbCf12FG phil@PhilsLaptop :~$ echo ${stringx^^} ABCF12FG 
http://stackoverflow.com/questions/14661373/how-to-escape-colon-in-path-on-unix http://apple.stackexchange.com/questions/53003/how-to-add-a-directory-to-my-path-that-includes-a-colon-and-a-space Looks like you're out of luck. Don't use colons in directory names. bash does kinda sorta offer a parallel solution. For example, you can do something like: /usr/{,local/}bin/sudo somecommand et voila: you're now portable across Linux (/usr/bin/sudo) and Solaris (/usr/local/bin/sudo)
&gt; Looks like you're out of luck. Don't use colons in directory names. lol, that's lame. I assumed there is some escaping mechanism, but reading the SO link tells me there is none. God damn it, there are so many ugly warts in the ancient posix, when is the service pack to the standard coming out? ;)
other notes: jq Json parser https://jqplay.org/
While it would be a nice little project to try to come up with a bunch of bash scripts to scratch your itch, I'd recommend you have a look at [Ansible](http://www.ansible.com/get-started).
You would need to spawn ssh from expect. I would probably put the expect script in a separate file and invoke expect from my bash script. There are plenty of examples of using expect to handle ssh connections. Something like this would handle the login part. set timeout 60 spawn ssh [lindex $argv 0] "bash -s" &lt; [lindex $argv 1] expect "yes/no" { send "yes\r" expect "*?assword" { send "mypass\r" } } "*?assword" { send "mypass\r" } Which you would then invoke from your bash script with the hostname and script to pass as arguments. Note: I didn't actually test the expect portion.. I would find a way to do it correctly without expect, personally. :) Even if it meant using an expect script to distribute an ssh key. Oh the irony. As for timeouts, I'd probably just pass some ssh flags that reduce the connection timeout wait, and test the output to omit it if that was relevant. If I was executing on a ton of hosts, I'd also probably fork the execution. I used a mass-forked ssh + bash -s combo to routinely do things on around 2k hosts.
`for x in $var` and `for x in $(cmd)` are bad bad idioms. For loop should be used with enumerables (eg inlined list of elts, params $@, arrays ${arr[@]} or any combination of them). When you work with a file or a command output, especially with a newline delimiter in general you should opt for `while read ...; do ...; done &lt; file`. Also why spawn `cat` and `sed` if you can trivially strip \` in bash? $ while read -r line; do echo "${line//\`}"; done &lt;&lt;&lt; $'`aaa`\n`bbb`\n`ccc' aaa bbb ccc 
while read is definitely the better / correct way of doing things, however for simple use, I find novice scripters struggle grasping the "input at the end", that makes it hard for them to visualize the execution of the loop. /shrug. As for the sed... Yeah. That's just the dangers of reddit pre-morning tea. :)
* Find the name of all zip files located in the `/home/beanz/downloads` directory: find /home/beanz/downloads -type f -name "*.zip" | while read line; do echo "${line%\.zip}"; done * Filter through this to only display zip files whose name begins with "A": find /home/beanz/downloads -type f -name "A*.zip" | while read line; do echo "${line%\.zip}"; done * Append something on a new line in an existing file: echo "something" &gt;&gt; /home/beanz/existing.txt * Create a new file containing something: echo "something" &gt; /home/beanz/newFile.txt
the problem is that noobs should learn the kosher way right off the bat. Wanna bet most noobs will then use that easy straightforward idiom for everything, including sorting their mp3 directory and what not? Whitespace issues are nasty and potentially disastrous when you are dealing eg with the full spectrum of allowed filename chars in your sensitive file collection. It's hard to do it right with for loop without going into detail how shell works under the hood, IFS, word splitting and all (in act it's flat out impossible to cover parsing file lists accounting for all legal chars without the null delimiter, which is only available in case of `while read`). In the end it takes more know-how to be correct exlusively with for loop than to say "for loop for this, while read loop for that". One could also mention `readarray`, which creates the array on a per line basis, which then can be used with `for loop` as god intended. readarray -t arr &lt; file readarray -t arr &lt; &lt;( cmd ) for elem in "${arr[@]}" The internet is chock full of halfassed snippets and that shit is harmful in the long term when it appears at the top of googlezor results, perpetuating the cycle of bash abuse.
Since the name of the zip file changes and all you know is its extension, you can use a glob (assuming there will be only one zip file in the folder). zipfile=*.zip When bash (or any other shell) assigns a value to the variable, it first expands the glob (*.zip) to all filenames that match its pattern. In this case, that should be the one and only zip file in your folder. Assuming your json file has \"key\":value pairs and you know what key you need to modify, you can use sed to modify the file in-place. sed -i -e 's/some-key.*zip/some-key\":'$zipfile'/' some-file.json tweaking the regex to match all the necessary text to be replaced. 
Thanks for the tip on using `command`, that's exactly what I was looking for. I've updated the post to use command instead and quote the commands.
Wow... sshkeys isn't an option? That sucks. What you want to do is have two scripts: a bash wrapper script that does all the interaction with you and it manages the expect script. I was forced to do this very thing a few years ago, so I have some scripts on-hand to reference. One approach that I take is that I do not store passwords in either script. EVER. Instead, you want the shell script to read your password (twice and compare for sanity), put it into a file, and chmod 600 it. At the very end of the run, after expect is all done, the shell script removes the password file. You should also trap for that. What this does is gives it some basic level of security, and only the most prying eyes (root, angry colleagues with sudo) will be able to see it. Here's a relevant snip of one of my expect scripts: # Check for the .pwdfiler file if {[file exists .pwdfiler] == 1} { # Reads admin password from file '.pwdfiler' which is set to chmod 600 # Slightly more secure this way set filer [open "/home/$whoami/bin/.pwdfiler" r] set adminpass [read $filer] } else { send_user -- "I could not find the required file .pwdfiler\n" exit 1 } Voila. Your password is now an expect variable called $adminpass You will probably want the expect script to spawn an scp of the script that you want to run, then ssh in and do the rest. &gt;I'd like to have the script to move on to the next server if it gets a timeout on connection. You want to set a default action (and you should make this a habit: after every expect clause should be a default action), another snip of code for you: spawn ssh -q -o StrictHostKeyChecking=no -t $server expect { # Set the default action default { send_user "INFO: Unable to make connection to $server\n" exit 1 } By default, expect will timeout after 10 seconds and hit the default action. If you want to give a more specific actions for timeout, eof that are different from the default action, then you can do that by setting your timeout value, and giving a timeout action e.g. set timeout 4 spawn ssh -q -o StrictHostKeyChecking=no -t $server expect { # Set the default action default { send_user "INFO: Unable to make connection to $server\n" exit 1 } # Set the timeout action timeout { send_user "INFO: Timed out while making a connection to $server\n" exit 1 } # Set the eof action eof { send_user "INFO: EOF received on $server\n" exit 1 } Because the wrapper script is running expect in a loop, when an instance exit 1's, the wrapper script simply moves on in its loop to the next host in your hostlist file. And there you have the desired behaviour that you want.
What I ended up doing was capturing the output of filename=$(ls|grep *.zip) sed -i -e "s/FileNameHere/$filename/" thefile.json and it worked just fine! Thank you for pointing out a sample for SED, I'm going to be looking more into this!
I believe the ls | grep method will be a bit more intensive, though probably at this scale doesn't matter much. Just for sake of discussion, you can also do something along the lines of this: # grab the file name and ditch the path filename=$(basename "$fullfile") # grab the extension only from the filename variable we just set extension="${filename##*.}" # grab the filename itself minus the extension from the filename variable we set (more applicable to this instance) filename="${filename%.*}"
&gt; filename=$(ls|grep *.zip) Don't do the above. It's wrong in several different ways and can easily break. Instead use what /u/sbicknel suggested: filename=*.zip This works fine, as long as you're sure there's only one .zip file in the directory. If you want to guarantee to just catch one zip file, in case there might be more, do: set -- *.zip filename=$1 
Hmm, that might do what I need. I'll play with it when I'm back on my Linux machine tomorrow.
&gt; I know about rsync --progress, but I wanted an overall progress that I could neatly keep on one line that updates. Newer versions of `rsync` have the `--info` flag which has an option to only display overall progress: `--info=progress2`. If you don't have access to the latest version of `rsync` /u/somidscr21's suggestion will also work. Just put `pv` between input and output instances of `tar`, `cpio`, etc. I tested this, but not thoroughly: tar cpC srcdir . | pv -s $(du -sb srcdir | cut -f1) | tar xC dstdir
As a starting point I would..: * Leave the hostname file as-is if it's automatically generated - it's easy enough to parse out the backticks * Use `ssh`'s `-T` option to disable terminal allocation and pass the script file to the remote host by hooking it to the `ssh` command's stdin * Use `sshpass` instead of trying to roll my own auto-login scheme via `expect` * Concatenate the script file to itself to double up on the commands instead of running the script twice * Send stdout and stderr to their own log files An example: $ cat hosts `localhost` `127.0.0.1` `example.com` $ cat remote_script.sh printf 'Your random number is: %d\n' $RANDOM $ cat local_script.sh #!/bin/bash export SSHPASS="$(&lt;password_file)" while IFS='`' read _ host _; do echo "Running script on $host" sshpass -e ssh -T -o StrictHostKeyChecking=no user@$host &lt; &lt;(cat remote_script.sh remote_script.sh) done &lt;hosts &gt;&gt;output.log 2&gt;&gt;error.log $ ./local_script.sh $ cat output.log Running script on localhost Your random number is: 32522 Your random number is: 10611 Running script on 127.0.0.1 Your random number is: 23331 Your random number is: 24068 Running script on example.com $ cat error.log ssh: connect to host example.com port 22: Network is unreachable There are other issues to consider that weren't mentioned in your description such as whether each host will use the same username and/or password. I assumed above that they did, but if they don't you'll need to correlate the relevant bits of information with each hostname. (In which case you might as well create your own host file instead of using the one with backticks) Or what about output. Do you want a running log of all output for every host as I did above, or do you want each host to have its own logs? If the latter you'll want to move the `&gt;&gt;` redirections up to the `sshpass` command. Should it include timestamps? Command running times? Etc. When making posts like this it's helpful to include these four things: your input, your expected outcomes and/or output, any notable environmental specifications/limitations/etc., and what you've already tried and why.
Neither pretty nor perfect, but if I understood your requirement, I think this might work. Though the geniuses of this sub will probably present a much cleaner solution: spdqbr@MyMachine 00:44:40 /cygdrive/c/temp/test 1954 $ filef . |-sorted | |-business | |-homework | |-pleasure |-unsorted | |-MATH220homework1.pdf | |-myPleasure.pdf | |-WidgetsBusiness.pdf spdqbr@MyMachine 00:44:43 /cygdrive/c/temp/test 1953 $ for folder in sorted/*; do folder=${folder/sorted\//}; find unsorted/ -iname \*$folder\* -exec mv {} sorted/$folder \; ; done spdqbr@MyMachine 00:45:01 /cygdrive/c/temp/test 1953 $ filef . |-sorted | |-business | | |-WidgetsBusiness.pdf | |-homework | | |-MATH220homework1.pdf | |-pleasure | | |-myPleasure.pdf |-unsorted Be careful with this with spaces in the file or folder names. Broken down: for folder in sorted/* # loop over the folders in sorted do folder=${folder/sorted\//} #strip the "sorted/" prefix find unsorted/ #look in the unsorted folder -iname \*$folder\* # for files containing the current foldername (case insensitive) -exec mv {} sorted/$folder \; # move them to that folder; done
Prefix your code with 4 spaces to make it readable. #!/bin/bash echo test history -c My `history` binary doesn't have a `-c` switch. What is it supposed to do?
echo is doing exactly what you asked it to do, in this case, printing out text you are feeding it. What I think you want to do is called [command substitution..](http://wiki.bash-hackers.org/syntax/expansion/cmdsubst) something like echo $(history) It might also help to understand what you are trying to achieve as there is probably a better way to do it. p.s. The very first line should be: #!/bin/bash Its called a shebang line and tells the shell interpreter to spawn a subshell to execute the file. Also, make sure the file is executable chmod u+x the_filename_goes_here have fun...
history is only enabled for interactive shells. Why are you trying to manipulate history via a script?
Regexp.
This sounds like an [XY Problem](http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem). 
 for dir in "sorted/*"; do mv unsorted/*${dir##*/}* "$dir" done 
This is adapted from [Stack Overflow](http://stackoverflow.com/questions/687948/timeout-a-command-in-bash-without-unnecessary-delay): while read passwd; do while read user; do while read host; do ( /usr/bin/medusa -v 6 -g 1 -r 1 -R 0 -T 6 \ -h $host \ -u $user \ -p $passwd \ -M ssh &gt;&gt;results 2&gt;&amp;1 &amp; sleep 10 kill SIGTERM 0 ) 2&gt; /dev/null &amp; done &lt; hosts.txt done &lt; users.txt done &lt; passwd.txt Obviously, test this before using. 
it behaved the same way. tries to exec the string to a space as a command. we run bash 4.1.2 on redhat linux
I'm not sure what you mean that it tries to "exec the string to a space as a command" What I expect to happen on the x=$(...) line is that it will execute the value inside the parens then store the output of that command in x. Is that not what you were trying to do?
So it works from the command line, but not from inside a script? Have you changed IFS at some point or something? I'm kinda stumped to explain what you're seeing. My suggestion would be to put set -x at the start of your script then run it. That will make bash print out everything it's doing before it does it, perhaps you'll be able to see where it's doing something unexpectedly extra. If you want to just put it around a small area of the script you can do set +x to turn off that extra printing.
The problem is not that it's the variable is in the tr command, the problem is the single quotes: sh -c '... "${p}" ...' The single quotes will prevent variable substitution. You can either use double quotes for the `sh -c` call (and then escape all the double quotes inside that), or maybe more simply change the looping structure to something like find . -type f -name '*.csv' -print |\ while read fileName do #stuff done
&gt; Also, in my experience $() tends to be preferred instead of ``. I've never heard why though. `$()` allows nesting, e.g.: `echo $(echo $(echo foo))`.
I went with this instead, it worked. if [ $answer == "Y" ] || [ $answer == "y" ] Thanks for explaining that. It helped me out a lot.
Only use the `[` command in sh scripts. Use the superior `[[ .. ]]` and `(( .. ))` in bash (see [BashFAQ #31](http://mywiki.wooledge.org/BashFAQ/031)). Or in this case, if you're asking for confirmation for something, a function like this might do. confirm() { local ans read -r -p "$1" -N1 ans printf '\n' [[ $ans = [Yy] ]] } if confirm "Are you sure? "; then if confirm "Absolutely, positively sure? "; then printf "Alright, alright, I'll wipe your hard-drive already!\n" else printf 'Puh! Close one.\n' fi else printf 'Thought so...\n' fi
perhaps quoting? You probably don't need the quotes inside $() and do need them in the echo. parm="https://public.opencpu.org/ocpu/library/" x=$(curl -X GET $parm) echo "$x"
mcstafford, this worked like magic; alas, I cannot figure out what is happening. My assumptions: - cat &lt;&lt; EOF = read what follows and stop when you get to EOF. - &gt; xml.txt = append whatever cat grabbed to the file xml.txt - ${VAR} = tells cat to expand variables. Things that have me throughly confused: * slashes and quotes are not escaped. * nothing is escaped! * no ; delimiter/terminator at the end of each line. This is so full of wonder and magic. Thank you, sir or madame...or bot.
Your assumptions are mostly correct. It's called a [here-document](https://en.wikipedia.org/wiki/Here_document). You got one thing wrong: variables are expanded by the shell, not by cat (or any other command). In UNIX-like shells, this is always true: variables, wildcards, etc. are expanded by the shell before any program is launched and the result of the parsing is what gets passed on to the program, so the program never sees any variables or wildcards. Understanding this can solve a lot of mysteries. Within here-documents, most of the usual shell grammar doesn't apply, so not many things need to be escaped. Variables are still expanded, though, so to include a literal $ you do need to escape it. The below is from the bash manual. 3.6.5 Here Documents -------------------- This type of redirection instructs the shell to read input from the current source until a line containing only WORD (with no trailing blanks) is seen. All of the lines read up to that point are then used as the standard input for a command. The format of here-documents is: &lt;&lt;[-]WORD HERE-DOCUMENT DELIMITER No parameter expansion, command substitution, arithmetic expansion, or filename expansion is performed on WORD. If any characters in WORD are quoted, the DELIMITER is the result of quote removal on WORD, and the lines in the here-document are not expanded. If WORD is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion. In the latter case, the character sequence `\newline' is ignored, and `\' must be used to quote the characters `\', `$', and ``'. If the redirection operator is `&lt;&lt;-', then all leading tab characters are stripped from input lines and the line containing DELIMITER. This allows here-documents within shell scripts to be indented in a natural fashion. 
&gt; variables, wildcards, etc. are expanded by the shell before any program is launched and the result of the parsing is what gets passed on to the program, so the program never sees any variables or wildcards. McDutchie, that clears up a lot. Based on the uncountable number of errors I have experienced, I am going to guess we can control how the shell parses/passes the content to the command: quoted vs non-quoted variables. I am going to play with this new knowledge in the hopes of implementing in-place editing: I am tired of manually updating the main .xml file. I'll report in once I have successfully added automatic updating. Wish me luck, and thank you for explaining how the shell handles variables. ~ iMorpheus 
&gt; Based on the uncountable number of errors I have experienced, I am going to guess we can control how the shell parses/passes the content to the command: quoted vs non-quoted variables. Yes. Note that single and double quotes are different. Within single quotes, nothing is parsed. Within double quotes, variables and other things starting with a $ (e.g. command substitution) are parsed and substituted. What both have in common is that anything enclosed in single or double quotes is one argument, even if it contains spaces. That's why you often see variables enclosed in double quotes -- this is to make sure it's always one argument to some command, because if the value of the variable contains spaces then the variable would expand to multiple arguments, with any leading or trailing spaces stripped. For instance, given: F="one two three" the command: touch $F would create three files named "one", "two" and "three", whereas touch "$F" would create one file named "one two three", and touch '$F' would create one file named "$F". Good luck. 
It works, so it's fine. I'm going to look like a right pedant here, but this is for the benefit of other readers as well. Instead of piping the output of "echo" into "ex", you can use a here-document here as well, making things a bit more readable: ex "$RSS" &lt;&lt;EOF %s/&lt;\/channel&gt;//g %s/&lt;\/rss&gt;//g w q EOF This leaves two empty lines, though. Even better to delete the lines containing these tags altogether: ex "$RSS" &lt;&lt;EOF /&lt;\/channel&gt;/d /&lt;\/rss&gt;/d w q EOF Hope this helps someone.
You seem to have caught the programming bug. Your enthusiasm is neat. I caught it at age 12, and I remember the feeling. (I'm now 41.) Thanks for the gold, by the way! &gt; Does this imply I could use /WORD/d to delete a line? Yes. Well reasoned. We're now programming in `ex`'s own little command language (the here-document that you're feeding into `ex` can be seen as a tiny program written in `ex`'s own language). Variants of it are also supported by `ed`, `sed`, `vi`, and other line-oriented text manipulation programs, so it's worth learning a little more. **Regular expressions** In your example, WORD is actually a regular expression, and /WORD/d will delete any lines matching that regular expression. By default, it means "delete any line that contains WORD" (so it will delete anything else on that line as well). But in regular expressions, certain characters (such as: . * ^ $, called *metacharacters*) have special meanings, so you'll want to escape them (with preceding backslashes) unless you want to use their functionality. But you may well want to. For instance, a dot (.) means: match *any* character in place of the dot, and a * means: match zero or more of the preceding character. So, /ab..ef/d will delete lines containing "abcdef", "abCDef", "abQxef", "ab&amp;\^xf", etc., and /abc*de/d will delete lines containing "abde", or "abcde", or "abccde", or "abcccde", or "abccccde", etc. You can also combine `.` and `*`. The combination `.*` means: match zero or more of *any* character. So, I'll leave it to you to figure out what this does: /ab.*de/d Regular expressions have many more features, although some depend on the program you're using. They're a very powerful search mechanism and you'll want to read up on them sometime. They are hard to wrap your head around, but the reward of grokking them is huge. **Addressing** In `ed`/`ex`/`sed`/`vi` et al, the bit before the `d` is called an "address", which is what tells the "d" command exactly what to delete. An address can be a regular expression enclosed in `//`, like above, but it can also be other things, like a simple line number. For instance, `5d` means: delete the fifth line in the file (no matter what's in it). Other commands take addresses as well, even "s" (substitute) which you've already used. The substitute command takes the form `s/regex/replacement/flags` where "regex" is a regular expression telling `s` what to substitute, and "replacement" is simply the replacement text (not a regex), and "flags" are additional parameters for the `s` command, for instance the `g` flag means: continue replacing even after you've already found a match, i.e.: replace every match, not just the first one. But `s` just edits single lines, it doesn't add or remove lines. Now you understand why your original code left empty lines. Like I said, the `s` command can also take an address, which precedes it, just like with the `d` command. (It's also allowed to add a space for legibility, but it's not mandatory.) For instance: 5 s/X/Y/g means: replace all instances of X with Y, but only on line 5. And: /WORD/ s/X/Y/ means: replace only the first instance of X with Y, and only in lines matching the regular expression "WORD". &gt; **QUESTION** &gt; &gt; So, is it possible to turn this: &gt; &gt; EstDur=$(echo "($DUR+0.5)/1" | bc); &gt; &gt; into this &gt; &gt; bc &lt;&lt; EOF &gt; $EstDur &gt; $DUR+0.5/1 &gt; EOF &gt; &gt; ???? Yup. Again, well reasoned. (Did you try it out?) A here-document is just another form of input redirection. The difference is that input redirection using `&lt;FILENAME` reads from a file called FILENAME, and input redirection using `&lt;&lt;DELIMITER` reads from the program itself until it encounters your chosen `DELIMITER` (it's common to use `EOF` but by no means mandatory), so you're incorporating your data in the program text. That data can be any kind of text, even another program. It depends on what you feed it into. You've been feeding `ex` little programs in the `ex` command language. As you know, variable substitution is also done, but you can turn that off by enclosing `DELIMITER` in single quotes. So if you know you're not going to use variables for a here document anyway, use `&lt;&lt;'DELIMITER'` and it will read text unparsed until encountering `DELIMITER` and you don't need to worry about escaping anything at all. 
This smells of homework.. Read the man pages of diff and comm. Then play with their output with grep, head, and tail.. And use redirection to write that into your new file. To have it ask the filename you want to look at "read" for bash.