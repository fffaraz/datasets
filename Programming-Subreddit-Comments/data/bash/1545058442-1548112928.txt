I am trying to merge changelog of two files and then need to delete common lines. 
You're just repeating the problem statement. Have you actually tried either/both solutions presented thus far (mine and u/NotMilitaryAI's)? Do either of them work for you? More importantly, are you dealing with changelogs from the same source, or are they maintained by different folks? Both solutions are somewhat sensitive to whitespace differences, so if the changelogs are maintained by different people, a subtle change in whitespace (e.g. `Wed Feb 24 2016` vs `Wed Feb 24 2016`, or word-wrapping at different widths) can result in deduplication failure. (Note that my solution is a little less sensitive, matching only changelog headers as opposed to `awk`'s "match entire record").
&gt; Have you actually tried either/both solutions OP's got a few more than 'both' - been spamming this question everywhere.... Fairly safe bet that it's not homework.... Unless some tutor really fancies explaining down the road why you can't merge 2 different source trees this way too....
I port fish prompt to bash: xxx@host /u/l/etc&gt; 
Drop it into [https://regexr.com/](https://regexr.com/). Appears to be any non digit, followed by a digit, followed by a non digit, followed by a digit 1-6, followed by a non digit followed by a digit.
Thanks, I tried that site, but it says it's invalid (which is entirely possible), and doesn't grab any of the sample data I paste into the window.
What is it you are trying to grab? 
The data is a string of digits we queried from a database. For this section of the script, the intent was to filter out internal calls from external calls. If the regex were true, it would be a 6-digit extension, and thus an internal call that we can skip processing. If the regex were false, it would be an 11-digit phone number, and thus an external call that needs processed. I'm trying to figure out *exactly* the requirements that this regex is looking for, and from there I can move forward to other conditionals and figure out what's going on with our script.
&gt; Appears to be any non digit ... &gt; followed by a non digit Almost, but not quite. Those are _negative lookbehind_ and _negative lookahead_ assertions respectively. As assertions, they don't actually consume any characters from the string being searched. Since they are negative assertions, they assert that there is _not_ a digit preceding or following the run of 6 digits. That does not necessarily mean there is a non-digit in those places. For instance, in the string: 123456abc there are no characters whatsoever preceding the run of 6 digits. To put it simply: asserting whether a non-digit is present is very different from asserting whether a digit is not present.
&gt; but it says it's invalid (which is entirely possible) It's not invalid in Perl. There are dozens of different regex flavours in the wild. The regex flavour used by your browser (and is exercised by that website) is not the same as that used by Perl. Perl supports negative lookbehind assertions; JavaScript does not.
That's a lot clearer than some explanations I've seen. So now, my understanding is: this requires one to six digits, with nothing preceding or following those six digits, to be true?
&gt; nothing Make that "not a digit". It can be some other character, or it can be nothing.
First, we'll explain the middle part, since that's the pattern being looked for... \d{1,6} This catches between 1 and 6 numeric digits (0-9). It will thus catch any numerical string between 0 and 999999. The part before... (?&lt;!\d) ... is called a negative lookbehind, and the part after... (?!\d) is called a negative lookahead. Negative lookaheads, which are ways to specify character that is not allowed to follow what you want to match, are regularly used tools, but negative lookbehinds, which are ways to do the same before the pattern, are not very well supported anymore. The Regexr website doesn't apply it and gives a warning to that effect. It's possible the system you are trying to use is having trouble with it too, which is why it's not working for you.
Awesome, thanks
Thanks for the breakdown, makes sense!
FTW
You're _sooooooo_ close... ``` host HOSTNAME| awk '{print $4 "/26"}' ``` `print` with multiple non-comma-separated arguments just concatenates them together. **Further Reading:** * [GNU Awk Manual: `print` Examples](https://www.gnu.org/software/gawk/manual/html_node/Print-Examples.html)
Thank you so much, and thanks for the reading material. I didn't know i could do that lol.
Actually, you don't even need to use pipes. Awk (by which I'll assume you actually mean `gawk`) is powerful enough to process both files all by its lonesome: ``` $ cat run.awk # First file contains the lookup table ARGIND == 1 &amp;&amp; $2 == "abc" { want[$1] = 1 ; print "Want: " $1 } # Second file contains the actual data ARGIND == 2 &amp;&amp; $6 in want &amp;&amp; $7 == "typez" { tl+= $5 ; print "Tallied: " $6 " (" $5 ")"} END { print tl } $ gawk -f run.awk file1.tcl file2.tcl Want: et1 Want: et2 Want: et55 Tallied: et1 (5) Tallied: et55 (3) Tallied: et1 (3) 11 ``` I don't know how much faster it'll run on your dataset, but I think you'll agree that it's a lot more comprehensible than your pipeline.
This does not answer your question, but if you end up running this multiple times to find the sums of varying keys, it may be quicker in the end just to tabulate them all at once. Something like $&gt; while read c1 c2;do (awk -v var=$c1 '($6 == var &amp;&amp; $7 ~ /[Tt]ypez/) {sum += $5} END {print var"="sum}' file2);done &lt;file1 et1=8 et2= et55=3 et12= Or with some simple filtering on file1 while read c1 c2;do ([ "$c2" = 'abc' ] &amp;&amp; awk -v var=$c1 '($6 == var &amp;&amp; $7 ~ /[Tt]ypez/) {sum += $5} END {print var"="sum}' file2);done &lt;file1 The `while read` stuff is just mapping file1 columns to the vars c1 and c2. $&gt; while read c1 c2; do (echo $c1 $c2); done &lt;file1 et1 abc et2 abc et55 abc All the matching and grep'ing is done in awk (regex cause not sure if you meant to ignore case) awk -v var=$c1 '($6 == var &amp;&amp; $7 ~ /[Tt]ypez/) {sum += $5} END {print var"="sum}' file2 Of course this will be slower than you problem, because it is awk'ing that file for each matched key, especially if file2 is large, but it will pull way ahead if you end up running your command multiple times for multiple keys That said, I'm sure you could probably do it all in awk by itself, but I just know enough to fumble around with it :) Good luck!
I'd do it with a single awk like this: awk 'FNR == NR { if ($2 == "abc") a[$1]; next } tolower($7) == "typez" &amp;&amp; $6 in a { sum += $5 } END { print sum }' file1 file2
IIRC, printf wants a "\n" for a newline (without the quotes). Does that do the trick?
You asked essentially the same question in this subreddit less than 24 hours ago: https://www.reddit.com/r/bash/comments/a6z2pk/hi_everyone_i_could_need_a_regex_help_in_writing/ I answered your question, and you acknowledged my answer. Why are you asking again? If you didn't understand something in my answer, respond there instead.
ok, to be more clear, I want that the printf result seems like this: .-. \_ - \_ - \_ - ( ). \_ - \_ - \_ (\_\_\_(\_\_) \_ - \_ - \_ - \* \* \* \* \* \* \* \* and not: .-. ( ). (\_\_\_(\_\_) \* \* \* \* \* \* \* \*" &amp;#x200B; \_ - \_ - \_ - \_ - \_ - \_ \_ - \_ - \_ - &amp;#x200B; that is what's happen at the moment...
thanks didnt find my previous question anymore.
That's what **My Profile** under your Reddit user menu is for.
In future, enclose your code in code blocks, otherwise it's almost impossible to read. I assume that your original script looks like this: #!/bin/bash # Moderate rain? printf " .-. ( ). (___(__) * * * * * * * * " # Overcast? printf " _ - _ - _ - _ - _ - _ _ - _ - _ - " and you want to put the two side by side, like this: .-. _ - _ - _ - ( ). _ - _ - _ (___(__) _ - _ - _ - * * * * * * * * First, you need to ensure that all the output lines in your script are of equal width, padding with spaces where necessary. Assuming your script is called `print.sh`: $ bash print.sh | cat -E $ .-. $ ( ). $ (___(__)$ * * * * $ * * * * $ $ _ - _ - _ -$ _ - _ - _ $ _ - _ - _ -$ `cat -E` prints `$` at the end of each line. Make sure they're aligned for each figure, as seen above. Once you've ensured that, then the `paste` utility is your friend. It takes two or more files, and pastes corresponding lines from each file side-by-side, separated with a TAB by default: #!/bin/bash # Putting the two symbols side-by-side paste &lt;(printf " .-. ( ). (___(__) * * * * * * * * ") \ &lt;(printf " _ - _ - _ - _ - _ - _ _ - _ - _ - ") The `&lt;(printf ...)` constructs are *process substitutions*, essentially "run this command, and pass its output as if it was a temporary file". `paste` gets the two "files" it's expecting, and does its magic accordingly. I've also written a [`columnate()` bash function](https://www.reddit.com/r/commandline/comments/9zttx7/help_with_column_formatting_in_bash_please/eaccyv7/) to answer a different question almost a month ago. It takes multiple strings and prints them in aligned columns. If `paste` is too inflexible for your purpose, maybe you can get some ideas from that function. **Further Reading:** * [`bash` man page](http://man7.org/linux/man-pages/man1/bash.1.html), **Process Substitution** section * [`paste` man page](http://man7.org/linux/man-pages/man1/paste.1.html)
Thank you for your precious insight. You saved a life today.
You must be some kind of Nobel genius perhaps? Really honored!
Run it with `bash -x yourscript` to get a debug trace. A typical reason for not seeing anything would be that the loop end up looping over zero elements
&gt;A typical reason for not seeing anything would be that the loop end up looping over zero elements How can that be, when the script runs just fine in xterm? Also the actual script had the full file path for the package list, so its not like I was trying to run the script once in the home folder and the other time in another folder.
It sounded like you ran it once on a host system and once in a VM. I'm any case, about debug trace will show what's up
I take back what I said. I run it with the -x argument and I got all the output as expected. I wasn't asked for a password because I had just used sudo 2 seconds before running the command, so I don't know if I would be able to type it.
In future, enclose your scripts in code blocks, to avoid Reddit making it look like a$$. Whenever you ask "why isn't my script doing what I want it to do?", your first step should be to run it in trace mode. Either enable it manually with `bash -x myinit.sh arg1 arg2 ...`, or change the shebang line at the top to `#!/bin/bash -x` and run it as normal. I'll let others nitpick about stuff like "use `case` instead of a comparison ladder of successive `if [[ $a1 == blah ]] ...`", and the fact that interactive input (`read a1`) in init scripts may cause your system to stall at startup. All that aside, there may be other bugs in your script (that's why you should run in trace mode), but assuming your loop is running correctly,^(1) it's easy to see why your script only starts one node when you input `all`. I will even predict that it's *always the puppet node*... Because the only thing your `master` and `puppet` functions does is *set variables*. By the time your loop ends, `puppet` has overwritten all the variables `master` set with its own values. If you want to run `start` based on the variables set by each function, you should be calling it *inside* the loop, not all by itself near the end of your init script. Now, if you're like most programmers I know, you've already stopped reading and just slapped a call to `start` in the loop...and been unpleasantly surprised. Your script currently reads like you're just slapping things together without careful thought. In my books, there's only one correct way to fix your script: 1. Stop and really **think about your desired process flow**, i.e. figure out precisely what needs to be done for each input, and in what order. Does master/puppet multiplicity apply only to `start`, or all other functions? Does it make sense to prompt for master/puppet selection in an init script that may be running on a headless node, or would putting your selection in a config file make more sense? etc. so forth. 2. `bash -x` to see exactly what your script does. (There's a third-party [bash debugger](http://bashdb.sourceforge.net/) that may also help, but I don't use it myself, so I won't comment further.) 3. If it doesn't match \[1\], adjust your script accordingly and go to \[2\]. **Footnotes:** ^(1) To my surprise, `${functions[i]}` seems to behave in the same way as `${functions[$i]}`, i.e. the subscript `i` is automatically dereferenced instead of being treated as a string literal. I don't recall any bash docs that say this should be so, but to be perfectly clear about intent and to avoid later breakage should this be fixed, *always* use `${functions[$i]}`.
&gt; I've also written a columnate() bash function Nice - shame OP deleted the Q so it won't be visible in searches...
I think what is happening here is that you're using the -A flag here combined with "5" to show the 5 lines after the match. from manpage: -A num, --after-context=num Print num lines of trailing context after each match. See also the -B and -C options. Try just using something like: grep -iE "&lt;(ProductName|Count)&gt;.*&lt;/(ProductName|Count)&gt;" sample.txt
 grep -E "&lt;ProductName&gt;|&lt;Count&gt;" Will do, but this really is 'read the manual' level homework.... With the time you have saved, learn how to use `sed` to strip off the unwanted bits.... And maybe look up why there are better ways to parse XML than `grep`ing.
echo bar|grep 'foo\\|bar'
ok, thanks very much!!!....u are a pro!!
 [05:13] poboxy@poboxy-PC &lt;11660:126&gt; [~/temp] :-&gt;:)&lt;-: $ cat data &lt;ProductName&gt;apple&lt;/ProductName&gt; &lt;Id&gt;0001&lt;/Id&gt; &lt;Type&gt;watch&lt;/Type&gt; &lt;Price&gt;500&lt;/Price&gt; &lt;Count&gt;3000&lt;/Count&gt; [05:13] poboxy@poboxy-PC &lt;11661:127&gt; [~/temp] :-&gt;:)&lt;-: $ grep -e ProductName -e Count data | awk -F'[&gt;&lt;]' ' { print $3 } ' apple 3000 [05:13] poboxy@poboxy-PC &lt;11662:128&gt; [~/temp] :-&gt;:)&lt;-: $ awk -F'[&gt;&lt;]' '/ProductName|Count/ { print $3 }' data apple 3000 [05:13] poboxy@poboxy-PC &lt;11663:129&gt; [~/temp] :-&gt;:)&lt;-: $ 
I think the better question here is, should you use grep to parse xml? And the answer is generally no. If you can control the input to grep so that it will always be in that format and there is no way to put in in a string&lt;Produ... etc I think its fine. But if you can't guarantee thos the I would ise an xml parser like xmlstarlet Example would be xml sel -t -c '//Product...' -n xmlfile
Yes, but you will need to combine *grep* with the power of other tools. Below is a solution that will work using *grep* and *cut* in a *for* loop. `for i in ProductName Count; do cat xml | grep $i | cut -d"&gt;" -f2 | cut -d"&lt;" -f1; done` (where xml is the file containing the xml data) The great thing about using cut is the option to specify the delimiter (-d) and choose which field (-f) to display based on that delimiter. Manuals: `man bash` `man cut` `man grep`
`products.xml`: &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;Collection&gt; &lt;Product&gt; &lt;ProductName&gt;apple&lt;/ProductName&gt; &lt;Id&gt;0001&lt;/Id&gt; &lt;Type&gt;watch&lt;/Type&gt; &lt;Price&gt;500&lt;/Price&gt; &lt;Count&gt;3000&lt;/Count&gt; &lt;/Product&gt; &lt;Product&gt; &lt;ProductName&gt;android&lt;/ProductName&gt; &lt;Id&gt;0002&lt;/Id&gt; &lt;Type&gt;gear&lt;/Type&gt; &lt;Price&gt;300&lt;/Price&gt; &lt;Count&gt;2000&lt;/Count&gt; &lt;/Product&gt; &lt;/Collection&gt; Command: `xmlstarlet sel -t -v '//ProductName|//Count' -n products.xml` Output: apple 3000 android 2000
Try xmlstarlet
Also, grep supports multiple `-e` patterns: grep -e '&lt;ProductName&gt;' -e '&lt;Count&gt;'
You have probably had an egrep "&lt;ProductName&gt;|&lt;Count&gt;" that would have sorted you out.... Not that anyone approaching this seriously would use `(e)grep` for anything bar a quick command-line based visual check to see if the text they were searching for *was actually present* in the file.
_&lt;sigh&gt;_ It's times like this that I really appreciate Quora's "questions belong to the community, so no backsies" policy. Anyway, thanks much for the heads-up!
``` [...] Complete log: */.snakemake/log/2018-12-18T104741.092698.snakemake.log ``` Have you looked in this log? What does it say?
The same as above: ClusterJobException in line 256 of /fast/projects/Dropseq-AG-Damm/work/2018-11-09_results_run5_speciesmix/pipeline.snake: Error executing rule convert_bamheader on cluster (jobid: 15, external: 7027806, jobscript: /fast/projects/Dropseq-AG-Damm/work/2018-11-09_results_run5$ Job failed, going on with independent jobs. Exiting because a job execution failed. Look above for error message Complete log: /fast/projects/Dropseq-AG-Damm/work/2018-11-09_results_run5_speciesmix/.snakemake/log/2018-12-18T104741.092698.snakemake.log &amp;#x200B;
Here’s a hint: the folder actually takes up no space, only the files inside it do. You can use `du` to find out the size of files. See the man page for options.
I don't use snakemake, but it's almost certainly one or more of `samtools`, `cut` or `grep` exiting with a non-zero return code. The easiest way to debug this is to replace your 4-step rule with a script that encapsulates all 4 steps, and runs in "fast-fail &amp; trace" mode (`-ex`): #!/bin/bash -ex mkdir -p stats/SERUM-ACT mkdir -p log/SERUM-ACT samtools view bam/SERUM-ACT/exon_tagged_trimmed_mapped_cleaned.bam &gt; bam/SERUM-ACT/exon_tagged_trimmed_mapped_cleaned_header.txt cut -f 12,13,18,20-24 bam/SERUM-ACT/exon_tagged_trimmed_mapped_cleaned_header.txt | grep -f stats/SERUM-ACT/good_barcodes_clean_filter.txt &gt; bam/SERUM-ACT/exon_tagged_trimmed_mapped_cleaned_header_filtered.tsv Then your rule becomes: myscript.sh &gt; /tmp/debug.log 2&gt;&amp;1 Run the rule, then check `/tmp/debug.log` to see where things went pear-shaped.
Should be doable but how easy it will be depends on the files.. are they all equal size? How big are the files? How many files are we talking?
The simbols is an Escape character. In this case they are there just to colorize the output. 
Is there anything i can do to get rid of it? Cheers
Maybe to print all files; names + size to csv file. Open it in excel and do simple summary. When you hit 20 GB just move those files to different folder. Dunno, just an idea. Probably somebody has better one
Nice little use of functions with a switch. There can be multiple extensions for the same type. It's not uncommon to see a `*.tgz`, which is exactly the same semantically as `*.tar.gz`, and there are others in the compressed tarball family. Thing is extentions are only useful for a visual aid, you can have a file which is a compressed archive and have a completely different extention or no extention at all. For these cases you should probably match against a substring of `file archive.ext` That would be a bit of extra work. However I would suggest just defaulting to extracting as tarball family. Switch on zip, rar, 7z, jar, and if it doesn't hit any of them, default to using `tar`. It already can determine which type of tarball it is so it would make sense to default to it. 
Why is this better/easier than storing functions in a file and loading the file or using auto-load to load functions?
Thank you so much you gave me excellent insights. I will default to tar as you said. Thanks again for taking your time to explain.
`cp files-to-copy... target-dir`
What are you trying to accomplish by doing that?
I want to run a particular command which accepts a single delimited argument in batches, potentially parallelizing the execution. 
I'm not even sure why the user prompt ends up in a log. Can we see the code? Those special characters are created via PS1 command, usually loaded from ~/.bashrc or ~/.bash_profile, but those might be the defaults for kali, as I don't know much about that distro.
Yes but how to choose thw right ones i need
 find ${source_directory} -maxdepth 1 -name '*2*' -exec cp {} ${target_directory) \:
&gt; When I try this command with a simple bash script it is working without any errors. It might not *print* an error...... But in your bash test script add echo Bash exited with a.... $? after your `grep` command Bash exits with a `1` if nothing is found...... Which I'd bet is being picked up as an error due to something (snake?) not being as clever as it thinks it is..... or just designed deliberately that way. A bit like Intel's chips ;-)
you can use globbing. lets say all your files has the prefix `file`, and the ones you want to exclude have the number `2` after it. `cp file[^2]* target-dir` will copy every file other than starting with `file2` to the `target-dir`
That is about as vague a question as you can post.... But you might want to use sed... If you are trying to do what I think you are trying to do.....
As the [`paste` man page](http://man7.org/linux/man-pages/man1/paste.1.html) says, right at the top: paste - merge lines of files So unless you're somehow generating *files* of X lines each, `paste` simply won't work. Since this is r/bash, here's a purist solution, no external commands involved: $ cat plus_rows.sh #!/bin/bash # plus_join: join all args with + character plus_join() { local IFS="+"; echo "$*" } # Number of lines to combine each time quantum="${1:-10}" accum=() while read; do accum+=("$REPLY") if [ ${#accum[@]} -eq $quantum ]; then plus_join "${accum[@]}" accum=() fi done # Don't forget to process any leftovers [ ${#accum[@]} -gt 0 ] &amp;&amp; plus_join "${accum[@]}" # Use all the cores!!! $ seq 7 203 | ./plus_rows.sh 13 | xargs -P $(nproc) -L 1 echo GOTCHA: GOTCHA: 7+8+9+10+11+12+13+14+15+16+17+18+19 GOTCHA: 20+21+22+23+24+25+26+27+28+29+30+31+32 GOTCHA: 33+34+35+36+37+38+39+40+41+42+43+44+45 GOTCHA: 46+47+48+49+50+51+52+53+54+55+56+57+58 GOTCHA: 59+60+61+62+63+64+65+66+67+68+69+70+71 GOTCHA: 72+73+74+75+76+77+78+79+80+81+82+83+84 GOTCHA: 85+86+87+88+89+90+91+92+93+94+95+96+97 GOTCHA: 98+99+100+101+102+103+104+105+106+107+108+109+110 GOTCHA: 111+112+113+114+115+116+117+118+119+120+121+122+123 GOTCHA: 124+125+126+127+128+129+130+131+132+133+134+135+136 GOTCHA: 137+138+139+140+141+142+143+144+145+146+147+148+149 GOTCHA: 150+151+152+153+154+155+156+157+158+159+160+161+162 GOTCHA: 163+164+165+166+167+168+169+170+171+172+173+174+175 GOTCHA: 176+177+178+179+180+181+182+183+184+185+186+187+188 GOTCHA: 189+190+191+192+193+194+195+196+197+198+199+200+201 GOTCHA: 202+203 The magic sauce in `plus_join` is explained in the **Special Parameters** section of the [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html) as follows: &gt;When the expansion \[of `$*`\] occurs within double quotes, it expands to a single word with the value of each parameter separated by the first character of the `IFS` special variable.
``` $ du -ah . | sort -k2 76M . 4.0K ./dest 76M ./source 10M ./source/file01 5.0M ./source/file02 4.0M ./source/file03 3.0M ./source/file04 2.0M ./source/file05 1.0M ./source/file06 10M ./source/file07 5.0M ./source/file08 4.0M ./source/file09 3.0M ./source/file10 2.0M ./source/file11 1.0M ./source/file12 10M ./source/file13 5.0M ./source/file14 4.0M ./source/file15 3.0M ./source/file16 2.0M ./source/file17 1.0M ./source/file18 $ dirsplit -m -s 20M source -p dest/dir_ Building file list, please wait... Calculating, please wait... .................... Calculated, using 4 volumes. Wasted: 1878213 Byte (estimated, check mkisofs -print-size ...) $ du -ah . | sort -k2 76M . 76M ./dest 20M ./dest/dir_1 5.0M ./dest/dir_1/file02 5.0M ./dest/dir_1/file08 2.0M ./dest/dir_1/file11 5.0M ./dest/dir_1/file14 2.0M ./dest/dir_1/file17 20M ./dest/dir_2 3.0M ./dest/dir_2/file04 3.0M ./dest/dir_2/file10 10M ./dest/dir_2/file13 3.0M ./dest/dir_2/file16 20M ./dest/dir_3 10M ./dest/dir_3/file01 2.0M ./dest/dir_3/file05 1.0M ./dest/dir_3/file06 1.0M ./dest/dir_3/file12 4.0M ./dest/dir_3/file15 1.0M ./dest/dir_3/file18 19M ./dest/dir_4 4.0M ./dest/dir_4/file03 10M ./dest/dir_4/file07 4.0M ./dest/dir_4/file09 4.0K ./source ```
I don't want to take away from your personal achievement, but you've rewritten one of the most commonly shared `.bashrc` functions. So here's a little turbo-boost for you to merge in: # Function to extract common compressed file types extract() { if [[ -z "${1}" ]]; then # display usage if no parameters given printf '%s\n' "Usage: extract &lt;path/file_name&gt;.&lt;zip|rar|bz2|gz|tar|tbz2|tgz|Z|7z|xz|exe|tar.bz2|tar.gz|tar.xz|rpm&gt;" else if [[ -r "${1}" ]]; then case "${1}" in (*.tar.bz2) tar xvjf ./"${1}" ;; (*.tar.gz) tar xvzf ./"${1}" ;; (*.tar.xz) tar xvJf ./"${1}" ;; (*.lzma) unlzma ./"${1}" ;; (*.bz2) bunzip2 ./"${1}" ;; (*.rar) unrar x -ad ./"${1}" ;; (*.gz) gunzip ./"${1}" ;; (*.tar) tar xvf ./"${1}" ;; (*.tbz2) tar xvjf ./"${1}" ;; (*.tgz) tar xvzf ./"${1}" ;; (*.zip) unzip ./"${1}" ;; (*.Z) uncompress ./"${1}" ;; (*.7z) 7z x ./"${1}" ;; (*.xz) unxz ./"${1}" ;; (*.exe) cabextract ./"${1}" ;; (*.rpm) rpm2cpio ./"${1}" | cpio -idmv ;; (*) echo "extract: '${1}' - unknown archive method" ;; esac else printf '%s\n' "'${1}' - file not found or not readable" fi fi } # Provide a function to compress common compressed Filetypes compress() { File=$1 shift case "${File}" in (*.tar.bz2) tar cjf "${File}" "$@" ;; (*.tar.gz) tar czf "${File}" "$@" ;; (*.tgz) tar czf "${File}" "$@" ;; (*.zip) zip "${File}" "$@" ;; (*.rar) rar "${File}" "$@" ;; (*) echo "Filetype not recognized" ;; esac } Improvements beyond that? Well, whenever these functions (or some very similar variant of them) gets shared, there's always somebody who brings up `atool`. So you might want to research that, then start your scripts or functions (however you wind up structuring them) with something like: if command -v atool &amp;&gt;/dev/null; then # do your thing with atool else # do your thing the manual way fi So that should give you a bit of food for thought. Good luck, and keep us updated!
\+1 for the right tool, but as old Kenobi would say: &gt;[https://mrtrix.readthedocs.io/en/latest/reference/commands/dirsplit.html](https://mrtrix.readthedocs.io/en/latest/reference/commands/dirsplit.html) "This is not the ~~droid~~ man page you're looking for." Try [this one](https://linux.die.net/man/1/dirsplit) instead, or just `dirsplit -H` (probably the more useful option).
Thank you very much for sharing, I will certainly look into it. I thought there were far better tools implemented to do the same thing but to learn i figured it was a good idea to try it myself. I am going to wonder in this subreddit more!
I corrected that link seven minutes before you replied. I edited it to add the link, realized it was the wrong one, and edited it again to correct it all in the span of thirty seconds so your timing is uncanny. 
Very nice! I bet I can reuse some of that code. This is brilliant
How do you autoload? I think the author thinks it is easier to autoload funcs based on dir. Instead of doing through functions. And I agree that that is easier. But I'm failing to see the usecase for it... it would be nice if the author showed how he uses it. Like proper usecases. While I was writing this I actually thought of a usecase. At work we have a root dir of our build dir. Then a lot of other build dirs. Could go into any build dir and build using a custom function that don't clutter the build space :) (not necessary for us since we use maven and it is pretty easy to build specific projects, but if we didn't already have a way of building random projects this might be nice)
he probably means sourcing the functions in some destination folder while IFS= read -r -d $'\0' file; do source_file "$file" done &lt; &lt;(find -L "$PATH_BASH_CONFIG" -type f -not -iname *.swp -print0 | sort -dz) 
teaching people that its OK to \`curl | bash\` is something i wish we'd all stop. 
 xargs -L10 echo | tr ' ' + # watch spaces xargs -L10 bash -c 'IFS=+; echo "$*"' parallel -kn10 --pipe paste -sd+
Here's mine: PS1="$(if [[ ${EUID} == 0 ]]; then echo '\[\033[01;31m\]\u@\h'; else echo '\[\033[01;32m\]\u@\h'; fi)\[\033[33m\] \D{%F %T}\[\033[01;34m\] \w\[\033[00m\]\n\$([[ \$? != 0 ]] &amp;&amp; echo \"\")\\$ " It looks like this: https://imgur.com/NWkCIHK
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programminghelp] [Script stops at line 5](https://www.reddit.com/r/programminghelp/comments/a85ao3/script_stops_at_line_5/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
A few tips: * Run `bash -ex install.sh` to have it fail-on-first-error (`-e`) and trace its execution (`-x`). You may be surprised at what you find. * Run your script through [shellcheck](https://www.shellcheck.net/) to see what issues it can identify. * When using heredocs, I usually use the `&lt;&lt;-EOF` variant, and indent the heredoc content itself with TABs. That visually gets the content out of the way of the main logic, which makes for easier reasoning and debugging. That said, one problem should already be obvious. In fact, GitHub's syntax highlighting on your script couldn't make it clearer: ``` cat &lt;&lt; EOF | arch-chroot /mnt/ [...] cat &lt;&lt; EOF [...] EOF [...] EOF ``` Which `cat` do you think the first `EOF` is paired to? _Never_ nest heredocs with the same delimiter: ``` cat &lt;&lt; EOF | arch-chroot /mnt/ [...] cat &lt;&lt; EOF2 [...] EOF2 [...] EOF ```
I'd prefer: ``` find "${source_directory}" -maxdepth 1 -type f \! -name '*2*' -exec cp -t "${target_directory)" '{}' + ``` since: * the OP apparently wants to copy files that _don't_ have "2" in the name, * the OP specified _files_, so `-type f` to be sure, and * it's likely that GNU `cp` is available to the OP, thereby enabling a reduction from "one `cp` per file" to "one `cp`, period".
Ok, so changing the code around to what you can see now on the GitHub fixed it. Thanks for telling me about those arguments and about nestling heredocs.
Careful for this one it will print any product and any count regardless of context.
Yep, that's true. Thanks for introducing me to xmlstarlet in your earlier comment!
This doesn't seem like a good fit for Bash. Perhaps use [GNU Parallel](https://www.gnu.org/software/parallel/) instead.
I don't see any sensible way to do this with bash without storing each output in separate files, e.g.: for i in "${!Xin[@]}"; do printf -v out1 Xout1_%02d "$i" printf -v out2 Xout2_%02d "$i" { foo1 "${Xin[i]}" &gt; "$out1" &amp;&amp; foo2 "${Xin[i]}" &gt; "$out2"; } &amp; done wait for file in Xout1_??; do Xout1[10#${file##*_}]=$(&lt;"$file") &amp;&amp; rm -f "$file" done for file in Xout2_??; do Xout2[10#${file##*_}]=$(&lt;"$file") &amp;&amp; rm -f "$file" done A general purpose language would be more suitable for this task
I would try using the 'parallel' tool and and the bash 'mapfile' command to read its output into your arrays. Like this: mapfile -t Xout1 &lt; &lt;(parallel foo1 ::: "${Xin[@]}") mapfile -t Xout2 &lt; &lt;(parallel foo2 ::: "${Xin[@]}") This example only works if the output of foo1 and foo2 are just single lines of text. It won't work if you want to capture multi-line text in your array elements. In that case you will have to see what kind of interesting parameters parallel has to modify the output, like maybe there's a way to get it to insert a zero character as delimiter to then use with mapfile. If your foo1 and foo2 commands are functions in your script, you have to do `export -f foo1 foo2` to make them visible to the parallel tool.
That works. I was thinking of FPATH in Korn shell. Guess Bash doesn't have that but googling it showed some solutions to emulate FPATH in Bash.
That's really nice!
Like from the neck down or waist down?
Solution found for future people with this problem: transfer=`output of command` eval $(sed -n 's/^x_snd_user=/user=/p;s/^x_snd_appli=/application=/p' &lt;&lt;&lt; "$transfer) echo $user $application
Thanks for the tip. Ill have to try it out.
Assuming you just want to load the values of certain variables under different names, *and don't want to load any other variables*, then The Beatles thought of it first: "All you need is `sed`"...and a separate file for your patterns: $ cat sed.pats s/^x_snd_appli=/x_sonic_hedgehog=/p s/^x_route_from_xfer=/y_boring_flamethrower=/p s/^x_ftp_command=/z_scp_ftw=/p $ cat src.vars x_snd_user='Jane Smith' x_snd_appli='sox' x_snd_text='People get ready' x_rcv_user='John Doe' x_rcv_appli='vlc' x_rcv_text='Damp parkas' x_dup_from_xfer='0' x_route_from_xfer='1' x_route_to_xfer='2' x_reply_by_xfer='3' x_reply_to_xfer='4' x_routed_to_XIB='N' x_end_xfer_script='echo "DONE"' x_ftp_command='scp serious.ly' $ sed -n -f sed.pats &lt; src.vars x_sonic_hedgehog='sox' y_boring_flamethrower='1' z_scp_ftw='scp serious.ly' Essentially, you tell `sed` to *not* print anything by default (`-n`) and read a crap-ton of operations from a script file (`-f sed.pats`). Each operation is your typical replacement (`s/`*find\_pat*`/`*replace\_str*`/`), with a `p` at the end to **p**rint the result.
Thanks but a few hours late! Already found the (same) solution! 
This is a good example of when to move to an actual programming language. &gt; Preferably not with something that needs to be installed as that's not possible. Your target system probably has Python, but if not Go is a good option since you can just drop in a statically-linked binary.
Bash was just the most convenient in my case, lots of system commands to run (before and after this issue) and Python is only 2.4. Yes, our systems are very outdated. :-)
I've actually solved the threading issue with Bash a few times. Here's my take on your problem. It's a relatively simple (crude) way to fire off a set number of forked child processes, but still run a specific number of iterations. \` func () { \#-- These will make sense shortly. local TMP\_NAME="${1}-${2}" local TMP\_FILENAME="/tmp/Xout-${TMP\_NAME}.tmp" &amp;#x200B; \#-- Remove the temp file, if it exists. It's probably from a previous run. if \[ -e ${TMP\_FILENAME} \]; then /bin/rm -f ${TMP\_FILENAME}; fi &amp;#x200B; \#-- Xin is some array generated elsewhere in the code for nn in $(seq 0 $((( ${#Xin\[@\]} - 1 )))); do &amp;#x200B; \#-- Output the values to a unique TMP file, where we can retrieve it later. echo -e "Xout1\[${TMP\_NAME}-$nn\]=\\"$(foo1 "${Xin\[$nn\]}")\\"" &gt;&gt; ${TMP\_FILENAME} echo -e "Xout2\[${TMP\_NAME}-$nn\]=\\"$(foo2 "${Xin\[$nn\]}")\\"" &gt;&gt; ${TMP\_FILENAME} done } &amp;#x200B; \#-- This is the number of total tasks to run. MAX\_TASKS=80 &amp;#x200B; \#-- This is the maximum number of child processes to execute per-iteration. \# It's to make sure we don't overload the system. MAX\_CHILDREN=4 &amp;#x200B; \#-- Loop through all iterations. \# Each one generates a unique temp file. We retrieve the contents later. for N1 in $(seq 1 $(( ${MAX\_TASKS}/ ${MAX\_CHILDREN} ))); do for N2 in $(seq 1 ${MAX\_CHILDREN}); do &amp;#x200B; \#-- Start up to MAX\_CHILDREN child 'func' calls. \# Don't fork the child, if we are running the max amount for this iteration. \# This ensures that we don't overload our system. if \[ ${N2} -lt ${MAX\_CHILDREN} \]; then func ${N1} ${N2} &amp; else \#-- Print a status msg to STDERR. Let the user know that we are still "alive". echo "\[Run time ${SECONDS} seconds\] Max children per-iteration reached. Not forking." 1&gt;&amp;2 func ${N1} ${N2} fi done done &amp;#x200B; \#-- Retrieve the saved data from the temp files. declare -A Xout1 Xout2 for TMP\_FILENAME in /tmp/Xout-\*.tmp; do eval "$(cat ${TMP\_FILENAME})"; done &amp;#x200B; \#-- This should show you that everything worked :) declare -p Xout1 declare -p Xout2 \`
Let's see if this works. (I can't seem to get copy/paste working correctly to Reddit's comment editor, for some reason) This pattern allows you to define a set number of "child" forked processes, but still run through a set number of iterations. `func () { local TMP_NAME="${1}-${2}" local TMP_FILENAME="/tmp/Xout-${TMP_NAME}.tmp" #-- Remove the temp file, if it exists. if [ -e ${TMP_FILENAME} ]; then /bin/rm -f ${TMP_FILENAME}; fi #-- Xin is some array generated elsewhere in the code for nn in $(seq 0 $((( ${#Xin[@]} - 1 )))); do #-- Output the values to a unique TMP file, where we can retrieve it later. echo -e "Xout1[${TMP_NAME}-$nn]=\"$(foo1 "${Xin[$nn]}")\"" &gt;&gt; ${TMP_FILENAME} echo -e "Xout2[${TMP_NAME}-$nn]=\"$(foo2 "${Xin[$nn]}")\"" &gt;&gt; ${TMP_FILENAME} done } #-- This is the number of total tasks to run. MAX_TASKS=80 #-- This is the maximum number of child processes to execute per-iteration. # It's to make sure we don't overload the system. MAX_CHILDREN=4 #-- Loop through all iterations. # Each one generates a unique temp file. We retrieve the contents late for N1 in $(seq 1 $(( ${MAX_TASKS}/ ${MAX_CHILDREN} ))); do for N2 in $(seq 1 ${MAX_CHILDREN}); do #-- Start up to MAX_CHILDREN child 'func' calls. # Don't fork the child, if we are running the max amount for this iteration. # This ensures that we don't overload our system. if [ ${N2} -lt ${MAX_CHILDREN} ]; then func ${N1} ${N2} &amp; else #-- Print a status msg to STDERR. Let the user know that we are still "alive". echo "[Run time ${SECONDS} seconds] Max children per-iteration reached. Not forking." 1&gt;&amp;2 func ${N1} ${N2} fi done done`
I've had a bit of experience with Bash "parallelization" and Bash. I tried my hand at your problem. This allows you to set a specific number of iterations to run, but also a finite of child processes that can be run in parallel (to avoid overloading the system). Reddit formatting was getting foobar for some reason, so I created [this public Gist](https://gist.github.com/zish/9fa639c134e9e29474f9cc2b3a532319).
Parallelism and Concurrency are where I draw the line on where I should stop using Bash. Please, for the love of all that is Holy, maintainable, and intelligible: use the right tool for the job. If you've got anything more than a few branches in your logic and serial transactions then you should be using Python, Golang, PHP, or even Ruby. If you've got a LOT of transactions, then use Golang or even Java/C++. But don't use concurrency with Bash. Your colleagues will hate you. Your boss might never promote you because of the brittle nature of your code and the fact that no one else will want to work on it after you got it working. 
No worries. It has made my life a lot easier. Grabbing random data and piping it to a sql query has been a godsend :) but context is key and also the -n is almost mandatory to get useful stuff out ;)
&gt; Parallelism and Concurrency are where I draw the line on where I should stop using Bash Typically id agree, but the main purpose of the function is building RPM packages from source SRPMS, and to my knowledge there arent equivilant functions to `rpm`, `rpmbuild` and `dnf` in other languanges, and these are sort of essential. The loop im trying to speed up is a relatively minor par. That said, it is a particularly annoying minor part that takes 5-10+ minutes when I know it could be reduced down to a couple of seconds (a few other loops i was able to replace with function calls acting on a list of values, so I know how fast it *can* be done in).
&gt;minor part Yeah. Its minor now, for you. I can tell you from personal experience (working on Build Engineering team at a big cloudy company) that new team members will run into problems with your 5-10 minute time saver -- and they will need help. They will spend ***days*** f\*cking with it to get it to work ***when*** it breaks. As an aside: DNF, Yum, and RPM are written in Python. Debian packages are written in Perl. You have to wonder why the people who created and maintain those tools didn't use Bash (I'm sure they knew it well, but chose to use a scripting language instead). 
Thanks for sharing this. ill probably use this in the future (I like being able to have some control over how much parallelization you use); though for this problem it gave me an idea for a solution that i think works quite well. The idea is to use something like the following # aa = &lt;some array&gt; Lmax=$((( $(wc -L &lt;(echo ${aa[@]} | sed -E s/' '/'\n'/g) | sed -E s/'^([0-9]*).*$'/'\1'/) + 1 ))) # maximum line length (in bytes) + 1 N=${#aa[@]} # number of elements in the array # make sparse file with N * Lmax elements using writeless sparse allocation touch tempFile fallocate -d -o 0 -l $((( $N * $Lmax ))) tempFile # write elements in a forked process using `dd`. have each dd call skip ahead by nn * Lmax bytes --&gt; each write happens in a unique block of Lmax bytes for nn in $(seq 0 $((( $(echo ${#c[@]}) - 1 )))); do dd if=&lt;(echo ${aa[$nn]}) of=tempFile seek=$((( $nn * $Lmax ))) oflag=seek_bytes &amp; done # read the data back from disk Xout=($(cat tempFile)) My disk is fast enough to not have the I/O be a problem, but if that wasnt the case Id imagine you coulld make `tempFile` as a ramdisk instead of a standard file
I love the nudge towards scp. 
While I've already posted a `sed`\-based solution, I should note that it breaks horribly if the variable you're substituting is a multi-line string. Here's a generic pure-`bash` solution that works under all circumstances, including the case where the input is a full script that populates your desired variables programmatically. In essence, we do the following in a subshell (to isolate shell environment changes): * `source` the input, letting it do whatever it needs to set things up * use `declare` to set new vars to the same values as the old vars * use `declare` again over the new var list to get a `source`able script &amp;#8203; $ cat map_vars.sh #!/bin/bash trap "rm -f /tmp/dest.vars" EXIT declare -A var_map=( [x_sonic_hedgehog]=x_snd_appli [y_boring_flamethrower]=x_route_from_xfer [z_scp_ftw]=x_ftp_command ) ( . src.vars for new_var in "${!var_map[@]}"; do old_var=${var_map[$new_var]} declare $new_var="${!old_var}" done declare -p "${!var_map[@]}" &gt; /tmp/dest.vars ) . /tmp/dest.vars echo "x_sonic_hedgehog=$x_sonic_hedgehog" echo "y_boring_flamethrower=$y_boring_flamethrower" echo "z_scp_ftw=$z_scp_ftw" $ cat src.vars x_snd_user='Jane Smith' x_snd_appli='sox' x_snd_text='People get ready' x_rcv_user='John Doe' x_rcv_appli='vlc' x_rcv_text='Damp parkas' x_dup_from_xfer='0' # Here's a dynamic variable x_route_from_xfer="$(date -d "7 days ago" +%Y-%m-%d)" x_route_to_xfer='2' x_reply_by_xfer='3' x_reply_to_xfer='4' x_routed_to_XIB='N' x_end_xfer_script='echo "DONE"' # And here's a multi-line one x_ftp_command='scp, serious.ly! You should be using scp. Right?' $ bash map_vars.sh x_sonic_hedgehog=sox y_boring_flamethrower=2018-12-15 z_scp_ftw=scp, serious.ly! You should be using scp. Right? 
&gt;pushing the envelope in a shell forum &gt;"don't use shell" -everyone in forum This accepts an arbitrary number of functions as args. Uses shared memory `/dev/shm` as rendezvous to save disc ops. The `{ command group &amp; } 2&gt;/dev/null` is for silent job control. foobar(){ #foo1 #foo2 #fooN Foo=("$@") shm="/dev/shm/foobar" for n in ${!Foo[@]}; do for i in ${!Xin[@]}; do { echo Xout${n}[$i]="$("${Foo[$n]}" "${Xin[$i]}")" &gt;&gt; $shm &amp; } 2&gt;/dev/null done done wait 2&gt;/dev/null source $shm &amp;&amp; &gt;|$shm for n in ${!Foo[@]}; do declare -agp Xout${n}; done }
A lot of learning from your reply .Thanks a lot for such detailed explanation 
No prob
Make sure that directory is in your $PATH, and if their execute bit isn't set, add it with 'chmod'. 
I might have led you to falsely believe that I know *something* about bash. But I really don't :D I'm new to Unix systems, been using it for a month. Can you elaborate, please? Very appreciated :)
You could add a line to your `.bashrc` to make sure the folder is in your path. For example, i have a folder called `.scripts/` in my home directory, so my `.bashrc` has the line: `export PATH=$HOME/.scripts:$PATH` Whenever I write a new script, I simply `mv` it to that folder if it isn’t already, and then `chmod +x ~/.scripts/&lt;script name&gt;`. After that, you don’t even need to `source` your `.bashrc`, it will already recognize it from the `$PATH`
I knew before I started replying that it would be faster and easier to type the actual commands in the reply than to describe them, but I don't think it helps much to "give a man a fish." When you googled "add directory to bash path" and "set execute bit for script" was there something confusing you'd like us to clear up? Or did you not try that?
Do I understand it correctly that the .sh file has to be in the home directory? Can't I just reference it like so?: &gt;chmod +x /usr/local/bin/script.sh Also, why all of a sudden they use a plus sign instead of a minus sign for options? *man chmod* doesn't give me any information on pluses, instead it lists options with minuses. &amp;#x200B;
Another thing is the &amp;PATH variable. Do I really need to update its list of directories, if I will add an alias for *rmtrash* and *rmdirtrash*? &gt;alias &lt;Name\_with which you wanna call it &gt;='/usr/local/bin/filename.sh' It already specifies where to look for the scrip,t right?
Now those are good questions. You can reference the file with the full path like that, yes. `chmod` uses plus signs because the bits you're changing can be both added and removed - `chmod -x` is the opposite of `chmod +x` 
Thank you very much for answering! :)
Yep, if you specify the entire path in the alias it will work without needing to use your $PATH. 
I tried implementing the command into bash, but it somehow didn't work and bash still responds '/usr/local/bin/rmtrash no such file or directory'. What I did: * added x bit to both rmtrash and rmdirtrash in their respective directories * edited \~/.bashrc adding on the last line their aliases, swapping their full paths for names: rmtrash and rmdirtrash * updated bash with "source \~/.bashrc
`no such file or directory` is pretty straightforward, run `ls /usr/local/bin/rmtrash`, I bet it's not there where you meant to put it. 
Yes, that was exactly the problem. I managed to get bash to recognise both commands. You've been very helpful to me. For future readers: install *trash-cli* as well, or else the commands won't work. Also for future readers: trash is found under \~/.local/share/Trash/files &amp;#x200B; &amp;#x200B;
I'm guessing the `$LS_COLORS` environment variable is not set?
That's almost certainly the case. `LS_COLORS` is usually set by running: ``` eval "`dircolors -b`" ``` somewhere in `${HOME}/.bashrc`. I've never seen it in the system-level `/etc/bash.bashrc`, so if you have a hand-crafted `.bashrc`, you'll have to add the above line in somewhere.
I'm getting a headache just thinking about scaling that. unownist(){ gawk -F\' -v patterns="$*" ' BEGIN{ split(patterns,p,OFS) } #OFS is one Space by default { for(i in p) if($1~p[i]) printf("%s=\"%s\"\n", p[i], $2) } ';} Process standard input. Split argument string into array `p`. If any element of `p` matches the first column, print a bash variable declaration of that element with second column as value. $ unownist rcv_user dup end_xfer &lt; this.txt rcv_user="" dup="0" end_xfer="" Use process substitution to source that output. $ . &lt;(!!) . &lt;(unownist rcv_user dup end_xfer &lt; this.txt) this.txt is your file from above, but it could just as easily be another process substitution if you wanted to bring in command output. 
Parallel doesn't ship with RH Santiago, so I have to work around it. Job Control is capable, but requires rendezvous. The following accepts an arbitrary number of functions/commands as args. Uses shared memory `/dev/shm` as rendezvous to save disc ops. Silences job control by redirecting the stderr of a command group. foobar(){ Foo=("$@") shm="/dev/shm/foobar" for n in ${!Foo[@]}; do for i in ${!Xin[@]}; do { printf "Xout${n}[$i]=\"%s\"\n" \ "$( "${Foo[$n]}" "${Xin[$i]}" )" &gt;&gt;$shm &amp; } 2&gt;/dev/null done done wait 2&gt;/dev/null source $shm &amp;&amp; &gt;|$shm for n in ${!Foo[@]}; do declare -agp Xout${n}; done } Basically we're simulating a multidimensional array `Xout` by appending very specific declarations to a file. When we're finished, we source those declarations and declare the resulting arrays. /u/jkool702, please let me know if you have questions or issues. 
Omit the `-o` flag. If you want each match to stand out use `--color`. 
Add /dev/null as your last parameter. It should show you the file name because you're telling it multiple files to look at so it has to show where the match is. 
Probably not possible with even GNU grep, so here's a solution with awk: awk -v re="load[(]'[^']*'[)]" '{ while (match($0, re)) { print FILENAME ":" substr($0, RSTART, RLENGTH); $0 = substr($0, RSTART + RLENGTH) } }' -- *.php
I'm unable to replicate your problem: ``` $ head *.php ==&gt; file1.php &lt;== load('blue'); ==&gt; file2.php &lt;== load('green'); load('yellow'); ==&gt; file3.php &lt;== load('black') $ grep -o "load('[^)]*)" *.php file1.php:load('blue') file2.php:load('green') file2.php:load('yellow') file3.php:load('black') ``` What does your `file2.php` actually contain?
grep -H
This isn't relevant if OP doesn't have access to GNU/Linux, but gawk `match()` has an optional third argument which puts all matches in a specified array. Much more fun than substrings. geirha(){# &lt;regex&gt; [file] ... [fileN] re=$1; shift gawk -v re="$re" ' match($0,re,a){ print FILENAME ":" for(i in a) print a[i] OFS }' $@ } 
This is from a Managed File Transfer application (mass file transfering in and outside the network), scp won't do us any good... :-)
It's not far off to imagine the user has GNU awk but not GNU grep (and vice versa), but you're missing a bit there. 1. You still need a loop to iterate the array 2. The array will start with 1 as first key, not 0 3. `$@` should be quoted 4. `re` should be local to the function -- HenryDavidCursory() { local re=$1; shift gawk -v "re=$re" ' match($0,re,a){ for (i in a) printf("%s:%s\n", FILENAME, a[i]) } ' "$@" }
Did you actually test that loop? I don't think it means what you think it means.
&gt; Did you actually test that loop? Touché. I did not. So you'll still need a loop involving substr like my original. &gt; Why declare `re` as local? Because in bash, variables are global by default.
`match($0,re,a)` is the condition; it will be applied to every line. henry work 18:24:38ET ~ [988]$ geirha '#.*#' &gt;foo #one two three# bar -:#one two three# ^C 
&gt; `match($0,re,a)` is the condition; it will be applied to every line. Right, so your solution works if there's at most one match per line, but OP specifically needs it to work for lines that have multiple matches.
This may be way off base, but my first thought is that the location doesn't matter as much as the file permission. Home directory, 0400.
All of my script contain a source master.sh that contain all the Keys to all the things as well as a small library of global functions. 
Usually, such keys are stored in environment variables. 
Something like [this](https://www.learnhowtoprogram.com/javascript/ember-extended/api-keys-as-environment-variables)? That looks like the perfect solution.
I'd recommend hewing to the [XDG Base Directory Specification](https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html), especially if you plan to release your script for others to use. That way, every user (including yourself) and/or tool can easily find the files your script drops in their user spaces, and adjust/clean them up where necessary. Since an API key is a configuration item, the XDG spec recommends it be placed in `$XDG_CONFIG_HOME` (defaults to `${HOME}/.config`). Look in that directory on your desktop system, and you should find more than enough examples.
Unnecessary with GNU grep (which doesn't show the OP's problem), and gives the same output with BSD grep (probably what the OP used).
Yes. I usually have a file with the keys. Don't include it in source control. Source it before running. 
OK, so for this method, if might be best to store a script_name.conf file in a new subdirectory here, and to set the directory variable with something like this? if [ "${XDG_CONF_HOME}" == "" ]; then KEY_DIR="${XDG_CONF_HOME}/script_name" else KEY_DIR="${HOME}/.config/script_name" fi
If `grep -Ho` doesn't work for you, try this gawk loop. janderson(){ #janderson &lt;regex&gt; [file1] ... [fileN] re=$1; shift gawk -v re="$re" '{ while(match($0, re, a)){ printf("%s:%s\n", FILENAME, a[0]) $0=gensub(re, "", 1) } }' "$@" } 
There's a much less verbose way: ``` KEY_DIR="${XDG_CONFIG_HOME:-${HOME}/.config}/script_name" ``` 
&gt; Usually, such keys are stored in environment variables. Unfortunately this is true, but environment variables are not particularly secure. They leak into other processes extremely easily (especially through a process-management language like Bash), and any other processes running as the same user can easily read them out of `/proc/$pid/environ`. If at all possible, make the program needing a secret read it itself from some configuration file. Alternatively, secrets can be made available to a particular process hierarchy only by using the kernel keyring.
This code uses Unix epoch time which is an absolute number of seconds elapsed since a specific date. This is the best way to work with time in bash since you can use integer arithmetic and comparisons to deal with any typical duration of time. Is there a specific step or line that you have questions about?
The actual if test, that's comparing seconds, or days?
&gt;I'm having some trouble understanding this code Probably because, as Phil Karlton once said: &gt; There are only two hard problems in Computer Science: cache invalidation and **naming things**. Some of the names in the above code snippet...could've been better. Here's a cleared-up version: ``` expiry_days=30 let expiry_secs=$expiry_days*60*60*24 last_modified=$(stat -c %Y $BASH_SOURCE) secs_now=$(date +%s) let secs_old=($secs_now-$last_modified) if [ $secs_old -gt $expiry_secs ] ``` 
Seconds. The first line is in days, the second line converts that into seconds, and then everything else is just in seconds. 
Even better. Thanks!
Also suggest making a function called something like "file\_last\_modified\_days" and making it re-usable. if (( $(file\_last\_modified\_days"${file\_path}" &gt; 30 )); then ... fi &amp;#x200B;
By choosing not to use a variable don't you give up any chance of automating this? The prompt will always require human input, no? I would prompt if var not set, but if var set, do not prompt. This gives you options at least. Re: solution, create .tmp file in $HOME/tmp and set var to the .tmp file. Read from the .tmp file when you need the value. Make sure you clean up .tmp files someplace.
The user will only be prompted to input the API key the first time the script is run, during the initialization phase. After that, as long as the stored API key remains valid, no further input will be required. I'm working with the solution /u/anthropoid proposed using the `$XDG_CONFIG_HOME` method. Once the key is stored in the `conf` file, the script will check for its existence, test its validity against the REST API, and then proceed as normal.
I'm all for clean security practices, but what's the realistic attack vector here? If we operate under the assumption that other processes on the same machine can't be trusted, I think you've got bigger problems. A config file containing the secret won't save you there, either. I think environment variables are the correct solution here. Even in the worst case, a leaked key isn't _that_ damaging as it can just be revocated and reissued.
The big problem I have with environment variables is that they are inherited across process execution, and you have to go out of your way to stop that.
Great tip. Context: what is XDG and when is it appropriate to use it?
In the future, please run the following on your script before pasting it here: `sed 's/^/ /' [script]` You can nest loops! read -a PatchNumbers -p "Enter patch numbers separated by spaces: " for p in ${PatchNumbers[@]}; do for f in #FILE LIST GOES HERE #COMMANDS #GO HERE #((nmbr++)) done done 
Excellent questions. The XDG Base Directory Standard (BDS) is a simple specification of where an application's data (config, data, cache, etc.) should go in a user's filesystem. While it lists specific directories for this purpose (under the user's home directory), it also allows users to override these settings for BDS-compliant applications via environment variables. This means BDS-compliant applications allow users to *run simultaneous instances with different isolated "spaces"* for any reason at all. For example, if the official Dropbox client were BDS-compliant, I could sync with multiple Dropbox accounts cleanly, by simply providing different values for `$XDG_CONFIG_HOME`, `$XDG_DATA_HOME`, etc. Instead, I'm forced to use a hack solution of multiple fake `$HOME`s to achieve the same goal. Yuck. As to when it's appropriate to use them, I'd say "as often as you can stand it". With tiny one-off scripts for your personal use, it's perfectly reasonable to scatter your config and data wherever you see fit, but if it's a serious program for general consumption, it's usually a good idea to employ BDS in your scripts/applications. If nothing else, the user ability to define multiple data spaces is a boon that's often not initially obvious. And, of course, having a unified way of designating spaces for your app's stuff means that *third-party management tools* can easily clean up when needed. As for choosing BDS over some other standard, I'm not aware of any other standard, so there's not much to choose from. Sidenote: I'm sometimes asked "why should I use BDS for command-line stuff?" Yes, it's true that the originating *XDG* is short for **X Desktop Group**, now known as freedesktop.org. However, there's nothing in the BDS that's specific to GUI applications; popular command-line apps like [youtube-dl](https://rg3.github.io/youtube-dl/) use BDS, so you should too. (I should probably flesh this out into a proper blog entry.)
&gt;for p in ${PatchNumbers\[@\]}; do for f in #FILE LIST GOES HERE \#COMMANDS \#GO HERE \#((nmbr++)) done done Thanks , you mean after the first done or before the second for I should insert my 'Action1'
Interesting because it looks nice without needing powerline-fonts installed. personally I prefer [pureline](https://github.com/chris-marsh/pureline) (and I assume everyone has their favorite powerline version)
Ouch, you running centos 5?
" if \[ "$1" \]; then i " &amp;#x200B; for what this "i" stand for?
Remove the i
Bash will read everything after `then` as a command to run. If you have `i` followed by a newline, then `grep`, this is what happens: Bash will see `i`, and then look for a binary named `i` in the execution list for `PATH`. If the binary with that name isn’t found, it raises an error “command not found”. Since it can’t find `i`, it never looks for `grep` but just exits.
What's the output of this? `set -x; trazi "lorem ipsum"; set +x`
omg I'm so stupid xD I didn't even noticed THAT "i", I thought it was throwing error because of "-Ri".. 
What wget does.
[https://curl.haxx.se/](https://curl.haxx.se/) &amp;#x200B; &amp;#x200B;
[https://www.gnu.org/software/wget/](https://www.gnu.org/software/wget/)
Potentially fuck your system, never use it.
Could you elaborate?
Care to elaborate?
Sometimes it helps to have an extra pair of `i`s....... 
he wants to say: curl does what wget does, i think so. 
seafly is in the [Pure ZSH](https://github.com/sindresorhus/pure) style, a simple, yet informative, prompt that uses Unicode symbols and no reverse colors (though single line tall rather than double in Pure's case). The powerline family of prompts is very different flavour. Good to have choices. Cheers.
It does what a browser does, makes requests but over the command line. I use it when testing apps I make. I can make very sophisticated requests to my projects, just like if a user was using an app or another browser.
Only if you pipe it into your shell without reading what you're doing.
I use wget for downloading files, curl I use for checking things more than anything else. I know curl CAN be used to download files, but I always forget the args.
Yet again, you have no idea what the fuck you're talking about.
I have this in my .vimrc inoremap kj &lt;Esc&gt;
You only need to write one action, which goes in the second for loop.
Did you curl bash a script as root without checking what it does first? curl is incredibly useful and won't "fuck your system" if you use some common sense.
What makes you think that? Type curl cht.sh And then wget cht.sh And compare what happens. 
What directory does it say you're in, and what is the exact output of ls?
I'm in the home directory. And ls just gives the username of my laptop when I run it
Been awhile since I used it, but just figure it out, enter: `cd /mnt/c/` and that should take you to your C: drive root.
Thanks! I was able to navigate from here to the desktop, but I find it annoying how when I type ls, it shows that some files can't be accessed cos access has been denied
Can you provide an example of a file you had a problem with?
You got close, but just barely missed it. \`rm \*{1..46}.jpg\` should work. Two dots are number expansion in bash.
But what did **[1-46]** do? Why were only certain numbered files get deleted?
Doesn't answer the question, but you kinda took the square peg approach. Why not just `rm 0*.jpg`?
[1-46] is a shorthand of [12346] which matches 1, 2, 3, 4 or 6
Thanks
Thanks!
LOL wanted to try it out.
 #!/bin/bash echo hello Don't use the `-c` flag. The shebang (`#! /path/to/interpreter`) is an instruction to the shell to use specified binary to interpret the file *Shell* `#!/bin/bash` &lt;-- shell, use this binary to interpret file *Interpreter* `#!/bin/bash` &lt;-- Line begins with a hash. This is a comment, ignore. `echo hello` &lt;-- output text
I can cd into ‘My Documents’, but when I run the ls command it says. ls: cannot read symbolic link ‘My Documents’: Permission denied. Same for when I cd into Administrator when I’m in the Users directory. When I run ls it says ls: cannot open directory ‘.’: Permission denied
`rm {00000001..46}.jpg` is a good form to remember, too. It will work for exactly those 46 files and no others, in case you ever want to delete those files but e.g. not 00000101.jpg, and will let you create a numbered sequence with leading zeroes that doesn't already exist for globbing. 
Yes, I can do that. I can put a command on the next line and give it the rest of the file as input. Good enough. Thanks. &amp;#x200B;
&gt; Good enough There's no "good enough" to it. This is the proper way to do it.
I believe the bash syntax you're looking for is `{1..46}`
Addendum, from your comment &gt; What I want to accomplish, after I figure it out, is to give bash a command and use the rest of the file as input to that command I think you may be misunderstanding how this works If you don't want bash as the interpreter, then just use any other interpreter you want. Without concrete details, I can only guess, but as an example, lets say you want to use PHP Then this will work #! /usr/bin/php &lt;?php echo "I am PHP\n"; ?&gt;
Slight correction: {} is brace expansion and .. is a sequence expression (either integers or single characters). As of Bash 4 you can even increment letters, e.g `{a..z..2}`.
What I actually want to do is to pipe the error output from the command through sed. But even if I use bash on the #! line, I still haven't figure out how to do that without also including stdout in the input to sed. &amp;#x200B;
 #!/bin/bash sample_output() { echo "I am output to stdout" echo "I am output to stderr" &gt;&amp;2 } another_sample() { echo "1234" echo "abcd" &gt;&amp;2 } # Call sample_output function # Redirect stderr to stdout # Redirect stdout to /dev/null (discard) # pipe through grep sample_output 2&gt;&amp;1 &gt;/dev/null | grep '^I am output to' # Call another_sample function # Redirect stderr to stdout # Redirect stdout to /dev/null (discard) # pipe through sed another_sample 2&gt;&amp;1 &gt;/dev/null | sed 's|^|==sed substitution== |'
That's very good, except that I want stdout to go where it normally goes, while the error output goes through sed.
Like this? #!/bin/bash sample_output() { echo "I am output to stdout" echo "I am output to stderr" &gt;&amp;2 } # Call sample_output function # Redirect stderr to stdout # Redirect stdout to std-stream #3 (which we're using as a temp placeholder) # pipe through grep # Redirect std-stream #3 back to stdout { sample_output 2&gt;&amp;1 1&gt;&amp;3 | sed 's|^|==sed== |' } 3&gt;&amp;1
 #!/bin/bash -c "echo hello" That doesn't do what you think it does. If it's in `myscript.sh`, then `./myscript.sh arg1 arg2 arg3` actually resolves to `/bin/bash '-c "echo hello"' myscript.sh arg1 arg2 arg3`, i.e. everything after `/bin/bash` in your [shebang](https://en.wikipedia.org/wiki/Shebang_(Unix)) line is passed to `bash` *as a single argument*. Poor `bash` then goes "oh, my user just gave me *fourteen single-character options*: `-c`, `-_`, `-"`, `-e`, `-c`, `-h`, `-o`, etc. so forth". The error you got confirms that it choked on the second option, which is `-_`. (NOTE: `-_` is actually meant to be `-` followed by a space. I just haven't figured out how to convince Reddit to preserve that space in inline code.) &gt;What I want to accomplish, after I figure it out, is to give bash a command and use the rest of the file as input to that command, and then pipe the output through sed. That statement is somewhat ambiguous. Here's my take: $ cat input.sh #! facilitator.sh This is a test. It demonstrates that the program launched through a shebang can itself be another script, as long as it can be run normally. It also demonstrates that the "program" doing the launching need not actually be a program. This is, after all, not a valid shell script. $ cat facilitator.sh #! /bin/bash # First arg is the calling "script", so: # your_command_here "$1" # or, since its caller probably passed additional arguments: # your_command_here "$@" # but for demonstration purposes, we'll do: sed 's/^/FACILITATED: /' &lt; "$1" $ ./input.sh FACILITATED: #! facilitator.sh FACILITATED: This is a test. FACILITATED: FACILITATED: It demonstrates that the program launched through a shebang FACILITATED: can itself be another script, as long as it can be run normally. FACILITATED: FACILITATED: It also demonstrates that the "program" doing the launching FACILITATED: need not actually be a program. FACILITATED: FACILITATED: This is, after all, not a valid shell script.
Does this work? command 2&gt;&amp;1 1&gt;/dev/null | sed No idea if I'm using best practices or not.
In a nutshell: my_command 2&gt;&amp;1 &gt;/some/where/else | sed Remember that redirections are resolved in the order they're specified, so the above: * copies the standard output file descriptor to standard error (hence pointing at the pipe), then * redirects standard output to `/some/where/else`
Thank you -Great help 
Sounds like you got your answer but it would have been wise to test the shell expansion you were going to send into rm to ls FIRST. Then you would have had a clue you weren't going to delete what you wanted. 
[removed]
&gt; Don't use the -c flag. Important to note, there is no ~spoon~ `-c` flag. That is, while bash does support the `-c` flag on a direct call, the interpreter hasn't finished determining how to invoke bash, and with the shebang invocation, only the flags for `set` are supported.
Why all the hard work? sample_output 2&gt; &gt;(sed 'script') Excuse me if I've misunderstood.
Try this: sample_output 2&gt; &gt;(sed 'script') It should redirect stderr to a process substitution, leaving stdout alone.
Pipe stderr to sed; leave stdout alone. foobar 2&gt; &gt;(sed 'script')
I like your attitude
Yes, that works. It seems weird, but it works. &amp;#x200B;
Yours works fine. Where would I find it in the bash manual? "2&gt;"? I wouldn't even know what to search for. I only find 2&gt;&amp;1. &amp;#x200B;
I found it in the RHCSA objectives, lol.
I didn't know that, nice! Thanks!
So this is really going to depend on what you're actually trying to do, but Sed has two buffers: Pattern and Hold. Pattern space is the current buffer (by default the current line) and Hold space is like a spare. You can use `N` to append the Next line to the current Pattern buffer, then operate on it. The following sed finds lines containing "foo", expands the Pattern Space to the Next line, and deletes the Pattern Space. sed '/foo/{N;d}'
Yes, that's exactly the answer I was looking for, and does exactly what I wanted. Thanks.
I love posts like these. When I was new to C programming I once wrote something like: #include "stdio.h" int main() { int x = 0; int myArray[10]; printf("x is %d\n", x); while ( x != 10); { printf("x = %d: What is going on1!?\n", x); myArray[x] = x; x++; } return 0; } Can you spot the problem? `while ( x != 10);` should be `while ( x != 10)`. The extra semicolon was saying `while (true){ //doNothing }`.
&gt;So writing `#!/bin/bash -c "echo hello"` is loosely equivalent to: #!/bin/bash set -c "echo hello" Nope. If that were true, you'd get a different error message: $ set -c "echo hello" bash: set: -c: invalid option set: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...] i.e. it would complain about `-c` instead of `-&lt;space&gt;`. See [my answer](https://www.reddit.com/r/bash/comments/aa0odi/binbash_invalid_option/eco601y/) for an explanation of what actually happens behind the scenes. `#!/bin/bash -c "echo hello"` is almost exactly equivalent to: #!/bin/bash set '-c "echo hello"' "$@" &gt;Only they give different errors because one is thrown by the interpreter, and one is thrown by bash. Nope, `bash` throws the error in all cases, and I'm not sure what you mean by "the interpreter".
&gt;Yours works fine. Where would I find it in the bash manual? Under **Process Substitutions**: &gt;Process substitution allows a process's input or output to be referred to using a filename. It takes the form of &lt;(list) or &gt;(list). The process list is run asynchronously, and its input or output appears as a filename. The classic-shell equivalent of the sample command is roughly as follows: ``` TMPF=$(mktemp -u) mkfifo $TMPF sed 'script' &lt; $TMPF &amp; sample_output 2&gt; $TMPF rm -f $TMPF ```
didja figure it out, buddy?
&gt;Nano \~/.bashrc &gt; &gt;\#Record Terminal Logs &gt; &gt;test "$(ps -ocommand= -p $PPID | awk '{print $1}')" == 'script' || (script -f $HOME/Documents/logs/$(date +"%d-%b-%y\_%H-%M-%S")\_shell.log) sry ive been gone for a while :O ye its a bashrc (not that i know much about the differences, reading up on it now)
That `,+1` you tried to use works like this: sed -r '/your-search-here/,+1 d' This will trigger on the lines that matches your search pattern, then apply the `d` command to that line and also the next line because of the `,+1`.
What if you wanted to delete just the line after foo, but keep the foo line?
LOL Thanks! Same here!
Thanks! Will safely tread next time!
Lowercase `n`; see my edit
Thanks!
I do assume the pattern is `SxxEyyy - &lt;rest&gt;` and you want to renumber the yyy part to a two digit part to show the correct name for the TV show, so KODI or similar will process it correctly. Is that correct, or are there any other thiongs going on with the yyy part. e.g. is it sometime one or two characters, is it sometimes 4? If it is only 2, then a loop would be easy enough, I think.
Not at my computer to write said script but maybe I can offer a starting point. If your existing scripts are patterned in that they can at least be sorted (i.e. can you do this: `ls -1 | sort`) then you’re in a good starting point. If not, then there needs to be a step zero where you normalize the naming scheme, otherwise it becomes more difficult. Assuming the files have a standard naming pattern, here’s how I would tackle it: use something like shopt -s nullglob myfiles=(/path/to/files/*) (https://mywiki.wooledge.org/ParsingLs) Now iterate over the array. c=0 for i in "${myfiles[@]}"; do ... done Within the loop you do the renaming. You need to pull out the “filename” portion you gave as an example. `sed` would work, but here’s a quick and dirty way: filename=“$(echo “$i” | awk ‘{print $3}’ | awk -F ‘.’ ‘{print $1}’)” The first awk would give you `Filename.ext` since it splits on spaces, then the second gives you `Filename` cause you specify the period as the deliminator. There’s tons of ways to do this part. Get your episode number: epnum=“$(printf ‘%02d’ $c)” ((c++)) That will pad the number with a leading zero so it’s always two characters long. Now you can do your `mv`. mv “$i” S01E”$epnum”_-_”$filename”.ext And I think that’s it? Doing this all on mobile and from memory so please excuse any errors. Hope it helps. Big warning though: make a backup of your files first!! Or make a test batch first. Don’t just run this on your original files.
You'd be correct, its shows that I'd like Plex to process, it works as it is but just for personal preference I'd like it to be named a bit nicer, rather than Episode 348 etc. So far, its only upto 3 digit numbers but it make wind up making it to 4 digits though.
Thanks so much, I'll test it out right now, and yea I've already accidentally *almost* wiped my server because of a typo so I try my best to be extremely careful now and wrap everything in an if statement so if the directory doesnt exist, it doesnt still try to run the commands which was where my error came up last time.
Also use `echo` in a first run in front of the `mv`, so you van see what it would do. Only if you are happy, remove the echo.
So I seem to have it, with one exception. I setup this: if [ ! "$filename" == "$filename_old" ] then filename_old="$filename" ((c++)) fi to check if the filename matches so that I dont end up with S01E01 - Video1.mp4 S01E02 - Video1.srt S01E03 - Video1.description &amp;nbsp; It works, except the first file is always 1 number behind: S01E00 - Video1.mp4 S01E01 - Video1.srt S01E01 - Video1.description 
So did you edit the original question to fix the problem? Make it hard for those who come along later to see what the fuss was about. If you're going to edit it, at least make it clear what the original problem was!
Assuming you're using extended globs already, if all you want to do is strip known patterns from the name, you can just use alternations like: mv "$file" "${file##+([0-9])@( - |. )}" Otherwise, you can use `case` like: case "$file" in +([0-9])' - '*) : do something ;; +([0-9])'. '*) : do something ;; esac For more advanced (and probably better-performing) bulk file renames you might want to look into Perl's `rename` utility. There are a few other tools designed for that kind of thing, but I think it's the most well known
Awesome, thank you so much, I didn't know I could use alternations in there! My other question now is that you said Perl would be better performing, but I was under the impression that Bash actually beat out all the other languages just because it is native to Linux, am I wrong in that? Thanks again!
I'm not sure what you mean by "native to Linux". bash isn't Linux-specific, and I can't think of any sense in which it'd be more "native" than Perl. Because you're (presumably) using bash as your interactive shell, there *are* some things that can be faster to do within that shell as compared to calling "external" utilities, due to the overhead associated with starting those other utilities up. But that overhead is usually measured in milliseconds, and once another interpreter like AWK or Perl or Python is running it tends to make up for the initial delay by executing code faster than bash. For example, comparing apples to apples, looping over files or lines of text is generally faster in other languages than in the shell, just due to the way loops are implemented in the latter. Also, in this particular case, each iteration of your loop will require invoking the `mv` utility, which means that start-up overhead comes into play for every single file you rename. But the kind of speed difference we're talking about usually isn't very important for most things like this. If your bash script takes 50 ms to rename the files versus 5 ms for Perl to do it, you're probably not going to notice. So, unless you're renaming thousands of files, just pick the one you like better. If you want to stick with "pure" bash that should be fine.
Ahh, sweet I learned something new haha. Thank you for that break-down there.
Use a `RunningTotal` of duration instead of the fixed count of 25 items.... and then feed that into [ffmpeg](https://trac.ffmpeg.org/wiki/Concatenate) -` ffmpeg -i "concat:clip65.mpg|clip12.mpg .... ` The videos really need to be the same spec (size, framerate)... It is possible to concatenate non similar files, but you'll end up with something that looks like a load of `...[12:a:0][13:v:0][13:a:0][14:v:0][14:a:0]concat=n=14:v=1:...` 
I'm not wanting to output a file that long, just a list of files that can be fed into the broadcaster. Thanks anyway though.
So just send the list of names that your `RunningTotal` spits out. No need for Bash..
Let me try to explain this simply: The database has over a thousand entries right now. I need to pull a list that's 2 hours long. It may be 25 items, it may be 90 items, or it may be 3 items. That doesn't matter, as long as the total running time is as close to 2 hours as it can get.
Instead of grabbing 25 items, start with a running time of 0.... I am assuming that your database has the duration as a field... Add random video not played in less than 21 days to your result list... and add it's duration to running time.... Repeat until running time &gt; 120 minutes. return result as `filler`
Pure Bash: zach(){ unset index for file in ~/path/File*; do ((index++)) dest=$(printf '%s%03d' "${file%???}" $index) #tricky mv -iv "$file" "$dest" #interactive, verbose done } The #tricky part breaks down like this: 1. `printf '%s%03d'` Format first argument normally, second argument as a digit padded to 3 zeroes. 2. `${file%???}` Remove last three chars from $file. If the number of chars is variable, do `${file%E*}E` to remove everything up to the E before replacing it.
Pure Bash: (Should work with 3+ but please correct me) zach(){ unset episode for file in ~/path/File*; do ((episode++)) #1 season="${file%%E*}" #2 name="${file#*-}" #3 dest=$(printf '%sE%03d%s' "$season" "$episode" "$name") #tricky mv -iv "$file" "$dest" #interactive, verbose done } 1. Silently increment Episode count. We will pad this with Zeroes in the `printf` format string. 2. Get Season by subtracting everything after and including the left-most "E". 3. Get name (and extension) by subtracting everything before and including the first hyphen. The tricky part is the printf format string `%sE%03d%s`, which means "print first variable, print the letter E, print second variable as a digit padded to three zeroes, print last variable.
But is there a search pattern for "already found it, don't go anywhere"?
Why do some of the selected video items not get run? How does the contents of the clip determine whether or not it gets played?
For a shellscript, I would use the `rename` utility, for simplicity's sake: rename -e 's/[0-9]*//' -e 's/ - //' *mp3
SQL to select limited by sum? https://stackoverflow.com/questions/17109602/limit-sql-by-the-sum-of-the-rows-value
Maybe it would be easier to just find a pattern they all follow, grep them, say they’re all starting with a number and a dot, or maybe they all have an extension .mp3 at the end alongside the number at the start. You could egrep that with ^ and $, then mv them to whatever name you like. 
For a shellscript, I would use the `rename` utility, for simplicity's sake: rename -e 's/^[0-9]*.//' *mp3; rename 's/- //' *mp3
What you're describing is a [bin packing problem](https://en.wikipedia.org/wiki/Bin_packing_problem), and in this wild world of open-source, there's a utility for it, [fpart](https://github.com/martymac/fpart). Its nominal role is to take a user-supplied set of directories, and figure out how to divide their contents according to the user's specifications, but the author thoughtfully implemented a mode that takes an arbitrary list and bin-packs those instead. The script below generates a list of 1000 fake videos, each of which is between 5 and 10 mins long. It randomizes the list with `shuf` for good measure, then tells `fpart` to segment it into 2-hour (7200-secs) bins: #!/bin/bash for i in {0001..1000}; do l=$((300+(${RANDOM}%300))) m=$((${l}/60)) s=$((${l}%60)) # fpart's expected list format: &lt;secs&gt; &lt;name&gt; printf "%d Video %s (%02d:%02d)\n" $l $i $m $s done | tee orig.lst | shuf | fpart -a -s 7200 | sort -n -k1 | tee packed.lst I think you can figure out the rest. Oh, and `ORDER BY RAND()` is usually an SQL performance-killer, so I'd just dump out the full list of candidate videos, `shuf` it, then turn `fpart` loose.
**Bin packing problem** In the bin packing problem, objects of different volumes must be packed into a finite number of bins or containers each of volume V in a way that minimizes the number of bins used. In computational complexity theory, it is a combinatorial NP-hard problem. The decision problem (deciding if objects will fit into a specified number of bins) is NP-complete.There are many variations of this problem, such as 2D packing, linear packing, packing by weight, packing by cost, and so on. They have many applications, such as filling up containers, loading trucks with weight capacity constraints, creating file backups in media and technology mapping in field-programmable gate array semiconductor chip design. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
You have it marked as "Solved" so this might not be needed. What I use for my mp3 files is [easytag](https://wiki.gnome.org/Apps/EasyTAG) Not only will it be able to rename the files as you please, it will name the files for you bases on an online database, so you do not need to name the numbers and albums yourself. I have ripped all my CD's and this save my life, making it very easy to do with several hundred CDs I had and adding a new CD is easy after ripping it to my HD.
type in `man curl` to find out. If you go down to `-s` it will tell you. You should learn about the `man` comand. 
is that manual? 
Yeah if you're not sure about a command the `man` (manual) will tell you about the command. 
cool thanks 
When I was a kid, the Sr. guys would say “RTFM”, which stood for “Read the [fine] manual” I’m not surprised that newer guys haven’t heard of it, in the age of google 
thats cool haven't seen it yet. Still getting familar with it. I new about help 
I thought it stood for "read the Fucking manual"
It means “be silent, but shit yourself if something goes wrong.”
It does, but I was being polite. 
FYI \`--sS\` (two dashes) would be an incorrect method of setting those flags, use \`curl -sS foo\`, or \`curl -s -S foo\` instead. Also fyi, the \`-sS\` are shortcuts for the long form.. e.g., \`curl --silent --show-error foo\`. As you learn and get comfortable with \`foo -h\`, \`foo --help\`, \`foo help\`, or \`man foo\`, you will usually see the commands usage printed out for you in the first couple lines of text. &amp;#x200B;
https://www.abeautifulsite.net/installing-composer-on-os-x
Not sure why you linked that. But if you are commenting on the `--Ss` part, I see curl -sS https://getcomposer.org/installer | php It shows one dash, but if you look at your post title, you typed **--sS cmd?** With two dashes. That's the point I was making. It was probably just a typo in your reddit post, but just wanted to make sure you knew. &amp;#x200B;
So why haven't you?
I don’t have an answer for you. I’m sorry. But I do have a question: what are you using that backspace for to where this is a pertinent question? Surely there’s a better way to achieve whatever you’re trying to achieve, no? Can we help you?
&gt; you can noticeably backspace faster than in your X terminal Missing a word? If you want to set your keyboard repeat rate under X, check out your desktop environment's keyboard settings, or use `xset` if you don't use a desktop environment that gives you settings. If you want to set your keyboard repeat rate at a virtual TTY, use `kbdrate`. Note that it requires superuser privileges, as it applies to all VTYs, even ones for which you do not have an active session.
Something helpful about dealing with man-pages: If you start typing with `/` while in the viewer, it will do a search in the text. Something helpful to type for your question would be: / -s This is four space characters in front of the `-s`. It should jump you directly to where the option is described in the man-page. You can repeat a search with `n` and jump to the next search result. And you can type `N` (Shift+n) to jump back to the previous result. The search pattern is not normal text, it is a "regular expression" = "regex" pattern. Interesting things you can do with that are for example: /-s\b The `\b` mean "word-boundary". It makes it so the search will only match things like `-s` or `-s,` and not match things like `--cert-status` where there's a `-s` hidden somewhere inside a word. /^ *-s The `^` means "start of the line" and the `*` means "repeat the previous character". This `^ *-s` will look for a `-s` search result that is at the beginning of a line and has any number of spaces in front of it.
Excellent asnwer, I would add this list of shortcuts too for further reference: [https://gist.github.com/tuxfight3r/60051ac67c5f0445efee](https://gist.github.co]m/tuxfight3r/60051ac67c5f0445efee) I am forever using ctrl-u when I know that I have mistyped a password to save me backspacing 5000 more characters than I've typed. I like Ctrl-L too. And although whoever wrote the info in this URL says it's for bash, I think that these are shortcuts for readline which is (as far as I understand) not specific to any one shell.
xy anyone? 
Regardless of the keyboard repeat rate, there are a number of bash shortcuts worth learning, to speed up your terminal interaction in general: * Alt + d: Delete all characters after the cursor on the current line. * Alt + t: Swap the current word with the previous word. * Ctrl + t: Swap the last two characters before the cursor with each other. You can use this to quickly fix typos when you type two characters in the wrong order. * Ctrl + w: Cut the word before the cursor, adding it to the clipboard. * Ctrl + k: Cut from the cursor to the end of the line, adding it to the clipboard. * Ctrl + u: Cut from the cursor to the beginning of the line, adding it to the clipboard. * Alt + b: move the cursor back a word at a time * Alt + f: move the cursor forward a word at a time * Ctrl + a: move the cursor the beginning of the line * Ctrl + e: move the cursor the end of the line * Ctrl + y: Paste the last thing you cut from the clipboard. The y here stands for “yank”. 
Not sure if helps, but I usually just do `Alt+Backspace` when I want to delete a lot of things fast.
I think find is a better tool to use for this. This would be cleaner in a script, but if you want a one liner try something like: find dir_2 -type f -name *.mp4 -exec bash -c 'vid=$(basename ${1%.*}) &amp;&amp; mv dir_1/${vid}.jpg dir_2/ 2&gt;/dev/null' _ {} \; find dir_2 -type f -name *.mp4 -exec bash -c '' Says, execute a bash command for each \*.mp4 file found. The bash script starts with vid=$(basename ${1%.*}) Set the variable `vid` to the filename without the path or extension. `basename` strips the path. The `${1%.*}` part strips the file extension. So rock.mp4 -&gt; rock. &amp;&amp; mv dir_1/${vid}.jpg dir_2/ 2&gt;/dev/null then mv the file `${vid}.jpg` to `dir_2/`. It will either find the file and move it, or not find it and fail. The `2&gt;/dev/null` ignores those errors. The `_ {}` stuff is so we can get the $1 var instead of $0. 
Do not script with `ls` You can use `for file in dir_1/foo_bar_*.jpg; do blah blah blah; done`
I'd personally do: for f in dir_1/*.mp4; do mv -f "${f%.mp4}.jpg" dir_2/ 2&gt;/dev/null done That simply: * takes every MP4 video in `dir_1`, * strips off its `.mp4` extension, * forms the thumbnail filename by adding `.jpg`, then * tries to move it to `dir_2`. Since it'll print an error if a video doesn't have a corresponding thumbnail, I added `2&gt;/dev/null` to redirect all error messages to the standard Bit-Bucket of Infinity. You can leave that out if you want to be absolutely sure you're not missing any other error conditions than "no thumbnail".
I'd do it with bash itself, without external tools: left="dir_1" right="dir_2" shopt -s nullglob for file in "$left"/*.jpg; do x="$file" # cut off path and .jpg x="${x##*/}" x="${x%.*}" if [[ -f "$right"/"$x".mp4 ]]; then # remove 'echo' if output looks good echo mv -v "$file" "$right"/ fi done You can experiment with it at the command line. Turned it into a one-liner, the code looks like this: for f in "dir_1"/*.jpg; do x="$f"; x="${x##*/}"; x="${x%.*}"; [[ -f "dir_2"/"$x".mp4 ]] &amp;&amp; echo mv "$f" "dir_2/"; done
Why not just use Network Manager and let it change your DNS settings while you connect to different networks.
Thanks for replying. How can I change my DNS settings on different networks?
You typically get stuff like that from DHCP so most dhcp clients (dhclient, whatever is in Network Manager) will it for you.
But I need a fixed IP while at home. If I use something like dhcp, it is not sure that I always get the same local IP.
So just assign it to your Mac in your DHCP server
I'd have to do it via my router and it's not possible because I'm running also pihole and I have deactivated dhcp.
Any DHCP server can do it
Thanks for your answers! I'll look into it.
Don't ever use `ls`. https://mywiki.wooledge.org/BashPitfalls#for_f_in_.24.28ls_.2A.mp3.29
Yeah this is better solved with DNS and not static entries
Dhcp gives out dns servers. And to some of your other posts here: you can use dhcp to get info like ntp and dns server addresses while still using a static ip. 
1) use a hostname that's always set to the external ip address, ie. home.domain.com resolves to 123.123.123.123 with a low ttl, 60s max imho 2) set a /etc/hosts entry in pihole for home.domain.com to 192.168.x.x, any client on the local network will get that ip address for home.domain.com instead of the external public ip Make sure you set a reasonably low ttl to ensure that your laptop looks up the ip address every time that you reconnect to the local network.
&gt;nextcloud server If you're already running a Nextcloud server, you have the resources to run a lightweight internal DHCP/DNS server like [dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html). It's so lightweight that it can be run on off-the-shelf _home routers_ (that have very little RAM and storage) using third-party firmware like DD-WRT, but is still powerful enough to assign static IPs via DHCP (matching each node's MAC address against an allocation list) and "mask" whatever domain names you want with internal IP addresses. No messing with individual `/etc/hosts` required, and is largely a set-and-forget operation. See [the dnsmasq entry on ArchWiki](https://wiki.archlinux.org/index.php/Dnsmasq) for more ideas on what can be done with this one simple tool.
Regexes are your friend here: ``` #!/bin/bash for d in *[Cc]hapter\ *; do # Process only directories with "Chapter xx" somewhere in their names if [[ -d "$d" &amp;&amp; "$d" =~ .*[Cc]hapter\ ([0-9]+).* ]]; then # We've got the chapter number, so let's format it as a 3-digit number... $DEBUG mv -v "$d" "Chapter $(print %03d ${BASH_REMATCH[1]})" fi done ``` First dry-run the script with: ``` $ DEBUG=echo ./rename_comics.sh ``` to make sure it'll do what you expect, then run it for real with: ``` $ ./rename_comics.sh ``` ### Further Reading The `[[ expression ]]` portion of the [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html#SHELL_GRAMMAR)
Since the script is Bash-specific, you could use some nice Bash features, like `[[ ]]` instead of `[ ]`.
Is there an advantage to leaving the expression in \`find expression\` unquoted? I was always using double quotes.
Posts like this are a healthy reminder that the requirements I regularly work with could be much, much worse. 1. You have a directory of (likely stolen) content with no consistent structure. 2. You provided two (2) obfuscated examples, so we have no real concept of the actual problem. 3. The solution you're asking for doesn't seem to account for the fact that there are multiple volumes of chapters, so I can't even visualize an ideal. 4. There is a 75% chance that these "comic" are pornography. 
It's a good habit to quote things unless there is a reason not to. In this case, it was ok, but often it's not. If you want to learn more, here's a few pages that explains word splitting and pattern matching that I like: [https://mywiki.wooledge.org/WordSplitting](https://mywiki.wooledge.org/WordSplitting) [https://mywiki.wooledge.org/glob](https://mywiki.wooledge.org/glob)
Hello, thank you for your reply. 1. I can't either confirm nor deny that statement. 2. I didn't give 2 random examples. That is the shortest pattern of describing them that I could do. 3. I didn't want to account volume into the system so I decided to simply cut it off. 4. This one I can't either confirm nor deny. Most likely because if there's a pornographical comic with &gt;100 chapters, I don't think I'd read it for pornography purpose anyway.
Thank you so much for your answer. I'd test them later and read the expression page you attached. It seems to be what I needed, because I was confused about how to conclude them all together in 1 single expression.
There are a few `=~` pattern tests where `[[ ]]` is already used, has to be since `=~` is not POSIX. I see your point in regards to multiple `&amp;&amp;`s being conflated into a `[[ ]]` test rather than multiple `[ ] &amp;&amp; [ ]` tests. Yes, that would be an improvement, will do so. Thanks. 
Sure. Another tip: when using `[[ ]]`, you don't need to quote variables on the left side of the comparator.
Yes, just reading up on that now. Another nice benefit, I agree. Basically, I am now replacing all `[ ]` usages with `[[ ]]`, and removing all quotes around variables whilst also conflating `[ ] &amp;&amp;/|| [ ]` into a single test. Expect a new update soon :) Cheers, I appreciate the feedback.
What a great idea 💡 
Biggest question here: what problem are you trying to solve? This feels like a specific solution to a broader problem. There may be a better solution together than messing with etc/hosts.
Why Mutt instead of mail?
Neat! I starred your repo. I'll have to play around with it this weekend.
Because I don't know how to use mail, and I found directions for how to use my Gmail account with Mutt. Would it be easier to do with mail? If so, how would I use mail for this? Can I configure it to use my Gmail account?
Cool, please let me know if you have any feedback. It works well for me, but I only use it for certain things, so there are probably edge cases.
Mutt does non-interactive email sending just fine. You just need to use the right incantations. Assuming you want to send `My_File.txt` as an _attachment_, without any accompanying message text: ``` $ mutt -s "Subject Here" -a My_File.txt -- my.email@example.com &lt;/dev/null ``` Note that the `--` is needed to terminate the list of attachments (`-a` can specify multiple files to be attached). If you actually want to send the _contents_ of `My_File.txt` as the message _body_, not as an attachment, then: ``` $ mutt -s "Subject Here" my.email@example.com &lt;My_File.txt ``` ### Further Reading `man mutt`
Hmm, quotes don't seem to be helping my predicament actually. Here's an example: \`\`\` regex="\[3D View\] Box" find . -regex "./$regex.\*" \`\`\` I've tried using \`-iname\`, removing variable quotes, removing string quotes, etc. Either the file isn't found with no error message, or an error pops up complaining about how it failed to compile the regular expression. Escaping the brackets like so \`\\\[3D View\\\]\` in the regex seems to work, but is there any way to automatically escape these special characters inside \`regex\`? Otherwise I'll have to scrub all shell special characters from my filenames which I didn't want to really do.
This eases a lot. Got to try if the gif's it makes doesnt take too much space.
Hey, `alphapapa`, as an Emacs user I'm getting the impression you're the most prolific developer on all GitHub! I'm a pleased with your work, but I'm starting to worry about your health! Go outside, man! :D
google keyword, 'muttrc gmail example' 
You have a field $coord. See what the content is for that and handle accordingly. You need to look at each surrounding filed by adding and/or reducing the $coord by 1. See that you wrap around if it is larger or smaller than the total lines.
Hey ! Thanks for you answer ! I tried this to create a living cell function : living_cell_function() { if (($x&gt;0)) &amp;&amp; (( $x &lt;= $num_lines+1)) &amp;&amp; (($y&gt;0 )) &amp;&amp; (( $y&lt;=$num_columns-1)) then if [[ "${tab[$((x-1)),$((y+1))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x)),$((y+1))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x+1)),$((y+1))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x-1)),$((y))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x+1)),$((y))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x-1)),$((y-1))]}"="o" ]] ; then ((Counter++)) ; fi if [["${tab[$((x)),$((y-1))]}"="o" ]] ; then ((Counter++)) ; fi if [[ "${tab[$((x+1)),$((y-1))]}"="o" ]] ; then ((Counter++)) ; fi fi display_function } &gt; The counter variable ( will stated above in my script ) is equal at 0 and send back the number of living cells around the cell in x,y, The first loop "if" is if you are not on the edge of the array... But i have a big doubt on the synthax... And I don't know how to interlock it in the script. &amp;#x200B;
Your tutors must hate you. 
Indeed. Thanks to sympathize ! (:
I assume that `tab[$x,$y]` contains `o` for a live cell, or is empty otherwise. This makes things a _lot_ simpler for `living_cell_function()`, because accessing a _nonexistent_ cell (e.g. tab[-1,-1]) returns an empty string _exactly as if it was a dead cell_, so you don't need to do the bounds checking at the beginning that you seem to be all confused about. The key element in `living_cell_function()` is building a "neighborhood string", that's just the contents of all 8 cells around (x,y), i.e.: ``` ${tab[$((x-1)),$((y-1))]}${tab[${x},$((y-1))]${tab[$((x+1)),$((y-1))]}... ``` This string would contain one `o` for each live cell. The length of this string (`${#neigh_string}`) would therefore be the number of "live" cells around (x,y). I think you know the rest.
Thank you! I just used the first one you sent and it worked as I was hoping. I do have a question about how it works though. I was reading about using /dev/null because I have seen it used before but never used it myself (I'm relatively new to Bash). But when I have seen it used elsewhere they used &gt; instead of &lt; in front of /dev/null. To me it would make sense to use &gt;/dev/null to redirect standard output to /dev/null. Is there a reason you used &lt;/dev/null instead? 
Redirecting standard input from `/dev/null` literally tells Mutt that there's no input for it to consume interactively, i.e. the desired message body has no text, just your specified attachments. Redirecting standard output to `/dev/null` just throws away whatever normal output the program generates. Redirecting standard _error_ (`2&gt;/dev/null`) does the same thing with whatever diagnostic output the program generates.
The "gron" utility might be helpful [https://github.com/tomnomnom/gron](https://github.com/tomnomnom/gron) What you pasted comes up as invalid JSON, but I suspect it got manged when you posted. Try using reddits "inline code" formatting. &amp;#x200B; &amp;#x200B;
That makes sense. Thanks for explaining!
So I cleaned up the JSON a bit and changed the `FileSystemId`s so that they would each would be unique. Though you specify grep in your question, I'd suggest using `jq` instead. VAR='{ "FileSystems": [ { "SizeInBytes": { "Timestamp": 1546441199.0, "Value": 6144 }, "Name": "name", "CreationToken": "token", "Encrypted": true, "CreationTime": "time", "PerformanceMode": "generalPurpose", "FileSystemId": "FS_ID1", "NumberOfMountTargets": 2, "LifeCycleState": "available", "KmsKeyId": "key", "OwnerId": "owner", "ThroughputMode": "bursting" }, { "SizeInBytes": { "Timestamp": 1546441199.0, "Value": 6144 }, "Name": "name2", "CreationToken": "token", "Encrypted": true, "CreationTime": "time", "PerformanceMode": "generalPurpose", "FileSystemId": "FS_ID2", "NumberOfMountTargets": 2, "LifeCycleState": "available", "KmsKeyId": "key", "OwnerId": "owner", "ThroughputMode": "bursting" } ] }' $ echo "$VAR" | jq '.FileSystems[] | select(.Name == "name") | .FileSystemId' "FS_ID1" $ echo "$VAR" | jq '.FileSystems[] | select(.Name == "name2") | .FileSystemId' "FS_ID2" PS: Note that everything after `jq` is part of a single-quoted string.
This sounds like a fun enough experiment, but god do I hope I never end up needing to use a tool that does this with its menus. 
I would recommend jq. It is almost a standard right now. [https://stedolan.github.io/jq/](https://stedolan.github.io/jq/) [https://stedolan.github.io/jq/manual/](https://stedolan.github.io/jq/manual/) Test it here. [https://jqplay.org/](https://jqplay.org/) &amp;#x200B;
To make the BEL sound you can use `echo -ne '\a'`. This option only has one sound though. You could differentiate with a double beep: `echo -ne '\a'; sleep 0.15; echo -ne '\a'`. Otherwise you can play a sound clip with something like `aplay`. For that option you'd need sound clip files. 
Thank you, I’ll try this.
I don't think you'll be able to have the movement up and down within dialog do anything. The actual 'menuing' is all within the dialog application itself, only when you choose an option (press enter) on a highlighted item within dialog does a command get executed. It might be possible using some of the more exotic configurations, or if you choose to rewrite the dialog program, but natively I don't think you could pull that off. However, once an item is chosen (press enter), you could then chain some kind of sound-generating command that into the commands executed on that particular menu item. For example, using the code from askubuntu.com's example: #!/bin/bash HEIGHT=15 WIDTH=40 CHOICE_HEIGHT=4 BACKTITLE="Backtitle here" TITLE="Title here" MENU="Choose one of the following options:" OPTIONS=(1 "Option 1" 2 "Option 2" 3 "Option 3") CHOICE=$(dialog --clear \ --backtitle "$BACKTITLE" \ --title "$TITLE" \ --menu "$MENU" \ $HEIGHT $WIDTH $CHOICE_HEIGHT \ "${OPTIONS[@]}" \ 2&gt;&amp;1 &gt;/dev/tty) clear case $CHOICE in 1) play ./Option1.wav echo "You chose Option 1" ;; 2) play ./Option2.wav echo "You chose Option 2" ;; 3) play ./Option3.wav echo "You chose Option 3" ;; esac
didn't know jqplay.org thanks 
Works for me: $ [[ "foo" =~ .* ]]; declare -p BASH_REMATCH declare -ar BASH_REMATCH=([0]="foo") Perhaps you’re using an older version of Bash?
Thanks! That helped me think outside the box... or shell for that matter: root@archiso ~ # echo $SHELL /usr/bin/zsh I'm not a smart man! ;)
LOL, congrats on fixing the issue :D and good luck with the Arch install!
Thanks! :) I'm mostly messing around creating a little installation script for myself. Forgot that it uses zsh in the iso as interactive shell.
You can't. That first line tells the computer what type of script it is. You are not trying to deceive sbcl, you are trying to deceive the operating system. The way to do that is change that first line.
You didn't explain how the behavior differs in the two examples, but it's almost certainly not due to knowing it's invoked on the shebang line. &amp;#x200B; Programs can behave differently based on detecting if their input is from a terminal (ie. interactive) or a non-interactive input such as a pipe. I'm guessing that is the reason for the behavior you are seeing (whatever it is).
I only want to deceive the program, not the operating system. I want the operating system to know the truth, but I want to program to believe a lie. How can I lie to the program to make it think it's invoked on the "#!" line?
&gt; It's probably because the shell doesn't properly handle the "--script" arg on the shebang. But what I want to do in that case, is to find a way to simulate that improper handling of the --script argument, to make the first example have the same behavior as the second example, even if that behavior is caused by incorrect handling of the --script argument. 
&gt;to make the first example have the same behavior as the second example \*What\* is the difference in behavior? And what OS are you using? &amp;#x200B; On Linux, the /usr/local/bin/sbcl shebang version is asking the kernel to launch "/usr/local/bin/sbcl --script", which it presumably cannot find. No shell is involved, and the sbcl program doesn't launch at all, because it cannot be found by Linux. Other OSes may behave differently (as my previous link explains in more detail).
It's Ubuntu. Both examples work fine. There is just one difference. I want to fool sbcl into thinking the first example is like the 2nd. The difference is in its error output. The programmers of sbcl wanted it to not issue certain error messages when invoked in a pipeline with the --script argument. They didn't want those error messages to be suppressed when sbcl was invoked from the "#!" line. I want it to work differently from how they wanted it to work, by deceiving sbcl into thinking it was invoked from the "#!" line. &amp;#x200B;
You need to add a fake `#!` line in the input you give to the program, like this: #!/bin/bash /usr/local/bin/sbcl --script &lt;&lt; EOF #!/usr/local/bin/sbcl --script , This is an sbcl error to test the behavior of sbcl. EOF The only way the program can tell it is being run as the result of a `#!` line is by seeing the `#!` line at the start of the input.
Good idea, but, unfortunately, that doesn't do it. It still behaves the same. It ignores the extra #! line, of course, but it also doesn't use it to determine how it was invoked.
I assume this is a followup to [your previous question about shebang lines](https://www.reddit.com/r/bash/comments/aa0odi/binbash_invalid_option/). If you understood my answer there, you'd realize that the two are definitely *not* the same, and in fact the first one should've thrown an error due to a missing filename argument after `--script`, based on the [sbcl man page](https://www.systutorials.com/docs/linux/man/1-sbcl/). To recap from your previous question: In your second script, SBCL is actually called as follows: /usr/local/bin/sbcl --script myscript.sbcl where `myscript.sbcl` is the second script. If you want that sort of behavior, you'd need to dump the input into a temp file and specify that as the argument to `--script`, something like: #!/bin/bash TF=$(mktemp) cat &lt;&lt;EOF &gt;$TF ; /graft/bin/sbcl --script $TF ; rm -f $TF (write-line "This is an sbcl error to test the behavior of sbcl.") EOF You *might* be able to use a process substitution for this, but both alternatives are ugly as hell. So, and this is the most important question: **What problem are you trying to solve, that simulating shebang conditions (instead of actually** ***using*** **a shebang) is the right answer**
Could you give examples of what the command does when invoked each way? That would be helpful for figuring out how it's determining the difference.
I hope you realize that `[3D View]` _as a regex_ literally means "match **one** character out of the set `{3,D, ,V,i,e,w}`". If you thought it would match the literal string `[3D View]`, you might want to spend some time [here](https://www.regular-expressions.info/tutorial.html), to get an in-depth understanding of what regexes are...and aren't. Reading between the lines, it looks like you're trying to `find` everything under the current directory whose pathname starts with the literal string `[3D View] Box`. If that's the case, you're using the wrong `find` test: ``` path_starts="[3D View] Box" find . -path "./${path_starts}*" ```
Okay. That at least gave me some information to work with... The --script option takes an argument (the script to run). &amp;#x200B; In the second case (sbcl shebang line), the sbcl command is passed the argument that was used to invoke the script as the next arg after --script. sbcl ignores the first shebang line (ie. the parses sees it's a shebang, and ignores that line), and then executes the script. &amp;#x200B; In the first case, the --shell arg expects a script argument, but none is given. When I run these two scenarios, I get error output for the sbcl shebang version, and with the shell shebang version it basically just waits for me to input a script name to execute. &amp;#x200B; For the shell shebang version, I suggest you don't use the --script argument to sbcl, since you aren't trying to execute a shell file, the input is from the HERE document. Instead, use the sbcl commandline args that --script implies, ie: #!/bin/sh /usr/local/bin/sbcl --no-sysinit --no-userinit --disable-debugger --end-toplevel-options &lt;&lt;EOF , blah blah blah EOF with maybe --non-interactive added as well.
This is actually a slightly different problem than before. Like before, I want the error output to be filtered, which was solved with a process substitution. But it turns out that in some situations, sbcl outputs different error messages depending on whether it was invoked from the shebang or from a command. I want to deceive sbcl into thinking it was invoked from the shebang so it will give me the same error messages it would have given me there. It has good reasons for doing it differently, but those reasons don't apply to my particular situation, which is why I want to deceive it. Ideally, I should be able to do it with a command line argument to sbcl, but there doesn't seem to be one for that purpose.
&gt;and in fact the first one should've thrown an error due to a missing filename argument after --script, based on the sbcl man page. I ended up spending some time on this as well, having never used sbcl, but discovered one odd thing: if you don't provide a script name, sbcl just seems to start up the interpreter and continue (ie. no error message or abort). It's weird.
&gt;but it also doesn't use it to determine how it was invoked. This has **nothing** to do with sbcl somehow knowing "how it was invoked". It's 100% due to the --script argument needing (you guessed it) a script. An actual filename containing a lisp program. Since you are using a HERE doc to provide the input in the shell shebang case, you can't use the --script argument; use the other arguments that it implies to get the error output behavior you want. &amp;#x200B; In the sbcl shebang version, the OS actually provides a filename after the --script option; it's the pathname of the file that was used to launch the script (it's traditional for Unix programs to get their program name as the first argument).
Sorry, my example is terrible--the `regex` variable is renamed to `filename` so it makes a bit more sense. I'm trying to find any filenames beginning with `[3D View] Box`.
Well, I've now shown you how to recreate the conditions. It's ugly as sin, but it does exactly what your desired model does.
Probably an SBCL implementation quirk.
Renaming the variable doesn't make a difference. You're asking `find` to `-regex` on a pattern with regex special characters...but you're expecting them to be treated as literal characters. That's not gonna work no matter what you call the intermediates. You're far better off using standard shell-type wildcards and `-path` instead. 
To clarify some issues, try it with and without the fake "#!" line, and with and without the comma at the start of the sbcl error line. Doing those gives you four different behaviors. The comma makes a different kind of error, which is suppressed in some situations. The fake shebang line seems to cause it to change, once again, how it decides which errors to suppress. &amp;#x200B;
Using `-path` doesn't seem to make a difference--the brackets still need to be manually escaped unfortunately. 
The differences are in the error message output. The original goal of the sbcl developers in this was to suppress verbose error message output when sbcl is used in pipelines. But there were complications, and I haven't yet figured out all the decisions they made about how it should work. In any case, 100% of the difference between different ways of invoking it is in what error information is output for what error conditions.
&gt;To clarify some issues, try it with and without the fake "#!" line, There is no "fake" shebang line. You need to adjust your thinking or you'll never get out of this loop. &amp;#x200B; &gt;The comma makes a different kind of error When you run "sbcl --script", without a script argument, the program starts and waits for input. The parsing the initial comma somehow causes the program to exit without error (on my Ubuntu system), even though when running sbcl without the --script option, there is a traceback ("Comma not inside a backquote.) I tried enabling all the same command-line switches that are implied by script, but couldn't get them to behave the same as script. It's possibly a quirk. &amp;#x200B; &gt;The fake shebang line seems to cause it to change It's the --script option. Since you aren't loading a script file in the shell launched case, you don't want the --script option.
Sorry, working on an empty stomach makes me _not_ see things. There's one missing component to my original solution: ``` path_starts="[3D View] Box" find . -path "./$(printf %q "${path_starts}")*" ``` From the bash man page: ``` printf [-v var] format [arguments] ... %q causes printf to output the corresponding argument in a format that can be reused as shell input. ``` In this case, `%q` causes `printf` to properly escape the glob characters in your pattern: ``` $ printf %q "[3D View] Box" \[3D\ View\]\ Box ```
&gt; the program starts and waits for input That just means it's getting the script from stdin. That's what the here document is for. &lt;&lt; EOF. That gives it the script on stdin.
&gt; There is no "fake" shebang line I meant the one proposed by another person who commented. The 2nd shebang line, in one of the comments.
Sweet, so I used the printf %q trick with -regex instead of -path--looks like printf %q is a good trick in general for cleaning up special characters. I have to say though, I'm not too crazy about all this quoting... the string in the variable is quoted, the variable itself is quoted, and the expression is quoted as well. It's making me realize I don't understand quoting in BASH at all lol.
&gt;That just means it's getting the script from stdin I know... I'm telling \*you\* that the --script argument changes how the initial comma is handled, and that's it's unnecessary anyway since the input comes from stdin not a file. &amp;#x200B; Well anyway, good luck. I hope you figure it out.
While I'm happy for you, there's another reason I recommended using `-path` instead of `-regex`, something you probably weren't even aware of. In: ``` find . -regex "./blahblah.*" ``` the first `.` in the regex matches the current directory `.` _as an arbitrary character_, not a literal `.`, so your regex actually worked by _luck_ rather than intent. As for shell quoting...you get used to it after a while.
Haha, not trying to be stubborn about the regex thing, it was more curiosity. &gt;the first . in the regex matches the current directory **. in all paths as an arbitrary character,** not a literal . I'm having trouble understanding what you mean by the bold part--the ./ part is supposed to be the pathname, right? I'm just using it as a relative path, unless you mean something different? Anywho, whatever you're trying to say seems right because for the last hour I've been testing `-regex` and `-path`, and I can't get the former fully functioning no matter what.
&gt; I was using `./` as a relative path since for some odd reason, the `-regex` option wants the path included, not just the regex for the filename. That's because you did: ``` find . [...] ``` so all paths returned by `find` (and therefore matched _in full_ by both `-path` and `-regex`) start with `./`. You can see it for yourself by running just `find .`. As for: &gt;the first . in the regex matches the current directory . in all paths as an arbitrary character, not a literal ., so your regex actually worked by luck rather than intent. I meant that in the regex `./blahblah.*`, all the periods are regex special character that matches any one character, and not a literal match for `.`. The shell wildcard expression equivalent for that regex is therefore `?/blahblah*`. 
You can't. You do not "invoke" anything. That first line is not for the script. It is for the operating system to know what to do. The script knows what to do. In case of bash `#!/bash` on the first line in a script named `test.sh` means the same as `bash test.sh` without that first line. It means "What is below this is a bash script". You can not make it mean something else.
You could use bash itself to find your files instead of the 'find' tool. It would look like this: shopt -s nullglob for file in ./"$filename"*; do # do something with '$file' here done The 'nullglob' option makes it so the `*` search will return an empty list if there are no matches. By default it will create one entry with a `*` text character if it can't find a real file. You can also do a recursive search through sub-folders with bash with a `**` wildcard, just like what the 'find' command can do: shopt -s nullglob globstar for file in ./**/"$filename"*; do # ... done The 'globstar' option enables that `**` feature. It's by default disabled.
This [blog post I wrote up](https://blog.twentytwotabs.com/the-smallest-bash-program-in-the-universe/) may be a good read if you want to understand how shebang lines are interpretted by the kennel. tl;dr is that if the kernel is asked to execute a file, and it starts with `#!`, the kernel will grab from the file up to two whitespace delimited words that follow. It'll then invoke the executable whose name is the first word with the second word as an optional argument. It'll then append the filename as the final argument, and let 'er rip. If you're running the bash shell, Bash will use itself as the interpreter for executable files that look like text but do not start with a `#!`. Your two examples work differently because the kernel will run the interpreter with the script's name as the final argument. It will not pass the script into the standard input of the interpreter.
Wouldn’t this just do the trick? find . -type f -iname “[3D View] Box*” -exec blah {} \;
You can use [parameter expansions](http://mywiki.wooledge.org/BashFAQ/073) to get the part before and after the last `.`. base=${1%.*} ext=${1##*.} gs ... "-sOutputFile=${base}_.$ext" "$1"
Objectively correct. I just wanted to add that PE seemed arbitrary and ridiculous to me until I realized that `#` is to the *left* of `$`, and `%` is to the *right*. 
Well, that's my game changed.
Unless I'm missing something, process substitution is the cleanest solution here. /usr/local/bin/sbcl --script &lt;(cat &lt;&lt;-'EOF' your lisp goes here EOF ) /u/mresto, please note a couple of things about `&lt;&lt;-'EOF'`: 1. `-` ignores tabs (allows your own indentation). I don't write Lisp so I don't know if this is desirable. 2. `'EOF'` is quoted, which prevents bash expansions (variables, arithmetic, etc.) in the here-doc.
Ah, I see what you're saying now about `.` utilizing the regex meaning (match any single character) rather than the shell meaning (current directory). Btw, it looks like the initial period of `find . -regex exp` doesn't serve a purpose, and all that matters is that the path inside the regex expression is correct.
Yes, that's what I ended up doing after discussing it in r/lisp today. Thanks.
Here's the full code here. It's checking to see if each thumbnail in `./` has a corresponding video file in `./dled_files` and moves the thumbnail into ./dled\_files if so. It removes the extension from each filename string so they can be compared. for file in *; do if [[ $(find . -regex "./dled_files/$(printf %q "${file%.*}").*") ]]; then mv "$file" "dled_files/" fi done I'm currently learning more about how the printf %q command functions. It works for files with \[\] in them now, but files with () and| aren't getting moved when they should.
Not that I can tell. It doesn't work with filenames that have \[\], (), or | for me.
It does if you escape the [] ``` ▶ find . -name "\[3D View\] Box*" ./[3D View] Box Bla ```
Ah my mistake. The important part is avoiding manually escaping \[\] () and | though. I'm currently using this but it's not working with files that have () | and, only \[\]. for filename in *; do if [[ $(find -path "./dled_files/$(printf %q "${filename%.*}").*") ]]; then mv "$file" "dled_files/" fi done &amp;#x200B;
Okay, this is actually a bit annoying to do, but I found a trick online about how to do this: exists() { [[ -f $1 ]] } for picture in *; do if [[ -f $picture ]] &amp;&amp; exists "${picture%.*}".*; then mv -v "$picture" dled_files/ fi done The trick here is the "exists()" function. About why the function is needed, I first tried using tests like the following, which does not work: [[ -f *.txt ]] [[ -n *.txt ]] (you can read about those `-f` and `-n` by typing `help test` at the command line) It seems bash treats the `*` as normal text when inside a `[[` and things then always resolve the same way, it's just totally impossible to test if something exists like that. But when adding that "exists()" function to the mix, when writing something like `exists *.txt`, bash will first turn that `*` into real filenames before it calls 'exists()' and the `$1` inside the function will be a filename and the `[[ -f` test in there will work. About how I developed the solution, I first created a bunch of empty example files with `touch` and `mkdir`: $ tree . ├── dled_files │ └── test.mpv ├── test2.jpg └── test.jpg 1 directory, 3 files I then defined that "exists()" and tested it: $ exists() { [[ -f $1 ]]; } $ exists *.jpg &amp;&amp; echo true true $ exists *.asdf &amp;&amp; echo true Then I tried to do experiments to approach what you want to do step-by-step: $ for pic in *; do echo $pic; done dled_files test2.jpg test.jpg There's that directory in the output, so I filtered it out: $ for pic in *; do [[ -f $pic ]] &amp;&amp; echo $pic; done test2.jpg test.jpg Now I added a look into the sub-folder with that "exists()" function: $ for pic in *; do [[ -f $pic ]] &amp;&amp; exists dled_files/"${pic%.*}".* &amp;&amp; echo $pic; done test.jpg And that's it, it's the one file that has a movie existing in the sub-folder. Now let's add a test file with problematic characters: $ touch '[3d] test.jpg' $ touch dled_files/'[3d] test.mpv' This first had a problem. It needs `"` quotes added to the 'echo' at the end to work right: $ for pic in *; do [[ -f $pic ]] &amp;&amp; exists dled_files/"${pic%.*}".* &amp;&amp; echo "$pic"; done [3d] test.jpg test.jpg
That was in fact my first attempt, and it didn't produce the same error at all: ``` $ cat test1.sbcl #!/usr/local/bin/sbcl --script , This is an sbcl error to test the behavior of sbcl. $ ./test1.sbcl Unhandled SB-C::INPUT-ERROR-IN-LOAD in thread #&lt;SB-THREAD:THREAD "main thread" RUNNING {100215EC53}&gt;: READ error during LOAD: Comma not inside a backquote. Line: 2, Column: 1, File-Position: 27 Stream: #&lt;SB-SYS:FD-STREAM for "file /home/me/test2.sbcl" {1002166DF3}&gt; Backtrace for: #&lt;SB-THREAD:THREAD "main thread" RUNNING {100215EC53}&gt; 0: (SB-DEBUG::DEBUGGER-DISABLED-HOOK #&lt;SB-C::INPUT-ERROR-IN-LOAD {10021766B3}&gt; #&lt;unused argument&gt;) 1: (SB-DEBUG::RUN-HOOK *INVOKE-DEBUGGER-HOOK* #&lt;SB-C::INPUT-ERROR-IN-LOAD {10021766B3}&gt;) [...] $ cat test2.sbcl #!/bin/bash /usr/local/bin/sbcl --script &lt;(cat &lt;&lt;-'EOF' , This is an sbcl error to test the behavior of sbcl. EOF ) $ ./test2.sbcl Unhandled UNBOUND-VARIABLE in thread #&lt;SB-THREAD:THREAD "main thread" RUNNING {100215EC43}&gt;: The variable S is unbound. Backtrace for: #&lt;SB-THREAD:THREAD "main thread" RUNNING {100215EC43}&gt; 0: (SB-INT:SIMPLE-EVAL-IN-LEXENV S #&lt;NULL-LEXENV&gt;) 1: (EVAL-TLF S 0 NIL) [...] ```
&gt; Btw, it looks like the initial period of `find . -regex exp` doesn't serve a purpose, and all that matters is that the path inside the regex expression is correct. If you mean that the `.` in `find . -whatever` isn't strictly necessary, then yes, but only if you want to start your search from the current directory...and only if you're using _GNU_ find. macOS and many (all?) of the BSD flavors use BSD find, for which the search path is _mandatory_.
Your post there mentioned that you eventually discovered that you had to add 8 characters of padding to get a process-substituted SBCL script to work in the same way as my temp file solution. I suspect it's precisely because of the peculiar nature of process substitutions, and I'll detail this in a separate post.
Ah, I'm sorry, you said that indeed. This is a little weird but maybe something like this? #!/bin/bash while read file; do escaped_filename="$(echo $(basename ${file}) | sed -E 's/[][)(|]/\\&amp;/g')" find dl_folder -name "${escaped_filename}" | egrep -q ".*" &amp;&amp; echo "found" || cp -v "${file}" dl_folder/ done &lt; &lt;(find . -maxdepth 1 -name "[[(|]3D View[])|] Box*" -print) find does not really have a proper exit status so you can do `| egrep ".*"` just to see if it found something, egrep has exit status 0 if it has found something and 1 if it doesn't match and you can use that to run a different command or not with `||` (or) and `&amp;&amp;` (and) This seems to work... $ touch "(3D View] Box Bla" "|3D View| Box Bla" "[3D View] Box Bla" $ bash bla.sh found found ./(3D View] Box Bla -&gt; folder/(3D View] Box Bla
Damn, thanks for finding that out. Do you remember where you read about this peculiarity of the test `[[ ]]`? I have the worst time using special characters with search engines. Also I removed the `[[ -f $pic ]] &amp;&amp;` since the for loop is already looping through all the files in the current directory (unless there's something I'm missing).
Hey thanks for the response. Looks like it's working now thanks to an earlier suggestion above: #!/bin/bash exists() { [[ -f $1 ]]; } for thumbnail in *; do if exists dled_files/"${thumbnail%.*}"*; then mv "$thumbnail" "dled_files/" fi done &amp;#x200B;
Upvote for an explanation of a concept that obviously took a bit of research! Thanks for the info!
u/[stassats](https://www.reddit.com/user/stassats) says he fixed that sbcl bug today. And also changed the error message logic to make this no longer an issue.
Triple-BQ doesn't work; you need to start every line with 4 spaces.
Well I stand thoroughly corrected. This example turned out to be a buggy tool, but I've definitely been strutting around treating Proc Subs as files. Could you point me at some material that would improve my understanding of pipes as compared to files? I'm finding lots of `|` redirection but nothing relevant so far.
`cmp` compares files on a byte-by-byte basis, and will tell you if files are identical or similar Basic mock-up below script_continue=0 for file in f1.txt f2.txt f3.txt f4.txt; do cmp -s originals/$file downloads/$file if [ "$?" -ne 0 ]; then script_continue=1 break fi done if [ "$script_continue" -eq 0 ]; then echo "No files differ, exiting...." &gt;&amp;2 exit 0 fi # Rest of script
Thank you so much. Looks to be exactly what i needed. I'll use the code, and hopefully learn something as well..
There’s no need to use `$?` here, you can just use the command as the condition directly: # ... if cmp -s "originals/$file" "downloads/$file"; then # ...
&gt; looks like `printf %q` is a good trick in general for cleaning up special characters. It’s a good trick for cleaning up special characters *for the shell*, but you should always be mindful of what you’re escaping for. `-path` accepts shell patterns, so `printf %q` is the right escaping method for it; on the other hand, `-regex` accepts regular expressions, where `.` also has a special meaning even though `printf %q` doesn’t escape it.
Thank you! Worked perfectly and I learned something too, appreciate it very much 🙏 👍
I am running Linux Mint 18.2 for reference
try this with V undefined, set to '' and set to 'xyz' if [[ ${#V} &gt; 0 ]]; then echo "not empty"; else echo "empty"; fi
To make sure I know how this works, the '&gt; 0' portion of that, would that work because the string, having anything, would be lexicographically sorted after 0?
There are a few ways. You can test the length of the value with: [[ ${#V} -gt 0 ]] Alternatively, you can use an explicitly arithmetic arithmetic expression, with: (( ${#V} &gt; 0 )) This can even be simplified to: (( ${#V} )) since an arithmetic expression that evaluates to a non-zero value is true. Rather than using the length of the string, we can instead use one of these: [[ $V != '' ]] [[ -n $V ]] [[ $V ]] Another approach, which is especially useful when you've set the `nounset` option, is to use: [[ ${V:+set} ]] since this will expand to `set` if `V` is empty or unset, whereas the three previous variants will fail if `nounset` is on and `V` is unset. There's also `[ ... ]` with single brackets. but there's absolutely no need to ever use that in Bash. It's overall harder to use correctly.
While this is _technically_ correct, and will do the right thing in this specific instance, it would possibly be more correct to use `-gt` rather than `&gt;`. `-gt` is a numeric comparison, `&gt;` is a lexicographic comparison. It just so happens that all positive numbers are lexicographically after `0`. If you wanted to compare the value with 10, as an example, `&gt;` could do the wrong thing: [[ $x &gt; 10 ]] would be successful if `x` contained `2`.
Got it! Thank you so much for the advise! Hopefully this will be the final tweak to my first ever bash script! The script digs a domain from a list of domains against the TLD's so accounting for the odd name servers (i.e. [com.au](https://com.au), [com.de](https://com.de), etc) was difficult, but this should hopefully be the last fix. &amp;#x200B; The script if you are interested: note: The line grep 'NS ' tempdom.txt should have a tab after NS, but it doesn't ever copy. \#!/bin/bash STRFILE=dom.txt for x in \`grep -v \^# $STRFILE | awk '{print $1}'\`; do TLD="$(echo $x | awk -F'.' '{print $2}')" ODD="$(echo $x | awk -F'.' '{print $3}')" if \[\[ $ODD &gt; 0 \]\] then TLD="$(echo $x | awk -F'.' '{RS = "";printf $2".";printf $3}')" fi TEMPTLD="$(host -t ns $TLD)" TLDNS="$(echo "$TEMPTLD" | awk '{print $4;exit}')" echo "nameservers from $TLD for $x" dig -t ns $x @$TLDNS &gt; tempdom.txt grep 'NS ' tempdom.txt done
For some reason the -gt is not giving the expected result. The if reads the following, yet is affecting the 'then' portion negatively whereas the '&gt; 0' is working. $ODD will be a string like 'de' or 'au' if being a string would affect anything. if \[\[ $ODD -gt 0 \]\] then TLD="$(echo $x | awk -F'.' '{RS = "";printf $2".";printf $3}')" else TLD="$(echo $x | awk -F'.' '{print $2}')" fi
 if ! [[ $variable ]]; then
&gt; For some reason the -gt is not giving the expected result. [[ $ODD -gt 0 ]] means "is `$ODD` _numerically_ greater than zero?" Note that /u/bigfig was using a `#`. If you had: [[ ${#ODD} -gt 0 ]] it would work: this means "is the length of `ODD` numerically greater than zero?" But seriously, I don't understand why you don't just use: [[ $ODD ]] The whole point of my other post was to point out that there are simpler ways of testing whether variable was non-empty than testing its length.
Boy do I feel triumphant now that it works as I dreamt up a few days ago. Thank you so much for the help and enjoy the final script if you should ever need: &amp;#x200B; \#!/bin/bash STRFILE=dom.txt for x in \`grep -v \^# $STRFILE | awk '{print $1}'\`; do ODD="$(echo $x | awk -F'.' '{print $3}')" if \[\[ ${#ODD} -gt 0 \]\] then TLD="$(echo $x | awk -F'.' '{printf $2".";printf $3}')" else TLD="$(echo $x | awk -F'.' '{print $2}')" fi TEMPTLD="$(host -t ns $TLD)" TLDNS="$(echo "$TEMPTLD" | awk '{print $4;exit}')" echo "nameservers from $TLD for $x" dig -t ns $x @$TLDNS &gt; tempdom.txt grep 'NS ' tempdom.txt done
Ah, I see you stopped reading my comment half-way through.
Not sure what your expectation from TTL is; \`nmap\` already has OS detection (\`-O\`) and TTL alone is not very reliable for detection (128 is \*probably\* Windows, and any other value is probably \*not\* Windows). MAC address will be available after you ping the hosts using e.g. \`arp -a\`, but like OS detection, \`nmap\` has that built in; when you scan a local subnet as root, the MAC addresses will be listed (\`sudo nmap -sP -n 192.168.x.0/24\` or similar, depending on your local network). So \`nmap\` will get you everything you want in one command: \`sudo nmap -n -F -O 192.168.x.0/24\` (or similar).
Crap, you're right. My half-asleep self totally missed the point of that second half as I tried to think and read at the same time. Note to self: popping out of bed at 1am with a possible solution may not always lead to the most cognitive of thinking. Taking your advice into account, though, the script is simpler and works perfectly: #!/bin/bash STRFILE=dom.txt for x in `grep -v ^# $STRFILE | awk '{print $1}'`; do ODD="$(echo $x | awk -F'.' '{print $3}')" if [[ ODD ]] then TLD="$(echo $x | awk -F'.' '{printf $2".";printf $3}')" else TLD="$(echo $x | awk -F'.' '{print $2}')" fi TEMPTLD="$(host -t ns $TLD)" TLDNS="$(echo "$TEMPTLD" | awk '{print $4;exit}')" echo "nameservers from $TLD for $x" dig -t ns $x @$TLDNS &gt; tempdom.txt grep 'NS ' tempdom.txt done 
Crap, you're right. My half-asleep self totally missed the point of that second half as I tried to think and read at the same time. Note to self: popping out of bed at 1am with a possible solution may not always lead to the most cognitive of thinking. &amp;#x200B; Taking your advice into account, though, the script is simpler and works perfectly: &amp;#x200B; \#!/bin/bash STRFILE=dom.txt &amp;#x200B; for x in \`grep -v \^# $STRFILE | awk '{print $1}'\`; do ODD="$(echo $x | awk -F'.' '{print $3}')" if \[\[ ODD \]\] then TLD="$(echo $x | awk -F'.' '{printf $2".";printf $3}')" else TLD="$(echo $x | awk -F'.' '{print $2}')" fi TEMPTLD="$(host -t ns $TLD)" TLDNS="$(echo "$TEMPTLD" | awk '{print $4;exit}')" echo "nameservers from $TLD for $x" dig -t ns $x @$TLDNS &gt; tempdom.txt grep 'NS ' tempdom.txt done
&gt; if [[ ODD ]] Want to check that again?
Good catch! Fixed and headed to bed. Thanks so much for the help! :)
yes nmap https://stackoverflow.com/questions/13212187/is-it-possible-to-get-the-mac-address-for-machine-using-nmap 
`if [ -z "${var}" ]; then echo 'var is empty'; fi` `if ! [ -z "${var}" ]; then echo 'var is not empty'; fi`
Also, instead of using !-z, you can use -n to test for not-null.
Mentioned but a little different solution. If you are enforcing bound variables you need to do the "${x:-}" thing or you will get an error if x is not assigned. Also note (I think this is Korn shell only) if you check for empty on $1, $2...so on...you need to do the same, i.e., "${1:-}", where as you don't need to do that in Bash. The code below works for Bash and Korn. I know everyone writes Bash these days but I grew up on Korn. By the way, file read/writes are **way** faster with Korn shell. Anyone know why? Something I have observed a number of times but have not got any good test cases. Maybe that is another post. ```bash if [[ -z "${x:-}" ]]; then echo "empty" fi ```
The proper way to do it. `if [ -z ${var+x} ]; then echo "var is unset"; else echo "var is set to '$var'"; fi` See [https://stackoverflow.com/a/13864829/1617897](https://stackoverflow.com/a/13864829/1617897) &amp;#x200B;
You are welcome. Good luck.
&gt;if \[ -z "${var}" \]; then echo 'var is empty'; fi \[\[ -z "${var}" \]\] &amp;&amp; echo "\\$var is empty" || echo "\\$var is not empty"
You should clarify what is the output of the script and how `netcat` is supposed to use that. However you can use pipes: `script | netcat` or, depending on the string in script output, evaluate the output and pass it as an argument, e.g. `netcat &lt;options&gt; $(script)` Adjust to your needs.
Two ways. * Edit the script to put the output in a parameter and then run the program with the parameter. * Turn the script into a parameter I would say that the best way is to edit your script to run netcat as well. Imagine that the output of your script would be `-v -l -s 10.11.12.13 -p 6000 localhost 2`, You would wanting to run `nc -v -l -s &lt;your-addr&gt; -p 6000 localhost 2 `. You can do `nc $(scriptname)` A better way would be to let your script handle it. At some point you will have, at the end, `echo $OUTPUT` #!/bin/bash &lt;here goes your script&gt; OUTPUT="$Whatever you already had" echo "$OUTPUT" # Comment this line and add the following nc "$OUTPUT" That's it. You now run the script and it does it all for you. If that does not work, we need to know the script, what the output of the script is and what command you would expect to run.
Gonna need more clarification. As you can tell you got two answers addressing two VERY distinctly separate problems. Are you trying to use output of a script as netcat arguments? Or are you trying to take the output of a script and send it to a remote server somewhere using netcat?
If you are trying to do something illegal, you're going to have to be a lot more descriptive.... If not, then just post examples of what you are using, what you have, and what you are trying to get....
First ever? What happened to hello world haha lel
Can't you just dry-run rsync which only downloads if the file being overwritten is different and use it's output in an IF? Saves on redundant downloading and stuff.
Sounds kind of like a [quine](https://en.wikipedia.org/wiki/Quine_\(computing\)) though that would be tremendously difficult to implement. You could cheat and hash the script ignoring the embedded hash, something like this maybe? #!/usr/bin/env bash hash='8fba4164de146ad55eafb2f26699eff6' script=$(realpath $0) my_hash=$(tail -n+3 $script | md5) echo "Embedded hash: ${hash} Calculated hash: ${my_hash}" Running that once to get the "calculated" hash, then writing the hash into the script, and running it again results in this: voltaic@computer [✔] $ ./test.sh Embedded hash: Calculated hash: 8fba4164de146ad55eafb2f26699eff6 { ~/tmp/hash-quine } voltaic@computer [✔] $ ./test.sh Embedded hash: 8fba4164de146ad55eafb2f26699eff6 Calculated hash: 8fba4164de146ad55eafb2f26699eff6 
This is the right solution. Whenever something includes a hash of itself, the hash value is never part of the computation. Here's an example from RFC791 describing IPv4 checksums: &gt;The checksum field is the 16 bit one's complement of the one's &gt;complement sum of all 16 bit words in the header. For purposes of &gt;computing the checksum, the value of the checksum field is zero. And here's from the XZ compression file format specification: &gt;3.1.7. CRC32 &gt; &gt;The CRC32 is calculated over everything in the Block Header &gt;field except the CRC32 field itself. A file containing its own SHA1 sum is a newsworthy event.
Sweet, looks like you understood what I was getting at, that's awesome! Thanks I'll see if that works for me.
You could embed the hash in the filename. However, self integrity check is hard, and you should lay out your threat model: what scenario do you want to protect yourself from?
Just set a variable to hold the first 3 octets of your local network - eg `netwk="192.168.1."` then host $netwk$i Unless you are looking to run a scan on the entire internet, in which case you can learn a bit more script first ;-)
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
What are some possible values of `$MIN` and `$MAX`? Depending on how specific they are, you could potentially do something like: for addr in 10.0.0.{5..13}; or for addr in 10.0.{1..3}.{1..255}; But these both require that you know ahead of time which parts of the address will be constant. If you need more control than that, you should probably shell out to some other language ([e.g., Python](https://stackoverflow.com/a/47196808)). $ MIN=10.0.0.250 MAX=10.0.1.5 $ for addr in $(python3 -c "from ipaddress import IPv4Address as ip; print('\n'.join(str(ip(n)) for n in range(int(ip('$MIN')), int(ip('$MAX')) + 1)))"); do echo "$addr"; done 10.0.0.250 10.0.0.251 10.0.0.252 10.0.0.253 10.0.0.254 10.0.0.255 10.0.1.0 10.0.1.1 10.0.1.2 10.0.1.3 10.0.1.4 10.0.1.5
thanks ill look into the nested loops. Yea, im looking to scan the entire internet lol, IPv4 addresses anyways. The values in MIN and MAX I don't know before hand. 
Every single octet could be different. I thought I could do it in bash because of what else my script is doing.
 for i in {$MIN..$MAX}; do host $i done You can't use variables in a range like that. ^(Well... you can... but it requires `eval` which is generally best avoided.) Instead, you should use a c-style loop e.g. for (( octet=min; octet&lt;max; octet++ )); do host 192.168.1."${octet}" done 
sure, all you have to do is write a web gui around it....
You can do something like this if you want: #!/bin/bash for ip in 198.162.{0..4}.{0..4}; do host $ip done And IPv4 addresses are of course just 32 bit unsigned integers, so you can iterate through them like regular numbers. I grabbed a set of Bash functions from Stack Overflow to convert to and from IP and decimal: #!/bin/bash # https://stackoverflow.com/questions/10768160/ip-address-converter dec2ip() { declare -i a=$((\~(-1&lt;&lt;8))) b=$1; set -- "$((b&gt;&gt;24&amp;a))" "$((b&gt;&gt;16&amp;a))" "$((b&gt;&gt;8&amp;a))" "$((b&amp;a))"; local IFS=.; echo "$\*"; } ip2dec() { declare -i a b c d; IFS=. read a b c d &lt;&lt;&lt;"$1"; echo "$(((a&lt;&lt;24)+(b&lt;&lt;16)+(c&lt;&lt;8)+d))"; } FRST=192.168.0.0 LAST=192.168.1.0 for ip in $(seq $(ip2dec $FRST) $(ip2dec $LAST)); do host $(dec2ip $ip) done If it were me though, I'd go with Python and the ipaddress module for something like this. Bash doesn't really seem like the right tool for IP addr manipulation. But of course, as we've seen in this thread, it's definitely possible. &amp;#x200B; All the best.
thank you. ill see what i can do. the reason im not using python is because i haven't figured out how to ssh to a system easily with python. i keep running into key issues.
thanks i didn't know that. ill try it out. thanks!
I see. What are you doing with ssh? You could always call out to a bash script from the python one, depending on what you’re trying to achieve.
You can just use python's simple httpserver. You'll just need some html tags in your output, which can be done with a few echo statements. This is not exactly robust, but is an easy way to serve static-ish^* content over http. -------------------- \* Dynamic content can be served too, but gets complicated real quick. You'll need flask/django/something else to keep it manageable.
What im doing is connecting to a linux host, running the host cmd on a hostname so i can get its IP. Then get the host address range with a /26. After that i need to run the host cmd again on every ip address in that range to get some hostnames. I've already got most of it figured out expect or the iterating through ip addresses 
Sounds like you need [shellinabox](https://github.com/shellinabox/shellinabox). It hasn't been updated since 2016, but it's still in the official Debian and Ubuntu package repos, and possibly in whatever other Linux distro you use. At worst, you get to build it from source. Once it's installed, you can either let your buddies login to your system through shellinabox (risking whatever they might do accidentally or intentionally), or create a service definition for each script, to limit their access. The [man page](https://github.com/shellinabox/shellinabox/wiki/shellinaboxd_man) has all the necessary details.
for i in $(seq 1 254); do ... $i; done
Underwhelming release.
&gt; There is no "fake" shebang line. You need to adjust your thinking or you'll never get out of this loop. #!/usr/local/bin/sbcl --script , This is an sbcl error to test the behavior of sbcl. And that's not an SBCL error on line 2. That's a `-bash: ,: command not found` I'm thinking OP has never used Bash before.
OP doesn't know bash very well.,
 The --script argument expects a lisp script (FILENAME) The alternative is here-docs into STDIN.
Yep. I answer a lot of questions in a lot of subs, so knowing how to "triage" and move on is important. :)
`$EPOCHSECONDS`:14 `$(\date +%s)`:13 `$EPOCHREALTIME`:15 `$(\date +%s%N)`:15 WHY Everything else is exciting or meaningless to me. Can't wait to not have access to it at work for the next 10 years.
`$EPOCHSECONDS`:14 `$(\date +%s)`:13 `$EPOCHREALTIME`:15 `$(\date +%s%N)`:15 WHY Everything else is exciting. Can't wait to not have access to it at work for the next 10 years.
what would be the point of this? Any malicious party could easily edit the script file calculate the hash and update the checksum. Its a cool idea. But I am missing the practical application.
Depending on your needs, you could do this with rundeck or awx/ansible-tower.
&gt; `$EPOCHSECONDS`:14 `$(\date +%s)`:13 &gt; &gt; `$EPOCHREALTIME`:15 `$(\date +%s%N)`:15 &gt; &gt; WHY What do you mean by this?
[Define a `command_not_found_handle` function.](https://www.gnu.org/software/bash/manual/bash.html#Command-Search-and-Execution)
Here's a short function that you can add to your .bashrc to experiment whenever you are confused about bash will do with `"` quotes: args() { printf "%d args:" $# printf " &lt;%s&gt;" "$@" echo } Here's an experiment at the command line, first I create an "order" command that prints something: $ order() { echo "hello world"; } $ order hello world Now I'm capturing its output with `$()` and calling that "args" command I mentioned earlier: $ args "$(order)" 1 args: &lt;hello world&gt; $ args $(order) 2 args: &lt;hello&gt; &lt;world&gt; Here's what happens if there are line-breaks in the output of "order": $ order() { echo hello; echo hey; } $ order hello hey $ args "$(order)" 1 args: &lt;hello hey&gt; $ args $(order) 2 args: &lt;hello&gt; &lt;hey&gt; Last thing, bash makes an exception where you assign variables using `$()`. You don't need to add `"` quotes there, it will still capture the whole output into the variable, see here: $ x=$(order) $ args "$x" 1 args: &lt;hello hey&gt;
&gt;Last thing, bash makes an exception where you assign variables using &gt; &gt;$() &gt; &gt;. You don't need to add &gt; &gt;" &gt; &gt; quotes there, it will still capture the whole output into the variable, see here: &amp;#x200B; Thank you. Got it
CGI would probably be the easiest was to run bash scripts from a web server
Haven't tested this yet but. Maybe he means amount of characters? 
$EPOCHSECONDS and $(date +%s) both output 10 characters (digits) on my system.
I think he meant amount of chars to type plus enter....
when `$()` ( command substitution ) is not wrapped in quotes, the result of that command will be word splitted ( i.e bash will split the result of that command substitution when it sees any whitespace ). Using quotes preserves the whitespace i.e bash reads the output of that command as one. for example $ f() { echo "I Love Bash. Do You" ;} $ myarr=( $(f) ) $ echo "${#myarr[@]}" # result will be 5, because bash is splitting on whitespace $ myarr=( "$(f)" ) $ echo "${#myarr[@]}" # result will be 1, because bash reads everything as one, wordsplitting does not happen 
thanks!
qwe 2&amp;&gt;1 &gt; /dev/null || echo "Command not found" BEWARE: This executes qwe with all side effects (data loss, security breach, alien invasion, zombie apocalypse)
This works for me: #!/bin/bash echo "scale=4;15+14+14+14+14+30" | bc | read testvariable echo "$testvariable"
Do the following to save the result: testvariable=$( echo "scale=4;15+14+14+14+14+30" | bc ) Something else about bash, there's a "&lt;&lt;&lt;" thing that you can use instead of echo and a pipe: bc &lt;&lt;&lt; "scale=4;15+14+14+14+14+30" With the variable it then looks like this: testvariable=$( bc &lt;&lt;&lt; "scale=4;15+14+14+14+14+30" )
If only there was some kind of shell feature so you didn’t have to type each character. Perhaps it could somehow… I don’t know… *complete* your input?
Take a deeper look into nmap. Its not just a versatile cli tool it provides a whole scripting language. Just read the nmap documentation on their website and you will see. 
If only... I wish I knew some function like that, maybe even something that you could just press a button and it could give you possibilities if there were more than one and complete if it is a single. Now that would be an awesome feature. Too bad that all the maintainers of bash loves to write and knows that handwritten code is best code...
Do you actually write shell in the shell? 9/10 times I'm using a text editor. 
`\foobar` will run foobar "naked", ignoring aliases. I believe that `\date` is equivalent to `command date`. I have `alias date='\date -u +%FT%TZ'`, so I get the following: [ 540 ]$ echo $(date +%s) date: multiple output formats specified
Lots of good alternatives here. To answer your question more directly, it hangs because `read testvariable;` is waiting for input. The command sequence on stackoverflow works because `echo "abc" | ( read foo; ... )` is different from: `echo "abc; (read foo; ...)`
https://mywiki.wooledge.org/Quotes is good reading.
weird how no text editor ever got the hint that any kind of completion would be useful, and they all make you type in every single character /s (Okay, sarcasm aside – I mostly use Emacs, and in Emacs the `dabbrev-expand` command, usually bound to `M-/`, is very useful as a generic autocompletion in any kind of text, without requiring language-specific support. Basically, I’d need to type `$EPOCHSECONDS` once in the script (or in any other buffer in the same Emacs instance) and then I could type something like `$EPO M-/` and it would auto-complete the rest of the variable name.)
Vim has that builtin :) ctrl-n and it will check other words in all buffers.
Nice tip! Gonna use the heck out of that one
Of course it does, my point is every decent editor has it! Emacs is just the one I mainly use. (But since I use Vim for commit messages, I sometimes run `:DiffGitCached` just so the diff will be loaded and its contents available for `Ctrl+N` completion.)
Perhaps a GIF of it in action would help in the readme Also, while vim is obviously better than emacs, those who have yet to embrace it might use different editors so instead of hard coding vim in the script, it might be better to use the $EDITOR environment variable which the user can set to their preferred editor
which php will tel you the path to the binary. find / -type d name ‘*php*’ to see the directory locations
What do mean php folder?
huh it says no such file or directory but I definitly have it installed 
It depends on what you called it. But you can type find . -type f -name php_folder
If typing `php -v` works for you, then: type php will tell you what you're actually running with that command. Do **NOT** use `which php` as often suggested; that only tells you where a binary called `php` can be found in your PATH. `which` incorrectly ignores any aliases and bash functions called `php`, which would take precedence during execution.
%s is actually a non-standard format specifier of strftime(3), which in turn means it's non-standard for date(1) as well. `EPOCHSECONDS` allows us to get seconds since epoch without having to rely on the system at hand having %s. Plus it's magnitudes faster $ TIMEFORMAT="real: %3lR, user: %3lU, sys: %3lS" $ time for i in {1..1000}; do d=$(date +%s); done real: 0m0,865s, user: 0m0,633s, sys: 0m0,265s $ time for i in {1..1000}; do d=$EPOCHSECONDS; done real: 0m0,005s, user: 0m0,005s, sys: 0m0,000s
How did you install it? Where i use to work the previous sys admin installed via a bitnami app which sent the binaries to /opt/bitnami/php 
If you mean where you should put the PHP files for your web application this typically depends on your web server. For example, if you're using Apache the document root of the default virtual host should be somewhere in `/var/www`. For instance with Debian it should be `/var/www/html`, you can change that of course by editing `/etc/apache2/sites-available/000-default.conf`. If you mean the directory where the PHP binary is located, you can find that by running `which php`.
 #!/bin/bash -eu PREFIX=pic FILETYPE=.png FILES=$(find . -iname "*$FILETYPE") for file in ${FILES[@]}; do directory=$(dirname $file) basename=$(basename $file) mv $file $directory/$PREFIX$basename done
Hey PC, Thank you so much, the fun way worked perfectly. So that is done! The first suggestion however, gives me this error message: mv: cannot move './10.png' to 'pic./10.png': No such file or directory I had found that one in stackoverflow or some other place and had the same error. Do you happen to know what's wrong? Just for curiosity now... :) Thanks for the help! Have a nice whatever part of the day it is where you're from! 
'pic"$(basename {})" I would think, but it's late
Tried it out, came out the same... './10.png' to 'pic./pic10.png': No such file or directory
&gt; I believe that `\date` is equivalent to `command date`. It certainly is not. `\date` will bypass aliases, but not shell functions.
And if we want to be even more pedantic – `command` can of course also be overridden by a function or alias.
&gt; %s is actually a non-standard format specifier of strftime(3), which in turn means it's non-standard for date(1) as well. Huh, TIL. Is there a POSIX way of getting the Unix timestamp (without calculating it from year, month, etc.), then?
why don't you install mlocate and run updatedb and then locate php
You can start by optimizing you grep line. The following should be far less resource intensive than a `grep -r`: `grep "MndtCxlReq" $(find .. -name '*.xml')`
nevermind, I solved by a simple grep with xargs #!/bin/bash cd /data/sia/seda/response/processed; grep -rl "MndtCxlReq" *.xml |xargs grep -re 'MndtId' -re "IBAN" -re ":Nm" ; exit
this is what I asked, thank you anyway for your help
Have you looked into `mkfifo` with a lock variable ?
&gt;Please, if you are interested, provide constructive feedback on how to improve or correct the script. I was just doing what you asked :)
Well, each individual write will always append to the end of the file, the kernel guarantees that (except on NFS). But writes from multiple processes writing to the file may be interleaved, so if a process writes a line in several steps, then lines may still be fragmented.
&gt;I suppose I can avoid hitting the disk by using a FIFO instead of a file. Good idea. Are there any other benefits to using a FIFO instead of a file in this scenario? How can I create a lock/mutex in SH? &amp;#x200B;
I see. So if I had multiple lines written in one operation, it shouldn't get clobbered. Thanks!
http://mywiki.wooledge.org/BashFAQ/045 https://stackoverflow.com/questions/6870221/is-there-any-mutex-semaphore-mechanism-in-shell-scripts
Thanks. This is great.
you sir are a scholar and a gentleman.
No problem at all thanks mate
so I can't seem to cd into usr folder. type yielded usr/bin/php but why can't I see this directory?
figured it out sudo cd and /usr /before for anyone reading through this later. Also I found out using the GUI open finder hit cmd shift G and file path you want to open. 
Are you trying to install composer? What is the full command used? 
You're going to shoot yourself in the foot. **Don't install things manually on a Linux system, use your package manager!** https://launchpad.net/ubuntu/+source/composer Clearly explain what you have and what you want, or there's going to be serious breakage.
&gt; a excutible folder `composer.phar` This is presumably a file and not a folder &gt; into my /usr/bin/ php Is this supposed to be `/usr/bin/php`? Don't overwrite your `php` binary. &gt;using `sudo cp /usr/bin/composer` This is not a valid command. You need to specify a source. Maybe it would be easier if you copy-pasted from your terminal?
`type qwe &gt; /dev/null || echo "Command not found"` does the same without needing to beware. 
sorry about the typo yes trying to move it to bin. and yes trying to install composer, as I'm not familiar with this stuff.
either way what does build do?
That's odd. Can you share the full command you ran?
Not trying to talk you out of doing this in bash as an exercise, but if you actually want this done efficiently you're better off using a CIDR-aware tool like sipcalc or nmap. $ nmap -sL 172.217.6.46/29 -oG - # Nmap 7.01 scan initiated Wed Jan 9 13:02:19 2019 as: nmap -sL -oG - 172.217.6.46/29 Host: 172.217.6.40 (sfo03s08-in-f8.1e100.net) Status: Unknown Host: 172.217.6.41 (sfo03s08-in-f9.1e100.net) Status: Unknown Host: 172.217.6.42 (sfo03s08-in-f10.1e100.net) Status: Unknown Host: 172.217.6.43 (sfo03s08-in-f11.1e100.net) Status: Unknown Host: 172.217.6.44 (sfo03s08-in-f12.1e100.net) Status: Unknown Host: 172.217.6.45 (sfo03s08-in-f13.1e100.net) Status: Unknown Host: 172.217.6.46 (sfo03s08-in-f14.1e100.net) Status: Unknown Host: 172.217.6.47 (sfo03s08-in-f47.1e100.net) Status: Unknown # Nmap done at Wed Jan 9 13:02:19 2019 -- 8 IP addresses (0 hosts up) scanned in 0.04 seconds `-sL` in nmap is the list-only scan, it doesn't send any traffic (except the reverse DNS lookups). 
That sounds like the error you'd get on macOS if SIP was enabled (which it is by default). Copy it to `/usr/local/bin` instead. As far as "build", idk. From context I guess it's a file, but I'm not aware of any common scenario where the Composer PHAR would be named `build`.
this is where I found that build command https://forums.appleinsider.com/discussion/189702/cant-copy-file-to-usr-bin-even-when-logged-in-as-root What is SIP and I don't think it will work in that folder.
&gt;this is where I found that build command https://forums.appleinsider.com/discussion/189702/cant-copy-file-to-usr-bin-even-when-logged-in-as-root That doesn't seem to have anything to do with this. `build` there is just some directory containing a file. If you're confused by that I think you probably need to read more about how file paths and shell commands work. Unfortunately I don't have anything in front of me that goes into it at that level, but maybe someone else here has a suggestion. &gt;What is SIP System Integrity Protection. It's mentioned in your link. &gt;I don't think it will work in that folder I'm sure it will, or with very minor changes you can make it work there. In order to put it in `/usr/bin` you would have to disable SIP, which is absolutely **not** necessary or recommended.
Based on other comments, you're trying to install PHP Composer on macOS. I **strongly** suggest you install and use the [Homebrew package manager](https://brew.sh/) instead, then use that to install and update Composer (and a more recent PHP, for that matter). That way, you don't have to use root at all, and can keep all your installed stuff up-to-date with a single `brew upgrade` command.
&gt;is the &gt;&gt; operation to a file thread safe? No. It requires at least two separate I/O operations (open-file-for-append and one or more writes-to-file), so it's entirely possible for multiple bash processes to open the same file and get the same append point, then when the dust settles, only one script's writes would've survived. &gt;I was hoping I could use a temp file as my queue Consider using a temp *dir* instead, and create automatically-named temp files in it with `mktemp`, so each script's output is safely separated, without resorting to locks that impede maximum concurrency: #!/usr/bin/env bash mkdir -p /tmp/snmptraps # FD 3 gets all the SNMP traps and annotates them with nanosecond timestamps exec 3&gt; &gt;(while read; do echo $(date +%s%N); done &gt;/tmp/snmptraps/$(mktemp)) ... echo This is a test trap &gt;&amp;3 ... echo Trap 2 &gt;&amp;3 ... Then, when you're ready to process them all: sort -n /tmp/snmptraps/* | while read timestamp trapmsg; do ... done rm -fr /tmp/snmptraps
 grep -rl "MndtCxlReq" *.xml |xargs grep -re 'MndtId' -re "IBAN" -re ":Nm" Useless use of recursive `grep`. :) If your XML files are all in the same directory, then specifying recursion (`-r`) in the first `grep` is pointless. If not, you'll need a `find` to list all the filenames to `grep`, so `-r` is pointless again. Regardless, all `-r` flags in the second `grep` are pointless, for the obvious reason that you're already passing individual filenames. A more robust version of your pipeline would therefore be: find /data/sia/seda/response/processed -name \*.xml -exec grep -l MndtCxlReq {} + -print0 \ | xargs -0 grep -h -e MndtId -e IBAN -e Nm That handles: * potentially *millions* of XML files (for which a shell would choke trying to expand `*.xml`) * in both a flat directory and a deep hierarchy, as well as * paths containing whitespace (`find -print0 | xargs -0` is a common shell idiom that's well worth learning), while * not polluting the final output with filenames (`grep -h`).
Your one-liner _almost_ worked. It failed because `find` always prepends `./` to the basename component of a file before doing the `-execdir` for disambiguation reasons (e.g. filenames beginning with `-` may be mistakenly processed as command options). To make it work, use `rename` instead: find . -iname '*.png' -execdir rename ./ pic {} + A nice side effect is a _massive_ reduction in the individual `rename` processes launched.
See [my other reply](https://www.reddit.com/r/bash/comments/ae4t5q/bulk_file_naming/edomoc2/) for a one-liner that _does_ work, along with an explanation why.
so how do I use it to install composer? brew install composer?
Yes. I also recommend installing a more recent version of PHP, instead of relying on the ancient system one.
ok installed homebrew, installed get, was able to install composer, and codecept except now when I run bootstrap cmd nothing. command not found?
oh yea did that via codecept instructions. https://codeception.com/quickstart
/u/PC__LOAD__LETTER i was able to do what i wanted. I put the first three octets in another variable and just iterated 0-255 and went through all the hostnames. 
Grep will have a return code of 1 or 0. So you want to do: wget args | grep -i -q pattern if [ "$?" == 0 ] ; then echo found it else echo nope fi
1. [wget](https://www.gnu.org/software/wget/manual/wget.html) saves websites to the current directory. If you want to pipe it's output you need to use `-O -`. 2. This is not how bash [comparison operators](https://www.tldp.org/LDP/abs/html/comparison-ops.html) work. You need to use `-ge`. 3. Bash [test constructs](https://www.tldp.org/LDP/abs/html/testconstructs.html) are enclosed in parentheses or brackets. 4. Checking the output of a command is *not* the same thing as checking the [exit status](https://www.tldp.org/LDP/abs/html/exit-status.html) of a command. 5. [Command substitution](https://www.gnu.org/software/bash/manual/html_node/Command-Substitution.html) syntax is `$(command)`. This is how I would do something like you're trying to do: (wget -q website -O - | grep 'text' &amp;&gt;/dev/null) &amp;&amp; echo 'has text' 
With github: 1. Install Hub (brew install hub on a Mac, for example) 2. Alias git=hub 3. git pull-request 
Is there something similar for GitLab? lab?
The biggest inconvenience I have with opening Merge Request via the link Gitlab provides me on feature branch push, is that it always set `master` as target branch. Because of our team workflow (gitflow-like with features branches that can be branched off `develop` or `master`) I went to a script slightly more complicated than yours so that target branch is correctly deduced. And I did put that in `pre-push` script as I tend to avoid custom bash aliases and commands. Anyway, if OP script falls short for aforementioned reason, and someone searches a way to achieve that, I documented the thing on [SO](https://stackoverflow.com/questions/53284908/git-hook-hack-to-workaround-bad-target-branch-when-opening-merge-review-on-gitla/53611747#53611747).
Don't know. Hub is made by github, so maybe if gitlab has written it :-)
There's [awk's srand()](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/awk.html#tag_20_06_13_12): &gt; **srand**(**[***expr***]**) &gt; Set the seed value for *rand* to *expr* or use the time of day if *expr* is omitted. The previous seed value shall be returned. Which means you can do: $ awk 'BEGIN{srand(); print srand()}' 1547106439 However, the text does not specify that "time of day" means seconds since epoch, so there isn't really a reliable way that I know of.
I'd go with awk for this as it can look for both patterns in one pass: awk ' /text 1/{t1 = 1} /text 2/{t2 = 1} END{ if (!t1 || !t2) exit(1) }' Combined with curl: if curl -sSf "$url" | awk '/text 1/{t1=1} /text 2/{t2=1} END{if (!t1 || !t2) exit(1)}'; then printf 'It contains both texts\n' fi 
Perhaps also provide an example of the desired output and representative example data of the files you are searching in.
I didn't think that would be necessary since the grep on its own works, but I will update the post.
I think your approach won’t work if the first occurrence is more than 30 lines after the pattern. From what I understand, you’re trying to find the first line after a certain repeating pattern. Could you use `uniq` to collapse all the repetitions and them just peek at the next line after your pattern? I hope I’m understanding the problem correctly. Generally, this kind of problem *sounds like* a job for `awk`. Set a variable to 0. If the variable is 0, then when you find the pattern for the first time, you add 1 to a variable. Now if the variable is 1, check if the line is NOT the pattern. Print that line. I’m not sure if `awk` supports loop breaking. If you’re concerned, you could use a little python parsing script instead of bash.
I'll check that. Thanks
I agree on both points – Although I doubt the second match would be more than 30 lines away, I still don't feel great about the double grep operation, it seems really clumsy. &amp;#x200B; &gt; From what I understand, you’re trying to find the first line after a certain repeating pattern. Could you use &gt; &gt;uniq &gt; &gt;to collapse all the repetitions and them just peek at the next line after your pattern? I hope I’m understanding the problem correctly. &amp;#x200B; Hm, not quite, there's no repeating pattern, it's just a pattern that needs to to be found after the first occurence of a different pattern. Everything in between could be random AND additionally the second pattern could not exist at all. In that case, JUST looking for the next occurence of the second pattern could mean that it grabs it from the next instance, resulting in wrong data. Limiting the scope to 30 lines was a workaround for that, but I suppose the loop could also terminate if, for example, it encounters"brik ID" without having found the second pattern. I agree that awk or maybe perl sounds like a much better tool for the job, but sadly I don't know the first thing about them. I'm currently trying to reimagine the whole thing in Python, which might work out better. &amp;#x200B;
If I'm reading this right then you have multiple issues You're calling `fileCheck` without any arguments. So when inside `fileCheck` - `$1` will always be empty (the variables `$1`, `$2`, etc. are not global. They're local to where you are. There's the main scripts `$1`, `$2`, etc. for arguments supplied, but functions will have their own) So you need to be calling `fileCheck` with an argument fileCheck "$1" Also you're doing (copy/pasted from your post) if [ "$1" == " " ]; then There's a space inside the quotes, so this is checking if string is a single space. This is obviously wrong, but don't just delete the space. Use if [ -z "$1" ]; then instead
I think it might be a good opportunity to learn `awk`! I think I understand now. You have pattern1 and pattern2, and you want to check whether pattern2 occurs after pattern1 (it might also occur before, but we don’t care about that). I’m on mobile, so I won’t attempt an awk script. I still like the idea of making a variable equal to 1 after coming across pattern1 and then if the variable is 1 and you detect pattern2, you exit successfully. What’s nice about awk is it’s quick and lightweight for problems like this. Since you’re in the bash subreddit, you are probably interested in learning awk. You can do the same in python for sure, though. Let me try to find a good awk resource. [This one](https://www.tutorialspoint.com/awk/awk_basic_examples.htm) is ok. I would also just look for a bunch of examples. I recently found [this awesome site for simplified man pages](https://tldr.sh) that has great examples of common uses for all kinds of shell commands (they have a mobile app, too, which is awesome)
thanks, with the fileCheck argument I could solve the issue
 # The file containing the patterns, one per line $ cat patterns EXAMPLE2ABC aiwohdefee3iCh1Ve8ch upuWH8QgTBOBQaH81kj-ZA lalalala # Example directory hierarchy $ find . -type f -print ./patterns ./Example2/example2.xml ./Gramex-2018-01/dab-2018-01-eftovrig.xml-1664717 ./Example1/non-matching.xml # Test run $ find . -type f -exec awk 'NR == FNR { p = p s "\"" $0 "\""; s = "|"; next } /&lt;\/musa&gt;/ { i = 0 } match($0, p) { i = 1; c = substr($0, RSTART + 1, RLENGTH - 2) } i &amp;&amp; i &amp;&amp; /Hovedkunstner|solist/ { printf("%s;%s-%s\n", c, FILENAME, $0); i = 0 }' patterns {} + EXAMPLE2ABC;./Example2/example2.xml-&lt;person role-source-system="MUSA" role-code="4" role="solist"&gt;Example Example&lt;/person&gt; upuWH8QgTBOBQaH81kj-ZA;./Gramex-2018-01/dab-2018-01-eftovrig.xml-1664717-&lt;person role-source-system="MUSA" role-code="4" role="Hovedkunstner"&gt;Sergey Eletskiy&lt;/person&gt; This is no robust approach, it makes many assumptions about the format of your data. awk is no suitable tool for processing XML. Consider using tools like [xmllint](http://xmlsoft.org/xmllint.html) or [xmlstarlet](http://xmlstar.sourceforge.net/) instead. # NR and FNR are only equal for the first file, which contains the patterns NR == FNR { # Build a regular expression, assuming the patterns do not contain # special characters, and assuming the matching strings are surrounded # by quotation marks p = p s "\"" $0 "\"" s = "|" next } # Assume &lt;/musa&gt; closes a block containing the requested lines /&lt;\/musa&gt;/ { i = 0 } # A line matching the regular expressions opens such block match($0, p) { i = 1 # Remember the pattern that matched c = substr($0, RSTART + 1, RLENGTH - 2) } # A line containing either "Hovedkunstner" or "solist" within # a matching block i &amp;&amp; /Hovedkunstner|solist/ { printf("%s;%s-%s\n", c, FILENAME, $0) # Print only the first matching line i = 0 }
I have found more often than not recently that doing anything hard with grep in Bash is usually better served by learning to write an .awk script. There seem to be surprisingly few good .awk scripts out there. Plenty of one-liners. Scripts are a bit quirky to work with at first but the speed and power can't be matched by pure bash solution. You can also break the problem down and use a mixed Bash/Awk solution. This doesn't solve your problem short term but might help more long term.
My experience has been if something becomes too complicated, or too long, or if you ever had to share the scripts, those are reasons it makes sense moving them to a location like `/usr/local/bin/`. I recently ran some benchmarking to see why my own bashrc was talking over a second to load, and it wasn't the few functions I had there, it was [thefuck](https://github.com/nvbn/thefuck). Of course add too many, and yeah, now you should think about moving them to a local bin directory. But just a collection of short functions? I say keep life simple, keep 'em in your bashrc. Maybe a middle ground for you is to do something like moving those functions to their own file (call it `.bashrc_functions` or whatever) and just source that in your main bashrc file. It adds some organization if you don't mind managing two files now.
&gt;Awesome idea. I'll use this approach. Thanks. &amp;#x200B;
Upvote for [promoting an XML tool](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags).
I work in network operations, so many small functions that can be rearranged on the fly are invaluable to my workflow. I have a shared directory with `go=rx` perms and a quick comparison of the Epoch time since it was last sourced vs last modified. This lets me push changes to multiple users in real time without losing flexibility.
For comparison, in my experience I benefit a lot from making lots of very small independant scripts. Very small as in, I have a `git-authors` script with only this : ```bash git log --format='%an &lt;%ae&gt;' $* | sort -u ``` And another `git-report`, which, while longer than that, in essence contains only : ```bash for author in $(git authors $*); do git insertions "$author" $* # git-insertions is another (python) script, much longer done | sort -h ``` [All][0] of those very small scripts allow me a lot of flexibility when putting together other scripts. [0]: https://github.com/jRimbault/rc/blob/master/scripts/
Yeah, and dealing with the different implementations of `awk` can be tricky. One system might have `gawk`, another might have `mawk`, another might have `nawk`, another might have `oawk`... and how they implement `rand()` and `srand()` may differ, or they may not implement those functions at all. Here's some code from my archive for some other methods: if date +%s | grep "^[0-9].*$" &gt;/dev/null 2&gt;&amp;1; then date +%s elif command -v perl &gt;/dev/null 2&gt;&amp;1; then perl -e "print time" elif command -v truss &gt;/dev/null 2&gt;&amp;1 &amp;&amp; [[ $(uname) = SunOS ]]; then truss date 2&gt;&amp;1 | grep ^time | awk -F"= " '{print $2}' elif command -v truss &gt;/dev/null 2&gt;&amp;1 &amp;&amp; [[ $(uname) = FreeBSD ]]; then truss date 2&gt;&amp;1 | grep ^gettimeofday | cut -d "{" -f2 | cut -d "." -f1 
I am a *nix sysadmin. I primarily work on Linux and Solaris, but sometimes I'll be on AIX or HPUX. I treat my `.bashrc` as a monolithic "digital tool box", it is full of functions, aliases, OS specific tweaks/fixes etc and it's in the order of 2300 lines. There's no discernible load time issues with it. Because I manage it using github, one demarcation line that I have for it is that no client sensitive information goes into it. Near the start are these lines: # Some people use a different file for aliases # shellcheck source=/dev/null [[ -f "${HOME}/.bash_aliases" ]] &amp;&amp; . "${HOME}/.bash_aliases" # Some people use a different file for functions # shellcheck source=/dev/null [[ -f "${HOME}/.bash_functions" ]] &amp;&amp; . "${HOME}/.bash_functions" I never use `.bash_aliases`, personally, it's mostly just there for completeness. Sometimes I use `.bash_functions` for client and/or host specific functions. Let's say, for example, there's one `rsyslog` server for an entire fleet. On that server, I might have a handful of functions for parsing all the log entries that arrive... let's call them `logparse1()`, `logparse2()` etc. Because these functions are specific to that particular client, I won't put them into my main `.bashrc`, and because they only matter on one host, there's no point clogging up my `.bashrc` with them anyway. Into `.bash_functions` they go, and they can stay there unless... If I create a function that is useful to my colleagues, I share it with them. If it gains popularity and a place in our workflow, we will split it out to a script that resides in `git` and is deployed somewhere common on our customer's hosts (e.g. `/opt/ourcompany/bin`, that way we're not messing up `/usr/local`, and if the customer relationship ends, we can cleanly remove our intellectual property) That's how I structure things. No dotfile management tools, no `~/bin` directories full of hundreds of unmanageable scripts etc
Without really knowing how installPie works, here's a couple methods that might be of help: if ./installPie 2&gt;&amp;1 | grep -q "the installation has been completed"; then # continue script else # require input fi So that ensures the scripts output all goes to stdout, so that grep can look for your string. But you mentioned that installPie wants its own input, in the form of hitting 'enter'. You can usually simulate that with `echo | someCommand`. I wish I had something handy to verify if this would work, but I think this would work: if $(echo | ./installPie 2&gt;&amp;1) | grep -q "the installation has been completed"; then ... Otherwise, I might write the output to a temp file first: installResult=$(mktemp) echo | ./installPie &gt; installResult 2&gt;&amp;1 if grep -q "the installation has been completed" installResult; then ... Run your script through [https://www.shellcheck.net/](https://www.shellcheck.net/) as well to check for any errors or problems.
&gt;2&gt;&amp;1 I will rewrite the bash script, and test it out. thanks &amp;#x200B;
You can solve issue 1 by using an `until` loop. As for Issue 2, while I'm not sure it'll work, [xdotool](https://manpages.ubuntu.com/manpages/trusty/man1/xdotool.1.html) seems like what you're looking for; it lets you simulate keyboard / mouse commands (and can also move + resize windows). cd Install MSG="the installation has been completed" # Create an empty output file the log data: LOG="./status.log" touch "$LOG" # Until the log file contains the status message, sleep. until [[ `grep "$MSG" ./status.log` ]]; do sleep 10 # Upon completion, hit the "Enter" key: done &amp;&amp; xdotool key Return &amp; # PS: These chained commands are being run in the background # The `tee` command outputs the information to the screen as well as the logfile. ./installPie | tee "${LOG}" &amp;#x200B; Fair warning: no idea if this would actually work, but I think it should. I can imagine there being an issue with the last line just hanging forever (because `Enter` would be provided to the `tee` command rather than the install script), but that could easily be resolved by replacing: done &amp;&amp; xdotool key Return &amp; with done &amp;&amp; xdotool key Ctrl+c &amp;
My 2 cents. Group functions with similar domains in scripts (modules if you will). Have one file that sources in all of the modules and source that into your scripts or shell. Set up a system to push them out from a central source. This works very well and is maintainable and portable. I almost never use aliases over functions. 
I've always favored scripts over global-level functions for discrete components, mainly because I get **namespace isolation** for free with scripts, whereas I'd have to remember to `local` all the locals with functions. It's really easy to miss (or typo) one or more `local` declarations as you evolve code over time, leading to mysterious errors when one or more functions accidentally overwrite a global variable that's subsequently used with something else. In a loop, you get weird bugs that are a pain to figure out. None of this is a problem with scripts, since they start with completely separate namespaces, and therefore can't mess with your interactive environment _or other calling scripts_ without a lot of contortions. They're also a lot less messy to use from other languages, since they're real programs rather than shell-specific constructs that are invisible to your OS. I only ever write global-level functions when I actually _want_ to mess with the current environment, and I don't ever expect to use them outside of an interactive shell. Oh, and I haven't defined a new alias in _decades_. Their only real advantage is ease of definition; otherwise, both functions and scripts slap them silly with vastly greater functionality any day.
If the installPie requires you to press enter you can look at using an expect script which can do this for you.
+1 was going to suggest using Expect for this as well
Thanks for you answer. The `local` part is something I need to think about.
Thanks. Aliasses for me are basically things like `alias mount="sudo mount"` If it is anything more, it will become a function.
The github idea is interesting. I must look into it.
Thanks for your feedback on using a separate directory that can be used by many. As it is just for me, it wil not be that usable, but interesting nontheless. 
Thanks. This might indeed be the way to go. Sometimes maintaining multiple files is easier than one big one.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
hello, i will give it a try .
You can use [shellcheck](https://www.shellcheck.net/) to help debug, but yes, the heredoc is not properly formatted. Instead of doing these case statements all on a single line, I would make them their own functions if for no other reason to make it more readable, rather than fancy. That said, I stumble around using heredoc all the time, so a couple references that hopefully will help. [https://stackoverflow.com/questions/2953081/how-can-i-write-a-heredoc-to-a-file-in-bash-script](https://stackoverflow.com/questions/2953081/how-can-i-write-a-heredoc-to-a-file-in-bash-script) [http://wiki.bash-hackers.org/syntax/redirection?s\[\]=heredoc#here\_documents](http://wiki.bash-hackers.org/syntax/redirection?s[]=heredoc#here_documents)
All the heredocs in your `case` statement are wrong. The heredoc terminator must be the only word on a line by itself, and the bash man page even provides an example: &gt;**Here Documents** &gt; &gt;This type of redirection instructs the shell to read input from the current source until *a line containing only* `delimiter` *(with no trailing blanks)* is seen. The format of here-documents is: [n]&lt;&lt;[-]word here-document delimiter Also, I don't see any content in your heredocs, and there's really no reason to squeeze multiple commands into a single line like that. Your `case` statement would probably work a lot better as follows: case $chosenDB in 1) SSH -i ./SSH/dev_db.pem ubuntu@$IP &lt;&lt;'__REFRESH__' &gt; /Users/$USER/Documents/$0.log 2&gt;&amp;1 &lt;case 1 input to SSH here&gt; __REFRESH__ G ;; 2) SSH -i ./SSH/dev_db.pem ubuntu@$IP &lt;&lt;'__REFRESH__' &gt; /Users/$USER/Documents/$0.log 2&gt;&amp;1 &lt;case 2 input to SSH here&gt; __REFRESH__ S ;; 3) SSH -i ./SSH/dev_db.pem ubuntu@$IP &lt;&lt;'__REFRESH__' &gt; /Users/$USER/Documents/$0.log 2&gt;&amp;1 &lt;case 3 input to SSH here&gt; __REFRESH__ G; S ;; esac
Thanks so much! I’ll review when I get back to the desktop, but just to point out, the content in the here doc is supposed to be the function called. So for example: SSH to Ubuntu Open here doc Run function G Close here doc The reason why I am doing it that way is because without the here doc, the script ends once it runs the SSH command. 
cool, didn't know that!
it's actually hit enter after "the installation has been completed" message show up on the screen. will this command still work? echo | someCommand should I change it to somecommand | echo &amp;#x200B;
The first one (`echo | ./installPie`) is what you'd want. Since `echo` produces a newline, it can simulate 'Enter'. However, if there's other prompts the script needs, [expect](http://www.admin-magazine.com/Articles/Automating-with-Expect-Scripts) may work for you better, as /u/dat720 suggested.
`xmllint` is fine for the job. You're lucky enough to have structured data, don't make the mistake of attacking it with tools meant for unstructured data. $ xmllint --xpath '/brik/musa[@content-id="upuWH8QgTBOBQaH81kj-ZA"]/persons/person[@role="Hovedkunstner" or @role="solist"][1]' test.xml &lt;person role-source-system="MUSA" role-code="4" role="solist"&gt;Sergey Eletskiy&lt;/person&gt; 
&gt; the content in the here doc is supposed to be the function called. So for example: &gt; SSH to Ubuntu Open here doc Run function G Close here doc It's not going to run function G unless it's also declared on the destination machine. You're telling it to send a `G` to the remote server, not run your script's `G()`. &gt; The reason why I am doing it that way is because without the here doc, the script ends once it runs the SSH command. Heredoc or no, the SSH command is the last thing in your script, I can't imagine what else you expect it to do after that but end. 
This looks like a question for your particular linux distro's sub. Also there's always the Arch wiki. This isn't directly related to bash
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux4noobs] [pls help](https://www.reddit.com/r/linux4noobs/comments/aeyp99/pls_help/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
yeah i know bot.. its me.. out of desperation, pain and stupidity
Quote every variable. echo "${line1}"/"${line2}"/"${line3}"
Same result and how is that different then encapsulating the entire line in quotes?
also check dmesg output when it stops. I had a weird issue once where the driver was crashing. 
I was guessing, but you are right that's not it. If so, the problem lies elsewhere. Might be when you assign line1. So if you would do echo "${line1}" you will probably also receive Folder1. Maybe you just have to give us the complete script (or portion) in which you have this problem. 
Not really a direct answer to your actual question per se but have you looked at using zsh in stead of bash? Use ohmyzsh on top of it and it literally does this out of the box. https://ohmyz.sh
This is what I've got that works perfectly. parse\_git\_branch() is called within my PS1, and echos out the branch and the different symbols from parse\_git\_dirty() if there are changes etc. Could easily be changed to switch to color diffs instead of text symbols. # get current branch in git repo function parse_git_branch() { BRANCH=`git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/\1/'` if [ ! "${BRANCH}" == "" ] then STAT=`parse_git_dirty` echo "[${BRANCH}${STAT}]" else echo "" fi } # get current status of git repo function parse_git_dirty { status=`git status 2&gt;&amp;1 | tee` dirty=`echo -n "${status}" 2&gt; /dev/null | grep "modified:" &amp;&gt; /dev/null; echo "$?"` untracked=`echo -n "${status}" 2&gt; /dev/null | grep "Untracked files" &amp;&gt; /dev/null; echo "$?"` ahead=`echo -n "${status}" 2&gt; /dev/null | grep "Your branch is ahead of" &amp;&gt; /dev/null; echo "$?"` newfile=`echo -n "${status}" 2&gt; /dev/null | grep "new file:" &amp;&gt; /dev/null; echo "$?"` renamed=`echo -n "${status}" 2&gt; /dev/null | grep "renamed:" &amp;&gt; /dev/null; echo "$?"` deleted=`echo -n "${status}" 2&gt; /dev/null | grep "deleted:" &amp;&gt; /dev/null; echo "$?"` bits='' if [ "${renamed}" == "0" ]; then bits="&gt;${bits}" fi if [ "${ahead}" == "0" ]; then bits="*${bits}" fi if [ "${newfile}" == "0" ]; then bits="+${bits}" fi if [ "${untracked}" == "0" ]; then bits="?${bits}" fi if [ "${deleted}" == "0" ]; then bits="x${bits}" fi if [ "${dirty}" == "0" ]; then bits="!${bits}" fi if [ ! "${bits}" == "" ]; then echo " ${bits}" else echo "" fi } It's certainly longer, but it works. Can't remember who I stole it from, but discovered it after much googling.
thank you i will give it a try 
Git comes with a [shell function](https://github.com/git/git/blob/master/contrib/completion/git-prompt.sh) to do all this. You might find inspiration there.
AMEN! I will never understand why so many people insist on reimplementing something that already ships with stock Git (supporting both bash and zsh, too).
While most people probably do just want a stock solution that works (and just don’t know that it exists), many people don’t look too hard because they enjoy the process of figuring it out themselves and having both a functional tool and gained experience at the end of the process.
Exactly. I build my own prompt, which overall probably does about what the git prompt does, just worse, and I had to check their approach many times, but it was extremely instructive.
&gt;the content in the here doc is supposed to be the function called As /u/MurkyCola pointed out, that won't work if the remote server doesn't also have the definitions of `G` and/or `S`. You can dump out the function definitions with `declare -f` and add that to the stuff you ship to the remote server. You should also use `declare` to retrieve and pass on any global variables referenced in `G` and `S`, otherwise your invocations on the remote server would fail spectacularly. Since the SSH portion is common to all cases, your `case` statement can be reduced to the following, without heredocs: case $chosenDB in 1) declare -f G ; echo G ;; 2) declare -f S ; echo S ;; 3) declare -f G S ; echo "G; S" ;; esac | SSH -i ./SSH/dev_db.pem ubuntu@$IP &gt; /Users/$USER/Documents/$0.log 2&gt;&amp;1
Bash is essentially ubiquitous at this point. Sh scripts are technically more portable since they’re POSIX compliant, but these days that’s essentially insignificant in practice. Unless you’re doing something very specific, and you’d know if you were, bash will almost certainly be what you should choose. So: bash, hands down. 
https://www.tldp.org/LDP/Bash-Beginners-Guide/html/ Here you go. 
However, unless you use arrays, you don't need bash functionalities. Most "bash" scripts are actually badly set sh scripts. Note that some systems don't ship bash (BSDs) and that going through otherwise simple scripts to replace `[[` with `[` is a PITA. Some of my clients expressly request sh scripts because they still use ancient Unix variants (AIX, etc.)
Yeah, I’ve never in my career been forced to use an environment without bash. Unless you’re planning to support some old Unix systems, bash is fine. 
&gt; Before executing the command we need to change the permission of script &gt; &gt; `$ sudo chmod -R 777 firstScript.sh` &gt; &gt; OR &gt; &gt; `$ sudo chmod +x firstScript.sh` &gt; &gt; Now script is ready to run &gt; &gt; `$ sudo ./firstScript.sh` &gt; &gt; Output : `Hello pal` sudo? chmod 777? You have *got* to be kidding. 
+1 for Machtelt's guide... (***Don't*** be put off by the date on it) 
If you want to run script from root user an you loged in as non root user (in ubuntu we need sudo for non root user and 777 is for full permission that is read write and execute 
Nopety nope!!!!
No you don't. And if you don't know why, you should be reading as opposed to putting tutorials with bad advice up.
The Peter Principle in effect, folks. but anyway.... Good job, little buddy. You keep on writing tutorials!! 
change ownership not permissions! 
You can abuse 'set -o xtrace', a.k.a. `set -x`, to pipe both the command and its output into another command. Example (using 'cat' instead if your script for demonstration): $ (PS4=''; set -x; echo "hello world") 2&gt;&amp;1 | cat echo 'hello world' hello world Discussion: * `(`...`)` groups several commands into a subshell. * `PS4=''`: empty the xtrace prompt so the command is not prefixed by `+ `. * `set -x`: activate xtrace. * `2&gt;&amp;1`: since xtrace is written to standard error and not standard output, we need to redirect standard error to standard output in order for it to be read by the pipe.
A list of tutorials, ranked: http://wiki.bash-hackers.org/scripting/tutoriallist A good bash tutorial will point out what's bash vs sh so you can learn both at once. Learn bash, understand which bits are bash-specific. Also, avoid the TLDR advanced bash scripting guide; it's got many issues which have been pointed out to the author who hasn't fixed a thing.
Assuming you want your `tt` script to discover the piping command by itself, a similar question [was asked in 2011](https://superuser.com/questions/244413/how-do-you-determine-the-actual-command-that-is-piping-into-you) on SuperUser. There are a couple of solutions that date from the same period, and they still work as far as I can tell. However, modern `lsof` on modern Linux platforms^(1) enables a *much* shorter solution: NAME=$(lsof -t -p $$ -a -d 0 +E | while read p; do [ $p -ne $$ ] &amp;&amp; echo "$(tr \\000 " " &lt;/proc/$p/cmdline)" done) In a nutshell, the above tells `lsof` to: * focus on FD 0 of the script's PID, i.e. the pipe (`-p $$ -a -d 0`) * display the pipe's endpoint file information (`+E`), so selects the processes on both ends of the pipe * print PIDs only (`-t`) `tr` replaces the null bytes separating each argument in the command line with spaces. If that's a little too ugly for your taste, you can use the equivalent: ps --no-header -o args $p Note that this solution, and indeed all the solutions mentioned in the SuperUser link above, require that the input process live long enough for `lsof` to capture its process info. A simple `echo "hello world"` will likely exit too quickly to work. The only *guaranteed* solution requires you to annotate the process input in a manner similar to what /u/McDutchie suggested, but it's obviously very intrusive and not self-contained. ## Footnotes ^(1) Specifically, this works if `lsof` was compiled with `-DHASUXSOCKEPT`. You can check your local `lsof` as follows: lsof -v 2&gt;&amp;1 | grep HASUXSOCKEPT ## Further Reading * [lsof man page](http://man7.org/linux/man-pages/man8/lsof.8.html)
If you are going to use an alias make the name unique. What happens when I create a script with the command "mount" or "sudo mount" and you run it? Maybe is works maybe it doesn't. Things like this break expected behavior and portability and are in general a poor practice for building coherent systems. Explicit better than implicit. Have I done things like this? Yes. Have I lived to regret it? Yes. Hope that helps.
That's pretty cool, and looks straightforward to deconstruct and learn how it works. Thanks!
This is nice! I think I'm going to use it directly first, and then go on to tweak it if there are parts I want to change (would be a good learning exercise too). Thanks!
this sub is not meant for soliciting work requests. if you have a question im sure someone here would gladly answer it.
ah okay. I just needed this ASAP and didn't see any rules about it. I'm looking to grep for a certain thing in a log file and use tail to determine if a new instance comes in and email X if it does 
[removed]
Have you seen the "swatch" command? You can have it tail a log file and take an action when it sees a matching expression. I use it for server logins and other things to send me a slack message.
Without knowing more about the specifics of your environment, it's hard to say for sure, but if you're just trying to send an email if a string is located in a given file, then this should be modifiable for your situation: echo "Enter String Value" read STRING echo "Enter File Name" read FILE_NAME echo "Enter Email Address" read EMAIL_ADDR RESULTS="" RESULTS=$(grep ${STRING} ${FILE_NAME}) if [[ ${RESULTS} == "" ]]; then echo "String not found" else echo "String found" echo ${RESULTS} | mailx -s "String Found in ${FILE_NAME}" ${EMAIL_ADDR}
Wikipedia has good descriptions of the different CLI shells. Check Bourne (sh), Bash, Korn and Z shell. Unless you are using something like associative arrays, you can use any of them. I used Korn shell at a previous job and the scripts I wrote there work with Bash
That is not a compromise I am willing to make. The idea of the command is to either process a file or a pipe and upload it to pastebin.com from the command line. So I would do e.g. `ps aux` and then want to share the result, so I arrow up, add `|tt` and be done with it. So it has to be something that is done from within the script itself. If it is not possible, I would rather use `read` and copy and paste the command again then to add things to the command line. 
In general you are right. With something like `sudo mount` or I have yet to have an issue in the 20 or so years I use Linux. The others I have are`grep --color`, `head -n $(($LINES-3))` and 2 or 3 others.
&gt; A simple echo "hello world" will likely exit too quickly to work. Seems that that is the problem. Thanks fore making it clear that this will not be the way to proceed. 
If your goal is to be able to access your last command and use it, why not use bahs history to access commands? Alternatively, you could set your pronpt_command to save your commands to a temporary cache then have your "tt" command access that cache. In essence, you would then run your command. Then just type tt rather than up-arrrow pipe tt. I'm on mobile but if that seems like a reasonable solution I can definitely flesh out my idea more. 
[removed]
Systems that don't have `bash` will almost always have some form of `ksh`, and converting from `bash` to `ksh` scripts is a lot easier than, say, converting from `fish` scripts. The only time I've ever had to write pure Bourne has been for Solaris package scripts. Outside that, `ksh` is the lowest common denominator in my experience. https://www.in-ulm.de/~mascheck/various/shells/
What you're after can actually be a bit tricky - I know, because I've scripted for this exact problem at the enterprise level. My advice is to look for an existing tool like logwatch or logwarn, or to change the requirements (e.g. "alert if a new instance of string arrives within the last hour")
&gt; z.lua an alternative to z.sh: you might want to tell us what the hell either of those things are
&gt; So I would do e.g. ps aux and then want to share the result, so I arrow up, add |tt and be done with it. So it has to be something that is done from within the script itself. What you want is fundamentally impossible. A pipe simply reads data from standard input, passing it on to a process. It has absolutely no way of having any knowledge of the command that generated that output. But my technique can be made just as convenient with a shell function: tt() { (PS4='$ '; set -x; "$@") 2&gt;&amp;1 | nl } Now you can do `ps aux`, arrow up, type Ctrl+A, tt, space and be done with it. 
I think what you want is a wrapper script, not a pipe, so you want to prepend, rather than append. I'm not going to give the full sample since I'm on mobile but it should be like the following...... # do setup and redirection if needed # echo whatever you wanted to echo echo "$*" #print the name of the command being run, with its arguments $@ #shell expand like above but keeping the separators in tact and run it With that, you'd be able to run your original, up arrow, home, type the wrapper at the front, and run it again. Hopefully that meets your desired usecase.
There’s one thing bash has which /bin/sh doesn’t: *support*. It’s far easier to write a bash script than it is to write a /bin/sh script; linters don’t work, expansion is full of traps, and never, ever, ever do you want to try converting a call to `getopts` using the classic Bourne shell. The one thing you *will* get is nearly universal performance, regardless of platform (windows don’t count). If you’re writing for the absolute lowest common denominator, then yes, write a /bin/sh compatible script. Otherwise, use `#!/usr/bin/env bash` and save yourself the headache. 
No, the diagram is wrong.
It's like [this](https://cdn-images-1.medium.com/max/800/1*l6ZTq6taGrkLkLP-hTZZuQ.jpeg)
Not necessarily bash, but *some* shell. The shell is basically the program executed at user login. Shells are defined per user in */etc/passwd*. You can also use the `-s` option to specify a shell when switching users vi `su`. Try it out!
Yeah, but even that kind of diagram doesn't say very much, and what it does say can be misinterpreted. "So user processes need the kernel to use the CPU and RAM, right?" 
&gt; Not necessarily bash, but some shell. Incorrect. Any process can fork a new process, and any process can load a new program image. Shells aren't "special" in that regard. &gt; The shell is basically the program executed at user login. Only on some systems. It's perfectly possible for a system not to have a traditional shell at all.
&gt; When we run python, java, node, c, c++ etc. applications, all these applications gets executed via bash only? To answer this bit specifically, "running a program" on most systems involves: * creating a new process; * loading a program into that process. (The exact details of these steps differs from system to system.) This is certainly something a shell can do, but there's nothing really stopping any other program from doing so. _Shells are just ordinary programs._
Thanks for the corrections before my ninja edit. Yes, you are right, shells aren't even necessary and other process can call programs as well. Guess I was just trying to simplify what bash was.
How would you use the CPU and RAM without the kernel?
&gt; How would you use the CPU and RAM without the kernel? Use of CPU and RAM by userspace is fundamentally different to use of disks and network ports (the other two items at that layer in that diagram). Disks and network ports are accessed through kernel-provided APIs. CPU and RAM are just... there. Now, yes, the kernel is involved in providing the CPU and RAM (through scheduling your userspace process and through programming the MMU, respectively), but _use_ of these resources doesn't involve the kernel.
That's because `mail` is waiting for end-of-file (in this case, `grep` exiting and closing its end of the pipe), then it will go ahead and send out whatever it received. `grep` in turn is waiting for EOF on its own input (i.e. `ssh` finishing up). `cat syslog.log` finishes, but `tail -f syslog.log` doesn't, hence the results you see. The proper way to do this is to set up something like `logwatch` on your remote server to send out regular log reports. If there's some reason you can't do that, then you *could* send out one email for each error line, like this: sudo ssh root@REMOTEIP tail -f SYSLOG.log | grep error | while read; do mail "Error Found" admin@myemail.com &lt;&lt;&lt;"$REPLY" done
Umm... you don't need to use a temp variable here. FILENAME="Some text that was input when calling the script" FILENAME=$(echo ${FILENAME// /_}) Or even (inside your script): printf "$(tr ' ' '_' &lt;&lt;&lt; "$@" )"
Would this just be put in a [file.sh](https://file.sh) and ran to work? I just want it to send a email out each time a error lines happens
You know, I must have been doing something wrong when I tried this earlier. You are correct of course, that does work perfectly. Thanks for the help! 
That `$(echo ...)` you are using isn't needed, you can just write: filename=${filename// /_} Something else that's not too important, I've seen people recommend to write variable names in lower-case instead of UPPER-CASE. This is supposed to help with not touching the system's environment variables by mistake. Those are usually all upper-case, so when you use lower-case in your script the names will never clash. That said, I don't know many actual examples of this going wrong for anyone. The only thing I can remember right now is a person using $SECONDS which has a special meaning in bash scripts. Bash will automatically add to $SECONDS as real time passes. That person would have avoided the problems if he would have used $seconds instead.
[Swatch](https://linux.die.net/man/1/swatch) is possibly what you want.
The really nasty one to accidentally overwrite is `IFS`. That plays merry hell with word splitting and `read`, and it's an easy one for novices to use because the name expands to any number of meaningful TLAs.
If you find yourself typing anything (including this) regularly, you should put it in a script.
&gt; That is not a compromise I am willing to make. Then you better learn C and patch BASH. :) 
I can provide a first hand example, but first: I seem to recall there being quite a few threads at stackexchange and/or stackoverflow where people have come across this issue, most often battling with `$PWD` And on the topic of `$PWD`... one day, a colleague asked me to take a look at a script that was broken. A number of colleagues had looked at it and hadn't been able to figure it out. It took me a few minutes of looking at it before I realised... PATH=`pwd` It took me a few more minutes of staring at that line to process that, yes, this really happening. The script's author wanted to store the invoking directory and blissfully thought something along the lines of "I'll store the current path, so PATH makes sense". A completely innocent brainfart with a hilarious outcome.
The OP is an impersonation account by a troll. Joshua "Locke5" Combs of Piqua, OH who has been harassing redneonglow and posting personal information about his family members (including a full-length article about a 3 year old girl) for over a year now. OP Joshua "Locke5" Combs of Piqua, OH, who continues to harass redneonglow on Discord and his family, makes a Reddit account DivineArcade, claims to own a previous social network account of redneonglow, also has similar interests, plus likes to ask for help with bulk downloads of porn, to try and humiliate redneonglow. What do you want, Joshua "Locke5" Combs?
 wget www.foo.com/favicon.ico 
The shell is by far the most convenient way to execute arbitrary programs. In fact, it was pretty much designed to do that and that's, to a relatively large extent, all it does. But plenty of other programs execute programs. When you boot up, the init process kicks off a bunch of stuff. When you log in from a GUI, the login manager runs stuff. When you click desktop icons, your desktop manager runs stuff.
The OP is an IMPERSONATION ACCOUNT by a troll. Joshua "Locke5" Combs of Piqua, OH who has been harassing redneonglow and posting personal information about his family members (including a full-length article about a 3 year old girl) for over a year now. OP Joshua "Locke5" Combs of Piqua, OH, who continues to harass redneonglow on Discord and his family, makes a Reddit account DivineArcade, claims to own a previous social network account of redneonglow, also has similar interests (redneonglow runs 3 Minetest servers, but the Minetest Discord has never heard of "DivineArcade" either), plus likes to ask for help with bulk downloads of porn, to try and humiliate redneonglow (and fails).
Probably because it's not obvious to most people, being buried in the midst of hundreds of files in a typical Git distro package.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/commandline] [\[meta\] STOP DELETING SOLVED QUESTIONS!!!](https://www.reddit.com/r/commandline/comments/afhbr1/meta_stop_deleting_solved_questions/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I've seen it happen quite a bit too, I'm sure some are homework questions and they're trying to get rid of any evidence in case the school/university scan the web for the questions.
[Obligatory](https://xkcd.com/979).
I have literally taken to writing down the username of everyone I take any significant amount of time responding to, so that I can tag them as a timewaster if they do it; it's extremely frustrating. Heck, I'd even consider writing a plugin to help do it. Learning subreddits should include this mention this basic etiquette in their sidebars. And questions should be up for (minimum) a few weeks before being eligible for deletion, because this is \*not free consulting\*. As anthropoid says, it's taking the time to benefit the community, and hiding people's responses (ie. contributions) would ideally be nigh impossible.
Seems like a good opportunity to code something that archives and reposts deleted questions. Everything would be reposted as the bot but it would be respectful to add a reference to the users that answered. No reference for the jerk that deleted the question obviously. I probably won't be able to do it due to having my hands full with other projects but if anyone does please let me know so I don't come back to it when I get free time. Also, bash is probably not the best way to do this.
&gt; I'm sure some are homework questions [meta: stop asking homework questions on reddit]
That's OK - second questioner will be back when their log rotates and (unless they are using BSD,) their script stops working... ( `-f` vs `-F` )
Then someone should repost them all, perhaps with a bot.
&gt; Heck, I'd even consider writing a plugin to help do it. Reddit Enhancement Suite has a user tagging system.
Yes, I use that tagging for exactly this reason, but the part I'd like to automate is remembering the username of posts I respond to, in case they then get deleted shortly after getting some substantive responses. Once a post is deleted, the username is no longer shown to responders, so can't be easily tagged (if not otherwise remembered).
Ah I see. I thought you were writing usernames down separately.
I was (well, cut-n-paste into a doc), but then keeping track of who posted which question can be slightly cumbersome. Most deletions (in my experience) happen shortly after some good initial responses, so the data is only relevant for a short while, but it's still annoying that I even have to do it. I'd estimate about 1 out of 20 or so, of the questions I respond to, get deleted (for subs explicitly about learning programming or other beginner targeted subs). That may even be generous; when it happens frequently it is obviously extremely de-motivating. Luckily the vast majority of posts stay up, often with the poster thanking people for the responses, which happily is even more motivating.
Automod can mirror the OP as a comment. 
IMO, it’s fine to ask those questions as long as they’ve actually tried something and don’t just ask to do their homework for them. It’s fine if the question is like "I need to do X and I tried Y and Z, but they don’t work". But when it’s "How do I do X", I’m not likely to even finish reading the question. Just my 2¢.
/r/cpp_questions has this same problem. A lot of the time the posts that get deleted immediately are also throwaway accounts. I've started looking for post history before helping people now.
I think I will add it to systemd. One more extra thing it does makes no difference. And that way I do not annoy people who use Bash on e.g. Windows.
Unfortunately, that doesn't solve the root problem: Deleted posts are Reddit \_orphans\_, accessible only if you have the original link. No search or view will show deleted posts, so everyone's contributions are as good as gone forever.
Updoot for adding to the sidebar.
Funny you should mention that. I belatedly remembered to comment on exactly that issue...and then I discovered the OP had deleted the post. At this point, they're on their own as far as I'm concerned.
Going forward, I'll prefix all my initial answers with a link to the OP: &gt;/u/anthropoid, I see you're asking about `BASH_FLOBALOB`. Here's what you need to know... That's an easy way to ensure their identities aren't completely scrubbed.
I mean nobody cares if I ask a question about something I'm trying to figure out at work. It's more like, don't copy and paste homework questions without trying to solve them yourself first.
&gt; Heck, I'd even consider writing a plugin to help do it. [Relevant xkcd](https://imgs.xkcd.com/comics/the_general_problem.png)
Yeah, I really don't understand why this occurs.
`tr -dc 'a-z0-9\-_.'`
Only works as long as the input is restricted to ASCII. I suggest this instead: sed 's/[^-_.[:alnum:]]//g' (In theory, `tr` also supports character classes like `[:alnum:]`, but apparently at least GNU coreutils’ `tr` doesn’t support multi-byte characters, so if you use UTF-8 it’s a moot point.)
Instead of a plugin, how about having the automod mirror the contents as a stickied comment? I am not sure if edits can be tracked, but it should be not too difficult to simply copy (I never dealt with automod and this is pure speculation). 
But this is the reddit way. Delete everything at some point.
* reddit cache is ONLY 1000 posts. * reddit search feature is junk; recent change upheld by admin/s. * deleting posts is encouraged and supported by tos.
reddit submissions disappear, from public view, after count = 1000.
design flaw in reddit, which admins uphold. 
&gt; everyone's contributions are as good as gone forever. and admins do not care, either ; reddit's search feature is junk.
I've rewritten the entire program from scratch and its no longer an obfuscated puzzle to figure out. I've commented everything I thought needed an explanation and you should be able to follow how it works now. The program has also been highly optimized and will be even snappier on your system now. :)
&gt;unless you use arrays Well fuck. What kind of automation do you write for these clients? Is it difficult writing documentation in Latin? 
Automation was out of scope for my mission (I just had to write the scripts). It was kind of nightmarish though, from what I heard.
What’s the advantage of writing a file manager in bash?
Most of all It's fun (and I wanted to see if it could be done) What else can I say? ¯\\\_(ツ)_/¯
*New* reddit does support them..... ``` #But #new #reddit #is #crap ``` Stick to the 4 spaces, peeps...
Ohhh! I can finally read it! I'll grab it and add my zip prompt entry to it: ```z) read -rp "Zip name: "; [[ $REPLY ]] &amp;&amp; zip -r "$PWD/$REPLY" "${f[l]}";;``
I am looking at the page and trying to figure out what exactly the dogpoop part is.
It doesn't matter. You can't search shit on reddit.
Here's how it'd work in the new version: # Zip the current dir item. z) cmd_line "zip name: " [[ $cmd_reply ]] &amp;&amp; zip -r "${PWD}/${cmd_reply}" "${list[scroll]}" # full redraw to show the new file. redraw full ;; 
Yep. Only using old reddit. It is also the only website where the fonts look horrible on my Linux in Chromium. All other websites look ok.
Thanks!
No problem! This new way has the benefit of being able to cancel a prompt with the escape key or by pressing enter (when there's no input). :) I'm also thinking about adding definable hotkeys so you can just add: export FFF_KEY1="zip -r input cur_item" I need to do some serious thinking before I work on implementing this but you get the idea.
I skip them - not gonna spend time trying to decipher them, not gonna add myself to 'new' Reddit's user count
For those of us using New Reddit, I just found a way to have our properly-formatted code-blocks and eat triple-backtick cake too. I simply write/paste triple-backtick code, then click on the *Switch to Fancy Pants Editor* link at the bottom of the edit box, then click on the *Switch to Markdown* link to go back to Markdown. Hey presto, my triple-backtick code is automatically converted to the 4-space-indent style. This was in triple-backticks... until I clicked the switch link twice... Now it's an indented block instead. This round-tripping edit also sanitizes my hand-written Markdown in other ways, so I'll be using it by default from now on.
There are other external search engines besides Reddit's own, and *all* of them will fail to index deleted posts. I'd say this *does* matter. You can try this for yourself: Search for the title/content of each of the posts I linked to on your favorite search engine, then search for the title/content of any other Reddit post. You'll get the expected results from the latter, but not the former.
After reading about user / kernel space and system calls, the concept is getting clearer.
Um, you’re wrong? That code you linked looks well formatted to me. Use 4 spaces instead of indent and we should all be happy 
[removed]
Homebrew is amazing if you're coming from a Linux world, want to really unlock more power and flexibility from MacOS, or are looking for advanced/power user options. Installing Bash is just one minor thing it can do. I'd like to offer [this article I found](http://www.nyx.net/~mlu/pages/computing/installing_and_configuring/installing_and_configuring_command-line_utilities/) (seriously, I'm not the author and it saved me from having to write a lot more) that's related to upgrading Bash, installing many of the GNU and similar programs that regularly ship with Linux. It's great for those of us who live in both worlds.
It drives me crazy when, once in a while, I'll click on a link and it suddenly switches to new reddit.
While I typically use Old Reddit + RES, the following are possibly useful `.bashrc` functions! # Functionalise 'command -v' to allow 'if exists [command]' idiom exists() { command -v "${1}" &amp;&gt;/dev/null; } alias iscommand='exists' # Try to enable clipboard functionality # Terse version of https://raw.githubusercontent.com/rettier/c/master/c if iscommand pbcopy; then clipin() { pbcopy; } clipout() { pbpaste; } elif iscommand xclip; then clipin() { xclip -selection c; } clipout() { xclip -selection clipboard -o; } elif iscommand xsel ; then clipin() { xsel --clipboard --input; } clipout() { xsel --clipboard --output; } else clipin() { printf '%s\n' "No clipboard capability found" &gt;&amp;2; } clipout() { printf '%s\n' "No clipboard capability found" &gt;&amp;2; } fi # Indent code by four spaces, useful for posting in markdown codecat() { indent 4 "${1}"; } # Function to indent text by n spaces (default: 2 spaces) indent() { local identWidth identWidth="${1:-2}" identWidth=$(eval "printf '%.0s ' {1..${identWidth}}") sed "s/^/${identWidth}/" "${2:-/dev/stdin}" } As an example `▓▒░$ type clipin | codecat | clipin` And here we go, Ctrl+V: clipin is a function clipin () { xclip -selection c } 
I guess it depends on the situation, if there are no other places under `path` that you'd expect to find `q.txt` `a.txt` or `b.txt` you could let `find` do the work: find path \( -name q.txt -o -name a.txt -o -name b.txt \) -exec python {} \; but that would get cumbersome if there are lots of names or if there are some in the tree that you do not want to use, or if there are just a lot of files and directories that you don't want to have to search. 
Put quotes around the process substitution. Also it's a uuoc.
Thanks, why does this work? also how should I be using cat?
As /u/beatle42 rightly pointed out; if you're working with files - find is your friend here. You can likely make a much tidier find than you will by triple nesting stuff.. Especially when you come back to look at it in 6 months time whilst trying to work out what it used to do.
hey take a look at \[strtfime\]([https://linux.die.net/man/3/strftime](https://linux.die.net/man/3/strftime)).
If your pathname elements are all one character as in your example, you can use globbing (pathname expansion): for P in path/[579]_something/1-[135]/[qab].txt; do python "$P" done For arbitrary-length elements, you can use brace expansion (if the braceexpand shell option is active): for P in path/{5,7,9}_something/1-(1,3,5}/{q,a,b}.txt; do python "$P" done 
[fzf](https://github.com/junegunn/fzf) maybe?
You need the quotes because otherwise your shell will perform "word splitting" on the output of jq, which basically means that every whitespace-separated word in the output will expand to a separate argument. You don't need to use cat because jq can read a file. Instead of `cat foo | jq whatever`, use `jq whatever foo`.
That makes sense, thanks
`cat` is for con`cat`enating files. Yes, it dumps them to `stdout` in a conveniently pipable fashion, but in general it should be used for merging files or text things that have a file descriptor, like `stdout` from different commands, like this: `cat &lt;( netctl list | awk '/*/ {print $2}' ) &lt;(dig +short myip.opendns.com @resolver1.opendns.com) | tr '\n' ' '`. In my example, I'm taking the output of `netctl | awk ...` and the output of `dig...` and combining them, then replacing the newline with a space using `tr`. `cat`, as far as I'm aware, is the only/best way to do this. With your particular example `jq` takes a file name after the search pattern, so you could do this `jq -r '.date.formatt' config.json` As for the quotes... I'm not entirely sure. `shellcheck` would yell at you about quoting to prevent word splitting or globbing, which I'm guessing has something to do with it, but I'll definitely yield to superior knowledge here.
This is reasonable in the case here where it's all paths, but sometimes it's just extra arbitary arguments I want to give to whatever is being ultimately called. Still, I appreciate the answer in this case.
Thanks this looks to be the best solution. I always forget that you can loop over the expansion and that it's not just useful for generating big argument lists. Cheers.
yw. Actually, generating a big argument list is precisely what it does. What happens is that any pattern(s) gets expanded into separate arguments first, *then* the shell enters the `for` loop to iterate through those arguments.
You could say "fahdkrnejejdbrntk" but it probably wouldn't mean much.
`shift` away "start", then loop over the remaining arguments case $1 in start) shift # if no more arguments, add all nodes if (( $# == 0 )); then set -- puppet master fi for node; do case $node in puppet) ... ;; master) ... ;; esac done ;; esac
So I think I need to clarify: right now my script used $2 to receive the input that it needs to select which option to launch. Is there a way to make is like $2\* so that it will accept any argument including the second one so I can have more flexibility in what starts for example if I do Command: service ES start master puppet then it reads the the $2 which is master but also reads the puppet argument. 
Probably important to note that the square brackets will only match files that exist on the filesystem, the braces will generate arbitrary strings. If you're trying to create files rather than open existing ones, will be an important distinction.
 find $searchValue -exec du -bs {} \;
There's a tool named `agrep` that allows errors for the search pattern.
Neither `find` nor `du` will expand wildcards to file paths; that is the shell's job, and bash doesn't do it in quoted parameter expansions. bash will do it *unquoted* expansions, because globbing happens after expansion, but the exact behavior is dependent on the value of `IFS`, which makes it somewhat unreliable (especially if the pattern contains whitespace). In the *general* case you described, it would probably work: find $searchValue -exec du -bs {} \; Of course, if you're going to do it this way, there doesn't seem to be any point to using `find` at all (unless you think there might be so many matching files that you exceed `ARG_MAX`); you could just do: du -bs $searchValue Keeping in mind that the same limitations apply, obviously. If you wanted it to work really reliably given arbitrary inputs, you'd have to do like: old_IFS=$IFS IFS= du -bs -- $searchValue IFS=$old_IFS Alternatively, you can use `find`'s `-name` predicate, which brings its own globbing implementation: find -name "$searchValue" -exec du -bs {} \; 
Partially works. Breaks for anything with spaces. I'm open to multiple solutions as space test is part of another function.
Do the search value looking like this to deal with spaces: "${searchValue}"
Solved! Why find? Bad habits. More like habit as it typically gets the job done. Also, being able to -0 to tackle both crazy filenames and the ability to multi-thread with xargs. I'm finding that I'm not going to see a speed benefit which is a bit shocking. Figured I didn't need the -name as I was already providing the full path. Ultimately, the IFS manipulation paired with the unquoted variable gets me to where I need to go.
/u/josephsmith99, you seem to be looking for a Windows solution. I don't do Windows, but I get the impression that even with [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about), there's a limit to how useful `bash` (or any other Linux shell) can be in this scenario. I suspect dealing with Outlook, Acrobat Pro DC *et al* from the command line would be difficult to near-impossible, so you may be forced to look into GUI-based automation software. Anyway, here's a rough sketch of how to do it in Linux for $0: &gt;Emails come in.. For IMAP or POP mailboxes, [getmail](http://pyropus.ca/software/getmail/) &gt;Automatically: system auto-checks every specific time period (maybe every 1-hr?) The `cron` scheduler is standard-issue on every Unix system I've ever encountered. &gt;Automatically: emails are opened, attachments downloaded to specific folder, and email moved to 'processed' or some other folder Configure `getmail` to pipe its output to [munpack](https://linux.die.net/man/1/munpack), then file original message somewhere &gt;Automatically: Acrobat Pro DC loads it's OCR batch process, does it's work, and populates an 'OCR' folder of the newly saved PDFs Not sure what other Acrobat capabilities your company uses, but [pdfsandwich](http://www.tobias-elze.de/pdfsandwich/) should create OCR "sandwich" PDFs quite nicely &gt;Automatically: email(s) generated and sent out with all attachments from that time period (maybe a max of like 20 per email to avoid smtp stopping it?) Every Linux distro has a command-line mail client, and most of them handle adding attachments just fine
Hey man, thanks for this, I think it's a lot more likely that I should just use this, rather than cook up a whole custom solution (like I just did and it got some data wrong). I think I should just grab all the data using this – Can you do several queries into xmllint? Like, example, if I have a path that looks like this: `rap/publications/publication/briks/brik/musa` `rap/publications/publication/briks/brik/musa/media-code` `rap/publications/publication/briks/brik/musa/album` `rap/publications/publication/briks/brik/musa/album/title` `rap/publications/publication/briks/brik/musa/album/label` `rap/publications/publication/briks/brik/musa/album/catalog-number` `rap/publications/publication/briks/brik/musa/album/musa-number` `rap/publications/publication/briks/brik/musa/album/track` `rap/publications/publication/briks/brik/musa/album/track/side` `rap/publications/publication/briks/brik/musa/album/track/cut` `rap/publications/publication/briks/brik/musa/album/track/partcut` `rap/publications/publication/briks/brik/musa/album/track/title` `rap/publications/publication/briks/brik/musa/album/track/work-title` `rap/publications/publication/briks/brik/musa/album/track/record-country` `rap/publications/publication/briks/brik/musa/album/track/associated-country` `rap/publications/publication/briks/brik/musa/album/track/isrc` `rap/publications/publication/briks/brik/musa/album/track/record-year` `rap/publications/publication/briks/brik/musa/album/track/catalog-number` `rap/publications/publication/briks/brik/musa/persons` `rap/publications/publication/briks/brik/musa/persons/person` `rap/publications/publication/briks/brik/musa/persons/person` `rap/publications/publication/briks/brik/musa/persons/person` &amp;#x200B; And I want to grab everything in the /brik node that has /musa/mediacode as 2 and /musa itself contains &lt;musa content-id=somethingsomething dr-production="**true**"&gt; Then could basically do this whole thing in one line. I guess I should make a new post for this in /r/xml or something.
Deleted last comment because this makes sense to me now lol. Thank you this really helps me!
There’s way too much information to decode this bash script. You get used to it. I…I don’t even see the script. All I see is blonde, brunette, red-head. Hey, you uh… want a drink?
Huh pretty [tab] [tab] #cool
As your wish, document has been updated.
I don't really have a practical application. Mostly curious if it's possible using a shell script. General curiosity on how one would generate a hash for a file that contains hash of a file before it even physically exists. But compare that at runtime against itself to stop the run. 
["This... is wrong"](https://masksofmonsters.files.wordpress.com/2014/08/dk_fox.jpg) But damn if I'm not impressed
When I hear \`json\` I reach for \`jq\` . \`python\` also has great tools for parsing \`json\` correctly. No doubt \`ruby\` has too. I always do my best to resist the tempation to parse \`json\` or \`xml\` with line-oriented tools like bash/sed/awk/grep etc.
Yeah the parsing is already done, using jq. I just need to get the logic of getting the loop working. 
Whaaat.... 
Seems like a nice utility, but there's no bash in it, so it's probably better suited for /r/commandline.
Good point. Thanks!
This is all sorts of horrific.
/u/manderso7, since you've obfuscated the URL, I can't actually test my suggestion, but this should be fairly close to what you're looking for: # Get current year-month ym=$(date +%Y-%m) # Iterate over all campaign IDs for current month for id in $(curl ... https://api.sillyapi.com/mycompany/rest/campaigns | jq -r ".data[] | select(.id &gt;= \"$ym\") | .id"); do curl ... https://api.sillyapi.com/mycompany/rest/campaignresults/$id | jq -r .data done
It might be as simple as "we're developing on Python 3.6, so please don't ask us to support anything older than that".
Like Bash needs a tool for obfuscation. It’s barely intelligible on its own.
The Bashfuscator requires _Python_? What kinda shit is this? Come back when it's self hosting...
Ah! I missed that! Sorry!
It needs Python 3.6 because it uses f-strings mainly. They're faster and more readable than using format() or the old '%s' format strings methods. And because I use 100s of those string formatting statements, I wanted to use the option that was easiest to write and read
I understand all of your concerns, but this tool was primarily created to spread awareness of Bash obfuscation, and educate on to how to detect and deobfuscate obfuscated Bash payloads. I am currently working on documentation that describes how each of Bashfuscator's obfuscation modules work, how you can detect them, their side effects, ect. My goal isn't to set the world on fire, rather it is to help it. I know it can and probably will be used for evil, but that's the risk you run when developing open source security tools.
It's be nicer if they didn't show poor style. At a glance, unquoted variable expansion, all caps var names, pwd over $PWD and the deprecated back ticks. I quit reading at that point.
OK you've got my attention... What's wrong with all caps var names? Because they don't add value over camel case, or is there a widely accepted standard convention? Why are back tickd deprecated? I think I get the other objections. 
See the last lines http://wooledge.org/~greybot/meta/varcap http://wooledge.org/~greybot/meta/%60 Back ticks aren't deprecated ... but Posix documentation recommends against using them in new scripts. Which I call deprecated but it's not really. If you want to learn bash or shell scripting, here's a list of tutorials, ranked. https://wiki-dev.bash-hackers.org/scripting/tutoriallist
&gt; ... but I imagine it could also be used by companies to obfuscate their commercially-deployed bash scripts. That wouldn't be very effective. It just evaluates to the original script at runtime and executes it, so you can easily get it by enabling debugging (`bash -x` to start Bash with it enabled, or `set -x` to enable it in an existing shell). I'm sure that there are also other, better ways to get it; a much quicker method I also discovered for longer scripts without all the garbage output and having to run the original, unobfuscated script was prefixing it with `echo`, then replacing until the command substitution in the resulting script with another `echo`.
I think it's necessary, terrifying work. Security will remain the greatest threat to open-source philosophy. This kind of development is a crucial exercise, however painful it might be in the short-term. It's also just begging for punchlines; don't take it personally.
Variables should start lowercase to avoid confusion/complications with environment variables, e.g. `$USER`, `$TERM`. Backticks are hard to read and harder to nest. `look "\`at 'this nonsense'\`" new $(command $(substitution "$(is 'much')" $(easier 'to read')))
I don't recall specifics, but this resource misled me multiple times before I gave up on it. It's not just outdated style, it's a fundamental misunderstanding of the expansions. Here, try [greycat's Bash Reference.](https://mywiki.wooledge.org/BashSheet) Also, the [Bash Pitfalls](https://mywiki.wooledge.org/BashPitfalls) section is an excellent exercise that will burn out some nasty habits. 
It’s very nice 
That logic makes a lot of sense. Thank you. Unfortunately, I'm getting weird errors: `./pl_assoc_2.sh: command substitution: line 15: syntax error near unexpected token \`('` `./pl_assoc_2.sh: command substitution: line 15: \`curl -X GET -H "Authorization: Bearer $var" -H "Cache-Control: no-cache" "``https://api.sillyapi.com/mycompany/rest/campaigns``" | jq -r ".data"[] | select(."cutoffDate" &gt;= \"$ym\") | ".id")'` &amp;#x200B; Here is what I added: `8 # Get current year-month` `9 ym=$(date +%Y-%m)` `10` `11 # Iterate for all campaign IDs in current month` `12 for id in $(curl -X GET -H "Authorization: Bearer $var" -H "Cache-Control: no-cache" "``https://api.sillyapi.com/mycompany/``result/campaigns" | jq -r ".data"[] | select(."cutoffDate" &gt;= \"$ym\") | ".id"); do` `13 curl -X GET -H "Authorization: Bearer $var" -H "Cache-Control: no-cache" "``https://api.``sillyapi``.com/``mycompany``/rest/campaign``results/$id" | jq -r ".data"` `14 done` `15` &amp;#x200B; What did I do wrong? I tried with () around the final curl statement and without them, same error message. The line number the error mentions is the last line in the script. &amp;#x200B;
Btw, to make things every more complicated, backticks actually don't have to be escaped when nested. You just may need to wrap a backticked expression in double quotes to prevent further processing on command's output. Because escaping isn't required, it's super hard to know which backticks match up without mentally parsing the entire command. Hence the benefit of the dollar paren version.
&gt; I always forget that you can loop over the expansion and that it's not just useful for generating big argument lists. It *can* be, though it requires the use of `eval`. In this case, I bewlieve the following should work eval $(find path/{5,7,9}_something/1-(1,3,5}/ -maxdepth 1 -type f -name {q,a,b}.txt | sed -E s/'(.*)$'/'python "\1"; '/) Rather than loop over the individual files, it: * uses `find` + brace expansion to generate a (newline separated) list of files to run, * pipes it to `sed` to add a leading `python "` and a trailing `"; `, effectively producing a list of individual commands to run, and finally * evaluated the commands `eval` Now, whether or not this is prefferable to what /u/McDutchie posted is aother question entirely, and one I honestly dont know the answer to. But, it *can* be done
I rate this as garbage.
&gt; cat, as far as I'm aware, is the only/best way to do this [combine outputs from differrent commands] This isnt necessairly a better way, but I found you can also do this by encompassing everything in `echo $(...)` commands. e.g., echo "$(echo "$(pipedCmdChain1)"; echo "$(pipedCmdChain2)"; echo "$(pipedCmdChain3)"; &lt;...&gt;)" I also fuind `cat` especially seems to work better when echo'd. For example (on my system anyhow) running cat file | command1 | command2 | ... | commandN &gt; outfile doesnt output anything to the file - `outfile` is empty. However, echo "$(cat file | command1 | command2 | ... | commandN)" &gt; outfile works like you would want it to. idk exactly why this is, but if i had to guess id guess that echo adds something analogous to a `wait` call and forces the previous command to fully finish before echo sends it to te next command in the chain (which, for something like "writing a file to disk", may require knowing how much space it needs for a given file when it starts writing that file).
&gt;What did I do wrong? In a nutshell, your quoting is completely b0rken. For instance: "``https://api.sillyapi.com/mycompany/``result/campaigns" What's with the double backticks (\`\`) in all your URLs? They serve no useful purpose. Also: jq -r ".data"[] | select(."cutoffDate" &gt;= \"$ym\") | ".id" Your bizarre quoting here turned a single `jq` filter into...something that makes no sense. I **strongly** suggest you copy-and-paste the commands in my earlier reply, instead of trying to retype it by hand. Also, spend some quality time understanding how bash interprets quoting. ## Further Reading * [Quotes and Escaping | Bash Hackers Wiki](https://wiki.bash-hackers.org/syntax/quoting) * [Quoting | Bash Reference Manual](https://www.gnu.org/software/bash/manual/html_node/Quoting.html)
Interesting. I'll have to experiment with that. So far I haven't had any issues with my example pipeline, but that doesn't mean I won't find a problem with a similar command in the future.
&gt;I need to copy the composer.phar file into my /usr/bin/ folder Why would you need to do that? 
I lol'd at that, thank you XD
Very astute points, I was waiting for someone to bring up Bash's debug mode (\`set -x\`) and the fact that you can replace the \`eval\` statements that wrap an entire obfuscated layer with \`echo\`, making the obfuscated script print out the next obfuscated layer instead of execute. Yes, Bashfuscator's payloads can be deobfuscated with those methods.... mostly. Assuming the operator did not employ more complex obfuscation methods. Try running this through debug mode: [https://pastebin.com/Hs5KmEMZ](https://pastebin.com/Hs5KmEMZ) This is the result of the \`token/special\_char\_only\` obfuscator module, and breaks Bash's debug mode ;). Now, you still can replace \`eval\`s with \`echo\`s and deobfuscate this, but that may not always work. I have something in the works that will break even that deobfuscation method: [https://github.com/Bashfuscator/Bashfuscator/issues/9](https://github.com/Bashfuscator/Bashfuscator/issues/9)
Yes, it is necessary, I'm open-sourcing this research so that defenders and attackers alike are on the same playing field. Otherwise, some motivated intelligent threat actor could develop something like this and defenders would be scrambling to detect/decipher the threat actors Bash commands/scripts. It's kinda scary how little AV/EDR vendors detect Bash obfuscation.... for example a simple Bash script that deploys a usermode rootkit gets around 50/70 hits on VirusTotal, but after applying Bashfuscator to it, 0/70 engines detect it. This kinda scared me, but what really scared me was when I realized simply Base64 encoding the same script also achieved 0/70 detections.... \*facepalm\*
Apparently since php which is what it works in. Is also in that directory it has to be in same directory
I used Homebrew but it still isn't working
the `echo` approach can be useful if you find yourself in a shell that doesnt understand the `&lt;(...)` command. this is the case, for example, in `mock`, which is a fedora tool that generates a mock (chrooted) OS to build/test packages in. Generally though id say using `cat` is probably the better option. As you said, `cat` was literally designed specifically for merging things together, so...
Thanks, I'll try that out. 
I really like it. While I agree on some of the style arguments, it's a command reference, not a style reference people. There are much nicer ways to give feedback without being a dick. My 2 cents for OP, that I didn't already see covered is that instead of this: lines=(`cat "logfile"`) # Read from file We have a nice built-in now: mapfile -t lines &lt; "logfile" Which works much nicer in loops since it handles spaces in the line. for line in "${lines[@]}"; do echo "${line}"; done
Thank you for that, however I think the additional quotes showed up due to a poorly formatted copy/paste into the text box here. I'm focusing on finding the id's of the current month, not downloading it yet, and have the following: `# Get current year-month` `ym=$(date +%Y-%m)` &amp;#x200B; `# Iterate for all campaign IDs in current month` `id=$(curl -X GET -H "Authorization: Bearer $var" -H "Cache-Control: no-cache" "``https://api.sillyapi.com/mycompany/rest/campaigns``" | jq -r ".data[] | select(.id &gt;= \"$ym\" | .id")` your code suggestion was: for id in $(curl ... https://api.sillyapi.com/mycompany/rest/campaigns | jq -r ".data\[\] | select(.id &gt;= \\"$ym\\") | .id"); This is what I get when I run the script: `./pl_assoc_2.sh` `% Total % Received % Xferd Average Speed Time Time Time Current` `Dload Upload Total Spent Left Speed` `135 412 0 412 0 128 170 53 --:--:-- 0:00:02 --:--:-- 131` `jq: error: syntax error, unexpected $end, expecting ';' or ')' (Unix shell quoting issues?) at &lt;top-level&gt;, line 1:` `.data[] | select(.id &gt;= "2019-01" | .id` `jq: 1 compile error` `% Total % Received % Xferd Average Speed Time Time Time Current` `Dload Upload Total Spent Left Speed` `0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0` `curl: (23) Failed writing body (0 != 16384)` Am I hosing the lines again? Thanks for your assistance and patience. &amp;#x200B;
That's Apple's System Integrity Protection. https://support.apple.com/en-us/HT204899 /usr/bin is a bad place to put third-party binaries you've downloaded. 
Look up "Clean Code" and any highly rated programming book with the word "refactoring" in it. We (scripters) need to start thinking like programmers. This video series is worth just getting a one month subscription for. It will change the way you look at code forever. [https://www.oreilly.com/library/view/clean-code/9780134661742/](https://www.oreilly.com/library/view/clean-code/9780134661742/)
Here's the `jq` expression in my original answer: jq -r ".data[] | select(.id &gt;= \"$ym\") | .id" and here's your latest `jq` expression: jq -r ".data[] | select(.id &gt;= "$ym" | .id" Do you see the difference? It's especially clear in `jq`'s own error message: jq: error: syntax error, unexpected $end, expecting ';' or ')' (Unix shell quoting issues?) at &lt;top-level&gt;, line 1: .data[] | select(.id &gt;= "2019-01" | .id &gt;I think the additional quotes showed up due to a poorly formatted copy/paste into the text box here. It's one (bizarre) thing for additional backticks and quotes to be injected, quite another for a critical parenthesis to go missing. If you actually copy-and-pasted my original code into your script, and it still somehow got b0rked to hell like that, you have a serious system problem that needs to be addressed first, and I can't help with that.
maybe that's my issue. I'll look into it. Thanks.
As a beginner and someone trying really hard to learn bash scripting, thank you this its really helpful.
Could you please tell me what's that external search engine that can be used to search reddit?
&gt; I have something in the works that will break even that deobfuscation method: https://github.com/Bashfuscator/Bashfuscator/issues/9 Very interesting and creative, but wouldn't sourcing the script be able to bypass that? Or pasting it directly into the shell and expanding the command substitution, which I now realize would probably be easier than using `echo`.
Patronising and useless reply.
Any public search engine worth using allows you to narrow searches to specific sites. For instance, with Google, this search query shows both instances of my post up top: "stop deleting solved questions" site:reddit.com
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/linux] [du appears to give the wrong total when using --threshold](https://www.reddit.com/r/linux/comments/agwea6/du_appears_to_give_the_wrong_total_when_using/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Oh, yeah, I'm using that too. I thought there's some other specific site for searching. Thanks.
I suspect this is intentional. Whether it's "desired" or not is, of course, a subjective opinion. In [the commit](http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=commitdiff;h=f8afbb424c050578617bc9ddf4ccf69ed2bdb953;hp=40b2770a22828e347c9ff0640f4ee0702319e8e7) where this was introduced, there is no indication that the `--threshold` option was intended to affect the values calculated by `--summarize` or `--total`. It only changes which entries are emitted. But since you're using `--summarize`, you don't see those entries anyway.
Thanks. It's non-intuitive to me, but I guess if the features were added iteratively then maybe it makes sense not to affect one another.
My thought is to store the URLs in an array and then iterate through them: # FYI, these could be listed horizontally, but I think they look nicer stacked vertically. URLS=( 'http://site1.com' 'http://site2.com' 'http://site3.com' 'http://site4.com' ) for url in "${URLS[@]}"; do retVal="$(curl -s "$url")" statusVal=$? # The '$?' variable stores the exit code of the previous command, # which seems to be what what you were trying to do with the `req1` value. if [[ $statusVal -ne 0 ]]; then echo "Something went wrong while trying to curl page:" echo -e "\t '${url}'" echo "Exit code returned: ${statusVal}." echo -e "Attempting next url...\n" continue fi printf "%s\\n" "Your public IP is '$retVal'" break done &amp;#x200B; PS: On the off-chance that your example *wasn't* simply a toy example, there are far better ways to get your public ip than by using curl. e.g. alias getmyip="myip=$(dig +short myip.opendns.com @resolver1.opendns.com)" Running the command `getmyip` would retrieve your public IP and store it in the variable `myip`.
Thanks @[NotMilitaryAI](https://www.reddit.com/user/NotMilitaryAI) I had also thought about the array, thanks to your help I solved a problem! I'll mention you in the comment of this function :)
Haha, no prob
&gt;`alias getmyip="myip=$(dig +short myip.opendns.com @resolver1.opendns.com)"` You actually want to use single quotes for that, i.e.: alias getmyip='myip=$(dig +short myip.opendns.com @resolver1.opendns.com)' otherwise it literally resolves to: alias getmyip="myip=1.2.3.4" where 1.2.3.4 is the static value of the user's IP _at alias definition time_.
Ah, good point. Textbook example of a minor, innocuous errors that could take days to debug....
Yep I see it now. Thanks for your assistance. I've been looking at this through vim and not noticing my syntax mistakes. I'll try using another platform for now to see if it will help me see my mistakes easier.
What makes this method far better?
Well, the original approach didn't work; OP was evaluating the return-value rather than the exit code. &amp;#x200B; As for keeping the URLs in an array rather than individual variables, it simplifies the code; you can test each URL by simply iterating through the loop rather than writing a new unique `elif` statement to test each one.
A general comment first: you should fix your indenting. With this chaos it's hard to see where conditional or loop blocks begin and end. If two 'case' clauses have exactly the same statements, like in yours, you can combine them like so: case $node in puppet | master ) set_puppet_variables check_ES_reqs restart ;; esac 
For one thing, you're taking a lot of stuff that needs to be done regardless of case and repeating it in every case. This: case $node in puppet) set_puppet_variables check_ES_reqs restart ;; master) set_master_variables check_ES_reqs restart ;; esac Is the same as this: case $node in puppet) set_puppet_variables ;; master) set_master_variables ;; esac check_ES_reqs restart 
Also, it looks like you never restart without doing whatever check_ES_reqs does. Why isn't `check_ES_reqs` part of your `restart` function?
Don't you mean case $node in puppet | master ) set_"$node"_variables check_ES_reqs restart ;; esac since either `set_puppet_variables` *or* `set_master_variables` wants to be called depending on the (convenient) value held in `node`
Why are you guys still wrapping that in a case statement when you took out the need for branching different cases? `case $node in; all | possible | values ) do_something ;; esac` can be replaced with `do_something` by itself. 
&gt; `cat`, as far as I'm aware, is the only/best way to do this. You could just run the commands sequentially, no? { netctl list | awk '/*/ {print $2}'; dig +short myip.opendns.com @resolver1.opendns.com; } | tr '\n' ' '
&gt; Why are you guys still wrapping that in a case statement after you took out the need for branching different cases? Possibly because OP said `There is more options in this case statement but they are all formatted like the one below`... and we don't know just how alike 'like' is.... 
Good point.
The conditionals example can be expressed much more succinctly: if [[ -z "$string" ]]; then echo "String is empty" elif [[ -n "$string" ]]; then echo "String is not empty" fi can be expressed as: [[ -z "$string" ]] &amp;&amp; echo "String is empty" || echo "String is not empty"
/u/where_Am_I77, others have already covered the stuff I would have, so I'll share instead the general stuff that is _far_ more important than a thousand bash tricks: 1. **Avoid the premature optimization trap** that many a newbie falls into. When you're still struggling with bash syntax and idioms, don't be afraid to write out your logic in "longform" and *debug it that way*. Refactoring and other niceties can come in later; doing it too early can result in, say, all your functions doing a restart (even "stop"), or a refactored function that `case`s the same values you check at the global level, just to do something for _puppet_ but not _master_. 2. **Fix your indenting.** Bash isn't as sensitive as Python to tabs and spaces, but your *brain* is. Sloppy indentation of code blocks only ends up tricking yourself. 3. **Version your scripts.** Snapshot your progress at every stage. I use [Git](https://git-scm.com/) myself, but there have been Holy Wars fought over version control systems (VCS), so [you do you](https://www.urbandictionary.com/define.php?term=You%20do%20You). Pick one VCS you can live with, and use it religiously. If nothing else, snapshot your debugged "longform" script before trying to shorten it, so you have a stable base to go back to when you (inevitably) screw up `my_start_script_1.1`. 
/u/where_Am_I77, others have already covered the stuff I would have, so I'll share instead the general stuff that is *far* more important than a thousand bash tricks: 1. **Avoid the premature optimization trap** that many a newbie falls into. When you're still struggling with bash syntax and idioms, don't be afraid to write out your logic in "longform" and *debug it that way*. Refactoring and other niceties can come in later; doing it too early can result in, say, all your functions doing a restart (even "stop"), or a refactored function that `case`s the same values you check at the global level, just to do something for *puppet* but not *master*. 2. **Fix your indenting.** Bash isn't as sensitive as Python to tabs and spaces, but your *brain* is. Sloppy indentation of code blocks only ends up tricking yourself. 3. **Version your scripts.** Snapshot your progress at every stage. I use [Git](https://git-scm.com/) myself, but there have been Holy Wars fought over version control systems (VCS), so [you do you](https://www.urbandictionary.com/define.php?term=You%20do%20You). Pick one VCS you can live with, and use it religiously. If nothing else, snapshot your debugged "longform" script before trying to shorten it, so you have a stable base to go back to when you (inevitably) screw up `my_start_script_1.1`.
&gt; "${domains_with_dash=[@]}" Why is there an equal sign in there? $ echo ${domains_with_dash=[@]} -d example.com $ echo ${domains_with_dash[@]} -d example.com -d www.example.com $ 
This is not true in general, but you are right in this case because the specific conditions being tested make the `elif` basically translate to an `else`, and only one command is being executed in each case. 
That worked! Thank you very much, it works perfectly now. I technically only use the first element of the array further down (for the Common Name). So you're right I can declare a variable that stores the first element of the array and leave the array with \["-d www.example.com", "-d [example.com](https://example.com)"\] stuff. &amp;#x200B; Thank again for your assistance, I knew this would be the right place to come to.
/u/s3viour, I don't see a function anywhere, much less an attempt to pass an array into it. Nevertheless, this is probably your problem: #At this stage the domains_with_dash array var should be populated with ["-d example.com","-d www.example.com"] If `certbot-auto` is typical of most command-line utilities, what it *really* should look like is `["-d","example.com","-d","www.example.com"]`, i.e. your`domains_with_dash` array should contain *four* elements, not two. That's achieved by fixing your assignment as follows: domains_with_dash+=(-d "$domain") And as /u/MurkyCola mentioned, `${domains_with_dash=[@]}` is probably not what you intended.
You'll be looking at summat like declare -a domains=("example.com" "www.example.com") declare -a domains_with_dash=() for arrindex in "${!domains[@]}" do domains_with_dash[arrindex]="-d ${domains[arrindex]}" done echo ----------------------------- echo here is your original array.... ${domains[*]} echo ----------------------------- echo Here is your new array.... ${domains_with_dash[*]} 
Never tried it, but I'm guessing it would work. I'll have to experiment.
It can be expanded as an `elif` as such: [[ -z "$string" ]] &amp;&amp; echo "String is empty" || [[ -n "$string" ]] &amp;&amp; echo "String is not empty"
Add the script and rebuild..?
i added th script to the image, and i also rebuilt it. But whatever i try i cant make docker run it. When i run it manually it just works, but even if i add it to docker as a CMD it doesnt run.
Please show the command you're using to start it and the error message that it produces. And as a suggestion for the future, also paste the Dockerfile so it's easier to see what's going on without clicking links to see the basics. 
Thanks for the reply. Well the thing is it doesnt produce errors, it works fine. But i am modifying the [start.sh](https://start.sh) file, this is being ran on boot of the container. In this [start.sh](https://start.sh) i want it to run /etc/openvpn/updatePort.sh &amp;#x200B; i tried adding this behing a bash execute command like this (line 171): `exec /bin/bash /etc/qbittorrent/start.sh &amp; /etc/openvpn/updatePort.sh` but the [updatePort.sh](https://updatePort.sh) just isnt being ran. &amp;#x200B; [startscript](https://github.com/MarkusMcNugen/docker-qBittorrentvpn/blob/master/openvpn/start.sh)
Seems like a good idea, it reminds me of a more thorough version of Quantum Espresso's [check_failure](https://gitlab.com/QEF/q-e/blob/develop/environment_variables#L83). I'm curious why do you use `eval` to [execute the command](https://github.com/deanplus/bash-repl/blob/master/exec.sh#L25), and since you redirect the output, how does it handle commands that already redirected output?
I'm on my phone so I'm not able to check it properly, but if you're just using one &amp; you're just executing the first command and forking it to the background, you need two &amp;&amp; if you want to run the second script as well. 
The usage of eval helps to avoid problem with piping just like what you described. *using eval* ```sh $ eval "echo 5 &gt; a" &gt; b $ cat a 5 $ cat b ``` *using test* ```sh $ test "echo 5 &gt; a &gt; b" $ cat a $ cat b 5 ``` I think the correct behavior is with eval. You first run the desired command. Then "steal" the output from stdout and stderr. 
set -e
set -e exit on **none zero** exit codes. you see the problem? On some places other exit codes should be allowed. On others you don't even care about the exit code. Most of the time you don't care about the output of some commands (e.g. `apt-get update`). You just want to know it was executed and successfully. This is a different type of REPL. The difference is on the print. You decide which commands are important to you (can cause exit, their output should be visible and more). Hope you see the difference.
Nifty idea! Way more slick than `set -x` output. It does have some fundamental restrictions that stop it from being a drop-in tool though. For example, this working command gives several errors: awk -F, '{ if(++n[$1] == 2) { print("Duplicate instance of " $1); } }' file.csv While this one gives no errors but does the wrong thing: sed -i '2i\ # Copyright FooBar, 2019 \ # Licensed under the terms of ... \ ' file.sh 
There is a [known bug](https://github.com/deanplus/bash-repl-plus/blob/master/README.md#known-bugs) section. Added your post to it. I'll try to sort out the issues this weekend. Thanks. 
`file.csv`: hello,world `file.sh`: #!/bin/sh echo "Hello World" 
I think you need to use [ENTRYPOINT](https://medium.freecodecamp.org/docker-entrypoint-cmd-dockerfile-best-practices-abc591c30e21)
It is incorrect. It may be trying to assign the string ~/catkin_ws to the variable CATKIN_WS, but the $ at the beginning is invalid. When you use a $ in front of a variable, that means "express the value of the variable". To assign the string, drop the $. If you think it is trying to assign the result of the execution of ~/catkin_ws to CATKIN_WS, then still drop the $ and use backticks as you suggested.
Have a look to arpasmr/fulmini repo on github: you'll see an example of bash running another bash and simulating a cron. [https://github.com/ARPASMR/fulmini/tree/cartopy](https://github.com/ARPASMR/fulmini/tree/cartopy) (it's on branch 'cartopy') Any suggestion welocomed!
Thank you all for your help. I really appreciate it and I promise to post a properly indented script because I can see now how important that is. Also I am embarrassed to say that I do not have a VSC. I just downloaded Git and will play around with it. Also your suggestions have really moved me forward. This is where my script has progressed to Thank you case "$1" in start) if [ "$2" == "" ]; then set -- master puppet fi for node; do case $node in puppet | master) set_"$node"_variables check_ES_reqs start ;; esac done ;; stop) if [ "$2" == "" ]; then set -- master puppet fi for node; do case $node in puppet | master) set_"$node"_variables check_ES_reqs stop ;; esac done ;; restart) if [ "$2" == "" ]; then set -- master puppet fi for node; do case $node in puppet | master ) set_"$node"_variables check_ES_reqs restart ;; esac done ;; status) if [ "$2" == "" ]; then set -- master puppet fi for node; do case $node in puppet | master) set_"$node"_variables check_ES_reqs rh_status ;; esac done ;; esac exit $? 
try doing this at the end of your startup\_desktop: &amp;#x200B; `home="$(echo $HOME)"` `exec "${home}/scripts/move_desktop"` &amp;#x200B; This could also have the same effect: `home="$(echo $HOME)"` `bash "${home}/scripts/move_desktop"`
Thanks - I'll give this a try in the morning.
... unless it's an attempt at indirection, in which case it needs `declare`: $ CATKIN\_WS=some\_other\_variable $ declare $CATKIN\_WS='\~/catkin\_ws' $ echo $some\_other\_variable \~/catkin\_ws 
The 2nd option didn't work; haven't had a chance to test the 1st one yet.
/u/ajgringo619, if by: &gt;when run from startup, the last program gets skipped you mean: &gt;all programs get sent to their respective desktops...except LibreOffice then I'm fairly sure the problem is this: LibreOffice pops up a *startup splash screen* by default that's *also* titled "LibreOffice", and your script is catching *that* window and sending it to a different desktop. When the LibreOffice program has finished loading all its components, the splash screen will disappear, and then the main LibreOffice window appears on your current desktop as per normal. (The reason you see this issue on startup but not when run interactively: LibreOffice loads slower on startup because none of its components have been cached in memory yet. When you test it interactively, you've already started LibreOffice up at least once, so it's likely to have been cached partially or in full, hence the splash screen goes away much sooner.) Try giving LibreOffice a `--nologo` option to suppress the splash screen, something like this: /usr/bin/flatpak run --branch=stable --arch=x86_64 --command=/app/libreoffice/program/soffice --file-forwarding org.libreoffice.LibreOffice --nologo &amp; Also, it looks like you're assigning applications to desktops that happen to correspond to their `window` array subscripts. If that's your intent, you can just use the subscript and ditch the `t` variable: #!/bin/bash # This script moves my preferred programs to their proper desktops # Define program windows that we want to move window=( Chromium Evolution Console Virtual HomeBank LibreOffice ) # Move programs for i in "${!window[@]}" do # Wait till we get at least one matching window until [ -n "$(wmctrl -l | grep "${window[$i]}" 2&gt;&amp;1)" ] do sleep 1 done # Send it to the appropriate desktop wmctrl -r "${window[$i]}" -t $i done
I haven't done any serious scripting in a while, but I'm pretty sure `rm -f /tmp/$rand` is a bad idea. I think you can do something like `rand=$(mktemp)` and then do `rm -f $rand` and that way you won't have to worry that if somehow $rand doesn't get set you're wiping out the /tmp dir. 
Success - thanks a bunch! I first tried my script with the --no-logo switch for LibreOffice and the script actually finished...but instead of LibreOffice being in desktop #5, it switched places with Chromium (supposed to be on desktop #1). 
Admittedly idk if this actually would help in this specific situation, but in general i think the "recommended" way of doing stuff like this is through `systemd` (mint uses `systemd`, right?). Im pretty sure the following would work for this case to replicate your startup script using systemd: # need to be root su # setup service [[ -d /etc/systemd/personal ]] || mkdir /etc/systemd/personal cat &lt;&lt; EOF &gt; /etc/systemd/personal/move-windows-on-logon.service [Unit] Description=calls window moving script on login after the graphical environment has loaded After=multi-user-target,graphical.target [Service] Type=oneshot RemainAfterExit=yes User=&lt;USERNAME&gt; ExecStart=/home/USERNAME/scripts/move_desktop.sh [Install] WantedBy=graphical.target EOF ln -s /etc/systemd/personal/move-windows-on-logon.service /etc/systemd/systyem/move-windows-on-logon.service # enable service systemctl daemon-reload systemctl enable move-windows-on-logon.service 
Thank you for the fun project idea! Something I wish my work Macbook did when I hook it up at work, yet never bothered to look into.
True. Better random naming will be given.
Can someone eli5, I dabble a little in bash scripting for work and we were taught to always “chmod -m 777”.
This article is as close to ELI5 as it's going to get, you should read it carefully: https://www.maketecheasier.com/file-permissions-what-does-chmod-777-means/ 777 gives you, the group, *and* everyone else, read, write and execute permission. Giving write permission to the world is usually a bad idea. Default to 755 instead, or if you need to give write permission to a group, 775.
Awesome, thank you!
Well, you could move it into the command: export TEST_ENV_HASH="$(echo "${BITBUCKET_REPO_SLUG}${BITBUCKET_PR_ID}" | shasum | head -8)" But there’s no way to do the various kinds of parameter expansion, such as `${parameter:offset:length}`, without introducing a named variable, as far as I know.
Hmm... that didn't work for me.. /opt/app # echo "${ONE}${TWO}" | sha1sum | head -8 40829e35281695d8b16c050052c14f1a94eb7944 - I could solve it with awk tho: /opt/app # echo "${ONE}${TWO}" | sha1sum | awk '{print substr($1,0,8)}' 40829e35 
/u/galaktos's original command was slightly off: export TEST_ENV_HASH="$(echo "${BITBUCKET_REPO_SLUG}${BITBUCKET_PR_ID}" | shasum | head -c 8)"
Ah, thanks! Which one would you say is the preferred solution and why? Cheers!
I've played around with systemd a little bit; maybe I'll change it later. Thanks again for all of your help.
sorry, yeah, I swear I had `-c8` in my mind but then somehow dropped the `c` when writing the comment \^\^
It depends on whether you have a need for the full SHA1 hash as well. I usually do, so I generally extract the substring in bash as a separate operation, but if you don’t need the full value, the single pipeline above is good enough. 
It depends on whether you have a need for the full SHA1 hash as well. I usually do, so I generally extract the substring in bash as a separate operation, but if you don’t need the full value, the single pipeline above is good enough. 
Sorry, I was unclear. I meant which one-line solution was preferred. awk or head And no, I don't have a need for the full hash. :)
I don't think either would be preferred. `awk` is a heavier duty program, on my machine the executable is 647k, compared to `head`, which is 43k. So you could make an argument for using the smallest tool for the job would be `head`. For my two cents I think this would be a better job for `cut` (also 43k on my machine). `cut -b -8` is equivalent to `head -c 8` in this case when the input is only one line long.
I did a little bash script to solve your problem #!/bin/bash FILENAME='file' # Name of the file with the columns declare -a YELLOWINFO # Array to keep the info I=0 # Variable to append to the array while read p # Read line by line do INFO=$( echo "$p" | cut -d' ' -f1 ) # Get the first column COLOR=$( echo "$p" | cut -d' ' -f4 ) # Get the last column if [[ $COLOR == 'yellow' ]] # If the color is yellow then YELLOWINFO[$I]="$INFO" # Append to the array ((I++)) # Increment the variable i fi done &lt; $FILENAME echo ${YELLOWINFO[@]} # Print the whole array You only have to modify the file name and if your file has a different delimiter than a space, you must modify the cut -d' ' -f1 and in the *d* flag between the single quotes change it to whatever your delimiter is
I know this is bash, but the right tool for that would be *awk*.
You don’t need the `I` variable to keep track of the index, by the way – you can append also to the end of the array like this: YELLOWINFO+=("$INFO")
I agree, so here’s the AWK program to print the first column for all lines where the fourth column is “yellow”: awk '$4 == "yellow" { print $1 }' Depending on what exactly you want to do with this information, you could, for example, pipe it into some `while read -r info; do …; done` loop.
i did it like this, storing the indexes of the columns on interest from the header, so it would cope if the order or extra columns appear later... but this would be much better done with awk. `#!/bin/bash` `COL1=` `COL2=` `while read -a ROW; do` `if [ -z "$COL1" ]; then` `I=0` `for VALUE in ${ROW[@]}; do` `test $VALUE = Col1 &amp;&amp; COL1="$I"` `test $VALUE = Col4 &amp;&amp; COL2="$I"` `let ++I` `done` `[ -z "$COL1" -o -z "$COL2" ] &amp;&amp; {` `echo "can't find columns in header"` `exit 1` `}` `fi` `test "${ROW[COL2]}" = yellow &amp;&amp; {` `echo "do something with ${ROW[COL1]}"` `}` `done &lt; input.txt` &amp;#x200B; Then testing it... `neil@tvpc:~/rr$ ./test.sh` `do something with 2301` `do something with 5632` `neil@tvpc:~/rr$` &amp;#x200B;
I don't really know *awk,* but I figured it out how to do it, and it's a lot easier awk '{if($4 == "yellow") print $1}' file and that one-liner, does what I did (except for the array). &amp;#x200B; Thanks for your suggestion 
The indenting is still really inconsistent here, and it's like the logic is turned inside-out to *add* replication. This should do the same thing in 15 lines: nodes="$2" if [[ -z "$2" ]]; then nodes="master puppet" fi for node in nodes; do set_"$node"_variables check_ES_reqs case "$1" in start) start ;; stop) stop ;; restart) restart ;; status) rh_status ;; esac fi 
I totally forgot about that, thanks for the remainder, it actually makes the code more readable the way you say
I went through and commented up a version of your script the way I would approach it. I think the main difference is that I focus on isolating pieces of code that get reused. For example, if we need to update the default node names, we only need to change a single line in `node_names_from_option`. In the nested-case-statement version, we would have 4 duplicate lines to change (which makes it more likely we'll forget one, resulting in bugs.) Hope this helps. # I like to wrap my main entry point in a function. # I find this makes it easier swap out pieces of functionality # if something needs to be debugged. Also, I like seeing the # important action at the top of the file. main() { local thing_to_do=$(thing_to_do_from_option "$1") local node_names=$(node_names_from_option "$2") # Ensure arguments were passed, and exit with an error otherwise. if [[ -z "$thing_to_do" ]]; then # The "1&gt;&amp;2" at the end redirects the message to # the error output stream, which is a best practice for errors. echo 'The first argument should be a thing to do' 1&gt;&amp;2 exit 1 elif [[ -z "$node_names" ]] echo 'The first argument should be a thing to do' 1&gt;&amp;2 exit 1 fi do_thing_to_all_nodes "$thing_to_do" "$node_names" } thing_to_do_from_option() { # I like naming my positional arguments, # even if it's used very soon afterwards. local option="$1" local thing_to_do case "$option" in start) thing_to_do='start' ;; stop) thing_to_do='stop' ;; restart) thing_to_do='restart' ;; status) thing_to_do='rh_status' ;; esac # This returns the value. echo "$thing_to_do" } node_names_from_option() { local option="$1" # Check for a non-empty string if [[ -n "$option" ]]; then echo "$option" return fi # Otherwise, use defaults: echo "master puppet" } do_thing_to_all_nodes() { local thing_to_do="$1" # Note that we have to remove the "thing_to_do" argument # or else the loop will treat an argument like "stop" as a node name. shift local nodes="$@" for node in $nodes; do # Let's put in some debugging messages for this loop. echo "Doing '$thing_to_do' to node '$node'" do_thing_to_node "$1" "$2" done } do_thing_to_node() { local thing_to_do="$1" local node="$2" if [[ ! "$node" =~ puppet|master ]]; then # This prevents us from operating on something other than puppet or master return 1 fi set_"$node"_variables check_ES_reqs $thing_to_do } # Note that we still need to invoke main with the script's arguments. main $@ # Happy exit will happen automatically if there wasn't an error exit earlier 
I'm a little confused how sourcing the script could bypass the layer dependency method. Each layer is wrapped in a subshell, so that wouldn't do anything. Of course, you could remove the subshell, and then deobfuscate the script layer by layer, but then you would open yourself up to malicious aliases or functions that could persist in the parent shell environment. &amp;#x200B; The second point wouldn't work I believe, as just expanding the command substitution would throw an error, as Bash wouldn't know what to do with the output. You'd have to either print the results of the expanded cmd substitution, or \`eval\` it. And then you run into the same problem as before. 
Interesting take on "how I should simplify this to reduce the amount of lines of code" 
Nice, thanks! I'll go with cut (mostly from a readability perspective).
&gt; I think the main difference is that I focus on isolating pieces of code that get reused. You're both overlooking that the reuse here only comes from putting the same `for` in every `case`. [Just put the `case` within the `for` and the whole thing collapses into a few lines with no re-use.](https://old.reddit.com/r/bash/comments/ah1qc6/simplifying_a_bash_script_made_of_repeat_functions/eegm0xv/) The input validation is nice, but that doesn't justify separate functions either. You only need to validate the script's arguments once, and validating the functions' arguments is a side effect of needlessly making a new function for every builtin you use. 
So true... I didn't understand Bash very well before writing Bashfuscator... to the uninitiated, it looks like random symbols sometimes, though not as bad as Perl in that respect
I don't know about bash, but your window manager might have some settings for that. Or there's a Python package to do that, https://pypi.org/project/keyboard/.
I'd use `head` for _clarity_. I want only the first 8 characters of `sha1sum`'s output, not the first 8 characters of _every record_ that `awk` (and /u/mTesseracted's preferred `cut`) would produce. `awk` and `cut` only _incidentally_ produce the same result in this case; I'd rather be more exact in my code.
This is probably best handled by your desktop environment/window manager (as /u/Erelde mentioned).
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
I'd look into [evemu](https://www.freedesktop.org/wiki/Evemu/) from freedesktop.org. This is a set of cross-desktop tools to record and/or play back X events from your choice of input device. If you're on Debian/Ubuntu, simply install the `evemu-tools` package and run: $ evemu-record /dev/input/by-id/&lt;your_kbd_device&gt; then start tapping on your keyboard. The output is copiously commented, so you should have no trouble figuring out what to `grep` for.
&gt; But there’s no way to do the various kinds of parameter expansion, such as `${parameter:offset:length}`, without introducing a named variable, as far as I know. Well, you could use `$_`, which is set to the last argument, to avoid polluting your environment with random variables like so: : "$(shasum &lt;&lt;&lt; "${BITBUCKET_REPO_SLUG}${BITBUCKET_PR_ID}")" export TEST_ENV_HASH=${_::8} 
/u/DivineArcade, I don't see any way to tell `qiv` directly, so I suspect you'll have to move its window *after* launch with `wmctrl`: qiv -p green.png # Wait for window with "green.png" in its title to appear until wmctrl -l | grep -Fq green.png; do sleep 1 done # Now move it to (10,10) wmctrl -r green.png -e 0,10,10,-1,-1 ## Further Reading * [`wmctrl` man page](https://linux.die.net/man/1/wmctrl)
&gt; Of course, you could remove the subshell, and then deobfuscate the script layer by layer, but then you would open yourself up to malicious aliases or functions that could persist in the parent shell environment. There are plenty of malicious things something within a subshell can do; you simply shouldn't run a random script in an environment which you wouldn't want it to mess up.
Can you show an example of what you want to be able to do? I mean, show what you want to be able to type at the command line and what you want bash to then print in the terminal as an answer.
sure: ` $ temp.cpu` ` &gt; 56.61 °C` 
sure: ` $ temp.cpu` ` &gt; 56.61 °C` 
A backup script seems to be everyone's first real script. Good job. Once I got tired of making my own, I found Borgbackup and Borgmatic. Haven't looked back. 
Your issue is straightforward. The command is evaluated and then the results are passed the as the value to be substituted for your alias. In order to address this, use a different alias. `cat /sys/class/thermal/thermal_zone0/temp` would have the results you want. As for the next step, look at the BC utility.
Yes, been here, done this. Use borg backup. It will condense this down to a single command, and likely work better too.
I would perhaps do that like this in .bashrc: temp.cpu() { sed -r 's/(..).$/.\1 °C/' &lt; /sys/class/thermal/thermal_zone0/temp } This command seems to work for me here. My experiments at the command line looked like this: $ cat /sys/class/thermal/thermal_zone0/temp 27800 $ sed -r 's/(..).$/.\1 °C/' &lt; /sys/class/thermal/thermal_zone0/temp 27.80 °C It's also possible to do that text editing with bash itself, without an external tool like 'sed'. Doing that would look like this: temp.cpu() { local temp=$(&lt; /sys/class/thermal/thermal_zone0/temp) echo "${temp%???}.${temp:(-3):1} °C" } About how I developed this, first here's an example variable to experiment: $ x=123456789 $ echo $x 123456789 Printing everything except the last three characters is done like this: $ echo ${x%???} 123456 And printing just the last three characters like this: $ echo ${x:(-3)} 789 Combining things together looks like this: $ echo "${x%???}.${x:(-3)} °C" 123456.789 °C You can also shorten that number after the period: $ echo "${x%???}.${x:(-3):1} °C" 123456.7 °C Then when I translated that to a function, I added that "local" keyword to the variable so that it will only exist inside the function and will not be added to your command line environment when you run it. 
I'd rather not have to fiddle around to make sure I have the same version of the client and server and stuff like that. Backups _need_ to be resilient to failure and can't be so fickle as to break if you have the wrong version of software. I've been using this script for a while (with minor tweaks), and it's never let me down. That being said, note that much of the script is meant to deal with stuff I want to do before and after backing stuff up, which means that even if I were to use borg or whatever, I'd still have to manually create the LUKS header backups, specify which directories to backup, and cleanup afterwards. The only lines that would be affected (as far as I can tell) would be the actual `rsync` stuff, which is already pretty short...
Oh, this is by no means my first real script haha - just thought I'd put it out there to see if people had thoughts and suggestions for improvements. I'm currently working on a Haskell version with real options and whatnot (just for the heck of it).
This alias has a terrible mistake that's really easy to make. The temperature printed by it will be set the moment that bash starts up and reads the .bashrc file. The alias will then later always print that old temperature, it will not print the current temperature. To fix this, the quotes have to be changed from `"` to `'`, or the alias could be changed into a function instead. When using `'` quotes, bash will not interpret the text inside the alias when it sets the alias. That `$(&lt; ...)` code that reads the file will then be part of the text in in the alias and you will get the current temperature each time it is used.
&gt; alias temp.cpu="echo $( &lt; /sys/class/thermal/thermal_zone0/temp) / 1000 | bc" You're making one of the same mistakes as OP. That's going to evaluate the substitution only once, when you create the alias. $ alias foo1="echo $(date)" $ alias foo2='echo $(date)' $ type foo1 foo1 is aliased to `echo Sat Jan 19 19:03:03 PST 2019' $ type foo2 foo2 is aliased to `echo $(date)' $ foo1; foo2 Sat Jan 19 19:03:03 PST 2019 Sat Jan 19 19:05:22 PST 2019 $
I think I'd ultimately do it like this, personally: temp.cpu() { printf "%.1f °C\n" $(bc -l &lt;&lt;&lt; "$(&lt; /sys/class/thermal/thermal_zone0/temp) / 1000") } That text editing earlier is just a bit too weird. This here works with numbers and simply does a division by 1000, and then the 'printf' command is easy to customize to how many digits you want after the decimal point and it also does rounding.
With your current structure, you could replace your 'cat locations.txt' command with this: # check if there are arguments if (( $# &gt; 0 )); then # print the arguments, one per line printf "%s\n" "$@" else # there were no arguments, so print your original file cat /boot/scripts/backup_locations.txt fi You could stuff this into the spot where you currently just have 'cat'. Behind the 'fi' word at the end, you would put the `|` (it has to be on the same line) and then your existing `while` loop would start. Something else, check out a tool named "shellcheck". Your distro probably has a package for it. You can also try it online at www.shellcheck.net without having to install it. The tool will help you avoid easy to make mistakes in bash scripts. It will tell you for example where to add `"` quotes to make it so your script won't break if there are space characters in filenames.
this is perfect, except instead of done &lt; input.txt I need done &lt; SomeLinuxCommand
ifs gets reset back to normal when your script is done in the system anyways, So it's fine if you forget it. It will goof you up if you need it to be something else later on in the script and forget to change / unset it 
You should read more into how bash executes scripts and how environment variables work / scope of variables in general.
It's a bit ironic that someone who claims `set -euo pipefail` enables some sort of strict mode (it doesn't) to force you to write "good code", would also recommend removing space from IFS. The only reason to do that globally, is to make broken code break a little less often. For example, broken code like for f in $FILES; do ... done where someone has crammed filenames into a single string value, and relying on IFS to split it back into filenames. Even if we assume that none of the filenames contain newlines or tabs, wordsplitting is not the only thing that happens to that string. There's also pathname expansion, which will go through all of the words resulting from the wordsplitting, and if they contain glob characters (like `*`, `?`, `[...]`) it will attempt to replace those words with matching filenames. So, it doesn't really help in any way with making "robust scripts", more the opposite. And you definitely should **not** export it.
/u/chiraagnataraj, looks like a fairly straightforward script. Just a quick recommendation though: Take a look at [rsnapshot](https://rsnapshot.org) if you haven't already. It's a venerable backup system that handles pretty much everything your current script does, and is significantly more configurable to boot (logging, version retention, include/exclude patterns, etc.). Anything that rsnapshot doesn't support directly (e.g. LUKS metadata backup) can be encapsulated in one or more scripts that just need to generate the content you want to backup; `rsnapshot` will take care of the rest, and your backup process will be less buggy than if you try to DIY the entire process. TL;DR: System backup is one area where a roll-your-own-solution is generally *not* advised. That said, everything from here on is generic advice, not specific to your script: 1. `rsync_args` should be an array. Building command invocations by concatenating strings is actually an *anti-pattern*: it fails miserably if you suddenly need to deal with spaces in pathnames, and should be avoided unless you have a good reason otherwise: 2. Create a temp dir to consolidate all the stuff to backup, instead of "polluting" your home dir. This way, you can also set a `trap` to auto-cleanup *everything* on exit, so you don't have to do it manually, or accidentally leave a new temp backup file behind, or accidentally delete something important from your home dir: 3. `for each in $(seq ${#vfiles[*]})` is bizarre and completely unnecessary. Iterate over each array's contents directly, like this: 4. System-level backup scripts should *always* be run as root, and any copies should *always* preserve metadata. That way, when you need to restore your files, you don't end up with incorrect ownerships/permissions, and no one who gains online access to your backups can trivially read your encryption keys, shadow passwords, and other secret stuff. In other words: 5. There should be no need to `sudo` anything in your script, as the entire script should be run as root. 6. Every `cp` should be `cp -a`. 7. `chown chiraag:chiraag &lt;LUKS_root_metadata&gt;` is seriously misguided. Don't do that. And a final (optional) tip that may save you headaches down the line: 8. Switch your `UPPERCASE` variable names to `lowercase`. The vast majority of bash variables with special meaning are all UPPERCASE, so this helps avoid accidental clashes and subsequent weird bugs.
Thanks for validating my habit of storing flags in arrays. What are some special lowercase Bash variables?
I forgot to mention that I’m not in any desktop environment, I’m in the terminal. 
I use something like this almost everywhere. foobar(){ # assuming file has one mount per line Mount=( "$@" ) # accept arbitrary number of arguments [[ -t 0 ]] || # if stdin is not terminal it's a pipe while IFS= read line; do Mount+=( "$line" ) done for m in "${Mount[@]}"; do # sanitize and rsync done }
 foobar(){ # assuming file has one mount per line Mount=( "$@" ) # accept arbitrary number of arguments [[ -t 0 ]] || # if stdin is not terminal it's a pipe while IFS= read line; do Mount+=( "$line" ) done for m in "${Mount[@]}"; do # sanitize and rsync done } $ foobar host{A..Z} &lt; /path/backup_locations.txt
Thanks for posting, will try them out when I get some more photos!
There are only two I've seen in all these years. Both are obscure, and not used by any `bash`ers I know: `auto_resume` fine-tunes job control, while `histchars` finesses command history.
Very true. But what really does help script robustness is IFS= # a.k.a. IFS=''; no separator characters = no split at all set -f # a.k.a. set -o noglob ...that is, disabling both split and glob completely. Most scripts rarely or never need them, and they end up causing a lot more harm than good in practice because, in real life, people forget to quote variables all the time. For the specific cases where split or glob are needed, they are easy to enable for individual commands in a subshell, e.g. cp *.txt dir becomes (set +f; cp *.txt dir/) which also has the added advantage that you can now read in the code exactly where glob happens. For 'for' loops that need split, instead of for i in $(seq 1 10); do echo $i done you can do IFS=$'\n'; for i in $(seq 1 10); do IFS= echo $i done which may be "ugly" but has the advantage of visibly specifying in the code *exactly* where and how split happens. Plus you only split on the characters you need, not others. Another advantage is that this technique avoids unexpected pitfalls with split and glob being active at the same time. E.g. if 'seq' were another command that outputs lines containing `*` or `?` or some other glob character, you'd get into trouble the normal way, but this way avoids trouble. Of course this technique is incompatible with much existing code; it would need to be adapted. But it certainly avoids a lot of pitfalls. 
done &lt; &lt;( \# some command\[s\] echo Col1 Col2 Col3 Col4 echo 2301 NY prd yellow echo 3499 KY dev blue ) &amp;#x200B;
Apologies, I was a little *too* narrow in my description. `evemu-record` taps into the *Linux input subsystem*, so it's not X-specific. (If you're running a BSD flavor or some other Unix variant, you're on your own.) I tested it by plugging a Logitech wireless keyboard/trackpad combo into my headless server (no X running), then ran `evemu-record` on the server in a separate `ssh` session, then started tapping and mousing on the Logitech. It captured all the keyboard/trackpad events, no problem. The only trick is to identify the input device path you need. I recommend you first look under `/dev/input/by-id`, where you'll find fairly descriptive pathnames like `usb-Apple_Inc._Apple_Keyboard-event-kbd`. If that doesn't work, look at the contents of `/proc/bus/input/devices` to identify the appropriate device. For example, my wired Apple Keyboard appears as follows in `/proc/bus/input/devices`: I: Bus=0003 Vendor=05ac Product=024f Version=0111 N: Name="Apple Inc. Apple Keyboard" P: Phys=usb-0000:00:14.0-3.4.2.2/input0 S: Sysfs=/devices/pci0000:00/0000:00:14.0/usb3/3-3/3-3.4/3-3.4.2/3-3.4.2.2/3-3.4.2.2:1.0/0003:05AC:024F.000A/input/input25 U: Uniq= H: Handlers=sysrq kbd event9 leds B: PROP=0 B: EV=120013 B: KEY=10000 0 0 0 1007b00001007 ff9f207ac14057ff ffbeffdfffefffff fffffffffffffffe B: MSC=10 B: LED=1f I then look at the `H: Handlers` line to find the event device name (`event9`), then the device path to record from is simply `/dev/input/event9`.
Sounds exactly like what I’m looking for. Thank you. Any tips on how to trigger a bash script once I get evemu to work? 
also maybe have a look at [triggerhappy](https://github.com/wertarbyte/triggerhappy)
&gt; alias temp.cpu="$( &lt; /sys/class/thermal/thermal_zone0/temp ) " Well, first of all those are double quotes, which expand anything starting with `$` in place. IOW, the above becomes: alias temp.cpu='56605 ' When defining aliases, always use single quotes instead, which don't expand anything; you want expansion to be at execution time, not at definition time. This also makes your second problem obvious: your alias doesn't actually have a command in it, so of course the result of the expansion becomes the command. Hence your 'command not found' error. You need a command like 'echo' there. Third, the space at the end of the alias definition means "expand any further aliases after this one", which is not what you want here. So the immediate fix is: alias temp.cpu='echo $( &lt; /sys/class/thermal/thermal_zone0/temp )' But then you might as well do: alias temp.cpu='cat /sys/class/thermal/thermal_zone0/temp' But of course everyone else is right that a shell function is a better idea anyway; aliases have their place, but only if they do something a shell function couldn't do better. With shell functions, you avoid those quoting gotchas, for one thing. temp.cpu() { cat /sys/class/thermal/thermal_zone0/temp } 
That's kinda Bash Scripting 101. Not very efficient, but it gets the job done, and you can add a `grep` to pre-filter the torrent of events: ``` evemu-record &lt;kbd_dev_path&gt; | while read ev; do case "$ev" in *" 0001 0002 0000"*) # Catch "1" on key release /usr/local/bin/my_key_1_catcher ;; *" 0001 0026 0002"*) # Catch "L" on auto-repeat /usr/local/bin/my_key_L_rapidfire ;; ... esac done ``` Alternatively, look at u/shobble's suggested [triggerhappy](https://github.com/wertarbyte/triggerhappy) for what looks to be a potentially friendlier solution, if you want something that's an always-on service, for instance.
**UTOE** (Useless Test Of Existence) nitpick: shopt -s nullglob for i in $COPY_DIR/*.CR2 do [[ -e ${i} ]] || continue Since you've enabled `nullglob`, your `for` loop won't run at all if no files match the glob expression, so your test is effectively: true || continue A slightly less pointless test: [[ -s "$i" ]] || continue skips over *empty* files, which may trigger noisy errors in subsequent steps. That *may* happen if you somehow run out of space on the SD card, but your camera merrily continues to let you take shots. Also note that I quoted the path variable for safety.
could do it like this... here i open a file-descriptor 3... by default make it point to your backup\_locations.txt - but if supplying something at command line, point fd3 to the text of the arguments, the just call your processing function, asking it to iterate over fd3... #!/bin/bash read_commands() { while read LINE; do echo "processing $LINE" done } # default: open fd3 on your normal list of locations exec 3&lt; backup_locations.txt test $# != 0 &amp;&amp; { # however, if providing a location as argument, reset # fd3 so it's the entire text of the arguments exec 3&lt; &lt;(for DIR in $@; do echo "$DIR"; done) } # now call your main function, reading from fd3 read_commands &lt;&amp;3 then to test it... (default case) neil@tvpc:~/bin$ ./test.sh processing /neil/pictures processing /neil/porno processing /neil/warez &amp;#x200B; (special case: 1 directory) neil@tvpc:~/bin$ ./test.sh /neil/mp3 processing /neil/mp3 neil@tvpc:~/bin$ &amp;#x200B; (special case: multiple directories) neil@tvpc:~/bin$ ./test.sh /neil/movies /neil/work processing /neil/movies processing /neil/work neil@tvpc:~/bin$ &amp;#x200B; &amp;#x200B;
Very nice! I tried to make it a tad more readable to be able to understand it (with some help from [`shfmt`](https://github.com/mvdan/sh)): let C=COLUMNS, L=LINES N=() S=$(seq 0 $((C * L - 1))) for I in $S; do N[I]=$((RANDOM &amp; 1)); done while M=(${N[@]}) printf %c ${M[@]} | tr 01 \ @ do for I in $S; do T= for Y in -1 0 1; do for X in -1 0 1; do ((T += X | Y &amp;&amp; M[(I + X + C) % C + (I / C + Y) % L * C], N[I] = T == 3 || M[I] &amp;&amp; T == 2)); done; done done sleep .2 done
neat ! i wrote this one a few months ago : #!/bin/bash declare -A g t for((;i++&lt;$3;x=RANDOM%$1,y=RANDOM%$2,g[$x,$y]=1)){ :;};printf \\e[2J for((;;)){ for((i=0;i&lt;$1;i++)){ for((j=0;j&lt;$2;j++)){((c=g[$i,$j]?-1:0)) for((a=i-2;++a&lt;i+2;)){ for((b=j-2;++b&lt;j+2;g[$a,$b]&amp;&amp;c++)){ :;};} ((c==2))||((t[$i,$j]=c==3?1:0));((t[$i,$j]))&amp;&amp;p+=O||p+=.;};p+=\\n;} printf \\e[H$p;p=;for i in ${!t[@]};{ g[$i]=${t[$i]};};} (you have to pass it 3 arguments, width, height and number of cells) not as small as yours sadly
 neil@tvpc:~/bin$ unset FOO neil@tvpc:~/bin$ eval export FOO=$(echo SOMETHING | sha1sum | head -c 8) neil@tvpc:~/bin$ export | grep FOO declare -x FOO="c63b8336" &amp;#x200B;
Thanks! Welp, I seem to have combined two patterns from [here](https://mywiki.wooledge.org/ParsingLs) 😂 I just want to note that initially I was using the exact anti-pattern listed there, and I remembered that I probably shouldn't be doing that and changed it accordingly. I seem to have gone overboard 😂
Thanks for your suggestions! I'll go through them one by one. &gt; Just a quick recommendation though: Take a look at rsnapshot if you haven't already. It's a venerable backup system that handles pretty much everything your current script does, and is significantly more configurable to boot (logging, version retention, include/exclude patterns, etc.). Anything that rsnapshot doesn't support directly (e.g. LUKS metadata backup) can be encapsulated in one or more scripts that just need to generate the content you want to backup; rsnapshot will take care of the rest, and your backup process will be less buggy than if you try to DIY the entire process. I get this kind of comment a lot (see some of the comments below). I've been using this script for quite a few years now (along with minor tweaks) and recovered from several system-level failures (including my laptop battery frying my hard drive along with all other hardware), so respectfully, I think the script works "well enough". &gt; rsync_args should be an array. Building command invocations by concatenating strings is actually an anti-pattern: it fails miserably if you suddenly need to deal with spaces in pathnames, and should be avoided unless you have a good reason otherwise: Interesting! Thanks for the heads-up: I should probably change that. I didn't know it was an anti-pattern. &gt; Create a temp dir to consolidate all the stuff to backup, instead of "polluting" your home dir. This way, you can also set a trap to auto-cleanup everything on exit, so you don't have to do it manually, or accidentally leave a new temp backup file behind, or accidentally delete something important from your home dir: Yeah, I should definitely use `mktemp` and put temporaray files in there. &gt; for each in $(seq ${#vfiles[*]}) is bizarre and completely unnecessary. Iterate over each array's contents directly, like this: Yeah, I'm not sure why I was doing this originally, but as you said, it can be done much simpler given the current iteration of the script. &gt; System-level backup scripts should always be run as root, and any copies should always preserve metadata. That way, when you need to restore your files, you don't end up with incorrect ownerships/permissions, and no one who gains online access to your backups can trivially read your encryption keys, shadow passwords, and other secret stuff. In other words: I agree that system-level backups should be run as root, but most of this is backing up my personal files, not system-level files. Indeed, if you look at the usage of `cp` in the script, it's not used for anything I would automatically restore. That is, everything I'm using `cp` for would need manual intervention anyway (header restore, `dpkg --set-selections`, etc). You're probably right that I shouldn't `chown` my header data. The LVM snapshot image that I create _is_ run with root privileges. &gt; Switch your UPPERCASE variable names to lowercase. The vast majority of bash variables with special meaning are all UPPERCASE, so this helps avoid accidental clashes and subsequent weird bugs. Oh right, that's a good idea, isn't it. Thanks for your detailed input! Some of your suggestions definitely aren't trivial, so there will be some refactoring involved.
Thank-you. The edited solution works great and required minimal changes to the rest of the script.
If you're wondering where the extra variables come from, they come from files like [this](https://github.com/chiraag-nataraj/firejail-profiles/blob/master/private-profiles/firefox.private) or [this](https://github.com/chiraag-nataraj/firejail-profiles/blob/master/private-profiles/chromium.private). Also, given what I've learned recently, I should probably transition the arguments into a Bash array (as /u/anthropoid is likely to point out) 😂
this seems like a terrible idea: ` rm -rf ${PROFILE}`
It only happens if `$RMPROF` is set though... how else would you suggest I clean up the temporary profile? I suppose I could remove the `-f`.
it might eb necessary, but I'd potentially add a fair bit more error checking and consider the various ways it might go wrong, since it wouldn't be too hard to wipe out fairly important stuff as it stands. Maybe look at linting with shellcheck, possiblby under `set -eu` as well, and ensure everything is properly quoted to handle spaces.
Thanks for the shellcheck recommendation, checking it out now.
Just another note that `$RMPROF` is set to `0` at the beginning of the script and there is exactly 1 codepath through which `$RMPROF` is set to 1 (aka if a private profile is requested). That being said, it certainly doesn't hurt to be more cautious regarding sanitizing variables and whatnot.
thanks, i'd never heard of shfmt, but will check it out. i wrote it "as is", as a one line, but checking your formatted version (and I guess having a break of several weeks), I realised i can indeed shrink it further, with a little trick regarding initialisation... this now comes in at 255 bytes all pushed together... let Z=C=COLUMNS,L=LINES;N=();while for I in $(seq 0 $((C*L-1)));do ((T=0,N[I]=$RANDOM&amp;1,Z))||for Y in -1 0 1;do for X in -1 0 1;do ((T+=X|Y&amp;&amp;M[(I+X+C)%C+(I/C+Y)%L*C],N[I]=T==3||M[I]&amp;&amp;T==2));done;done;done;Z=;M=(${N[@]});do printf %c ${M[@]}|tr 01 \ @;done pretty formatted... let Z=C=COLUMNS,L=LINES N=() while for I in $(seq 0 $((C*L-1))); do ((T=0,N[I]=$RANDOM&amp;1,Z)) || for Y in -1 0 1; do for X in -1 0 1; do ((T+=X|Y&amp;&amp;M[(I+X+C)%C+(I/C+Y)%L*C],N[I]=T==3||M[I]&amp;&amp;T==2)) done done done Z= M=(${N[@]}) do printf %c ${M[@]} | tr 01 \ @ done &amp;#x200B; &amp;#x200B; &amp;#x200B;
great stuff! i shall have to study this to work it out. the two dimensional associative array trick i can see
it pretty much uses the naive algorithm for the gave of life, so it's surprisingly not very hacky, just made very compact using all the bash tricks i know of :)
here's a quick first hack... *must* be much more elegant solutions than this! maybe the approach is okay, just needs a better implementation, possibly different language * read in the first line (the legend) * find the number of bytes before "Class" * ignore the 2nd line of output, the line =======... * then for real lines, using sed, strip leading bytes and trailing additional columns * ...giving us just the Class column &amp;#8203; #!/bin/bash IFS= BYTES=0 lshw -short | while read LINE; do ((BYTES)) || { I=0 while test ${#LINE} != 0; do [[ $LINE =~ ^Class ]] &amp;&amp; { BYTES=$I break } LINE=${LINE:1} ((++I)) done ((BYTES)) || { echo "can't find Class in header" &gt;&amp;2 kill 0 } continue } [[ "$LINE" =~ ^= ]] &amp;&amp; continue echo "$LINE" | sed "s/^.\{$BYTES\}\([^ ]*\).*/\1/" done if i then test... here are the first 6 lines of my lshw output, so can see i also have different numbers of columns populated... root@tvpc:~# lshw -short | head -6 H/W path Device Class Description ==================================================== system UN42 (SKU) /0 bus UN42 /0/0 memory 64KiB BIOS /0/1 memory and if i try my (poor) script... root@tvpc:~# ./z system bus memory memory memory memory memory memory . . . I await **FAR MORE** elegant solutions :) &amp;#x200B;
You can directly bind keyboard shortcuts in your shell configuration. bash, zsh. That's basically how all shell shortcuts works (C-r, C-z, etc).
i been scripting (mainly sh / ksh) for 20+ years and only now i learn this for x in list; { something; } ?!? yikes, this is great stuff! thanks
if there isn't like 5 ways to write something, it isn't bash :)
blown away to learn this new form of for... that's taken 13 bytes off it! i don't see this documented in my man page - both forms of for are listed requiring "do" if i'm reading it properly, but hey, it works... let Z=C=COLUMNS,L=LINES;N=();for((;;)){ for I in $(seq 0 $((C*L-1)));{ ((T=0,N[I]=$RANDOM&amp;1,Z))||for Y in -1 0 1;{ for X in -1 0 1;{ ((T+=X|Y&amp;&amp;M[(I+X+C)%C+(I/C+Y)%L*C],N[I]=T==3||M[I]&amp;&amp;T==2));};};};Z=;M=(${N[@]});printf %c ${M[@]}|tr 01 \ @;} &amp;#x200B;
i'd guess that it's a consequence of the C-style for loop...
This seems like a problem that could be more easily solved by poking around in `/sys` directly. That's where `lshw` gets its data.
https://devhints.io/bash
Start here http://tldp.org/LDP/Bash-Beginners-Guide/html/Bash-Beginners-Guide.html
Switch to lower case variables, best practice is to use lower case for your own variables and upper case for env variables, that way you won't hit some strange scenario where it take the wrong variable 
Yup, took care of that. Thanks for the suggestion.
`man bash` is the authoritative list of all the things available in bash. It starts with options, definitions etc. TLDP.org hosts quite a few demonstrative examples of what could be done. &gt;what would be the best practices to get a feed back at the end saying that all packages installed well? really enclose each packages install lines in if statements? maybe save the output of each? In bash each process *can* have an exit code. Usually installers (like apt-get) return 0 on success. Any non-zero value indicates a failure. So, you could have your program like this: apt-get install &lt;thing1&gt; &amp;&amp; echo "Installed thing1"; This will print the the "Installed thing1" only if `apt-get` returns `0`. This is a simple way of using conditionals. If the installation fails, the echo statement will not execute. Also, most package managers/installers can accept multiple arguments to be installed at once. apt-get install &lt;thing1&gt; &lt;thing2&gt; ... &lt;thing1000&gt; ... It will install all the things listed. However, a quirk of apt is that if even one of the package names are misspelled (or not available in the repos) it fails and doesn't do anything. &gt;and what if it's needed to reboot at the end of an installation mid way script...? split the script in two files? mayve it comes back magically where it rebooted?+with a special scripting? Do not count on rebooting the system. It is very unwise to think the script could start upon reboot. It can be done, but there are a lot of things that could go wrong. It's best if you manage reboot manually. Usually, linux systems can be updated without needing a reboot. 
thanks for your reply, I'll try to use the return 0 of each separated apt get installs.. maybe build a string so I can display it at the end of the script. you seam to know a lot about the subject. 😉 I'm curious if i should look deeper at using bash files to create a web interface to edit configuration files.. like a really basic router firmware, kind of thing. any framework or advise on how to do this in a nice way? 
No. "bash" and "web" should not mix. You can serve simple static html files, but even that is not a good idea. Use a proper web-backend. If you don't know any, you may start with python and flask. If you need to edit files remotely, use ssh, sftp, scp etc., to connect remotely. They are available in most IDEs and text editors, sometimes with a plugin. 
Thank you!
Thank you!
You can use `NF` to count the Number of Fields. That might help a bit. If NF == 1, print $1 might help for those cases where you want to list the device
u/RandomlyAdam, it seems you're using "column" as a synonym for "field", instead of the common visually-oriented definition. I assume what you really mean is "I need to output the values in the **Class** column" (which is *always* column 3). In that case, and assuming you're using GNU awk rather than a generic brand, this works nicely: $ cat output_class.gawk NR==1 { # Parse lshw header match($0, /^(.*)(Class +).*$/, cols) # Switch to fixed-width columns, combining pre-Class columns into one FIELDWIDTHS = length(cols[1]) " " length(cols[2]) } NR &gt; 2 { print $2 } $ lshw -short | gawk -f output_class.gawk system bus processor processor ... But since this is r/bash: $ cat output_class.sh #!/bin/bash # Parse lshw header while read line; do if [ -z "$class_start" ]; then # HEADER PARSING PHASE if [[ $line =~ ^(.*)(Class *).*$ ]]; then # Get the appropriate offsets class_start=${#BASH_REMATCH[1]} class_len=${#BASH_REMATCH[2]} fi else # Skip header divider [[ $line == *=* ]] &amp;&amp; break fi done # Now tell cut to, well, cut it out cut -c $((class_start + 1))-$((class_start + class_len)) $ lshw -short | ./output_class.sh system bus processor processor ...
Far easier: `sudo lshw -json | grep '"class"' | tr -d '^ ' | tr -d ',$' | sed 's/\"class\"://g`
holy smokes. this right here is what i was looking for. You missed the single quote at the end of your sed statement. ;)
I'll take a look at flask looks cool.. For editing files I'd really like to rather try to do it on the browser. maybe not editing directly the original file in a giant textbox... lol using IDE is easy and probably the fastest/savest, I agree but I'm over thinking/trying to simply the deployment of my hardware solution that needs very small configuration.. the tree end files are in json/toml.. an ultra basic login/service status/ configurator &lt;which eventually change the setting and reboot services&gt; would be so useful still flask is the best route ? 
Think of what needs to be done to achieve that: * flask will help you host a web-interface/service * program to translate the json/toml into a html form * a program to read the changed options and write/update them on the file None of the above is beyond the limits of flask/python. It can be done, but do you have the time to learn, implement, troubleshoot and maintain it? If yes, go ahead. There are libraries to read and write json trees. Add some bootstrap and you have a presentable looking web interface. If you are into programming, this is a good exercise. 
ah!, the killer question! I have absolutely no time to implement, troubleshoot and maintain. not now. but now that it's clear what I need to do, I can put it on the wish list! :) thanks for your points, really appreciated. 
If this is for personal use, you can skip the maintain part and call it done when you achieve your current goal. It takes a few hours to do this when you are well versed with flask. A day or two if you are learning as you go along. However, using a remote editing plugin on your favorite editor will be easiest. I understand not wanting to edit json by hand. 
it's for a commercial product that already works, but trying to make it easier to use. \*mostly for me, and eventual customers that will want to deploy it on their own. &amp;#x200B; right now customers still need me a lot in the installation anyway, so it's okay for me to do command line configurations using vim. &amp;#x200B; but I'm thinking ahead. and it will be easier to maintain in the long run when I have multiple instances installed. :)
See also the answers to [this post](https://www.reddit.com/r/bash/comments/9ah3lg/recommend_resources_for_a_beginner/).
Maybe not for beginners, but it covers interesting points... - Learning the bash shell, Cameron &amp; Bill, O'Reilly.
Oh! That makes sense. What’s the point of Triggerhappy and the other programs suggested then?
Those are for system wide keybindings.
You can't do this with grep. If you only have those lines you show in your file, you can do this with awk: $ awk 'int($NF) &gt; 100' testfile http://sg.ovies.m/.ss/ (Status: 200) [Size: 128] http://sg.mo.v/.dev/ (Status: 200) [Size: 12328] http://som.b/.hal/ (Status: 200) [Size: 1238] If you also want to filter out other lines that might be in the text, you can add that like this: awk '/\[Size: [0-9]+\]/ &amp;&amp; int($NF) &gt; 100' If you are interested in what's happening with that `int($NF)`: `NF` is a variable that tells you the amount of fields that awk found on the line. The `$` makes awk read a field. Writing `$NF` then reads the last field on your line. The `int()` function is needed here before you can start comparing because the field has that `]` text character in it as well, not just the digits. This makes awk treat the value as text and the comparison with a number will not work right. The `int()` function turns text into a number, and it can deal with that extra `]` text character fine.
How about this? ${p/'$HOME'/$HOME}
Sorry, I wasn't clear enough. I wan't full variable expansion, as If I had written the substituted command. Your solution is specific to one variable. What if I wanted to write `$GO_HOME/src/github.com/...`
Oh, good point! Thanks for that.
Just to be a complete dick to /u/ropid and their lovely answer, you *can* do this with GNU grep, but only because it supports Perl Compatible Regular Expressions (PCRE). You also probably shouldn't. `grep -P 'Size: [1-9][0-9]{2,}'` This matches any 3-or-higher-digit numbers that don't start with a zero.
thanks that worked.but what if i want to print out only lines where Size: is between 1 and 100 .how isit done ? 
Awk: awk '/\[Size: [0-9]+\]/ &amp;&amp; int($NF) &gt;= 1 &amp;&amp; int($NF) &lt;= 100' Perl: perl -ne 'print if /\[Size: (\d+)\]/ and $1 &gt;= 1 and $1 &lt;= 100'
This definitely isnt the best way, but heres another option using `sed`: eval $(echo "$a" | sed -E s/'^([^\(]*\(Status\: ([0-9]*)\) \[Size\: ([0-9]*).*)$'/'\(\( \2 &gt; \3 \)\) \&amp;\&amp; echo '"'"'\1'"'"';'/)
I like this site, though usually only for more advanced stuff https://wiki.bash-hackers.org/scripting/tutoriallist
I know I'm just making this harder and I'm sorry `grep -P ' 1?\d{1,2}.$'`
GNU Grep: `grep -P ' \d{1,2}.$'`
GNU Grep - range from 1 to 100: grep -P ' (\d\d?|100).$' Not including 1 and 100: grep -P ' ([2-9]|\d\d).$'
It's called Indirection $ foo=henry; bar=foo; echo ${!bar} henry
The simplest solution is also the most dangerous: eval git -C $p pull `eval` is definitely not recommended in general, but it can be the most sensible solution in controlled environments. In particular, it'll substitute *shell* variables like `BASH_VERSION`, which external commands like `envsubst` simply can't touch. (It wasn't clear what you meant by "all bash variables", so I'm mentioning this for completeness.) Speaking of which, I'm surprised you got `$(envsubst ${p})` to work for you, because `envsubst` takes its substitution input from *stdin*. I assume you meant something like `$(envsubst &lt;&lt;&lt;"$p")` instead.
I recommend against the Bash Beginners Guide. It is outdated, doesn't teach good practices, and in some places it is just plain wrong. This is also why it is not listed in the sidebar. Read http://mywiki.wooledge.org/BashGuide instead.
Yes. I didn't check it properly. The process substitution only works from second line. (Which is more wired) I moved `envsubst` outside and piped it between sed and while loop. Which works exactly the way I want it to. But you are right eval might be better in this case. I thought of using eval but dismissed it immediately for security concerns. But the config file will only be modified by me. So it might be ok to use here. On mobile sorry for forgetting.
Use global variables. `export GMAIL_ID=gjjdasndo11238` `export GMAIL_SECRET=gjjdasndo11238` And refer to those instead.
That's the way to go, every user got their own credentials. 
For doing this, the user, except for me, must know the app's client secret, which they won't.
So, there's no way for me to give my app its own OAuth identity?
nope. You are not the only person with this problem. Look up open source twitter clients for example, they all go this route.
This script looks quite nice. I would suggest 2 things (ignoring the original question, which i answered earlier) - Use shellcheck - have the username be configurable
Thanks for the suggestions. Regarding the username, it's already configurable. Also, thanks for informing me about shellcheck, I didn't know about it.
Ah, that's bad. Hopefully, I'll find/create some other method in the near future when I'll be more knowledged about the web, because I know I'm not okay with this.
shellcheck is awesome, i learned so much, just by using it and reading the wiki pages corresponding to its findings! So, let's talk about line 54: 'https://www.googleapis.com/gmail/v1/users/utkarsh.naveen@gmail.com/messages?q=is:unread' \ You missed using $clientId here :) 
Ah, I found a typo. I should have used `$email` (sourced from $configFile) instead of my own email. Also, I don't need to provide client ID since the `$accessToken` variable takes care of it.
I have fixed the typo, also could you please hide my email address with \`$email\`?
sure, no problem :)
Noted, thanks!
Thanks!
Thank you!
Thank you very much!
Use a configuration file; don't track _your_ configuration file with the project, but provide an example. Could be something as simple as a file like: oath_key=&lt;enter your oath key from google&gt; that you include in the top of your script with: source myapp.conf
OP here: This is basically [https://www.reddit.com/r/bash/comments/aebhov/open\_a\_new\_gitlab\_merge\_request\_from\_the\_command/](https://www.reddit.com/r/bash/comments/aebhov/open_a_new_gitlab_merge_request_from_the_command/) distributed as a generalized Bash script (which now supports other OSes than Mac OS X) with a proper license (MIT).
Line 36: `if [[ $(ping -c 1 8.8.8.8 | grep -o "Unreachable") = "Unreachable" ]]; then` ... is a bit overwrought and clumsy, using word matching to sniff after only one of the many possible responses when a ping fails. Especially when you compare to how this should work: `if ! ping -c 1 8.8.8.8; then` ... since `ping` only returns a successful exit code (0) when it receives a reply. And since you don't do anything when `netstatus == "down"`, if you did something like this: `if ! ping -c 1 8.8.8.8; then exit 3; fi` or a shorter idiom that works the same: `ping -c 1 8.8.8.8 || exit 3` ... then you don't need to wrap your whole main functionality in `if [[ "${netStatus}" = "up" ]]; then ... ; fi`
can't test it just now, but from teh look of it this is only going to work when using remotes over http(s), rather than ssh, maybe?
Yeah, looks the same to me. Maybe it's something about the terminal program you are using? I vaguely remember I had problems in I think gnome-terminal.
Same issue in gnome-terminal and urxvt... And all my Ubuntu ssh terminals inside and outside tmux. Seems like it should be an easy fix but no dice so far.
Just curious, what happens if you do `IFS='' applyACL.sh or whatever the script name was`
also have you tried setting IFS right before you call setfacl?
Please post your existing code! Check out 'formatting help' for how to use code blocks.
absolutely no need for eval in what you're doing from looks of things, nor do i see the need to read in lines from your list of paths into an array does... mkdir "a b c" setfacl -m u:$USER:rwx "a b c" work? if so, clearly there's no issue at all with your setfacl accepting a single argument containing spaces - and the problem is that you're passing paths containing spaces as entirely separate arguments, as below, which just means you are not quoting your path in the call to setfacl. $ setfacl -m u:neil:rwx a b c setfacl: b: No such file or directory setfacl: c: No such file or directory firstly, which "setfacl" are you calling? ("file setfacl") to confirm you're calling eg /bin/setfacl and not some nonsense wrapper script thing that somebody has clobbered your path with? secondly, you could try setting set -x at the top of the script, so you can see the flow and what it's actually invoking and from where, as there are multiple calls to setfacl in the script - and then maybe post some examples of the output where it doesn't work &amp;#x200B; essentially, though - you're trying to something akin to below? although xargs here doesn't add quotes around arguments with --verbose enabled, this all works fine here $ mkdir -p "a/b c/d e f/g h i/j k" $ find . -type d -print0 | xargs -0 -n 1 --verbose setfacl -m u:neil:rwx setfacl -m u:neil:rwx . setfacl -m u:neil:rwx ./a setfacl -m u:neil:rwx ./a/b c setfacl -m u:neil:rwx ./a/b c/d e f setfacl -m u:neil:rwx ./a/b c/d e f/g h i setfacl -m u:neil:rwx ./a/b c/d e f/g h i/j k &amp;#x200B;
works for me on GNOME Terminal 3.22.2 Using VTE version 0.46.1 +GNUTLS