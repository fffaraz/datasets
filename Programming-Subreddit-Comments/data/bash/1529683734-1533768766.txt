Just FYI, you want "#!/bin/bash" on the first line.
Nice, good job. Welcome to bash.
Thanks everyone for all the useful tips. The more i get exposure the better, again thanks and cheers!
Pretty cool. Thanks. 
I am not sure about the difference between curl (which I am using) and wget other than the recursion functionality. It may be that wget --user-agent="" is what causes that, since I tried my script and added more curl requests at the same time with no errors. However, I still need to take a look at the code in order to have a better understanding of the error. 
Thank you I really appreciate your time for looking at my code, and giving me a note. I will fix it now.
There's also https://www.imdb.com/interfaces/ that imdb makes available.
Your script really intrigues me, and looks like it could be something really useful to me. If you don't mind, I copied the script and am going to play around with the code a bit to see if I can streamline it. When (or if, lol) I finish, I'll send you back the modified code.
NO THAT'S FORBIDDEN. Just kidding, for sure it makes me happy. Fork it and do whatever you like with it. I will be happier when you send your modifications back. Wish you can see the smile on my face right now :). Again thank you very much. 
I saw that but I had some limitations with it. Mostly was the idea of downloading all these files will take more time. Is there a way around? like to grep from all files with out downloading? In fact, I would like to go that route since it seems more stable than relying on the html format.
Very cool! Thanks for sharing.
It happened some time ago. It doesn't seem to happen now: I have just run the script 6 times in a row and there was no server error.
Actually, you really want `#!/usr/bin/env bash`. CC u/StickyTwinkie . ~ # /bin/bash tcsh: no such file or directory: /bin/bash ~ # which bash /usr/local/bin/bash
You should check your script in https://shellcheck.net (I see some `[` that shouldn't be used with bash, a broken shebang, full caps variables...)
In the interest of portability, yes, that is what you want. Just make sure that whatever you use, it goes on the first line.
Thank you very much, I appreciate your simple way of explaining the problem. I guess everyday you learn something new, and today I learned more than one thing, you're awesome dude. I will fix it now.
Thanks! Actually the second one is what I thought I was looking for. Unfortunately it seems like the problem was caused by something else. Here's the specific line of code (it's sed input in the final script, and the only "echo/write" statement): `echo "libname ${libref_lookup_arr[0]} meta library=\"${libref_lookup_arr[4]}\""` This produces: `libname AT_3 meta library="/home/SASINFO/3_Dev/SchemaManagement/AT/Data"` `libname AT_VA meta library="LIBNAME AT_VA SASIOLA TAG=AT_VA PORT=10000 HOST="`[`sasva.someurl.edu`](https://sasva.someurl.edu)`" SIGNER="`[`https://sasweb.someurl.edu:666/SAS"ASTAuthorization"`](https://sasweb.someurl.edu:666/SAS"ASTAuthorization") `;` `libname AT_Auto meta library="/home/SASINFO/3_Dev/SchemaManagement/AT_Auto"` `"ibname byups_1 meta library="libname byups_1 list;` `libname admtasks meta library="/home/SASINFO/AdminTasks"` `"ibname ore_1 meta library="libname ORE_1 list;` `"ibname ore1 meta library="libname ORE1 list;` `"ibname ore3 meta library="libname ORE3 list;` `libname imlt_3 meta library="/home/SASINFO/3_Dev/SchemaManagement/surveys/IMLTPre/Data"` `"ibname ore_1 meta library="libname ORE_1 list;` `"ibname ore3 meta library="libname ORE3 list;` `"ibname ore_1 meta library="libname ORE_1 list;` As you can see, the l in "libname" is being eaten up by the \\" after the ${libref\_lookup\_arr\[4\]} When I remove the double \\", the output is normal: `libname AT_Auto meta library=/home/SASINFO/3_Dev/SchemaManagement/AT_Auto` `libname byups_1 meta library=libname byups_1 list;` `libname admtasks meta library=/home/SASINFO/AdminTasks` `libname ore_1 meta library=libname ORE_1 list;` `libname ore1 meta library=libname ORE1 list;` `libname ore3 meta library=libname ORE3 list;` `libname imlt_3 meta library=/home/SASINFO/3_Dev/SchemaManagement/surveys/IMLTPre/Data` `libname ore_1 meta library=libname ORE_1 list;` `libname ore3 meta library=libname ORE3 list;` `libname ore_1 meta library=libname ORE_1 list;` Do you know what's causing this? I can't understand why a literal \\" would mess things up...
Unfortunately it seems like the problem was caused by something else as I added the \\"'s into the source file. Here's the specific line of code (it's sed input in the final script, and the only "echo/write" statement): `echo "libname ${libref_lookup_arr[0]} meta library=\"${libref_lookup_arr[4]}\""` This produces: `libname AT_3 meta library="/home/SASINFO/3_Dev/SchemaManagement/AT/Data"` `libname AT_VA meta library="LIBNAME AT_VA SASIOLA TAG=AT_VA PORT=10000 HOST="`[`sasva.someurl.edu`](https://sasva.someurl.edu/)`" SIGNER="`[`https://sasweb.someurl.edu:666/SAS"ASTAuthorization"`](https://sasweb.someurl.edu:666/SAS%22ASTAuthorization%22) `;` `libname AT_Auto meta library="/home/SASINFO/3_Dev/SchemaManagement/AT_Auto"` `"ibname byups_1 meta library="libname byups_1 list;` `libname admtasks meta library="/home/SASINFO/AdminTasks"` `"ibname ore_1 meta library="libname ORE_1 list;` `"ibname ore1 meta library="libname ORE1 list;` `"ibname ore3 meta library="libname ORE3 list;` `libname imlt_3 meta library="/home/SASINFO/3_Dev/SchemaManagement/surveys/IMLTPre/Data"` `"ibname ore_1 meta library="libname ORE_1 list;` `"ibname ore3 meta library="libname ORE3 list;` `"ibname ore_1 meta library="libname ORE_1 list;` As you can see, the l in "libname" is being eaten up by the \\" after the ${libref\_lookup\_arr\[4\]} When I remove the double \\", the output is normal: `libname AT_Auto meta library=/home/SASINFO/3_Dev/SchemaManagement/AT_Auto` `libname byups_1 meta library=libname byups_1 list;` `libname admtasks meta library=/home/SASINFO/AdminTasks` `libname ore_1 meta library=libname ORE_1 list;` `libname ore1 meta library=libname ORE1 list;` `libname ore3 meta library=libname ORE3 list;` `libname imlt_3 meta library=/home/SASINFO/3_Dev/SchemaManagement/surveys/IMLTPre/Data` `libname ore_1 meta library=libname ORE_1 list;` `libname ore3 meta library=libname ORE3 list;` `libname ore_1 meta library=libname ORE_1 list;` Do you know what's causing this? I can't understand why a literal \\" would mess things up...
I'm sorry, I can't follow what you're showing me here. Nothing about the quotes in that `echo` statement should cause any problems, so I think there's some context that's missing. Can you show more of the script? Also, on reddit you can indent a line by four spaces to make a code block.
Have you tried using screen/byobu?
No. You mean screen the terminal as in /dev/tty? (Sorry on mobile atm)
[Exec bash as the last command](https://askubuntu.com/questions/630698/how-can-i-keep-the-gnome-terminal-open-after-a-program-closes): Exec=gnome-terminal -e 'bash -c "gedit; echo Monkey; bash"'
I tried to run `ls -l` as an experiment, and came up with the following for different terminal programs I have installed: xterm -e bash -c 'ls -l; exec bash' urxvt -e bash -c 'ls -l; exec bash' gnome-terminal -- bash -c 'ls -l; exec bash' termite -e 'bash -c "ls -l; exec bash"' I don't know if this is a good way to do it. I might have overlooked some bash option. I remember in Windows the 'cmd.exe' from Microsoft has two different parameters. One is like bash's `-c` and closes cmd.exe after the command is done, while the other one keeps it running at a prompt (I think it was `-k`?).
Ah that seems like it will do it. Simply adding the shell command at the end of the passed string brings me back to the shell.
I think he means GNU screen, allows resuming of terminal sessions
Thank you, you're a very good teacher. I think I did fix all the notes you mentioned except the "\[" I didn't get that one? Would you mind explaining that more? I am sorry I know I am asking too much, just excuse my ignorance please. 
[ is a POSIX test. [[ is more modern, bash alternative. Shellcheck should have told you about it though.... Weird.
thanks, just fixed it.
That's definitely what u/Torocatala was referencing, but OP's question was the opposite. Screen lets you resume a terminal session after closing a terminal. OP want's to keep a terminal open after a program run in the terminal exits.
You're not alone. Thanks for the tip. I've never used (or even heard of festival), and I was happy to learn about it. 
are you using Network Manager? its dispatcher.d system supports quite a few more actions than up/down https://developer.gnome.org/NetworkManager/stable/NetworkManager.html
So... you want to pause the for loop? Do you have any code you can share? Example input/output? More information? You're not giving us much to go on...
 shopt -s extglob # for extended pattern matching mystdout='time $number' # this matches anything that starts with 'time ' case "$mystdout" in time?( *)) pause=$(printf '%.0f' $number) # floor to int sleep $pause;; esac 
Thanks for the heads up on Festival. 
For those unfamiliar with [Festival](http://festvox.org/)
thanks this worked
Yes, there is support for indirect reference in bash with ${!VAR}. Example: ONE=10.1.1.1 TWO=10.1.1.2 THREE=10.1.1.3 FOUR=10.1.1.4 for VAR in ONE TWO THREE FOUR do echo ${!VAR} done 
If you have an opposite problem and you look too serious during work, you can let festival to tell you random jokes. Just put this into the loop: curl -s -H "Accept: application/json" https://icanhazdadjoke.com/ | jq -r '.joke' | festival --tts
Bash supports indirect reference with ${!VAR}: ONE=10.1.1.1 TWO=10.1.1.2 THREE=10.1.1.3 FOUR=10.1.1.4 for VAR in ONE TWO THREE FOUR do echo $VAR=${!VAR} done 
Use an associative array (`declare -A`) declare -A ip=(); for host in one two three four; do ip[$host]=$(dig +short "$host.xyz.com") done declare -p ip
You could just write this: for var in $one $two $three $four; do ... done
You can use `eval` though [you need to be really careful](http://mywiki.wooledge.org/BashFAQ/048): for VAR in ONE TWO THREE FOUR; do eval "${VAR}=`host ${VAR}.xyz.com| awk '{ print $4 }'`" done 
'dig +short ...', that's a new one to me, thanks
 alias ddig="dig +noauthority +noadditional +noqr +nostats +noidentify +nocmd +noquestion +nocomments" alias adig="dig ANY +noauthority +noadditional +noqr +nostats +noidentify +nocmd +noquestion +nocomments" alias rdig="dig +noauthority +noadditional +noqr +nostats +noidentify +nocmd +noquestion +nocomments -x"
Me too... Thanks a lot OP
If you don't want/can't use bashism like /u/-BruXy- suggests, try `eval`. It's what I rely upon when I'm dealing with embedded/busybox systems. Works great.
No problem. I'm actually kind of surprised people haven't heard of festival. It's been around forever, and the default voice hasn't improved at all!
That is great!
Isn't this just printing the statement that would have to be evaluated to assign anything? I understand that the OP wants to assign to a variable with a dynamically generated name. 
He seems like an all-around cool guy. Bash is one of my favorite tools! After nearly twenty years of using it, I still discover new things in Bash every so often. Also I notice my shell scripts getting shorter and shorter as I learn more bash constructs combined with more obscure-ish tools like comm and awk.
What does comm do? Also give an example please! Cheers!
https://linux.die.net/man/1/comm
He's got a great resume and shells out solid advice. This dude has enabled so much to happen, and is so humble about it.
NB: He’s the author of Bash, but the current maintainer is Chet Ramey. Just in case someone else was confused like me :)
"shells out" - part of me hopes that was a totally unintentional BASH pun.
[Format JSON](https://pastebin.com/q2vY8GYN) when you're trying to pull data from it `cat json.file | python -m json.tool` Anyways. The issue is you're trying to parse `attachments.mountpoint` as if it were an element. It's not, it's an element of the **array** of attachments. Works for me `cat file.json | jq -r '.attachments | .[].mountpoint'` (Note the quotes around the `jq` filter)
Wow, thank you! You have no idea how much time I spent trying various combinations. Seriously, much appreciated. 
You can also play around with jq on: [https://jqplay.org](https://jqplay.org)
&gt; python -m json.tool Or `jq .`, since we’re already using `jq`.
man rsync shows, that '-a' (--archive) is a helper that - among other things - includes '-r'. (archive mode; equals -rlptgoD)
&gt;coughs Childish Gambino warned me, but I was still caught slippin up. Good call
I wonder which terminal emulator he uses. The one that ships with macOS (lol, probably not), something from Emacs, iTerm, ..?
Why wouldn't he? I don't think he cares about the OS paradigm as much as others seem to...
OK, perfect. Thank you. I thought that was the case. 
Mostly because macOS Terminal is quite shitty?
man rsync
Same for `cp` by the way, at least in the GNU coreutils version.
The $ has a special meaning in sed. So do \ characters.
$ cat a echo -e "$timestamp\\t$var_SLES\\t$check1\\t$check2\\t$comments $ cat a | sed "s/\\\t\$check2//g" echo -e "$timestamp\\t$var_SLES\\t$check1\\\t$comments (Yes, I'm using an extra cat needlessly. Pedants buzz off)
When you use `"` to surround your parameter for 'sed', then bash itself will take a look at that parameter first before it starts 'sed'. Bash will see that `$check2` you have inside the text and will think it is a bash variable that it is supposed to replace. When you are at the command line, this `$check2` is probably not set, meaning it is empty, so bash will replace it with empty text. The end result will be, bash will start sed with this parameter here: s/\\t//g
You don’t need the pipe in there. You can just do: cat file.json | jq -r '.attachments[].mountpoint'
psql itself is formatting output. It probably has options to tell it to return raw text with no formatting, it may even have an option to output CSV format results, which would be far easier to parse in a shell script .
looks like adding \pset format unaligned to the sql commands you're sending to psql might do the trick
This worked great, thank you so much! 
Also, unless you need ALL of the columns in the table, you could also replace your select * from ... with a select that grabs just the columns you need 
I could probably make do with this solution but I do want *juuuust* enough info to break the width. I think I can use this to do some things.
Did you see my other post?
yes it resulted the same. I have been trying to tweak it still but I continue to get everything squashed together. 
Thank you, I've been doing this for a while and I've never really understood why I have to pipe an array to find an element. Much appreciated, updated the OP
Even better :) 
no prob! I've gotten *deep* in `jq` recently and have been slowly getting better understanding of how the queries work and what can and can't be [easily] done. btw, minor correction: your line should be: cat file.json | jq -r '.attachments[].mountpoint' you've got an extra dot in there before the brackets which will cause an error.
How about you grep output of ps for vnc process, if it is less than or equal to 0, then launch vnc. So, ps followed by if and then statement - have looked at this before?
I should call it a day lol, thanks again
Assuming "inverting a color" means xor-ing it with #FFFFFF (0xFFFFFF) and that the color is in `$color`: printf -v color '#%06X' $((0x${color#\#} ^ 0xFFFFFF)) 
Serious question: what do you find shitty about the stock OS X terminal emulator?
Agreed. I know a guy at work who is more effective with just bash, sed, awk, diff, and core utils than most hardcore developers I know (and yes, I know they have different objectives). But he knows how to wield a handful of essential tools with such a depth that I haven’t seen before. There are some really nasty things you can do with sed that blew my mind (I don’t have any real examples to show as I’m on mobile now, but zomg...)
Comm is like diff, but more flexible -- you give it two files, and comm outputs them into three columns: lines unique to the left file, lines unique to the right file, lines common to both files. What good is this? You can leave out one or two of those columns, and then it is like doing set operations, or SQL JOIN statements.
Well for one you can't split a pane for two shells
By "inverting a color" I meant calculating the negative value of that color. You know, when you have a picture and you "invert" every color, you convert whites to blacks, yellows to blues and so on... So if my input color variable stores a #FFFFFF value, I'd like my output variable to be #000000. That's what I meant, I hope I was able to explain it better.
Not sure how to do it with Bash but you could use a node.js: https://github.com/onury/invert-color Hope it helps, pretty sure you could get some inspiration on how the calculation works in the code.
This sounds interesting, thank you. Unfortunately I don't know much of node.js, but I might try and give a look at those files tomorrow. But if someone is aware of another easier solution, I would of course appreciate that :P
Then the answer is correct 
Great! Thanks! I'll try that tomorrow and let you know
I have to say, this was an awesome question and /u/McDutchie['s answer](https://old.reddit.com/r/bash/comments/8ucx8a/can_you_use_bash_scripting_to_calculate_the/e1efk0l/) did a great job of answering. I would have had zero idea how to go about it. and the explanation is top notch!
Oh. I use tmux so I never even considered that. 
Well, that's certainly a decent workaround. However, multiplexing is one of those things that IMO any good terminal emulator should be able to do natively. It's like a web browser doesn't really need the functionality to download files because you can just wget/curl the link (often actually my preferred method). I also use tmux, although not for multiplexing (for me all multiplexing is with i3 or Emacs). Also, scrolling back the buffer is IMO far from ideal in tmux..
Nevermind: I think I found a solution: `#!/bin/bash` `FILES=/home/nfrederickx/ruby_faceapp/man/*` `for f in $FILES` `do` `echo "Processing $f file..."` `STR=$f` `WORDTOREMOVE=/home/nfrederickx/ruby_faceapp/man/` `echo "STR = $STR"` `FOO=${STR//$WORDTOREMOVE/}` `echo "FOO = $FOO"` `faceapp female $f $FOO` `done`
There's an interesting tool named `parallel` that should be helpful for this. You will probably have to install the package for it first as I think it's normally not installed by default on most distros. I don't quite understand how to do more complicated stuff with it. I hope the following will work right: Let's say you are inside the folder with the images. There's three images, and their names are this: abc.jpg def.jpg ghi.jpg You want the output to go into a sub-folder named `output/`. Running `faceapp female` with the next parameter being just the filename would be this: parallel faceapp female {} ::: *.jpg This would execute these three command lines: faceapp female abc.jpg faceapp female def.jpg faceapp female ghi.jpg Now, you said you want another parameter for a different folder. 'parallel' can cut off path and extension of a file when you use `{/.}` on the command line. This could be used to build the second parameter you want, perhaps like this: parallel faceapp female {} ../output/{/.}_female.jpg ::: *.jpg This will run command lines like this: faceapp female abc.jpg ../output/abc_female.jpg I experimented with all of the stuff like this here at the command line, using 'touch' to create test files, and using 'echo' to just print stuff instead of running actual programs: $ touch abc.jpg def.jpg ghi.jpg $ parallel echo ::: *.jpg abc.jpg def.jpg ghi.jpg $ parallel echo faceapp female ::: *.jpg faceapp female abc.jpg faceapp female def.jpg faceapp female ghi.jpg $ parallel echo faceapp female {} {/.}_female.jpg ::: *.jpg faceapp female abc.jpg abc_female.jpg faceapp female def.jpg def_female.jpg faceapp female ghi.jpg ghi_female.jpg $ parallel echo faceapp female {} {/.}_female.jpg ::: ./*.jpg faceapp female ./abc.jpg abc_female.jpg faceapp female ./def.jpg def_female.jpg faceapp female ./ghi.jpg ghi_female.jpg $ parallel echo faceapp female {} ../output/{/.}_female.jpg ::: ./*.jpg faceapp female ./abc.jpg ../output/abc_female.jpg faceapp female ./def.jpg ../output/def_female.jpg faceapp female ./ghi.jpg ../output/ghi_female.jpg 
Awesome, Thanks for taking the time to explain everything. This is much easier, and will come in handy later. Thanks!
This works flawlessly. Thank you so much!
There's a mistake on line 45. The `"` in the middle, in the word `DOESN"T`, is breaking stuff.
HAHAHAHA wow. I wasted like 2 hours wracking my brain over this, it's always something dumb. Goood catch, thanks!
This looks interesting, but although it’s a Bash script, I’m not sure it belongs in /r/bash – wouldn’t /r/Python, /r/pythoncoding or perhaps /r/programming be more appropriate?
Yeah it falls under multiple categories. Will share it on those threads as well. Thanks !
start the script with `#!/bin/bash -x` to debug it, or put `set -x` and `set +x` around the code you want to debug. It will show you how bash interprets your code and how it expands variables. Takes the guessing out of scripting
Try eval $name=test_$name
There are a few ways you can do it: # Use `printf -v` to write to $name printf -v "$name" 'test_%s' "$name" # Use `typeset`, `declare`, or `local` # (but beware the effect this has on scoping) declare $name=test_$name # Use name references with `typeset -n` or `declare -n` declare -n ref=$name ref=test_$name # Use `eval` (for safety, let `eval` expand the variable) eval "$name=test_\$name" # Use `read` (this is slow) read -r $name &lt;&lt;&lt; "test_$name" 
In the fiest case bash interprets the variables and passes the string to echo program which simple prints the values. In the second case bash agin interprets but then tries to execute the strings which are not valid programs like echo.
Depends what you are doing. Part of my job requires me to copy folders between servers. I use rsync -Rave ssh servername:/file_location /
OK. Thank you. I like how it spells Rave
He definitely does not care about the OS paradigm, if he is using a Mac and (presumably) running OSX. I wonder if he uses the stock version of Bash that ships with OS X. That would be version 3.2. From 2007. I also wonder if RMS weeps whenever he sees a picture of Brian Fox with a Macbook.
``` for name in one two three; do name="$name=test_$name" echo "$name" done ```
I might suggest `pause=${$number%.*}` as a simpler way to convert a float to an int.
More specifically: for name in one two three; do eval ${name}=test_${name} echo ${!name} ${name} done Output: test_one one test_two two test_three three 
Can you rephrase the question?
You want loops and should probably look in to shellcheck. gcp_tmp="/gcp_tmp/gs+convert+pdftk-${$}" for input in *; do [[ -f "${input}" ]] || continue # only handling files gs -o "${gcp_tmp}-txt" -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERVECTOR -dFILTERIMAGE "${input}" || break gs -o "${gcp_tmp}-img" -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERTEXT "${input}" || break convert -density 300 -quality 100 -colorspace Gray "${gcp_tmp}-img" "${gcp_tmp}-ras" || break # input 'myfile' -&gt; output '/gcp_tmp/new_myfile' pdftk "${gcp_tmp}-ras" multistamp "${gcp_tmp}-txt" output "/gcp_tmp/new_$(basename "${input}")" || break done rm -f "${gcp_tmp:?}-???" 
Look in to shellcheck, it rocks. gcp_tmp="/tmp/gs+convert+pdftk-${$}" for input in *; do [[ -f "${input}" ]] || continue # only handling files gs -o "${gcp_tmp}-txt" -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERVECTOR -dFILTERIMAGE "${input}" || break gs -o "${gcp_tmp}-img" -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERTEXT "${input}" || break convert -density 300 -quality 100 -colorspace Gray "${gcp_tmp}-img" "${gcp_tmp}-ras" || break # input 'myfile' -&gt; output '/tmp/new_myfile' pdftk "${gcp_tmp}-ras" multistamp "${gcp_tmp}-txt" output "/tmp/new_$(basename "${input}")" || break done rm -f "${gcp_tmp:?}-???" 
Thanks. I got it working using -"for var in *.pdf;do raster.sh $var;done" #!/bin/sh -e [ $# -eq 1 ] || exit 1 var="$1" gs -o /tmp/novector.pdf -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERVECTOR $var &amp;&amp; \ gs -o /tmp/onlyvector.pdf -sPAPERSIZE=a4 -sDEVICE=pdfwrite -dFILTERTEXT -dFILTERIMAGE $var &amp;&amp; \ convert -density 300 -quality 100 /tmp/onlyvector.pdf /tmp/vector-raster.pdf pdftk /tmp/vector-raster.pdf multistamp /tmp/novector.pdf output /home/arun/raster/$var &amp;&amp; \ rm /tmp/novector.pdf /tmp/onlyvector.pdf /tmp/vector-raster.pdf 
for file in $(find . -type f -name '*.pdf'); do gs=... ... ...; done
 #!/bin/bash echo "This is Bash" exec sh -c "awk '/^### PYTHON/,/### END/' $0 | /usr/bin/python3" ### PYTHON print('This is Python') 
You could try to do this with the "here-doc" thing that bash has. It would look like this: python &lt;&lt; 'EOF' print("hello") EOF The only problem I can see about that is, I'm not quite sure how to pass parameters to the python script. I know pretty much nothing about Python. With Perl, it seems you can use "/dev/stdin" as the script name and can then pass normal parameters after that. Here's an experiment about that at the bash command line: $ perl /dev/stdin abc def ghi &lt;&lt; 'EOF' &gt; for my $arg (@ARGV) { &gt; $count++; &gt; print "argument #$count: '$arg'\n"; &gt; } &gt; EOF argument #1: 'abc' argument #2: 'def' argument #3: 'ghi' 
I think this is a perl (and ruby) specific hack: #!/bin/bash echo "In bash"; perl -x "$0" exit $? #!perl print qq|In perl\n|; 
This looks like something that would be more suitable to my needs! Thanks! However, is there a way that you could just put this into a `while` loop? Or is there a way to *not* have to call the other language? My theory is that it could allow you to actively switch between the two languages with having to call that language (i.e. ### PYTHON).
Kinda, technically yes? This is a bad pattern to set though, probably shouldn't do this.
Any specifics as to why not? 
You can't share data structures or libraries between the two, and any other developer will need to be familiar with both.
Working from your idea #!/bin/bash python3 /dev/stdin $1 &lt;&lt; EOF import sys print(f"Hello {sys.argv[1]}") EOF Invoke with: `./test.sh Jerry` 
I don't really have a suggestion but I'm interested in why you would need this. Can you give more information on your use case?
this is python calling bash commands: import subprocess print("compiling c launcher") bash = "g++ -o runapp run.c" subprocess.check_output(bash, shell=True)
Yes, heredocs are great with python. In a python heredoc pattern you can use double quotes around bash positional arguments i.e. $1, $2, etc. Or environment variables. Bash changes then to their values before the python executes.
that isn't a bashism; it isn't bash doing that. You should ask your distro's community, as ... you didn't mention what distro or terminal emulator you're using.
Though there are some tools for xml parsing (xmllint or xml2) I recommend using python for this class of tasks.
[removed]
If Python or Perl are not available, you could also look into html-xml-utils.
Sorry, didn't think to include that. It's the terminal built into Mac OS X. I just found it strange that certain scripts were doing it, and certain scripts aren't.
This is the only way I'd consider, run the better language... 
Yes, I reluctantly do this is if I need to set environmental variables (such as `DB_DSN`, `JAVA_HOME` or `LD_LIBRARY_PATH`) before launching a script. It becomes unwieldy quickly, but it's useful when there are special scripts that need unique settings, perhaps as we incrementally migrate to new libraries etc. #!/bin/bash export FOO='Set in bash (wrapper shell script).' sed -e '1,/^#.*\/python$/ d'&lt; "${0}" | python - exit $? #!/usr/bin/python import os print(os.environ['FOO']) 
There os already a tool that does exactly this: gnu stow. First install it: `apt install stow` Then read the manual: `man stow`. Or you can read it [here](https://linux.die.net/man/8/stow) What I do on a new machine: ``` # install the tools $ sudo apt install git stow # clone my dotfiles $ git clone https://github.com/username/dotfiles.git ~/dotfiles # “stow” the dotfiles $ cd ~/dotfiles $ stow */ ``` This will result in all the files being symlinked as you wanted. You do need a slightly different layout though: ``` dotfiles/ bash/ .bashrc .bash_logout .profile zsh/ .zshrc tmux/ .tmux.conf ``` You get the idea :-)
http://amoffat.github.io/sh/ would probably be a good fit for you. When you import it in a python script, it gives you a pythonic interface to any command in your PATH.
&gt;I’m basically working with massive XML files sent out by a database. Do you have access to that database ? (I'll guess you don't given your question, but it's worth asking) If you do, you could spare yourself the intermediate XML and query the database directly.
''' My main theory is that you pre-define the language at beginning of each script (i.e. #!/usr/bin/env python/bash); ''' This is not a theory, it is fact. You are initially defining which interpreter to use. I don't know of a way to launch two in parallel, and not quite sure why you would want to. The subprocess python module (as mentioned in this thread) is great for running shell commands requiring access to stdout and stderr. If you don't care about returning output (and your script is relatively secure) you could also run a shell command with os.system('bash command').
isolate the data you want (grow, awk, sed, etc) and then redirect it to a file ( &gt; file ). would need more details and sample data to provide more advice. 
A fun thing about bash is, you can experiment with everything directly at the command line. You don't have to edit and re-run a script all the time. Everything that works in a script also works at the command line. Try to just print to the screen. You can print a file with `cat`. Try to do a sequence of commands that prints those file contents you want to have. You can type a really long command line that executes several commands by using `;` between your commands. After you have this, you can use `{` and `}` to create a group. You can redirect that group's output with one `&gt; filename` after the `}`, for example: $ { echo "hello"; echo "hi"; echo "hey"; } &gt; testfile $ cat testfile hello hi hey 
First and foremost bash is glue you use to appropriately hook up programs that do very specified things. The philosophy used is divide and conquer. So break up your problem into well defined pieces, develop a proposed solution, and see how that fits. If not the solution is so good, try again. Do you really expect a fully developed answer to such a vague question?
If the `install.sh` is in the git repo how will it be able to install git? You will need git in order to clone the repo right? Here is my [install.sh] (https://github.com/jnalley/dotfiles/blob/master/install.sh). It is pretty similar to what you are requesting.
Oh, okay I see now. I didn't know that you could hook it up to other languages, and that would make it much more easier. Thanks and my bad for the poorly worded question.
Actually, you don't hook it up to other languages, but you can call/execute other programs from bash. And sometimes these programs happen to be interpreters for programming languages. (to which you can pass a file with the program, or pass the commands directly in a string.) What's more, you probably already have called programs from bash which are not really part of bash but will most likely be find on it. For example `cat`. For fun: You can use `type` to see what/where it is. Most likely `which cat` will return `/bin/cat`, but `type cd` will return that it is a shell builtin. So `cd` is really part of bash. 
Probably an easier way, but I'd just put something like: find ./ -name "*.jpg" -exec mv {} /sorted/folder/path \; or throw it in a loop.
So I've tried this... and also tried the other commenters option and nothing is moving... find /Users/baxter/Desktop/Photo-Pictures-Etc/New\ Desk -name '*.jpg' -exec mv {} /Volumes/Archive/JPEG \; Any idea what would be causing no files to move? There' isn't even an error output.
Did you try just: `find /Users/baxter/Desktop/Photo-Pictures-Etc/New\ Desk -name '*.jpg'` To see if it's actually spitting out file names? 
No output for either: Baxters-MacBook-Pro:~ baxter$ find /Users/baxter/Desktop/Photo-Pictures-Etc/New\ Desk -name '*.jpg' Baxters-MacBook-Pro:~ baxter$ find /Users/baxter/Desktop/Photo-Pictures-Etc/New\ Desk/ -name '*.jpg'
Try to `cd` in the New\ Desk folder and run it with `find ./` then do `find ./ -name "*.jpg*"
So it seems my jpgs are actually listed as .JEPG. Case sensitivity lol. 
You got me on that one. Some file in the directory. Should become apparent as you start moving stuff around.
So this is what actually worked. find ./ -type f -name "*.JPG" -exec cp -v {} /Volumes/Archive/JPEG \; Thanks for the tips though!
I haven't had my start-of-the-day coffee yet... something like this, maybe? case $1 in (a) ff_profile='main' ;; (*) if [[ -d "${HOME}/.firejail/firefox-${1}" ]]; then ff_profile="${1}" else firejail --seccomp \ --profile="${HOME}/.config/firejail/firefox.profile" --private \ /usr/bin/firefox -no-remote -private "$@" exit 0 fi ;; esac
Consider [associative arrays](https://www.gnu.org/software/bash/manual/html_node/Arrays.html) instead.
While there are ways to do that ([printf -v](https://www.gnu.org/software/bash/manual/html_node/Bash-Builtins.html), [eval](https://www.gnu.org/software/bash/manual/html_node/Bourne-Shell-Builtins.html), [nameref](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameters.html)) I suggest you consider [associative arrays](https://www.gnu.org/software/bash/manual/html_node/Arrays.html) instead.
 find . -type f -name '*.html' -exec sed -i~ $'s/ /\t/g' {} + This leaves the originals with a `~` suffix.
sed -i ‘s/ /\t/g’ *.html
`sed -i 's/ /\t/g' *html` Works better. You don't need to use back ticks if running from terminal.
sed -i ‘s/ /\t/g’ *.html --- ^^reddit ^^sedbot ^^| ^^[info](https://github.com/ndri/reddit-sedbot)
Answer already provided, but OP give this a read to understand what SED is doing: [https://www.gnu.org/software/sed/manual/sed.html#The-\_0022s\_0022-Command](https://www.gnu.org/software/sed/manual/sed.html#The-_0022s_0022-Command) And for wider reference, use this bad boy: [http://www.theunixschool.com/p/awk-sed.html](http://www.theunixschool.com/p/awk-sed.html)
Thanks
good bot
You have to indent your code with four spaces, or the spaces get squeezed to a single space.
Possibly fixed, though it’s still showing up weird on mobile. Hopefully OP gets the gist without needing to copy/paste though.
Looks good now.
I think you need rsync [https://www.google.fr/search?q=rsync+backup+script](https://www.google.fr/search?q=rsync+backup+script)
Maybe I'm missing something but what does this offer instead of a keychain/keyring?
Separated config for each context/project.
Can you please give some more context about what you're doing and wanting to achieve?
This could be quite useful for some!
Everyone here keeps talking about Sed, but on GNU systems, at least, a better tool might be `unexpand`, e.g.: unexpand --all --tabs=4 INPUT... &gt;OUTPUT This is in one way better than the Sed solution since it correctly handles the cases where the spaces do not align to tab boundaries. For example, an input file containing: ······foo·bar·····baz will turn into: » ··foo·bar» ···baz I've highlighted the tabs and spaces here for clarity. Note in particular that the tabs go only as far as the next tab stop. If you are outputting the converted file through a device with hard tab stops (8 is typical, but there's nothing stopping anybody from using 4, I guess), this will ensure that the output remains visibly identical even when the input has badly aligned, indented and spaced lines.
Sorry I should've been more clear. I have an assignment to write a script that is a modified mv command called mymv. The script tries to rename the specified file, but if the destination file exists, instead creates an index number to append to the destination file, a sort of version number. So if I type: **$ mymv a.txt b.txt** but b.txt already exists, **mymv** will move the file to b.txt.1. I hope that is a little bit more helpful!
Unfortunately the assignment requires me to use the mv command. Thank you for the suggestion though!
I might be able to assist. Now, I'm no expert in this so take this with a grain of salt. I also dont have a computer right now to test this but it might help you search for an answer. It seems you basically want to pass two files into mymv as command line arguments. You then want to pass those arguments to an if statement to check existence. That would be done with something like if [[ -f "$arg1" ]]; then #file exists else #no exist Now if the file exists, you replace #file exists with something like mv $arg1 ${arg1}.1 Note: everything I said here is very static, meaning its not really set up to change dynamically like something a for loop would accomplish. But at least this might lead you in the right direction to getting your assignment done and learning at the same time. Also, I'm on a phone so if my response looks like garbage...I apologize. 
 #!/usr/bin/env bash if [[ -f "$2" ]]; then mv "$1" "$2.$(date +%Y-%m-%d)" else mv "$1" "$2" fi That should do the trick. But you shouldn't ask the internet to do your assignements for you. You should definitely search the internet for answers, but asking for them is another altogether. Anyway.
I see... ▓▒░$ touch a.txt ▓▒░$ mv --backup=t a.txt b.txt ▓▒░$ ls -1 *.txt* b.txt ▓▒░$ touch a.txt ▓▒░$ mv --backup=t a.txt b.txt ▓▒░$ ls -1 *.txt* b.txt b.txt.~1~ Sadly, it looks like those tilde characters can't be removed via supplementary arguments to the `mv` (or `cp`) command, so it seems that your options are to either abandon this approach altogether and handle the versioning logic manually, or to use this approach followed by a post-move find-and-rename. You might find that capturing the output of `mv` will help you here. ▓▒░$ echo "cat" &gt; a.txt ▓▒░$ mv --backup=t -v a.txt b.txt 'a.txt' -&gt; 'b.txt' (backup: 'b.txt.~4~') ▓▒░$ grep -H cat *.txt* b.txt.~4~:cat So we can see that in the example output, `mv` tells you the filename that it's backing up to. In other words: if you capture that output, you'll be able to pluck out the filename for the backup file. That way, you know the exact filename that you want to rename... Of course, there are plenty of different approaches to this problem. `logrotate` might be one approach from left-field. 
\#clone vundle into the proper directory git clone [https://github.com/VundleVim/Vundle.vim.git](https://github.com/VundleVim/Vundle.vim.git) \~/.vim/bundle/Vundle.vim \#install vundle plugins vim +PluginInstall +qall That last line is right from the vundle github readme. It will work assuming your needed plugins are in your vimrc, so make sure your script sets up your vimrc before running that command.
&gt;git clone https://github.com/VundleVim/Vundle.vim.git \~/.vim/bundle/Vundle.vim Is there some way to check if the vundle is already cloned on the system before cloning it?
I've never needed that in my use case, I just have those lines in my own script I run on fresh linux installs. But, off the top of my head, if I were you, I would have my script check and see if ~/.vim/bundle/Vundle.vim existed on the system. If that file didn't exist, I'd clone the repo, if it did exist, I wouldn't clone the repo. I don't know how to do that off the top of my head though. And I probably wouldn't even worry about it. If I had already cloned the vundle repo, I wouldn't be afraid to overwrite it with a fresh clone. 
I use vim-plug rather than vundle, but I have this in my vimrc. " automatically install vim-plug and run PlugInstall if vim-plug not found if empty(glob('~/.vim/autoload/plug.vim')) silent !curl -fLo ~/.vim/autoload/plug.vim --create-dirs \ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim autocmd VimEnter * PlugInstall | source $MYVIMRC endif When vim starts, it checks if vim-plug is installed. If it isn't, it will install itself, and any plugins. Personally, I find this easier than using an install script, since I can just scp my vimrc to a system and I'm set.
depends on the app but you can check your tty1 or whatever tty spawned your x session to see if it dumps the output there.
I don't really know a way of doing this. If this is something you'd regularly want, get in the habit of launching those programs from the terminal. If you're investigating a crash, or some other big glitch in a program, check to see if it outputs to a log file, and hope that log file has the output you were looking for.
There has to be an executable in your path to be able to do that. FWIW, I have the following in my \`.bash\_profile\` which allows me to open files in Chrome, I do: CHROME\_PATH='/Applications/Google\\ [Chrome.app/](https://Chrome.app/)' chromeopen() { bash -c "open -a $CHROME\_PATH $1" } Then to open some file all you need to do is \`chromeopen some\_file.html\`. You can also launch Chrome by running what I assume is the executable, \`/Applications/Google\\ [Chrome.app/Contents/MacOS/Google\\](https://Chrome.app/Contents/MacOS/Google\) Chrome\`
Huh. That's interesting. In linux, I can just type "chromium" to launch chromium(I don't use google chrome). Or I can type chromium whatever.html to open that file in chromium. Something, I did not know until I tested it just now. neat. I use firefox though. Same thing probably works.
I just used chrome as an example, I was trying to do this on sitck fight, which needs to be launched via steam (it keeps crashing 8 mins after I first join a game). So that is why I wanted to know how to do this to an app that is already running.
Thanks, I will look into this...
tty1 would not work, but tty did, although it did not show any useful info. I am going to try to use console
yeah sorry about that. I got more distracted asking how mac worked. My log file is probably a decent idea though. I bet steam or the game you are trying to play have a log file where it would write crash data to.
strange... it has stopped crashing...
could not find a log, but I am using the app called console now... strange, it has not crashed yet...
I will look for a log though...
Application in *nix usually print to /dev/stdout unless configured otherwise (ie. print to a log file or redirect /dev/stdout elsewhere). I'm going to assume based on your question that this is not a process that was intentionally daemonized and therefore not writing output to a file that you could read. In cases where you started a program and backgrounded it, then closed the parent process (your shell, usually), you can use [reptyr](https://github.com/nelhage/reptyr). This may not always work but for what you're trying to do I think it may be your best option.
Thanks, I will try it
I am using mac, not linux
+1 for reptyr. I was able to read the output from a process that I started over an ssh tunnel (on my computer) and lost connection (from my phone). Works great
Could not find any steam logs on stick fight, but while looking through console I found this: `localhost StickFight[15975]: (LaunchServices) [com.apple.launchservices:default] LSExceptions shared instance invalidated for timeout.` (Found this once) `localhost kernel[0]: (IOHIDFamily) StickFight is not entitled` (Found this 5 times) That is the only thing I could find about stickfight and none of them are 8 mins apart.
strace -f -p pid 
I get an error 'command not found' I am also using mac, so if it is a linux command, that will explain why it is not working
Did you try just daisy-chaining the commands? ssh [args] host1 'ssh [args] host2' 
Thanks!
If your ssh sessions require a password you'll probably have to use expect.
You don't even need a script. Just set up the host in your SSH config and give it a `ProxyCommand`. If it has to be in a script, use `ssh`'s `-J` option. 
Many options if you're willing to manually test it: * Run interactive docker container: `docker run -it ubuntu` * Open new VM locally (virtualbox, etc) * Open a new cloud VM * Create a new user locally * Convince someone else to use your dotfiles * Buy a new computer * Make your install script idempotent, remove any trace of your dotfiles then reinstall * Fix it slowly over years as you run into rough edges
ssh -J is the best bet for sure. Failing that, make a tunnel with -L
Bad idea. Use keys
I know you already mentioned you’re on a Mac, but in case anyone else has the same problem: on most modern Linux systems, stdout and stderr of graphical applications will be connected to the systemd journal. For example, you can see the entire output of Thunderbird during the current boot with `journalctl -b /usr/bin/thunderbird -o cat` (add `-f` to show new lines as they come in).
nice utility. although using .ssh/config is still better since the same config could be used for scp as well, not just ssh
In your \~/.ssh/config write : Host secondServer User username Hostname add.re.ss.of.the.server Port 22 IdentityFile ~/.ssh/yourkey # if you use passwords don't write this line... ProxyCommand ssh firstServer nc %h %p Host firstServer User username Hostname add.re.ss.of.the.server Port 22 IdentityFile ~/.ssh/yourkey # if you use passwords don't write this line... Then you would just do : user@local:~$ ssh secondServer It will log into the first then the second server.
Definitely. You can get remote servers to 'remember' connections and allow automatic login (based on the locals key)
Can I ask what you're hoping to achieve? Is it a restricted server? If so, you'd be MUCH better off talking to your security team about adding rules/exceptions for you.
Radssh allows for to use a host as a "jumpbox", in other words. A host used to to authenticate to jump to another box or server. http://radssh.readthedocs.io/en/v1.1.0/shell.html
dtruss -f -p &lt;pid&gt;
No! No. Don't listen to anyone else here. (Except /u/leBoef for `-J`, but `ProxyCommand` is more generic than what you need.) `man ssh_config` and search for `ProxyJump`. You don't need a script, your `.ssh/config` can do this for you transparently.
Make a new account.
ProxyCommand has been augmented in newer versions by ProxyJump. I wrote an article on this topic a while ago too if it helps. https://blog.linuxserver.io/2016/12/02/perform-multiple-ssh-hops-with-ssh-config/
TIL, thanks!
Thanks, the game stopped crashing, but if I have this issue again, I will use this command.
Your are probably looking for something like: parallel --joblog my.log --resume ./myprogram ::: input.files* 
 man ip
&gt;would this project be better to write in python? No, Python doesn't offer any benefits in this case. 
At a quick glance it seems OK. Why do you need to record the details so often? Are you benchmarking hardware?
TIL as well, thanks. I haven't used the old version of this in at least a decade. Interesting.
Not exactly, I am benchmark a program. To make a long story short, there is a python program that I KNOW uses a lot of resources. I have to present this information but knowing is simply not enough so I need to record data for a few minutes at most that I can graph to show the usage and use for comparison for a RPI with \~0 load. 
On mobile, but Give me a bit and I’ll write something up and get you the resources I used to learn. 
rsnapshot
I'd recommend to take a look at borg. They supply a script on there website to manage the backup procedure that is easily modified and you can decide how many backups to keep saved. Just put this in you crontab. You can restore all the backups and even mount each snapshot if you want to browse it. However, I am not sure what happens, if you work on the files during backup.
To be honest you really ought to instead learn how to manage users using something like Puppet. It’s easier, and a better way of doing things. 
 Why do you want to do this in bash? Have you considered ansible for example? 
You can add the user to a group with sudo access to save having to append their details to the bottom of /etc/sudoers You can use [/etc//skel/](http://www.linfo.org/etc_skel.html) to create your default files and permissions. The only thing in your list that isn't either unnecessary (the visudo part) or already part of adduser/useradd functionality is #6, but if you already have the key text you could put it in /etc/skel if it's going to be the same key for all users. 
I tinkered with this and found that your paste has DOS-style line-endings that may have been confusing awk. I also explicitly set the field separator in awk to a single space and stripped leading spaces and tabs from the paste data. There might be a way to collapse the sed and tr commands. Here's what I ended up with. ``` tr -d '\015\009' &lt;sampq.paste | sed -e 's/^ *//' | awk -v id=plexmediaplayer ' BEGIN {FS=" "} $1 == "index:" {idx=$2} ($1 == "application.process.binary") &amp;&amp; ($3 == "\"" id "\"") {print idx; exit} ' ```
How about putting your $HOME into some version control system? I'm using mercurial, but any other VCS should work * You have very fine grained control of what you backup in .hgignore (you don't want to backup .swp files for example I guess) * You can compare the backups easily (hg diff) * You can sync the backup to distant location easily (hg push) * You have integrity check built in (hg verify) * You can comment on your changes if needed (I automatically commit only certain directories, for others like scripts I have weekly reminder that there is something uncommitted)
yeap, borg is amazing [here](https://pastebin.com/TAXAYg3m) is my borg script
Borg and Anacron! Thats the way to do it. Tho not sure if it becomes corrupted if I reboot or shut computer before it has done its work.
rsync is your friend....
Use an ansible playbook. 
If you're interested in incremental AND encrypted backups, I made a bash script designed to be run as a cron job that uses Duplicity. [https://github.com/amlamarra/BashScripts/blob/master/backup-configs](https://github.com/amlamarra/BashScripts/blob/master/backup-configs) The comments explain most of it, but here's a few notes. You'll need to save your passphrase to a file (/root/.duplicitykey) and make sure only root can read it. You can remove the mysqldump command if it doesn't apply to you. Near the end is where you set which directories are to be backed up. For you, it would be "home/username" (leave off the first slash /).
Pretty much anything based on rsync will do the job. If you like to use a GUI I recommend Back In Time.
They supply a script? Thats pretty cool. Care to let us know where its located in their website?
You may want to start gathering input vars $1 for the name... For example: NAME=$1 adduser ${NAME} &amp;&amp; echo "password" | passwd --stdin ${NAME} ... For the point 3, uncomment the group wheel at sudoers file and add the following line to your script: usermod -aG wheel ${NAME} Now, for the point 4,just do: ssh-keygen -f \~/.ssh/id\_rsa -q -P "" Then, touch the rest is easy enough... echo "YOUR KEY TEXT" &gt;&gt; /home/${NAME}/.ssh/authorized\_keys ... I hope this helps... EGMWeb
The awk stuff seems to work just fine, like /u/diseasealert pointed out it's probably just some non-printing chars screwing it up. id=$(pacmd list-sink-inputs | awk -v id=plexmediaplayer '$1 == "index:" {idx = $2}; $1 == "application.process.binary" &amp;&amp; $3 == "\"" id "\"" {print idx; exit}'); pacmd move-sink-input $id 5
Check out rnsapshot. It's awesome. It is made to do exactly the kind of thing you are doing (on the backup side). On the restore side, you just go find one of hte "snapshots" and copy the files out (or rsync) them yourself. It makes incremental backups using hardlinks for files that haven't changed and leaves you with a directory full of time-stamped directories that are snapshots of your source at whatever period you decide to cron it at, keeping as many copies as you choose etc.
Have you considered using LDAP or NIS+ or some other way to manage accounts centrally, versus at the machines themselves?
Backintime is pretty nice and simple.. Wrapper around rsync... https://en.m.wikipedia.org/wiki/Back_in_Time_(software)
Non-Mobile link: https://en.wikipedia.org/wiki/Back_in_Time_(software) *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^197953
**Back in Time (software)** Back In Time is a backup application for Linux. It has versions that integrate favorably in GNOME and KDE SC 4 and is available directly from the repositories of many Linux distributions. Released under the terms of the GNU General Public License (GPL), it is free software. Back In Time uses rsync as backend and has the characteristic feature of using hard links for files which are identical in snapshots at different times, thus using disk space just once for files that remained unchanged. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thanks to both of you. If you don't mind one more question. How would I adjust this command so I can also use it for : pacmd list-sinks I tried id=$(pacmd list-sinks | awk -v id="PulseEffects(apps)" '$1 == "index:" {idx = $2}; $1 == "name:" &amp;&amp; $3 == "\"" id "\"" {print idx; exit}') but it gives me the index id of the first one listed : 0 instead of 1 in this output : https://pastebin.com/raw/f0TKjGkd thanks!
So, same problems as last time with the line separators and leading tabs. Also, the line with the index: field on it had a leading '\*' that had to be stripped off. Also, it looks like you were a little mixed up about the name: versus the device.description: -- one is PulseEffects\_apps and the other is PulseEffects(apps). When parsing data out like this, you just need to be careful and take your time. I got it to work this way: #!/usr/vin/env bash tr -d '\015' &lt;sampq.paste | sed -e 's/^ *//;s/\t//g;s/^\* //' | awk -v id=PulseEffects_apps ' ($1 == "index:") {idx = $2} ($1 == "name:") &amp;&amp; ($2 == "&lt;" id "&gt;") {print idx; exit} '
Pretty cool, now I need to find a use for it. 
Seems like you could use git for that pretty easily.
I'd like to use this script too but I think your answer and original question assumes all files to be checked for duplicates are under the same directory. I want to know if it's possible to use this for multiple directories for my purposes. I guess the only thing that needs to be changed to support that is to use basename (i.e. cat-video.mp4 instead of ~/video/cat-video.mp4) instead of path to file and instead of `find .` you can use `find &lt;path1&gt; &lt;path2&gt; &lt;path3&gt;`? How would you go about doing that if you're using perl? Also, in the regex used for perl `[\w\-=]`, what does the `=` sign do? If it's literal, why did you add it? Thanks!
borgmatic!
Thanks man, how do I use it on the command instead of the file though? I tried replacing sampq.paste but it didn't work
&gt;my wandering mind has been wondering how one would create a bash script that would perform the following steps: One of the golden rules of sysadmin is: "If you have to do something more than once, automate it." So congrats. You really should do this with something like Ansible, and/or, if you have a sufficient number of hosts, you should look into an authentication directory like FreeIPA. But, to do this with `bash`, here are some thoughts: &gt;As I'm spending hours adding a user to a bunch of CentOS servers You need to look into some method to orchestrate commands across multiple hosts, this is one thing that Ansible does, but in a pre-Ansible era (and pre-puppet/chef/cfengine/whatever), you'd typically use something like cluster-ssh, parallel-ssh, or a plain old `ssh` loop. A plain old `ssh` loop looks something like this: for host in $(&lt;serverlist); do ssh "${host}" somecommand; done Yes, you can do this with `sudo` in the mix as well in an automated way. But you shouldn't, because in some cases you risk revealing your password to anyone running `ps`. You should use Ansible instead. Or at the very least, setup a few NOPASSWD rules in `sudo` for your account. Here's a mish-mash of code thrown together that probably won't work, but it's something for you to start with. https://pastebin.com/JXSX8GrL Hint: test it, find what's wrong with it, figure out how to fix it, merge in the ssh loopy stuff I just mentioned, re-test and find out what's wrong with that, figure out how to fix that. Fixing broken stuff is one of the best ways to learn.
That `find path1 path2 path3` idea you had will work. The 'find' tool allows adding several locations at the front of its command line. You don't have to change anything about the 'perl' part of the command line. About that regex rule and the `=` character in there, I remember I had googled what kind of characters YouTube uses for its video IDs. There was a forum post somewhere that said that `-` and `=` are characters which can show up. And the `\w` in perl-regex is supposed to mean `[a-zA-Z0-9_]`. The `\w` is technically a bit wrong because Perl can do Unicode. All kinds of international characters also count as a "word" character for the `\w`.
https://borgbackup.readthedocs.io/en/stable/quickstart.html Scroll down to *Automating Backups* There is the bash script
Pipe the output of `pacmd list-sinks` into tr instead of using the file redirection to stdin.
Oops, you're right. I think I found a problem though ([dup is just a wrapper](https://ptpb.pw/ACnq1dJdh808SiTQDSTj4FHeEXh1). Only modification I made to the perl part is removing the space and dash after `?:` since I just want a space separating between the date (including the parentheses) and the unique ID): Try this: mkdir -p /tmp/sg1500/extracted-media/faye-wong-王菲/ /tmp/wd8000/xattr/faye/ touch '/tmp/sg1500/extracted-media/faye-wong-王菲/1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827).mkv' '/tmp/wd8000/xattr/faye/1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827) Aq7kgDz8_rs.mkv' dup /tmp/sg1500/extracted-media/faye-wong-王菲/ /tmp/wd8000/xattr/faye/ Output: ## key: 1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827) # /tmp/sg1500/extracted-media/faye-wong-王菲/1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827).mkv /tmp/wd8000/xattr/faye/1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827) Aq7kgDz8_rs.mkv It wants to remove the file with the unique ID instead of the file that doesn't, when it should be the other way around. 
This here seems to do what you want: find . -type f | perl -lnE 'if (m{^.*/(.*\))(?:(?: -)? [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", sort { $b cmp $a } @{$h{$_}}); } }' The same with line-breaks: find . -type f | perl -lnE ' if (m{^.*/(.*\))(?:(?: -)? [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", sort { $b cmp $a } @{$h{$_}}); } } ' The changes I made to what was posted two months ago are this: That `" -"` character sequence you mentioned is made optional like this: `(?: -)?`. At the end of the code, where the sorting is done on the file names, the sorting is reversed by adding `{ $b cmp $a }`: sort { $b cmp $a } @list; I experimented on the following test files: $ find . -type f | perl -lnE 'if (m{^.*/(.*\))(?:(?: -)? [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", sort { $b cmp $a } @{$h{$_}}); } }' ## key: 1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827) # ./1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827).mkv ./1999] Faye Wong Eyes on Me (Music Awards Japan) - nobita813 (20130827) Aq7kgDz8_rs.mkv ## key: another test (hi) # ./another test (hi).mkv ./another test (hi) - 12345678901.mkv ## key: test (hello) # ./test (hello).mkv ./test (hello) Aq7kgDz8_rs.mkv 
Thanks a lot ropid, this script saved me hours of tedious work :)
Install [netdata](http://my-netdata.io/), it should be able to provide the data you need. You can also dump selected time intervals and reload them later for interactive viewing. This is the very first thing that I install on new machines, it's super useful for diagnosing system behavior.
If you don't mind redirecting to stderr then you can use code as: abort() { echo " abort" $1 1&gt;&amp;2 echo "continue" } for i in {1..3}; do (( i == 2 )) &amp;&amp; eval $(abort $i) echo $i "ok" done 
I searched through `man bash` and found this setting for the `shopt` command here: &gt; compat43 &gt; If set, bash does not print a warning message if an attempt is made to use a quoted compound array assignment as an argument to declare, makes word expansion errors non-fatal errors that cause the current command to fail (the default behavior is to make them fatal errors that cause the shell to exit), and does not reset the loop state when a shell function is executed (this allows break or continue in a shell function to affect loops in the caller's context). You would enable it like this at the start of your scripts: shopt -s compat43 
Thanks. That does it. I had Googled for compatibility options and for shopt lists and hadn't found it. I gave up on the bash manpage because it was so long. I realize now, after Googling for "compat43" that although "shopt" is mentioned in several places in the manpage, that the shopt options are almost at its end.
I just did `/continue`, then hit `n` a bunch of times. I got to the spot where 'compat43' was discussed pretty fast. I also have `j.5` added to my `$LESS` environment variable which helps with searching fast. It makes it so the line with the search result ends up centered on the screen, and you can then read the previous lines without having to scroll.
Your problem, apart from sourcing a poorly written script, is that the output of the `host` command is not predictable for you. So ask yourself: "How can I make the output of the `host` command predictable?", or: "`host` is not predictable, is there a similar tool I could use that is?" Consider something like: getent ahostsv4 somesite.com | awk '{print $1}' | sort | uniq | paste -sd ',' - 
So your command doesn't seem to match the results... If the filename is always of that format, you could use cut with a delimiter and field numbers. So something like: find ... | cut -d- -f2 | cut -d_ -f2 This would at least parse out the date for you. You would need to loop through the file list and pull these out into atemp file or bash array. The much back after sorting. On the other hand, sort itself has some very powerful options and may be able to do this by itself. 
&gt; sourcedir="~/.dotfiles" targetdir="~" backupdir="~/.backupdotfiles" In all of these cases, the `~` is quoted, so it's treated as a literal character. `~` only expands to a home directory: * if it is _unquoted_ at the beginning of a word; * if it is _unquoted_ and immediately follows a `:` or the first `=` in a variable assignment. See [the documentation](https://www.gnu.org/software/bash/manual/html_node/Tilde-Expansion.html) for full details.
You could try using "$HOME" instead.
Tilde has no special meaning inside the double quotes, it won’t expand. Use $HOME, or take it out of the quotes. https://unix.stackexchange.com/questions/151850/why-doesnt-the-tilde-expand-inside-double-quotes
&gt; I want to extend this to check if the symlink is already correctly pointing inside the .dotfiles directory. You probably don't want to do this, since that would be a [TOCTTOU](https://en.wikipedia.org/wiki/Time_of_check_to_time_of_use) bug. You would be better off using `--force` (aka `-f`) to create the symlink regardless after removing any existing entry. (Better even than that would be to create the symlink under a temporary name and moving it to the desired name. But that's a rather more advanced technique that probably isn't necessary here.) &gt; How can i do this? If you really, really must determine the target of a symlink, use [`readlink`](http://man7.org/linux/man-pages/man1/readlink.1.html).
This isn't a bash solution, but it might be helpful... I gave up manually linking my dotfiles (I'd even written various bash and Python scripts to do the work) and started using Stow https://www.gnu.org/software/stow/ ... it does the job much better than I ever could. cd dotfiles stow -t ~ .
 awk 'BEGIN{FS="-"}{split($2,a,"_");if(b&lt;a[2]){b=a[2];c=$0}}END{print c}' filenames.txt
An `-ef` test will tell you if two files are equal, whether hardlinked or symlinked. E.g.: if [[ ~/.bashrc -ef ~/.dotfiles/bashrc ]]; then printf 'They are the same file\n' fi
I switched to GNU Stow after wasting a lot of time trying to build my own solution. It works exceedingly well.
&gt;Thanks, I couldn't quite find a nice way to do it with cut or sort. But thanks for the input.
Thank you very much, that's the sort of neat trick I was looking for, I need to lean more about awk...
Some simple line breaks can clean this type of thing up. You just need to check the return code of each command. Either save the code like in my example or exit I immediately. If it were me, id redirect errors to a log file and the screen, I'd also backup the previous profile so on failure you could restore the original. https://pastebin.com/Bk9b3z0a
Here is how I would do this: * Store the repeated path for source and destination directory into one variable each. Sure, these lines are almost identical, but I I can't think of any way to shorten that that's not overly clever and harder to read. * Side note: no quoting necessary here, as the right-hand side of an assignment does not undergo word splitting and glob expansion. But it wouldn't hurt, either. * Use brace expansion to make the `mkdir` command more compact. If you're not used to it, it might seem cumbersome, but I feel it's a good compromise between readability and compactness. * Use a loop over the changing part of both paths; again, this could be made shorter with some brace expansion trickery, but wouldn't become any more readable. * Put the `cp` command as a condition into an `if` statement; if the command fails, abort with exit status `1`. * If we reach the end of the loop, everything went well and we can return with exit status `0`. srcdir=$HOME/.firejail/firefox-$2/.mozilla/firefox/main destdir=$HOME/.firejail/firefox-$3/.mozilla/firefox/main mkdir -p "$destdir"/{extensions,extension-data,chrome} for dir in chrome extension-data browser-extension-data extensions; do if ! cp -r "$srcdir/$dir"/* "$destdir/$dir"; then echo "Addons not copied" exit 1 fi done echo "Addons copied" exit 0
The most important thing for readability for me is functions. The script has a main() function containing nothing but other functions. By looking at the main you should be able to tell exactly what the script is doing. Also for readability you have the coding style (the google one for bash is what I use) and/or Shellcheck. hth m2c
🙂 ,Effective Awk Programming by Arnold Robbins is a good book for learning awk.
curl and pipe to "uniq -d" would be my guess off the top of my head
Wow, thanks man, works perfectly. I'm obviously no coder and of course I try and use the first thing google can find for me. Really do appreciate the help! Cheers!
egrep '(foo|bar)'
That'll find "foo" or "bar", OP wants to find files with both.
 grep -il word_one | xargs grep -il word_two On mobile, so can't quite verify syntax is correct. The general strategy here is to find all the files that contain the first word, then search those files for the second word. `grep` may have something built-in, but if so I haven't discovered it yet.
You can do grep -ir 'word\|word\|word' or grep -irE 'word|word|word'
This is simple and should work. Good one!
Derp. if [[ $(egrep -ow '(foo|bar)' file | sort -u | wc -l) -gt 1 ]]; then echo "Both patterns found!"; fi
Same idea as /u/v1nny – find files matching the first term, then check if they match the second term: find -type f \ -exec grep -q word_one {} \; -and -exec grep -q word_two {} \; -and -print It’s slightly more robust, since it doesn’t rely on `grep` to print the filenames to be interpreted by `xargs` again. (Yes, you can separate them with null bytes to make the `xargs` version more robust as well, don’t @ me.) The `-and`s are optional, but I think they make it slightly more readable (`-exec` is usually the final action, not a condition). For improved performance, if one of the patterns is much more common than the other one, search for the rare one first so that the second `grep` doesn’t have to run over as many files.
You don't need the parens with egrep do you? I thought you dont
If you want to do a logical 'OR' you need the parens. 
I tested with this here as input: $ printf "abc\ndef\nghi\nabcdef\ndefghi\n" abc def ghi abcdef defghi It's the same here for me: $ printf "abc\ndef\nghi\nabcdef\ndefghi\n" | egrep 'def|ghi' def ghi abcdef defghi deep@hrhr ~ $ printf "abc\ndef\nghi\nabcdef\ndefghi\n" | egrep '(def|ghi)' def ghi abcdef defghi The same parts get highlighted by grep for me. 
TIL! I think I picked up the parens from a style guide somewhere. 
Yeah I was gonna say, I use it every day without the parents lol. I wish there was a solid way to logically and without piping grep into another grep syntax.
Are you saying doing something like this is wrong? What scenario can cause this to be a TOCTTOU Bug? ``` # check symlink already linked correctly elif [ "$(readlink -f $targetdir/$file)" == "$sourcedir/$file" ]; then echo " -&gt; Already corrrectly linked to $sourcedir/$file" continue ```
That misses files where the two words are on the same line.
Try grep [usual options] -e word1 -e word2... [files] Good thing is it also works with inverted search, as in adding -v to options will omit the lines containing any of the word1, word2...
 for f in $(find); do grep -ql word1 $f &amp;&amp; grep -ql word2 &amp;&amp; echo $f; done
Unfortunately multiple -e indicates a logical “OR”, not “AND” like OP wants.
In bash specifically, yes. The latter is a bashism and doesn't necessarily work in other similar shells, whereas the former does. 
Thanks you!
Can you please explain further how it will be a TOCTTOU Bug?
&gt; Can you please explain further how it will be a TOCTTOU Bug? Based on your other, now-deleted, thread it looked like you were intending to create the symlink if it didn't already exist. If your logic is: if the symlink doesn't exist { create the symlink } there is a window between the test and the action where the symlink could be created by something else. That means "create the symlink" has to handle the symlink being there already anyway... which means you need not have bothered testing for its existence in the first place.
Two easiest ways. grep term file ¦ grep term Or better yet awk '/term1/ &amp;&amp; /term2/' file
I happened to be searching for this very thing yesterday. http://tldp.org/LDP/abs/html/io-redirection.html The key to searching for this is to use the term "redirect" or "redirection".
This is worth bookmarking as a tldr version [https://askubuntu.com/questions/420981/how-do-i-save-terminal-output-to-a-file](https://askubuntu.com/questions/420981/how-do-i-save-terminal-output-to-a-file)
&gt; The full goal would be to do a search like: &gt; &gt; grep '6.1.25' /tmp/samplefile &gt; &gt; instead of what it's doing now, which is: &gt; &gt; grep 6.1.25 /tmp/samplefile Both of these mean exactly the same thing. Your problem is not anything that the shell is doing (which you would prevent with quoting), it’s how `grep` interprets the argument (which quoting doesn’t directly affect). If you don’t need any patterns, the easiest solution is probably to use `grep -F` (search for **f**ixed strings). To be on the safe side, also use `-e` to make sure that the argument is interpreted as a pattern, not as flags, even if it begins with a dash. That is: grep -F -e "$uservar" /tmp/samplefile
It is, and more. Give it a try. 
Yay, even in the AUR https://aur.archlinux.org/packages/pycharm-community-eap
Never mind have to install from the official site for the latest version, it has been awhile since I did that. Haha, hope they have instructions.
No, the -o only returns the word, not the line. 
Oh, right!
Ah, but this file foo foo would return a false positive.
In this context, OP likely wants ``-w`` (match whole words) as well, which would stop an argument of ``6.1.2`` from matching ``6.1.25``.
Neat, I didn’t know that option!
date POSIX manual page. -n, -r and -s should be cross compatible and standard IIRC. Also, the whole "+..." Syntax has lots of standard placeholders.
&gt; date POSIX manual page. -n, -r and -s should be cross compatible and standard IIRC. Unfortunately POSIX specifies only `-u`, to perform operations as if `TZ=UTC0`.
Are you talking about [this manpage][posix-date]? Obviously if I restrict this to work only on linux, I can just use the -d flag, and format to my heart's content. But for BSD date, the -d flag sets dst handling. My onhand copy of GNU date (`date --version` gives 8.25) doesn't support the -n flag. It looks like creating a tempfile with the given modified time and using the -r flag might be promising, and certainly a cleaner approach than implementing leap year conversions in bash arithmetic expressions. [posix-date]: http://pubs.opengroup.org/onlinepubs/9699919799/utilities/date.html
&gt; The route I'm thinking of going is requiring modern bash and using printf's %(...)T format, which exposes strftime. The problem is converting from date to an epoch timestamp, which is possible to implement, but ugly and somewhat fragile. Just to clarify, you also need to be able to _parse_ dates into epoch timestamps?
&gt;The problem is converting from date to an epoch timestamp, which is possible to implement, but ugly and somewhat fragile. You got that right # Calculate how many seconds since epoch # Portable version based on http://www.etalabs.net/sh_tricks.html # We strip leading 0's in order to prevent unwanted octal math # This seems terse, but the vars are the same as their 'date' formats epoch() { local y j h m s yo # POSIX portable way to assign all our vars IFS=: read -r y j h m s &lt;&lt;-EOF $(date -u +%Y:%j:%H:%M:%S) EOF # yo = year offset yo=$(( y - 1600 )) y=$(( (yo * 365 + yo / 4 - yo / 100 + yo / 400 + $(( 10#$j )) - 135140) * 86400 )) printf -- '%s\n' "$(( y + ($(( 10#$h )) * 3600) + ($(( 10#$m )) * 60) + $(( 10#$s )) ))" }
Explicitly, the goal is to convert a YYYYMMDD date into a rendered string, which needs to support locale-based stuff (e.g. `%b`). This leaves me with two obvious options: 1. sniff uname + feature detection to use date for rendering 2. somehow convert to epoch timestamps, then use printf with `%(...)T`, which needs an epoch timestamp. I'm asking if there's a better way to do it.
&gt; somehow convert to epoch timestamps So I actually think this is the harder problem. You need to get access to the POSIX `getdate` function somehow, but I can't think of any easy (and portable) way to do that from a shell script.
# From date to epoch date -d '06/12/2017 07:21:22' +"%s" 
Thanks, but &gt; I've seen Frankenstein systems with GNU date before the BSD date on the path, so simple uname sniffing is a non-starter. Do you have a good way to feature sniff short of parsing the help string?
The quickest and easiest way to check if it's a Linux or *BSD system must be by: uname -a | grep -i "Linux" 
&gt; I reckon it is used for the time stamps, but why like this? Does anyone know how to get the same results as with running the history command into the .bash_history file? Those are Unix timestamps, literally the number of seconds since the start of 1970. You could convert them to human-readable timestamps with, say: while read -r line; do if [[ $line =~ ^#([0-9]+) ]]; then printf -v line '# %(%Y-%m-%d %H:%M:%S)T' "${BASH_REMATCH[1]}" fi printf -- '%s\n' "$line" done &lt;.bash_history 
&gt; I know this is vague Very. What does "agent" and "present" and "certain amount" mean?
It's absolutely not the right answer here. It's an interesting challenge though, and that's why I'm doing it.
So what we have is this: when the server hub has agents on it (clients connected to server) systemd will start a daemon so the agents can talk. The agents are in a directory. If there is more than let’s says two agents, we need this daemon to start up. The directory I’m talking about is like an agent manager file that shows you how many agents are running on it. The problem is, this daemon cannot tell whether this server has agents connected to the server when it starts up or if it’s a single server without agents on it. So it has to be started manually. So I need a condition to start a daemon when there are agents on the server. These agents have their own ip or number assigned to them.
So what we have is this: when the server hub has agents on it (clients connected to server) systemd will start a daemon so the agents can talk. The agents are in a directory. If there is more than let’s says two agents, we need this daemon to start up. The directory I’m talking about is like an agent manager file that shows you how many agents are running on it. The problem is, this daemon cannot tell whether this server has agents connected to the server when it starts up or if it’s a single server without agents on it. So it has to be started manually. So I need a condition to start a daemon when there are agents on the server. These agents have their own ip or number assigned to them. You can tell there are agents from a directory that lists them. 
So really you're asking a systemd question, not a Bash question? I still don't understand what you're asking. Let's go through this one bit at a time. &gt; when the server hub A what? Is the server hub some remote system? The local system? A piece of software? A device? &gt; has agents on it (clients connected to server) So agents are client connections? &gt; The agents are in a directory. No, they can't be client connections, because client connections have nothing to do with the filesystem. So what are we talking about here? A file? A line in a file? Something else? What puts these things in the directory? &gt; If there is more than let's says two agents, we need this daemon to start up. Why two? Why not one? Why not zero? And by "daemon" do you mean the server hub, or something else? &gt; The directory I'm talking about is like an agent manager file How can a directory be "like" a file? Did you actually mean file all along -- as in, are you using the word "directory" in the sense of "a list of records", rather than its more specific sense of "a set of files"? &gt; The problem is, this daemon cannot tell whether this server has agents connected to the server when it starts up In "when it starts up", are you referring to the server or daemon? And is that important? Why do you only care when "it" starts up? &gt; or if it's a single server without agents on it. How can it be a server if it doesn't serve anybody? Is this thing not actually a server after all? &gt; These agents have their own ip or number assigned to them. Is that significant?
Interesting approach using 1600 as a reference year. Getting the day of the year is also relatively easy. I'll give it a shot.
This makes sense. Thanks! 
Yes, "How can I make a systemd service, foo.service, automatically start as soon as a particular directory, /path/to/dir, contains more than two files." 
That's a lot simpler! The immediate answer is "you go to /r/systemd or /r/linuxquestions and ask there, because you're on the wrong subreddit". :-) But I can tell you now that this cannot be solved with systemd alone. systemd can start a service as soon as a directory is "not empty", but it can't wait until a certain number of directory entries appear. You might think "OK, I'll just have a script that checks the number of entries and immediately exits if there's too few", but systemd will just want to start your script again. Bad. So let's put systemd out of the picture and think about how this can be solved with a shell script. You probably want to look into using the `inotifywait` utility to do the directory watching, much as systemd would. When woken up you can determine how many entries are in the directory with something like: shopt -s nullglob entries=( /path/to/dir/* ) count=${#entries[@]} Then perform a test on the value in `$count` as necessary.
Here's a sneak peek of /r/systemd using the [top posts](https://np.reddit.com/r/systemd/top/?sort=top&amp;t=year) of the year! \#1: [\[systemd-devel\] \[ANNOUNCE\] systemd 236](https://lists.freedesktop.org/archives/systemd-devel/2017-December/039996.html) | [0 comments](https://np.reddit.com/r/systemd/comments/7k06vl/systemddevel_announce_systemd_236/) \#2: [netdata now monitors systemd-nspawn](https://np.reddit.com/r/systemd/comments/87qihr/netdata_now_monitors_systemdnspawn/) \#3: [Starting sessions with systemd](https://blogs.gnome.org/laney/2018/06/26/starting-sessions-with-systemd/) | [0 comments](https://np.reddit.com/r/systemd/comments/8w6w9z/starting_sessions_with_systemd/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/8wfgsm/blacklist/)
You both rock so much. I'll update the flair to indicate this is resolved. Using -F was super helpful, and then -w even more so. I should have realized that this wasn't a bash issue when the parameter was passed successfully to grep. Thanks again!
Thanks for taking the time to respond. Do you have any insight on a script that would go in, edit foo.service, daemon-reload it, then: restart &lt;service&gt; ?
Replace \`command\` with the command you want to run: \`\`\` store=$(i3-msg -t get\_workspaces \\ | jq '.\[\] | select(.focused==true).name' \\ | cut -d"\\"" -f2) \`\`\`
This might be better done using PowerShell on a Windows host. If you're doing this on an AD-joined Linux host and you have an account that can bind to AD, then I likely have most of the code you need already. `winbind` or `sssd` - what's your poison?
I've done this with gawk. On mobile now, can't get to my scripts library, but you can use mktime in one direction and strftime in the other. Worst case scenario, pass your date into gawk, reformat it, return it, assign to a variable. 
FYI auditing using `bash` history files like this will only get you so far. You really need to setup up `auditd` and `psacct`/`acct` or similar and use those instead.
We would be running our GLOG instance on Ubuntu and can run the script from there. The instance won't be domain joined, but we will be able to run LDAP queries. It would collect all AD information and then the script would dump a CSV file which would be put into the GLOG lookup table as a reference list. Does this make sense? Would your script be possible for this kind of scenario? Appreciate it!
So, are you tring to get the `+` symbols to align with the `|` symbols? If you know a with you want to use you can use something like: printf "| %15s | %15s | %100s |" "version" "update_type" "package_url" Or if you want dynamic lengths you can pre-process your outputs VERSION="4.9.7" TYPE="minor" URL="https://downloads.wordpress.org/release/wordpress-4.9.7-partial-1.zip" printf "| %${#VERSION}s | %${#TYPE}s | %${#URL}s |" "${VERSION}" "${TYPE}" "${URL}" Is that something like what you are looking for? 
I was looking for the same thing recently. I noticed that "Learning the bash Shell" covers this topic in Appendix C. "[Loadable Built-Ins](https://www.safaribooksonline.com/library/view/learning-the-bash/1565923472/apc.html)". They mention that example code can be found in the bash [tarball](http://ftp.gnu.org/gnu/bash/bash-4.4.18.tar.gz) under "bash-4.4.18/examples/loadables". Hope that helps.
OK wow thanks, this should work yes
I was messing around, and created a function wrapper for it. I may try to make it more generic for my own uses later. https://ideone.com/Zr4E24
They're timestamps counting epoch seconds, but they don't have to be. There's an environment variable called HISTTIMEFORMAT that takes a date format (e.g. 'HISTTIMEFORMAT="+%Y%m%d-%H:%M:%S"')
I have this set in /etc/profile Still .bash_history files show the UNIX timestamps
Perfect! This is what we need in the situation. Why I didn't know about this earlier... Thank you!
 \w/
┌─[root@Fedora]─[~]─[09:31 am]- └─[$]› echo $PS1 \n\[\e[38;5;70m\]┌─[\[\e[0m\]\[\e[38;5;196m\]\u\[\e[0m\]\[\e[38;5;40m\]@\[\e[0m\]\[\e[38;5;196m\]Fedora\[\e[0m\]\[\e[38;5;70m\]]─[\[\e[0m\]\[\e[1;34m\]\w\[\e[0m\]\[\e[38;5;70m\]]\[\e[38;5;70m\]─[\[\e[0m\]\[\e[0;31m\]\@\[\e[0m\]\[\e[38;5;70m\]]-\[\e[0m\]\n\[\e[38;5;70m\]└─[\[\e[0m\]\[\e[1;37m\]$\[\e[0m\]\[\e[38;5;70m\]]› \[\e[0m\] 
To clarify: GLOG as in - Google Logging? Regardless, if it's not domain joined, then using winbind/sssd tools doesn't really make sense. I mean it's doable, but you'd probably be better off with raw ldapsearches.
Hah I was so close. Thank you. It works
Thanks man, that's great. The book appendix is pretty brief, but the source examples are very concise and readable; looks like there's some useful stuff in there. For things like assoc arrays I think it's gonna come down to RTFSRC, which is not entirely surprising. 
Sorry glog as in gray log. 
Funny, I was just having the same problem yesterday. DATA="{\"key\": \"${VALUE}\"}" curl -i -X POST -H 'Content-Type: application/json' -d "$DATA" https://site.com
Thank you for your reply. I was saying what I posted above actually works. It was more of a question of why it does. 
I don’t understand the question. The variable is in double quotes, not single quotes.
In bash, when two words touch each other, they get fused into a single word. You can do things like this here, for example: $ echo abcdef abcdef $ echo 'abc''def' abcdef $ echo "abc""def" abcdef Now in your command line for 'curl', the confusing parameter is this one here: '{"text": "message: '"$size"'"}' Bash will see three words: '{"text": "message: ' "$size" '"}' After bash works through the quotes and the variable, it will see these three words (let's say that $size is "123"): {"text": "message: 123 "} And then last, because the three words touch each other, they get fused into a single word for the 'curl' command line: {"text": "message: 123"}
It works because you're closing the first single quote. '{"text": "message: ' "$size"'"}' 
Oh, my mistake.
\[\e[1;34m\]\u@\[\e[1;91m\]\h\[\e[m\] \[\e[1;34m\]\w\[\e[m\] \[\e[1;34m\]\$\[\e[m\] \[\e[1;37m\]
`\[$(tput bold)\]\h\[$(tput sgr0)\]:\w\$`
OK. That's simple enough. Thank you. 
Thank you everyone for your responses. It makes sense now. 
 \[\033[0;37m\]\342\224\214\342\224\200$([[ $? != 0 ]] &amp;&amp; echo "[\[\033[0;31m\]\342\234\227\[\033[0;37m\]]\342\224\200")[\[\033[0;33m\]\u\[\033[0;37m\]@\[\033[0;96m\]\h\[\033[0;37m\]]\342\224\200[\[\033[0;32m\]\w\[\033[0;37m\]]\n\[\033[0;37m\]\342\224\224\342\224\200\342\224\200\342\225\274 \[\033[0m\]
 [000] luke@build64-par:~ $ echo "${PS1@Q}" $'\\[\E[1m\\][$(v=$?; [[ $v = 0 ]] &amp;&amp; c=\'\\[\E[38;5;2m\\]\' || c=\'\\[\E[38;5;1m\\]\'; printf %s%03i $c $v)\\[\E[m\E(B\\]\\[\E[1m\\]]\\[\E[m\E(B\\] \\[\E[1m\\]\\[\E[38;5;2m\\]\\u@\\h\\[\E[38;5;4m\\]:\\w\\[\E[m\E(B\\]\\[\E]0;\\u@\\h:\\w\a\\]\\n\\$ ' Pro tip: To correctly copy escape codes and such, use `printf %q`, which will insert the appropriate quoting/escapes; or if you have 4.4+, use @Q as shorthand: $ printf '%q\n' "$PS1" $ echo "${PS1@Q}"
\t \u@\W
I use WP-CLI for the similar tasks. It's damn useful! Are you redirecting the output to a file and this is where the formatting gets lost? One way I work around this is using `column -t` another way is from https://github.com/wp-cli/wp-cli/issues/1102 and it's to use SHELL_PIPE=0. You can find more examples in the WP-CLI scripts here https://guides.wp-bullet.com
/etc/profile is only read by login shells. (/etc/profile then ~/.bash_profile then ~/.bash_login then ~/.profile) for login shells. If not a login shell, bash only reads ~/.bashrc. maybe that's it?
 \n\[\e[0;1;31m\][\[\e[0;1;36m\]master\[\e[0;1;31m\]]\[\e[0;m\]\[\e[0;1;31m\][\[\e[0;1;35m\]S\[\e[0;1;31m\]]\[\e[0;1;36m\]\[\e[0;1;31m\][\[\e[0;1;36m\]\u\[\e[0;1;31m\]@\[\e[0;1;36m\]\h\[\e[0;1;31m\]:\[\e[0;1;36m\]\w\[\e[0;1;31m\]]\[\e[0;m\]\n\[\e[0;1;31m\][\[\e[0;1;36m\]09\[\e[0;1;31m\]:\[\e[0;1;36m\]23\[\e[0;1;31m\]:\[\e[0;1;36m\]48\[\e[0;1;31m\]]\[\e[0;m\] \[\e[0;m\] ► 
 \h:\W \u\$ Mac user here.
\[davey@dell-arch\]\[\~\] $ echo $PS1 \\\[\\033\[0;31m\\\]\[\\\[$(tput sgr0)\\\]\\u@\\h\\\[\\033\[0;31m\\\]\]\[\\\[$(tput sgr0)\\\]\\w\\\[\\033\[0;31m\\\]\]\\e\[0m\\n\\$
 \[\033[0;96m\]\T\[\033[0m\] $
 \[\033[0;96m\]\T\[\033[0m\] $
 \[\033[0;31m\]\[\033[1;31m\]\[\033[1;30m\](\[\033[1;31m\]\u\[\033[0;31m\]@\[\033[1;31m\]\h\[\033[1;30m\]) -&gt; \[\033[0m\]\[\033[0;31m\]$(__git_ps1 "${HOTBLK2}(${REGRED2}%s${HOTBLK2})")\[\033[0m\]\n\[\033[0;31m\]\[\033[1;31m\]\[\033[1;30m\](\[\033[0;31m\]\w\[\033[1;30m\])\[\033[0;31m\]&gt;\[\033[0m\]
Either pass the script's arguments into your function, e.g.: some_function "$@" or save the script's arguments in a global variable: declare -ar args=( "$@" ) Note that this array is zero-indexed, so to get the first argument you would use `"${args[0]}". 
Stole it from a guy named demongoat 20 years ago on IRC. Only added git bash prompt.
I would call the function with parameters, instead of using a global variable. I mean something like this: function_name() { local dir for dir in "$@"; do ... done } ... function_name "${@:2}"
What's wrong with the following? function_test() { for profile in "${1}"; do # ... } function_test "${@:2}" Then I do `test_script create a b c` but only `a` is created. So `function_test "${@:2}"` is supposed to pass `function_test` the array of all arguments to the script starting at the 2nd argument to the function. Then in the function, shouldn't `${1}` refer to that array, since it's the only argument for that function? Why does it apparently only pass the 1st element of the array?
Thanks. Question though: I read that an array is simply a variable with multiple values, so why is it in `function_name` I don't refer to the array `${@:2}` that is passed to it (as the first and only argument) as `"${1}"`? Instead, `"${1}"` in `function_name` refers to the first element of the array of `${@:2}`.
The simplest way is probably to use the builtin BASH_ARGV array variable (since bash 4 at least I think), though it's not portable at all but well neither is the declare builtin.
Bash unfortunately doesn't work the same way as other programming languages. So first yes an array is simply a variable, but function arguments can only pass variable via names or value, you cannot actually pass an array reference. About passing multiple arguments, the easiest way to understand how bash works is to try to see how your function arguments will be expanded before being passed, and to do that you need to understand the difference between "$@" and "$\*": the first will be expanded by each separate argument, and the latter by a concatenation of all arguments separated by the first character of the IFS variable. So a few examples about this: ``` a=( a0 a1 ) b=( b0 b1 ) c=( c0 c1 ) f() { for arg do echo "$arg" done } f "${a}" "${b[@]}" "${c[*]}" ``` So here if you try to think how the function call will be expanded this will give: ``` f "${a}" "${b[@]}" "${c[*]}" f 'a0' 'b0' 'b1' 'c0 c1' ``` Will output: ``` a0 b0 b1 c0 c1 ``` If you know python the "$@" is roughly similar to calling a function with *args, it expands the list content to each argument instead of copying or passing a reference. Now there are some ways to actually pass a variable by reference, in bash the declare builtin has the -n option to create a variable which is a reference of another one (there is also the form `${!var}" but is more limited). Second solution is to eval, but you need to know what you are doing when using eval: ``` g() { declare -n a_reference="$1" # first solution echo "${a_reference[1]}" eval echo '${'"$1"'[1]}' # second solution } g a ``` Output: ``` a1 a1 ``` About local, I'd say it's a matter of taste and coding style, but be aware that local is POSIX, but well arrays aren't either.
It depends. My prompt automatically switches between six different modes based on terminal width and whether or not I'm root. Using `printf '%q\n' "${PS1}"`: `$'\\[\E[31m\\]▓▒░[$(date +%y%m%d/%H:%M)][LCL]\\[\E(B\E[m\\]\\[\E[32m\\][\\u@\\h\\[\E(B\E[m\\] \\W\\[\E[32m\\]]\\[\E(B\E[m\\]$ '` `$'\\[\E[31m\\]▓▒░\\[\E[32m\\][\\u@\\h\\[\E(B\E[m\\] \\W\\[\E[32m\\]]\\[\E(B\E[m\\]$ '` `$'\\[\E[31m\\]▓▒░\\[\E(B\E[m\\]$ '` ![Alt text](https://cdn.rawgit.com/rawiriblundell/dotfiles/master/termtosvg_e_udnoec.svg "setprompt demonstration") 
&gt; the simplest way That's debatable. First, in order to get the script's own arguments the script needs to have been _started_ with the `extdebug` shell option enabled, and as far as I know that can only be achieved by executing Bash with the `--debugg
Just looked at the documentation you are definitely right, though I gave it a try: $ bash -c 'shopt | grep extdebug; declare -p BASH_ARGV' bash a b c extdebug off declare -a BASH_ARGV=([0]="c" [1]="b" [2]="a") So that's kind of weird, it seems to be working by default and with extdebug being disabled, not sure what going on here. Totally agreed on the rest.
Hmm, yeah. Weird. I just checked the source code, and it looks like the code that pushes and pops values into the arrays is guarded by a `debugging_mode` test... but only on function calls, not for the script's own arguments. I'm not sure I'd want to rely on that. I suspect it's a bug.
Well that's interesting, thanks for digging :)
i like \`godmode\`, great name
Well, if shell functions count: mkcd() { mkdir "$@" || return shift "$(( $# - 1 ))" cd -- "$1" } Make any number of directories, passing options to mkdir. Then cd to the last-mentioned directory. (note: options need to come before operands, as in proper unix -- GNU-style options after operands won't work) 
Best alias I’ve ever seen: alias fucking=sudo 
This here makes 'man' use a limited line length when started in a very large terminal window. It makes things easier to read in wide terminal windows, and it makes it so 'man' hopefully doesn't have to be restarted if I decide to resize the window: man () { local width="${COLUMNS:-100}" (( width &gt; 100 )) &amp;&amp; width=100 MANWIDTH="$width" command man "$@" } 
Heh... yeah, it's a subtle nod to Quake, Doom and the like :D
alias stfu=shutdown -P now
Here’s a trio that I use regularly: Secure “telnet” - handy when I need to debug SSL connections like IMAPS/SMTPS etc. alias stelnet='openssl s_client -quiet -connect' This one is a simple hack to get my external IP: alias whatsmyip='curl ipecho.net/plain; echo' Finally, download YouTube videos with sensible titles etc: alias youtube-dl='youtube-dl -o '\''%(title)s.%(ext)s'\''' 
I have a whole bunch, these three have seen a lot of use lately though: \# Convert comma separated list to long format e.g. id user | tr "," "\\n" \# See also n2c() for the opposite behaviour c2n() { while read -r; do printf -- '&amp;#37;s\\n' "${REPLY}" | tr "," "\\\\n" done &lt; "${1:-/dev/stdin}" } \# Wrap long comma separated lists by element count (default: 8 elements) csvwrap() { export splitCount="${1:-8}" perl -pe 's{,}{++$n &amp;#37; $ENV{splitCount} ? $&amp; : ",\\\\\\n"}ge' unset splitCount } \# Convert multiple lines to comma separated format \# See also c2n() for the opposite behaviour n2c() { paste -sd ',' "${1:--}"; } I've been using those as part of cleaning up a bunch of sudoers files that previous sysadmins have neglected. So let's say, for example, there's a user alias with a bunch of accounts listed out of order in one line e.g. \`User\_Alias PANTS = batman,kong,sigh,knelt,spotty,sweat,airborne,caregiver,lawyer,mistaken,dashboard,behind,tides,whom,stapler,sleep,specimen,appear,unpainted,afloat,ancient,dingbat,adult,aiming,waiting,note,catcall,mortify,render,halt,gravitate,quest,loosen,yanks,dent,sliced,lugged,draper,curved,sprig\` Essentially what the above functions can do is to break that line down, maybe some extra parsing with some other functions like \`trim()\` get used and then build it back up to something readable e.g. ▓▒░$ echo "$line" | c2n | sort | n2c | csvwrap | indent adult,afloat,aiming,airborne,ancient,appear,batman,behind,\\ caregiver,catcall,curved,dashboard,dent,dingbat,draper,gravitate,\\ halt,knelt,kong,lawyer,loosen,lugged,mistaken,mortify,\\ note,quest,render,sigh,sleep,sliced,specimen,spotty,\\ sprig,stapler,sweat,tides,unpainted,waiting,whom,yanks So now we get more readable sudoers files e.g. User\_Alias PANTS = \\ adult,afloat,aiming,airborne,ancient,appear,batman,behind,\\ caregiver,catcall,curved,dashboard,dent,dingbat,draper,gravitate,\\ halt,knelt,kong,lawyer,loosen,lugged,mistaken,mortify,\\ note,quest,render,sigh,sleep,sliced,specimen,spotty,\\ sprig,stapler,sweat,tides,unpainted,waiting,whom,yanks As far as aliases go, these are Linux specific: alias diff='diff -W $(( $(tput cols) - 2 ))' alias sdiff='sdiff -w $(( $(tput cols) - 2 ))'
Fantastic, it fits everything
sudo bang bang 
I feel like you may like [this.](https://github.com/nvbn/thefuck)
damn those aliases are long... `alias clr='tput reset' #quickly clears terminal and rids scrollback` `alias pr='sudo pacman -Rsnc'` 
True - longer than 2-3 characters, but much shorter than the commands they reproduce and eminently memorable. Besides, tab completion shrinks nearly all of them to 3-4 keystrokes anyway. I like the ‘clr’ alias - that’s neat! Mind if I steal it?
don't mind at all
Its kinda boring but: alias hgrep="history | grep"
Wow=“git status” Very=git Much=git Such=git Wow Very pull Such commit Much push
I don't like that I have to use this one, but it's handy: alias net='sudo systemctl restart network-manager.service &amp;&amp; nmcli radio wifi off &amp;&amp; sleep 5 &amp;&amp; nmcli radio wifi on' Found this one on reddit: alias up='for ARG in update upgrade autoremove; do sudo apt ${ARG};done' I have a bunch of youtube-dl aliases like this one: alias ytda='youtube-dl -f bestaudio -o "/home/x/Music/%(title)s.%(ext)s"' I like this one from the ranger wiki: alias r='ranger --choosedir=/home/x/.rangerdir "$(cat /home/x/.rangerdir)" || ranger --choosedir=/home/x/.rangerdir' Search and play the first result from youtube in mpv (tsp to queue the command) function vv { tsp mpv ytdl://ytsearch:"$*" } Simple brightness control function br { sudo brightnessctl -q s "$1"% }
I don't like that I have to use this one, but it's handy: alias net='sudo systemctl restart network-manager.service &amp;&amp; nmcli radio wifi off &amp;&amp; sleep 5 &amp;&amp; nmcli radio wifi on' Found this one on reddit: alias up='for ARG in update upgrade autoremove; do sudo apt ${ARG};done' I have a bunch of youtube-dl aliases like this one: alias ytda='youtube-dl -f bestaudio -o "/home/x/Music/%(title)s.%(ext)s"' I like this one from the ranger wiki: alias r='ranger --choosedir=/home/x/.rangerdir "$(cat /home/x/.rangerdir)" || ranger --choosedir=/home/x/.rangerdir' Search and play the first result from youtube in mpv (tsp to queue the command) function vv { tsp mpv ytdl://ytsearch:"$*" } Simple brightness control function br { sudo brightnessctl -q s "$1"% }
That's pretty nice, I'm sometimes annoyed at wide man pages! Not that it matters much, but you could do away with the temp variable: man () { MANWIDTH=$(( ${COLUMNS:-100} &gt; 100 ? 100 : COLUMNS )) command man "$@"; }
 cdl () { local cands=("$1"*/); cd "${cands[-1]}" } I use this to cd into the (lexically) last directory that starts with the argument. Say I have directories `dir1`, `dir2` and `dir3`, I can use cdl dir and I end up in `dir3`. I wrote that when I repeatedly had to cd into one of many directories that had a timestamp appended, and I usually was interested in just the newest one. Notice that negative indexing requires Bash 4.2 or newer. If that's not available, one can use cd "${cands[${#cands[@]}-1]}" instead.
Give me a diff between two files, in two columns, but don't show me the lines that are the same: alias cdiff='diff -wayd -W$(tput cols) --suppress-common-lines'
100 char... What kind of unholy setting is that? 80 char width master race!
I used to have that until I discovered it was the culprit behind my shell taking so damn long to start up every time I opened a new terminal
alias NEW='sudo apt update &amp;&amp; sudo apt upgrade &amp;&amp; sudo apt dist-upgrade &amp;&amp; sudo apt autoremove'
Find only symbolic links in current directory: `alias sym="find . -maxdepth 1 -type l -ls"` I also have these two, that either opens the current directory in another terminal window, or in my filebrowser: `termdir() { konsole -e "pwd"; }` `alias filedir="nautilus $(pwd)&amp;"` Make an ssh-tunnel through an already known server: `st() { ssh -i ~/.ssh/id_ed25519 -L "$1":127.0.0.1:"$2" [insert server address here]; }` Quick access to your i3 config file through either nano or Sublime Text: `alias confs="subl ~/.config/i3/config"` `alias confn="nano ~/.config/i3/config"` Show file out put like ls, but show the file rights in octal: `lo() { if [ $# -eq 0 ]` `then` `ls -al | awk '{k=0;for(i=0;i&lt;=8;i++)k+=((substr($1,i+2,1)~/[rwx]/) \` `*2^(8-i));if(k)printf("%0o ",k);print}'` `else` `ls -al "$1" | awk '{k=0;for(i=0;i&lt;=8;i++)k+=((substr($1,i+2,1)~/[rwx]/) \` `*2^(8-i));if(k)printf("%0o ",k);print}'` `fi` `}`
alias ggwp='git add -A &amp;&amp; git commit -m "Done" &amp;&amp; git push' function vbc { open ${@:-.} -a Visual\\ Studio\\ Code; } function sub { open ${@:-.} -a Sublime\\ Text; } function tree { find . -type d -maxdepth ${@:-2} | sed -e "s/\[\^-\]\[\^\\/\]\*\\// |/g" -e "s/|\\(\[\^ \]\\)/|-\\1/" } function clean\_ds { find ${@:-\~} -name ".DS\_Store" -delete }
 alias lslan='sudo nmap -sn &lt;network-pi-address&gt;/24' Nothing really special, but it's something that saved me from quite a bit of typing as I do this all the time.
`alias ..='cd ..'` `alias sl='ls'` `alias gittree="git log --graph --all --abbrev-commit --decorate --format=format:'%C(bold blue)%h%C(reset) - %C(bold green)(%ar)%C(reset) %C(white)%s%C(reset) %C(dim white)- %an%C(reset)%C(bold yellow)%d%C(reset)'"` The first is just convenient, the second prevents race conditions when typing, and the third is so useful I cry whenever I'm on someone else's computer and can't use it. It shows you a colored chronological tree of all your commits, complete with branches.
not an alias but a function: psgrep() { ps axuww | sed -n -e 1p -e "/sed.*$*/d" -e "/$*/p" } But an alias which i use very often is `alias ta='tmux at -d'`
&gt;alias clr='tput reset' #quickly clears terminal and rids scrollback Seems no different to `clear` \-- which I have an alias for as '`cls`' (from DOS)
That line checks if that file exists.
It tests the file to determine if it exists and is a regular file.
You should use zsh and `setopt autocd`.
`cd() { cd "$1" &amp;&amp; ls }` Simple, but eliminates the most common operation otherwise. Combine with zsh's `setopt autocd` for best effect, so you can just type a directory (eg `..`) to cd into it without an ls.
lolnoobs. 
Why?
Unless you are using fancy flags on grep, try doing C^r on the command line, it live searches for previous commands.
I didn't know about that that, thanks! That said, I usually need to wind up filtering quite a bit because I'll run many very similar commands. Being able to string a few greps, grep -v's an egreps tends to be more efficient. I also often want to grab multiple past commands at the same time for lab protocols so scrolling through past commands one at a time can get tedious.
You can think of `[ ... args ... ]` as a fancy way to run the program, `test` with arguments, `...args...`. `test -f ~/.bash_aliases` succeeds if the file, ~/.bash_aliases` exists. For more details on `test` run or Google `man test`. The `if` statement will have the shell run the commands after the `then` only if the command before the semi-colon succeeds. Combining these deets, the whole contraption will source the `~/.bash_aliases` file if it exists.
Bro the default shell on MacOS is already bash... And you may want to remove your IP address from your post. You seem to be using a public resource for your Mac, or a previously used public resource considering your *hostname* is your IP address. If you want to change the beginning portion of your bash login to something different simply use the `hostname` command 
Your default shell is bash. If you want to change the prompt, you can run `bash --noprofile --norc` in a regular terminal. This runs a shell with zero customization. This will change the prompt to look as as you wanted. However, some aliases and functions will not be available. 
Do you want to change your PS1?
172.18.#.# is in the 172.16.0.0/12 subnet, which is a private LAN subnet Telling him not to post his 172.18.0.0 IP is like telling him not to post his 192.168.#.# address. Also the "hostname" command will only temporarily change his Mac's host name, the correct command to do it permanently is: sudo scutil --set HostName newhostnamehere 
First, `bash` is the default shell on macOS. If you want to check it or change it, go to your Terminal preferences, to the first tab (General or Startup), and look at the "Shells open with" option. You can select the default login shell, or a full path to a shell (e.g. `/bin/bash`). Second, the text on your prompt is defined by the `PS1` variable. Change the variable and it'll change. Maybe what you want is `PS1='\s-\v$ '`.
Lol 172.18 isn't a public ip
No shit, still bad form to go about handing any of that information over online. 
I forget what most of this does but I use it 100 times a day so it must be doing something for me: type cd cd is a function cd () { if (( $# &gt; 0 )); then if [[ "$1" == "-" ]]; then builtin cd -; else if [[ "$1" == "+" ]]; then builtin pushd &gt; /dev/null 2&gt;&amp;1; else if [[ "$1" == "--" ]]; then dirs -v; return $?; else if [[ "$1" == "." ]]; then builtin pushd "`pwd -P`" &gt; /dev/null 2&gt;&amp;1; else builtin pushd "$@" &gt; /dev/null 2&gt;&amp;1; fi; fi; fi; fi; else builtin cd "${HOME}"; fi; if [[ "$?" -ne 0 ]]; then builtin cd "$1"; fi; pwd }
 curl ipinfo.io A bit more than just your IP - also includes hostname, ISP and location data. You can use `ipinfo.io/ip` for just the IP, and `ipinfo.io/8.8.8.8` etc.
I got a few grep ones alias igrep='grep -iP' alias vgrep='grep -vP' alias ivgrep='grep -viP' `P` uses perl regular expressions which I usually want so I included that by default. I must say I usually forget about the aliases and just type the flags though.
LOL there was never a debate whether or not sharing your subnet IP is a safety concern or not, the objective was to educate OP on the fact that the ip was included in the message, since clearly he/she was unaware. Better to get in the habit of being conscious of something like that than to assume la-dee-da lets all of us share our subnets, since what's the harm right?
Cool. I use the `whatsmyip` alias to send to colleagues when they need to do external testing remotely so all they need is the IP address and nothing else. Still, ipinfo.io is a good one to stick in the toolbox too. Thanks!
The word you're probably looking for is 'sandbox'. Look into `chroot` and its descendants.
http://www.grymoire.com/Unix/Awk.html
Another cd alias. Allows user to specify the number of levels to go up (e.g. \~$: up 3) alias up='main () { cd $(printf "&amp;#37;0.s../" $(seq 1 $1));}; main $1'
thanks! Just having a bit of an issue with having a for/if loop inside of awk now 
Not sure if this is any help, but I ran into issues converting dates on GNU/Linux, BSD, &amp; MacOS (Dawrin). My solution was based on "OS sniffing" which you had issues with, but works fine for me. Code is here: https://github.com/oodsway/logsum/blob/master/logsum Line 87: function format_date () which uses case-esac based on OS.
u/ASIC_SP(i think) created a great set of tutorials for text processing utils. https://github.com/learnbyexample/Command-line-text-processing
thanks for the feedback and recommending my tutorial :) /u/AllenIverson3, see [Control Structures](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_awk.md#control-structures) section for some examples on if-else and loops I'd also suggest to see [Filtering](https://github.com/learnbyexample/Command-line-text-processing/blob/master/gnu_awk.md#filtering) section first - it has examples for idiomatic awk usage and shows how they are equivalent to code written explicitly
for some terminals when you use clear and scrollback there is still text at the top... tput reset fixes that
Ask a question rather than asking to ask a question. No shortage of people who will offer assistance.
FYI - the computation here is incorrect for leap years. Specifically, it will fail for all dates between Feb 29 and Dec 31 in a leap year because it counts the leap day twice (once in `%j` and again in the leap day computation). The simple solution is to adjust the day of the year to not include a leap day: j=$(( 10#$j &lt; 60 ? 10#$j : 10#$j - (yo % 4 == 0 &amp;&amp; (yo % 100 != 0 || yo % 400 == 0)) )) 
100 char... When you are not limited with the size of IBM punch card! 
I have created this command while a go, I call it "cd to". function cdt () { INPUT="$1"; if [ -d "$INPUT" ]; then cd "$INPUT"; elif [ -e "$INPUT" ]; then cd "$(dirname $INPUT)"; else cd $INPUT; fi } It is handy when you need to change directory and do not car it the last item of path is the file or directory. It is especially handy when you for example searching for a file and the very next thing is to jump to that location: alias findfile='find . -type f | grep -E ' $ findfile my_precious_file.txt ./deployment/scripts/maintenance/docker.py $ cdt `!!` Another handy alias is to "change directory to git-repo root": alias cdgr='cd $(git rev-parse --show-cdup) ' 
If you want to fool someone or something, Docker will make this fairly easy. `docker run -it ubuntu:latest` will give you a shell where you can mess around a self-contained filesystem. With some extra work, you can redirect all network traffic to a single host of your choice. If you want to honeypot evil hax0rz, you can do much the same with a virtual machine for improved isolation. 
Excellent! Looking forward to seeing your updates, and if it's ok with you I'll shoot an email to Rich from http://www.etalabs.net/sh_tricks.html to let him know...
Pushed up the [latest version][0] just now. I opted to inline the day of year computation so I was able to simply omit the leap day adjustment. I could see turning the function into a full utility, but I don't have any need for it, so it's not a priority for me. [0]: https://github.com/stevenkaras/bashfiles/blob/master/bin/organize
I have a lot of VMs and network pcs that are started through wakeonlan or over the network. Repeatedly ssh'ing when the systems are not yet up gets old so this alias will use netcat to ping the ip until the system is live and then logs in. alias sshborg='while ! nc -vz 192.168.1.1 22; do sleep 3; done &amp;&amp; ssh -X -i ~/.ssh/passkey user@192.168.1.1 -p22'
&gt; psgrep() { ps axuww | sed -n -e 1p -e "/sed.*$*/d" -e "/$*/p" } That deletes all lines containing "sed". What if you were "grep-ing" for sed? Here's a better version, using grep: psgrep() { ps auxww | { read -r header; printf '%s\n' "$header"; grep "$@"; } } It avoids the need to filter out the grep itself from ps's output, since grep isn't running at the moment ps reads the process table; `read` is blocking waiting to read the header line at that point.
That's called infinite recursion.
Ive only used it very basically so far but I want to look at automating it all so I can just run a shell script and let it run through all the sites and check/update everything. Those guides look perfect though, thanks!
Okay. I just SSHd into my Pi and it ran just as you said ... Initially tried on my MacOS which showed no difference to clear. TIL :P.
Get ready to enjoy the ride :). That is a very good idea, there is such a script on that site already I believe that loops through a folder structure in /var/www and does a backup before updating everything, maybe you can use that as a base.
it should only filter out a sed line with the arguments, not just any sed. But timtowtdi and i kinda like your solution more :)
&gt; it should only filter out a sed line with the arguments Ah my bad, I read the `$*` in there as part of the regex, which would match zero or more `$` characters, but of course it's double quoted, so the shell expands the `$*`. Still, injecting data into sed is best avoided.
i originally made it for csh in which it's i think the only way to do it without resorting to a separate script. That alias is `alias psgrep "ps axuww | sed -n -e 1p -e '/sed.*\!*/d' -e '/\!*/p'"` i didn't really think about it much when i converted it. I'm going to use your suggestion however :)
Sure, Bash's `read` builtin can do this: while read -s -r -N 1 char; do printf 'You entered: &lt;%s&gt;\n' "$char" done Control sequences will typically be mapped to low-valued ASCII characters. Other non-printable keys and key combinations will be read as sequences of characters.
Thanks. The signals bit is interesting but thankfully I don't need to worry about them, it looks like. I actually just discovered the read command right before reading your comment. I've been messing around with it and there's only one hangup, do you know how to get access to the input in C? I can use the stdlib system() function to execute the command, and getenv() to read environment variables, but it doesn't seem to save the variables outside of the system() call. Don't worry about it if you don't, this is progress enough for one night.
read(0,buffer,10) ; will read 10 characters (or less) from standard input. If you are using bash, but you need to provide the input to a C program that bash is going to call: program "$VARIABLE" or export VARIABLE program One of the obscure things about unix is the concept of cooking and the tty device driver and raw input. If you type the command "tty" you will see what tty device driver is processing your keyboard input. stty -a will show you all the settings of the tty device driver. The tty device driver controls whether your keys are processed immediately by the program or whether you keys are grouped together and processed when you press enter. stty raw is one way to turn off the "cooking" of the tty device driver. 
Use something built into the C standard library. Perhaps "getchar()" would be good for a single key press?
&gt; do you know how to get access to the input in C? To do this in C you will probably want to put the terminal in "raw" mode. Read up on `tcgetattr`, `tcsetattr` and `cfmakeraw`.
If you're just trying to record (and perhaps later play back) keyboard input, is perhaps the [`script` command](http://man7.org/linux/man-pages/man1/script.1.html) what you're looking for?
 \n\W \j
I'll look into that, thanks.
Alright, lots of things to look into. Thanks for all the info!
Nope. All the standard IO functions read from stdin, but with the default terminal settings, keyboard input is buffered and not sent to stdin till it sees a newline.
Wiki: https://wiki.termux.com Community forum: https://termux.com/community IRC channel: #termux on freenode Gitter chat: https://gitter.im/termux/termux Mailing list: termux+subscribe@groups.io Search packages: pkg search &lt;query&gt; Install a package: pkg install &lt;package&gt; Upgrade packages: pkg upgrade Learn more: pkg help $ &amp;qed bash: syntax error near unexpected token `&amp;' $ &amp;=[and] bash: syntax error near unexpected token `&amp;' $
Wiki: https://wiki.termux.com Community forum: https://termux.com/community IRC channel: #termux on freenode Gitter chat: https://gitter.im/termux/termux Mailing list: termux+subscribe@groups.io Search packages: pkg search &lt;query&gt; Install a package: pkg install &lt;package&gt; Upgrade packages: pkg upgrade Learn more: pkg help $ &amp;qed bash: syntax error near unexpected token `&amp;' $ &amp;=[and] bash: syntax error near unexpected token `&amp;' $
r/commandline
Funnily enough, this wasn't the plan. The plan was to make a sandbox for a game I'm trying to make. So... I suppose that WAS the plan, but with different intentions. Thanks!
Not bad, could save you a couple minutes each time you make a repo. The logic assumes you're in your git rep's root dir. You could replace \`\[\[ -d .git \]\]\` with something like \`\[\[ -n $(git status -uno 2&gt;/dev/null) \]\]' and it would work in sub-directories too. 
 if [[ -d .git ]]; then ... else echo "Not a git repo." fi will incorrectly fail if you are in a subdirectory of your repository. if [[ $(git rev-parse --is-inside-work-tree 2&gt; /dev/null) == 'true' ]]; then will tell you if you are in a valid git repo. If you want to work from the top level directory you can do cd $(git rev-parse --show-toplevel)
Excellent point!
Excellent points!
You do not have things like $1 available for an alias. An alias is just text replacement for the command lines you type. You should translate your alias into a "function". It would look like this: tit () { tail -f $(ls -t | egrep -e [0-9].log | head -1) | grep "$@" } You put this into your .bashrc, same as with an alias. Where you see an "$@" in this function, that's where bash will insert the arguments you type when you call the 'tit' command.
You can write it as a shell function instead. You can pass values into functions and take advantage of environment variables. They are much more flexible than aliases.
As soon as I posted this I remembered that functions were possible in your .profile. I'm a bit rusty, it seems. Also, it needs to be an if statement within: tit(){ if ! [ -n "$1" ]; then tail -f $(ls -t | egrep -e '[0-9].log$' | head -1); else tail -f $(ls -t | egrep -e '[0-9].log$' | head -1) | grep $1; fi } 
&gt;You can write it as a shell function instead. You can pass values into functions and take advantage of environment variables. They are much more flexible than aliases. See above. I had a brain fart :)
If anyone cares, I updated it with the suggestions mentioned here: gp() { if [[ -n $(git status -uno 2&gt;/dev/null) ]] then read -p "Enter GitHub username: " username currentBranch=$(git branch | grep \* | cut -d ' ' -f2) status=$(git push 2&gt;&amp;1 | tail -n 2) if [[ "$status" == *"&lt;name&gt;"* ]] then newGit=$(curl -u "$username" https://api.github.com/user/repos -d "{\"name\": \"$(git rev-parse --show-toplevel | grep -o [^\/]*$)\" }") gitLink=$(echo $newGit | python -c "import sys, json; print(json.load(sys.stdin)['clone_url'])") $(git remote add origin $gitLink) echo "new remote made at $gitLink" output=$(git push -u origin $currentBranch) echo $output elif [[ "$status" == *"--set-upstream"* ]] then eval $status elif [[ "$status" == *"up-to-date"* ]] then echo "up to date" else echo "pushing to remote" $(git push) fi else echo "Not a git repository" fi } Basically the same, but works from within the git repo
Link count. 
What are the advances of having functions instead of 'real' shell script. I tend to use scripts as I then can easily edit them later and expand as needed. Also keeps things separated nicely. Are there any advantages of having it in .bashrc as a function? 
I haven't check it out yet. Most people will just say another todo application. My favorite one out of all of them is topydo. [https://github.com/bram85/topydo](https://github.com/bram85/topydo) Yours is written in bash and I did look at the codes and it looks like you did some good work there. I only suggesting would be a gif video or some screenshot slides of your todo application in action. That might draw more towards your hard work. I'll go ahead and check it out and give you more feed back about your work.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
You need to escape the dollar sign so it is interpreted as a literal. echo "--- \\$(date) ---" [Check this link out for more info.](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_03_03.html)
Thanks a lot! I added some screen shots.
did you write this application?
You need to single-quote your strings. echo '--- $(date) ---' &gt;&gt; /var/testy/nvme.log instead of echo "--- $(date) ---" &gt;&gt; /var/testy/nvme.log
Don't like it. You have to pick your editor. Don't know why it can't just be my default editor to avoid this question. I can't even just hit Enter to default to my current text editor. I have to choose which one I like to use. Then when I add task, it takes me to my text editor. I have to retype my request in. Why can't this be send to a buffer and I just hit return and my request been added. Instead retyping out my request inside my text editor. I gave it a try and don't like it. I'll stick with topydo.
Are you familiar with [Todo.txt](http://todotxt.org/)? It began as a command-line bash shell script.
No.
yeah. I tried an application Yishu based on it. Looks good!
when you enter tasks and press enter, it opens text editor so that you can type details of the tasks. you can save empty file if you don't want to give any details.
It has one draw back: it requires only dropbox to sync.
Dropbox is not a requirement of the shell script todo.sh. It is only required by some of its compatible graphical clients, ones that use the same file format.
Didn't catch that. Since the title of the message is what I type as Add. So I didn't had to repeat the message. It would be nice just skipping going to my text editor, if I didn't had to add to my message. Like Enter ends it, then add to it with a click. Or some kind of key combo to skip or go to the text editor. Even a flag of some sort to skip going to my text editor if I didn't need to go to it. People like to skip extra steps if they can. Even if its just one extra click. Just my opinion though. Still I think you did good work. Sorry for some negative from my part. 
To prevent command substitution in the here-document you need to quote the first `ENDMASTER` like this cat &gt; script.sh &lt;&lt;-'ENDMASTER' See the 'Here Documents' under 'REDIRECTION' in the bash manual.
In this case it won’t matter, but with an actual shell script, bash spawns a subshell to run it in, so things won’t persist in your current shell. For example, the function below makes a new directory and then cd’s into it: mkcd() { mkdir $1 &amp;&amp; cd $1 } But, if this was made as a shell script: mkdir $1 &amp;&amp; cd $1 Then running that script won’t actually “cd” you into the newly made directory. The new bash subshell running the script will cd into the new directory, but the script is now over, and it “exits” back to your original, interactive shell. Your “pwd” will still be the original directory, not the newly made one.
Looks nice, except for the selection of the editor. That is a not needed step. If you want people to be able to edit it, use it as an option later. Look into `whiptail` and/or `dialog` to make it look nicer. Especially selection can improve that way. People select and not need to (mis)type
You can use something like this to lock safely. declare -r lockfile_path="/tmp/$(basename "${BASH_SOURCE[0]}").lock" mutually_exclusive_main() { # This is auto-populated by bash at bottom of function local instance_fd ( # Don't wait for lock on lockfile (fd automatically allocated) if flock --nonblock --exclusive ${instance_fd}; then main "$@" else echo "Unable to run. Already running." 1&gt;&amp;2 return 1 fi ) {instance_fd}&gt; "${lockfile_path}" } mutually_exclusive_main "$@" OR mutually_exclusive_blocking_main() { # This is auto-populated by bash at bottom of function local instance_fd ( # Wait for lock on lockfile (fd automatically allocated) flock --exclusive ${instance_fd} main ) {instance_fd}&gt; "${lockfile_path}" } 
You can use [shellcheck](https://www.shellcheck.net/) to eliminate possible problems.
I know your looking for suggesting to improve your work. I think a timestamp would be nice for your entries. Most developers design their work around them. If it works for them, then the developers is happy with it. So many todo applications. Just wondering why you chose this project. I know some developers, just like do something simple to improve on their work. Which is always a good thing. Keep up the good work. If your still working on this project, to improve with some of our ideas or from yours. Give me a update of your todo application. I still would like to try it out. I'll be easier on the negative part. Especially if your happy with the end results.
try: find . -name "*.HEIC" -type f -exec /home/manderso/tools/tifig -p {} {}.jpg \; 
Don't quote it all in one string. That's why find thinks the entire thing is one command name. Split it into tokens by quoting correctly (or not at all since "{}" isn't interpreted by bash): find ... -exec ~/tools/tifig -p {} {}.jpg \;
That did it. I'm not sure why though. Is {} and {}.jpg the tokens in this case? Thanks btw.
thanks for the feedback, really appreciate it. Will do shell check :-)
Sure. You can keep yourself updated with the changes I make, by clicking on `Watch` button on top of the project page on github. Thanks a lot for your help. I will definitely consider feedback from you and will make my script better.
Allow me to repeat myself, after you type `add some todo` and press enter, it immediately opens a file in the text editor you provide when you run the application the first time. It does this so that you can add description to the task you are creating. How do I make this step obvious to user? Should I include in README file so that user will read it and expect it and not be confused like you were the first time? Thanks for taking time to give feedback to me.
I just save the reddit post. I'm not a member of github. I'll give you a couple of weeks. Then I'll take a peek again. Might even help you out, or at least improve it for myself. Keep up the good work.
Thanks for your feedback. I'll consider it.
Thanks for the feedback. I'll make that step less painful to the user.
README should always be a must. Then improve it if needed after the application progress. Maybe a # commit note in the application Add todo task(todo.txt) quick save or addition note if needed then save I'm not good with words. But, you get the point.
I'm not entirely sure on what the code would look like but using sed will most likely get the job done. 
So this works for your example: sort | sed ' s/[^:]*::/ /g; s/^ \+/&amp;- /; ' But it breaks if you have a line like `a::b::c::d` without all the prefixes (`a::b::c`, `a::b` and `a`) also present. For example, if your input was missing the `nginx::package` line, the output would be the incorrect nginx - config - redhat - params - service
Thank you. I'm trying to understand how it works. I see it sorts the line (presumably to group the same ones together), then replaces lines starting with ":" including everything "*" up to the "::" and substitutes spaces. The next expression substitutes lines starting with a single space with a hypen space for the first occurrence. I'll have to google what the ampersand does.
It works perfectly on a _large_ input file. Thank you so much!!!
I haven't used other tools but I can vouch for shellcheck. Catching problematic code is one thing. But it has really good documentation and explanations about each issue it encounters. 
Can you show an example that doesn't work? Also, in the example you showed in your main post, did you make a mistake when copying the text? Is the 'nginx' line really the second line? If it's really the first line, then you don't need the "sort" command. That command might have broken something else in your large file.
Swap out exec for GNU parallel if you have more than 1 core...it’s great
See the PROMPTING section of `man bash`.
Best advice right there, read the man page. If you learn how to do that you'll start getting a solid understanding of troubleshooting techniques and resources that Linux readily gives you. However, I like this article specifically for customizing your prompt: https://www.maketecheasier.com/8-useful-and-interesting-bash-prompts 
Most tasks are concise and should be passed in the command as a parameter. Why not make the editor for details optional?
I know you're looking for exec. This is an alternative because basic loops are powerful and you can use it with any output, not just find: find . -name "*.HEIC" -type f | while read line; do ~/tools/tifig -p $line $line.jpg ; done 
[Here's an online bash prompt generator.](http://bashrcgenerator.com/)
There is also the [Bash Prompt HOWTO](https://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/) - even though a bit dated, still a quite good overview.
it is kind of optional. If you just exit or same the empty file when it opens editor, it doesn't save the description of the task. Anyways, I get your point. Thanks... I'll make it more obvious or give user an option.
Just let it do it for you. Bash-it. [https://github.com/Bash-it/bash-it](https://github.com/Bash-it/bash-it) It's nice to know how to do all this manually. But, sometime you can see what all this customize will really do to your terminal. By, using somebodies else work and examining it. Then learn how to do it manually.
The ampersand stands for the full matched text (in this case, the spaces), so that the indentation is not lost.
This is the only correct answer posted so far in this thread. The other solutions won't prevent `$(date)` from being expanded within the here-document.
You could also use something like dotbot, it has an install script too and if everything works correctly, you should be able to symlink the files :)
&gt;or only if it changes because I've changed users (like if I ssh into another computer, or if I switch user accounts) The usual way to do this is to convert certain target elements to a number under 7, 15 or 255. For example: echo $(( $(hostname | cksum | tr -d " ") % 255 )) And then use that to select a colour for that element i.e. you login to a different host, it generates a different number and so you get a different colour. There's a couple of problems with this approach: 1) This depends on a modulo. Modulo bias is a thing. In this use-case, it's insignificant though... compared to... 2) Well, try this out: `for i in {0..255}; do tput setaf $i; echo -n "[ test $i ]"; done; tput sgr0; echo` You might find upwards of 30-40 of the available colours are just outright unusable. So you might have something like a 1/6 chance that your calculated colour will suck. [My](https://cdn.rawgit.com/rawiriblundell/dotfiles/master/termtosvg_e_udnoec.svg) [approach](https://gist.github.com/rawiriblundell/2c4631611052509a55c658cd0efaab35) is to manually define colours myself. So if I switch host or user, and need a visual marker, I can run a command which sets my prompt colours to whatever I choose. I have also built-in the capability to read a file, `.setpromptrc`, which is handy from a sysadmin point of view. Let's say I have a fleet of servers and want to colourise my prompt based on, say, network zone, or environment (dev/uat/pre-prod/dr/prod etc)... it's a nothing task for me to deploy `.setpromptrc` files. When I log into a host in a DMZ, for example, the `.setpromptrc` is read and the prompt colour changes. &gt;Also, I'm hoping to not display the full path to the working directory unless I have just cd into a new directory. You can achieve that without touching your prompt. It's usually something along the lines of: cd() { builtin cd "${1:-$HOME}" &amp;&amp; echo "${PWD}"; } &gt;I'm also trying to get a feel for how do similar things in the future, so any advice, reading suggestions, or explanations of steps are greatly appreciated. Dig through dotfiles on github. Don't blindly copy what you find unless you understand the code and why you might want it.
Hi, you'll want to use .zshenv , details [here.](https://unix.stackexchange.com/questions/126956/functions-defined-in-zshrc-not-found-when-running-script) Found that by googling: "zshrc" "source" "in a script"
&gt;I stand on the shoulders of giants I'll stand with you. High-level languages like Python may be even more "giant", but the simplicity of bash makes it so easy to just whip up anything without dealing with libraries, complex data structures etc. There's a lot that can't be done in bash, but it has a very unique feeling of freedom and control that I miss with other languages.
Agreed - as someone who basically only programs for myself - i use bash for everything. One thing that i think can make bash so intuitive and easy is that you can use "the file / folder " metaphor for everything and not worry about memory (just use temp files) or arrays and such (ditto). I get that this would be unacceptable in a production environment but that aint what bash is for.
Are you sure this post belongs in /r/bash and not in /r/zsh?
Haha my bad sorry :)
Made a long version with a lot of explanation and deleted that. Here a short and better readable version. Not every `whois` will give you a line with OrgName. So what you must do is add `begin` on the first line of your file and `end` at the last line. Then you run the following: `netcat whois.cymru.com 43 &lt; file-with-ipnumbers.txt` You can then do with that output whatever you desire. To get more options do the following: `whois -h whois.cymru.com help` and look on [their web page here](http://www.team-cymru.com/IP-ASN-mapping.html#whois) 
Bash is fine for this job. Not sure about your skill level, but since you managed to install Arch, I think you'll be fine using any bash tutorial out there. Here is an example script using _for_ and _beep_ (the latter must be installed first), but there are many, many other ways of doing it: #!/bin/bash # This script beeps, waits 20 minutes, then repeats forever usin an infinite for loop. # Press ctrl+c to exit for (( ; ; )) do beep sleep 20m done Before you'll be able to run the script, you need to give it permission to execute. If you saved your script file as a file called "twentybeep", you can use the chmod command like this: chmod a+x twentybeep After that, you can run it: ./twentybeep 
I did `chmod 755 twentybeep`. I got this error `./twentybeep.sh: line 6: beep: command not found `. I guess `Beep` is not in my $PATH. Thanks for help!
Did you install beep? Haven't used Arch in a while, but I guess this will do the trick: pacman -S beep
Yea that was it! THANK YOU!
I see you've already found a solution, but why not use Cron/systems Timers? I could add a solution here using Cron if your interested.
Here's another solution; run `crontab -e`. This opens up the crontab file in `$EDITOR`. Add the following line */20 * * * * /path/to/beep This will make it so that you don't need to open a terminal and starts with system start. However, the drawback being this happens on multiples of 20 on the clock, independent of when you start using the computer. 
If you're ok with just using your motherboard speaker or don't want to rely on a custom program you can use `echo -e '\a'` to beep.
I don't now about them, but someone suggested this: Here's another solution; run `crontab -e`. This opens up the crontab file in `$EDITOR`. Add the following line */20 * * * * /path/to/beep This will make it so that you don't need to open a terminal and starts with system start. However, the drawback being this happens on multiples of 20 on the clock, independent of when you start using the computer. :) If you have better solution and time to write it I will try it for sure :).
while :; do echo \\a; sleep 1200; done
 The advantage of using crontab is that even if the script crashed or you are not logged in, the program will run. The downside is that it will run even if you are not logged in and it will run as long as the machine is on at fixed times. So it will run on e.g. 00, 20 and 40 minutes after the hour. You can set it up to run e.g. only during weekdays during working hours, but it will be fixed times. So it very much depends on what you exactly need it for if sleep is better in the script or crontab. What happens if the script fails or closes? Would that be bad?
Hey! I'm thank you very much for this nice explanation. I think that crontab will be better in my case, if you have the code in your head, please write it down, but don't waste your time if you are not familiar with it ( I assume you are xD). I will go and search for it when I have time. It seems pretty good thing to know about. Thank you!
`info crontab` lists out some examples. I am not familiar with it enough to be fluent. However, [ubuntu help](https://help.ubuntu.com/community/CronHowto) article is very detailed and covers most, if not all of the features. You can combine a few things and achieve what /u/houghi mentioned.
&gt; USER=$(/bin/bash) Assuming you are running this from an interactive shell, it starts a new interactive bash session. Its stdin and stderr will be the terminal, while stdout is the pipe set up by the `$()` syntax. Bash's interactive prompt is written on stderr, so you'll see the prompt, but any command you run that writes to stdout will appear to have no output (that output is being captured into the variable `USER` of the parent shell). 
From fiddling around I found that you don't need to assign the variable and export it, you get the same behavior just from running `$(/bin/bash)` So what it seems like is happening is that the command substitution is taking over the stdout file descriptor. If you do `$(/bin/bash) &gt; out`, then your terminal session displays no output but all the output is being written to the file. All environment variables are preserved and whatnot. Interestingly, since the command substitution doesn't catch stderr without redirection to 1, stderr can still write to the terminal in this state. So knowing this, you can then do `$(/bin/bash 1&gt;&amp;2)` and you still have all your normal terminal output.
This makes a lot of sense. Thank you!
You're using quotes wrong. Use variable=$(some commands here) and not variable=$("some commands here") 
Unfortunatly i'm still getting an error on these lines `PASS=$(awslogs get /aws/lambda/${line}-decrypt ALL -s1d | grep successfully | wc -l)` `FAIL=$(awslogs get /aws/lambda/${line}-decrypt ALL -s1d | grep Error | wc -l)` The ${line} is not being replaced and I don't know why not.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Side note: lots of wasted processes, and if `awslogs` is slow (I have no idea) you may want to minimize the number of times you have to call it. Though I'll admit this isn't really clearer, stats=($(awslogs get blah | awk ' /successfully/ { s++; } /Error/ { e++; } END { printf "%d %d", e, e; }')) PASS=${stats[0]} FAIL=${stats[1]} uses one `awslogs` call and 2 processes instead of 2 and 6 (!). Similarly, can't read that entire LASTPROCESSED line but it seems likely that you can do it with one awk call instead of a zillion little tail, grep, cut, sed things.
Yeah, exec doesn't do what you think. It replaces the currently running process with the command given (type `help exec`). If you want to do more than one thing, just write them without exec. beep foo beep bar And so on.
&gt;stats=($(awslogs get blah | awk ' /successfully/ { s++; } /Error/ { e++; } END { printf "%d %d", s, e; }')) PASS=${stats\[0\]} FAIL=${stats\[1\]} that looks brilliant, many thanks! 
I think i figured it out actually: date -d "$date" '+%Y-%m-%d_%H:%M:%S'
&gt; date -d "$date" '+%Y-%m-%d_%H:%M:%S' I think you just want `date '+%Y-%m-%d_%H:%M:%S'`
thank you
 #!/bin/bash FREQS=( 61.74 82.41 98.00 92.50 82.41 123.5 110.0 92.50 ) for freq in ${FREQS[@]}; do beep -f $freq done 
`${title/associative/commutative}`
I'm not sure I understand what you mean here. As i read it, the second statement would be expanded as: if [ *needle* == "Find the needle in the haystack." ]]; ... which, at least to my eyes, would always return false, or am I missing something? 
Clean, nice.
Yea, looks like I meant commutative.Oh, well. Can't edit it.
The asterisks (*) expand to any character, but only if they appear on the right hand side of the comparison. Thus `A = B`, while `B != A` when B contains a pattern. 
What is an example output of the line `echo "Quering infomation for bucket - ${line}"` ? Also can you show us some sample output from just doing `echo -e "$buckets"` after line 3?
&gt; while :; do echo -ne '\a'; sleep 1200; done Needs -e to recognize escapes, -n will also suppress the newline so there's nothing to stdout.
``` $ shellcheck myscript Line 3: exec beep -f 61.74 ^-- SC2093: Remove "exec " if script should continue after this command. ```
Bash functions are not macros according to what I know of the definition of the word. But also bash functions are not pure functions in that the may have side effects on the environment.
Are functions in any programming language more than abbreviated forms of things? Things that I think might help answer your question (I'm not entirely sure what you're asking): * Functions cause variable scoping. Bash uses *dynamic scoping*, which is different than the *lexical scoping* that most programmers are more familiar with. * You can set `trap "${code} RETURN` that will run when the function returns (similar to `defer` in Go). * Functions declared like `name() { ... }` run in the current shell, functions declared like `name() ( ... )` run in a subshell
To my mind: no. Aliases, yes. 
&gt; A shell function, defined as described above under SHELL GRAMMAR, stores a series of commands for later execution. When the name of a shell function is used as a simple command name, the list of commands associated with that function name is executed. Functions are executed in the context of the current shell; no new process is created to interpret them (contrast this with the execution of a shell script). A 'macro': &gt; Macros are used to make a sequence of computing instructions available to the programmer as a single program statement, making the programming task less tedious and less error-prone.[1][2] (Thus, they are called "macros" because a "big" block of code can be expanded from a "small" sequence of characters.) So yeah, I guess you could say that.
Just adding to what has already been pointed out... When run with `set -x` you can see the differences: $ bash haystack +haystack:4:: NEEDLE=needle +haystack:5:: HAYSTACK='Find the needle in the haystack.' +haystack:6:: [[ Find the needle in the haystack. == *\n\e\e\d\l\e* ]] +haystack:6:: echo true true +haystack:7:: [[ *needle* == \F\i\n\d\ \t\h\e\ \n\e\e\d\l\e\ \i\n\ \t\h\e\ \h\a\y\s\t\a\c\k\. ]] +haystack:7:: echo false false 
Ah, that's what I get for typing it out by memory. You need to use `builtin cd`.
It'll do your first alias, but for any directory.
No because a macro is not simply an abbreviation that is expanded. That would be an alias, which is still not a macro. Macros, even C’s, are more than just abbreviations since they can be passed arguments which can be spliced in. And functions are not abbreviations either because they have local scope.
date -d "$date" '+%F\_%T' works too.
No functions are much better. They can process a variable number of arguments, they can return values, they can be recursive. And best of all: you can use them in pipes. function arg1 arg2 arg3 | program that needs structured data That said, my new favorite thing is to: { cat &lt;&lt; EOF This is also structured data $VARIABLEtoExpand1 $VARIABLE toExpand2 } | program that need structured data. 
Thanks. This was the answer I was looking for. One key thing: functions are very useful in constructs like if func_1; then... Where func_1 computes a complicated boolean thing (perhaps involving many booleans). 
Bash evaluates boolean expressions efficiently: In 'BooleanA || BooleanB', BooleanB is not checked if BooleanA is true. In your example BooleanA would be the result of running 'filter &lt;in.$$'. If 'filter' exits with Non-zero, BooleanB (the result of running {} in your example) is checked.
Thank you. That is what I thought would happen but I wasn't sure because I've never seen this syntax. I don't really do much shell scripting.
At the other side, you have &amp;&amp;. Which does the opposite. In 'BooleanA &amp;&amp; BooleanB', BooleanB is only checked if BooleanA evaluates to true. ('false &amp;&amp; whatever' is always false, so no need to check 'whatever'). Which means you can chain commands so that the following ones only get executes if the previous ones are succesfull. But you can also do something like this: `command &amp;&amp; echo "success" || echo "fail"`. Unwanted side effect is that if your success command fails, the fail command will still be execute despite the initial command being successful. 
Except Bash functions can only return a status code. Yes you can capture stdout but that comes with its own problems.
But it already works in any directory
you could use a `for` loop over the files like for name in *mp3; do ffmpeg -i "$name" -write_xing 0 -c:a copy "${name/.mp3/2.mp3}" done which will let the glob expand `*.mp3` to all the mp3 files in the current directory, then run the command on each in turn. The last `${name/.mp3/2.mp3}` will use some parameter expansion to replace the `.mp3` with `2.mp3` to give you the filename you want. One caveat with that part though, it will replace the first occurrence of `.mp3` in the name, so if you have `.mp3` in the middle of a name you probably won't get the result you want with that.
Thank you. Tried this on test folder and it worked perfectly! I need to study bash more, this seems like magic to me.
Bash already has a shift command for exactly this
`getopt ":a:" _opt` will do. Search online for how to use that (I'm on mobile right now)
If they are in a directory, then do something like this. `source_files=$(ls -1)` OR `source_files=$(find . -name '*.mp3' -maxdepth 1)` Then your code can be something like this. `for file in $source_files;do ffmpeg -i ${file) -write_xing 0 -c:a copy ${file}2;done`
 while getopts ":a:" opt; do case $opt in a) youtube-dl -a "$OPTARG" rm -i "$OPTARG";; *) youtube-dl "$*";; esac done I haven't tested it, but you'd get the idea.
 man bash
 ▓▒░$ uwot() { printf '%s\n' "What?"; } ▓▒░$ uwot What? 
This is bad advice. Please see [Pitfall 1](http://mywiki.wooledge.org/BashPitfalls#pf1).
LOL I'm dumb. 
Try man --all --html="firefox" bash to read in your browser (Replace 'firefox' by your favourite browser).
What if the option is not `-a` but just `a`? `getopts` wouldn't work then, right? Although `getopts` does solve my problem I would also like to find a solution for arguments without the dash (e.g. `yt-script a ~./downloads/youtube-list.txt`). I could convert all my scripts to use `getopts` for argument processing, but I don't plan on making the switch because `geopts` doesn't support long options like `--batch-file` and more importantly doesn't support optional arguments with default values (e.g. `yt-script a ~/custom/path/youtube-list.txt` or just `yt-script a` and the value `~/downloads/youtube-list.txt` will be used since there's no file specified). Also, not having to type dashes is nice and I can tell whether a script's argument is implemented by the script or is used by some application that the script may be a wrapper for (e.g. the option `a` in `yt-script a &lt;file&gt;` is implemented by the script, whereas `yt-script -a &lt;file&gt;` is implemented by youtube-dl, if that makes sense. 
I don't use tor, so I don't have anything I can test with to help with the first point, I also don't like read loops, I prefer to stick things in an array and then iterate. Here is what I have for the second two questions: #!/bin/bash mapfile -t LINKS &lt; links.txt LINK_STATUS=linkStatus.txt echo -n "" &gt; ${LINK_STATUS} #Clear the file for LINK in "${LINKS[@]}"; do torsocs curl -s ${LINK} &gt;/dev/null 2&gt;/dev/null if [ $? -eq 0 ]; then echo "[+] the LINK is online" | tee -a ${LINK_STATUS} else echo "[-] the LINK is offline" | tee -a ${LINK_STATUS} done 
It's worth noting that using the 'local' keyword on variables defined in the function can help mitigate side-effects on the environment. function setX() { local X=4 echo f1 ${X} X=${1} echo f2 ${X} } echo m1 ${X} setX 5 echo m2 ${X} ---- Output: m1 3 f1 4 f2 5 m2 3
Man I don't know how to thank you enough other to just say thank you. I need this program because I am making a site with .onion links and I need something to help me with checking them if they work and the only thing I could find is bash because for some reason torsocs only work in bash.
For testing your link, you probably want to set the `-m / --max-time` switch for curl to take so it doesn't take forever. Also, just because you get a 0 exit code, doesn't necessarily mean that the website is actually working. Here's how I would do it (untested): #!/bin/bash input_file="~/links.txt" output_file="~/output.txt" ## Check that the input file is there if [ -f "$input_file" ]; then urls=$(cat "$input_file") else echo "$input_file not found" exit 1 fi ## Create/Clear output file ready for appending cat /dev/null &gt; "$output_file" ## Loop over each of the links from the input file for url in urls; do ## Get the HTTP status code for $url ## https://en.wikipedia.org/wiki/List_of_HTTP_status_codes code=$(torsocks curl -s -L -m3 -w "%{http_code}" -o /dev/null $url) ## Curl options breakdown ## -s | Silent ## -L | Follow Redirects / Links ## -m3 | Max time allowed, 3 seconds ## -w "%{http_code}"| Only write out the http status code ## -o /dev/null | send all other output to /dev/null ## If we got an HTTP 200(OK) status code from the last command, then if [ "$code" -eq 200 ]; then status="is online" ## If the connection timed out and we didn't get status code elif [ -z "$code" ]; then status="is offline" ## Catch any unexpected results else status="is unknown" fi ## Output status to STDOUT and append to $output_file echo "$url $status" | tee -a "$output_file" done
Man, I really need to learn curl one of these days. I know you said you didn't test it, but two things of note for OP: The `for` loop in this example should look like: for url in $urls; do or for url in ${urls[@]}; do Also, you need to switch online and offline test orders because as it is, the online test will throw an error if $code is unset and it will skip get to the offline test and go straight to the else case.
No problem. Your thanks is more than enough. Just want to point out that /u/finalduty's use of curl flags is a better approach than my hostAlive() function. I'd suggest either pasting his `torsocks curl` command into my first solution and forgetting my second, or just using his version altogether. Or, you can of course bastardize what you think are the better ideas of both.
&gt;Also, not having to type dashes is nice You could use a bash alias, something like: alias ytda='script.sh -a' &gt;the option a in yt-script a &lt;file&gt; is implemented by the script, whereas yt-script -a &lt;file&gt; is implemented by youtube-dl, if that makes sense You could call `-a` something else in your script, somethign that doesn't exist in youtube-dl, like `-y` &gt;more importantly doesn't support optional arguments with default values &gt;(e.g. yt-script a ~/custom/path/youtube-list.txt or just yt-script a and the value ~/downloads/youtube-list.txt will be used since there's no file specified) You could do this: while getopts ":y:" opt; do case $opt in y) if [ -f "$OPTARG" ]; then youtube-dl -a "$OPTARG" rm -i "$OPTARG" else youtube-dl -a "~/downloads/youtube-list.txt" rm - i "~/downloads/youtube-list.txt" fi;; *) youtube-dl "$*";; esac done &gt;geopts doesn't support long options like --batch-file https://stackoverflow.com/questions/402377/using-getopts-in-bash-shell-script-to-get-long-and-short-command-line-options/7680682
&gt;The for loop in this example should look like: &gt; `for url in $urls; do` Yeah, you're right, have edited my post. &gt; Also, you need to switch online and offline test orders because as it is, the online test will throw an error if $code is unset Ah yeah, I mixed up my integer and string comparisons. Having the offline test first and/or changing the online test to `[ "$code" == 200 ]` would solve it. &gt; The only other thing I would add, is to print the code in the "unknown" case, so you know what you need to handle if you ever come across it. &gt; `status="is unknown [$code]"` +1
I find the Bash manual far easier to understand than its manpage. In [this section](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameters.html) it says that user-declarable parameters (aka _variables_) must be _names_. [This] defines a _name_ as: A word consisting solely of letters, numbers, and underscores, and beginning with a letter or underscore.
/u/gotbletu did a great video about integrating the app into your workflow that goes a little further and deeper than this article. [Watch It Here](https://www.youtube.com/watch?v=rEWpVwZpzjA) 
How losing time as a dev to trash project ... 
FYI you can also use gvfs-trash to deal with the "desktop" trash can from the command line.
Hi First of all I would like to say thank you for the code it really helped but there is another problem. See when I run the script it does pickup sites that work and that do not work but I get an error with torsocks that slows down the runtime of the script. Here is what I get in terminal: [http://xmh57jrzrnw6insl.onion/](http://xmh57jrzrnw6insl.onion/) is unknown \[000\] [http://5plvrsgydwy2sgce.onion/](http://5plvrsgydwy2sgce.onion/) is online [http://wiki5kauuihowqi5.onion/](http://wiki5kauuihowqi5.onion/) is online [http://wikitjerrta4qgz4.onion/](http://wikitjerrta4qgz4.onion/) is unknown \[000\] 1532520483 ERROR torsocks\[4027\]: Connection refused to Tor SOCKS (in socks5\_recv\_connect\_reply() at socks5.c:549) [http://3fyb44wdhnd2ghhl.onion/](http://3fyb44wdhnd2ghhl.onion/) is unknown \[000\] 1532520603 ERROR torsocks\[4034\]: Connection timed out (in socks5\_recv\_connect\_reply() at socks5.c:553) [http://j6im4v42ur6dpic3.onion/](http://j6im4v42ur6dpic3.onion/) is unknown \[000\] 1532520724 ERROR torsocks\[4044\]: Connection timed out (in socks5\_recv\_connect\_reply() at socks5.c:553) [http://p3igkncehackjtib.onion/](http://p3igkncehackjtib.onion/) is unknown \[000\] [http://kbhpodhnfxl3clb4.onion](http://kbhpodhnfxl3clb4.onion) is unknown \[000\] The problem is with the ERROR torsocks gives because it slows down the script I think to get an error it takes like 3-5 min and I will have around 3000 links inside the links.txt so the worst case scenario is 625 days if all links give an error like that so it really isn't optimized. Here is what I tried: I searched the google for answer and all links point to the [https://tor.stackexchange.com/questions/10395/error-torsocks-unable-to-resolve](https://tor.stackexchange.com/questions/10395/error-torsocks-unable-to-resolve) so I did what the answer said and in torsocks.config changed the port form 9050 to 9150 but it doesn't seem to work as the script still gives me an error. Also I noticed when I ran the script couple of times that some links work the first time then don't the second then do the third. Is this a glitch in the torsocks? As I saw that a lot of people seem to have glitches with torsocks. Also I would like to mention that I run Ubuntu on VMWare perhaps I should make a dualboot and then run it? Do you think that would solve the problem? Once again thank you guys for helping me!
You're missing the --color=auto flag in your alias.
Your new ls alias replaced the original one, which was: alias ls='ls --color=auto' ..so you need to add that part to your new alias as well.
Depends on the system. Not all will have gvfs-trash installed.
Tnx! How do I know what the default settings for a command are? 
% which foo
In the case of alias, you just type alias to see your currently set aliases.
Use the type builtin. type ls 
creating your own trash cmd: * with GNU tools: https://unix.stackexchange.com/questions/452496/create-a-recycle-bin-feature-without-using-functions * generic bash function: https://unix.stackexchange.com/questions/453827/making-alias-of-rm-command
I've had enough bad experiences to just add ´rm() { mv "$@" /path/to/trash; }´ to my .bashrc.
You can put your grep statement before the if and send it to /dev/null. Then your if statement will check the return code of the grep. grep ... &gt; /dev/null 2&gt;&amp;1 if [ $? -eq 0 ]; then ... fi 
Run the script with `bash -x yourscript` to show a trace. You might find that the line has a trailing space or carriage return or something that `echo` doesn't show
OPs code already checks the exit code of grep 
No, ops code is checking the output of grep. 
Let's do an experiment and see. What are you saying it checks the output against?
I might be missing the logic here. You are checking to see if `source.txt` contains `toCheck.txt` values, but there is no match between the two files.
I see now. I was reading his script wrong. Now that I'm at a computer and testing I see there are no brackets for the if/then, which was throwing me off earlier. 
Why not just alias rm to mv to a trash directory? You'd have to unalias to clean out that directory but emptying the trash should be a multi-step process anyway because that's where the safety comes from.
Bash already supports Unicode. Just make sure you've configured a UTF-8 locale (use the 'locale' command to check).
`if` succeeds if the status of the command that's run is `0`. For example: if true; then echo "it is true" else echo "it is not true" fi it has nothing to do with the output of the command.
Yes, I learned something new today. Today was a good day. 
what's really gonna bake your noodle later on is when you realize that `[[ ... ]]` is really just the `test` command with some extra syntactic sugar and returns `0` on success in the same fashion, so `test -e foo` is the same as `[[ -e foo ]]`.
I think `toCheck.txt` is supposed to look like: aaccnet.org 1science.com With that change I'm getting: checking aaccnet.org "urls" : [ "r/http://www.aaccnet.org" ], found checking 1science.com "urls" : [ "r/https://1findr.1science.com/home" ], found I don't know what OP did different.
This is likely it, because his script works for me, ([With an adjustment to `toCheck.txt`]()https://old.reddit.com/r/bash/comments/91s8yp/help_with_if_grep/e315xn8/) and breaks if I add a space. 
A better way to test the condition without printing out output to `grep` is to use the `-q`, `--quiet` or `--silent` flag. if grep -q ${PATTERN} ${FILE}; then Unless of course you care if the return value is a 1 vs a 2.
Discussed on the mailing list in 2017: [http://lists.gnu.org/archive/html/bug-bash/2017-06/msg00000.html](http://lists.gnu.org/archive/html/bug-bash/2017-06/msg00000.html) One of many quotes from maintainer Chet Ramey: "If I undertake the effort to put this into bash, and commit to supporting it forever (which is how these things go), I'm not going to orphan non-UTF-8 users. And no matter which way we go here, I can't see any advantage in allowing invalid multibyte sequences in identifier names."
&gt; You could do this: Maybe there's a misunderstanding--it [doesn't work](https://stackoverflow.com/questions/11517139/optional-option-argument-with-getopts). I.e. in your example, `script.sh -y` doesn't satisfy `y)` case so it falls into the `*)` case. In that thread, a suggestion was to use upper/lowercase arguments to differentiate between an argument with an optional value vs. a default value--it's definitely not a good solution but I'm still considering it (the problem for me is using getopts is supposed to make scripts feel predictable and familiar and needing to resort to a compromise like this defeats that purpose). [There](https://stackoverflow.com/a/11517897) are [some](https://stackoverflow.com/a/45443082) solutions but I'm not sure whether these are more of hacks/workarounds than clean and complete solutions. Since there's no native support for arguments with optional values and I haven't seen a canonical or consistent way to implement it via getopts, it's why I'm hesitant to make the switch. If you can understand these implementations, I'm curious what you think of them and which one you prefer. I came across the thread about long options when I looked into getopts and again there are many solutions--most if not all are hacky. Actually, [this particular answer](https://stackoverflow.com/a/28466267) and a reply to it to improve it further seems like it's worth checking out. Thanks for reminding me. 
Thanks, I had a misunderstanding.
Yes, this is what I had in mind, as Unicode defines character properties expressly for standardizing what a ‘valid identifier’ means.
I see. Thank you.
This tells you the command location, not what options are enabled through an alias. 
`alias |grep 'ls='` would specifically give the result for `ls`
Meh, it does on zsh though... Ah well.
Are you trying to remove files of the same name in different directories? Duplicates in what way?
I use fuzzywuzzy a python library for fuzzy matching. If you’re looking for exact file duplicates rather than names being close you could use a combination of find/exec/md5sum/sort/uniq to gather than information 
Files with the same name in either the same directory or another. I guess I'd first like to echo all the files with same names and their locations so that I could review them, then choose to delete. 
&gt; I guess I'd first like to echo all the files with same names and their locations so that I could review them, then choose to delete. Is this hard to do with a simple script? Easy enough: find DIRS -type f -printf '%p\t%f\0' | \ sort --zero-terminated --key=2 | \ uniq --zero-terminated --skip-fields=1 --group=prepend | \ cut --zero-terminated --field=1 | \ tr '\0' '\n' Replace `DIRS` with a list of base directories for `find` to recurse into. But perhaps a better approach might be to use something like [`hardlink`](https://linux.die.net/man/1/hardlink) to find and link identical files, then scout around for files whose link count is greater than 1.
Wrote a script that I can use to monitor for changes in files and run some command for e.g. rebuilding, deploying, etc This is my first script, would appreciate some feedbacks
Yep it was a hidden new line. I could only see it with this debug log trick, Thanks so much!
Thanks had no idea could do this.
It looks like your post was URL-encoded twice? Here it is decoded: &gt; I want to compile a *CMakeList.txt* file `make -f CMakeLists.txt` is the command which I'm using but I'm getting the following error; `*** missing speratror` ?
I would say fslint is the tool you need!
Whats your login shell and its config?
Login shell is /bin/bash and the config is default but those env variables I've added to .bashrc # .bashrc # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi # Uncomment the following line if you don't like systemctl's auto-paging feature: # export SYSTEMD_PAGER= export TNS_ADMIN=/icinga/oracle/tnsnames.ora export ORACLE_HOME=/usr/lib/oracle/12.2/client64/ export PYTHON_EGG_CACHE=/tmp/.python-eggs export SYBASE=/usr/local/freetds export LD_LIBRARY_PATH=/usr/lib/oracle/12.2/client64/lib/ export NLS_LANG=American_America.UTF8 # User specific aliases and functions 
See [here](https://www.gnu.org/software/bash/manual/html_node/Readline-Interaction.html#Readline-Interaction) and [here](https://www.gnu.org/software/bash/manual/html_node/Command-Line-Editing.html#Command-Line-Editing) and [here.](https://www.gnu.org/software/bash/manual/html_node/Modifying-Shell-Behavior.html)
Thanks for the links but I don't think readline interactions have anything to do with it. I'm missing default behaviour of the bash shell and I didn't change anything besides adding those env variables.
If Unicode identifiers are allowed, not just the data processed by the script, but the *syntax validity of the script itself* would become locale-dependent. That seems like a bad idea to me.
In what terminals have you tried this?
Additional Info: I've tried in ssh session using either putty or mobaxterm as client and on the VM console in vSphere. There is no gui environment installed so I'm directly on the shell when connecting through vSphere console. The behaviour is consitent and doesn't occure on any other servers so I think it's save to say that it's got nothing to do with sshd.
I've had the same issue in one of my servers too
This is unrelated to your problem: But actually you can use ctrl+u to clear a line. However, it does not create a new line. But if this is what you really want to, you can just use ctrl+u ctrl+o. This is part of the emacs emulation bash has. It actually cuts from cursor to the beginning of the line. You can paste it back if you want to with ctrl+y.
Thanks that's useful to know even though I just start a second shell when logging in as a workaround at the moment.
Sorry, what do you mean URL-encoded twice ?
Consider quoting `$cmdRun` and `$fldr` on lines 22 and 42, respectively to avoid word-splitting. If the first argument is intended only to be a path to a folder, you could perform an initial check to ensure the supplied path exists and points to a folder rather than a file or a symlink (unless you wish to incorporate a way to manage symlink paths). A quick way to check for both existence and nature given a file path supplied to `$1` is: `[ -d "$1" ] || exit 1` Lastly, but perhaps most importantly, if you're wishing for feedback, it's will be much easier for others to provide if you supply a file header in the form of a comment block that tells a reader what the program is designed to do; what arguments the script requires; and what the output ought to be in each possible situation that could arise. Going in and reading the script, it took a long time to gauge what its actual purpose was, and `"Wrote a script that I can use to monitor for changes in files and run some command for e.g. rebuilding, deploying, etc"` is suitable for the title of a post, but lacks context and an informative introduction to the script for someone who knows nothing about it, i.e. anyone but you.
Well, Python has to be UTF 8. It didn’t seem to experience any issues.
Well, your post starts like this: &gt; I%2520want%2520to%2520compile… which is probably supposed to be &gt; I want to compile… but for some reason, all the spaces were replaced by `%20`, and then all the percent characters were replaced by `%25`. This process is known as [percent-encoding](https://en.wikipedia.org/wiki/Percent-encoding) or URL encoding, though I have no idea why it happened to your post.
Weird. To clarify, when you first ssh in ctrl+c doesn't clear the line, but then when you start a new shell it does? It almost sounds like you're default login shell is trapping the signal. What do see when you list all traps by just typing `trap`? Also, what's the output of `set |grep SHELLOPTS`?
If you don't necessarily need to display output for thing that has not been found, I suggest that you use a cleaner and a more efficient approach of `grep -f` instead of iterating through an array, e.g.: grep -Fof toCheck.txt source.txt | xargs -i echo '{} found'
Output of `trap` [root@host ~]# trap trap -- '' SIGINT trap -- '' SIGQUIT trap -- '' SIGTSTP trap -- '' SIGTTIN trap -- '' SIGTTOU Output of `set |grep SHELLOPTS` [root@host ~]# set |grep SHELLOPTS SHELLOPTS=braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor [[ ":${SHELLOPTS:-}:" == *":${1:-}:"* ]] || return $?; 
That looks like it right there, you're trapping SIGINT by default and doing nothing. On my (ubuntu) system, my `trap` output is only the custom stuff I have done. I don't know how to find out where that got set without grepping the whole filesystem, lol. 
Thanks, hopefully move onto my question.
I figured as much when I saw SIGINT in there. Thanks for this great tip, I know what to look for now. I think can solve this now.
To clarify, my `trap` output is essentially nothing. Try it on another machine / new shell on the same machine and confirm (and here to satisfy my curiosity please). Also my whole filesystem grep would be something like this: `cd /; grep -rE 'trap.*SIGINT'`.
I have another CentOS 7 host on which only the following singals are set: [root@Odin ~]# trap trap -- '' SIGTSTP trap -- '' SIGTTIN trap -- '' SIGTTOU I tried removing the QUIT and INTERRUPT signals from trap like this `[root@host ~]# trap - SIGINT SIGQUIT` but I think it gets reset on logon becaus when I logoff and logon again they're back. Anyway it looks like I'm on the right track. :) 
A nice and ugly hack could be to add that trap removal to your bashrc, haha. Glad I could help though, looks like all my procrastination with bash customizations are good for something. 
&gt; A nice and ugly hack could be to add that trap removal to your bashrc Actually I was just thinking about doing this because this been a really long day and i'm exhausted ^^ Thanks again you really were a great help!
No worries. Don't forget to change the flair on your post if you think it's solved. 
Right, but only for going up a directory. I'm saying that you gain the ability to navigate to *any* directory merely by typing its name.
So it does something completely different? What happens if I have multiple directories with the same name?
Use `set` e.g. foo () { set -- foo bar baz; echo "$@"; shift; echo "$@"; echo "$1" } Result: foo bar baz bar baz bar
... wat?
I think you are confusing wild cards and some regex. You can do: ls -l /dir/june* /dir/july* ls -l /dir/{june*,july*} ls -l /dir | grep -E '(^june|^july)' 
No, it's the same thing, but more general. &gt; What happens if I have multiple directories with the same name? The filesystem doesn't let you do that? Maybe I'm not explaining well what it does. When you type something and hit enter, zsh will first do all its normal PATH and alias checks. If something doesn't match, then before it gives an "unknown command" error, it checks to see if it's a directory (as resolved from cwd), and if it is, it changes directory into it.
It's a bash extended globbing, it's a limited form of regex: https://www.linuxjournal.com/content/bash-extended-globbing 
This is an "extended pattern matching operator", or "extglob" for short. Extglobs all look like a character (`?`, `*`, `+`, `@`, or `!`) followed by a parenthesized list of globs separated by `|`, and give pathname expansion ("globbing") the full power of regexps (without backreferences). They have to be turned on with `shopt -s extglob`. For more information, look at the end of the "Pattern Matching" section of the `bash` manpage.
Please share your solution!
You're going to need to provide precise and detailed background information for anyone to understand what your question is. As it is, it reads like "When to use shampoo vs. When to use toothpaste"? It depends on what you want to do. 
oh wow. I was completely unaware of this usage of `set`. this is incredible! thanks!
I haven't found the root cause yet because I was done for the day but for now I've put `trap - SIGINT SIGQUIT` in /etc/bashrc which is ugly but works. There must be a logon trigger somewhere but I haven't found it yet.
Do you know if this is implemented on all versions of `bash`? or was it introduced later?
How to clear a line in a shell like Bash: ctrl+k = clear to the RIGHT of the cursor ctrl+u = clear everything to the LEFT of the cursor 
Or `fdupes` if you don't want to reinvent the wheel
`extglob` appeared way back in `bash` 2.0x days, so for all intents and purposes it should work on all versions of `bash`. The oldest version I've seen in the wild was 2.04 on Solaris 8, and that supported `extglob`... See https://tiswww.case.edu/php/chet/bash/CHANGES
 mkcd () { mkdir -p "$@" &amp;&amp; cd "$_" } Mine only makes one directory at a time. TBH I've never thought about the need to make several at once. The one thing that mine does that yours won't has to do with the `-p` option. 
echo "some text `date +%Y-%m-%d\_%H:%M:%S`" &gt;&gt; /path/to/filename # will output (without the quotes) "some text 2018-07-27_00:30:33" to /path/to/filename
 gush () # 'gush' stands for "grep uniq sort history" and only shows a single copy of a command, #+even it there are multiple copies in your history. Numbers are included, so you can #+rerun a command by typing something like !42 or !9353:p { history | grep -i -- "$1" | sort -k2 -u | grep -v 'gush' | sort -n } 
&gt; try doing C^r on the command line \^R, \^R, \^R\^R\^R\^R\^R if there are a lot of matches and your command isn't the last result. But agreed, really handy. Check out my `gush()` function in this thread though.
Mine will do that too, just add the '-p' as you would with 'mkdir': mkcd -p some/directory/here some/other/directory/here 
A `CMakeLists.txt` file is not a make file and you can't directly call make on it. Instead, it writes a make file for you. Typically, you would want to do an out-of-source build (meaning you put all the files generated during this process outside your source), and that goes something like this, $ cd /path/to/source-directory/ $ mkdir build $ cd build/ $ cmake ../ You may want to provide additional options to cmake, like CMAKE_BUILD_TYPE (one of Release, Debug, RelWithDebInfo or RelMinSize(?)) or CMAKE_INSTALL_PREFIX. The build type dictates options used by the compiler (optimization level, -g or no -g, etc.) and the install prefix is the path you want to install the compiled files to. They're options, you don't *have* to use them. Anyway, now cmake will have written a Makefile in the build directory, so you can now compile your code with, $ make and the code will be built inside the build/ directory you created. You can zap this build directory anytime you like when you're done with it. Hope this helps! 
Are you able to share the contents of `CMakeList.txt`?
https://en.m.wikipedia.org/wiki/Glob_(programming)
The bash hackers wiki is a nice resource for this kind of thing: [here you go]( http://wiki.bash-hackers.org/syntax/expansion/globs)
I wasn't aware that I couldn't compile with the CMakeList.txt file; and you're explanation requires a little more reading, as there is very little information I could find on using CMake. I have a build directory, although it doesn't contain many files. 
Yes, let me post
Thanks! It makes sense what you said. I should update the script and make it more readable.
The link to the file; https://ln.sync.com/dl/a9b4cd6e0/bu6xdfd7-n2ckd9y3-sgedbjdw-46akyrr4
This is a CMAKE file. Consider running it with `cmake` instead of GNUmake.
You can read about globbing characters in the bash manual under the "Pattern Matching" section: man bash then search for Pattern Matching /Pattern Matching and hit n until you reach the section nnn They are similar to, but simpler than, regular expressions. They are used to select sets of files as targets of file operations. *.txt (all files ending in .txt) [a-m]*[n-z] (all files starting with a to m and ending with n to z) *.{txt,pdf} (all files ending in .txt or .pdf) ?ash.txt (all file starting with any character followd by ash.txt)
I was informed their one in the same, alright if you say it has to be compiled with CMake.
i find recursive scripts to work best. Thistlethwaite's conjecture. Tom Thistlethwaite May 10 I suspect the answer may be yes. On a quantum scale, the temperature can be below the absolute value of 0. Law 3 states that the entropy of the system at absolute zero is exactly equal to 0. If it is considered a fact, negative mass/energy is required if the temperature is below absolute zero.
i.e. z sh. bash wget.reddit.bash.com .z shell recursive. b lang. Thistlethwaite's conjecture. Tom Thistlethwaite May 10 I suspect the answer may be yes. On a quantum scale, the temperature can be below the absolute value of 0. Law 3 states that the entropy of the system at absolute zero is exactly equal to 0. If it is considered a fact, negative mass/energy is required if the temperature is below absolute zero.
nice one
Not sure exactly what you are describing, but it sounds like something that would be easy with `xdotool`.
&gt; I wasn't aware that I couldn't compile with the CMakeList.txt file That's not what I said. cmake is a tool to write a makefile for you. You describe at a high level what you want to do to compile some code and cmake generates a makefile for you. The reason why people use it is because it's easier to write a cmake file for a large project than a makefile. &gt; there is very little information I could find on using CMake [Much](https://cmake.org/cmake-tutorial/) [of the](https://cmake.org/documentation) [documentation](https://cmake.org/cmake/help/latest/index.html) for cmake is written for the person writing the cmake file. Really, most of what a user needs to know is in my comment above. Run cmake on a directory containing the top level CMakeLists.txt file and it writes you a make file and then, you just proceed like you would with any make file. Feel free to google. That's how everyone learns new things. :)
Check if your package manager has options to support noninteractive installing. For example, `apt-get` has the `-y` flag.
As someone else said there's probably an option to specify your package manager to avoid displaying this. If there is not then you can wrap your script with [Expect](https://linux.die.net/man/1/expect) (if you don't like the syntax you can take a look at pexpect in Python). `expect` is useful to control programs allocating a pseudo tty, like for instance an ssh session or in this case your pager. Again there must be a way to skip this pager, could you tell us what package manager you are using?
I think I'd do it in two steps. First, you would use 'find' to create a list of folders. Second, this list would be used as input for a bit of code that checks for those files existing. It could look like this: find MAIN_FOLDER_HERE -type d | while read -r path; do if [[ -f "$path/image1.jpg" &amp;&amp; -f "$path/image1.png" ]]; then echo "$path" fi done
Here's what's happening in your command: * Find all files * filter everything with "01.jpg" into a list * from this list filter everything with "01.png" So, you are trying find png in a list of jpgs, grep rightfully tells you it found nothing. The way to do it is `find -type f | grep -E '01.jpg|01.png' &gt;output.txt`. To find all dupes, you can sort the find output and manually look for duplicates. 
 foundlist='' searchname='image01' pnglist="$(find $PWD -name $searchname.png 2&gt;/dev/null)" for f in $pnglist; do path="${f%/*}" if [ -f "$path/$searchname.jpg" ]; then foundlist="$foundlist$path\n" fi done echo -e "$foundlist"
use `PROMPT_COMMAND=some_function`, where you set the PS1 at the end of some_function then you can use global vars. 
How do I do a check for interactive shells? Never encountered any issues.
This will check your shell options, it's from the default Ubuntu .bashrc. I recently ran into a problem using `scp` that was from setting the PS1. # If not running interactively, don't do anything case $- in *i*) ;; *) return;; esac 
Using ideone, I came up with (a sample that you try for yourself)[https://ideone.com/8TGtPA]. grep -P "image\d+\.(png|jpg)" | sed -r 's/jpg/png/g' | sort | uniq --count | sed -r 's/ +([0-9]+)/\1/g' | awk '{ if ( $1 == 2 ) print $2 }' | grep -oP ".*(?=/)" | uniq There's a spot on ideone to edit the input. You can try additional paths to see how it responds. To make this complete, pipe your `find` command into the commands above, then pipe the output to a file: find -type f | grep -P "image\d+\.(png|jpg)" | sed -r 's/jpg/png/g' | sort | uniq --count | sed -r 's/ +([0-9]+)/\1/g' | awk '{ if ( $1 == 2 ) print $2 }' | grep -oP ".*(?=/)" | uniq &gt;&gt; outputtest.txt 
So I think I'm headed on the right track, but I can't `printf` or `echo` anything in `PROMPT_COMMAND` function __set_prompt() { local prev_color="97;44"; $(__git_info); #This will cause issues retVal=$?; if [[ $retVal -ne 0 ]]; then prev_color="30;43"; fi; # printf $prev_color; export PS1="$(__host)$(__dir)$(__git_status)$(__arrow) " }
All in one `find` command: find . -type d -execdir test -f {}/image1.png \; -execdir test -f {}/image1.jpg \; -print You can build up from there if you like.
Hmm, I can print inside my setting function and see the output: set_ps1(){ echo "$PWD" PS1="\u@\h|$(date '+%H:%M')|\w" } PROMPT_COMMAND=set_ps1 When I source that it will print the PWD before each prompt. 
Another way just for fun: find . -name image2.jpg -execdir test -f image2.png \; -exec dirname "{}" \;
Also why are you calling your `__git_info` in a sub-shell?
tree -alf &lt;your folder&gt; | egrep “&lt;first string|second string&gt;”
Cause otherwise it will print out to the prompt, won't it?
If the function has any stdout then yes. Sub-shell calls are an expensive operation though because they `fork` your shell. I'm not sure what you're trying to do with it since you don't store the output but if you want to just ignore the output you can do __git_info &amp;&gt;/dev/null will redirect stdout and stderr to the null file. Also if you're starting to use PROMPT_COMMAND and storing everything in globals (since that's the only way to return in bash without a sub-shell), checkout `printf -v` for storing into variables.
Maybe you are getting whitespace characters (space/linebreak/tab etc) in your variables and that is confusing the argument list? What happens when you run this: #!/usr/bin/expect -f set IPaddress [lindex $argv 0] set Username [lindex $argv 1] set Password [lindex $argv 2] set Cert [lindex $argv 3] set CertPW [lindex $argv 4] set TrustPointName [lindex $argv 5] puts $IPaddress puts $Username puts $Password puts $Cert puts $CertPW puts $TrustPointName Also, maybe a network management tool like RANCID would be helpful for this sort of thing.
Well don't we want the output? I posted what `__git_info` does in the OP. At the end, it does `printf` with the info for the PS1.
I don't know a *better* way than using `getopt` or `getopts`, but I can find different. But I have no idea what problems you are running into so I don't know what "better" means to you. I'd suggest you just use `getopts`, it's going to be quicker than re-inventing the wheel. As far as a default file when the user specifies f, but does not specify a file, I don't see a good way to let it know d isn't a file. If you don't want to specify a file, it's best not use use the flag indicating you are specifying a file. Here's a *different* approach w/o `getopts`, but I have no way of knowing if it satisfies whatever problem you are having with your current solution, and it's definitely not going to help with specifying that the next argument is a file, and then giving another flag instead. NEXT_IS_FILE=false for arg in "$@"; do if ${NEXT_IS_FILE}; then echo ${arg} NEXT_IS_FILE=false elif [ "$arg" == "f" ]; then NEXT_IS_FILE=true fi done 
I actually wanted to learn getopts but read that it does not natively support "optional arguments" (i.e. the scenario mentioned where `-f &lt;file&gt;` uses specified `&lt;file&gt;` if specified, otherwise `-f -d will have `-f` use a default value) and long options (e.g. --file). For using getopts with optional arguments, there's this [solution](https://stackoverflow.com/a/11517897) and [another one](https://stackoverflow.com/a/45443082). I'm not sure if these are hacky attempts or complete solutions (if you can understand it, what do you think?) but the lack of a canonical way to have optional arguments with getopts leads me to believe it's the former. As for long options, it's admittedly not as important for me--I found [this](https://stackoverflow.com/questions/402377/using-getopts-in-bash-shell-script-to-get-long-and-short-command-line-options#comment74260227_28466267). P.S. In your first example with `shift`, will `shift`'s effect still take place after that code? If so, it may not be ideal unless you pay close attention to it (which makes changing the script in the future a little more tedious) if you want to add on to the script. Otherwise, it's the simplest example so far, which is great. 
I removed the first example. Those solutions look fine to me. You could always use a seperate flag for specifying default behavior. The reason it doesn't work without *looking* hacky is because it makes no assumptions about your data. For all it knows, `-d` could be a valid argument for `-f`. If you know you will always need a filename, you could just add that to the script, and not specify `f` at all when you don't need to deviate from default. FILE="defaultFile.txt" for arg in "$@"; do if [[ "$arg" == "f" ]]; then FILE="${2}" shift fi shift done echo "${FILE}" Run: my-script a b c f /tmp/example.txt d e #Outputs /tmp/example.txt my-script a b c d e #Outputs defaultFile.txt Another option is quoting your paramters. This way, it knows exactly what characters are to be grouped with a flag. FILE="defaultFile.txt" for arg in "$@"; do echo arg = $arg if [[ "$arg" == "f"* ]]; then if [[ "${arg}" != "${1##* }" ]]; then FILE=${1##* } fi shift fi shift done Run: my-script "a" "b" "c" "f /tmp/example.txt" "d" "e" #Outputs /tmp/example.txt my-script "a" "b" "c" "f " "d" "e" #Outputs defaultFile.txt 
Doesn't cisco support a rest api or a netconf configuration? I would opt for that overan expect script if the api supports cert install.
 find -type f \( -name image1.jpg -o -name image1.png \) -printf %h\\n | sort | uniq -d
So you'd need to update the command and run it again for each image, right? Once for image1, once for image2, etc.
`find -type f | grep '01\.jpg\|01\.png'`
I like to use a while loop along with `shift` to parse arguments. `shift` essentially moves `$2` to `$1`, `$3` to `$2` and so on. $1 is dropped, so you need to store the value in some other variable if you need to use it later on. file="" flag_a="" flag_b="" flag_c="" while [ $# -gt 0 ]; do case $1 in a) flag_a="$1" ;; b) flag_b="$1" ;; c) flag_c="$1" ;; f) file="$2" shift ;; *) # unknown argument - maybe show help? ;; esac shift done
 #ARGUMENT PARSER # Use -gt 1 to consume two arguments per pass in the loop (e.g. each # argument has a corresponding value to go with it). # Use -gt 0 to consume one or more arguments per pass in the loop (e.g. # some arguments don't have a corresponding value to go with it such # as in the --default example). # note: if this is set to -gt 0 the /etc/hosts part is not recognized ( may be a bug ) while [[ $# -gt 0 ]]; do arg="$1" case $arg in -r|--required) REQUIRED="$2"; shift ;; -o|--optional) OPTIONAL="$2"; shift ;; ---option) OPTION=true; shift ;; *) echo -e "Unknown argument:\t$arg"; exit 0 ;; esac shift done 
Follow up question: is globbing a form of regex?
Be careful with `rm -rf`, especially with wildcards. This is Linux, it will happily oblige to your request and shoot you in the foot. If you need a complicated removal use `find`. Google it.
`rm -rf **/{*.o,*.exe,*.out}`
The asterisks are read as wildcards, which match files in the current directory. If you want to match all files in a directory tree, maybe you'd better `find` them: find . \( -name '*.o' -or -name '*.out' -or -name '*.exe' \) -delete [source](https://superuser.com/questions/126290/find-files-filtered-by-multiple-extensions)
Warning: don't do this. The default behavior of a globbing pattern (`*`) will only match files in the current working directory. There's a shell option to enable recursive globbing (`shopt -s globstar`), which will then let you do `**/*.out` to match files in sub-directories. The proper way to recursively delete files based on a partial file name is with `find`. Something like `find /path/to/main/dir -name '*.o' -or -name '*.out' -or -name '*.exe' -delete`
This is how I would do it; requires `shopt -s globstar` to be enabled, though.
The solution using the find command is what I would consider best practice and correct, however you could use ls | xargs -I % rm -rf *.out etc etc That is a capital i used for the argument there by the way. It’s not a super clean solution but it does the job
&gt; ls | xargs -I % rm -rf *.out This is utter nonsense. For each word of every filename in the current directory, that re-runs `rm -rf *.out`. The `ls` and `xargs` is completely pointless, and they should really never be combined at all. It also does not come close to a solution for the problem.
And the portable version of that, without the GNUisms (-or and -delete): find . \( -name '*.o' -o -name '*.out' -o -name '*.exe' \) -exec rm -f {} +
I'd use extglob instead of brace expansion there shopt -s globstar extglob rm -f ./**/*.@(o|out|exe)
Don't you need another `expect` after `send -- "$Cert\r" and before `send "quit\r"`?
What about using `rm --` instead of `rm`? Does POSIX guarantee that the paths `find` produces will always start with the starting path (i. e. `./-some-file-with-dashes` instead of `-some-file-with-dashes`)?
If `.` is provided as the paths to search, then POSIX does guarantee that all the pathnames passed to the -exec command shall start with `.`. From [POSIX find](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/find.html): &gt; all pathnames for other files encountered in the hierarchy shall consist of the concatenation of the current *path* operand, a &lt;slash&gt; if the current *path* operand did not end in one, and the filename relative to the *path* operand. 
There should be a bot response for anytime ls | xargs is posted.
&gt; find /path/to/main/dir -name '*.o' -or -name '*.out' -or -name '*.exe' -delete That will only delete the files ending with `.exe`. the files ending with `.o` and `.obj` will be left alone. You need parenthesis to make the logic right. See the other find answers.
Sure, the answer to your question is
This should do the trick: for f in $(find /var/mobile -type f -name '*.m4a'); do mv "${f}" "/System/Library/$(basename ${f/%a/r})"; done You can replace `mv` with `echo` to see what it'll do before actually doing the work. The quotes are important. If any filename has spaces in it, you don't want mv to throw a fit when it can't move "The" to "Beatles.m4a".
You can use 'find' command to find m4a files, then pass it to mv command.
If there are space characters in the file names, this won't work. In that case, you would need to do something like this: find /var/mobile -type f -name '*.m4a' | while read -r f; do mv "${f}" "/System/Library/$(basename ${f/%a/r})"; done Or you can switch to using bash's own file search thingy instead of the external 'find', and then using a 'for' loop still work even with space characters in the file names. That would look like this: for f in /var/mobile/*.m4a; do mv "${f}" "/System/Library/$(basename ${f/%a/r})"; done This won't search through sub-folders like the 'find' tool does.
Thankyou so much
Use `rename` command. It is usually provided by `rename` or`perl-rename` package.
You're right! I always forget about the list given to \`for\`. I bet \`shopt -s globstar; for f in /var/mobile/\*\*/\*.m4a\` would work.
`grep -c str *aaa* | awk -F ":" '$NF == 1 { print $1 }'`
I don't know much about, nor use TOR, so can't be of much help with regards to intermittent failures such as you're describing. I think each request might be routed differently through the network and maybe you're hitting bad nodes or the remote servers are blocking you for some reason with something like fail2ban. Dunno, just throwing ideas out. There is also a builtin command called `timeout` that you can use ([man timeout](https://linux.die.net/man/1/timeout)). To work it in to your script, you can try something like this:
This isn't a `bash` problem, this is a `python` problem. Nonetheless, [the first result in Google](https://stackoverflow.com/questions/22301307/unit-tests-have-failed-for-beautifulsoup) says you're trying to install the wrong package and that you should try `pip3 install beautifulsoup4`
I'm going to assume you have a strong understanding of CMake, therefore I was hoping I could either, email you; or team viewer session, rather then posting on reddit or private messaging via reddit ? As I have a few questions as to getting cMake to compile the program which I want to get compiled for already over three months ;-(
This. Also, their [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is wonderful, I’d check it out. 
I don't think so, there are shared capabilities for sure, but I'd say globbing is to regex like nano is to vim.
My typical rate is $500/h with a 2-hour minimum. It looks like you may be outside the US? Let me first check on the tax consequences...
Well, yeah, that's what I was thinking. Nano and Vim are very different, but they're both text editors. Tools of the same kind, but with an enormous distance in scope.
use `-H` option and use `-h` option if you do not want file names
&gt;puts $IPaddress puts $Username puts $Password puts $Cert puts $CertPW puts $TrustPointName Thanks for the feedback. If I use the "puts" command to show my variables I see ofc, my User/pw, etc. and the $Cert variable like shown above in my initial post. It has some line breaks, ofc - but I thought this shouldn't be an issue? :(
Yeah, Cisco supports rest api. We're already using it for some automatic VPN deployment via a WebInterface. But I can't use it for the certificates - already checked that. I have some other scripts which allows us to request/sign certs by our internal CA. We use them globally, so my expect script above is just a small extension to install them on an ASA.
Afaik (and tested), it's possible to send multiple `send` statements before another `expect` is needed.
You're kind of close, but you've got your parentheses and brackets horribly misnested, and you seem to have tried to use a character class for the job where one isn't needed. Try this: shopt -s extglob for file in *; do mv --verbose --no-clobber "$file" "${file##+([0-9])*( )}" done 
Wow, thank you. I appreciate you breaking it down for me too. Just to clarify, the ## only works if the pattern is at the beginning of the expanded variable? 
That's right. It looks like you were trying to use `${file/pattern/replacement}`... or perhaps `${file/pattern}`, since you knew your replacement was empty. That's fine, and an alternative approach would have been `${file/#+([0-9])*( )}`. Here the leading `#` anchors the pattern to the beginning of the expansion. But I suspect `${file##...}` is perhaps slightly more idiomatic in this situation.
@audiosf Got it via the rest api for cisco! I just wrote a nodeJS script, like our others for the API and successfully deployed my PKCS12 encrypted cert on my firewall! :)
Thanks again, that's good reading. I keep wanting to learn Bash better, but it seems so overwhelming with how confusing things can be, sometimes I don't know where to start without instantly getting lost. It's more confusing than programming languages for me. 
&gt; It's more confusing than programming languages for me. It certainly has its quirks. It isn't really a good _programming language_ at all. It's a process management language that just so happens to have acquired various general programming language features over time.
For sure. Did you read specific material to get better or just a lot of Google?
&gt; Did you read specific material to get better or just a lot of Google? I've been using Bash for about 20 years now. In that time I would have read its manual dozens of times. I've also read [the relevant section](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/toc.html) of [POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/) a few times. "Getting better" at using a language isn't really the important thing. "Knowing how to find out what you need when you need it, and knowing how to recognise _correct_ and _authoritative_ information, rather than mere hearsay" is far more important. So I'm always a bit sceptical about any results Google may throw up. Not because they're necessarily wrong, but because I don't feel comfortable trusting them until I've verified them against authoritative sources.
&gt;and use -h option if you do not want file names Cheers. I use some dodgy Solaris which doesn't seem to have that option :(. We also don't have access to man pages in the servers so can't check what I CAN do... :)
if your grep version doesn't support `-H`, you could add `/dev/null` as another input file (or any other empty file) so that grep will see at least two input files grep -c string *2018073* /dev/null | grep -v ":0" 
for file in *; do mv $file `echo $file |sed 's/blah/feh/'`; done that's a little simpler
I learned a few months back that the `rename` command will use regular expressions to do substitutions. You should be able to do something like this: rename 's/\.m4a$/.m4r/' /var/mobile/* That only does the renaming. To do the complete rename &amp; move, it's possible to do this: rename -n -v 's/\.m4a$/.m4r/' /var/mobile/* | grep -oP "(?&lt;=as ).*$" | xargs -p mv -t /System/Library **Note:** there are some "safety measures" inserted in the commands above. The `-n` on `rename` just prints the output without executing the rename; take out this parameter and it will execute the rename. The `-p` on `xargs` lets you look at the expanded `mv` command and choose whether or not to execute it. Relevant links: https://www.computerhope.com/unix/rename.htm https://www.computerhope.com/unix/xargs.htm https://www.computerhope.com/unix/umv.htm 
That 'rename' command can also move files if you use a regex rule that changes the path in your filenames. You can do this: rename -n 's/\.m4a$/.m4r/ and s{^/var/mobile/}{/System/Library}' /var/mobile/* That `s{}{}` is a different way to write `s///`. What's a bit annoying is, there's two different 'rename' tools on Linux. On my distro here, the 'rename' tool you mean is named 'perl-rename'. And the command 'rename' is a totally different, much simpler tool. It's this one here: http://man7.org/linux/man-pages/man1/rename.1.html
That's super interesting. I didn't know you could move a file by editing its full path. Its the kind of thing that makes sense after you know its possible, but I'd never have gotten the idea on my own to attempt it.
try. true. boolean. cmd.-ring\^ exponent 0... as if Pure.lang. C-§-1
Huh. Good idea! Thank you, Mr /u/ASIC_SP.
\`-vvv\`
You're redirecting the txt file into grep instead of your read loop. while read ip; do curl ... done &lt;myiplist.text | grep
I can see a couple issues, In your while loop, you use "IP" as your variable, but in the curl command you use the lowercase $ip. These two need to match cases. You also seem to be missing a semicolon between the `grep` and `printf` statements, which won't end well. Further, you're piping the input file in to the `printf` command where you want to be piping it in to the `do/done` command. I would suggest the following: for ip in $( cat myiplist.txt); do curl --connect-timeout 6 -k -sL -o /dev/null -w "%{http_code} %{url_effective}\n" https://$ip/installationfolder/; done I prefer using a `for` loop over a `while` loop as I think it ends up being tidier. I also added the `-o /dev/null` option to `curl` to get rid of the output and avoid the need for the `grep/printf` at the end of the previous line. Let me know how you get on :)
Well sweet deal, I really appreciate everything. I do have one last thing to bug you with though! Here is a picture of the script I'm running. It isn't stripping the numbers from the beginning of the name though on the files when I run it, which is odd because I feel like the code is identical to what you helped me with. https://imgur.com/a/UVEOqXf 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/EnEbyhx.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
It doesn't look like that script enables the `extglob` shell option. You will need that for `+(...)` and `*(...)` to work. Your Linux vendor may have set things up so that's automatically enabled in interactive shells, but that won't apply in this non-interactive script.
Score, you saved me again. Thanks brotha, I appreciate everything you've done. 
You should be able to get away with variables for this:
Damn how did I miss that!! 🤦🏼‍♂️ That var works, thanks
Use -f instead of grepping for 200. -f, --fail (HTTP) Fail silently (no output at all) on server errors. This is mostly done to better enable scripts etc to better deal with failed attempts. In normal cases when an HTTP server fails to deliver a document, it returns an HTML document stating so (which often also describes why and more). This flag will prevent curl from outputting that and return error 22.
it worked.thanks
i was thinking about "ls -l | awk '{ print $3 }' | sort | uniq" -- why does "printf" prints the user only one time and not for each file?
I was finally able to fix this. I was able to narrow it down to a bug in RVM. https://github.com/rvm/rvm/issues/4422
Remove `uniqu`. Also parsing ls output is fragile. Here you'd have to write : ls -l | tail -n +2 | awk '{print $3}' | sort
i was asking about printf in /u/[finalduty](https://www.reddit.com/user/finalduty) answer.. but you answered my question anyway, it's not because of printf, but because of parameter "u" in sort command. i somehow missed it. :&gt;
You should investigate `xclip`.
If you know there's only going to be one file in the folder and all you need is the name: bash-4.2$ cd test; ls &gt;&gt; /tmp/file bash-4.2$ cat /tmp/file testile.txt &gt;&gt; means append to the end of the file instead of overwriting 
Nice update. That's much, much better. &gt;Eg: `./rerun ./htmlfiles/ "http-server ./htmlfiles/"` It would be more intuitive to users to allow the second argument to be entered without quotes. Then you could define `cmd` like this: `cmd="${@:1}"` and you'd have well-defined access to any parameters that are sent to the command.
Don't use `ls` for this, you could end up with all sorts of funny characters in the output. bash has filename globbing built in, so you could simply do: (cd test &amp;&amp; echo * &gt;&gt; /tmp/file) I've also put this in parenthesis so it launches in a subshell so you stay in the directory you've run it from, and used `&amp;&amp;` instead of `;` so the echo will only be executed if the cd was successful.
Number 9 (`$(...)` versus backticks) is correct as far as it goes, but the example So use: for file in $(ls); do not for file in `ls`; do is so completely broken that it makes me second-guess the whole article. Item 6 uses single [ instead of [[ even though the author spent the item right before insisting that you should be careful about bash vs sh. Otherwise the other advice seems okay even though insistence of curly brace `${var}` style is a little silly. The effort expended on braces should be put toward being careful about quoting.
`curl -sw '\n%{http_code}\t%{url_effective}\n' $ip | tail -1 | grep -v ^000`
I would add: use zsh and embrace fully the autocompletion
On top of that I can add that using `#!/usr/bin/bash` shebang is not the best idea. On some systems (like Mac) bash is located at /bin/bash, or the environment may be set up to prefer other version of bash located elsewhere. I would go for `#!/usr/bin/env bash` instead. Nevertheless, I agree with you that it's a good set of advices overall.
bash does autocompletion pretty well, whats does Z shell expand on?
Oneliner, supports one or multiple files: find /path/to/dir -type f &gt;&gt; /path/to/dir2/file How it works: * `find` - Command to search for files - https://linux.die.net/man/1/find * `/path/to/dir` - Where to search for the files * `-type f` - Selects only files (not sockets, dirs, etc) * `&gt;&gt;` - Append command output to file (https://www.tldp.org/LDP/abs/html/io-redirection.html) * `/path/to/dir2/file` - Name of the file you want to append to NB: This will give you the complete file path as well. If you want only the filename itself, we can wrap the find command in `basename` (https://linux.die.net/man/1/basename) to remove any path from the front. basename $(find /path/to/dir -type f) &gt;&gt; /path/to/dir2/file
In addition to the [pitfall](http://mywiki.wooledge.org/BashPitfalls#pf1) pointed out by /u/thestoicattack, the aliases are a bad idea. Don't fundamentally change the behavior of standard commands. You'll end up relying on that `alias rm='rm -i'` bailing you out, then one day you log in to a server where that alias wasn't set up yet, or you got a different shell, and suddenly it doesn't bail you out. It's not that bad of an alias, just don't name it `rm`. Give it a different name, like `rmi`or `ri` or `r`. Also, don't export PS1. There's no need to, it just needlessly uses up space in the environment. 
zsh does a great job inferring what you're looking for. /u/l/b [Tab] --&gt; /usr/local/bin
`ls` is a shell script is a definite smell.
&gt; I would prefer it if people used two or four spaces—not tabs—to indent, Ah come on don't impose your deficient editor (and the lame workaround it forced you to pick up) on us.
A script called **trash** or **del** can let you change your mind later: #!/bin/bash #&lt;trash: Find a safe place to move a discarded file or directory. # Remember to take out the trash once a week or so. export PATH=/usr/local/bin:/bin:/sbin:/usr/sbin:/usr/bin tag=${0##*/} logmsg () { echo "$(date '+%F %T') $tag: $@"; } die () { logmsg "FATAL: $@"; exit 1; } # This is intended to be a safe place for a given user to move a # file without copying it across a filesystem boundary. As root: # mkdir /home/.trashcan # chmod 1777 /home/.trashcan # mkdir /home/.trashcan/yourname # chown yourname /home/.trashcan/yourname sub=".trashcan/$USER" case "$#" in 0) die "usage: $0 [files or directories]" ;; *) ;; esac # Look for each file's mountpoint, then move that file to the # trashcan directory beneath it. This avoids cross-filesystem # copies, since we're just renaming the file we want to delete. for arg in $@ do mp=$(/bin/df -P "${arg}" 2&gt; /dev/null | awk 'END {print $6}') test -s "$mp" || die "cannot find mountpoint for $arg" dest="$mp/$sub" test -d "$dest" || die "$dest: not found" logmsg "$arg --&gt; $mp/$dest" mv "$arg" "$mp/$dest" done exit 0 
Literally pitfall #1.
Thanks. I am also planning to add a parameter where path of files can be appended to the command. It can work like this `./rerun htmlfiles http-server`
Regarding number 6, this is why Bash's `[[ ... ]]` construct is just so much better than `[ ... ]` (aka `test`). `[[` and `]]` are shell keywords, which means the parser can be smarter with them. `[` is a shell builtin, which means it has to use ordinary command parsing. In short, using: [[ $variable == foo ]] is _perfectly safe_, even when `$variable` may be empty or begin with a hyphen. The only gotcha you've got to keep in mind is that the RHS of `==` and `!=` is a glob, not a string, so quote it if (and in my opinion, only if) it contains glob characters that you don't want being treated as a pattern. Another nice thing about `[[ ... ]]` is that it mirrors `(( ... ))`, another shell construct for performing numeric operations.
Kindly use GH gist, so we revise it.
You can use the same trick you used at the start, add 0 to coerce it into a number. `0+$5 &gt;= 90 { count++ } END{ print 0+count }`. 
okay, that worked, thanks. Do you know if it's possible what i asked in the other question?
&gt; ;; QUESTION SECTION: So that's the question sent to your DNS server in the DNS request. Look at the `ANSWER SECTION` for the answer. :-)
You can initialize your variables in a `BEGIN` section: awk ' BEGIN { count = 0 } 0+$5 &gt;= 90 { count++ } END { print count } ' "FILE"
Oh man thanks for pointing this out! The status section says "Servfail" which I assume it means there is a problem with the server. Does this suggest there's malicious blocking? For example when using the same DNS server for another site, where it works, the status shows "noerror". I am wondering why the server works with some sites but not others. 
Hi thanks again. You're a legend! I found this article on Brazilian sites that had the exact same problem: https://ooni.torproject.org/post/not-quite-network-censorship/ In short, it seems that it is a misconfiguration problem with the DNS server rather than a malicious action. I think this is also the case with my query. It just is amazing how this looks like deliberate blocking but in reality it isn't. 
I just put this in my bashrc that will show the true source of an executable and if it's an alias: mywhich(){ # check for alias + ls executable source (follow links) [ -z "$1" ] &amp;&amp; return 0 local var1=$(alias "$1" 2&gt;/dev/null) [ "${var1::5}" = 'alias' ] &amp;&amp; echo "$var1" var1=$(which "$1") [ -n "$var1" ] &amp;&amp; ls -lh $(readlink -f "$var1") || return 1 }
Wow, I don't understand all of this, but I'll try it for sure. 
The first `if` checks if the audio file exists and makes it if it doesn't (it assumes pico2wave is installed). Then the while loop starts that runs indefinitely. After sleeping it looks for any processes named 'bash'. If it finds one it sends a notification (again it assumed you have notify-send installed), the it plays the audio file (again assuming you have aplay installed). 
O thanks!
`mv file1.txt file1-$(wc -l file1.txt).txt` Am on mobile so check that before you put it in a loop.
You can't use an arbitrary key without making your script wait for input with a `read`. You can however trap SIGINT so that when you hit ctrl+c it will run the code you designate. Add this to the top of your code: trap ctrl_c INT # trap ctrl-c and call ctrl_c() function ctrl_c() { echo "** Trapped CTRL-C from $0, restarting ..." /bin/bash $0 exit 130 } It's not really advisable to restart another version of your same script when killing that script. It would make it a hassle to kill but it's also redundant. Better off to just kill and restart I think, it's only a few key presses: ctrl-c, up, enter. 
Another solution, to address that mv "$f" "$f_$(wc -l | cut -d' ' -f1)" 
&gt; The output it not what I am expecting. Well, what is the output? :) Your code looks mostly correct to me, so my guess is that you’re getting `file_1.txt_5340.txt` when you want `file_1_5340.txt`. To fix that, change the `mv` command to this: mv $name ${name%.txt}_$size.txt
What exactly do you mean? I get the same results: $ wc -l /tmp/test 3 /tmp/minikube_completion $ wc -l &lt; /tmp/test 3 
"3" is the same as "3 /tmp/test"?
Aah! Misunderstanding. I thought you meant the file name will be on a new line and for some reason it would echo out `4` or something. 
You could generalise this to handle arbitrary file extensions as follows: mv "$name" "${name%.*}_$size.${name##*.}" 
This is fucking excellent! It's actual useful and done with just 109 lines of code. 
Right! So the full thing would be something like this for f in *; do mv "$f" "$f_$(wc -l &lt; $f)" done 
Thank you very much :)
It looks like one of the variables isn't expanding or something like that. &gt;for file in $( ls ); do mv $file $file\_$( wc -l $file) ; done; &gt; &gt; &gt; &gt;usage: mv \[-f | -i | -n\] \[-v\] source target &gt; &gt;mv \[-f | -i | -n\] \[-v\] source ... directory
&gt;mv $name ${name%.txt}\_$size.txt YES! This worked! `paste files.txt sizes.txt | while read name size; do mv "${name}" "${name%.*}_${size}.txt"; done; ls -lht;` &gt;20B Aug 2 09:32 sizes.txt &gt; &gt; 111B Aug 2 09:31 files.txt &gt; &gt;0B Aug 2 09:27 file\_10\_1.txt &gt; &gt;0B Aug 2 09:27 file\_9\_9.txt &gt; &gt;0B Aug 2 09:27 file\_8\_8.txt &gt; &gt;0B Aug 2 09:27 file\_7\_7.txt &gt; &gt;0B Aug 2 09:27 file\_6\_6.txt &gt; &gt;0B Aug 2 09:27 file\_5\_5.txt &gt; &gt;0B Aug 2 09:27 file\_4\_4.txt &gt; &gt;0B Aug 2 09:27 file\_3\_3.txt &gt; &gt;0B Aug 2 09:27 file\_2\_2.txt &gt; &gt;0B Aug 2 09:27 file\_1\_0.txt
See further down in the comment thread: https://www.reddit.com/r/bash/comments/93r27c/append_files_line_count_to_its_name_with_mv/e3gx04g/ Specifically, here is a test setup I just ran (cause I finally booted up my computer) stresler@LEVIATHAN:~/bin/linecounter$ for f in ./test/*; do ls $f; done ./test/bar.txt ./test/baz.txt ./test/foo.txt stresler@LEVIATHAN:~/bin/linecounter$ for f in ./test/*; do wc -l &lt; "$f"; done 8 2 5 stresler@LEVIATHAN:~/bin/linecounter$ for f in ./test/*; do mv "$f" "$f"_$(wc -l &lt; "$f"); done stresler@LEVIATHAN:~/bin/linecounter$ ls test bar.txt_8 baz.txt_2 foo.txt_5 stresler@LEVIATHAN:~/bin/linecounter$ 
[removed]
Can you give an example of a problem you are having? 
Send me a,script that failed in DM
I love ImageMagick. It's super easy to stack multiple filters todo tons of stuff in one invocation. To do what you want try this: convert source.png -flop source.png +append output.png 
Post one of the scripts to pastebin and let's see it. 
Whoa you made it! Thanks, but now I feel a little ashamed for not having learnt this single thing by myself. Go have a good day. You deserve it.
As r/zubie_wanders said, post the scripts up on pastebin.com and it'll be available to the community for evaluation. 
[removed]
Ok, I just figured it out myself. But I have one more problem: I need to get rid of this line: /download/Package I tried grep -Fv 'a href=\"\/download\/Package' but it doen't work for some reason.
FYI, your comment got caught by the spam filter. You should probably avoid using link shorteners, or at least that particular one if Reddit thinks it’s spammy.
You are escaping a " that doesnt need to be escaped while in a ' 
I've got some problems w/ it as well, I'll post it in a few.
&gt; using an actual HTML parser xmllint is a fantastic candidate, OP. I highly recommend this app with `--html` and `--xpath` parameters.
there is a link to the first one of the scripts in the post now
With linux (and most *nix) the computer does what you tell it to, as long as you have access. Its a tool, if you're using it you decide how its used. If you don't understand a command, don't run it until you do. If "trial and error" is the way you want to learn, (which is a perfectly valid way to learn) make sure you either do your "trial" on a system with nothing important on it or make sure you have backups and fully understand how to recover them. 
Strictly speaking about bash you have the --restricted option, that might be part of what you are looking for. [restricted shell documentation](https://www.gnu.org/software/bash/manual/html_node/The-Restricted-Shell.html)
Setting up a virtual machine is something you won't regret, in particular because it's surprisingly easy. I recommend VirtualBox. It's available for free on every OS that matters. You said you haven't set up a VM yet because you're lazy. Well, I know something about laziness too. Setting up a VM is *way* less work than thoroughly researching every command to make certain it is safe before executing it.
Agreed. If someone is willing to invest the time into learning Docker (and we're not talking about a huge amount of study), the payoff is big, particularly if you're only interested in the command line. I suggested VirtualBox primarily for the easy learning curve. (and VB is useful for so many different cases, whereas Docker serves a niche purpose) Anyway,... yeah if you're willing to learn Docker, do it. I support this choice.
New reply since you updated your post: #First script: You expect a second input: cmd=$1 ourpath=$2 result=1 for directory in "$ourpath" oldIFS=$IFS IFS=":" But then reject a second input: if [ $# -ne 2 ] ; then echo "Usage: $0 command" &gt;&amp;2 exit 1 fi You need to decide whether you want the user to specify a path string or not. Also, remove the quotes from $ourpath. for directory in "$ourpath" for directory in $ourpath #Second script: You have a typo: `validchars` != `vaildchars` validchars="$(echo $1 | sed -e 's/[^[:alnum:]]//g')" if [ "$vaildchars" = "$1" ] ; then 
If you're "too lazy" to setup a virtual machine to act as a test bed, then dont test commands until you're sure you know what they do. Read the man pages.
Thank you so much, I played with the the commands you said were wrong and now it works like the book said it would!!! Thanks you so much!!
That is for (bad) sandboxing, for example if you want to run a web script so that it cannot be easily hacked to perform unexpected operations. For normal use the restrictions such as inability to change working directory makes it unusable. 
I'm not clear what they were trying to show you with the `cat &lt;&lt; EOF` since I never use that style, but does this do what you want? initializeANSI echo -e ${yellowf}This is a phrase in yellow${redb} and red${reset} echo -e ${boldon}This is bold${italicson} this is italics${reset} bye-bye echo -e ${italicson}This is italics${italicsoff} and this is not echo -e ${ulon}This is ul${uloff} and this is not printf "${yellowf}This is a phrase in yellow${redb} and red${reset}\n" printf "${boldon}This is bold${italicson} this is italics${reset} bye-bye\n" printf "${italicson}This is italics${italicsoff} and this is not\n" printf "${ulon}This is ul${uloff} and this is not\n" 
Be very careful with `rm -rf` in scripts folks. rm -rf ${PATH_TO_MY_FOLDER}/* Oops, I forgot to set my variable!! That means the command I just ran was interpreted to `rm -rf /*`, and it will happily comply as best it can. 
Not really a bash thing - more of a Linux thing. Most of the filesystem cannot be written to by a non-root user. Many devices (/dev) cannot be used by non-root users. Some distributions leave /sbin and /usr/sbin off the path for non root users to make utilities that need root not "show up". In short, normal user accounts are pretty safe. Running things as root, however, gives you plenty of rope. Check out [Suicide Linux](https://qntm.org/suicide)
Also don't forget to check success of commands like cd. If you are not in the directory that you think you are in, rm anything can be devastating...
I can help, but would need a little more information. The file format you specify (Two-Words-date\_in\_YYYYMMDD-irrelevant\_number.pdf), is there any part of the filename that changes with each download other than the date? I wouldn't use a "while true" loop, I'd use a systemd timer or a cron job to execute the script however frequently you want it to check for new downloads.
No, the date is the only part of the file name that changes. In regards to your second comment, the trouble is that I don't necessarily do this at regular intervals (like I'm supposed to). That's why why I was thinking a "while true" loop. In all honesty though, the "while true" thing was just an idea. The code in my post is an amalgamation of pseudocode and actual code.
use tr head and tail
Something like this should do what you want. printf '%s %s\n' "$(/bin/first)" "$(/bin/second)" printf '%s %s\n' "$(/bin/third)" "$(/bin/fourth)" 
Definitely the way to go
So given these four inputs: This is thing number one. It’s fairly boring. This is thing number two. It has a very long line. I am already getting tired of making something up. Why didn’t I use the Lorem Ipsum or whatever? This was a terrible idea, why am I still continuing with thi— File three is aligned but with spaces not with tabs (this is important) File four is, uh, honestly I’m running out of ideas. This command will wrap each input at 40 characters and them paste them next to each other at a distance of 50 characters, with a separator line between them. $ expand -t50 &lt;(paste &lt;(fold -w40 1) &lt;(fold -w40 2)) &lt;(printf '%*s\n' 100 | tr ' ' '-') &lt;(paste &lt;(fold -w40 3) &lt;(fold -w40 4)) This is thing number one. This is thing number two. It has a very It’s fairly boring. long line. I am already getting tired of making something up. Why didn’t I use the Lorem Ipsum or whatever? This was a terrible idea, why am I still continuin g with thi— ---------------------------------------------------------------------------------------------------- File three File four is, uh, is aligned honestly I’m running out of ideas. but with spaces not with tabs (this is important) If any of the inputs may contain spaces, you’ll want to `expand` those early on (probably before the `fold`), otherwise they’ll be blown up by the outer `expand -t50` (which is intended to only expand those tabs added by the `paste`).
By the way, [here’s a screenshot of that comment](https://i.imgur.com/qxwakKu.png), because reddit seems to be using two different Markdown parsers now and they disagree about whether I indented the code in the list correctly. (For what it’s worth, I’m following [CommonMark](https://commonmark.org/) rules.)
This will loop through the arguments fed to the script and launch functions : while [[ $# -gt 0 ]]; do case "$1" in action_one) action_one_function ;; action_two) action_two_function ;; esac shift done
Great! There's a lot here for me to experiment and discover. Thanks for the suggestions.
tbl and groff should be able to handle this.
The internet.
I think the key is to keep reference handy. Memory comes through usage, the ones that are most applicable to my use case are remembered. I don't stress over amount of material memorized. 
It doesn't work because your script tests if ALL your command line args, `$@`, = some_action. You could either A) use a bash pattern match that sees if `$@` contains a substring for that option: `if [[ "$@" = *action_one* ]]; then`, or treat each one seperately in a `while` + `shift` (as shown by /u/Erelde) or using `getopts`. 
Tbh? Don’t try. Just reference SO or other like repos of knowledge. One only needs to check the Twitter of every major dev ever to see they do the same.
You have to use it. Things you do commonly you'll remember. Don't worry about learning all of everything. No one does. I reference things all the time and I know advanced and obscure bash commands/substitutions.
What problems?
&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vvv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vvC+++####SHA3x3333x33x33x3x3x33324x4x423234x432x4x4423423a234a234a234ssadfasdfsafdgsa&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vvv&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vvC+++####SHA3x3333x33x33x3x3x33324x4x423234x432x4x4423423a234a234a234ssadfasdfsafdgsa&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1&lt;A.shell&lt;Handshake?SSH #sha.3.fb.com.xss.edge.C++....C++ Bash! Su.-1vvvvvvvv
&gt;As the old man says by learning never stops!!
The way I remember it (shamelessly stolen from someone else): it's the same as `cp`, `mv`, etc. - the first argument exists already, the second argument doesn't exist yet.
Hey there, Could you clarify for me, please, what you mean when you say "the first arguments exists already"? Cheers, 
Cheat Sheets, References and Search Engines. You'll memorize the repetitive ones. The rest you keep looking them up. Until they become repetitive to memorize those as well.
Look into using `find` with `exec` or `xargs`. https://www.everythingcli.org/find-exec-vs-find-xargs/
Would stow work for this? [https://www.gnu.org/software/stow/manual/stow.html](https://www.gnu.org/software/stow/manual/stow.html) [https://www.youtube.com/watch?v=zhdO46oqeRw](https://www.youtube.com/watch?v=zhdO46oqeRw)
I always think of ln like cp. same syntax. 
Here's an uncomfortable truth: memorizing stuff won't make you elite programmer you are aspiring to be. When you memorize something, you use it habitually and vice versa. You memorize things like your password and the floor your office is on and the route from home to work. Memorization isn't flexibility. A person who only knows the path to work and has never explored the rest of the city isn't prepared to find a new route when there's traffic. And a programmer who only knows specific memorized methods to a solution isn't prepared to adjust their solution when the circumstances change. The others in this thread suggesting that you just look it up have the right idea. I assume you're interested in programming because you love what technology can do. You probably use bookmarks for common websites instead the URL and typing it each time, and you probably use calendar reminders in Outlook instead of keeping track of the time all day long in anticipation of appointments. You're familiar with utilizing tools for functions that fit with the strongest attributes of the tool. Use reference materials as a tool in this same way. Exploration gives you a sense of what's out there and what you can look for, and even potentially what obstacles exist. For example: you know that there's a way to copy files and also create the directories that contain the files (several layers deep), but you've forgotten the name of both the `cp` command and the `--parents` parameter. A quick Google search for "unix shell copy" should give you `cp`, and if you run `man cp` (or Google the equivalent) you can browse the available parameters. You might recall `--parents` when you see it, or if it doesn't even ring a bell then you can read through what each parameter does. Chances are if you're reading through the available parameters, you might see a parameter that looks interesting. If you have time, try out the parameter on something inconsequential. If you don't have time, hopefully you'll remember that the operation exists (even if you don't remember the name of the parameter or the syntax). If you put your emphasis on learning that tools and operations exist, place exploration over memorization, and learn how to use reference materials effectively -- not only to quickly find what you need, but also to branch out and discover new things -- I think you'll be a lot more successful than you'd be if you memorized volumes of knowledge. 
How about: find test/ -type f -exec ln -s {} test2/ \;
Its cool isn't it. I used to write scripts before learning I can write for loops on the command line.
Exactly.
Thank you. Now I remember this too ;)
Being a programmer, or just a tech job in general, I think requires you to be a good researcher. I cannot imagine not knowing how to google/search effectively for solutions to my problems. I can basically find anything I am looking for online. They really should teach effective online research in school as a mandatory subject. Yeah it is good to know things, but it is also good to know how to ask for help.
Also, use tools to remember for you. In the case of bash : shellcheck.
That's cool! I need to play around with this. Thanks for the tip. I really like Explain Shell. I really dig how I can submit a command to Explain Shell via my browser's search/address bar. I don't even need the 1.5s I save, but it makes me happy.
You want to store the directory structure of the USB drive? Might be easier to save the output from `tree` or something similar to reference.
I'd suggest using the `tree` command ([man tree](https://linux.die.net/man/1/tree)) and piping that to a file. e.g: tree /home/il/il/Videos/test/ | tee /home/usb_files.txt To answer your question regarding symlinks, you'll need a script that replicates the directories and then symlinks the files in to them. Try this: #!/bin/bash original_dir=$(pwd) src_dir=~/tmp/dir1 dst_dir=~/tmp/dir2 ## Copy directory tree. The --include and --exclude options only selects directories rsync -ai --include "*/" --exclude "*" $src_dir/ $dst_dir/ ## Change to $src_dir to start, exit if that fails cd $src_dir || exit 1 ## Iterate over all directories that can be found under $src_dir, including itself. for dir in . $( find . -type d | sed 's|\./||'); do ## Change to the directory to find files in, then symlink them to the new location. cd $src_dir/$dir &amp;&amp; find $PWD -type f -maxdepth 1 -exec ln -sv {} $dst_dir/$dir \; done ## Change back to original directory. NB: Only useful if copy/pasted directly to terminal cd $original_dir We have to fiddle around a bit to get the paths and filenames just right so they link in well. Also, this doesn't handle files being removed from the USB that well. You would need to add `rm -rf $dst_dir/*` in before the rsync command. I deliberately left that out so that no one accidentally destroys files they wanted to keep. This script gives us the following: [finalduty ~/tmp]$ tree dir1 dir2 | sed 's|/Users/andyd|/home/finalduty|' dir1 ├── file1 ├── file2 ├── subdir1 │ ├── file1 │ ├── file2 │ └── subsubdir1 │ ├── file1 │ └── file2 └── subdir2 ├── file1 └── file2 dir2 ├── file1 -&gt; /home/finalduty/tmp/dir1/file1 ├── file2 -&gt; /home/finalduty/tmp/dir1/file2 ├── subdir1 │ ├── file1 -&gt; /home/finalduty/tmp/dir1/subdir1/file1 │ ├── file2 -&gt; /home/finalduty/tmp/dir1/subdir1/file2 │ └── subsubdir1 │ ├── file1 -&gt; /home/finalduty/tmp/dir1/subdir1/subsubdir1/file1 │ └── file2 -&gt; /home/finalduty/tmp/dir1/subdir1/subsubdir1/file2 └── subdir2 ├── file1 -&gt; /home/finalduty/tmp/dir1/subdir2/file1 └── file2 -&gt; /home/finalduty/tmp/dir1/subdir2/file2 6 directories, 16 files 
+1 for `/etc/apt/sources.list.d/`. This works well with config management tools as well as you just add/remove the whole file. I strongly recommend doing it this way. To add to what you wrote, instead of using `grep -v`, OP can use `sed -i.bak '/&lt;regex&gt;/d' $filename` to remove a file. So for example, OP can go: # sed -i.bak '/deb http:\/\/example.com/d' /etc/apt/sources This will create a backup file called `/etc/apt/sources.bak` and remove **ANY** line that matches the regex from `/etc/apt/sources`, so it's advised to test the output first by removing the `-i.bak` option. Alternatively, a safer option would be to comment the lines until the user is comfortable with how it works. eg: # sed -i.bak '/deb http:\/\/example.com/ s/^/#/ /etc/apt/sources
like you learn any other language, practice. Apart, build your library - every single thing you do with bash can be automated - everything you do should be in script form - you will use them over and over again, fix over and over again and enhance. This practice alone burns those functions to your brain - the brain prefers to safe things deemed required for survival - if you show your brain you need this for survival it will take all the input &amp; keep the important survival information, lol
 cp --recursive --symbolic-link /home/i1/i1/Videos/test/* /home/i1/i1/Videos/test2
 cp --recursive --symbolic-link /home/i1/i1/Videos/test/* /home/i1/i1/Videos/test2 
At least I tried to help, and I was not on my computer at that time. Maybe you should have spent more time in helping OP, than down voting and linking to prove your "bad advice" post. 
[removed]
One way would be to run the file through *wc* **after** its downloaded: curl "https://www.website.com/page&amp;model=${m//+/%2B}&amp;" -# &gt; ${Script_dir}/comp/"${m}"_hdds_incompatible.txt wc -c ${Script_dir}/comp/"${m}"_hdds_incompatible.txt 
This didn't work for some reason, but I was able to do it with echo $(stat --printf="Size: %s" "${Script_dir}/comp/${m}_hdds_compatible.txt") Thanks. Do you know how to download mayn files like this with parallel or any other way?
Okay, how would I do this with lftp? Can I somehow just pass the array to it or will I have to use a config file? If I just replaced curl with lftp in the loop, it would still download every file, wait for it to finish and then download the next file etc, right? 
&gt; Okay, how would I do this with lftp? Oh blimey. Look at the documentation? You can start off downloads in the background, and use `wait` to wait for their completion. &gt; If I just replaced curl with lftp in the loop, it would still download every file, wait for it to finish and then download the next file etc, right? Not exactly... I think of `lftp` more a shell. It can run an "lftp script" using the `-f` option.
show us the newest version of the script with all the edits
Bash-Sciprt: `echo -e "\nModels: ${Models[@]}" echo "Downloading In-/Compatibility-lists:" echo "set net:connection-limit 10" &gt; "${Script_dir}/comp/lftp.cfg" echo "set xfer:clobber yes" &gt;&gt; "${Script_dir}/comp/lftp.cfg" for m in "${Models[@]}" do echo 'echo getting /comp/'"${m}"'_hdds_compatible.txt' &gt;&gt; "${Script_dir}/comp/lftp.cfg" echo 'get "https://www.website.com/api/compatibility/findHclList?lang=en-global&amp;search_by=products&amp;model='"${m//+/%2B}"'&amp;category=hdds&amp;usage_id=12&amp;recommend=t" -o "'"${Script_dir}"'/comp/'"${m}"'_hdds_compatible.txt"' &gt;&gt; "${Script_dir}/comp/lftp.cfg" "${Script_dir}/comp/${m}_hdds_compatible.txt") echo 'echo getting /comp/'"${m}"'_hdds_incompatible.txt' &gt;&gt; "${Script_dir}/comp/lftp.cfg" echo 'get "https://www.website.com/api/compatibility/findHclList?lang=en-global&amp;search_by=products&amp;model='"${m//+/%2B}"'&amp;category=hdds&amp;usage_id=12&amp;recommend=f" -o "'"${Script_dir}"'/comp/'"${m}"'_hdds_incompatible.txt"' &gt;&gt; "${Script_dir}/comp/lftp.cfg" #echo $(stat --printf=", Size: %s" #this doesnt work "${Script_dir}/comp/${m}_hdds_compatible.txt") done echo "bye" &gt;&gt; "${Script_dir}/comp/lftp.cfg" lftp -f "${Script_dir}/comp/lftp.cfg" echo "done."` The Script i need cls | tail -n1 in, is a different one. It should like this: #!/bin/bash lftp &lt;&lt; EOF set net:connection-limit 10 set xfer:clobber yes open "https://archive.website.com/packages/" cls # I can get all directories with this. # now I need to go into every directory and get the last directoryname. this doent work as bash doesnt work in lftp it semms, so i am not sure how do to this: declare -a directories for m in "${directories[@]}" do echo "${m}: $(cd ${m}; dir | tail -n1)" &gt;&gt; some_new_file done # bye EOF
Ah, so you have a script making a custom script, that's tricky. First, I would say if you want to run them in parallel have you looked into just doing `lftp` or `curl` with the `&amp;` then `wait` option? Second, are you downloading just files or is it folders too? I'm trying to understand if you want to print out each file size / if it downloaded a folder all the file sizes within that folder?
It is faster now, that i use the lftp-config, so parallel is no longer an issue really. For the second part: I just want to print the filesizes of the files, that I downloaded with lftp. Those are files only, not directories. Third, this is the part with the folders: I only need to get the name of last folder in the subfolder, as the Version-numbers I need are only the names of the last folder. lftp cls does the same as ls in bash: list only the foldernames without attributes, size etc. But it does this like this: FOLDER1/ /FOLDER2 /FOLDER3 /FODLER4 What is basically waht to do is build a array of the fodlernames, then cd into them and get the result of dir | tail -n1 which outputs drwxr-xr-x - 2018-01-02 23:45 7.3.0.21018 bascially, I only need the "7.3.0.21018"-part. If I could somehow output these result to a file, I could just awk over them i n bash and get the numbers, so that is not the bigger problem. So basically I need to populate an array of the subfoldernames, cd into each of them and get the fodlername, that "dir | tail -n1" outputs, if I do it manually. then output the results to a file.
For finding file sizes, from what I see you already know the name of the files because you specify them with the `-o` option. The easiest thing would probably be to just loop over `${Models[@]}` after you download them and do something like `ls -l "make_the_filename_again-$m.txt" |awk '{print $5}'` to get the file size. For the folders thing I'm not 100% sure what you're asking. To get an array of all the folders in the current directory you can do this: `dir_array=($ls -d */))` assuming your directory names don't have any spaces (otherwise you need to do a `find`). Then you can loop over them like normal. Also if you were asking how to get the last directory of a path, for instance if your `PWD=/path/to/some/dir/name1`, you can get name1 with `${PWD##*/}`.
I need those commands for lftp. dir_array=($(ls -d */)) gives me unknown command, when im in lftp. this is the problem. i know how to do it in bash, but i dont know how or if i can send bash commands to lftp and the output again to bash...
Have bash run your lftp script, then when it's done continue with a normal bash script. #!/bin/bash bash mylftpscript.sh dir_array=($(ls -d */)) ... 
A bit on the side of what you asked but why not rm out/* bin/* -r?
I'm not aware that bash provides any way to do this. If it's helpful, though, zsh does, and if you restrict yourself to bash-compatible syntax it should usually (but probably not always!) produce the result you expect: $ get_tokens() { zsh -fc 'print -rl - "${(@z)1}"' -- "$1"; } $ get_tokens '(echo hello;) 2&gt; /dev/null' ( echo hello ; ) 2&gt; /dev/null 
I don't think awk has a hash structure like that so, if it works at all, it's treating it like a string. It looks like you are appending var to the end of Stock.txt and then trying to subtract a number from a string for each line in the file. Instead, you need to pass var into awk, split in on space in BEGIN, find the line that contains var[1]" "var[2], and subtract the provided numeric value, var[3], from $3 on the matching line.
The `printf` command is capable of printing octal codes, amongst other things - [man printf(1)](https://linux.die.net/man/1/printf) $ printf '\145\143\150\157\n' echo 
This only prints out the word "echo." I want to run the command echo with arguements 
`$(printf '\145\143\150\157\n') hello` ?
I have included an example in the post. The command needs to run through either a bash or sh CLI. It can be loaded from a file, but the file cannot contain alpha chars (a-zA-Z)
I'm sorry, my explanation was not clear. The entire command must not contain any alpha chars (a-zA-Z). In this case "printf" would not be accepted.
I think i am on the right track, but I have one problem: for v in "${PackageArray[@]}" do echo 'echo '"${v//\/}"' Version: ' &gt;&gt; "${Script_dir}/comp/lftp2.cfg" echo 'open "https://website.com/download/Package/spk/'"${v//\n}"' test; dir | tail -n1 | awk "{print $5}" -o "${package_versions}"' &gt;&gt; "${Script_dir}/comp/lftp2.cfg" this expands as this in my cfg.file: echo Package Version: open "https://website.com/download/Package/spk/Package/ test; dir | tail -n1 | awk "{print $5}" -o "${package_versions}" But I need to remove the newlines after an element of the array is expanded, like this: echo Software Version: open "https://archive.synology.com/download/Package/spk/ActiveBackup/test; dir | tail -n1 | awk "{print $5}" -o "${package_versions}"
See if this works.... Run the whole command inside back ticks and assign that to a variable then echo it out. Like this.. #!/bin/bash varName=`command` echo varName exit 0
i="\\145\\143\\150\\157\\40\\150\\145\\154\\154\\157" eval \`printf $i\`
Looks interesting. Seems pretty useless for Bash but I always like to see pointless esoteric implementations just for fun :)
Okay thanks. The problem was in my Array. I didn't know readarray would add newlines. I use readarray -t now and everything works. It took me far too long to figure this out :D
Haha, that's programming. Right now at my work I've been trying to find and fix the same bug for 2.5 weeks. 
lol me too
To be good software you should have some testing, like [travis ci](https://travis-ci.org/). You can also have a fancy badge for passing builds like on the [matplotlib github](https://github.com/matplotlib/matplotlib). I think it can also [test in cent os](https://djw8605.github.io/2016/05/03/building-centos-packages-on-travisci/), so you could test it that way. What errors is shell check giving you?
Don't forget about lshw! It has all kinds of nice stuff for what you are doing. lshw -class system in particular has some nice goodies.
[removed]
If you can't find out how to do this with the network settings tools built into Ubuntu, you can cheat a little like this: You can manually edit the file named `/etc/resolv.conf` and set your DNS servers. After you have edited the file, you can then lock it so that it can't be changed anymore: sudo chattr +i /etc/resolv.conf That command makes the file "immutable", making it so it can't be changed or overwritten.
Apparently `pv` can actually do this: pv -L 100K load-script.sql | mysql The argument is the data rate per second, with optional K/M/etc. suffix (base 2, i. e. factor 1024, not 1000).
What happens if you run it in the ssh in the background? `func(){ssh ... '...' &amp;;}`?
What if you just run it without putting it in the bg and then exit? `func(){ssh ... 2&gt;&amp;1; exit; }` ?
Thank you, /u/Working_Lurking. This is definitely not the first draft!
That sounds promising. I'll take a look at that suggestion and will bug you if I get stuck. 
&gt; ignore CTRL+C, CTRL+Z and quit signals using the trap In the name of God, why? 
Were you the one who said `&amp;&gt;`? Did you edit out that part in your comment and if so, why? If I were to remove `&amp;` (i.e. without putting it into background), then Firefox would load nothing. [Not sure if this is the reason](https://www.reddit.com/r/linux4noobs/comments/9575t5/ssh_localcommand_not_working/e3r3gla/).
I concur with /u/causa-sui , BATS is better than anything I know. Also, why are you ignoring SIGINT in the code?
Firefox will not load the web GUI. [Not sure if this is relevant](https://www.reddit.com/r/linux4noobs/comments/9575t5/ssh_localcommand_not_working/e3r3gla/)
You were looking for system info. That is pretty much exactly what "ls hardware" does.
So while I can't reproduce your command, this [stack exchange post](https://askubuntu.com/questions/349262/run-a-nohup-command-over-ssh-then-disconnect) might be some help. Pretty much my best guess now would be: func(){ ssh -L 9999:localhost:8384 pi 'nohup firefox 127.0.0.1:9999 &amp;&gt;/dev/null &amp;' } What I was doing to test it was trying to run a `sleep 10; notify-send message` so that I would know if it slept and exited and still lived and carried out my command becuase `notify-send` would pop up with a gui notification 'message' 10 seconds later. Pretty much I couldn't get it to work as a one-liner or stand alone function. However if I made a script named sshtest.sh which had for contents: sleep 10 notify-send 'message from ssh' Then I ran `ssh localhost 'nohup bash sshtest.sh &amp;&gt;/dev/null &amp;'` which works perfectly. 
`func` appears to just ssh to pi then exit the session without launch firefox at all. What I want to do is simply ssh tunnel web GUI via firefox cleanly. I only want the ssh session to live until firefox is closed. The only reason I run it as a background process is because for some reason, firefox cannot load the web GUI without it being started in the background. I think it has to do with `LocalCommand` as mentioned in the thread I've linked. I'm not sure how you're able to execute anything without using `LocalCommand=`--I tried that and it doesn't execute the command.
I was curious what unit tests would look like in this use case? To my mind, the feedback you receive from the script are just strings specific to your environment. How do you test that? Could you, in theory, get 100% coverage? 
Well for one I was trying to see how hard it would be to automate your script and when I ran `bash linux_sys_info.sh &lt;&lt;&lt;'9\n'` the program crapped itself and I had to kill the whole terminal to kill the process. SIGINT, SIGQUIT, and SIGSTP are how the OS talks to processes. If you make your script ignore these you can create diffcult or impossible to kill processes. I don't see your logic as to ignoring these signals in response to ctrl-c being unresponsive for a portion of your script. Explain what you mean by it would be unresponsive, like it wouldn't respond to ctrl-c at first but when it finally returned control to the script it would then kill it?
Explain to me what does the `pi` part of the command for?
So this does exactly that on my localhost, I have a file named sshtest.sh in my home directory that contains: export DISPLAY=':0' firefox &amp; wait I don't know why I had to define DISPLAY to get it to work but whatever. Then if I run `ssh localhost 'bash sshtest.sh'`, it opens a firefox window and my terminal waits. Once I close the firefox window my ssh command ends and I see my PS1 again. 
I'm certainly not wedded to the idea of using a signal trap. When I tested the function on disks with TBs of storage and wanted to quit the process with `Ctrl + C/Z` the system would just hang and not do anything. I'd have to open another terminal, find the process in `top`, then kill it with the PID. I think you've made a good case for why this isn't needed. 
This is doable completely within `awk` but you asked in a bash subreddit so I'll explain how to do it in bash. Lets say you have `var1='name1 name2 num'`, and your database file is Stock.txt. First you need to find where 'name1 name2' occurs in Stock.txt. To do that we need to extract 'name1 name2' from var1. This can be done with `cut`. Now you can use that to `grep -n` on Stock.txt along with `cut` to extract the line number where it occurs along with the current number in stock. Since it's integer math you can do that within bash math mode, `$(())`, and use conditionals, `if`, `else`, and `elif` to see if it will be negative and output accordingly. Then you can use `sed` to replace that line with the new one with the updated stock number. 
DISPLAY is an environment that tells X windows where to forward the window. In this case, you're forwarding it over the ssh tunnel.
\[removed\]
Nice problem! Give this a go: version_cmp() { if (( $# != 3 )) || [[ $1 != +([0-9])*(.+([0-9])) || $2 != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) || $3 != +([0-9])*(.+([0-9])) ]]; then printf 'Usage: version_cmp VERSION { == | != | &gt; | &gt;= | &lt; | &lt;= } VERSION\n' &gt;&amp;2 return 127 fi local op=$2 local -a x y IFS=. read -r -a x &lt;&lt;&lt;"$1" || return $? IFS=. read -r -a y &lt;&lt;&lt;"$3" || return $? while (( ${#x[@]} &amp;&amp; ${#y[@]} &amp;&amp; x[0] == y[0] )); do x=( "${x[@]:1}" ) y=( "${y[@]:1}" ) done # shellcheck disable=SC2086,SC1105 (( ${#x[0]} $op ${#y[0]} )) } It's minimally tested!
Nice problem! Give this a go: version_cmp() { if (( $# != 3 )) || [[ $1 != +([0-9])*(.+([0-9])) || $2 != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) || $3 != +([0-9])*(.+([0-9])) ]]; then printf 'Usage: version_cmp VERSION { == | != | &gt; | &gt;= | &lt; | &lt;= } VERSION\n' &gt;&amp;2 return 127 fi local op=$2 local -a x y IFS=. read -r -a x &lt;&lt;&lt;"$1" || return $? IFS=. read -r -a y &lt;&lt;&lt;"$3" || return $? while (( ${#x[@]} &amp;&amp; ${#y[@]} &amp;&amp; x[0] == y[0] )); do x=( "${x[@]:1}" ) y=( "${y[@]:1}" ) done # shellcheck disable=SC2086,SC1105 (( ${#x[@]} $op ${#y[@]} )) || (( ${x[0]-0} $op ${y[0]-0} )) } It's minimally tested! Note that it will treat your 2.1.4.0142 example as if it were actually 2.1.4.142; that is, it ignores leading zeroes in each version component. 
Nice problem! Give this a go: version_cmp() { if (( $# != 3 )) || [[ $1 != +([0-9])*(.+([0-9])) || $2 != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) || $3 != +([0-9])*(.+([0-9])) ]]; then printf 'Usage: version_cmp VERSION { == | != | &gt; | &gt;= | &lt; | &lt;= } VERSION\n' &gt;&amp;2 return 127 fi local op=$2 local -a x y IFS=. read -r -a x &lt;&lt;&lt;"$1" || return $? IFS=. read -r -a y &lt;&lt;&lt;"$3" || return $? while (( ${#x[@]} &amp;&amp; ${#y[@]} &amp;&amp; x[0] == y[0] )); do x=( "${x[@]:1}" ) y=( "${y[@]:1}" ) done # shellcheck disable=SC2086,SC1105 (( ${#x[@]} $op ${#y[@]} )) || (( ${x[0]-0} $op ${y[0]-0} )) } It's minimally tested! Note that it will treat your example `2.1.4.0142` as if it were actually `2.1.4.142`; that is, it just compares each version component numerically, so leading zeroes are not considered. I think that's probably OK.
In that case one of your version numbers mustn't actually be a real version number. Either that, or I've screwed up the validation.
The script seems to run correctly, but I think am calling it wrong. Whats the correct way to call it like this? if (( $("version_cmp ${PureVerInstalled} == ${PureVerAvailable}") ));then echo "same Version!" fi
 As I said, if you just want to compare versions for equality, don't use my function.
I was saying I don't understand why I have to set DISPLAY when it's set before I ssh. It's not forwarding over X11 because I'm not ssh'ing with `-X`, it's simply opening a GUI window on the 'server' and since I'm on the the same machine I'm ssh'ing into I see the window pop up. 
I am quite new to bash, so I thought i had to (( .. )) :) It works now sometimes, but sometimes it does not. For example heres a bash -x of when it doesnt work. The Version numbers are exactly the same. Is this maybe because the last number has a leading 0? 'OAuthService: Installed: 1.0.3.0029 vs. available: 1.0.3.0029 + version_cmp 1.0.3.0029 == 1.0.3.0029 + (( 3 != 3 )) + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + [[ == != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) ]] + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + local op=== + local -a x y + IFS=. + read -r -a x + IFS=. + read -r -a y + (( 4 &amp;&amp; 4 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 3 &amp;&amp; 3 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 2 &amp;&amp; 2 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 1 &amp;&amp; 1 &amp;&amp; x[0] == y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 209: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + (( 1 &amp;&amp; 1 )) + (( x[0] == y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 216: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + version_cmp 1.0.3.0029 '&lt;' 1.0.3.0029 + (( 3 != 3 )) + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + [[ &lt; != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) ]] + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + local 'op=&lt;' + local -a x y + IFS=. + read -r -a x + IFS=. + read -r -a y + (( 4 &amp;&amp; 4 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 3 &amp;&amp; 3 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 2 &amp;&amp; 2 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 1 &amp;&amp; 1 &amp;&amp; x[0] == y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 209: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + (( 1 &amp;&amp; 1 )) + (( x[0] &lt; y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 216: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + version_cmp 1.0.3.0029 '&gt;' 1.0.3.0029 + (( 3 != 3 )) + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + [[ &gt; != @(==|!=|&gt;|&gt;=|&lt;|&lt;=) ]] + [[ 1.0.3.0029 != +([0-9])*(.+([0-9])) ]] + local 'op=&gt;' + local -a x y + IFS=. + read -r -a x + IFS=. + read -r -a y + (( 4 &amp;&amp; 4 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 3 &amp;&amp; 3 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 2 &amp;&amp; 2 &amp;&amp; x[0] == y[0] )) + x=("${x[@]:1}") + y=("${y[@]:1}") + (( 1 &amp;&amp; 1 &amp;&amp; x[0] == y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 209: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + (( 1 &amp;&amp; 1 )) + (( x[0] &gt; y[0] )) /home/thomas/Dokumente/syn-git/debug_entpacker/entpacker.sh: Zeile 216: 0029: Der Wert ist für die aktuelle Basis zu groß. (Fehlerverursachendes Zeichen ist \"0029\"). + echo -ne '\e[101msome error occured: ' some error occured: + echo -e 'OAuthService: Installed: 1.0.3.0029 vs. available: 1.0.3.0029\e[0m' OAuthService: Installed: 1.0.3.0029 vs. available: 1.0.3.0029'
Just in case anyone here doesn't know: to install pv on CentOS sudo echo "[ivarch] name=RPMs from ivarch.com baseurl=http://www.ivarch.com/programs/rpms/$basearch/ enabled=1 gpgcheck=1" &gt;/etc/yum.repos.d/ivarch.repo rpm --import http://www.ivarch.com/personal/public-key.txt yum -y install pv 
I think /u/aioeu has a nice solution, it just seems like a sledgehammer to drive a nail with all those regex. Here's my solution, it just get's the first number of each version and keeps comparing them until it gets an inequality. shopt -s extglob # extended pattern matching ver_valid(){ case "$1" in +(+([0-9])?(\.))) echo true;; *) echo false;; esac } ver_cmp_gt(){ # true if version of arg1 &gt; arg2 if ! $(ver_valid $1) || ! $(ver_valid $2); then echo "Invalid input"; return 1 fi local ver1=${1#*\.}; local ver2=${2#*\.} local rev1=${1%%\.*}; local rev2=${2%%\.*} while [ "$ver1" != "$rev1" ] &amp;&amp; [ "$ver2" != "$rev2" ]; do rev1=${ver1%%\.*} rev2=${ver2%%\.*} if [ $rev1 -gt $rev2 ]; then echo true; return 0 fi ver1=${ver1#*\.} ver2=${ver2#*\.} done if [ $rev1 -gt $rev2 ]; then echo true; return 0 fi echo false } 
If found this solution when I googled the same question a few years ago, and I have been using it since: https://stackoverflow.com/a/4025065
The 'sort' tool for sorting text lines can sort version numbers with its `-V` parameter. It also has a `-c` parameter where it reports if input lines are sorted or not through its exit status. You could use that to see if your $PureVerAvailable is newer than $PureVerInstalled. You would pipe your two version numbers into sort's input as two text lines. Doing that could look like this: if ! printf "%s\n%s\n" "$PureVerAvailable" "$PureVerInstalled" | sort -cV &amp;&gt; /dev/null ; then echo "Update to $PureVerAvailable available!" fi Or maybe like this to make it a bit more self-documenting: if ! printf "%s\n%s\n" "$PureVerAvailable" "$PureVerInstalled" | sort --check --version-sort &amp;&gt; /dev/null ; then echo "Update to $PureVerAvailable available!" fi There's a `!` at the beginning of the line to negate the logical true/false result of sort's exit status. I feel that `!` will be a bit confusing in the future, but there's no way to reorganize things to remove it, because using `sort -c` or `sort -c --reverse` you can only test `&lt;=` or `&gt;=`, but not the `&gt;` or `&lt;` you need.
Didn't know `sort` could do this, +1. 
It's also found in the epel repository (official repo? semi-offical repo? not sure, but better than random third party stuff IMO) yum install epel-realease -y yum install pv -y Also, not relevant to pv, but Centos SCL is another official repo that has lots of software not found in the regular centos repos, e.g. modern ersion of python, node etc yum install centos-release-scl -y
The problem is the parser doesn't see those `\[ \]`, it only sees the `$(__git_status)`, and by the time it gets around to running that, it's long done replacing`\[ \]` anymore. What I'd do is use PROMPT_COMMAND to set a variable containing the git info or empty string, then react on that in PS1: _git_color=$(tput setaf 0; tput setab 3) PROMPT_COMMAND='_git_info=$(__git_info)` PS1='...${_git_info:+\[$_git_color\]$_git_info}...' Now the parser sees them and replaces them. On a side note, don't export PS1. There's no point to it.
I fiddled around with it and it seems to be from the expansion inside the single quotes of '$(__git_status)' and it looks like you're trying to add the printf to the $info variable, but actually you're just printing after the assignment of $(__git_info), a better solution might be info=$(printf '\\[\e[30;43m\\]%s' "$(__git_info)") if you want to call a command at every prompt one solution that I subjectively find better is to use PROMPT_COMMAND which tells bash to run a command before every prompt string ($PS1) is shown, here is my solution: function update_ps1 () { local info info="$(printf '\\[\e[30;43m\\]%s' '$(__git_info)')" local __host='\[\e[30;102m\] \h \[\e[0m\]'; local __dir='\[\e[1;97;44m\] \w \[\e[22m\]'; local __arrow='\[\e[1m\]▸ \[\e[0m\]'; local __git_status="\\[\\e[30;43m\\]$info" export PS1="${__host}${__dir}${__git_status}${__arrow}" } PROMPT_COMMAND='update_ps1' 
You could perhaps leverage `sort -V`: # Total items in array max="${#PureVerAvaiable[@]}" for elem in $(seq 0 .. $((max - 1))); do installed="${PureVerInstalled[$elem]}" available="${PuerVerAvailable[$elem]}" latest=$(printf "%s\n%s\n" "$available" "$installed" | sort -V | tail -n1) if [[ "$latest" -eq "$installed" ]]; then echo "up to date" else echo "newer version available" fi done
[With only single escape](https://imgur.com/MEPFBGe.png)
 &gt;It works now sometimes, but sometimes it does not. Will, it's probably won't. As I said, it's *minimally* tested. :-)
Technically, the issue I'm trying to solve is this: https://gfycat.com/MenacingLikableCranefly It happens when in a git repo so I know it has something to do with escapes. The bash history gets funky with the displayed commands.
I think I figured it out. Previously I was using bash rematch which apparently just checked for a matching IP address format at the beginning and ignored what came after; then the batch rematch variable was set to just the matching part. Since IP4\_ROUTE\_0 is formatted as something like "64.18.150.92/32 192.168.0.1 600", my previous code was working even though I had no idea it was a longer string like that. 
I changed the checks to the following: if [ "$1" = "tun0" ] &amp;&amp; [ "$2" = "vpn-up" ] &amp;&amp; [[ "$IP4_ROUTE_0" =~ ^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3} ]] &amp;&amp; echo "$IP4_ROUTE_0" | grep -Pq '^(?!(10\.|172\.(1[6-9]|2\d|3[01])\.|192\.168\.).*)(?!255\.255\.255\.255)(25[0-5]|2[0-4]\d|[1]\d\d|[1-9]\d|[1-9])(\.(25[0-5]|2[0-4]\d|[1]\d\d|[1-9]\d|\d)){3}'; then Now it works, I still get the IP address from the bash regex rematch that refers to the IP address but it only succeeds now if the grep matches (which does the more complex checking of making sure it's a valid external address)
We all here to learn something new =) Yeah, need to mention that `git rev-parse` and `git symbolic-ref` produce the same result. But why writing escape codes is a bad advice? I first looked at official git completion [scripts](https://github.com/git/git/blob/master/contrib/completion/git-prompt.sh#L248), they just use escape codes. `tput` has much more functionality and I do not see a reason to use it just to output the escape codes.