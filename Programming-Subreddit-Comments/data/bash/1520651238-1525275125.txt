I also need to put it in a file named CNAME (with no extension) and put it in a folder titled $a$b. Thank you though, that's most of it.
And to do exactly what you want (I think): for i in {0..255}; do n=`printf "%02x" $i` mkdir $n echo $n.example.com &gt; $n/CNAME done
No, I would like it in hexadecimal, but basically I just needes the second to last line, the rest I have above. Thank you
The printf statement turns it into a hexadecimal
I experimented around and found out how to make bash produce the whole list without loops: printf "%s\n" {{0..9},{a..f}}{{0..9},{a..f}}.example.com
you only need one for loop. read up on printf or google how to print hexadecimal numbers with printf. 
I think I figured it ou, I just need to figure out git first.
functions will inherit the exit status of the last processed command, and you will need to guard for that; sample() { [[ ! -e /etc/fstab ]] &amp;&amp; return 1 echo "fstab exists so I'll keep processing this function" return 0 # all ok }
Correct, my explanation could have been better. ;)
Thanks for adding that info. But it's not necessary in the example shown. `echo` will return 0 (syntax errors aside). ;)
You completely missed the point
Just trying to prevent this turning into a Stack Overflow topic where small example scripts are critiqued 'ad nauseam'. ;)
What do you mean by "commit 256 repositorie"? Like you want to create a repo in your account with those names? This would be a start. #!/bin/bash -eu hex='0 1 2 3 4 5 6 7 8 9 a b c d e f' for a in $hex; do for b in $hex; do repo=$a$b mkdir -p $repo ( cd $repo &amp;&amp; git init ) done done
You can use `shift` to pop off expected args from the front of $@ in your function. When you're done, $@ will be what you passed in (that is, the rest of the args). Here is an example: #!/bin/bash -eu foo() { arg1=$1; shift arg2=$1; shift echo "arg1=$arg1" echo "arg2=$arg2" echo '$@'=$@ } example run: $ bash tmp.sh rest of the args arg1=arg1 arg2=arg2 $@=rest of the args
I use this one _all_ the time. It reads from stdin (feed it a file or pipe the output of some command into it) and uses netcat to upload it to termbin.com. It will spit out a link that you can use to view the paste. In my `~/.bash_profile`: paste() { nc termbin.com 9999 &lt; /dev/stdin } export -f paste example use, which I used to upload this comment from my Linux terminal to termbin, and then switch over to my other computer to copy it from the browser to reddit. (I'm using a chromebook in developer mode that doesn't support copy/paste from the command line... what can I say I'm a glutton for punishment.) I uploaded this comment like: $ cat reddit-comment.txt | paste http://termbin.com/sjy1 I also use this bit of code quite often for prompting in bash scripts: ask() { prompt="$@ (y/n): " while true; do read -p "$prompt" resp case $resp in [Yy]* ) return 0;; [Nn]* ) return 1;; * ) echo -e "\n$prompt";; esac done } Like: $ foo() { if ask 'Do you want to xyz?'; then echo 'answer=yes'; else echo 'answer=no'; fi; } $ foo Do you want to xyz? (y/n): n answer=no $ foo Do you want to xyz? (y/n): n answer=no
 #!/bin/bash required=( program1 program2 program3 ) for prog in ${required[@]}; do if ! hash $prog 2&gt;/dev/null; then echo "error: $prog not installed" exit 1 fi done
The only directory change you're doing is `cd ../`, so you're just going to be climbing backwards as you run, which isn't going to work. That's also a fragile way to find the directories in the current directory. This would be more precise: for d in $(find . -mindepth 1 -maxdepth 1 -type d); do cp $WORKDIR/submit.slurm $d done
You can use `env` to print out all the variables set in the current shell. Beyond that, I think you're trying to solve this in the wrong way. Instead of trying to figure out where you might have variable namespace collision, try to make **everything possible** local variables inside functions. Don't use global variables in functions, and declare every single variable as `local`. Also, defensive coding. Make all of your assumptions explicit, and make sure your code will fail clearly when one of those assumptions are broken. In essentially every bash script I use `-eu` on the shebang line, and `-eux` when I want verbose debugging. This will trigger an error when any command fails without being uncaught, and will also trigger an error when any undefined variable is used (instead of expanding it to an empty string). The return value of every command needs to be accounted for; write your code with the assumption that these calls will fail. Many programmers avoid this approach because it makes their scripts feel brittle. In reality, a "brittle" script will force you to correctly handle errors, and is always better than a "reliable" script which eats errors and fails silently with unknown side effects.
I like this one. It's clean and readable. Thank you.
Glad you like it - in general I really like the pattern of defining lists of things and iterating over them. It helps to keep a clear separation of config and logic. Cheers.
Very clever solution
I want to update them, but U also need to creatr them, thank you. Do you know if there is a command fo enable GitHub Pages on the master branch.
Worked absolutely perfectly. Thank you.
Oh yeah, the astrix. Duh. Thank you
Oh, I just realized, you can also remove the mkdir command and just do cp media-defaults $h
One way is to assign the output to a variable and then print the variable to STDOUT var=$(your command) echo "${var}" Does that help?
Like this: display "$(find /directory -type f | shuf -n 1)" 
it works. Is it the same as using backticks instead of $() ?
Yes, but $() has some benefits. See: https://stackoverflow.com/questions/9449778/what-is-the-benefit-of-using-instead-of-backticks-in-shell-scripts
This is called interpolation and is one of my favorite functions of bash!
A shorter version: for ((i=0;i&lt;256;i++)); do cp -r media-defaults "$(printf %02x "${i}")" done
&gt; PS.: My target data structure is hash array, can you serialize it too this way? You mean an associative array? Hmm. Try this: qstring='' for idx in "${!myarray[@]}"; do printf -v q "[%q]=%q " "$idx" "${myarray["$idx"]}" qstring+=$q done Then get back the same way, with `declare -A myarray &amp;&amp; eval "myarray=($qstring)"` Again, *don't* process untrusted data with `eval` (such as from files you don't control); there is an easy to exploit code injection vulnerability when using `eval`. 
I ended up modifying it a bit getLiteralInput() { # Use the "history" function to get the literal input to a function, even if that input was not quoted # One set of single quotes surrounding the entire input will be removed, if they exist # # NOTE: this will give the last input entered into the terminal by a human. Any inputs into this function are ignored. # It will NOT give inputs to sub-functions and should NOT be relied upon by auto-run functions (e.g., functions called by ~/.bashrc). echo "$(history 1)" | sed -E s/"^ *[0-9]{1,9} +[^ ]+ +(.*)$"/'\1'/ | sed -E s/"^ *('? *.*[^(?: *'? *$)].* '?) *$"/'\1'/ | sed -E s/"^'([^'$]*)'$"/'\1'/ } This change allows it to fetch the correct part of the "history 1" output regardless of which function it was called from, and will remove a single set of single quotes if the encompass the entire output. On the functions I want to utalize this except when they are called by ~/.bashrc I just add a flag ('--no-literal' or something similiar) that makes it use whatever was input into the function instead of fetching the last history item. Its worth noting this still doesnt work if the input contains a few restricted characters (such as '(' and/or ')' and/or '&amp;'). This is a bit annoying, but Im not sure how you would fix this, since even if you were telling bash to go into some sort of debugging mode where the error didnt cause things to stop, you would still need to figure out how to enable that before bash threw an error (assuming you dont want that sort of thing always enabled). 
You could pipe to xargs too. Not saying it's a good idea, or cleaner solution, but imho any time you can use xargs, damn it you should.
&gt; Beyond that, I think you're trying to solve this in the wrong way. Instead of trying to figure out where you might have variable namespace collision, try to make everything possible local variables inside functions. Don't use global variables in functions, and declare every single variable as local. Oh I definitely agree. Ive made a point to do this is every semi-recent function ive written. The problem is in a function that I started writing literally within the first week or two on learning bash seriously (i.e., outside of occasionally simple terminal commands). When i started writing that function I dont think I even knew about the 'local' command (a lot of my previous coding experience was in Matlab, where everything is local by default and must be declared as global). I probably *should* just re-write it, but 1) it works, and 2) with all the modifications ive made it has grown into literally 1110 lines of code (including comments and blank lines)., and it seems like a lot of work to fix something that isnt really broken (outside of a few fixable issues that pop up now and again). Really, I wanted to be able to easily list any global variables so that I could go back and make them local unless they actually needed to be global. The only issue with that is finding them. good to know about `-eu` and `-eux`, thanks. On a related note: I dont suppose you know if there is something to add a legit 'try / catch' block to bash, do you? Some quick googling seems to suggest that there isnt, with the closest alternative being `(cmd1 &amp;&amp; cmd2) || cmd3`, though this only works if both `cmd1` and `cmd2` fail in such a way that failure returens 0 and success returns 1, which in my experience isnt always the case (actually, the `-eu` flag might make this more reliable, though there is still an issue when a program decided to return 0 for success and 1 for failure).
Dont have a ssh server here to test the script on bottom, however, the following snipped works for yes/no if id -u root $1 &gt; /dev/null 2&gt;&amp;1; then echo "yes" else echo "No" fi ### something like this maybe works? #!/bin/bash AWESOMEDIR=/opt AWESOMETXT=serverlist.txt MYUSR=root TESTINGUSR=someuser READS () { cd $AWESOMEDIR cat $AWESOMETXT } TESTS () { READS | while read line do ssh $MYUSR@$line | if id -u $TESTINGUSR $1 &gt; /dev/null 2&gt;&amp;1; then echo "yes the user exists" else echo "No, the user does not exist" fi } TESTS
Any option for shoe-horning long options into `getopts` is going to be hacky. For a lot of people, 100 lines is the limit they give for a shell script before they switch to another language. I think that's stupid. One of my rules-of-thumb is that if you absolutely *need* long options, *then* you should use another language. Otherwise, just avoid them. That said, [this one](https://stackoverflow.com/a/7680682), from your SO thread is one that I have used with success.
use [for-loop](https://mywiki.wooledge.org/ParsingLs) for iterating over directory names.. (assuming you have something in common, for ex: files starting with `[` or files ending with `_dir`, etc).. otherwise use `find` command use [parameter expansion](https://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion) to modify the name while testing, use `echo mv ...` so that you can see how renaming is going to happen...
hmm `for dir in *; do ...` - nice idea ! thanks
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
to loop through entries in the current directory: for f in ./*; do if [ ! -d "$f" ]; then echo "File: $f" else echo "Dir: $f" fi done 
You got most of it right. You can just wrap all that in a for loop and it'd be done. for filename in $(ls /abs/path/directory1/*.jar); do filename=$(basename $filename) if [ "$(md5sum directory1/$filename ....] # notice the $filename # basename gives just the file name without directories. # rest of the logic goes here done
will this loop stop once it runs out of files? I also need to compare the base directory to two different directories, can this be done in one loop?
I mean, instead of the following (which is a mistake if there are spaces in file names): for filename in $(ls directory1/*.jar); do ... You should write this: for filename in directory/*.jar; do ...
Neat. Didn't know the glob can be used without `ls` in the script. And yes, having a space in a filename throws off the script. 
I understand that this literally puts it into one line but I was thinking more along the lines of using ["$(md5sum 1/$filename |awk '{print $1}')" = "$(md5sum 2/$filename |awk '{print $1}')" = "$(md5sum 3/$filename |awk '{print $1}')"]; something like this where it can check the first directory with the second and third within one linebreak
Yes, it will stop when it runs out files in `directory1`. It can be done in one loop. It might start looking ugly if you want to do it one-liners like you are doing currently. I would use some more additional variables for readability. 
Yeah, you would now try to fit all of that somehow into that 'for' loop. It will be a really long command line. You need to be careful to always add spaces around the `[` symbols. They won't work otherwise. You can't do: [ a = b = c ] You have to do: [[ a = b &amp;&amp; a = c ]] Perhaps the following works: for file in 1/*.jar; do md5=$(md5sum &lt; "$file"); base=$(basename "$file"); for dir in 2 3; do file2="$dir/$base"; [[ $md5 != $(md5sum &lt; "$file2") ]] &amp;&amp; echo "changed: $file2"; done; done Here's the same with line-breaks for easier reading: for file in 1/*.jar; do md5=$(md5sum &lt; "$file") base=$(basename "$file") for dir in 2 3; do file2="$dir/$base" [[ $md5 != $(md5sum &lt; "$file2") ]] &amp;&amp; echo "changed: $file2" done done Doing `md5sum &lt; file` instead of `md5sum file` is a trick to make the filename disappear from md5sum's output.
Just run that command multiple times. Also, you might look at quoting your filename when using $string like `"/path2/results.$string"` in case your search contains whitespace.
Thanks, it's acceptable, but then, I think some group of users should raise an issue, because [""]="" syntax doesn't allow hash&lt;-&gt;array interaction and requires to use loop in above code. The needed straightforward syntax is keys and values alternating, like in Zsh.
this is very helpful, thank you. I just have one more issue where I need all the jars in directory 1 to compare with 2 and 3, but 2 and 3 may have extra or missing jars. In the case where they are missing, the script fails. I just need to compare the ones that are found in 1 to 2 and 3. 
You can test for a file existing like this: [[ -f filename ]] I think what might work is to add it to the stuff I shared earlier like this: for file in 1/*.jar; do md5=$(md5sum &lt; "$file") base=$(basename "$file") for dir in 2 3; do file2="$dir/$base" [[ -f "$file2" &amp;&amp; $md5 != $(md5sum &lt; "$file2") ]] &amp;&amp; echo "changed: $file2" done done And on a single line for testing from the command line: for file in 1/*.jar; do md5=$(md5sum &lt; "$file"); base=$(basename "$file"); for dir in 2 3; do file2="$dir/$base"; [[ -f "$file2" &amp;&amp; $md5 != $(md5sum &lt; "$file2") ]] &amp;&amp; echo "changed: $file2"; done; done 
I guess I wasn't sure if the concurrent nohups would be an issue for the nohup proc/output file, etc. Yeah, on the path - I was just banging out the example.
Is there any difference if you manually run the ssh command but give the full path to `Powershell.exe`? A quick `which Powershell` in Babun for me shows this: `/cygdrive/c/WINDOWS/System32/WindowsPowerShell/v1.0/Powershell`
Is the script not being run or are you just not seeing the STDOUT output? If you were to add a "touch /tmp/foo.txt" command in your script and then run the SSH command (without the above file already existing), does it get created?
Threading doesn't effect the pipe's behavior. The pipe only connects STDOUT from tool_1 to STDIN on tool_2. Multiple threads can write to that stream of data, but it's still a single stream. Nothing is multithreaded between the two tools. It's like 24 people talking into 1 telephone.
I guess the big question for me here is: is there any difference between what is sent by SSH when doing it on one line vs logging in and doing it manually? Does it send some kind of flag that indicates it is not being done manually which could be causing some sort of problem with Process Monitor?
Thanks. That clarifies things. Dan
Use the command renane, you dont need necessary a boucle for. Just look that : Mkdir test{a..z} Rename ‘s/test/pomme/‘ *{a..z} Now you have 26 file with pommea, pommeb, pommec,... Just do that for your files.
curl...?
What does thay mean? What are you confused about.
I am trying to make 256 image repositories on GitHub, with the pattern xx.domain.com This specific question is to check precicely where brayden fucks up. And thank you.
Any reason not just using output of `diff --brief dir1 dir2`?
Why 256? Beyond it being 2^8?
They run concurrently, not in parallel ;) 
I'm not sure if there is a thing like ~/.bashrc within CygWin that would allow you to run that command in every new session... Another very hacky solution would be to use __xdotool__ here like so: ssh foo@192.168.21.2; xdotool type "Powershell ./test.ps1" &amp;&amp; xdotool key KP_Enter which should type the command and hit enter for you.
2^8 is exactly what I need. I am using all possible 2 dogit hexadecimal numbers as names.
Right but why
Because it's easier for hexadecimal.
[&gt; Note the -t flag](https://malcontentcomics.com/systemsboy/2006/07/send-remote-commands-via-ssh.html)
I don't believe Bash has a mechanism for this. Installing packages is OS dependent. I would **love** for someone to prove me wrong.
Place the requirement in te `README.md`, and make sure your users read it before running the script. And make your script check for the `pv` command before running (remember searching in the `$PATH`), and show an error and quit if it doesn't detect it, or another behavior of your choice.
generally speaking, gas is more expensive in los angeles county than orange due to county taxes.
If that's the case, I'd need to check if `pv` is installed and, if it is not installed, check which OS is running, then add the package accordingly. Seems like a lot of overhead, but doable. Still, I'm lazy...
if ! command -v pv; then...
putting this near the beginning should do what you want I think: if ! hash pv 2&gt;/dev/null; then echo "pv command not found!" exit 1 fi
 if ! which pv; then which yum &amp;&amp; yum install pv which apt &amp;&amp; apt install pv fi That's the easiest one I can think of. It covers all 3 test distros, but of course there are many different package managers, so this isn't a truly *general* case. Or just 'exit 1' and say "pv required".
The traditional way to let a user know about what they need to install the software is to add a file named `INSTALL.txt` that describes everything.
You can create all the repos for OSes you want to support, which is a pita. Or you could package it all into a container that would make it dependent on a container runtime - becoming more common, because you ship with your own dependencies. Or you can exit with a message for the user to install themselves. That's all I can think of. I'd do a combo of b and c, because different users might want to manual install, or like that it works out of the box. 
That's usually what I do, exit 1 with a nice message stating required package not installed
They run concurrently, not *necessarily* in parallel – but since even smartphones have multi-core CPUs these days, I’d be very careful about asserting that anything *doesn’t* run in parallel ;)
For 'pv' specifically, you could add an alternative output to your script so that it still keeps working when pv is missing on a machine because something went wrong with those required preparations about installing packages. You could wrap the use of 'pv' inside a function and inside that function do two paths: one using 'pv' and the other just running 'cat'. Something like this: progress() { if type pv &amp;&gt; /dev/null; then pv "$@" else printf "Transferring... " 1&gt;&amp;2 cat printf "\n" 1&gt;&amp;2 fi } If I didn't make a mistake, this 'progress' function should be able to replace every current use of 'pv' inside your script without any other changes needed.
imo, you should let the user install the package(s)
I think you are the one confused here. He was trying to help you, gave you a hint to use curl. What you asked for can be achieved by using curl. `man curl` Should be a good start. 
It's OS dependant. I wrote an abstraction layer in bash to cover about 7 different distros for a framework that requires various packages managed by the distributions package manager. It handles package installs, removal, disk partitioning, and OS configuration. I thought it was going to be a nightmare but turned out to be really fun 
Your pathes look confusing, is your username "Desktop"? i'm assuming it is, then its just touch folderA_B/fileB.txt assuming folderA_B is present. If it is not: create that directory beforehand mkdir folderA_B touch folderA_B/fileB.txt 
It works for me, see here: $ awk ' &gt; BEGIN { # a comment here &gt; print "hello" # another comment &gt; # some comment &gt; }' hello 
I include it within the program itself, so the user is informed of what is missing and thus what is needed; a dependency check. I don't think much of a separate file the user has to look in to see what's needed, honestly. You can find an example of how I check for dependencies here: https://github.com/terminalforlife/installit/blob/master/insit
My method would be to just use bash itself: A=({A..Z}{A..Z}) declare -i C=0 for D in *; { if [ -d "$D" ]; then printf "%-30s %s\n" "$D" "${A[C]} $D" C+=1 fi } Except `mv` of course. :P
I will always parse these myself, and it's not an issue at all to do. I don't like getopts and I prefer the flexibility of doing it manually.
I think it'd make sense to write that as a feature which can be called (sourced), like what you might find in a function library.
Do you have `interactive_comments` enabled? This will tell you if you do: shopt -qp interactive_comments &amp;&amp; echo yes
imo writing more code to use getopts is sorta self defeating, especially when the while loop isn't to bad with multi match or whatever you call it, heres a slightly better example #!/usr/env bash FILENAME='' print_help() { echo 'this is help'; } while [[ -n $1 ]]; do # will end when no more args case "$1" in -f|--file-name) # cas can do several shift # move $2 to $1 FILENAME="$1" ;; -h|--help|-H) print_help exit 0 ;; *) # a wiled card echo 'bad arg' print_help exit 1 ;; esac shift # to make sure $1 is consumed done echo "$FILENAME" this is just my opinion again but i try to keep my bash scripts simple, there are too many things in bash that make even flow control hard that adding in more complexity's inst really worth it to me. as far as bash libraries personally i dont use them, as i want my scripts to be run on any os i install with out worrying about dependency's.
Nevermind, I was missing a closing parenthesis in a previous function.
Stick it in the README(.md|.txt)? Having written instructions on what's necessary to set up for your script is super common, and you can link to an external source describing how your dependency can be installed. Trying to do it automatically is in the right spirit, but there are so many different package managers, let alone operating systems, that you'll often miss just as many as you get right.
Use a return value. Here's a simple example: Script: #!/bin/bash foo() { printf b if [[ $1 &lt; 5 ]]; then return 0 else return 1 fi } for i in {0..9}; do printf "\n$i -&gt;" printf a foo $i || continue printf c done echo Output: 0 -&gt;abc 1 -&gt;abc 2 -&gt;abc 3 -&gt;abc 4 -&gt;abc 5 -&gt;ab 6 -&gt;ab 7 -&gt;ab 8 -&gt;ab 9 -&gt;ab
rsync --dry-run source target ?
Fair point. That's what I get for trusting someone else's code with only a cursory glance. 
Do you have a source for that? It sure sounds correct. Thanks. 
Yes, it looks like listing the requirement and having a check in the script itself would be the prudent way to address this situation. 
Fair point. 
I hadn't considered that. I'll test that approach, though I'm inclined to have the program exit if `pv` is not installed, and leave the rest to the user. 
With respect to **B**, are you referring to `Docker`? I'm inclined to having the script heck for the package and exit if it is not installed, leaving it to the user to add the package manually. Thanks. 
Yes, I'm leaning towards the script checking for the package and quitting if it doesn't find it. Thanks. 
I think that CentOS by default requires you to add [Extra Packages for Enterprise Linux repository configuration](https://pkgs.org/download/epel) before you can add `pv`. I like the idea of letting the script check for `pv` and exiting the program if it is not installed. 
Having a file named `INSTALL` is part of the GNU Coding Standards. Even if most programs aren't GNU, the standards do a good job of describing "this is what \*NIX users traditionally expect when they download sources to a program." https://www.gnu.org/prep/standards/standards.html#Releases
How exactly are you using 'pv'? That function might need some tweaks if you use it on a filename.
Thanks you, /u/LukeShu. 
Really, you're recommending packaging a shell script in a container?
Thanks, but still nothing for D in $(&lt;list3.txt); do ... "test -s /etc/crypttab &amp;&amp; hostname" ;done FWIW, if know it is connecting to the servers and "uname -a" returns output, but putput on true isn't.
&gt;Not OP, but how do you prevent allowing identical arguments from a script using getopts? Excellent question. To my knowledge (so I could well be wrong), there's no way to force `getopts` to behave in a strictly POSIX way. If you want to be absolutely explicit with how your options behave, then you need to handle your options manually (see the link below for an example). &gt; &gt; The only reason you would ever use getopts is to allow single-letter option combining (-xvf handled as -x -v -f). It has no other purpose. The trade-off for this is that you cannot use long arguments of any kind (GNU-style "--foo" or Tcl-style "-foo"). &gt; &gt; &gt; &gt; Never use getopt(1). Traditional versions of getopt cannot handle empty argument strings, or arguments with embedded whitespace. There is a version of getopt(1) in util-linux, but you should not use it. Why not? Because you would need to write special safety-checking code to ensure that you've actually got this nonstandard getopt, and then you would still need to write a fallback option processor for when you don't have it. So you're doing twice as much work and getting no significant benefits for it. &gt; &gt; &gt; &gt; The POSIX shell, and others, offer getopts which is safe to use From http://mywiki.wooledge.org/BashFAQ/035 &gt;P.S. If you were to use another language like Python which you've mentioned, would the code for a typical script be more time-consuming and/or difficult to write than a bash-equivalent? This depends on the language you use, your familiarity with that language and what you're wanting your code to do. Sometimes a shell script will have less lines of code than, say, a `python` script. Sometimes a `python` script will have less lines of code. There are examples out there that show one line shell scripts that take half a dozen lines in `python` for example. Counter to that, there are `python` scripts that might be a dozen lines long, and to replicate that you'd need maybe 500 lines of shell code. At scale and/of complexity, though, you would expect a more complete language like `python` to be simpler to write. It all depends on the complexity of what you're wanting to do. &gt;Of course Bash has its advantage of being ubiquitous, but I'm curious how a programming language like Python or Ruby can be advantageous in ways that Bash can't (for example, is it as simple as having nicer syntax and more complete features with a trade-off being they might not be available in a typical server environment, or are there more significant trade-offs?). `python` is fairly ubiquitous and thanks to things like Ansible is *the* language to learn right now. Some people find `python`'s syntax nicer, some people don't - that's a subjective thing. I don't think there are any significant trade-offs. Fundamentally though, once you're familiar with one or two languages, then reading other languages is usually straightforward. &gt;I'm currently learning programming and am wondering if it's worth using a language like Python for my scripts instead, assuming I'm not intending to use these scripts on production servers for work or something (although I might share scripts with other desktop users). Main reason I'm considering is that presumably these actual languages don't suffer from Bash (and Bash tool) limitations. Yeah, other languages don't tend to suffer from all the limitations and weirdness of Bourne shell and its dialects. But they may suffer from their own limitations such as version compatibility issues (e.g. `python2` vs `python3`). There's nothing to stop you from scripting in both languages. I would encourage you to learn systems administration concepts though... there's way too many developers entering the devops/SRE field, trying to automate sysadmin and just absolutely fucking things up. Hope that helps? 
Works for me though, so something's not right for you: [redacted]$ ssh redacted "test -s .bashrc &amp;&amp; hostname" redacted.re.dact.ed [redacted]$ ssh redacted "test -s .pants &amp;&amp; hostname" [redacted]$ FWIW, the `if` loop works for me too: [redacted]$ ssh redacted "if [[ -s .bashrc ]]; then hostname; fi" redacted.re.dact.ed Do you have `ssh` keys deployed? I'm wondering if you can test without `sshpass` to see if that's the problem (i.e. process of elimination)
Hmmm, You are on to something, works without sshpass. I'll have to look into that. Thank you.
Containers are much more than process isolation and dependency shipping is a great use. Why should I care which version of a dependency is on your machine? Which shell you're running even? Let me just send you what you need. Obviously not appropriate everywhere, but in this case it would probably be a 5 line docker file.
You can still use `type`, but `type -P` to check the PATH, which ignores aliases and functions.
That works well, it just depends how you use it. My preference will always be either `type -P` or a simple `[ -x /path/here ]`.
I know is not really what you asked but you could try with ansible? run some ad-hoc command that gives you what you want
In the echo example, you need to escape the double quotes that are part of the string because they're terminating (closing) your opening quote, which confuses echo. You'll also have to escape special characters like '$'. The dollar sign character is used to indicate the start of a variable name or an expansion, so to print it literally you need to tell echo to ignore its special meaning. echo "export PROMPT_COMMAND='RETRN_VAL=\$?;logger -p local6.debug \"\$(whoami) [\$\$]: \$(history 1 | sed \"s/[ ][0-9]+[ ]//\" ) [\$RETRN_VAL]\"'" A special note about this part: `[\$\$]:` The second dollar sign doesn't actually need to be escaped because the characters that immediately follow it (`]:`) are invalid for variable names or expansion expressions, so all three characters get treated literally. However, I chose to escape it anyway because I believe that makes it more clear to a human reader that the second dollar sign is meant to be interpreted literally. 
Try this ''' echo 'export PROMPT_COMMAND='"'"'RETRN_VAL=$?;logger -p local6.debug "$(whoami) [$$]: $(history 1 | sed "s/[ ][0-9]+[ ]//" ) [$RETRN_VAL]"'"'" &gt;&gt; /tmp/test.txt ''' Wrote it on my phone so I do not guarantee it works, but it seems to do what you wanted it to do.
Something like awk -F '-' '($2&gt;$1) {print $0}' $ cat numbers 0-1 1-1 2-1 3-5 2-2 4-6 6-4 $ awk -F '-' '($2&gt;$1) {print $0}' numbers 0-1 3-5 4-6 
It looks like your way is 5 milliseconds faster than the way I came up with on my own.
What does the output of aws ec2 describe-instances --filter "Name=tag-key,Values=arecord" Name=tag-value,Values=*broker*" --query "Reservations[].Instances[]. [PrivateIpAddress]" look like?
Look into jq. It's how I do it. You could also use a regex with grep -oP
I see. I'd do something like this then: aws ec2 describe-instances --filter "Name=tag-key,Values=arecord" Name=tag-value,Values=*broker*" --query "Reservations[].Instances[]. [PrivateIpAddress]" | grep -Po '^\s*"?\K[0-9\.]+'
Is that valid json? if so, you can pipe it through `jq -r 'flatten[]'`
Writing an argument parser that handles all the edge cases is annoyingly complicated; why would you bother to rewrite that over and over again? That's precisely why we have libraries.
I don't find it complicated, and I don't write it over and over anymore. See: https://github.com/terminalforlife/vimconfig/blob/master/plugin/tflsnips.vim
`pv` is used to provide real time feedback on a function that uses `find`: function disk_space(){ printf "%s\n" "Processing ..." local largestfiles=$(find / -type f -exec du --separate-dirs --human-readable {} + 2&gt;/dev/null |pv ) # find largest files by disk space; pv to provide feedback # find / -type f -exec du -Sh {} + 2&gt;/dev/null write_header "Disk Usage" df --human-readable --total | awk 'NR==1; END{print}' # printf "%s" "Retrieving largest files..." printf "%s\n" "------------------------------" printf "%s\n" " Top 10 Disk Eating Files " printf "%s\n" "------------------------------" printf "%s\n" "${largestfiles}" |sort --reverse --human | head --lines=10 # sort -rh | head -n 10 pause }
So you are optimizing to the milliseconds a code that runs roughly once every hour and parses a small file? Why?
It actually runs 6 times an hour and this is just a small portion of the entire script. The number of lines it's running against also increases daily. The machine it will be running on is less powerful than the machine I was testing on, so it might make a more substantial difference in production. Finally, I'd much rather try to be as efficient as possible, so if it's a question of just changing that small portion of the code while I have it open, why not?
I'd use: while IFS="-" read -a X; do if [ ${X[1] -gt ${X[0]} ]; then : Do your thing here. fi done Unless you need the best performance, but for just 850 lines, this should be quick enough, and doesn't spawn any new processes.
&gt; jq -r 'flatten[]' no its not a JSON. its just a array output
Here's the snippet I'm currently using... grep "-" data.txt | grep -v "(..)" | awk 'BEGIN{FS=OFS=" "}{print $5}' | awk -F '-' '($2&gt;$1) {print $0}' | wc -l The unprocessed data.txt file has a lot more in it that can be discarded immediately. Only lines with a "-" are required for this. Then, of those lines, any that have a "(..)" string can be discarded (it is always two characters between the parentheses). That's what the two greps do. From that point, the "#-#" in the OP can be pulled from column 5 with a space delimiter.
I had to guess a bit about how data.txt might look like. I hope the following works. This moves the two greps into the first awk, and the "wc -l" into the second awk: awk '/-/ &amp;&amp; !/\(..\)/ { print $5 }' | awk -F '-' '$2 &gt; $1 { count++ } END{ print count }' To make it just one 'awk' call, I came up with this: awk '/-/ &amp;&amp; !/\(..\)/ { split($5, a, "-"); if (a[1] &lt; a[2]) { count++ } } END{ print count }' 
Check 'case' statement if you do not want to use if/elif/else
`man passwd`: [...] -S, --status Display account status information. The status information consists of 7 fields. The first field is the user's login name. The second field indicates if the user account has a locked password (L), has no password (NP), or has a usable password (P). The third field gives the date of the last password change. The next four fields are the minimum age, maximum age, warning period, and inactivity period for the password. These ages are expressed in days. [...]
Both of these seem to work fine (and both about twice as fast on the pi than the double grep, double awk version). Can you explain this for me so I understand it moving forward? I'm guessing that the "/-/" means that the line has to have whatever is between the slashes? Which would mean that the "!/\(..\)/" would mean look for "(..)" (having escaped the parentheses) with a negation (!). It then splits the contents of column 5 into a using "-" as the delim, and then counts the amount of times the second number is larger than the first and prints the final amount. Am I getting that correct?
Here it is with line-breaks to maybe read it easier (it actually still works if you use it like this): awk ' /-/ &amp;&amp; !/\(..\)/ { split($5, a, "-"); if (a[1] &lt; a[2]) { count++; } } END{ print count; } ' I have "nawk" installed just because of its man page. Perhaps try installing it as well and take a look it with `man nawk`. It seems to me whoever wrote that document tried very hard to squeeze everything about awk into as few sentences as possible. Everything in awk looks like this: pattern { action } Awk hunts down lines that fit that 'pattern' part, then runs the '{ action }' part for those lines. That `/.../` stuff is basically like grep. Inside the `//`, that's a regex pattern. It's `\(..\)` instead of `(..)` because it's the regex patterns like with `egrep` or `grep -E`, where `(` has a special meaning and has to be escaped if you mean the actual text character. The `!` is logical negation, the `&amp;&amp;` is logical "and". The `split()` function, I've found that in the `man nawk` text I mentioned earlier. The second argument is a name for a variable. That variable will get used as an array. After the split, `a[1]` is the first thing that was found, `a[2]` the second one. About the `count++` line, when that's run the first time, the 'count' variable will have nothing in it, but it seems that "nothing" gets treated the same as a zero. After the first `count++`, you will have a `1` (one) inside 'count'.
Make liberal use of the `man` pages, and just use an Internet search engine plus YouTube; ample resources out there. By the way, I'd look into the `select` loop.
Why use a website when you have `man` at your fingertips? Anywho, there are plenty of sites which share manual pages; you need only search for them.
Thanks. I actually haven't touched on 'man', but this is exactly what I needed. 
I do like reading longer man pages in https://linux.die.net/man/ I find navigating and searching in browser a little easier than in command line.
Here's something I wish I would have known in the beginning: You can search in the normal tool that's used to display the man-page (it's the tool named "less" which you might know from using it as a file viewer for text files). You can jump to the beginning or end of the file with `g` and `G` (Shift+g). When you type `/` on the keyboard, you get a search prompt at the bottom of the screen. After you type something there and hit Enter, you'll jump to the first search result in the file. You can then press `n` to go forward in the file to the next result, and `N` (Shift+n) to go backwards. The thing actually takes "regex" search patterns like the 'grep' tool. You can search for things like an `-r` parameter like this: ^ *-r This is supposed to mean, the `-r` you search for has to be at the start of the line with a bunch of spaces in front of it. Another thing that's interesting to do is searching for this: -r\b The `\b` means "word boundary". It makes it so your search will not trigger for things like `--rsh` or `--dry-run` in for example rsync's man-page. You can type `-i` to switch between case-sensitive and case-insensitive searching. You will have to read the message at the bottom of the screen to know what mode you switched to after using that `-i`.
`man` man
I think your question has been answered, but a related issue is how to learn to use the linux command line. I recommend this book to people who want to learn it: http://linuxcommand.org/tlcl.php (there is a pdf download link on that page). The book covers the basic concepts and may teach you about some stuff you wouldn't know to search for. For your question here, it is discussed in Chapter 5. 
just remember that ~man~ is not the same as ~help~.
I use man pages. I sometimes Google for answer, ending up on an online copy of the man page. `man` is your man!
You definitely need to know `man`, but also try [explainshell.com](http://explainshell.com). You paste a command and it tells you about shell syntax and flags used.
If they're defined in /etc/passwd some of the things you're looking for don't really exist, like there's no "Account Disabled" property. The best info you're going to do is this in this comment https://www.reddit.com/r/bash/comments/84xuy8/get_all_users_account_status/dvt93oz/ plus a little bit of scripting to get full name and uid in there but that's easy enough.
Is this Linux only? If so, you can often get a start by looking at `/etc/login.defs` to find out what UID range you need to search within when working with `/etc/passwd`, this way you're not having to write some big `grep -Ev` filter to exclude system/package accounts. Usually you're looking for something indicating the low boundary of the UID range e.g. `UID_MIN`. Traditionally that was 500, these days it tends to be 1000, but you don't know for sure. So you add a default for hosts where this isn't defined or it's some weird version of Linux that defines it somewhere else. Your code might look like this: uidMin=$(awk '/^UID_MIN/{print $2}' /etc/login.defs) uidMin="${uidMin:-500}" Then you can cull a list of users from `/etc/passwd` using a simple `awk` command, and read the output into an array e.g. mapfile -t userArray &lt; &lt;(awk -F ':' -v min="${uidMin}" '$3&gt;=min {print $1}' /etc/passwd) From here you theoretically have a list of local - actual - users. How you proceed depends on which tools are available to you e.g. for user in "${userArray[@]}"; do chage -l "${user}" done Although you might like to look at each user's entry in `/etc/shadow` which has a more portable, [predictable](https://en.wikipedia.org/wiki/Passwd#Shadow_file), parsable format. Another thing you might like to do is check the `last` time a user logged in and if they haven't (or haven't in a while), notify. Chances are the user needs to be archived and removed. Obviously I've been there, done that (amongst other things, I've written a password age monitoring script for NRPE which has since been migrated to check_mk)... can't share code though sorry - my employer's IP. 
Be aware that sometimes the man pages can be a bit difficult to understand, but they should be the starting point for anyone wanting to learn more about commands.
As in, Python style lists? If you just want to print the output of a shell array, it's incredibly easy to do with bash alone: printf "%s\n" "${X[@]//[!0-9.]}" Where X is the variable array, of course. If it's not actually an array, but the output is "array-like", then just remove the `[@]` part, add a trailing space after the `\n` and away you go. No need to load up another process. It's making use of parameter expansion (see `man bash`) to remove from left-to-right (`//`) everything (globally) which doesn't (`!`) match `0-9.` characters. EDIT: Sorry, got confused with `##` and `//` which I tend to alternate between. The `//` is substitution syntax, whereas `##` is from left-to-right removal.
Just I have been using CentOS/Redhat. Normally, we can use "chage" to get the user account expiry details in linux. $ chage --list Username Last password change : Apr 01, 2009 Password expires : never Password inactive : never Account expires : never Minimum number of days between password change : 0 Maximum number of days between password change : 99999 Number of days of warning before password expires : 7 But I want to extract the user account expiry details and password expiry details like below. Desired Output : Username |Full name |UID |LastChange|M| Max |W|Passwd Exp|Dis |Acct Exp John John W. 01.04.2018 0 99999 7 never never Tom Tom S. 05.06.2018 0 99999 7 never never
ss64.com
 Thanks again. Later I will fix output format. My last question is : I want to withdraw the fourth field of the output of lastlog. I want to add last of output. Username Port From Latest auser pts/31 c-73-123-11-86.h Sun Jan 19 13:52:08 -0800 2018 Username |Full name |UID |LastChange|M| Max |W|Passwd Exp|Dis |Acct Exp | Port | From |Latest John John W. 01.04.2018 0 99999 7 never never pts/31 c-73-123-11-86.h Sun Jan 19 13:52:08 -0800 2018 Tom Tom S. 05.06.2018 0 99999 7 never never pts/31 c-53-123-12-76.h Sun Jan 15 12:52:08 -0800 2018 
That's fairly easy... `last -n 1 "${userName}"` and pipe it into either `awk` or `cut`... I'll leave that as an exercise for you
I have done something like below. Am I correct ? please correct me if I am wrong. Specially , is that true awk usage within lastlog function? and what do I need define within 'while IFS' ? lastlogPort() { last -n 1 "${userName}" | awk '{print $2}' } lastlogFrom() { last -n 1 "${userName}" | awk '{print $3}' } lastlogLatest() { last -n 1 "${userName}" | awk -F '[[:space:]][[:space:]]+' 'NR&gt;1{print $NF}' } # Print out our headers printf -- '%s\n' "Username |Full name |UID |LastChange|M| Max |W|Passwd Exp|Dis |Acct Exp | Port | From |Latest" for user in "${userArray[@]}"; do while IFS=':' read -r userName pwdValid userUid _ userGecos _ _; do while IFS=':' read -r _ pwdState lastChange minAge maxAge warnTime acctInact acctExpire; do printf -- '%s\n' "${userName} $(cleanGecos) ${userUid} $(pwdLastChange) ${minAge:-0} ${maxAge} ${warnTime} $(pwdExpires) $(pwdLockState) $(acctExpires) $(lastlogPort) $(lastlogFrom) $(lastlogLatest)" done &lt; &lt;(grep ^"${user}" /etc/shadow) done &lt; &lt;(grep ^"${user}" /etc/passwd) done 
At a glance that looks about right. You don't need to define anything within `while IFS`, what's happening there is we're telling the `while read` loop to use `:` as a delimiter, so that it reads each field into a variable Let's take the first line for example while IFS=':' read -r userName pwdValid userUid _ userGecos _ _; do And a randomly chosen line from `/etc/passwd`: sshd:x:122:65534::/var/run/sshd:/usr/sbin/nologin The outcome from that line read should be: userName=sshd pwdValid=x userUid=122 userGecos= For fields that we don't use, we simply provide a dummy variable. You could give it a name like `novar` or `dummyvar` or whatever, I just use an underscore. This approach saves you from the naive beginner approach of: userName=$(grep "${userName}" /etc/passwd | cut -d ":" -f1) pwdValid=$(grep "${userName}" /etc/passwd | cut -d ":" -f2) ... Which has the downside of unnecessary lines of code, making multiple passes of the same file and using a pipeline of two externals... it's really inefficient. The upside is that it's easily understood.
Thanks for the detailed explanation! This all seems fairly logical. I'll update my script!
Don't use UPPERCASE variables unless you know when/why you need to. Don't use `.sh` extensions. They're meaningless on a *nix system. What if you rewrite this in `python`? Any documentation or other scripts that reference `deploy.sh` now have to be updated. Congrats: you just made unnecessary work for yourself. You can use extensions for libraries. It looks like your array assignment might be wrong (i.e. commas) I wouldn't bother making this interactive either. Instead have a single parameter - it's essentially the same mental roadblock. ./deploy staging ./deploy prod If you absolutely have to have interactivity, at least provide some args to override them.
I always use uppercase variable names; this being some sort of no-no seems silly to me. It doesn't really matter, unless you're strictly adhering to convention. YMMV, basically.
Thanks for you reply. I really like the http://shellcheck.net. And removing the `.sh` is also a great tip. 
The idea behind no uppercase variables, if I recall correctly, is so that you don't accidentally overwrite a system variable since they're (theoretically) in all uppercase. That explanation was enough for me to stop using all uppercase and go CamelCase (I believe that's the name for the case type like that).
I wonder how many of my shit smearing friends are Picasso without the right tools
I have edited my script. Could you please below function control again? Here is my updated script : lastlogPort() { last -n 1 "${userName}" | awk '{print $2}' } lastlogFrom() { last -n 1 "${userName}" | awk '{print $3}' } lastlogLatest() { last -n 1 "${userName}" | awk ' { for( i=1;i&lt;=NF;i++ ) { if ( $i ~ /Mon|Tue|Wed|Thu|Fri|Sat|Sun/ ) { j = 0 str = "" for ( j=i; j&lt;=NF;j++ ) { str = ( str ? (str FS $j):$j ) } print str break } } }' } # Print out our headers printf -- '%s\n' "Username |Full name |UID |LastChange|M| Max |W|Passwd Exp|Dis |Acct Exp | Port | From |Latest" for user in "${userArray[@]}"; do while IFS=':' read -r userName pwdValid userUid _ userGecos _ _; do while IFS=':' read -r _ pwdState lastChange minAge maxAge warnTime acctInact acctExpire; do printf -- '%s\n' "${userName} $(cleanGecos) ${userUid} $(pwdLastChange) ${minAge:-0} ${maxAge} ${warnTime} $(pwdExpires) $(pwdLockState) $(acctExpires) $(lastlogPort) $(lastlogFrom) $(lastlogLatest)" done &lt; &lt;(grep ^"${user}" /etc/shadow) done &lt; &lt;(grep ^"${user}" /etc/passwd) done
Cool where is the script?
Sorry just added in the description: https://github.com/MagePsycho/nginx-virtual-host-bash-script
I use this to --dry-run / sync my primary drive with backup drives. Completely designed for my system. You'd have to re-work for yours. https://github.com/oodsway/shell_scripts/blob/master/datasync
Thanks it really helps to see how others solve their problem! 
So only Magento 2 is supported? 
Yeah for now. Similarly, you can add for any Application.
Generally, if I get to the point where I'm building arrays or hash tables, that's my cue to abandon bash and switch over to something more robust like perl or python. Sure, bash can do those things but it's like trying to hammer a nail with your shoe when you have a perfectly good hammer right next to you.
Yes! This guy nails it on the head
my 25 pipe curl to oracle lines still count as one line right?
Don't forget Python can execute bash and vice versa.
If I recall, that was actually why Python was developed in the first place.
won't, not can't
Thanks!! Good job.
My pleasure. Hopefully readers will pick up a thing or two.
It is honestly not that complicated to detected a `\n`.
also [bro](http://bropages.org/), bro
I recommend looking into building a menu using [case](https://askubuntu.com/questions/1705/how-can-i-create-a-select-menu-in-a-shell-script) or [select](https://stackoverflow.com/questions/3200252/prompt-user-to-select-a-directory-with-a-bash-script-and-read-result). That should point you in the right direction. 
Try this: "${randomNumber}p"
This did it. Thanks!
Fantastic, thanks! I literally came here wondering if anyone has posted tips to configure redline and go figure your post is up top.
Heh. I was just implementing this yesterday. I ended up going with one that takes paired parameters [as described in this Stack response.](https://stackoverflow.com/questions/192249/how-do-i-parse-command-line-arguments-in-bash)
The code parts on the website are very hard to read. It is displayed as a dark gray text color on a black background. The monitor I am using here is calibrated with a colorimeter, so gamma and such is set up well on my monitor. If I look at it with the 'inspect' tool in the browser, I see a CSS rule that defines red color for the text, but it is for some reason not getting applied to the code parts of the page. This happens in both Firefox and Chrome.
If I'm understand this correctly based on an admittedly cursory glance, I would suggest using echo with the `-n` flag (no newline character), or, IMO preferably, the `printf` command, which is also built into the shell.
Interesting, you are the second person to point this out. I am unable to recreate, but the source code highlighter is not highlighting properly. Let me add a global rule to change that as a fallback...
Thanks for the feedback, it looks like a couple people were having issues with the syntax highlighting JavaScript and the code was being rendered with a default color. I modified the default color of the code so it will render better without JavaScript.
What have you written so far?
#!/bin/bash # Counting the number of pipes per line and outputing the offending line to a log file filename=$1 #First input parameter path with filename #File to parse if [ "$filename" != "$1" ] then echo "Filename required" exit 1 fi #Read file line by line loop and output offending line number to log file if [ -f $1 ] then while read -r line do ((i+=1)) count=${line//[^|]} echo Line# "$i:" "${#count}" pipes per line compare to "$2" expected per line done &lt;$filename fi if [ "${#count}" == $2 ] then echo All lines have the expected pipes elif [ "${#count}" -lt $2 ] then echo Line# "$i" has less pipes than expected &gt; sm_qa_csv.ksh.log elif [ "${#count}" -gt $2 ] then echo Line# "$i" has more pipes than expected &gt; sm_qa_csv.ksh.log fi exit 0
#!/bin/bash # Counting the number of pipes per line and outputing the offending line to a log file filename=$1 #First input parameter path with filename #File to parse if [ "$filename" != "$1" ] then echo "Filename required" exit 1 fi #Read file line by line loop and output offending line number to log file if [ -f $1 ] then while read -r line do ((i+=1)) count=${line//[^|]} echo Line# "$i:" "${#count}" pipes per line compare to "$2" expected per line done &lt;$filename fi if [ "${#count}" == $2 ] then echo All lines have the expected pipes elif [ "${#count}" -lt $2 ] then echo Line# "$i" has less pipes than expected &gt; sm_qa_csv.ksh.log elif [ "${#count}" -gt $2 ] then echo Line# "$i" has more pipes than expected &gt; sm_qa_csv.ksh.log fi exit 0
https://gist.github.com/curusarn/85cbeccbc4b5a7d218c56330be99a29b ``` #!/usr/bin/env bash # this makes bash produce error if you use uninitialized variable (good scripting practise) set -u # $# is set to number of arguments # -lt means less than if [[ $# -lt 2 ]]; then echo "Not enough args" echo "USAGE: ./count_pipes FILE MAX_PIPE_COUNT" exit 1 fi filename=$1 max_count=$2 logfile=/tmp/logfile # i is just for counting lines i=0 total_count=0 while read line; do # printf is kind of like echo (except better for cases when you are priting variables) # I assume you know what pipes do # tr -cd '|' deletes all characters except '|' # wc -c counts characters (only pipes are left so it counts pipes in this case) # these `` let you use result of a command as a value count=`printf "%s" "$line" | tr -cd '|' | wc -c` let "total_count+=count" # -gt greater than if [[ "$count" -gt $max_count ]]; then echo "More than &lt;$max_count&gt; pipes on line &lt;$i&gt;" &gt;&gt; $logfile fi # increment line counter let "i++" done &lt; $filename echo "Total pipe count: &lt;$total_count&gt;" &gt;&gt; $logfile ```
 FILE="testout.txt" while read; do [ "$REPLY" ] &amp;&amp; B+="${REPLY//[!|]}" done &lt; "$FILE" printf "%d\n" ${#B}
Thanks- the `case` looks pretty simple! However, how can I get it to list the items a directory (instead of the hard coded "Option 1" etc)? ``` #!/bin/bash # Bash Menu Script Example PS3='Please enter your choice: ' options=("Option 1" "Option 2" "Option 3" "Quit") select opt in "${options[@]}" do case $opt in "Option 1") echo "you chose choice 1" ;; "Option 2") echo "you chose choice 2" ;; "Option 3") echo "you chose choice 3" ;; "Quit") break ;; *) echo invalid option;; esac done ```
Out of curiosity, what are you wanting to do and how are you generating your random number?
Your description of what you want seems very confused. I'm guessing that you simply want to combine the *.csv files into one, adding up the sizes for each job, and creating an output file that has the totals per job, in the same format. `awk` is definitely the correct tool for the job. Unlike `sed`, it's a complete programming language. Unlike the shell, it's designed for text manipulation, instead of text manipulation being kind of tacked on. `awk` trivially does the bulk of the job using an associative array, which remembers the total size per job. The basic strategy is that the main `awk` loop does nothing but read all data, adding up the sizes per job. The `END { }` clause then produces the output. You may want to subscribe to /r/awk. #! /bin/sh if ! test -d "$1"; then echo "usage: $0 DIRECTORY" &gt;&amp;2 exit 1 fi cat "$1"/*.csv | awk \ 'BEGIN { # separate input fields by commas FS=","; } # main loop (run once for each line, with fields pre-set by awk) { # add up the sizes for each job size[$2] += $1; } END { # output the totals for (job in size) { print size[job] "," job; } }' \ | sort -r -n &gt; "$1.csv" 
Thanks for your help.
Yeah, case is nice but since the options you want to present may not be a static list, select may be more if what youre looking for: #!/usr/bin/env bash #this restricts the "menu" to a single column. Try it without. COLUMNS=12 echo "The following files were found:" #this displays at the top of the list PS3="Choose a file or 'Q' to quit: " #this displays at the bottom of the list. dir=$(ls) select filename in $dir "Quit" do if [[ "$REPLY" == Q ]] || [[ "$REPLY" == q ]] || [[ "$filename" == Quit ]] then echo "Quitting" exit 0 elif [[ "$filename" == "" ]] then echo "'$REPLY' is not a valid option. Please choose a number or 'Q' to quit." continue fi echo "$filename selected" break done 
I just started learning Bash and I'm doing this as an exercise. Here's the complete script I came up with. #!/bin/bash #This scrip will print a random word dictionary=/usr/share/dict/american-english wordCount=$(wc -l &lt; $dictionary) wordNumber=$(shuf -i 1-$wordCount -n 1) randomWord=$(sed -n -e ${wordNumber}p $dictionary) echo $randomWord
Hi, I'm sorry I had no idea I was being unclear. I do want to combine the *.csv files into one file, but I do not want to add up the totals for each folder. The numbers represent the folder's size, which each .csv being a different date. I have added a 'desired output' pastebin to my original post. For clarity, adding it to this reply too: https://pastebin.com/5wiuq53n I'll dive deeper into awk, it seems that the consensus is this is the tool I should be using, rather than trying to write to line with sed. I wonder if that is still the case if I'm not trying to add up any totals...? i.e. I am literally just trying to format some text.
&gt; I'm more visual when it comes to parsing; I need only the original output and an example of the target output, otherwise my brain gets lost in the details, so I'm probably not being very helpful here. :( I have added a pastebin with desired output, here: https://pastebin.com/5wiuq53n I hope that makes it more clear, I want the current list output sorted in columns per project.
I think this post is to broad. The things you list seem just like things you might have touched on in your course rather than a base for a project idea. I get the gist that you want to make a program that is Computer Forensics related. If you want to write something like a malware analysis program maybe it would be better to do in Python or another object oriented language rather then bash. I say this because bash scripts are useful for things like system administration, monitoring, web app deployment, data sorting and automating tasks but I don't think I would want to write a malware analysis tool with it :) - (I am sure its probably possible and been done though!). Maybe checkout /r/Python/ ? and other security related reddits? You could search github projects to see some code examples etc...
That sample output makes it much clearer what you want. It's totally another job for `awk`, and the strategy is similar. It's not so trivial, but it's still very, very much cleaner to process everything in memory rather than try to edit your file after the fact. It's made easier because an awk associative array can have multiple dimensions. One issue is that you want one line per input file. That means the `awk` script needs to have knowledge of the files being processed. We can't simply `cat` them together and pipe into `awk` as usual, because that won't give `awk` any information of what's in what file. Instead, we have the shell pass each filename to the `awk` script as an argument (the `"$1"/*.csv` near the end). We omit the main loop entirely (so `awk` doesn't read from standard input at all) and do all the data reading in the `BEGIN` clause, having `awk` open and read from each file itself using [getline](http://awk.freeshell.org/AllAboutGetline). Note: `ARGC` = number of arguments, plus one (being the program name); `ARGV[n]` = argument n (`ARGV[0]` = program name, which we skip, so we start from `ARGV[1]`). If you often process textual data, learning `awk` will pay off again and again. [Do it](https://www.google.com/search?q=introduction+to+awk)! Also, this helped me sharpen my own `awk` skills a bit, so thanks. Note that this should work on any `awk` and any sh-like shell on any UNIX-like operating system. #! /bin/sh if ! test -d "$1"; then echo "usage: $0 DIRECTORY" &gt;&amp;2 exit 2 fi awk \ 'BEGIN { # separate input fields by commas FS="," # for each input file... for (i = 1; i &lt; ARGC; i++) { # for each record in that file... while (getline &lt; ARGV[i]) { # determine job numer, avoiding duplicates for (j = 1; j &lt;= length(job); j++) { if (job[j] == $2) break } # remember job name job[j] = $2 # remember job size for current file and job size[i,j] = $1 } } } END { # no output record separator; output newlines manually ORS="" # output the header for (i = 1; i &lt; length(job); i++) { print job[i] ", " } print job[i] "\n" # output the sizes for (i = 1; i &lt; ARGC; i++) { for (j = 1; j &lt; length(job); j++) { print size[i,j] ", " } print size[i,j] "\n" } }' "$1"/*.csv &gt; "$1.csv" if ! test "$?" -eq 0; then echo "transformation failed" &gt;&amp;2 rm "$1.csv" exit 1 fi 
You need to add one to counter in the while loop
That’s brilliant! Changed the code around a little and managed to get it to work - thank you for that! ☺️ 
If you're using bash, simple c style loop are available. ``` limit=5 echo "$limit" for ((i = 0; i &lt; limit; i++)); do echo "$i" done ``` The only gotcha is that you don't need the `$` sign before variables in the loop declaration.
Some universities, which teach Forensics publish their topics of completed projects, you can probably pick up some ideas from their websites. Also, why no to connect to Forensics guys in linkedin, who did similar programmes and try to get some from them
Good luck!
You can't do this: `if [ "$result % 2" == 0 ]` With the style you use, you could do let modulo="$result % 2" if [ "$modulo" == 0 ] Also you need to switch around either your return statements or the status check, because if the modulo 2 is 0, then it's divisible by 2 and so even, while you have that return as 1 and then print that as impair.
You don't need shift there; it's not an applicable place for shift; the for loop will already iterate over the next item. Remember to indent the contents of the function. Indentation is very important, much like paragraphs in written English is important, lest you wind up typing a huge wall of text which nobody will want to read. :P The error is because of the line: if [ "$result % 2" == 0 ] This fails because the syntax is incorrect (for what you're after), thus the function returns 0.
thank you! It worked. You the boss
The first thing I see is that you're doing a for loop which declares 'i' to be the variable, and then not using it. If you do "for i in $*" then on each loop you'd be using "$i" as the variable.
That's not an mistake, it's just unfortunate that the variable doesn't get used, in that way you're thinking.
It worked aswell. Also thank you for the tips, I'll keep all of that in mind, I was kinda confused about these basic concepts, thank you for your time 
I tried to find a different style for everything, just so you can see what else you can do in a bash script: #!/bin/bash calculate () { local sum i for i in "$@"; do (( sum += i )) done (( sum % 2 == 0 )) } if calculate "$@"; then echo number is pair else echo number is impair fi This uses the bash `(( ... ))` feature for calculations. It has similar rules as what you might know from Java or C. You can read a bit about it by looking through `help let`. You don't have to use `$` for variable names inside of it, except when you want to access $1, $2, etc. In a function, the last line that was executed is used for the error status of the whole function. You can for example write something like this: calculate () { local sum i for i in "$@"; do (( sum += i )) done if (( sum % 2 == 0 )); then true else false fi } Here, the last lines that will be executed will be the ones with the 'true' or 'false' command on them. You should use `local` to declare all variables you want to use inside your function. It makes it so the function will not overwrite variables that might already exist on the outside. Here is another version that uses the `shift` command: calculate () { local result while (( $# &gt; 0 )); do (( result += $1 )) shift done (( result % 2 == 0 )) } 
Thank you so much, this is incredibly helpful. I'm not up to speed on awk syntax yet, but I will dive into it and figure out how to replicate this script myself. Again, thanks a million :)
I enjoy having `completion-ignore-case` on for the file system, but I haven't figured out how to get completion to ignore case.
Refer to this AskUbuntu article: https://askubuntu.com/questions/87061/can-i-make-tab-auto-completion-case-insensitive-in-the-terminal # If ~./inputrc doesn't exist yet, first include the original /etc/inputrc so we don't override it if [ ! -a ~/.inputrc ]; then echo '$include /etc/inputrc' &gt; ~/.inputrc; fi # Add option to ~/.inputrc to enable case-insensitive tab completion echo 'set completion-ignore-case On' &gt;&gt; ~/.inputrc
You need the while loop. That part waits for the application to start. I would do some minor tweaking... #!/bin/sh /snap/bin/yakyak &amp; # Attach to the app just started WID = PID$! # this waits for the app to create a window while [ "$WID" == "" ]; do WID=$(wmctrl -lp | grep $PID | cut "-d " -f1) done # play around with the numbers here after looking at 'man wmctrl' wmctrl -i -r $WID -e 0,50,50,250,250
I'm confused by your use of `[ ! -a ~/.inputrc ]`, that does not seem 'correct'.
I was thinking there must be a better way to know when the app starts. Thanks for the reply! 
Sorry. Copy and pasted from the link I provided.
I think you might be able to do all the work with just one xdotool command line. Perhaps the following is possible (I can't try this as I use a tiling window manager that completely ignores where programs want to put a window): xdotool \ search --onlyvisible --name yakyak \ windowsize 25% 100% \ windowmove 75% 0% You would repeat that in a loop and wait for xdotool's exit code, perhaps like so: until xdotool \ search --onlyvisible --name yakyak \ windowsize 25% 100% \ windowmove 75% 0% do :; done That `:` after the 'do' is a bash command that does nothing. It has to be there as bash complains if there's no command between "do" and "done".
This worked great and it's much nicer to look at. Thanks!
Funny I just looked into this today for one of my scripts. I wrote a script that will launch a vnc window for both screens on a remote computer, and didn't want to remember the exact hostname. I saw it worked for ssh, so I knew there had to be a way. I just used the built-in `_known_hosts` function. complete -F _known_hosts ./vnc The only issue is that it will only work if I call the script from the directory specified in the parameter. The only way I know to fix it, it to put the path into my PATH variable echo "PATH=\"\${PATH}:${PWD}\"" &gt;&gt; ~/.bashrc complete -F _known_hosts vnc And here is the contents of my `vnc` script: #!/bin/bash function connectToScreen() { local SCREEN_NUM=${2} local QUIET="-q" local SCALE="-scale 0.45" local BACKGROUND="-bg" local SCREEN0="-display :0.0" local SCREEN1="-display :0.1" local PARAMS="${QUIET} ${SCALE} ${BACKGROUND} ${SCREEN0} ${VIEW_ONLY}" local PARAMS0="${PARAMS} ${SCREEN0}" local PARAMS1="${PARAMS} ${SCREEN1}" if [ ${#} -eq 0 ]; then { echo "Must specify host" } else { HOST=${1} ssh -t ${HOST} "/usr/bin/x11vnc ${PARAMS0}; /usr/bin/x11vnc ${PARAMS1}" SSH_X11VNC_STAT=${?} if [ ${SSH_X11VNC_STAT} -eq 0 ]; then vncviewer ${HOST}:0 &amp; &amp;&gt; /dev/null vncviewer ${HOST}:1 &amp;&gt; /dev/null fi } fi } connectToScreen ${1} 
I went with: array=( first second third fourth fifth sixth seventh eighth ninth tenth) for i in "${array[@]}" do if [[ $# = 0 ]]; then break else echo "The $i argument used is: $1" shift fi done But thanks so much for the help man! I hadn't thought of the other possibilities and I actually prefer your methods!
This works fine when executing while I'm already logged in, but fails to resize/re-position when executed automatically at startup. Why is that?
I’m not sure I understand: &gt; dumping of the two RPM databases doesn't leave the RPMs in the exact order as each other Why is this a problem if you’re piping the output to `sort`? Do the packages have different naming formats between the two systems?
You need to look into the available format fields of `rpmquery`. Here's something from a script of mine: rpmquery -a --queryformat "%{NAME};%{VENDOR};%{VERSION}-%{RELEASE}.%{ARCH};%{INSTALLTIME:date};%{INSTALLTIME}\n" You can either `sort` the outputs of both and `diff`, or you could use one as an input for `grep -vf` (hint: do that without the %INSTALLTIME format options)
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
There's an autoupdating suggestion script for zsh that has some great logic. Here's the whole git. https://github.com/zsh-users/zsh-autosuggestions Here's the logic 
What you do with the commas on the "colors=(...)" and "equal=(...)" lines is a mistake. You need to replace the commas with a space character. What you have right now, you will get one single element in your arrays. It's just spaces because inside that "=(...)", bash will use the same rules as when you type on the command line to produce a list of words. If you want to read some details about that, here's an article: https://mywiki.wooledge.org/WordSplitting You can see a bit more about how bash treats your lines by running the script like this: bash -x scriptname 
TOTAL brainfart, wasn't until I used vimdiff that I realized some were tagged as .686 and some were tagged as .6 and that was throwing off my diff command. I got it now, diff worked perfectly, thanks guys!
you can run `sudo -v` in a loop in the background of the script and keep refreshing the timeout. (you can ask for sudo privileges with the same command at script start up)
Thank you this works :)
&gt; Apparently the most recent edition of the Bash Cookbook has a solution, but it was excluded from the preview (so close!). Do you have a page number or some context? I have an edition of the Bash Cookbook (it was part of a Humble Bundle sale), so I might be able to look it up for you. Otherwise, I suppose you could split up the path into elements and use `mkdir` without `-p` on each element, checking the exit code every time. Or perhaps `chown --no-dereference`? (`chown -h` if you need to stick to POSIX.)
Oh, well, it was 363. I hope that's the correct book. In the Index it was under "Race Conditions".
Hm, if it’s the location I found, then I don’t think it’s directly applicable. To guard against `touch $temp_file &amp;&amp; chmod 0600 $temp_file`, it advises `mkdir -p -m 0700 $temp_dir`, or temporarily changing the umask before `touch`. But the whole section is just about guarding a temporary file, which isn’t what you’re after.
Is that the First Edition (2007)? That one didn't mention directory creation. The Second Edition (October 2017) does mention directory creation in the Index: https://books.google.com/books?id=0aA4DwAAQBAJ&amp;lpg=PP1&amp;dq=bash%20cookbook&amp;pg=PA692#v=onepage&amp;q&amp;f=false
Yeah, I have the first edition (eighth release; local file modification time November 2016).
Thank you from me too. I didn't realise that value was stored there; much more efficient. I'll be using: read &lt; /sys/devices/platform/coretemp.0/hwmon/hwmon0/temp1_input printf "%3d°C" "${REPLY%[0-9][0-9]}" Over here, and also OP, this might interest you: https://github.com/terminalforlife/i3config/blob/master/.libi3bview
What I've seen for BitTorrent programs is, they have a feature where they monitor a certain folder for new .torrent files showing up in there. You are supposed to use your web browser to browse stuff and save a .torrent file in that folder. The BitTorrent program then immediately deletes the .torrent file and starts the download. That's how it knows what download is already added, simply because of previously deleting it. You could try to copy that idea. You would have a script that creates a text file with the URL in a certain folder. You could run it manually or use it with the web browser through the "open with" addon that I think is available for both Firefox and Chrome. Then you have another script that works like some sort of daemon and monitors the folder for those text files. When there's files, it takes the first (oldest) file and moves it to a second "in progress" folder and starts youtube-dl. After the download is done, it deletes the file and goes back to the first folder and either starts with another file or goes back to monitoring if there's none. Maybe with that "in progress" folder, you could make things persist through a reboot. Hopefully youtube-dl can somehow restart a download. With that folder and individual files that only ever contain one single URL, you would avoid the problem of how to read and write to the same text file with two different scripts. You could prepare your files under a temporary name that is getting ignored by the daemon script before renaming them to their final name. The daemon will then only see something that's guaranteed to be finished written to.
tnx it worked! How about random time between 0,4 and 0,9 without it ever going lower than 0,4 or higher than 0,9?
Oh wow, this idea actually makes me think of using something inotify to detect new files in the queue folder and run youtube-dl off that--that's even better than executing some youtube-dl command periodically even when there's potentially nothing to run, since if there's no file in the queue folder, youtube-dl won't even need to run. 
It's literally impossible.
Check out https://github.com/noctuid/dotfiles/tree/master/aesthetics.
So when's the homework assignment due?
The code is here, and that page has links to the Firefox and Chrome addon web stores immediately at the start of the README: https://github.com/darktrojan/openwith Because of changes in the last two or three Firefox versions, you will also need to manually install a small Python script somewhere in your $PATH, and I assume the same is true for Chrome. The instructions for that are in the options screen of the addon after you have installed it.
I tried looking at what "i3status" is doing internally. It's a small C program that comes with i3-wm. It uses the `statfs()` C function (you can look at `man statfs` to see what it's about). This seems to be basically the same as using `df` in bash script. The i3status date/time output updates just once in 5 seconds, like this: 12:23:00 12:23:05 12:23:10 I would then assume the disk usage output also works the same, so not in real-time. Maybe there's no really good way to do this, and maybe `statfs()` or `df` are efficient enough, are never looking at the actual disk and are just looking at some data structure inside the kernel?
Thank you! :-)
You probably have *something* writing to disk most of the time (just run `iotop` and see for yourself), so I don’t think real-time monitoring for that makes a lot of sense. Just pick some small polling interval that you’re comfortable with. The `statfs` syscall itself is cheap (I assume the kernel keeps that information in memory – it’s just a handful of numbers): somewhere below fifty microseconds on my system. See for yourself: for i in {0..100}; do strace -T -e statfs df . 2&gt;&amp;1 done | grep -o '= [[:digit:]]\+ &lt;[[:digit:]]\+\.[[:digit:]]\+&gt;' | sed 's/.*&lt;\(.*\)&gt;/\1/' | sort -n If you want to avoid the overhead of constantly spawning new `df` processes, you can write a small C program that keeps calling `statfs` (or `statvfs`) and printing the result in a loop.
Running the `df` command every second is actually not anywhere near as bad as I imagine you're thinking. I wouldn't worry about it. I use that method myself: https://github.com/terminalforlife/i3config You mind find some of those useful, particularly the .libi3bview file, my function library for various outputs, some of which I use on my bars.
My method, albeit much more advanced, but potentially still education here, would be: A=(cat dog cow) B=(cat dog puppy) for C in `eval printf "%d\\\\\n" {0..$((${#A[@]}-1))}`; { [ "${A[C]}" == "${B[C]}" ] &amp;&amp; D="is not" || D="is" printf "%s %s the same as %s\n"\ "${A[C]}" "$D" "${B[C]}" }
You can produce a list of index numbers for the elements in an array like this: $ x=(a b c) $ echo ${!x[@]} 0 1 2 This is especially interesting to know about if you ever do something where you delete elements in an array, for example see here: $ unset x[1] $ echo ${!x[@]} 0 2 $ echo "${x[@]}" a c Bash is weird and the arrays are more like a hash it seems, are not really an array like in other languages. You can for example also do this: $ x[21585]=hello $ echo ${!x[@]} 0 2 21585 $ echo "${x[@]}" a c hello 
Well damn, I knew about all that, but for some reason never thought to use that (the first examle) in place of the `eval` or `seq` usage; never occurred to me! Thanks for sharing.
I'm new to bash, but I imagine you could store the number of times repeated in a variable, then do a while loop that loops while repetitions is greater than 1.
this doesn't seem to work , it gives back an error with the while [[ $repetitions &gt; 1 ]]
It works fine for me. What's the error?
https://gyazo.com/a0ed69292090bb280348c32c66e2469f
Hi, I'm a bot that links Gyazo images directly to save bandwidth. Direct link: https://i.gyazo.com/a0ed69292090bb280348c32c66e2469f.png Imgur mirror: https://i.imgur.com/Az4t71a.png ^^[Sourcev2](https://github.com/Ptomerty/GyazoBot) ^^| ^^[Why?](https://github.com/Ptomerty/GyazoBot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/u/derpherp128) ^^| ^^[leavemealone](https://np.reddit.com/message/compose/?to=Gyazo_Bot&amp;subject=ignoreme&amp;message=ignoreme)
ok found the problem, now i have one more lol anytime i put the number of times i want it to repeat it does 1 less than what i say.. if i say 5 it does it 4 times. 
good pickup, just make it [[ $repetitions &gt; 0 ]]
This `[[ $variable &gt; $number ]]` is an evil mistake. Check out this experiment at the command line about why it's a mistake: $ x=34 $ if [[ $x &gt; 100 ]]; then echo true; else echo false; fi true In this example here, bash thinks "34 &gt; 100". This is happening because the "&gt;" comparison is for text, not for numbers. When you use "&gt;", bash treats things as individual letters, and it thinks the text "3" &gt; "1", and then decides the text "34" &gt; "100". The solution is to write this: [[ $variable -gt $number ]] Or this: (( variable &gt; number )) This kind of thing is evil because it will work a lot of the time because "&gt;" will have the same result as "-gt", and then suddenly your script will not work because you run into an example where the text comparison produces a different result than the number comparison. You can get an overview of the comparison operators for `[[` with these commands: help test help '[[' It doesn't really explain the "-gt" stuff, it just lists them. Hopefully the names are obvious: * lt = less than * gt = greater than * le = less or equal * etc. And for `((`, you can read about what's possible inside it by doing: help let help '(('
Thanks for the tip. I noticed shellcheck.net also advised to use -gt.
 for file in ${answer}/*; do mv ${file} ${file}-$(date +"%Y-%m-%dT%H:%M") done The format's not exactly what you gave, but having slashes (which are normally directory separators) and spaces (which have to be escaped) in the name makes things a lot more awkward.
oh wow that's perfect, i didn't know about the slashes either that's good information to know thank you 
lol this bash scripting is pretty crazy, im just kinda self teaching my self ( with the help of you guys as well) and the littlest things can change a whole lot. Thank you for your input as well :) i'm sure ill be back soon enough with another noob question :) 
I just remembered, there's a command line tool named "shellcheck". It tries to hunt down those weird little mistakes that always happen in bash scripts. Your distro probably has a package for it. You can also try it online at www.shellcheck.net without having to install it.
&gt; eval exec "$lock_fd"'&gt;&amp;-' Don't use eval. Do this instead: exec {lock_fd}&gt;&amp;- &gt; So what is the proper way? Depends on the script. `flock -u` releases the lock even when there are background processes that still have the file open. Closing the lock_fd only releases the lock once all background processes have also closed it.
You can probably guess what this does: for (( x = 100; x &gt;= answer; x-- )); do # do something here with x done You could write it like this as well: x=100 while (( x &gt;= answer )); do # do something here with x (( x-- )) done That `(( ... ))` thing is something special for working with numbers. You can get a list of what it can do by typing `help let` and `help '(('`. It is very similar to what you might know from C or Java.
How could i make it start at 100 every time ? make c = 100? 
I only used eval because exec "$lock_fd"&gt;&amp;- and exec ${lock_fd}&gt;&amp;- failed, i.e. I was unfamiliar with proper use of automatic file descriptors in redirection i.e. no $ required before {}. Your answer straightened me out. Any further comments on the trap statement? 
Bash for quick and dirty scripts (up to 20 lines max), for everything else Python / Ruby / Perl (where Python would be my lang of choice). Reasons why Bash scales badly: * stringly typed * quoting (forget and quote and your script will behave in unexpected ways) * no datastructures except flat arrays and hashes * bad performance * crappy error handling * unconsistent / arcane syntax * basically untestable (you may test the script but it's nearly impossible to test smaller units like functions) * functions without the ability to return anything other than status codes (you can capture stdout but that will lead to other "interesting" problems
Python is a great thing to learn, and is almost everywhere.
The script requirement change to not echo every line, and add new counter variable and echo total count of bad rows or complete success rows. Could you please help with this?
The script requirement change to not echo every line, and add new counter variable and echo total count of bad rows or complete success rows. Could you please help with this?
 #!/usr/bin/env bash # Count the number of pipes per line and output the offending line to a log file [[ -z $1 ]] &amp;&amp; { echo "Filename not specified"; exit 1 ;} [[ ! -f $1 ]] &amp;&amp; { echo "File not found [$1]"; exit 1 ;} filename="$1" [[ -z $2 ]] &amp;&amp; { echo "pipe limit not specified"; exit 1 ;} pipelimit=$2 passcount=0 failcount=0 row=0 while read -r line; do ((row+=1)) buff=${line//[^|]} pipecount=${#buff} if [[ $pipecount -gt $pipelimit ]]; then ((failcount+=1)) echo "Line# $row has more pipes than expected [$pipecount]" &gt;&gt; sm_qa_csv.ksh.log elif [[ $pipecount -lt $pipelimit ]]; then ((failcount+=1)) echo "Line# $row has less pipes than expected [$pipecount]" &gt;&gt; sm_qa_csv.ksh.log else ((passcount+=1)) fi done &lt;"$filename" echo "total correct rows: $passcount" echo "total bad rows: $failcount" exit 
Do you want to examine them separately? Or together? e.g. if you specify a pipelimit of 4 then your loop encounters a line with 3 pipes and 2 commas, has it exceeded the limit? 
I'll look in the plug in but anything in single quotes will be literal, i.e. no need to escape, but this also means the variables and escape sequences will not work, (printf is different in this regard)
My personal limit is 100 lines including comments and whatnot, but this is very accurate, also python seams to be the current popular choice of the three.
As for your worries about python, it's definitely a full featured language not even by comparison to bash but it just is, you won't find out later that it lacks ints or that you can't return data from functions like another language.
I want to examine them separately. Sometimes the files may be comma separately instead of pipe separately
20 lines max is totally useless If my script consists only of invoking commands, managing processes, using file descriptors, job control I would rather write it in bash even if it is 500 lines of a good structure with functions and a proper entrypoint which is easy to follow. In other languages this would be much more cumbersome. But obvsiouly for every other reason you pointed out it is easier to in another language.
Will you be specifying a commalimit on the command line? Or will this value be the same as the pipelimit?
Yes, I want to specify a comma limit. The file with either be pipes separated or pipe separated not both simultaneously. 
*(you can see how involved this can get - even for simple problems). ;)* Will there only ever be these 2 characters used as delimiters? If you'd like more flexibility, the script can be written so you'd specify the delimiter character on the command line. Which means running the script for each character you want to check for. But if the delimiter will only be these 2 characters, then this is a simple addition to the existing script. 
(yes, I notice that. I committed to solving it though) Yes, just pipes or commas will only be used as delimiters. 
OK, only need to change the line that strips out all other characters, and add the comma char to the pipe char, like this: `buff=${line//[^|,]}`
I'm looking into doing something at the terminal level with urxvt now. It seems like you can extend it with perl.
I do everything in Python or Bash. Within those two I can basically do anything I need as a Sysadmin/Linux Dev-ops type role.
`unset HISTFILE` and then exit the shell would be the easiest way. You might also find the [`edit-and-execute-command`](https://www.gnu.org/software/bash/manual/html_node/Miscellaneous-Commands.html#Miscellaneous-Commands) Readline command useful when pasting shell commands. It lets you use your default editor to edit the command, and is bound to Ctrl-x,Ctrl-e by default.
I agree on some of your points, but there are a ton of use-cases for using bash, even for larger projects. Obviously, there are exponentially more use-cases for using a "real" language, but the needs for bash are very common. I have a very large project written in bash, totally well over 10K lines and performance wise, I disagree on "bad performance". It's super fast for iterating over arrays/lines of text. I've moved to calling out to other languages where necessary (python, ruby and perl, mostly) and the interpretter spin-up time for those are longer than the scripts would run outside of that. like any language, there are things that it's good at, things that it's bad at and places where it can lead to headaches if you don't understand it. C has this problem when it comes to memory management and lack of objects, python has this issue with indentation, etc. the error handling is "fine." Sometimes I wish I had exception handling, but with `set -e` and `set -o pipefail`, it's perfectly usable. add `trap` to the mix and you're pretty good to go for most uses. It's also very testable. I've got unit tests for most of my functions and they're run as part of CI/release. `bats` (https://github.com/sstephenson/bats) is life-changing here. And regarding what functions return, this can be a pain, especially when you have to have your functions have side-effects in order to get them to do what you want... for example, if you have a function that is supposed to return an array, you need to set some global to the array. Arrays also can't be passed between processes, which can cause a lot of headaches. but when you start running into that, you're starting to get to where it's no longer a good fit. I've had 2 projects in the past that started out as ruby, but I ported _to_ bash because they were heavily based around simply writing to files or shelling out to external commands. When you're building tools that are gluing together lots of external commands and parsing and reporting on their output, it's a lot simpler to write in bash than to deal with huge blocks of shell out and error handling code. Bash is super-close to the POSIX layer, so things aren't hidden from you like they are in other languages; command arguments, file handles and status codes are all first-class citizens. it's hard to find a language that's consistent. Ruby used to be one of the more consistent "modern" languages, with python being one of the least, but as languages mature and change, they take with them a lot of baggage. couple that with bash's need to be run interactively and use the same syntax and you wind up with some strange syntax considerations to keep things typeable. I guess this is why they made elvish (https://github.com/elves/elvish).
Holy shit. Where have you been /u/KnowsBash?
&gt; It seems that ***I'm terrible with*** bash FTFY
I've tried using python, but 99% of what I do is "outside" of python. Meaning bash is basically what I should be using. If I need a program to do something on it's own, python. If I need to do things with the system and run system commands, bash. 
Make sure HISTCONTROL=ignorespace. Then add a leading space to your command.
Everyone is scared of Perl, but maybe check it out anyways. It's always installed so a simple script runs anywhere. About using it as a replacement for bash scripts, it has something similar to bash's `` `...` `` or `$(...)` to run a command and capture its output, and running a command line and checking its error status isn't wordy to write, and if you want to run something while reading/writing to it through a pipe, that's also not a terribly big deal to do. I slipped into using Perl for some stuff here through a book "Perl One-Liners". Wherever you normally use 'sed' or 'awk' or 'grep' in a bash script to do something to the text output from some other command, you can always replace it with a Perl one-liner. The Perl interpreter seems to be able to start up and compile a single line of code just as fast as 'sed' does its thing. The book is basically just a whole bunch of one-liner examples, and I could then guess my way around how things work to make use of it for when either sed or awk weren't good enough for some problem, then slowly got seduced into looking more into Perl.
ksh; posixly good
Some nice suggestions, I hadn't heard of the ignorespace option. Really though the prevention begins with you in being careful. We've all done it, but we learn from the mistake and try not to make it again. I remember the first time I did "sudo rm -rf /"... notice I said the first time. Yea. 
Once, I did sudo rm -rf . /dir instead of ./dir. No amount of ctrl-c will save your day.
No worries. I don't really either but it can't be that hard :-) I think there possibly are some plugins that do similar things for it already.
Forking?
Python. You could learn zsh, the scripting is a lot more sane, but then you're descending into posix hell if you ever need to use your scripts anywhere else.
I don't think it can be prevented, but you can automate the fix pretty easily. Set up a job or add something like this to your *shrc sed -i '/.\{80\}/d' $HISTFILE where 80 is the max length of a line in the file.
&gt; history -r This will write current .bash_history over the current session. .bash history doesn't log to disk (by default) until you exit the session.
Gotta say at least open knows to escape the out it took me way to long to realize that.
You could set an alarm or maybe book an appointment to a sleep study.
To expand on that is even do the whole thing as a one liner. Do &amp;&amp; between youtube-dl and play as well so that you only get the song if YouTube-dl finishes successfully. Something like: `( [[ -d $dir ]] &amp;&amp; YouTube-dl &amp;&amp; play ding ) || play error_sound`
Yep, that's exactly how I'd do it. I was being a bit lazy last night when I replied in account of being on my phone.
Prepend each line of yr code with four spaces for it to be formatted as `code` in yr post. Will make yr poem more readable.
Wow, thank you! You are absolutely right, I think I will reimplement it as an if-then-else block. Thank you for the links aswell, really appreciate it!
There's no need to spawn a new shell process (subshell) there. Suffices: { [ -d "$dir" ] &amp;&amp; youtube-dl &amp;&amp; play ding; } || play error_sound
There's no issue with spaces in Linux, as far as I'm concerned; I'm not sure why people keep saying this. :\ There are ample tools at our disposal to handle them.
Just adding that ",minorversion=1" to the end of the option field can be done like this with a Perl one-liner: perl -pe 's/^\s*(?:\S+\s+){3}\S+\K/,minorversion=1/ if /^\s*netapp:/' filename You can make Perl edit a file by adding a `-i` parameter at the start: perl -i -pe ... I also tried to think about how to make it so it will only act on lines that do not already have a "minorversion=" thing in the option field, and I came up with this here: perl -pe 's/^\s*(?:\S+\s+){3}\S+\K/,minorversion=1/ if /^\s*netapp:/ and not /^\s*(?:\S+\s+){3}\S*\bminorversion\b/' filename
I tried awk '/netapp/{print "minorversion=1,"$3}' /etc/fstab but couldn't get how to print the whole thing. What does the $4=$4 do? And why in the beginning? Thanks!!!
You need to read the thing that follows as well when looking at that $4=$4 you mention: $4=$4",minorversion=1" Look at the right side of the `=` symbol. Apparently, how awk behaves, when you mention two texts next to each other, they get fused into one text. This means this here: $4 ",minorversion=1" Turns into whatever is currently in the fourth field of the line, plus ",minorversion=1" added to its end. This is then getting written into the fourth field through the `=` operator. The `print` then follows as a separate command. When you call it without parameters, it apparently takes all fields and prints them.
I just want you to know that RES tells me that I've up-voted you 55 times. Now it's 56. You're like, my rock, or something.
Thank you! I’ve never seen heredoc before, that was really nice. The case statement was way more elegant than what I did. I’ll also implement a function so that the code looks nicer. Thanks again for reading through the code, you gave really good advise :) 
I'm not a genius either but I can not figure out what you want to do, more info please 
Hi there, I built this little tool to help me put simple a quickly some format to my bash scripts, I thought you might find it helpful :) any feedback and contributions are welcome.
What's the CEO of Microsoft doing writing a bash script? 
You know how linux friendly windows is this days ;)
No idea what could cause this… I tried to reproduce it and everything seems to be working as expected: cd /tmp while true; do nano foo; sed -i 's/[[:lower:]]/\U\0/g' foo; done After I wrote some lowercase text to the file and closed nano, nano popped up with the file converted to uppercase. Worked repeatedly. My best guess is that it must be something odd about your `updateFiles` function… perhaps you can share it?
Thanks for your endeavors. As it turned out I was being an idiot and forgot one more function call in my loop. Thanks again.
You didn't give us very much info to go on here. I'm assuming you are using CUPS and the 'presets' you are talking about are print options like paper size, orientation, dpi, etc. Add the printer with `lpadmin`, set as default with the options you want using `lpoptions` with the `-o` option.
For what I read in the docs I based most of my info (https://misc.flogisoft.com/bash/tip_colors_and_formatting) it seemed to be compatible with plenty of terminals, and since it worked for me in MacOS I thought it was more or less standard. If this is only compatible with xterm and similar, what works for the others? tput only? and if there are many and widely used such terminal apps I might consider switching to use tput, or adding an option to chose between the two
The only thing that I am trying to get changed is the presets. I have already tried using CUPS. The actual setting is print backgrounds. And I finally figured out that it’s not even a printer setting. I don’t know why I didn’t think of it sooner. I was thinking about it all wrong. I had been trying to script it to printer settings when I needed to be script the browser plist. All is well now. I am sorry for the lack of info. I was driving. 
What a great little web-app! Well done.
what have you tried? what do you have so far? what are you stuck on?
We're gonna need some more info, because this sounds a bit like a scam.
Thank you!!! Not all that proficient with perl, but alway room for improvement. Thanks!
Thanks a lot! :)
he's embracing
I remember I've seen someone somewhere complain about scripts using "tput". What happened was that there was a situation where the terminfo database was wrong about certain setups, and with hard-coded escape codes colors etc. were fine, but when using tput things were wrong. Maybe close to everything uses the same escape codes for the basic features, so the chance to run into something not working with hard-coded codes just isn't realistic. And then maybe with the terminfo database things can in practice go wrong more. About where I've seen this, I guess this might have been an answer on stackoverflow.
&gt; I remember I've seen someone somewhere complain about scripts using "tput". What happened was that there was a situation where the terminfo database was wrong about certain setups. s/he should has repair the broken database... `tput` should be prefer because even if it works for the majority of the terminal nowadays, it doesn't with some of them. You also don't know in the future, other escape characters can be use by a terminal. With `tput` you'll be sure that your code is portable. *at least, let the user choose between ANSI hardcoded and tput* :) Read the full *discussion* section : http://mywiki.wooledge.org/BashFAQ/037
Can you post your code? Also, can you post the exact error?
extending
this is great, thank you!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
What you ask for exactly would be done like this with a "function": matlab() { "$HOME/MATLAB/bin/matlab" "$@" } Or like this with an "alias": alias matlab="$HOME/MATLAB/bin/matlab" I think this isn't a very good way to do this. A different way would be as follows: Choose a location in your home where you want to collect useful scripts. Examples: ~/bin ~/.local/bin Add this location to your $PATH in your ~/.profile or ~/.bash_profile login scripts. You might already have a location like that added to your PATH, so check for that with: echo $PATH After you have a location like this set up, create a link to matlab in there like this: ln -s ~/MATLAB/bin/matlab ~/.local/bin/matlab
You are using relative pathname here where .. denotes previous directory. Use absolute pathname for that. Use pwd and append the file name for eg /home/yourusername/directory/program. Absolute pathnames even though longer to type reduces your chances of error due to path.
export PATH=$PATH:/path/to/matlab
`export` stores the variable. $PATH is a colon separated list of locations the shell searches for matching files (programs) in, e.g. /bin, /sbin, etc. Matlab is not currently in your PATH since it fails to execute when you run `matlab`. Exporting PATH=$PATH:/path/to/matlab preserves your existing PATH variable (so you can still run your programs) but appends /path/to/matlab (so you can run matlab too).
Thank you for the explanation. There’s not enough commenters like you. 
Your first solution, to create a function, and the alias seem like different ways of doing the same things. Is there a reason/situation where one is better over the other?
idk about iterm2, but try something like (with checkwinsize on and then off): while : ; do printf "%s | %s\n" $COLUMNS $(tput cols); sleep 1; done $COLUMNS shouldn't update until the process is done(killed) - unless checkwinsize is on (on the other hand tput gets it correct either way in my term).
Should also note that variables available to other programs have a name, environment variables. If you have tab-completion you can type `echo $`, hit `tab` twice and get a list of all environment variables that have been set. 
When you use Bash special characters (hard quote, soft quote) you can write using ascii form, example: &amp;nbsp; echo -e '\x27' = ' ... Soft quotes (") indicate text with Bash expressions, shell search Bash syntax in tex and evaluate it. Hard quotes (') indicate pure plain text: $ echo '$$' =&gt; $$ ;; echo "$$" =&gt; 1234
Echo "alias matlab='which matlab'" &gt;&gt; ~/.bashrc You chan change 'which matlab' by the absolute path. Change the single quotes to ticks. On mobile, no ticks. 
That is not going to work. The matlab executable is not in his path, so which will return "No matlab in $PATH"
I recently tried to do something like this for the exact same reason (pasting filenames with spaces directly into the terminal). I ended up with the following solution that seems to work rather well. Some might call a hack more than a solution, but I figure 'if it works it works' getLiteralInput() { # Grabs the most recent entry from 'history' and attempts to smartly extract the actual command input into the terminal. # A single set of single quotes (but not double quotes) will be removed if they encompass the entire input. This provides the same input 'a' that would be passed to 'foo' in $(foo 'a'). # Note: This will NOT work sub-function calls, since it only grabs text entered directly into the terminal. echo "$(history 1)" | sed -E s/"^ *[0-9]+ +[^ ]+ +(.*)$"/'\1'/ | sed -E s/"^ *('? *.*[^(?: *'? *$)].* *'?) *$"/'\1'/ | sed -E s/"^'([^'$]*)'$"/'\1'/ }
I maybe have the start of a solution. I have a function that grabs the most recent history entry in an attempt to get the literal input entered into the terminal. Unfortunately, it doesnt natively work if bash throws an error, which typically happens with `&amp;` characters. BUT, if you can figure out a good way to add `2&gt;/dev/null`automatically at both the end of the inout and immediately before every `&amp;` character *before* bash throws an error it will work. I dont know the best way to do this, but it seems like the type of thing tha6 might be possible. getLiteralInput() { # Grabs the most recent entry from 'history' and attempts to smartly extract the actual command input into the terminal. # A single set of single quotes (but not double quotes) will be removed if they encompass the entire input. This provides the same input 'a' that would be passed to 'foo' in $(foo 'a'). # Note: This will NOT work sub-function calls, since it only grabs text entered directly into the terminal. echo "$(history 1)" | sed -E s/"^ *[0-9]+ +[^ ]+ +(.*)$"/'\1'/ | sed -E s/"^ *('? *.*[^(?: *'? *$)].* *'?) *$"/'\1'/ | sed -E s/"^'([^'$]*)'$"/'\1'/ } # get literal input of 'abcde &amp; 12345' a=$(getLiteralInput abcde 2&gt;/dev/null &amp; 12345 2&gt;/dev/null) a="$(echo "${a//'2&gt;/dev/null'/}" | sed -E s/'^(.*)\)$'/'\1'/)" a="${a// / }" echo $a
Or, you can use the env command. I think that shows all variables with their values
According to ``bash(1)``, interactive shells execute the contents of ``~/.bash_logout`` on exit; &gt; When an interactive login shell exits, or a non-interactive login shell executes the exit builtin command, bash reads and executes commands from the file ~/.bash_logout, if it exists. Fair warning though, *any* shell on the machine will run that code on exit, so if you SSH into the host a second time it could kill the X server started in the other session (unless you code a check for that). Also, if your SSH session is interrupted and the shell doesn't exit cleanly I don't think it will be run, not 100% sure on that one though.
This should print a list of the files: find . -type f That "." is the current directory where you are. You can change it to a different location. You can also use a list of several locations. It does not have to be just a single one. After checking that output, you can move the files like this (first just printing, nothing being actually done): find . -type f | xargs -I@ echo mv @ your-target-dir/. If the output this prints looks good to you, like what you would like to happen, remove the "echo" word you see on that command line to make it actually happen.
Also unsure about iterm2, but I've experienced all sorts of trouble without `checkwinsize` via other software. Especially if you have a tricked out `PS1`. I've even added a note along those lines in my `.bashrc`: # Check the window size after each command and, if necessary, # Update the values of LINES and COLUMNS. # This attempts to correct line-wrapping-over-prompt issues when a window is resized shopt -s checkwinsize Somewhat related: I've come across an issue with older versions of `vi` (e.g. Solaris 9 era and older) that flip out when there's more than 160 columns - giving you a 'terminal too wide' error. So this is an approach to deal with it that complements/overrides `checkwinsize`. Here I deal with columns only. I don't want this potentially donking up my Linux experience, so I limit it to Solaris, and I call `solresize()` everytime there's a `SIGWINCH` (i.e. the terminal has detected a window size change): if [[ "$(uname)" = "SunOS" ]]; then trap solresize SIGWINCH `solresize()` is very simple: if there's more than 80 columns, then set them to 160. I also allow a positional parameter just in case I need to manually call this for whatever reason: # Function to essentially sort out "Terminal Too Wide" issue in vi on Solaris solresize() { if (( "${COLUMNS:-$(tput cols)}" &gt; 80 )); then stty columns "${1:-160}" fi } Voila. Ancient `vi` stops bitching about the terminal being too wide. 
In this case there isn't, but functions are generally the preferable option. There's plenty that's been written about it in the past so I won't type it out again... take this discussion, for example: https://unix.stackexchange.com/questions/30925/in-bash-when-to-alias-when-to-script-and-when-to-write-a-function
Putting a space between '[' and '$var_name' makes it work for me. [ isn't really just a bracket in bash, it's another name to the test command.
I’m typing this on mobile but you could assign a variable like $(/usr/bin/stat -f%Su /dev/console) to pull the active console account but you’ll want to trap it in case something funky happens. 
$var_name ---&gt; "$var_name" 
Thank you! I knew there had to be a better way.
Why are you using a loop if you only want to run a single command?
Look into xargs; I think that will fit your needs.
I do a lotta bash for work, and I can attest to this comment. Spaces in the brackets of if statements that use variables is a pretty big deal in my environment at work. I ran OP's exact script with spaces in the brackets and it worked. Ran it without them and it didn't.
thanks!!! I tinkered around a bit and found a different solution but this one is better. Appreciate the help!
Thank you!!!
Please don’t use `find | xargs`. `find` can run commands by itself: find . -type f -exec mv {} your-target-dir/ \; Or, more efficiently (moves multiple files with a single command): find . -type f -exec mv -t your-target-dir/ {} +
`man udev` RUN{type} [...] This can only be used for very short-running foreground tasks. Running an event process for a long period of time may block all further events for this or a dependent device. Starting daemons or other long-running processes is not appropriate for udev; the forked processes, detached or not, will be unconditionally killed after the event handling has finished. So your script is probably just getting killed immediately. Seems that `udev` is not the way to do this. It will certainly register you plugging in a device, but I guess you'll need some extra machinery to launch a larger program. Maybe a cronjob to see if the associated dev inode exists?
Now that you've solved your problem, please let me mark up some things about `bash` for you. In the `if` statements it is recommended to use `[[` instead of `test` (which is the same as `[`). Write `help [` on the Terminal and it'll show you some help for the `[` and `[[` commands. It is also recommended to wrap variables inside double quotes, because without them, the value of a variable containing spaces would print them and the shell would think that it's a space to separate arguments, instead of counting the space as part of an argument. Also, about `read`, if you're using it in an interactive shell, I'd suggest to add the "-e" option, which makes it nicer for the user (for example, when using the arrow keys to move among the written text). You can also format your script by modifying newlines and indenting. What I'd do if I wrote your code is: echo 'Hello, how are you?' read -ep 'What is your name? ' name if [[ "${name}" == 'John' ]]; then echo "Hello, ${name}." else echo 'You are not welcome here.' fi Keep it up! You're doing great in `bash`!
Sorry for the formatting as I am on mobile, but instead of trying to accomplish this with udev which isn't always the friendliest when try to run gui application in the foreground. Why not have a small process that is always running in the background that can detect the controller and start retroArch. This answer on AskUbuntu.com should get you where you are looking to go. https://askubuntu.com/a/516336.
It's a pretty big deal everywhere, I'd say, which is why it's often said to be a good habit to just quote variables, unless they're guaranteed to be an integer, or you know for sure it's going to be assigned a string with no special characters or spaces. Omitting the quotes can be handy at other times as well, such as when you intentionally want the fields to be handled individually. No doubt you know all this, but perhaps someone else doesn't, so thought I'd chime in.
I recommend spacing those pipes out; this thing a lot of people are doing by cramming as much together as possible really frustrates me; it makes it a lot hardertoreadthecodemuchlikeifIweretotypelikethis! lol
In your case, perhaps not, but in general, yes. If set, bash checks the window size after each command and, if necessary, updates the values of LINES and COLUMNS. I recommend: `man bash`
I second this. I can't think of any real advantage to using `xargs` in a typical `find` scenario; `-exec` with `\;` or `\+`, depending on the situation, is more than enough, not to mention the option to use `-execdir`, `-ok`, and variations thereof.
&gt; You should also get into the habit of always using `"{}"`; this is very important to avoid issues with files or directories whose names contain spaces or special characters That doesn’t make sense – quoting is only a shell concept, `find` doesn’t even know if the command line the shell processed had `"{}"` or `{}`, its argument vector contains `{}` in both cases.
What's the script? Try double quoting the ascii art. 
If it's more than a few lines I'd use a heredoc: cat &lt;&lt;-EOF ÜÜßß±±±±±ÜÜÜÜÜÜÜÜÜÜÜÜ±±±±±±±±±±±±±±±±±±ÛÛÛ Û±±±±±±ÛÛÛ± ±ÛÛ±±±±±±±±±±±±±±±±±±ÛÛ Û²±±±±±ÛÛÛÛ ÛÛÛ±±ÜÜÜÜÜÜ±±±±±±±±±±ÛÛ Û±±±±±±ÛÛ ÛÛÛÛ±± ±ÛÛ±±±±±±±±ßÛÛ ÛÛÛ Û±±±±±±ÛÛ ±ÛÛÛÛÛ² °± Û±±±±±±±±±Û ÛÛÛ±±±ÛÛ ÛÛ±±±±±ÛÛ° ²ÛÛÛÛÛ² ÛÛ ÛÛ±±±±±±±±Û Û±±Û±±±± Û±±±±±±ÛÛ ÛÛÛÛÛÛÛ² ÛÛÛ±°ÛÛÛÛÛ Û±Û±±±±±±±ÛÛÛÛ±±Û±±±± Ü Û±±±±±ÛÛÛ ÛÛÛÛÛÛÛÛÛÛÛÛÛ±ÛÛÛÛÛÛÛ Û±Û±±±±±±±Û±±±±±Û±±±± Û±ÛÜÜÜ Û±±±±±ÛÛÛ° ÛÛÛÛÛÛÛÛÛÛÛÛÛ±±ÛÛÛÛÛ² Û±±±ÛÛÛ±±±±Û±±±±Û±±±±± Û±±±±ÛÜÜÜÛ±±±±±±±ÛÛÛ ÛÛÛÛÛÛÛÛÛÛÛÛÛ±±±±±± Û±±±±±±±ÛÛÛÛ±Û±±±±±±±±±± Û±±±±±±±±ÛÛ±±±±±±ÛÛÛÛ ±ÛÛÛÛ Û±±ÛÛÛÛÛÛÛÛÛ±±±±±±ÛÛÛÛ±±±±Û±ÜÜÜÜÜÜÜÜÜÜ Û±±±±±±±±±±Û±±±±±±±ÛÛÛÛÛ± Û±±±±±±±±±±±±±±±±ÛÛ±±±ÛÛÛ±±±±±ÛÛÛ Û±±±±±±±±±±Û±±±±±±±±±±ÛÛÛÛÛÛÛÛÛÛÛÛ±±±±±±±±±±±±±±±ÛÛÛÛÛÛ±±±ÛÛÛÛ±±±±±ÛÛÛÛÛÛÛÛÛ Û±±±±±±±±±ÛÛÛ±±±±±±±±±±±±±±±±±±±±±±±±ÛÛÛÛÛÛÛÛ ±ÛÛÛ±±±±±ÛÛÛ±±±±±Û±±±±±ÛÛ Û±±±±±±ÛÛÛÛÛ±±±±±±±±±±±±±±±Û± ±Û± ±ÛÛÛ±±±±±ÛÛÛ±±±±±Û±±±±ÛÛÛ Û±±±±Û ÛÛÛ±±±±±±±±±±±±±±Û± ±Û± ±ÛÛÛ±±±±±ÛÛÛ±±±±±Û±±±ÛÛ Û±Û ÛÛÛÛÛÛÛ±±±±±±±Û±±±±±±Û± ±Û± ±ÛÛÛ±±±ÛÛÛÛ±±±±±±Û±±ÛÛÛ Û Û±±±±±±ÛÛ±±±±±±±±±±±±±Û± ±Û± ±ÛÛÛ±±±ÛÛÛ±±±±±±Û±±ÛÛ ÛÛÛ±±±ÛÛÛ±±±±±Û±±±±±±Û± ±Û± ±ÛÛÛÛÛÛÛ±±±±±±±±ÛÛÛÛ ÛÛÛ±ÛÛÛÛ±±±±±Û±±±±±Û± ±Û± ±ÛÛÛ±±±±±±±±±±ÛÛÛÛ ÛÛÛ Û±±±±ÛÛ±±±±ÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛÛ±±±±±±±±±±ÛÛÛÛ ÛÛ±±±±ÛÛÛ±±±±±±±±±ÛÛÛÛ±±±±±±±±±±±±±±±±±±±±±ÛÛÛ ÛÛÛ±±±ÛÛÛÛ±±±±±±±±±±±ßßßßßßßÛ±±±±±±±±±±±ÛÛÛ EOF 
It's a simple bash script that runs top, df -h, last etc. #!/bin/bash " ███████╗██████╗ ██╗ ██╗███████╗██████╗ ██╗ ██╗ ██╔════╝██╔══██╗██║ ██║██╔════╝██╔══██╗╚██╗ ██╔╝ ███████╗██████╔╝██║ ██║███████╗██████╔╝ ╚████╔╝ ╚════██║██╔══██╗╚██╗ ██╔╝╚════██║██╔═══╝ ╚██╔╝ ███████║██║ ██║ ╚████╔╝ ███████║██║ ██║ ╚══════╝╚═╝ ╚═╝ ╚═══╝ ╚══════╝╚═╝ ╚═╝ " echo" echo "===============================================================================" echo -n "Hello "$USER"" echo echo "which server would you like to review?" read var1 ssh -t $var1&lt;&lt;'EOF' touch .hushlogin echo "===============================================================================" echo echo "&gt;&gt;&gt; SYSTEM PERFORMANCE &lt;&lt;&lt;" echo top -b -n1 | head -n 10 echo echo "===============================================================================" echo echo "&gt;&gt;&gt; STORAGE STATISTICS &lt;&lt;&lt;" echo df -h echo echo "===============================================================================" echo echo "&gt;&gt;&gt; USERS CURRENTLY LOGGED IN &lt;&lt;&lt;" echo w echo echo "===============================================================================" echo echo "&gt;&gt;&gt; USERS PREVIOUSLY LOGGED IN &lt;&lt;&lt;" echo lastlog -b 0 -t 100 | head -n 3 EOF 
Example: #!/bin/bash # One of the simplest ways to get it to work without escaping any chars with echo. cat &lt;&lt; "EOF" ____________________ / \ | In case of | | Frustration | \____________________/ ! ! ! ! L_ ! / _)! / /__L _____/ (____) (____) _____ (____) \_(____) ! ! ! ! \__/ EOF 
For new lines, you can use `echo -e`. This will interpret `\n` as new lines. Change your banner text to have `\n`s at the end of each line. Also, check your terminal width, in number of characters ("tput cols" on linux). It is possible your text is wider than the terminal and it is breaking into a new line. Check the number of equals in the echo and make sure that it is less than the width. 
Use a quoted here document: cat &lt;&lt; "EOF" Your ascii art here EOF This way, Bash will leave all your spaces and metacharacters alone.
You're 100% correct, even with the basic scripts I've been writing to learn the concepts, I'm finding it hard to look back easily over them. I downloaded Sublime a few hours ago, it appears to be nudging me in the direction you've stated. Now the only real question is [tabs or spaces ...](https://i.ytimg.com/vi/V7PLxL8jIl8/hqdefault.jpg)
See [the Code Blocks section of the Reddit Markdown Primer](https://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/). It will help you in displaying your ASCII art on Reddit.
That worked, you're the man! 
When I try this "systemctl is-active" thing here, it also shows if something is active through the error status, not just visually as that "active" word. This means you can do the following, no need to compare text: if systemctl --quiet --user is-active servicename ; then ... fi The "--quiet" makes it not print that "(in)active" word to the terminal.
I second the `$(...)` syntax. Also, this is purely subjective, but the double-negation of `!= "inactive"` takes my brain several seconds to grok. I'd rather: if [ "systemctl --user is-active servicename" = "inactive" ]; then systemctl --user start servicename else systemctl --user stop servicename fi
shellcheck is your friend; great tool
You call a function by giving its name, as in read_web_log | awk '{ print $2; }'
You are calling a function from within a function, I just think you are misunderstanding the semantics of the code you've written. The expression: `$(read_web_log)` does indeed call the function `read_web_log()`, but the `$( ... )` semantics will capture the standard output stream of `read_web_log()` and store it into a string, then the expression is replaced with that string and evaluated. So the line of code `$(read_web_log) | awk '{print $2}'` is taking the content of the file at the path in `$WEB_LOG` as a string, and trying to evaluate that string as a command. I think for what you are trying to do, writing your entire script as an AWK program might probably be easier.
lol
The following seems to be working function read_web_log() { # cat $WEB_LOG for line in $WEB_LOG; do echo $line done } function get_IPs() { awk '{print $2}' }&lt;$(read_web_log) 
Thanks. That answers the question!
The biggest advantage of using `&lt;` rather than using `cat` to dump a file into a pipe is that using `cat` requires forking the Bash process twice, once launching the `cat` executable and once again launching the `awk` executable. This is slower and can create a noticeable performance loss if you are looping over a large number of input files. Using `&lt;` requires Bash to fork only once to launch the `awk` process. Bash will open the file handle to the log itself, and then Bash will redirect the open file handle to be used as the input stream of the `awk` when the `awk` process is forked.
Thank you, /u/Ramin_HAL9001 for the detailed explanation. 
A guess: It’s the colon in the filename. 
where?
In the date, between hours and minutes. 
Something like sed -e N -e 's/\n/\t/' | sort -k4 -n # optional: | tr "\t" "\n" Assuming all the uptimes are in days, or at least comparable units, and the hosts have no spaces.
sed -e 's/\nUPTIME/_UPTIME/g' &lt;filename&gt; is this correct? It just turns it into: server1: UPTIME: 1 days Sorry, I did not explain this very clearly. Each HOST and UPTIME pair is seperated by a single new line. So, this would be the format: HOST: server1 UPTIME: 5 days #EMPTY LINE HERE HOST: server 2 UPTIME: 1 days 
Yup. That should do it. Just remember to replace the underscore with a space. 
Try this: sed -e '/^HOST:/ { N; s/\n/\t/ }' This is a rule that triggers for lines that start with "HOST:". The "N" adds the following line to the buffer where the "HOST" line is currently in. The "s///" then replaces the line-break between the two lines that are in the buffer with a tab character. I guess you might also want to add a rule that removes the empty lines: sed -e '/^HOST:/ { N; s/\n/\t/ }; /^$/ d' Then for sorting, the "sort" command has parameters where you can select which field to use to sort. I don't know how things can be made to work if there's a mix of things like "32 min" and "7 hours" and "6 days" and whatnot. If it's always just "x days", that will work fine using the sort command.
NVM I got it
I would say that the best approach is to fix whatever is generating your uptime report in the first place e.g. $ for host in $(&lt;serverlist); do printf '%s\n' "${host} $(ssh $host uptime)"; done [redacted] 09:52:16 up 223 days, 22:23, 0 users, load average: 0.28, 0.18, 0.12 [redacted] 09:52:31 up 251 days, 19:40, 0 users, load average: 0.00, 0.01, 0.00 ^C It's easier to have all your information on the one line where you can simply `sort` by a specific field. Given your example input of: $ cat inputfile HOST: server1 UPTIME: 5 days HOST: server2 UPTIME: 1 days You could try something like: $ awk '/HOST.*$/{printf $0" ";next;}1' inputfile HOST: server1 UPTIME: 5 days HOST: server2 UPTIME: 1 days Then you can just pipe to something like `sort -nk4`
Seconding shellcheck, absolutely a life saver when you're learning the peculiarities of shell scripting.
awesome thanks!
thanks for all your help guys
You might like to update to bash 4 anyway. Otherwise, did your [preferred search engine]-ing bring up this link? https://superuser.com/questions/144683/bash-auto-complete-adds-trailing-slash-after-filename
here's the code I have bDate=$(date) isoLocation="$HOME/desktop/" echo 'Drag and drop backup now' read backupLocation mkisofs -o "$isoLocation" -JR "$backupLocation" cdrecord -v dev=1,0 "$isoLocation" error is always "No such file or directory. Invalid node - "
Exec 2&gt;/dev/null # don't display any error messages
When you try to work against the system like this, everything becomes difficult and fragile. Have you considered just reading filenames from stdin instead of history? 
Can you post your code?
This is my gut reaction as well. Instead of suppressing errors, the script should delay parsing the history file. Perhaps saving history into a temp file, and setting a trap to delete that file on exit. Once the history is in a temp file it will be possible to run that file through sed, grep or perl to transfor it into something that will not cause errors.
Thanks! 
add that to the .bashrc file, although I wonder why there is no PATH at all it seems.
Without adding the PATH, I get: `:/Users/MyMacBook/ .local/bin`
Looks like that's your problem, somewhere along your way you've overwritten the original PATH. Wherever you've done that, you'll want to change it to &gt;PATH=$PATH:/Users/MyMacbook/.local/bin 
:) Personally, I find tabs easier to deal with, and converting them to spaces if the need should arise is fairly trivial. Are you on GitHub at all? Happy to help where and when I can.
I *think* I've found it in /etc/profile where: `PATH-"/bin:/sbin:/usr/bin:/usr/sbin"` is the first line? It's possible I've added something similar to several files during previous attempts to fix ls.
Yet, graves as command substitution work just fine; at times, I find them cleaner, depending on where I use them. I only use the other method if the need arises (such as for nesting), otherwise, no real issue that I've ever observed. I don't use shellcheck; never saw the point in it, as it always seemed like an unnecessary and at times silly crutch to me. Manuals and common sense tell you the correct or conventional syntax, and observing other people's approaches gives you a good idea of what works well outside of your own methods. Besides, people have their own quirks which help them to be productive or efficient, much as in the way of writing regular language. It's what works for me, at least.
Or, my preference, `PATH+="/Users/MyMacbook/.local/bin` unless that's for some reason different on a Mac.
See my reply to the top comment. (sorry im on a mobile app and don't know how to link it) 
This is more or less what the script does. The problem is that bash is trying to execute the input before i can parse the file history, which in turn stops execution. I have a mostly functional solution (see my reply to this comments parent comment). This comments put the input so it is never executed. If the command requires additional function calls t can execute these after you read the history to get the literal input. 
How different is zsh than bash
If this format suits you, this is what I came up with: FILE="Your filename here, of course!" printf "%-3s %s\n" "DAYS" "HOST" while read X; do if [ "${X:0:4}" == "HOST" ]; then H="${X#*:}" elif [[ "$X" =~ [0-9]+ ]]; then D="${X//[!0-9]}" printf "%4d %s\n" "$D" "$H fi done &lt; "$FILE" | sort -nk 1 Assumptions: 1. Server names must never contain spaces (assuming you made a typo). 2. The data formatting is **consistent**. 3. You're able to put this in a little script or something, for reuse. 4. Linux. I've no idea if this will work on similar systems, like Mac or BSD. 5. You're okay with semi-pretty output. (easy to modify for 'raw' output) Hope that helps at all.
I only used it a little bit. Just looking at it as a user and working on the command line, it seems pretty similar to bash. It's not crazy different like "fish" is. I mean, doing something like the following on the command line works the same in both bash and zsh: for x in *.txt; do some command here using "$x"; done In scripts you would still use bash like normal. You'd use zsh just when at the command line.
You can also use `&lt;&lt;- "EOF"` to have this heredoc ignore tabs, meaning you can format it per any indenting standard (assuming tabs of course), without messing up the code.
The first thing I see is that the shebang should be: `#!/bin/bash`
Yes. `exec 2&gt; /dev/null`
Doh, typo on the shebang, good catch. Didn't know about the spacing either :) 
Strongly recommend just using bash 4; it's really good!
Thanks for your well thought out reply. You motivated me to look in to the topic further as previously I'd just blindly accepted the shellcheck info and the info from bash hackers wiki and you're absolutely write. My personal opinion is unchanged but I can respect your justification for continued use. This is the best explanation I found and which I believe we can both agree on: https://unix.stackexchange.com/a/126928/283372
ShellCheck points out common issues like that
Yeah, I wouldn't mind getting into some basic Python. I'm more comfortable in ruby but (despite asking for help) it doesn't seem overly complicated :) Do you remember the rough outline of how the script looked? I've not used multithread before so I'm a bit in the dark. And thanks for the response!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
So many things... But to your implied question, look at the path of the actual file you create from $2 Hint, it's not what you're supplying to ls
Thank you for declaring that this is homework. It's usually pretty easy to tell anyways so I appreciate the honesty. if [ "$1" = "" ]; then if [ "$2" = "" ]; then echo "Usage: backup oldfilename newfilename" fi fi Would only print the usage if both parameters are missing when I think you want it to print when either is missing. Also, you usually want to do an immediate exit after printing the usage text. if [ "$1" != "" ]; then touch $1 if [ "$2" != "" ]; then touch $2 fi fi I'm not sure what the intent of this block is. If it's the create the file (using `touch`) if it doesn't exist, then the conditional expressions are wrong. The way it's written if the file exists the modification time of that file will be updated to current time. Not sure if that's intentional. Plus if you do the exit if either of these are null then you don't need either if statements here. if [ -e $1 ]; then cp $1 ~/$2.bak echo "$1 has been backed up to $2.bak" echo "Here is a long listing of the two files:" ls -l $1 $2 fi Ok you kind of lost me here. Do you actually want to check to see if both "$1" and "${1}.bak" exists or do you want the user to specify an arbitrary second parameter that then gets the .bak added to the name. The way I read the initial problem statement, it sounded like the former, meaning "$1" and "${1}.bak" . You should put `$1` in double quotes. Again, the flow here works but is a bit awkward and unnecessarily interdependent. The outer `if` would be unnecessary if you exit when either "$1" or "$2" is empty/null.
&gt; if [ "$1" = "" ]; then &gt; if [ "$2" = "" ]; then &gt; echo "Usage: backup oldfilename newfilename" &gt; fi &gt; fi &gt; &gt; Would only print the usage if both parameters are missing when I think you want it to print when either is missing. Also, you usually want to do an immediate exit after printing the usage text. Okay, so the echo statement should effectively be the last option before closing out the conditional statement? This part, and others, confused me. I am trying to that if there is no user input (ie. backup.sh $1 $2) then echo the statement reflecting the method of usage for the script. &gt;if [ "$1" != "" ]; then &gt; touch $1 &gt; if [ "$2" != "" ]; then &gt; touch $2 &gt; fi &gt;fi &gt; &gt;I'm not sure what the intent of this block is. If it's the create the file (using &gt;touch) if it doesn't exist, then the conditional expressions are wrong. The way it's &gt;written if the file exists the modification time of that file will be updated to &gt;current time. Not sure if that's intentional. Plus if you do the exit if either of these &gt;are null then you don't need either if statements here. Nor am I, honestly. I didn't even think about the possibility of the files already existing and therefor simply being "touched" and updated. I'm all confused on this one...and oddly, it checks out as correct when I run the homework grading program on it. Although, I think that when our professor picks the code to death, that may very well change for the worse... 
Thanks for the advice, I'll take a look at those articles!
Could also do some xargs magic as well as this. Then fixing the names wouldn’t be as tedious 
First, instead of cd'ing to the folder with the script first you can also call the script directly (e.g. "/root/etc/scripts/script1"). Well, now to the thing. There are several ways to pipe data between processes in `bash`. I'll write the relevant ones to this matter. By running `cmd1 | cmd2` you execute "cmd1" and make its standard output be the standard input of "cmd2" (also executing it). By running `cmd1 &gt; cmd.txt` you run "cmd1" and redirect its output to the file "cmd.txt", overwriting its original contents. To simply append the new data to the old, use `cmd1 &gt;&gt; cmd.txt`. I hope that solves your second question. If you want the output to also be written, you should pipe the info through "tee" like this: `cmd1 | tee cmd.txt`. By running `cmd1 &lt; cmd.txt` you send the contents of the file "cmd.txt" as the standard input of "cmd1". By running `cmd1 &lt;&lt; EOF` you let "cmd1" listen to the following lines until there is a line with only "EOF" on it, then uses the text as standard input. By running `cmd1 &lt;&lt;&lt; cmd`, `cmd1 &lt;&lt;&lt; "cmd"`, or `cmd1 &lt;&lt;&lt; 'cmd'`, you run the program "cmd1" with the string "cmd" as its standard input. I hope that solves your first question. Good luck!
Thank you so much! This gave me a lot of good ideas. For my purpose, I think I will do: 1. launch the script from its explicit path 2. use the cmd1 &lt; cmd.txt method to initiate the bash script. All the best to you!
`json` is JavaScript Object Notation, a data format. something like this would probably work: import StringIO input = StringIO.StringIO() output = StringIO.StringIO() input.write('run') subprocess.call(['/root/etc/scripts/script1', '-i'], stdin=input, stdout=output, shell=True) It's been a while since I've done this but something like that should work. Also, this is the bash subreddit, best place to ask would be the python subreddit. 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
I have an alias that opens .bashrc in an editor (nano in my case), and runs source on it after you close your editor. Super convenient. alias bashrc='nano ~/.bashrc &amp;&amp; source ~/.bashrc' I like using nano but of course you can change it to vi, pico, etc and include sudo if you wish. You(whetu) probably already knows this but just saying for anybody else reading who may not. 
Your doing this all wrong. Look into Ansible and life will be much better.
If you enable the "globstar" feature: shopt -s globstar You can write this: for i in **/*.avi; do ...; done Bash will recursively hunt through all sub-folders when it sees `**/` in a wildcard pattern. That `shopt -s globstar` command would go into your .bashrc to enable it permanently.
Thank you for the reply. This works well, until the user changes a value on their user.py file. For example, if they were to DEBUG to equal True, the output ends up being: [...] # Debug settings - only enable True for debugging issues # Default = False DEBUG = True DEBUG = False # Enable auto-checking if an update is available on GitHub # If set to True, but unable to access the internet, the default behaviour will not display anything # If set to False, no outbound internet checks will occur # Default = True CHECK_FOR_UDPATES = True 
I must've misspoken on the output. My output is exactly what you're seeing in the test case. Thanks for the suggestions though. I think the requirements of each setting being on identical lines can raise issues later. I ended up getting a response overnight on StackOverflow, here: https://stackoverflow.com/questions/49659719/compare-2-files-line-by-line-ignoring-the-last-word-when-comparing-and-extract Someone was able to write a one-liner using awk that either omitted all blank lines, or preserved them but just omitted any new comments.
A few pointers to 'clean up' your script; (ba)sh has built-in string tests. To test for an empty string, instead of doing [ "$1" == "" ], you can write [ -z "$1" ] or for a non-empty string, write [ -n "$1" ] Double quote the variables to expand them before the test runs. Writing [ -n $1 ] will always evaluate to true because it will test for the literal string '$1', rather than expanding the variable and testing for the value. Also, testing if either $1 or $2 exists can be written as such; if [[ -z "$1" ]] || [[ -z "$2" ]]; then echo "Usage: backup oldfilename newfilename" fi with &amp;&amp; meaning 'AND' and || meaning 'OR' This whole section confused me too, as someone else mentioned, because you're testing if $2 is empty ONLY if $1 is empty. But, logically, if $1 is empty, doesn't that make $2 automatically empty? (or, $2 would become $1 and it would not test for $2 being empty) You can rewrite the 'touch' part of your script the same way, but testing for -n "$1" Also, as someone mentioned, do you want the user to even specify a $2 (newfilename?) And should that be a full path, or just a name? Where does that file need to end up? The way you've written it now, $1 needs to be a full file path, whereas $2 just needs to be some random text which will be the new filename. Personally, I'm a fan of having my backups have the exact same name as my input file, except with the .bak addendum. (So, ${1}.bak would be sufficient, while also just needing one input from your user, keeping the filename intact, and they can 'drag' a file into the terminal if you're on a GUI) if [ -e "$1" ]; then echo "Backing up file $1 to ${1}.bak" cp "$1" "${1}.bak" echo "File backed up. Here is a long listing of the two files:" ls -l "$1" "${1}.bak" else echo 'No file found at "$1", exiting' exit 1 fi exit 0 I hope some of this helps with your homework, and understanding shell scripting. There are a million ways to script this (and to improve on the script once it works as intended), so there's always room for tinkering. :)
I am so thankful for all of you. It’s inspiring me to want to pick this scriptlet up and see what I can build it in to. Thanks!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
This just makes me realize I need to up my `awk` game. You can change the post flair to solved now if you're not looking for anymore help. 
Can you show me how to generate a dummy key file to test against?
Thank you, RFC-1925, for voting on AutoModerator. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Me too, I can't believe it was a one-liner. Flair changed, thanks.
I came up with this here: perl -n0777E 'for (unpack "C*") { exit 1 if ($_ &lt; 32 or $_ &gt; 126) and $_ != 10 and $_ != 13; }' This reads the whole file into memory, so will only work well if it is relatively small. It converts the bytes of the file into a stream of decimal numbers and checks their values. I looked at this document here to decide what numbers to check: https://www.cs.cmu.edu/~pattis/15-1XX/common/handouts/ascii.html I chose the range from 32 to 126, then also check for the new-line character which is number 10 and allow that as well. I also added the "carriage return" = 13 as well as that's used on Windows for line endings. What you might perhaps want to add as well is the "tab" character which would be the number 9. The thing quits immediately after the first byte that does not fit those rules. It prints nothing and reports about its success through its exit code. If it's `$? -eq 0`, then the file has only readable characters, and if it's `$? -eq 1`, then the file has garbage in it. On that note, you do not have to check the value of `$?` manually like this: some command here if [[ $? -eq 0 ]]; then echo "success!" else echo "failure." fi You can write it like this instead: if some command here; then echo "success!" else echo "failure." fi The `if` will run the command and check `$?` for you.
What's with the xargs use? Can't `kill` take more than one process? Like kill -STOP -- "${pid_arr[@]}" Also, `kill -9` seems a bit heavy-handed: https://mywiki.wooledge.org/ProcessManagement#I.27m_trying_to_kill_-9_my_job_but_blah_blah_blah... Prefer `[[` to `[`. `$ff_arr` seems a little unnecessary. Why not pipe `ps` directly? ps ax | awk -v skip=$((tabs_to_skip + 1)) \ '/firefox/ { seen += 1; if (seen &gt; skip) { print $1; }' That would save you some subshells and some grep | awk nonsense. Of course, all of this is ignoring the inherent fragility of grepping ps to find out what processes are running.
I think OP means *[Electrolysis](https://wiki.mozilla.org/Electrolysis)* (multi-process Firefox).
Well, I've confused Quantum and Electron. not sure how Electrolysis relates to that
&gt; What's with ... Because I have odd gaps in my knowledge. Replies like this are why I post here. thanks :) 
The question is still "how"? I mean, my requirements are: A function/alias/something, called `getliteral` that, when called as a subfunction of `foo`, will output the literal unevaluated input given to `foo` but doesn't require any changes to how `foo argIn1 argIn2 ... argInN` is called (since otherwise i mine qs well just manually quote it). Ideally this will also work when directly calling `getliteral` from the command line, but that is less important. The best way ive figured out how to do this is to throw away the actual input and use what was recorded in the history with the extra recorded stuff filtered out. Running and redirecting the errors is less than ideal... Id rather throw away the original command entirely after it is recorded in the history, but again the question is how. If you know how to make this happen, please do share. Im not saying my way is the best way, but it is the only way i know of that seems like it could work. 
I just guessed the closest-sounding thing related to Firefox… [Electron](https://www.wikidata.org/wiki/Special:GoToLinkedPage/enwiki/Q21614124) is something very different AFAIK. But I guess Quantum also makes sense :)
Depends on your terminal, but see here for [examples](https://misc.flogisoft.com/bash/tip_colors_and_formatting).
Heh. I've been alllll over that page. I'm using bash. How would I format that one line to have the name of the file appear in yellow?
You'll have to capture the output from running ls and then echo it with the color code ahead of it.
But if you are just using ls to dump out the name of a file just put the color coding before an echo of the $1
if you have to use ls -l, there's your problem. i think you're going to have to explore why --color isn't working for you.
This should do the trick: COLOR_FG_YELLOW="$(tput setaf 3)" COLOR_RESET="$(tput sgr0)" ls -l $1 | sed -E "2,\$s/(\S*)$/$COLOR_FG_YELLOW\1$COLOR_RESET/" 
Ill have to look into this more, but im not sure this ends up working with how I actually use this function in my scripts. For example, in a script designed to make a `cd` function that works with windows file/folder paths, it is used roughly like like: mycd() local cdPath="" cdPath="$(getLiteralInput)" if (( $(checkForWindowsStylePaths "$cdPath") == 1 )); then cdPath="$(convertPath_dos2unix "$cdPath")" fi cd "$cdPath" } This is probably workable here with a bit of re-aliasing the function names, but to be perfectly honest a Id kind of rather have a hacky approach that works 99% of the time and can be incorporated with a single line than the "right way" that always works but requires re-alising everything in the entire function stack I want to use it with. Like I said though ill have to try it out. Maybe there is a more straightforward way to implement this for my use case that im not seeing.
&gt;If you can figure out how to get a tab pid &lt; the main firefox process, let me know. I wrote a shell-wrapper script for firefox, opera and chromium-browser a long time ago (before firefox Quantum): inpath firefox opera chromium-browser Directory '/home/common/bin/': 'chromium-browser' 'firefox' -&gt; 'chromium-browser' 'opera' -&gt; 'chromium-browser' Directory '/usr/bin/': 'chromium-browser' 'firefox' -&gt; '/usr/lib/firefox/firefox.sh' 'opera' -&gt; '/usr/lib/x86_64-linux-gnu/opera/opera' The script prints the main browser process' pid with option '-sa'. It seems to work [all right with Quantum](https://i.imgur.com/BiqsjrL.png). I can post the script if you are interested. 
Hrmm. That didn’t seem to do it. Keeping in mind I am a COMPLETE newb with scripting, how would I insert that into my existing code?
Is it possible to use find and maybe cut to show the info?
Did you try using `ls --color -l` instead of `ls -l`? Is there a reason why you do not want to use that, and instead want to add the colors yourself? Is it not working?
I think maybe what you want is for your script to run ls -l --color=auto $1 . At a command prompt run $ which ls The output might be show that ls is aliased to something.
Ok, Tech Giants, we get it. You think particle physics is neat. Now find something else to name your projects after. 
That would be great! Would you mind using a GPL compatible license?
&gt;Would you mind using a GPL compatible license? It is just a simple [bash script](https://pastebin.com/KDqRScrR). It has nothing to do with freeing resources. You can probably figure out how finding 'the main firefox process' works just by reading it - Check 'ServiceAvailable()'. If you want to test it you'll have to drop the script in a PATH directory 'before' '/usr/bin' and create the links as is shown in my previous post. 
You could put it all in a while loop to repeat until you exit the loop. When you select yes/no you continue to the next question and if you press the wrong button it'll ask the question again. Also this might help: http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_09_05.html
So basically just put the code above into a while loop?
Are you receiving an error? If so, what is the error? Otherwise, when the script runs, what result are you receiving ? 
Yeah, something like a while x &gt; 1do everything you have at the moment then once everything complete x+1 so it terminates 
I don't get an error. It runs, asks the questions "DO you want to install tyminator? &lt;y/n&gt; If I put y, it will install it and work fine. If I put n it exits the script entire(Which I don't want it to do, I want it to continue the script) If I put anything other than y or n it will says "Please input y or n" then exit the script(Which I don't want it to do,I want it to start back at the beginning where it asks "Do you want to install Terminator? &lt;y/n&gt;"
&gt; isoLocation="$HOME/desktop/" I'm pretty sure it's because Linux's desktop path is "$HOME/Desktop/", with a capital "D". I'll try doing the same thing though.
Oh, I see. Yea you can do a while loop or put it into a function and call it recursively. Someone already wrote about the loop, so here's the function example: test.sh #!/bin/sh #define your function here testfunc(){ echo "enter something" read something echo "something was $something" #call the function again to loop testfunc } #call function testfunc 
exit 0 is going to exit your script with a no error return code. It sounds like you don't want that. I'm surprised no one has mentioned that yet. The loop is the right way to go, but you need to get rid of that exit command. It sounds like, based on what you're saying it does, that your syntax is fine, and it's your logic that is the issue. The following is psuedo code, but it's an attempt at showing you what you want. $valid = 0 while valid == 0 read -p "Do you want to install terminator? &lt;y/n&gt; " prompt if [[ $prompt == "y" || $prompt == "Y" ]]; then sudo apt-get install terminator $valid = 1 elif [[ $prompt == "n" || $prompt == "N" ]]; then $valid = 1 else echo "Please input y or n" fi end while #Rest of script here
Yeah, what im using is an example. Sorry.
When trying this, with my 2ndl ine having $X = 0 I get this error when trying to run the code line 2: =: command not found
No problem, I got it now. My code is above in the first post.
Looks like you have a solution, so I’m not going to answer your question directly, but offer a “style” suggestion :) When I deal with user input in a bash script, I usually employ a “case / esac” block as I find it easier to read, and allows for simpler logic paths for “unknown” input. So here’s an excerpt from a script that prompts for a single y/n response (ie, it continues after reading 1 character): read -n 1 -r -p "Continue (Y/n)? " myCont case $myCont in Y|y) # Fall through - expected case echo -e "\n\nContinuing with the upgrade, as requested.\n" ;; *) # Anything else...abort echo "Bailing as out, as requested." exit 4 ;; esac The great thing about “case” is each option can be constructed as a regex so ‘Y|y’ means exactly what it looks like, but if we wanted something a little more elaborate you could also use ‘[Yy][Ee][Ss]’ and drop the ‘-n 1’ in the read statement. This is a fairly contrived example, but I’m sure you can see how useful this construct can be for evaluation multi options based on user input. Also, you might want to get into the habit of using ‘read -r ...’ as this prevents using backslash to escape characters which is a common way to break scripts.
I fancied up your script for you. * It now accepts [Y,y,N,n,Yes,No,YeS .. etc] * It now pulls packages from a list, so if you are installing more than one package, you can just add it to the list instead of repeating code * It now has color. (Yay color!) TEXT_DEFAULT="\e[0m" BOLD="\e[1m" NO_BOLD="\e[21m" RED="\e[31m" GREEN="\e[32m" ORANGE="\e[38;5;202m" WARNING_FG=${ORANGE} ERROR_FG=${RED} #Add more packages to list to install more. PACKAGES=( terminator myFavoritePackage ) YES_PATTERN="^[y](es)?$" NO_PATTERN="^[n](o)?$" for PACKAGE in "${PACKAGES[@]}"; do prompt="" while ! [[ ${prompt,,} =~ ${YES_PATTERN}|${NO_PATTERN} ]]; do read -p "Do you want to install ${PACKAGE}? &lt;y/n&gt; " prompt if [[ ${prompt,,} =~ ${YES_PATTERN} ]]; then echo -e "Installing ${BOLD}${PACKAGE}${NO_BOLD} ..." sudo apt-get install ${PACKAGE} -y &amp;&gt;/dev/null if [ ${?} -eq 0 ]; then echo -e "${GREEN}${BOLD}${PACKAGE}${NO_BOLD} installed${TEXT_DEFAULT}" else echo -e "${ERROR_FG}Failed to install ${BOLD}${PACKAGE}${NO_BOLD} :(${TEXT_DEFAULT}" fi elif ! [[ ${prompt,,} =~ ${NO_PATTERN} ]]; then echo -e "${WARNING_FG}Please input y or n${TEXT_DEFAULT}" fi done done 
I'll try this and see how I like it. Thanks. :) Most of them I want to install, it's just one MAYBE 2 of them that I might not need right away or anything.
Now wondering if anyone can help me understand the wget command. I get it but not sure how to get the correct link. I'm trying to download [Atom](https://atom.io/) the .deb file. I can't seem to get the right link, when I use wget -O https://atom.io/download/deb as the link seems to link to but I don't get a proper .deb file.
No this isn't for school, this is for my own use. Trying to get a feel for bash a bit. I feel like I should move this over to python since that's what I wanna learn more but right now this is just something to kinda help me get started and motivated.
The problem you're always going to have here is that you're parsing the output of `ls`. One of the golden rules of shell scripting is that you do not parse the output of `ls`. Consider using `stat` instead, but do be aware of the difference between BSD `stat` and GNU `stat`. Something like: stat -c "%A %U %G %s" "$1" Which will (assuming GNU `stat`) output the permissions, ownership and size. So we can build on that e.g. # var="$HOME/.bashrc" # printf '%s %s\n' "$(stat -c '%A %U %G %s' ${var})" "${var}" -rw-r--r-- root root 4000 /root/.bashrc Cool... cool... now we add colour to the mix: # printf '%s \e[31m%s\e[0m\n' "$(stat -c '%A %U %G %s' ${var})" "${var}" -rw-r--r-- root root 4000 /root/.bashrc Obviously it won't show up in reddit, but the filename appears in red. Obviously adjust for BSD `stat` if that's what you're using, and/or if your script ever has the possibility of using it. Alternatives are demonstrated here: https://unix.stackexchange.com/questions/77173/bash-colorize-second-column-of-output --- I get the impression here though that what you're wanting to do is to show some sort of comparison of the files? Assuming the source file isn't being written to, you might find it easier to just checksum both files e.g. # cp .bashrc /tmp/ # sha256sum .bashrc /tmp/.bashrc edfc52f87eaf2d9e976e62828b6f4da834fba643a4a69cb209c32fc342b6745b .bashrc edfc52f87eaf2d9e976e62828b6f4da834fba643a4a69cb209c32fc342b6745b /tmp/.bashrc 
That was an interesting read, thanks! I never nest graves; that's something which would drive anybody nuts trying to read. lol An example of when I prefer graves: NAME=`command substitution` In that case, I'm gaining nothing by using `$()`, and I feel like \```\` is cleaner.
Useful tool. Thanks for sharing. 
Look st the source code. They describe how to decrypt in the comments. https://github.com/Multibit-Legacy/multibit/blob/master/src/main/java/org/multibit/crypto/KeyCrypterOpenSSL.java If you encrypt a string with this class you can decrypt it with the OpenSSL command: openssl enc -d -aes-256-cbc -a -in cipher.txt -out plain.txt -pass pass:aTestPassword To generate the password I would use crunch. Use diff to check the validity of the decrypted file. Stick all that in a while loop and you're good to go.
I considered this, but usb drives don't show up in fstab, at least on my system. Also, I've run this to completion exactly once since posting and it didn't work. I've since acquired a second usb drive, so I'll actually be able to run it all the way through more than once a week. Anyway- thanks for taking a look. I'm always open to PRs! 
I wrote a script that can perform a dictionary attack on your encrypted private key: https://github.com/unawarecitizen/BTCRecover-Multibit
You can just pass multiple url's at once to youtube-dl and it will download them back to back. So the issue is then how do you fetch the URL's you wish to download? The most optimal way would be to use your scripting language of choice and hook into the twitch api to fetch what is needed. The relevant documentation for it being located here. https://dev.twitch.tv/docs/api/reference/#get-videos 
This here seems to work: youtube-dl 'https://www.twitch.tv/pgl_dota2/videos/all' --playlist-end 5 I just tried starting it but cancelled immediately so I don't know for sure if it will work. It had start downloading the latest video of that user "pgl_dota2". I also tried the following which made me think that it will work correctly: youtube-dl --flat-playlist 'https://www.twitch.tv/pgl_dota2/videos/all' It seemed to me that it treats the "Videos" listing page on the Twitch website like a playlist, hopefully making that "--playlist-end 5" parameter work like you'd expect. About the rest of what you want to do, you should probably have separate directories for each user, at least for the download. You could use the file search feature of the file browser you like to use to see all videos from all folders on one screen sorted by date. If you don't like this, you could try to do some work-around where you work inside a directory for each user but later move everything over to the main directory and give it a name with the user added to it, so something like: username - original filename here.mp4 To update the last five videos of someone in a directory, I think you could start out by just letting youtube-dl run inside that directory and do its thing. I'm not sure, but I think it will not download a video again if it finds that it already exists on disk. Then in a next step, delete everything besides the five newest files. In your script, you could also try to get more control over everything that's happening by going through a sequence like this: youtube-dl --playlist-start 1 --playlist-end 1 ... youtube-dl --playlist-start 2 --playlist-end 2 ... youtube-dl --playlist-start 3 --playlist-end 3 ... Maybe there's a way to check if something already exists before downloading each file. Your script could then choose what to do instead of just letting youtube-dl do its thing.
`youtube-dl 'https://www.twitch.tv/…/videos/all' --max-downloads 5` should give you the newest 5 videos. To weed out old downloads you could use the video id. As far as I can tell *all* videos on twitch have an increasing number as id so you can save the files with `-o '%(id)s_%(title)s.%(ext)s'` for example. This gives file names like this: `v245123456_Title_Of_Video.mp4`. You can then delete all files except the five with the highest id.
Thanks, both of you
If you're ever having trouble with aliases, stop and use a function instead: log () { git log --pretty=format:"%h - %an, %ar : %s" } This requires no escaping or extra quoting, just put in the command you want and use it like you would an alias.
You're missing a trailing `"$@"` for the full alias replacement experience. ;-)
The rules for using `(( ))` to do tests are the same as in the C programming language. If the result of a calculation is zero, this means "false". If the result is not zero, it means "true". You can experiment with ideas at the command line. You do not have to edit and run a script all the time. You can do for example things like this: $ x=14 $ echo $(( x % 7 )) 0 $ if (( x % 7 )); then echo true; else echo false; fi false 
Your for loop was throwing firstnum on the floor and resetting it to zero. #!/bin/bash echo -n "Enter first number:" read firstnum echo -n "Enter second number:" read secondnum for ((counter = $firstnum; counter &lt;= secondnum; counter +=2)); do if (( counter % 7 )) ; then echo "$counter" else echo "$counter is divisible by 7" fi done
Thanks for your quick and useful response. The only issue I have now is the solution is printing odd numbers instead even numbers. How do do you think I can solve this?
I don't really understand your question. The code is asking for a number to start counting from. It then counts up in increments of 2 (rather than 1). If your starting number is odd, then all the numbers it prints will be odd. If your starting number is even, all the numbers if prints will be even. I hope this helps with your homework assignment. ;-)
Thank you very much, you have been very helpful.
Sorry, I have one final follow up question if thats okay? How do I go about adding multiple if statements in the same script for instance if I wanted to find other numbers that were divisible in the sequence. This is my solution however it generates syntax errors. #!/bin/bash echo -n "Enter first number:" read firstnum echo -n "Enter second number:" read secondnum for ((counter = $firstnum; counter &lt;= secondnum; counter +=2)); do if (( counter % 7 )) ; then echo "$counter" else echo "$counter is divisible by 7" elif (( counter % 10 )) ; then echo "$counter" else echo "$counter is divisible by 10" elif (( counter % 13 )) echo "$counter" else echo "$counter is divisible by 13" fi done 
https://ryanstutorials.net/bash-scripting-tutorial/bash-if-statements.php if SOMETHING ; then # do something elif SOMETHING ; then # do something elif SOMETHING ; then # do something else ; then # do something fi
You can't do `elif` after you have already done `else`. Do multiple `if`, like this: if ... then ... else ... fi if ... then ... else ... fi
Is it possible to do this and have the number in the sequence printed once? When I carry this out the numbers are repeated three times. i.e. 2 2 2 4 4 4 etc.
echo "You're insane, and I love it."
&gt; printf '%s\n' "You’re insane, and I love it." Please, we’re not savages here.
Next step : ncurses? :)
Thank you so much, I reallly appreciate the information you and FewerNotLessDotCom have provided.
date '+%Y-%m-%d' as shown [here](https://ss64.com/bash/date.html)
`man date` https://linuxconfig.org/date-1-manual-page
I cancelled pia because of disconnects and openvpn. Switched providers and it almost never disconnects.
 date -I
&gt; The answer says what this line does is "get the command line argument and check for integer by addition," but I don't understand what this means. Someone could also call the script with `foo` or whatever as the first argument. In that case, the expression `$(( 0 + ${1:-64000} ))` will fail, since `foo` cannot be converted to an integer, and the `exit 1` will run. Without this check, the script might blow up in unexpected and fatal ways later when `$1` (or `NUM_TO_KEEP`) is expected to be a valid integer but isn’t. &gt; In the regex `[^-][^-]*-[0-9][0-9]*-[0-9][0-9]*.tar.bz2`, why is the very first `[-]` necessary if it's followed by `[^-]*`? Because `[^-]*` matches zero or more not-dashes and would therefore allow `-some_digits-some_digits.tar.bz2` (without any prefix). A more straightforward way to prevent this would be to use `[^-]\+` instead of `[^-][^-]*` – both mean *one* or more not-dashes. (A peculiarity of “basic regular expressions” is that `*` shouldn’t be escaped to obtain its special meaning but `+` should – it’s possible that the author tried to use `[^-]+`, found that it didn’t work, and then worked around this using `[^-][^-]*` because they weren’t aware that they needed to use `\+`.) &gt; What does `sed 's/-.*//; s,^\./,,'` do exactly? Checking the man page doesn't help too much since `sed` has its own syntax. I recommend you spend a bit of time reading the `sed` manual (`info sed`; it’s not overly long, and you can stop before you get to the more obscure sections); IMHO it’s fairly well-written and will let you get a lot more use out of `sed`. That said, here’s what the command you quote does: first remove everything after the first hyphen (`s/-.*//` – **s**ubstitute it with an empty replacement), then remove a leading `./` (`s,^\./,,/`). The `;` separates multiple commands, and the first letter (here, **s** in both cases) specifies the command in question. The separator after **s** can be any character – the forward slash **/** is a common choice, but if the pattern or replacement contain slashes themselves, some people like to use a different separator to avoid having to escape the slash in the pattern and replacement; in this case, the separator for the second **s** command is the comma **,**.
Do you hate having to sign into sites or clicking that "subscribe" button just so the service has an excuse to send you junk email? Me too (if you answered yes...god help you). This script uses curl and rsstail to help you "keep up" without needing any accounts. It's mainly for Linux, but I see no reason why a UNIX/MacOS system would be able to run most of it. If you cannot install rsstail for some reason, the script will skip the parts that require it and just use the ones that need curl, which most have by default. I also tried to make it as easy as possible to edit the lists (inside script) and have on/off switches for social sites you might not care about. The included lists are just to help you understand how it works and to get you started.
shouldn't you use $1 in the basename and mv part? 
The tilde `~` expands to your home directory, usually something like `/home/yourusername`. Instead of it, you can also use `$HOME` (or, better, `"$HOME"` to prevent word splitting): `"$HOME"/.bash_history`. Tilde is not expanded everywhere, and especially not within quotes, so `$HOME` might be more widely applicable. The location of the bash history file can be configured in the `HISTFILE` environment variable, which defaults to `~/.bash_history`.
&gt; $ date '+%s' --date='2018-04-08' this only works for gnu bash, not on bsd
Why not just post a direct link to github and embed a video/demo in the README? The video autoplays and the site is cluttered and feels like I am on dailymotion. Neither of which one expects, when one is looking for a description/source code of an application. 
It's a JS thing that replaces the src of an img with a larger image if the img has large dementions.
`for loop` with a `basename` copy and `convert` from the `imagemagick` package.
Thank you. Is there a way to resize it to 1.5 Mpx or resize by half.
You should be able to resize by half. [Here](https://www.smashingmagazine.com/2015/06/efficient-image-resizing-with-imagemagick/) is a pretty good article going into **way** more detail than I have ever had to do. I would help you more, but I haven't used convert in a couple years and I would have to relearn the settings I used. 
I think it auto plays because of the way it uses web browsers to seed because the bandwidth is shared between current viewers like a torrent. Because of this, there is no embed code to post. You can grab a magnet link and even get the .mp4 URL(see links below); however, that MP4 URL never stays the same. And, I would have just placed an exact copy of the video description in a README anyway. YouTube-dl needs to support BitChute, but they currently don't. I made two scripts because of this: https://github.com/TheOuterLinux/BitChuteNotes/blob/master/Command-Line/bitchute-play and https://github.com/TheOuterLinux/BitChuteNotes/blob/master/Command-Line/bitchute-dl. One plays BitChute videos using mpv and the other one downloads them. 
I might actually just go back through my videos and include a short script to copy in paste for w3m and TTY users. 
Thanks
No problem. Wish I could have offered more.
Perhaps use 'trap' to capture a certain signal, then send a signal. Try using the signals HUP, USR1, USR2 for that.
Thanks for the reply, I am a very much open to any input I can get. So I have a script that a coworker put together in JSON (I know, pretty weird but the device im working on understands JSON commands). I can invoke the script by inputting its path and a few flags. Once it launches, I have to STDIN "run" which I am doing by "&lt;&lt;&lt;run" and this runs the script as expected. However, when the script is done, I have to exit the script by sending "quit". I think I might be able to do send a kill command for that, however, I want to end the "script" command's capture of the terminal session within the bash script. To do this, I need to manually press "CTRL-D" on my keyboard, this is what I hoped to automate via BASH so basically: "script" command starts recording terminal session --&gt; BASH script starts--&gt; appends log--&gt; script quits---&gt; "script" command exits closing my log (making it viewable). I unfortunately have to use "script" as the terminal session is actually in a pseudo terminal so its not standard output I can pipe to a text file (I tried the tee command and all variations of carets but no dice" Thanks again, 
I think key here is to know what things cause a child process to be created. Functions and builtins can be executed in the same process, or a child process. The best way to learn about these subtleties is through experimentation. Sprinkle `echo $BASHPID &gt;&amp;2` and `trap -p &gt;&amp;2` through your code and see what happens.
Think high dpi displays. Apple coined this term "retina" with their iPhone/MacBook displays. In terms of web, often a website will swap out standard raster images for double dimension versions (hence the 2x) compressed into the same dimensions on page either via CSS or (shudder) JavaScript shims to ensure a sharp display. 
Very useful! 
You didn't mention json_pp. Good list.
There's a variable $RANDOM for getting a random number. Each time you access it, you will get a number from 0 to 32767 as the result. You would do some math with it to get the range you want. For example, try running this at the command line: for x in {1..10}; do echo $(( 1 + RANDOM % 7 )); done
Hm, you got me there – I didn’t test that, and I guess neither did the author of the StackOverflow answer? I *think* what’s happening is that Bash interprets plain strings in arithmetic expressions as variable names, recursively. Compare: FOO=BAR BAR=BAZ BAZ=2 echo $((FOO)) # prints “2” When you’re calling `./script foo`, the arithmetic expression expands to `0 + foo`; `foo` is an undefined variable and equivalent to `0`. On the other hand, `./script "'foo'"` results in the expression `0 + 'foo'`, which is a syntax error: &gt; ./script.sh: line 3: 0 + 'foo' : syntax error: operand expected (error token is "'foo' ") Honestly, I had no idea Bash does that, and I’m still not convinced my suspicion here is actually correct. But in the meantime, I suppose a better way to protect against non-numerical arguments would be: if ! [[ $1 =~ [1-9][0-9]* ]]; then printf &gt;&amp;2 '%s: first argument must be numeric: %q' "$0" "$1" exit 1 fi
 _floor=0 _range=10 # Will get you a number between 0 and 9 echo $(( $RANDOM % ${_range} + ${_floor} )) If you need a larger number, I've used date '+%s' to generate larger. Not often I need larger than that.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
This looks overly complicated. Nagios checks should generally be fairly simple things. And to make them even simpler, build yourself a library of functions that can be reused across any Nagios check that you write. Then source it into your check scripts. For example, I went through an exercise of cleaning up a bunch of shitty NRPE checks that I'd inherited, which included putting together some common functions such as `printOK`, `printWarn`, `printCrit` and `printDebug`, as well as standardised variables like `thisJob` and `thisHost`. It should be obvious what they do. Then when it came time to migrate some of those checks to local `check_mk` checks, most of the work was done by simply updating the library. So, in this case, you shouldn't need `getopt` when positional parameters are more than sufficient. Especially given that being in a specific usage environment, you don't really need to cater for edge cases as much as you would for a general use script. I'd probably start off with something like this: *Note: None of the code below is tested... I'm only one coffee into my morning, so there might be errors* # If run with '-h' or less than 2 parameters are given, print usage if [[ "$1" = "-h" ]]||(( $# &lt; 2 )); then printCrit "Usage: ${thisJob} host1 host2 .. host[n]" exit fi From then on, you can assume that `$@` contains the list of hosts you need to work with. Now, your code: active_array=() for jenkins_host in "${host_array[@]}"; do cmd=$(ssh -qn ${jenkins_host} 'pgrep java;exit 0') if [[ "$?" != "0" ]]; then echo "CRITICAL: Unable to check host: ${jenkins_host}" exit ${ST_CR} fi [[ -n "${cmd}" ]] &amp;&amp; active_array+=(${jenkins_host}) done Can be reimplemented something like: rcArray=() for host in "$@"; do pidCount=$(ssh -qn "${host}" "pgrep java | wc -l") if [[ "$?" != "0" ]]; then printCrit "Unable to check host: ${host}" rcArray+=( 2 ) else printOK "Found ${pidCount} Jenkins instances on ${host}" rcArray+= ( 0 ) fi done Then you can test `rcArray[@]` and exit appropriately.
 mv "foo-foo bar-bar-bar1*" "foo-foo bar-bar-bar1/" mv "foo-foo bar-bar-bar2*" "foo-foo bar-bar-bar2/"
yes but what if there are a lot of files and with random names. How can I loop over all of these files and look for that bar1 content?
Obviously the directories have to exist first
"foo-foo bar-bar-bar1*" matches every single file that starts with the string "foo-foo bar-bar-bar1". This includes files named "foo-foo bar-bar-bar1 5644.jpg" and "foo-foo bar-bar-bar1 5648.jpg". Notice the asterisk (sometimes called a star).
Sorry for not presenting my issue across correctly. Maybe I am wrong in thinking too.. I am looking for this part of the file extension as a variable: foo-foo "bar-bar-bar1a* foo-foo *arb-arb-arb1d* foo-foo *abr-abr-abr1e* I need extension-2, which is random, moved into the directory based on "extension-1 extension-2". The third part is meaningless for organization. 
I don't think I understand your problem. What do you mean by "extension-2" or "extension-1 extension-2". File extensions aren't really a _thing_ in Unix-alike filesystems. Filenames are just file names. I also don't understand how you have your filenames setup. Can you use terms besides foo and bar? They are confusing here. Give me a small sample of your actual filenames. I don't care if it's porn or whatever.
cc-19 a-15-1a 6767.jpg cc-19 a-15-1a 6768.jpg cc-19 a-15-1a 6769.jpg cc-19 a-15-1a 6770.jpg cc-19 a-15-1b 6781.jpg cc-19 a-15-1b 6784.jpg cc-19 a-15-1b 6787.jpg cc-19 a-15-1b 6788.jpg cc-19 a-18-1a 6790.jpg cc-19 a-19-1e 6781.jpg ext1 = cc-19 ext2 = a-xx-xx \\this is what I need to use as the variable *I think.. ext = meaningless
Can you just mv "cc-19 a-15-1a*" "cc-19 a-15-1a/" If it really is a lot of different folders, then I might use a for loop with awk. 
It is thousands of folders.
I solved it with some help in stackoverflow: while read file; do fname="${file##*/}" ## strip path information, leaving filename dname="${fname% *}" ## get all chars before the last space dname="${dname%%* }" ## trim ext 3 from filename mkdir -p "$dname" ## create the directory (no error if it exists) # (add -i to be prompted if dir exists) mv -i "$file" "$dname" ## move the file to the new directory done &lt; &lt;(find . -type f -name "*.txt")
`find . -maxdepth 1 -regextype sed -regex '\./[0-9]\{3\}\.txt'`
Alright, if it's thousands of files you might be fine with having to install stuff. There's a command line program that has a bunch of different names: perl-rename prename rename I heard on Mac there's a thing named "homebrew" to install simple tools like this, similar to how you would install stuff with a package manager on a Linux distro. Maybe google something like "install perl-rename with brew". The following command seems to work for what you want to do: perl-rename -n 's{^(.*?)\s*\d+\.jpg$}{$1/$&amp;}' * The `-n` parameter makes it just print stuff, but not actually rename files. If its output looks good, you would use it again without the `-n`. When I try with your example filenames here for me, I get this result: $ perl-rename -n 's{^(.*?)\s*\d+\.jpg$}{$1/$&amp;}' * cc-19 a-15-1a 6767.jpg -&gt; cc-19 a-15-1a/cc-19 a-15-1a 6767.jpg cc-19 a-15-1a 6768.jpg -&gt; cc-19 a-15-1a/cc-19 a-15-1a 6768.jpg cc-19 a-15-1a 6769.jpg -&gt; cc-19 a-15-1a/cc-19 a-15-1a 6769.jpg cc-19 a-15-1a 6770.jpg -&gt; cc-19 a-15-1a/cc-19 a-15-1a 6770.jpg cc-19 a-15-1b 6781.jpg -&gt; cc-19 a-15-1b/cc-19 a-15-1b 6781.jpg cc-19 a-15-1b 6784.jpg -&gt; cc-19 a-15-1b/cc-19 a-15-1b 6784.jpg cc-19 a-15-1b 6787.jpg -&gt; cc-19 a-15-1b/cc-19 a-15-1b 6787.jpg cc-19 a-15-1b 6788.jpg -&gt; cc-19 a-15-1b/cc-19 a-15-1b 6788.jpg cc-19 a-18-1a 6790.jpg -&gt; cc-19 a-18-1a/cc-19 a-18-1a 6790.jpg cc-19 a-19-1e 6781.jpg -&gt; cc-19 a-19-1e/cc-19 a-19-1e 6781.jpg I think for you the tool will be named just "rename" after you install it with homebrew, not "perl-rename" like here for me.
SWEET! I wonder how that will work if the filename has multiple spaces between the "extensions".. "cc-19 a-a5-1b 4456.jpg"
Worth noting the following syntaxes can be used instead of if/then/else/fi in bash; exit status notation. # if/then [ test ] &amp;&amp; command # !if/then [ test ] || command # if/then/else [ test &amp;&amp; command || command # one liner [ test ] &amp;&amp; { command; command; command; } # multi line [ test ] &amp;&amp; { command command } || { command command } Also worth mentioning that you should always put quotes around variables as follows: [ "$variable" = blah ] Otherwise if the variable is empty, you get an error like this: [: =: unary operator expected My experience is that quotes around strings are not necessary.
Texting while driving? :o
 FIND=BARCODE REPLACE="$(sed -n -e 's/{PRINTER INSTRUCTION REGEX}//p' &lt; "$1" | head -n 1)" sed -i "s/$FIND/$REPLACE/g" xml.file
You should know that paste() would conflict with /usr/bin/paste part of gnu coreutils.
The following block I have in a file called waitometer it exports a function called wait_pid which I can then use in scripts where there is anything that takes time and is usually silent, it backgrounds the command or command block, provides a spinning ascii wheel and monitors the pid for completion. export CHAR=( 55 134 174 57 ) special() { local i=1 while [ $i -le $2 ]; do local oct=\$\'\\$1\' eval echo -n $oct ((i++)) done } backspace() { special 10 1 } wait_pid() { local CNUM=0 local PID=$1 trap "kill $PID 2&gt;/dev/null" EXIT while kill -0 $PID 2&gt;/dev/null; do [ $CNUM = $((${#CHAR[@]}-1)) ] &amp;&amp; CNUM=0 || ((CNUM++)) special ${CHAR[$CNUM]} 1 sleep 1 backspace done trap - EXIT } export -f special export -f backspace export -f wait_pid Example usage script: #!/bin/bash . waitometer echo -n "Doing something that takes time: " { sleep 5 sleep 1 } &amp; wait_pid $! echo DONE
Fantastic idea! I'll try it out 
Never thought of this. It will confuse me but I am willing to give it a shot.
That's part of the problem I'm having, yes.
I guess *someone* here can't remember how to do the math, hm? This kind of thing is a bit hard to do if you don't know where to start, and then after you know what to do with regards to the math, there will also be bash being annoying because it can only do integer calculations and will produce terrible rounding errors. About the math, you can do your transformation with a linear function of this form: f(x) = a * x + b Together with those two examples you had about what should happen with the values for $pt and $tl, you have two equations that are supposed to be true at the same time. Your question is then about this here: 144 = a * 1 + b 72 = a * 280 + b You now have to find out what to use for those a and b numbers to make both equations true at the same time. At this point, you can just be lazy and use for example the Wolfram Alpha website. Put both equations in there with a comma. It then gives you this result: a = -8/31, b = 4472/31 This means the answer for you is this: pt = (-8 * tl + 4472) / 31 Now you have a pretty annoying problem that you are asking about how to use this in bash, and bash can only work with integers by itself. You can use an external program that can do floating point math, like awk or bc. You could also do something tricky with integers where you multiply all your values by for example 1000, then do the calculation with those values, then divide by 1000 at the end. Experimenting at the command line about how to do this with bc, I came up with this here: for tl in {1..280}; do pt=$(bc -l &lt;&lt;&lt; "x = (-8 * $tl + 4472) / 31; scale=0; (x + 0.5) / 1"); echo "$tl -&gt; $pt"; done This seems to round the numbers correctly. If you compare the output of that for loop with what happens just using bash itself and integers, you will see some results are different, which is a rounding error: for tl in {1..280}; do (( pt = (-8 * tl + 4472) / 31 )); echo "$tl -&gt; $pt"; done So while this looks a lot simpler, it's sadly a bit wrong. You will have to use that "multiply with 1000" idea I mentioned earlier. It's tricky and the end result looks like this: for tl in {1..280}; do (( pt = ((-8 * tl * 1000 + 4472 * 1000) / 31 + 500) / 1000 )); echo "$tl -&gt; $pt"; done This does round correctly. You should use the pure bash solution, not the bc one, even if things look really complicated. This is because of the performance. If I measure with "time", that simple for loop that just tests the values {1..280} looks like this with 'bc': $ time for tl in {1..280}; do pt=$(bc -l &lt;&lt;&lt; "x = (-8 * $tl + 4472) / 31; scale=0; (x + 0.5) / 1"); echo "$tl -&gt; $pt"; done ... real 0m0.350s user 0m0.236s sys 0m0.179s And using just bash alone looks like this: $ time for tl in {1..280}; do (( pt = ((-8 * tl * 1000 + 4472 * 1000) / 31 + 500) / 1000 )); echo "$tl -&gt; $pt"; done ... real 0m0.002s user 0m0.002s sys 0m0.000s 
Thank you for the response. It looks like pyudev may be the way to go. I'll try creating a script and adding it to my xinitrc file to listen at boot. Or something. Do you happen to know how to tell it to look for a specific USB device? &amp;nbsp; This is my current script (I definitely just ripped it from that link you shared): #!/usr/bin/env python # Based on: # http://askubuntu.com/questions/508236/how-can-i-run-code-whenever-a-usb-device-is-unplugged-without-requiring-root/ # http://stackoverflow.com/questions/469243/how-can-i-listen-for-usb-device-inserted-events-in-linux-in-python import functools import os.path import pyudev import subprocess def main(): BASE_PATH = os.path.abspath(os.path.dirname(__file__)) path = functools.partial(os.path.join, BASE_PATH) call = lambda x, *args: subprocess.call([path(x)] + list(args)) context = pyudev.Context() monitor = pyudev.Monitor.from_netlink(context) monitor.filter_by(subsystem='usb') monitor.start() # Call these again whenever a USB device is plugged or unplugged: for device in iter(monitor.poll, None): # I can add more logic here, to run only certain kinds of devices are plugged. #call('xinput_disable_mouse_acceleration.sh') #call('xset_my_preferences.sh') call('mario.sh') if __name__ == '__main__': main()
Actually, I just found how to filter for the specific device using an if statement. The pertinent code is: for device in iter(monitor.poll, None): # I can add more logic here vendor_id = device.get('ID_VENDOR_ID') # Get vendor_id using lsusb in bash. It is the first number. if vendor_id in ['0079']: call('mario.sh') It works fine. But now I have the issue of the program starting whenever I both plug and unplug the controller (which the script does inform me of in a comment). Since I will be ending retroarch with controller input, do you know of a way to change this to only occur when the device is added, and not when it is unplugged? I'm seeing results that recommend adding action == 'add' to my if statement, but this only returns an error of "global name 'action' is not defined."
I looked at my earlier post again, and I noticed I made the math much too complicated. It is a lot easier if you think about how things can be represented visually as a 2D graphic. That `f(x) = a * x + b` formula is for a line that you can draw on an X/Y plot. The four $tl, $pt values you mentioned, you can think about as two points that the line is supposed to go through. It's something like this: pt ^ | | | A__ &lt;-- (1, 144) | \__ | \__ | \__ | \__ | B &lt;-- (280, 72) | +--------------------&gt; tl Now about that `pt = a * tl + b` formula that can draw that line, the `a` in the formula is the slope of the line, and `b` is the vertical offset. You can get those through looking at the horizontal and vertical distance between your two points. You get this: tl_min = 1 pt_min = 144 tl_max = 280 pt_max = 72 a = (pt_max - pt_min) / (tl_max - tl_min) b = pt_min - a * tl_min You would save those two $a and $b values, and would then be able to translate any $tl into a $pt like this: pt = a * tl + b
Well, that gets us closer at least. :) [Still a few minor bugs, but it's mainly positioning.](https://i.redd.it/e28vjfzilbr01.png)
Picture for the lazy: https://i.imgur.com/ShkHyGR.png
Sure. You can use extended globbing to achieve this, using `+([[:alpha:]])` instead of `*` with `du`, but do note that the shell option `extglob` must first be enabled.
Thanks. I’ll check it out. Using grep to filter out would it be an option because running on all letters would take ages at certain conditions. Limiting to a range should help reduce the load and time.
Sure, using `grep` would be faster, if you're dealing with a massive amount of data.
Do you mean like this? du -hsx [a-d]* | sort -rh | head -50
Cool! This work, thanks ;-)
Man RGB makes everything better! xD
Lol love it!!
Since number of digits is small enough, you could use glob also.. find . -maxdepth 1 -name '[0-9][0-9][0-9].txt'
Not entirely sure what you’re after, but you could always do something like this: while [ some test for completion is false ]; do echo -n “***” sleep 3 done The “sleep 3” is completely arbitrary, you may even want to make a variable. However, the trick is the “echo -n” which prints somthing but doesn’t append a new line at the end, e.g. echo -n “Bite” echo -n “me” echo “looser” echo “Nah, you’re ok - JK” While display this: Bitemelooser Nah, you’re ok - JK See how it works?
Haha damn autocorrect! But I think I might not have explained my self properly haha. Basically what I am tyring to do is... If I had echo " *** Updating PC *** " How could I make the asterisk symbol to go bold, and then smaller, and then the next one along go bold, then back to small, then the 3rd one go bold... and it sort of rolls and repeats itself. Almost like it is a loading symbol 
Look into tput if you want an elegant way to do this. If you want a simple way to do this, you can use carriage returns (\r) to completely replace the previous line. If internet searches for those two things don't help you find what you need, I could write a simple example.
Definitely tput. Thanks for jumping in. The ‘\r’ hack can be handy for certain use cases but this sort of fine-grained character-level modifications can get really ugly, really fast. I’ve hacked some nasty ncurses stuff in the past to do fancy terminal graphics/colour/layout malarkey but that’s way overkill. So tput seems a good compromise.
Your problem is that size can't really be done, *emphasis* is the closest text manipulation to what you're after. I mean I *get*, conceptually, what you're trying to emulate (three bouncing dots that you've seen on plenty of mobile apps, right?) but the terminal by its very nature won't allow what you want. Not without a LOT of work. Try something like this: for em in dim sgr0 bold sgr0 dim sgr0 bold sgr0 dim sgr0; do tput "${em}" printf -- '%s' '*' done | paste -sd '' - If you're happy with the emphasis differences shown, then we can work with that. I think you might have an easier time with a classic spinning wheel (I have code for that ready to go), or maybe something like a moving pipe symbol (think `--|----`, or `*|**`), or using a foreground/background colour combination via `tput` (e.g.: a green square cursor moving around)
Try `-regextype egrep`. It's probably the type you will like best. What you tried in your screenshot would have worked with it.
I figured out how to go bigger, should be simple to modify it to roll :D while :;do for i in 5 6 7 8 9 a;do printf " \\U1f7b$i%.0s" {1..3};sleep 0.1;done;echo;done
For someone writing scripts involving regex and also using tools like `sed`, is there a particular recommendation, either `egrep`, `sed` or other for `find`'s `-regextype` (and other tools which support multiple styles of regex)? I'm not an expert regex user so I'm still learning a lot, and I want to learn something that I will actually use a lot rather than learn something and then needing to convert to something else that's more sensible for what I'm doing. Maybe like a comparison of the different `regex` options for `find` for real-world usage. I'm going to take a glimpse at the [differences](https://www.gnu.org/software/findutils/manual/html_node/find_html/Regular-Expressions.html).
Don't worry about this too much. If you want to learn regex, most important is just using any regex regularly. Reading it will then get easier and when you reach the point where that's no big deal, you don't really have to worry about the different regex versions. I guess I want to say, the biggest problem about regex is being able to read it. About the regex used here, grep vs. egrep specifically, the differences are super simple. They can both do the same, it's just the rules for `\` being different. In the default setting, you have to write it like this: .*/[0-9]\{3\}.txt And in the "extended" version, you can leave the `\` out: .*/[0-9]{3}.txt The things that are swapped around are: \(...\) --&gt; (...) \+ --&gt; + \? --&gt; ? \{...\} --&gt; {...} The `.` and `*` and `[...]` stay the same on both sides. The stuff in grep/egrep is based on what's in the POSIX standard about regex. You can read about that standard in `man 7 regex`. It calls the default grep rules "obsolete". About other regex engines, the one that can probably do the most is what is in the Perl programming language. Other tools that offer regex are often trying to be kind of similar to the Perl rules, for example PHP or Ruby or what comes with Python. There is a popular C library "PCRE" = "Perl Compatible Regular Expressions" for use in random tools that want to offer regex. Grep also has a switch `-P` to use PCRE. About what's perhaps most useful with PCRE is this here: Example text: abc&lt;def&gt;ghi&lt;jkl&gt;mno If you use this regex here to match: &lt;.*&gt; You will get this result: &lt;def&gt;ghi&lt;jkl&gt; This is because the `*` is "greedy" and tries to find the longest match. With the regex like what's default in grep, you have to do this: &lt;[^&gt;]*&gt; With the regex like in Perl, there's a "non-greedy" version of `*` that looks like this: &lt;.*?&gt; And that will find: &lt;def&gt; Ideas from Perl like that `*?` are just really good, so I guess that's why a lot of tools that support regex try to offer more than just the basic stuff. Another thing that might be confusing about using that `-regex` parameter in find: it wants you to match the whole length of the line of text, not just part of it. This means you can't do the following like you would do with grep: /[0-9]{3}.txt$ It wants you to have a `.*` at the start to eat all characters in the path, and you don't need a `$` to match the end: .*/[0-9]{3}.txt
&gt; foo='--arg1="bar" --arg2="baz"' &gt; some_executable $foo &gt; &gt; Now, I'd expect the above to be the same as: &gt; &gt; some_executable --arg1="bar" --arg2="baz" &gt; &gt; ...but instead it seems to be interpreted as: &gt; &gt; some_executable '--arg1="bar" --arg2="baz"' Not quite – if you define function some_executable { printf '%q\n' "$@"; } you can see that it’s actually interpreted as some_executable '--arg1="bar"' --arg2="baz"' i. e., it’s still split into two arguments, but the quotes aren’t removed. If you need quotes inside the variable content to be interpreted, I’m afraid `eval` would be the way to go. But you’re right that it should be avoided if possible :) Can you use arrays? Something like this: foo=('--arg=bar bar' '--arg=baz baz') some_executable "${foo[@]}" The syntax is a bit arcane, but this will allow you to store any number of strings, each of which can contain arbitrary whitespace, in a single variable. You can append to the array using `foo+=('--arg=qux qux')`.
Thank you! That's really helpful. It didn't occur to me that quoting the flag _and_ the argument (ie. '--arg=baz baz') would be valid, but it works fine. I've switched over to using an array and it's working great.
This should do what you want: foo='--arg1=''"bar"'' --arg2=''"baz"' It's basically just 4 strings concatenated together: '--arg1=' '"bar"' ' --arg2=' '"baz"' 
Ok, OP, you piqued my interest. Here's my half-hearted attempt at it... I call it a "half-Larson", because it works its way left to right then starts at the left-most point again. If it went left to right, then right to left, that would be a full Larson. Named for the guy who developed the effect for the Cylons in Battlestar Galactica and KITT in Knight Rider. #!/bin/bash progWidth="${1:-3}" sleepTime="0.2" resetTerm() { tput sgr0 exit 0 } # Try to set things back the way they were, however we exit trap resetTerm HUP INT QUIT TERM EXIT # GNU sleep can handle fractional seconds, non-GNU cannot # so we default to 1 second resolution in that scenario if ! sleep "${sleepTime}" &amp;&gt;/dev/null; then sleepTime=1 fi tput sc # Capture position tput civis # Hide cursor while true; do # Infinite loop for ((i=0;i&lt;progWidth;i++)); do # Iterate horizontally for em in dim sgr0 bold sgr0 dim; do # Emphasis sequence printf -- '%s' "$(tput "${em}")*" # Output emphasised char sleep "${sleepTime}" # Pause for effect tput cub1 # Move left one char done tput cuf1 # Move right one char done tput rc # Return to saved position done 
 #!/bin/bash f_abs() { echo "define abs(x) {if (x&lt;0) {return -x}; return x;};abs($1)"|bc -l } tl_min=1 tl_max=280 # negative values here because $pt decreases for $tl pt_min=-144 pt_max=-72 tl_range=$(($tl_max-$tl_min+1)) pt_range=$(($pt_max-$pt_min+1)) ratio=$(echo "$tl_range/$pt_range"|bc -l) for (( tl=$tl_min; tl&lt;$tl_max; tl++ )) do #pt=$(echo "$bc_defs;abs($tl/$ratio+$pt_min)+1"|bc -l) pt=$(f_abs "$tl/$ratio+$pt_min") echo "(tl=$tl) (pt=$((${pt%%.*}+1)))" done # this boundary condition must be explicit echo "(tl=$tl_max) (pt=$(f_abs $pt_max))"
I'm pretty sure I'd do this differently if I spent more time on it. I don't really like this approach.
Thank you all for your kind replies and helpful suggestions! Much appreciated!
Awesome! Cheers dude. Will give it a try.
Ok cheers for the suggestion!
##r/linux --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/853qg2/ignore_list/)
You can use wildcards like this: *f* *[0-9]* *5 *foo* foo* You can experiment by printing the result with `echo` or use `ls` (but will look confusing if the wildcard matches folder names), or do this complicated thing here to get one line printed per match: printf "%s\n" *[0-9]*
Looks nice. Have you thought about combining the "Divider / Status Line" and the "Current Location / Status Line" ? It seems more reasonable to only add 1 extra line per command than 2. I've been toying with something similar to this by combining my PS1 and the tmux status line, I've been trying for a minimal aesthetic though, because if I want a ton of stuff there's always powerline. 
Have you tried putting the `&amp;` symbol on the `echo` command instead? For example: echo `xdg-open /home/user/my.pdf` &amp; Also, the above line is equal to the following, and people recommend the later one: echo "$(xdg-open /home/user/my.pdf)" &amp;
I figured I might as well, despite it's similarity to other answers. https://github.com/ShelterPush/retroarch-at-controller-insertion
What's the point of echoing the commands instead of just running them in the background? 
I have no idea. I have *very* basic knowledge of bash and it was under my impression this was the way to go. I'll gladly take advice if you have any.
Something else to look into is the `nohup` command. This will immediately run whatever is passed to it into the background and allow the terminal session to be closed without also killing the command.
I know it’s an extra line, but I like the clean, visual divide of the flat grey line. There’s also not enough room on one line for the path and a few of the indicators that can come up on the status line. I like having some status info printed to the screen below command output so I can scroll back and review it. If you only care about current status for some things the tmux status line makes more sense. I keep it minimal by showing things only when they are relevant—like command exit codes, system load, etc. The status line is always there, but it’s only as cluttered as it needs to be. Even Powerline has a lot of stuff that only shows up when you need it. However you do it, I recommend getting some more information in your terminal. It helps. You know more about what’s going on. You’re not constantly typing commands to check on things.
I tried it while googling for an answer, but unfortunately I was making the same mistake and put it between the tick marks :)
Tab completion is an interactive shell feature and therefore controlled entirely by zsh. You would write your completion code for zsh the same no matter how the scripts are implemented.
&gt; If I run your first example, won't both applications close at the end of the script? No, by sending the process to the "background", it's no longer tied to the terminal or session that you have open. The program's stdout / stderr streams will still display in the terminal (normal text output, and error text output; you can redirect those elsewhere), so it may appear that closing the terminal will also close the opened applications, but it won't. If you want things to be cleaner, you can do this: #!/bin/bash xdg-open /home/user/Documents/my.pdf &gt;/dev/null 2&gt;&amp;1 &amp; /usr/bin/texmaker /home/user/my.tex &gt;/dev/null 2&gt;&amp;1 &amp; `&gt;/dev/null` is a way to redirect stdout (file descriptor 1) (also could be written `1&gt;/dev/null`) to the "file" `/dev/null` (aka "the bit bucket"). `2&gt;&amp;1` specifies send file descriptor 2 (stderr) to file descriptor 1 (stdout). Long story short, the application opened by `xdg-open` (whatever your pdf viewer is) and `texmaker` will send their output to `/dev/null` instead of to your terminal.
The `find` command is perfect for what you're trying to do. You can use the `-name` flag to specify the name of the file you're searching for. It also supports globbing. For example: find ./ -name "*substring*" This will search the current directory and all subdirectories for any files that contain the string `substring`. If you need to find a file, but don't know exactly where it's located, or even the full name of the file, you can use the `locate` command. This queries a database of files to find the full path to the file you're searching for. This command also supports globbing similarly to `find`. root@alpha:~# locate libpam /lib/x86_64-linux-gnu/libpam.so.0 Note: Since locate depends on a database, that database needs to be updated for locate to be accurate. There's usually a cronjob set up by default that runs nightly to update the mlocate database; However, if you need to update the database manually after a file was just created, you can use the `updatedb` command to do so. This takes longer the more files are present on your system. Both of these commands also support regex searches with certain flags. You can check the man pages for both for more info on that.
Would be useful to know what error are you getting and if you're using any plugins. Try curl -v 'host:9200/_cat/health', it should be http 200
very good point. posted late last night without thinking! not doing that anymore! So im connecting to aws from the outside so when I used curl and curl -o and it showed the progress bar, nothing happened at all. when you issue commands and then the nothing happens and the cursor just sits on the new line, what does that usually mean? thanks
mpv
Just use -v please, at least we'll know if you reach elastic at all
thank you
 #!/bin/bash while true; do # put your path and extensions in the find command, or use ls playfile=$(find . | sort -R | head -n 1) vidlen=$(ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "$playfile") # thanks derblub vidlen=$(printf "%.*f" 0 $vidlen) playlen=$((RANDOM % 30)) starttime=$((RANDOM % (vidlen - playlen))) stoptime=$((starttime + playlen)) vlc --start-time $starttime --stop-time $stoptime --play-and-exit $playfile done
Looks good :)
vidlen=${vidlen%.*}
Useless comment./
&gt; i want to play a random video from my video folder with a random start time then it should **play for a random time up to 30 seconds then vlc exits and it loops** 
As others suggested, using `find` is the way to go. There are ways to make `ls` do what you want (possibly with an added pipe to `grep` or `sed` or `awk`), but `find` is faster and more reliable. I wrote a code that automated a lot of the use of `find`. [LINK](https://pastebin.com/zVpJwBbG). I cant promise it is 100% bug free, but it should work. The syntax is rSearch [-r] [-s] [-dir=&lt;...&gt;] StringToSearchFor With no flags it will search for any file in the current directory or its subdirectories that contain `StringToSearchFor` anywhere in the path. the `-r` flag will cause `StringToSearchFor` to be treated as a (extended) regular expression. the `-s` flag calls `sudo find ...` instead of `find ...` the `dir=&lt;...&gt;` causes the search to happen in directory `&lt;...&gt;` instead of the current directory. Inputs can be in any order. the flags arent case sensitive, but the search string is (unless you have changed the default bash behavior with `shopt`). The actual find call is always of the form [sudo] find -O2 "${directory}" -regex ${regularExpression} If you dont use the `-r` flag, the regular expression is formed using `'^.'$(escapeRegexChars "${StringToSearchFor}")"'.*$'`. `escapeRegexChars` is included in the linked code, and does exactly what you would expect a function called "escapeRegexChars" to do.
Disregard everything I said, I can't read.
And blocked
and downvoted!
it will likely just download the half partially created file. 
It will either download the partial file, or it will spit an error due to the file size changing during download. This isn't really the best way to do backups, either. If you want it off site, why not establish an encrypted VPN to the other VPS, and mount a remote file system and do the mysql dump to there. Only big downside is the backups will be slower due to the network latency. There are other solutions for mysql backup, too... It depends on what you need. 
You could use "lsof" to check is the file you want to interact with is open. (List open files) used with grep, then if the fikenjsnopen by any process you can leave it alone, sleep for 5 minutes etc
not the most direct way, but you could do it by collapsing each block separated by a newline into a single line, sorting by the second column, then reformatting it into newline-separated blocks like so: gsed 's/^\s*$/=/g' test.list | awk '{if($0 != "="){printf " "$0}}{if($0 == "="){print ""}}' | sort -k2,2n | gsed 's/ /\n/g'
Whats happens if you put sed and xargs on same line? Or of you put a backslash as last char?
Nothing happens, same result. A way to test my script is to simply example files like`(20180414) Shadybunny - New Arena meta testing _ 3-1 Mage - !review v250183659.mp4` (changing the date and the uploader--`20180414` and `Shadybunny` in this case). Then change the `dir` variable to the directory containing these test files. You should get the same results as I do, where the output indicates everything is working as expected except the deleting the file part.
Yeah, that's a big problem about that Perl solution. The parameters are in `man perlrun`. The meaning is: * `-e` the following argument will be a line of perl code * `-n` put a loop around the perl code that makes it read from the input stream or the file automatically * `-0777` read the whole file into memory as one big text instead of line-by-line The code, I don't know how to explain it well. There's a bunch of obscure stuff going on in Perl. For example, there is an invisible `$_` variable used whenever you don't mention a name. The `$_` is where the lines of text from the file will show up in for the code, and it's what the `split` command works on in this code here. The `$a` and `$b` and the `&lt;=&gt;` are special stuff used in that `sort { ... } list` command. It's two elements of the list you are supposed to compare, and the `&lt;=&gt;` thing is a type of number comparison that produces results `-1` or `0` or `+1`, which is what the `sort` command expects. The `$x` and `$y` are my own variables. Them being needed is another obscure thing about Perl. I couldn't directly use the result of the regex rule that finds the second line in `$a` and `$b` because of something about "scalar context" vs. "list context". Maybe better would be to try to do that "turn line-breaks into TAB characters" instead, then using the normal sort program, then reverse that TAB thing. Doing that with Perl looks a lot easier to understand: perl -pe 'y/\n/\t/ if not /^$/' testfile | sort -k 2,2n | perl -pe 'y/\t/\n/' The `-pe` makes Perl run in a way where it reads a file line-by-line, applies the Perl code that's on the command line, then prints the line. The `y///` is the same command that's also in the 'sed' tool. It translates characters. The `if not /^$/` makes it only apply if the line is not an empty line. A difference about Perl and sed is, in Perl the `\n` line-break character at the end of the line is visible for your code. In 'sed', the `\n` isn't there. This means in Perl you can translate that last `\n` into a `\t`, but in sed you can't easily do it, making a sed version complicated to do. About the `sort -k 2,2n` is how you can sort on the second "field" of the input lines with 'sort'. It needs to be `-k 2,2`, not just `-k 2`, because it will continue looking towards the end of the line otherwise. The `n` means use number sort, not text sort. The `perl -pe 'y/\t/\n/` at the end turns the TAB characters back into new-lines.
Yea it would. I think the best way might be to trigger the secondary backup right after the first one completes. Just trigger the secondary through the same automation thats starts the initial one right after it finishes.
#!/usr/bin/awk -f BEGIN { xmin = 1 xmax = 280 for ( x = xmin; x &lt;= xmax; x++ ) { y = (-8 * x + 4472) / 31 printf ("%d%s%.0f\n", x,"\t",y) } }
 #!/usr/bin/awk -f BEGIN { xmin = 1 xmax = 280 for ( x = xmin; x &lt;= xmax; x++ ) { y = (-8 * x + 4472) / 31 printf ("%d%s%.0f\n", x,"\t",y) } }
This. pattern="bla" echo -e "no match\nbla here is a match\nno match" | grep --color=auto "$pattern" &gt;**bla** here is a match
THANK YOU! The one trick I ddn’t think of.
I have the following in my `.bash_profile`, it makes it so when I call grep it has color enabled and uses 'extended regex': colorized_grep() { grep --color -E "$@" } alias grep=colorized_grep
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
Script formatted properly this time, apologies Script(test.sh): #!/bin/bash func() { echo "${@:1:$#-1}"; } filename=$(basename "$1") name=$(func $filename) echo "$filename" echo "$name" 
properly formatted scrip this time, apologies. #!/bin/bash func() { echo "${@:1:$#-1}"; } filename=$(basename "$1") name=$(func $filename) echo "$filename" echo "$name" 
For me it works as expected regardless of the working directory. (Termux)
Desired result is to strip off the last "word" or string without spaces. Thus, foo* bar When executed from the workingdirectory it the script returns foo* bar 1234.file bar
So you want to pass a function a space-separated list of words and have it return all of the words, except for the last one?
correct, and the script works unless executed when the bash prompt is in the working directory and even if absolute paths are used.
What does `$filename` look like in each instance? e.g. add `echo $filename` after setting it. Perhaps then you will spot your issue.
Perhaps try adding `echo $@` to your function to see what is being passed to it.
script: #!/bin/bash func() { echo $@; } func1() { echo "${@:1:$#-1}"; } filename=$(basename "$1") name=$(func $filename) name1=$(func1 $filename) echo "$filename" echo "$name" echo "$name1" from any directory other than working directory: user@host:~&gt; /home/user/test.sh /home/user/workingdirectory/foo\*\ bar\ 1234.file foo* bar 1234.file foo* bar 1234.file foo* bar from the working directory: foo* bar 1234.file foo* bar 1234.file bar 1234.file bar 1234.file foo* bar 1234.file bar Duplicated issue on Debian/Bash 4.4.12 Also, the asterisk seems to be causing the issue, script works as expected without the asterisk. Any ideas to make it work with an asterisk? 
Reddit is not showing the command right. Something is stipping a backslash infront of the asterisk when posted.
You could consider adding ShellCheck to your bag of tricks: Line 2: echo "some text" | grep --color='auto' '$var' ^-- SC2016: Expressions don't expand in single quotes, use double quotes for that.
Learned something, thanks!
&gt; Any ideas to make it work with an asterisk? That depends on your objective and what it is that you are attempting to accomplish. You've been vague about that, focusing on your specific script, which is a bit of an xy problem.
For example, if you want to run your function (removing the last space-separated word from the provided string of space-separated words) against all of the files matching your pattern `/home/user/workingdirectory/foo*\ bar\ 1234.file` you might be better suited to write a loop around the filenames, and feed each item to your function. For example: #!/bin/bash strip_last_word() { echo "${@:1:$#-1}" } for file in $1; do base=$(basename $file) name=$(strip_last_word "$base") echo "$file" echo "$base" echo "$name" done
Good point!
Did you name the script `dict` or `dico.sh`? First execute `chmod 755 dico.sh` assuming you named the script the same as on GitHub. Then run it using `./dico.sh word`.
Thanks for your help. (I'd previously done chmod +x but not 755). When I do the command you suggest the script runs but I get a grep message and it doesn't find common words (eat, house): $ ./dico.sh house usage: grep [-abcDEFGHhIiJLlmnOoqRSsUVvwxZ] [-A num] [-B num] [-C[num]] [-e pattern] [-f file] [--binary-files=value] [--color=when] [--context[=num]] [--directories=action] [--label] [--line- buffered] [--null] [pattern] [file ...] this word wasn't found. Again, thanks. 
Thank you for your patients and diligence crankysysop! Objective is to remove the last word (that happens to be a filename) from the argument passed to the script. Using your example script (stripped down) #!/bin/bash strip_last_word() { echo "${@:1:$#-1}" } base=$(basename "$1") #have to quote "$1" to work name=$(strip_last_word $base) #can't quote $base, need separate words passed to function echo "$name" Save to ~/script.sh and cd to ~/ and make executable Now touch bob\*\ sue\ 1234.file If executed from ~/ it does not work, example below. cd ~/ ./script.sh ~/bob\*\ sue\ 1234.file If executing from any other directory it works. Executed from tmp directory below cd /tmp ~/script.sh ~/bob\*\ sue\ 1234.file Why does the "executed from directory" affect this script? 
That's because recent version of osx uses bsd grep instead of gnu grep. Bsd grep does not support pcre mode (i.e. -P option). You can install gnu grep with brew: `brew install grep --with-default-name` 
Hey good questions. A couple of best practice tips before getting into the question you asked: 1. Shell scripts should end with the extension .sh. 2. Don't set your variables to numbers, set them to words like ONE or TWO. $1 represents the first argument your script takes, so you shouldn't create a variable called 1. $2 would be the second argument your script takes and so on with $3 and $4, etc. If that doesn't make sense, PM me. But take my word for it, it will not work to make your variables equal to numerical values. Now that that is out of the way, what is the use-case for what you are trying to accomplish? That way I can get a better understanding of what you are trying to do and hopefully provide costructive feedback.
If I understand your question right, you want to be able to run your command like so: ./test.bash hi And if the first argument is 'hi' then you want the output to echo 'hello', correct?
I had a thought, I could add the if statement to the start of each function, and the case options will just set flags to true, then call all functions at the end of the script. funct() { if [[ run_function = 'true' ]] then echo $number fi } while getopts ab: option do case $option in a) run_function='true' ;; b) number=$OPTARG ;; esac done funct Does this look better, or is there a more optimal way?
 FOO=BAR BAR=hello echo ${!FOO} prints "hello". Ie. an indirect variable reference. See here: http://tldp.org/LDP/abs/html/ivr.html http://tldp.org/LDP/abs/html/bashver2.html 
Thanks for the response. In regards to the variable names, I apolgize for using 1 and 2 and creating confusion. I just did it to make it it simple and forgot they already had meanings, even though I used the other meanings later in the post. Simple brain fart. I basically wanted to be able to run "test.sh var1" and be able to get the value from the variable "var1=hi"
Or ask upstream to use default regexps instead of PCRE (and use only POSIX options...)
So your file has a literal `*` in the name, and that isn't intended to be expanded as a bash glob? This smells like homework. You didn't share the (I assume differing) output of your two different test cases, so I can't really advise.
 Not all passwd's suppport "--stdin" Look at "chpasswd" instead 
What were you expecting to happen? Are you using GNU diff or other?
The `if` won't care what the command is, it just looks at the exit status. In the case of a pipeline, the exit status of the pipe is the exit status of the last command in the pipeline (generally).
As beatle said, don't parse the output of `ls`. If you want to do something with the contents of a folder, use `for i in $catalog_path/*; do ...; fi`, instead of `for i in \`ls $catalog_path\`; do ...; done`. Also, your code has an extra `done`. If I wrote your code, I'd do it like this: if [[ "${domain}" == "us.oracle.com" ]]; then for i in "${catalog_path}"/*; do if "${oradev_sudo}/sudo" \ -u oracle "${run_cat_path}/runcat.sh" \ -cmd unarchive -inputFile "${catalog_path}/${i}" \ -offline "${offline_path}" \ -folder "${folder_path}" \ -overwrite clean | grep -q "error" ; then ... fi done else ... fi 
Hmm thanks guys.. also, will the if | grep suppress the output of the command? Right now it gives output of what the command is running, if I use this if | grep, will it no longer output the command details, and only will output incase there is an error?
Is there a way then, to have the output still show, and only error out in case "error" is found? if for command if "error" exit
First, are you sure the exit status of `command` isn't good enough? If that won't work, you could switch to a tool called `ack` (or `ack-grep` on some systems I guess). Our pipeline would then be if command | awk --passthru "error"; then exit fi
[removed]
For something like this you may want to look at the curl options: curl -o -I -L -s -w "%{http_code}\n" http://URL https://stackoverflow.com/questions/38906626/curl-to-return-http-status-code-along-with-the-response curl -o -I -L -s -w "%{http_code}\n" http://www.cnn.com 200
$? gives you the return value of the last command, not output. A command can still return a number, but print output that you can capture. For example, grep prints the matched text along with a return value of 0, for success. 
Yeah I thought so, I think I would have to write something out for each code, I was hoping I could just write something out if it came back with no information there could be away around it other than that. I had already tried it before and took a break. 
Or inversely, you can just search for specific code responses. if (var == 404) echo "404 .. oops" else echo "all good man"
For all those that come in search of answers, the following will curl the https status and the subsequent line below is just a sanity check to print the variable. HTTP_STATUS=$(curl -s -o /dev/null -w "%{http_code}" 'http://example.com') echo $HTTP_STATUS
Out of curiousity, do you know if the `${!foo}` syntax can be expanded to deal with multiple chained indirect references? e.g., something like FOO=BAR BAR=hello hello=test test=apple apple=bananna #etc... I wrote up a short script to deal with this using aliasing and eval, and while this works for n"simple" variable names I imagine that it might fail for some situations where the variables have more "exotic" characters. myderefeval() { eval "$(echo "eval "'"$('"${*}"')"')" } myderef0() { local valout="" local valtemp="" if [[ "${1}" == '-genEvalStr' ]]; then echo 'eval "$(\myderef0 '"${2}"')"' else echo 'valOut='"${1}"'; valTemp="${!valOut}"; while ! [[ -z "${valTemp}" ]]; do valOut="${valTemp}"; valTemp="${!valTemp}"; done; echo "${valOut}"' fi } alias myderef='myderefeval myderef0 -genEvalStr' 
Hey, jkool702, just a quick heads-up: **curiousity** is actually spelled **curiosity**. You can remember it by **-os- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Call me crazy, but couldnt you just stagger the timings? e.g., make the backups at `1,3,5,7,9,11,13,15,17,19,21,23 o'clock` and download/export the newest 2 at `0,4,8,12,16,20 o'clock`? unless the backups are *really* slow or the devices are referencing clocks that are significantly incorrect, it would seem that this would avoid the problem entirely.
Probably simpler with a variation of your loop: a=FOO while [ -n "${!a}" ]; do a=${!a} done echo $a would print "banana" in your example (assuming it was the last in the chain). Ie. test for the indirect variable directly, rather than using temp variables. Still, I'd seriously question the sanity of requiring the use of this many indirections, though. :) Using an actual data structure that can key on arbitrary strings would allow more exotic characters, and not pollute the variable namespace. At that point, it's probably more sensible to use another language with better data structures, like Python, Perl, Go, etc. rather than attempting to do so much meta-programming in the shell.
Your post is not very well formatted so it's hard to follow and know exactly what you want but it appears that you are misunderstanding how grep works. The `BEGIN {}` section is run only once before the first line. You don't need to call `getline` to process each line. Each block denoted by `{}` at the top level with or without matching qualifier `/regex/ {&lt;commands&gt;}` will be run on each matching line. Here is an example awk script: ``` BEGIN {total=0} # sets the variable named sum to 0 at the beginning /^add / { total=total+$2 } # for any line that starts with the word "add" then add the second field to the sum variable /^subtract / { total=total-$2 } # for any line that starts with the word "subtract" subtract from sum {print; print " total =",total} # print each line as you go and the running total END {print "The total is",total} ``` 
&gt; test for the indirect variable directly, rather than using temp variables. Good catch. Admittedly the difference between having / not having an extra few characters stored as a local variable in the function is probably not more than a handful of cpu cycles and a few bytes in memory, but it makes the code cleaner and is good practice. Still, I'd seriously question the sanity of requiring the use of this many indirections, though. :) Fair enough. Though I tend to question my own sanity on a semi-regular basis, so... Id tend to agree though that bash might not be the right tool for this type of thing. &gt; rather than attempting to do so much meta-programming in the shell. Out of curiousity, do you know of another good way to get `eval-in-caller` type of functionality? (That doesnt involve making everything global, though this doesnt really qualify as a "good" way). My normal approach when I need to do this and cant figure out an alternative is basically to do something like `alias foo='eval foo1'` where `foo1` takes the inputs given in the call to `foo` and uses them to setup a string that has the command needed to be run. I know that as long as the command only relies on the inputs you can evaluate whatever you needed to do in the sub-function and then pass the result by refference to the caller, but (asaik) that doesnt work in cases like this where you need access to things that arent passed as inputs in the function call and arent global.
Hey, jkool702, just a quick heads-up: **curiousity** is actually spelled **curiosity**. You can remember it by **-os- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
damn...thats twice in a row.
Have you got any other code beyond your `du` one-liner?
&gt; passing by reference I suppose this is the better method, so long as you remember to actually declare the variables with `-n`. &gt; What's strange is that line with `echo` getting printed. I havent used zsh (though i might need to start, since it sounds like it would come in handy for quite a few limitations in bash, beyond just the one were discussing here), BUT im guessing that zsh automatically is expanding the call to something analogous to eval echo "\$$(eval echo "\$$(eval echo "\${!FOO}")")" (which will work to implement this in bash too). Depending on what exactly zsh is doing when it expands the multiple `${!...}` calls, its not that hard to imagine that the `echo` from each step gets sent to stdOut and the next operation in the chain, rather than just to the next operation in the chain. &gt; echo ${${foo##*/}%.*} That is rather nifty. Its much more straightforward than what id usually use, which would either be a 2 step approach or something like echo "${foo}" | sed -E s/'(^.*\/|^)([^(\/*$)]?[^\.]+)(\.[^\.]*$)?$'/'\1 \2 \3'/
&gt; Out of curiousity, do you know of another good way to get eval-in-caller type of functionality? I admit, in several decades of shell usage, I'm not sure I have *ever* used the "eval" command even once. :) So I haven't much insight to offer at the big picture level. But whatever you are trying to achieve seems like it would be much easier with a lisp-like shell language. Out of curiosity (that bot can eat a dick, btw), I googled and found this: https://github.com/rpav/ScriptL May be something worth thinking about. Googling "bash metaprogramming" reveals a few interesting links and tips, including the shell 'printf "%q"' feature (see 'help printf') for printing arguments as properly shell quoted strings, which was news to me.
Hey, primitive\_screwhead, just a quick heads-up: **curiousity** is actually spelled **curiosity**. You can remember it by **-os- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I haven't thought about it yet, but I think I saw some check_mk tickets for the servers I will be putting on this. The main sticking point is taking the du output and turning it into some type of variable that I can use.
this is as easy as output=$(df -sh) echo "$output"
You can set the password using useradd too. you just need to set the password as the encrypted password value depending on which md5/sha/etc you are using.
There's the `local` keyword for use inside functions. It accepts the same parameters as `declare`. For functions, that `-n` feature can sometimes be useful. In the past I ran into problems while writing scripts where I first thought I needed references. Whenever that happened, after working a bit more with the actual code, I always found that things could be reorganized to not need references. The reorganized stuff usually didn't feel worse than the original idea that needed references. The only examples I found where references were useful was with arrays. Using that `-n` parameter, you can do the following to make a function create an array: $ foo() { local -n array="$1"; array+=(hello hi hey); array+=(what); } $ foo test $ echo "${test[@]}" hello hi hey what Or you can do this: $ foo() { local -n array="$1"; unset array[1]; } $ test=(aaa bbb ccc) $ foo test $ echo "${test[@]}" aaa ccc I think I wouldn't try to use zsh for scripts. Learning its features seems like a bad idea to me because you can't be sure it's always installed on any machine like bash is. If something is annoying to do in bash, I would rather try to replace that bash script with Python or Perl and learn those. That said, it's a bit sad that this doesn't work in bash, isn't it? That kind of behavior in zsh seems pretty neat. I also just got the idea to try the following in zsh, because I remembered the "dirname" and "basename" tools: % foo=/some/path/here/hello.txt % echo "$(basename "$foo")" hello.txt % echo "${$(basename "$foo")%.*}" hello So apparently those cut operations work for other stuff as well, not just for variables. This would be neat to have in bash, but it sadly says this when you try the same in it: $ echo "${$(basename "$foo")%.*}" bash: ${$(basename "$foo")%.*}: bad substitution I feel looking into zsh would only be a good idea for the interactive features it seems to make possible. I've for example seen somewhere that you can make your config do something similar to auto-suggestions in IDEs, where while you are typing it will preview what would happen if you press TAB.
I use eval more than id like, though i do try and use it less and less as I go (ive only been seriously using bash for 9 or 10 months). One place I find myself using it often is for running things like things like echo $(eval "foo arg1 ... argN") inside of a function. It seems to automatically "fix" a good number of tedious to track down errors. Like, for example, in a function i have that calls find -O2 "$directoryPath" -regex "$reguarExpression" throws an error that `'&lt;directoryPath&gt; ' cant be found`, since somewhere in the process of extracting and filtering and user-provided input prior to making this function call, there (apparently) was an extra `' '` that was added to `$directoryPath`. Now m sure if I went through every regular expression in each filtering step again very carefully i could find it, of I could add yet another filter to remove it, but echo $(eval "find -O2 $directoryPath -regex $regularExpression") works just fine.
Interestingly, another approach can use the fact that the step-pattern for y is periodic, with some truncation at the beginning and end. Hence, it can be done with 3 loops with increment and decrement operators only, for lead-in, middle loop and lead-out - there's no need to use division at all. 
&gt; since somewhere in the process of extracting and filtering and user-provided input prior to making this function call, there (apparently) was an extra ' ' that was added to $directoryPath. Note, for this *particular* case (of stripping beginning and trailing whitespace), you could do this: find -O2 $(echo "$directoryPath") -regex "$regularExpression" But the general principal is (again) one of the reasons I've moved away from this kind of shell programming; the intermixing of whitespace for both data and argument separation causes untold numbers of problems with modern directory trees. Not really the fault of shell languages, just something they weren't designed for all those years ago. BTW, I'm starting to use Xonsh shell in part because of things like this; just another random suggestion. BTW, its been enlightening to see your explanations, so don't think I'm criticizing. Just hoping to point out that some things are inherently trickier in bourne shell languages, and that there are alternatives.
I appreciate you sharing with the community, but I don't see how this is any easier than just using 'find' with -maxdepth and -mindepth on its own, particularly considering that you lose access to -exec functionality 
So Ive run into this problem enough to have a solution on hand. If I run find -O3 "$(echo "${dirSearch}" | sed -E s/'^ *" *([^ ].*[^ ]) *" *$|^ *'"'"' *([^ ].*[^ ]) *'"'"' *$|^ *([^ "'"'"'].*[^ "'"'"']) *$'/'\1\2\3'/)" -regex "${argIn}" It will remove spaces, including those inside one bounding set of single out double quotes. The problem is that "extra spaces in directory names" if just one of many issues that seem to fix itself (many of them dont throw a nice informative error message either), and I figure my time is better spent writing more code and slowly applying fixes as I figure them out naturally rather than try and debug something that is already running. I definitely agree with you its not ideal, its just a matter of priorities. (Im also a last-year PhD student, and i already skate by on about as little sleep as I can, so my general priority list tends to be `debug / rederive theory behind non-working code` &gt; `improve efficiency on codes what deal with big data` &gt; `sleep` &gt; `improve working codes`). &gt; don't think I'm criticizing Oh not at all, i appreciate the tips. Im far from an expert in bash. Im decently crafty when it comes to hacking together solutions for things I dont know how to do, but a lot of the time theres a better "real" solution, and im glad to take some (constructive) criticism and suggestions since they save me a ton of time and effort of delving through stack exchange threads to try and figure out *what* that solution is. Ill have to look into Xonsh, though admittedly I already have a lifetimes worth of languages on my "to learn / get better at" list, so I do try to avoid needlessly adding to that :)
"Quote" "your" "things". "Please"
echo "Subject: disk usage \n\n $(du -sh)" | mail someone@exemple.com
Ok, one thing at a time. &gt;I am trying to write this as a automated run for when the server hits 85% used. If you don't have a monitoring system that can trigger your email script, you can `cron` this. You mention check_mk, so this might be do-able as a local check_mk check - but check with whoever is running your monitoring first. In your case you're concerned with home directories on a RHEL7 host, so it's fair to assume `/home`. And we can use `df`, with GNUisms: fsUsage=$(df -h --output=pcent /home | tail -n 1) Then use it in a simple comparison, in this case we strip the `%` symbol for the comparison: if (( ${fsUsage%\%} &gt;= 85 )); then [actions go here] fi Example: if (( ${fsUsage%\%} &gt;= 85 )); then echo "/home is using ${fsUsage}"; fi /home is using 95% Ok, so next you want to identify the biggest consumers of space in `/home`, which can be done in many ways, including your `du` one-liner. I'd adjust it a bit like this, myself: du -s /home/* | sort -nk1 | tail -n 5 | awk '{print $2}' That will get you the list of directories. Next you want to find the owner of those directories. The bad, naive approach is to parse `ls`. Don't parse the output of `ls` - consider that one of the highest laws of `bash` scripting. Use `stat` instead, again we can happily assume GNU `stat`, throw it all together and we get something like this: fsUsage=$(df -h --output=pcent /home | tail -n 1) if (( ${fsUsage%\%} &gt;= 85 )); then for homeDir in $(du -s /home/* | sort -nk1 | tail -n 5 | awk '{print $2}'); do dirOwner=$(stat -c %U "${homeDir}") dirUsage=$(du -sh "${homeDir}" | awk '{print $1}') userMsg=""${homeDir}" on $(hostname) is using ${dirUsage}'s of space. Please clean up" echo "${userMsg}" | mailx -s "Disk usage alert: $(hostname):${homeDir}" "${dirOwner}"@someorg.tld done fi Obviously this could get a bit more complex, say in an AD'd environment where the username might not always match the email address, and so you have to pull that info out of AD. That's a fun little exercise. What about a scenario where it's one moron downloading 1.5T of Sybase prod data to a dev environment and using their homedir, like a moron, and the next 4 top space hogs aren't space hogs at all and don't really deserve an email? Etc etc. Finally, have you considered quotas?
Uploading php files, or files with php code in it is usually not allowed for security reasons. 
Yes that i why i want HTML Shell Or I want to change permission of that folder so i can access PHP file
Thanks. Reading though that did help alot of my question on how to format it. And yes luckily this server and the series I am running them on are AD - there are other servers that use different account names. Automating those will come. And for quotas, I will be adding a threshold of utilizing over 10GB.
Hey, Svenderman, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Look at e.g. my https://gitlab.com/moviuro/moviuro.bin/blob/master/gif-soup script. It forks a background job (mpv), waits for some SIG (USR1 &amp; 2), and also has built-in cleanup on SIGTERM and similar. Hope it helps.
I should mention that in the script something both instances will be run, sometimes just one, sometimes other applications will be started, so every time both instances will need to run in the background, and simply doing something like this below doesn't work. #!/bin/bash trap 'kill $pid; exit' SIGINT mdk3 wlan0 &lt;options&gt; &amp; pid=$!
I'll give it a read, thanks.
I think I could maybe try an if statement that checks if other variables are set (ones that trigger starting other applications), if not then run in the foreground, if any are set then run in the background and set the PID in a variable and call a function that will run the kill $pid when SIGINT.
After much monkeying around I figured out the function strip_last_word() { echo "${@:1:$#-1}" } does not get it's argument from name=$(strip_last_word $base) The function gets the argument from from the user@host:~/&gt;script.sh (argument) I would think that inside an argument is it sheltered from command execution arguments. Am I incorrect in my assumptions? Thanks again for you help crankysysop! Resolved my issue by removing the function all together with: filename=$(basename "$1") name=$(echo "$filename" | awk 'NF{NF-=1};1')
You can use `kill 0`. That zero is a special PID that will make the signal go to the whole group (meaning to the script itself and to all children). Using that, you don't have to keep track of the PIDs. Try adding the following to your script and see what happens: trap 'exit' INT trap 'kill 0' EXIT I'm not sure if the trap for the INT signal is needed. I was worried that the EXIT trap might not execute when you hit Ctrl-C, only when you exit normally with the 'exit' command.
&gt; I don't see how this is any easier than just using 'find' with -maxdepth and -mindepth on its own, particularly considering that you lose access to -exec functionality `-mindepth` and `-maxdepth` are now imcluded as options. I wouldnt want `-exec` functionality in something like this since im sure there are still a handful of edge cases where things dont get parsed like they should. This is probably harmless for search functionality (assuming you arent piping the output to `rm` or something like that), but I could see this causing major issues with `-exec`. The "point" was to make it quick and easy to do a search using `find`, and have common options automatically set up with the ability to toggle a few more common options using flags. As an example: rSearch -s -depth=0,3 "$str" gives the same as sudo find -O3 "$(pwd)" -mindepth 0 -maxdepth 3 -regex '^.*'"$str"'.^$' Using rSearch requires ~60% fewer characters (29 vs 71), and (imo) is easier to remember the syntax since you only have a handful of common options. Calling it is at least 2x as fast strictly from a number of keystrokes perspective, and if you dont remember the syntax off the top of your head (or never learned it in the first place) you only have ~20 lines of text describing the flags and syntax versus *many* more with `man find`. Its not meant as a full-fledged replacement to `find`, but it does make calling it quite a bit easier and more straightforward (imo) for a good number of common searches you might want to perform.
This has nothing to do with Bash or even shell scripting in general. Please try a different subreddit – perhaps /r/webdev or /r/programminghelp?
Do you have control about the script or whatever is creating the backup file in the backup folder? Maybe you can change it so that it first creates the file in a different location or with a different filename, then uses a `mv` command to rename/move it to the final name. The renaming operation is "atomic", meaning it's guaranteed that another program never sees anything half-finished about it. Using that would guarantee that the problem you wonder about can't ever happen. About that idea someone mentioned of using `lsof` to check if a file is open, that looks like this in practice when using it for local files: lsof +r1 your-filename-here If the file is open, this `+r1` will make it loop and repeatedly check again with one second sleeps in-between.
If I send something to the background, wouldn't the script just finish and immediately those processes? Would I need to have the script just not exit until it gets ctrl+c?
Wow, that's a lot of symbols. I don't even want to read your script. Things like this might make it easier to read: --- if [[ $argIn =~ ^.*-+[Qq]([Uu][Ii][Ee]|[Tt])?.*$ ]]; then +++ if [[ ${argIn,,} =~ ^.*-+[q](uiet)?.*$ ]]; then Have you looked into `getopts` and `shopt`? They might be able to simplify things further for you. 
&gt; Don't parse the output of `ls` Huh. I guess I never really cared to handle odd characters such as newlines and spaces in filenames, but if `glob` works just as well, and it's even simpler, I might as well switch. Looks like I have a few scripts to adjust.
If I send something to the background, wouldn't the script just finish and immediately those processes? Would I need to have the script just not exit until it gets ctrl+c? 
If I send something to the background and then kill all PID on exit, then the script immediately finishes an kills all the processes also immediately. trap 'wait $pid' EXIT trap 'kill $pid' SIGINT This seems to work. I can send the process to the background so other process can be started, and the script won't end until when everything is sent to the background, it will just wait for a ctrl+c Does this seem logical, or is there a better way?
I like it, except I think I'd try to do this without that `$pid` variable, just so there's less to worry about. When you run `help wait` at the command line, it says you can use `wait` with no parameters to make it wait for all child processes. And for `kill` there's that special `kill 0` to make it work with any number of child processes. It would then look like this: trap 'wait' EXIT trap 'kill 0' SIGINT I hope I understood everything right about `wait` and `kill`, so if you want to use this, make sure to test it.
There are certainly people who take the view that if you put spaces or other special characters in your filenames, you get what you deserve. Still, if you're working on files other people give you, best to be prepared I figure.
Check out a tool named "shellcheck". Your distro probably has a package for it. You can also test it online at www.shellcheck.net without having to install it. It tries to find all kinds of weird mistakes that are possible in bash scripts.
Gnu diff latest 
I'll just paste the relevant section of my ~/.bashrc here, since it seems to address the issues you've mentioned: ### History {{{ # Default behaviour is to have a per-session command history (HISTSIZE), # which is saved to HISTFILE after the end of the session, and then to # check HISTFILE against HISTFILESIZE and truncate the file if it has more # lines on next startup. # More here: # https://stackoverflow.com/questions/19454837/bash-histsize-vs-histfilesize # Only append to HISTFILE, don't overwrite. shopt -s histappend # Record multi line commands as one liners for easier searching in HISTFILE. shopt -s cmdhist # Instead of immediately executing history expansions, display the result # in the realine buffer first. shopt -s histverify # Set the max number of lines stored in the history file to -1, which # inhibits truncation. HISTFILESIZE=-1 # Set both the number of commands remembered in the session, and the number # loaded at startup in the interactive shell session to -1, unlimited. HISTSIZE=-1 #More useful timestamps in HISTFILE. {'%F':'%Y-%m-%d', '%T':'%H:%M:%S'}. HISTTIMEFORMAT='%F %T ' # Record each line to HISTFILE as it's issued, instead of at the end of the # shell session. PROMPT_COMMAND='history -a' # Dont' record some commands. export HISTIGNORE="&amp;:[ ]*:exit:ls:l:ll:lll:bg:fg:fc:history:clear:help" # Don't record commands preceded with spaces. export HISTCONTROL='ignorespaces' ### }}} 
Spaces I can see. But who puts newlines in their filenames? That is just nefarious. At the same time, I can see the viewpoint that if you doesn't handle all valid inputs, then you can't say you are adhering to standards.
Your "PROMPT_COMMAND" variable has every history command other than the one you want. Change it to `PROMPT_COMMAND="history -a"` and you should be set. `history -a` will append history lines from the current session, and any commands in `PROMPT_COMMAND` will be run each time your prompt is displayed. The net result is that all sessions will write history to your `HISTFILE` as commands are issued. You can learn more [here](https://www.gnu.org/software/bash/manual/bashref.html#Using-History-Interactively).
Thanks yet again. I don't think I have used multi line commands on the cli before, but I just made a test and it seemed to still place them all on the same line, separated by semicolons, but `shopt -s cmdhist`still seems like something I should add on my .bashrc I came upon [this one.](https://unix.stackexchange.com/questions/48713/how-can-i-remove-duplicates-in-my-bash-history-preserving-order) It seems to preserve history order, which I'm not sure is possible with uniq. `cat ~/.bash_history | nl|sort -k2 -k 1,1nr|uniq -f1|sort -n|cut -f2 &gt; tempHistory.txt &amp;&amp; mv tempHistory.txt ~/.bash_history` Now I'm trying to see if I find something that could work if I decide to enable `HISTTIMEFORMAT`, but I'm way out of my depth already, but learning some cool new stuff. 
So with `winbind`, you could perform an `ldapsearch`-esque lookup fairly easily like this: userSID=$(wbinfo -n "${someUser}") net ads sid "${userSid}" -U someBindAccount%somePassword 2&gt;/dev/null | awk '/^mail:/{print $2}' Sadly, `sssd` doesn't have a `wbinfo` type tool. `net ads sid ...` works fine-ish, but you kinda need the SID first. Most commonly recommended way to get it? `ldapsearch`... seems a bit pointless using `ldapsearch` just to get a SID (and then [convert it](https://serverfault.com/questions/851864/get-sid-by-its-objectsid-using-ldapsearch)) to use with `net ads sid` just to get an `ldapsearch` style output... just to get a user's email address... which you can simply get from the first `ldapsearch`! You *could* use something like `rpcclient` or try your luck with `ldbsearch` e.g. `ldbsearch -H /var/lib/sss/db/cache_$(dnsdomainname).ldb -b cn=sysdb '(name=${someUser}*)' 2&gt;/dev/null | awk '/objectSIDString/{print $2}'` But this potentially has a few more gotchas than a simple `wbinfo -n`. OTOH, if there is a sssd cache entry for the user, you can get their email address in one hit: `ldbsearch -H /var/lib/sss/db/cache_$(dnsdomainname).ldb -b cn=sysdb '(name=${someUser}*)' 2&gt;/dev/null | awk '/^mail:/{print $2}'`
Do you know if there's a way to suppress the "^C" that's displayed when exiting?
How does your script look like currently? If you are starting everything in the background, you could put a `read -s` command at the end of the script. That will read a line of text from the keyboard while not showing anything that the user types on the screen. This also suppresses those `^C` characters for me here. You could use a loop and repeatedly call that `read -s` command while not doing anything with what it's reading so that just Ctrl-C is quitting your script: echo "Press Ctrl-C to quit!" while true; do read -s done
Yeah, I keep a directory of "unsafe" filenames that I can test as inputs to scripts like that. I'd wager newlines mainly come from either someone trying to mess with your script or from an automated process that is creating filenames programatically. It doesn't come up all that often though in my world thankfully!
Got it. Thank again for this, much appreciated!
You can use sed for this. However, &gt; I can't use python commands because the command in that autoinstaller needs to be in only one line You can put quite a bit of Python in one command with `python -c` and use of semi-colons. If you can have the one command be a script to execute, then of course you can make a full Python program and just execute it. Normally the user needs to generate their own secret key and manage it themselves, not the installer.
Thank you for the reply. This command does give me the key, but what should I use to do the replacement ? python3 -c 'import settings; print(settings.SECRET_KEY)'
Assuming: 1. you want any field that matches in settings.py to be replaced by the same field's value in settings2.py 2. settings.py is the file you want modified and it's contents used everywhere a field doesn't match 3. settings2.py has the value to be used in place in settings.py ---- myhostname:test username$ cat settings.py VARIABLE_1 = 'DEFAULT_VALUE_2' VARIABLE_3 = 'DEFAULT_VALUE_3' SECRET_KEY = 'DefaultKeyMustBeChanged' myhostname:test username$ cat settings2.py VARIABLE_2 = 'VALUE_3 VARIABLE_3 = 'VALUE_4' SECRET_KEY = 'ANewValueIsHad' myhostname:test username$ awk 'BEGIN{FS=OFS="="}NR==FNR{a[$1]=$2;next} $1 in a{$2=a[$1]}1' settings2.py settings.py VARIABLE_1 = 'DEFAULT_VALUE_2' VARIABLE_3 = 'VALUE_4' SECRET_KEY = 'ANewValueIsHad' 1. You can see that VARIABLE_1 does not exist in settings2.py, so it's value is not modified. 2. VARIABLE_3, and SECRET_KEY exist in both settings.py and settings2.py. So both values are retrieved from settings2.py, and the corresponding value in settings.py replaced. The above just prints the value to the screen, you can easily just overwrite settings.py with the modified value, or output to a 3rd file which is actually read from.
Thank you so much. This looks exactly like the solution I need, however when I ran the first command and then I tried ot print the variable with &gt;echo $RES I got a blank line. Were you able to successfully extract the key from this file with that command ? https://github.com/TheDemocracyFoundation/epitome/blob/development/Epitome/settings.py
Thats a good idea. I was getting tired of writing `[Aa]` over and over again. &gt; getopts I know this exists, but I've been avoiding using it strictly because I like to at least try to solve issues like this on my own before going with "standard" solutions. It also gives me more fine grained control over *exactly* how the inputs parse. &gt; shopt Ive tried to avoid using this in functions since i dont want to assume something is set (or unset) but i also dont want to change the setting from what it is currently set at. This means i end up checking if it id set right, changing it if it isnt, recording if it was changed, running what I need to run, then (if needed) change it back. Though your comment made me realize i could write a function to largely automate this, so maybe i will do that. 
Have you heard of `tput`? (If you haven’t, you deserve a longer and more detailed reply than this, but I’m sorry, I’m just so incredibly tired of people hard-coding escape sequences instead of using `tput` or `terminfo`. Perhaps someone else will do a better job.)
There was a space missing before the equals sign, so this worked with that file: RES="$(awk -F ''\''' '$0~/SECRET_KEY =/ {print $2}' settings.py)" 
The problem is that everyone learn to color their prompt with that awful tutorial : [https://misc.flogisoft.com/bash/tip\_colors\_and\_formatting](https://misc.flogisoft.com/bash/tip_colors_and_formatting)
This is the best I've got so far: mac_targets=$(cat mac_targets.txt) var=$(echo 'and' for mac in $mac_targets do echo 'wlan.ta == '$mac' ||' | tr '\n' ' ' done) var=$(echo $var | rev | cut -c 3- | rev)
I'm sorry to suggest Perl all the time, but I just like playing around with it. I first came up with this here: $ perl -n0777E 'say join " || ", /(\S+)/g' testfile aa:aa:aa:aa:aa:aa || bb:bb:bb:bb:bb:bb || cc:cc:cc:cc:cc:cc That "testfile" I used is this: aa:aa:aa:aa:aa:aa bb:bb:bb:bb:bb:bb cc:cc:cc:cc:cc:cc The `-E` parameter introduces a line of Perl code. The `-n` is about reading files silently, printing nothing unless the Perl code decides to print stuff. The `-0777` parameter makes it load the whole file in one go instead of reading it line-by-line. About the Perl code: The `/(\S+)/g` is a regex pattern that finds sequences of "not space" characters. The `/g` at the end makes it return a list of all matches in the text. The `join " || ", ...`, turns the list into a line of text with that `" || "` text put between the list elements. The `say` is a command that prints a line of text. Now about adding that `wlan.ta = ` text, and adding the `and `, that sadly requires to introduce strange stuff, and it makes the code look like this: $ perl -n0777E 'say "and ", (join " || ", map { "wlan.ta = $_" } /(\S+)/g)' testfile and wlan.ta = aa:aa:aa:aa:aa:aa || wlan.ta = bb:bb:bb:bb:bb:bb || wlan.ta = cc:cc:cc:cc:cc:cc I'm pretty confident that this is also possible to do with 'awk', but I can't quite come up with an idea on how to do it. Perhaps wait for someone to show up and suggest some neat 'awk' code instead of this somewhat obscure Perl stuff.
I played around a bit more and came up with this, which seems neat: $ echo $(&lt; testfile) | sed -r 's/ +/ || /g; s/^/and /' and aa:aa:aa:aa:aa:aa || bb:bb:bb:bb:bb:bb || cc:cc:cc:cc:cc:cc That `echo $(&lt; testfile)` seems to make bash turn the file into a single line of text with spaces. The 'sed' command line then first uses a `s/search/replace/` command to turn the spaces into `" || "`. The `/g` flag is important to make it look for all spaces on the line, not just the first match. After that, it runs a second `s///` command that adds the `"and "` to the beginning of the line.
That seems really efficient, my use of sed is lacking. What is the logic behind echo $(&lt; testfile) causing that output?
Ok, I will update it to use tput. Thanks for the feedback!
I think this here shows what's happening: $ echo "$(&lt; testfile)" aa:aa:aa:aa:aa:aa bb:bb:bb:bb:bb:bb cc:cc:cc:cc:cc:cc $ echo $(&lt; testfile) aa:aa:aa:aa:aa:aa bb:bb:bb:bb:bb:bb cc:cc:cc:cc:cc:cc I feel like I don't fully grasp the rules of this, but I guess it's related to bash's "word splitting" behavior. The missing `"` seem really important and make bash reinterpret the file. After it reinterprets the file, it puts the result onto the command line for `echo ...`, turning all words in the file into a separate parameter for `echo`. The `echo` then prints a space between its parameters. About that `$(&lt; ... )`, that is basically the same as doing `$(cat ... )`, except the `cat` is an external program while that `$(&lt;` is a trick to make bash itself do the same job without an external program being needed. I think this does not work in a more basic /bin/sh, might only work in bash, so make sure your script is starting with `#!/bin/bash` (or use `cat` instead of `&lt;`).
You have a gratuitous `cat`. Instead, just grep the file. There's no need to read the entire file, and then read the output again. If it thinks `-i` is a `cat` flag, do you actually have more of a script written than you've posted here?
`paste` can get you most of the way there... something like: $ paste -d '' &lt;(for (( i=0;i&lt;count;i++ )); do echo " || wlan.ta == "; done) mac | paste -sd '' || wlan.ta == a:aa:aa:aa:aa:aa || wlan.ta == bb:bb:bb:bb:bb:bb || wlan.ta == cc:cc:cc:cc:cc:cc 
yw
Solved! ec2-user. My condolences for anyone who ends up on this page.
If you can swing it, jinja is a full templating engine and what Ansible uses; I'm imagining you opening the file and rendering the template into a new one. It may be simpler though to just use Python string formatting.
Now it works, it gets the correct key into the RES variable. However sadly it does not replace the value :(. For example if you have this file in your Home directory https://pastebin.com/4azkn4wu And this in your Desktop https://github.com/TheDemocracyFoundation/epitome/blob/development/Epitome/settings.py Does it work for you? Man if we can fix this it would help a lot. Thank you 
No i just have the following set to a key command using i3's config `bindsym $mod+shift+t exec "urxvt -e sh -c '$(xsel -p); $SHELL'"`
Ok, cool. Can you post in more detail about what you want to happen?
I would like selected text to get executed as a command in urxvt. Basically save a few steps form the usual copy, open terminal, paste, and execute when i see some text i would like to run in terminal. 
Middle mouse click can save you some part of the trouble
can you recommend some resources for getting started with perl?
I really appreciate the time you took to leave a comment, and I'm especially grateful for the links. Cheers!
Solved.. how?? 
solved. how?? 
The SSH user should have been `ec2-user` instead of `ec2user`
&gt; My condolences for anyone who ends up on this page. Ok, well why don't we make this constructive? [Babun](http://babun.github.io/).
https://www.howtogeek.com/109369/how-to-quickly-resize-convert-modify-images-from-the-linux-terminal/
Ok, I think I got it. To delete the "@2x" thing off their filenames there are several ways. What I'd do is: cd /folder/with/files/ for file in *; do prefix="${file%%@*}" suffix="${file##*.}" mv "${file}" "${prefix}.${suffix}" done It basically strips off the part before the first "@" (the name you want), and the part just after the last "." (the extension), and puts them up together, obviating the part after the first "@" (including it) and before the extension.
As DaftPump said, use ImageMagick. [Here](http://www.imagemagick.org/Usage/resize/) is how to do what you want.
You are better off asking the site operator for a hard link. Best case scenario, your wget or curl fails while you cycle through around a trillion random permutations, worse case you start downloading random stuff from URLs. You might be better off pursuing something like selenium 
The way it is setup is that only the valid permutation will actually return anything but a 404. I was trying to automate the process a bit since it's a pain in the neck to obtain the URL. Unfortunately, the site operator wants you to use their flash player for the URL instead, which I understand. They allow access if you go through the effort of finding the URL, but they make it hard to get. The 8 char string was instituted a few years ago when too many people were using the URL and bypassing their frontend player. Oh well, I can continue as is without the automation.
This worked perfect -- thank you so much!
TL;DR: find /a/path/with/files -type f -name '*.yml' -exec docker-compise -f {} pull \; `for var in $(subshell); do` is a bad pattern because it performs both glob expansion(!) and word-splitting on the output of the subshell. If you really do want a for-loop, it would be better to to use readarray/mapfile to safely read the output of the subshell in to an array variable, then loop over that: mapfile -t files &lt; &lt;(find /a/path/with/files/ -type f -name "*.yml") for file in "${files[@]}"; do docker-compose -f "$file" pull done That code splits the output of `find` on newlines. Which works great for 99% of filenames, but would break on filename that contain newlines. We can do better: split the output on `0x00` bytes (the one byte not allowable in filenames)! mapfile -t -d '' files &lt; &lt;(find /a/path/with/files/ -type f -name "*.yml" -print0) for file in "${files[@]}"; do docker-compose -f "$file" pull done Now, if you don't need a real loop (the command inside the loop is fairly simple), we have the `xargs` program to create the loop for you: find /a/path/with/files/ -type f -name "*.yml" | xargs docker-compose -f {} pull That gets rid of the glob-expansion of your version, but still has the space problems (xargs splits on whitespace). So let's tell xargs to split on newlines, to match find's output: find /a/path/with/files/ -type f -name "*.yml" | xargs -d '\n' docker-compose -f {} pull 
What you try to do is simply not possible with a 'for' loop (it actually is, but the trick for that is a bit obscure). You can have a loop read text output from another command like this instead: find /a/path/with/files/ -type f -name "*.yml" | while read -r file; do docker-compose -f "$file" pull done You can still use a 'for' loop if you let bash itself find the files through wildcards, and do not use the external find program. Bash can do a recursive search like this: shopt -s globstar # enable the special '**' wildcard feature for file in /a/path/with/files/**/*.yml; do docker-compose -f "$file" pull done If you don't need a recursive search through sub-folders, you can also just use this: for file in /a/path/with/files/*.yml; do ...; done About that 'while' loop earlier, it will have a strange problem you might sometimes run into where changes to variables on the right side of a `|` are not seen by the rest of the script. If you ever run into that problem, you have to turn things around like this: while read -r file; do docker-compose -f "$file" pull done &lt; &lt;( find /a/path/with/files/ -type f -name "*.yml" ) 
GNU `xargs` comes bundled with GNU `find`; if you can count on `find` being there, I'd say you can count on `xargs` being there. Plus, both are part of the POSIX specification. `;` usually is a bit of Bash syntax that separates "simple commands"--the same as newline usually does. But here, the `\\` escapes it, so that it is passed to `find` as a normal argument, rather than being treated as a bit of syntax. /u/galaktos' version accomplished this by wrapping the `;` in single-quotes. The quotes accomplished the same thing as the backslash. As for what the `;` actually does: With `find`, when using `-exec` to specify a command, you have to use either `+` or `;` to tell it when the command ends (and normal `find` arguments resume). It's kinda silly that you have to do that even when it's at the end of the `find` command, but that's the way it is. If you use `+` it tells `find` to replace `{}` with a *list* of files, and so it probably only runs the command only once, with the entire list (it may chunk the list run the command multiple times if there are thousands of files); while if you use `;` it tells find to replace `{}` with *one* filename, and run the command once for each file.
&gt; Very interesting, I didn’t know `mapfile` splits on null bytes if the delimiter is empty! Apparently `read` supports the same thing, even though I can’t find a reference for it in the documentation of either command. Yeah, it's not entirely clear. But the docs say "-d The first character of delim is used to terminate each input line, rather than newline."; if you interpret that as literal C (`delim[0]`) then you realize that the "first character of delim" is the null-terminator. &gt; (I suppose `''` and `$'\0'` are actually ^almost equivalent because of the way C strings work…) One takes 1 byte and the other takes 2, but it's impossible to distinguish between them in an args array in C.
&gt; One takes 1 byte and the other takes 2, but it's impossible to distinguish between them in an args array in C. Yes, `execve` seems to terminate each argument at the first null byte. I suppose it has to, because the new process is getting a copy of the argument data, not a part of the original process’ memory space.
 function get_my_thing() { curl "http://someserver.net/somepathher/$(date)${1}/restofpath" } Then however you get that 8 char string you can just: get_my_thing ABCDEFGH and you have your thing in your current working directory. You might need to change the formatting for the date command. Check out the date man pages.
This is only if you already know the 8 character string.
I think it's important to mention that mogrify replaces the file in question; do not run this command on the original file if it's the only copy you have unless you're okay with losing the original.
I've always wondered this. Thank you for the explanation.
I'm not entirely certain it's random, but it's seemingly random. Here are three of the most recent ones. pcdmnjq9 38a0t5sq yj4h6nq5
It's probably the last 8 digits of a hash of the date. Possibly salted. I can't check, cause you didn't give the dates.
All three of those are from the 19th. The "somepathhere" was different. I wasn't home yesterday to get yesterday's. I don't have a copy anymore of the ones from earlier in the week. I'll grab today's when I get back home this evening. 
* pngcrush * jpegoptim * gifsicle 
You will need multiple tools, but just write a bash script that detects the image type and applies whatever level of compression you want. pngcrush is good for starters. I did this same thing a few years ago. There is a Mac app called ImageOptim that I reverse engineered (it is basically an AppleScript wrapper), and I implemented a few more tools to make a Mac app droplet that did what I wanted. Google for the functionalities that you need and then write a script that incorporates them together into a single tool. 
You can use curl with tinypngs website, here is a [script](https://github.com/1ud0v1c/tinypng). Maybe adapt it to your needs or something...
You can use curl with tinypngs website, here is a [script](https://github.com/1ud0v1c/tinypng). Maybe adapt it to your needs or something...
I've used https://imageoptim.com before, which works both for mac and Linux (https://imageoptim.com/command-line.html). It uses many methods at the same time. But remember to resize your images first to fit your minimal requirements, you often gain the most by that.
I believe you could do something like a ``pwd | sed -e "s;\([^/]\)[^/]*/;\1/;g"`` command in your $PS1, this one keeps the `/` between folders but you can easily change that up.
Possibly there's a third way here. In my `$PS1` I have the name of the current directory. If I want to know the full path, I just run `pwd`. Some people like to change `cd` so that when you change a directory, it prints the path of that directory. Certainly, if I, as someone who is unfamiliar with your setup, were to sit at your terminal, I might find something like `ulsc/300` to be bizarre... Another option is a multi-line prompt a'la `oh-my-zsh` etc. You might like to investigate that...
As an alternative, did you ever try a prompt with line-breaks in it? I like that kind of look for the prompt just because it's visually easier to see the prompts when paging backwards through a terminal window. Try for example the following (in bash, no idea about mksh): PS1='\[\e[00m\]\n\[\e[01;32m\]\u@\h \[\e[01;33m\]\w\[\e[00m\]\n\$ ' For what you ask for exactly, in bash you can put code into your PS1 through using `$(...)`. You just have to make sure you use `'...'` quotes when adding so that it won't get interpreted the moment you set PS1. There's also a PROMPT_COMMAND variable that can contain a command line that will get executed before each prompt gets printed. You can write a function that generates a new PS1, then add that function name to PROMPT_COMMAND. Last thing, there's a PROMPT_DIRTRIM variable in bash. It trims the length of that `\w` in PS1, but it does not look at number of characters, it looks at the number of directory names in the path. If you use `PROMPT_DIRTRIM=2`, those example paths you mentioned would turn into this here: .../home/pthfdr .../cpufreq/policy0
I'd try to find out where that Flash file gets the URL from. Maybe it accesses a URL that serves a JSON document or similar with the paths in it. You script could then download that document itself and get the URLs from there. About how to find what the Flash is doing, maybe download the file and look at it with `strings filename.swf | less`, and maybe you'll see something interesting. Or perhaps you can find out what URLs it accesses by running it while tracking what it's doing online with Wireshark. I think something might also be built into Google Chrome's web developer tools to find out what URLs it accesses. I don't know if it can track what the Flash Player is doing, but you could just take a look at it.
I hadn't thought about downloading the swf and checking that. I typically have to use Wireshark to get the URL, though sometimes I can get it via Chrome (I have to be fast opening the dev tools). 
However, you don't want to call tput every time the prompt gets printed :) so you get into the slightly hairy quoting of `$PS1`, which is not necessary for an unrelated example.
Use a var like `color_black=$(tput setaf 0)` and voilà. It's way better than Escape Code.
&gt; The thing is just, these particular codes seem to be used by everyone. It seems to be some sort of standard. No. It isn't. On the contrary, "Because there's a large number of different terminal control languages, usually a system has an intermediate communication layer. The real codes are looked up in a database for the currently detected terminal type and you give standardized requests to an API or (from the shell) to a command. One of these commands is tput. Tput accepts a set of acronyms called capability names and any parameters, if appropriate, then looks up the correct escape sequences for the detected terminal in the terminfo database and prints the correct codes (the terminal hopefully understands). " -- http://wiki.bash-hackers.org/scripting/terminalcodes
The escape codes Just Work (tm) for me across all my systems, whereas I could never get tput to. Don't know why, but half the systems I copy my prompt to are minimal server installs that I don't have root on, and I also use it across both BSD and Linux, so it might be that tput doesn't exist on some of them.
&gt; The escape codes Just Work (tm) for me across all my systems, whereas I could never get tput to. `terminfo` and `tput` are on UNIX system since System V... If it doesn't work on your system then there's something wrong with it. `tput` is best pratice. It is the standard way of doing it. That doesn't mean other ways don't work. It only means that if someone else don't use the same setup as yours your code have more change to run flawlessly.
You might like to investigate `$TERM` So, for example, the `xterm` terminfo entry on Solaris is wrong. It specifies a non-standard window size and no colours, and so `tput` based `PS1`'s do not work as standard on Solaris. Escape codes are, by default, the most portable option. $ echo $TERM xterm $ tput colors -1 BUT if you simply add something like this to your `.bashrc`: if [[ $(uname) = "SunOS" ]]; then for termType in xterm-256color xterm-color xtermc dtterm xterm; do export TERM="${termType}" if tput colors 2&gt;&amp;1 | grep "unknown terminal" &gt;/dev/null 2&gt;&amp;1; then continue else break fi done # Any other Solaris related tweaks here Then you should find that Solaris starts behaving with `tput` just a little bit more: $ tput colors 8 Perhaps you'll have similar findings for BSD/AIX/HP-UX/Whatever-else-you-happen-across... FWIW I have a FreeBSD-10 VM here and it reports: $ echo $TERM xterm-256color $ tput colors 237 
First declare CMD lowercase var, read only one char &amp;nbsp; declare -l CMD while : do read -c 1 -p 'Do you want to install Terminator? &lt;y/n&gt; ' CMD if [ "$CMD" == 'y' ] ; then echo 'Installing Terminator...' sudo apt-get install terminator -y &amp;&gt;/dev/null echo 'Done' break elif [ "$CMD" == 'n' ]; then break else echo 'Please input y or n' fi done #Rest of script
This also doesn't work. filter="' and !(wlan.ssid == \"\")'" tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'$filter' and wlan.tag.number == 0'
I'll give more of the script for anyone who wants to try figure this out. interface='wlan0' sniff_probes='true' target_macs='true' mac_list='mac-list.txt' sniff() { if [[ $sniff_probes = 'true' ]] then tshark -i $interface -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and !(wlan.ssid == "")'' and wlan.tag.number == 0'$macs -T fields -e wlan.ta -e wlan_radio.channel -e wlan.ssid 2&gt;/dev/null &amp; fi } filter() { if [[ $target_macs = 'true' ]] then macs=$(echo $(&lt; $mac_list) | sed -r 's/ +/ || /g; s/^/and /') macs="' $macs'" fi } filter sniff mac-list.txt cat mac-list.txt aa:aa:aa:aa:aa:aa bb:bb:bb:bb:bb:bb $macs echo $macs 'and wlan.ta == b4:9c:df:1b:ec:bc || wlan.ta == ac:bc:32:90:09:1f' If I just type it all out on the command line it works. tshark -i $interface -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and !(wlan.ssid == "")'' and wlan.tag.number == 0''and wlan.ta == b4:9c:df:1b:ec:bc || wlan.ta == ac:bc:32:90:09:1f' -T fields -e wlan.ta -e wlan_radio.channel -e wlan.ssid However the above script does not.
It will work like this: filter=' and !(wlan.ssid == "")' And where you want to insert it in that one parameter, do this here: 'wlan.fc.type_subtype == 0x0004'"$filter"' and wlan.tag.number == 0' That parameter is then getting built out of three parts: 'text here' "$filter" 'another text here' Bash will fuse those three parts together into one word when they touch directly (no spaces between them). I have this function in my .bashrc to experiment with problems like this: args () { printf "%d args:" $# printf " &lt;%s&gt;" "$@" echo } Here's an experiment with your original idea at the command line using that 'args' function: $ filter=" and !(wlan.ssid == "")" $ args tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'$filter' and wlan.tag.number == 0' 11 args: &lt;tshark&gt; &lt;-i&gt; &lt;wlan0&gt; &lt;-I&gt; &lt;-n&gt; &lt;-Y&gt; &lt;wlan.fc.type_subtype == 0x0004&gt; &lt;and&gt; &lt;!(wlan.ssid&gt; &lt;==&gt; &lt;) and wlan.tag.number == 0&gt; That's hard to read here because it's a super long line. Here's just that one parameter that causes the problem, and it seems it turned into several parameters: &lt;wlan.fc.type_subtype == 0x0004&gt; &lt;and&gt; &lt;!(wlan.ssid&gt; &lt;==&gt; &lt;) and wlan.tag.number == 0&gt; This problem is fixed by writing `"$filter"` instead of `$filter`. Then there's another problem that happened earlier, the $filter doesn't contain the `""` you wanted inside it. See here: $ echo "$filter" and !(wlan.ssid == ) That's fixed by using `'` when setting it, instead of the `"` you used. What happened there for you is, bash saw something like this: filter="abc""def" And turned it into: filter=abcdef
Doesn't work. Try it out on a Mac or Kali machine. This script fails. filter=' and !(wlan.ssid == "")' tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'$filter' and wlan.tag.number == 0' This command works. tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and !(wlan.ssid == "")'' and wlan.tag.number == 0'
You forgot using `"` where you insert `$filter` into that one parameter. I mean like this: tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'"$filter"' and wlan.tag.number == 0' The missing `"` is what's splitting up the parameter into several words.
I got ahead of myself, you're right, that example does with with double quotes. However, my larger script in my other post doesn't work, even with double quotes around the $mac variable.
I think here you are making a mistake where you add `'` into the $macs variable. Those `'` will show up inside the parameter that the tshark program will see. I mean it will see something like this for that one parameter: wlan.fc.type_subtype == 0x0004 and !(wlan.ssid == "") and wlan.tag.number == 0' and wlan.ta == b4:9c:df:1b:ec:bc || wlan.ta == ac:bc:32:90:09:1f' But it probably wants to see this: wlan.fc.type_subtype == 0x0004 and !(wlan.ssid == "") and wlan.tag.number == 0 and wlan.ta == b4:9c:df:1b:ec:bc || wlan.ta == ac:bc:32:90:09:1f I think you will want to add just one space to the front of $macs, like so: macs=" $macs" And then later where you actually use $macs, you have to again use `"`, meaning `"$macs"` and not `$macs`, same as with that $filter problem you had.
I replied to you about that problem in some other place in the thread, please check that out.
The quotes you tried to add into the $macs variable are seen as normal text characters by bash. They do not have the special meaning that quotes have in bash code. Did you get things to work? If it doesn't work, show how your whole script looks like right now.
I won't paste the whole script because it'll be a mess, this should be enough to illustrate it though. filter=$(echo $(&lt; mac-list.txt) | sed -r 's/ +/ || wlan.ta == /g; s/^/and wlan.ta == /') filter="' $filter'" shark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and wlan.tag.number == 0'"$filter" I'm not sure where to go from here, parsing the list of mac addresses is being the problem.
It needs to be like this: filter=$(echo $(&lt; mac-list.txt) | sed -r 's/ +/ || wlan.ta == /g; s/^/and wlan.ta == /') filter=" $filter" shark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and wlan.tag.number == 0'"$filter" Or you can add that one space character in front of the "and" through changing the 'sed' command: filter=$(echo $(&lt; mac-list.txt) | sed -r 's/ +/ || wlan.ta == /g; s/^/ and wlan.ta == /') shark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and wlan.tag.number == 0'"$filter" 
I've tried to re-do the whole sed one-liner, still doesn't work though. filter=$(cat mac-list.txt | tr '\n' ' ' | sed 's/.$/\x27/' | sed 's/ / || wlan.ta == /g' | sed 's/^/\x27 and wlan.ta == /') echo $filter tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and wlan.tag.number == 0'"$filter" ./test.sh ' and wlan.ta == aa:aa:aa:aa:aa:aa || wlan.ta == bb:bb:bb:bb:bb:bb' tshark: Lua: Error during loading: tshark: The character constant "' and wlan.ta == aa:aa:aa:aa:aa:aa || wlan.ta == bb:bb:bb:bb:bb:bb'" was unexpected in this context. The $filter variable looks exactly like what I want it to be, but it just doesn't work.
Can you show an example tshark command line that definitely works?
I may have figure it out. I did a diff between the variable entered manually and the variable when parsing the mac list. 1c1 &lt; and wlan.ta == aa:aa:aa:aa:aa:aa || wlan.ta == bb:bb:bb:bb:bb:bb --- &gt; ' and wlan.ta == aa:aa:aa:aa:aa:aa || wlan.ta == bb:bb:bb:bb:bb:bb' The first is from the manual entered variable. It's not taking the single quotes as literal characters. Therefore this appears to work: filter=$(cat mac-list.txt | tr '\n' ' ' | sed 's/.$//' | sed 's/ / || wlan.ta == /g' | sed 's/^/ and wlan.ta == /') tshark -i wlan0 -I -n -Y 'wlan.fc.type_subtype == 0x0004'' and wlan.tag.number == 0'"$filter"
Thanks for your help.
You probably have carriage returns in your `emails.txt` file that are messing with you. I'd suggest running something like `dos2unix` and see if that clears up your problem. Also, `for val in $(cat file)` is a pretty risky construct. Instead, you might want to read [this FAQ](https://mywiki.wooledge.org/DontReadLinesWithFor) for some considerations and other options
Take your script to shellcheck. (Sidebar link).
Thank you. The dos2unix did it. 
I agree with beatle42. Sometimes MS programs will add extra characters, like ^M, to the end of a line in a text file. Either use the suggest dos2linux on the text file, or you can open the file in vim and use the command :set ff=unix, then save the file.
As the error message says, the `done` at the end of your script is causing the error. `done` is not a command. It's a keyword that denotes the end of a block. It is used in loops like: while SomeCommands; do SomeOtherCommands done which executes SomeOtherCommands as long as the last of SomeCommands returns a true/success/0 status. There are a few other loop constructs with `do`...`done` as well. 
Well, you have don't have anything "done" refers to. Like a "do" statement. 
So I can get rid of the done part? Or should I just put the whole script inside a do statement?
"do ... done" cannot be used alone. They are always part of a "for" or "while" or "until" loop.
It'll get easier with time. Just remove the `done` for now.
Thanks for the reply. I never really understood why you would use perl until I saw your post and suddenly it all made sense. I'll have to give it a shot. 
You don't use a 'done' with a function definition. Also, you have to call the function at some point. #!/bin/bash function menu { echo "===========================================" echo "Menu by me" echo "===========================================" echo "" echo "Please choose from the options below using the corresponding numbers:" echo "" echo "1: Show the manual path" echo "2: Add an item to the manual path" echo "3: Set the manual path to /man" echo "4: Enter a filename" echo "5: Edit the file" echo "6: Display the head of the file" echo "7: Display the tail of the file" echo "8: Display a long list of processes" echo "9: Display the number of processes running for a specific user" echo "0: Quit the program" echo "" echo "Enter your selection here:" read answer case "$answer" in 1) echo "no" ;; esac } menu
It might be unrelated to your problem, but I think all those `echo` could be replaced by a [heredoc](http://www.tldp.org/LDP/abs/html/here-docs.html): cat &lt;&lt;- EOF =========================================== Menu by me =========================================== Please choose from the options below using the corresponding numbers: 1: Show the manual path 2: Add an item to the manual path 3: Set the manual path to /man 4: Enter a filename 5: Edit the file 6: Display the head of the file 7: Display the tail of the file 8: Display a long list of processes 9: Display the number of processes running for a specific user 0: Quit the program Enter your selection here: EOF As for your problem, you left a `done` alone at the end of your script.
Looking through the traffic, I found some interesting URLs publicly accessible that contained the code.
When you do `read answer`, you create a variable named `$answer` with the line that was typed saved in it.
so the variable creates itself? How do I then manipulate that to add that answer to the manual path? I'm ignorant about the syntax more than anything else, really.
You can experiment with things at the command line, you don't have to edit and run a script all the time. Try using `read` like this, directly at the command line: $ read answer &lt;-- type something here $ echo "$answer" &lt;-- will print what you typed earlier The syntax for setting a variable is this: name=word Again, you can play around with it at the command line. Now you might want to first experiment with how that "word" part works. You can use `echo` to print words. A special thing about bash is, if two words touch each other without a space, they get fused into one word, there's no "+" operator or anything to add strings together to form a new string. You can do this to add two words together: echo "$variable""$another_variable" Or you can do: echo "$variable$another_variable" The `"` quotes protect space characters that might be in the variable values. It's important to understand that the syntax for setting a variable is exactly this: name=word The following is wrong, it has to be exactly one word, and that's why you need `"` quotes if there's spaces in your values: name=word word word I don't know how $MANPATH works. Are you supposed to add `:` character between the paths? That $MANPATH thing is empty here for me.
You can't modify a variable from within a script that is run in a subshell. Typing `./myScript` generates a subshell. I'm not sure why you want to modify MANPATH on the fly, it's typically done in `~/.bashrc`. What you can do, is write a script to write to ~/.basrc or /etc/profile, and then reload the terminal. echo "MANPATH=\${MANPATH}:${NEW_PATH}" &gt;&gt; ~/.bashrc 
I adjusted my above reply, because while I know of no way to do it, I didn't mean to show that level of certainty. I was almost sure it created a subshell, but when I tried: #!/bin/bash echo ${BASH_SUBSHELL} I got `0` back. A hacky workaround that I know to make the variable immediately usable is to source the script instead of running it. myScript.sh: #!/bin/bash MANPATH=${MANPATH}:/new/path and then source it instead of running it. source myScript
I tried it with and without and the wrapping still occurs. 
It's a little overwhelming when you hadn't touched Linux before, let alone scripted in a language you've never heard of, until two weeks ago :D
You need to share what you tried exactly. You might have made different mistakes each time you tried something.
That's odd, I tried this on mine and removing **all** the \\[ and \\] on the last line, i.e. export PS1="$(__host) $(__dir)$(__git_status) $(__arrow) " seemed to fix the wrapping in my terminal. I had to create a dummy function __parse_git_branch() to make it work, since that wasn't included in your example, but that should make no difference, unless your __parse_git_branch() has an error in escaping the prompt length calculation. function __parse_git_branch() { printf "dummy"; } function __host() { printf "\[\e[0;92m\]\h\[\e[0m\]"; } function __dir() { printf "\[\e[1;34m\]\w\[\e[0m\]"; } function __git_status() { printf "\[\e[0;33m\]\`__parse_git_branch\`\[\e[0m\]"; } function __arrow() { printf "\[\e[1;34m\]▸\[\e[0m\]"; } export PS1="$(__host) $(__dir)$(__git_status) $(__arrow) " 
Yeah you're probably right. I was changing lots of stuff. 
Yep that worked! Thank you! :)
Happy to hear that :) 
 $ echo "Anharm.Pot. : cm-1 = -61.13847 ; Kcal/mol = -0.175 ; KJ/mol = -0.731" | awk '{print $5}' -61.13847 
I used your script as a template (In combination with part of one of my scripts) to mark whether my current svn repository is up-to-date or not with a green check or red x. Just wanted to share: #!/bin/bash BOLD="\e[1m" NO_BOLD="\e[21m" RED="\e[31m" GREEN="\e[32m" BLUE="\e[34m" SYM_BALLOT_HEAVY="✘" SYM_CHECK_HEAVY="✔" #Returns 0 when completely up-to-date #Returns 1 when all files are up-to-date, but svn not at latest revision. (Files in other branches/directories may have changed) #Returns 2 when some files are out-of-date. function svnUpToDate() { # Example use: # DIR="PATH_TO_LOCAL_SVN" # if ! svnUpToDate ${DIR}; then # { # svn up ${DIR} # } # fi # # Recommend executing this first: # # ssh-keygen -t rsa # ssh-copy-id user@svn.server.ip.address if ! isAtLeastVersion "${SVN_VERSION}" "1.9.0"; then { echo "svn version 1.9.0+ is required for this command. Your version is ${SVN_VERSION}" return 1 } fi if [ ${#} -eq 1 ]; then { DIR=${1} } else { DIR="." } fi URL=$( svn info ${DIR} --show-item=url ) CURRENT_VERSION=$( svn info ${DIR} --show-item=revision ) LATEST_VERSION=$( svn info ${URL} --show-item=revision ) LOCAL_LAST_CHANGE=$( svn info ${DIR} --show-item=last-changed-revision ) REMOTE_LAST_CHANGE=$( svn info ${URL} --show-item=last-changed-revision ) #echo "[${CURRENT_VERSION}]" == "[${LATEST_VERSION}]" if [[ "${CURRENT_VERSION}" == "${LATEST_VERSION}" ]]; then { #echo "Up-to-Date" return 0 } elif [[ "${LOCAL_LAST_CHANGE}" == "${REMOTE_LAST_CHANGE}" ]]; then { #echo "Up-to-Date with latest change for branch/tag/trunk" return 1 } else { #echo "Out-of-Date with records from all branches" return 2 } fi } function __svn_up_to_date() { if svn info ${PWD} &amp;&gt; /dev/null; then { if svnUpToDate &amp;&gt; /dev/null ;then { #printf "\[\e[0;33m\]yes\[\e[0m\]"; printf "[${GREEN}${SYM_CHECK_HEAVY}"; } else { printf "[${RED}${SYM_BALLOT_HEAVY}"; } fi printf "${TEXT_DEFAULT}]"; } fi } function __host() { printf "${LIGHT_GREEN}\h"; } function __dir() { printf "${BOLD}${BLUE}\w${TEXT_DEFAULT}"; } function __arrow() { printf "${BOLD}${BLUE}▸"; } export PS1="$(__host):$(__dir)\$(__svn_up_to_date)$(__arrow)${TEXT_DEFAULT} " 
Seems you're a crossposting spammer... 
try adding a space at the front of the regex, like ' [-+]?...'
I made a file and added some other rows with other data: $ cat file.txt Anharm.Pot. : cm-1 = -61.13847 ; Kcal/mol = -0.175 ; KJ/mol = -0.731 Foozap.Bar : cm-1 = 34 ; Kcal/mol = -99 ; KJ/mol = 34 Foonad.Baz : cm-1 = 24 ; Kcal/mol = -1 ; KJ/mol = .42 Fog.Bargog : cm-1 =75 ; Kcal/mol = -21 ; KJ/mol = 3.4 $ cat file.txt | grep ^Anharm.Pot Anharm.Pot. : cm-1 = -61.13847 ; Kcal/mol = -0.175 ; KJ/mol = -0.731 $ cat file.txt | grep ^Anharm.Pot | cut -f 2 -d: cm-1 = -61.13847 ; Kcal/mol = -0.175 ; KJ/mol = -0.731 $ cat file.txt | grep ^Anharm.Pot | cut -f 2 -d: | tr ";" "\n" cm-1 = -61.13847 Kcal/mol = -0.175 KJ/mol = -0.731 $ cat file.txt | grep ^Anharm.Pot | cut -f 2 -d: | tr ";" "\n" | grep "^ cm-1" cm-1 = -61.13847 $ cat file.txt | grep ^Anharm.Pot | cut -f 2 -d: | tr ";" "\n" | grep "^ cm-1" | cut -f 2 -d= -61.13847 
&gt; The numbers can change but the formatting doesn't. Maybe it's because I'm old, but I never assume formatting won't change somewhere down the road and break stuff (fields might get rearranged or new fields added). So I'd suggest extracting the `cm-1 = -61.13847 ;` part with `grep -oE "cm-1 = [^;]+ ;"` and then grab the value with `cut -d\ -f3`, `cut -d= -f2|tr -d ' ;'`, or `grep -oE "= [-\.0-9]+"|tr -d ' ='` (you get the picture) 
This would be my approach. One thing I would change is the last one to `| awk '{print $NF}'` and you won't have the proceeding whitespace. 
Actually, you can replace the `cat|grep|cut` with one call to `awk` `awk -F ':' '/^Anharm.Pot/{print $2}' file.txt | tr ";" "\n" | grep "^ cm-1" | awk '{print $NF}'` I'll tag out now so that someone else can keep reducing this to a single `awk` one-liner :)
I &lt;3 U
Oh nice! I am terrible with `awk` so I just stick with the basics. I like your first version better than your edit version though since you can't replace the "cm-1" with a variable. Here's another go from me: awk -F ':' '/^Anharm.Pot/{print $2}' file.txt | grep -o "cm-1[ ]*=[ ]*[-]*[0-9]*.[0-9]*" | awk '{print $NF}' Which can be made more general with: NAME=Anharm.Pot FIELD="cm-1" FILE="file.txt" awk -F ':' "/^${NAME}/{print $2}" ${FILE} | grep -o "${FIELD}[ ]*=[ ]*[-]*[0-9]*.[0-9]*" | awk '{print $NF}'
 #!/bin/sh for j in {1..$y}; do # we loop as many times as there are lines (Y axis) for i in {1..$x}; do # columns printf '%s' "1" # missing logic here # no \n done printf '\n' done
Just a 3x3 grid of random numbers? Yes, it's possible a for loop inside a for loop would be simplest, but a number of other ways could be utilized as well. Without knowing what your output specifically would be, when given an input, and how the two are related, its difficult to get more specific.
Basically if the user says “ 6 columns and 2 rows “ it would print that 
If the users says 6 columns and 2 rows, it prints what? What is this for, what are you actually trying to achieve? 
2 rows of 6 numbers 
What 6 numbers specifically, or are they random? We would need to know exactly what we are outputting, be it a random value, or is it based on something related to the number of columns and rows. This is starting to sound suspiciously like homework.
I'm 28 learning linux commandline, not homework just trying to learn things. the numbers would be random only thing it would ask is " how many rows do you want" read x ... " how many columns do you want" read y... then print that out . 
`$2` is bash syntax. You can use bash to interpret bash syntax, for example via the eval builtin command. You must implement the interpreter yourself if you are concerned about the security implications of using eval and only want a subset of bash syntax to be interpreted.
Two months ago you said you were taking a class called "operating systems" that had given you an assignment in bash. Did you finish that course already?
If you're trying to print a grid of numbers, the user above gave you a good example. On the line: printf '%s' "1" You would replace "1" with whatever number you wanted to print in the grid, in the double quotes. The command is printf, and you are passing it 2 arguments. The first argument is formatting. You can look up printf formatting for more types of values. '%s' means the value being printed is a string. "1" is the string you are telling it to print.
Yes it’s summer now, and I wanna make sure I keep using the stuff I was taught. So if your goal is to just try and “expose” me then just please stop posting here. If I don’t keep practicing more advance things then I’ll never get any better by learning from experienced people. 
Look through OP's post history to see what they're trying to achieve.
What are you trying to do?
I am not here to post an answer, but I am detecting a lot of UUOC’s in this thread. $ echo bar &gt; bar.txt $ foo=$(&lt;bar.txt) $ echo $foo bar No need to use cat when you can use faster bash built-ins. 
Holy crap, I just read though that. He expects people he is asking for help to "follow instructions". He doesn't want to modify the tmux configuration file because it's somehow not portable? He doesn't want to read the manual. I mean, I understand it's not always intuitive, but to not even want to try reading it at all? He then settles on a 3rd party solution that is definitely not as portable as a configuration file, and guess what that 3rd party solution does? It modifies a supposedly non-portable tmux configuration file... My god that was infuriating to read.
The example I'm giving is an extremely dumbed down version of what I'm working on. Basically, out-of-the-box, an RTMP streaming script I made called "StreamPi" (https://www.bitchute.com/video/2yWzuFMUaBra) is used as "/path/to/StreamPi $1." However, I added an overlay image option so now by default it can be used as "/path/to/StreamPi $1 $2." The $1 part is the rtmp url or out.flv and the $2 part is the path to the overlay image. I also have a Settings script that uses a dialog form to change default settings instead of asking people to open the script to do it. If no one edits anything, I want the out-of-the-box way to work with $1 and $2 as normal. I think I've actually figured something out using if then statements, but it kind of doubled the size of my script because of "if-then" for all cases involving $1 and $2. However, uncompressed, the project is only 2.6MB and less then 1MB if tar.gz. I also added a "Podcast Mode" for streaming from TTY in which you can either use your webcam with or without an overlay or an image with audio. I also have a "Command Center" that you launch inside of tmux to show a translator that sort of doubles as a speech synthesizer depending on how you use it, whatever IRC cli client it can find, whatever cli music player it can find, choose between alsamixer and pamixer (builds included for pamixer so no install necessary), nano for notes. And in the very middle, it will use either vlc or mpv to play an ascii version of your stream (no audio) for monitoring. If no X is detected as running and are runlevel 5, it will display the Command Center on one screen and the stream (no audio) on the other like if you were using a laptop hooked up to a TV. I've got a lot of check and balances going on, which is why I was hoping there was something dumb I just wasn't seeing. But it's find because when I'm done, I'm putting it on GitHub and maybe people will have suggestions. I'm about 90% of the way there. I like to test things for about a week or two and then I'll make a video showing the updates.
Down on rfelsburg's comment thread, OP stated that the numbers are random. Since it's only a decimal digit, the string is `"$((RANDOM%10))"`.
It is not possible to pass an array on the command line. You can only pass a list of arguments: ./download-data "state_code" "county_code" Note that the `$(`…`)` syntax denotes a [command substitution](http://wiki.bash-hackers.org/syntax/expansion/cmdsubst). It has nothing to do with arrays. This explains your 'command not found' error. 
I agree, this does not seem possible.
You can pass arrays in, but it isn't super intuitive. https://stackoverflow.com/questions/1063347/passing-arrays-as-parameters-in-bash 
You should reorganize your data into something that works as a single stream of text, then write into standard input of the script. This will have no limitation for length like you might later run into if you use arguments. If that's not easily possible, you might want to start thinking about switching to a different scripting language like Python or Perl or Ruby. In those you would use a library to do the reorganization into a stream of text. The keyword for searching about this is "serialize data structure". There's native methods that are unique to the language or you could use a format where there's libraries for all like JSON for example.
If you are using bash 4.3 or later you can indirectly reference variables with via `declare -n` which is a nifty way to pass arrays to functions. This means you can define an array and just pass the name of the array to the function. In this example I pass the array `${numberarray}` to the function `sum`: sum() { declare -n _inputarray=${1} local _result _number for _number in "${_inputarray[@]}"; do (( _result += _number )) done printf '[+] Sum: %d \n' "${_result}" } numberarray=(1 2 3 4 5) sum numberarray 
You can create a list of numbers like this: {1..3} You can experiment with it at the command line using `echo`. I mean, type the following and see what happens: echo {1..3} You can repeat a piece of code using the elements in this list with the `for` keyword. You can read about it like this: help for About what that stuff there wants to say is, here's a concrete example that you can type a the command line to see what will happen: for x in {1..3}; do date +'%Y-%m-%d' -d "$x days ago"; done The `for` puts one of the list elements into a variable `$x`, then runs the commands that are between the `do` and `done` words, then repeats until it is done with the list. Inside the `do ...; done` block, you have access to the `$x` and can use it in your commands. In a script, you would add line-breaks for easier reading. It would then look like this: for x in {1..3}; do date +'%Y-%m-%d' -d "$x days ago" done Next thing you need to know is, that `{1..3}` feature sadly only works with literal values. You can't use a variable in it. I mean, the following does not work: start=1 end=3 echo {$start..$end} # does not work It will print a text `{1..3}`, not a list. If your start or end points are in variables, you have to do this: start=1 end=3 for (( x = start; x &lt;= end; x++ )); do # commands here using $x done Or this: start=1 end=3 for x in $(seq $start $end); do ... done Another fun thing you might want to know, you can do stuff like this: $ echo a{1..3} a1 a2 a3 $ echo {a..c}{1..3} a1 a2 a3 b1 b2 b3 c1 c2 c3 $ echo test_{red,green,blue} test_red test_green test_blue 
First, a note about including code in posts. Indent each line 4 spaces to have a properly formatted listing. There are several problems here: for i in grep "procedure " program.txt| sed 's/.*procedure \([a-zA-Z0-9]*\) .*/\1/'; do apare=grep -c $i program.txt apare1=$((apare-1)) linie=grep -n $i program.txt | cut -d: -f1 | tail -n $apare1 echo "Functia: $i apare de $((apare=apare-1)) ori." echo "Este apelata pe liniile: $linie" done First, the do and done keywords require a semicolon or a new line separating them from the if condition and the loop body. If, do, and done are separate internal commands to bash. If you type help for at the prompt, the help returned will be: for: for NAME [in WORDS ... ] ; do COMMANDS; done Notice the semicolons separating for, do, and done from everything between them. Your assignment to the apare and linie variables should be capturing the output of the grep command on the right-hand side of the assignments, but you failed to use either backticks \`grep...\` or the preferred $(grep...) syntax for doing so. for example: count=$(grep -c "$i" program.txt) Also, when you use a variable, quote it: "$i" rather than $i. This will prevent unwanted word splitting.
Will 3 always be 3? Or are you trying to account for a variable number there? Like it could be 7? Then you'd want to run that day back command 7 times? If that's the case you want a loop were that veriable is your counter. Only right that line of code once and repeat it for every increment in "3" or whatever number. I don't know if that's what you want so I won't get into elaborating more but if that is what you want and you still more of an idea of how to pull that off in bash lemme know.
[This is my go-to link for understanding dotfile loading order.](https://shreevatsa.wordpress.com/2008/03/30/zshbash-startup-files-loading-order-bashrc-zshrc-etc/) You might be better off to figure out why `steam` is upset. What happens when you try to launch it from the command line?
This is the log printed when I run `steam` and `oh-my-bash` and `fzf` are in ~/.bashrc` before login. ---- Running Steam on manjarolinux 17.1.8 64-bit STEAM_RUNTIME is enabled automatically Pins up-to-date! Installing breakpad exception handler for appid(steam)/version(1522709999) free(): invalid pointer [2018-04-27 07:18:30] Startup - updater built Apr 2 2018 15:23:43 crash_20180427071830_1.dmp[1888]: Uploading dump (out-of-process) /tmp/dumps/crash_20180427071830_1.dmp /home/coolkarni/.local/share/Steam/steam.sh: line 927: 1885 Aborted (core dumped) $STEAM_DEBUGGER "$STEAMROOT/$STEAMEXEPATH" "$@" crash_20180427071830_1.dmp[1888]: Finished uploading minidump (out-of-process): success = yes crash_20180427071830_1.dmp[1888]: response: CrashID=bp-c2c7f7b2-f0bd-4980-89c4-80a372180426 crash_20180427071830_1.dmp[1888]: file ''/tmp/dumps/crash_20180427071830_1.dmp'', upload yes: ''CrashID=bp-c2c7f7b2-f0bd-4980-89c4-80a372180426'' ---- Interestingly. I renamed my `bashrc` to `~/.bashrc_old`. So now my `bashrc` is empty. I rebooted. then in terminal emulator did `source ~/.bashrc_old` then `steam`. This did not cause any issue. So the issue happens only if `oh-my-bash` and `fzf` are already in `~/.bashrc` on login.
ok, so restore `~/.bashrc` and comment out the `fzf` and `oh-my-bash` lines and then try again. Then add one at a time, retrying each time. You've got the start of some process-of-elimination going, may as well continue down that path.
We really need more information about what you're trying to achieve, but to give a literal answer. &gt; How do I turn a number, 3, into 1 2 3 x=3 echo $(seq 1 $x)
All right. Seems that's the only option. I was hoping for quick fix. But looks like I will have to debug through it. `fzf` is definitely not the issue. Will have to check for `oh-my-bash`. I will post my findings here later for completeness.
Nice. Thanks! I will give it a try.
 sed 's/.*cm-1 *= *\([-+0-9.]*\).*/\1/g' This also works if there are no spaces around the `=`. Maybe OP should include `,` in case the numbers contain thousands separators. Explanation: .* Means the regular expression matches any character. cm-1 *= * Matches `cm-1` and the equal sign with any number of spaces around it. \([-+0-9.]*\) Matches any character that is inside the square brackets. The parentheses are a "capturing group", everything inside of it will be accessible with `\1` in the replacement string. We first match the entire line, so that it will be completely removed, then we capture a number after `cm-1` and an equal sing. In the replacement pattern we access the captured group with `\1`. 
&gt; The numbers can change but the formatting doesn't. Maybe it's because I'm old, but I never assume formatting won't change somewhere down the road and break stuff (fields might get rearranged or new fields added). and then grab the value with `cut -d\ -f3`, `cut -d= -f2|tr -d ' ;'`, or `grep -oE "= [-\.0-9]+"|tr -d ' ='` (you get the picture) --- ^^reddit ^^sedbot ^^| ^^[info](https://github.com/ndri/reddit-sedbot)
What the fuck. Noone exposed you. If you want to practice more advanced things, read a tutorial or something and write some useful scripts.
`qwer` is easier to type than `^D`?
Well TIL. I never knew that existed. 
Use `&lt;&lt; 'EOF'` instead of `&lt;&lt; EOF`. Why are you using "sudo"? That shouldn't be needed for your user's own .bashrc file.
Thank you! Any chance of an explanation of the different parts of the 2nd line? At the very least could you at least tell me what I can google to learn more about this?
awesome thanks, and you're right about the sudo thing, I've fixed it.
&gt; This will cause bash to exit if you enter any invalid command No it won't. That handler function is executed in the subshell process that was already forked to be replaced by the external command that turned out not to exist, so it will only exit that subshell. (The manual does not point this out clearly, which I reported as a bug that has been fixed for bash 5.0.)
If the code is going to be reused you can make it into a function. It looks like really the only think changing would be the executable. So make that an argument.
Oh wow, you're right. I confused myself by testing it in a subshell.
&gt; bash 5.0 Is that going to be the next major bash release?
The version number of the current git code is 5.0.0-alpha, so my guess is yes.
Hmm, makes sense to me. Thanks.
A `.bat` file is a [Windows batch file](https://en.wikipedia.org/wiki/Batch_file), and your path looks like a mount to a Windows partition. Are you trying to run a window batch file in bash? What operating system are you running?
on Windows, trying to use Task Scheduler.
Using Gitbash.exe to execute a .bat file written in bash.
on Windows, trying to use Task Scheduler. Using Gitbash.exe to execute a .bat file written in bash.
What happens in `git-bash` if you navigate to the location of the script and run it with something like `bash myscript`?
From `man bash` Here Documents This type of redirection instructs the shell to read input from the current source until a line containing only delimiter (with no trailing blanks) is seen. All of the lines read up to that point are then used as the standard input for a command. The format of here-documents is: &lt;&lt;[-]word here-document delimiter No parameter and variable expansion, command substitution, arithmetic expansion, or pathname expansion is performed on word. If any characters in word are quoted, the delimiter is the result of quote removal on word, and the lines in the here-document are not expanded. If word is unquoted, all lines of the here-document are subjected to parameter expansion, command substitution, and arithmetic expansion, the character sequence \&lt;newline&gt; is ignored, and \ must be used to quote the characters \, $, and `. If the redirection operator is &lt;&lt;-, then all leading tab characters are stripped from input lines and the line containing delimiter. This allows here-documents within shell scripts to be indented in a natural fashion. 
The suffix should be meaningless on a Linux / *nix system. More precisely the suffix has meaning to *you*, but the hint used to locate the interpreter is in the first line. Quickly looking at your source I think your first line should be `#!/bin/bash` moreover you should [mark your script as executable using chmod.](http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_01.html)
I also use a bash script to run commands/start applications after GUI login: GraphicalSessionAutostart --help 'GraphicalSessionAutostart' runs graphical session's auto-start's (short lived) commands and then launches auto-start's (long running) applications in background. It should be invoked at graphical session start by the distribution's display manager. I.E. by a desktop file in '/etc/xdg/autostart' directory. Usage: 'GraphicalSessionAutostart'. Notes: -'GraphicalSessionAutostart' logs to 'CommonLog' ('CommonLog --help'). -Each user can add his/her personal (long running) applications to the default list by editing '$HOME/var/GraphicalSessionAutostart' file. -Current auto-start's default commands (short lived): GuiDesktopTextScalingFactor pulseaudio --kill -Current auto-start's default applications (long running): MainMenuAndNetTraffic --verbose --icon $COMMON_MISC_DIR/Icons/MainMenu.png --directory $HOME/.MainMenu ___________________________________________________________ Here is last run's log: 2018/Apr/28 09:07:08: GraphicalSessionAutostart: Starting new graphical session for user: 'manolo'. 2018/Apr/28 09:07:09: GraphicalSessionAutostart: $COMMON_MESSAGES_SYSTEM_DIR: 2018/Apr/28 09:07:09: 2018/04/28 09:05:41 InternetConnectionOn 2018/Apr/28 09:07:09: 2018/04/28 09:05:41 LocalNetworkOn 2018/Apr/28 09:07:09: 2018/04/28 09:05:37 UsbControlledPowerStripAvailable 2018/Apr/28 09:07:09: GraphicalSessionAutostart: Waiting for graphical session to stabilize. 2018/Apr/28 09:07:11: GraphicalSessionAutostart: Waiting... 2018/Apr/28 09:07:11: GraphicalSessionAutostart: Command 'GuiDesktopTextScalingFactor' ran successfully. 2018/Apr/28 09:07:11: GraphicalSessionAutostart: Command 'pulseaudio --kill' ran successfully. 2018/Apr/28 09:07:12: GraphicalSessionAutostart: 'MainMenuAndNetTraffic --verbose --icon $COMMON_MISC_DIR/Icons/MainMenu.png --directory $HOME/.MainMenu ' launched. 2018/Apr/28 09:07:12: GraphicalSessionAutostart: All auto-start commands and applications ran/started successfully. ___________________________________________________________ Let me know if you want more details.
holy smokes the author of that blog is a genius
What you’re saying is true for Unix systems in general, but on Linux, when [registering additional executable formats](https://www.kernel.org/doc/html/latest/admin-guide/binfmt-misc.html), you can also match on the file name extension instead of a magic string inside the file.
run your script with -x argument to trace / debug it. Use $() format instead of `` for commands
I'd take a different approach. Since it has to be bash, use nmap. Like this: nmap -n -sn 10.0.0.0/8 -oG - | awk '/Up$/{print $2}' Replace above subnet with your subnet and you have a list of IPs of Hosts, that you can use to loop over, for whatever steps you want to follow up with. -n stops resolving of dns, -sn stops nmap from doing a portscan, -oG - will provide grepable output. This requires your hosts to reply to icmp. For the future some kind of IP management would do wonders for you in terms of avoiding having to solve problems like this at all :) Hope this helps
This seems like a weird way to go about log handling... have you considered setting up a `syslog` server instead? Otherwise, you might like to consider not relying on `ifconfig` for your local IP addresses. `ifconfig` is considered deprecated and problematic, and you should use `ip` first. Here's a not-entirely-complete function for you to reference: get-local-ip() { # Start with the 'ip' command if command -v ip &gt;/dev/null 2&gt;&amp;1; then ip -o -4 a show up | awk -F '[ /]' '/brd/{print $7}' return "$?" # Failover to 'ifconfig' elif command -v ifconfig &gt;/dev/null 2&gt;&amp;1; then ifconfig -a \ | awk -F ':' '/inet addr/{print $2}' \ | awk '{print $1}' \ | grep -v "127.0.0.1" return "$?" fi # If we get to this point, we hope that DNS is working if command -v nslookup &gt;/dev/null 2&gt;&amp;1; then # Because nslookup exits with 0 even on failure, we test for failure first if nslookup "$(hostname)" 2&gt;&amp;1 \ | grep -E "Server failed|SERVFAIL|can't find" &gt;/dev/null 2&gt;&amp;1; then printf '%s\n' "Could not determine the local IP address" return 1 else nslookup "$(hostname)" \ | awk -F ':' '/Address:/{gsub(/ /, "", $2); print $2}' \ | grep -v "#" return "$?" fi fi } I have yet to decide on what behaviour to take when a VPN connection is present, among other things... So what you want to do is filter the output of `arp`, like this: arp -a | awk -F '[()]' '/wlan0/{print $2}' | grep -vF &lt;(get-local-ip) Note that I've squashed all your `arp` handling down to a single `awk` command This eliminates your `if $a != $b;` check. You can then `scp` and `ssh` your way through your loop. Alternative things to consider: * Deploy the script permanently to all of your hosts and set it up with `cron` * Ansible
So like this? tar xvf haystack.tar.gz tail -n 2 *metal* | grep needle
do you want me to actually give you the answer or point you in the direction? :)
Thanks for the reply! I appreciate it
Thanks for reaching out! Im going to look into your advice
If the files are indeed identical, ignore the names and compare [sha1/md5 sums](https://shapeshed.com/unix-sha1sum/).
Good point. Actually, the filenames are of the format: The mind behind Linux | Linus Torvalds \- TED \(20170403\).mp4 # this is an old file, delete this The mind behind Linux | Linus Torvalds \- TED \(20170403\) \- o8NPllzkFhE.mp4 # this is the new file, keep this Then you would look at all the characters from the beginning of the filename up to the \*last\* \`\)\` character and if they match, the files are guaranteed to be the same Youtube video. Updated original post.
Does this have to be a one-line command? Some the functions you perform don't tie in smoothly with what you're ultimately tasked to do. Think about it in the way it was posed.... You've already got 25% complete. 1) Extract the contents of the file. ✓ 2) find the file with the word "metal" in the filename (see the notes about -iname vs -name) 3) in the target file, read the contents + filter out the line that reads "needle" (maybe cat?) 4) in the target file, read the contents + find the last line Those are 4 separate commands. Explore solutions to the individual tasks.
You don't *have* to use find. Given a tarball containing: files/ files/foo.txt files/bar/ files/bar/heavy_metal.txt The file `files/bar/heavy_metal.txt` contains: Metalica - Master of Puppets Panteraa - Psycho Holiday Def Leopard - Photograph Metal Church - Needle and Suture This will extract the files and get the last line of the file that has 'metal' in it's name. (I'm not too clear on what your exact requirements are - are you looking for the last line of the file or do you need to search the file for the word **needle**?) #!/usr/bin/env bash # argument 1 for script - defaults to files.tar.gz tarball=${1:-files.tar.gz} # read each filename from the output of 'tar -xvf' while read -r filename; do # some versions of tar (osx) prefix the filename with 'x' # this removes the prefix (and space) filename=${filename##* } # skip directories [[ -d ${filename} ]] &amp;&amp; continue # make filename all lowercase filename=${filename,,} case ${filename} in *metal*) echo "Found: ${filename}" tail -1 "${filename}" ;; esac done &lt; &lt;(tar -xvf "${tarball}" 2&gt;&amp;1) # ^ this line extracts the tarball and redirects stderr to stdout This method is probably sketchy though - you should probably use find like others have suggested :-) You can modify the above script to use find. Hint: Currently the filenames come from the output of 'tar -xvf'. What would you need to do to have the output come from 'find'? 
I don't know how the 'dpkg' command works so I can't really help with concrete details. I'll just talk about bash in general. What you are doing right now is definitely not working. Inside the `[[` you are supposed to do tests on text and such. If you want to run a command and use it in there, you have to capture its text output with bash's `$(...)` feature. About what it does, type the following two lines at the bash prompt to get a feeling for it: First: date Then next: echo "the current date is: $(date)" If you want to look at something in the output of the 'dpkg' command, you would do something like this: [[ $(dpkg ...) == "..." ]] Bash will first run the command inside the `$(...)`, capture its output, insert that text onto the original command line, and then finally run that command line. You can get a help text about the `[[` brackets like this: help test help '[' help '[[' Inside the `[[`, you cannot use anything else except the tests mentioned in `help test`, plus the bit of extra stuff mentioned in `help '[['`. The `==` is a special version of `=` that can do a bit more stuff. You can use wildcards in it: [[ "there is some text here" == *"some text"* ]] # &lt;-- will match You can look at how `if` works like this: help if In that first "COMMANDS;", the one directly after the `if` word, you don't necessarily have to use `[[`. You can also use a normal command. The "exit status" of that command will be checked to decide if things are seen as "true" of "false", and to see if the `then` section will be executed or not. You can for example check the contents of a file with grep: ## the "... &amp;&gt; /dev/null" hides the output of grep if grep 'search-pattern' 'filename' &amp;&gt; /dev/null; then echo "grep found the search-pattern in the file" else echo "grep didn't find it" fi Like I mentioned, I don't know how 'dpkg' works. I would guess it tries to tell you something about search results in its exit status. If that's the case, you do not have to use `[[` and can also directly run it with `if`. The good thing about that would be that your script would work in all languages. If someone doesn't use English as their language setting, you looking for certain text in the output will probably not work. Last thing, you can't start your script like this: #!BIN/BASH It has to be this: #!/bin/bash That part of the script is about a real file name on the disk.
mkdir work cd work tar -ztvf haystack.tar.gz find . | grep metal | grep needle
Yes and also I think that if you have spaces in the output you have to double quote the $() to make sure the whole thing is one string. so I would go like this : if [[ "$(dpkg ...)" == "Installed..." ]]; then hth
It's not recommended to use `sudo` within your script. Rather, invoke the script with `sudo`. if [ "$EUID" -ne 0 ]; then echo "Please run the script as root." else main fi main() { # the rest of your program here }
Would it be possible to do a regex on the file dates then feed that into rm with xargs?
&gt; invoke the script with &gt; &gt;sudo Do you mean the *whole* script should not contain `sudo` or just anything enclosed in a conditional? Thanks.
The *if* line now reads as follows: `if [[ $(dpkg -s update-manager-core) == "install ok installed" ]]; then` As per your instruction, I added the *$* and enclosed the `dpkg` portion in brackets. This should be correct in terms of an *if* statement, right? Also, thanks for pointing out `#!/bin/bash`, I changed that, too. Updated version in same link: [https://github.com/brandleesee/blc/blob/master/scripts/upgrade/release\-upgrade.sh](https://github.com/brandleesee/blc/blob/master/scripts/upgrade/release-upgrade.sh) 
It means when you run your script run it as: sudo script rather than script and remove the instances of sudo within the script
Ah! Well understood. Script updated here: https://github.com/brandleesee/blc/blob/master/scripts/upgrade/release-upgrade.sh Also added *sudo* in instructions: https://github.com/brandleesee/blc/blob/master/scripts/upgrade/README.md This approach is correct, right? Thank you.
Makes sense. Shall change my other scripts in this fashion. Thank you.
I created these two test files: $ ls -1 The mind behind Linux | Linus Torvalds - TED (20170403).mp4 The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4 Then experimented in that folder and came up with this: $ find . -type f | perl -lnE 'if (m{^.*/(.*?)(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; @f = sort @{$h{$_}}; print "# ", (join "\n", @f); } }' ## key: The mind behind Linux | Linus Torvalds - TED (20170403) # ./The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4 ./The mind behind Linux | Linus Torvalds - TED (20170403).mp4 The code used in there with line-breaks looks like this: find . -type f | perl -lnE ' if (m{^.*/(.*?)(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; @f = sort @{$h{$_}}; print "# ", (join "\n", @f); } } ' The idea is to work on it in a text editor and change where the `#` characters are to avoid mistakes, then later turn filter out empty lines and the `#` lines and use it to delete files.
Like I mentioned, I don't know the dpkg program, but I bet what you are currently trying to do will not work. Where you use `[[` and `==` to compare text, that comparison will only be seen as "true" if the text output of the dpkg program is exactly that "install ok installed" text you are comparing it with. There can't be any differences, no extra spaces, words, line-breaks, nothing.
If you make the script executable: chmod +x release-upgrade.sh then there is no need to run bash directly as in your readme: sudo bash release-upgrade.sh You can simply run it like this: sudo release-upgrade.sh Further, if you are going to run it with sudo, you need to have a mechanism within the script for ensuring that the code is running with elevated privileges. /u/6e696e67 gave you the [code for this](https://www.reddit.com/r/bash/comments/8fq4da/correct_conditional/dy5pgn8/): if [ "$EUID" -ne 0 ]; then echo "Please run the script as root." else main fi main() { # the rest of your program here } 
How does the "exit status" of dpkg work? The program might use it to report about success or errors. If that's the case, this would make things a lot easier. You could then just write: if dpkg ...; then ... fi There would be no need to check anything if that's how the exit status works. You might want to google about this topic. About checking the actual text, try using wildcards: if [[ $(dpkg ...) == *"some words here"* ]]; then ... fi Or use the "grep" program and a pipe: if dpkg ... |&amp; grep -q "some words here"; then ... fi 
I haven't tried it, but the bash builtin "read" can be used with a timeout of zero, and if there is no input available it will indicate that in the return code. Another option would be to use "read" with a timeout of 1 second in place of the sleep command. .
I agree with /u/whetu's point about using a script on the host, Ansible, or similar. However, below is an example for getting local addresses that is adapted from a scanning script that I made for myself: #!/bin/bash ######################## # IP Address Functions # ######################## ip2dec(){ local a b c d ip=$@ IFS=. read -r a b c d &lt;&lt;&lt; "$ip" printf '%d\n' "$((a * 256 ** 3 + b * 256 ** 2 + c * 256 + d))" } dec2ip(){ local ip dec=$@ for e in {3..0} do ((octet = dec / (256 ** e) )) ((dec -= octet * 256 ** e)) ip+=$delim$octet local delim=. done unset e octet dec printf '%s\n' "$ip" } cidr-low(){ printf $(dec2ip $(cidr-low-dec "$1")) } cidr-low-dec(){ # Print the lowest usable address in a CIDR range. # Assumes valid input in one of the the following two formats: # - 10.10.10.0/24 # - 10.10.10.0/255.255.255 # Calculating netmask manually because ipcalc does not support # calculating the minimum/maximum addresses in all distributions. local network=$(cut -d'/' -f1 &lt;&lt;&lt; "$1") local netmask=$(cut -d'/' -f2 &lt;&lt;&lt; "$1") local plus=1 if grep -qP "255.255.255.255|32" &lt;&lt;&lt; "$netmask"; then # /32 networks are single-host networks, # wherein the network ID is the only usable address. local plus=0 fi printf $(($(ip2dec "$network")+$plus)) } cidr-high(){ printf $(dec2ip $(cidr-high-dec "$1")) } cidr-high-dec(){ # Print the highest usable address in a CIDR range. # Assumes valid input in one of the the following two formats: # - 10.10.10.0/24 # - 10.10.10.0/255.255.255 # Calculating netmask manually because ipcalc does not support # calculating the minimum/maximum addresses in all distributions. local network=$(cut -d'/' -f1 &lt;&lt;&lt; "$1") local netmask=$(cut -d'/' -f2 &lt;&lt;&lt; "$1") if ! grep -qP "^\d{1,}$" &lt;&lt;&lt; "$netmask"; then # Netmask was not in CIDR format. local netmask=$(printf %.$2f $(awk '{ print 32-log(4294967295-'"$(ip2dec "$netmask")"')/log(2)}' &lt;&lt;&lt; "")) fi # Subtract 2 for network id and broadcast addresss # (unless we have a /32 address) local subtract=2 if [ "$netmask" -eq "32" ]; then # /32 networks are single-host networks, # wherein the network ID is the only usable address. local subtract=0 fi printf $(($(ip2dec "$network")+(2 ** (32-netmask))-$subtract)) } location="${1}" # CIDR validation effort (will not cover super-wacky input such as 192.168.300.2/42) # Accepts one of the following formats: 192.168.0.0/24 , 192.168.0.0/255.255.255.0 if ! grep -Pq '^(([0-9]){1,3}\.){3}([0-9]{1,3})/((([0-9]){1,3}\.){3}([0-9]{1,3})|\d{1,2})$' &lt;&lt;&lt; "$location"; then echo "BAD CIDR RANGE: ${location}" exit 1 fi # Decimal values of CIDR limits. low=$(cidr-low-dec "$location") high=$(cidr-high-dec "$location") # Determine if the low end is within one of our routes. count=0 __low_is_in=0 __high_is_in=0 # Get local networks (route table entries with a "gateway" of 0.0.0.0) # Exclude tun networks from consideration. for range in $(route -n | grep -v 'tun' | awk '{if ($2 == "0.0.0.0"){ print $1"/"$3 }}'); do __current_low=$(cidr-low-dec "$range") __current_high=$(cidr-high-dec "$range") count=$((count+1)) if [ "$low" -ge "$__current_low" ] &amp;&amp; [ "$low" -le "$__current_high" ]; then if (( ! ${__low_is_in} )); then __low_is_in=$count fi fi if [ "$high" -ge "$__current_low" ] &amp;&amp; [ "$high" -le "$__current_high" ]; then if (( ! ${__high_is_in} )); then __high_is_in=$count fi fi if [ "$__low_is_in" -eq "$count" ] || [ "$__high_is_in" -eq "$count" ]; then if [ -n "$statement" ]; then statement="$statement&amp;&amp;((ipAsDec&gt;=$__current_low)&amp;&amp;(ipAsDec&lt;=$__current_high))" else statement="((ipAsDec&gt;=$__current_low)&amp;&amp;(ipAsDec&lt;=$__current_high))" fi fi done if [ -n "${statement}" ]; then arp_address_list=$(arp -n | grep -vP "(^Address)|incomplete" | awk 'function ip2dec(ip){split(ip,octets,"."); return octets[1] * 256^3 + octets[2] * 256^2 + octets[3] * 256 + octets[4]} {ipAsDec=ip2dec($1); if('${statement}'){print $1}}'); # Sort and output printf "%s\n" ${arp_address_list} | sort -nu -t'.' -k1n,1n -k 2n,2n -k 3n,3n -k 4n,4n fi Usage: ./list-local-addresses.sh 192.168.0.2/24 Giving the script a CIDR range will return all IP addresses in your ARP table from that range. Other notes: * The script does not cause any traffic that would create ARP entries. I would run something like `nmap -sP 192.168.0.0/24`to populate the table. * The script-running machine's own addresses will not in the ARP table.
You can source the countdown function in your script. 
You don't want to read in the countdown function since you can't be guarnteed it's on the read command when the button is pressed. Like /u/falderol suggested, you should either add the function to the script directly, or by sourcing. As for the pressing enter requirement, try the `-n` option for the `read` command -n nchars read returns after reading nchars characters rather than waiting for a complete line of input. So use 'read -n 1', and it will accept the first character pressed without waiting for the return character.
I'll be honest, I'm a beginner at scripting, so I'm not sure I totally understand what you wrote. I would add /u/falderol's script but it needs to be sent to a new terminal window. I'm not sure how I'd send countdown &amp; mypid=$! to a new terminal. I've been doing trial &amp; error for the past 2h (before reading your post) and ended up with this which seems to work, I removed the sleep command like you mentioned and added -n 1 to read, and ended up with the following, does what you said about the buffer being held still apply without the sleep? #!/bin/bash countdown() { secs=60 while (( secs &gt; 0 )); do printf '%s\033[0K\r' "$secs" ((secs--)) read -rs -n1 -t1 if [ $? == 0 ] then exit fi done shutdown now } export -f countdown gnome-terminal -e 'bash -c countdown' thanks
Thanks for the in-depth response! I appreciate it.
&gt; does what you said about the buffer being held still apply without the sleep? Technically it does, but since it's spending 99.9% of the time on the read command, it's much better. Although an `exit` in that function is probably just fine for you, I would use a `return` rather than an `exit` from the function. Sourcing would mean putting what the above poster put into another script, and name it say, `countdown.sh` and then wherever you want to use it, you would type `source countdown.sh`. Is there any reason you can't just add this function into your other script? Like at the top? The following is untested: #!/bin/bash TIMEOUT=60 function countdown () { for i in $(seq ${TIMEOUT} -1 0); do echo ${i} sleep 1 done shutdown now } #Put your other script here: # # # #Start countdown one above has finished. echo "Press any key to abort reboot" countdown &amp; #Start coundown in a new process COUNT_PID=${!} #Get PID of countdown read -n 1 #Do not use -t here kill -15 ${COUNT_PID} #Send SIGTERM signal to coundown process 
Trap was going to be my first suggestion as well.
Awesome, I changed it a little bit so it launches a new terminal, would you mind just verifying I didn't change anything important? #!/bin/bash export TIMEOUT=60 function countdown1 () { for secs in $(seq ${TIMEOUT} -1 0); do printf '%s\033[0K\r' "${secs}" sleep 1 done shutdown now } export -f countdown1 countdown2 () { echo "Shutting down in ${TIMEOUT} seconds. Press any key to abort." countdown1 &amp; COUNT_PID=${!} read -n 1 kill -15 ${COUNT_PID} } # my original script export -f countdown2 gnome-terminal -e "bash -c 'countdown2'" thanks!!
I don't see anything that should be an issue. The only reason it might be a concern before is it could get confusing if you were to add multiple read points later, and it might be hard to tell when it is taking in what input. The bit that might have confused you before was the mention of a buffer. When you do a `printf` for example, while it may appear to print to the terminal instantly, but it doesn't. It writes to stdout which is a buffer, and it gets flushed to the terminal once in awhile. Same with input. So when things are moving really fast like in a loop and there may be multiple points where you read, it can get hairy, so IMO, the best practice if you want multiple Input/Output events to happen at the same time (read and write) is to wait for input on a seperate thread. Again, what you were doing read in the loop was probably technically fine, and lots of people do it that way with no problems, I just personally think wating for the input before moving on is the safest way.
Hey, PageFault, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
ahhh! I get it now. thanks for all your help, I really appreciate it :) thanks to everyone else who answered too!
Maybe I’m missing something here, but the shutdown command is already interruptible: # Shutdown (halt) one minute from now shutdown -h +1 “Going down in 1 minute” Cancel a pending shutdown shutdown -c “Shutdown cancelled” Both commands need super-user privileges, but you get the idea :) If your script is an exercise in trap handles for learning and fun, it’s a good use case to mess around with :)
Doesn't `&lt;ctrl-r&gt;test` kind of do what you want? In any case, try removing the `"$1"` from the alias.
ctrl\-r only shows the last entry, not all entries with the search string, it seems. Removing the $1 will make my grep not find anything. I mean, that's the thing I want to grep for. Not sure what that would accomplish...? Am I misunderstanding something from your answer?
Update to this: using "$@" helped me figure this out. Apparently, for some reason when I use the aliased version of this \(with ? or h, doesn't matter\), grep wants to read my \~/.inputrc If I `history | echo "$@"` for test purposes, the output is `grep: show-all-if-ambiguous: No such file or directory` `grep: on: No such file or directory` `grep: test: No such file or directory` I'm just not sure \_why\_ it's reading my \~/.inputrc but I'm switching to the functon. I'll just have to muscle\-memory myself into using h rather than ? 
Aliases don't have arguments, so `"$@"` is just expanding to the arguments of the shell. For an interactive shell, there's usually no arguments, so the "$@" at the end of an alias appears to work as it simply expands to nothing. You must've set the interactive shell's argument by accident or something.
I hadn't tried it, no, since I didn't know the why of your suggestion. \\ Thank you for explaining, I didn't know it worked like that. 
Thanks for the explanation, I didn't know it worked like that. I'll look into why it expands to the arguments in my inputrc. Cheers!
At a guess, you ran `` `cat ~/.inputrc` `` instead of `cat ~/.inputrc` at some point. With the first line starting with the word `set`, that would run the `set` command with the remaining words as arguments.
I've never used arguments to aliases since my understanding was that the alias itself expands to whatever is on the right side of the =. I would use just `history | grep`. I use this style for various commands, usually ones with a lot of options, and have never had one break.
press ctrl+r again to cycle through the matches
Glad it helped. I was on mobile for my first reply so kept my response brief and to the point.
Here's ShellCheck: Line 1: alias \?='history | grep "$1"' ^-- SC2142: Aliases can't use positional parameters. Use a function. And [its wiki](https://github.com/koalaman/shellcheck/wiki/SC2142) has examples and links to the relevant Wooledge BashFAQ and StackOverflow post.
 select myval in $(my_command) do perform_operation_on $i done The prompt and other things are configurable
WOW, amazing (I still get floored learning all these new things bash can do). Your suggestion worked perfectly. It produced my list of options without even needing a case statement like I thought. I do have a few questions: I am seeing a new prompt I’ve never seen before that is produced along with the menu list: 1) Apple 2) orange 3) pear 4) mashedpotatoes 5) beer #? What is ‘#?’ ? And it looks like I can only exit out of the script by pressing control+c. Typing ‘exit’ at this special prompt does not exit out. After I finish working the rest of my script just wondering how can I properly exit out of it? Many many thanks sir
I enjoyed the discussion under this post. I have to ask though, why not use ctrl-i?
If I can bother you with one more question: How can i chain multiple select loops so that the user can input various choices presented and then have all the choices act as variables for a final command for the script to process? I am googling it but if you happen to have some tips to solve this please let me know. Thanks again 
Scratch one question off the list. To exit out of the script I added a break before done: select myval in $(my_command) do perform_operation_on $myval break done
After your `while getopts` loop, do: shift "$((OPTIND - 1))" This will clear out all the options, leaving only non-option arguments starting with `$1`.
Alright i think I’ve got this figured out, please correct me if I’m wrong or if I can do something better: #!/bin/bash #user input 1 select myval in $(my_command) do perform_operation_on $myval1 break done #user input 2 select myval2 in $(my_command) do perform_operation_on $myval2 break done #user input 3 select myval3 in $(my_command) do perform_operation_on $myval3 break done #user input 4 select myval4 in $(my_command) do perform_operation_on $myval4 break done #connect all the inputs and run the final command (just an example) some_command $myval1 -e $myval2 ; echo $myval3 ; echo $myval4
Ok now I just need to learn how formatting works all of my ‘#’ signs vanished. 
Beautiful, thanks. Follow-up question because this didn't solve the problem I was hoping it would, what's the best way to get stdout from one comment to pipe into a bash script? I thought removing the option would put stdout to $1 with a pipe but it's not working. I can use tee to get it, but then it hangs the script when there's no pipe.
In a pipeline, the standard output of one command becomes the standard input of another. `$1` etc. are command line arguments, not standard input. If you want to read from standard input, you have to use the `read` command.
Four (4) spaces in front of every line of code formats them as code.
Damn. Maybe I should just rewrite it in Python then to make pipes work better.
Why? If bash is really good at anything, it's working with pipes. What is it you're trying to achieve? What command are you trying to pipe into your script, and what kind of output does it produce?
If you compiled it on Debian, it’s probably different libraries then Ubuntu. Since you are compiling it, you no longer have bash compatibility so you will most likely need to compile on Ubuntu. 
&gt; What command are you trying to pipe into your script, and what kind of output does it produce? Arbitrary text. I want to be able to pipe the output of one command into this script to send a push notification to my phone with the output when it finishes. The problem is, the script isn't just for that. You can specify static message strings instead. Using `read` or `tee` to get stdin fails when there is no stdin. The script just hangs, waiting for input from the terminal until enter is pressed. Using a timeout is hacky. I want to be able to check IF there's anything to get from stdin which it appears Python will let me do but Bash will not.
Yes it will. Check if stdin is on a terminal with `[[ -t 1 ]]`. If it is, then you're reading from the keyboard, if it's not, then it's a pipe or input redirection.
Oh, awesome. That's working. Thanks!
&gt; ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment. Apparently that doesn’t work right now, so I’ve banned the bot for a week – let’s see if it behaves better then? (Let me know if anyone has opinions on banning it more permanently.)
About the current output, you can delete empty lines and `#` lines like this with sed after checking it over: sed -r '/^$/d; /^#/d' Here is the Perl command with line-breaks for easier reading: perl -lnE ' if (m{^.*/(.*?)(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", sort @{$h{$_}}); } } ' About what you mention with the `(...)` characters, I'd try this regex rule: m{^.*/(.*\))(?: - [\w\-=]{11})?\.\w{3,4}$} The result looks like this: $ find . -type f | perl -lnE 'if (m{^.*/(.*\))(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", sort @{$h{$_}}); } }' ## ignoring: ./testfile.mp4 ## ignoring: ./another.mp4 ## ignoring: ./another - 0123456789a.mp4 ## key: The mind behind Linux | Linus Torvalds - TED (20170403) # ./The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4 ./The mind behind Linux | Linus Torvalds - TED (20170403).mp4 I originally wanted it to output things like this: # rm './The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4' rm './The mind behind Linux | Linus Torvalds - TED (20170403).mp4' Meaning, I wanted it to be something that can be saved, looked over manually (with the highlighting from your text editor, your editor's hotkey to swap `#` comments around), then run as a shell script. The only problem was just, when I wanted to add the quotes into the text, like so: s/'/'\\''/g; # replace existing ' characters with '\'' s/.*/rm '$&amp;'/; # add rm '...' This is supposed to turn a line like this: Joe's shopping list.txt Into something that bash likes in a script: rm 'Joe'\''s shopping list.txt' The problem was, this was too annoying to experiment with at the command line because the whole perl code is already surrounded by `'` quotes, and inserting `'` into the perl code makes you have to type a lot of confusing `'\''`. I tried to do that again and came up with this here: find . -type f | perl -lnE 'if (m{^.*/(.*\))(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", map {'" s/'/'\\\''/gr =~ s/.*/rm '\$&amp;'/r "'} sort @{$h{$_}}); } }' Its output looks like this: $ find . -type f | perl -lnE 'if (m{^.*/(.*\))(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n", map {'" s/'/'\\\''/gr =~ s/.*/rm '\$&amp;'/r "'} sort @{$h{$_}}); } }' ## ignoring: ./testfile.mp4 ## ignoring: ./another.mp4 ## ignoring: ./another - 0123456789a.mp4 ## key: The mind behind Linux | Linus Torvalds - TED (20170403) # rm './The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4' rm './The mind behind Linux | Linus Torvalds - TED (20170403).mp4' ## key: testfile () # rm './testfile ().mp4' ## key: testfile's hello () # rm './testfile'\''s hello ().mp4' About aligning things, adding space characters to the text used for the "join" works: $ find . -type f | perl -lnE 'if (m{^.*/(.*\))(?: - [\w\-=]{11})?\.\w{3,4}$}) { push @{$h{$1}}, $_; } else { say "## ignoring: $_"; } END { for (sort keys %h) { print "\n## key: $_"; print "# ", (join "\n ", sort @{$h{$_}}); } }' ## key: The mind behind Linux | Linus Torvalds - TED (20170403) # ./The mind behind Linux | Linus Torvalds - TED (20170403) - o8NPllzkFhE.mp4 ./The mind behind Linux | Linus Torvalds - TED (20170403).mp4 That `## key: ...` thing was really just a debug message. I added it when I had problems with the regex rule not working right. You could change that `print` command into this (I would want to keep the empty line it also outputs): print "\n";
I'd personally like to see it just go permnantly. If they had a "message bot with 'unsubscribe'" option (and it worked) to quit getting messages I wouldn't mind it as much. I just don't find it useful.
TIL about shc... Seems the wrong approach to securing credentials, imho. It might be a better approach to handle encryption / decryption of 'secret' data within the script, rather than encrypting the whole script. Makes auditing, review and development more difficult.
No, because if there's no stdin from a pipe I want the script to fail and output useful help text about the message field being empty. If I do it this way, it will just hang waiting for input if there's no arguments or stdin supplied.
Why are you using awk as a variable since its a command as well?
 dont_use_a_command_as_variable_name=`awk '{ print $1}' holidays` if [ now = "$dont_use_a_command_as_variable_name" ] That should fix about four mistakes, I think. Not sure if there are any more.
idk what read event means lol, not sure I understand what you were doing here, because you put $event but it isn't declared as anything? maybe im just dumb and have no idea what im reading.
Careful, the details are important. These two: ' and ` are not the same symbol. For example: var1='pwd' var2=`pwd` The first one assigns the string "pwd" to var1. The second one assigns the output of the command "pwd" to var2. So you're gonna need the second kind to assign the outputs of "date" and "awk" to your variables. Also, spaces matter. You need a space between "if" and "[", and another between "[" and your test condition, and another between your test condition and "]". For the test condition itself, it should be "$today" = "$holiday" I messed that up in my quick answer above. You always need the "$" for variables, except when you assign a value to it. For the date and awk commands themselves, I don't know their syntax by heart, so I don't know if they're correct or not.
WE HAVE PROGRESS! #!/bin/sh today= date +"%-e-%b" holiday=`awk '{ print $1}' holidays` if [ today = "$holiday" ] then echo " today is a holiday" else echo " Today is not a holiday" fi ^^^^ This give me no errors at all, the problem is. It says " today is not a holiday. here is the file i have it reading from OUTPUT: 1-May Today is not a holiday file i have it reading from 1-May National Coding Day 11-Feb Clean Out Your Computer Day 11-Feb 12-Feb Darwin Day 12-Feb
I get really confused when I see a bunch of those together. That's why I hate regex. If I do the same `foo="bar"; echo ${foo/a/\'baz\'}` I get the same result. (I left out one of the backslashes preceeding `a`. 
`${var/match/replace}` only matches and replaces the first instance. `${var//match/replace}` matches and replaces all instances.
It's not a regex rule. This bash feature uses the same wildcard patterns as what you know from file matching on the normal bash command line. For example: $ foo=abcdefghi; echo "${foo/c*g/HELLO}" abHELLOhi $ foo=abcdefghi; echo "${foo/d???/HELLO}" abcHELLOhi 
Thanks for the reply, your prob right any recommendation on what software i can use?
Nothing pre-packaged, but it depends on what you are trying to do. gnupg would be a good tool to encrypt the credentials, and you could call gpg in your script in order to prompt for a password and decrypt the creds. You could also use something like gnu pass ( https://www.passwordstore.org/ ) depending on how many credentials you need to store. Here's some other ideas; https://unix.stackexchange.com/questions/212329/hiding-password-in-shell-scripts
Thank you so much that did the trick 
Are you asking people to write scripts for you?
You might need to be a little more specific in your questions. Start writing the scripts. If you don't know where to start, then start searching the Internet for how to write Bash scripts. When you get stuck somewhere, then show the smallest sample of code for where you're having trouble &amp; explain the problem.
 #!/bin/sh today= date +"%-e-%b" event=`awk '{ print $1}' holiday` if [ today = "$event" ] then echo " today is a holiday" else echo " Today is not a holiday" fi Ok, so first off, you need to fix your variable assignments. Backticks were superseded in the early 80's, `$()` is the preferred form, so... today=$(date +%e-%b) event=$(awk '{print $1}' holiday) Note that I've also adjusted the format of your `date` call. Format quotation isn't strictly necessary unless you've got spaces, probably it is a good habit to do it regardless. Next issue: What happens when you run that `awk` command? At the moment we get this: # awk '{print $1}' holiday 1-May 25-Dec So now your `if` test is essentially going to ask "is today's date the same as 1-May newline 25-Dec?" Which, as you can tell, makes no sense. And it will obviously get worse when the holiday file is grown. What you need to do is either work through your holiday file line by line, or to search it, and other posters have touched on both approaches. Personally, I would go for the search approach as I think it scales better. But you could possibly improve the performance of the line-by-line approach by sucking the file up into an array and iterating through said array... Next, a stylistic suggestion - tuck your `do`'s and `then`'s out of the way and use 2 space indenting: if [ today = "$event" ]; then echo " today is a holiday" else echo " Today is not a holiday" fi Does that seem a bit more readable? Finally, what you're wanting to do is fairly simple so it could be a little bit terser e.g. if grep $(date +%e-%b) holiday &gt;/dev/null 2&gt;&amp;1; then printf -- '%s\n' "Today is a holiday ($(grep $(date +%e-%b) holiday | cut -d ' ' -f2-))" else printf -- '%s\n' "Today is not a holiday." fi
This is very good information, I appreciate you taking your time to help me out. Much more complex than I thought it was going to be. 
Here are the docs on this feature, from `info bash`, for all the gory details: &gt; `${parameter/pattern/string}` &gt; &gt; Pattern substitution. The pattern is expanded to produce a pattern just as in pathname expansion. Parameter is expanded and the longest match of pattern against its value is replaced with string. If pattern begins with `/`, all matches of pattern are replaced with string. Normally only the first match is replaced. If pattern begins with `#`, it must match at the beginning of the expanded value of parameter. If pattern begins with `%`, it must match at the end of the expanded value of parameter. If string is null, matches of pattern are deleted and the `/` following pattern may be omitted. If parameter is `@` or `*`, the substitution operation is applied to each positional parameter in turn, and the expansion is the resultant list. If parameter is an array variable subscripted with `@` or `*`, the substitution operation is applied to each member of the array in turn, and the expansion is the resultant list. So glob substitution (not regex, as the syntax seems to imply), with typical bash syntax oddities mixed in.
I'm going to save this so i can go back and refer to it, this is a great way of using grep thank you for putting so much time into this, just excited to be learning little things here and there until i start school again in the fall 
Run it with bash -x script, this will show you what command it's getting stuck on. 
Are you providing the script with an argument when running it? If you don’t give any argument it will hang at the last line 
I am providing it with an argument :/
Then follow the instructions in the other comment and run; # bash -x script.sh argument It should tell you where / why it’s hanging 