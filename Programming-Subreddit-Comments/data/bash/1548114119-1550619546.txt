I can grep my way into an answer here. Works the best if the formatting of your file stays the same, with the "Size" column and all. I added some lines to the file for demonstration purposes. &amp;#x200B; someone@MacBook-Pro:~$ cat testfile http://sg.ovies.m/.ss/ (Status: 200) [Size: 128] http://sg.mo.v/.dev/ (Status: 200) [Size: 12328] http://som.b/.hal/ (Status: 200) [Size: 1238] http://m.cb/.ho/ (Status: 200) [Size: 8] http://sm.jo/.hK/ (Status: 200) [Size: 2] http://sm.jo/.hK/ (Status: 200) [Size: 456] http://sm.jo/.hK/ (Status: 200) [Size: 70] http://sm.jo/.hK/ (Status: 200) [Size: 99] http://sm.jo/.hK/ (Status: 200) [Size: 12] http://sm.jo/.hK/ (Status: 200) [Size: 35790] http://sm.jo/.hK/ (Status: 200) [Size: 567] http://sm.jo/.hK/ (Status: 200) [Size: 0] http://sm.jo/.hK/ (Status: 200) [Size: 1258765] http://sm.jo/.hK/ (Status: 200) [Size: 90000] http://sm.jo/.hK/ (Status: 200) [Size: 34] http://sm.jo/.hK/ (Status: 200) [Size: 1] http://sm.jo/.hK/ (Status: 200) [Size: 111] http://sm.jo/.hK/ (Status: 200) [Size: 110] http://sm.jo/.hK/ (Status: 200) [Size: 100] http://sm.jo/.hK/ (Status: 200) [Size: 101] &amp;#x200B; someone@MacBook-Pro:~$ grep 'Size: [0-9][0-9][0-9]' testfile http://sg.ovies.m/.ss/ (Status: 200) [Size: 128] http://sg.mo.v/.dev/ (Status: 200) [Size: 12328] http://som.b/.hal/ (Status: 200) [Size: 1238] http://sm.jo/.hK/ (Status: 200) [Size: 456] http://sm.jo/.hK/ (Status: 200) [Size: 35790] http://sm.jo/.hK/ (Status: 200) [Size: 567] http://sm.jo/.hK/ (Status: 200) [Size: 1258765] http://sm.jo/.hK/ (Status: 200) [Size: 90000] http://sm.jo/.hK/ (Status: 200) [Size: 111] http://sm.jo/.hK/ (Status: 200) [Size: 110] http://sm.jo/.hK/ (Status: 200) [Size: 100] http://sm.jo/.hK/ (Status: 200) [Size: 101] This looks for 3 digits together, along with "Size: ". Anything longer will also match, as long as the first 3 digits are 0-9, then 0-9, then 0-9. &amp;#x200B; someone@MacBook-Pro:~$ grep 'Size: [1-9][1-2][8-9]' testfile http://sg.ovies.m/.ss/ (Status: 200) [Size: 128] In this example I try to grab the number 128. First digit 1-9, second digit 1-2, and third digit 8-9. Didn't match any of the longer numbers because they don't match that specific pattern. &amp;#x200B; someone@MacBook-Pro:~$ grep 'Size: [1-2][2-3][3-4]' testfile http://sg.mo.v/.dev/ (Status: 200) [Size: 12328] http://som.b/.hal/ (Status: 200) [Size: 1238] Trying to match the numbers that start with 123 will also match the longer numbers. &amp;#x200B; So you **CAN** do this with grep ^(pretty much.) I also saw that you wanted to know how to match numbers under 100. &amp;#x200B; someone@MacBook-Pro:~$ grep -Ew 'Size: [0-9][0-9]|Size: [0-9]' testfile http://m.cb/.ho/ (Status: 200) [Size: 8] http://sm.jo/.hK/ (Status: 200) [Size: 2] http://sm.jo/.hK/ (Status: 200) [Size: 70] http://sm.jo/.hK/ (Status: 200) [Size: 99] http://sm.jo/.hK/ (Status: 200) [Size: 12] http://sm.jo/.hK/ (Status: 200) [Size: 0] http://sm.jo/.hK/ (Status: 200) [Size: 34] http://sm.jo/.hK/ (Status: 200) [Size: 1] \-E gives us some extended regular expression abilities, basically so we can do an 'either or' match with | . And then -w matches just the entire word/number and nothing else. In this case, matches only two digit or one digit numbers along with "Size: ", so we don't get the expanded results like before. Without the -w this would match all the numbers in the list. 
You missed my point. That's what I'm already doing, the $configFile is exactly for this purpose. But this doesn't provide me with a way to give the other users the app's client ID and client secret. They'll have to generate their own, which I'm trying to avoid.
Thanks, that's a good suggestion and I also learned something. So basically it throws the control out of the script, right?
I took a look at the link you posted. the reason the list is displaying like that is due to the way '*select*' works in bash. It is always going to stack options like that once you have more than *x* options. An alternative, which isn't as ... uh ... automated, is shown below. You will have to explicitly define each option. #!/bin/bash options=("Option 1" "Option 2" "Option 3" "Option 4" "Option 5" "Option 6" "Quit") optcount=1 for opt in "${options[@]}"; do printf "$optcount) $opt\n" ((optcount++)) done run=true while $run; do printf "Please enter your choice: " read userinput case "$userinput" in "1") printf "You selected Option $userinput.\n" ;; "7") printf "Quitting ...\n" run=false break ;; *) printf "You selected $userinput, that is not a valid option.\n" ;; esac done This effectively does the same thing as what was shown on the linked example.
Did you forget the quotes around the PS1 declaration?
u/GoldenPandaGamer, you've fallen prey to one of the less-understood shell variables: &gt;**COLUMNS** &gt; &gt;Used by the `select` compound command to determine the terminal width when printing selection lists. Automatically set if the `checkwinsize` option is enabled or in an interactive shell upon receipt of a `SIGWINCH`. Note that I said "shell variable". If you try to set it outside your script, bash cheerfully and quietly overwrites it. There are also a few undocumented gotchas that only become obvious when reading the bash source code: 1. `COLUMNS` defaults to 80. 2. `select` tries to fill complete rows. 3. If, after doing \[2\], `select` ends up with only 1 row, it'll "pivot" into 1 column instead. All the above should explain why you see what you see. If you want to force 1 column, simply set COLUMNS to an appropriately small value: COLUMNS=1 select ... unset COLUMNS
Append: 2&gt;&amp;1
&gt; It is always going to stack options like that once you have more than _x_ options (4 presumably?). Actually, `select` formatting logic is entirely deterministic. See [my reply](https://www.reddit.com/r/bash/comments/aifx7o/i_need_help_with_list_formatting/eenq09e/) for the gory details.
Thanks for this! I never use 'select', so I never had the need. Glad to put another good bit in my pocket!
Hmm... Maybe the problem has to do with dynamically checking window size instead. Thanks for testing
Thank you so much! this works perfectly!
u/fuzzymidget, as others have said, there's nothing obviously wrong with your `PS1`...and until you show/tell us **what you actually see**, you probably won't get any other response than "welp, works for me."
So what's wrong with it? It ran OK for me. The orange in the picture. [http://i.imgur.com/YAGGEkO.png](http://i.imgur.com/YAGGEkO.png) I prefer my though. export PS1='\\\[\\e\[1;33m\\\]\\u@\\h \\w -&gt;\\n\\\[\\e\[1;36m\\\] \\@ \\d \\$\\\[\\e\[m\\\] '
They serve very different purposes: * bash keyboard bindings (`bind ...`): works only when *bash's readline library* is asking for input (i.e. mostly limited to command input and `read -e`) * `evemu-record`: keyboard watching program (on-demand), across all processes * `triggerhappy`: keyboard watching daemon (always-on), across all processes (might be able to use this on-demand, but no guarantees)
[https://imgur.com/a/DJ1OIic](https://imgur.com/a/DJ1OIic) I edited the post. But here's the wrapping problem I'm actually trying to fix which I assumed was from bad PS1 escaping.
I added a picture to the post. Here's the issue for me: [https://imgur.com/a/DJ1OIic](https://imgur.com/a/DJ1OIic) The wrapping is busted up and I'm not sure why. I assumed it's PS1 improperly counting line length.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/EuOy5Nt.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20eeo2w6j) 
you are redirecting the standard-output of "at" that it gives when you ask it to schedule the job, not the output of the scheduled command when it finally runs later. additionally, when invoking "at" to schedule a job, it outputs its status message, eg... warning: commands will be executed using /bin/sh job 4 at Tue Jan 22 04:05:00 2019 ... to *standard-error*, not standard-output which is why it appears on your terminal, rather than going to the file "log" the following will collect standard-output and standard-error of the "ls" to /tmp/job.txt AND collect standard-output and standard-error of invoking "at" to log.txt echo "ls &gt;/tmp/job.txt 2&gt;&amp;1" | at now +1min &gt;log.txt 2&gt;&amp;1 &amp;#x200B;
I thought `2&gt;&amp;1` just means that standard error (file descriptor 2) will get redirected to the same place that standard output (file descriptor 1) gets sent? See [here](https://www.brianstorti.com/understanding-shell-script-idiom-redirect/). Usually that just means you’re writing output and errors to the same log file. I’m not sure what’s going on with OP yet
Maybe: `at now +1 min &lt; echo “ls” &gt; output.txt`? On mobile. Can’t test.
The script will use open/xdg-open/cygstart to open the GitLab GUI for creating a Merge Request in your default browser. So it should work for both HTTP and SSH based remotes.
You're right. Will have to think about this use case.
It hurts me that it doesn't work with while/until
it hurts me that it doesn't work with anything but for tbh, case esac and so on are kinda meh
`rm -rf /volume/food/*/*` should do what you want.
how about: find volume/food/ -mindepth 2 -type d | xargs rmdir
Hello! I suck at find because **glob** is so good at solving my problems. Always use a dry run when you script `rm`, and definitely always do that when you use code written by strangers who have had a *certain amount* of wine to drink 2019-01-22T05:27:07-0500 henry@Television:~ [362] mkdir -p volume/food/{fruit,veg} 2019-01-22T05:27:13-0500 henry@Television:~ [363] touch volume/food/{fruit,veg}/{avacado,tomato} 2019-01-22T05:27:17-0500 henry@Television:~ [364] for fd in volume/food/*/*; do &gt; [[ -e $fd ]] &amp;&amp; rm -r $fd; done \ &gt; # USE A DRY RUN FOR FUCK'S SAKE \ 2019-01-22T05:28:33-0500 henry@Television:~ [365] ll volume/food/* volume/food/fruit: total 0 drwxrwxrwx 1 henry henry 512 Jan 22 05:28 ./ drwxrwxrwx 1 henry henry 512 Jan 22 05:27 ../ volume/food/veg: total 0 drwxrwxrwx 1 henry henry 512 Jan 22 05:28 ./ drwxrwxrwx 1 henry henry 512 Jan 22 05:27 ../
You can also use `ls -p` to list all regular files but not directories, and send that to xargs to delete them. 
This is my winner. Thanks, man! ; )
No probs, happy to help!
This is my winner. Thanks, man! ; )
It’s generally a bad idea to use the result of ls in pipelines afaik
`-p Write a slash (`/') after each filename if that file is a directory.` Am I using the wrong `ls`?
Is the trailing `/*` also necessary? I'd think that the first `/*` already covers every file and directory (recursively, since `-r`) within the `/volume/food` directory.
OP wanted to keep the subdirs under `food`.
Ah, you're right. Don't know why that didn't click for a second.
Oh, I hope not. I've been doing that in Linux for almost 20 years....
This works IF you don't have symbolic links `ls -p | grep -v '/$'`
Your main problem is the input format. It's deceptively hard to parse quotes. If you can modfiy the format to be easier to parse, you simplify the problem greatly. For instance, instead of "IT Support" g r-x y n Sections/IT/* Put the filename at the end, without quotes. Since it's the only field that can contain whitespace, you can split on all whitespace upto that point. So change it to: g r-x y n Sections/IT/* IT Support And now you can easily parse it with a loop like while read -r type acl recurse default dirlist file; do if [[ -z $type || $type = "#"* ]]; then continue fi ... done &lt; "$inputfile"
You should definitely stop doing that, if possible.
maybe that feeling is God telling us to nut up and learn C
BSD tar can do that, most of the time. bsdtar xvf foobar # works most of the time
Well, that got past the whitespace-in-pathnames section, though introduced another issue further along my input file -- namely: These lines work, when setting ``IFS=''``: Name|Type|ACL|Recurse|Default|Dir(s) ---|---|---|---|---|--- Admin|u|rwx|y|y|- "IT Support"|g|rwx|y|y|Sections/IT/* John|u|---|n|n|Sections/* But, The following lines don't work: Name|Type|ACL|Recurse|Default|Dir(s) ---|---|---|---|---|--- Mgr1|u|rwx|y|y|Users/John Users/Mary Users/Mark Users/Pingu So, the original idea of the script was to have one ACL per line, with the last "column" of the input line being a list of one or more directories to apply the ACLs to. Setting IFS gets past my first issue, but breaks those lines (near the end of the file) that contain several individual directories to apply the same ACL to. At worst, I can always just change my input file to make things less of a pain to deal with on the whole--I'd just prefer not to if I can, especially if that means learning more about scripting or bash.
thank you I've never heard of that I'll check it out
As a shell function: extract () { shopt -qs nocasematch for filename in $* do if [ -f "$filename" ] then case "$filename" in *.tar.bz2) tar xvjf "$filename" ;; *.tar.gz) tar xvzf "$filename" ;; *.bz2) bunzip2 "$filename" ;; *.rar) unrar x "$filename" ;; *.gz) gunzip "$filename" ;; *.tar|*.dmp) tar xvf "$filename" ;; *.tbz2) tar xvjf "$filename" ;; *.tgz) tar xvzf "$filename" ;; *.zip) unzip "$filename" ;; *.Z) uncompress "$filename" ;; *) echo "'"$filename"' cannot be extracted via extract()" ;; esac else echo "'$1' is not a valid file" fi done shopt -qu nocasematch } 
FYI, I use bsdtar on .sh files from GOG to extract embedded binary archives and it doesn't break a sweat. bsdtar is better in many many ways to GNU tar.
One possible reason you've never heard of it: It's actually part of the [libarchive](http://www.libarchive.org/) project, so depending on your platform, you may have to search for a "libarchive" package instead of "bsdtar".
Hmm. Alrighty. I'll try poking at it again and see if I can replicate the new problem. IFS was just a hunch because I've had similar problems before. I'll let you know if I come up with anything or not :)
 # whereis setfacl setfacl: /usr/bin/setfacl /bin/setfacl /usr/share/man/man1/setfacl.1.gz # apt search setfacl acl/bionic,now 2.2.52-3buil1 amd64 (installed) Access control list utilities It should be whatever canonical packages into Ubuntu Server; which I'm hoping isn't some crazy wrapper -- though, heck if I know. Hope the above provides you with what you're looking for (if not, lemme know). But yes, I'm trying to do that; but some of the paths have whitespace, which is causing all kinds of grief; eg. Works: setfacl -m u:$domainname\\$username:rwx . setfacl -m u:$domainname\\$username:rwx ./a setfacl -m u:$domainname\\$username:rwx ./a/* setfacl -m u:$domainname\\$username:rwx ".a/b c/d/*" setfacl -m u:$domainname\\$username:rwx ./a ./dir2 ``setfacl`` can accept multiple ACL statements, and multiple directories; though for whatever reason isn't handling the quoted paths I've been feeding it. ~I did try "set -x", which I've not used before -- so the output is a little hard for me to parse through...but trying to do that has led me down another idea, which is just to simplify as much as possible... **And I think I found the problem**, or at least where things are actually breaking down. So, in the main run loop, I have a section: for location in $dirlist; do &lt;send appropriate data to the correction function&gt; # For testing, echo the location to stdout echo "\"$location\"" done Here's what I get out of that echo: # Assume the following tree: /path/to/testenv |-dir1 |-dir2 |-dir3 |-Dir Five `-Sections |-sec1 |-sec2 |-sec.3 |-sec with spaces `-sec10 # The following section is due to "-" being translated to "$(pwd)/*" # $(pwd) ==&gt; "/path/to/test/env" "/path/to/test/env/dir1" "/path/to/test/env/dir2" "/path/to/test/env/dir3" "/path/to/test/env/Dir Five" "/path/to/test/env/Sections" [...] # Next, we have those sections I've been having issue with "Sections/sec1" "Sections/sec2" "Sections/sec.3" "Sections/sec" setfacl: Sections/sec : No such File or Directory "with" setfacl: with: No such File or Directory "spaces" setfacl: with: No such File or Directory "Sections/sec10" [...] So, it looks like the paths are expanding before they ever get sent off to any function, or anything else...*but why tho?* Man, this has me confused.
So, I've had to use IFS before (following a SO post, basically); but I'm not sure what that actually does. Do you know of any documentation that explains what that does and, preferably, with some examples? Secondly, I've found out where the root of the issue is -- the paths in my input file are being expanded prior to being sent off to a function...eg: ``Section/sec1/*`` expands to ``Section/sec1/dir1 'Section/sec1/dir two' [...]``...which is not the behavior I was expecting. [See my recent response for more details](https://www.reddit.com/r/bash/comments/aiff5r/prevent_string_splitting_within_a_function_setfacl/eepgezz/)
Not that it matters a whole lot, but in this example "IT Support" is a group name, not a file name. I'm applying ACLs to directories and their contents for each given user or group. Now, normally groups can't have names, but this is all related to Samba, hence quoting group names (as per documentation). ~Still, I've recently discoved that ``Sections/IT/*`` is being expanded at runtime, whereas I thought it would have come in as text without expansion.
thank you i'll give this a try if i can't figure out bsdtar
what's GOG? bsdtar worked like a charm running it on the specific file bsdtar -xvf scooby #gives me the next file name which is 1523baad971c884596ad57b1819224a8 so then i ran bsdtar -xvf 1523baad971c884596ad57b1819224a8 # output my next file 1c7e8a011c88dd731189672dcb7044dd. I think they compressed the file 100 plus times so the challenge is to write a script to keep automate that process until i get to the last file which is just a text document. &amp;#x200B; with my for loop i was using the variable file &amp;#x200B; file=$(scoobydoo) for extract in $file do bsdtar -xvf $extract &amp;#x200B; but it's not liking that at all so I'm doing research in a while true do loop so my script knows to decompress every new file that was extracted from the old one
GOG is a game reseller. I'd use a variables and maybe some `bsdtar tvf`... But I'm not doing your exercise :)
thanks for this script i learned some good stuff but it's not going to work for what i'm trying to do since none of the files have the file extensions
Like any sort of recursive function, you write the base case, then given the n^th case, you write it down as the something done to the n-1^th case. Did you even try...?
yep.
I'm on a mac, so `sha1` and `sha1sum` may produce different results (because of newlines), but if you will always hash the value the same amount of times I would keep it simple: $&gt; var="test" $&gt; echo $(sha1sum &lt;&lt;&lt; "$(sha1sum &lt;&lt;&lt; "$(sha1sum &lt;&lt;&lt; "${var}")")") 3f18e7bc4021e72e52fc1395ece85d82f912a74a - But if you need to do recursion and change the number of times you run the hash, I would not worry about elegance so much as readability: #!/usr/bin/env bash # set counter to first arg and store the rest of the args in $holder var counter="${1}"; shift holder="$@" function sumvar() { # hash the contents of our $holder var and store in a temp variable local var=$( sha1sum &lt;&lt;&lt; "${holder}" ) # update our $holder variable holder="${var}" # recursive part - keep hashing until counter = 0 counter=$((counter-1)) ! [ $counter -eq 0 ] &amp;&amp; sumvar } sumvar echo $holder Which gives the same result as above $&gt; ./foo.sh 3 test 3f18e7bc4021e72e52fc1395ece85d82f912a74a -
Disclaimer: SHA1 is not secure. I don't think you need real recursion for this. You can just loop over a dynamic string N times. As specified by your usage: alex(){ n=$1; string=$2; for ((i=0;i&lt;n;i++));{ string=$(sha1sum &lt;&lt;&lt; "$string"); } echo $string }
thanks. I was going to tweak it to use binary instead of hex until the last iteration.
I was just using sha1 as an example, I changed it to sha256 in my attempt at a solution.
`alex(){ n=$1; string=$2; for ((i=0;i&lt;n;i++));{ string=$(openssl dgst -binary -sha256 &lt;&lt;&lt; "$string"); }; echo $string | xxd -l 16 -p; }` `$ alex 9 test` `-bash: warning: command substitution: ignored null byte in input` `388ac4d0564a03d8196c946b6d98f29f`
 #!/bin/bash if [[ $1 -gt 1 ]]; then echo "$(bash "$0" "$(($1 - 1))" "$2")" | sha1sum else echo "$2" | sha1sum fi though the non-recursive version much longer and I think it's a lot more readable: #!/bin/bash i="$1" SHA="$2" while [[ $i -gt 1 ]]; do SHA="$(echo "$SHA" | sha1sum)" i=$((i - 1)) done echo "$SHA" | sha1sum
I don't think it is broken, just verbose. What version of Bash are you running?
Another idea - use an interactive bash script. Use a template-json with empty fields (with some defaults and fallbacks), and to fill it up just run a script that asks all the questions and takes answers. This is completely doable within bash without needing a web-server. 
That's bash telling you that, on one iteration, the binary digest output contained at least one null (0) byte. That's *verboten* in C string handling, and since bash \[a\] is written in C and \[b\] eats nothing but strings, it *removes* all null bytes and warns you about it, meaning you digested a shortened input at one stage. For 100% accuracy with binary data, you need to store intermediate results in a (temp) file, not in bash: alex2(){ n=$1; string=$2; tmpf=$(mktemp) # Make sure not to put an automatic newline in the file echo -n "$string" &gt; $tmpf for ((i=1;i&lt;=n;i++)) { if openssl dgst -binary -sha256 &lt; $tmpf &gt; $tmpf.out; then mv $tmpf.out $tmpf else echo "FATAL ERROR: Unable to generate digest on iteration $i, aborting." return 1 fi } xxd -l 16 -p &lt; $tmpf rm -f ${tmpf}* } Or you could simply drop the `-binary` option to `openssl`, and deal strictly with hexadecimal strings. Or use `sha256sum` directly: alex3() { n=$1; string=$2 for i in $(seq 1 $n) { string=$(sha256sum &lt;&lt;&lt;"$string" | sed 's/ .*$//') } echo $string } It all depends on what you consider the "correct" way of handling intermediate results: binary all the way, or digesting the hex representation thereof. That would largely depend on what you're using the digest for: All the above options go from input to digest output in a deterministic fashion, so I could theoretically still use them as "unique" IDs for the associated content. However, only `alex2` will produce the same results as a program that calculates the "chained" digests internally.
You have to put the comparison into an actual test context: if [ $Joe -lt 5 ] &amp;#x200B; or alternately, using the test command/statement directly: if test $Joe -lt 5 &amp;#x200B; Note that the argument to "if" is a command exit code (ie. the value of the "test" command, or the "\[" command, which is essentially just an alias for "test")
An alternative (and in my opinion, better) approach is to use an arithmetic expression: if (( Joe &lt; 5 )) then ... fi
Try the same setup but with running file on each file piping to regex to look for a set of types. Compressed files show as file types when ran against file.
&gt;EDIT: Just to demonstrate the point that /bin/test is a command, and it is also the exact *same* command as /bin/\[, at least on some systems: Nitpick: Since both `test` and `[` are bash builtins, your `cmp` test isn't actually relevant. The `test` man page (and every other man page for a command which a bash builtin may shadow) even acknowledges this: &gt;NOTE: your shell may have its own version of `test` and/or `[`, which usually supersedes the version described here. Please refer to your shell's documentation for details about the options it supports. In this case, it's more useful to [RTFM](http://man7.org/linux/man-pages/man1/bash.1.html#SHELL_BUILTIN_COMMANDS): &gt;**SHELL BUILTIN COMMANDS** &gt; &gt;\[...\] &gt; &gt;`test` *expr* &gt; &gt;`[` *expr* `]` &gt; &gt;Return a status of `0` (true) or `1` (false) depending on the evaluation of the conditional expression *expr*. \[...\]
Actually, it is really important to know/understand that `[`...`]` does not create a "test context" and in fact it's just a command like any other, just like `test`, `ls`, etc. It's really evil that way: it is designed to make you think it's shell syntax, but it's not. There's even a `/bin/[` external command version. This can be really treacherous, e.g. `if [ 1 &gt; 2 ]; then ...` evaluates as true and creates an empty file called "2" on your disk (because the `&gt; 2` is an output redirection like for every other command). There's also a `[[`...`]]` version which *is* integrated into the shell syntax and does create its own lexical context. It is safer to use.
&gt;Nitpick: Since both test and \[ are bash builtins, your cmp test isn't actually relevant. I explicitly mentioned the use of builtins. I think pointing out the existence of the binaries is an enlightening way to demonstrate how this way of using test (and the '\[' syntax) for scripts came to be.
Looks like a terminal bug to me, in that it's somehow getting confused by the ANSI escape codes in your PS1. I can confirm that LXTerminal doesn't exhibit this issue: https://imgur.com/a/stCG2we 
I have never seen that first syntax before. That's cool.
Well crap. I've lived with it for a few months... I suppose it's just my life now, lol. No idea how to continue to troubleshoot.
It's kinda hard to determine what the real issue is, since you've not shown us: 1. **exactly** what command you typed in 2. **exactly** what you saw (error messages, etc.) In any case, I can confirm that bash does no such interpretation: $ touch 2 $ \ls -l total 0 -rw-rw-r-- 1 aho aho 0 Jan 23 10:45 2 $ mv 2 1 $ \ls -l total 0 -rw-rw-r-- 1 aho aho 0 Jan 23 10:45 1
What `mv` does with its arguments has nothing to do with Bash.
 data='ls -A ~/library/Application\ Support/Google/Chrome/default/databases/chrome-extension_edacconmaakjimmfgnblocblbcdcpbko_0' data_folder='~/library/Application\ Support/Google/Chrome/default/databases/chrome-extension_edacconmaakjimmfgnblocblbcdcpbko_0' data | evalData=$data number=`evalData` mv ~/downloads/2 ${data_folder}/$number &amp;#x200B;
This is merely a problem with quoting. Unless you have a good reason not to, you should always ensure parameters are inside double-quotes, e.g.: mv ~/downloads/2 "$data_folder/$number" 
I removed the other two lines to show clearly what I'm trying to do
You've also changed the error message you got. The problem _now_ is that you've used `~` inside a single-quoted string. All forms of expansion, including tilde expansion, do not occur inside a single-quoted string.
`[[` is safer, faster, disables word splitting, and comes with built-in regex operator `=~`. To my knowledge there are no downsides outside of portability.
`((` is way more than integer comparison. It's basically "C mode" all the way down to bitwise math. https://wiki.bash-hackers.org/syntax/arith_expr https://wiki.bash-hackers.org/syntax/ccmd/c_for
Got it. I moved the tilde outside of the string and it's now in the \`mv\` expression. 1. get the filename of a single file in the \`data\` filepath. The folder only has one file, so \`data\` returns the filename which is '1'. 2. move the '2' file in \`\~/Downloads\` to the target path 'data\_folder' and rename it as the filename in the 'data' filepath (which is 1). So 2 is moved to the target path and renamed to 1.
Holy moly, Batman! So much context missing from *I need to rename a file '2' to '1'*. Several things are clear now: 1. Based on the directory names, you're running macOS. If you haven't already done so, follow the instructions [here](https://itnext.io/upgrading-bash-on-macos-7138bd1066ba) to upgrade your bash to the latest-and-greatest; many answers past and future will assume access to bash v4+ features like associative arrays, which simply aren't available with the antediluvian macOS system bash. 2. You're trying to replace an SQLite database used by a Chrome extension. You should probably stop Chrome first, to avoid potential data corruption. 3. There's only one (SQLite) file in the directory you're moving to. (At least, there *should* be, if you're expecting this to work.) In that case: data_folder=~/library/Application\ Support/Google/Chrome/default/databases/chrome-extension_edacconmaakjimmfgnblocblbcdcpbko_0 mv ~/downloads/2 "$data_folder"/* This does several things your attempt doesn't: 1. It lets bash do pathname expansion on assignment to `data_folder`, thus properly resolving the `~` 2. It lets bash do filename completion on the final command argument, to get the proper full pathname to the (only?) file in your target directory. Oh, and if you *still* get a `mv: ... is not a directory` error, that means the last command got expanded to the equivalent of: mv ~/downloads/2 "$data_folder"/existing_file1 "$data_folder"/existing_file2 ... It also means you should've checked the target directory first, to determine which file you wanted to overwrite.
High level -- [I'm restoring a Session Buddy db file from one computer to another](https://sessionbuddy.com/backup-restore-advanced/). So yes, there's only one db file in the target\_path. Yes, in my script, I quit Chrome prior to the restoration commands lol. &amp;#x200B; I'm running version 5.0.0(1)-release (x86\_64-apple-darwin18.2.0)
I'll assume you actually checked, because there might also be a `1.journal` file in there, if Chrome failed to update the main DB (`1`) during its last run for some reason. Since you're not doing _exactly_ the same thing the link you pointed to instructed (you're replacing the DB file only, instead of the entire folder), you'll have to _delete_ that file if you see it, or risk SQLite-in-Chrome getting completely confused and potentially trashing the DB.
I can think of a few more downsides: * It copies the arcane options syntax of the `[` command for the most part. Commands should be more legible than that. * It introduces yet another entirely distinct shell grammar context with its own complex rules to learn. Parentheses, `&amp;&amp;`, `||`, `&lt;`, `&gt;`, glob characters, etc. all mean different things within than outside. * Confusion. Because `[[` looks so similar to `[`, far too many scripters don't realise they are *fundamentally different* and their scripts use them interchangeably. Big mistake. * Although it integrates into the shell grammar, it's also still a command trying to masquerade as shell grammar -- so you still need spaces around the `[[` and `]]`, and it's still not intuitive that it isn't actually part of the `if` construct.
yes, something like this: target_path=~/library/Application\ Support/Google/Chrome/default/databases/chrome-extension_edacconmaakjimmfgnblocblbcdcpbko_0 touch "$target_path"/1-journal if [ -f "$target_path"/1-journal ]; then echo exists else echo null fi in my DB folder, there was only the single '1' db file. During the \`mv\` operation using the wildcard, if the '1' file exists when I move the renamed '2' SQLite db, does it simply replace the pre-existing '1' file? Following the execution of the script, the original '1' db was replaced with the '2' SQLite db file from \~/Downloads and when I opened Chrome, the restore was verified.
Do not script using `ls`. What you want is `find`
I switched to \`-f\`, thanks for noticing &amp;#x200B;
These are all good points, but from my perspective they're all positive arguments for ditching Old Test entirely. Everything you listed is also a problem with the old method. * GNU-style options are concise and effective * Distinct grammar allows for complex logic * It's not confusing if we stop using `[` * `[[` should replace `if`, not be hindered by it 
This solution here uses recursion: $ hash() { if (( $1 &gt; 0 )); then sha256sum | tr -d ' \-\n' | hash $(($1 - 1)); else cat; echo; fi; } Here it is in action: $ hash 1 &lt;&lt;&lt; "test" f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2 $ hash 2 &lt;&lt;&lt; "test" 5e144a0cb4504bc902690c138946b3cfc8d71141eb23af512038024c8e19609f $ hash 3 &lt;&lt;&lt; "test" 7fcd172af88c06e808c4a8e0a3f2cc69d0a6cb495d3224649dc2a7ceddf02600 And here's how I tried to check if it worked right: $ sha256sum &lt;&lt;&lt; "test" f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2 - $ printf "f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2" | sha256sum 5e144a0cb4504bc902690c138946b3cfc8d71141eb23af512038024c8e19609f - $ printf "5e144a0cb4504bc902690c138946b3cfc8d71141eb23af512038024c8e19609f" | sha256sum 7fcd172af88c06e808c4a8e0a3f2cc69d0a6cb495d3224649dc2a7ceddf02600 - There's an annoying thing about new-lines happening with this kind of problem. When you send data into sha256sum with "echo" or with "&lt;&lt;&lt;", you get a new-line added and sha256sum's result will change. You need to make sure you use newlines everywhere exactly the same. See here: $ sha256sum &lt;&lt;&lt; "f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2" a5d172f02c722bcd1593f632378eabd891b5eff7264c1f36d791509845948d2b - $ printf "f2ca1bb6c7e907d06dafe4687e579fce76b37e4e93b7605022da52e6ccc26fd2" | sha256sum 5e144a0cb4504bc902690c138946b3cfc8d71141eb23af512038024c8e19609f - I made the decision to cut the new-lines away with 'tr' from the checksums that sha256sum prints, then at the end I added a new-line back so that the final output looks like how normal Unix tools output text. Something else: Using recursion in bash might be a bad idea. It seems to be much slower than using loops and variables. It also has a rather low limit, and it will make bash crash when the limit is hit. I tried to experiment a little: $ count() { if (( $1 &gt;= 1 )); then count $(($1 - 1)); echo $1; fi; } $ count 3 1 2 3 The same as a loop: $ loopcount() { local x; for (( x = 1; x &lt;= $1; x++ )); do echo $x; done; } Comparing speed, I get this: $ time count 5000 ... 4998 4999 5000 real 0m2.598s user 0m2.589s sys 0m0.004s $ time loopcount 5000 ... 4998 4999 5000 real 0m0.025s user 0m0.019s sys 0m0.007s Bash crashes when I try to run `count 10000`.
What do you get from `shopt | grep win`
You are on the right track. One way you can expand on this is like so... &amp;#x200B; for extract in $file do f\_exten=$(echo $extract | awk '{print $2}') if \[\[ $f\_exten == \*bzip2\* \]\];then bunzip $f\_exten elif \[\[ $f\_exten == \*gzip\* \]\] gunzip $f\_exten fi &amp;#x200B; f\_exten is the file in question. If its really extract, then change out the $f\_exten to $extract &amp;#x200B; Hope this helps! &amp;#x200B;
haha thanks for your help and don't worry this isn't graded just an old file that was probably used in the past for class
oh nice! I haven't seen the double square brackets before going to see what that means in the code too thank you so much
that's a good idea regex didn't even cross my mind
Well, I found a solution -- though, its not what I was expecting at all. From what i can tell, my listing of relative paths in the input file were, at runtime, being expanded. I didn't know that would occur, as I assumed ``read`` would view the entire line as plain-text; just like ``sed`` would (right?). So, to resolve this issue within minimal changes to the over design, I've had to: 1. Ensure that I'm **always** double-quoting paths, both in the script itself, and the input file 2. In the run loop of the script, ensure that I remove ``"`` for the directory listing -- such that it is still passed as plain-text, but will be processed appropriately 3. Simplify Anyway, I've updated the gist to show my solution, should anyone run into a similar situation. Likewise, [this post](https://unix.stackexchange.com/a/261735) lead me to the reason behind path expansion, as well as how to prevent it in the future. While IFS and similar responses *weren't* the solution, they still told me to look a little deeper than I had previously--so, I'm really appreciative, as I'm not sure I would have stumbled across the solution on my own otherwise (at least, not in a timely fashion). Thanks!
checkwinsize on That's what I was expecting, but no closer to the issue :/
UPDATE: Thank everyone here for your assistance. u/moviuro bsdtar is pretty awesome! This is my final script that worked all the way until the last file left to decompress w/ is a gzip i think i read that bsdtar doesn't work w/ gzip but anyways thanks everyone! &amp;#x200B; \#!/bin/bash &amp;#x200B; while true; do new=$(ls /root/mystery/misty | grep -v [new.sh](https://new.sh) | grep -v list | xargs bsdtar -xvf 2&gt;&amp;1) new1=$(echo $new | sed s/'x '//g); echo $new1 ls /root/mystery/misty | grep -v $new1 | grep -v [new.sh](https://new.sh) | xargs rm done &amp;#x200B; &amp;#x200B;
No, seriously, you shouldn't use `ls` at all in a script. It's not intended for script use. http://mywiki.wooledge.org/ParsingLs What you want is `find`which will return the full path, making "$target_path/$filename" an unnecessary construction. You can simply use the -print0 formatted value of `find` to set $full_path_and_filename 
thanks for the responses I got it to work using (( Joe &lt; 5 ))
If $ML is an IP address (as in single), I don't understand why you're using a loop. If it's multiple addresses, I don't understand why you're using "while true" (e.g. infinite loop). Also also, I'm curious why you're expecting "hostname" for an IP lookup, instead of "pointer". In the case of a single IP: `ML="10.0.1.2"; ssh user@server1 host $ML` In the case of a bunch of IPs (e.g. in a file) for ML in `cat IP_list.txt`; do ssh user@server1 host $ML; done 
Sorry, $ML is multiple ip addresses, that im looping through. the infinite loops was one thing that worked for me, i thought i could just break out of that loop one i have what i wanted.
You want a for loop, not a while loop. for ML in (list); do &lt;commands&gt;; done
The reason I didn't do a for loop is because it asks me for my credentials to 'server1' every time, and i get empty results ssh server1 " for i in $ML; do host $i | grep 'hostname'; done"
Damn. Ok, can you try this: 1) Download [this gist](https://gist.github.com/rawiriblundell/2c4631611052509a55c658cd0efaab35) to somewhere like `~/.setprompt` 2) In your `.bashrc` file, comment out your `PS1` and `PROMPT_COMMAND` vars 3) Enter `. "${HOME}/.setprompt` 4) Save and exit 5) `. ~/.bashrc` to load the changes All going well, you should have a working prompt. Try `setprompt -h`, read about the available options, play with it and ultimately see if you can replicate the broken line wrapping behaviour. If the line wrapping behaviour occurs with `setprompt`, then it's probably the case that your `PS1` is fine, and the issue is caused by something else.
I'm still not getting the host name grep - are you scanning for a specific hostname (e.g. it's .... do host $i | grep 'my_server'; done)? or are you looking for the literal word hostname? Is ML a list of 12 IPs that are all unrelated? or is it a range? It's going to be difficult to put a local array into a remote loop, but you could do something like `ssh server1 for IP in {1...254}; do host 192.168.1.$IP | grep 'hostname'; done`
Yes im scanning for a specific host name. so ML looks like `echo $TRDI.$i` TRD is the first 3 octets in an ip address and i loops through 1-254. Its all a range. thanks ill try that out and let you know
so rather than $LR and $ML, I would just specify {1..254} and use `host first.three.octets.$IP` in the SSH command. 
Neat! But since you phrased the problem as `shift`, my mind went to a different solution at first, and it turns out that this also works: args=("$@") set -- "${arr[@]}" shift 1 arr=("$@") set -- "${args[@]}" I. e., temporarily exchange the array and the positional parameters, use regular `shift`, then exchange them again. Probably less efficient, but perhaps more readable?
The connect=$(...) notation executes the command and returns its stdout. I think you just want the interpolated string instead which would be connect="...". If you don't want the variables interpolated yet, then use connect='...'. It looks like you want the command to actually execute in the second assignment, so there you would use powerstatus=$(...) instead of powerstatus="...".
the only problem is the first three octets can change based on user input
I think the reason that you don't get anything is because connect=$(...) captures only the stdout, but the help message is returned on stderr. Try the following and see what happens: connect=$(racadm -r xxx.xxx.xxx.xxx -u root -p password 2&gt;&amp;1) echo "$connect" &amp;#x200B;
&gt; there are no downsides outside of portability. This is true, but there are edge cases where users will be confused. On debian-based systems, "sh" is dash, not bash, and dash doesn't support [[ This results in a confusing circumatance where `./foo.sh` and `bash foo.sh` work, but `sh foo.sh` doesn't.
So this is my test script, tried to copy what you have above . /opt/dell/poweredge/deploycfg.sh test=$(racadm -r $idracip -u $idracun -p $idracpw --nocertwarn 2&gt;&amp;1) echo "$test" and I get no output when running [root@deploy ~]# sh /opt/dell/poweredge/deploy2.sh [root@deploy ~]# 
It looks like you ran the wrong script. [deploycfg.sh](https://deploycfg.sh) != deploy2.sh
yeah I did, my eyes are burning from looking at the screen for too long. Someone else gave me an answer that did the trick. Here's the final script that works. . /opt/dell/poweredge/deploycfg.sh connect="racadm -r $idracip -u $idracun -p $idracpw --nocertwarn" powerstatus=$($connect getsysinfo | grep Power\ Status | grep -E -o 'ON|OFF') echo $powerstatus
My bad, I see that you are sourcing [deploycfg.sh](https://deploycfg.sh) in [deploy2.sh](https://deploy2.sh). In that case, I would guess that [deploycfg.sh](https://deploycfg.sh) is either terminating the entire process early or maybe it is redirecting output elsewhere. You might want to experiment with some simpler commands first until you get the concepts figured out. For example: test_string1='$(date -d now)' echo "test_string1 is '$test_string1'" test_string2="$(date -d now)" echo "test_string2 is '$test_string2'" test_string3="$(date -d never)" echo "test_string3 is '$test_string3'" test_string4="$(date -d never 2&gt;&amp;1)" echo "test_string4 is '$test_string4'" &amp;#x200B; &amp;#x200B;
https://git-scm.com/docs/git-submodule
&gt;it asks me for my credentials to 'server1' every time Nope. With the command line you posted: ssh server1 " for i in $ML; do host $i | grep 'hostname'; done" ssh does *one* connection to server1, then executes the `for` loop there. If you were prompted to login *N* times, then you actually ran some other command line. That said, there are a couple of possible reasons why you might get an empty output, even with the above command line... Both `$ML` and `$i` are expanded on the client side, so the command that was sent over is actually: for i in $ML; do host current_i_value | grep 'hostname'; done i.e. you're looking up the same (possibly non-hostname) value *N* times. Also, assuming the IP addresses are mapped to domain name via DNS `PTR` records, your `grep` may itself be wrong. For instance, with `host` on my system: $ host -V host 9.11.3-1ubuntu1.3-Ubuntu $ host 192.168.1.1 1.1.168.192.in-addr.arpa domain name pointer gw1.my,dom. $ host 192.168.1.1 | grep hostname I suggest you first run this command: ssh server1 "for i in $ML; do host \$i; done" (note the backslash to prevent variable substitution of `i` on the client side). Look at the output, then add the appropriate `grep`: ssh server1 "for i in $ML; do host \$i; done" | grep something Note that I'm grepping on the *client* side, to also filter out any other output that may come from a login session, like the "message of the day" and other unnecessary details.
Thanks for your clarification! Maybe I wasn't clear in my post, but my goal was to capture the status message generated after scheduling a job with "at". I just don't understand why this output is considered to be "standard-error" and not "standard-output", because I can't see any warning or error in it when I run it.
I'm sorry. I meant grep. It was a long day when I posted that. Use a grep regex to look for specific filenames. Ensure it captures exactly what you need.
Was going to comment the same. This is cool as a personal project to learn, but it’s reinventing the wheel. Git submodules are superior in this case because you could create a single package from which you could clone and replicate anywhere you want. Lose your personal server? Spin up a new one, clone your base repo, and pull all of the modules. Boom. 
Seems to be largely the same as the `repo` tool from Android, right?
The shell keywords (reserved words) `while`...`do`...`done` are among a few that define a code block in the shell. Others are `{`...`}` [yes, they are keywords so they need whitespace around them], `if`...`then`...`fi`, `case`...`esac`. If you add an input or output redirection at the end of any of these, it applies to the entire block. So for the example you're citing, reading from standard input would actually read from `url_list.txt` for the entire `while` loop. Presumably there is actually a command like `read url` within that loop that reads from standard input, though you didn't cite one. If the input redirection were attached to that `read url` instead of to the loop as a whole, then it would repeatedly read the first line of that file, because it would repeatedly open and close the file for every `read` command. Whereas, if you add it to the loop block, it opens it once for the entire loop run, and repeated reads will read subsequent lines from the file.
&gt; Presumably there is a command like `read url` within that loop that reads lines from standard input, though you didn't cite one. `while read url`
That's ok I figured egrep
Yes – like I said, within that loop.
? OP uses a variant of this: while read line; do : # .... done &lt; file This is the most common way to read a file line-by-line in pure Bash, as far as I’m aware, and apart from a few nitpicks (`IFS=`, `read -r`) it’s also correct. There’s no need to put the `read` inside the loop’s body, since the condition commands also get the same stdin.
The first problem I see is that &gt;echo -n " Enter your score : " read SCORE Should be read -p "Enter your score : " SCORE Try that and see if it helps. 
&gt; For exemple, if I write this : &gt; &gt; if [[ "${tab[$((x-1)),$((y+1))]}" == $CELL ]] ; then ((Counter++)) ; fi &gt; &gt; It's the same as &gt; &gt; [[ "${tab[$(($x-1)),$(($y-1))]}" == $CELL ]] &amp;&amp; ((Counter++)) &gt; &gt; It's correct ? It's not quite the same. Both will only execute the command if the condition is met, but they act differently if the condition is not met. If the condition is not met, the first form will continue with the exit status (`$?`) set to 0, whereas the second form will continue with the exit status set to 1. This may be important, depending on your script. &gt; Why this test doesn't work ? Because your first example didn't just use `if`, it used `elif` (else if) to execute only one of the possibilities instead of all the possibilities that meet the criteria. In other words, your second script is more like if [ $SCORE -lt 10 ] then echo " It's enought" fi if [ $SCORE -le 12 ] then echo " It's ok ! " fi if [ $SCORE -le 14 ] then echo " Not bad ! " fi if [ $SCORE -le 16 ] then echo " Good ! " fi if [ $SCORE -le 20 ] then echo " Very good ! " fi 
But how does it works. Does shell reads &lt;file first and then loops through all lines? I thought that this is something that should be at first line but as I said, I'm coming from powershell where things are different but I'm currently all Linux so it's for some scripting but I don't fully get how it is working. Thanks again! 
You're absolutely right, total brain fart, for some reason I didn't see that `read url` in the OP.
What the code is doing is reading through the file `url_list.txt` line-by-line, assigning each line to the variable `url`, and then appending the contents of webpage the file `site.html`. The `&lt;` symbols is essentially saying "use this file is input for the while loop." &amp;#x200B; Some relevant pages from my go-to BASH guides: * [Redirection: Redirecting Input | Bash Hacker's Wiki](https://wiki.bash-hackers.org/syntax/redirection) * [Input and Output: File Redirection | Greg's Wiki](http://mywiki.wooledge.org/BashGuide/InputAndOutput#File_Redirection) * [IO Redirection | TLDP](https://www.tldp.org/LDP/abs/html/io-redirection.html)
You need to use the command `continue` after the `echo` to stop processing the current loop and start it from the beginning, otherwise you just execute the next test in line, which may or may not also be true.
Thank you very much! :) 
Break it down this way.... While read url do curl "$url" &gt;&gt; site.html done &lt; url_list.txt and look at the first line..... `While read url` - where does the reading of an item that is called `url` *come from*? The file that is fed into it.... `url_list.txt` And the output of `curl` is fed into `site.html`
No prob. Honestly, I've been using bash for over a decade and had to actually test it out to be sure. I generally use a pipe to accomplish to accomplish such tasks: cat url_list.txt | while read url; do curl "$url" &gt;&gt; site.html done but using the &lt; redirector is generally a better idea because the pipe will cause everything within the loop to occur in a sub-process (i.e. variables changed within the loop won't be reflected outside of the loop).
That's the beauty of shell scripting and Linux it self. There is always a different way to do something. As long as it's working for me - it's good but I'm always curious if there is another way but in this case, I was unsure what &lt; does at the end of the script because I thought that it should be at the beginning not the end :) Thanks again 
Yup, very true. And yeah, no prob.
Sorry, I'm unfamiliar with that
I've not used git submodules before. I'll try it out
Its seen as a complete block. Read expects input and gets it from &lt;file. &lt;file is really part of the whole instruction. If there is no input for read it just waits patiently until it receives it (by the user). So for example. while read line; do echo $line; break; done This line will hang until the user would type something and hit enter. Now the the iteration will start, sending the same thing typed to stdout and leave the loop. Additionally: while read line; do echo $line &gt;&gt; file; done This will append everything typed by the user to file until, the while loop is manually stopped. (by ctrl+c) Also, in your example, the file is not read completely by the shell and than looped. It is read _and_ simaltaneously looped. So this line will loop for ever if file is not empty: while read line; do echo $line &gt;&gt; file; done &lt; file
Si it's impossible to use this syntax : [[ "${tab[$(($x-1)),$(($y-1))]}" == $CELL ]] &amp;&amp; ((Counter++)) For my other code ? 
Well, My previous update was less than accurate. Still hammering out a solution to get the script functional in production; though the root cause of the initial error is still correct and, based on my changes to the input and initial parsing of the file, that portion is resolved. --- Now, the remaining bits is figuring out how to properly handle all of the directories and files with *special characters* in the name, eg ``dir: /path/to/MINE!!!/``` -- hooray Windows, allowing silliness like this.
I would probably convert it to epoch time and then calculate the difference between the two, then walk through a nested `if`s for example: today=$(date +%s) date2=$(date +%s -d 2020-01-01) difference=$(( date2 - today )) while [[ $difference -ge 31557600 ]] then difference=$(( difference - 31557600 )) yeardiff=$(( yeardiff + 1 )) fi while [[ $difference -ge 2629800 ]] then difference=$(( difference - 2629800 )) monthdiff=$(( monthdiff + 1 )) fi ... and then build upon that
How is that a problem? I don't see it making any difference at all for his purposes. 
I downloaded the source code of "at" and took a look, it looks like it pretty much outputs everything to standard-error - but there are no obvious helpful comments describing why. I suspect it might just be historical. "at" is a very old command, stretching back to early versions of UNIX... and if that's what classical versions did when it became well-established, then it's pretty hard to subsequently change behaviour without breaking lots of stuff that has already been created and working. It might be documented in POSIX.2 spec, which states "standard behaviour" for confirming implementations, but i don't have access to it at the moment. &amp;#x200B;
Haha, I was so confused – “I know /u/McDutchie is competent… did they really miss the `read url` even when I quoted it?” :D
The easiest thing I can think of is make a time ordered list of your friendly date strings and iterate it. friendlydate(){ tin="$(date --date="$1" +%s)" ordered=('last week' 'yesterday' 'today' 'tomorrow' 'next week') olen=${#ordered[@]} for ((ii=0; ii&lt;(olen-1); ii++)); do t0="$(date --date="${ordered[ii]}" +%s)" t1="$(date --date="${ordered[ii+1]}" +%s)" if [[ $tin -ge $t0 &amp;&amp; $tin -le $t1 ]]; then echo "Time is ${ordered[$ii]}" return fi done echo "Time is after ${ordered[$olen-1]}" } If you try `friendlydate "$(date --date='today')"`, it will output `Time is yesterday` because `date --date='today'` puts out a time that will be a few seconds behind when it checks it in the script. You could tweak this to be more accurate though. 
Apologies, function was the wrong word to use. By function I actually mean an external shell script that takes in arguments. Looking at the debug logs for certbot-auto you were correct in saying it takes in the following: \["-d","example.com","-d","[www.example.com](https://www.example.com)"\]. I managed to implement it in a different way and it seems to work, but thank you for you time in assisting me. :)
 bind '"\e[1;5A": history-search-backward' bind '"\e[1;5B": history-search-forward' ? this is based on running "strace cat" and then pressing ctrl+up and ctrl-down on my machine, to see what it sends. it then worked for me when i did the bind's
wot?
bind '"\e[1;5A": history-search-backward' bind '"\e[1;5B": history-search-forward' should work. This is because ctrl and arrow produces a difference keycode, you can find out what it is by using ctrl-v in a terminal followed by a key combination.
 env X='' ls'' runs the ls command, with a modified environment, including the variable X set to nothing &amp;#x200B; env X=''. ls'' runs the ls command, with a modified environment, including the variable X set to a full stop &amp;#x200B;
And `X='' ls''`? 
And? Filenames can have carriage returns in them if you like.... What if..... you [read the spec](http://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html#tag_03_170)??
Thx :) 
If you write `X=''. ls''` on BASH, X is unset and ls is executed. 
The strace package on Arch? I've downloaded it and, [this](https://imgur.com/a/QCHfDjZ) is the output when I press ctrl+up/down then enter. I've tried your bindings and the ones I received from strace cat but none of them worked (or rather, I don't know what codes to look for when using strace cat).
 bind '"\eOa": history-search-backward' bind '"\eOb": history-search-forward' ?
I'm soo sorry for having misunderstood the issue there: X='' (is executed first) ls (after) '' (last) This results in X set to nothing and `ls` executed
actually, "showkey -a" might help... below is my up, down, ctrl-up and ctrl-down... the first two characters \^\[ is the escape \\e &amp;#x200B; neil@tvpc:\~$ showkey -a Press any keys - Ctrl-D will terminate this program \^\[\[A 27 0033 0x1b \^\[\[B 27 0033 0x1b \^\[\[1;5A 27 0033 0x1b \^\[\[1;5B 27 0033 0x1b &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
Thank you so much, even my teacher couldn't explain this behavior when I asked him. I was really curious about this and I couldn't forget it :)
Try adding "yarn" after ngserve on the last line: `complete -F __project_complete ngserve yarn`
rm *:02*
See also: https://mywiki.wooledge.org/glob
Thank You!
I like your approach! Did you know Bash uses the `!` operator to automatically iterate over the indices of an array? for ii in ${!ordered[@]}; do
Thank you! They work perfectly. :)
So the cells only move one grid-cell at a time so they're very predictable, so you can just check for edge values and set the new position accordingly e.g. if the length of the world-grid is 4 you just check if a cells position is 4 and then set it to 0. You can do this using modulus but it's more effort than it's worth imo, since they cells never travel more than 1 during a tick.
The problem is that I don't see how to make sure that the border is the upper, lower, left or right border... &amp;#x200B; Since the length of my array depends on my variable $num\_lines and the width of my array depends on $num\_columns, you should make sure that : &amp;#x200B; if [ ${tab[$(($x))]} -eq $num_columns ]] then ${tab[$(($x))) = "0" fi Or something like this ? And the same for $y and $num\_lines ? 
You mean exclude directories ? Sure but I think you will need help from grep. Put the excluded paths in a file. [xyx]$ cat /tmp/exclude.txt /var/spool/postfix/pid/unix.cleanup /var/spool/postfix/pid/unix.local /var/spool/postfix/pid/unix.smtp /var/spool/postfix/pid/unix.defer Then process your find output using grep. find /var/ 2&gt;/dev/null | grep -vwf /tmp/exclude.txt This is using GNU grep etc.
Thank you ill try it when i get home :) 
I am, thanks. Although I'm not a fan of using it personally. In this instance though I didn't want to iterate to the last element in the array because I wanted to check if the time was between element `ii` and `ii+1`. 
It turned out I had a need for something similar, so I wrote [`dateh`: `date` for humans](https://github.com/gromgit/bash_scripts/blob/master/dateh). It wraps GNU `date` with additional output specifications: * `@{d}`: relative date (e.g. yesterday, next Friday), if computable * `@{w}`: relative week (e.g. last week, 3 weeks' time) * `@{m}`: relative month (e.g. last month, 3 months' time) * `@{y}`: relative year (e.g. last year, 3 years' time) Comments welcome.
`complete -F __project_complete yarn` just doing that was enough. Thanks!
Forget about exiting vim, this is the real challenge!
Cool.. Where can we find it?
Argh. Managed to edit the source link out somehow. Thanks for the heads-up.
I like this. I appreciate it. Will have to do some tweaking(and learning). I'm still a novice at bash scripting syntax. I was hoping since the date command had the reverse function built in, there would be some way to tap into that to do the opposite, but I suppose not. The reason I was wanting this is I am using **calcurse** for my daily schedule. I'm having it text me each morning my next 3 days of appointments. I thought it would be nice for it to say instead of today's date on output it could say, "Today". Or instead of tomorrow's date, it could say, "Tomorrow", and then for the 3rd day it could say the day of the week. This is how I currently have it formatted: ``` #!/bin/bash days=$1 if [[ -n $(calcurse -Q --days "$days" --filter-type=cal) ]] then calcurse -Q --days $days --filter-type=cal --format-apt=' - [%(start:%l:%M%p) ] %m\n' --format-event=' - %m\n' | mail -s "" {myphonenumber}@mms.att.net fi ``` So this texts me something like: ``` 01/24/19: - [ 6:30PM ] Bible Study @ Church 01/26/19: * Jordan's Birthday - [ 7:30PM ] Shogun Hibachi &amp; Sushi ( w/ John, Emily, Jared, and Others) ```
The gimmick "... for humans" evocates Kenneth Reiz, famous in the python community, who (over-) use it in his projects. You may want to use a tagline more personal, or not, your choice ! ;)
I will definitely check this out! look at my comment above about how I'm wanting to use this with **calcurse**. Maybe you can give me some pointers on how to integrate your script. Going to download it now and play around.
looks like you can ctl+z to put it in the background, then you can kill the job. Or you could switch virtual consoles with ctl+alt+the f keys and kill the process
find ./ -type f -iname “*:02.txt” -exec rm {} \;
The "for humans" tag is actually quite intentional, in that the format specs added are reminiscent of the handwavy way we humans use dates in daily conversation. In fact, I'd originally planned a `@{h}` spec that automatically chooses from among the 4 new specs, depending on the distance of the reference date from current time. That may well appear in a future release.
I ran into this same problem as well. Here is roughly how I checked the borders. &amp;nbsp; ## Just for reference ## # leftSide=$(( $x % $num_columns == 0 )) # rightSide=$(( ($x + 1) % $num_columns == 0 )) # topLine=$(( $x - $num_columns &lt;= 0 )) # bottomLine=$(( $x + $num_columns &gt;= $gridMax )) if (( $x - $num_columns &lt;= 0 )); then echo "top line" elif (( $x + $num_columns &gt;= $gridMax )); then echo "bottom line" elif (( $x % $num_columns == 0 )); then echo "left side" elif (( ($x + 1) % $num_columns == 0 )); then echo "right side" else echo "An error occurred" fi &amp;nbsp; This is just a rough example as you'd need to check both top/bottom and left/right, but you get the idea. &amp;nbsp; If it turns out the cell is on a border and needs to appear on the opposite one, it would move: # On left border moving left to appear on the right border $x + ($num_columns - 1) # On right border moving right to appear on the left border $x - ($num_columns - 1) # On top border moving up to appear on the bottom $x + ($gridMax - ($num_columns - 1) ) # On bottom border moving down to appear on the top $x - ($gridMax - ($num_columns - 1) ) &amp;nbsp; I hope this points you in the right direction. Let me know if you have questions.
I've never used calcurse, but I just took a quick look at it...and it looks like it does its own date formatting, instead of relying on an external `date` utility. So you'll have to do some calcurse post-processing in your mailer script, like this: #!/bin/bash days=$1 if [[ -n $(calcurse -Q --days "$days" --filter-type=cal) ]] then calcurse -Q --days $days --filter-type=cal --format-apt=' - [%(start:%l:%M%p) ] %m\n' --format-event=' - %m\n' | while IFS= read l; do # Check for date line [[ $l =~ ([0-9]+/[0-9]+/[0-9]+): ]] &amp;&amp; echo "$(dateh -d "${BASH_REMATCH[1]}" +@{d}):" || echo "$l" done | mail -s "" {myphonenumber}@mms.att.net fi The `while` loop I inserted in your script basically turns this: 01/24/19: - [ 6:30PM ] Bible Study @ Church 01/26/19: * Jordan's Birthday - [ 7:30PM ] Shogun Hibachi &amp; Sushi ( w/ John, Emily, Jared, and Others) into this: yesterday: - [ 6:30PM ] Bible Study @ Church tomorrow: * Jordan's Birthday - [ 7:30PM ] Shogun Hibachi &amp; Sushi ( w/ John, Emily, Jared, and Others) where "today" is Jan 25, 2019.
[removed]
This is awesome! _Thank you so much._ I'll have to study and learn from the bits you put in there. I submitted an issue on your GitHub. I'm getting some wrong days of the week.
I figured it was for some type of calendar application. I tried to look up if there's a python library for something like this but didn't have any luck. If you were interested I would be willing to collaborate on a small bash utility or function to implement this. 
Thank you for your help ! This is just an example or can I copy these two codes directly ? Are you sure of the syntax ? But it helps me to understand ! So Thank you:) 
/u/anthropoid decided to make a pretty cool utility I guess after seeing my post. Reddit Post: [dateh: date for humans](https://redd.it/ajpvr2) [Repository](https://github.com/gromgit/bash_scripts) Seems to be on the right track of something cool. I'm getting some issues with it returning the wrong days of the week for which I submitted an issue. Maybe he wouldn't mind some assistance, to not duplicate effort. I could see this tool being useful. Surprised there wasn't already something for this.
That would be a dope feature.
Yea I just saw his post and I concur about not duplicating effort. 
Anytime! Here is something that *partially* works (the syntax is correct, you'd just need to declare "$gridMax"). &amp;nbsp; if (( $x - $num_columns &lt;= 0 )); then # top line to bottom line newLocation=$(( $x + ( $gridMax - ($num_columns - 1) ) )) elif (( $x + $num_columns &gt;= $gridMax )); then # bottom line to top line newLocation=$(( $x - ( $gridMax - ($num_columns - 1) ) )) elif (( $x % $num_columns == 0 )); then # left side to right side newLocation=$(( $x + ($num_columns - 1) )) elif (( ($x + 1) % $num_columns == 0 )); then # right side to left side newLocation=$(( $x - ($num_columns - 1) )) else # May want to do something else here so you don't mess up the display echo "An error occurred" fi &amp;nbsp; I say partially because I noticed you are simulating a two dimensional array. This doesn't account for that so you will need to make the correct changes. A quick guess would be the top &gt; bottom / bottom &gt; top will change "$y" and leave "$x" the same. Left &gt; right / right &gt; left will change "$x" and leave "$y" the same.
Something like this? adress="google.com" echo "Q" | openssl s_client -host "${address}" -port 443 -prexit -showcerts | sed -n '/-----BEGIN CERTIFICATE-----/{p; :loop n;p; /-----END CERTIFICATE-----/q; b loop}' \ &gt; "${address%.*}"cert.cer
i also been working on a game of life, my sole goal was to make it as small as possible in bytes, to the degree it actually runs slow, due to doing unnecessary work as that makes it smaller in size, so don't expect readability but so be it... copy and paste the entire line in a shell, should work N=();for((Z=C=COLUMNS;L=LINES;)){ for((I=C*L;I--;)){ for((T=J=8;Z?N[I]=RANDOM&amp;1:(X=J%3-1,Y=J/3-1,T-=X|Y&amp;&amp;M[(I+X+C)%C+(I/C+Y+L)%L*C],N[I]=T==5||M[I]&amp;&amp;T==6),J--;)){ :;};};Z=;M=(${N[@]});printf %c ${M[@]}|tr 01 \ O;} but now... getting to the point, i also faced the dilemma of what to do at the edges. i made mine wrap left/right and top/bottom here's how mine works... * i have a single dimensional array size COLUMNS \* HEIGHT * i iterate across every square (essentially... for (POS = 0; POS &lt; COLUMNS \* LINES; ++POS) * i generate the 8 offsets ( "-1 -1". "0 -1". "1 -1". "-1 0", "1 0", "-1 1" "0 1" "1 1") (i actually generate "0 0" also, but discard that) * i then have to calculate the position of the neighbours, based on my position (I) and offsets (X + Y) but.... taking into account the borders. i do it like this I = the square i'm counting neighbours for X=my offset x position (-1, 0 or 1) Y=my offset y position (-1 0 or 1) L=height of screen (number of lines) C=width of screen (number of columns) then (I+X+C)%C+(I/C+Y+L)%L\*C is the index of the neighbour &amp;#x200B; you can test out the algorithm with the little script... #!/bin/bash pos() { X="$1" Y="$2" ((P=(I+X+C)%C+(I/C+Y+L)%L*C)) echo $P } L=40 C=20 I=0 pos -1 -1 pos -1 0 I=19 pos 1 0 pos 0 -1 pos 1 -1 &amp;#x200B; &amp;#x200B;
Before doing anything involving rm it's usually wise to check what would be deleted. Ls *02 
 $ echo | \ openssl s_client -showcerts -host www.google.com -port 443 2&gt;&amp;1 | \ openssl x509 -inform pem -noout -enddate | \ cut -d "=" -f 2 Mar 13 08:17:00 2019 GMT $
This reminded me of [this exercise](https://www.reddit.com/r/commandline/comments/8gjx36/linux_date_command_can_it_do_cardinals/), maybe you could merge the two somehow?
Doesn't work
[removed]
Nice! I made a couple of tweaks as shown here: certexpiry() { local host="${1}" local hostport="${2:-443}" echo \ | openssl s_client -showcerts -host "${host}" -port "${hostport}" 2&gt;&amp;1 \ | openssl x509 -inform pem -noout -enddate \ | cut -d "=" -f 2 } For readability, the variables are all using consistent style and the one-liner is split into one-line-per-pipe. For safety, the variables are quoted and restricted to a local scope (i.e. they're contained within the function)
Just pass `-enddate` into OpenSSL.
`tee`. Example: echo "Exception found. to bypass please run command.sh -x $name" \ | tee -a /path/to/logfile \ | mailx -s "command.sh - exception found" [admin@domain.com] 
Thanks! thought I tried that and didn't work, just have just been using &gt;&gt; insteed of tee
Yeah, a thinko on my part. Fixed version on GitHub now.
Done. `dateh` now supports `@{o}` and `@{O}` for short (`29th`) and long (`twenty-ninth`) ordinal day-of-month forms.
ASCII and ye shall receive: `@{h}` and `@{H}` now output the most suitable relative representation. The only difference between the two is when we come down to the day level, then they output the same thing as `@{d}` and `@{D}` respectively.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/commandline] [dateh: date for humans](https://www.reddit.com/r/commandline/comments/ajxmn1/dateh_date_for_humans/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
No love for the arithmetic conditional `if (( ... ))`?
Thanks for noticing. I already used `if ((...))` half the time; I just wasn't as consistent as I should've been. Will fix that in a later update.
Are you sure? That works here.
Didn't work for me. You can't kill sl with Ctrl-c..
From sl's man page: &gt; Let the user exit (CTRL + C): &gt; sl -e It does let you use Ctrl+C if you use the -e switch.
I think this needs to be a library or something. This is what `date` should've done all along!
In my current repository, `git branch -a` output looks like this: * master remotes/origin/HEAD -&gt; origin/master remotes/origin/master And when you run `echo $branches`, the command starts with `echo * master …`, and Bash will expand the `*` glob to all files in your local repository.
That is because `echo $branches` will also echo "*", which is part of the output of `git branch -a`. And `echo *` by itself will output the files in the current directory because of globbing. To fix it, use `echo "$branches"` instead, wrapping the variable in double quotes to prevent this behavior.
My original implementation was in fact a library, with a `date()` function that shadowed the system (GNU) `date`. This is why [the initial commit](https://github.com/gromgit/bash_scripts/blob/d86d0b91e61d6e7ae3115aa710e92d80c7f539bf/dateh) is littered with `command date` invocations, to support said shadowing. (In hindsight, that was a **stupid** "optimization" that would almost certainly have led to a boatload of tricky debugging and head-scratching down the line.) If there's enough interest, I can make it a "dual-mode" script, usable as both a standalone executable and as a bash library. I'm just not sure it's worth the effort, since it's trivial to use from any bash script as-is. (Incidentally, this is one reason why I generally [favor implementing standalones over libraries](https://www.reddit.com/r/bash/comments/aejokm/functionsalias_or_script/edry6ng/). If nothing else, standalones can be used from any language, while bash libraries are firmly stuck in bash-land.)
Awesome, nice work.
I'm not talking about a bash library, I am talking about c library. I am currently dealing with handling user-supplied dates and functionality that you added would be very useful.
What do you get when running the command without options?
Always always always double quote variables in bash.
`branches=`git branch -a`` will work too.
Exactly the same output as when you run `date` without options. `dateh` understands everything `date` does because it actually runs `date` behind the scenes. For all intents and purposes, `dateh` *is* `date` plus additional format specifications, which only come into play when: * you specify a date format (`+...`), and * that format contains one or more of the specifications I mentioned in my post.
Ah. Well, the output side of things is easy. The base output formatting just requires [strftime(3)](http://man7.org/linux/man-pages/man3/strftime.3.html), and `dateh`'s customizations are basically some simple math and string work. It's date *parsing* that's a real pain, and the `date` command's impressive flexibility takes that pain to a whole new level. Some research reveals that its "magic sauce" resides in the [parse-datetime](https://www.gnu.org/software/gnulib/MODULES.html#module=parse-datetime) module of [gnulib](https://www.gnu.org/software/gnulib/), but the documentation is pretty poor and the GPL may conflict with your use case.
First real script... eh. My first real script is something that reads text from a file. An example would be "02/Jan/19 - Remember this!" and an image here for context: [https://i.imgur.com/H09oNjl.png](https://i.imgur.com/H09oNjl.png) and shows in a message box when I boot. It also does weekly ones, formatted in "Monday - ..." and so on. It also can send via a push API (currently coded to use Pushbullet) which is useful if I had my raspberry pi going - but I'm on my pc daily so not currently needed. &amp;#x200B; But yeah, my backup script wasn't as early as it should have been. I use rsync along with an array listing what locations I want to be backed up. Reminds me, I need to get that going again. &amp;#x200B;
New to bash scripting, keeps blowing my mind. So if there are commands/syntax that allow me to send an email then I should be able to notify me or others by test message as well and other mediums. Havent had this much fun on a computer since dialup
“history | grep” has usually been sufficient for me. Your name is in the script by the way, intentional? Just checking :)
I just define $HISTFILE as something like .history/history-$(date).
Just set putty to log everything and install that app on linux which adds timestamps to every terminal line.
Time passes between invocations, do not call `date` repeatedly for the current time.
If you are trying to chmod 600 .txt files in a directory this should work find . -name "*.txt" -exec sudo chmod 600 '{}' \; + The . says it will use results from the current directory (assuming this is desired from your usage of 'ls') + -name "*.txt" looks for files ending in ".txt" in the given directory + -exec allows for execution on the results + {} would be each result, you end this with \;
Firstly, avoid using `ls` for scripts... for all sorts of reasons - like you can have carriage returns in filenames, and it really can get messy.... If you'd like some reading...](https://mywiki.wooledge.org/ParsingLs) Secondly, quote your variables - especially ones that can have spaces in (like filenames)... OK... Now getting rid of the `ls` is easy.... for file in * Next, you are testing for a file "*.txt" . Unfortunately, you don't have a textfile called `*.txt` If you want to prove it, run this... touch '*.txt' ls *.txt if [ -f "*.txt" ] ; then echo I found it! ; fi
``` HISTFILE="$HOME/.shell_history-$(date)" ``` Though really, C-r should be enough already ?
&gt; quote your variables +1!!!!
Is the entire job queue file written before the first one starts?
Well I was actually just using the file as an example. in reality is is the output of a command called 'racadm' which pulls all the currently running jobs on a poweredge server and returns them. The previous commands in my scripts clears all the jobs, then kicks off a firmware update which adds jobs to the queue. I would like to really have a loop that runs the command every 30 second while the jobs are either != "complete" to ensure that all the jobs are complete before moving on to the next step. There's more I can add to that, but that basic functionality would be enough to get me past this hump.
I'd do something like this: while true; do ${racadm_command} | grep -q "Status=In Progress" || break echo "Jobs still running" sleep 30 done
Good point. I've since reduced `date` usage to the absolute minimum of 2 calls in quick succession, to minimize "drift". See the new `dateh` in [its new repo](https://github.com/gromgit/dateh).
So you want a solution without being able to say if there is any point in time that you could get a list of jobs in which all say completed, when in fact the complete update is incomplete? 
Thank you so much for the explanation and even adding resources because mine must be outdated or something. Changed to linux two weeks ago now, havent had this much fun since dial-up days.
Thank you for the answer, especially about what each part does, just adds to my own bash understanding. I just really need to wrestle with for loops and nesting if/else statements right now. I know how all important they are in all programming (didn't get into bash just for programming, felt like I owed it to this beautiful, and all other, distros to be able to fully use it).
You said you're running a command to generate the output, so in the example below I've called this "COMMAND" here i run the command, parse it with sed to extract the statuses, which i then sort and count and pass to grep to check for any occurrences of "In Progress" I duplicate the output (tee) to stderr, which I then redirect back to stdout... so we also get to see the counts on the terminal (hence running the check in a subshell) the exit status outside the subshell is the exit status of the final command ran (grep) #!/bin/bash readonly StatusPollSeconds=3 while true; do echo "checking job statuses..." ( COMMAND | sed ' /^Status=/!d s/// ' | sort | uniq -c | sort -g -k1,1 -r | tee /dev/stderr | grep --quiet "In Progress" ) 2&gt;&amp;1 if (($? == 0)); then echo "all jobs completed" break fi echo "some jobs still in progress - sleeping..." sleep "$StatusPollSeconds" done then, testing... checking job statuses... 2 Completed 1 In Progress some jobs still in progress - sleeping... checking job statuses... 2 Completed 1 In Progress some jobs still in progress - sleeping... checking job statuses... 3 Completed all jobs completed &amp;#x200B; &amp;#x200B;
Not sure in this exact case, but when I run native executables from another language like groovy, it’s an asynchronous process. If you want to “wait” for it to complete, you have methods to do exactly this, which block until the process returns 0 or error. I imagine you need to do something similar- monitor execution and block. 
I would add a '-type f' part just to make sure that there is no directory ending in .txt which gets chmod 600 treatment.
That's true, nothing is stopping something from appending an extension to a directory.
Maybe there is also options like "Queued", so `grep --quiet -v "Complete"` can solve this. 
I'm not sure I understand the question. But let me explain the job queue behavior and maybe that will help. When you kick off a firmware update the first thing it does is start a job that checks the firmware versions of all the devices against a catalog and if there are new firmware versions available it will start a job for each of the devices that need to be updated. I can't seem to locate any information for what all of the status options are, so I'm going off of what I know and I'll just have to test it out on different servers. So the options I know of are "Completed" "Failed" and "In Progress" or something similar. So in summation, there will always be at least one job, which is checking the catalog, but that job will start other jobs if needed before it's completed. So at least one job should always exist.
alternatively, using the commands you provided and the way you wanted to check... just redirect the (processed) output of your command as stdin to your while loop... set a flag if status is anything other than "Completed" #!/bin/bash while true; do echo "checking jobs..." Completed=1 while read Status; do [[ "$Status" != Completed ]] &amp;&amp; { echo "found incomplete job with status '$Status'" Completed=0 } done &lt; &lt;( COMMAND | grep "Status" | cut -d'=' -f2 ) ((Completed)) &amp;&amp; break echo "sleeping for a few seconds" sleep 3 done echo "all jobs completed" &amp;#x200B; and testing... checking jobs... found incomplete job with status 'In Progress' sleeping for a few seconds checking jobs... found incomplete job with status 'In Progress' sleeping for a few seconds checking jobs... found incomplete job with status 'In Progress' sleeping for a few seconds checking jobs... all jobs completed &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
I recommend a visit to /r/talesfromtechsupport once in a while. That's how you get so paranoid and try to avoid these exotic edge cases. People storing their documents in the recycling bin is another classic on Windows systems.
&gt; mine must be outdated or something Strictly speaking, the most relevant resources are likely *already on your machine* - since they are the *correct* versions of the documentation for the software that you currently have on your machine. The versions on `linux.die.net` and the default pages shown on many other sites (including the website of any program in question - which is likely to be ahead of yours) may differ in options or capabilities available - e.g. [at](https://linux.die.net/man/1/at) on `linux.die.net` likely differs from the version you have on your computer `man at` - your manpage probably shows the ***-M*** option... If you only looked online, you might be filling up your local user's mailbox (that you might not even know about) whenever you used it since there was no way of disabling email without playing silly buggers externally to the program and deny `at` access to `sendmail`... &gt; since dial-up days OK, so probably not homework.... :-D
Think I will stick to this sub and sidebar links to learn from now on. Thanks fellars/mellars
No? PuTTY is something you use when you're on Windows. Better options exist for Linux, *BSD, and macOS. You don't need to install "that app" (no idea which one you're referring to) to add timestamps. You literally just define HISTTIMEFORMAT in .bashrc. Give the bash man page a read. If you're relying on PuTTY for standard shell solutions, you're doing it wrong.
Not dial-up internet days, we were just poor. So i'd have to call a friend to google things for me. :P my problem with man is that its actually intimidating beyond belief, its like "here's EVERYTHING try to find what you need in the wording youre expecting" . &amp;#x200B; What I want is Chapter 1 of the ling you sent me about changing variables. &amp;#x200B; &amp;#x200B;
2 is still a bug; in the essential functionality of your tool no less.
Maybe try the following? #!/bin/bash N = &lt;....&gt; # prompt timeout, in seconds osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3 | read -t ${N} IN_FILE ./mainexec "${IN_FILE}" or perhaps ./mainexec "$(osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3)" If mainexec is starting before it should, adding an `echo` might help to gather the full input ./mainexec "$(echo "$(osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3)")" 
Sounds like you haven't actually looked at my revised code. I only call `date` once for the current time; the second `date` call is to resolve the _user-supplied_ reference timestamp, without which the concept of "relative date" has no meaning.
&gt; There've been other languages that use "" for strings and never for variables Therein lies a bit of a tale..... You remember when filenames were characters, then a dot, then 3 more.... And then we wanted more descriptive names, and spaces were added and stuff, but there were limits - you couldn't have a whole bunch of 'funny' characters like `*` and `?`.... Well, Linux allows all that and more - you can even have characters with carriage returns in them! The *only* characters that are prohibited are `/` and `NULL` echo secret &gt; 'This is my "secret" file!' echo Not secret &gt; TotesSecret echo SECRETTTT &gt; 'This is my secret file! with a carriage return in it' ls T* OK... now, you are wondering how on earth you are supposed to deal with that... Say you want to print the filenames starting with T to a file.... for f in T* ; do echo $f &gt;&gt; my-filenames ; done cat my-filenames Looks OK.... Now you want to get the secret out of all the files.... Now, the *right* way to read would be to `read` (man read.1p), but we're lazy, and hey, wotevs... for f in T* ; do cat $f &gt;&gt; my-secrets ; done cat my-secrets Uh?... Errors and only one secret? So, didn't someone mention quotes somewhere.... let's try that.... for f in T* ; do cat "$f" &gt;&gt; my-new-secrets ; done cat my-new-secrets Yay! ---------------- So quotation marks (here, in this context) tell the commands where the boundaries are in the filenames - and there are different types of quotes that are handled in different ways (try using single quotes...)... And although the `my-filenames` file happily reproduced the quotes in the filename, quotes will do their best to trip you up time after time System variables are a mixed bag - sometimes they are quoted, and sometimes not - e.g. a randomly picked file from my distro - `/etc/X11/xdm/Xstartup` has most variables quoted and some not - these will be variables that the devs have restricted for their own use, *or* system variables which can only contain certain characters (e.g. usernames and certain directories). If in *any* doubt... quote!
&gt; What I want is Chapter 1 of the link you sent me about changing variables. Scroll right down to the bottom... Bang in the middle of the footer is a link [Home](https://www.tldp.org/LDP/abs/html/index.html) Or you can just chop bits off the URL until you find something useful... https://www.tldp.org/LDP/abs/html/ Hmm? https://www.tldp.org/LDP/abs/ Urg... https://www.tldp.org/LDP/ Ahh....
This should work, and is a bit simpler than the other solutions posted while [[ -n "$(cat "/path/to/status/file" | grep -E '^Percent Complete' | grep -v -E '^Percent Complete\=\[100\]')" ]]; do echo "Jobs are are still in progress" sleep 3 done thhis isolates all the "percent complete" lines (1st grep), then returns any that arent `Percent Complete=[100]` (2nd grep), and the `while [[ -n ... ]]` tells while to keep on going as long as the dual-grep command retuns something (i.e. isnt empty, which occurs when all the percent complete lines equal `[100]`). If the file needs to be initialized and theres a chance this would get run before any "percent complete lines" exist (which would return empty and stop the while loop), you could use while [[ -n "$( [[ -f "/path/to/status/file" ]] &amp;&amp; [[ -n cat "/path/to/status/file" | grep -E '^Percent Complete' ]] &amp;&amp; cat "/path/to/status/file" | grep -E '^Percent Complete' | grep -v -E '^Percent Complete\=\[100\]' || echo "1")" ]]; do echo "Jobs are are still in progress" sleep 3 done This does the same thing, but will return "1" automatically (this could be any non-empty string) if either a) the file doesnt exist, orb) it doesnt contain any lines starting with "Percent Complete"
I think that's what I'm looking for, but when I run that with my command like so while true; do echo "checking jobs..." Completed=1 while read Status; do [[ "$Status" != Completed ]] &amp;&amp; { echo "found incomplete job with status '$Status'" Completed=0 } done &lt; &lt;( racadm -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view | grep "Status" | cut -d'=' -f2 ) ((Completed)) &amp;&amp; break echo "sleeping for a few seconds" sleep 3 done echo "all jobs completed" I'm Getting the following output /opt/dell/poweredge/deploy1.sh: line 12: syntax error near unexpected token `&lt;' /opt/dell/poweredge/deploy1.sh: line 12: ` done &lt; &lt;(' 
u/6vNyLG2kaQqMmEj7u, do you get any output from `mainexec` on your terminal, when you run `initexec` from the command line? If so, `mainexec` may actually be *erroring out* when you launch it from the macOS desktop, because its writes to possibly-unbound stdout and/or stderr are failing. To test this hypothesis, have `initexec` redirect all of stdXXX when launching `mainexec`: ./mainexec "$IN_FILE" &lt;/dev/null &gt;/tmp/mainexec.out 2&gt;/tmp/mainexec.err then launch from the desktop again and see if it runs to completion this time. Regardless of what happens, you should also inspect `/tmp/mainexec.out` and `/tmp/mainexec.err` to see if there are any interesting messages that you want to handle on a more systematic basis, e.g. piping them to the system log via `logger`.
do you definitely have the first line of the script as #!/bin/bash ?
&gt; But let me explain the job queue behavior and maybe that will help. &gt; ... &gt; should.... .... isn't a word seen in many manuals. &gt; I can't seem to locate any information for what all of the status options are Speaking of which... ever tried [the manuals?](https://www.dell.com/support/manuals/us/en/19/dell-opnmang-sw-v8.1/idrac_racadm_pub-v3/introduction?guid=guid-6b6564ab-ee63-49ef-a7d6-47154c343841&amp;lang=en-us) 
Hmm, nothing seems to be working. I am stumped..
Ok, thanks for the non-helpful response. I've gone through that manual as well as http://cfdlab.unsw.wikispaces.net/file/view/dell-poweredge-drac7-1.50.50-command-line.pdf No where does it have a list of the possible status's. 
Yeah I copy pasta'd that whole thing and only replaced the COMMAND. and that line is at the top. I'm calling the script by running sh /path/to/script.sh
ok running with bash explicitly is working. Guess I need to use that. Sorry.
I think I've actually got it. I implemented the initexec script in C using popen() and realized why mainexec stops halfway. Halfway through execution, mainexec prints "TERMenvironment variable not set." before exiting. This looks like a problem with the C code in mainexec. Sorry and thanks to all!
I’m on my phone but something like this? ``` #!/usr/bin/env bash while IFS=“,” read var1 var2; do mysql -e “INSERT INTO mydb.mytable (field1, field2) VALUES (‘${var1}’, ‘${var2}’);” done &lt; /dev/ttyAMA0 ```
One last thing. Is it possible to do something like this #!/bin/bash while true; do echo "checking jobs..." Completed=1 while read Status; do [[ "$Status" != Completed ]] &amp;&amp; [[ "$Status" != Failed ]] &amp;&amp; { echo "found incomplete job with status '$Status'" Completed=0 } done &lt; &lt;( racadm -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view | grep "Status" | cut -d'=' -f2 ) ((Completed)) &amp;&amp; break echo "sleeping for a few seconds" sleep 3 done echo "all jobs completed" To keep the loop going if any of the Status's are anything other "Completed" or "Failed" I've tried the above and currently I have one job that is completed and one that is failed, but the loop still runs. 
I think something like this (using tty as a substitute): while read line &lt; /dev/tty; do echo -e "I just got: $line" done
Hmmm. Try this basepath='/path/to/dir/containing/executable/binary' for nn in {'.lock','.status','.log'}; do [[ -f "${basepath}/${nn}" ]] &amp;&amp; \rm -f "${basepath}/${nn}" touch "${basepath}/${nn}" chmod 777 "${basepath}/${nn}" done chmod 777 "${basepath}/mainexec" echo "MAINEXEC IN PROGRESS" &gt; ${basepath}/.status [[ "$(${basepath}/mainexec "$(echo "$(osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3)" 2&gt;&amp;1 1&gt;${basepath}/.log)" &amp;&amp; \rm -f "${basepath}/.lock" &amp;&amp; \rm -f "${basepath}/.status" &amp;&amp; touch "${basepath}/.status" &amp;&amp; echo "1" || \rm -f "${basepath}/.lock" &amp;&amp; echo "0" || echo "UNABLE TO WRITE TO .LOCK FILE" | tee -a "${basepath}/.log" &amp;&amp; exit 0)" == "1" ]] &amp;&amp; echo "MAINEXEC COMPLETED" &gt;&gt; ${basepath}/.status" || echo "MAINEXEC FAILED" &gt;&gt; "${basepath}/.status" while [[ "$(cat "${basepath}/.status")" == "MAINEXEC IN PROGRESS" ]] while [[ "$(cat "${basepath}/.status")" == "MAINEXEC IN PROGRESS" ]]; do echo "mainexec was forked. waiting for mainexec to finish....." sleep 5 
Hmmm. Try this (there could be a few typos...i input this directly into the reddit response window. This should provide a few ways to check on the what is going on when mainexec terminates, and should prevent bash from skipping ahead basepath='/path/to/dir/containing/executable/binary' for nn in {'.lock','.status','.log'}; do [[ -f "${basepath}/${nn}" ]] &amp;&amp; \rm -f "${basepath}/${nn}" touch "${basepath}/${nn}" chmod 777 "${basepath}/${nn}" done chmod 777 "${basepath}/mainexec" N=60 # timeout for prompt osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3 | read -t ${N} IN_FILE echo "MAINEXEC IN PROGRESS" &gt; "${basepath}/.status" [[ "$("${basepath}/mainexec" "${IN_FILE}" &amp;&amp; \rm -f "${basepath}/.lock" &amp;&amp; echo "1" || \rm -f "${basepath}/.lock" &amp;&amp; echo "0" || echo "UNABLE TO REMOVE .LOCK FILE" | tee -a "${basepath}/.log" &amp;&amp; exit 0)" == "1" ]] &amp;&amp; echo "MAINEXEC COMPLETED (EXITED WITH ZERO STATUS)" | tee -a ${basepath}/.status" || echo "MAINEXEC FAILED (EXITED WITH NONZERO STATUS)" | tee -a "${basepath}/.status" if [[ "$(cat "${basepath}/.status")" == "MAINEXEC IN PROGRESS" ]]; then echo "MAINEXEC was forked. waiting for program to finish....." while [[ "$(cat "${basepath}/.status")" == "MAINEXEC IN PROGRESS" ]]; do sleep 1 done else echo "MAINEXEC was not forked. If it terminated early it might be an issue with the executable binary" fi [[ -f "${basepath}/.lock" ]] &amp;&amp; echo "MAINEXEC FAILED TO REMOVE LOCK. PERMISSIONS ISSUE?" | tee -a "${basepath}/.status"
your change above worked fine when i tested it. if you say you've only got a Completed and a Failed job, but it keeps loopig, then what status does it output in the line... echo "found incomplete job with status '$Status'" 
actually it doesn't seem to stop looping even with all the jobs set the complete. even without my edit. With all the jobs complete and the the original while read Status; do [[ "$Status" != Completed ]] &amp;&amp; { echo "found incomplete job with status '$Status'" Completed=0 } When I run racadm -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view | grep "Status" | cut -d'=' -f2 I get what I expect to see which is just Completed Completed Not sure what I'm doing wrong here.
Thanks a bunch I've returned with this from your script: MAINEXEC FAILED EXITED WITH NONZERO STATUS MAINEXEC was not forked. If it terminated early it might be an issue with the executable binary After the mainexec acted improperly. For some reason, running ./mainexec ran it completely. I just can not seem to get this to function properly without manually running it in a terminal.
Running `dateh -d now '+%A (@{D})'` at midnight should either write *Sunday (today)* or *Monday (today)*. Because of the bug your tool could also print *Sunday (yesterday)*.
temporarily add to the cut line... cut -d'=' -f2 | tee /dev/stderr and it'll duplicate what's being passed back to stderr when it actually runs as a whole, which will then appear on your terminal, eg... neil@tvpc:~/bb$ ./test checking jobs... Completed Failed Completed all jobs completed neil@tvpc:~/bb$ &amp;#x200B; you can also debug by adding set -x at the top of the script, so it output what it's running, eg... neil@tvpc:~/bb$ ./test + true + echo 'checking jobs...' checking jobs... + Completed=1 + read Status ++ ./dummy -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view ++ tee /dev/stderr ++ grep Status ++ cut -d= -f2 Completed Failed Completed + [[ Completed != Completed ]] + read Status + [[ Failed != Completed ]] + [[ Failed != Failed ]] + read Status + [[ Completed != Completed ]] + read Status + (( Completed )) + break + echo 'all jobs completed' all jobs completed neil@tvpc:~/bb$ &amp;#x200B;
Running `dateh '+%A (@{D})'` at midnight should either write *Sunday (today)* or *Monday (today)*. Because of the bug your tool could also print *Sunday (yesterday)*. 
Just use the superior OS to connect to linux and parse the output and terminal logs. It's like you've never had an actual job working with linux. Oh wait.
another thing to be aware of with commands such as chmod 660 "$FILE" if the filename was to look like a command line argument (eg. begins with a hyphen) then the command you're running may interpret it as such. eg. you have a file called "-p" chmod 660 "$FILE" ends up being chmod 660 -p which causes chmod to give an error, as it's not a valid command line argument - and can be the source of some catastrophic bugs for certain commands and argument. most, certainly all GNU commands accept a command line argument "--" which just states "this is the end of the command line arguments" and it's good practice to always use it. eg... $ ls -p $ for FILE in *; do chmod 660 "$FILE";done chmod: invalid option -- 'p' Try 'chmod --help' for more information. $ $ for FILE in *; do chmod 660 -- "$FILE";done $ with "find" to get the list of files to modify, it's pretty safe, as it lists the files prefixed with ./ but it's still good practice to use -- never the less.
Ok here' my script now #!/bin/bash set -x while true; do echo "checking jobs..." Completed=1 while read Status; do [[ "$Status" != Completed ]] &amp;&amp; { echo "found incomplete job with status '$Status'" Completed=0 } done &lt; &lt;( racadm -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view | grep "Status" | cut -d'=' -f2 | tee /dev/stderr ) ((Completed)) &amp;&amp; break echo "sleeping for a few seconds" sleep 3 done echo "all jobs completed" and here's my output [root@deploy ~]# bash /opt/dell/poweredge/deploy1.sh + true + echo 'checking jobs...' checking jobs... + Completed=1 + read Status ++ racadm -r 172.16.0.39 -u root -p STSI_1111% --nocertwarn jobqueue view ++ grep Status ++ cut -d= -f2 ++ tee /dev/stderr Completed Completed != Completed ]] '\'''o 'found incomplete job with status '\''Completed 'ound incomplete job with status 'Completed + Completed=0 + read Status != Completed ]] '\'''o 'found incomplete job with status '\''Completed 'ound incomplete job with status 'Completed + Completed=0 + read Status + (( Completed )) + echo 'sleeping for a few seconds' sleeping for a few seconds + sleep 3 I do see that the the string is being cut off in the results. Seems to be resolved by removing the single quotes from $Status but doesn't affect the the results
Use whatever gets your job done, but when your solutions aren't standard, and duplicate features already built into your distro, you're going to get flak. Ignoring bash features, and using PuTTY for logging instead is about the most Windows type behavior I can think of.
well i think this "racadm" command bizarrely outputs lines with Windows end-of-line encodings (carriage return and newline), which UNIX expects simply newline. i can mimic the same behaviour if i take your output from original post, then run "unix2dos" on it, then feed it to my script, eg... ++ grep Status ++ cut -d= -f2 Completed Failed Completed != Completed ]] != Failed ]]d '\'''o 'found incomplete job with status '\''Completed 'ound incomplete job with status 'Completed so, this is easy fixed by using "tr" to remove the bogus carriage returns... cut -d'=' -f2 | tr -d '\r' | tee /dev/stderr then works fine for me
Why tho? You know about auditd/audispd?
i would think possibly you have to be careful about a (first) read with the examples below only bring back a partial line. if something was in the process of being written whilst you started, you could miss some initial characters. from that point onwards, would be fine, but need to validate you get full first line IMHO.
Really appreciate the help here. that was the issue. Thanks so much man. 
If I had to guess, id guess that the way you call mainexec in the non-interactive script doesnt setup the environment in the same way that it is setup with running it from an interactive terminal. Something like this might work...change initexec to something like this #!/bin/bash # sudo might not be needed sudo su --login --shell=/bin/bash ${USER} -c &lt;&lt; EOF source ${HOME}/.bashrc # if you source any other files add them here basepath='/path/to/dir/containing/executable/binary' N=60 # timeout for prompt osascript -e 'display dialog "Drag a file" default answer ""' | cut -d ':' -f3 | read -t ${N} IN_FILE "${basepath}/mainexec" "${IN_FILE}" EOF
I have yet to run into trouble with a Bash history of unlimited length.
Thank you, I ended up using this example. Much apprecaited. 
Thanks for your comment, it seems the while read line saves me from that issue.
pretty amazing for doing from a phone! you know some bash!
Making your own custom prompt is a fun and enlightening thing to do starting out. You can google 'bash custom PS1'. 
&gt; “history | grep” has usually been sufficient for me. I have my history file size set to 2000; this way, using grep for something relatively recent works 95% of the time, but if it's October and I have to look up what I did back in May, storing it elsewhere is much easier. I can also make the logs readable to everyone or just my group in case I'm not there and I have to walk someone through what I did. I'd rather not do that by default with my home directory. &gt; Question: why does bashlog exit 1? I'm old, and on my first Unix boxes (Solaris) saving a process here and there would make a noticeable difference in response time. The real work is done via exec at line 4, so the script should never make it to line 5; if it does, I want some indication of an error. 
Yup, but I'm not always on Linux, or for that matter using bash. My other machines are Solaris-10/11 and FreeBSD, and my preferred shell is ZSH. I'd rather look at a logfile that's consistent regardless of the OS, and see what command I ran corresponding to the modtime of whatever file I'm curious about.
My preferred shell is ZSH, and it's had an *infuriating* bug for years that occasionally corrupts history files by adding control characters. When that happens, I only find out when I logout and see an error message, and by then it's too late -- you lose your history. I'd switch to Bash if ZSH didn't have so many other cool things...
I've corrupted my history a few times, but now I sync it to a git repo once a day with a Cron job so I can recover it in case of problems.
[ss64](https://ss64.com) is pretty good for basic syntax/options of much of what's available by default, but may not be up to date (IDK as I'm just not very knowledgeable about Bash).
I started out by taking tasks at work and scripting them with bash. I never actually used them for work purposes but it gives you something fun to do. For instance, say you rent out equipment and as of right now there is no form for someone to fill out. You can write a simple script to generate that form in a text file. Sounds boring, but it's a start. You can then advance from there. Say the same script can be modified and instead of a text file, you use pandoc to output to a .docx. Another script I always found enjoyable is a system scan utility. Just start by grabbing system stats like hostname, number of processes, total RAM. You start getting into things like cut, sed, and awk. There are also a lot of different ways to get the same thing so you end up learning more as you Google stuff. Like instead of using hostnamectl you can just cat the information from /proc But I understand the pain in trying to find useful things to script sometimes. But you'll find it soon enough. 
right on. thanks for sharing!
https://www.tldp.org/LDP/Bash-Beginners-Guide/html/chap_01.html
I'll give you a thumbs up for this. It has a good flow to it. I never used anything besides the typical &amp;&amp; for chaining commands and never really thought about waiting for others to finish. I will say though that the section about forking throws me off when I read it. That first sentence just messes with me. Either I'm just reading too fast or my brain doesn't function early in the morning. Regardless, I learned something new so you did good, son. 
The things I started writing scripts for was when I ditched my statusbar. Instead of that I wrote scripts that could tell me the time, active workspace, battery level, volume level, and things of that nature in a notification whenever I needed them to. You could look into doing something like that.
I'm assuming you're referring to "Bash runs as a process on your machine, by forking, a child process is created to execute some command(s) without blocking the script." If so, would this be more understandable: Normally, each command that you execute with bash runs as a separate process. When you run a bash script, every command within that script runs within that process. Forking is a way to create a new process separate from the bash script's process for a specific command so that the script may continue executing without being blocked.
I'm sorry, I should have clarified a little better. It's the way the sentence is structured in my opinion. How it sounds to me is "Bash is a process on your machine because of forking" (which I guess it technically is) it's the comma and what comes before and after. How you described it there is perfect and I do know what forking is, but I understand your confusion. I read the sentence like 5 different times and every time I have to pause and restart, if that makes sense. I honestly think part of it is my brain not interpretting it correctly because it is currently 1 am and it's difficult for my brain to say okay comma...(pause)...continue reading. 
That makes perfect sense, I'll see if I can come up with a different grammatical structure to make it clearer. Thank you for the input!
You could render it clearer just by ending the sentence at "on your machine." See: &gt;Bash runs as a process on your machine. By forking, a child process is created to execute some command(s) without blocking the script.
The issue was in fact in mainexec itself. Thank you for all of your assistance 
You may also be interested in [GNU Parallel](https://www.gnu.org/software/parallel/parallel_tutorial.html#Number-of-simultaneous-jobs).
While Gnu parallel is more powerfull, it is often not installed by default. Another option is using xargs with the -P flag. xargs is almost always installed by default, and also much more common to run commands in batch mode with. 
The easiest way is to use bash math instead: date -d @$(( $(date +%s) / 10 * 10 )) +%H%M%S which breaks down as follows: * `date +%s` gets the number of seconds since the Unix epoch * `$(( ... / 10 * 10 ))` uses bash's built-in integer math to round down to the nearest 10 seconds * `date -d @... +%H%M%S` feeds the rounded epoch-seconds value back into `date` to generate the final desired timestamp
I've used parallel in the past, my use cases have always been limited to the types of scenarios able to be handled natively. Often the scripts that I write that need this functionality are to be shared with by team members, so avoiding non-default dependencies is really beneficial. You make a fantastic point though. I'll add a where to go from here section at the end that mentions GNU Parallel. Thank you 
You could do this here: printf "%06d\n" "$(( $(date +"%H%M%S") - $(date +%S)%10 ))" This uses the `$(( ))` math mode that's built into bash itself instead of the external "bc" tool, and it uses the 'printf' command to add that zero you want at the front if the number is just five digits instead of six. I think in theory something might go wrong with your way of doing it if the system hangs for a bit in the middle of this code, between the two calls to the external 'date' command. If what you are doing is something inside a bash script, you could call 'date' just once and save it in a variable, then work with the contents of that variable, perhaps like so: time=$(date +%H%M%S) time=${time:0:4}$(printf "%02d" $(( ${time:(-2)} - ${time:(-2)} % 10 )) ) echo $time This uses the `${variable:start:length}` feature of bash to cut up the text in that $time variable split off the part with the hours+minutes and the part with the seconds.
I thought xargs -P was only for running a specific command in Parallel (ie multiple iterations with different arguments), please let me know if my understanding is wrong. Even if limited to my understanding, I think it's a great addition to the end of the post. Thank you
Thank you, I'll update that as soon as I can.
Works like a Charm, thanks!
Ah yes, I completely forgot about the "no date reference" case. I'll fix that in an upcoming major update that also makes `dateh` do its own option handling, rather than deferring the bulk of it to GNU date. About half of `date`'s options are irrelevant to `dateh` operations anyway, like `--set` and the format-related flags. Thanks for the heads-up.
"Parallel" is an inherently confusing term, as a couple of comments promoting GNU parallel and xargs have already illustrated. Your writeup covers concurrent _unrelated_ jobs, while the aforementioned tools are designed for SIMD-style (Single Instruction, Multiple Data) parallelism, i.e. running several instances of the same thing, with different inputs to each. Personally, I'd rename your article to something like "how to run concurrent jobs in bash scripts", and perhaps write a separate one that covers GNU parallel and xargs.
While it's true that `read` will always read to the end of each line unless you tell it otherwise, u/bubbobz's point is that unless: * `/dev/ttyAMA0` is properly buffered, and * all reads from all consumers (not just your script) are guaranteed to terminate on line boundaries, you may very well start your first `read` in the *middle* of a line, leaving you with fewer fields than you expect and/or half of the first field's true value. Your script will have to check for both those conditions.
Ahh. thank you for explaining that in detail. I've not had any issues so far, but ideally I would like to prevent a potentially issue. I did a bit of googling and it doesn't seem there is any obvious way to protect against this other than to make some assumptions of about the data. (i.e. confirm string starts with a special character or something) In my case I have no control over the output format, with that being said any ideas that come to mind that might prevent a partial read?
A quick online search for `ttyAMA0` seems to point towards it being a Raspberry Pi's serial port or Bluetooth. The Linux kernel does some buffering on such ports, but it's definitely not on a per-line basis, so that first read will almost certainly be of a partial record, and should be discarded. You definitely also don't want to do any heavy processing in between reads, to avoid having the kernel buffers overflow and ending in yet more b0rked records. The oldie-but-goodie [Serial HOWTO](http://tldp.org/HOWTO/Serial-HOWTO.html) is probably worth a read, to familiarize yourself with How Serial Works, especially in the possible absence of flow control.
I'm definitely considering this feedback, because you're right. Based on the comments there's definitely a reaction to parallel and confusion. I'm not sure if it's beneficial for people searching for solutions to this problem to have this result under parallel vs concurrent. My go-to search before writing the post was obviously parallel—to which I found nothing useful.
I use a bash script I wrote long ago. It runs jobs in background ([here you can see it used to run a couple of services](https://i.imgur.com/gpTMBRY.png)) and can be used to run several jobs in parallel.
Wow, backticks in the examples. That makes it feel like it was written in the 90's or earlier. I recommend you use $(command) instead of backticks.
well, your script certainly works. now, when you say it's checked by an automated system, i'm presuming it actually runs your script with various test cases? if so, then i would say possibly your script, though it works, isn't meeting the requirement you stated "The only thing the script is expected to **r**eturn is a number" possibly you're scoring 0 because it is expected the result of \[script\] 4 5 + to be strictly... "9" ...rather than "4+5=9" &amp;#x200B; **OR** possibly, it's expected that the script actually return the result as an exit status, eg... you need to calculate the result (eg. into variable RESULT) then do exit "$RESULT" at the end of the script - though that limits the return value to be in the range 0..255, but possibly makes this automated test system much easier to write
I have tried removing everything that is not a number as well - still a 0 from the system. Also, the multiplication is technically incorrect - the asterisk problem means that any nonsentical operation (for example "=") will default to the answer of multiplication. As for a "result" variable, nothing even hints to create it, so while I will still try it tomorrow, I don't expect a good outcome. Finally, the part of the task mentioning that neither runtime nor error messages is written in red (should be important, right?) makes absolutely no sense to me at all. Any ideas?
one observation: the power function is not correct. *anything* to the power of 0 should be 1. your script does... [script] 3 0 ^ ---&gt; 3 &amp;#x200B; I'm not understanding your wild-card problem. you have a quoted asterisk "\*" as the case to match against - that won't match against somebody running the script giving a bogus = as the operation, so looks fine to me (it *would* match incorrectly if it wasn't in quotes though) &amp;#x200B; for "neither runtime nor error messages should appear", i would take that to mean *nothing* should appear on the terminal when you run it. there are two ways your script can cause output to appear. runtime messages: this would be your "echo" lines. if you remove these, then they're gone. simples. error messages: this will be output from "let" or other commands that you call you give them something invalid. example: if I run your script and give it arguments "- - -" then it outputs an error... search online for "stdout and stderr" (standard output and standard error) to learn about how UNIX processes generally have TWO output streams - and how to redirect them so they don't appear on your terminal &amp;#x200B;
That's correct, but if you want to run different commands, well you can still do that. echo -e "sleep 1; echo hi;\0echo hi; sleep 5;" | xargs -I {} -0 -n 1 -P 4 bash -c "{}"; But it is not really meant for that. 
one thing that helped me many many years ago was to look at real world (supposedly well written) scripts now many of the UNIX commands you might use at terminal are actually scripts themselves. if you do file /bin/* /usr/bin/* | grep shell then it well show you items that are scripts. you can look at these (see NOTE below) and you can also generally type "man \[COMMAND\]" to get information on what the command is supposed to do. you can get a feel for conventions and styles and ways of doing things from studying all these. NOTE: there are multiple different shells that can write scripts in. most of the scripts in /bin + /usr/bin actually use "sh" (bourne shell) which was the first UNIX shell. bash is far more modern but remains virtually compatible with bourne shell - but further adds a lot of additional features and better ways of doing some things, so be aware of all that.
Without referral spam: https://www.humblebundle.com/books/programming-cookbooks
I assume these are e books
Wait til all the trains have passed! :p If crtl+z or ctrl+c fail, only real solution is use another terminal using gnu screen or tmux, then pgrep sl and kill it.
Thank you for your help! I'll be honest, after today's 5 hours on this task I'm not super eager to research the commands you mentioned, but I will surely implement the changes you have pointed out and see how it goes. Even if it doesn't go well, to be honest, it's not a big deal. I'll dump the subject and my average will be more than able to compensate for it, although the First-Class Honours won't be as shiny as it could be. 
man lp
I tried using parallels with a script to run backups but it is tricky because you have to be sure to segregate the processes and make sure they don't step on each other when writing files. In some cases I got some performance gains but in other cases the improvements were fairly minimal but that may have been due to the architecture of system or my implementation which remained experimental. You make a good point about avoiding non-default dependencies. Thank you for the useful guide. 
What does a score 0 indicate? Does it mean that every single test case is wrong (systematic failure)? If (and only if) this is the case, and if you can retry as much as you want, my suggestion would be to try to check whether maybe the lecturer made a mistake. For example, this test would check whether maybe the test harness runs the script with `sh` (a common mistake in shell programming): ``` #!/bin/bash case "$3" in "+") echo "$(( $1 + $2 ))" esac ``` and this would additionally check if maybe the lecturer incorrectly thought that the value should be returned as the status code instead of on stdout (another common mistake): ``` #!/bin/bash case "$3" in "+") exit "$(( $1 + $2 ))" esac ```
Shell scripting is hard to do well. Very hard. It is not a nicely "designed" language, it is full of pitfalls and historical quirks. And the problem is that it is not well understood by most people, and those people write scripts and write tutorials that then add to the huge amount of misinformation on the web, which then gets indexed by google, etc. A lot of that information is out of date, or broken in some obscure way the author never considered, or written by people who are trying to help but unfortunately are barely ahead of you on the learning curve and not experienced enough to give good advice. I suggest to ignore all of it except these two: * http://mywiki.wooledge.org/BashGuide * https://guide.bash.academy/ The tldp documents are ancient, don't use them. Hint: have you tried reading 'man bash' ... it's hard work, long and confusing, right? Well, you won't be writing good shell scripts until you understand 30-50% of what it says, without having to look it up :) I hope this awareness helps you, good luck. 
Shell scripting is hard to do well. Very hard. It is not a nicely "designed" language, it is full of pitfalls and historical quirks. And the problem is that it is not well understood by most people, and those people write scripts and write tutorials that then add to the huge amount of misinformation on the web, which then gets indexed by google, etc. A lot of that information is out of date, or broken in some obscure way the author never considered, or written by people who are trying to help but unfortunately are barely ahead of you on the learning curve and not experienced enough to give good advice. I suggest to ignore all of it except these two: * http://mywiki.wooledge.org/BashGuide * https://guide.bash.academy/ this one has [exercises](https://guide.bash.academy/?=Exercises#h0.2) The tldp documents are ancient, don't use them. Hint: have you tried reading 'man bash' ... it's hard work, long and confusing, right? Well, you won't be writing good shell scripts until you understand 30-50% of what it says, without having to look it up :) I hope this awareness helps you, good luck. 
Which editor are you using? Doesn't sound like much of a `bash
I still regularly use backticks in my scripts 😬 I never picked up the reason why $() is the "right" way, do you have something I could read up on that would set me straight? I'll often use them interchangeably and just vary them on the style of the script I'm working on (if it's a pre-existing script).
Using Terminal on Mac, which uses bash. Sorry. I’m kind of new to learning how to code. So I’m still unsure. 
Using Terminal on Mac, which uses bash. Sorry. I’m kind of new to learning how to code. So I’m still unsure. 
u/dont_look_behind_me, off the top of my head, here's a short script that tallies both individual and total login times. Assumes Linux-style `last` output, dunno about other platforms: #!/bin/bash declare -A user_times user_sessions while read l; do # Capture: username, days (optional), hours, minutes if [[ $l =~ ([^ ]+).*\(([0-9]+\+)?([0-9]{2}):([0-9]{2})\) ]]; then u=${BASH_REMATCH[1]} # Strip the trailing "+" from days d=${BASH_REMATCH[2]%+} # Strip leading "0" from hours and minutes, else they're octal h=${BASH_REMATCH[3]#0} m=${BASH_REMATCH[4]#0} t=$(( d * 1440 + h * 60 + m )) (( total_time += t , user_times["$u"] += t , user_sessions["$u"]++ )) fi done &lt; &lt;(last) for u in "${!user_times[@]}"; do echo "${u}: ${user_times[$u]} minute(s) in ${user_sessions[$u]} session(s)" done echo "TOTAL TIME: ${total_time} minute(s)"
Which editor are you using _to create your makefiles_? You'll need to configure it to stop expanding tabs into spaces. For instance, with Vim, adding: ``` set noexpandtab ``` to the end of your `~/.vimrc` file should do the trick. Or are you creating your makefiles by copy-pasting (Command-C, Command-V) from web pages or other sources? That's guaranteed to make tab characters disappear, and you'd either need to find a way to download the original files, or live with editing the makefiles afterwards.
Also, what are the *exact* steps you did that: &gt;changed my tab command into four spaces ?
thanks for a blunt, real, answer... &amp;#x200B; &amp;#x200B;
ac -p
And the best way to get good at it is by writing your own scripts! So, just to be clear, I'm not trying to discourage you, I'm trying to helpy by minimising the time you waste looking at bad resources.
Hm. This post inspired me to get my backup script working again. I used Rsync, which would basically just keep a copy of my files on a raspberry pi and update changed files, and after 30 days it would remove all the files backed up and start again (so not to consume space). &amp;#x200B; Re-writing the script and most definitely plan to use 7z to zip files hourly (depending on how much I'm backing up, might change that!). &amp;#x200B; Not sure how exactly I'll go about it as to whether or not I take pieces from my old script. &amp;#x200B; Here's what I have so far if you're curious: [https://pastebin.com/X247df4v](https://pastebin.com/X247df4v) In the end, I want to have something easily portable, for if my PC spontaneously combusted. Maybe I'll forgo rsync and do it on my actual PC, maybe I'll use a raspberry pi again. Or zip it locally and use google drive as a cloud (thankfully I don't need much space!) &amp;#x200B; The joys of having a lack of things to keep safe :D
I forgot to mention this, highly recommended: https://www.shellcheck.net/ (click "load an example" to see what it does)
man bash is worth about $100 easy.
I like being able to grep through my entire history. It helps when there's a command that I know I used a year or more ago that I need to look at. I'd have to define HISTSIZE=50000 or something and I don't know if that would cause issues.
Yes, in PDF, ePub, and Mobi formats. I think Humble Bundle does printed book bundles, but not for nearly as cheap of course and so they're more rare if they even do them any more.
I have tried modifying the script to fix some of the things you mentioned. Still a 0, after two more hours wasted. Gonna look into the exit status thing when I can. &amp;#x200B; &gt;\#!/bin/bash &gt; &gt; &gt; &gt;exec 2&gt; /dev/null &gt; &gt; &gt; &gt;case "$3" in &gt; &gt; "+") let "$(...)=$1+$2";; &gt; &gt; "-") let "$(...)=$1-$2";; &gt; &gt; "\*") let "$(...)=$1\*$2";; &gt; &gt; "/") let "$(...)=$1/$2";; &gt; &gt; "\^") if(($1==0)); then "$(...)=0; &gt; &gt;elif (($2==0)); then "$(...)=1; &gt; &gt;else &gt; &gt;for ((i=1; i&lt;$2; i++)); do ((power \*= $1)); done &gt; &gt;let "$(...)=$power" &gt; &gt;fi;; &gt; &gt;esac; &amp;#x200B;
&gt; is echo piping it's output to cut? yes &gt; is cut then outputting to my screen(standard ouput)? yes &gt; redirection operator is supplying input to cut yes &gt; what is outputting the result to my screen? cut 
Not quite, `&lt;&lt;&lt;` isn't a redirection operator it's a `here string`, and it's worth noting that it's `bash` specific &amp; not portable. Whereas the first example is `POSIX` compatible. Otherwise agreed.
Hey, Tnx for the feedback! One last question. Take this snippet while read line do cut -c3 done If the input is: Hello World how are you then first line is skipped, i.e. the output is: r w Which is not the case if I pipe via echo or redirect. Why is that? 
Don't forget that you can try everything directly at the bash command line. You don't have to edit and run a script all the time. Experimenting with your two snippets at the normal prompt looks like this: $ while read line; do echo $line | cut -c1; done hello h abcdefghijklm a If you type `Ctrl-D` on an empty line, you get an invisible "end of file" character and the `read` command will stop reading. $ while read line; do cut -f1-3 &lt;&lt;&lt; "$line"; done hello hey hi what something hello hey hi what something a b c d e f g h i j k l m n a b c d e f g h i j k l m n a b c d e f g h i a b c hello hi what some thing yes no hello hi what This `cut -f1-3` tool apparently looks for tab characters to decide where to cut, not just any space. When experimenting at the bash command line, where you would have line-breaks inside a real script, you can type `;` characters instead. There's a `help` command that gives you hints about where to put the `;` characters: $ help while while: while COMMANDS; do COMMANDS; done ... The unusual part here is that there's no `;` after the `do` word. This is an exception to the "use `;` instead of line-breaks" rule. A thing to remember is `help test`. It's where you can find a listing of all operators that are available inside `[ ... ]` and `[[ ... ]]`., 
&gt; Don't forget that you can try everything directly at the bash command line Ooh, that's neat! Tnx for pointing that out! 
Your first bit of standard input is consumed by read while checking the condition for the while. The output is stored in line which you don’t use. Then it enters the loop and runs cut. Without a pipe or file cut reads from standard input so all of your subsequent lines are going to cut.
Thank you for your help ! But I don't understand how to get gridmax.... I don't understand what $gridmax represents.... $gridMax represents all cells? So $gridMax = $num_columns * $num_lines?
&gt; is echo piping it's output to cut? Not strictly the `|` tells `bash` to send what the program before the `|` sends to `stdout` into `stdin` of the program after it.... As the `.def` file from which the source code is built shows, [`echo'](http://git.savannah.gnu.org/cgit/bash.git/tree/builtins/echo.def) - echo is pretty simple - it *only* knows of `stdout` As for the second half - again - *unless directed otherwise* by it's arguments cut reads from `stdin`, and sends output produced to `stdout` and/or `stderr` - which in this case, `bash` passes both to the screen. look at echo hello it does what you expect echo hello | echo goodbye only results in the `goodbye` being seen.... the `hello` is available on `stdin` for the second `echo` to read, but it is never read.... as echo hello | cat - echo hello | cat /dev/stdin echo hello | cat /dev/fd/0 show.... &gt; The redirection operator is supplying input to cut Well.... actually `read` [`man bash`](https://linux.die.net/man/1/bash) ('read' is quite a long way down - search for `-ers` - the only occurrence is in the description) - gets the 'input', as a variable called 'line' and *that* is passed to `cut` by use of a `herestring` which parses the item - ` "$line" ` in this case *on each loop* and feeds the result into cut.... on you guessed it... standard input &gt; what is outputting the result to my screen? Ultimately, `bash`
That is great! I will check it out, for sure.
I will check this out. Thanks
hahaha excellent
u/D1DgRyk5vjaKWKMgs, if you want to do _anything_ to handle `flock`'s failure to lock, you simply have to break your current command line: ``` [ "${FLOCKER}" != "$0" ] &amp;&amp; exec env FLOCKER="$0" flock -en "$0" "$0" "$@" || : ``` into something like: ``` if [ "${FLOCKER}" != "$0" ]; then exec env FLOCKER="$0" flock -en "$0" "$0" "$@" || mail -s 'script failed' root &lt;&lt;&lt;"script could not acquire lock" fi ``` Note that the `A &amp;&amp; B || C` formulation runs `C` if _either `A` or `B`_ fails. Shellcheck is quite blunt about this: ``` a &amp;&amp; b || c ^-- SC2015: Note that A &amp;&amp; B || C is not if-then-else. C may run when A is true. ``` so use with care.
This ^. I love built in shell commands.
man rename
There's two different "rename" tools on Linux distros. Maybe check them out to see what they can do. There's a simpler one that is part of the "util-linux" project from kernel.org and can do simple text replacement, and there's another one that is a Perl script. That Perl version can do complicated rules that look similar to what you might know from the 'sed' tool: perl-rename 's/reddit/stupid/' * That would give you that "stupid_is_as_reddit_does" result. To get both 'reddit' words replaced, you would do this: perl-rename 's/reddit/stupid/g' * Using the other, simpler 'rename' tool looks like this, it can only do the first occurrence and it has no other features: rename reddit stupid * On some distros the "perl-rename" tool is called "rename", and the simpler "rename" doesn't exist. With the Perl version you can do all kinds of stuff because the rule used for renaming is actual Perl code. It can also move files to different folders if your editing rule adds a path to the name, and it takes care of creating the folder. Both rename tools have options to just print what they would do without actually renaming the files.
Yep! gridMax=$(( $num_columns * $num_lines ))
Backticks will continue to be supported as there are too many old scripts out there using them. Here is one take on why it's usually better not to use backticks. [http://mywiki.wooledge.org/BashFAQ/082](http://mywiki.wooledge.org/BashFAQ/082)
some errors crept in... this $(...)= syntax is not valid you're not initialising power &amp;#x200B; #!/bin/bash exec 2&gt; /dev/null case "$3" in "+") let RES=$1+$2;; "-") let RES=$1-$2;; "*") let RES=$1*$2;; "/") let RES=$1/$2;; "^") if(($1==0)); then RES=0 elif (($2==0)); then RES=1 else power=$1 for ((i=1; i&lt;$2; i++)); do ((power *= $1)) done let RES=$power fi;; esac; # to print to terminal echo $RES # to return result as exit status exit $RES &amp;#x200B;
I have run your answer on the automated system. Full marks for this particular task. Thank you for all of your help! &amp;#x200B; The only thing I still don't understand is why I got this task, but that I will never know. The only thing I've learned from it is to stay as far as possible from shell scripting in the future. Kinda sad, considering that the subject looked really good at the start, but the flame is gone. &amp;#x200B; Respect to you guys who deal with this though! &amp;#x200B;
So how good are these books exactly? I've heard good things, but i'm not sure if it's overblown, or deserved?
So I can't find this specifically mentioned in Bash's changelog, but this does not appear to be a problem in more recent versions of Bash. Bash uses `\x01` internally as a separator for various things, and there have historically been quite a number of bugs with this particular character.
First, preface your code with 4 spaces instead of using ticks. It's much easier on the eyes. I'm on a mac and am using Bash 5, I get $&gt; a=($'\x01') $&gt; printf '%u\n' "${#a[0]}" 1 When I switch to Bash 3, I get '2'. I'm not sure why or what the issue is, and not sure if this info helps at all. Sorry. However, if you end up doing any amount of scripting on os-x, I suggest installing and updating bash and all the other gnu tools (awk, find, watch, grep, which, etc, etc). I use homebrew for this: brew update &amp;&amp; export HOMEBREW_NO_AUTO_UPDATE=1 # install one or as many as you like brew install coreutils brew install bash brew install binutils brew install diffutils brew install findutils --with-default-names brew install gawk brew install gnu-indent --with-default-names brew install gnu-sed --with-default-names brew install gnu-tar --with-default-names brew install gnu-which --with-default-names brew install gnutls brew install grep --with-default-names brew install gzip brew install screen brew install watch brew install wdiff --with-gettext brew install wget Some of these require editing your $PATH if you want to do stuff like `tee` instead of `gtee`. Others you can append '--with-default-names' to get the same effect. Each packages install message will let you know (and you can `brew info $package` to see what your options are). Google 'how to use gnu utils on os-x' to see some write ups for more examples. And of course if you do install other shells, you can `cat /etc/shells` to see what's available on your system. If you don't like it, or need to switch back, just edit your PATH and/or `brew uninstall $package`
Thank you. I'll work-around it for now.
Thanks. I do use homebrew, and I have a bunch of those packages installed. Not bash, though. I've built up a substantial collection of bash scripts over the years. Should I port them over to bash 5? I'll mull it over. One question: I haven't seen HOMEBREW\_NO\_AUTO\_UPDATE until your response. After googling it, I wonder, why would anyone want to make it the default? I've always found the auto-update behavior useful. Sorry about the formatting. I'm on the new reddit interface and I used the "Inline Code" button instead of the "Code Block" button like I was supposed to.
You shouldn't have many issues running older bash scripts, but obviously you cannot use newer bash features on older versions. But ymmv. If you do have problems running older scripts, you can try running them with certain compatibility modes set. See https://www.gnu.org/software/bash/manual/bash.html#The-Shopt-Builtin and scroll down to the compatNN sections. I personally have never had to do this, but I do not get too crazy with my scripts. I tend to keep it basic if I can. The `HOMEBREW_NO_AUTO_UPDATE` was simply in-case you copy/pasted those commands. I didn't want you to have to sit through brew updating before each `brew install`. And I actually run with HOMEBREW_NO_AUTO_UPDATE=1 full-time. I perform brew update manually anytime I need to. It's a habit. Just like `apt-get update &amp;&amp; apt-get upgrade ...`. I got tired of having to wait for brew to update - again - even though it just updated a few minutes ago because I installed another package. 
You've ever heard of Linux for human beings?
Yeah, I'm running BASH 4.3 on Ubuntu 16.04 and it's not an issue on my machine.
You can multiply the --exclude flag of rsync command to achieve desired effect : rsync -avz --exclude 'test/*.c' --exclude '*.file_type' source/ destination/ 
Hard since i need to find the files path first, that was general example and sub dirs are a possilble scenario. Also as said, i dont know where the files are, all i know is from where i dont want them
I'm aware of the tool, but it is not available by default on certain machines I work on. 
Thank you, /u/ropid. I was aware of the "simple" rename tool, but not the Perl version. As it turns out, though, the former is not available by default on certain machines I work with. &amp;#x200B; I'm not looking to re-invent the wheel, but would like to have a tool that is both useful and reasonably portable. Looking at how these tools work will be a step towards those ends. 
Something like ‘mv *2*2* dest’?
Sounds like you just want to use find's -prune option to tell find not to descend into certain directories you specify.
Sure looks so. Ty, ill test it
I'm not at my desk, but basically what this page says: http://www.liamdelahunty.com/tips/linux_find_exclude_multiple_directories.php
Try counting the occurences of the character '2'. Something like following : echo "ba22na2na" | tr -cd '2' | wc -c Output is e.
When you index into neighbouring cells do it module the height / width of the grid. Simple as that.
ye its not to hard. define 3 variables. search . source. target run a command to get all folders. then run a pregmatch / like on the folder names (if dtatement) then do a check to see if dir exists in target. if not create it and finally copy the data over. also you might wanna add some logging to it.
I tried something almost like and it didn’t work, I’ll your solution, thanks 
there is a command in Linux called `rename` check it out. I was doing Unix/Linux system administration since the late 1990's and never used that until like 2014 or something... it's a hidden gem!
I have created a few helper scripts that make doing stuff like this trivial. I can post them later today if you still need them. 
[removed]
I understand your way but I would have to loop though all the files and then move them. Thanks for your answer!
It would be nice! I I was looking all night for a way but didn’t found anything that could completely help me. This is the kind of thing you don’t really see in man and -help.
It was really that! I tried the same thing but I was missing the ‘*’ after the last 2... Thanks a lot!
Check out find and pipe to xargs
No problem :) do you understand why the pattern works? If not I can help.
I understand it, I’m not really good at expressing what I want to do in bash. Thanks again!
Try `"cd ..; mirror -R /home/me/$(printf %q "$1"); bye"`.
u/spryfigure, your `fullcp.sh` would look something like this (without my usual error checking): #!/usr/bin/env bash search="$1" src="$2" target="$3" cd "$src"; mkdir -p "$target" # Find all paths that match $search find * -name \*${search}\* | while read p; do # Get the first component in each path (i.e. the immediate child file/dir of $src)... p1="${p%%/*}" # ...and cp it over if it doesn't already exist in $target [[ -e "$target/$p1" ]] || cp -a "$p1" "$target/" done
Thanks. Especially the last test condition is good to see. It wasn't clear to me how to do this in an elegant way.
Does escaping the input help? .... mirror -R /home/me/${1/ /%20}; bye or `${1/ /\\ }`
None of the current proposed solutions address the original problem. The command line arguments are delimited by spaces and your input contains spaces. Assuming all the input is only 1 file then you can use `$@` to grab them all and use that as the filename. Something like this: #!/bin/bash eval "ls -l '$@'" would correctly `ls` a file with spaces. If your input contains multiple files all with spaces then your problem is more fundamental and you'll need to find a different way to delimit and read them in.
This would still miss the second command line argument, `(mom)`, in his example.
This would still miss the second command line argument, `(moms)`, in his example.
Change the IFS environment variable. e.g. ``` IFS="\t" ```
&gt; This would still miss the second command line argument, (moms) second? $1 = `things/recipes (moms)` &gt; you would want to do a double forward slash That is true 
Use `$@` instead of `$1` and you should be golden.
Ah that sounds right, or course! Actually there are two arguments I'm expecting and the last one I can control and will never have spaces. Can I parse them as an array of arbitrary length? I know that calling script.sh with n number of tokens will always result in $1, $2...$n-1...$n where concatenating $1 - $n-1 will always be my folder name and $n will always be a static value. Examples: script.sh folder_nospaces Sonarr script.sh a folder name with spaces Sonarr I always know the last, or *n*th, argument is the string **Sonarr**. Arguments $1 through $n-1 will be a folder name with spaces.
What incentive is there to use an abstraction on top of the various shells instead of just using a richer language that already exists? When my Bash scripts become complex or mission critical, I tend to rewrite them in PHP or Python. I admit, the amount of documentation is impressive, so I will read this and possibly answer my own question, but until then, the question of why remains.
Great question. Maybe it's the same incentive that has been keeping people writing shell scripts in the first place, for decades, in spite of the language's well-known deficiencies. There is something about the shell language that is uniquely powerful, that makes it uniquely suited for the sort of thing that shell scripts do. Perhaps it's how it can trivially run external commands and string them together in pipelines. Whatever it is, many people remain willing to put up with a great deal of frustration to get the benefits. Since it's an evident fact that people are not about to stop writing shell scripts, I decided to try to reduce the frustration instead. An important part of what makes languages like PHP, Python or Perl richer is the fact that they come with extensive standard libraries, something that the shell language lacks. Modernish aims to fix that situation. My personal incentive, TBH, is simply to prove to myself that it can be done. I noticed all shells come with certain settings that are potentially capable of reducing a lot of pitfalls, e.g. default global field splitting and pathname expansions can be disabled, and enabled only for commands that need them. I floated some of these ideas in comp.unix.shell in 2015 and was basically laughed out of the newsgroup. Instead of arguing back I decided to start coding. It got a bit out of hand... Another of my original inspirations was [Rich’s sh (POSIX shell) tricks](http://www.etalabs.net/sh_tricks.html). It aims to prove how bad the shell language is by showing how the solutions to common problems are "extremely perverse and inefficient". I've rarely seen anyone defeat their own point so inherently: that very page is the beginning of a shell library that is capable of improving the language.
&gt; second? In OP's comment, $1 == things/recipes (moms) Wrong, you and OP don't understand how bash delimits positional arguments.
 args="$@" fnam="${args% *}" # delete last argument fnam="${fnam// /\\ }" # replace ' ' with '\ ' echo "$fnam"
My brain is close to exploding but I see where this can take me. Thanks, I'll fart around with it. 
Yup, did some testing and you're correct. Didn't occur to me until you said it but (moms) becomes @2, as it should. 
No prob. Here's a tutorial on bash string manipulation that can explain all the syntax: https://www.tldp.org/LDP/abs/html/string-manipulation.html
He's right, we're both having a brain fart. (moms) becomes $2. It's our feeble human brains that think of **/this/path right/here** as one thing and we're calling it $1. But the shell is seeing **/this/path** as $1 and **right/here** as $2 because command line parameters are delimited by spaces. 
holy crapola - aye - been effing about in arrays and not thinking straight - my bad - will scrap it 
Aye - I had my head in arrays from doing something else earlier...
'scuse brain fart from earlier... If your last parameter is something else, then you can catch all but the last with "cd ..; mirror -R /home/me/${@:1:$#-1}; bye" 
I'll be damned. Thanks. I'll need to combine that with some other stuff I've learned here to do a replace on the resulting string. 
dunno who downvoted ya - hope my new submission is a bit more elegant....
On line 2, Sed is treating var1 as an input file. You can use a variable as standard input like [this:](https://en.wikipedia.org/wiki/Here_document#Here_strings) var2=$(sed 's/^/ - /' &lt;&lt;&lt; "$var1") awk solution because i'm an addict send help: var=$(awk '/^s/{print " - /" $0}')
Okay; so it's clearly obvious this isn't for me - but congrats on getting to this release stage.. In my decade+ of shell scripting; I have never once - not even in the beginning - thought to myself that the existing test syntax is difficult or confusing. I do see how [ vs [[ might cause confusion - however that's a 5 minute lesson to figure out.. Having to learn a whole new syntax as to avoid having to learn a different syntax.. And your examples; perhaps you'd be better picking ones that are not harder than the original shell version: modernish --use=loop -c 'LOOP for i=1 to 10; DO putln "$i"; DONE' vs for i in {1..10}; do echo "${i}" ; done It's not like there's even any benefit to using your version over standard shell version. I love the shell; I use it daily - for complex and simple tasks alike. I could honestly never see myself needing what you've given. A competent shell user already understands the pitfalls that your language(?) looks to avoid and knows how to work around it. Those who don't understand the pitfalls are unlikely to understand how your version helps them. Furthermore all it's doing is stopping them learning the correct way of actually writing proper shell. As u/danemacmillan rightly pointed out - when shell is no longer suitable; you goto python, perl or something else - not some kind of shell+. I wish you all the best in your endeavours - but I can't see myself ever using it.
You can add text to a variable without external commands, like this: var1=$(grep '^s' file) var1=" - ""$var1" You could also do something like this: var1=" - ""$(grep '^s' file)"
&gt; for i in {1..10}; do echo "${i}" ; done A couple of points here, not to convince you to use it, just to clarify: * Brace expansion is non-standard and not portable. Modernish aims to enhance POSIX sh, so it does solve some things that shells like bash and zsh already solved in their own way. If you're writing bash-specific scripts, modernish can still be used to add functionality bash doesn't offer, while the rest can be ignored. * What I perhaps should have made clearer: the point of adding `LOOP for` is their added `--split` and `--glob` operators for use in the [safe mode](https://github.com/modernish/modernish#user-content-use-safe): this allows safe splitting and globbing while avoiding the quoting hell caused by globally enabled split and glob (as in, 99.9% of variable quoting is no longer needed). * Check out [`LOOP find`](https://github.com/modernish/modernish#user-content-the-find-loop). That cannot currently be done in any shell. &gt; A competent shell user already understands the pitfalls that your language(?) looks to avoid and knows how to work around it. Ay, there's the rub... I think many others have found that the effort required to become "competent" is unreasonable and gave up long before reaching that point. Of course I understand that if you're already invested in being competent in dealing with the shell's idiosyncrasies and limitations, you might be reluctant to take on anything new after that. There is a precedent for language enhancement libraries such as this taking off in JavaScript (modernizr, jQuery, etc) so I remain hopeful for now. &gt; I wish you all the best in your endeavours - but I can't see myself ever using it. That's absolutely fair. Thanks for your critique. 
I can't seem to prepend spaces. Check this out. $ var=void; var=" - /$var"; echo $var - /void
The extra spaces are there inside $var1. They are disappearing in the line where you use 'echo'. You can preserve them by adding `"` quotes on the 'echo' command line, see here: $ x=hello $ x=" ""$x" $ echo $x hello $ echo "$x" hello 
I'm very tired; thanks :)
Thank you so much! I'm sure there are worse addictions :P And if you don't mind helping me a bit more, how do I preserve line breaks? Right now I have my file like this: secsp3-openssl1-13887 security slessp4-apache2-mod_jk-13888 security slessp4-autofs-13875 recommended slessp4-cmpi-provider-register-13851 recommended But the Output is all on one line like this: ' - /'secsp3-openssl1-13887 security' - /'slessp4-apache2-mod_jk-13888 security' - /'slessp4-autofs-13875 recommended' - /'slessp4-cmpi-provider-register-13851 recommended' Instead of this: - secsp3-openssl1-13887 security - slessp4-apache2-mod_jk-13888 security - slessp4-autofs-13875 recommended - slessp4-cmpi-provider-register-13851 recommended
I'm guessing you did this to get your one-line output: $ var=$(sed -n '/^s/{s,^, - ,;p}' file) $ echo $var By *not* quoting `$var`, you gave bash permission to do word splitting when it expanded that variable, which in turn destroyed all your carefully-crafted whitespace. Quote it, and bash gracefully lets its contents be: $ var=$(sed -n '/^s/{s,^, - ,;p}' file) $ echo "$var" - secsp3-openssl1-13887 security - slessp4-apache2-mod_jk-13888 security - slessp4-autofs-13875 recommended - slessp4-cmpi-provider-register-13851 recommended
No worries, it happens (brain farts and down votes). 
And just like HenryDavidCursory I made the mistake of not quoting the variable. Thanks for taking the time to respond even tough I should have realized based on the other comments. Thank you! :)
You must be *really* tired (and yeah, I've coded under the influence too). The bash one-liner should probably read: unset var; while read line; do [[ $line =~ ^s ]] &amp;&amp; var+="${line/#/' - '}"$'\n'; done &lt; file else everything gets bunged into a single line.
I bought the dead-tree edition of the bash cookbook back around 2012, at a time when I thought I really knew what I was doing. I figured that it couldn't hurt, but I learned sooo much from it. I don't recall any specifics, but I say that if you don't have the book either get the bundle or buy the book on its own.
Non-affiliate link here: https://www.humblebundle.com/books/programming-cookbooks
For the price, I think it's definitely worth picking up the whole bundle.
I thought this was a little more complicated than it is, anyways, I am glad you found a solution. 
go to old reddit. AKA only reddit.
You've only showed the desired output - if you could show a sample of the input as well, it would be easier to offer suggestions. Is the exact timestamp unique for each "trace" ? Why not just use that to find all the lines associated with a specific one? Eg, this part: &gt;2018-10-17T03:11:31+02:00 &amp;#x200B;
The Logs I quoted are part of the Logfile, that I analyze. The whole Logfile is the input, it is too long to fully post here and should be irrelevant as I only need the quoted part. The timestamps differ, so I can not use them to find the CallTraces. I guess I have to do something like If "cut here" exists some lines before the Call Trace, then output from "cut here" to next occurence of "end trace" else grep -ia "Call Trace" "$MESSAGES" -A25 Whats the best way to do that?
If the columns are consistent--which they seem to be in your sample output--you could do something like: awk '{print $1, $2, $3}' logname.log Just modify the print $n to include the fields you actually want. 
I think you misunderstood, maybe this will help: \- find just the "Call Trace" line \- extract the timestamp of that line \- then extract all lines with that timestamp &amp;#x200B;
Can you show a sample that includes the bits you want, as well as some of the bits you dont? It doesn't have to be the "whole log" just enough of it to determine how to match what you want vs what you don't
I need all the fileds. What i quoted is what I need. But how do I make awk print eveything from ------------[ cut here ]------------ to ---[ end trace * ]--- except if there is no line "cut here", then default to my current grep? How do I make this for multiple occurrences of the Traces?
Heres some more Lines: 2018-07-21T14:10:04+02:00 NAS index.cgi: login.c (1717) Token Authentication Fail. 2018-07-21T14:32:08+02:00 NAS index.cgi: login.c (1717) Token Authentication Fail. 2018-07-21T14:43:42+02:00 NAS drnode: Delete session of user[fhem.cam] by user-delete hook 2018-07-21T14:43:42+02:00 NAS kernel: [236682.197395] find usrquota, objectid=259, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.204245] find usrquota, objectid=263, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.210724] find usrquota, objectid=264, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.219161] find usrquota, objectid=267, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.232154] find usrquota, objectid=268, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.243153] find usrquota, objectid=772, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.254497] find usrquota, objectid=820, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.272614] find usrquota, objectid=823, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.278370] find usrquota, objectid=824, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.283972] find usrquota, objectid=825, uid=1031 2018-07-21T14:43:42+02:00 NAS kernel: [236682.289877] find usrquota, objectid=826, uid=1031 2018-07-21T14:46:17+02:00 NAS cgid: user_shadow_info_get.c:56 SLIBCBdbGet(FHEM.CAM) failed [0x2000 bdb_get.c:40] 2018-07-21T14:46:17+02:00 NAS cgid: user_check_expired.c:51 SLIBUserExpiryDayGet failed: fhem.cam 2018-07-21T14:46:17+02:00 NAS cgid: session.c:373 fhem.cam has already expired. 2018-07-21T14:46:41+02:00 NAS cgid: user_shadow_info_get.c:56 SLIBCBdbGet(FHEM.CAM) failed [0x2000 bdb_get.c:40] 2018-07-21T14:46:41+02:00 NAS cgid: user_check_expired.c:51 SLIBUserExpiryDayGet failed: fhem.cam 2018-07-21T14:46:41+02:00 NAS cgid: session.c:373 fhem.cam has already expired. 2018-07-21T14:57:22+02:00 NAS index.cgi: login.c (1717) Token Authentication Fail. 2018-07-21T16:10:49+02:00 NAS kernel: [241909.330002] ------------[ cut here ]------------ 2018-07-21T16:10:49+02:00 NAS kernel: [241909.335301] WARNING: CPU: 3 PID: 9482 at fs/btrfs/usrquota.c:1801 btrfs_usrquota_transfer+0x605/0x650 [btrfs]() 2018-07-21T16:10:50+02:00 NAS kernel: [241909.474682] CPU: 3 PID: 9482 Comm: SYNO.FileStatio Tainted: P O 4.4.59+ #23739 2018-07-21T16:10:50+02:00 NAS kernel: [241909.491689] 0000000000000000 ffff88031dbffcc8 ffffffff812b623d 0000000000000000 2018-07-21T16:10:50+02:00 NAS kernel: [241909.500055] ffffffffa0676498 ffff88031dbffd00 ffffffff8104794d ffff880044bd0f78 2018-07-21T16:10:50+02:00 NAS kernel: [241909.508427] ffff88036ee89740 ffff8800376c6dc8 ffff8802d8e2c040 ffff8800376c6000 2018-07-21T16:10:50+02:00 NAS kernel: [241909.516797] Call Trace: 2018-07-21T16:10:50+02:00 NAS kernel: [241909.519627] [&lt;ffffffff812b623d&gt;] dump_stack+0x4d/0x70 2018-07-21T16:10:50+02:00 NAS kernel: [241909.525462] [&lt;ffffffff8104794d&gt;] warn_slowpath_common+0x7d/0xc0 2018-07-21T16:10:50+02:00 NAS kernel: [241909.532266] [&lt;ffffffff81047a46&gt;] warn_slowpath_null+0x16/0x20 2018-07-21T16:10:50+02:00 NAS kernel: [241909.538891] [&lt;ffffffffa066d2b5&gt;] btrfs_usrquota_transfer+0x605/0x650 [btrfs] 2018-07-21T16:10:50+02:00 NAS kernel: [241909.546960] [&lt;ffffffff8126b867&gt;] ? aa_capable+0x47/0x50 2018-07-21T16:10:50+02:00 NAS kernel: [241909.553002] [&lt;ffffffffa05f3090&gt;] btrfs_setattr+0xe0/0x3f0 [btrfs] 2018-07-21T16:10:50+02:00 NAS kernel: [241909.560002] [&lt;ffffffff811539fa&gt;] notify_change+0x2ea/0x6b0 2018-07-21T16:10:50+02:00 NAS kernel: [241909.566323] [&lt;ffffffff8112dbd2&gt;] chown_common+0x112/0x150 2018-07-21T16:10:50+02:00 NAS kernel: [241909.572548] [&lt;ffffffff8112f3c9&gt;] SyS_fchown+0x69/0x80 2018-07-21T16:10:50+02:00 NAS kernel: [241909.578383] [&lt;ffffffff81567a04&gt;] entry_SYSCALL_64_fastpath+0x18/0x8c 2018-07-21T16:10:50+02:00 NAS kernel: [241909.585703] ---[ end trace 627ebf58bc5e7ef8 ]--- 2018-07-21T17:03:34+02:00 NAS mediaparserd: mediapar_media.c (50) Failed to get misc info from file [/volume1/homes/hscholz/+BANK/bitcoin.de/Jaxx-v1.2.13-win-x64/Jaxx-vv1.2.13_win32-x64/jaxx-assets/icudtl.dat]. 2018-07-21T17:03:38+02:00 NAS mediaparserd: mediapar_media.c (50) Failed to get misc info from file [/volume1/homes/hscholz/+BANK/bitcoin.de/Jaxx-v1.2.13-win-x64/Jaxx-vv1.2.13_win32-x64/jaxx-assets/resources/app/unit_tests/balances-HD-test.ts]. 2018-07-21T17:03:39+02:00 NAS mediaparserd: mediapar_media.c (50) Failed to get misc info from file [/volume1/homes/hscholz/+BANK/bitcoin.de/Jaxx-v1.2.13-win-x64/Jaxx-vv1.2.13_win32-x64/jaxx-assets/resources/app/typings/lodash/lodash.d.ts]. I need this part: 2018-07-21T16:10:49+02:00 NAS kernel: [241909.330002] ------------[ cut here ]------------ 2018-07-21T16:10:49+02:00 NAS kernel: [241909.335301] WARNING: CPU: 3 PID: 9482 at fs/btrfs/usrquota.c:1801 btrfs_usrquota_transfer+0x605/0x650 [btrfs]() 2018-07-21T16:10:50+02:00 NAS kernel: [241909.474682] CPU: 3 PID: 9482 Comm: SYNO.FileStatio Tainted: P O 4.4.59+ #23739 2018-07-21T16:10:50+02:00 NAS kernel: [241909.491689] 0000000000000000 ffff88031dbffcc8 ffffffff812b623d 0000000000000000 2018-07-21T16:10:50+02:00 NAS kernel: [241909.500055] ffffffffa0676498 ffff88031dbffd00 ffffffff8104794d ffff880044bd0f78 2018-07-21T16:10:50+02:00 NAS kernel: [241909.508427] ffff88036ee89740 ffff8800376c6dc8 ffff8802d8e2c040 ffff8800376c6000 2018-07-21T16:10:50+02:00 NAS kernel: [241909.516797] Call Trace: 2018-07-21T16:10:50+02:00 NAS kernel: [241909.519627] [&lt;ffffffff812b623d&gt;] dump_stack+0x4d/0x70 2018-07-21T16:10:50+02:00 NAS kernel: [241909.525462] [&lt;ffffffff8104794d&gt;] warn_slowpath_common+0x7d/0xc0 2018-07-21T16:10:50+02:00 NAS kernel: [241909.532266] [&lt;ffffffff81047a46&gt;] warn_slowpath_null+0x16/0x20 2018-07-21T16:10:50+02:00 NAS kernel: [241909.538891] [&lt;ffffffffa066d2b5&gt;] btrfs_usrquota_transfer+0x605/0x650 [btrfs] 2018-07-21T16:10:50+02:00 NAS kernel: [241909.546960] [&lt;ffffffff8126b867&gt;] ? aa_capable+0x47/0x50 2018-07-21T16:10:50+02:00 NAS kernel: [241909.553002] [&lt;ffffffffa05f3090&gt;] btrfs_setattr+0xe0/0x3f0 [btrfs] 2018-07-21T16:10:50+02:00 NAS kernel: [241909.560002] [&lt;ffffffff811539fa&gt;] notify_change+0x2ea/0x6b0 2018-07-21T16:10:50+02:00 NAS kernel: [241909.566323] [&lt;ffffffff8112dbd2&gt;] chown_common+0x112/0x150 2018-07-21T16:10:50+02:00 NAS kernel: [241909.572548] [&lt;ffffffff8112f3c9&gt;] SyS_fchown+0x69/0x80 2018-07-21T16:10:50+02:00 NAS kernel: [241909.578383] [&lt;ffffffff81567a04&gt;] entry_SYSCALL_64_fastpath+0x18/0x8c 2018-07-21T16:10:50+02:00 NAS kernel: [241909.585703] ---[ end trace 627ebf58bc5e7ef8 ]---
Hrm. This is unfortunate. It appears the time stamp is not the same for the whole section. &amp;#x200B; &gt;2018-07-21T1**6:10:49**\+02:00 NAS kernel: \[241909.330002\] ------------\[ cut here \]------------ &gt; &gt;2018-07-21T16:10:49+02:00 NAS kernel: \[241909.335301\] WARNING: CPU: 3 PID: 948 &gt; &gt;2018-07-21T1**6:10:50**\+02:00 NAS kernel: \[241909.474682\] CPU: 3 PID: 9482 Comm: SY &gt; &gt;2018-07-21T16:10:50+02:00 NAS kernel: \[241909.491689\] 0000000000000000 ffff880 &gt; &gt;2018-07-21T16:10:50+02:00 NAS kernel: \[241909.500055\] ffffffffa0676498 ffff88031d &amp;#x200B;
`awk '/Trace/{a=1}/"trace"/{print;a=0}a' log.log`
 awk '/Trace/{a=1}/"trace"/{print;a=0}a' log.log In this example, `awk` prints the lines including and between "Trace" and "trace". If "trace" isn't always there, though, you'll need to determine what delimiter you can substitute. 
u/Thursday_throwaway2, I suggest a different approach: Since you're focusing on kernel stack traces, which are dumped out within milliseconds, you might want to do a approximate *time-based* grep instead, like this: #!/bin/bash grep -ia "Call Trace:" "$@" | while read l; do # Get seconds-since-startup timestamp from Call Trace line if [[ $l =~ kernel:\ \[ *([0-9]+)\.[0-9]+\] ]]; then tsecs="${BASH_REMATCH[1]}" # Grep again for anything +/- 1 sec from that timestamp grep -E "kernel: \[($((tsecs-1))|${tsecs}|$((tsecs+1)))\.[0-9]+\]" "$@" fi done This should catch the entirety of the trace, along with whatever events triggered the trace, which might appear *before* any "cut here" line (assuming it even appears in your logs). It's also unlikely to capture any irrelevant lines *after* the trace, since it's limited to a 2-second window around the trace itself...unless your Synology box is actually on fire or otherwise about to fail catastrophically, and the kernel is logging like crazy. As a test, I ran it on a system that recently suffered a out-of-memory incident: # get_call_trace /var/log/kern* It captured all **2900+** lines of the OOM-related trace plus contextual info...and nothing else.
Here's how I would do it (assuming the variable MESSAGES is the filename of the log file you're analyzing): if grep -q 'cut here' "$MESSAGES"; then perl -p -e 'undef $_ unless /cut here/ .. /end trace/' "$MESSAGES" else grep -ia "Call Trace" "$MESSAGES" -A25 fi 
any reason to use `-p` and `undef $_` rather than just `-ne'print if /pat1/../pat2/'?
No reason, I just didn't think of it. Thanks for pointing that out. Your version is a lot cleaner.
heh, just making sure i wasn't missing something. I use it a lot for this sort of thing ebcause I can never remember the awk syntax
I’ve added some more widgets to the new reddit sidebar.
This is super neat. 
+1 on backgrounding. Dead simple and works well in my script. https://bbs.archlinux.org/viewtopic.php?id=235984
You can set the variable you want on the line preceding the function call- that way it's set when the function is run. I don't see where you're asking them for any input other than left, sub, or right though- so you'd need to create another user input sequence to capture that variable value, either before or after your other user input sequence.
Thanks for the help. I added another `echo` statement within the bodies of my if statements (taking your advice to capture the variable value) and that allowed me to pass in the arguments into each function call. Appreciate the help! Thank you!
 substring() { # main.c echo $1$2$3 # &lt;--- $1='s. s.main.c .c' $2 and $3 are empty variables line=$(echo ${2%$3}) echo ${line#$1} } In your s) case, if you want to parse the 's. s.main.c .c' as separate args, try s) echo "Executng submatch function()" args=$OPTARG # &lt;---- remove quotes echo "$args" substring $args # &lt;---- remove quotes ;; 
I didn't see anybody mention ranges, which I use all the time. /u/anthropoid has a good solution but here's how to literally do what you asked: awk '/cut here/,/end trace/; /end trace/{exit 1}' trace &amp;&amp; grep -A25 'Call Trace' trace 
I'm guessing it doesn't see any input data to read when you do that, and then it behaves how it does if there's an "end of file", same as when you type `Ctrl-d` at an empty prompt.
u/CasualMangaGuy, you asked: &gt;Why does my interactive terminal shut down if fd 0 is redirected or closed? Because it's hooked up to an **interactive** shell, whose sole purpose in life is to read commands from stdin (a.k.a. FD 0) and execute them. Once you close it, the shell exits, and since your terminal is connected to that shell, it goes away too. Note that it's the *file descriptor* that's the key here, not its underlying file/tty/etc. You can copy FD 0 as many times as you want, potentially ending up with two dozen other FDs all pointing at the same tty, but the moment FD 0 is closed (e.g. `exec 5&lt;&amp;0-`, which copies FD 0 to FD 5 and then closes the former), it's game over. As for redirecting stdin from a file (e.g. `exec 5&lt;&amp;0 0&lt;file`), that executes every command in that file, as if you typed it in manually...and then runs into a literal end-of-file, so it closes the shell. If one of the instructions in the middle of that file closes FD 0, your "script" will stop dead at that instruction: $ cat file echo 1 echo 2 exec 5&lt;&amp;0- echo All leaders suck echo {3..10} $ bash $ PS1='subshell$ ' # just to distinguish this shell from its parent subshell$ exec 5&lt;&amp;0 0&lt;file subshell$ echo 1 1 subshell$ echo 2 2 subshell$ exec 5&lt;&amp;0- subshell$ exit $
Awesome! That worked! I appreciate the help. What's interesting is that I was on the #bash asking for help and someone said that I didn't actually "quote enough" -- and so I figured I should quote all my variables, but I guess that's what did me in. Anyway, I appreciate you pointing out the issue. Real quick: Can you tell me why unquoting the $OPTARG and $arg variable produce the right output? My understanding is that at the end $arg only passes in entire variable as one string to $1 because it's been quote, but without -- it passes in each variable separately, is that correct?
Put your search text into an awk variable by using the `-v...` parameter, then replace your current `/thedata/` search with `index($0, variable)`, for example: awk -vx="thedata" 'index($0, x) { print "insert"; print }'
Thanks. awk is a great instruction, but I have never been able to master it.
Whilst it does indeed work; it seems overkill to use index rather than just match the line. Your version also prints 'insert' on one line and the file contents on another. A simpler version would be: awk -v x='thedata' '$0 ~ x { print "insert "$0 }' in_file
I used index() because I wanted it to be a simple text search instead of regular expression search. I was unsure about printing the word "insert" on its own line. It's what OP is doing in that code he showed, so I thought it's supposed to be like that.
I didn't realise `exec &lt;file` causes the shell to read input from the file. I should have known! Thank you for the excellent answer!
You're welcome, but just to avoid any confusion, closing FD 0 only has this behavior when done _interactively_, i.e. at the command prompt. When done in _scripts_, all that happens is every subsequent command in the script that tries to read from stdin will immediately get an EOF. The script itself will continue to run to completion, unless aborted by an error return or something similar.
Of course. That's why I specifically said "interactive" in my terminal.
Ouch. $ insert(){ awk -v m="${1%%.*}" -v i="$2" '$0~m{print i}1'; } $ insert text.with.dots "Inserted Line" &lt;&lt;EOF &gt; foo &gt; bar &gt; quux &gt; text &gt; quuz &gt; rab &gt; oof &gt; EOF foo bar quux Inserted Line text quuz rab oof
Not really related, but I just want to say that double-quoting is unnecessary for variable assignments because word-splitting is not performed on the result of any parameter substitution when it is assigned to a variable. So, you could simply `m=${1%%.*}` and `i=$2`. But, yeah, it's a good habit to double quote.
Fuck you!
I know I'm sexy but no thanks!
 while read -r name url ; do git clone repo/"${name}".git command "${url}" done &lt; input_file source should read: name url if you want name : url then change the read to: while read -r name _ url; do When you use while with a single variable name; it takes the full line as the var name. When you use multiple variable names; it does word splitting and associates the relevant split with the relevant variable name. If you have more splits than you give variables; the last variable incorporates its current position till end of line; input of 1 2 3 4 5 6 $ echo 1 2 3 | while read -r foo bar; do echo "${bar}" ; done This would; for example - echo '2 3' as foo captures 2nd var till end of line. Hope this helps.
If your input is in a file called urls.txt cat urls.txt | while read line; do name=$(echo $line | cut -d ':' -f 1 | tr -d ' ') url=$(echo $line | cut -d ':' -f 2 | tr -d ' ') git clone repo/"$name".git command $url done
Isn't the name already part of the URL with git? e,g, (some random git project I had open: https://github.com/Nyr/openvpn-install If that is the case I could go for #!/bin/bash SOURCE="https://github.com/Nyr/openvpn-install https://github.com/unixabg/RPI-Wireless-Hotspot .... " for URL in $SOURCE do NAME=$(basename $URL) git clone repo/$NAME.git command $URL done If the name is not part of the URL, you can do #!/bin/bash SOURCE="https://github.com/Nyr/openvpn-install;some_name https://github.com/unixabg/RPI-Wireless-Hotspot;other_name .... " for URL in $SOURCE do NAME=$(echo $URL|awk -F';' ' '{print $1}') URL=$(echo $URL|awk -F';' ' '{print $2}') git clone repo/$NAME.git command $URL done That naming could be done into one line, most likely, but it should workl, as long as there are no spaces in the URls or the names and no ';' either. Use a different character if that is the case.
with input... neil@tvpc:~/pp$ cat file.txt source () { foo : https://foo.com bar : file:/etc/bar.txt dee : ftp://warez.com/pub/incoming/porn.avi } then #!/bin/bash sed ' /source/,/}/!d 1d $d ' file.txt | while read LINE; do set -- $LINE echo "arg1: $1, arg3: $3" done and testing... neil@tvpc:~/pp$ ./test arg1: foo, arg3: https://foo.com arg1: bar, arg3: file:/etc/bar.txt arg1: dee, arg3: ftp://warez.com/pub/incoming/porn.avi neil@tvpc:~/pp$ &amp;#x200B;
I don't know why it's like that, but you can put the `&gt; filename` anywhere you like on the command line. That's just how it is. See here: I'll use this command line to test: $ echo aaa bbb ccc ddd eee aaa bbb ccc ddd eee And here's this command redirected into a handful different files: $ echo aaa bbb ccc ddd eee &gt; test1 $ echo aaa bbb ccc ddd &gt; test2 eee $ echo aaa &gt; test3 bbb ccc ddd eee $ echo &gt; test4 aaa bbb ccc ddd eee $ &gt; test5 echo aaa bbb ccc ddd eee The folder where I ran those commands now looks like this: $ ls test1 test2 test3 test4 test5 And this here is the content of those files, every one of them looks the same: $ tail * ==&gt; test1 &lt;== aaa bbb ccc ddd eee ==&gt; test2 &lt;== aaa bbb ccc ddd eee ==&gt; test3 &lt;== aaa bbb ccc ddd eee ==&gt; test4 &lt;== aaa bbb ccc ddd eee ==&gt; test5 &lt;== aaa bbb ccc ddd eee 
Tnx for your reply! Now I know that is just the way it works, which is nice. I cant find this information in the echo man pages though. Where can I find it? 
this redirection of streams is done by the shell (bash), not by echo itself. however, note: when you type echo in bash shell, you're (by default) not running what the "man echo" page is documenting, you're running an echo that is built-in to the shell type enable and it will show you all the commands that are built-in to the shell and currently enabled. you can disable them - and then bash would search path for an executable called "echo", most likely in /bin which is what "man echo" is documenting. to see the usage for built-ins - AND for documentation on stream redirection, do "man bash"
Bash tries to expand globs before the command is run - and while `2*` isn't counted as a glob, `2 *` is counted as 2 followed by a glob - which is expanded... the glob `*` expands to all the files in the current directory 
It's not the 'echo' command doing this work. It's part of how the shell works. That `&gt; filename` works with any command or program that prints output. You can read about this in `man bash`. Type `/^REDIR` and Enter, and it should jump you to the section where it's documented.
`2*` is counted as a glob too, it just so happens that there are no files starting with `2` in the current directory.
read the output of man bash and search through for "Pathname Expansion"
Aye - I doh'd the second I hit enter - and then wondered if files containing a space in their name would be picked up without messing with IFS...
As has been pointed out, `2*` is a glob that matches nothing so bash doesn't expand the argument. If you don't like that behavior you can experiment with `failglob` (create a failure when there are no matches) and `nullglob` (expand to nothing when there are no matches): $ echo 2* 2* $ shopt -s nullglob $ echo 2* $ shopt -s failglob $ echo 2* -bash: no match: 2* 
for atom in "${Atoms\[@\]}" . . . 
Thank you so much!! Worked perfectly ....could you briefly explain what is different between our scripts?? For instance, why does array name need to be within " ", why is there a $ in front and why is Atoms in {} ...I have a matlab background and a basic linux background (cd, ls, mv ...basic for loops) but I hadn't come across something like this in my needs for bash scripting ....thanks again!!
So in general in bash, to get the value of a variable, you need to precede it with a dollar $... X=4 echo $X &amp;#x200B; For arrays though, the syntax is special, perhaps a little more clunky that in other languages. To get the value of an element in an array, the syntax is... ${Atoms[subscript]} but you can get all the value of all elements with ${Atoms[@]} or ${Atoms[*]} &amp;#x200B; This enclosing the variable in { } might look odd, but becomes a little more easy to understand when you learn that are lots and lots of special forms of ${VARIABLE...}, so array manipulation just forms one of them, eg... X="a string with several letter a's" echo ${X//a/} which would output the string, but removing all letter a's. all these special ${...} forms are listed in the man page for bash ("man bash") &amp;#x200B; As to why i did for atom in "${Atoms[@]}" rather than for atom in ${Atoms[@]} it's to do with how element values containing spaces would be handled. Without the quotes, it would treat *each word* in the entire array as a separate value. eg... neil@tvpc:~$ X=(1 "2 3 4" 5) # note: 3 elements neil@tvpc:~$ for x in ${X[@]}; do echo $x; done 1 2 3 4 5 neil@tvpc:~$ for x in "${X[@]}"; do echo $x; done 1 2 3 4 5 neil@tvpc:~$ &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
You’d need escape character for the space to work re file names.
as an aside, you 2nd read, where you're just wanting a single string (LDA or PPE) doesn't (shouldn't IMHO) be "read -a" - which reads in an array, it can just be "read", to read in a line/string. &amp;#x200B; Later, though you've stated that PP is an array, you output the value using as a normal variable $PP (rather than the clunky element syntax). In this instance... bash just outputs *the first element in the array*. Which is working, but sort of working by accident.
I put [ANSI escape sequences](https://bluesock.org/~willkg/dev/ansi.html) in my bash prompt. `man bash`, then `/^PROMPTING`
Well then maybe test before you post. These are not bash variable assignments, so word splitting matters.
Problem is more likely that your PS1 prompt doesn't contain anything that forces it to appear in that light yellow. It only appears in this colour initially *as an artefact* of your "Welcome..." line changing the current foreground colour. As soon as you run "ls", it changes the foreground colour itself and resets it back to white at the end... hence your prompt then appears in white. Try with the following PS1... escape codes at the start to force foreground colour to that light yellow every time... PS1='\033[38;5;226m\h@\u \W ' or PS1='$(tput setaf 226)\h@\u \W ' &amp;#x200B;
This is due to the way shell (for e.g. bash) processes things. Even before the command `echo` gets to work the shell processes everything in the command line. Think how you process an equation like `a+b/c**4`. You'll follow the operator precedence in algebra. Similarly when shell sees a command like `echo &gt; 6 is a number` it processes it character by character and interprets each token based on prespecified rules. A character like `&gt;` is interpreted as known redirection operator. The redirection operator has a higher precedence than the other tokens here so it binds more tightly. To `bash` `&gt; 6` means open a file named `6` for writing and set it as standard output. Then it identifies the word `echo` as a known built-in command (if there are no aliases or function of same name) and everything else on the command line as arguments to `echo`. Note at this stage `echo` doesn't even know what the arguments are or where its output will be sent. All of that is handled by shell which then calls `echo` and supplies the arguments, gets the output and sends it to the file. Also note the order in which the redirection operator appears is important. For example in the command below `&gt; 2.txt` is processed last so it wins ``` $ echo hello &gt; 1.txt &gt; 2.txt world $ cat 1.txt $ cat 2.txt hello world ``` Check the `SHELL GRAMMAR` section of `man bash` for more details. 
Does this work for you? I'm curious about the file structure used by VASP; could you please post the content of the result? Please note that this script will truncate (pre-erase) the POTCAR file in your working directory. #!/bin/bash while [[ ! $pp =~ LDA|PBE ]]; do case $pp in L|l)pp=LDA;; P|p)pp=PBE;; *)read -n1 -p "LDA/PBE? " -s pp;; esac done; read -a Atom -p "${pp}: Enter space-separated atom_valence, i.e. Pb_d: " cat $(printf "~/POTCAR/potcar_${pp}/%s/POTCAR " ${Atom[@]}) &gt; ./POTCAR 
Ah, yep, you're right they are not cause they are arguments. Well, to be fair, your code formatting is shit.
Have you heard of Python? It might suit your reading level a little better.
Have you heard of sed? It might simplify your brain damage: insert() { sed -nE -e "/${1%%.*}/i $2" -e 'p'; }
Have you heard of sed? It might simplify your brain damage: insert() { sed -E "/${1%%.*}/i $2"; } &amp;#x200B;
Awk thread
Sure. insert() { awk "/${1%%.*}/ {print \"$2\"}; 1"; }
That is simultaneously weak golf and bad practice. Look, I regret being rude to you, but you posted bad advice in a learning thread without the courtesy of testing your own code. 
Oh no, what's wrong now smarty-pants? You can't handle a little bit of awk syntax mixed with a little bit of Shell syntax? So funny. Have you heard of Python? It might suit your reading level.
Thankis, I'll try to use this one, but it's not working for me like this. I get Syntaxerror, unexpected Word `*(' for the line if [[ $l =~ kernel:\ \[ *([0-9]+)\.[0-9]+\] ]]; then I also tried if [[ $l =~ "kernel:\ \[ *([0-9]+)\.[0-9]+\]" ]]; then which lets the Script run, but finds no Traces for me.
Sorry, I accidentally deleted a backslash in the process of fixing the code output to work on both old and new Reddits. That line should read: ``` if [[ $l =~ kernel:\ \[\ *([0-9]+)\.[0-9]+\] ]]; then ``` i.e. the space after `\[` should be escaped.
Shameless hijacking here! I stumbled on your message and couldn't resist. You can give a try to this: [Simple GetOpts](https://github.com/sakishrist/sgo) And you can let me know if you find anything that can be improved
That works, thanks. I would like to increase the times from +/-1sec to +/-2sec. Would it work like this? grep -E "kernel: \[($((tsecs-2))|${tsecs}|$((tsecs+2)))\.[0-9]+\]" "$@" or more like this? grep -E "kernel: \[($((tsecs-2))|$((tsecs-1))|${tsecs}|$((tsecs+1))|$((tsecs+2)))\.[0-9]+\]" "$@" 
You can look up 'Pattern substitution' in `man bash`.
The form `A=(foo bar baz)` is an array mass assignment, making `A` an array. It's equivalent to `A[0]=foo A[1]=bar A[2]=baz`. The `${1//,/ }` is a bash parameter expansion that changes all commas in `$1` to spaces and substitutes the result for the expansion. Since the expansion is not quoted, by default its result is subject to *both* field splitting (a.k.a. split) and pathname expansion (a.k.a. glob). The field splitting is clearly wanted here; commas are replaced by spaces, so this is meant to assign a comma-separated list of values to an array. However, because pathname expansion is also constantly active by default, the way used here is unsafe: if any of the values contain glob characters `*`, `?`, `[` or `]` then anything that looks like a pattern is matched against files on disk and replaced if any paths match. This unsafe use of field splitting is dangerous but incredibly common. (It's why I've implemented the [safe mode](https://github.com/modernish/modernish#user-content-use-safe) in the [modernish](https://github.com/modernish/modernish) shell library.) Assigning the result of a field splitting operation to an array is also generally pointless; you might as well get rid of the array and use an unquoted expansion directly. But do turn off globbing with `set -f` (or `set -o noglob`) before you do that, or it's not safe.
Looks like you're in IT, so a relevant page from the [bash manual](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html#Shell-Parameter-Expansion) 
Definitely the second. The first only matches T-2, T, or T+2. For an easier read, and especially if you want to grab larger intervals, use `seq` to generate the time part of the regex: grep -E "kernel: \[\ *($(seq -s \| $((tsecs-2)) $((tsecs+2))))\.[0-9]+\]" "$@" I also added a match for potential leading spaces in the timestamp section, since any messages that you might want to match early in the boot process have timestamps like this: Feb 2 20:52:24 server kernel: [ 141.269414] Call Trace: &gt;And can you explain what BASH\_REMATCH does? In a nutshell, the `[[ str =~ regex ]]` command matches `str` against the regular expression `regex`. The `BASH_REMATCH` array simply holds the matches found: * `${BASH_REMATCH[0]}` contains the entire substring that was matched, * `${BASH_REMATCH[1]}` contains the first (parenthesized) group match, * `${BASH_REMATCH[2]}` the second group match, etc. So, in the following: $ l="Feb 2 20:52:24 server kernel: [ 141.269414] Call Trace:" $ [[ $l =~ kernel:\ \[\ *([0-9]+)\.[0-9]+\] ]] &amp;&amp; declare -p BASH_REMATCH declare -ar BASH_REMATCH=([0]="kernel: [ 141.269414]" [1]="141") you can see that `${BASH_REMATCH[0]}` was set to the substring matching the regex (`kernel: [ 141.269414]`) and `${BASH_REMATCH[1]}` to the (only) group match therein (`141`). ## Further Reading * [About Regular Expressions](https://www.regular-expressions.info/) * [`bash` man page](http://man7.org/linux/man-pages/man1/bash.1.html), search for `=~`
&gt;This unsafe use of field splitting is dangerous but incredibly common. And for the safe (and arguably more readable) way: IFS=, read -r -a A &lt;&lt;&lt;"$1" The above tells bash to: * `read &lt;&lt;&lt;"$1"`: feed the first positional parameter as an unexpanded string to `read`... * `IFS=, read`: which splits it along comma boundaries... * `read -r -a A`: and assigns the collection of words to the (indexed) array `A` in "raw" mode (backslashes are normal characters, not escape prefixes).
I was able to get my script to work using the built in getopts. Thanks for the post though. I'll consider trying this when I get a chance.
Thank you for the explanation, it's working now.
thanks for updating, ive switched over to old reddit and can see it now
Completely off-topic, but: oh hey, another VASP user?
It’s hard to help you without knowing the context and sample data. Try instead of output of decrypt cat file with known samples and iterate on that. Put echo before grep to visualize parameters that grep getting from your script. Consider adding set -xv and set +xv around problematic lines to get debug info. Use https://www.shellcheck.net to follow best practices.
First, please improve your formatting. Second, bash seems like a poor tool for this job. You want to parse way harder than it should. Possible? Sure. Reasonable? Probably not.
Let me rephrase the problem and provide a MWE: $ ta=( a b c ) $ grep "${ta[@]/#/-e }" &lt;&lt;&lt; "a" # returns nothing $ grep ${ta[@]/#/-e } &lt;&lt;&lt; "a" # returns "a" $ grep -E $(echo "${ta[@]}" | sed 's/ /|/g') &lt;&lt;&lt; "a" # returns "a" I should have provided this as the example from the start, sorry about that. I tried all of the things you mentioned, which is how I came to realize that the quotes were the problem.
Finally, the moment you need a bash array, is the moment you need to consider anything else, Python is pretty popular.
&gt; First, please improve your formatting. I'm genuinely not sure what you mean? The code examples I provided are one-liners, so they don't make sense on their own lines. &gt; Second, bash seems like a poor tool for this job. You want to parse way harder than it should. Possible? Sure. Reasonable? Probably not. I'm actually not trying to parse anything - I just want to handle the case where the user wants to search for multiple patterns, and `grep` explicitly has a construct for that. I will update the post with a MWE that illustrates my point, since it seems the specifics of the code above obscured my question.
Put some white space, it's noisy to read code in line with speech. See? White space separates thoughts. "Searching multiple patterns" also known as parsing output. 
Example: I don't know how to do X, here is my code: for i in frob; do interbulate frobulator $x done
I removed my specific usecase, since it was obscuring my actual question, so as a side bonus, it got rid of a bunch of the inline code. &gt; "Searching multiple patterns" also known as parsing output. No. Parsing implies interpreting user input, usually in terms of building an AST. This is basically a search function where multiple arguments should be OR'd together, so I wouldn't call that parsing.
See definition 2. https://www.google.com/search?q=parsing&amp;oq=parsing&amp;aqs=chrome..69i57.967j0j9&amp;sourceid=chrome-mobile&amp;ie=UTF-8
I'm not trying to argue, I want to help. Just pointing out some issues here.
u/chiraagnataraj, I think you want this: $ ta=( a b c ) $ oldifs="$IFS"; IFS="|"; grep -E "(${ta[*]})" &lt;&lt;&lt;"a"; IFS="$oldifs" a The "magic" is simply a combination of array `[*]` expansion and a judicious redefinition of `IFS`. From the bash man page: &gt;`${name[*]}` expands to a single word with the value of each array member separated by the first character of the `IFS` special variable so `"(${ta[*]})"` expands to `"(a|b|c)"`. And, of course, it's generally a good idea to save `$IFS` before setting it to something else, and restoring it afterwards. There's a similar formula for positional parameters: oldifs="$IFS"; IFS="|"; grep -E "($*)" whatever; IFS="$oldifs"
&gt; Finally, the moment you need a bash array, is the moment you need to consider anything else, Python is pretty popular. Just want to say...not really true? There are numerous reasons to prefer bash, especially for things that interact with other command-line tools including system tools. This isn't for lack of knowledge of Python (although I personally would prefer Haskell) - it's just that this isn't the sole reason you would move to another language.
Quote all your substitutions so it will work if you ever include spaces in names: currentDIR=$(dirname "$NAUTILUS_SCRIPT_SELECTED_FILE_PATHS") 
&gt; examine or analyze minutely. I'm confused. Also, we're going down a semantic rabbit hole here, and I don't think either of us will be satisfied with the outcome of this discussion :D
Thanks! Yeah, I knew about the `$IFS` trick - it just seemed a bit hacky ;P That being said, I realized I should turn this assembly into a function anyway (I was using it in multiple places in my code), so nominally I could use this without issues. I think I'll settle on the for-loop method (I modified it in my actual code to ensure a one-element array doesn't end up with a pipe stuck at the end, which screws up the pattern) - I guess I'm spoiled by `concat` and `intersperse` and friends in languages like Haskell!
My mistake, I realize now, it's the first bullet point, but the second description. COMPUTING analyze (a string or text) into logical syntactic components, typically in order to test conformability to a logical grammar. 
Also, very fair, that wasn't my intention. Glad to see you got your answer.
I like your approach, I'm curious if something similar could be achieved via [https://github.com/koalaman/shellcheck/wiki/SC2001](https://github.com/koalaman/shellcheck/wiki/SC2001)
Yeah, that was my first instinct (i.e. replace `-e` with `|` in the first attempt), but that ends up not working since `echo` inserts spaces in the final output (when you need the elements effectively concatenated).
&gt;Yeah, I knew about the $IFS trick - it just seemed a bit hacky It's a bash idiom, and like all idioms, it'll look hacky until you get used to it, after which it just seems like the natural thing to do. :) Since you mentioned Haskell's `intersperse` (or more probably `intercalate`): # intercalate &lt;delimiter&gt; &lt;item&gt;... intercalate() { local oldifs="$IFS"; IFS="$1"; shift 1 echo "$*" IFS="$oldifs" } ta=(a b c) grep -E "($(intercalate "|" "${ta[@]}"))" &lt;&lt;&lt;"a" Also, the code you finally settled on: tas="" for i in "${ta[@]}" do tas+="${i}|" done grep -E "${tas}" &lt;&lt;&lt; "a" # returns "a" has a major bug in it, in that it generates a regex with an empty subexpression, e.g. `a|b|c|`. BSD grep correctly calls it out as an error, but GNU grep quietly accepts it...and matches *everything*: $ seq 1 10 | grep -E "1|3|" 1 2 3 4 5 6 7 8 9 10 To fix that, simply: grep -E "${tas%|}" ...
&gt; Also, the code you finally settled on has a major bug in it, in that it generates a regex with an empty subexpression, e.g. a|b|c|. BSD grep correctly calls it out as an error, but GNU grep quietly accepts it...and matches _everything_ Yeah, figured that out. I modified the loop in my own code to iterate until the final element, but your solution is much more elegant (I'm not quite sure why I didn't think of it, since I _am_ using parameter expansion elsewhere)! Thanks :D
You can, but it's a two-stage process: # Dump array as a space-delimited string... tas="${ta[*]}" # ...then transform it inline grep -E "(${tas// /|})" &lt;&lt;&lt;"a" and, of course, if any element of `ta` contains spaces, you'll get something quite different from what you probably want.
&gt;that was my first instinct (i.e. replace `-e` with `|` in the first attempt), but that ends up not working since `echo` inserts spaces in the final output (when you need the elements effectively concatenated) It has nothing to do with `echo`, and everything to do with what `${ta[@]/#/-e }` really means. As the bash man page says: &gt;`${parameter/pattern/string}` &gt; &gt;\[...\] If `parameter` is an array variable subscripted with `@` or `*`, the substitution operation *is applied to each member of the array in turn*, and the expansion is the resultant list. In other words: ${ta[@]/#/-e } ==&gt; "${ta[0]/#/-e } ${ta[1]/#/-e } ${ta[2]/#/-e } ..." so your first try didn't work: grep "${ta[@]/#/-e }" &lt;&lt;&lt;"a" ==&gt; grep "-e a -e b -e c" &lt;&lt;&lt;"a" but your second did (mostly because your search terms contained no spaces): grep ${ta[@]/#/-e } ==&gt; grep -e a -e b -e c &lt;&lt;&lt;"a"
&gt;Recently Fedora Magazine posted an article about making nautilus scripts to do everyday tasks. If you're talking about [Integrating scripts in Nautilus to perform useful tasks](https://fedoramagazine.org/integrating-scripts-nautilus/)...I'm not sure I'd call a two-year-old tech article "recent". :) Anyway, I don't do Nautilus, but a quick read of Ubuntu's [NautilusScriptsHowto](https://help.ubuntu.com/community/NautilusScriptsHowto) indicates that `NAUTILUS_SCRIPT_SELECTED_FILE_PATHS` is a newline-separated list of selected files. Your current script would therefore break with multiple file selections (which I suspect you didn't try). I'm also not sure why you decided to use `NAUTILUS_SCRIPT_SELECTED_URIS` as well, since you don't get any useful difference the way you use the value anyway. Here's a slightly more robust version of your script, that deals with many issues you skipped over (multiple selections, filenames with spaces/special characters, multiple periods, no extension, etc.), but doesn't deal with others (e.g. multiple MKV videos from the same directory overwriting each other with the same name in your destination directory): ``` #!/usr/bin/env bash while read f; do current_dir="$(dirname "$f")" fname="$(basename "$f")" rname="$(basename "$current_dir")" [[ $fname =~ ^.*(\.[^.]*)$ ]] &amp;&amp; ext_with_dot="${BASH_REMATCH[1]}" || ext_with_dot="" mv "$f" ~/Videos/"${rname}${ext_with_dot}" done &lt;&lt;&lt;"$NAUTILUS_SCRIPT_SELECTED_FILE_PATHS" ```
Consider using [Percona Xtrabackup](https://www.percona.com/doc/percona-xtrabackup/2.4/index.html) instead of just mysqldump.
${x}1 ?
It worked! Big thank to you my friend, much appreciated!
change y="$x1" to y="$(echo $x)1" 
Notice me, Senpai pj(){ for((i=2;i&lt;$#;i++)){ printf "%s$1" "${!i}"; }; printf %s "${!#}"; }
Not sure if I understood the question but doublequotes should also work. x="/dev/sda" echo "$x"1 Doublequotes are considered as weak quotes and a lot of stuff including the variables will be interpreted by the shell. You should always doublequote variables you are calling. Single quotes are strong quotes and nothing will be interpreted between them.
i did also consider using percona's xtrabackup but i saw no benefit at all using it. What makes you think xtrabackup is more suitable than mysqldump?
i might consider adding some more options on the mysqldump utility. I tend to use no table lock but single transaction instead. &amp;#x200B; Maybe some switch to not have to hardcode a password into the script like adding a command line option parser being able to provide a \~/.my.cnf file or something like that: &amp;#x200B; Also if you want to provide the possibility to be able to do Point in time Recovery u should consider getting the binlog information corresponding to your backed up DB into your dump. I just got the actual binlog file and position before executing the backup. #command line parser: while [ "$1" != "" ]; do case $1 in -u | --username ) shift username=$1 ;; -p | --password ) shift password=$1 ;; -m | --mycnf ) shift mycnf=$1 ;; * ) usage exit 1 esac shift done if [ ! -z "$mycnf" ] ; then auth="--defaults-file=$mycnf" ; fi if [ ! -z "$password" ] &amp;&amp; [ ! -z "$username" ] ; then auth="-u $username -p$password" ; fi options="--single-transaction --add-drop-database --add-drop-table --routines --triggers" #makes mysqldump calls sexy like: if mysqldump $auth $options $db &gt; "$dumpfile" ; then #success else #fail fi &amp;#x200B; &amp;#x200B; but thats just some add on's :)
If you've considered it for your use case and seen no benefit then there may be none for you, I don’t know your specific circumstances. I had thought that is you are keeping 10 backups then the incremental backup feature might be of use. For me, I like being able to take hot backups of a busy Magento database without affecting site performance, mysqldump locks the site up for several minutes.
i didn't want to be offensive with my comment btw. was just wondering haha :) &amp;#x200B; Yes i know Xtrabackup is capable of Incremental Backups but i smh. didn't get it to work properly and decided having a full dump every day with binlog backup &amp; binlog information gives me the possibility to restore to any specific point within these 10 Days without restoring something out of our backup solution. If i need a state even earlier i can just restore the &lt;file&gt;.tar.gz i need and the binlogs after that dump. &amp;#x200B; For that Magento Database: Does it also lock the site if you use --single-transaction instead of --lock-tables?
Thank you all for your answers, good to know how to do it in different ways. I shall keep this in mind. &amp;#x200B; It all worked out!
Instead of array you can try file, perhaps even virtual file and use grep -f to pass new line separated patterns.
should probably be: # /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) # and Bourne compatible shells (bash(1), ksh(1), ash(1), ...). export JAVA_HOME = /usr/lib/jvm/java-11-oracle Lines that start with a hash (#) are comments. You needed to put your addition at the end of the file after the comments. Why were you doing this in /etc/profile? You should be able to do this in \~/.profile or possibly even in \~/.bashrc unless you have multiple users on your system that need access to JDK.
You can't have spaces in a declaration, and the comment isn't recognised if there isn't whitespace before it.... export JAVA_HOME=/usr/lib/jvm/java-11-oracle 
dosent work.
I was following a youtube turorial, seems like I messed up something more &amp;#x200B;
Are you sure that you don't mean export JAVA_HOME=/usr/lib64/jvm/java-11-oracle note - ***lib64***
I tried that but the problem persisted. It is now solved, I forgot a space before # in line 1. &amp;#x200B;
&gt; It is now solved, I forgot a space before # in line 1. You don't need the `#` or anything after it - it is a comment.... My first comment - export JAVA_HOME=/usr/lib/jvm/java-11-oracle is the same as export JAVA_HOME=/usr/lib/jvm/java-11-oracle # /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) If you just removed the two spaces around the `=` sign and left the left the rest of the line that I removed on... i.e. export JAVA_HOME=/usr/lib/jvm/java-11-oracle# /etc/profile: system-wide .profile file for the Bourne shell (sh(1)) Then you really need to learn how to follow directions when asking for help. 
You need to remove the spaces either side of the `=` and add a space at before you comment at the end of the line, so it should look like this: `export JAVA_HOME=/usr/lib/jvm/java-11-oracle # /etc/profile: system-wide .profile file for the Bourne shell (sh(1))`
This just transforms the data to seconds since the beginning of time (01/01/1970) !#/bin/bash DATE_A=$(date --date "Apr 15 19:58:39 2019 GMT" +%s) DATE_B=$(data --date "Tue Feb 5 14:39:08 EST 2019" +%s) echo $DATE_A echo $DATE_B
I realize this does not answer your question directly, but I have found it's much easier to just use gnu utils instead of the default osx/bsd variants. brew install coreutils This installs 'gdate' along with many of the basic gnu commands (dd, head, less, cut, etc). You can even add `PATH="/usr/local/opt/coreutils/libexec/gnubin:$PATH"` to your `~/.bash_profile` and not have to prepend **g** to every command. Once done, you can simply do the following to covert dates to epoch $&gt; date "+%s" -d "Tue Feb 5 14:02:56 CST 2019" 1549396976 $&gt; date "+%s" -d "Apr 15 19:58:39 2019 GMT" 1555358319 coreutils: $&gt; ls /usr/local/opt/coreutils/libexec/gnubin b2sum@ chmod@ cut@ du@ fmt@ join@ mkdir@ nohup@ pr@ rm@ sha384sum@ stat@ tee@ tsort@ users@ base32@ chown@ date@ echo@ fold@ kill@ mkfifo@ nproc@ printenv@ rmdir@ sha512sum@ stdbuf@ test@ tty@ vdir@ base64@ chroot@ dd@ env@ groups@ link@ mknod@ numfmt@ printf@ runcon@ shred@ stty@ timeout@ uname@ wc@ basename@ cksum@ df@ expand@ head@ ln@ mktemp@ od@ ptx@ seq@ shuf@ sum@ touch@ unexpand@ who@ cat@ comm@ dir@ expr@ hostid@ logname@ mv@ paste@ pwd@ sha1sum@ sleep@ sync@ tr@ uniq@ whoami@ chcon@ cp@ dircolors@ factor@ id@ ls@ nice@ pathchk@ readlink@ sha224sum@ sort@ tac@ true@ unlink@ yes@ chgrp@ csplit@ dirname@ false@ install@ md5sum@ nl@ pinky@ realpath@ sha256sum@ split@ tail@ truncate@ uptime@ \[@
Not sure why you were downvoted... your solution would also work but the cleaner method would be to bracket the variable `y="${x}1"` 
Great answer! I am surprised how easily date deals with various date formats without any hand holding. I have gotten a solution working with a combination of your answer plus my edit.
Yeah I gotta agree, I don't like how Apple makes little tweaks to the GNU utils I'm familiar with. The problem with this is I can't assume every developer I am going to pass this script to has installed that package.
Apache2 is easy enough to get going, and you already know bash, so you could go (way, way) old-school and [run your bash script as a CGI program.](http://www.yolinux.com/TUTORIALS/BashShellCgi.html). 
I had some custom log parsing scripts that I wanted to share with my internal team. My experience with WebDev has been mostly Ruby based so I opted to take that route. For ease, I translated my BASH to Ruby and then stood it up with [Sinatra](http://sinatrarb.com/). I had no use for CAPTCHA as my deployment is not facing the world. But there are plenty of [Ruby Gems](https://github.com/bmizerany/sinatra-captcha) to add features such as these with ease. I'll be checking out [Roda](https://roda.jeremyevans.net/) for my next web application. Perhaps the structure of Roda will click with you better than Sinatra? Bonus: As a ToDo, consider integrating a lightweight database that will capture and store all of the user requests. It might be interesting to review the historical queries of your users. 
It works the same in macOS. 
If you wanted to be an asshole you could run the bash script as a cli program lol, but you'll need to render some html either way. I'd suggest a different language but a low complexity stack so you can write a route handler that's 1:1 with your script and go about your day. For instance, python/flask, nodejs/express and ruby/sinatra would all get you up and running pretty quick, barring unfamiliarity with the language itself.
It's actually the other way around. The utilities that come with the Mac are based on the BSD utilities since the core of macOS is based on some BSDs, which are much older. It's the GNU coreutils that add all these extra features and we're all spoiled by them.
This sounds like a fun project -- want to do it together? I can host a server for us to use for testing/development and I have actually been looking at getting a cloud server to use for my own web ideas. Happy to host this if we get it off the ground. PM me if you're interested! Send your bash script also and I can look at making it into some *JavaScript*, or whatever we decide to use?
I was inspired by `pass` and wanted to create a tool that's just as simple to use. Currently still under very heavy development and open to suggestions/critiques/yelling about how insecure &lt;x&gt; construct is :D I do plan on implementing re-encryption by specifying new keys (currently, if `.gpg_id` exists, it will always take precedence over `-k` arguments) as well as allowing the selection/editing of multiple categories.
The guy just wants a simple front-end for a bash script, I don’t think translation to Ruby, using captchas, or standing up a database would be in scope. Though I agree that they’re cool.
Check this out OP: https://www.reddit.com/r/sysadmin/comments/74efji/web_interface_for_5_bash_scripts/ This one looks nice https://github.com/bugy/script-server
CGI output doesn’t need to be HTML.
Though, be careful with the way way old-school shell injection.
Keep up the good work. I’m going to keep an eye on this, since I’m always looking for a better way to keep long-term bookmark database. Any plans for browser integration?
Hey this seems neat, gonna give it a try.
I'm not sure about browser integration in the sense that I'm not entirely sure how powerful WebExtensions can be and what they can do. For example, something that would be useful is allowing `bkmk` to be selected for the primary bookmark storage instead of the built-in browser stuff. I don't know if this is possible given the current APIs though. I wouldn't be surprised if we could write a shim like ffpass that, while not totally integrated, would at the very least be accessible from the browser using native messaging. For right now, though, I'm focusing on getting this to where I want it to be before thinking about other things like browser integration or wrappers for other platforms.
Nice project. Thank you dude. 
I think if you remove the "{" before top and the "}" after head -n 6 it will work. Since you are splitting that into 2 commands on the same line it isn't find the one in the front. &amp;#x200B; Hope that helps.
OP, this could be very useful for me too. Please let us know your end result.
I remove the brackets, because while didn't seem to like them. But it does not log the Memory yet. Change file to the outputfilename and change sleep to your preferred timeout: file=/volume1/uptime2.log; while true; do top -bn2 | grep "Cpu(s)" | grep -v grep | sed "s/.*, *\([0-9.]*\)%* id.*/\1/" | awk '{print 100 - $1"% CPU load"}' &gt;&gt; $file;ps -Ao user,comm,pid,pcpu,pmem,rss --sort=-pcpu | head -n 6 &gt;&gt; $file; echo -e "\n" &gt;&gt; $file ; sleep 1; done
right - technically you can io anything you like, but if the goal is to get away from the cli then that suggests a ui which suggests html.
What version of Top are you using?
Does this need to be a script? Because there are tools already for this kind of thing eg atop, sysdig etc
You might check out [recap](https://github.com/rackerlabs/recap/tree/master/src) even if just to use as a reference. 
think the following has it escaped properly... nohup sh -c 'while true; do { top -bn2 | grep "Cpu(s)" | grep -v grep | sed '\''s/.*, *\([0-9.]*\)%* id.*/\1/'\'' | awk '\''{print 100 - $1"%"}'\''; ps -Ao user,comm,pid,pcpu,pmem,rss --sort=-pcpu | head -n 6; } &gt;&gt; /volume1/uptime.log; sleep 30; done' &amp; &amp;#x200B;
this works, thanks :)
When I type top -v it says procps-ng version 3.3.10
From OP's second paragraph: &gt; I have no idea about web development &gt; I'm not adverse to re-writing it in another language &gt; To make it more complicated, it should probably include a CAPTCHA to mitigate abuse. I included the idea of a database as a stretch goal outside of his stated scope. 
I was just thinking of a browser plugin or bookmarklet. There are lot of other ways to accomplish that though. I’m going to try it out soon. 
Something like below for the ip ([0-9]{1,3}\.){3}[0-9]{1,3} Add this for username? ([a-zA-Z0-9]+@) In a hurry, so i only glanced at the question. But toss those in a test however you need them. 
It does what you said, tells set to use everything after as arguments. So it doesn't parse the second -- as an option but as an argument. Which means here, it sets $2 to be "---" You can test yourself: set -- something -- else echo $1 $2 $3 
The other element to this is the `exec "$@"` on line 36. The script changes its own arguments, and then execs them. I don't know why it is doing this instead of just exec'ing what was provided to the `set --` command, but I assume there is a reason.
I think I get it now. It is executed as part of the Dockerfile.
I don't want to check for an IP pattern since it may sometimes be a hostname instead of IP. (I will clarify that in my post.) For the user-name, this is what I got: &gt; test1=protocol://user@ip.ad.dr.ess/some/path/on/server &gt; echo ${test1##*([a-zA-Z0-9]+@)} protocol://user@ip.ad.dr.ess/some/path/on/server So, that's not quite working for me. Thanks though.
Look at the 7.3 sample [here](http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-7.html)
Got more examples of the type of output your checking? Once i get back to the laptop later i can help a bit more, annoying on mobile.
I added more examples at the top: test1="protocol://user@ip.ad.dr.ess/some/path/on/server" test2="protocol://ip.ad.dr.ess/some/path/on/server" test3="protocol://user@myHostname/some/path/on/server" test4="protocol://myHostname/some/path/on/server" The function getIPAddress() gives the exact output I want, so we can test with that. My goal is to eliminate the `if` statement and make one unconditional pattern. The output for those inputs should be: ip.ad.dr.ess/some/path/on/server ip.ad.dr.ess ip.ad.dr.ess/some/path/on/server ip.ad.dr.ess myHostname/some/path/on/server myHostname myHostname/some/path/on/server myHostname Protocol could be http, svn, svn+ssh, ftp, file, whatever. I just know it will end with `://`. 
So something like this could be used or modified as it's not the most nice looking. ^(.*)://(([a-zA-Z0-9]+@[a-zA-Z0-9\.]+/)|([a-zA-Z0-9\.]+/)) Could also be added on to make it more exact like ^(http|https|ftp|etc)://(([a-zA-Z0-9]+@[a-zA-Z0-9\.]+/)|([a-zA-Z0-9\.]+/)) Depending how you work it, if i rememer right with bash it will place them into their own values like $1 $2 depending how you do it. There's multiple ways. if you were matching something like myHostname/some/path/on/server you will have to modify it. 
With the way I am using it, the matching part will be deleted. So I am matching everything proceeding IP/hostname. I think I need to step back and understand why simple things aren't working before we get into complex things. lets assume protocol is always exactly the string "protocol", and user (If present) is always exactly the string "user". I don't want to match anything after the '@' sign. I don't care what is after. When I try: &gt; test1=protocol://user@ip.ad.dr.ess/some/path/on/server &gt; echo ${test1##*://user@} ip.ad.dr.ess/some/path/on/server That works as expected. However, if I try to block `user@` in regex parens, there is no match. &gt; test1=protocol://user@ip.ad.dr.ess/some/path/on/server &gt; echo ${test1##*://(user@)} protocol://user@ip.ad.dr.ess/some/path/on/server Same when I try to make the previous regex conditional: &gt; test1=protocol://user@ip.ad.dr.ess/some/path/on/server &gt; echo ${test1##*://(user@)?} protocol://user@ip.ad.dr.ess/some/path/on/server For reference, I am using these as a guide: https://www.tldp.org/LDP/abs/html/string-manipulation.html https://www.tldp.org/LDP/abs/html/x17129.html I'm specifially lookting at: The question mark -- ? -- matches zero or one of the previous RE. It is generally used for matching single characters. Maybe because of the bit that says "It is generally used for matching single characters." is why you haven't used it in your examples, but it looks like exactly what I need to me. Even still, I can get any of it to work.
Ok, I think I may know the answer, but I don't know how to be sure. I suspect my pattern is using globing, and not regular expressions. Thus, what I am trying to do is impossible. I believe I will need to use regular expressions, not gobbing to do what I need.
u/PageFault, regexes are indeed your friend for this, but as you realized, you were actually using glob patterns instead. Here's the easiest way to get what you want: #!/usr/bin/env bash urls=( http://anthropoid@192.168.1.23/this/is/a/test.html http://192.168.1.23/this/is/a/test.html http://anthropoid@example.com/this/is/a/test.html http://example.com/this/is/a/test.html ) for url in "${urls[@]}"; do if [[ $url =~ ^[^:]+://(([^@]+)@)?([0-9]+(\.([0-9]+)){3})/ ]]; then # Uncomment the next line to see how bash stores its regex matches # declare -p BASH_REMATCH user="${BASH_REMATCH[2]}" ipAddr="${BASH_REMATCH[3]}" else user="" ipAddr="" fi echo "$url ==&gt; user='$user' ipAddr='$ipAddr'" done ## Further Reading * [`bash` man page](http://man7.org/linux/man-pages/man1/bash.1.html) \-- search for the `=~` operator to see how it works. * [Regular-Expressions.info](https://www.regular-expressions.info/) \-- more than you probably remember about regexes, even if you're an expert. * [Regex101](https://regex101.com/) \-- a regex playground to explain and test arbitrary regexes. *NOTE*: For this particular problem, select the PCRE flavor and change the delimiter to `%`, otherwise it'll complain about the slashes in your regex being interpreted as delimiters themselves.
I think it is more correct to say it is executed when the docker container starts. see $ help set help set set: set [-abefhkmnptuvxBCHP] [-o option-name] [--] [arg ...] Set or unset values of shell options and positional parameters. Change the value of shell attributes and positional parameters, or display the names and values of shell variables. ... -- Assign any remaining arguments to the positional parameters. If there are no remaining arguments, the positional parameters are unset. It is setting the positional arguments. It is a way of building up a the command to be executed and appending further further arguments to the command. Take a look at the mongo entrypoint: https://github.com/docker-library/mongo/blob/master/3.6/docker-entrypoint.sh
Yes your description is the correct one. I meant to say that when you type: $ docker run -d --name foo jenkins:3.5 arg1 arg2 arg3 etc.. Then those arg1, arg2 and arg3 arguments will be placed into `"$@"
Command groups need to end on a line break or `;` while true; do { stuff; more stuff; }; done
Your first problem is you didn't escape the literal brackets. `[[a-zA-Z0-1]*@]?` should be `\[[a-zA-Z0-1]*@\]?
If you want to match literal brackets around a character set, `[[a-zA-Z0-1]*@]?` should be `\[[a-zA-Z0-1]*@\]?`. This should get you started on the right track. IFS=/ read -a Url &lt;&lt;&lt; "protocol://user@ip.ad.dr.ess/some/path/on/server" echo ${Url[2]#*@}
Disclaimer: I don't even know C. This will increment and print `s` until its square is greater than `n` until (( s*s &gt; n )); do echo $((s++)); done
Thanks for contributing to this subreddit! The general consensus around here is that tldp.org is seriously outdated and poorly maintained. For example, the section you referred contains 'old test' or `[`, which is deprecated. Please consider looking into resources which are actively maintained. Thanks again.
Crazy! I don’t think I’ve ever seen an until loop in the wild!
I still think it's a great resource. What do you recommend over it?
Have a read of this: https://wiki.bash-hackers.org/scripting/tutoriallist Also: the sidebar.
You were close. Just remove the spaces after -e grep "${ta[@]/#/-e}" The above expands to grep -ea -eb -ec which is equivalent to grep -e a -e b -e c
Ah works perfectly! Thank you :)
Bash is the most ubiquitous modern "shell", or command language. It's like duct tape you can use to strap processes and languages together. If you can do it on a command line, someone is out there doing it in Bash. This means that your starting point is going to depend on what you want from it. This is a good little community for fast answers; just be aware that hardly anyone votes.
Sounds like I found the right place then finally because what I have learned from subreddits and voting is that the voting process is normally biased towards a certain agenda. Nice simple explaination and nice to meet you.
`bash` is one of the interfaces to communicate with the operating system. Just like on a desktop you can create/run/play/modify files, start applications etc., it can all (mostly) be done through bash. There are a few exceptions - things that bash can do, but the desktop can't, and vice versa. Bash is a programming language, and as such can be used as such. The GUI is user friendly easy to interact with, as well as being able to graphical applications such as media players.
Bash is the program that is displaying the command prompt in a terminal, and waiting for you to type a command line. It is then executing the commands you type at the terminal. It has a tiny programming language built in so that you can do loops and if/then/else structures and such. Knowing about bash is interesting for automating stuff you do at the terminal on a machine. The same things you can manually type at a command prompt, you can put into a script file. And the reverse, everything that works inside a script you can also type at the command prompt.
If I wanted to start coding in linux type stuff I would start with this language? Also do you have a place to start? I found this and I think it looks like a good place to start. https://ryanstutorials.net
Think it as a crafting table. Many tools are already built-in. &amp;#x200B; [https://www.gnu.org/software/bash/manual/html\_node/Basic-Shell-Features.html#Basic-Shell-Features](https://www.gnu.org/software/bash/manual/html_node/Basic-Shell-Features.html#Basic-Shell-Features) &amp;#x200B; A man with it's tools and with the crafting skills. Can do about anything they want to created. 
Bash chapter looks fine, go for it. You will learn the basics and that will make you think to work on your own project on bash. Good Luck
Is this a good place to start from? It seems a little advaced just taking a quick glimse at it. I have been looking for a developer but getting scammed over and over and was told BASH is where to start learning. I am a complete noob.
Where to start? Start using the Linux terminal whenever a gui is not necessary. Get used to moving, manipulating, etc files from the terminal. Using the Linux terminal is an interactive sh (not bash) prompt. The differences between sh and bash are insignificant at the early stages. A good thing to learn first would be redirection operators. Bash makes it pretty easy to create and edit files without ever opening Ann interactive editor. Once familiar with the basics, you can learn the intricacies of variables, loops, logic, and anything else you would normally learn with a new language. Tip: When you are starting out and using cp or mv, use the -i option for an interactive prompt when you are about to overwrite an existing file. If you overwrite a file with these commands, it is gone. Forever. 
Bash is actually also available by default on macOS, and thanks to the Linux subsystem in windows 10, you can now use Bash on all 3 of the major OS's. That being said, bash is like DOS, if DOS evolved past 1993. It is a text based interface to communicate with the operating system. And can be used to write bash scripts, to do all kinds of things. Any program you have, that can be used in the terminal, can be controlled with bash. I've got a bash script that I can call, with a youtube link, and 2 timestamps, and it will download the youtube video, and edit the video to only include the time between my two timestamps. Do you have a set of repetitive commands you type, to create a new project, or backup a set of folders? Bam, you write a bash script that includes those commands, and your repetitive work, couldn't be any easier.
Thanks for taking a look.
Looking it up quick but still not knowing the answer gui is the browser?
Bash is not a good "first programming language". Learn any other proper programming language, likey python, ruby, java - anything but bash, first. Bash's conventions are a bit loose and if you don't know basic programming/debugging, it will be hard to write good-enough programs. Bash programming inherently, is not enough to be good programmer. Read `man bash` when you have the time. It is very detailed. When you have a task you want to do, but do not know how, tldp.org is a good place to look for. It has working examples of loops, conditions, piping, handling files etc., which should be more than enough to get a head start. 
&gt;couldn't be any easier. We can beg to differ lol.
Somebody recommended to me to start with BASH over anything. So I should really start with HTML then PHP then BASH? This was my original thought process.
Graphical user interface. When you have a desktop environment installed everything essentially runs is graphical even the a terminal emulator is a gui. All I'm saying is you can do almost anything on your computer on the terminal except gui specific stuff like your browser.
Yep I figured thats what you meant.
Open up a terminal which uses bash as the shell. Get some cheat sheets and watch some bash beginner YouTube videos. That's where to start. https://www.cheatography.com/davechild/cheat-sheets/linux-command-line/ https://www.youtube.com/results?search_query=linux+bash+navigation+for+beginners Google for more info after you absorb this info. Practice in the terminal. Type out this few commands date cal bc Now to man to see what these do man date man cal man bc Google if you get stuck, to learn more about them. What I did is learn 10 commands at a time. When I understand them. I go to the next 10. https://www.hongkiat.com/blog/basic-linux-commands/ Navigation is what you really want to learn first. https://www.digitalocean.com/community/tutorials/basic-linux-navigation-and-file-management 
I would agree with blitz. Is good to learn how to use the terminal, butt you want to learn programming basics in a more structured language. Bash had lot of intricacies the a more "proper" language would not have. Because bash has so much direct access to external tools, it can be difficult to figure out what command is built into bash and what is an external program. Take your common if statement for example: if [ -f file.txt ]; then echo 'it exists!'; fi The if is a built in bash command but the bracket [ is actually an external program!
Thanks a lot!
Might I ask what you are trying to learn? If there is a goal in mind, knowing it may help us. Otherwise, deciding how to prioritize or sequence HTML, PHP and bash is mere guesswork. 
Develop an onion website.
In that case you don't need bash at all. Just learn any web framework. 
I do not remember exactly why I thought I needed this but what do you recommened then? For bare minimum atleast if I don't end up giving up and try paying somebody again.
Bash is really only a stop-gap for automating operating system and build processes. You're much better off just learning Python, as it's easy to get into, but if you look deeper you'll get much more important information about how programming actually works.
I'm with blitzcraft on this. If the goal is web development, you probably don't want to start with bash. You might never use it, and HTML, CSS, a backend language and familiarity with databases should all take priority over shell scripting. 
I am not really looking for a job. I am looking to creat websites for my own businesses. With that said if I just need a plain website build for security wouldn't PHP be an easier route to take?
I mean in all honesty with you want to create a website for your business, just spin up a [WordPress](https://wordpress.com/) instance. You actually need a degree to create anything that's going to compete with a base WordPress instance in speed, reliability, and appearance.
.onion site. Wordpress wouldn't work for that?
Launching and controlling other processes. It has functions to check files and conditionally process them. There are mechanisms to run processes in the future and monitor their completion as other processes are running. It allows logging so you can cgeck if something worked correctly, diagnose what went wrong and when. It glues stuff together. 
...What exactly does your business do?
I sell products from China.
well it depends what you're doing. I still see a crap ton of bash scripts I can't read. The syntax gets real weird. But something like using cp and rsync to backup a few directories, can be easily expressed in a couple of lines.
If you are a Windows person consider Bash to be the rough equivelant of Power Shell, in fact when using Bash you will notice a lot of similarities between Bash and Power Shell, Microsoft took a lot of good aspects from Bash. If you are looking to get into programming knowing enough bash to get you by will suffice. You wouldn't develop a commercial solution in Power Shell, nor would you in bash (although I have seen pure bash solutions in enterprise environments), you are more likely to develop in Python, Ruby, Perl, PHP etc but given bash is where you will spend a lot of time on a Linux system its helpful to know what you are doing, find a cheat sheet, print it and keep it on your desk, these are useful references to find new cool tricks in bash: [https://cheat.sh/](https://cheat.sh/) [https://www.commandlinefu.com](https://www.commandlinefu.com) Also nixCraft have a lot of good guides and articles [https://www.cyberciti.biz/](https://www.cyberciti.biz/) Use bash as a tool to do what you need but don't feel the need to learn everything about it.
Asking how `bash` can help you sell products on a website is a bit like asking how a toolbox can get you to Hong Hong.. They will help, but are not the primary parts needed. Better ways - 1) purchase a ready made ecommerce solution = buy a car 2) pay someone to make a website for you = get a taxi These are fairly secure, safe ways of doing it... ... then 3) buy the domain and webspace, purchase or use free versions of the software required and build a means of transport yourself.... how safe / secure / reliable they are depends on how you build and run it Yes - you can build anything from a go-cart, through a bicycle to a limo using the same tools, but no matter what you choose you need more than just the tools...
For the bare minimum - python is an easy enough language to start. Flask with python is quite approachable. You can get yourself to host a website in a matter of days. Even if you didn't have any experience programming.
I like this answer to your question: https://www.quora.com/What-is-the-use-of-Bash-Unix-shell-programming My take on it is that bash is useful for the things he listed, to glue programs together, and to do things with files. In my job I work with files, I move them between disks using cp or rsync, then I start programs to manipulate the files, then I take the manipulated files and do something different with them. Let's say you have 50 video files that you want to move from one disk to another, but first you need to run ffmpeg to change the codec from h264 to h265, you can do all of this with a for loop. 
Hi, just to throw some more info at you that might help clear up some terms and confusion :) This is a terminal emulator (called Terminator). It's the GUI that helps to visualize anything to do with "commands". [Terminal Emulator](https://sakis.cc/cloud/s/ciqndZXec3jWxpF) Inside this terminal emulator, there is currently bash running. It's in interactive mode, meaning it waits for you to write stuff to it. [Bash](https://sakis.cc/cloud/s/bj7BP43D9Wy7g5o) This is the prompt. It prompts you to write stuff. All interactive command line programs have a prompt so that you know when it (bash or whatever else) has stopped processing, and is waiting for you again. These prompts are customizable in many cases. [Prompt](https://sakis.cc/cloud/s/ekAqKpo6ksDBk3p) This is another terminal emulator (called Gnome Terminal), just so that you have an example of another one. It visualizes things a bit differently, but the command line interface that it runs is again bash. Think of it like a web browser. You can open the same pages from Firefox and from Chrome, they might visualize a few minor things differently and they have different options in their menus and settings. Still, you will pretty much get pretty much the same "basic" experience. [Other terminal emulator](https://sakis.cc/cloud/s/EZKpFr693XMmZgc) Now this is another shell (or command line interface). This one is not bash, this one is called zsh. It is similar to bash, but has more advanced features. You might see it looking differently most of the time, but it is configurable like bash, and can be made to look exactly like it. The main difference is in it's features. For instance it can colorize your commands. Bash cannot do that. [zsh shell](https://proxy.duckduckgo.com/iu/?u=https%3A%2F%2Fi.stack.imgur.com%2FoxRmM.png&amp;f=1) These are a few commands that you can type in the command line interface. See how the prompt appears every time a command is completed (instantly in this case, since the commands are fast. Bonus: type sleep 3 in a terminal. This will demonstrate how the prompt will only appear when the command is done.) [Commands](https://sakis.cc/cloud/s/5yJfNqoE7k7jGnq) Let's say you need to perform those commands every day for some reason. You might want to make a script (a script is something like a program) like the following: [Script](https://sakis.cc/cloud/s/DrgAJpn4Sz92ExN) Then you can run the script like this. The script now ran in non-interactive mode; notice how the prompts and commands from the above example do not appear, only the prompts and command related to the script launching are shown. In non interactive mode, the script generates output only when it is instructed to (in this case ls did generate the output you see). [Run script](https://sakis.cc/cloud/s/LqFQmNrfpkYJMBT) Hope this helps a bit. :)
Your Welcome. I get in a habit watching YouTube video's every single morning while your drinking your morning coffee. I learn so much, by doing this great habit. What every you want to learn. You just search at [YouTube.com](https://YouTube.com) and sit back and enjoy watching. I usually watch at least 9 videos about the same subject. Never learn from just one person. Some people got it right, some people might not have it right, some people miss a important part, or one is just more understandable then the other person. 15 years using Linux and I still do this habit. Linux is a unlimited learning tool. I know because after 15 years I'm still learning new things. Enjoy Linux as I have for the past 15 years. Any question, I'm more then happy to help out.
I've been coding for years, and still have trouble with bash beyond simple scripts. I would recommend Python, C or Perl to learn. I say Perl because it's super useful on command lines and is easy to learn. 
&gt; Using the Linux terminal is an interactive sh (not bash) prompt. The differences between sh and bash are insignificant at the early stages. `sh` does not have any history. I'd say that being able to call up previous commands would be a significant difference for a newbie.
I am not trying to be discouraging, but you have a long way to go to get to your intended purpose of running an onion site to sell products. Start with trying to learn "how to make a website" on a regular platform like an AWS instance. (Free for one year) and frankly, you will need that time to learn. You would start out with running a Linux server, like Ubuntu or AWS Amazon Linux which is redhat based. You would need to use the command line in Linux to do this, which *is bash*, which is why you would need to learn bash. With it, you would install and configure things like the Apache web server or nginx webserver. You don't use bash to develop the website, you use it to administer your Linux was instance. Once apache is installed, you can install things like php and an editor so you can write HTML, CSS, php code that is your website. There is so much to learn here. There is basic website stuff like how to make a menu, boxes, colors, fonts, and actual login for users. How do you protect user logins and make them safe and in the case of an Onion site, anonymous and safe from hacking or FBI? (Assuming there is something sketch with what your selling). How do you handle credit card transactions and shipping? How do you protect the people using your site? You need to figure out all the skill needed to do this on the regular web before embarking on onion site that need much more security. If you can't make a while website, complete with secure login, secure databases, secure credit card transactions and secure hardened linux, you have no business trying to run an onion site. 
One thing you should definitely learn from bash is regular expressions.
&gt; https://www.cheatography.com/davechild/cheat-sheets/linux-command-line/ Thanks for the link :)
Thanks for the post. I will try it tonight. As for the recent part of the post, I should have said they recently reposted it. But it was new to me, lol. It is a super-handy trick. Yesterday I used ogmdemux, mkvmerge, and ffmpeg in a script to extract the video, and English &amp; Japanese audios out off an ogm encoded anime show. I then recompiled it in Mkv with the English audio enabled, and Japanese disabled. I also had the subtitles added but disabled them, Incase I want to watch subbed instead of dubbed. Ogm didn’t work with my Media server. I was surprised how much smaller the mkv video was. I used libx265 and it seems like nothing was loss in the conversion.
Please provide example input and the relevant code you're actually having problems with. There is literally zero information in your post that will help someone give you an answer. 
 cat `filename` | while read line do echo $line | tr ',' '\n' count=$(echo $line | tr ',' '\n' | wc -l ) mandatory=$(echo $line | wc -l) echo 'COUNT ******' echo "NEW OFFENCE" if [ "$line" != "" ]; then # Do something here echo "mandatory" fi if [ $count -eq 17 ]; then echo "test 1 passes you have the corrected amount of fields" else echo "test 1 has failed you have the incorrect number of fields" fi done example of my code so far and and I don't have any relevant code as I have found nothing on how to do it or even where to start and a output of what my code does AAA123462 7 BB10016 13/02/2019 7 13/02/2019 "information" 
Can you make it look vaguely readable? (hint - look at the 'formatting help` link below *every* comment form on the entire site)
apologies don't really use reddit and no there shouldn't be any special characters as the commas are converted into a newline. cat filename | while read line do echo $line | tr ',' '\n' count=$(echo $line | tr ',' '\n' | wc -l ) mandatory=$(echo $line | wc -l) echo 'COUNT ******' echo "NEW OFFENCE" if [ "$line" != "" ]; then # Do something here echo "mandatory" fi if [ $count -eq 17 ]; then echo "test 1 passes you have the corrected amount of fields" else echo "test 1 has failed you have the incorrect number of fields" fi
apologies don't really use reddit and no there shouldn't be any special characters as the commas are converted into a newline. cat filename | while read line do echo $line | tr ',' '\n' count=$(echo $line | tr ',' '\n' | wc -l ) mandatory=$(echo $line | wc -l) echo 'COUNT ******' echo "NEW OFFENCE" if [ "$line" != "" ]; then # Do something here echo "mandatory" fi If [ $count -eq 17 ]; then echo "test 1 passes you have the corrected amount of fields" else echo "test 1 has failed you have the incorrect number of fields" fi
Probably best update your main post with the code... I was talking about commas inside the quotes... "mandatory field","mandatory field","mandatory, I think" would get 3 fields converted into 4 lines for your count
Using a different account but yes comma inside quotes are banned from the input end before the csv is created, When I jump on my laptop I’ll edit the original post, thanks.
Yeah!! It's the software I'm using to do my PhD in material science :)
&gt;almost as if they created just an executable auto-extracting encrypted archive that just "unzips" itself and runs the normal bash script afterward Well this is sort of purpose of self-extracting shell archives. Back from the old ages where uuencode/uudecode ruled the world and email / news was the thing to distribute data. :-) Shell scripts vs. compiled source code are quite a different takes on how to deal with things and also quite a different solutions to different problems. One line in bash (tar -cvzf etc.tar.gz etc) can translate into hundreds of syscalls you'd need to do in C. You automate some system tasks with shell scripts, that is their purpose. You want something fast or low level - you go for Assembly or C/C++, you want something robust and enterpris-y you go Java, you want something in the middle you go Python. Note: Not to start flame wars, language names are purely fictional for dramatic purposes!
Cool name.
I have only seen the obfuscation ones.
If you’re trying to optimize down to the binary level, bash is absolutely not the right language. 
&gt; POSIX sh does not have any history; [Actually, it does](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/fc.html).
Ah, my bad. It doesn't seem to be available on `dash`, though, which is the most popular sh implementation other than Bash with `POSIXLY_CORRECT` as far as I know. That also seems much less convenient than being able to simply use arrow keys. sh is fine for scripting as a beginner, but I really wouldn't recommend it for interactive use.
What is the problem you're trying to solve? Perhaps we can offer an alternative solution. 
No. Simply because it wouldn't be a bash script then. Even if it were "compiled", it would still need bash to run. Thus it'll only ever be that good. 
No idea if this would work but if you could write in OilShell maybe there is a chance you could then compile that (it is Python I believe). OilShell should handle pure Bash as I understand it. Let me know if possible if you do this, I would love to know. I have not had a chance to play with OilShell and still sort of hate trying to create exe's in Python.
u/inconvenientdoubt, you're right that the state-of-the-script-compilation-art even today seems to be crypto-wrapping scripts into executables that, on execution, pass the script text to the appropriate shell. [shc](https://github.com/neurobin/shc) is the most up-to-date example of this method that I know of. That said, I looked into `sh`-to-C source translation over a decade ago, and while I've since lost my notes, a quick mental review suggests that it's actually quite doable...if you ignore all the interactive geegaws (readline, history, completions, etc.) and ban the two things that require a full shell implementation: `eval` and `source`ing of dynamically-generated files (doing so for static files is of course the shell equivalent of a C `#include`, but it's not always obvious whether a `source` file present at compile time is actually a leftover artifact from a previous script run that should _not_ be included). But it's a **LOT** of work no matter how it's done (hack the `bash` source code to emit C, or write a compiler from scratch) for insufficient gain (a typical shell script is pretty lightweight w.r.t computing resources), so it remained a thought experiment for me. Is there a specific problem you're trying to solve, that "compile bash scripts to C with optimizations" is the correct answer?
I'm not really "trying to solve a program", I just thought that maybe I could compile my most used personal scripts and perhaps gain some tiny little bit of performance. I'm sorry if it sounded otherwise, it was almost just a curiosity, nothing critical.
No, I'm sorry if it had some "asking for help" vibe, it was more of a curiosity, I didn't expect so many answers (thanks all, and I'm sorry!), because I thought I once had read there was some sort of "real compiler" that was abandoned, maybe revived. Then perhaps it would be of some use to optimize some scripts, but no big deal, really.
It's another method. yours is much cleaner! I agree.
Ok. Thank you 👍
Let me know if you need and further tips regarding the points i mentioned
This should be able to do it. [https://www.thegeekstuff.com/2012/05/encrypt-bash-shell-script/](https://www.thegeekstuff.com/2012/05/encrypt-bash-shell-script/) But like others said, if you are looking to produce a binary, then this is not the most efficient way to do it.
'tail' is a shell built in, so I would advise naming your script / function something other than tail, but: You can out this in a function, something like : function tail() { ssh -t "$1" 'tail -f /var/log/apache2/error_log | grep "$2" ; } And add that function to your ~/.bashrc However, this is a very rudimentary way to use this sort of function, with no safeties built in, but it'll function as you described where you can type tail server domain With server filling $1 and domain filling $2 If you want some more safety (like checking if server is within a specific range, or if it can be pinged etc) you'd have to write a .sh script, and alias it in your bashrc instead. 
Eyy, fancy seeing you here
Thank you so much, also tail in the above reference was a placeholder but I will keep that in mind when working on this :)
&gt; 'tail' is a shell built in,... Just wanted to correct this, but: Um actually, it's distributed with GNU coreutils. (Sorry, I know this would technically belong in /r/UmActually, but it caught my eye).
Here's a sneak peek of /r/UmActually using the [top posts](https://np.reddit.com/r/UmActually/top/?sort=top&amp;t=all) of all time! \#1: [Um, actually - The Fifth Element was inspired by comic book, The Incal.](https://np.reddit.com/r/UmActually/comments/9nra72/um_actually_the_fifth_element_was_inspired_by/) \#2: [Dinosaurs don’t go to heaven or hell](https://i.redd.it/byt2fa29fef21.jpg) | [1 comment](https://np.reddit.com/r/UmActually/comments/aokabr/dinosaurs_dont_go_to_heaven_or_hell/) \#3: [Looking for some good um actuallys](https://np.reddit.com/r/UmActually/comments/adp05y/looking_for_some_good_um_actuallys/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
You should name your new command differently, not "tail". Name it for example "srvtail" or "stail" or whatever else you like and that doesn't exist yet. When you start a bash script, the arguments you've typed on the command line for the script will show up inside these variables: $1 $2 $3 (and so on depending on how many arguments you had) Your script could look like this: #!/bin/bash ssh -t "$1" 'tail -f /var/log/apache2/error_log | grep "$2" Because this is just one line, you might instead want to put it into your ~/.bashrc as a function. It would look like this: stail () { ssh -t "$1" 'tail -f /var/log/apache2/error_log | grep "$2" } After this is added to your .bashrc, the new "stail" command will show up in newly opened terminal windows. You can also reload the .bashrc file in existing terminal windows with this command: source ~/.bashrc
Ah I worddd it like I did because (I think, can't check from phone right now) if you enter type tail Bash will return 'tail is a shell built-in'. :) 
 $ type tail tail is /usr/bin/tail Are you sure you're running bash?
Not exactly what you want, but you can easily remove everything up to the first underscore with: "${var#*_}" You can read more about Bash [variable substitution](https://www.tldp.org/LDP/abs/html/parameter-substitution.html). Be careful, if you give it "name_of_song.mp3" it will change that to "of_song.mp3". 
There's a great perl utility called `rename` that's available on Linux that's perfect for this. I don't know how to get it for Mac without using `brew`, though. rename 's/^[0-9]*_//' *.mp3 If all of your files have a prefix ending in `_`, you can simplify it a bit and do the following, which will remove up to and including the first `_` in the names of all the files in the current directory ending with `.mp3`. But as moocat said, if you have a file like `name_of_song.mp3`, it will rename it to `of_song.mp3`. The `-v` flag will cause it to print the files as they get renamed, and the `-n` flag won't overwrite any existing files in case you have two files that only differ by prefix. for f in *.mp3; do mv -nv "$f" "${f#*_}"; done
You can enable extglob in bash to match the files that start with one or more digits, followed by _, similar to what you can do with regular expressions. shopt -s extglob for file in +([0-9])_*; do mv -i -- "$file" "${file#*_}" done See http://mywiki.wooledge.org/glob#extglob and http://mywiki.wooledge.org/BashFAQ/100 for more
Your logic would not remove 'name_' because your regex doesn't match anything but a string of numbers followed by an underscore. It's fine. 
The first command won't, no, but the second command will.
 cd music for f in *.mp3; do mv "$f" "${f#*_}"; done Should do what you want. Just be sure to always have a good backup before doing any mass renaming.
dm me if you want the bundle for free.
Nautilus passes the selected files on as arguments to the script (`"$@"`). It also, for no good reason, mash them into an environment variable with no way to safely separate them later. Use `"$@"`, since then you can handle any filenames you throw at it, even if they contain newlines. Also, it runs the script in the directory you have open in nautilus, and passes only the filenames as arguments, so you don't need to parse out the basename and dirname from absolute paths. #!/usr/bin/env bash # give feedback with notify-send if available if type notify-send &gt;/dev/null 2&gt;&amp;1; then notify() { notify-send -a "${0##*/}" "$@"; } else notify() { printf &gt;&amp;2 '%s\n' "$*"; } fi for file in "$@"; do ext= if [[ $file = *.* ]]; then ext=.${file##*.} fi mv "$file" ~/"Videos/${PWD##*/}$ext" &amp;&amp; notify "«$file» moved to «$_»" done
This. No point reinventing the wheel.
This. No point reinventing the wheel.
Meh, the bash cookbook is really not that great. The other books might be though.
Very impressive. I did see some comments about the $@ on other sites but didn't understand how it worked. It works beautify. I especially like the notify feature. I do have notify-send installed and it will be helpful with other scripts.
Believe it or not the bash book is one of the best in the bundle. A lot of the others are super out of date. The CSS one is from nearly 10 years ago 
I've previously asked a similar question and the best solution anyone could come up with for "how to practice Bash" was doing [http://overthewire.org/wargames](war games from OverTheWire).
Ah, I stand corrected. Regardless, as it's a program in its own right, it'd be unwise to name your function 'tail' 
Have you tried it?
I like hanging out in the "Bash" tag on Stack Overflow to either try and answer things or see how others answer them. There are a few very knowledgeable people on there. But if you look for a place to practice what the BashGuide preaches, SO is pretty good (because many people don't follow the practices).
Very true! Just wanted to clarify that `tail` isn't a built-in heh xD
I was toying with this some more while it seems to be sshing into the server and running the command it does not seem to be accepting the input for the grep &amp;#x200B; tail server domain The server is only actioning this part ssh -t "$1" 'tail -f /var/log/apache2/error_log' It does not seem to be filling in the $2 variable 
Here is a stupid/fun/practice project: 1. Write a small script to download all the plain text files at gutenberg-books that: 2. identifies what types of files you have downloaded 3. delete files that are not plain text 4. rename all of the files to the title of the book 5. create folders for the different languages 6. move files to folders based on language 7. little bash script to search the titles or authors of the books I wanted a shit ton of text files to have as a sort of data dump to play around with for all sorts of bits so I did the bit up above as a learning exercise, worked a treat, downloaded 38,000 files, 230,000,000,000 words or so, so many files that it was impossible to do anything manually with them so you are forced to automate it all. &amp;#x200B; write a mini bash script wiki, not a wiki about bash, but a menu that you can run that will allow you to search the documentation you write about learning all this stuff, give it a neat little header that displays how many file you have, how may words there are in it, the last document to be edited, give it a search function via grep [Check out my crappy bash wiki that I love](https://pastebin.com/nUYhLWmU) I love that thing, it is so fast and easy, i will tidy it up with a few if do statements and a bit more feedback for things like when files don't exist, add the last edited document to the header, but yeah, simple as heck, easy to make, functional, useful. open it up from anywhere and return to where you were with a stupid bash alias like this: alias wiki="cd \~/wiki &amp;&amp; ./wiki.sh &amp;&amp; cd -" &amp;#x200B; All of a sudden you will realise that you can nest more menus inside that menu, or that the menu can open up other menus and you can have a massive bash menu to accomplish mundane tasks &amp;#x200B; You are going to have to create a non root user a ton of times if you keep playing around with linux so make a little script that creates a users, check to see if th euser exists, if not create it, change that users home folder to be not accessed by other users ala sudo chmod a+x 0700 /home/username , make it so that their password expires every 30 days, they cant use the previous 10 passwords, give them sudo privileges or not. You don't want to do this manually every time so you script this, you could add it to your bash menu, I use ubuntu so the commands I would need to script are: `sudo adduser username # add the user` `sudo chmod a+x 0700 /home/username # change the permissions of their home folder` `sudo chage -l username # To view user status` `sudo usermod -aG admin username # add user to sudo user group` `sudo chage -M 30 username # to make the password expire after 30 days` &amp;#x200B; grep the /etc/passwd to check if the username exists like: `read -p "Username" username` `if [ "grep "$username" /etc/passwd" = $username]` `then` `sudo adduser $username` `else` `echo "Username already exists you mong"` `fi` that is a very basic example, but see how you can chain it all together to make a user profile for any new user that is the same every time: `read -p "Username: " username` `if [ "grep "$username" /etc/passwd" = $username]` `then` `sudo adduser $username` `sudo chmod a+x 0700 /home/$username` `read -p "Add to admin? : " answer` `if [ "$answer" = y]` `then` `sudo usermod -aG admin $username &amp;&amp; echo "User added to sudo group"` `else` `echo "User not added to sudo group"` `fi` `else` `echo "Username already exists silly billy"` `fi` Take out if else statement about the sudo and change the function so you create a user with or without sudo privileges every time, mostly it is about chaining together commands that you shouldn't be writing out everytime. you could add that to a menu by making it a function: `add_user(){` `read -p "Username" username` `if [ "grep "$username" /etc/passwd" = $username]` `then` `sudo adduser $username` `else` `echo "Username already exists you crazy cat."` `fi` `}` The resource you need to do all that stuff is the internet and you just get googling, fire up a free tier aws virtual machine and get practicing! I fucking love this shit and immensely enjoy it so I am very happy just googling the crap out of anything and working through things until I have a solution, once it is solved I have a bite sized chunk of script that is reusable in any script I make and makes repetitive tasks basically automatic
Where does bash come in?
[Direct link to the Humble Bundle site](https://www.humblebundle.com/books/programming-cookbooks) so you don't have to go through twitter and then through a URL shortener to use a referral link.
Three backticks. 
u/SonicSkunk, you want the r/PHP subreddit, two floors down and through the fire doors. :)
&gt; Edit: How on earth do I make that code a single block? On Reddit you indent it by four spaces first. # This is indented by four spaces
I'm not sure how that function even worked as he didn't close the single quote. Try this: tail() { ssh -t "$1" "tail -f /var/log/apache2/error_log | grep $2" }
I believe it was because I didn't include that same one on my original post 
HaHaHa, nice response ;-)
?
You are asking a PHP related question in a subreddit about bash.
Looks like homework. Ask a specific question and we'll help, but we're not going to do it for you.
In case you're wondering why you're getting downvoted. Those examples in the last three pastebins there are seriously broken, to the point that they'll rather throw error messages at you than do anything useful. Your "crappy bash wiki" has some bugs and bad practices too, but at least those bugs are less likely to be triggered.
Don't need you to. I figured it out. Stay on that high horse bud. Only a matter of time before you get knocked down.
readline has to be told which characters advance the cursor, and which don't, otherwise it will think that for example \e[91m makes the prompt 5 characters longer than it is. Since bash 4.4, you can use the new `${param@P}` parameter expansion to get the same parsing as `PS1` undergoes, which converts `\[` and `\]` to the markers used by readline to determine non-printable sequences. # bash 4.4 red=$(tput setaf 1) bold=$(tput bold) reset=$(tput sgr0) read_prompt="\[$bold$red\]B\[$reset\]&gt; " read -ep "${read_prompt@P}" foo For older bash versions you have to know that it translate the `\[` and `\]` to bytes 0x01 and 0x02, so you can do s=$'\x01' e=$'\x02' read_prompt="$s$bold$red${e}B$s$reset$e&gt; " read -ep "$read_prompt" foo
Does that work better at evaluating whether a user exists or not? in terms of good practice or is there a best practice solution? ''' read -p "username: " getent passwd $username &gt; /dev/null if \[ $? -eq 0 \] then echo "Test Passed" else echo "Test Failed" fi &amp;#x200B; ''' &amp;#x200B; &amp;#x200B;
sure, apart for the unnecessary `[` command read -rp 'username: ' username if getent passwd "$username" &gt;/dev/null; then ... fi
Thanks ! I'll test that and let you know :-)
Thank you, I can see how that works, I will go do some googling 
 Either if [ ! -d "$DIRECTORY" ]; then echo "your message" mkdir "$DIRECTORY" fi Or just create it and ignore the error if it doesn't exist. This will also create any parent directories as well. e.g., '/foo/bar/baz' mkdir -p "$DIRECTORY" 
Thanks
I think this'll do it (not at my computer so just an educated guess) test -d /tmp &amp;&amp; echo "already exists" || echo "doesn't exist"
That. Don’t treat preexisting dir as an error state unless you require specific owners/permissions, the permissions differ, and are unable to set them.
I feel like awk is a better bet than sed. AFAIK, there's no way to access the current line number in sed, but with awk, you can look at NR.
u/Jackson-Lee, assuming you're using GNU sed, you can use its `&lt;first&gt;~&lt;step&gt;` address extension to do it all in a single invocation: # insertFunction &lt;str&gt; &lt;n&gt; &lt;file&gt; insertFunction() { sed -i "${2}~${2}a\ $1" "$3" } # Testing it... $ seq 20 &gt; /tmp/t.txt $ insertFunction "Additional line" 3 /tmp/t.txt $ cat /tmp/t.txt 1 2 3 Additional line 4 5 6 Additional line 7 8 9 Additional line 10 11 12 Additional line 13 14 15 Additional line 16 17 18 Additional line 19 20
Why the newline? `"${2}~${2}a$1"`
Good stuff. Why the newline for a single command? sed "${2}~${2}a$1"
Awk solution! Because Awk! awk '!(NR%3){print "INSERT"};1' &lt; file one two INSERT three four five INSERT six awk '1;!(NR%3){print "APPEND"}' &lt; file one two three APPEND four five six APPEND
Thanks for the chuckle
&gt;Why the newline for a single command? Force of habit from decades-old non-GNU `sed` days. :)
Dear god I can't imagine the horror
@geirha so I have a 4.3.48(1)-release version so i used the second choice, thanks a lot !
Very cool. Needs a volume warning! There's smoke coming off my DAC hahaha &amp;#x200B;
Awesome dude !
Thank you, I will add this to the readme! 
u/hannenz, it is indeed GNU echo, almost certainly the `echo` binary in the coreutils package from [Homebrew](https://brew.sh/): $ brew info coreutils coreutils: stable 8.30 (bottled), HEAD GNU File, Shell, and Text utilities https://www.gnu.org/software/coreutils Conflicts with: aardvark_shell_utils (because both install `realpath` binaries) b2sum (because both install `b2sum` binaries) ganglia (because both install `gstat` binaries) gegl (because both install `gcut` binaries) idutils (because both install `gid` and `gid.1`) md5sha1sum (because both install `md5sum` and `sha1sum` binaries) truncate (because both install `truncate` binaries) /usr/local/Cellar/coreutils/8.30_2 (473 files, 8.9MB) * Poured from bottle on 2019-02-03 at 18:44:51 From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/coreutils.rb ==&gt; Options --HEAD Install HEAD version ==&gt; Caveats Commands also provided by macOS have been installed with the prefix "g". If you need to use these commands with their normal names, you can add a "gnubin" directory to your PATH from your bashrc like: PATH="/usr/local/opt/coreutils/libexec/gnubin:$PATH" The *Caveats* section explains it all.
Yes, this really explains it all. Thank you :-)
I need more. I hope you decide to make more, because that was awesome. ^I'd ^honestly ^give ^you ^gold, ^but ^I ^have ^no ^money.
Ever since I learned that utilities like Sox and Mpv can play waveforms of a given frequency, I've always thought about basically sequencing my own music in Bash. Then I think to myself, "my time would probably be better spent writing a proper sequencer," and I end up never doing it. But hey, bash has everything you need to write a programmable sequencer, so why not? It turned out to be pretty cool!
Aww, thank you! I will do some more pieces once I figure out how to do some things (like mixing of multiple audio channels) in bash without significant slowdown.
Can't wait to see what you come up with then.
Actually this code also generates the waveform, which is surprisingly hard in a language that doesn't support floating point numbers! First versions of this sounded out of tune because of rounding issues. I improved it as much as reasonably possible, but it's still can be up to 0.5 Hz out of tune on specific notes. (Which is, actually, not even a rounding issue but the fact that I use constant number of samples to represent one cycle of a sound wave) And there is issue with each note having an integer number of wave cycles, which means that whole note isn't exactly the same as 4 quarters. Another huge challenge (one I still haven't solved) is multi-voice sequencing. Doing mixing in bash is really, really slow so I need to come up with some hack or just use an external tool. 
Looking at your source code now, I see! Yeah it is kind of funny that you went to the trouble of making the waveform in Bash but wrote a C program to mix it. Another strategy you could use: output MIDI to Timidity. If you're going to go the route of using native code for mixing, I recommend you check out [Sox]( http://sox.sourceforge.net/sox.html ) (you can probably install it using your package manager), it has command line options for both generating wave forms and for mixing. If I were going to write a sequencer in Bash, it's the method I'd use. To mix waves together, you need to sum them together into an array of integers before dumping it to the WAV stream. So declare an array and fill it with samples (although this might be a bit slow in Bash): declare -a BUFFER=(); sum_to_buffer() { let FREQ="${1}"; shift; let PW="${SAMPLE_RATE} / ${FREQ}"; let HALF_PW="${PW} / 2"; # This generates a sawtooth wave, rather than a square wave. for (( i = 0; i &lt; $HALF_PW; i++ )); do let BUFFER[$i]="((${i} * 255) / ${PW}) + BUFFER[$i]"; done; for (( i = $HALF_PW; i &lt; $PW; i++ )); do let BUFFER[$i]="((${i} * 255) / ${PW}) + BUFFER[$i]"; done; } buffer_as_Cstring() { # first format a string containing the content of the buffer as a C-string: for i in "${BUFFER[@]}"; do printf '\\x%X' "${i}"; done; } dump_buffer() { printf "$(buffer_as_Cstring)"; } 
I'll definitely check out sox, thank you. And yeah, I know about timidity but honestly I enjoy working with PCM a lot more :) And I tried various methods of mixing sound in bash, but they are it's way too slow for real-time audio work. Or maybe I did something wrong.
When writing the header, why do you split some fields? Things like what I think are Subchunk1Size are split between print statements. &gt; printf "RIFF\\0\\0\\0\\0WAVEfmt **\\x10**" &gt; printf "**\\0\\0\\0**\\x01\\0\\x01\\0\\x44\\xac" I've been trying to piece the header together into variables so I can understand, but it doesn't work, so I'm getting something wrong somewhere. (Going by format specification [here](http://soundfile.sapp.org/doc/WaveFormat/)) function printHeader() { #RIFF chunk descriptor local ChunkID="RIFF" #4 Bytes local ChunkSize="\0\0\0\0" #4 Bytes local Format="WAVE" #4 Bytes #fmt subchunk local Subchunk1ID="fmt\x20" #4 Bytes local Subchunk1Size="\x10\0\0\0" #4 Bytes local AudioFormat="\x01\0" #2 Bytes local NumChannels="\x01\0" #2 Bytes local SampleRate="\x44\xac\0\0" #4 Bytes local ByteRate="\x44\xac\0\0" #4 Bytes local BlockAlign="\x01\0" #2 Bytes local BitsPerSample="\x08\0" #2 Bytes local ExtraParamSize="" #2 Bytes (optional) local ExtraParams="" #X Bytes (optional, defubed by ExtraParamSize) #data subchunk local Subchunk2ID="data" #4 Bytes local Subchunk2Size="\xff\xff\xff" #4 Bytes chunkDescriptor="${ChunkID}$ChunkID}${ChunkID}" fmtSubchunk="${Subchunk1ID}$Subchunk1Size}${AudioFormat}${NumChannels}${SampleRate}${BlockAlign}${BitsPerSample}" dataSubchunk="${Subchunk2ID}${Subchunk2Size}" printf "${chunkDescriptor}${fmtSubchunk}${Subchunk2Size}" }
I would think something like this: #!/usr/bin/env bash sxiv /path/to/image &amp; convert -flags /path/to/image fg %1 would work. The ampersand at the end of the sxiv line means to run that command in the background and immediately continue with your script so the convert command runs. Then we follow up with the fg command, which puts the first job in your shell back into the foreground again and doesn't return until sxiv quits. If you have other jobs running this could be weird though. This should really be done smarter but is fine unless things get complicated.
There's also a built-in command "echo" in bash itself. That's what gets run when you use the echo command in a script or at the prompt. You can read its help text with `help echo`. That /bin/echo program is normally not used by shell scripts. For the GNU version of the program on a Linux machine, it prints help text if you do `/bin/echo --help`. I have no idea if there's anything actually using the echo program.
Instead of echo, you might want to use `yes`. (Read `man yes`). 
Instead of echo "y" I put yes 'y' and I still got stuck at the prompt
It appears that parsect (the application I'm trying to auto fill) is running and the rest of my script isn't going until after I fill out it's prompt?
The rest of your script will run after that parsecd program has shut down. Your script right now seems to be totally broken. There's a tool named "expect" to deal with terminal programs that print prompts and want the user to type something. Maybe see if you can find a guide for how to write an 'expect' script, that kind of guide hopefully will explain how to deal with a problem like yours.
Thanks! I'll give it a shot
Super clever, nice work.
"no you want GEICO not gecho!!"
Here to suggest expect.
Lots of performance improvements you could make. Some quick ones are replacing the external `printf` call with the builtin, as you call it every note. That by itself might do it.
So, you want your vps to run the script, when you start it up? Then put those functions in the `~/.bashrc` file. The echo statement does not work because it only prints out the string. It is not clear from your description, what you want to achieve. 
I want to execute that whole command, I'm just not familiar with the syntax with bash. In node I can just do `ssh.execCommand(wget -O spi https://pastebin.com/raw/xXc5q8Ar &amp;&amp; sed -i 's/\\r$//' spi &amp;&amp; sed -i 's/usrn=guru/usrn=proxyUsername/g' spi &amp;&amp; sed -i 's/usrp=guru/usrp=proxyPassword/g' spi &amp;&amp; bash spi -ubuntu)` to execute the command in my ubuntu vps
&gt; vps to run the script, when you start it up? &gt; Then put those functions in the ~/.bashrc How is this supposed to work?
TIME . &lt;(wget -qO- https://pastebin.com/raw/xXc5q8Ar | sed -n '/^ubt/,/^}/{ s|\r||g; /usrn=/{s|guru|proxyUN|}; /usrp=/{s|guru|proxyPW|}; p }'); ubt
&gt; Some quick ones are replacing the external printf call with the builtin, as you call it every note. That by itself might do it. In bash, printf is a builtin. And even if it wasn't it's called once every 11025 samples, that's not enough to cause any noticeable slowdown. &gt; You’re testing one of your for loops every iteration with quite a bit of code just to run a fixed number of times. Calculate this once then compare against it in your loop. Yeah, that's a good catch. I'll try that if I'll need additional speed improvements.
Hello and Thank you!, this was what I was asking for when asking the question. But I have tried this script and it doesn't seem run. I want to give a little background/story. I'm trying to run this script on start up for AWS EC2 ubuntu14.04lts. I can create an instance using the aws sdk with Node.js to create the instance, and upon creating the instance I want run this script (which I can pass in when I create the instance). I know this command 100% works because I have used it in another project to create proxies from another VPS provider. (For other VPS providers ssh-ed into the VPS and then ran the command instead of using the command as a start up script. I can't do it for AWS because their Linux ec2 servers aren't root:pass by default, its with a key instead) Sorry but is there a way to get this script running with ubuntu? 
also don't know why they're downvoting you
Would you please share the output of `type ubt` after running my command?
I think you can use `%%` to refer to the "last job", which may ease that complexity.
It's GNU/Linux, baby; the school kids are the posers in this clique. (( current != previous )) &amp;&amp; echo $(( current - previous )) This checks if the variables "current" and "previous" are inequal. If so, it outputs the difference. Note how the second [Arithmetic Expression](https://www.gnu.org/software/bash/manual/html_node/Shell-Arithmetic.html) has a `$` in front of it. This causes the expression to return its value instead of acting like a command. 
Thank you! Is the approach of writing to four files (current and previous for each comment and link) in /tmp okay?
u/ups-question, "neither works" is a spectacularly unhelpful diagnosis. What error messages did you get?
What's "okay" depends on your requirements and style. It does sound unnecessary. Please post your existing code! 
Huh, if you don't need OpenFiles array afterwords, I'd rather not manipulate (delete items) array in a for loop, so something along the lines of \`\`\` for i in OpenFiles\[@\]; do if \[ -n "$i" \]; then subl $i; sleep 0.1 else log "warning, skipping empty openfiles item" fi done \`\`\` Anyways could be that you pass some nonsense in your second for loop, can you get more info on what's happening via some \`echo "subl with \[$i\]"\` placed before $subl call? Anyways what's opening the files? Is it that $subl thingy? Also \`$SYSDBtac\` may be potining to a file, however \`"${SYSDBtag}":10000 \` would become something like \`/some/path:10000\` right?
I found the Issue now. It was a time ( XXX ) That spawned a subshell and for that reason obv. the Variable wasn't working anymore. I removed time ( ) and now it works again :)
Here we go #!/usr/bin/env bash # get current reddit karma # TODO: compare to previous values and show up or down arrow and difference # prankousky 2019-02-06 # depends on jshon # http://kmkeen.com/jshon/ # provide username username="prankousky" url="https://www.reddit.com/user/$username/about/.json" # individual user agent following reddit's API terms useragent="User-Agent:karma-in-i3-statusbar.sh (created by /u/prankousky, used by /u/$username)" # download json curl -H "$useragent" $url &gt; /tmp/reddit$username.json # extract each karma value to dedicated file cat /tmp/reddit$username.json | jshon | grep comment_karma | grep -Eo "([0-9]{1,})" &gt; /tmp/comment$username cat /tmp/reddit$username.json | jshon | grep link_karma | grep -Eo "([0-9]{1,})" &gt; /tmp/link$username # notification notify-send "reddit karma updated ☺" and #!/usr/bin/env bash # display reddit comment karma and link karma in (i3) statusbar; # prankousky 2019-02-06 # this block will provide a description on right-click in statusbar case $BLOCK_BUTTON in 3) pgrep -x dunst &gt;/dev/null &amp;&amp; notify-send "&lt;b&gt;reddit karma 🔗: link karma ✍: comment karma";; esac echo "✍ $(cat /tmp/commentprankousky) 🔗 $(cat /tmp/linkprankousky)" the latter script gets called directly in i3bar and displays the last line of the script directly in the bar. thoughts so far: * now that you've mentioned that writing four files sounds unnecessary, i already thought that i'd **only** have to write the "previous" value * the "current" value can be used *if* i combine both scripts rather than having one to get the values and another to display them i will try combining both scripts; if there is a way to do this without writing any files to the disk, i'd be very interested, just can't think of a way. 
It is a built in, but the many distros (Ubuntu, Debian, to name a few) have an external printf which supersedes it. You could use ‘builtin printf’, it’s only a pinch slower than `echo` as it checks if there’s a `%`. And you’re right, every note isn’t really that often given how often we’re doing samples. 
Thank you, this worked!
&gt; It is a built in, but the many distros (Ubuntu, Debian, to name a few) have an external printf which supersedes Well, I guess it's a problem with these distros, not my script. At least, it's not me who's breaking the standard. In the distro I use the preference goes to builtin and that's the correct behavior.
What distro do you use? 
Void Linux
Neat project bro. I've heard tell of a json query tool called `jq` that might be even more efficient at this, but have a gander: read karma_link karma_comment &lt;&lt;&lt; $( wget -qO- https://www.reddit.com/user/$username/about/.json \ | awk -v FS=: -v RS=, '/(link|comment)_karma/{printf("%s",$2)}' ) The awk says "Colon-delimited fields, comma separated records. Print the second field of records that match link_karma or comment_karma." Now you can just operate on the `karma_` variables
Awesome! Could my completed script.sh look something like this? I accounted /u/HenryDavidCursory 's comment. #!/bin/bash insertFunction() { sed "${2}~${2}a$1" $1" "$3" }
&gt; How would I assemble the final code? Not being funny, but aren't you asking us how *we'd* assemble the final code? The only input you appear to have put into this so far is to paraphrase the question / assignment given to you. Can you show ***any*** code for this *whatsoever*?
What you need to watch out for is sanitizing your replacement string to avoid returning capture groups `\1`, `\2`, etc, `&amp;`, or an extra`/`. sed -e "s/$2/$(sed 's:[\\&amp;/]:\\&amp;:g' &lt;&lt;&lt; "$3")" "$1"
I apologize and noted your point for future. I felt that I had enough detail and just needed final guidance regarding syntax positioning.
&gt; I felt that I had enough detail Now, forgive me from saying, but &gt; sed -e 's/[a-zA-Z]/X/g' -e 's/[0-9]/N/g' does not contain much detail at all - and the syntax positioning might involve more than replacing any single random letter with a fixed one repeatedly, followed by replacing every number with a different character, one at a time. Might I suggest - as 'final guidance', that you take some time reading your lecture notes, some of the recommended reading material that would have been listed, and possibly some of the many `bash` and `sed` guides that are floating around on the interwebs, since it is fairly obvious when someone has put pretty much zero effort in. 
`sed ‘s/$2/$3/g’ $1`
The function definition is OK, though because you dropped the `-i` flag to `sed`, it sends the modified output to stdout, instead of modifying the original file. Also, if this is meant to be a standalone script, it doesn't do anything useful because you didn't call `insertFunction`. For a standalone script, you can dispense with the function definition and simply: #!/bin/bash # insert_n &lt;str&gt; &lt;n&gt; &lt;file&gt; # At minimum, check if &lt;file&gt; is readable if [[ ! -r "$3" ]]; then echo "FATAL ERROR: '$3' is not readable, aborting." exit 1 fi sed "${2}~${2}a$1" "$3"
&gt;`sed ‘s/$2/$3/g’ $1` Two big problems with that: 1. `$2` and `$3` don't get substituted because you used single quotes. 2. A bare `$1` fails if you give it a filename with whitespace in it. Better: sed "s/$2/$3/g' "$1"
Quotes are still wrong.
You rock! Tested successfully on Ubuntu $BASH_VERSION 4.4.19. There was an initial run issue I had to fix "/bin/bash^M: bad interpreter: No such file or directory" Solution: vi filename :set ff=unix To finish the script off, I need to open the file and write the insert changes to the file. I tried adding what I've commented out plus other variations with no luck. #!/bin/bash #exec fd&lt;&gt;$3 insertFunction() { sed "${2}~${2}a$1" "$3" } insertFunction $1 $2 $3 #echo insertFunction $1 $2 $3 &gt; $3
Yeah, my headphones now have a faint high-pitched rattle.
Thank you. Unfortunately, I was not able to use this code. (perhaps due to me working on a macbook atm while usually doing this on an arch linux machine? Though bash *ought to* work the same on OSX) When I run it, I get multiple lines of output (starting with `HTTP(1.1 200 OK`), so possibly the -qO- option does not work properly on OSX..? When trying to echo $link_karma or $comment_karma, I get an empty line. 
You're passing `line` to cut as a filename, try this: while read line; do id="$(echo "$line" |cut -f 1 -d @)" ... done &lt; file
Besides using echo "$line" | cut -d@ -f1 There is also the following method (but it is only good for a bash script like yours, not for a #!/bin/sh script): cut -d@ -f1 &lt;&lt;&lt; "$line" Something else, check out a great tool named shellcheck. It's super helpful because bash is weird and there's a thousand mistakes you can make. Your distro will have a package for it. You can also try it online without having to install it at www.shellcheck.net.
&gt; id=${line%%@*} Takes what's at left of @. I would guess `=${*@%%line}` takes what's it's at the right then. One more thing: This second variable would take the name as a whole (single string) or word by word as an array. For example, 
u/errrzarrr, since your data is already in `file`, you can *greatly* streamline your script as follows: #!/bin/bash cut -d@ -f1 file | while read id; do ... done That runs `cut` on the entire file at one go, leaving you with the simple task of reading in the IDs and doing whatever else you need with them.
No, what you are guessing about how to get the other side is not working. The other side you get like this, with a different operator: id=${line#*@} The way those `#` and `%` features work is, they are trying to match and delete something from either the front (`#`)` or the back (`%`) of the text. And when you write `%%` instead of `%`, the `*` wildcard tries to find the longest possible match. By default it stops at the shortest match. This is not quite as important here, but it would matter if there were several `@` text characters in your value. I mean if for example the variable would have been this: line='003@aaa bbb@ddd' Then doing `${line%@*}` would have matched and deleted just the `ddd` at the end, and the result would have been `003@aaa bbb`. When you do `${line%%@*}`, the `*` expands to the longest possible match and you then get a result `003`. About your array and simple text variable question, this feature can be used for both. What matters is what you feed into it. When you write `${line}`, that is just a single piece of text. If you write `${line}` but it is really an array, you are selecting just the value of the first element of it. If you want to access all elements in your array, you will have to write `"${line[@]}"` instead. And then you can still add those `#` and `%` features to this. Here's an experiment at the command line about how that looks like: $ x=('001@aaa' '002@bbb' '003@ccc') $ echo "${x[@]}" 001@aaa 002@bbb 003@ccc $ echo "${x[@]%%@*}" 001 002 003 $ echo "${x}" 001@aaa $ echo "${x%%@*}" 001 In the last two command lines of this experiment you can see what happens if you access the array with just its name and without the `[@]` thingy added to it. It will work on just the first element of it. The neat thing about bash is, you can experiment with everything directly at the normal bash prompt. You don't have to edit and run a script all the time to test an idea. You can use pretty much everything that works in a script at the normal prompt, it's just a bit weird to write everything on a single line for things like a `while` loop (the trick there is a command always has to go directly after the `do` word, you are not allowed to put a `;` after the `do` word).
What version of wget? `wget --version`
Your main problem is quoting: the contents of `$line` are `003@ccc ddd eee`. Your command cut -d@ -f1 $line gets expanded to cut -d@ -f1 003@ccc ddd eee *and then split by whitespace*, so `cut` sees three arguments instead of one. The quick fix is to just use cut -d@ -f1 "$line" instead. However, you don't need `cut` in the first place; you could set the field separator to `@` and read the parts directly into variables: while IFS=@ read -r id name; do mv "$id" "$name".zip # Or whatever your exact command is done &lt; file
I think you can use something like this: date "2018-02-14T21:39:18Z" -t yesterday can't test it rn maybe tomorrow if I remember (ping me if you want)
u/Lutarisco, just let `date` do all the hard work: #!/bin/bash from=( 1549288800 1549332000 1549893600 1549936800 ) to=() for t_from in "${from[@]}"; do d_from="$(date -d "@$t_from")" # Get the date for this timestamp ymd_from="$(date -d "@$t_from" +%Y-%m-%d)" # Then offset it t_to="$(date -d "21:00 $ymd_from - 1 day" +%s)" d_to="$(date -d "@$t_to")" echo "$d_from (@$t_from) -&gt; $d_to (@$t_to)" to+=($t_to) done I broke up the `date` invocations above for clarity; my original solution was just: for t in "${from[@]}"; do to+=($(date -d "21:00 $(date -d "@$t" +%Y-%m-%d) - 1 day" +%s) done but that's just incomprehensible. :)
Everyone in this thread missed that you're trying to assign a variable in a subshell. `|` runs the command to its right in a child shell that cannot influence its parent environment, so your entire while loop is basically a ghost. Use redirection instead of subshells when you're assigning variables: while read line; do id=$(cut -d@ -f1 &lt;&lt;&lt; "$line") #here-string #OR id=${line%%@} done &lt; file `&lt;` uses a file as standard input `&lt;&lt;&lt;` is called a "herestring"
He's trying to assign a variable; the subshell is problematic
u/Lutarisco, I misread your requirements, so my previous implementation was incorrect. This one takes exactly two calls to `date` and a tiny bit of math, and is now in a function that allows you to specify the desired reference time: # last_time [-t &lt;time_ref&gt;] &lt;epoch&gt; ... # Prints the most recent epoch time of &lt;time_ref&gt; (default: 21:00) before &lt;epoch&gt; last_time() { local time_ref while getopts ":t:" opt; do case "$opt" in t) time_ref="$OPTARG";; \?) echo "Invalid option: -$OPTARG" &gt;&amp;2; exit 1;; :) echo "Option -$OPTARG requires an argument" &gt;&amp;2; exit 1;; esac done shift $(( OPTIND - 1 )) local t_from for t_from in "$@"; do # First try 21:00 on the day of &lt;epoch&gt; local t_to=$(date -d "$(date -d "@$t_from" +%Y-%m-%d) ${time_ref:-21:00}" +%s) # If it's past &lt;epoch&gt;, step back 1 day (( t_to &lt; t_from )) || (( t_to -= 86400 )) echo $t_to done }
/u/errrzarrr, please notice how this loop uses a `&lt; file` redirect instead of a `cat |` pipe. Pipes run in subshells, meaning they can't assign variables to the "parent" shell (the one you're using). 
Thanks for noticing...and for giving me an idea for a post. :)
Do it!
I follow the (general_specific) variable naming scheme, so the above code sets `$karma_link` and `$karma_comment`. 
Thanks for contributing to this subreddit! Please don't take any criticism personally; we're here to improve as a group. Let me know if you have any questions. --- Line 5: It's good practice to use `printf '%s\n'` instead of `echo -e`. I usually define a function for logging, such as: log(){ printf '%s\n' "$@" &gt;&gt; /home/kernel/compile_kernel_test.log; } Line 7: Typo? `compile_kernel_test.logi` Line 23/40: `((count++))` will both initialize and increment the "count" variable. --- It looks like you need to wrap your "if make" statements in a date comparison loop. You can get the current "epoch time" (seconds since 1970) with `date %s` on modern GNU systems. You can get a specified date with the `-d` or `--date` flag. You can compare integers using the `((` construct, which is called "Arithmetic Expression".
 I stumped as to what use such a seemingly arbitrary transformation would have as suggested by OP. Can anyone enlighten me?
Minor note think you missed the plus! `date %s` should be `date +%s`
so yeah I read too fast and I'm out of context. My command is : date -d "2018-02-14T21:39:18 last hour" you can also use `last day last day` to get two days back
u/learningphase, I'll just address your key requirement: &gt;it should run for a specified time (8hrs/12hrs/24hrs) &gt; &gt;upon stoping the test manually or after time completion it should give me how many times it compiled the kernel. There are at least three key `bash` skills you'll need to learn for this: 1. how to do an infinite loop 2. how to launch a background shell (to effect the 8/12/24hr timeout) 3. how to use the `trap` builtin command, so you can print the number of compile rounds on exit Here's a simple script that illustrates all three: #!/usr/bin/env bash # Launch a subshell to kill the main shell after 30 seconds ( sleep 30 ; kill $$ ) &amp; # Set an EXIT trap handler count=0 trap 'echo "RESULT: Completed $count loops"' EXIT # 'Round and 'round, 'round it goes # Where it stops, nobody knows... while true; do # Pick a number from 1 to 10 (( s = RANDOM % 10 + 1 )) echo "Sleeping $s seconds" sleep $s (( count++ )) done Feel free to let it run the full 30 seconds, or interrupt it at any time. ## Further Reading: * [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html) * [How "Exit Traps" Can Make Your Bash Scripts Way More Robust And Reliable](http://redsymbol.net/articles/bash-exit-traps/)
make a function in .bashrc or .bash_profile function myFunc(){ npm install whatever }
This might be a lot simpler than you were probably expecting. But first, let's look at exactly how commands are executed on the commandine and what `npm` actually is. I just want to make sure you have this foundation because it seems like there's a gap in your knowledge based on your question. Are you familiar with the `which` command? It will tell you where the command you ran lives on your filesystem. It leverages the `PATH` environment variable (see: `printenv PATH` to see the value of it). `PATH` is a list of filesystem paths, separated by a `:`, where your shell will look, in order, to find the "bare" command that you just typed. For example, if your `PATH` variable contains `/bin:/usr/bin:/usr/local/bin` and you run `npm install`, your shell will take the first word (`npm`) and search for `/bin/npm`, then `/usr/bin/npm`, then `/usr/local/bin/npm` and whichever it finds first (that is marked executable (`chmod +x &lt;path&gt;`), it will execute and pass the arguments to it. The `which` command will output where this command lives. Try it: `$ which npm` and see what it outputs. A little note, here... When you run `npm` on the commandline, the shell doesn't *only* search for an executable in those paths; it first checks for aliases and functions that are defined with that name--they take precedence. So if you have an alias or a function defined in your `~/.bash_profile` for example, that will get run regardless of what scripts live on your filesystem. So now to the "subcommand" part; the `install` from `npm install`. This is just an argument being passed to `npm`. It's up to the script how to handle arguments, and with `npm`, it treats the first argument like a subcommand to execute. Some commands (like `cp`, `mv`, `rm`, for example) take file paths as arguments. It's really up to the tool/script to decide how it parses its arguments and how it uses them and how it's structured to find its subcommands. `git` has a *ton* of subcommands and routes to other executables that it has stashed away somewhere so `git commit` actually calls a script called `git-commit`, but this is a feature of `git`, not your shell. Let's just do a quick example for making your own thing like this. Create a file in your home directory called `command-example` and put this in it: #! /usr/bin/env bash echo "Welcome to the program" # create a variable "subcommand" and set it to the first argument to our script # then shift so our argument list is now every arg after the first subcommand="$1"; shift # based on what the value of our subcommand variable, # do something different case "$subcommand" in help) echo "Outputting help." ;; install) echo "Will install things." ;; info) echo "everything you passed: $@" ;; *) echo "Unknown command: $subcommand" exit 1 esac Now, run `chmod +x ~/command-example` to make it executable. If you're in your home directory, you can now run `./command-example` to run it. Try it: $ ./command-example help Outputting help. $ ./command-example info hello world everything you passed: hello world If you move this file into `/usr/local/bin/`, you can just run `command-example` without needing the leading `./` There's a lot of things here which you may not be familiar with. The `case` statement, variable assignments, `shift`, the "shabang" line (`#! ...`). Everything except the shabang is bash-specific. But please, google a bit, now that you have some specifics. 
You are mixing things up. `nmp` is the command and `install` is a parameter. To see where you can improve, the best is to show what you already did. That way we mnight be better able to follow your thought process.
They are called sub-commands. Depending on what you use (the language you write your program in) to make your programs, you have different options. This is for example the section of the argparse module of Python dealing with sub-commands: https://docs.python.org/dev/library/argparse.html#sub-commands
Not sure exactly what you're looking for, could this be it? [https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases](https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases)
Not quite, what im wanting is to provide some commands that do a lot of different things to people working inside of a project. Docker stuff is the most common - I have an alias like \`alias dstart="docker-compose pull &amp;&amp; docker-compose build &amp;&amp; docker-compose up -d &amp;&amp; dlogs"\` and would like to allow all of my coworkers to have that available on their machine too - but with the aliases checked into version control and not have to have everyone updating their aliases individually when we want to add or change a new command. 
Got it. Best thing I can think is make a folder of scripts in your repo, with stuff such as docker_startup.sh, and just call those when you need them. Then again, there may be a really simple way to do this I don't know. Hope you find the answer!
You can though the variable has to be approved server-side. Check for `PermitUserEnvironment` in the `sshd` manpage.
Yes; it comes down to the quoting used. If you pass variables inside single quotes they will be interpreted remotely. If they are passed inside double quotes; locally and before command creation. i.e; host1 ssh's to host2 and echo's $HOSTNAME ssh user@host2 'echo $HOSTNAME' host2 ssh user@host2 "echo $HOSTNAME" host1 In the case of the first command; host2 receives 'echo $HOSTNAME'. In the second command; host2 receives 'echo host1'. If you're referring to ssh'ing to another server and having a set of variables available to you from the original box ; that is possible but would need an AcceptEnv line in the sshd_config to allow it. 
You could expand the variable before you do whatever you are supposed to do ssh krusell@redd.it echo $variable should echo the local variable on the remote computer.
Would a Makefile fit the bill ?
This could not be what you’re looking for because you mentioned not wanting them to source it in their profile. But what if you had your team alias cd to check for a .alias file when they enter a directory and then ask them if they want to source it if it exists?
ahh, that might would work - I always forget about make. That might be the right answer
Wow, did not know this, cool
you might be wanting [direnv](https://direnv.net/) everyone needs to install it, but it'll load/unload dir specific configs as you move around.
I need to pass it to grep on the remote machine and somehow get the output on the local machine
I need to pass the variable to grep on the remote server and get the output on the local machine. 
https://en.wikipedia.org/wiki/ANSI_escape_code
ANSI escape codes huh? &amp;#x200B; Thanks for not only the link, but your extra tips and tricks, have a great day!
 $ cat test #!/bin/bash OPTIONS=--disable-server CMD=/usr/local/sbin/termr [[ -z "$CMD" ]] || { OPTIONS="$OPTIONS -e sudo '$CMD'" } echo xfce4-terminal $OPTIONS $ ./test xfce4-terminal --disable-server -e sudo '/usr/local/sbin/termr' $ &amp;#x200B;
 $ cat xfce4-terminal #!/bin/bash I=1 for X; do echo "arg $I: $X" let ++I done $ cat test #!/bin/bash OPTIONS="--disable-server --other --options" CMD="/usr/local/sbin/termr" ./xfce4-terminal $OPTIONS ${CMD:+-e "sudo $CMD"} $ ./test arg 1: --disable-server arg 2: --other arg 3: --options arg 4: -e arg 5: sudo /usr/local/sbin/termr &amp;#x200B;
 a="foo" ; ssh root@server "grep $a /var/log/somelog" 
on a mac, so using gdate for GNU date, came up with: `( export TZ=America/Santiago startat='@1549936800'; [ $(gdate -d$startat +%H) -lt 21 ] &amp;&amp; gdate -d "$(gdate -d "$(gdate -d $startat) -1 day" "+%F 21:00:00")" +%s || gdate -d "$(gdate -d$startat "+%F 21:00:00")" +%s )`
You have to use an array. It looks like this: options=( --disable-server -e 'sudo ssh -t backup screen -DR screen02' ) To preserve those space characters in that one element at the end, you have to access the array exactly like this: xfce4-terminal "${options[@]}" The `"` here are very important.
I would never have gotten that I had to look for arrays. Thank you so much. You have no idea how frustrated I was after several hours over many days.
GNU Make is a good choice
This is a really excellent answer.
Oh man. That's _very_ important
Oh man. That's _very_ important 
Cool man. 
Thank you! Fixed in lieu of ritual suicide
In general: [[ $task == "building command lines" ]] &amp;&amp; solution="use arrays" Concatenating strings is just way too fragile for anything but the simplest cases.
This is rad, thanks.
```kunst``` is a deamon that extracts the album art from the songs playing in ```mpd``` and displays them in the a little window. It doesn't loop on a timer, instead it waits for ```mpd``` to send a ```player``` event. When it receives a ```player``` event, it wakes up and extracts the album art of the current playing track. This maks ```kunst```really lightweight and makes it idle at ```~0%``` CPU usage. **Note:** ```kunst``` is meant to be used with files that have embedded album art. But, if a song that is currently playing, does not have an embedded album art, an image of a music note will be shown.
I don't use this tool, but following your logic it seems like the script ends after the stdin from `close_write` is consumed after the first change. I would try wrapping this entire business in a `while :; do` loop, then run your script in the background. Thanks for the exposure.
I think `read` is closing STDIN; can you try setting `read -t0` so it stays open?
Hi, actually the loop idea appears to have got around it. Setting read -t0 actually caused it to crash out of the script when receiving the first file.
This works! For some reason I thought it would be more complicated. Thanks
Well, I AM a simple case, so that explains a lot.
Neat and weird! Does it still require the `-m` flag inside the infinite loop?
I'll check.
It appears not! :)
It's because of the `exec` in there. It replaces the loop with dotnet, so there's no longer any loop to read inotifywait's output.
u/dchurch2444, here's your script after proper formatting: #!/bin/sh inotifywait -m /home/auto_ftp -e create -e moved_to -e close_write | while read path action file; do echo "The file '$file' appeared in directory '$path' via '$action'" exec dotnet toolbank/ImportToolbank.dll "$file" done And here's how its execution breaks done: 1. `inotifywait` and the `while` loop are executed in separate subshells, connected by a pipe^(1) 2. `inotifywait` sends its first notification 3. `while read ...` consumes it and `exec`s your .NET app, which **replaces the subshell in which** `while read` **runs**. In other words, `while read` *no longer exists at this point*. 4. Your .NET app runs to completion and exits, which closes its end of the pipe. 5. At some point in the future, `inotifywait` sends a second notification down a now-"broken" pipe, receives a `SIGPIPE` signal in return...and dies. To fix this, simply drop the `exec`: dotnet toolbank/ImportToolbank.dll "$file" so the `while` loop's subshell doesn't get nuked, then carefully consider what the `exec` command does. As a rule of thumb, `exec &lt;command&gt; ...` inside any kind of loop is almost certainly an error. ## Further Reading * [bash man page](http://man7.org/linux/man-pages/man1/bash.1.html), search for the `exec` command ## Footnotes ^(1) This is what normally happens. However, if you did *both* of: $ shopt -s lastpipe $ export BASHOPTS then your script would inherit the `lastpipe` shell option. Read my recent blog post [The \`lastpipe\` Maneuver](https://tclish.org/2019/02/14/the-lastpipe-maneuver/) for details on this obscure option, but in a nutshell, that would cause `while read` to run in your main script process, so the moment your .NET app gets `exec`'d, **the entire script dies**.
Wow. Thank you. I've learned a fair bit from that. Truly appreciated.
Better use `"$*"` instead of `"$@"` to replace echo: log () { local IFS=" " printf '%s\n' "$*" &gt;&gt; /home/kernel/compile_kernel_test.log }
1. there's really no need for `#!/usr/bin/env bash` just do "#!/bin/bash". Bash isn't like python. It's highly unlikely you're going to end up with multiple versions of it on your system or any system for that matter. 2. Your problem doesn't seem to be bash related but ifconfig related. I'm guessing, there's a problem with however the vnet24 tap interface is setup? Is this a KVM thing?
It is indeed, libvirt and qemu via virt-manager. It's a virtual network specifically. Also thanks for the heads up regarding the hashbang.
what do you get for `$ ifconfig | nc termbin.com 9999` ?
&gt;$ ifconfig | nc termbin.com 9999 [https://termbin.com/jkqv](https://termbin.com/jkqv)
Ok then the answer is quite simple : interface vnet24 does not exit ! `ifconfig | grep vnet24`
True, but I've tried it on an interface that had not gone down and I receive the same issue. I tried with vnet22 instead and got the same result.
That's weird, there's nothing wrong with your script it works well for me... try if you get the same with `lo` interface ?
Same issue, could it either be SELinux or the fact that "bash is hashed" according to my OS?
Oooooooh, SELinux. [https://linux.die.net/man/8/ifconfig\_selinux](https://linux.die.net/man/8/ifconfig_selinux)
I ran **ps -eZ | grep ifconfig\_t** and it didn't return anything? Should it have?
Honestly, i do not know. You might ask on Redhat subredits or Redhat forums instead ? Waiting for further answers here
FWIW you shouldn't be using `ifconfig` on Linux, it's part of the net-tools suite which is treated as effectively deprecated/abandoned in favour of the iproute2 suite. And this isn't new either - [this has been the case for some years](https://wiki.centos.org/FAQ/CentOS7#head-e6388c11ed84c9b8a643a69d85a2ae426aaa9f0b).^[1] The command you're looking for now is `ip`, and your sequence of commands would probably be something like this: ip link set vnet24 mtu 9600 ip link set vnet24 down ip link set vnet24 up You've got repeated behaviour there, so if this is going to be a common action for you, you could suck the commonality up into a function e.g. set_vnet24() { ip link set vnet24 "${@}" } Then your commands become something like: set_vnet24 mtu 9600 set_vnet24 down set_vnet24 up If you wanted, you could then build on that function to enable multiple actions, something like set_vnet24 mtu 9600, down, up And the function would loop through that delimited list, treating each field as an action... Or you could add positional parameter handling (or further: `getopts`) to the function. The most basic parameter would be a `downup` or `replug` or something similarly named, that would invoke the `down` and `up` actions with a single function call i.e. set_vnet24 mtu 9600 set_vnet24 replug ^[1] To be fair, backticks were deprecated 30-35 years ago and that message hasn't completely got through...
For portability reasons I always use the env form. It ensures that the version of bash in the path is used and it’s not tied to a specific location in the file system. 2 examples where you would want this on bsd, where bash is at /usr/local/bin/bash and macOS which ships with bash3 and you only get a modern bash with something like homebrew which also plops it at /usr/local/bin/bash. When I was playing with bash5 before it was officially released I had it in /usr/local/bash5/bin/bash and updated my PATH to put that first. So having my newer scripts load the right one was nice but I had to update some old ones to use the env shabang. But this reason is less common.
It's actually considered good practice to `#!/usr/bin/env bash`. /u/spizzike explained it [here](https://www.reddit.com/r/bash/comments/ar1hxq/whats_my_problem_here_totally_new_to_bash/egklx1a/). Another good reason is outlined [in this stackoverflow post](https://stackoverflow.com/questions/21612980/why-is-usr-bin-env-bash-superior-to-bin-bash/21613044). A lot of companies will use their own versions of software, which may not be the standard in your distro's package manager. That might result in two different versions of bash, the one in /bin/ being the distro standard, and not the one you want to use.
If `ifconfig` is returning "unknown host", have you checked to make sure the interface exists? As mentioned by /u/whetu, `ifconfig` is deprecated. You should start getting used to using `ip` instead. Run `ip link` to list all your available interfaces, and verify that vnet24 is actually the interface you want to work with.
How is your input file setup? The cut command is looking for a : in between the fields. 
I like being able to print multiple lines in one go, but good eye.
I see now! My input file was like this: Homer Simpson 3 and I fixed as per your solution and it works: Homer:Simpson:3 Thank you, you rock!!
Please show me an example of an alt version of Bash 
This script is actually quite horrible. All the unnecessary pipes, `cat $file | wc -l`? Why, `wc -l $file` works fine. ${LASTNAME,,} (the two commas, use ^^ for uppercase) is instantly lower case in bash4, no reason to put it in a new variable by dragging it through tr, and even then, use `tr '[:upper:]' '[:lower:]'`. And this is fore some bonus points, `echo $whatever | cut ...`, this is not wrong but learn to use awk wherever you can, awk is soooo powerful, or sed for that matter I see people do stuff like `cat $file | grep x | cut -d: -f2 | sed ‘s/y/z/g` where awk could do it in a single command `awk -F: ‘/x/ {gsub(“y”,”z”); print $2} $file`
What happens if you use ‘cat &lt; /dev/tty’ instead of just cat at the end. I can’t test right now, but I think that would grab input
Obviously this works with cat, but for some reason I was running into a wall with Expect. I have a tendency to overlap testing when I get frustrated, so I'll give the simple solution another shot tomorrow.
&gt;You have to use an array. Creating an array looks like this: &gt; &gt;options=( --disable-server -e 'sudo ssh -t backup screen -DR screen02' ) &gt; &gt;When later accessing the array, you have to do it exactly like this to preserve the space characters in that last element of the array: Wouldn't you need to escape the single quotes so they would be transferred to the command? options=( --disable-server -e \\'sudo ssh -t backup screen -DR screen02\\' )
Yeah, those single quotes will not be part of that word but that's intentional. The quotes are there for bash, to make it stop doing "word splitting". I have a helper function "args" in my .bashrc to experiment with bash's word splitting. Here's how it sees the two different examples: $ args --disable-server -e 'sudo ssh -t backup screen -DR screen02' 3 args: &lt;--disable-server&gt; &lt;-e&gt; &lt;sudo ssh -t backup screen -DR screen02&gt; $ args --disable-server -e \'sudo ssh -t backup screen -DR screen02\' 9 args: &lt;--disable-server&gt; &lt;-e&gt; &lt;'sudo&gt; &lt;ssh&gt; &lt;-t&gt; &lt;backup&gt; &lt;screen&gt; &lt;-DR&gt; &lt;screen02'&gt; 
I was thinking without the quotes transferring to the final command the execute flag for xfce4-terminal would only execute "sudo" instead of the entire command because of word splitting
About that final command and the array, here's another experiment: $ options=( --disable-server -e 'sudo ssh -t backup screen -DR screen02' ) $ args "${options[@]}" 3 args: &lt;--disable-server&gt; &lt;-e&gt; &lt;sudo ssh -t backup screen -DR screen02&gt; Those `"` quotes behave a bit strange when there's an array, there's still splitting going on between the entries of the array but not inside each entry.
What I'm trying to say is that with &gt;options=( --disable-server -e 'sudo ssh -t backup screen -DR screen02' ) will create this command &gt;xfce4-terminal --disable-server -e sudo ssh -t backup screen -DR screen02 and because it didn't transfer to single quotes to the final command, its basically only going to run this &gt;xfce4-terminal --disable-server -e sudo
Bash will behave exactly the same as if this here would have been the command: xfce4-terminal --disable-server -e 'sudo ssh -t backup screen -DR screen02' Those `'` exist only at the bash prompt, they are not seen by the program. See here again an experiment with that 'args' command: $ args test 'hello world' test 3 args: &lt;test&gt; &lt;hello world&gt; &lt;test&gt; $ x=(test 'hello world' test) $ args "${x[@]}" 3 args: &lt;test&gt; &lt;hello world&gt; &lt;test&gt; Those `'` quotes are stripped away by bash when it builds the arguments for a program. That 'args' command by the way this functionhere, I have it in my .bashrc for whenever I want to test what arguments bash will actually create: args() { printf "%d args:" $# printf " &lt;%s&gt;" "$@" echo } 
Outstanding! Learn something new everyday! I was literally typing the command into my terminal with and without the quotes and watching it err. After your explanation above along with pasting in the options array and trying out &lt;term&gt; "${options\[@\]}" did I truly get it. Thank you for the clarification :D
You're probably looking for shell-level redirection via `exec`. Try this: ``` #!/usr/bin/env bash cat # Read from stdin up to EOF if [ -t 0 ]; then exec &lt;/dev/tty # Reopen stdin with /dev/tty while read -rp "Type something: "; do echo "You typed '$REPLY'" done else echo "Not a terminal, can't do anything more." fi echo "We're done here." ```
You missed an excellent oportunity to debug the script though. 
I have no idea how you would do that dynamically, nor does it make much sense. I just did a little test and put `source .bash_variables` at the bottom on my `.bashrc` and then had one line in my `.bash_variables` that reads `myNumber=10` and it works fine that way. I suppose you could make a one-liner to put variables into the file and another one-liner to take them out pretty easy. Not automatic, but pretty minimal effore.
Can you explain why you want to do this? What is this file going to be used for?
you mean like a \_for loop\_ ?
You *could* technically do it by adding the following to the end of your `.bashrc`: # The state-saver state_vars=() trap 'declare -p state_vars "${state_vars[@]}" &gt; ~/.bash_state' EXIT [[ -r ~/.bash_state ]] &amp;&amp; . ~/.bash_state This sets an EXIT trap handler that saves the state of all the variables whose names you added to the `state_vars` array as and when you realized you wanted them: $ state_vars+=(my_var1 my_var2 ...) Of course, it also saves `state_vars` itself, so you don't have to keep remembering to set it at each session. (Since it's defined at the interactive shell level, it's not likely to be stomped on...unless you load some other interactive shell library that had the same idea.) Then on each session start, it loads the saved state in as the last act before giving you control. Having said all that, this might work fine with serialized session access to a remote server (i.e. only one session on one user login at a time), but its gets unworkable very quickly for folks like me who have multiple windows under the same user going at the same time. You've probably had one session's saved history stomped on by another session, but this is worse: You could lose _all_ your state, simply because the session you quit later didn't "get the `state_vars` memo" that was updated by a session exited earlier. So use with care. ## Further Reading * [a basic EXIT trap handler tutorial](http://redsymbol.net/articles/bash-exit-traps/)
idk if this is quite what you are going for, but to m\make a function read from either STDIN (file descriptor 0) or normal function inputs ($1, $2,...) ive done something like this foo() { local nIn="" local -a inAll local -a flags local kk=0 flags=( 0 0 0 ... 0 0 ) # parse inputs and remove (hard-coded) flags for nIn in "${@}" if [[ ${nIn} =~ ^-+flag0$ ]]; then flags[0]=1 elif [[ ${nIn] =~ -+flag1$ ]]; then flags[1]=1 ... elif [[ ${nIn] =~ -+flagN$ ]]; then flags[N]=1 else inAll[$kk]="${nIn}" ((kk++)) fi done # if no non-flag inputs, cat stdin and use that as input (( $kk = 00 )) &amp;&amp; inAll[0]=$(cat &lt;&amp;0) # do stuff } The potential issue i found with this approach (and why stdin is only checked when there arent normal function inputs) is , on my system at least, if there isnt actually anything in stdin the function will get stuck befind a `/bin/bash -i` function call without any arguments that gets made as part of executing `$(cat &lt;&amp;0)`. i.e. foo "bar" #works echo "bar" | foo # works foo # stops function execution
u/DivineArcade, if that's all you want to output, it just takes a single command: $ seq -f "urls[%.0f]=" 0 99999 Most folks don't realize that `seq`'s format string can contain the same arbitrary text as bash/C's `printf`. However, it only understands floating-point format specifications, hence the weird incantation between the brackets above. ## Further Reading * [GNU coreutils: `seq` invocation](https://www.gnu.org/software/coreutils/manual/html_node/seq-invocation.html) * [printf(3) man page](http://man7.org/linux/man-pages/man3/printf.3.html), to understand what the format specs that `seq` understands actually output
for i in \`seq 0 100000\`; do echo urls[$i] =; done &gt; array.txt
&gt;the same [...] as bash/C's printf Then let's use it: `printf 'urls[%d]=\n' {0..99999}`
Depending on your platform, that could trigger an “Arg list too long” error. Using seq is safer in this regard. 
printf is a bash builtin, there is no "arg list too long" error. The availability of seq as a non-standard utility depends on the platform.
Good points. I concede. 
This would be much much easier to do in Python. Just make a list of URLs and import the `random` module to make random selections
 paste '-d\0' aray.txt videos.txt &gt; done.txt sed '6r done.txt' redirect.html &gt; redirect.done.html
I think in this specific solution it would actually be: &amp;#x200B; for i in $(seq 100000) do echo "urls[${i}] = " &gt;&gt; array.txt done &amp;#x200B;
Thank you *so much.*
I got this working with an in-line script via proc sub: &lt;/dev/tty /usr/bin/expect &lt;(cat &lt;&lt;-'EOF' # spawn # expect # interact EOF ) "${ExpectArguments[@]}"
&gt; Why does exec refuses to use the dollar sign syntax, and instead accepts this weird stuff? Because that's [what the manual says](https://www.gnu.org/software/bash/manual/html_node/Redirections.html): &gt; Each redirection that may be preceded by a file descriptor number may instead be preceded by a word of the form `{varname}`. In this case, for each redirection operator except &gt;&amp;- and &lt;&amp;-, the shell will allocate a file descriptor greater than 10 and assign it to `{varname}`. Note that this _assigns_ the file descriptor to the shell variable. In your first command, you were trying to _use_ a shell variable as a file descriptor &amp;mdash; specifically, you were trying to close the file descriptor whose value was in `$A`. The documentation says: If &gt;&amp;- or &lt;&amp;- is preceded by {varname}, the value of varname defines the file descriptor to close. In other words, just: exec {A}&gt;&amp;- 
I suppose it is! It's straight from the red hat administration guide so I just follows the directions 
I spent about two hours trying to figure it out, I didn't fully understand one of the commands 
Never underestimate the bitchiness of Reddit
Okay, bash has some weird stuff when you dive deeper into it. Thansk for the quick response. 
No offense taken, but did you actually *try* my solution? The &gt; symbol is *outside* the do/done loop, so it treats the output as a single stream. If I had had the &gt; inside of the loop, then yes, it would have overwritten it 99999 times, and the text file would just be: urls[100000] = but that's not what I wrote. Seriously, try it.
What command didn't you get and what happened when you tried it?
Why doesn't this insert a nullchar?
Not that I can think of.
Because `paste`'s internal logic unescapes sequences like `'\0'` and `'\n'` in the `-d` argument. And since `'\0'` is a C string terminator, there's no effective difference between `paste '-d\0'` and `paste -d ""`.
The other thing to keep in mind is that there's a huge preference for typing less. These aren't barriers, but there's certainly a rather steep slope...of like...a foot. Everything you need to know is just a `man` away, and bear in mind that it's all just text. The same thing you were reading in some browser window. That goes for the program output, too.
&gt; Has there ever been a movement to make the command line more user friendly? Maybe give some examples as to what you're envisioning being different? Some things you may have overlooked that make CLI user-friendly (and by user, I mean sysadmins or experienced users looking to automate tasks - not necessarily new users who likely won't want / need text-based control) * man pages - every command should have manual pages describing what it is, what it does and what options are available. * Long option names - `-h` vs. `--help` - Aids readability. * Tab-completion - Try doing `grep --&lt;tab&gt;&lt;tab&gt;` - Automatic completion of options (also acts as a quick on-the-fly reference)
I backed what i thought was going to be the next big thing,but it did not take off :( [http://xsh.org/](http://xsh.org/)
Here’s a crazy example of what I’m envisioning. Think of manipulating code and automating, sysadmin’ing, configuring, and coding in VR with your hands in a 3D environment. I know that’s a bit pie in the sky. I know the terminal is meant to be as lean as possible to get out of the way of server tasks, but I’m just being creative here
The terminal is an interface to the computer. There are options to make it "friendlier". zsh is good. However, what you are saying about VR doesn't make sense. 
u/DivineArcade, at least it puts [your previous question](https://www.reddit.com/r/bash/comments/ararlb/how_to_make_a_txt_with_each_line_containing_urls/) in context, but that's a really ugly way to populate a JavaScript array. There are many other cleaner ways to get what you want, for instance: ``` #!/usr/bin/env bash cat &lt;&lt;EOF &gt; redirect.done.html &lt;script type="text/javascript"&gt; var urls = [ $(tr \; , &lt;videos.txt) ]; var random = Math.floor(Math.random() * urls.length); window.location = urls[random]; &lt;/script&gt; EOF ```
Command linux shell should immitate Microsoft PowerShell. An example of difference here: [https://docs.requarks.io/wiki/install/installation](https://docs.requarks.io/wiki/install/installation) (maybe one days [https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/](https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/)) Bash: curl -sSo- https://wiki.js.org/install.sh | bash PowerShell: [Net.ServicePointManager]::SecurityProtocol = [Net.SecurityProtocolType]::Tls12; iex ((New-Object System.Net.WebClient).DownloadString('https://wiki.js.org/install.ps1'))
http://psdoom.sourceforge.net/
It’s just an example. Think Minority Report. It just seems to me in 2019 there should be other universal ways of efficiently interacting with a computer. We have custom built GUIs and GUI based operating systems, but I just feel there’s something truly creative and innovative out there waiting for someone to architect.
There are experimental/beta UIs that make use of VR. Even show multiple monitors in VR that can be used. But at that point, it's no longer a commandline. For example, opening up a directory using a file explorer serves the same functions as `ls` command. Almost all command line functions can be achieved with GUI applications. 
Direct the output into the file and try xargs -n 2 .. 
What's the current state of this project
&gt; Also thanks for the heads up regarding the hashbang. For me, this is an unimportant quibble with no clear answer, so I leave this one to personal preference. If you think that the `env` way is the best for you, then great. If not, also great. There's a comment on stackoverflow or stackexchange somewhere that essentially reads: &gt;The *upside* of env is that it finds the first instance of python in your path. &gt;The *downside* of env is that it finds the first instance of python in your path. That is to say - unless you're validating your PATH, who knows what you might be running? What if you're an idiot and your `~/bin` is both world writable and early in your PATH? I personally think that if that's an attack vector on your systems, you probably have bigger issues, but it happens. Another issue with `env` is that it prevents you from using flags. This is a non-issue for `bash`, however. FWIW I work on machines ^g of ^o some ^v level ^t of importance. Our security guys class `env` along with `setuid` and `setgid` as a scripting no-no. But that's no big deal for us - because we're dealing with fleets of servers. So from an administrative point of view we can easily ensure via Ansible that either `/bin/bash` is in place or it is linked to where it needs to be. Our attitude is that any scripts that we share publicly is on the consumers of those scripts to ensure that the shebang is correct for them.
That's super cool man, I dig it. I may end up playing with it myself.
Please don't do this, and use something like /u/anthropoid's suggestion instead. You should be asking in a Javascript subreddit how to populate an array from a file or string of video URLs, not asking in a Bash subreddit how to do this elaborate and unnecessary workaround to generate bad Javascript code. This is an [XY problem](https://en.wikipedia.org/wiki/XY_problem).
u/Divot-Digger, it sounds like what you're after is simply a line in your crontab like this: ``` SHELL=/bin/bash # Run web perf test every hour 0 * * * * echo "$(date +%F,%T,)$(curl -so /dev/null -w '%{time_total}\n' https://down.load.site)" &gt;&gt; ~/logs/webperf.log ```
That worked perfectly. Thank you so much, @anthropoid.
Wow. This is some impressive stuff. I don’t quite see myself using it. Just feels like a great way to shoot yourself in the foot. And also I’m not quite sure I’d find it more useful than whatever cli util I’d use instead. But still cool. 
Why though
Reminds me a bit of Jupyter...
Nice, willing to share?
bloated
But is there any reason to write `-d '\0'`, why not write `-d ''`?
haha sorry forgot about that [https://github.com/sdushantha/kunst](https://github.com/sdushantha/kunst)
[https://github.com/sdushantha/kunst](https://github.com/sdushantha/kunst)
I really hope you forgot your /s
I can't think of one.
`''` and `'\0'` are not identical. The former will not work in many implementations, the latter is defined: &gt;`\0` Empty string (not a null character). [...] and &gt;The commands: &gt; `paste -d "\0" ...` &gt; `paste -d "" ...` &gt;are not necessarily equivalent; the latter is not specified by this volume of POSIX.1-2017 and may result in an error. The construct `'\0'` is used to mean "no separator" because historical versions of paste did not follow the syntax guidelines, [...] From: http://pubs.opengroup.org/onlinepubs/9699919799/utilities/paste.html
Computers revolutionize every aspect of conscious thought, but most living people will never break the surface of that capacity. You write with western grammer; let me assume that you have the ability to choose what path to study, or even whether to study at all. Some may think you have a relatively easy life, but the fact is that your distance from immediate physical harm raises extremely difficult questions, like # What exactly do you want? It took me a *long time* to answer this for myself. Mercifully, nobody tried to make me learn GNU/Linux before I did, or I would have vehemently rejected it as unintuitive, pretentious, and needlessly complex. That would be tragic, because the last year of struggle has paid off harder than anything else I've ever dabbled in. I wish you the best of luck with the hard questions. If you have more specific ones, feel free to ask.
I didn't get how the separation worked in the text or how the command interpreted it. It would create messed up users. 
You could put in some `echo`'s and you could use `set -x` to see what was actually happen in the background. And at what point the problem occured. Then see the command that made the problem. When you know the program you can read up on it, and cut isn't the hardest command to read up on. `man cut` 
I’ve grown to really love Linux and appreciate its power. It’s just hard for me to imagine that the human-computer interface is going to continue to look like this in another 50 years. What I want in this interface is for a more intimate connection between the human mind and the machine. I want human capacities to be fully leveraged—spacial intelligence, visual analysis, simple language, physical motion. Humans are great at a lot of things, and computers are great at a lot of things, and sometimes I feel there’s too many people trying and failing to be computers and too many computers trying and failing to be people.
That is really cool, nice job.
lose the pointless bash -c in that -exec find This.Is.My.Folder -type f -exec ln -sf -t SymlinkDirectoryToSymlinkTo {} + in particular, when you do need a shell with -exec, make sure you don't inject find's filenames into the shell. Pass them on as arguments instead. E.g. find ... -exec bash -c 'something with "{}"' \; # WRONG find ... -exec bash -c 'something with "$1"' bash {} \; # Good
One option is to use `find ... -print0` together with `xargs -0`. This tells find to use the 0x00 as filename separator instead of space and tells xargs the same. So your command line would look like this: `find "This.Is.My.Folder" -mindepth 0 -type f -print0 | xargs -IFILENAME -n 1 ln -s -f "FILENAME" "SymlinkDirectoryToSymlinkTo"` &amp;#x200B; &amp;#x200B;
Thank you kindly! This seems to work much better and works with whitespaces :) 
It's already 50 years old and it looks pretty similar to how it started. You can go spend some time on /r/unixporn if appearance is paramount to you, but minimal text interfaces will always be more efficient.
Here's a sneak peek of /r/unixporn using the [top posts](https://np.reddit.com/r/unixporn/top/?sort=top&amp;t=year) of the year! \#1: [\[OC\] A little project that I've been working on... inspired by one of the greatest posts of this sub](https://v.redd.it/f755ua16qhz11) | [452 comments](https://np.reddit.com/r/unixporn/comments/9ysbx7/oc_a_little_project_that_ive_been_working_on/) \#2: [\[Awesome\] Mechanical Love](https://i.redd.it/glzrkk83f4621.png) | [501 comments](https://np.reddit.com/r/unixporn/comments/a900p7/awesome_mechanical_love/) \#3: [\[OC\] Always keep some Terminal with you](https://i.imgur.com/E5lNLmA.jpg) | [150 comments](https://np.reddit.com/r/unixporn/comments/9ndo8o/oc_always_keep_some_terminal_with_you/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
I know it’s 50 years old, which is why I think it would be crazy for it to go another 50 years. It’s not about appearance. There has to be a more efficient, flexible, comprehensible way to interface with a computer than a zillion cryptic commands with a zillion options each. I’m talking about a paradigm shift
I get it from [https://stackoverflow.com/a/50204589/1528712](https://stackoverflow.com/a/50204589/1528712)
Let us know if you find one.
Maybe i'm not understanding what you're asking, but it feels like you could accomplish this with the command 'git log &lt;filename&gt;'
`git log` assumes you have a local copy of the repo, which the OP presumably doesn't want to maintain.
Wouldn't you be able to do shallow clones and use git to manage all that ?
I like it how it is. Anything more would just make it bloated.
If you want ONLY the commit date of the most recent commit: &amp;#x200B; `git log --pretty=format:"%cd" -n 1`
Your VAR2 contains a carriage-return character.
 I too wonder this 
I don't think there's a shortcut. All keystrokes are first processed by the _toplevel_ SSH, so the only way for a second-level SSH to begin recognizing an escape prefix is after the toplevel has seen `&lt;Enter&gt;~~`.
And which is easily dealt with inline: tr -d \\015 &lt; example.csv | while IFS=, read -r VAR1 VAR2 VAR3; do ... done or, more precisely: sed 's/\r$//' example.csv | while IFS=, read -r VAR1 VAR2 VAR3; do ... done Use the first alternative if you're sure that there are no bare carriage return characters in the actual data, and the second if you want to play it safe and strip only the end-of-line one.
Perfect, thanks!
Ahhh, thank you!!! Makes perfect sense
these solutions did not work, sadly... am still experimenting but neither had an effect.
You can do that like this: for file in "$DOWNLOAD_DIR/debug_$DATE/result/smart_"{sd,nv}*.result That `{...}` thing has to exist outside of the `"` quotes to work right, same as the `*` wildcard.
&gt; {sd,nv} That worked, thanks :)
1. What exactly do you mean by "neither had an effect"? Are you still getting the same "illegal unquoted character" error with both solutions, or a different error now, or something else entirely? 2. Are you (unsuccessfully) trying to access `VAR`*x* outside the `while` loop? If so, read [my recent blog post](https://tclish.org/2019/02/14/the-lastpipe-maneuver/) on why it didn't work, and how to do so correctly.
[Doesn't look like it](https://github.com/openssh/openssh-portable/blob/master/clientloop.c)
ssh interacts with the pty it's connected to, it doesn't know what program is running in it. One thing you could do is set the escape char to be something else for some of your sessions: (`-e '+'` or similar).
forgot to update this. the tr-d \\\\015 &lt; example.csv worked after I made somec orrections to my script syntax. thank you 
First of all, is it a requirement to not work on a properly formatted file? I try to leave as much of bash's work to bash, it makes it easier to read (and understand) the plot later on. Second, don't use `cat` that way. Instead of cat FILE | grep "expr" | program2 do grep "expr" FILE | program2 And see `man grep` for more specific matching to avoid so many pipes. As for plotting from a specific x-value, I would either do a workaround: set xrange [2000:*] Or use the new (v5) way of: plot "file" u 1:2:3 [2000:*] (I have not yet used that one yet.. Please be aware that values beyond the ranges are still included in some of the `gnuplot` STATS vars, if you intend to use those.)
You covered most of what I was going to say, but I wanted to add that OP should look into the "using" modifier to avoid the `cut` and `tr` pipes: http://gnuplot.sourceforge.net/docs_4.2/node133.html
what about a regex for it? something along the lines of `(?&lt;=Final residual = )(.*)(?=,)` to grab anything between Final residual and the comma after that