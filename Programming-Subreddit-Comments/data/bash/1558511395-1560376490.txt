 echo "$x" | awk -F0 '{print NF-1}' And one if pure shell OIFS="$IFS"; IFS=1; set -- $x; IFS="$OIFS" zeros="$*" # String length of $zeros minus (number of positional parameters minus one) echo $((${#zeros} - ($# - 1)))
Interesting. Maybe GNU sed processes the file in smaller chunks to save memory. Or maybe it's an implementation issue. I understand your issues regarding the feature set. I try to stay away from more exotic modes and features, keeping my needs simple allow to use simple tools, so sometimes it's a matter of doing things differently, or breaking a complex task into smaller, simpler tasks. But arguably, some can't live without zsh fancy prompts and popup menus for instance, so it's not for them. For me it all started with the inconvenience of using GUI apps. They break, they're heavy, they're ugly, they change constantly, they require tons of dependencies, they don't work through ssh, you can't pipe them or stream them... so I started moving to the CLI in the mid-2000s. Later, I started to turn "suckless" without even knowing about them, and slowly realized how slow and bloated Bash and many subsystems were for my needs - polkit, dbus, NetworkManager, PackageKit, you name them. I first had to discard the non-POSIX and more exotic stuff before changing my shell loksh. The conversion to ash was straightforward since everything was already POSIX, but there are a few things to build in ash before it's really usable, by default it doesn't even process the PS1 env variable. And it was slightly slower than loksh when compiled with -Os. As a matter of fact I hesitated between busybox and loksh+[lobase](https://github.com/Duncaen/lobase) (sbase broke too much stuff), but I find that busybox does things right even though it's not portable. I also statically compile it against musl, it's light and efficient. A few things I miss: busybox vi lacks the visual select mode and a few other things, like I can't delete a paragraph (`d}`) or uncomment a column. Bummer, so I still use `vis` a lot (I have issues with `nvi`, and `vis` is simpler). `find`lacks regex, another big bummer. `psutils`lacks many options in its commands, and `ps` and `free` don't show the memory usage right - or at least they differ from GNU. However, while I kept psutils, find, vi and a few others, I could remove all the GNU coreutils, even busybox `ls`happens to do all I need. So far I wouldn't go back, and at last I feel like I'm done digging the rabbit all. Everything is essentially as simple as it can be, like my production containers at work. ;-)
I'm definitely in the same boat. While my system probably isn't as debloated/customized as yours, I prefer the simpler software too. A mindset like that probably starts with heavy ricing until you realize you don't need all that shit. Somebody with his 200 char wide airline prompt, which only needs 300ms to print, will look at mine, think it sucks and assume I don't know any better. I know better though, because I've been there myself but simply evolved :) One great advantage is that you can feel right at home in a complete default environment. If stuff really annoys me, it's probably like 3 lines I can type from memory and put them in my shell config and feel pretty much right at home. In contrast to that, a totally riced out zsh/fish/$shell/vim will make default feel very alien and you basically have to carry your gazillion lines long config files around. Horrible. BusyBox is great, my dream distro would be a mix of Gentoo and Alpine. I can totally live with everything BusyBox except `ash` and `vi`. Maybe I should try `ash` again. Whenever I need to change something in the line which is more complex than 1 backspace, instead of making use of its incomplete vi-mode, I could simply press v to edit in `$EDITOR`. Need to force myself to do that more often. I thought I needed `HISTIGNORE` but fudge that, I can clean my shell history using awk. I tried out vis and I think the concept of structural regular expressions is great. I just found that it just adds more typing for a gain in control I don't really need(ed). It's been a couple years since I tried though and zero syntax highlighting is kinda tough, so I already considered checking it out again. Another reason why I considered switching is that nvi on Gentoo is quite buggy. It segfaults on certain actions (I should really report those bugs).
Or `while [ -n "$1" ]`.
The whole idea that math expressions equalling zero return false left me with a mental trauma.
one more! x=100110110 x=${x//1} echo ${#x}
 IFS=1 set -- $x IFS="" echo "${#*}" IFS="$OIFS"
In all sane markdown versions ```This is a multiline code``` as well as lines starting with for spaces. Mobile has no markdown helper whatsoever, so the OP did it either out of habit, or used fancy pants editor, which does, whatever it does, under the hood, no point in blaming.
Find where are your aliases defined and remove said definition...?
Please format your code AS code.for val in for alias in foo bar baz; do unalias -- "$alias" done
When I tested it, I tried something similar but it didn't work and it seemed like `${#*} was wrong half the time. It seems in shells other than bash, to get rid of spaces between fields, you have to save the content of `$*` to a variable first while `IFS` is empty. So I guess the best way would be your solution with an added variable. Nice one though!
Bash isn't exactly a language that you use in an IDE. You either type it directly in a terminal, or you just use a text editor (vim, emacs, nano, gedit, atom, ...). Here are a few threads with people asking the same question: * https://www.linuxquestions.org/questions/linux-software-2/ide-for-bash-shell-programming-797620/ * https://ubuntuforums.org/showthread.php?t=2093938 Here's a plugin for atom: https://atom.io/packages/ide-bash
here is the output : [https://imgur.com/gallery/V51xOUP](https://imgur.com/gallery/V51xOUP)
I'm so grateful, thanks so much
Your script is hard to debug since you've hard coded the logic to depend on your arguments (file inputs to`mov`). In other words it's hard to tell without knowing what you expect the format of your input files to be. I'll try to help though. Why can't you find your files? Well, if `find` doesn't return a match it's because that file doesn't exist where you're searching. I don't see any `mv` commands anywhere, so was the file moved previously? You're implicitly searching in the path `./$2`, is that what you want? Side note: to avoid rerunning `$(head -n 11 difference |tail -n 1)`, you use `sed` to do it in one command and store it as a variable and use that. Like this: fname=$(sed '11!d' difference) echo "the file $fname was moved to $(find "$2" -name "$fname")"
hello thank you for your response what i want to do is take 2 directories (dir1 and dir2, the two directories exist within a tree) in an argument ($1 $2) and display the files that exist in dir1 and moved to dir2
So any files that exist in dir1 but not dir2 you want to move to dir2?
As for "why the code is not showing where my files were moved" where you're doing the actual "moving" in the code? The code you posted does not move any file so `$(find "$2" -name $(head -n 11 difference | tail -n 1)` comes up empty. You need to move those files you got in difference to `$2` for `find` to "find" it there. As for "why the second conditions where no file has been moved" , I cannot reproduce it. It shows the message for me if the diff doesn't provide any output. Check your code to make sure it is as shown above.
I just want to know IF they exist , if they were moved or deleted
Also, why do want the 11th line of the difference file?
If differences exist then `diff` will show you that. Unless you move or delete them then the result of the `diff` is the current status of the file system, why would they be moved or deleted?
You call `mov` at the top, but it hasn't been defined yet.
You re-`source` ~/.bashrc on every iteration of your loop. If your aliases are defined there, they're being re-`alias`ed. As /u/ladrm said, you should probably see if they are defined there and remove them from being defined in the first place.
ahh my bad i wanted to type the first line , Btw my problem here is with the $2 in the find command when i did echo $2 it shows me ch1/pictures/2017 i just want to take the ch1 and put in the find command find $2 -name $1 &gt;&gt; when i call the function like Bash comp-arbo7 ch1 ch2 the find command doesn’t take ch2 as a directory but the whole thing ch1/pictures/2017
Sorry i wanted to say IF they were moved and my problem here is with the $2 in the find command when i did echo $2 it shows me ch1/pictures/2017 i just want to take the ch1 and put in the find command find $2 -name $1 &gt;&gt; when i call the function like Bash comp-arbo7 ch1 ch2 the find command doesn’t take ch2 as a directory but the whole thing ch1/pictures/2017
And sorry for the ambiguity, bc i study bash in french
And sorry for the ambiguity, i study bash in french
&gt; ahh my bad i wanted to type the first line I don't know what you mean, but no worries. &gt; when i did echo $2 it shows me ch1/pictures/2017 Did you only pass ch1 as the second argument to your script? If so something seems wrong, it should only `echo` ch1.
Counted way more than one command.
https://imgur.com/gallery/V51xOUP look what i did , but when i added echo $2 to see if find take ch1 as an argument i get ch1/pictures/2017 instead
If you passed ch1/pictures/2017 and you just want ch1 you can get it from: updir="${2%%/*}"
I think that's the wrong picture, it's the same one as above.
Pretty sure that's not the code he's using, otherwise it would give an error like `mov: command not found`.
But if i want to change the directory , like ch1/pictures/family/2016 that’s won’t work I would a to do a command that only take the first word « ch1 »
Can we move this to reddit chat? This is getting too long for a reddit post.
Okay this is confusing. Since you mentioned that you're not proficient in English which is fine, I suggest you post the exact code you're using (the exact content of comp-arbo7.bsh and use it in an example situation that is similar to the actual one. For example create two test directories with similar structure. Then post what output you're getting AND what you expect. Also use the formatting features in Reddit like inline code and code blocks instead of pasting them directly. See the formatting [wiki page](https://www.reddit.com/wiki/commenting).
It was written this month by one of our Training Architects for our blog!
Just extending yours.
damn dude fix your coding style [https://google.github.io/styleguide/shell.xml](https://google.github.io/styleguide/shell.xml)
Hey. Um. This is kind of massive and unformatted. You may want to use some kind of pastebin service or a gist on Github instead so it's a little easier to read. &amp;#x200B; Second, you need to tell us what it's doing and what you're expecting. Where do you get when it stops working?
Hello my apologies this is the pastebin [https://pastebin.com/94j01YRa](https://pastebin.com/94j01YRa) . It's also to create a backup of a script file and when running it I seem to get an error with the " echo '1 - Create a backup copy of a script file' " line.
Can you share the output that you get when trying to run it? There are a lot of quoting issues and such, but nothing that strikes me as completely broken.
You can use a simple function like below. Note that this will only remove the alias for existing bash session. To remove them permanently you have to remove them from `.bashrc` or wherever they are defined. Remove_Alias() { for elem in $(alias -p | sed 's/alias \([^=]\+\)=.*/\1/'); do if grep -q "\&lt;$elem\&gt;" &lt;(echo "$@"); then echo "Unaliasing $elem" unalias $elem fi done } Use it as below $ alias l ll la alias l='ls -CF' alias ll='ls -alF' alias la='ls -A' $ Remove_Alias l ll la Unaliasing l Unaliasing la Unaliasing ll $ alias l ll la bash: alias: l: not found bash: alias: ll: not found bash: alias: la: not found
So I kind of rewrote the whole thing. It seems to work even though it has a lot of problems -- mostly because it requires you to type in filenames without offering any tab completion or anything like that to do it. Would be better done as a bash function, but I'll leave that as an exercise for the reader. &amp;#x200B; [https://gitlab.com/snippets/1859500](https://gitlab.com/snippets/1859500)
The looping is quite unnecessary. `unalias` accepts multiple arguments.
Thank you very much! This helps a lot. I can see where the problems were and helps me for future use.
-P prefix
`wget -P /public_html -O &lt;file to download&gt;` ?
This is what I'm doing but my script syntax is like this: `scriptname config_name command --flags options`. Right now I'm doing this:
I do this before I define my aliases, I generally don't like nor trust other folk's aliases. unalias -a
If they're defined up the stream say, out of /etc, don't modify those just to suit your personal preferences. That's what your own environmental/initialization files (.bashrc, .bash_profile) are for.
What does `man wget` tell you? You’ll find there is no -P option.
Your wget must be old. Wget 1.20 says: -P prefix --directory-prefix=prefix Set directory prefix to prefix. The directory prefix is the directory where all other files and subdirectories will be saved to, i.e. the top of the retrieval tree. The default is . (the current directory).
Why’ve you used python for what is essentially just a bunch of shell alias and functions?
Because i hate copy files to .bashrc and love automation If sb want can just copy script var from python file to their .bashrc Simply i want to keep it easy to update and remove Next ver gonna have online update checker or maybe package manager to control aliases and funcs from store
`GNU Wget 1.12`
Based on my quick test just now, the issue I ran into was that if *$P* is empty, *ls* will execute as if the argument is */\** and you'll get the directory listing of the root directory. If *ls* gets an unresolvable argument, it'll print something to *stderr* instead of *stdout*. In that case, *nl* would get nothing through the pipe. Additionally, the error code remained *1* afterwards. This is on macOS using bash 5.0. My regular shell is zsh version 5.7.1 and it seems to do the same thing.
Here's with the help 'sed': count=$(sed -nr 's/^SELECT COUNT.*'\''([0-9]+)'\''$/\1/p' Countvalue.txt) That weird `'\''` sequence in there is only for bash. The 'sed' program will see a `'` character in that spot, it will not see that weird `'\''`. The script that sed will execute is this here: s/^SELECT COUNT.*'([0-9]+)'$/\1/p A different way you might solve this is with a combination of 'grep' and 'cut' and 'tr' or other tools. Here's different ideas that all work to get the number: grep '^SELECT COUNT' Countvalue.txt | grep -oE '[0-9]+' grep '^SELECT COUNT' Countvalue.txt | cut -d ' ' -f 5 | tr -d \' awk '/^SELECT COUNT/ { print $NF }' Countvalue.txt | tr -d \' awk '/^SELECT COUNT/ { gsub(/'\''/, "", $NF); print $NF }' Countvalue.txt perl -nE '/^SELECT COUNT/ and /(\d+)/ and say $1' Countvalue.txt All of these will work inside a `count=$(...)` line in your script.
Check `PIPESTATUS` in bash manual which is an array variable containing the exit codes of pipe elements. For older shells check environment variable `pipefail` which returns the last non-zero exit code in pipeline.
cool [https://unix.stackexchange.com/a/267031/348048](https://unix.stackexchange.com/a/267031/348048) I guess this is what set at yours
very cool. Just tried \`set -o pipefail\` and it works :)
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
From my brief reading, $# represents variables passed into your script, not the ones you get in tour read lines. Instead of testing $#, you should be testing the variables.
Thank you. That does help me out. By testing the variables I can have one for x and y separately.
 count=$(awk '/COUNT/ &amp;&amp; gsub(/[^0-9]+/,"")' countvalue.txt) This prints lines which contain `COUNT` AND have non-numeric characters to delete, all of which will be deleted. This will work as long as the value you want to extract is the only numeric. gsub is basically `sed/.../.../g`
I mean yeah, but... `find`?
 awk -F "'" '/^SELECT COUNT/ {print $(NF-1)}'
Here's a portable way of doing it without using external commands: #!/bin/sh file="/path/to/your/file.plist" # set counter to 0 number_line=0 # set delimiter to newline while IFS=' ' read -r /dev/null ; do # read each line # increment counter number_line=`expr "${number_line}" + 1` # direct file as input to loop done &lt; "${file}" fi &amp;&amp; printf 'The number of lines in %s is %s.\n' "${file}" "${number_line}"
Is there a different flag with `wget 1.12` ?
 ssh user@host ‘uptime’ Put that in a bash script and set cron to run it every 30 mins.
A lot of the formatting is preference and not required -- I work a lot in Python so my mind just expects and prefers the indentation levels (I used two spaces throughout). &amp;#x200B; Also for tests I prefer the look of \`\[\[\` vs \`test\` cos it is just easier to read. There wasn't anything wrong with how you were doing it. &amp;#x200B; Happy to answer any questions about the changes made if that helps.
Are you just trying to keep the connection from dropping? Check out the man page for **ssh\_config** and look at **ServerAliveInterval** and **TCPKeepAlive**. This is my [\~/.ssh/config](https://github.com/jnalley/dotfiles/blob/master/ssh/config). If you really need to "run a command" you can do something like this: `ssh &lt;hostname&gt; 'while :; do date; sleep 1; done'`
I'm not entirely sure of your intended usage here, but I made a bash function that should do what you're wanting: &amp;#x200B; #!/bin/bash backup() { # Usage: backup filename.ext # Will create a "filename.ext.bak1" file unless it already exists # In which case it will create "filename.ext.bak2" and so on # Sets backed up filename to original_file.bak backup_name="$*"".bak" # Starts count at .bak1 count=1 # Makes sure the file exists, if not exits with error if [[ ! -f "$*" ]] ; then echo "File not found:" echo "$@" return 1 fi # Begin a loop to check to see if there is already a .bak1 file and # increase the count until the file is not found while true ; do if [[ -f "$backup_name""$count" ]]; then count=$((count + 1)) continue else cp "$@" "$backup_name""$count" return 0 fi done }
You would need to save this file somewhere and then do `source` [`backup.sh`](https://backup.sh) and that will make it available to you on the command line. If you like it you can add it to your bashrc file so it is always available. &amp;#x200B; Let me know if you have any questions.
 for file in *.bak* ; do echo ${file%%bak*}bak$(echo $file | grep -o '.bak' | wc -l); done should give you something to start *writing something yourself*...
That's a good idea, but I am not sure if it will count that as user activity or not because it foes not show up as a last log on.
Ah, I see. Just tried at work, This is a smarter flag than -O, as -O will clobber all files to the one output, where as -P is a base directory. Cool.
Oh, sorry - I misunderstood what you were trying to do. Try using the `-t` flag to force pseudo-terminal allocation. That should make the session interactive and it should show up in lastlog.
 cp filename.ext{,.BAK}. # will add a .BAK to filename.ext Filename.ext -&gt; filename.ext.BAK Repeat again cp filename.ext.BAK{,.BAK} # you will get filename.ext.BAK.BAK
 cp --backup=numbered
The command line for the equivalent find is quite messy since you cannot specify a set of directories to find within.
In university I had a part time job at the school’s research facility which boasted a VAX. That’s one thing I really liked about VMS was it always made numbered backups whenever you modified a file. When you were satisfied that everything worked, you just issued a purge command.
I know, but I would like to add a “n+1” .bak extension to the original file, where "n+1" is based on the backup file with the most ("n” times) .bak extensions
&gt; I'm not entirely sure of your intended usage here In the first case I would like to add a “n+1” .bak extension to the original file simply with `bak file`, where "n+1" is based on the backup file with the most ("n” times) .bak extensions. In this way I don't have to reply to`cp` about overwriting "file.bak", because it will recognize it and create "file.bak.bak" automatically
`cp --backup=numbered file ~/Documents` only creates another `file` without tildes and numbers as if I was using only `cp`
This really smells like an XY problem. What are you _actually_ trying to achieve?
His boss created a way to see if employees are actually working and he's trying to game the system. At least, that's my bet.
Hi, Here’s a non hacky way to it Use boybu It’s a terminal multiplexer When you leave the ssh session your activity is persistent and will not go away You can run long commands and go away and come back to see them finished
Good point! I added a few traps
Good point! Has now err_die() and just err()
More accurately `$#` contains the number of commandline arguments passed to the script, that is words you typed after the script name e.g. bash script.sh word1 word2 or ./script word1 word2 if you want to verify user input here you probably want numbers, so `[[ $x =~ ^[[:num:]]+$ ]]` or `[[ $x -eq $x ]]`. the second will fail not being able to perform math operation on non numbers and will output an error.
Admittedly im not sure exactly how `fallocate - i` deals with this. In general, `fallocate` allocates space rounded up to the next filesystem block. Id guess it does the same when punching a hole, and then somehow marks any extra bytes in between as extras. If this cant be done (without making the whole file sparse) it probably sets the extra bytes as all 0's,which should be ok for OP's use case since this corresponds to null characters in ascii.
Thanks Stallman, a couple tweaks and it worked perfectly.
I suspect the output from the two commands is not the same. The JSON you posted contains an array within an array (that is, the top-level array contains one element, another array) so `jq length` will be 1. To get the length of the array inside it you'd need `jq '.[0] | length'`. Also, I presume that you manually hacked up this example to change the values because it's invalid JSON - you need to remove the comma after the second element of the first "IpRanges".
I have access to an AWS EC2 instance, but it is designed to shutdown after 45mins of inactivity. But I want my server to keep running.
can you ask that policy be changed? if i were you, i'd use a mix of ssh, expect and cron to pull this off. That way you can genuinely log into the box, evoke the bash shell, and fire off a handful of commands and then exit the connection. Expect should be able to do this for you https://likegeeks.com/expect-command/ But if a work machine is set to do this every 45 minutes - maybe they need to rethink how they're trying to save money within AWS. Maybe look at a smaller instance type. Or purchase a reserved instance and leave it up 24/7. Or at the very least - expand the window, so people can actually work.
That's OK, - I am assuming that one is declared immediately after the next as shown.. However... +%F_%H-%M-%Y will odd look a little bit odd...
I think you want %S at the end ... no point in repeating the year from %F. ``` BKUP_DIR="/var/local/$(date +%F_%H-%M-%S)" ```
Got it, thank you!
&gt; Everything else is fine. cool, thank you for the tip!
Looks fine to me. Nothing peculiar.
This is the proper way, yes. Note, however, that the interpolation of `BKUP_DIR` into `BKUP_LOG` occurs when `BKUP_LOG` is defined. If `BKUP_DIR` is changed later, `BKUP_LOG` is not automatically changed to reflect the change of directory.
 echo ${0##*/}
Perfect! That did the trick. Thanks so much.
What specifically are you lost about? srs question, not trying to be a dick. Just make sure the variable you are calling has been previously defined.
A screenshot of some code, blurry code even, what do I win?
A very bad picture
So then get the policy changed.
Unfortunately that's not an option.
makes sense. I've got all of those defined at the beginning of the script, and they are not redefined anywhere else. thank you for pointing that out, I'll be sure to watch for that.
Definitely: a few upthings Possibly: a sarcastic comment Unlikely: gold
That, my friend, is your terminal. Something is running in it too. Using context clues I'd say it's a bash script
Can you elaborate on the trailing characters? I get the ${0}, but I'm unsure what the ##\*/ regexp is for.
That is some really good observational skills you got n0shmon. yaay! thank you.
&gt; I'm unsure what the ##*/ regexp is for Ahhh.... You will have trouble working that out then... It is not regex... &gt; Can you elaborate on the trailing characters? Np... The curly braces take you into the land of [parameter expansion](https://www.gnu.org/software/bash/manual/html_node/Shell-Parameter-Expansion.html#Shell-Parameter-Expansion) - obligatory manual link.. ;-) ${parameter#word} ${parameter##word} both remove `word` from the *start* of `parameter` One `#` means the *smallest* match, while `##` means the largest.. `word` in our case is `*/` i.e. anything ending with a `/` character Therefore should you call your script with the absolute name `/home/darkciti/projects/don't-try-this-at-home.sh` the largest match of `*/` will be `/home/darkciti/projects/`... leaving just `don't-try-this-at-home` Were you to use just one `#`, the result would be `darkciti/projects/don't-try-this-at-home.sh` You could also use ${parameter/pattern/string} to achieve the same result - replacing `*/` with nothing, but you would have to escape the `/` ... echo ${0/*\//} and while it is useful for deleting stuff in the middle of a parameter, for stuff at either end, people are more familiar with using `#` at the start, and it's opposite number`%` used for removing stuff from the *end* of `parameter` Oh... If you are wondering why `#` denotes the start and `$` the end...... Just look at your keyboard... If you aren't in England, the chances are that ***$***parameter is in the middle... :-D
The bottom says something like: Adventure, my boy! Have faith in your emulator. Something triggered that. Or something.
The double quotes are unnecessary because word splitting does not occur for variable assignments.
[Watch Dogs](https://youtu.be/eXWVkB74d-E?t=127)
animated show... link above...
That's quite a bit more clever than just calling "basename $0"
Clicked on "Save" to ensure I get back to this...
`echo "$(basename "$0")"`
`sed -E 's/[^0]//g'` deletes all occurrences of non-zero characters and it then prints the result with the newline character `\n` appended at the end of the result. Then, `tr -d '\n'` deletes that newline character. `wc -m` counts the number of characters. Since the characters can only be a series of `0`, it counts the number of `0` characters. As someone pointed out to me in your previous post, `sed -E 's/[^0]//g' | tr -d '\n'` isn't ideal. The better way is to replace that pipeline with `tr -cd 0` which deletes all characters that are not `0` including the newline character.
&gt;zero\_count=$(echo "$x" | tr -cd 0 | wc -m) Ohhhhh I see that makes perfect sense, glad this subreddit exists! :D
You should have asked this in the same post instead of creating a new one.
This is not so trivial problem as a bash script can be run in different ways. You can run it by calling its name or you can source it. Also you can run another file that is a symbolic link to the first. Below is a script that shows many ways of getting the scriptname including the one in the other answers and what happens when you invoke it in different ways (I have truncated some of the output but it should be clear) **TL;DR**: The most robust solution is `"$(basename "$(realpath "$BASH_SOURCE")")"`. This however does require `realpath` installed in the system. $ cat Testdir1/subdir/dir1/Scriptname.sh #!/bin/bash set -v echo $(basename $0) echo "$0" echo "${0##*/}" echo "$(basename $BASH_SOURCE)" echo "$(basename $(readlink -nf $0))" echo "$(basename "$(realpath "$BASH_SOURCE")")" Output (truncated nested calls): $ Testdir1/subdir/dir1/Scriptname.sh echo $(basename $0) Scriptname.sh echo "$0" Testdir1/subdir/dir1/Scriptname.sh echo "${0##*/}" Scriptname.sh echo "$(basename $BASH_SOURCE)" Scriptname.sh echo "$(basename $(readlink -nf $0))" Scriptname.sh echo "$(basename "$(realpath "$BASH_SOURCE")")" Scriptname.sh Now we source the script $ . Testdir1/subdir/dir1/Scriptname.sh echo $(basename $0) bash echo "$0" /bin/bash echo "${0##*/}" bash echo "$(basename $BASH_SOURCE)" Scriptname.sh echo "$(basename $(readlink -nf $0))" bash echo "$(basename "$(realpath "$BASH_SOURCE")")" Scriptname.sh Finally we run another script that is a symbolic link to the actual script $ ls -lt Show_Scriptname lrwxrwxrwx 1 user user 102 May 23 20:45 Show_Scriptname -&gt; /home/user/Testdir1/subdir/dir1/Scriptname.sh $ ./Show_Scriptname echo $(basename $0) Show_Scriptname echo "$0" ./Show_Scriptname echo "${0##*/}" Show_Scriptname echo "$(basename $BASH_SOURCE)" Show_Scriptname echo "$(basename $(readlink -nf $0))" Scriptname.sh echo "$(basename "$(realpath "$BASH_SOURCE")")" Scriptname.sh
Wow, thank you for the fantastic and thorough response. :D
Each iteration of your loop appends (`&gt;&gt;`) to `file.csv`. To create a new file try adding the `$i`. `&gt;&gt; "file$i.csv"`. Note that if you're going from tabs to commas you'll have to escape existing fields that contain a comma as part of their value.
First thing to learn when starting out is not to fret about shit that looks complicated.... The next thing to learn is that ***you won't learn*** if every time you forget the above, you ask for someone else to explain it... Which kind of leads into the next bit... ***Don't*** be afraid to meddle with stuff to see what it does.... ***DO*** look at the **man**-pages... there is one for **everything** While the interweb is great, software changes, and the manpages on your computer are for the versions of the software on your computer... Searching Google for something and getting an answer is of no use whatsoever if the version of software that the author of the blog or forum post was using is 12 years older (or even possibly 6 months younger) than yours and your software acts differently or just refuses to run with an error since an option or argument is not valid on the version of software on your machine... Run man echo and it will give you a list of various different manpages - just hit enter to get the default... OK - now all manpages are laid out in a similar way.... You will see options - for example -n do not output the trailing newline and -e enable interpretation of backslash escapes with some "sequences" listed below it, e.g. \n new line so you can *avoid* printing the trailing newline that `echo` normally finishes with by running echo -n "hello!" and you can print a new line by running echo -e "hello! \n" which, since all `echo`s are terminated with a new line already, you get a new line straight after your one, which results in a blank line before returning to your prompt... If you don't want it, you can always run echo -e -n "hello! \n" which is a bit of a waste, since not only is it equal to echo hello! it is almost 3 times as many characters to type.... but it does show that there is more than one way to do things in Linux... Just because someone does something a different way, it doesn't automatically mean that you are "wrong" - and the reverse is also true... I was going to walk you through the other commands, but since you seem to be a user who deletes their questions, I'll close with advising you to look at man wc and man tr which is a command that you could remove from your chain knowing what you might know about `echo` by now.... as for `sed` - the manpage is ***always*** the most accurate resource for the version your commands will be typed in, but a look [here](https://www.gnu.org/software/sed/manual/html_node/The-_0022s_0022-Command.html#The-_0022s_0022-Command) is the next best thing for working out what sed -E 's/[^0]//g' does.....
IMO there are much easier ways of doing this than the complicated sed substitution. Use `awk` or `python` $ x=1001 $ echo $x | awk -F0 '{print NF - 1}' 2 $ python -c 'import sys; print sys.argv[1].count("0")' $x #even simpler from REPL 2
OP deleted their other question.... I'd not have bothered with my comment if I had noticed that before I started it...
&gt; That's quite a bit more clever than And faster
one way is to be sure the directory with the python you prefer in it comes earlier in your PATH environment variable
Don't use `ls`, you can just use globbing like below for files in /home/Jen/project1/subjects/*/file.txt do sed ‘s/[ \t]/,/g’ $files &gt;&gt; ${files%%.*}.csv done Note the above will create the `.csv` file in the directory where the `.txt` file is.
You can throw "alias python=python3" in your bash_rc or bash_profile.
The only additional thing I would add is that it's often better style to write BKUP_LOG=${BKUP_DIR}/mylog.txt so there's no ambiguity with ${BKUP}_DIR
s/ REPLACE THIS / WITH THIS /g s is the substitute command g means all occurrences of REPLACE THIS REPLACE THIS is \[\^0\] which means any character that is not 0 WITH THIS is blank so replace non-zeros for nothing which deletes them
If you just need the count of zeroes, try: printf "%s" "$string" | grep -c "0"
That does not work at all. `grep -c 0` counts the number of lines containing `0`, instead of counting the number of `0` characters, which is what the OP wants.
`find /path/to/project -name '*.txt' -exec bash - c 'var="$0"; mv "${var}" "${var%.txt}.csv"' \;` The find command gets the files, then we have to start a new shell so we can remove the '.txt'. The '%' in the parameter expension gets rid the trailing '.txt'. Haven't tested this out tho, I'm on mobile.
I'd rather use `echo "$x" | awk '{print NF}' FPAT=0`. Using `FPAT` is clearer.
Man pages or bust
+1 I remember discovering bash parameter expansion when reading the man page a few years ago. I had been using all manner of clunky work arounds until then. Discovering parameter expansion suddenly made bash feel so much cleaner. These days I wish that php etc supported parameter expansion in the same way.
won't work. `$0` will be 'bash'. using find is the right idea for this. specially if you have more than one file in each directory. then iterate in a loop over find results. for file in $(find . -type f -path '*/*.txt'); do cat ${file} &gt;&gt; file.csv # I guess you're trying # to put all files' contents into one csv done
Pretty sure you have to specify `python3` Depends how you installed python?
Ohh, if you want to append it to one csv file it gets simpler. \`find . -type f -name '\*.txt' -exec cat "{}" &gt;&gt; myfile.csv \\;\` This command will append the output of all \`.txt\` files in the current directory and it's subdirectories to \`myfile.csv\`. The \`-type f\` option is indeed a good habit.
Sorry, too quick to read :) Since it's binary, however, this could be much faster: x="000100111111000010111" y="${x//1/}" y="${#y}" Only works for binary (0 and 1).
Yeah, that works but double quotes are totally unnecessary for variable assignments, because word splitting does not occur for assignments.
That's true. It's just a (good) habit.
It's only good habit to use them for command arguments. For assignments, it's bad to chuck unnecessary quotes cause they're an eyesore.
`-path '*/*.txt'` or `-mindepth 1`, op may not want to process the current directory.
potato potato.. I don't really see them.
I also tend to suspect people don't understand word splitting when I see them use unnecessary double quotes.
Pretty much what I was gonna suggest, but I think you'd want to output the file to: `"${files%.*}.csv"` rather than `"${files%%.*}.csv"` * `${files%%.}` * Deletes everything after the *first* instance of a period * `${files%.}` * Deletes everything after the *last* instance of a period &amp;#x200B; I was actually entirely unaware of the double-percent option until just now. That will really come in handy in the future.
OP also asked how to do the task in another Reddit post. He wanted non-bash. You can do some fiddling with `IFS` to achieve the same thing in POSIX sh.
Yeah, you're right about the double percentage. In this case it wouldn't matter as there is only one period. But it is better to be safe.
explainshell.com is pretty awesome.
* `samp ='basename ${sample}'` &lt;-- use double quotes otherwise it won't work * Here's a strategy that could work: * Check that each of the filenames matches the right pattern * For those that match, extract the two numbers and call your command Here's how you could do it: #!/bin/bash REGEX="^.*[0-9]_[0-9].fastq.gz$" for sample in ./test/*.fastq.gz do samp ='basename ${sample}' [[ $samp =~ $REGEX ]] &amp;&amp; ( echo "Processing sample ${samp}" nb1=$(echo "$samp" | sed -e 's/.*\([0-9]\)_[0-9].fastq.gz/\1/g') nb2=$(echo "$samp" | sed -e 's/.*[0-9]_\([0-9]\).fastq.gz/\1/g') salmon quant -i ../indexes/mus_grcm38 -l A \ -1 $nb1 \ -2 $nb2 \ -p 10 -o quants/$sample.quant \ --validateMappings \ --gcBias ) done
 for file in $(find . -type f -name '\*\_1.fastq.gz') do file1=${file} file2=$(echo "${file}" | sed 's/_1.fastq.gz/_2.fastq.gz/') # do things with them done I didn't fully understand your question and what you want to do. This loop goes through every file with a 1 after the underscore, so in the loop file1 will be \`exp1\_1.fastq.gz\` and file2 will be \`exp1\_2.fastq.gz\`. This command also searches in subdirectories.
You can use the history command to get all recent commands by the logged in user
Yeah, in all likelihood wouldn't make a difference, but the `subjects` subdirectories could potentially cause an issue.
`which python` gives to the path of the symlink of the 2.7 version. Delete it and create a symlink in its place which points to the 3.7 version.
See this SuperUser thread for a couple ways to view user console history. What you're looking for is .bash\_history or auth.log. [https://superuser.com/questions/309434/how-to-view-command-history-of-another-user-in-linux](https://superuser.com/questions/309434/how-to-view-command-history-of-another-user-in-linux)
I've tried this but it doesn't seems valid if another user is logged in at the same time. I've read somewhere that **history** basiclly reads the content of the **.bashrc_history** file of the current user. But the thing is that I need to find the history of, let say user A and user B without logging in with any of the accounts (I'm trying to extend a SNMP MIB to be more exact). Is that even possible ?
Try looking at the bash history file in that specific users home directory?
Try looking at the bash history file in that specific users home directory?
Try looking at the bash history file in that specific users home directory?
I've tried using **auth.log**, but the problem is that it requires root acces and as I'm trying to extend a **SNMP** mib, i don't think I can access this file (the user used during a **SNMP** request is *debian-snmp* in my case). Maybe I'm wrong about it tho.
If you're not root or in sudo group then you can't access other user's history. It's simple as that. If you do have admin access you can check `/home/username/.bash_history` for each user. But note that this is limited as a user can disable/truncate their history storage by setting `HISTSIZE` or `HISTFILESIZE`. Also commands can be executed on terminal which will be ignored by history if specific patterns are used. The most reliable way would be to use something like GNU acct which logs the commands when turned on. You should of course make sure that the authorized users know that their commands are being logged.
Ok thanks, that's waht I was affraid of. Trying to read the content of **.bash_history** file of each user can't be a thing by the way ? Or do I need root access too ?
I was thinking about it but do I need root access to read thoses files ? Can user A read the user B history file ?
Yea you would need root privileges
Sorry, my knowledge stops here. I think you're going to need root/sudo to be able to get the info you want in any event though.
Ok thanks anyway, you've confirmed smthing I was affraid of.
That's what I thought, thanks anyway !
Do you admin that server? You could create a group that has read permissions on those logs and add yourself and any other appropriate users. That way you could read the logs and parse the info you need. You should disclose that to all your users though, or they should understand that their console command history is not private. That may help avoid questions or conflict over privacy.
 for sample in ./test/*_1.fastq.gz do echo "Processing ${sample##*/}" salmon quant -i ../indexes/mus_grcm38 -l A \ -1 "$sample" \ -2 "${sample%_*}_2.fastq.gz" \ -p 10 -o "quants/${sample%_*}.quant \ --validateMappings \ --gcBias done What should be in -o option ? %\_\* snips off at last \_.
Either run it in a screen session with logging enabled or pipe the ssh command to tee
 sed -n '2,$p' input_file.txt | while read -r x y; do echo "replace this echo and implement your logic using $x and $y here" done
use sudosh - [https://github.com/cloudposse/sudosh](https://github.com/cloudposse/sudosh) Make sudosh the users' login shell. It works. **sudosh** can be used as a default login shell or as a filter. sudosh takes advantage of pty devices in order to sit between the user's keyboard and a program, in this case a shell. It was designed specifically to be used in conjunction with [***sudo***](https://linux.die.net/man/8/sudo)*(8)* and allows execution of a shell with logging. It is basically a VCR and will record shell sessions and also has the ability to play back the sessions as they were originally recorded. It records all input/output, keyboard input, and timing information so that the session acn be played back in the original format.
Thank you! Your link pointed me to the right direction.
This is fundamental to the *nix user security model. You seem disappointed that non-root users aren't allowed to snoop at each other's activities. There are services like `psacct` or `auditd` when server admins have a bonafide need to record user commands, but if you're not a server admin you shouldn't be allowed to see other users' activity.
ah cool, so if i understand correctly, the x &amp; y are ID &amp; Price &amp;#x200B; would it be too much to ask what sed -n '2,$p' does? :D
https://www.tldp.org/LDP/abs/html/fto.html
You could try setting the user's default shell to tlog. https://github.com/Scribery/tlog
It seems that sudosh isn't actively developed anymore. Tlog seems more active these days and utilizes standard syslog over the binary format of sudosh. https://github.com/Scribery/tlog
This is more tricky than you might think. To test if a file exists, you use `[[ -e filename ]]` in bash, or `[ -e filename]` in sh. However, The [[ ]] keyword doesn't do pathname expansion, so `[[ -e *.png ]]` will just look for a file named literally `*.png`. With the `[` command, which follows normal parsing rules, `[ -e *.png ]` is still wrong, because the glob is expanded before the command is run, so you may end up running something like `[ -e a.png b.png c.png ]` which is an error. [BashFAQ 4](http://mywiki.wooledge.org/BashFAQ/004) explains how to do the test you are after correctly. The next problem will be the command in the `then`-block. Again pathname expansion is done before the command is run, so your command will end up as a garbled something garbled like sips -s format icns /Users/XXX/Desktop/a.png /Users/XXX/Desktop/b.png /Users/XXX/Desktop/c.png --out /Users/XXX/Desktop/bar.icns /Users/XXX/Desktop/foo.icns You likely have to run sips once per file, so you need a loop over the png files, and then the test for any png files becomes unnecessary. Just loop over them. for file in /Users/XXX/Desktop/*.png; do sips -s format icns "$file" --out "${file%.png}.icns" done See [BashFAQ 73](http://mywiki.wooledge.org/BashFAQ/073) for an explanation of what that parameter expansion (`${file%.png}`) does. Lastly, if there's no png files, you'll likely get an error message from sips that there's no such file named `*.png`. That's because if a glob does not match any filenames, it stays unchanged, so the for loop will iterate once. There are some ways to avoid that. One is enabling the `nullglob` bash option, another is to add a -e test inside the loop body. shopt -s nullglob for file in /Users/XXX/Desktop/*.png; do sips -s format icns "$file" --out "${file%.png}.icns" done or for file in /Users/XXX/Desktop/*.png; do [[ -e $file ]] || continue sips -s format icns "$file" --out "${file%.png}.icns" done
This will help: https://apple.stackexchange.com/questions/298775/cron-and-command-not-found
Sed stands for Stream EDitor. Sed loads the file into it's pattern space line by line. The `2,$p` means that it should print (p) from line 2 to the last line ($). Sed normally prints it's whole pattern space every 'cycle', so each line. That's where the `-n` flag is for, to suppress that. This is all in the man page. Especially for options like `-n` the man pages are very useful and quick.
Same as you configure the shell used for cron jobs, you can also configure the working path for all cron jobs with: PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin and if you want the mails to be sent to a specific email address: MAILTO=your@email.com
I cannot help, but I want to ask something for clarification: are you trying to make a script that accepts `-e` as a flag?
Add on the top of fhe script, after shellbang: source /home/pi/.bashrc
Awesome, cheers!
I know a way to get auditd to log commands run by a user, if you put that together with the time a user logged in you can get the result you are looking for, let me know if you want that config
I think OP is attempting to test for existence without the test brackets
# Here's what I would do. It's goofy, but functional. if [[ $(ls -lh *.png|wc -l) -gt 0 ]]; then
Just wondering, is there any other way to do this? If so I'd greatly appreciate it!
I haven't studied my bash very well yet, but can't you do: &amp;nbsp; declare -A myarray ; while read id price ; do myarray [$id]=$price; done &lt; &lt;(tail -something my file) Where with tail you read everything but the last. Of you can work with file descriptors, and read in the first line before the loop and not save it. (Sorry for bad formatting, replying from Mobile)
Depending on what you're doing on each row, this could be a perfect opportunity to use \`awk\`.
This might help: https://askubuntu.com/questions/85558/verify-if-crontab-works
A couple of alternatives to that sed command: # delete line 1 with sed sed -e 1d # tail starting from line 2 tail -n +2
Try using the ssmtp full path. Do the command which ssmtp to get the full path. Or source like other users are saying.
One more! while read id price do [ "$id" = ID ] &amp;&amp; continue ...processing... done &lt; input
I should start using (:terminal) more often, indeed.
It's in the line `movies=\`...\`` \- all your results are concatenated into a single line. You might use an array. Try this: movies=( `...` ) echo "${movies[@]}" | while ...
So I tried your suggestion. Here's the result: 13 Abby (1974) https://player.vimeo.com/external/308141728.hd.mp4?s=c94f941ccd57cdb4a9ea3acb1f9247cba00a1399&amp;profile_id=174 01:28:54 2018-12-30 0 14 Abraxas, Guardian of the Universe (1990) https://player.vimeo.com/external/305522143.hd.mp4?s=b84cf299ff3ab7c26f61d2217f8a84ff29286199&amp;profile_id=174 01:26:43 2019-03-03 0 15 Alice, Sweet Alice (1976) https://player.vimeo.com/external/328210348.hd.mp4?s=efc2bda77baefd9bb5467ab40d971ca3bc3136ed&amp;profile_id=174 01:46:31 2019-04-07 0 21 Alison's Birthday (1981) https://player.vimeo.com/external/337290335.hd.mp4?s=06402316f340be62251343af1c7af65b8d857b80&amp;profile_id=174 01:32:25 2019-05-26 0 26 Ankle Biters (2002) https://player.vimeo.com/external/311367127.hd.mp4?s=b6089411ce6b66d1c1b210a00242cc8627edca9c&amp;profile_id=174 01:20:56 2019-04-14 0 28 Assignment - Outer Space (1960) https://player.vimeo.com/external/302863512.hd.mp4?s=552b11476dc8bf0080eb77c93cc84c758d30a62e&amp;profile_id=174 01:12:44 2018-12-02 0 29 Asylum (1972) https://player.vimeo.com/external/337291663.sd.mp4?s=80a17af3bedd71088e3b003c4f6559b559a8cf8c&amp;profile_id=165 01:28:58 2019-05-26 0 33 Attack From Space (1965) https://player.vimeo.com/external/305529475.hd.mp4?s=7a7dacf11104bcb94dd0e0dcf48c369c335862b2&amp;profile_id=174 01:15:33 2018-12-16 0 37 Attack of the Puppet People (1958) https://player.vimeo.com/external/311358778.hd.mp4?s=5ec953958f3d81c286fee62970a7a0c671c7f7ec&amp;profile_id=174 01:19:21 2019-01-20 0 439 The Alligator People (1959) https://player.vimeo.com/external/300335699.hd.mp4?s=fa47544bb2efdf227e0511e9871663632073a044&amp;profile_id=174 01:14:17 2018-11-18 0 443 The Amazing Transparent Man (1960) https://player.vimeo.com/external/333099710.hd.mp4?s=425b4c2610f1d61968dce3d9889f73c2710eeb7e&amp;profile_id=174 01:20:21 2019-05-05 0 449 The Atomic Brain (1963) https://player.vimeo.com/external/316947161.hd.mp4?s=49ed3dec3adeb3cae3875f2944ef50658ea9af7d&amp;profile_id=174 01:33:37 2019-02-17 0 Changing it to an array put it all on one line. The way I had it before gave me 12 lines.
This might actually be a pretty good solution. I do have curl, in fact that's how I'm performing the file download. The thing is that, the only way for the files to have the exact same bit size is if someone removed and added exactly the same number of characters (although they might be different). This could be a pretty good tradeoff between performance and usability. In this case, removing would be a pretty rare scenario AND removing and adding an even rarer. I might go with this one, and then diff if the files size is different. 99.9% of the times, this will result in no deltas, so making any kind of computation is definitively something that's far from ideal, especially with my polling times and the filesize growing
You can add a newline for each element by using `printf '%s\n' "${movies[@]}"`instead. I would recommend you to just use `for line in "${movies[@]}"` though.
Considering that one of the primary fields definitely contains spaces, that would be a bad thing.
Have you used shellcheck? This specifically will check for premature loop exits like this.
I just tried it. Absolute waste of time. All it did was complain about my use of cd without " || exit" or " || return" neither of which have any bearing on the issue.
Sorry. It had been useful for me in similar cases.
The script is sound, it just doesn't loop. That's the one issue.
Will SELECT CHAR(10) + * FROM... work? You can put it between fields selected, not sure if `* + CHAR(10) FROM` works... You'd probably want to increase that `tail` to account for the extra line... 392 films, huh?
Something reads all the input you pipe into the while, inside the while. First time people encounter this, it takes quite some time to realize. `ssh` does this for example unless you `&lt;/dev/null` or use `-n` flag for `ssh`. I would check the `scp`.
No change whatsoever.
Far more than 392 films, actually. Hence the limiters on the SQL string, which isn't the issue here. Adding the CHAR(10) bit just threw a MySQL error.
If you comment out both scp and ssh lines, what happens?
does the second loop use the second `ssh` command?
rats, soz... ahh... 609
It loops through the entries in the database with the only output being the /tmp/movie.txt file. It was the ssh lines. Added -n and it works.
The script is not sound if it's not functioning as intended... A little confused by your thought process.
&gt; "efficiency experts" So, I guess that means that you are aware of the terrible things you are doing.
Yes.
The script works, which is the intended goal. It might not be the epitome of efficiency or beauty, but it works.
Good collection. Surprised you didn't have The Cabinet of Dr. Caligari in there, or doesn't it meet certain criteria?
You can use dialog if that's the thing you're looking for
Oh, we do. It just hasn't been in rotation yet. This is populating that site with movies we've aired (went on the air Oct 31, 2018). We have close to 3,500 movies, old TV episodes, cartoons, shorts, etc. 12 movies per week enter rotation, and the viewers vote for the worst one, which gets yanked from rotation and NEVER aired again (we ship it off to The Farm). Saturday (and part of Friday, now) is special, we show Horror Host movies for 24 hours. 209,000 viewers a month, so we're doing something right. :) If you're interested, we're at OtherWorlds.tv.
may well do that...
You can using ascii esparce sequences, but it’s a little tricky Alternatively, use ncurses
Idk how to do arrow key selection, but you can make a script, menuSelection, then echo all of your options with a number next to them. 1. Enter your name. 2. Make an email. Etc. Then what I did was a bunch of if/else statements for if the user inputs the corresponding number to launch that shell script. The final else statement is just in case they type letters or a number that doesn’t exist: clear, ./menuSelection, and echo Please make a selection from the given options. Note: I’m on mobile so sorry there is no formatting in this post. But hopefully it helps. Not what you asked for, but another way to make a menu.
Sadly it's full screen and you loos context of previous output
I wrote 'SelectItem()' function but it doesn't use arrow keys, user must type a number. Example output: `./t` `Here is a list of items:` `1. 'One'` `2. 'Two'` `3. 'Three'` &amp;#x200B; `Select a number (1..3, 0: Exit):`
 if [ "$spacecount" &gt; "0" ] is your problem... https://www.tldp.org/LDP/abs/html/comparison-ops.html You are mixing your brackets up .... or your comparison operators... if (("$spacecount" )); then.... should do...
It can be done in pure bash. See [fff](https://github.com/dylanaraps/fff) which does effectively the same thing (arrow-key selection menu). The code is fairly documented so you should be able to apply the same techniques to your code.
That was definitely it. I see why it was doing that after you pointed that out. Sorry, I am new to bash scripting and still learning! All the different \[ \], \[\[ \]\], ( ), and (( )) can get confusing. Thanks for your help
Dialog is the closest I've ever seen to a TUI in bash. I've used it to great effect in building end-user login interfaces for our help desk users to shut down servers and run after-boot diagnostics. https://bash.cyberciti.biz/guide/Bash_display_dialog_boxes
I use dmenu for the, it is extremely useful.
They do take a bit of getting used to... Keep that link - https://www.tldp.org/LDP/abs/html/index.html handy, along with the slightly brusquer `bash` manual - https://www.gnu.org/software/bash/manual/html_node/index.html They are easier for a newcomer to navigate on a web page than using the manpages and control keys - but remember, the correct manuals for the correct version of the software that you are currently using is sitting not more than a few feet away.. 90% of commands have manpages that can easily be accessed without needing to search using the `/` key (the same character that you will use for searching in `sed`, `awk` and can even do in `firefox`), but it's a good habit to get into, and will make manpages a dream, not a nightmare...
Add "rm '0'" to finish function
you monster
If it works, then who cares?
Short answer: no. Use dialog, ncurses or a simple case menu based on [F]irst letter menus or numbers. Long answer: yes. You can use arrow keys, they input 3 chars so it's a hack to use read with very short timeouts. See: https://stackoverflow.com/a/11759139
Just making sure. Bummer that you dont want to learn proper techniques but I guess knowing, that there are better ways, is the first step.
Go to the directory above it and `chmod +x` the directory. If you `chown`ed it, you'll need to "undo" that too
You're a saint.
https://github.com/junegunn/fzf
I believe this should work for the alias. (I've never used any react stuff, so I assume the command is "create-react-app DIR".) `alias cra="create-react-app $1"` "$1" is the equivalent of the first argument you put into it. So `cra test` should expand to `create-react-app test`.
Most has been mentioned but to summarize and complement the list of options: * [slmenu](https://github.com/joshaw/slmenu) - slmenu is a dmenu clone for the console. * [smenu](https://github.com/p-gen/smenu) - smenu is a selection filter just like `sed` is an editing filter. * [fzf](https://github.com/junegunn/fzf) - fzf is a general-purpose command-line fuzzy finder. * Use `read`
This works without the quotes. Neat. Thank you.
Hm I'm now getting: -bash: alias: .: not found alias cra=create-react-app . $1 &amp;&amp; cd $1 Is the script in question.
I believe the quotes are required for aliases, as it works fine for me. (I tested with `echo` instead of `create-react-app`, but it shouldn't make a difference.)
I see, I guess it has something to do with the second part, which is &amp;&amp; cd $1.
Note the caveats when using the above - mainly if the directory name has any spaces, each word will count as an argument. Which usually has unintended side effects.
Couldn't you escape the spaces with `cra name\ with\ spaces`? Or just do `cra "name with spaces"`?
Please do not do this. Very few command line interfaces are improved by menus, as opposed to flags.
Aliases don't have arguments, so use a function instead. If you currently have an alias with that name set, remove it with `unalias cra`, else the following function definition will give a syntax error cra() { create-react-app "$1" &amp;&amp; cd "$1"; }
My script actually supports flags =) I just wanted to offer a simpler experience for newbies.
&gt; Aliases don't have arguments What makes you say that? I have a lot of aliases in my .bashrc that use arguments and all of them work perfectly fine.
What is the avg “good” ping for players in smash ultimate? I wonder if I can run this on my netgear nighthawk.
Functions are just convenient groupings of commands. They don't run in a separate process so they don't have their own PID. You will have to use an alternative, like a subshell, if you want it to have a unique PID.
They only seem to work. For example, this alias alias e='echo $1' seems to work okay: $ e blah blah but what actually happens is this: `$1` expands to the empty string, and blah just gets appended to the command. Aliases are simple string substitution. Try to see what happens with an argument here: alias e='echo "&lt;$1&gt;"' and then use it: $ e blah &lt;&gt; blah So if your alias just substitutes something that takes parameters, that's okay: alias e='echo' works; but if you want a function that processes the parameters, you can't use an alias. See also [the manual](https://www.gnu.org/software/bash/manual/html_node/Aliases.html): &gt; There is no mechanism for using arguments in the replacement text, as in `csh`. If arguments are needed, a shell function should be used (see [Shell Functions](https://www.gnu.org/software/bash/manual/html_node/Shell-Functions.html#Shell-Functions)).
Huh, I guess I learned something new today. Thanks for all the info.
I have one written in pure Bash: https://github.com/Crestwave/shmenu. It's &lt;150 lines, so you can embed it in a program.
$$ is the environment variable for the pid of the current process. $ echo "$$"; perl -e 'print $$' 314 351 $ echo "$$"; perl -e 'print $$' 314 352
Note the Cursor Movement section: http://www.tldp.org/HOWTO/Bash-Prompt-HOWTO/c327.html
Today I learned. Thanks
&gt;What is the avg “good” ping for players in smash ultimate? In [this video](https://youtu.be/F7XfFheooUU), the first opponent reports a ping of about 31 ms. For new users, I recommend setting your ping limit to 100 and run LagDrop with smart mode enabled to determine a ping time limit best for you. Setting it up is simpler than it looks. Let me know if you need any clarification about the set up process or if I missed anything that should be included in the instructions. &gt;Also is there any security issues that are presented when using this? No security issues that I'm aware of. All operation occurs on the router hosting LagDrop and no data is transferred except for the random bits used for pinging. Please share if you like it!
Thanks! I’ll report back if I run into any trouble. This will really help out me and a few of my friends!
You can also run \`pidof\` to find the PID of other apps.
Jesus Christ why would you want to do this?!?!?! Although I’m sure it’s possible with some weird magic.
Do you just want to reproduce the code from the header in bash or do you want an executable to be able to share objects with you bash script ?
basic shell scripting assignments that I gotta do for class. Turns out to be I could just make a .cpp executable file that the shell could run. All c++ OOP logic can be implemented in the executable.
&gt;source /home/pi/.bashrc Thank you ! I added the PATH and its working now.
Oh, cool! Glad you figured it out!
Thank you so much for the help!
I've been talking to my supervisor and told me that the document describing the TODO is two years old and he decided that it would be usefull enough to get the output of the *history* command. Therefore I've finished my project and the problem I've had isn't one anymore. Thank to all of you that took some time to help a trainee in need.
 mkdir -p "$1" With `-p`, not only will mkdir make all parent directories, it will exit true if the directoy already exists.
Okay, so you want a different action if a paths' parent doesn't exist? I think the simplest way is: if [ -d "$(dirname "$1")" ]; then mkdir -p "$1" # with -p, mkdir will exit true if it already exists # action A else # action B fi
Perhaps like this: if [[ ! -d $(dirname "$1") ]]; then # do a different action else if [[ ! -d $1 ]]; then mkdir "$1" fi # do action fi This would solve the problem of having to repeat the action. I'm thinking this is not a very good idea as it seems a bit hard to read/understand. Perhaps better would be to put the action into a function so that repeating it can be done by calling the function: do_action() { # do action } if [[ -d $1 ]]; then do_action elif [[ -d $(dirname "$1") ]]; then mkdir "$1" do_action else # do a different action fi Another idea, there's no error for an existing directory when you use `mkdir -p`, so a line with `mkdir -p` you could always run without checking. The problem is that `-p` will create the parent directories if they are not there, so what you would have to check would be the parent dir existing. Making use of `-p` you could organize things like this: if [[ -d $(dirname "$1") ]]; then mkdir -p "$1" # do action else # do a different action fi
I haven't used dirname much, but based on some quick messing around I was just doing, is your else ever even hit? With dirname, even if the path you enter is complete garbage/nonexistent, it will still return '.', which will be true for the -d check. Again, I may be totally off.
 if [ -d "$1" ] || mkdir -- "$1" 2&gt;/dev/null; then ... else ... fi
If all you want to do is to create a directory or not depending on the existence of parents then you can use simple short-circuiting. For more complex code a proper structure would be desired. [ -d "$1" ] || [ -d "$(dirname "$1")" ] &amp;&amp; mkdir "$1" The above will do nothing if \`$1\` exists else will check if parent exists and only then will create the directory.
As long as the path specified contains a `/` (which is what the script expects a user to do), then it will process the path as expected and therefore the else part can be reached. Otherwise, you're right.
This seems to be the best answer. Can't believe I overlooked the part where I can just check for parent directory existence... Is the parentheses used in the first statement used for the purpose of grouping commands for the conditional expression? How is it different from using braces `{}` to group commands, if the latter can even be substituted for the parentheses in this example?
Yes, you can spawn a subshell: ( my_function )&amp; my_fpid=$!
The parentheses run the grouped commands in a *subshell* whereas the braces run it in the *current shell*. In this case there isn't much difference as neither of these commands are dependent on or are affected by the shell environment. If you want the commands to run in the current environment then you should use braces (don't forget the semicolon to terminate the command before the closing brace).
When deciding whether to use subshell or stick with current shell, the latter is preferred for performance reasons unless you want a subshell is desirable for environment purposes?
Omg this would itch my stubbornness to death
Performance is not the issue here (it maybe in other cases where there is deep nesting of subshells). The point I was making in the previous comment is that if the code in the subshell is dependent on environmental variables that were changed in the parent shell (for example `$SHELL`, `$PATH` etc) then those changes will not be seen in the subshell. The main idea of using subshell is keeping the environment of the code separate from the parent shell. Also, if the code in the subshell changes an environment variable, that will not be seen by the parent shell. So if you want the grouped code to use the current environment, braces are preferred otherwise a subshell. Apart from environment braces should als be used when for example you want to exit the script if some conditions are not met. Exiting the subshell will not exit the parent shell. See the bash manual for [subshells](https://www.tldp.org/LDP/abs/html/subshells.html) and [braces](https://www.tldp.org/LDP/abs/html/special-chars.html#CODEBLOCKREF) for an in-depth explanation.
Yep. Regex.
It can probably be done in bash but you probably want to use an xpath parser to do the work for you. Something like xmllint will do it if your html is also XML: https://stackoverflow.com/questions/14045584/parsing-for-data-in-html-using-xpath-in-a-shell-script
Awesome. Thank you!
Obligatory [dont use regex to parse HTML link to StackOverflow](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454).
You'll need a 3rd-party utility to do it efficiently —— I've used pup in the past and it does a great job: https://github.com/ericchiang/pup
I knew I'd find this masterpiece somwhere in the comments. :-)
I didn't expect to find the masterpiece of your username in the comments.
I'm always happy when someone notices!
Yeah, but just for this one thing...
Take a look at python and Beautiful Soup
Of course there is! But are you sure that's the right way to approach this task?
Please correct me if I am wrong.
 echo '&lt;table&gt;&lt;tr&gt;&lt;th&gt;foo&lt;th&gt;&lt;th&gt;bar&lt;/th&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;x1&lt;/td&gt;&lt;td&gt;x2&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td&gt;1b&lt;/td&gt;&lt;td&gt;2b&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;' | perl -Mojo -E 'say x(&lt;&gt;)-&gt;at("tr:nth-child(2) td:nth-child(1)")-&gt;text' Mojolicious contains a fairly decent XML/HTML parser, its https://metacpan.org/pod/Mojo::DOM module supports CSS3 selectors, and doesn't need anything more than Perl to be installed. If your system is Linux or OSX, you already have Perl installed and Mojolicious is just one module install away. If you're on Windows, installing Strawberry Perl installs Mojolicious, too.
If you write a custom tool, it's bound for obsoletion. If instead you apply a standard tool (like an xml parser), you will have learned something useful. I know it's possible to do what you asked for, I have written HTML "readers and writers" in bash. But it's not a good strategy.
Thank you. I will look into the options you have provided.
Thank you
Oh, it can be done using less , sed or tr . Think of it .
Look at the second part of the first line: [ "$1" != "JEFF-WEB" ] That says [ "JEFFOLD10-WEB" != "JEFF-WEB" ] Which is true.
The string comparisons are working fine. Your use of quotes is correct as well. There's an error in your logic, though. Remember that `||` is a logical OR. So the condition on the first line is always true. Your parameter is always going to be either unlike the first string, or unlike the second. The opposite of a logical OR is a logical AND, which in shell uses the operator `&amp;&amp;`.
Look up the join command.
Well if your string is "JEFF-WEB", then... 1. It does NOT equal "JEFFOLD10-WEB" so the condition is true 2. It DOES equal "JEFF-WEB" so the condition is true Sounds like you want "&amp;&amp;" (and) instead of "||" (or) in your first if-statement.
bad logic, should just be: if [ $1 == "JEFFOLD10-WEB" ] || [ $1 == "JEFF-WEB" ]; then echo "this is a test system" else echo "this is not a test system" fi
When you arrive at these problems, try: bash -xv myscript.sh &lt;arguments&gt; to see what it's doing.
Apart from the logical errors pointed out by other answers, you can considerably simplify your code by using Bash regular expression matching using `[[...]]` operators as below $ Var=JEFFOLD10-WEB $ [[ "$Var" =~ ^JEFF.*-WEB$ ]] &amp;&amp; printf "This is a test system\n$Var\n" || \ printf "This is not a test system\n$Var\n" This is a test system JEFFOLD10-WEB Or inside a function this can be coded as Test_System () { if [[ "$1" =~ ^JEFF.*-WEB$ ]]; then printf "This is a test system\n$1\n" else printf "This is not a test system\n$1\n" fi } $ Test_System JEFFOLD10-WEB This is a test system JEFFOLD10-WEB $ Test_System FOLD10JEFF-WEB This is not a test system FOLD10JEFF-WEB
The command you are looking for is `join -t, -a 1 -a 2 file1.txt file2.txt` Edit: explanation of parameters: `-t,`: separate on comma (instead of default space) `-a 1`: include everything from file 1, not just those keys also in file 2 `-a 2`: include everything from file 2, not just those keys also in file 1
meaning that the entire expression is true... short circuiting... :D
I second this. There's also shellcheck
For the sake of completeness, `case` is another option, which I would prefer. case "$1" in (JEFF*-WEB) echo foo ;; (*) echo bar ;; esac
Thanks for the response! How different would this be if the names where on the second column and the values on the 1st in both?
`if [[$(pulsemixer --get-mute) = "1"]]; then` should read `if [[ $(pulsemixer --get-mute) = "1" ]]; then` (note the spaces after / before the `[[`/ `]]`
It was really that simple. Thank you so much I have been messing with this for a long time.
This should work: [https://bpaste.net/show/45eff610a863](https://bpaste.net/show/45eff610a863)
This actually doesn't it prints both symbols one after the other but thanks for trying.
Here's what I've been using: printf "printf '%s\\\n%s\\\n' | passwd %s\n" \
One more thing, naming a function the same as a built-in command (test) is not a good idea either.
Not very different at all. The command you are looking for is `man join`
As others pointed out, you are using a **OR** (`||`) operator in your conditions, therefore if one part is true the rest of the condition is also true. If you use a **AND** (`&amp;&amp;`) operator it will work much better.
Using awk can be usefull too
First expression is always true
https://web.archive.org/web/20190313210055/https://www.cse.msu.edu/~cse232/FAQ/bash_cheat_sheet.pdf
That's pretty funny 99% of that PDF is "\*nix" command cheat sheet and have nothing to do with "bash" or any shell for that matter. This is a "bash" cheat sheet: [https://devhints.io/bash](https://devhints.io/bash)
Replace foo with bar on line 3: \`awk 'NR == 3 { gsub("foo","bar") } { print }'\` Since the file only contains \`foo\` that does the job.
That performs the substitution on the third line of the input, whether or not it is actually the third occurrence of `foo`.
You can try to chain some seds, like sed | sed | sed, to replace first 3 lines to something else, but it is just a workaround, not a "proper" solution. Maybe perl expressions or awk can achieve this
Author and/or OP probably confused Linux shell and BASH.
Assuming you're using GNU Sed, you could probably get away with using the `-z` (aka `--null-data` or `--zero-terminated`) option. This changes Sed's behaviour so that input lines are null-terminated rather than newline-terminated... so if your input file doesn't contain any null characters, the whole file would be a single "line". It does mean Sed has to read the entire file into memory, however, so it's only suitable for reasonably-sized files.
i dont know dude, my computer networks instructor put up that link but that link was a dead end. he also put up the devhints
thank you very much!
Just the third occurrence or every third occurrence? In case of former just use `sed -i "3s/foo/bar/" file` and it should work.
So what does the file actually look like?
You can't really do this with 'sed'. You'll need a different tool. Is there only ever a single occurrence of "foo" per line? Using 'awk', you can count the lines and then do a replacement on the third count. It would look like this: Here's my test file: $ cat testfile foo foo foo foo foo Here's 'awk' counting the lines, doing a replacement on the third occurrence, and printing all lines: $ awk '/foo/ { count++ }; count == 3 { gsub(/foo/, "bar") }; { print }' testfile foo foo foo bar foo If there can be several "foo" per line and you want to count all of them, here's a way to do that with 'perl': $ perl -pE 's/foo/ ++$count == 3 ? "bar" : $&amp; /ge' testfile foo foo bar foo foo To edit the file, perl has the same `-i` parameter as sed: perl -i -pE ... The GNU version of awk ("gawk") can edit a file like this: awk -i inplace ...
`sed -i '3s/foo/bar/' file` to change the file in-place. `sed '3s/foo/bar/' inputfile &gt; outputfile` to write to another file.
Thank you, this worked :)
The sed `'s/pattern/replacement/&lt;number&gt;'` only applies to a particular line. Sed is a line based editor so it applies the command to each line. It does not keep track of the lines. One workaround is to use `-z` flag as suggested by the other answer but it would not work for cases where there is `foo` in the lines before the third line that starts with `foo`. Use `awk` for this. See below $ cat sed_subs foo baz foo baz,foo foo foo foo $ awk '/^foo/{m+=1; if (m == 3) sub(/foo/,"bar",$0)}1' sed_subs foo baz foo baz,foo bar foo foo The sed command with `-z` flag will fail in this case $ sed -z 's/foo/bar/3' sed_subs foo baz foo baz,bar foo foo foo On the other hand if you just want to replace the third occurrence use the sed command or just modify the awk command as `awk '/foo/{m+=1; if (m == 3) sub(/foo/,"bar",$0)}1' sed_subs`
You can use grep to find the line number of the 3rd occurence, then construct a sed command dynamically to replace in only that one line. `line=$(grep -n -m 3 'foo' testfile | tail -1 | cut -d: -f1)` `sed -e "${line} s/foo/bar/" testfile`
Thanks, I'll give it a try.
Thanks, that a great sheet!!
 printf "printf '%s\\\n%s\\\n' | passwd %s\n" "$USER_PASS" "$USER_PASS" "$USER_NAME" | arch-chroot "$MOUNT" Did you come up with this? I'm trying to figure out how it works.
Sorry for the delay - thanks for the answer =). Now my question is: when does the 2nd instruction get executed ? **while** or **after** the first? Thanks again.
The ampersand executes the subshell in the background, and $! is the PID of the last command. So that pid would be the pid of the subshell. A subsequent: kill $my_fpid would kill the entire subshell. Does that answer your question?
Can they overlap ? Given babababa, replacing 3rd baba with x, no change or babax ?
They're commands which need to be run from a shell. And the default shell in Linux is bash. So from a certain point of view, these are bash commands. But I'm sure you know all that, although a lot of newbies don't. But the devhints sheet is great.
Neat.
Heia Norge!
Although I can see how someone new to Linux and any *nix system might think that ... that’s the same reasoning upon which there are some who still believe the earth is flat. Neither, sadly, is true. The world more interesting than that.
Yup! Thanks again!
Man, your well commented code was such a pleasure to read!
Look into AppleScript (/[osascript](http://support.moonpoint.com/os/os-x/man/man_osascript.html)) for window manipulation.
Dude, you are just rewriting sacad: https://github.com/desbma/sacad
I thought this book hasn’t been released yet...?
Okay, i'll give it a shot, note that there might be tools especially for this that is more efficient, this is just what i came up with. I'd get a list of all the files, and for each extract the album name and the number, then create a folder with the name if it does not already exists, then copy/move the file into the directory with the number name, So in practice ``` for fname in `ls`; do aname=`echo $fname | sed -E "s_^(.*?)-[0-9]+\..+_\1_"` fnum=`echo $fname | sed -E "s_^.*-([0-9]+\..*)_\1_"` mkdir $aname cp $fname $aname/$fnum done ``` It is late, and i'm on mobile, so might be some errors in there, and far from the optimal way of doing it probably, but should get the job done, which is the most important part
Thanks Europe?
Yeah, it's my creation. I'll try to explain my reasoning... &amp;nbsp; The problem with your chroot example with variables is that the `echo` command executes inside the chroot. Inside the chroot, there is no `${Username}` or `${Password}` defined. Those live in the calling shell. &amp;nbsp; What I'm doing instead is printing the commands I want to run inside the chroot and then piping them to the chroot. This way I can print script variables and they are recognized because the variables live in the current shell. The `\\\n` translates to a newline because it's inside a nested `printf` statement (yes, I probably could use `yes` instead, but I don't). &amp;nbsp; So, what the command is doing is printing `printf '$USER_PASS\n$USER_PASS\n' | passwd "$USER_NAME"` *inside the current shell (with the variables resolved)* and piping that to the chroot.
I think it should be `sed -E ...` for extended regex, since you're not escaping the brackets and using a '+' . I'm also on mobile though.
There arent any brackets that are supposed to be escaped though? You might be right on the `+` though, i forget what's only avaliable with extended regex and not, i'll fix it
I thought that to form a group it should be with `\(...\)` without extended regex but you could be right.
Oh, is that a thing too? I usually only work with extended regex, so this is new to me Good to know, thanks
No, megabjarne Jk, yeah, europe
You could use the "perl-rename" tool. I think it's called just "rename" on Debian/Ubuntu. perl-rename --dry-run -v 's{-(\d+[.][^.]+)$}{/$1}' *jpg The `--dry-run` makes it only print what it would do. You would remove it to actually do the change. Here's how it looked like in my testing: $ touch testfile-{001..003}.jpg another-test-{01..03}.jpg some-other-test-{001..003}.jpg $ perl-rename --dry-run 's{-(\d+[.][^.]+)$}{/$1}' *jpg another-test-01.jpg -&gt; another-test/01.jpg another-test-02.jpg -&gt; another-test/02.jpg another-test-03.jpg -&gt; another-test/03.jpg some-other-test-001.jpg -&gt; some-other-test/001.jpg some-other-test-002.jpg -&gt; some-other-test/002.jpg some-other-test-003.jpg -&gt; some-other-test/003.jpg testfile-001.jpg -&gt; testfile/001.jpg testfile-002.jpg -&gt; testfile/002.jpg testfile-003.jpg -&gt; testfile/003.jpg The pattern also seems to behave well if there's a path in the file names: $ perl-rename --dry-run 's{-(\d+[.][^.]+)$}{/$1}' /tmp/t/*.jpg /tmp/t/another-test-01.jpg -&gt; /tmp/t/another-test/01.jpg ... About how to do the same with bash alone, this here seems like it should work: $ for f in *.jpg; do echo mkdir -p "${f%-*}"; echo mv "$f" "${f%-*}/${f##*-}"; done mkdir -p another-test mv another-test-01.jpg another-test/01.jpg mkdir -p another-test mv another-test-02.jpg another-test/02.jpg ... The two "echo" words would have to be removed to actually run the commands, how it's shown here it just prints text.
It's bad to write something like this because it can't deal with spaces in filenames: for fname in `ls` What you should do instead is, use bash's `*` wildcard instead of the `ls` command, like this: for fname in * Bash will then be able to deal with spaces because that `*` is a part of bash itself and not an external command like `ls`.
Don't parse the output of `ls`, use "globbing". In this case, you can use "parameter expansion" instead of `sed`. for img in *.jpg; do album="${img%-*}" newname="${img##*-}" mkdir -p -- "$album" mv -i -- "$file" "$album/$newname" done
Using pure bash substring removal (lines broken up for clarity) $ ls Image_Dir/ album-name-0010.jpg album-name-002.jpg album-name-004.jpg album-name-006.jpg album-name-008.jpg album-name-001.jpg album-name-003.jpg album-name-005.jpg album-name-007.jpg album-name-009.jpg $ for files in Image_Dir/* &gt; do &gt; Dirname="${files%-*.jpg}" &gt; Filename=${files##*-} &gt; mkdir -p $Dirname &gt; mv $files $Dirname/$Filename &gt; done $ ls -R Image_Dir/ Image_Dir/: album-name Image_Dir/album-name: 0010.jpg 001.jpg 002.jpg 003.jpg 004.jpg 005.jpg 006.jpg 007.jpg 008.jpg 009.jpg
 for file in *.jpg; do mkdir -p "${file%-*}" mv -v "$file" "$_/${file##*-}" done * `${variable%pattern}` remove right-most pattern from variable * `${variable##pattern}` remove all left-most patterns from variable * `$_` the last (right-most) argument of the previous command
 awk '{print NR==3 ? "bar" : $0}' If the Number Record (line number) is 3, print "bar", otherwise print the line as is.
Thanks for the detailed explanation! As a self taught, limited skills hobbyist scripting enthusiast, I tend to better understand and remember new concepts by breaking things down, running it in the terminal or small test scripts. This is definitely one of those cases. Earlier today, found and downloaded the Secrets of “printf ” by Professor Don Colton. Looks pretty basic, to the point and most importantly only 6 pages! Looking forward to playing around with your code and possibly learning some printf in the process.
Is `$_` same as `!$`?
You can use the extension here: https://addons.mozilla.org/en-US/firefox/addon/tab-auto-refresh/
Ay real quick, somewhat new to this, why do you pick bash over zsh? Just curious as I thought the purpose of zsh was to be a fast prompt but recently I’ve been seeing ppl say that they didn’t like it, specifically oh my zsh.
quick and dirty killall firefox. firefox &lt;url&gt;
This stackexchange page has some interesting pointers: https://unix.stackexchange.com/questions/37258/refresh-reload-active-browser-tab-from-command-line
Look at [entr](http://eradman.com/entrproject/). The website even has an xdotool script to reload the current page.
yeah thanks buddy... dis has the same thing .. i was lokin for
- `bash` is already installed by default on most distros. This means I can (usually) make a script on my PC, take it to another device with a different distro, then be able to run the said script without worrying too much about compatibility. - I don't even use a lot of what bash already offers. If I feel like getting a little experimental there is still so much more I can do with bash. So I have no need for `zsh` due to "lack of functionality" (which is a reason I see all too often). - `oh-my-zsh` (as well as the actual shell) is just so overly bloated. There is no reason for a shell (or it's configurations) to be *that* bloated. - `zsh` is not fast. At least when I used it, I found it to be slower. - I just really don't like `zsh`. It's one of those things that just "feels off" to use. If that makes any sense. - I might've missed a few other things (I literally just woke up), so I find out I did I'll either edit this comment or reply to you again.
Hvordan fant du ut av det? Musikken? Eller GitHub? :) &amp;#x200B; Btw, hvor i Norge er du fra?
You need to regex it up. I think this might work? Not tested find . -regex '\\./\[a-z\\-\]\\{3\\}p' -exec find {} -name D30 \\;
Worked perfectly.
I don't think ash supports exporting functions. Write it as a separate script, embed it in the `ash -c` call, or use a `while read` loop to process results instead
 find ./???p/archive/D30/ -type d -name folder-you-are-looking-for ?
Search for browsersync
 find . -path './????/archive/D30' -type d -maxdepth 3 unlike `-name`, `-path` accepts slashes. `?` is any 1 symbol, you can just put asterisk if you don't really care about first directory length.
That works. I did not know the path could take a pattern. That's exactly the kind of info I was hoping for. Thanks! edit: I threw in a "-maxdepth 1", which I think sped up the search.
Ah, good to know. Thanks!
Thank you, the while read did the trick.
Så det på GitHub;) Er fra Nordens Paris 😎
&gt; I did not know the path could take a pattern your entered command is expanded first - including globs - `find` is not doing the work for your basedir in this case - `bash` is... unlike /u/skidnik's answer - where `find` *is* doing the work Put an `echo` before each answer to see the difference...
`!` is for [History Expansion](https://catonmat.net/ftp/bash-history-cheat-sheet.pdf) which is for interactive command line usage. See `man bash`, `/^history e` henry@rose:~ cat &lt;&lt;EOF &gt;foobar &gt; echo foo bar &gt; echo !$ &gt; EOF henry@rose:~ . foobar foo bar !$ edit: see `man bash`, `/^ *special p` for Special Parameters like `$_`
Thanks for the information!
Have you considered locate instead?
you're testing for definedness/existence of the variables "$Japan" and "$China" - after you've defined them with the read statement. It would be better to read into a temporary variable then compare the content with string comparison operators. Also, a case statement would be less persnickety than a long series of if's.
Da kjenner du han her, https://www.youtube.com/watch?v=HGWt58WkEKw
I think the problem is that you've misunderstood how `read` works. `read` takes user input and splits it to the given variables, in your case China and Japan. So if user inputs `foo bar baz`, China would be set to `foo` and Japan would be set to `bar baz`. Also instead of making a function named loop, that just echoes something probably won't have the effect you want. Bash has loop constructs like `for` and `while`, use those instead. Also if you want to check if a variable is one of several options you might want to use a case instead of multiple if statements. http://ix.io/1Kw0
Hi! the problem is related to the 2 variables called china and japan, after your read. I think the best solution for you would be to get rid of them, and instead use only 1 variable called, for example, answer. Then with 2 if statement you would check if the user is interested in china or japan, like this: echo China or Japan? read answer if [ $answer == "China" ] then china loop elif [ $answer == "Japan" ] then japan loop else error fi Now care, in your loop function you have no way to check the answer again, cause when the user is asked for the first time to choose between the 2, the question on the screen is printed by the echo outside your function, so if you want it to loop properly you should move everything inside that loop and then, after you've defined it, call it. so this can be a way to do it: loop() { echo Would you like to learn about Japan or China? read answer if [ $answer == "China" ] then china loop elif [ $answer == "Japan" ] then Japan loop else error loop fi } loop Also, bash is case sensitive, so if you reply "japan" instead of "Japan" the error function will be called because you triggered none of the 2 if above. if you want it to accept both answers you should add an "or" with the double pipe, ||
[Hacksaw](https://github.com/neXromancers/hacksaw) is the thing for you, if I understand you correctly.
How will you tell `ffmpeg` which part of the screen you want to grab? If the `-s` option is smaller than the screen size which part of the screen does it grab?
I don't know how to select points on the screen without using something outside of bash. What you could do though is use the bash window itself as the screen selector. So say you want to record half of you screen, resize the bash window to the half you want to record, then use `xwininfo` (idk if that's present on Macs) to get your terminal window info and feed that to `ffmpeg`. Then you can move your bash window around and it'll keep recording the area where the bash window was to start. I tested it and it seems to work ok. Here's my version without all the extra `ffmpeg` options you have: xsinfo="$(xwininfo -id $WINDOWID)" x0=$(echo -e "$xsinfo" |grep 'Absolute upper-left X' |awk '{print $4}') y0=$(echo -e "$xsinfo" |grep 'Absolute upper-left Y' |awk '{print $4}') wd=$(echo -e "$xsinfo" |grep 'Width:' |awk '{print $2}') ht=$(echo -e "$xsinfo" |grep 'Height:' |awk '{print $2}') ffmpeg -y -f x11grab -s ${wd}x${ht} -i :0.0+${x0},${y0} /tmp/outtest.mpg I have noticed that it doesn't take the title bar of the window into account. So if you want to include that in your sizing the easiest thing is probably just hard code it in.
Since you mention this being a Macbook, is this not also something you could do with AppleScript and the built-in screen recorder? Personally I'd leverage AppleScript to automate starting the QT screen recorder, and then end it with a bash script to convert to a specific format.
You could write a for loop that echos " * " in front of each result of your grep. Seems like unnecessary gymnastics to me, but I just verified that it works.
For starters, remove `ls` - it's output is not guaranteed to be portable, and it's job in this instance can probably be accomplished much better by something else... Maybe find -O0 /sys/class/net/ -regex "^.*[0-9]$" -printf ' • %f\n' but nobody really knows what the heck you are doing.... Possibly apart from altering something that you pulled together from at least 6 different sources (1 whose author prefers using `awk` and 6 from those who favour `sed`) to achieve your goal. Refactor to make better code, not rubbish code run faster... and considering that `search` and `search and replace` are very different operations, is it any wonder that `grep` is faster according to the table in the article? Maybe `awk` is the best tool for what you are doing..... but because you don't know, are using four or five separate calls to different applications when it could all be done in one script as fast if not faster, yet your reading of one six year old article has closed your eyes to this possibility
Forgot to say that I am using a MacBook, but my OS is Manjaro Linux
 for f in /sys/class/net/* case "$f" in (lo[0-9]|lo) continue ;; (*) printf ' * %s\n' "$f" :: esac done
Thank you for the answer. &gt;but nobody really knows what the heck you are doing.... Possibly apart from altering something that you pulled together from at least 6 different sources (1 whose author prefers using awk and 6 from those who favour sed) to achieve your goal. I want to tell you that I am not at all an expert with bash, `awk` &amp;co, but actually the other instructions with `sed` and `awk` were written by me - and it took me days to understand how they were structured. The point is that in all other cases I used `sed` and `awk` to do research only: like `'s/^.\+ (hello). +$ /\1/'` and for this reason it was totally useless to leave it like this. (obviously this is just an example) &gt;Refactor to make better code, not rubbish code run faster... and considering that search and search and replace are very different operations, is it any wonder that grep is faster according to the table in the article? I recognize that in this case it is forced not to use `sed`, but in order to remove it as a dependency of my script it was only necessary to find a replacement to this case... &gt;Maybe `awk` is the best tool for what you are doing..... but because you don't know, are using four or five separate calls to different applications when it could all be done in one script as fast if not faster, yet your reading of one six year old article has closed your eyes to this possibility From my knowledge, `awk`'s purpose is to edit actual files and not to format the "stream output", but I must be wrong. But thanks for the suggestions anyway, I'll think better the next times. (again, sorry for my english)
[http://gnulinux.guru/?bash](http://gnulinux.guru/?bash)
I went ahead and functionalized it to add to my .bashrc: screenrec(){ # record the screen to a video file # args: [h|help|-h|--help] help message # [] record the whole desktop # [0] use the calling window's dimensions # [1] select the window to record local odir="$HOME/Videos/screenrec" local x0 y0 srec if [ -z "$1" ]; then srec=$(xdpyinfo |grep dimensions |awk '{print $2}') x0=0 y0=0 else local sinfo wd ht if [[ "$1" == 0 ]]; then sinfo="$(xwininfo -id $WINDOWID)" elif [[ "$1" == 1 ]]; then echo 'Select window to caputre now...' sinfo="$(xwininfo)" else #elif [[ "$1" == +(h|help|-h|--help) ]]; then echo "Usage:&gt; screengrab [|0|1] (blank: everything, 0: calling" echo " window's dimension, 1: select window to capture)" return fi x0=$(echo -e "$sinfo" |grep 'Absolute upper-left X' |awk '{print $4}') y0=$(echo -e "$sinfo" |grep 'Absolute upper-left Y' |awk '{print $4}') wd=$(echo -e "$sinfo" |grep 'Width:' |awk '{print $2}') ht=$(echo -e "$sinfo" |grep 'Height:' |awk '{print $2}') srec="${wd}x${ht}" fi mkdir -p $odir ffmpeg -y -f x11grab -framerate 40 -s ${srec} \ -i ${DISPLAY}.0+${x0},${y0} -vcodec libx264rgb \ $odir/recording-$(date +%Y%m'('%R:%S')').mp4 }
Many thanks! It's simple and works without spawning a subshell.
What I am trying to say is that the code that you have not posted that sits above or below the line that you *did* post might - in combination, be able to be made simpler - so the answer to the exact question *that you are asking* might not be the best one for *what you are trying to do*. asking how to add 5 and 6 together is OK... if you are adding 1 to 4 together above it, and 7 to 10 together below it and then adding the three together, the best answer is probably not how to add 5 and 6 together, but more a case of adding the numbers between 1 and 10 &gt; in order to remove it as a dependency of my script Is there a *need* to?... by all means remove any dependencies for www.github.com/strange-project/obscure-code, but `sed` is going to be on most platforms as a default install for quite a while... There is nothing wrong in using several different tools in scripts - indeed, the "do one thing and do it well" *promotes* the case for more *common* dependencies &gt; From my knowledge, awk's purpose is to edit actual files and not to format the "stream output", It can do loads.... want to remove that dependency for `wc -l` in your script? awk '{linecount++} ; END{print linecount}' or that wicked `sed`.... sed -n "$=" Where is the 'editing' or 'output stream formatting'? Oh, crap - you wanted to get rid of them.... grep -c $ might help then...
Meanwhile, in Bash for x in {2..100}; do for ((p=2;p&lt;x;p++)); do ((x%p)) || continue 2 done; echo $x; done
You can use the 'read' command with the '-p' option and text to display to notify what the user should input. Ex. read -p "China or Japan: " user_choice 'user_choice' is the vaiable to test in the if statement. if [[ user_choice == "China" ]]; then &lt;whatever you want it to do if user input equals China&gt; elif [[ user_choice == "Japan ]]; then &lt;whatever you want to do if user input equals Japan&gt; else echo "the aggresive error output you stated lol" fi *Something along those lines. Always turn to the documentation on commands (read command).
 for((;i++&lt;97;)){(let j+=i%{1..97}?0:1,j^2||echo $i)} that's a fun game
What does echo $? say after a dpkg error? If it's not 0, then you can do a conditional off that, following the command. apt-get install megasync if \[ $? -ne 0 \] then apt --fix-broken install fi
`dpkg` does not manage dependency, so it will quit once an error is found. It is mainly for installing or removing a local package or show its contents. To get dependencies and install them it is better to use a higher level tool like `apt`/`apt-get` or `gdebi`. Since `apt` \&gt; 1.1 you can install local packages using (from the directory where the deb file is) sudo apt install ./local.deb The equivalent `gdebi` command would be sudo gdebi local.deb
http://www.talisman.org/~erlkonig/documents/commandname-extensions-considered-harmful/ &gt;but I like to have them for my scripts Do as you like, there is nothing protecting you from using .py and .jpeg for your shell scripts.
You can name the file whatever you want, the important part is to have the right hashbang
macOS recognizes this as a valid extension.
Ok thank you!
Filename extensions aren't a thing in linux.
\#!/bin/bash
FYI, not all bash installations can be expected to be in `\bin\`, which would make the script unexecutable in such a system.
The purpose of a filename is to communicate information about its contents. I use `.zsh` all the time, and `.bash` occasionally. Anecdotally, `pass` (password-store) requires extensions to be written with a `.bash` extension.
Shh-bang
You disrespect traditional Unix-like OSs, like r/freebsd, r/openbsd which install bash in /usr/local/bin
Ain't nothing traditional about freebsd or openbsd.
The right *shebang*. https://en.m.wikipedia.org/wiki/Shebang_(Unix)
Compared to Linux, BSD is the most "Unixy" OS freely available.
To add to that, without using `env`, you also don’t consider aliases the shell and/or user may have. For instance, Alpine Linux (extensively used in docker images) uses Busybox as it’s shell which has an alias for `bash` to `busybox`. On my personal dev machine I have an alias to `zsh`.
You say that as if BSD was a singular thing. I think a good case could be made for illumos or minix to be more "Unixy". Most of the BSD, apart from maybe NetBSD have diverted quite far from Unix, as they should. There really isn't any reason to try to stick to Unix. And the BSDs being more conservative in this regard probably is partly the reason they lost so hard to Linux.
No, people thought that they can't use FreeBSD because of AT&amp;T lawsuit you uneducated banana.
I've gone the route of having my executable scripts be extension-less -- they could be compiled Rust programs for all I care when I'm using them. However, I use a lot of files that aren't executable by are just `source`'d to set up env and define some functions. These I name with `.sh` to make it clear they're only usable in this way. However, I have wondered whether I should use `.bash` when I'm using language features that only exist in bash. I don't have an answer for that.
Extensions are irrelevant when it comes to running the scripts. If you have made the script executable and included the shebang line you can use `./scriptname` to execute it. Even if you cannot make it executable (because of permissions) you can run any script by explicitly calling `bash scriptname`. The extensions are primarily used for the following reasons: * Communicate the purpose of the file to other people. In that case creating a `.bash` extension is fine if you want to convey that the script has features that **only** works with `bash` and not fully compatible with Bourne Shell (sh) (it makes sense to include that as comments in the file as well). * Some editors use the extension for syntax highlighting. More advanced editors like Vim for uses multiple criteria including shebang line so it shouldn't be a problem. Sometimes custom indentations are set based on extensions so you may also want to check if that is present in the configuration file. * Otherwise they are mainly useful for use in other scripts and version control systems (for example gitignore) to take advantage of wildcard and regex based matches. If you want to distinguish between bash compatible and POSIX compatible scripts you can use a different extension but the choice is up to you. `.bash` is fairly standard.
As if that was the end of history of BSD. That is not the only thing that people base their decision to use an operating system on. The lawsuit hasn't been relevant to people's choice for decades so using it as an excuse for why FreeBSD failed so bad is just laughable.
The extension may be relevant to certain text editors such as Vim but, otherwise, it's irrelevant to the shell at all.
BSD hasn't failed. You got NetBSD in a fridge/washing machine/clock, OpenBSD in router/firewall, FreeBSD on Netflix servers, NetBSD and FreeBSD userland in Mac OS. Reason why FreeBSD doesn't exist on desktop is that, it was never it's goal. FreeBSD was designed and developed as server OS, not desktop targeted. Linux has good desktop supoprt because it already had been acknowledged that making better desktop support for it is more proftable than FreeBSD. FreeBSD now has layer to port Linux's graphics drivers to it. Now FreeBSD can be used as desktop OS. I use it on my laptop daily. It is more responsive than Linux on HDD disks, has better separation from base system and programs, updates don't break it, your configs aren't annihiated if it updates, config format is constant, it has more understandable init system, FreeBSD has better TCP/IP layer, it more efficiently uses machine's horsepower.
If you read my last sentence in my post I know that, it just makes life easier when working in the terminal and by the extension know what file it is.
Ok thank you for that!
Since it makes no difference you could literally use anything you want. It's just a part of the filename.
Thank you for the info!
I see your point....but I wanted to know when most of the time if someone uses an extension at the the end a script like .sh if it would be "against social norms" if you will to end in .bash
What difference does it make. If someone doesn't like that you end your scripts in .suckmydick then they can fuck off.
There should be no reason a user of your command should need to be informed of the implementation details of your command. The original intent of using file extensions was to allow the runtime system of early systems to select the proper interpreter (or direct execution). (Like in MS-DOS where the CI relied entirely on file extensions which extended into the first versions of Windows Explorer) The Unix/Linux system implements the shebang mechanism down at the exec(2) level so you don't have to rely on a command shell determining your file's type to have it run properly. Also, leaving off a file extension allows you to re-implement the command in another possibly more appropriate language if you want without any changes to other programs which may be invoking it. &amp;#x200B; That being said, if it brings you some sort of satisfaction to add extensions to your commands to force your users to type more unnecessary characters then it's no concern of mine or Linux, only that it looks like a noob affectation.
&gt; It is also called sha-bang, hashbang, pound-bang, or hash-pling. If you want to be a shitty pedant then perhaps don’t cite a source that directly contradicts you?
I tend to use `.conf` or `.cnf` for source'd files
This is pretty hard to handle. What if one of the arguments has a dash up front? I think it's much easier to make your script like this: $ myscript -a "Hello to the World" -b "Goodbye now" ... So one argument and then split that (with `cut`). &amp;#x200B; Also, you might be interested in `getopts`.
bless you
&gt;If you read my last sentence in my post I know that, Then why did you ask if the extension is relevant? &amp;#x200B; The standard is \`.sh\` but you can put \`.exe\` if you want to. &amp;#x200B; &lt;/end thread&gt;
So from what I gathered is for a personal system no gives 2 shits about what is at the end, and in the field it better be .sh it people will give you shit... That kinda opens up my eyes but another comment says that extensions can imply how to run it...lets say for instance your script only works in bash so you want to emphasize that it only works in bash so...you give it the extension .bash Does that make sense?
Yes, this would make sense. &amp;#x200B; I've been writing shell scripts since 1998 and it was not knowledge to me until now though -- so this is a very assumptive practice. It would probably be better to put a comment after #!/usr/bin/env bash that says # Compatibility: Bash 4.0 and above Then 100% of people know it's a bash only, and bash 4 at that.
Use file instead. file blah.whatever. Actually better to ignore extensions, nothing enforces those.
Filename extensions in *nix are for the person, not the computer. The dot is simply another character in the filename as far as the OS is concerned. Very rarely will people specify the exact shell interpreter in the extension, but I have seen .bash and .zsh. But really just follow convention and use .sh. Nearly every computer you interact with today will be running bash rather than sh.
I've never seen anyone do it for bash, but sometimes seen .zsh since there are some syntax things unique to zsh and zsh is nonstandard.
Dude sweet. The subshell is a performance hit, but not nearly as bad as the OP.
you could just reset the variable manually and modify the shell expansion a little bit to make it a lot faster, i was just having some fun with golfing after seeing that post tbh
It's slick. How does `i%{1..97}` work?
let will just expand the expression first, so it'll test if you can divide the current iterated number by each number between 1 and 97 and add 1 to our variable in that case
Thanks
Nah, call it the right thing.
Yeah, I see your point...to be honest I didn't think this little n00b question would be such a big conversation lol
Right now, my script is set up to work as the following: `~ myscript -a Hello -a to -a the -a world -b Goodbye -b now` It's just a bit annoying to have to rewrite "-a" every time. I've been using argbash as an argument parser, which I believe uses getops.
I too name my scripts with a .sh extension. As you and others rightly point out Linux doesn't care at all but it's helpful to me. It's especially helpful to me because I use a Windows machine, WinSCP and then edit in Notepad++. Having an extension on the file helps Notepad++ to know how to colourise the syntax of the file. If I was a proper Linux engineer I'd use Vi on Linux &amp; the file extension wouldn't matter.
Honestly it shouldn't be. If you want to indicate bash vs sh scripts by naming some with .bash go for it. Like a prior poster mentioned it's probably a good idea to indicate bash 4+ in a comment near the top as well. Or just check the bash version and error out with a message.
Ash is the default /bin/sh on some modern distros so indicating bash compatibility is definitely a good idea.
&gt; \bin\ There is no such thing. You mean `/bin/`?
Awesome the -p was very useful and so was the elif instead of multiple if statements. Thank you
This should do the trick: #!/bin/bash if [[ "$1" == "-a" ]] then declare -a arrA shift index=0 while [[ "$1" != "-b" &amp;&amp; "$1" != "" &amp;&amp; "$1" != "--" ]] do arrA[index]="$1" ((index++)) shift done fi if [[ "$1" == "-b" ]] then declare -a arrB shift index=0 while [[ "$1" != "" &amp;&amp; "$1" != "--" ]] do arrB[index]="$1" ((index++)) shift done fi if [[ "$1" == "--" ]] then shift fi echo "-a args: ${arrA[@]}" echo "-b args: ${arrB[@]}" echo "other args: $@" examples: $ ./file -a args: -b args: other args: $ ./file -a 1 2 -a args: 1 2 -b args: other args: $ ./file -b 3 4 -a args: -b args: 3 4 other args: $ ./file -a 1 2 -b 3 4 -a args: 1 2 -b args: 3 4 other args: $ ./file -a 1 2 -b 3 4 -- hi mate -a args: 1 2 -b args: 3 4 other args: hi mate $ ./file -a 1 2 -- hi mate -a args: 1 2 -b args: other args: hi mate $ ./file hi mate -a args: -b args: other args: hi mate $ ./file -- hi mate -a args: -b args: other args: hi mate Had to look up some things like bash arrays, I rarely use them.
But if the user is running a script (which I feel like is where this situation of providing the shell as your filename ext is useful), your interactive shell will be bash, no? I haven't used Debian in a long time.
Thanks a lot for the help. I'll have a look into how this can be done using a regex, but the example above looks perfect :)
Right now I'm honestly not sure. We just moved and all of my machines except my Chromebook are still packed or I'd go test it out. I think the scripts run using whatever you've specified with the hashbang line, otherwise you wouldn't be able to directly run zsh scripts from bash (which I do all the time.)
I didn't know you could put if then statements in functions. Pretty useful. Thanks
This doesn't work when the `-a` arguments come after the `-b` ones tho. For that you would have to work with functions or a (big) `case` statement.
Yeah, I agree with you. Thanks for your input as well!
They don't run with whatever hashbang you use...I tried the fish shell and ran a bash script and it didn't work. I had to run 'bash ./script'
Fuckme.porno sounds reasonable
Hmmm i might put that into consideration
Yeah sorry, on mobile
I usually use `.bash` for anything that’s a plug-in/extension for my tooling to differentiate it from other files in the `lib` directory, especially when it has no shebang or is bash-specific and should only be sourced by bash. For executables I generally will avoid any kind of extension since it just feels weird to run `foo.sh file1 file1` on the commandline.
\&gt; Filename extensions aren't a thing in linux. Yeah, they are. While Linux may treat them equally, the applications you run can. Ever seen a .a or a .o file? File extensions are used all over Linux. \&gt; Since it makes no difference It is not \_required\_ and does not affect direct execution of a shell script, but it \_does\_ make a difference. It does make a difference in things like file browsers for determining icons, editors to determine icons and syntax highlighting, and when the OS tries to find a default program type for a file and MIME associations. I encourage you to take the file extension off of \*every file\* in your system and see if you feel \*no\* difference and have everything work perfectly smooth.
But if you make a tool you want it to be callable just like any other tool. Once I have an admin script like I like it and fully tested I move it to /usr/local/(s)bin with no extension.
Sure. I do that all the time.
I always see people saying to use the `/usr/bin/env bash` shebang, but Google's Shell Style Guide, which is linked on the sidebar, [recommends to use `/bin/bash`](https://google.github.io/styleguide/shell.xml?showone=Which_Shell_to_Use#Which_Shell_to_Use). They argue: "Restricting all executable shell scripts to bash gives us a consistent shell language that's installed on all our machines." What's the benefit of using `/usr/bin/env bash` over `/bin/bash`?
Google just wants to make sure your interpreter IS bash, and not dash, ash, etc. It just allows the script to use whatever \`bash\` is first in the users path. &amp;#x200B; This allows a user to control WHICH bash specifically is being invoked.
I understand that -- but shouldn't the developer decide which shell is being invoked? Usually when someone installs a more minimal shell like dash, they link dash to `sh`, not `bash`.
Sure, and they still are. With /usr/bin/env, you still have to specify which shell should be used, typically something like /usr/bin/env bash Or where this is more important, /usr/bin/env python in the second case, if we instead used /usr/bin/python it could be any version, but with vietualenv we can override this.
 declare -a a b z declare -n arref=z for arg do case $arg in -a | -b) declare -n arref=${arg#-} ;; *) arref+=($arg) ;; esac done It doesn't feel right to repeat the declare.
your "print" is executing /usr/bin/print which is a utility for setting up actions for displaying mail attachments. change to echo or printf
Do you have files with names like `a`, `b`, `c` and so on? If not then you probably want to stick with `echo` instead of `print`. The error `no such file "z"` is a good clue that `print` is trying to show you the contents of files. For the internal print error, my line 528 is different from yours, but I suspect you have files around with some single-letter names that are causing 'fun'
How did I miss that!!! Thanks for answering.
The mistake is that you used a program named `print` instead of the `echo` command of bash. The error message you are seeing is from that `print` program you are trying to use. It's probably a program that's intended for something totally different than showing a message in the terminal. Maybe that "print" command is something for printing to a printer? I mean that hardware device that puts stuff on actual paper. You could try running `man print` to see if there's a manual for the program, or maybe try `print --help` and see what it says. Something else, what's really neat about bash is, you can experiment with things directly at the bash prompt. You don't have to edit a script file and run it all the time when you want to test something. When experimenting at the prompt, you can write things on a single line by using `;` characters, like so: $ for x in {a..z}; do echo "$x"; done a b c ...
Sounds like you're trying to re-implement Tripwire : http://www.linux-magazine.com/Online/Features/Detecting-Attackers-with-Tripwire It monitors files and notifies when they're changed too, primarily as an IDS notification system.
convert -crop [https://imagemagick.org/script/command-line-options.php#crop](https://imagemagick.org/script/command-line-options.php#crop)
i'm not trying to do some fancy system-wide thing, i am a regular home user, no remote work or servers. i know backups are the only real way to deal with ransomware, and i have a good backup strategy, but i wanted the script anyway.
Not a criticism, just an observation.
I tried: `convert large.jpg -crop 640x640 small.jpg` Which gave me over 20 small images, but no all are 640x640.
You'll have to make it calculate the size from the dimensions of the file going in, so that it automatically adjusts itself and only outputs images which are square.
Not the problem, but you run date in rapid fire. The date might change during the loop sometimes.
it's deliberate, not an issue.
I have no idea how to do that. I have no experience with ImageMagick.
you can use IM's `identify` to grab picture info in a script before running the convert. but, crop also has the @ option for geometry which will crop to a grid. convert large.jpg -shave 6%x12% -crop 3x3@ small.jpg would create 9 ~equivalent sized tiles from the center of the picture (like in the example).
It's probably not imagemagick which you'll have to figure out. The other option you have is to make sure that any images you want to turn into a grid, are pre-cropped into multiples of 640 - 3840*2560 for a 6*4 ratio, for example.
There are much easier and more reliable ways to do this than manually searching for modified files and comparing a list of the names. The easiest way is to use `inotifywait` from `inotify-tools` (should be available in most linux distros). It can be set to monitor a directory like `inotifywait -r -m /path/to/dir` which monitors it indefinitely. Or you can customize an action to take if something is modified in the directory as while inotifywait -e modify /path/to/dir; do #Notify and take action done Another reliable way is to use `md5sum` and calculate the checksums of the files like below find /path/to/dir -type f -exec md5sum {} \; &gt; Checksums.chk Then you can compare the files listed in `Checksums.chk` using md5sum -c Checksums.chk
i looked into it but i was more comfortable with the find method.
https://github.com/junegunn/fzf
&gt; diskutil list | grep -i /dev/ | cut -f 1 -d ' ' `diskutil list external` would be more useful here. Why `-i` on the grep? That's just silly. &gt; find ~ -type f -maxdepth 2 -name "*.iso" | grep -i .iso The grep does nothing, `-name "*.iso"` already did that work for you. Why `-name` and not `-iname`? Why are you searching the entire home directory after your output (`ISO in Downloads`) implies all your ISOs are in `~/Downloads`? &gt; sudo dd if=$disk | pv | sudo dd of=$iso bs=1m I might be missing something but this (`of=$iso`) looks like you're overwriting the ISO file with the (empty) contents of your new disk. Have you actually run this? Did you check if it ruined your ISOs?
Holy moly what a mess I did here. I fixed it. How about now? #!/usr/local/bin/bash # Burn an iso into an external clear printf "\n--- [ USB list ] ---\n\n" diskutil list external printf "\nChoose one disk: "; read disk printf "Give it a name: "; read name printf "\n--- [ ISO in home ] ---\n\n" find ~/Downloads -type f -maxdepth 2 -iname "*.iso" printf "\nImage: "; read iso printf "\n\n--- [ Formatting ] ---\n\n" diskutil eraseDisk FAT32 $name MBR $disk printf "\n\n--- [ Unmount ] ---\n\n" diskutil unmountDisk $disk printf "\n\n--- [ Burning ] ---\n\n" sudo dd if=$iso | pv | sudo dd of=$disk bs=1m printf "\n\n Operation completed\n"
I suspect that your order of execution is to blame. You are currently piping `echo $movies` into a while loop. The echo command only runs once. I would suggest trying echo "&lt;SELECT STATEMENT&gt;" &gt; movies.txt while read -r line ; do ... done &lt; movies.txt rm movies.txt
The problem with that is that for instagram grids, you need a 1x1 ratio on all of the images.
well there are a few ways to go about this i think. but trying to keep it simple: col=3 row=4 convert large.jpg -gravity center -crop "${col}:${row}" +repage +gravity -crop "${col}x${row}@" small.jpg this will crop from center out to the aspect ratio can will produce squares when gridded with the same ratio.
well there are a few ways to go about this i think. but trying to keep it simple: c=5 r=2 convert large.jpg -gravity center -crop "${c}:${r}" +repage +gravity -crop "${c}x${r}@" small.jpg this will crop from center out to the aspect ratio that will produce squares when gridded@ with the same ratio.
Are you serious? There is no input validation. Literally nothing.
Is it possible to reutilize that menu for options not related to files (like a fancy way of doing select)
Input validation is important. Be *very* careful twitch those `read` statements. Maybe verify the input is sane (is it a file/device? - read up on the bash `test` function and also some of the more advanced invocations of `read`). Also, you can enhance the output of `pv` by letting it know how big the file/stream is that it’s being fed. On MacOS, I usually do this with the `stat` command and format directive. Eg: pv -s $(stat -f ‘%z’ “$iso”) Good luck with it.
If I knew about input validation I'd put it here. Everybody started somewhere. Chill out dude.
Thank you man, I'll fix it!
Not jumping on you like the other person. But just in general you want to make sure to only receive input that you want to receive (this input validation thingy). So if you expect a number make a check for that type...likewise for string, etc. Most likely you already know this. I'm just trying to drill it in that it's a general concept that applies to many contexts.
Yes, it can read from STDIN, and will return your choice to STDOUT. So you can incorporate it in any scripts you like.
To be honest I knew about it but didn't think it matter for such a small task. If I had to check argvs or inputs for every things I make I'll be insane. Especially when I post it in a /r/ where people understand what's going on when they look at a script. That said just because it runs doesn't mean has to be bad written. Thank you for the heads up and I'll take more tie in the future to double check and write better code.
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
In you're if statements where you compare 2 string but the variable in "
First, try escaping the hash mark.
I tested a small script that (without quotes) and it worked...
Use four spaces to indent code. Only the new shit redesign supports these non-markdown \`\`\` codeblocks.
Not `imagemagick` but the [pamdice](http://netpbm.sourceforge.net/doc/pamdice.html) command from `netpbm` may be an alternative. All you have to do is convert to the `.pam` file format and back to `.jpg` or whatever afterwards.
Just an fyi, if you are confident that $string will ALWAYS be set, then you don't need quotes around the variable. If you don't use quotes and $string is unset then you can see a similar issue to what you posted. Not saying this is the issue you are having but more of a heads up.
`less -S +F`
Exactly what I wanted, thanks.
 unexpected operator The script just doesn't want you to run it. Find a competent operator, and have that person try again.
Ive found the error, thanks
I didn’t knew that, thanks for the share!
In my mobile works great 🤔
Yes -eq solved
It doesn't work on the proper reddit site.
Ok im on the app, when i get home i do that
`less` is always more
can't speak for the downvotes, but i had missed the part about being squares when i first responded. i am curious though, did the newer solution produce decent results for you?
`less` also has search. Not sure if `tail` does.
[more is more](https://youtu.be/QHZ48AE3TOI)
It just means its executable. The -F flag of the ls command does this for you, and on a lot of systems just running "ls" is an alias for a more extended ls command with the -F option.
http://bfy.tw/NwfU
Use Perl maybe? $string =~ s/regex/replacement/g; Why can't you use sed or awk?
 while read line; do [[ "$line" =~ /regex/ ]] &amp;&amp; line='substitution' echo "$line" done &lt; file Don't do this if it can be avoided; Bash is for process orchestration, not text processing
You can group several commands that print stuff using `{ ...; }`, and can then redirect that whole block with one `&gt;`. It would look like this: { echo hello echo hi echo hey } &gt; file.txt Another thing, you mention "notepad". If that's the Windows program, it perhaps does not understand Unix line-ending characters (I heard the newest version in Windows 10 is fixed). When it makes that mistake, it will show the whole text file on one single line.
My guess: homework assignment
Ah thank you so much! I was banging my head as to why it wasn't working, such a simple solution. Yea I was trying to open it in Windows notepad, I have an older version of Win10. Opened it in any other text editor and it's exactly what I want. Thanks for the quick response!
tr command?
How do I run the file ?
Exactly =| Thanks for the comments!
If you are using a terminal with bash, and you are in the directory that contains the file (ie you have used cd to change directory to get there), then you simply type the name of the file at the bash prompt. To ensure that the file you are looking at in that directory gets run it is prob better to type ./acme for a file you see listed as acme (or acme\* depending on the flags being used by the ls command to list the files). The "./" before the filename tells bash to look for the file in the current directory. If you just entered the file name ie acme then bash would look in several other folders/directories as specified by the $PATH environment variable (which may or may not contain the current directory you are in). Simple!
Should be asked on r/linux4noobs, but I'll amswer anyway : Simplest way to extract a file is using tar -xf [FILE] and to run a file you can use ./FILE, which would work if the file has execution rights (if not, use chmod +x [FILE] I know, linux is confusing at first, but after some time it looks trivial, trust me.
If it's a homework assignment, your teacher should have explained to you the way they'd like you do accomplish this task. The point is they are trying to teach you a specific principle. If you just take the advice of random internet strangers on how to accomplish something, you might technically accomplish the task they laid out, but you won't demonstrate that you understand the principle they are trying to teach you. You shouldn't be asking us how to do this, you should be asking your teacher. That's literally what you pay them for.
The file which I ran with tar is gone and I now have a file with this name; `\245\377H\213D$\020H\213L$\030H\213T$\ H\307\004$*` ? Next time I'll post in linux4noobs hopefully I can get this resolved here.
as a sidenote: as far as i know Unix-like systems dont include the contents of the current directory in their environmental variables (for security purposes). So if you want to run the file you must type ./ to make it run, otherwise it will not work. (unless PATH already has the file for some reason ofc)
`ed` baby!
I understand why you wouldn’t want the current directory to be the first path used (don’t override my `ls` command with your malware, thankyouverymuch), but why would running something in the current directory be a problem beyond that?
Doesn't look like a executable. In the folder, run : ls -l. That'll tell you the size of the file, which may help , and the actual file name (s). To really find out, run: file "&lt;that weird file name&gt;". Use the quotes, but ditch the brackets. That'll tell you what kind of file Linux thinks you have.
$ed reply.txt P \*a Never forget about the editor that started it all. . :wq ? \^c ? adsglk;ahsd;ga;lks ?
Is that `run:file &lt;fileName&gt;` or `run:&lt;fileName&gt;` ?
I think that's pretty much the only reason why. Some people do add "." to their path, and as long as you add it to the end of the path list, it really shouldn't be a problem.
 #!/bin/bash For arg in $@ ; do &lt;command&gt; $arg done Now you can pass as many arguments to it as you like.
Add the text to an array and substitute it out with an array substitution. Since it’s homework, I’ll let you figure the syntax out
Good thought. I was hoping to avoid typing the additional words with a for loop. As a one-liner `$ for arg in 'arg1' 'arg2'; do command $arg; done` it's longer than if I just typed command twice. Thanks though. I'm expecting that there probably isn't a way to do it with simpler expansion based on my searches, but still wanted to throw the question out there.
Yeah, but if you save it as a script, make it executable, and make an alias to run it, you can do it with `alias arg1 arg2` every time after that.
But if I’m creating a wrapper file to run I’d might as well create the alias is .bashrc. I know there are ways to do it, I was just hoping there’s a little bash expansion trick for when I hop on machines that don’t have my customizations. :)
You could setup `xargs` to process multiple arguments with delimiter. `xargs` is also very powerful so you can add many more things to it. One possible setup would be to use an alias like `alias multi='xargs -I "%" -d "," command "%"'` where `command` is the command you want to use. Here we use string replacement so `xargs` substitutes the arguments in place of `%` character. An example is shown below $ alias Multi='xargs -I "%" -d "," ls -lth "%"' $ printf "file1.txt,file2.txt" | Multi -rw-rw-r-- 1 user user 498 May 21 13:01 file1.txt -rw-rw-r-- 1 user user 79 Mar 18 16:45 file2.txt You can use whatever delimiter you want. The default is newline.
The only way to truly learn is to use it. Think of the reason why you wanted to learn how to create bash scripts. Now try to implement that in code using Google, reddit, stackoverflow, etc.
What do you want to do with it? For myself, it was a way to automate a repetitive process. Just view it as a series of commands to begin with. Whatever you would type into the CLI, type into a text editor instead, and save it as a .sh file. Later, figure out what you want to do, and search the web to find examples of how to achieve it. For loops and while loops will make much more sense when you have an application for it, rather than as an abstract lesson.
xargs is cool. I've never really used it enough to learn how it really works, other than the occasional use by copy and pasting something from a forum. Looks like `$ echo 'arg1,arg2' | xargs -d, -l1 command` does what I expect. Quite close to your example. Still a bit more to type than I'd have liked but oh well haha Another questions in the same realm of things. I've been looking at the available builtin variables for one that expands to the current command. I see lots of posts about `$0` and `$@`, etc, but those seem to require use inside a script, not from the command line. I'm looking for `$ echo $0` to return `echo`, but instead it returns `-bash`, otherwise I could just do `$ command arg1 &amp;&amp; $0 arg2` Are there any bash expansions I can use when invoking a command that references it's own arguments? Like if I wanted to do something like `$ command arg1 arg2 $1 $2 $1 $1 $2 $2` for whatever reason, is that doable with a variable like these? My thoughts are if that is possible, then I can reference the command too, but everything I see either needs to be inside a script to work, or they reference the arguments of the *last* run command, not the one about to be ran. Thanks for taking the time to comment!
That’s bash design, not a Unix rule.
im interested in learning how to do commands through terminal in linux
could you explain more or give me a simple example that i could start with and test?
I don't really think of bash scripting as a "language" so to speak, but more of a tool to help automate command line tasks. Sure it can do a lot of normal programming stuff, like math and loops, but at it's core, it is a way to chain the execution of multiple commands and manipulate the results. It'd be better to take a step back and look at your environment as a whole. I assume (and hope) you're using Linux as your platform to learn, so try to think of some stuff in the command line you've done in the past. If you're having trouble thinking of "tasks", my unfortunate recommendation is not to worry about bash scripting for a bit, and focus on just learning the command line, the available commands, arguments and return values, etc. Then, once you have a better understanding of that, you'll find yourself needing to accomplish task X several times a week, that requires a handful of commands. Now you can take a look at scripting again, and make a script to automate running your task. Now rather than typing all the stuff by hand, you just run your script and boom it's all done. It's tough to learn something new if you have no real practical way of practicing. You can read a programming book cover-to-cover, but you can still only make a Hello World program or calculator so many times before you get bored and discouraged. It's better to look at stuff you do daily, and think "how can I automate this". That's what bash is best for.
In my opinion, that's a good place to start. Which distro do you use? Do you know how to update your PC from command line?
Steps to become a Linux master: 1. Figure out a task you want to accomplish through terminal in Linux 2. Research the task to find the required commands 3. Test it out. If it works, congrats, you just learned a new command. If not, return to Step 2. 4. Repeat
 #!/bin/bash x=2 y=5 let z=$x*$y echo $z Returns 10 Explanation since I'm bored (by line) 1. Shebang. Specifies the interpreter it expects to be used to run the script. 2. Initializes variable x with the value of 2 3. Initializes variable y with the value of 5 4. \`let\` allows us to set a variable equal to an arithmetic value (Otherwise \`z\` would literally be equal to "2\*5"). The \`$\` allows us to use the value of the specified variables (otherwise it would just consider them normal characters) 5. \`echo\` is another command that prints it's arguments to the screen. Since \`$z\` is now equal to \`10\`, it prints \`10\`. That's about it. You run a series of commands to do something. Gotta figure out what you actually want to do with it first though ;)
Before coming to your question I wanted to mention that the `xargs` string substitution is more powerful than simply replacing arguments. It can be used to replace any part of the command string including the options which can be used for example as below to run two separate `ls` commands that does different things (sort by time and sort by size) $ echo -n "-t,-S" | xargs -I "%" -d "," ls % file*.txt file1.txt file3.txt file2.txt file1.txt file3.txt file2.txt Coming to your question you can do what you specified using the following command below command arg1 arg2 !#:1 !#:2 !#:1 !#:1 !#:2 !#:2 The `!#` is bash history expansion for everything typed in the current command line. `:` is a separator and the numbers indicate the arguments (0 being the command itself). You can do a lot more with the arguments for example remove the ending part of a long pathname or replace strings using modifiers. See the `HISTORY EXPANSION` section of `man bash` for more details. An example is given below $ ls file1.txt file2.txt !#:2 !#:1 !#:1 !#:2 ls file1.txt file2.txt file2.txt file1.txt file1.txt file2.txt file1.txt file1.txt file1.txt file2.txt file2.txt file2.txt Note bash echoes the full command line before displaying the output.
Oh my goodness &lt;3 I totally was trying a handful of `!#:0` variations earlier, but since it was printing the full command before running I thought it was failing when I was just testing with `echo`. Alas, `$ command arg1 &amp;&amp; !#:0 arg2` works exactly as expected. Thank you!! I'm definitely going to keep looking into `xargs` too, since it seems super powerful and a good tool to add to my arsenal. :)
[removed]
Hey! If I am understanding what you are trying to do here correctly this should help not sure what tools version you are using but this could help. Checkout `man wget` there are some cool options like`--input-file= / -i` which also reads from STDIN `-i -` and `-nv` is 'not verbose' so it is a bit easier to parse `&amp;&amp;` will execute the next command if first is successful, then g`rep -v 'Downloaded'` will find lines without the word 'Downloaded' then pipe `|` the output through `awk '{print $6}` and grab the sixth column with the url and append `&gt;&gt;` downloaded\_files.txt `wget -i urlfile.txt -nv -a log.txt &amp;&amp; grep -v 'Downloaded' log.txt | awk '{print $6}' &gt;&gt; downloaded_files.txt` see `man awk` `man grep` and `man wget` for more info. Hope this helps
btw. I would have liked to just piped the output lines to straight in to the grep portions instead of first writing and reading a log file, there are some things I didn't have time to trouble shoot/ learn about using the `wget -i` option and buffer handling.
The web browser or firewall appears misconfigured for this site. Or it could be something weird on my side but I doubt it.
If this is simply a script for your own use and you can trust the operator (yourself) then input validation might simply be stuff to make sure you don't accidentally shoot yourself in the foot - like I mentioned earlier, make sure the source/destination are files/devices respectively. Also, maybe throw a message in there from the script explaining what is about to happen so you can bail out if you screwed up before it does any permanent damage. Here's some ideas. Make sure \`$disk\` is a device: if \[ -b "$disk" \]; then echo "$disk is block device - continuing" else echo "$disk is NOT a block device - bailing out (you goofed)" exit 1 # non-zero exit status = something went wrong fi &amp;#x200B; Same construct for files: if \[ -f "$iso" \]; then \# You get the idea... &amp;#x200B; Finally, to give yourself an "out" before the script does anything permanent: &amp;#x200B; echo "We have everything needed - here's what I'm about to do" echo "Erase disk '$disk' and label it '$name'" echo "Write this ISO to '$disk' - '$iso'" read -n 1 -r -p "Continue (Y/n)? " myCont case $myCont in Y|y) # Fall through - expected case echo -e "\\nContinuing with the ISO write, as requested.\\n" ;; \*) # Anything else...abort echo "Bailing as out, as requested." exit 1 ;; esac
The script it's mainly for me but it doesn't mean I cannot improve it live you just showed me! Thank you. Will dig into it and make the change!
Yep, firewall or something seems misconfigured
when you thought you were a math guy and realize you are only a plus guy.
sudo apt update. i need to look it up to make sure exactly. but i have done it before. when i was messing with the terminal the first time a few months ago i broke the os (ubuntu) and since then i stopped messing around because i really don't want to download everything i have over and over. i have some back ups but didn't know how to do the whole thing. only important stuff. i am having a hard time learning this. i am really a noob.
how do i make a calculator? any guides?
hahah i know what you mean. it was hard.
Very interesting read. Thanks
[http://lmgtfy.com/?q=bash+calculator+example](http://lmgtfy.com/?q=bash+calculator+example) Don't expect to have your hand held. Try first, then ask for help if you can't get past a roadblock. Don't be afraid to break something; it just gives you a chance to fix it and learn more.
thanks my dude.
can you explain remove repetitive actions on your computer please?
Of all the downvotes I have ever gotten, this is the one I will savor most: reporting that the website is fucked. On another note, check out the OP's post history.
There's a tool "parallel" that's a bit like 'xargs' but easier to use. I often use it as a 'for' loop replacement while at the normal command prompt because it's less to type: parallel command ::: arg1 arg2 This will translate into: command arg1 command arg2 The tool also has neat argument editing features that sometimes come in handy, for example: parallel convert {} {.}.jpg ::: *.png The parallel tool will create command lines like this: convert a.png a.jpg convert b.png b.jpg
I suspect they just learned the hard way how prone to abuse something like this would be. :(
ok, well the only advice I can give you is to make sure you know exactly what each command does, before you hit "Enter". Never copy and paste and execute commands you've found when web searching a problem you've found (there are some bad eggs out there). Most commands have a `--help` argument, or a manual page which you can read with `man &lt;command&gt;`. The thing about bash scripting is that part of it is just using bash, but a majority (in my experience) is using the command line interface of programs you've installed. And each program may have its own syntax, argument style, or even its own language. This is why I recommend learning to use the command line first. And if starting with sudo apt update sudo apt upgrade Is what it takes to get started, then so be it. But don't stop there. Start asking questions like, "if I wanted to type the command and then walk away, and come back to a completed upgrade, what would I need to use as an argument?
Let's say every day you log on to your computer and spend 10 minutes doing a task. Instead of spending that 10 minutes every day, you decide to write a script that does that task for you on every boot.
[removed]
Thank you very much! I knew about `wget -i` but don't always run it that way. Sometimes it's just `wget https://foo.diddly/file.qrz`. But your examples have helped and I think I can make it work! Thanks again!
r/programmerhumor?
Besides, there was already someone answering the questions, I've got a tip for you. If you ever use any command line commands, use `man COMMAND` and go through that. You will learn more from that, than asking any specific questions on the web and they are really well written to be understandable for nearly everyone.
2&gt; means to redirect any error outputs to the next argument /dev/null is basically just a blackhole where you can put output but not retrieve it. So basically, this is for error suppression.
Just noobing around. Anybody know any good coding humor channels? Thanks folks. Note: "Why don't you go ahead and start one." does not count as an answer.
Oh, yuck. It's relying on the `[` command failing if you try use `-eq` &amp;mdash; which is the "numeric equality" operator &amp;mdash; on something which isn't a number. And since that kind of failure would ordinarily produce an error message, that's gobbled up and thrown away by the `2&gt;/dev/null`. Seriously, yuck. Since this is Bash we should just use Bash features. What is an integer? Let's say a valid integer consists of: * an optional leading minus sign; * one or more digits, the leading one of which must not be 0. Then you could test whether `$number` contains an integer with: [[ $number =~ ^-?[1-9][0-9]*$ ]]
-eq means equals you have the short forms for the mathematical operations in bash. But I would say that next time you ask in communities be sure that you ask it in such a way that people dont just see and skip it...your isn't aligned properly in this post..take care of it next time please.
In bash if statements are not true if statements (like C for example) the `[..]` syntax is just a shorthand for `test` builtin (run `help test` to find out more). The test built-in takes a conditional expression as its argument like `"$number" -eq "$number"` in your example. It returns an exit status of 0 (success) if the expression is true 1 (failure) otherwise. The `if` statement will execute the statements under `then` or the `else` depending on these exit status. Since `test` is a command so it can trigger error if the expression syntax is not correct. For all commands in bash the error is shown on `stderr` (terminal) which is represented by 2. To suppress error you can redirect it to `/dev/null` using the redirection `2&gt;` which means they won't be printed or stored anywhere.
&gt; edit: though, I've never seen this used directly after an if statement, so I'm kind of curious about why it's there. It's not redirecting stderr on the `if` statement. It's redirecting stderr on the command used as the `if` statement's _condition_. Think of it as: if some-command-that-could-write-to-stderr 2&gt;/dev/null; then # ... fi
This is mostly there to cover cases where a variable is a non-integer. If that’s the case, it will behave correctly but also spit out an error that the values aren’t comparable with `-eq`
Yeah, for now the only one I know is the pne I mentioned. Note response: nah, I'm not that toxic to say crap like that to other people :P
Right! I got that, I just assumed that it would evaluate to false if it got string input, not raise an error.
Hahahahaha. Agreed. Some people confuse a simple question for an advice.
Your welcome. Also if you want the -a option will append the log file so if you use the same one it won’t over write it. Also if you are just passing ’ -i - ‘ it should handle it. It would look like ‘echo https://foo.diddly/file.qrz | wget -i - ‘ the hyphen should read what you pipe to it, be it a file path of urls or a single url according to the docs, but I haven’t tested it.
What are you trying to do exactly? If it's learn shell scripting in general, you might want to try googling for tutorials. It would be useful if you posed the question on the lines of "I am trying to write a shell script that does this and here is what I've done. And my question is so and so". If would give us a better way to help :) Some top level suggestions then there are several ways to do that. Here's an example (found using Google search "shell script check if input is number") https://unix.stackexchange.com/questions/151654/checking-if-an-input-number-is-an-integer Secondly, look at existing shell scripts - Linux boxes are full of them. They are excellent learning material. About 90pc of what little I know comes from here 😉 Lastly, wikibooks has a decent shell scripting book. Here's their explanation of the /dev/null bit: https://en.m.wikibooks.org/wiki/Bourne_Shell_Scripting/Files_and_streams You have to bear in mind shell script and general command line basics (like streams) are closely related. So going through a structured tutorial in the first instance might help - especially of you are new to Linux/ command line/ shell scripting or all of the above. Hope this helps. Good luck!
I'm no pro, but I would try: * saving the output into a file * grep for "recommended" * awk for the column with the driver name * input that into your apt command
This here seems to work to get the name (that "testfile" is where I've saved that example output you shared): $ awk '/^driver/ &amp;&amp; /recommended/ { print $3 }' testfile nvidia-driver-418 For a script, maybe do it like this: sudo -v # ask for password and save it for a few minutes sudo ubuntu-drivers devices | awk '/^driver/ &amp;&amp; /recommended/ { print $3 }' | while read -r driver; do sudo apt install "$driver" done I don't know how that "ubuntu-drivers" program works for people with different language settings. Will it translate its output into for example Spanish or German? If that's the case, you should manually force its output to English, like this: LC_ALL=C sudo ubuntu-drivers devices
The `[` command is the same as the `test` command. It's exit status will be zero (and thus the '`if`\-block' will be executed if the arguments following are a boolean expression that is true. `[` uses operators like `-eq`, `-lt`, `-o` for 'the logic'. Here are some examples: * `-eq`: left and right numbers are equal * `-lt`: left number is *l*ess *t*han right number * `-le`: left number is *l*ess than or *e*qual to right number * `-o`: left boolean expression *or* right boolean expression * `-a`: left boolean expression *and* right boolean expression * ... The statement `2&gt;/dev/null` redirects `standard error` (the 2nd I/O stream) to be written to the file `/dev/null`, which is basically a trash can. So no error messages will be shown.
Would it substitute the line "physically" or print the line substituted? Also, do you think `awk`/`sed` outperform bash in this side?
I must have missed that explanation :)
Check out `man test` to learn about the `-eq` it just means equals but between integers, but is is relying on redirecting an error so it probably not the best test... but I don't see a better one in man test. Here is a good answer to your question on 2&gt;/dev/null [https://askubuntu.com/questions/350208/what-does-2-dev-null-mean](https://askubuntu.com/questions/350208/what-does-2-dev-null-mean) Also google and Stack overflow and the man pages are probably a better resources for bash questions.
Ok, but if the file is too large it isn't a good idea. Isn't there a way to edit directly a line N?
I think the original is a hack, but it works for integers and it's straightforward to read. Yours is "bashist", but correct. However, anyone without your skills will be unable to read it. If an unskilled tech need to grok a script, I much prefer readable hacks than complex bashisms (even though I rely on these too). Source: have a great debt of inherited scripts from an East Asian subcontractor at work.
I think it's much more easy to understand a variable compared to an regex then and under the hood function that is expected to throw an error if everything is good.
Sometimes I create variables with names referring to the matching pattern for my regex and call them for this cases. I set isinteger to a regex so anyone can understand what I'm doing,
This replaces all lines which match the regular expression with the string 'substitution'. Dedicated text processing will always outperform manual iteration like this.
You're quoting the glob character (`*`) so it's not expanded. Try: for script in "$PATH_TO_SCRIPTS_REPO"/*.sh Also, you should quote `$script`, like `bash "$script" -H`.
works like a charm, thank you! :D bash is super confusing for me with all of that quotes and stuff...
Smart move. I like this solution.
\[ 1 -eq 2 \] Checks if 1 is EQual to 2 2&gt;/dev/null redirects errors (2&gt;) to /dev/null which scraps it
Yes and no. I see your point, however `[` can be read (and run) as test. That makes it a human readable (albeit awkward) sentence: test $A -eq $B compare to test whether $A is equal to $B. /u/fluzzi32 mentioned a neat hack. Set a var named e.g. $re_integer to your regex, and it solves the readability issue. In my own projects (for floats) I have used a function isNum calling awk to do the math and return 0 for number, read as: if isNum $foo ; then ... Which is great when you have to look at the damn thing two years later :)
You should be able to simply do **`sudo ubuntu-drivers install`**, no need for piping anything through awk.
Why isn’t it a good idea? There are creative ways to pull line n and still feed the text to array. You can change the inline file separator (IFS), to be a newline instead of a space, which means, it would set each line as an array value instead of each word. You can remove or replace specific indices as well. which is what you would want to do here. - change IFS - feed the text to an array - substitute the line
haha, that is best answer here! txh for that! ;) I don't have to reinvent wheel.
Check out a program named `fdupes`. It's for finding duplicate files. It looks at file sizes and compares contents.
It’s called British Summer Time. So it better not.
Thank you. I think Europe/London is more appropriate to use.
BST is to stick it onto a time that has no date attached. So you can still convert it to UTC. For a value that has both date and time component the Europe/London is better. If you have time only, use BST and whatever is the abbreviation for the winter time.
That helps! I will go with Europe/London since I have both date and time components involved.
Cool! Or join the flat earth society and start believing that there are no time zones. Haha.
Hahaha
I have also posted one qn. Would you mind taking a look and help?
find . -exec stat -c "%s %n" "{}" \\; This will find everything in the current directory tree (you can replace the "." with a different directory name) and display it with the size, followed by a space, followed by the name, like so: 12345 My file is really great.txt &amp;#x200B; find . -exec stat -c "%s %n" "{}" \\; | sort -n This is the same thing, but it's numerically sorted, so that files will be grouped by their sizes and identically-sized files will be on adjacent lines. Then we throw that into a loop that does stuff.... find . -exec stat -c "%s %n" "{}" \\; | sort -n | while read f; do s="${f%% \*}"; n="${f#\* }"; if \[\[ "$s" == "$s0" \]\]; then echo "'$n0' matches '$n'"; fi; s0="$s"; n0="$n"; done &amp;#x200B; A quick breakdown: s="${f%% \*}" &lt;-- this is a quick way to say "assign to s the value of f after removing all instances of " \*", or anything starting with a space". So it will turn "12345 My file is really great.txt" to "12345". You can do this with external command like sed or perl, but this is probably quicker and is at least shorter. n="${f#\* }" &lt;-- this is a quick way to say "Assign to n the value of f after removing the first instance of anything ending with a space". So it will turn "12345 My file is really great.txt" to "My file is really great.txt" . if \[\[ "$s" == "$s0" \]\]; then echo "'$n0' matches '$n'"l fi &lt;-- this compares the current file size against a previously-remembered file size and writes a thing if there's a match s0="$s"; n0="$n" &lt;-- Tells the loop to remember the old file size and name so that it can be compared against the next file. &amp;#x200B; Notes: I thought that using "while read" instead of putting it in a "for" loop would cause problems with remembering those variables for subsequent loops, but I guess I was wrong. But if you're curious, here's how I was originally going to do it: IFS=$'\\n'; for f in $(find . -exec stat -c "%s %n" "{}" \\; | sort -n); do s="${f%% \*}"; n="${f#\* }"; if \[\[ "$s" == "$s0" \]\]; then echo "'$n0' matches '$n'"; fi; s0="$s"; n0="$n"; done &amp;#x200B; Also, if you're doing this to try to remove superfluous files from your system to reduce clutter and save on space (and not, say, just being a student and answering a take-home test here), the ln command (without -s) is fantastic for allowing one file to exist in multiple places in a filesystem without taking up extra space.
BST isn't the timezone, it is the DST of the time zone. Set it to Europe/London and it will just do the right thing.
Thanks for your reply. When I set it to Europe/London and tried the output of date command. The time is shown as BST like below, `date` `Tue Jun 4 21:37:13 BST 2019` Is this expected? Are there any timezone as `/usr/share/zoneinfo/BST` ?
why wouldn't that be expected? That is the correct time for "Europe/London", as of a few mins ago.
Yeah. the time's correct. I was thinking about the "BST" that comes as a part of it. So are Europe/London and BST same? &amp;#x200B; I worked with U.S timezones but Europe, this is the first time!
Locations that have daylight savings have two abbreviations for their time zone - one with daylight savings, one without. GMT (I think) is the abbreviation for the standard time zone. BST is the abbreviation for the same time zone when daylight saving is being used.
BST is British Summer Time; it is the DST for GMT (Greenwich Mean Time) A deprecated timezone code would be GMT0BST, but you should be using "Europe/London" (or whatever city is closest, that observes the same DST changes) Forexample, I wouldnt use EST5EDT, I would use "America/Toronto" , or "America/New_York", or even "America/Nassau", etc.
If it waddles like homework and quacks like homework... What have you tried so far?
Got it. Thank you.
Alright. Thanks a lot..
I don’t have much bash knowledge and I must say I’d understand the script immediately with the regex, something used in countless languages, compared with me having to know that the -eq operator in bash is strictly arithmetic and will throw an error on non-numeric types. How about using the regex line with a comment, then everyone’s a winner!
You might want to check out `man run-parts`. It's built for what you're doing and is part of the base install for most common distros. It's got a bunch of features you might find useful. If you're doing your script as an exercise I won't try to talk you out of it, but in case you or someone else wants a robust, reliable way to do this out of the box you should be aware of it. NAME run-parts - run scripts or programs in a directory ... DESCRIPTION run-parts runs all the executable files named within constraints described below, found in directory directory. Other files and directories are silently ignored.
Open your browser email wallet keepass. This is what it can do?
&gt; I think the original is a hack, but it works for integers and it's straightforward to read. It may indeed be the only way to do it using POSIX shell and utilities only.
 /opt/bin/reddit
Try looking at the find command.
 &gt;So are Europe/London and BST same? Not quite. The timezone is named Europe/London. This uniquely identifies a particular set of rules to convert between Unix timestamps and human-readable strings. These rules are encoded in the [IANA timezone database](https://www.iana.org/time-zones). The timezone abbreviation you are seeing there is BST. That abbreviation may be different on different days of the year. Timezone abbreviations are _not_ necessarily unique, and they are subject to the whims of various governments.
First lesson to learn; 'scripting' is not 'programming' so script language (bash) is not programming language.
Have you started with any of the links in the subreddit sidebar? I've copied and pasted them below: &amp;#x200B; **GUIDES** * [BashGuide](http://mywiki.wooledge.org/BashGuide) – A Bash guide for beginners. * [Beginner's Guide to Command Line](http://cli.learncodethehardway.org/book/) – A crash course for some common unix and shell commands. * [Google's Shell Style Guide](https://google.github.io/styleguide/shell.xml) – Reasonable advice about code style. * [Sobell's Book](http://www.sobell.com/CR3/index.html) &amp;#x200B; **OTHER RESOURCES** * [ShellCheck](https://www.shellcheck.net/) – Automatically detects problems with shell scripts. * [BashFAQ](http://mywiki.wooledge.org/BashFAQ) – Answers most of your questions. * [BashPitfalls](http://mywiki.wooledge.org/BashPitfalls) – Lists the common pitfalls beginners fall into, and how to avoid them. * [The Bash-Hackers Wiki](http://wiki.bash-hackers.org/) – Extensive resource. * #bash – IRC channel on freenode. The main contributors of the BashGuide, BashFAQ, BashPitfalls and ShellCheck hang around there.
&gt; How about using the regex line with a comment, then everyone’s a winner! You should think so, and in many cases this is fine. It depends on the English language understanding of the person writing the comment though, which is something I have struggled with professionally. Another user suggesting simply setting a variable, $re_int="&lt;regex&gt;", which is even better.
This is wrong. Here's a [list of programming languages](https://en.wikipedia.org/wiki/List_of_programming_languages). Bash is Turing complete. I think you are confusing interpreted languages and compiled languages. (You can compile bash too, but it's not really useful.) They are both programming languages.
&gt; bash isn't as popular This is not true. BASH is everywhere. But people often use is as a starting point to go to e.g. perl, python, and advance from there. As an interpreted language, BASH is really easy to follow. #!/bin/bash echo "command 1" echo "command 2" This little script runs `echo "command 1"` and outputting `command 1` to the terminal, before running `echo "command 2"` and outputting `command 2` to the terminal. It runs from top to bottom. Pick any of the resources suggested by /u/datastry or purchase a book. I started out with the BASH bible, which has some old syntax but explains the concepts pretty easily.
Honestly, I like the original solution better. The only reason why anyone would call it a hack is because it prints an error message to stderr. If it silently returned non-zero instead, you could say it's working as intended.
You replied to my post asking about negated character classes :p You can use `case` too: isint() { case "${1#-}" in (*[!0-9]*) return 1 ;; esac; }
Arguably these are incorrect: $ isint ''; echo $? 0 $ isint -; echo $? 0 $ isint 000; echo $? 0
Good point. First 2 are easy fixes, the last one not so much.
It's most definitely programming.
This kind of parameter expansion performs a "search and replace" operation. Given a variable `$x`, you can do: ${x/foo/bar} to replace the _first_ instance of `foo` with `bar`, or: ${x//foo/bar} to replace _all_ instances of `foo` with `bar`. (The `foo` here is actually a glob, so you can use `*` and `?` as wildcards.) So in your particular example, the first two lines fill `$line` with the correct number of spaces, then: echo ${line// /-} replaces each of those spaces with hyphens, and echos the result.
Bash has a bunch of features for transforming/editing the contents of a variable while it is being accessed. The one that's used here is a search/replace feature. It replaces all space characters with a `-` character, see here an experiment at the command line: $ x="aaa bbb ccc: d e f" $ echo "${x// /-}" aaa-bbb-ccc:---d-e-f You can find documentation for this particular feature by doing `man bash` and then typing exactly `/parameter/` and Enter.
thats a fair point; if I recall, there are (were?) 2 BSTs
Some alternative BSTs I found in the database: $ TZ=America/Adak date -d 1980-01-01 # Bering Summer Time Tue Jan 1 00:00:00 BST 1980 $ TZ=America/La_Paz date -d 1932-01-01 # Bolivian Summer Time Fri Jan 1 00:00:00 BST 1932
Your PATH is an environment variable that dictates what programs you can run through a relative path instead of an absolute one with `./` and your .bash_profile is a bash script that runs when you log in, you can delete it all together if you want as its not needed. I only use it to define some environment variables like BROWSER etc. Also a heads-up that apple are changing the default shell from bash to zsh so watch out for that in any updates
Now I feel dumb
It would be nice to have better description of what you're trying to achieve as same byte size != same content If you need same content, I'd run some hashing function for all the files and look for files with same hash.
Use getopts and: for _word in $OTPARG; do _lots="${_lots} -b $_word" done
I just got it working with the for loop outside getopts, but yours is much better.
Jesus Christ, just what I need: changes right when I just started learning sth lol Should I stop learning bash and move onto zsh instead? From what I’m reading, those two are very similar.
I think there are more resources on bash, just keep in mind that some bash-isms wont work with zsh and if you want your scripts to be portable then thats a problem. Otherwise you can change your shell from zsh back to bash if you want
Thank you for the good advice. Is there a good reason for apple to change from bash to zsh? Or are they just being a holes?
&gt;Apple hasn’t explained exactly why it’s making this change, but bash isn’t exactly a modern shell as it’s implemented in macOS, and a switch to something less aging makes a lot more sense for the company. Apple is stuck using version 3.2 of bash that has been licensed under GPLv2, as newer versions are licensed under GPLv3. Apple has kept clear of using GPLv3 packages in macOS as the license is generally more restrictive to companies like Apple that sign their own code and it includes explicit patent grants, too. Looks like they want something newer aswell as something that isnt under a license as restrictive as the GPL, the license that covers zsh is closer to the 3 clause BSD license (excluding some shell functions which are covered by GPLv2) which is fitting for MacOS since its a BSD
may be a bit more clearly explained [here](https://www.tldp.org/LDP/abs/html/string-manipulation.html)
Thank you again for the detailed info. Very much appreciated!
In my dreams, he's a performance artist and his *true* "first lesson to learn" is about pedants that hang out on forums with zero desire to aid anyone. Instead they lurk for an opportunity to lob a correction at someone, like an unhinged sniper in a clock-tower taking aim at pedestrians in the square.
Who? Him or me? Or the both of us? I am definitely guilty of targeting "low hanging fruit" on forums sometimes. That's because I often browse reddit when I need to turn off :)
YEah just put a `-b` in front of those thing bro! $ printf -v VAR ' -b %s ' multiple things here $ echo $VAR -b multiple -b things -b here You can easily do that with parameters $ set -- multiple things here $ printf -v VAR ' -b %s ' $@ $ echo $VAR -b multiple -b things -b here
I agree with one clarification. You wouldn't use Python or Perl if you're a sysadmin to manage the system itself. A shell like bash, ksh or zsh are required.
for starters, feed the machine names in as a text file which you `read`... while read machinename ; do &lt;WHATEVER using "$machinename" &gt; ; done &lt; machine-names.list then when you add a machine, you just need to add the name to the file - if of course, you have set up the folder (mount?) so the script can access the files - although, that could be added to the script As for copying the files, you could set the target to ${machinename^^}HHZ which will change to uppercase - i.e. `macu` becomes `MACU` folders can be created with leading spaces using your existing command... mkdir 2019.{001..031}
True. Both perl and python have a history of powering the web. BASH powered the backend system.
Not you, but I can see how someone would make the argument that you're being pedantic because you corrected him. But that's a hypothetical, not my real intention. &gt;I often browse reddit when I need to turn off :) I am far too familiar with doing this too. (It's 2:20 am here)
I, and pretty much everybody I've ever worked with in my 20 year NIX career would definitely use bash, python and perl to manage systems, perl to a lesser degree these days but it was very popular for quite a while. I couldn't imagine a box that didn't run all three of those for management and automation. Of course now with containers, container managers and coreOS etc there are a lot of well built tools that abstract that management but somewhere behind all of that, there's a devOPS guy writing python (and some guys writing perl) scripts to get work done. With that said, I personally prefer bash, strongly, but that's just my personal preference.
You can't. If your conditions are independent of each other, just use multiple `if` statements: if true; then echo "hello"; fi if true; then echo "bye"; fi
Use a separate `if` statement. `elif` means "else if", so it is only evaluated when the first preceding condition is false.
Then how would I handle wildcard arguments if I'm using them to run different parts of the script?
This may help explain it... printf "" | wc -l echo "" | wc -l Here, you are adding a newline into the input with `echo` and i believe Here strings do the same thing.
&gt;grep 'abc123' &lt;&lt;&lt; "adasdasda" | wc -l # --&gt; 0 There is no output, no line that `wc -l` could count. &gt;echo "${test}" | wc -l &gt;wc -l &lt;&lt;&lt; "${test}" `wc -l` counts one empty line. &gt;mapfile -t testmapfile &lt;&lt;&lt; "${test}" The input of mapfile is one empty line, testmapfile will have one empty element. Note `&lt;&lt;` and `&lt;&lt;&lt;` always create \\n-terminated input ("lines").
Ah, OK. That kind of makes sense... So is the only way to do things "properly" to check whether the variable is empty before doing anything with it? `[[ -n "${test}" ]] &amp;&amp; mapfile -t testmapfile &lt;&lt;&lt; "${test}"`
&gt;properly You would assign the output of grep to an array: `mapfile -t testmapfile &lt; &lt;(grep ...)`
Ah! Thank you. I didn't think of doing that!
What is the purpose of a Wireguard vanity key? Why I ask the question: * In tor, I can understand why you might want to have a vanity key if you run a hidden service so your URL has some info in it rather than just randomness. * As far as I understand WireGuard is just a VPN application. Just curious how or why you might want a vanity key.
Because there is no comment section in the configuration files, so you might want to have a public key that tells you which machine it belongs to: OdiNgatdir[...] Or CaSTleghzuormva572[...] Instead of just random junk :)
Odd. Are you sure 'history' executes the right program?
It's also strange for me. \`history\` is the bash-builtin command
Did you set up a different history file than standard `.bash_history`? If so, change it back and see? I have no idea really, but that's something I would try. Very strange.
Ahhhhh gotcha. I have exactly one VPN; I hadn't considered the difference.
I echoed `HISTFILE`, which is the standard `.bash_history`. I did setup a new location after sourcing my personal environements file.
Make sure no other functions or aliases are replacing `history`: :~ $ type history history is a shell builtin Make sure your HISTFILE is readable and has a non-zero size. (Use the variable in the command, don't type the filename, so we don't have to trust your eyes when you tell us they're the same): :~ $ ls -l $HISTFILE -rw------- 1 stu stu 39312 May 25 11:55 /home/stu/.bash_history Make sure you have `history` in your SHELLOPTS. :~ $ echo $SHELLOPTS braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor The `history` shellopt is only set by default for interactive shells. If you're using some kind of process like a script or tool that calls the `history` command in a shell that isn't set interactive, the history command won't return anything (unless you `set -o history` first). Watch when I ssh to a host and run `echo $SHELLOPTS`, :~ $ ssh drop ... stu@drop:~$ echo $SHELLOPTS braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor Now watch when I run `echo $SHELLOPTS` non-interactively via ssh: :~ $ ssh drop echo \$SHELLOPTS braceexpand:hashall:interactive-comments :~ $ ssh drop history :~ $ # nothing
Code academy has a great course.
I ran into this recently and started testing the output with `[[ -n “$var” ]]`. I need to avoid using the `&lt;(...)` trick since that doesn’t expose the exit status of the command in the parens. So I store the output in a variable, maybe validating the exit status before passing to `mapfile` and then also testing whether it actually contains anything. I have yet to build a helper function to help me with this so I’m repeating this like 6 line incantation every time. Blah.
What is your $HISTSIZE? Maybe that's zeo somehow?
Thanks. I had worked around the problem. It't that the `HISTSIZE` wasn't set to a suitable value. I wanted the `HISTSIZE` to be unlimited. It's different before bash-4.3.
Thank you for your detailed explanation. I run the commands as you described. The result is the same with you. The problem is that the `HISTSIZE` wasn't set to a suitable value. I want the `HISTFILE` to be unlimited and set `HISTSIZE` to -1. But it's wrong in and before bash-4.2.
Mine is just set to 15000 or something
Now I just leave \`HISTIZE\` to empty for unlimited size.
I modified a little bit using the tips you gave me. Should be a bit better! #!/usr/local/bin/bash # Script to burn an iso image into a USB clear printf "\n--- [ USB devices ] ---\n\n" if [ ! -z "$(diskutil list external)" ]; then diskutil list external printf "Choose one disk: "; read disk else printf "No USB device found, please insert one\n\n" exit 1 fi if [ -b "$disk" ]; then #echo "$disk is block device - continuing" printf "\nNew device name: "; read name printf "\n--- [ ISO ~/ ] ---\n\n" find ~ -type f -maxdepth 2 -iname "*.iso" printf "\nImage: "; read iso if [ -f "$iso" ]; then printf "Write this ISO to '$disk' - '$iso'" printf "Erase disk '$disk' and label it '$name'" read -n 1 -r -p "Continue (Y/n)? " myCont case $myCont in Y|y) # Fall through - expected case printf "\n\n--- [ Formatting ] ---\n\n" diskutil eraseDisk FAT32 $name MBR $disk printf "\n\n--- [ Unmount ] ---\n\n" diskutil unmountDisk $disk printf "\n\n--- [ Burning ] ---\n\n" sudo dd if=$iso | pv -s $(stat -f ‘%z’ “$iso”) | sudo dd of=$disk bs=1m printf "\n\n Operation completed\n" ;; *) # Anything else...abort printf "\nQuitting..." exit 1 ;; esac else printf "\nFile doesn't exist" exit 1 fi else echo "$disk is NOT a block device - exiting" exit 1 fi
That's the proper setting. Not -1 ;-)
Just to further clarify, there is no language out that that treats its “else if” syntax the way OP wants. This isn’t about bash at all. This is what elif always means in programming.
I don't understand - doesn't this severely reduce entropy, and thus, security?
My config file has comments in it?
Will try, thank you
Nah, you just brute Force the generation and pick a key you like
It may, but they are not part of the format. Using one ID instead of weird ID + comment is also easier
Isn't picking the key you like the definition of removing entropy?
So is having a root password of "root"
!solved
My sysadmin scripts very often end up having sections of Perl in them. The ability to just randomly inject a multi-line `perl -e '... whatever I need ...'` in the middle of my bash script is super handy!
A related idea that might find useful: the terminators for `case` clauses: * `;;` at the end of a clause means "stop this `case` statement * `;;&amp;` at the end of a clause means "test the following patterns and execute their clauses if they match" * `;&amp;` at the end of a clause means "run the commands in the following clause as if it were part of this one" ``` test_a() { local input="" input="$1" case "$input" in *a*) printf "'%s' ran clause 'a'\n" "$input" ;; *b*) printf "'%s' ran clause 'b'\n" "$input" ;;&amp; *c*) printf "'%s' ran clause 'c'\n" "$input" ;&amp; *g*) printf "'%s' ran clause 'g'\n" "$input" ;; *) printf "'%s' ran clause default\n" "$input" ;; esac } ``` For example: ``` $ ./test_a abc 'abc' ran clause 'a' $ ./test_a bc 'bc' ran clause 'b' 'bc' ran clause 'c' 'bc' ran clause 'g' $ ./test_a bd 'bd' ran clause 'b' 'bd' ran clause default $ ./test_a bg 'bg' ran clause 'b' 'bg' ran clause 'g' $ ./test_a c 'c' ran clause 'c' 'c' ran clause 'g' $ ./test_a d 'd' ran clause default ```
Actually what I was looking for. Thanks
Marked the file as read only? Set the number of lines to 0 in the .bashrc file? Don't have permissions to write to the file?
Is this approximately what you want? ``` #!/usr/local/bin/bash process() { local usage="" read -r -d '' usage &lt;&lt;-EOF Usage: process «year» «machine_name» EOF if (( $# &lt; 2 )); then 1&gt;&amp;2 printf "%s\n" "$usage" return 1 fi local year="$1" local machineName="$2" [[ -d data ]] || { mkdir -p data || { 1&gt;&amp;2 printf "cannot create directory '%s'\n" "data" return 1 } local subdir="" for num in {1..31}; do printf -v subdir "./data/%s.%03d" "$year" "$num" [[ -d "$subdir" ]] || { mkdir -p "$subdir" || { 1&gt;&amp;2 printf "error creating directory '%s'\n" "$subdir" return 1 } } done } if [[ ! -d "$machineName" ]]; then 1&gt;&amp;2 printf "'%s' is not a directory\n" "$PWD/$machineName" return 1 fi local upperName="${machineName^^}" for file in "./$machineName/"*; do if [[ -d "$file" ]]; then local srcDir="${file##*/}" # srcDir is everything after the last '/' character for hhz in "$file/"*HHZ; do local srcFile="${hhz##*/}" if [[ "$srcFile" != "${upperName}HHZ" ]]; then 1&gt;&amp;2 printf "skipping file '%s' because its name looks weird\n" "$hhz" continue; fi local srcPath="$hhz" local destDir="" printf -v destDir "./data/%s" "$srcDir" if [[ ! -d "$destDir" ]]; then 1&gt;&amp;2 printf "destination diretory '%s' for input file '%s' does not exist\n" "$destDir" "$hhz" return 1 fi local destFile="" printf -v destFile "%s%s" "$srcDir" "$srcFile" local destPath="" printf -v destPath "%s/%s" "$destDir" "$destFile" printf "copying '%s' to '%s'\n" "$srcPath" "$destPath" cp "$srcPath" "$destPath" done else 1&gt;&amp;2 printf "skipping '%s' because it is not a directory\n" "$file" fi done } while read -r machineName; do process 2019 "$machineName" done &lt;&lt;-EOF stam macu two word name exists nonexistient_name nonexistintent two word name EOF printf "done\n" ```
For files like this: ``` ./stam ./stam/2019.002 ./stam/2019.002/STAMHHZ ./stam/2019.002/MACUHHZ ./stam/2019.001 ./stam/2019.001/STAMHHZ ./two word name exists ./two word name exists/2019.002 ./two word name exists/2019.002/TWO WORD NAME EXISTSHHZ ./two word name exists/2019.foo ./two word name exists/2019.001 ./two word name exists/2019.001/FOOHHZ ./two word name exists/2019.001/TWO WORD NAME EXISTSHHZ ./macu ./macu/2019.002 ./macu/2019.002/MACUHHZ ./macu/blah ./macu/2019.001 ./macu/2019.001/MACUHHZ ``` I see: ``` copying './stam/2019.001/STAMHHZ' to './data/2019.001/2019.001STAMHHZ' skipping file './stam/2019.002/MACUHHZ' because its name looks weird copying './stam/2019.002/STAMHHZ' to './data/2019.002/2019.002STAMHHZ' copying './macu/2019.001/MACUHHZ' to './data/2019.001/2019.001MACUHHZ' copying './macu/2019.002/MACUHHZ' to './data/2019.002/2019.002MACUHHZ' skipping './macu/blah' because it is not a directory skipping file './two word name exists/2019.001/FOOHHZ' because its name looks weird copying './two word name exists/2019.001/TWO WORD NAME EXISTSHHZ' to './data/2019.001/2019.001TWO WORD NAME EXISTSHHZ' copying './two word name exists/2019.002/TWO WORD NAME EXISTSHHZ' to './data/2019.002/2019.002TWO WORD NAME EXISTSHHZ' skipping './two word name exists/2019.foo' because it is not a directory './nonexistient_name' is not a directory './nonexistintent two word name' is not a directory done ``` and end up with files ``` ./data/2019.002/2019.002STAMHHZ ./data/2019.002/2019.002MACUHHZ ./data/2019.002/2019.002TWO WORD NAME EXISTSHHZ ./data/2019.001/2019.001STAMHHZ ./data/2019.001/2019.001TWO WORD NAME EXISTSHHZ ./data/2019.001/2019.001MACUHHZ ``` plus directories ``` ./data/2019.001 ./data/2019.002 ./data/2019.003 ./data/2019.004 ./data/2019.005 ./data/2019.006 ./data/2019.007 ./data/2019.008 ./data/2019.009 ./data/2019.010 ./data/2019.011 ./data/2019.012 ./data/2019.013 ./data/2019.014 ./data/2019.015 ./data/2019.016 ./data/2019.017 ./data/2019.018 ./data/2019.019 ./data/2019.020 ./data/2019.021 ./data/2019.022 ./data/2019.023 ./data/2019.024 ./data/2019.025 ./data/2019.026 ./data/2019.027 ./data/2019.028 ./data/2019.029 ./data/2019.030 ./data/2019.031 ```
Whatever. Finding the public key will be hard, no matter what you choose your public key to be.
"it will still be hard" isn't valid cryptography though.
Your config comments won't help when interpreting the output of `wg show`
Fair, but that still doesn't justify an extreme weakening of your keys.
It is. It's the idea of public key crypto.
Yes, and public key crypto assumes secure keys.
Do you need a language responsible for listen a tcp port, try using python and flask to do that! It is more professional!
Why not? I'd not do this in production but that doesn't sound like what you're doing. But I wouldn't use bash for the webserver part and dealing with JSON is where I stop using bash. Fortunately, Sinatra is fairly easy to set up and is a great way to provide endpoints, and Ruby is more than capable of executing and parsing code (much better than Python imho). Don't forget to version your API in the URL!
You have to reverse things and do `spinner &amp;`. You need to run `doScan` without a `&amp;`. The problem is that the things you use `&amp;` on are started in a second process. The changes to variables done in that other process are invisible to your script's main process.
In general you just need any backend connected to the internet that can accept http requests. Open a port, accept request, sys call to the appropriate controller which executed the script. Then most sys call implementations will read stdout which you can then send back as a response. Or you can process the output from your script and return that.
Yeah sure. Lots of ways- simplest way in my mind is to use a light web framework to front the scripts and provide all the url parsing and pathing. You could do it in nodejs with express, or groovy/Grails, or kotlin/micronaut, or ruby/rails, even php. Node is pretty easy to get up and running and has lots of docs. Your express app could just make native calls the bash scripts and render the results back to the client.
Using shell2http for a while for exactly same task and it works like a charm.
I actually have some experience with python and flask, but I've only made basic applications so my knowledge doesn't extend as far as making rest APIs with it, but I'll look into it.
Here are two solutions that I've used in the past. They're both great and should have any feature you might want. https://github.com/msoap/shell2http https://github.com/adnanh/webhook
Sure. Any decent language can help you accomplish this. If you would like to know how this is done in Go, have a look at the following code: package main import ( "encoding/json" "fmt" "log" "net/http" "os" "os/exec" "github.com/gorilla/mux" ) // create a data structure that can hold the response from the script type scriptResponse struct { Response string } func scriptHandler(w http.ResponseWriter, r *http.Request) { // this handler reads the environment variable PATH_TO_SCRIPT // so it knows where you script is located scriptDirectory := os.Getenv("PATH_TO_SCRIPT") params := mux.Vars(r) // adding ".sh" extension to whatever filename you specify script := params["scriptname"] + ".sh" fullpath := fmt.Sprintf("%s/%s", scriptDirectory, script) cmd := exec.Command(fullpath) results, err := cmd.Output() if err != nil { log.Println(err) return } // convert results into string and populate an instance of // the scriptResponse struct response := scriptResponse{string(results)} // encode response into JSON and deliver back to user encoder := json.NewEncoder(w) err = encoder.Encode(response) if err != nil { log.Println(err) return } } func main() { port := 8080 r := mux.NewRouter() r.HandleFunc("/api/script/{scriptname}", scriptHandler) log.Printf("Starting webserver on port %d\n", port) log.Fatal(http.ListenAndServe(":8080", r)) } You can run this code as shown below: $ go run main.go 2019/06/06 19:40:26 Starting webserver on port 8080 Do keep in mind the code assumes there is an environment variable `PATH_TO_SCRIPT` it can read which tells it where the scripts are located &amp;#x200B; To quickly set that environment variable for that one command, you could do the following: $ PATH_TO_SCRIPT=/your/script/directory go run main.go 2019/06/06 19:42:01 Starting webserver on port 8080 I have two scripts sitting around that look like this: $ cat helloworld.sh #!/usr/bin/env bash printf "Hello, World!" $ cat hey.sh #!/usr/bin/env bash printf "hey there!" You can make requests to the Go program to run the scripts and return results as shown below: $ curl localhost:8080/api/script/helloworld {"Response":"Hello, World!"} $ curl localhost:8080/api/script/hey {"Response":"hey there!"}
It's easy enough to put a pile of any kind of executable behind apache with cgi script enabled. It even performs reasonably well. [https://httpd.apache.org/docs/current/howto/cgi.html](https://httpd.apache.org/docs/current/howto/cgi.html) As with all things apache there are thousands of ways to configure this and they stack pretty well. The code path remains reasonably optimal. You will want to add one of apache's authentication modules to deal with your username/password requirement. I've used \`authnz-ldap\` quite a bit but there are many others to choose from.
JQ does a pretty good job dealing with json in bash. But I agree, I would rather handle it with ruby.
I already did. Python flask, if you're familiar with it. [https://github.com/atarola/pyjojo](https://github.com/atarola/pyjojo) #!/bin/bash # -- jojo -- # description: echo text on the command line # param: text - text to echo # -- jojo -- echo "echo'd text: $TEXT" echo "jojo_return_value name=bob" echo "jojo_return_value age=99" exit 0 pyjojo -d --dir /srv/pyjojo curl -XPOST http://localhost:3000/scripts/echo -H "Content-Type: application/json" -d '{"text": "hello world!"}' Rest response: { "retcode": 0, "return_values": { "age": "99", "name": "bob" }, "stderr": [], "stdout": [ "echo'd text: hello world!" ] }
or netcat has an option to bind to a port and cat a file
Have a look at http://cr.yp.to/ucspi-tcp.html
Wouldn't it make more sense to ignore the reports, and all of the output preceding them? You can even give pylint a format string for those, if it makes your parsing easier.
Much better :) There’s some style evolution you’ll no doubt go through, but for starting out, that’s a good result. Stay curious, keep practising and you’ll get the hang of it.
awk would be an easy way to parse it I think &lt;output&gt; | awk -F'|' '/warning/ {print $3}' I don't fully understand the depth of the script you want, but this is a place to start from
The output reports don't apply to the summaries (annoyingly). Parsing for metrics gathering.
Agreed awk is where I'd start. Looking to create a small function that allows caller to lookup a cell kind of like a SQL query or col/row lookup.
Style is definitely something will come with time :) I can tell even if I'm a beginner. Thank you very much for your help and support! Have a good one!
It is a \_restructuredtext\_ table cf [https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#tables](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html#tables)
When I have problems with cron jobs it's usually some missing environment variable. For a one-shot test you can run the script via at: echo "/your/script.sh your args ..." | at now I like to add the following to the head of my scripts: exec 2&gt; /tmp/some.err exec &gt; /tmp/some.log echo "${@}" &gt; /tmp/some.cmd env &gt; /tmp/some.env set -x
Do you really need to define single-string arrays in your script? what's the shebang at the start of the script? did you set it executable? use full executable paths when in cron. e.g. `/usr/bin/curl`. cron has no environment, including `$PATH`
Thanks, and happy cake day. When I run the cron job script w/ this command: `echo "/opt/splunk/etc/apps/d0d0d0d0d0/bin/pf_syst_sp.sh" | at now` it returns, `job 1 at 2019-06-07 10:51` and loads the data temp file again w/ at that time stamp. What should I see with that command? I'll try and add those commands to the header and see what happens when the script runs at it's scheduled time.
 &gt;Do you really need to define single-string arrays in your script? \-- nope, but it's my template, and easier to change certain variables each time as I know where they are, than have to scan the script for them. shebang is `#!/bin/bash` Yep, they're set to executable, otherwise they wouldn't run when I run them manually. I can try the full paths, but again, it works when running the script manually.
cron has no $PATH variable = it doesn't know where executables are.
You could use sed to delete the separator lines and then use awk to parse the remaining. Also as mentioned in the other answer you could get the output in a more parseable format. But all of this IMO is going backwards as the pylint module already does this. To access the data you can call pylint from a python script import pylint.lint Linter = pylint.lint.Run(["script.py"], exit = False) The above `Linter` object as a `linter` class that has all the stats and can be used to generate messages quite easily. For example to get all the stats in your table as a Python dictionary use `Linter.linter.stats`. To print reports as needed use `Linter.linter.generate_reports()`. You can do much much more detailed analysis using the methods of this class, see the documentation of the `pylint.lint` module for more.
Thanks for saying it in plain english, apparently, my brain couldn't make it out when you said it the first time. Will try that.
This should be at the top. A quick search gave me this: [parsing ASCII grid tables with Docutils](https://eli.thegreenplace.net/2017/a-brief-tutorial-on-parsing-restructuredtext-rest/).
Interesting documentation, thankyou for sharing that :-)
I am not sure I understand everything what you did here. I am a starter after all xD. But thanks for the help, I tried this code but it didn't work so much.
awk and clever grep usage can do it
That was a nice explanation. I've never used go but I have an interest in learning it, is that your prefered language? I am leaning more towards using either python/flask nodejs/express for this, and I feel like I can adapt your example to other frameworks, thanks!
Hey, CLOUDOPS1, just a quick heads-up: **prefered** is actually spelled **preferred**. You can remember it by **two rs**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
You're welcome. Go is defintely a "get shit done" type of language when it comes to writing things like APIs and microservices. But yeah, use whatever language you're most comfortable with and helps get the job done.
I'm kind of confused as to how arguments would be handled through CURL. Suppose I have [https://localhost/helloworld.php](https://localhost/helloworld.php) , and the PHP script just makes syscall to execute a shell script that returns {Hello, {name!}} where {name} is a parameter, maybe it's the first parameter ($1) e.g. "./helloworld bob" How would I pass the parameter "bob" through CURL?
Do you have any tips on how parameters could be passed through this? What if instead of just prinf "Hello, World!" the script executes printf "Hello, "$1" For example, if I wanted the first argument ($1) to be "Bob", e.g. "./helloworld bob" so that "Hello, bob" is printed to stdout. And thanks again for taking the time to explain!
They are loaded dynamically. bash_completion adds a default completion function which is triggered whenever you try to do completion on a command that currently does not have any completions set. E.g. if you run `git checkout &lt;tab&gt;` and there's no completion set for git, the default completion function is triggered, which sources `/etc/bash_completion.d/git` if it exists
Then add a line to the program and change another line name := r.FormValue("name") cmd := exec.Command(fullpath, name) The first line is the one being added that reads the URL query parameter The second line is the one slightly being changed so that you're executing your bash script with an argument (which is the query parameter you provide in your request) &amp;#x200B; Here is the bash script: #!/usr/bin/env bash printf "hey there!, $1" If you make your request, you'll see it works: $ curl localhost:8080/api/script/hey?name="Bob" {"Response":"hey there!, Bob"} If you don't provide a query parameter, you'll just get the following: $ curl localhost:8080/api/script/hey {"Response":"hey there!, "}
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
Doesn't look to be any error with the if statement itself, so something tells me maybe something is wrong with the command syntax? Are you sure it's running without error and doing exactly what you want? What does the cron command look like, does it just run this? Also, instead of repeating that long command with just one small difference, you should assign 'exposure-mode' to a variable and just have that big command displayed once, it'd make it much more readable.
I'd bet the leading zeroes are causing a problem with straight numeric comparison. :~ $ currTime=1200 :~ $ if [[ "$currTime" -ge "2000" ]] || [[ "$currTime" -le "0600" ]]; then echo dark; else echo light; fi light :~ $ currTime=2001 :~ $ if [[ "$currTime" -ge "2000" ]] || [[ "$currTime" -le "0600" ]]; then echo dark; else echo light; fi dark :~ $ currTime=0300 :~ $ if [[ "$currTime" -ge "2000" ]] || [[ "$currTime" -le "0600" ]]; then echo dark; else echo light; fi dark :~ $ currTime=0359 :~ $ if [[ "$currTime" -ge "2000" ]] || [[ "$currTime" -le "0600" ]]; then echo dark; else echo light; fi -bash: 0359: value too great for base (error token is "0359") -bash: 0359: value too great for base (error token is "0359") light :~ $ I'd drop minutes out of the picture if you're only concerned about what hour it is, but that might not solve the problem. I don't see a way to get `date` to show the hour in 24-hour format without a leading zero, but it wouldn't be hard to strip: currHour=$( date '+%H') currHour=$(sed 's/^0//' &lt;&lt;&lt; $currTime) Then your tests would need to be `[[ "$currHour" -ge "20" ]] || [[ "$currHour" -le "6" ]]` Hopefully someone shows up with a more graceful way to recommend hour-of-day tests in bash, but I think this would work better than what you've got.
[http://lmgtfy.com/?q=bash+sms+message](http://lmgtfy.com/?q=bash+sms+message) [http://lmgtfy.com/?q=bash+facebook+message](http://lmgtfy.com/?q=bash+sms+message)
You’re welcome! And thanks for the gold!!
I’m not sure if this is what you’re trying to do but sunwait might be relevant for you as an alternative way to solve the problem. https://github.com/risacher/sunwait
Use string comparison instead of integer comparison if [[ $currTime &gt; 0600 &amp;&amp; $currTime &lt; 2000 ]]; then ...mode=1 ... else ...mode=2 ... fi Note that there's no `&gt;=` and `&lt;=` operators, so you negate &lt; and &gt;, respectively, instead
That's pretty cool, thanks! I can definitely use this for what I plan to build eventually.
Thank you for the suggestion. The command syntax is proper. It works perfectly when I do it manually. It also runs automatically, just always with the daytime settings. Using a variable and inserting it into the command at the end would definitely be cleaner, but I am very green to bash scripting and thought messier code that works is better than having to trouble shoot syntax at this point.
Good stuff. I made those changes and we'll see how it goes tonight. Thank you.
Is it possible to make a script to message someone over WiFi/Lan??
To a point you're right! Getting things working is certainly the important thing, and if it can be cleaner then that's just an added bonus. As to why this isn't working properly, I'm not sure, I suspect /u/MurkyCola may be right, but I would not have thought of that.
Not sure I understand completely. Do you mean something like this? $ rename -n 's/^.*g\.//' really.long.filename rename(really.long.filename, filename) In this example, we specified a piece of a filename and replaced everything till that piece with nothing.
Thank you! I'll see how I get on with that. Apologies for the confusion, I was visualizing two scenarios: one where the crop is based on character count and placement is defined (a bit like left, mid and right in excel) or as your example, specify a partial string and replace it with nothing. I need to read up on this some more.
If you give me details about what confused you or what you saw when tried it, I can try to explain or help. The part that starts \`process() {\` is a function definition. The part that starts \`while read -r machineName; do\` reads a list of machine names (one per line) and calls the function process with year \`2019\` and each machine name.
Another method is to add any leading digit to get bash to not interpret it as octal: current="1$(date '+%H%M')" if (( current &gt;= 12000 || current &lt;= 10600 )); then ...
How about this: cd ~/folder; find . -type f -print | while read f; do sha256sum "$f"; done | uniq -Dw 65
I agree, that doing it directly, where the data is coming from, is the better solution but &gt; You could use sed to delete the separator lines and then use awk to parse the remaining. Prolly best to do it all in awk. I'd set FS to pipe and space and print the desired fields for lines where NF is greater than 1 (the lines without data).
&gt; shebang is #!/bin/bash Often ignored by cron unfortunately, at least for the implementations I'm familiar with. Depending on what actual shell ``/bin/sh`` links to on your system, this can have various results. Add ``SHELL=/bin/bash`` in the crontab file, or try explicitly calling it with bash: 07 08 * * * bash /opt/splunk/etc/apps/d0d0d0d0/bin/pf_syst_sp.sh
No.
cron does set PATH, it's just that it sets it to only a few dirs, like `/usr/bin:/bin` Don't use absolute paths to commands, just set your own PATH at the start of the script instead.
&gt; &gt; shebang is #!/bin/bash &gt; Often ignored by cron unfortunately cron isn't the one to parse the shebang, that's the kernel's job. Whether you execute it from an interactive shell or from cron is irrelevant, in either case the kernel will use the shebang. Your experiences with it "being ignored" is more likely that the shebang used `#!/usr/bin/env some-interpreter` with `env` failing to find some-interpreter in the limited default PATH set by cron. The solutions in those cases is to either change cron's PATH, or use the absolute path to the interpreter in the shebang.
I did not even know 'rename' existed. I always used mv.
Thank you! This is the way to go. Im flipping to this as a solution.
The solution to leading zeros is simple: `date -%H%M` instead of `date +%H%M` If you need date output for maths, always use `-` instead of `+`.
As u/MurkyCola said, the issue is with leading zeros. Date can output without them: `date +%-H%M` would output the format you need: it won't add leading zeros to hours, and will add to minutes. from man date(1): By default, date pads numeric fields with zeroes. The following optional flags may follow '%': - (hyphen) do not pad the field _ (underscore) pad with spaces 0 (zero) pad with zeros + pad with zeros, and put '+' before future years with &gt;4 digits
Let me archive this post in the comments because eventually OP will probably notice how stupid they are and just delete it. u/KodutuPoiss wants to know: **Subject:** Question **Text:** Is it possible to make a scrip that messages someone over MSM/Facebook?? [sic]
Well, probably yes. But hey...
To send messages via Facebook I suggest you to have a look to [the messenger developer documentation](https://developers.facebook.com/docs/messenger-platform), you'll probably need a Facebook developer account to use the API. On the other hand, there is multiple ways to send a message via your telecom carrier, it could be performed with a SIM card adapter and a proper driver, or you can pay a subcontractor like [twilio](https://www.twilio.com/sms) or even via your carrier itself (mine provides this functionality through an API). I hate those "Google it" answers, there is nothing constructive about it but they are not completely wrong, your question is obviously too broad, you should probably have started there in the first place. Good luck
I've used [xdotool](https://manpages.ubuntu.com/manpages/trusty/man1/xdotool.1.html) for similar tasks before (e.g. for automating the setup of a task that required a terminal with a dozen tabs open). That being said, without more info, I'm not sure that you'd really need to open a terminal for what you're trying to do. You would need a terminal to run the bash script in the first place.
Is it possible that you just want something to run in the background? In which case you could just: `$ ./shell-script.sh &amp;`
I agree with this, your best bet is to take a look at the developer documentation. Also it would probably be easier to write a client using a "programming language" rather then bash.
screen is a great tool as well. I've used that one for a long time.
Didn’t scroll the entirety because there’s a ton of one-time use commands that would be a bear to source every interactive shell... For iptables if you’re blocking it should be inserted before any other rules can intervene and moreover ipset is a better solution.
Good stuff! I wrote our company dev environment shortcuts into a dot denoted namespace for a lot of this similar functionality. &lt;namespace&gt;.&lt;func&gt;
It's regarding packaging stuff for linux, atvthe moment everything runs in on terminal window but I'd like to have it seperate window if something fails so I know on which programm it fails
Hey, Girtablulu, just a quick heads-up: **seperate** is actually spelled **separate**. You can remember it by **-par- in the middle**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
I'd suggest moving functions into `.bash_functions` and aliases into `bash_aliases`.
It's good, but at first glance I noticed a lot of things that make calls to external programs that can be replaced with pure bash internals, like `[` which is actually the binary `test` (or a shell built-in) vs `[[` which is a bash keyword, or your call to awk for string length that can be replaced with `printf "%s" "${#1}` https://askubuntu.com/questions/445749/whats-the-difference-between-shell-builtin-and-shell-keyword Simply put, because I'm on mobile, in terms of efficiency, it's something like this : keyword &gt; built-in &gt; external binary Another good read, why you should use `printf` and not `echo` https://askubuntu.com/questions/445749/whats-the-difference-between-shell-builtin-and-shell-keyword
Congratulations man, looks awesome! I'm building [mine](https://github.com/0ero1ne/config/tree/master/dotfiles) and add little things every day as I go learn more, will definitely look into yours. I made different files for aliases, functions and colors. Which way you recon is quicker?
I'd suggest making multiple files for each purpose and just including them to load in `.bashrc` I have seperated them by * General * System specific * Host specific * Navigation aliases * misc * functions * bash configuration * include file (this is included in `.bashrc` and is including the upper config files) I created a menu that lets me choose which file to edit.
That sounds even better!
./script 2&gt;&amp;1 logfile.log &amp;
 The Menu: 1) Bashrc file 7) Bash functions 2) General aliases 8) Bash configuration 3) System specific aliases 9) Bash includes 4) Host specific aliases 10) Bedit configuration (That's me!) 5) Bash navigation aliases 11) Show Aliases 6) Bash miscellaneous aliases 12) Quit Select File to edit: [The Bedit file (10)](https://termbin.com/429a) [The include file (9)](https://termbin.com/zwbm)
Well unless you'd want to set up your own actual gateway tied into the cellular network, you would probably use a curl one-liner to call the API/webservice of the service provider. However I also agree that you might want to wrap that in an internal API of yours, in which case bash is out of the game.
I would have moved most of these functions to their own script files.
uuuuh yea totally forgot that solution
As the others suggested it is much better split into multiple functions or separate scripts for specific purposes as at this point it is almost impossible to manage. Also they slow down the startup time of a interactive session due to the time necessary to source the whole file and also clutter up the namespace. Lot of these doesn't really need a function. There is really no point in having separate functions for killing different processes (vagrant, pulse etc) or making tarballs with different compressions (gzip, xz etc). All of these can be put in separate scripts that are more generalized and flexible. Git aliases can be moved to global git config rather than bashrc. IMO .bashrc should have those settings that are specific to bash (modifying PATH and other environment variables or other aspects of bash behavior) and those aliases/functions that are general purpose, small and useful enough to be needed at the start of every interactive session.
What one time use commands? You don't mean the ones wrapped in functions do you?
You can just echo the results to the file `echo "$time: $name: $age: $location" &gt;~/Desktop/Users.log`
You can do that with 'trap': trap 'tput cnorm' INT "trap" makes it so a special command line gets run when your script's process receives a signal. "INT" is the name of the signal that gets sent when you type Ctrl-C. Check out `help trap` for more info. You can also create a function where you collect everything there is to do when your scripts quits and then use that for your 'trap': cleanup() { # ... } trap 'cleanup' INT TERM EXIT The "TERM" in this example here is the name of the signal that the 'kill' command by default sends to a program. I guess that one is also a good idea to capture with 'trap', not just "INT". The "EXIT" isn't a real signal, it's something that bash pretends is a signal and triggers whenever your script exits. If you decide to use this, you won't have to put `tput cnorm` at the end of your script anymore, it will get run through this trap instead.
Have a look at https://github.com/scop/bash-completion and the 'examples' currently in use on your machine at /usr/share/bash-completion/
Wow. `[ ... ]` carries around a 42% performance overhead over `[[ ... ]]`. I knew they just had to be slower, but your post prompted me to try and crudely measure the performance difference between the builtin and the expression and the results are pretty telling: ``` $ time bash -c 'for (( i=0; i&lt;1000000; i++ )); do [[ "" ]]; done' real 0m2.137s user 0m2.136s sys 0m0.001s $ time bash -c 'for (( i=0; i&lt;1000000; i++ )); do [ "" ]; done' real 0m3.039s user 0m3.038s sys 0m0.001s $ bc &lt;&lt;&lt;'scale=3; 3.039 / 2.137' 1.422 ``` Thanks for the insight, /u/Gorian
Sure thing - cool that you tested it! There's a lot of intricacies in bash and things I've learned over 10 years that you don't always initially think about.
Zsh with oh-my-zsh has amazing tab completion for things like this
That's a lot of trouble to find your IP, when the server already knows it immediately when a client connects. Put this on the server: #!/bin/bash echo ${SSH_CLIENT%% *} | tee ~/sship Then from the client: ssh server-ip /home/raven/logmyip.sh
Wow! Thanks for this.
If you use the hostname you don't even need to know the IP, your router will determine it automatically.
Tried but not working.
&gt;gist.github.com/ageis/... Wait, so function names in bash can contain periods? :-o
Most home routers don't publish hostnames for clients that get assigned dynamic IP addresses.
Her is a function I wrote long ago: `SelectItem() { # $@: Request string + list of items (strings). Prints selected item, to StdOut, if successful.` `local Request="$1" # Prints Item_list + Request string to StdErr so that they are shown in 'Selection=$(SelectItem ...)' invokations.` `shift # Use example: 'SelectItem "Select a number" "1" "2" "3"'.` `if [ $# -eq 1 ]` `then` `Request="$Request (1, 0: Exit): " # To avoid automatic selection.` `else` `Request="$Request (1..${#}, 0: Exit): "` `fi` `local ItemNr=0` `while [ $ItemNr -lt $# ]` `do` `let ItemNr++` `echo "$(printf "%${##}d" ${ItemNr}). '${!ItemNr}'" 1&gt;&amp;2 # 'printf "%${##}d"': Print argument with ${##} width. '${##}': Number of digits in $#.` `done # Print Item list.` `ItemNr="";echo 1&gt;&amp;2` `until test "$ItemNr" -ne 0 -o "$ItemNr" -eq 0 2&gt;/dev/null &amp;&amp; [ $ItemNr -ge 0 ] &amp;&amp; [ $ItemNr -le $# ] # 'test': Test if number.` `do` `read -p "$Request" ItemNr 1&gt;&amp;2` `done # Read user's choice.` `if [ $ItemNr -eq 0 ]` `then` `return 90 # Selection cancelled.` `fi` `echo "${!ItemNr}"` `return 0` `}`
Do you written that function?
Yes. I said so in my first comment.
Thanks, it works. I will give credit to you for that function. What is your username on Github?
I don't use github. Use 'glesialo' if you want.
Ok
Is that true? All the routers I've had do it out-of-the-box without any special configuration.
 #!/usr/bin/env bash oldPS3=$PS3 PS3="Your selection? " echo "0) exit" select choice in $(oneliner); do ((!$REPLY)) &amp;&amp; PS3=$oldPS3 &amp;&amp; break echo "you picked $choice ($REPLY)"; done
If we're sharing monolithic `.bashrc`'s, [here's my 2493 monster](https://github.com/rawiriblundell/dotfiles/blob/master/.bashrc) There are some functions in there that I am going to eject to reference gists soon because I don't use them. I do this from time to time. Others have mentioned splitting functions and aliases out to different files etc, and if that's an approach that works for you, then I totally agree. Unfortunately, that doesn't work for me. I am a *nix sysadmin, and my `.bashrc` is my digital toolbox/backpack. I primarily work on Linux and Solaris, and a lot of Solaris' shortcomings have determined some of the engineering/design choices made in the code. If I am assigned by my employer to a particular client, said client isn't going to jump on-board with the idea of dotfile management systems. What I will always have permission to do, however, is to deploy a monolithic `.bashrc` file via `ansible` or `scp`. You'll note that towards the start of my `.bashrc`, I do look for and source `.bash_aliases` and `.bash_functions` files. This allows me to split out code that I can re-use across multiple customers AND happily store on a public github. That goes into this `.bashrc`. I will then have customer or host specific functions and aliases in those other dotfiles.
If it's just a bunch of simple parameters you want it to know about, you can add a line like this to your .bashrc: complete -W "-1 -2 -3 --one --two --three" my_script Here's an experiment about this at the command line: $ complete -W "-1 -2 -3 --one --two --three" my_test_command $ my_test_command -1 -2 -3 --one --three --two $ my_test_command -- --one --three --two $ my_test_command --t --three --two $ my_test_command --three I was hitting &lt;TAB&gt; at the end of those command lines. Things get complicated if you want bash to offer filenames and directory names and such, and maybe only after certain parameters. I don't know how that works.
Trap the ctrl+c signal, do stuff.
What is the output you want to see? Can you type what you want printed exactly for that example input you showed here?
This problem seems more suited to perl. Do you want to run your program over and over again, or have it resident and poll the file for changes? #!/usr/bin/perl %HASH = (); while(1) { open F, "file" or die; while(&lt;F&gt;) { chomp; @A = split "_"; if ( defined $HASH{$A[0]} ) { if ( $HASH{$A[0]} eq $A[1] ) { print "Group $A[0] remains unchanged\n"; } else { $HASH{$A[0]} = $A[1]; print "$A[0] changed to $A[1]\n"; } } else { print "Tracking new group $A[0]\n"; $HASH{$A[0]} = $A[1]; } } close F; print "\n"; sleep(2); }
The last price for Hawaiian Holdings, Inc. (Nasdaq: HA) was **$25.57** (as of 12:22 AM EST on Jun 10, 2019) The 52 week high is **$44.25** and 52 week low is **$24.24** ^^I ^^am ^^a ^^new ^^bot ^^and ^^I'm ^^still ^^improving, ^^you ^^can ^^provide ^^feedback ^^by ^^DMing ^^me ^^your ^^suggestions!
If I understood your requirements correctly it can be quite simply achieved using bash arrays. In below you can either use a file where your list of urls are stored or use bash process substitution (explicitly specify the process instead of function arguments): Selection () { Lines=$(wc -l &lt; "$1") Urls=(); Count=0 while read url; do Urls[$Count]=$url echo $(( Lines - Count )) $url (( Count++ )) done &lt; "$1" read -p "Enter Choice: " Choice Index=$(( Line - Choice )) [ -n "${Urls[$Index]}" ] &amp;&amp; echo "You selected ${Urls[$Index]}" || \ echo "Invalid Choice, $Choice" } $ Selection urls.txt 10 https://curseforge.com/minecraft/mc-mods/mekanism 9 https://curseforge.com/minecraft/mc-mods/unidict 8 https://curseforge.com/minecraft/mc-mods/mekanism-generators 7 https://curseforge.com/minecraft/mc-mods/mekanism-tools 6 https://curseforge.com/minecraft/mc-mods/gas-conduits 5 https://curseforge.com/minecraft/mc-mods/mekanismores 4 https://curseforge.com/minecraft/mc-mods/mekatweaker 3 https://curseforge.com/minecraft/mc-mods/mekanism-jei-fix 2 https://curseforge.com/minecraft/mc-mods/mekanism-patched 1 https://curseforge.com/minecraft/mc-mods/sodiumcraft Enter Choice: 3 You selected https://curseforge.com/minecraft/mc-mods/mekanism-jei-fix
the file names are based on julian dates and the number is a type of file. I just made up names in here to make it easier to type out on my phone. &amp;#x200B; It will currently give me how many of each file type and ignores the file names. It is a text document that just has a list of file names. the format is year\_julian date\_24 hour time\_type\_number or YYYY\_DDD\_HHHH\_TYPE\_Count &amp;#x200B; 2019\_100\_1020\_PDF\_7 2019\_101\_1020\_PDF\_5 2019\_100\_1020\_TXT\_4 2019\_101\_1520\_PDF\_2 2019\_100\_1120\_PNG\_2 2019\_104\_1120\_PNG\_2 &amp;#x200B; It currently just checks for each file type and outputs: &amp;#x200B; PDF: 3 TXT: 1 PNG: 2 &amp;#x200B; I'm trying to get it to say something like &amp;#x200B; Day 100: 3 Day 101: 2 Day 104: 1 &amp;#x200B; I can do this if I open the file up and check for dates, but since the time and julian date are constantly changing, I'm trying to make it ech both outputs, but without me having to open the file each time to know which dates are in the file.
I don't use bash besides scripting but couldn't environment variables be sourced only when starting a login shell at ~/.bash_profile, ~/.bash_login or ~/.profile?
Ok that worked. At first I was trying to do that with &gt;&gt; that's probably why it wasn't working. The script doesn't work the way I want it to work though. It sends the info to the log. However, if I do it again, the new information will save over the old information. I basically want it to be like &gt;name1: age1: location1: &gt;name2: age2: location2: etc. Any idea how I would do that?
In my first post I meant to do `&gt;&gt;`. If you do it this way what happens? Does nothing get written?
In my first post I meant to do `&gt;&gt;`. If you do it this way what happens? Does nothing get written?
Something like cut -d_ -f2 | sort | uniq -c
Oh that fixed it. Not sure what I did before, but now it's working.
A way to do the counting is this: $ grep -oP '^\d+_\d+' testfile | sort | uniq -c 3 2019_100 2 2019_101 1 2019_104 Here's a perl one-liner that does the same work, where you have more control about how things get printed: perl -nE '($day) = /(\d+_\d+)_\d+_\w+_\d+/ and $counts{$day}++; END{ say "Day $_: $counts{$_}" for sort keys %counts; }' It produces this output with your example input: $ perl -nE '($day) = /(\d+_\d+)_\d+_\w+_\d+/ and $counts{$day}++; END{ say "Day $_: $counts{$_}" for sort keys %counts; }' testfile Day 2019_100: 3 Day 2019_101: 2 Day 2019_104: 1 If you want it to look at only that "100", "101", "104", then you'd change what the `()` wraps around in the `/.../` search pattern. Here's that perl one-liner with line-breaks for use in a script and for easier reading: perl -nE ' ($day) = /(\d+_\d+)_\d+_\w+_\d+/ and $counts{$day}++; END { say "Day $_: $counts{$_}" for sort keys %counts; } ' input_file_name_here
TY so much, I'll try it out soon.
Awk version: awk -F_ '{ days[$2]++ } END { for (day in days) { printf "Day %s: %d\n", day, days[day] } }' bob.txt | sort -n
1) Install `siege` https://www.joedog.org/siege-home/ 2) rtm
Yup, I know.
The router I use is of my college, I don't have admin rights. Otherwise I would have assigned pi a static ip
Thanks very much. Yours and u/skidnik helped a lot. I had added the path for curl to all the scripts, but I still didn't get my data in. After looking at the error log, I saw that I had forgotten to add the path for jq. Did that, changed the cron time, and life's good. &amp;#x200B; Thanks again folks. I've got work to do on the rest of my "scripts" now.
Look up jmeter, pretty simple interface for something like this.
ab, jmeter, siege, etc use a tool that was written for this purpose
Bash not good for this. Consider something like Python that will provide out-of-the-box tools for interacting with APIs. It will be easy to find an API for stock prices, which will provide results in a consistent format. But I would recommend avoing the use of shell scripts for this kind of thing. There are things bash does well, but if you're not doing those things you're going to be bashing your head against the keyboard in vain.
Just for the `install.sh` or you mean the actual client in bash? It’s definitely technically possible, but using arrays and maps in bash is fairly arcane. I don’t know of any frameworks or patterns, but I would imagine starting with functions, a way to parse configuration, and some error handling would be a good start. Honestly doing this in python with argparse, or click would be ideal. Or even golang and kingpin or the like. With bash it’s of course technically possible to write tests but it’s messy, like data structures. With another more rich language you have a lot of this for free. That and bash projects that are fairly large tend to be hard to maintain.
I'd use `case`. case "$string" in (*" - ["*) echo "It's there!" ;; esac
You forgot to escape the space: [[ $string =~ \-\ \[ ]] What I normally use is: [[ "$string" == *'- ['* ]]
&gt;\[\[ "$string" == \*'- \['\* \]\] It outputs: test.sh: 4: test.sh: [[: not found
Btw, that output was appearing in all my tests...
It worked! Thanks :D
Looks like you ran the script with sh instead of bash.
Just an FYI, you shouldn't use "test" as the name of a script (test.sh is probably ok in this example you shared) as the test command is a built-in to the shell. In fact anything inside the square brackets is the modern replacement for test.
I found ocli and other frameworks to do this.
In the test script that i did it worked but in the original code its not: # Check if its an empty line if [ -z "$p" ]; then echo "Its Empty Line, skipping..." else case "$p" in (*" - ["*) echo "It's a book name!" # Create file fileName="$(echo "$p" | cut -c 7-)" echo "Creating file $fileName.txt" touch "$fileName.txt" ;; *) echo "It's text to insert in the book .txt!" "$p"&gt;&gt;"$fileName.txt" ;; esac fi Could you help me understand why?
&gt; could you help me understand why? If you can be a bit more specific than "it does not work", sure.
&gt;case "$p" in (\*" - \["\*) I first tested the case of your answer and it worked (in separate [test.sh](https://test.sh)) but on my other script the variable that contains `"- ["` its going for the default case `*)`instead of going for the first case option...
Is there a reason you put parentheses around `otto=9`? The parentheses starts a subshell so your `otto=9` is set in that subshell which goes away once the subshell is done. If you get rid of the parentheses, then it should work.
Remove parentheses
Ok, so it's hitting the default case, that's what I wanted to know :) Have you tried echoing the variable right before the `case`?
I didn't understood , could you be more clear please?
put `echo "$p";exit` above the `case` to see what the content of the variable actually is. Edit: Oh and btw. You don't really need the test for if `$p` is empty. Just add `"") echo "Its Empty Line, skipping..." ;;` to the case statement (before `*)`). It doesn't really matter but it's less indentation and shorter.
Removing the parens would make this work. That is because the parentheses cause a subshell to be created for that statement (or any statements inside the parens) so any changes to the environment will not be visible outside of that. This is due to the fact that a process can not modify another process’s environment including working directory and variables.
&gt; # Should print 9 explain why you believe it should print `9` What do you think otto=8 ;(otto=9; echo $otto) ;echo $otto will print?
Its this: - [x] text2 Changed as per your suggestion, the if for one more case option
There is no leading space, that's why it's not matching. Change to: *"- ["*) .... ;;
Yes, my bad.. thanks!
You are welcome.
This is the point of the post... doesn't `export` share the variables with subshells?
9 and 8. But I thought that by using the export the variable would be shared between the "main" shell and subshells. I didn't make it clear - apologizes.
 string=' - [x] sdhifeife' if [ -n "$string" ] &amp;&amp; [ "$string" != "${string/\[/}" ] then echo "'\$string' contains '['!" fi
 otto=7 export otto etto=8 bash echo "in a subshell...." echo "etto = $etto" echo "otto = $otto" exit echo "not in a subshell...." echo "etto = $etto" echo "otto = $otto"
curl+jq for ripping into api rather than page scraping.
You could try openssl s_client It wont do much more than connect and arbitrate the SSL, but it could be scripted.
I think `grep` is what you're looking for but the solution ain't as elegant as others though &amp;#x200B; `string=' - [x] foo'` `bool_hit=\`echo $string | grep "\- \["` result `- [x] foo` &amp;#x200B; in case it does'nt have the "- \[" `string=' - x] sdfdkjfd'` `bool_hit=\`echo $string | grep "\- \["` result return nothing cause the grep doesn't match the grep
Cannot second this enough: `curl`+`jq` has been the missing piece of my attempt to use shell skills in modern applications environments.
This comment should be pinned, put in the sidebar, and gilded.
ImageMagick
 escapedRsaKey=encode $rsaPublicKey This line is the problem. Now it sets the environment variable `escapedRsaKey` to `encode`, splits `$rsaPublicKey` and then runs it. What you probably want is escapedRsaKey=$(encode "$rsaPublicKey")
Thanks
Can you show the command that you're trying to run?
&gt; but the resize doesn't seem to work correctly. Now I might be a little presumptuous, but are sure that it is not a case of you not instructing it correctly? IM is powerful, but not the easiest beast to get to grips with.
ive not seen a function written this way. isn’t it supposed to be: encode () { do stuff }
I use the following for creating thumbnails. prog(){ for file in $PWD do /usr/bin/convert -define jpeg:size=240x180 $file -strip -thumbnail 500x150 xxxthumb$file.jpg done mkdir -p "$PWD/thumbs/" mv xxxthumb* "$PWD/thumbs/" }
Here's a command line method: [https://askubuntu.com/questions/271776/how-to-resize-an-image-through-the-terminal#271797](https://askubuntu.com/questions/271776/how-to-resize-an-image-through-the-terminal#271797)
Not familiar with jq. Will check it out, thx!
Yes.
ImageMagick for sure. Used it for many scripts in the past. Post what you have tried.
I have posted an edit above.
Exactly to width by height. &amp;#x200B; I have posted a command above to show what I am trying to do.
I posted an edit above.
Try something like the following ```for image in *.jpg do echo "Resizing $i" convert "${image}" -resize 1920x1080\! resized_"${image}" done```
Perfect, but I fixed the "Resizing $i" to "Resizing $image"
&gt;dementoons Oh man, I want a show called this now.
There's a new Dracula comedy show. Start there. I'm on mobile so I spell bad.
If I am not mistaken, you can just use `wait`. Read more about it with `help wait`. However, processes forked will not alternate the parent's variables, so you need to write them to disk and than retrieve them again when you want to write your csv file. There are probably more fancier ways to do this, but you might want to consider using a better programming languages in that case.
Have you considered not waiting until they're all finished, and just writing them to file as they come in?
Thanks. Can you clarify what you mean by “will not alternate the parents variables”? Each process I want to fork uses $column, to process out a very small output. $column is static for each of these calls. So depending on what I need to write to disk, I might be able to deal with it.
That would be cool, but then I’d have to sort them somehow since the output is intended to be a csv.
That’s a good option if all else fails. I can process it into a csv later. Thanks.
Bash doesn't have great synchronization mechanisms, so unless you want to rewrite it for `parallel` or something, it's way easier to write it to file asynchronously. That can either be your CSV file directly, or to individual numbered files that you can then concatenate when the processes have written them.
Yep, id just need to collected the output later into csv. Not a bad option at all.
Well, take a look at this example, and try it with and without the ampersand (&amp;). unset A A="test" &amp; echo $A If there is an ampersand the process is forked to a child. This means that this forked process will not alter the A variable in the parents scope, thus there is no variable to echo in the last line. For all your forked processes, this means that all outputted data is not available in the parent. To solve this you can read it to a temporary file and read it back, but looking at your code, it would just be much simpler if you would just write all data processing in one awk command. This would be much better way to speed up your process. That would you just have a single pass over your data, additionally you can make everything in one pipe, and safe the memory consumption as well.
This may work for me, because the processes data is quite small. Thank you!
Actually, my recommended approach would perform much better than the current approach especially for a large amount of processed data.
The variable $column is in general very large, can be as big as 10s of GB. And so the sed and ask scripts take some time to run while they create the desired outputs. But from what you said, I’d only have to write $charcount*, and $num_* to file. These are small.
OK so the way I would do it would be as follows: 1) create an empty temp directory 2) create an indexed array called something like PIDS 3) fork each function but instead of having it save to a variable, redirect it to a file in the temp directory. When forking, add the forked pid to the array 4) run wait on all the PIDs in the PID array from step 1. 5) use cat and append to write the files out to your CSV file.
Keeping the PIDs was my thought before posting. Thanks for the outline. I did not anticipate the part about storing tempfiles, so that is great. Thanks!
Any advice for looking into how to wait on pids? Tools to use?
Wait is a bash builtin - you literally just call wait $pid or wait $pid1 $pid2 etc. Wait also exits with the exit code of the command you are waiting on, however that is of limited value in a pipeline.
I mean, it would be better if you would pipe $column to an single awk command, and than let awk write to the csv file. This way the 10s of GB doesn't have to be cached by bash in the $column command. I know that you want parallel all the process in an attempt to make it faster, but you don't have to parallelize it if you are able to create a single awk command from it. Because than this single command can do one pass over all the data, and in the end that's what makes this slow. All awk commands have to process all this data individually. Instead of multiple processes running over the same data, it is better to have all data processed in one process. This will safe a lot of memory usage and is often fasten (unless your data processing is very CPU heavy, which in this case isn't the case.)
Ansi escape codes for color
I've done it already but always post without. # All Github repos status function gs() { clear; printf "\n\t ${Blue}---[ ${Yellow}Github repositories status ${Blue}]---${NC}\n" for f in ~/Github/*; do printf "\n${Blue}Looking at ${Yellow}$(basename $f)...${NC}" cd $f if [ -z "$(git status -s)" ]; then printf "\t${Green}OK" else echo git status -s fi cd .. done printf "\n\n" }
Thanks! It does work now, but just now the actual encoding itself. Now when I run it the rsaKey just shows me ----BEGIN. looks like the function itself isn't working
That’s true. The sed | awk | awk pipe is O(N^3) which is no bueno. Maybe I should write a single script in pure awk that does what you’re saying. That should be O(N).
getopts won't work work -fp or -nosort. either use 1 letter options or use shift/case/while
I've always followed this method for passing command line arguments to a bash script: https://stackoverflow.com/a/14203146 This allows you to handle each flag as an optional parameter and easily add more if needed with clean, readable code (in my opinion). Your search term would just be the * case using this method, btw
this is not useful and it's dangerous. you've enabled directory traversal vulnerability.
no GD? are you a copy+paste code warrior?
While writing cli is easier easier than tui or gui it often turns out that option parsing gets kinda messy. Looking at /usr/share/doc/util-linux/examples/getopt-parse.bash might help. &amp;#x200B; There is a shortcut in this particular case. Since all the cli args are passed to \`gosearch\` then just passing the full argument list as it's argument list seems like it might meet the goal. \#!/usr/bin/env bash gosearch $@ | blablabla
Mail is....difficult. If you are not in a corporate environment, then your system probably doesnt have mail setup. Not only that, but your ISP will probably block it. If you are trying to mail yourself on your local machine, then you might only be able to read the mail with the mail command. Mail is a PITA
Huh? I already have it sending emails. I’m asking how can I add context to it based on the command that failed.
Cool, $PROGRAM arg1 arg2 arg3 echo "$PROGRAM had a problem | mail -s "This is an error message from $PROGRAM"
Try collecting both the std out and error of the command: out=$(service $SERVICE stop 2&gt;&amp;1)
Thank your for the comment. &gt; There is a shortcut in this particular case. Since all the cli args are passed to gosearch then just passing the full argument list as it's argument list seems like it might meet the goal. So to my surprise, that is indeed all that is required in this case. This is what I did. #!/bin/bash : ' Usage of gosearch (https://github.com/ozeidan/gosearch): -c case-insensitive searching -f use fuzzy searching -fp fuzzy searching on file paths -n int maximum amount of results to display, set to 0 for unlimited results (default 500) -nosort do not sort the result set for performance gains when fuzzy searching -p do a prefix search (faster) -r reverse the sort order ' search_var=$1 gosearch -c -r -f $@ "$1" | LS_COLORS="di=0;35:ex=1:" lscolors | fzf --ansi exit 0 This allows me to have the parameters `-c -r -f` always active as I want, and optionally use the additional parameters at run time: `script -fp -n 0 'search_var'` I do not fully understand why this works, so I will have to read into it more thoroughly. Although I should familiarize myself with _getopt_ at sooner than later, I much appreciate you pointing out this simple solution! Saved me from a bunch of unnecessary complexity.
Can you explain?
&gt; ...you can use $@ and not parse them at all Turns out this is al that was required, I appreciate the comment.
Very informative info in that thread. Thank you :)
`$@` is documented in the Special Parameters section of `man bash`: &gt; **@** Expands to the positional parameters, starting from one. […] When the expansion occurs within double quotes, each parameter expands to a separate word. That is, "**$@**" is equivalent to "**$1**" "**$2**" ... There's some important information in there about word-splitting and quoting. What you really want is this: gosearch -c -r -f "$@" | ... You should basically always use `"$@"` versus `$@` — it ensures the parameters are passed along exactly as they were received. This might matter if any of the parameters contained any whitespace. For example, if your script is called with a single parameter `"a b c"` and you use `$@` to pass this to `gosearch`, the parameter will get split by whitespace and `gosearch` will actually receive three separate parameters. `"$@"`, on the other hand, prevents that extra whitespace-splitting from happening. Also, with `$@ "$1"` (what you currently have according to your comment) you are actually passing the first argument to `gosearch` twice (because the `$@` *includes* `$1` already).
`s/gosort/gosearch` (had me momentarily confused)
&gt; You should basically always use "$@" versus $@ — it ensures the parameters are passed along exactly as they were received. This might matter if any of the parameters contained any whitespace. For example, if your script is called with a single parameter "a b c" and you use $@ to pass this to gosearch, the parameter will get split by whitespace and gosearch will actually receive three separate parameters. "$@", on the other hand, prevents that extra whitespace-splitting from happening. Thank you for the link and concise explanation. I was confused on how white spaces were handled here. I understand _why_ it works now. &gt; &gt; Also, with $@ "$1" (what you currently have according to your comment) you are actually passing the first argument to gosearch twice (because the $@ includes $1 already). Fixed above comment to reflect this. This is one thing I actually knew and my error is proof I need sleep :/ Really appreciate your comment, very helpful to a novice like myself and good info to understand going forward.
 fail() { mail ... &lt; log exit 1 } exec &gt; log 2&gt;&amp;1 trap 'rm log' EXIT ...
Found a better way to use git command: `git -C FOLDER status -s` instead of `cd`. Looks a bit better now. # All Github repos status function gs() { clear printf "\n ${Blue}---[ ${Yellow}Github repositories status ${Blue}]---${NC}\n" for f in ~/Github/*; do printf "\n${Blue}Looking at ${Yellow}$(basename $f)...${NC}" if [ -z "$(git -C $f/ status -s)" ]; then printf "\t${Green}OK" else printf "\n%s\n" "$(git -C $f/ status -s)" fi done printf "\n\n\t ${Blue}---[ ${Yellow}Done ${Blue}]---${NC}\n\n" }
Redirect to `/dev/null` rather than a regular file?
If application.jar is the real culprit, you could try `application.jar &gt; /dev/null 2&gt;&amp;1` which will black hole all output from that program.
`csplit` is line-based, not character based. That is, it always splits the input at line boundaries. Regular expressions and line numbers are merely used to select the line at which a new file should be created. I suspect you've got a single-line input (JSON?).
Thanks. That sounds like the reason. I didn't realize that it doesn't split it at the matched pattern and the file does appear to be one line of input. It may be JSON, I'm not 100% sure. What I have is a file that I'm supposed to decode as part of a game and I'm still quite new. The format is `{"0x1": [{"text": "..." "value": ...}, {"text": "..." "value": ...}...]}`. So I thought I would try to split them apart and try to decode them separately. I guess csplit isn't the way to do it.
nope I got downvoted
Just so I understand this clear, is your application generating the log file or are you not wanting to create log file that gets generated from `nohup`? In any case like others have said, you can dump everything in `/dev/null`. Both stderr and stdout.
I wanted to comment on u/kalgynirae's comment. There are two variables with related behavior: $@ and $*. Both contain the arguments to the script. Using $@ or $* outside double quotes causes "shell word" expansion. Inside double quotes $@ breaks into tokens preserving command line quoting while $* expands to a single string. Which to use and when is not as simple as "should". #!/usr/bin/env bash echo 'bare $@' for f in $@ do echo $f done echo echo 'bare $*' for f in $* do echo $f done echo echo 'double quote $@' for f in "$@" do echo $f done echo echo 'double quote $*' for f in "$*" do echo $f done Run the above like this: ./star_arg_at this 'that and the' other thing And get the results as below: bare $@ this that and the other thing bare $* this that and the other thing double quote $@ this that and the other thing double quote $* this that and the other thing
Won't nohup still create a log file in the working directory though? Or doesn't it create the log file until the program has produced some output?
Instead of redirecting stdin AND stderr you can catch all and redirect like so: &amp;&gt;/dev/null redirect stdin and stdout to the dumpster bin
Thank you for your help. The Java application it's a Spring Boot application which has an internal logging and it's generating his own log files. I've modified the first file and not the second file which starts the java application. Thanks to your reply I've made the following modifications: &amp;#x200B; &gt;nohup ${ROOT\_DIR}/start.sh &gt; /dev/null 2&gt;&amp;1 2&gt; ${ROOT\_DIR}/foo.err &lt; /dev/null &amp;
You can lose the `2&gt;&amp;1` there, as you're immediately redirecting file descriptor 2 to a file after that.
&gt;Won't nohup still create a log file in the working directory though? Surprisingly, no. :-) `nohup` is smart enough that if you redirect standard output, it doesn't create and write the `nohup.out` file. That is, it only writes `nohup.out` if standard output is a terminal. The reason I say "surprisingly" is because I went looking for a "quiet" option in the `nohup` manpage, and found the above instead. Gave it a quick try and it behaved as advertised.
Yup! And `nohup` is smart enough that if you've redirected standard output, it won't create the `nohup.out` file. I'm guessing that internally, it calls `isatty(fileno(stdout))` to determine the state.
Try... while read item; do echo "$item" done &lt; song_list.txt This should print out all of your listed items. Now, alter it by asking it to perform the youtube-dl instead of displaying... while read item; do youtube-dl -f 'bestaudio[ext=m4a]' "ytsearch1:$item" done &lt; song_list.txt Cheers!
`fortune` is the utility to pick a random line from a file. If you don’t want to use that, you can pick a random line number with `$(( RANDOM % line_count ))`, assuming you’ve already set line_count to the number of lines in the file. You could then use head and tail to pick the line out. You do need to know then number of lines to get a truely random line.
Hey, eraserhd, just a quick heads-up: **truely** is actually spelled **truly**. You can remember it by **no e**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Hey /u/CommonMisspellingBot, just a quick heads up: Your spelling hints are really shitty because they're all essentially "remember the fucking spelling of the fucking word". And your fucking delete function doesn't work. You're useless. Have a nice day! [^Save ^your ^breath, ^I'm ^a ^bot.](https://www.reddit.com/user/BooCMB/comments/9vnzpd/faq/)
&gt; while read item; do youtube-dl -f 'bestaudio[ext=m4a]' "ytsearch1:$item" done &lt; song_list.txt This works wonderfully!!!! Thank you!
&gt; nohup is smart enough that if you redirect standard output, it doesn't create and write the nohup.out file. That's not what you recommended, though. If you call `application.jar &gt; /dev/null 2&gt;&amp;1` instead of putting `&gt;/dev/null` on the nohup command itself, you'll still wind up with an empty nohup.out file. The right way to do this is with `nohup [command] &amp;&gt;/dev/null`.
Yes, exactly. Thanks for clarifying. The `nohup` command is of course still needed.
&gt; Line 'source &lt;(PrintLibraryFunctions echoE SelectItem Trim) || exit $?' imports functions 'echoE', 'SelectItem' &amp; 'Trim' You sure about that? I don't see any arg parsing in your `PrintLibraryFunctions`. I'm pretty sure `&lt;(PrintLibraryFunctions echoE SelectItem Trim)` will behave exactly the same as `&lt;(PrintLibraryFunctions)` by itself, and import each and every function in the file.
Happy downloading!
Use the best tool. If it is json you might want to look at jq to decode.
Calling the git command twice doubles the work. I'd do the inside of the `for` loop more like this: status="$(git -C $f/ status -s)" if [ -z "$status" ]; then printf "\t${Green}OK" else printf "\n%s\n" "$status" fi
That's true! Didn't see that, ty
Sourcing from process substitution is pretty clever, but where is `PrintLibraryFunctions` defined?
Yeah, thanks. It does appear to be JSON (at least according to `file -i`). I ended up writing a python script to split the file. I have what appears to be escaped UTF-8 (minus the forward slash) mixed in with what may be regular ASCII. I've added forward slashes so far, and I'm trying to find a way to iterate through it to decode. I actually installed jq a few hours ago, but the jq man page is quite long.
After that, you might want to look into picard (https://picard.musicbrainz.org/) to properly tag your music, if needed!
It looks like your submission contains a shell script. To properly format it as code, place four space characters before every line of the script, and a blank line between the script and the rest of the text, like this: This is normal text. #!/bin/bash echo "This is code!" &gt; This is normal text. &gt; &gt; #!/bin/bash &gt; echo "This is code!" *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/bash) if you have any questions or concerns.*
He said the library must be on PATH. I would assume that that is a program, not a function.
Sir, I like this. I see you are a man of culture. How do I get more of your content?
You can use `jq` itself to safely and robustly build JSON: jq --arg auth "$auth" --arg email "$email" ' ."https://company-docker-local.jfrog.io" = { email: $email, auth: $auth } '
&gt;Yes it is a script (You can find a pastebin's link in my post). It must be executable and in a PATH directory.
[Please read this](https://www.reddit.com/r/bash/comments/bzuhi9/scheme_to_dynamically_import_functions_to_a/eqxyy6e?utm_source=share&amp;utm_medium=web2x)
Thank you :-) What kind of content?
There is argument parsing in 'PrintLibraryFunctions': `for FunctionName in "$@"` You can try it yourself: Drop 'PrintLibraryFunctions' in a PATH directory and then create (copy) and run 'ImportFunctions'.
I see that now, pretty clever.
What is the interface? How do you supply values? * ./cppthing value value? If so simply use two for loops * ./cppthing ENTER value ENTER? For... printf "%s\n" value1 value2 | ./cppthing * ...