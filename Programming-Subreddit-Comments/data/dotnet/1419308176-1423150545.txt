git git.
Git for sure. At work we use a self-hosted [GitLab](https://about.gitlab.com/) installation for storing our private repositories, it has worked out well for us. As far as clients go we use either [SmartGit](http://www.syntevo.com/smartgit/) or [Git Bash](http://git-scm.com/).
Not a lot of people change ORMs, because usage ripples through in the application, also with repositories: unless you project the entities fetched into types not used by the ORM (e.g. DTOs) and use those instances outside the repository, the entities fetched will leak the ORM characteristics outside the repository, breaking its abstraction. Even if it's subtle, it's still there. Typical example: myOrder.Customer = myCustomer; some ORMs now automatically add myOrder to myCustomer.Orders, others don't (e.g. NHibernate doesn't). This means that using the entity classes outside the repository will lead to subtle differences like this if you change ORMs and mitigating the whole abstraction you created to prevent that. So it's nice on paper, but not very useful in practice. 
Git is used by the Azure project backend for deployments, is integrated in Visual Studio Online, and has become (more or less) the defacto CVS for a lot of MS projects. It also has a great friend in GitHub. On pure featureset I think Git edges out Mercurial, but in terms of tooling and broader acceptance it's not really a competition any more. Bite the bullet, go Git. 
I'd go with Git. I think Mercurial is a bit easier to work with and started using it first but Git is ubiquitous and I've found it makes sense commercially.
`git add` and `hg add` does two different things and is a important reason why I prefer git: it allows you to stage changes before a commit.
I don't know what the command line equivalent is with Mercurial, but on the TortoiseHg side of things (the GUI for Mercurial), you get a visual diff with a checkbox for whether to include each section in your next commit. It doesn't let you split by line or make changes, but it's still very handy to have.
Thank you for the suggestion. TFS seems a bit overkill and we do use other tools outside of VS. I've used SVN (years ago) and really liked it, but it seems like people are switching to GIT or HG. Maybe I'm wrong... 
A repository is just a place to store data. This could be a database, a flat file, an API call (via traditional web services or something REST-y), etc. The idea is you can define the methods you use to retrieve the data, and then you can implement that interface in various classes. That way, you can work with the interface and switch out the implemented class. Like you could do an IConfigFile interface that stores data in a file. Then, you implement the interface in JSONConfigFile, XMLConfigFile, YAMLConfigFile, etc. Your app just gets the file, has some method of deserializing it into the proper class, and then you pass it to a method expecting the interface. That method doesn't need to care about the storage format (implementation details) at all. More examples would be logging backends (windows event log or text file), and database servers (SQL Server, Oracle, Postgre, MariaDB, etc). This stuff is pretty handy for plugins, too, if you're using things like MEF.
Agreed. As I said, it's definitely not for everyone. Same goes for all SCM, they all have flaws. Like all tech should be thoroughly looked into before committing. My opinion comes from using SVN, GIT, TFS and even some odd proprietary internal systems. For me , I just found TFS to "work" and slot nicely into our process, don't have to mess around with any command line tools or set anything up. I did find GIT/SVN a lot better for "micro" projects though (i.e. Home use) Different strokes for different folks I guess! On a side note, I'd heavily avoid using JIRA though. That thing is just a massive work-load creation tool! 
Because [Telemarketer](http://stackoverflow.com/questions/383947/what-does-it-mean-to-program-to-an-interface).
&gt; some people I know who are senior developers Then, those people are Sr. Developers in title only.
I used to use Tortoise for HG, but after reading a few tutorials I've found that for Windows Git Extensions is pretty excellent - https://code.google.com/p/gitextensions/ which comes with MYSysGit. I'd say that GUI is nice, but overall even Windows is moving towards CLI. 
May I recommend that you try the command line? For the most common commands, check-ins, branching, tagging, merging etc. it's no less hassle than with a graphical tool, and it remains the same across any project you will ever encounter. If you are moved to Borland Delphi, and they use FooGit, you can still use the git command line you were used to before. I use Visual Studio for history, meld for merging and the command line for everything else. The command line can also do more complex tasks, such as separating out a sub-directory into a new repository.
You'll hear a lot about interfaces making testing easier, and that is true, but not the only benefit. I think the biggest benefits come from being able to swap out implementations of an interface. Let's take a gaming example. Suppose you have your typical fantasy style RPG. Every character inherits from the base character class, but their implementation of those class attributes is different. However, each implementation must contain a shared set of members, properties, etc. So, you define an interface. Then, you can do things like inherit from that interface, or pass objects that are typed to your interface so that any class that implements your interface can be used as input for a method, or in that classes constructor, etc. These days, I am writing a lot of modular software, and I find myself defining interfaces first, and then using Rhino to mock out some implementations during testing. Really, I just want to test the "contract" of the interface.
By centralized, are you suggested local (inside your firewall) or hosted elsewhere? We have a myriad of compliance regulations and audit requirements that prevent us from going with distributed revision control.
Thanks for your comments and happy cake day.
You'll never be able to tell what port some other device forwarded to your server. Your server only knows what port it's listening on. Why would you need to know the forwarded port number?
The thing is: I type on my browser: http://myip:7654/login.aspx and the firewall forwards this to another machine on the default port. The page does some logic and rewrite the url using the Request.Url.toString() value, but since on this forwarded machine it is on the default port, it suppresses the port, so it becomes http://myip/somepage.aspx. I need the page to verify that the request came from Port 7654 so I can make the url right, like: http://myip:7654/somepage.aspx. Got it?
Thank you very much... and happy holidays.
Another example is in larger development teams you may write the C#, but a DBA is writing the designing the database. So you need a way to be able to program without depending on the database portion already being stable and completed.
Check request headers, some firewalls put the original source ip and/or url before redirect to them.
Can you just use relative URLs instead? That way the logic doesn't need to know the server or the port.
That was my point.
Does this help ya: http://stackoverflow.com/questions/21640/net-get-protocol-host-and-port
Unless the firewall adds an HTTP header (typically `X-Forwarded-Host`) to the request, you will not be able to tell where it originally came from. (you *might* be able to find the original ip/port in the `Host` header, but it's likely that the firewall will have modified that).
When was the last time you used TFS? 2005? What you are saying is completely wrong.
You can get setup with GIT through TFS in a few minutes as well, either through tfs online or local tfs. Its web support is nice and its easy to setup sprints / user stories / tasks if you want to (or you can ignore all that and just use it for source control)
As fokov said, can you use relative URLs? Other than that, could you just run your webserver on 7654 so there isn't this issue?
The forwarded-for header tells from which IP the call originally came from, not to which IP it originally was targeted to.
Will try that
It is from an application I have to give maintenance and it is on a client network, so I dont really know much specifics, why they had to put it like that and stuff. I will try relative urls, and try to check the headers for more information on the redirect. Thank you all for the information and help :)
I've used both. Smartgit's UI is much faster but it has the weird idea of keeping the commit log in an entirely separate window. SourceTree feels sluggish and in the more recent updates crashes often on me. Beware that SmartGit doesn't support all its features on both Git and Hg. For example you can't create a patch from Hg and the whole review process feature is missing. The integrated diff/merge in SmartGit is way better than K3Diff that SmartGit (and most other tools) use.
There are so many better tools than Tortoise. Tortoise has both a Git and Hg version by the way. Personally I would go with Git for the better tooling and go with GitExtensions which is great and free.
Visual Studio 2010. And how is it wrong? This is my experience with it. 1. TFS is centralized forcing you to have a connection to the central repository. Saving changes while offline and then checking out may leave you with losing changes. You have no history available when the connection to the server is lost. You effectively lose version control when TFS cannot reach the server. 2. It does keep track of who checked out what, both locally and on the server and will lock files for that user on both locations. 3. It has a task system, that in least in 2010 was horrible for code reviews. My team didn't use feature branches for whatever reason, so I have no idea how that works in TFS. But anyway changers quickly accumulates and making diffs is a lot harder than with just Git, a feature branch and Visual Studio.
&gt; Like all tech should be thoroughly looked into before committing. I see what you did there.
&gt; You're going to be in pain using non-visual tools. I completely disagree, but I have a strong Linux background.
1. Yes it is centralized. Saving local and getting latest won't lose changes, or will ask you to merge changes if there are conflicts. I never lost work when using it for years, and neither did anyone on my team. 2. Setting all files to exclusive lock is a very bad decision and locking files should only be used in rare circumstances and not be the norm. That's what Microsoft recommends. 3. Branching is definitely harder for sure which is why we've switched to TFS git instead of tfsvc. That said it's still easy to do a per (major) feature or per sprint branch that multiple people work on. Git is easy to have branches per person as well as larger feature branches that are shared which helps with code review. Tfsvc makes more sense if you have a lot of people working on a single project and that share branches (working same feature); while git makes it simple for each user to create their own instantly. I've got to say I end up spending much more time fixing merge issues with git than tfsvc because the changes / commits between large features is larger since people save/commit a lot of stuff locally and have longer breaks before they end up back at the main develop branch. I wish there was an easy tool to squash all minor commits to users own / small feature branches before they merge into the main feature / develop ones.
So you have multiple databases with the same schema? Not sure if any write-ups have been made on the subject other than this one I did on NHibernate an eternity ago. The concept is pretty much the same with entity framework. http://patrickhuber.wordpress.com/2011/10/25/change-connectionstring-nhibernate/ In the past, I've created a EF context from a factory object. The factory object is responsible for selecting the appropriate connection string for the current user and passing that into the context as a constructor parameter. You can then set a dependency on the factory interface in your page or controller like so: using(var context =_factory.CreateContext(this.User)) {} Other than that, its pretty much the same as any other app. You may want to add some authorization against uri resources if you are going to expose customer IDs to make sure one tenant can't access resources belonging to another tenant.
Thank you for you response Patrick! Indeed I will have multiple databases (maybe across multiple server in the near future) but all those databases have the exact same schema, and if I need to update one, I will update them all so it is consistent. NHibernate sure does bring back memories. I will read your blog post tonight when I get back from work! Thanks again and happy holidays
This thread has been linked to from elsewhere on reddit. - [/r/mistyfront] [Test Client for SignalR Hubs (/r/dotnet)](http://np.reddit.com/r/mistyfront/comments/2qb0ip/test_client_for_signalr_hubs_rdotnet/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
What does your workflow look like?
I've used autofac successfully to handle multi Tennant connection before it's fairly simple just make sure you don't get your tennant ids messed up
Normally I use VS2013's Git integration. I make feature branches, and the changes in those feature branches are atomic in the sense that they are for one specific feature. I also write test suites for my code. When everything passes, I send a pull-request (or if it is my own project, just merge into master).
Ah, yes. You have a code base that is not 100 years old. That is the difference with you and me. My product doesn't have specs, doesn't have tests, is written in ancient languages/technologies and is a mishmash of a little bit of everything and nothing all in one. There are about 15 people on the team and we're all bumping into each other. That's where the visualization stuff comes in really handy. Not to mention we have "releases" that can't be merged into master until they pass QA and User Acceptance Testing. To keep your ducks in a row, you pretty much have to see all the different merges and things taking place. In simplified workflows and good code bases, I'm sure that's no problem.
I've been in those environments. It is never too early to start unit testing and feature-branching.
We do actually use feature branches and we try to unit test where we can.
You can derive from AuthorizeAttribute and put your logic for checking roles in there. Maybe this will help http://www.diaryofaninja.com/blog/2011/07/24/writing-your-own-custom-aspnet-mvc-authorize-attributes
This. I mean it's just putting data into database in some proper form and pulling data out. There are a lot more interesting problem to solve than trying to make your data layer as generic and flexible as possible.
Building an appropriate authorize attribute or claims based auth is probably the right way to go with MVC in 2014. But if for some reason you need a different approach you could hit it from the other side by building a custom IPrinciple that implements your own IsInRole() method as that is what authorize calls when invoked. 
What are the other ones?
What are you using IronRuby for? As /u/JaCraig said, the IronRuby project is mostly dead at this point. Depending on what you are using IronRuby for, we can suggest alternatives. 
Yes, but Jabbr.net is a group of .NET chatrooms MADE with various .NET technologies, hosted on Azure, and holds many great minds in their conversations. And, of course, it's OSS.
Nothing really I just don't like leaving visual studio when changing languages.
If you're using Identity, you can get the user object from the HttpContext. There is a IsInRole method
You want to SET a role or just check if a user HAS a role?
I believe SE has an active chat room for .net stuff.
I probably could've explained better. I have everything set up and running. I've toyed a bit with it, but I want to do something more serious. My problem is that there are a lot of features and there doesn't seem to exist a place with them documented. I want general ideas on how to start, and continue, a site using all the features vNext has to offer. Since I'm new to this whole Microsoft technology, I'm not exactly sure what to expect and where to find things. I know the official sites are a great start point, but they seem lacking for some reason.
&gt;My problem is that there are a lot of features and there doesn't seem to exist a place with them documented. That's to be expected. ASP.NET 5 (vNext) is not going to be out for a few months and many of its features are still being worked on so we're not going to see any good documentation for a while. And some of its components like the Entity Framework are going to be released with missing features and they're still figuring out which features are going to make the cut. So, if you want good documentation use the ASP.NET 5 equivalent that we have today, which is ASP.NET MVC 5, Web API 2, and the Entity Framework 6 (with code first). ASP.NET 5 will also include Web Pages but you shouldn't concern yourself with that.
I'll answer the fourth one because that is a C# question, not a EF or MVC question (I'm not very familiar with those). var students = from s in db.Students select s That is a database query. It is using the Language Integrated Query ([LINQ](http://msdn.microsoft.com/en-us/library/bb397926.aspx)) feature. This hides the direct SQL queries. In this example, s is a temporary variable that is declared in-line. If it helps you conceptually, it is saying essentially: foreach (var s in db.Students) yield return s; But this is a DB call, so LINQ will actually convert it to a SQL call, contact the DB, and do the pretty packaging. SELECT * FROM Students; But since this is a DB, I would actually use the lambda syntax and async/await. var students = await db.Students.ToListAsync(); You can also get the same effect without the LINQ or async/await: var students = await db.Students.ToList(); Note that all of those will translate to the same DB query. The one with async/await is the best choice because it is using non-blocking network calls. For your third question, there is not a exhaustive list because anyone can create an extension attribute. If you want the stock ones, the Namespace description on MSDN likely has what you want, but I am not entirely sure.
Regarding to 3rd question: http://msdn.microsoft.com/pl-pl/data/gg193958.aspx Probably that's not everything, but the most useful ones. That's all I found for now.
And stay away from the System.Web assembly.
Haha. Why is that? Any more tips?
System.Web has heavy ties with the IIS. This dependency is dropped with ASP.NET 5, because there the dependency IIS is dropped. Another tip: Don't even assume it's ready for production. It's not. By far not. There can still happen a breaking change every day, the API is still in flux and it's still considered and early preview. It's an interesting view to the future, but it won't be usable in production in the next 6 months. So if it's a real project, forget about vNext for now.
Ah, that's cool. This dependence drop is one of the features I'm looking forward the most.
That is something you have now already if you replace MVC with something else. It's only new for MVC.
EF7 (which is the EF version for .NET core/vnext) code first api is different from EF6' code first API.
Depends on the tool. Works with eclipse. I guess it depends what you are looking for. 
And it's not done yet either. For example, attributes aren't even in yet (as of a month ago anyway)
this [custom forms authentication module](http://redd.it/2phg66) might be helpful. it's in vb but you can easily throw it in a vb to c# converter. you can set the roles of a user with the "SignUserIn" method easily and restrict access to actions with the custom "AuthorizeME" attribute.
Does that work on Macs? I tried to use it, but it gave me an SQLite.Interop.dll not found.
Firebird and its firebird embedded add-on to use the db file in-process. Has a great ado.net provider. See http://www.firebirdfaq.org/faq51/ how to set it up on mac. 
If for some reason it isn't finding your interop libraries, you can force it to use the version in the directory indicated by the process's "PreLoadSQLite_BaseDirectory" environment variable.
You should check out the database access page on the mono project site: http://www.mono-project.com/docs/database-access/
If you can use a NoSQL database, there's [RavenDB](http://ravendb.net/) and [db4o](https://en.wikipedia.org/wiki/Db4o) 
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Db4o**](https://en.wikipedia.org/wiki/Db4o): [](#sfw) --- &gt; &gt;__db4o__ (database for objects) is an embeddable [open source](https://en.wikipedia.org/wiki/Open_source) object database for [Java](https://en.wikipedia.org/wiki/Java_(programming_language\)) and [.NET](https://en.wikipedia.org/wiki/.NET_Framework) developers. It is developed, commercially licensed and supported by [Versant](https://en.wikipedia.org/wiki/Versant_corporation). &gt;db4o is written in [Java](https://en.wikipedia.org/wiki/Java_(programming_language\)) and [.NET](https://en.wikipedia.org/wiki/.NET_Framework) and provides the respective APIs. db4o can run on any operating system that supports Java or .NET. db4o is offered under multiple licenses, including the [GNU General Public License](https://en.wikipedia.org/wiki/GNU_General_Public_License) (GPL), the [db4o Opensource Compatibility License (dOCL)](http://www.db4o.com/about/company/legalpolicies/docl.aspx), and a commercial license for use in proprietary software. &gt;==== &gt;[**Image**](https://i.imgur.com/hlgTdI4.gif) [^(i)](https://en.wikipedia.org/wiki/File:Db4o-logo.gif) --- ^Interesting: [^Comparison ^of ^object ^database ^management ^systems](https://en.wikipedia.org/wiki/Comparison_of_object_database_management_systems) ^| [^Versant ^Corporation](https://en.wikipedia.org/wiki/Versant_Corporation) ^| [^Query ^by ^Example](https://en.wikipedia.org/wiki/Query_by_Example) ^| [^DataNucleus](https://en.wikipedia.org/wiki/DataNucleus) ^Parent ^commenter ^can [^toggle ^NSFW](/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cn6trp5) ^or[](#or) [^delete](/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cn6trp5)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
It's not going to be recommended for prod use either when vNext drops. Stable release is expected late 2015.
&gt;but it looks stable enough for production It's probably not. As you're finding out, things are still in a huge state of flux, and there is virtually 0 current documentation as things continue to move so quickly. I'd recommend hanging out on https://jabbr.net/#/rooms/AspNetvNext &lt;- plenty of the MSFT devs are there and are on hand to answer questions. There's also very little tooling support unless you're OK with running the preview builds of VS2015, Omnisharp is making headway but you'll have to figure out quite a lot on your own.
The this is, I don't want to invest a lot of time in a technology that will (hopefully) soon be replaced by other that actually has the features I'm looking for. I'm using VS2015 with any problems. &gt; https://jabbr.net/#/rooms/AspNetvNext Thanks, I'll take a look at it.
&gt; The this is, I don't want to invest a lot of time in a technology that will (hopefully) soon be replaced by other that actually has the features I'm looking for. There are key differences to be sure, but the core of how you work with vcurrent is the same as vnext. There is a lot of baggage with the project system that is all going away so it will save you the pain from learning that. My 2c is that if you need to get something into prod in the next few months, you'll still have less dramas on vcurrent. If you're not time conscious however and are happy with bleeding edge and 0 docs, than vnext is moving along quickly!
I'm trying to go freelance and, in this case, vNext has a lot of features that the current .Net don't. Simply using C# is already a very risk choice because most clients want their sites in PHP for ease of maintenance and the ability to host anywhere. Deploying to Linux or to Windows without IIS is the very least I have to offer in order to get clients.
I have use [VistaDB]( http://www.gibraltarsoftware.com/vistadb/what-you-get/vistadb-database-engine) before with a mild CMS I used to work on. I made the CMS run on .NET on Windows and Mono on MacOS. VistaDB was supported on both platforms easily.
Oh, of course. However, the main benefit of vNext will be lost though if system.web is back in the mix :/ Of course, devs like you and I will plan for a migration path from ef 6 to 7 to abstract the .net framework from the server, but there will be countless others who use ef6 with vnext and call it a day :/
Well, the API's are going to be pretty damn close, so hopefully it will literally be a case of ripping out the EF6 libs and dropping in the EF7 ones and calling it a day!
I sure hope so as well! Just keep in mind that there will be many incompatibilities across frameworks and libs. Choosing the right combination will provide the best migration experience. For those looking on this conversation, please test the migration before scaling up the implementation!
I liked VistaDB's feature list, I couldn't find it on Nuget, but I'll definitely read more about it.
I'm building a tool for SQL databases specifically, so that's not an option, unfortunately. Both of the those tools are pretty neat, so thanks for bringing those up.
I'll take a look, thanks!
From the changeset that added support for Annotations it looks like you just need to create an annotation and add it to the annotations collection. Link: http://scintillanet.codeplex.com/SourceControl/changeset/96716
Since DNN wraps everything in forms, you may need to use the mailchimp API instead
Your fighting a losing battle, the Stockholm syndrome is just to strong with some people. 
TSF is a turd for source control, stay as far aways as you can.
Not just DNN, it's ASP.NET WebForms. There is one glorious form. Trying to use form tags the way they were meant doesn't work in ASP.NET. I recommend you look into [submitting a form using javascript.](http://stackoverflow.com/questions/13001830/creating-and-submitting-a-form-with-javascript)
Is this built on top of ejabberd?
Is this built on top of ejabberd?
No daily blogs but have you tried greader/theoldreader? You can sign up to many blogs via rss and a local copy will be cached.
i've been pretty happy with winhost before i moved to a winity vps where i'm also pretty happy :-)
Azure is definitely worth checking out; I believe they have a free tier which might work out for your test apps. Winhost is inexpensive, but I've used them for a couple years now and frankly the performance is pretty bad. Updating anything sizable over FTP takes noticeably longer than similarly priced unix hosts and my ping monitor sends me periodic alerts about downtime fairly often. All that said, for test apps it should be fine, so long as you don't need to upload a massive amount of data.
http://www.dotnetrocks.com/
http://blog.cwa.me.uk/
Thank you for this! Actually, that seems to be the problem: there's no .add() or .List() or .Items or anything in the annotations collection. How do I go about adding to it?
When the Azure free websites are sufficient enough for your needs, then roll with that. They're free and perfect for testing. For everything else **Azure is too expensive for simple hosting needs**. The power of Azure comes with scaling out, but for hosting a single machine it is simply too expensive. This is a good and cheap provider: http://www.discountasp.net/
I'd almost be nervous buying anything from a website looking like that (especially a technology service). That aside, this is exactly what I'm looking for at a decent price. Do you know how it might compare to winhost?
I know the websites looks very suspicious, but a source that I trust told me it's reputable - that was enough for me. I didn't know about winhost, so I can't compare.
There used to be plenty of downtimes and failures some time back.. Like on the 29th February 80 % of the computed instances didn't work anymore. Or one time the certificate for the blob storages expired..
I have no doubt, I run load balanced website for these cases, however even if I wasn't. I still think it's better than what most average "web host" can provide.
Check out the morning brew, updated daily. http://blog.cwa.me.uk/
~~I'm sorry but~~ DiscountASP.net is crap. They one time charge you for each domain name you have (not an issue for some, but I have 20+ domains) and then charge you $10 extra for each MSSQL *database* you need - not $10 for access to MSSQL but $10 for each db. So $20 a month for really, really basic hosting with one domain. I moved all my stuff to [SmarterASP](http://www.smarterasp.net/) and it's been great so far. $10 a month.
I think what you might need here is an Odata endpoint for you to query, this allows filtering and sorting very easily Here is an example from msdn http://www.asp.net/web-api/overview/odata-support-in-aspnet-web-api/odata-v3/creating-an-odata-endpoint
It's not really an apology.
this is the method I am going to try. Thanks.
Neat that this works, but really **horrible** for deployment. The deployment process should be an automated one, with as little user-interaction as possible (which means just initiating the deployment).
It's a start I guess. I imagine if they can do this, they can add stuff like creating a deployment scenario (pick a source repo, pick target servers, configuration...) and later on you just initiate those automatic deployments. As it is it would certainly be useful for dev/demo self-service stuff.
Totally agree with you. Currently we're not targeting audience who already has a CI setup, but people who deploy via Visual Studio. Do you think we should build a API or integrate with CI systems? We support deployment to web farms, which could make an integration interesting I think.
I edited it just for you.
&gt; Then we upload this zip to a hidden URL Does that hidden URL have any sort of user authentication on it? This sentence just struck me as an attempt at security through obscurity, which isn't all that great if it's what you're relying on. E.g. if I manage to guess the URL which is used to deploy Bob's website, could I send a zip file that URL and expect the server to accept and deploy it?
Second a VPS if you can budget it. I've been very happy with boxes from NFO. They're a game server host so you aren't going to get anything but a clean Windows install, but they have solid machines and a fast DC. 
Using linq you can use a predicatebuilder to dynamically create a query and then execute it against an innumerable. http://www.albahari.com/nutshell/predicatebuilder.aspx
Great question, I'll update the blog post too. The hidden url contains a guid which is unique for each deployment. When the client (your web server) requests the zip a unique key for the client is required too (2-way authentication).
Thanks for all the recommendations. Lots for me to choose from!
http://highscalability.com/blog/2011/3/3/stack-overflow-architecture-update-now-at-95-million-page-vi.html I'm not entirely sure how much of Web Forms is being used at Stack Overflow. MVC 3 is listed, and I see no mention of Web Forms. However, I agree that one should focus on excellent design and using the best tool for the job.
Security updates, likely. That's the Microsoft ecosystem though, no getting around that. Are they at least scheduled?
If they're deploying via vs, then I'd prefer a web deploy over manipulating files. However, if you add easy site swapping between site folders to quickly rollback a botched build, that would be very attractive (to me anyway, if I weren't already using azure)
must be canadian
hmm.... I believe you have a point. Maybe I have been to fixed on the idea of preserving the workflow. I have tried to modernize minor stuff, but maybe it's about time to redo more... not that my manager is going to like the idea :) But I believe I can do it. In the meanwhile I will look into Predicate Builders as well, they seem like something that's nice to know.
Adding two that I didn't see in other's lists. Links to playerfm, but they should point out the original source. MS Dev Show https://player.fm/series/ms-dev-show The Web Platform Podcast https://player.fm/series/the-web-platform-podcast
You're right, I am able to instantiate from the annotations collection. The problem is that instance has no add() method? I am working in C#.
I generally yield to pragmatism over purism. Typically ORM libraries are useful enough to reference in the presentation/startup tier. Even though the repository/business layer is where you'll fire off the actual persistence operations. Unless I really need to add a DTO layer, I generally pass around my Entities and create (view)models and mapping layers where necessary. TLDR; why not use the EF dll (NuGet) reference [IOW: your entities] in your startup project?
One way to get this across is to quantify the time spent. If there are 100 peons using this screen 100 times a day and it takes them 30 seconds to search (data entry, unoptimized query, etc) getting this process down to 5 seconds is like buying the a few extra employees. A few extra full time employees is a lot of value for a product, much more worthwhile than an extra search field for one manager. 
Are you not using the NuGet package?
This along with http://www.alvinashcraft.com/ http://coolthingoftheday.blogspot.com/ http://www.dirkstrauss.com/the-daily-six-pack are on my daily/weekly reading list. And if you are interested in podcasts, try http://thesoundof.net/. It aggregates a lot of the .NET-related podcasts. 
It sounds like your current process just needs some simplification. To start, I think the logs of what updates have been applied to the database belong in the target database, not in an admin db. Also, IF EXISTS is fine but I'm guessing a missing IF EXISTS isn't the reason most of your scripts fail. What would you say the main culprit is, things not getting run? Getting run multiple times? For some tooling that might help, maybe something like [MigSharp](https://github.com/dradovic/MigSharp)? It's like a database project, but just a class library that has all of your migrations in it. It keeps track of which migrations it has run against the database, running the console application checks migrations available in the assembly with those already run for the database and only runs what's needed. The thing that I think might help you, is that if your scripts look like what I'm guessing they do, each script has some housekeeping code in it that Migsharp will probably manage for you.
Why not add a copy task to your msbuild script? I build a complete installer with compiled docs, assemblies etc. all with 1 msbuild script, it's not that hard to add a copy task to copy the files you want to the output folder and be sure things run OK from there. Check out the MSBuild community extensions to make your life easier. https://github.com/mikefourie/MSBuildExtensionPack
You don't explain what fails with the scripts, so there's little else we can do but guess. So guessing here, what I think happens is that you have separate smaller DBs, e.g. one for each dev, and the devs create changes on these databases and create scripts with these changes (manual or using tools). Then these scripts are grouped and you're trying to make a single change script from these separate scripts for the database the application is targeting. This of course isn't really going to fly: it will only work if there is no overlap between the separate smaller databases, but that's rare. To solve this, let each dev work with the complete schema, so all the tables. Changes are made either centrally (so devs pull the schema changes from this repository for their local dev work) or the changes made locally are committed to a central repository where dbas merge the changes into the central DB onto which the tests run. It's really a planning problem though: if dev A changes things dev B is working with and also changes, you have merging problems and you can only solve those with merge tools. Merge tools for databases do exist but they're more complicated than merging a couple of source files. But again, just guessing. 
Why are the scripts failing? Are the scripts failing because of data issues on other environments? or because scripts are run in different order to dev - i.e. developer 1 creates a script and applies it, developer 2 creates a script that relies on something created in developer 1's script but they check it in faster so gets run first in upgrading and then fails. First thing I would do is have a "development integration environment" and have scripts automatically applied to it after check in/nightly and report failures (possibly one with data in to catch data issues) - thus able to ensure you catch the issues quickly rather than next deployment. Other things I have found helps automatic running and recording of scripts (we have n application to do it) and keep scripts very simple with only one purpose - this makes resolving issues much easier (e.g. either table created or not, not where did it crash) 
What we do... We use SSDT (SQL Server Data Tools) for migrating schema changes (tables, columns, stored procs, etc) but then we have another step for simply running scripts for data (it's a homegrown application. Pretty simple). SSDT is a visual studio project type so it has fancy tooling and stuff. You can use msbuild to deploy it (actually it might be called sqlbuild or something similar). It will model the database and model your database project and compare the two. It then generates the alter statements or create statements or drop statements needed to get the models to match up properly. You'll fight with it a little bit at first but in the end it really works well compared to other options.
The nuget package is setup for my data project. It is not setup for my startup project. This is an issue particularly with MSBuild so local builds will copy the EntityFramework.SqlServer just fine.
A copy task is an alternative solution but it smells bad to me. The biggest hurdle is that our developers don't control our build process. We have a build team that composes the MSBuilds for all the products owned by the company. I can get them to add a copy command into the MSBuild but my problem with it is that our devs immediately lose sight of that piece to the puzzle. I prefer to keep all my mechanisms as close to the code as possible so the devs own it and not a build team that exists in a different state. So it's not a bad solution if you own the build process like at a smaller company. But here it would just go missing and no one would know wtf happened. This way, it's a file in the .sln with a comment that tells why it's there and its checked into TFS.
Well, if you want to get things magically being placed into folders of your choosing then I have to disappoint you: deployment is often a job on its own: you have to develop the scripts and code which puts everything into place so deployment is repeatable and correct. The good thing is that you only have to write it once, and it's very easy. :) I run one .cmd file which kicks off an MS build script which builds everything, from doc files to installers to additional zip files to chocolatey packages and the cmd then uploads the zips and installer to our server and publishes the chocolatey package. Repeatable, works always. Took some time but I can deploy a myriad of files which seem complex at first without worry that I miss something: everything is there, otherwise the build fails :)
We have a job for deployment. The problem isn't unpacking a zip on the server. The problem is that MSBuild doesn't drop the EntityFramework.SqlServer in the bin directory like it does all the other dlls. It is missed because MSBuild compiles all binaries into a single directory and then copies them into 'deployable folders'. But it doesn't know about the SqlServer.dll due to no code directly using that .dll. 
Post build event would work better for me since then our developers control it and not our build team. I don't like putting stuff like this on the build team because if something changes on the dev side, such as no longer using EF, no one knows to inform them about those changes until after our deployments/builds fail. But.... I don't see a reason to add unnecessary copy scripts when I already have a solution that forces MSBuild to do the work correctly.
This. SSDT allows you to treat your sql artifacts as code where you don't need to worry about the ddl cruft around the tables, price, indexes, etc. It also generates difference scripts to work more like desired state configuration which helps with the OP's problem of upper env failures. Avoid worrying about change scripts and generate them. SSDT generates an artifact called a DACPAC that is a package for SQL server and allows you to do the script generation outside of Visual Studio in something like Octopus Deploy or Microsoft Release Manager .
I've used this as well, I really liked it.
It depends on the scope of your system. Can you give any more details?
MVVM will gel well with any WPF UI because it ensures a specific, targeted, model for each view which is ideal for databinding. The other presentation patterns you mentioned are applicable to numerous aspects of your (eventual) system. They would serve adequately in most circumstances, and admirably if you create a tailored model for databinding (as part of the pattern or in parallel). But that would essentially reinvent the MVVM pattern. Go with MVVM for the UI and strong unit testing for the devices and you should be happy in WPF :) Modelling of the device controls happens in the Model objects that represents the controllers. Personally I would expect to have an base representation in the "Model", a concrete implementation that interfaces with the external device (in your Anti-Corruption layer, perhaps), access to these instruments through a service that is used by a ViewModel to display the info. That way you can test your concrete device (ie tempDevice.StartThermometerBankA()), device logic (ie tempSensor.ReadCurrentTemperature()), and your service API in isolation with a clean, bindable, package for the UI in form of the ViewModel (ie vm.LatestTemps).
Great, I'm glad I'm in the right track. Since I will be needing to coordinate the control of the different devices (issue a move command to the motor, wait until the motor move has completed, perform the measurement, issue another move command once measurement is complete. Would I create a DataAcquisitionManagerModel class and just call .TakeMeasurement() that handles all the coordinating. Or just handle the coordinating of all the deviceModels in a single ModelView?
You make one ViewModel per view. Normally that is a single screen, but oftentimes it can be a component on a screen. Having a view model composed of some data and 4-5 smaller view models is totally normal. The ViewModel should only contain descriptions of data and available actions. Modelling is the act of describing things, so it's a question of if DataAcquisitionManager has meaning to you. To me it sounds like a guy who works on a shop floor ;) I'd ask myself what *exactly* am I trying to do by waving these machines around and engaging their sensors? Am I making a Report? A StatusCheck? A Snowboard? I would then model a class to represent that action. For example, a SnowboardPress which relied on constituent components of motors, sensors, belts, etc to make a Snowboard. My ViewModel tells my SnowboardPressService to MakeANewBoard(), the service instantiates the Press (or uses a cached copy), and triggers its primary function. The Press would then trigger the specific actions in-order with appropriate status/success reporting. 
WPF (an MVVM implementation) will be perfect for a front end. You'll create a model that has values for the status of your process and each input in your system, then let WPF bind them on your UI. This is your UI layer, and below this will depend greatly on the scale of your project. If it's large scale automation, the UI should not be on the same machine as what's controlling the hardware. (I believe this is in the ISO 9000 manufacturing specs.) If this is the case, the layer underneath the UI will be a service layer, which basically relays actions to the server and data to the UI. The server would have a layer on top (most likely WebAPI) which handles requests and passes them down to the next layer which, if you're not doing a large scale automation, is where the small scale picks back up. I assume that you'll be running a process. That's the next layer. This is the layer that concerts the efforts of the outputs to get something done. You'll be unit testing the *hell* out of it. "Does it die horribly if I call this function at a time that doesn't make sense?" "Does it shit itself if the hardware fails?" It will also surely have a lot of pass-through functions, because underneath is... The systems layer. This layer assembles all the individual hardware pieces into the machine that has a purpose. If there's even a snowflake's chance in hell that another system will be built, this layer is necessary. It affords you the ability to have hardware built in different configurations with different I/O points, but still be compatible with the same process. You can also think of this as your state machine, while the process is your logic to traverse the state machine. How it works will depend on your hardware, unfortunately. The layer below is made up of classes for your individual hardware pieces. Below that I typically have a communication layer so I can make the hardware testable. Somewhere below or along side communication is your logging layer and any other similar supporting elements like alarms *et al*.
We use http://www.red-gate.com/products/sql-development/sql-source-control/ which is good but hideously expensive (they raised the price after we brought it). Basically you check the database into version control and then update other databases from there. They have a command line tool that we run from within Team City to update the various databases on demand.
This deletes and recreates tables to add columns, say goodbye to replication. It also doesn't handle data migrations well. It also fails silently. 
Not everyone can do this. There are those of us that must access hardware through our applications. Doing so through a browser is exceedingly difficult (impossible?) without specialized plugins.
I've found that the DBA types hate these tools because it points out how irrelevant they are. 
There are similar tools that are in pure sql, that can keep them happy. And nothing is stopping you from using pure sql with fluent migrator of course. It's a lot more concise in c# of course ;)
Better hope its https, you're essentially sending a plain text password that can't be changed.
What? None of what you said is correct. 1. In the publish profile you can set whether you want to drop everything and recreate (for a local db for instance). Otherwise, if you rename a column or something similar, you do it through Visual Studio so it properly adjusts a file called the "refactorlog". Any renames or similar operations will be noted in that file to hint to msbuild that it should do an sp_rename instead of a drop/create. You can edit this file by hand if you don't want to use the VS tooling. 2. It can handle data migrations well if you use Post-Deploy scripts. But honestly, we only use it for domain data 3. It does not fail silently if you use the proper publishing procedures (I don't remember what they are off the top of my head)
You are wildly uninformed on database projects (SSDT) judging by your response to my comment below. The tooling used to be a little shoddy but in VS2013 it's much better.
&gt; In the publish profile you can set whether you want to drop everything and recreate (for a local db for instance). Otherwise, if you rename a column or something similar, you do it through Visual Studio so it properly adjusts a file called the "refactorlog". Any renames or similar operations will be noted in that file to hint to msbuild that it should do an sp_rename instead of a drop/create. You can edit this file by hand if you don't want to use the VS tooling. The refactor log is essentially a crappy version of migrations. &gt;It can handle data migrations well if you use Post-Deploy scripts. But honestly, we only use it for domain data And this works poorly. I want to write my migration based on the snapshot of the database I know exists. I don't want someone elses change possible weeks/months down the track to break my migration. &gt;It does not fail silently if you use the proper publishing procedures (I don't remember what they are off the top of my head) I've seen plenty of failures, mostly in the post migration scripts but some with table changes as well. I was the only one on the team that actually caught these errors because I performed it locally and checked the logs. Others made it all the way into production because no one found the error. The miration script didn't stop, it just kept going.
I've used homegrown migrations like you described but not the specific tool you mentioned. In fact, we still use it when SSDT doesn't suffice (for example when we simply want to make data changes).
I should have been more clear about the branches. I mean it works great with long running branches (10.1, 10.2, 2.0). I've worked on products that get installed on client sites and their code can be several versions behind. With migrations that's OK, each step goes from a known state to a known state. Upgrading several versions was just a cumulative patch. You can't do this with SSDT. Information might get added, moved and removed over several versions. The longer they are out of sync the riskier it gets. And you can't use post migration scripts effectively.
This implementation was created several years ago when there really weren't any good migration tools available. We used it BEFORE we used SSDT. We continue to use it in some areas to make life easier.
Can you give an example of what information might get added/removed? Are you speaking in terms of data? If so, using a mixture of migration tool + SSDT works great. It's what we do.
Step1: Cloumn B gets added, information from Column A goes to B. Step2: Column C gets added, populated with data from Column B. Step3: Column B gets removed. Works fine with migrator, but SSDT would just see column A and C, not have the information necessary to get from A to C.
This is a pointless conversation. Just use whatever flukus is suggesting and enjoy your mediocre solution that has its own set of problems. Yay for being stubborn.
&gt; This is a pointless conversation. Just use whatever flukus is suggesting and enjoy your mediocre solution that has its own set of problems. Maybe if you actually pointed out what these problems were? And your saying SSDT is better, despite having to fall back to a migrator when it doesn't work. That is you being stubborn, not me. Edit - In fact, you've said absolutely nothing to explain why SSDT is better.
Another one would be when you want to split a table and move data from a column in table1 to a column in table2. It can't be done pre migration, table2 doesn't exist then. It can't be done post migration, the column is deleted by then. You have to do the whole thing manually and can't complete the process until the while thing is deployed, which could be months/years in the scenario I mentioned.
I actually agree with most of your points through out the comments, and I use the same instantiation hack to force the dependency to be recognized by MSBuild. However, the [stackoverflow solution is pretty close to the top when googling](http://stackoverflow.com/questions/14033193/entity-framework-provider-type-could-not-be-loaded).
hence me saying anything non ef :) haven't really had any issue with the ef migrator personally.
I'm a C# developer going about 3 years now, and I only got 3 of these right (Dynamic Mess, IEnumerable Nuance and Captured Variables). Quite a good challenge. Makes you think about how the language actually works. I like it, are there more? :)
The problem I see with this solution is that if you have multiple startup projects you have to add the package to each. Then, if you add a new startup project you have to *know* to add this package to a project that doesn't really need it from a compile time perspective. By adding the line of code in your data project you force all startup projects who use the data layer down the stack. If your strartup project references your business layer which refs your data layer then they will get the assembly via the compiler and not via some convention only you know about.
I'm not sure how to link to my own comments... So instead I will just copy paste my reasoning for why I think that method is smells bad. &gt;The problem I see with this solution is that if you have multiple startup projects you have to add the package to each. Then, if you add a new startup project you have to know to add this package to a project that doesn't really need it from a compile time perspective. By adding the line of code in your data project you force all startup projects who use the data layer down the stack to take this assembly along for the ride. This is a solution that is forced via a compiler and not via some convention only you know about.
Indeed. I would say testing is pretty high on my list for why I like using interfaces, but it isn't the only large benefit.
I didn't make myself clear.; I totally agree with you. I was just pointing out that "your" solution was easily googled (it's not the accepted answer, though). Sorry :)
I use [AppHarbor](https://appharbor.com/) . It's great. I have used Azure before, which was nice, but my trial expired. I waited a month and then tried to order more Azure services with my account, but it won't let me. Working with their support team has been less that helpful.
I'm from the southern US and it's a pretty common expression to begin a disagreement with "I'm sorry but ...". It's not a genuine apology for having your own opinion, but maybe just a politeness while arguing against someone else's opinion?
I got 1 and 3 right. I still need someone to smack me with a lead pipe to really understand what is happening with yield return. I use it, but I just can't visualize what is actually happening, step by step.
I just want to know how he got a running compiler environment embedded in his page! 
I wrote this over a year ago, good for some laughs: https://escapelizard.wordpress.com/2013/10/16/10-things-you-maybe-didnt-know-about-c/ But disclaimer: it's an old article and a bit messy now. 
Another vote for Git here. Mercurial and Git are equivalent to the Blu-Ray and HD-DVD battle for standards that went on a few years ago. They both solve the majority of the problems you have and can be interacted with in similar ways. For most purposes, you're more interested in the quality of the tools than the underlying provider. Mercurial seems to be largely well supported by third party tools, and at one time had a reputation for having far better tooling than Git (for Windows at any rate). I think Git has caught up and its popularity has surged ahead, too. Hg isn't dead but I think it's lost momentum. I can't speak to ease of setting up and using Mercurial, since I only investigated it before we decided on Git, but as far as Git goes, the setup is substantially easier than VSS or SVN. The newer Visual Studios have good built-in support for Git (and Mercurial, I believe) and aren't a bad place to start, but I'd go with a different third party package (or simply use the command line once you're comfortable with it). [SourceTree](http://www.sourcetreeapp.com/) is a free, newish Git tool by Atlassian that works pretty well. It's still a bit buggy, but the UI is a nice, sensible wrapper over the Git command line (any Git command it runs is visible to you, so you can learn the commands without having to compose them), and it's got a nice way of visualizing branches. I highly recommend it as a beginner's app to Git. I also highly recommend the [Git Flow](http://nvie.com/posts/a-successful-git-branching-model/) strategy for branching/merging. Jumping into Git from a rigid system like VSS can be overly liberating, and Git Flow is good for establishing a consistent, logical structure for releasing software.
Its not that difficult, you can leverage Roslyn for this. I created this for my own purpose (http://volatileread.com/utilitylibrary/snippetcompiler). It has limitations though. It runs in a separate AppDomain with restrictive permission set. I had to do this because, otherwise anyone can call write code which would kill the server e.g delete all files on the server etc. May be once I refine it, I can open it for public to embed anywhere. I'd be glad if anyone finds it useful.
yield return isn't really what is at issue here. It's just the fact the code does not actually execute on an IEnumerable&lt;T&gt; method until you actually enumerate over it. Think of it as lazy execution. I.e. this behaves like you would think at first glance: public void Main() { var chars = GetChars(null); foreach (var c in chars) /* kaboom */ Console.WriteLine("some char enumerated"); Console.WriteLine("done!"); } static IEnumerable&lt;char&gt; GetChars(string input) { if (input == null) throw new ArgumentNullException(input); foreach (char c in input) yield return c; } You can also understand it by debugging. The code in the method GetChars() is not executed when var chars = GetChar(null) is run. 
**Spoilers!** So the first code does indeed "fix the glitch" since "int j" scope is a single iteration of the for loop. The variable j cannot be captured by the Action, so only its *value* is captured, not its reference. The second actually produced "4, 4, 4, 4, 4" not "5, 5, 5, 5, 5" because i is incremented after each loop, then the loop exits before it can update j to be 5 as well. This is more rudimentary loop mechanics, but it's easy to glance and forget in this instance. 
You are not correct. Replace the loop with return null; and you get the exception raised. This is weird crap caused by compiler rewrite I guess. 
Ahh, yes that makes sense
Perhaps I'm not clear on which loop you are talking about, but I do not get the results you state... Mind pasting in the full code block you are using? "return null" or "yield return null" will not even compile if you're talking about the GetChars() loop. 
Cheers!
\&gt;How to toot your own horn
Thank you, congrats to you too, if this is the first time then I guess you are so excited :)
Thank you, the essence of the post is to motivate smart people in the community to share knowledge, I was shy at the beginning, but really once your start you can't stop! I wish you best of luck in the coming rounds, do not lose faith you will get it soon :)
Thank you, it was the begging, when you do not have audience, when you feel that you are writing for no one, but if you write with passion and you give too much; readers will feel this and you will build audience and get more confidence, pick up a niche, share your posts on social networks ,and gradually you will build audience. Good luck :)
Turn on all characters mode maybe? Or make regex ignore them. I think characters might count as two.
It is indeed HTTPS.
With all due respect, it's not an 'achievement', it's not something you can win when you try hard, a lot of other factors are at play: the region you live in (the # of MVPs is fixed, and MS awards MVPs all over the map), who nominates you (or IF you're nominated at all), and e.g. who decides who becomes an MVP. Sometimes it's enough to run a user group or write some articles, in other cases you have to lead big OSS projects and there are many cases around where people are at the helm of large popular OSS projects and still aren't nominated at all. When I got the award (C#) in 2004 I was very happy, I saw it as a crown on my work, but 8 years later when I didn't get renewed I had learned it's not that: MVPs are primarily a part of MS' marketing and PR machine. 
Thanks for your thoughts! The interesting thing is the regex works perfectly in a standard TextBox control. Why doesn't it work correctly in a RichTextBox Control? Or rather, the matches for the first few lines of a string do work well in a RichTextBox, but then the match index starts to go further and further off. The Regex is exactly the same in all these cases, just the control is different. Do different controls have different character offset calculations?
What makes a rich text box rich? Probably supporting more characters. The offset isn't your problem, the problem is what it thinks is a character. For example, can you do a \n in a textbox vs richtextbox. 
I see what you're saying. For my source text, I'm using the same raw string in both cases and repopulating both boxes for each test. I hope this has been enough to avoid newline characters creeping in anywhere.
Thanks again for sticking with me and for giving your advice on this. I have made a simple example of my code in winforms and here's an animated gif: http://i.imgur.com/pQh2GjI.gifv that shows exactly what's going on. On the left you see a textbox, and on the right a richtextbox. As you can see, they're getting their text from the same place (a copy of the textbox's raw text). The code is the same, but the selection index results are different. As you can see here, whereas the textbox correctly finds the term "an", the richtextbox does at first, but then starts to drift. What do you make of this? Thanks!
Yes, I think the length is the same. It's hard to say, since every time I try to determine the length, I get a different value (from Notepad++, versus textbox1.text.length, versus other character counters online).
In MSSQL 2005 or above you could query the [INFORMATION_SCHEMA](http://msdn.microsoft.com/en-us/library/ms186778.aspx). SELECT * FROM INFORMATION_SCHEMA.COLUMNS 
I like your writing, very clear and concise. Thank you for yor answer.
If you're sure it's .net then it's irrelevant if it was written in Visual Basic .net or C#, they both compile into .net assemblies (.dlls). You can reverse engeener (if you don't have source code) with ilspy app (free download on internet). You can't convert it to web application. You have to write it again. Some code can be copy/pasted, but yeah, it's very different technology.
This is really a decision that needed to be made early on before development really started, during requirements analysis, not after the fact. Depending on how well the developer adhered to good architecture practices it could range from a pain in the ass to almost impossible without lots of time, money, or resources. As someone who doesn't know software development this is likely something you couldn't and wouldn't want to do. As far as the language goes, Visual Basic is still a part of the .Net framework. I have never had the need to look at an applications byte code and determine which language of the .Net framework was used. I am sure a fair to decent knowledge of MSIL and the different compilation mechanisms between the different languages you could figure it out. I think a phone call to the dev would be orders of magnitude easier and I am not entirely sure knowing the source language would be of any value to you at this point. Finally, if your company just said go off and develop this without defining requirements and/or limitations to the developer and gave no requirements beyond that, it seems to me the developer delivered. Also, as far as your updates go, it doesn't have to work that way. You can use clickonce publishing among other strategies to have your application automatically update when a new version is detected in some arbitrary pre-defined network location. At this point I would think this would be the more viable path rather than rewriting as a web app.
Ah, great answers, thanks. Follow-up: does the app need to be developed with ClickOnce in mind or incorporated into the code? Or can I publish any app this way?
Very cool.
Before you throw the baby out with the bathwater, consider the advantages of a web application and consider what you are asking. * Roll outs of updates of an .exe could be solved with a network drive. * AD integration is going to take additional development regardless. * Maintaining a web server has it's own issues Most likely the developer was constrained. When you have limited resources and limited time, you end up with limited features. 
You could deploy it as a Remote App, only one assembly to update then, but RDP licenses aren't all that cheap either.
Perfect, thanks. This will help in my conversation with them.
That's my guess, especially in light of the other reply comments. They were indeed working with sparse/disparate input from our execs, and budget was, as with most, limited. 
I'm less worried about the update portion than I am about maintaining a separate set of users &amp; credentials. But based on what I've learned, I think I can live with it as a .exe now. Thanks.
I hear you. I started a simple app and am now faced with a mountain of "template-ware" I don't want. 
I think the closest is to start with the empty project template, then just add the nuget packages you need like mvc.
You could contract more dev hours and have them implement AD authorization. It also is not very difficult to implement as long as you have a fairly straight forward AD implementation.
If you're just after a web application with the basic structure and references in VS2013: File -&gt; New -&gt; Project and under Templates select the Web node and ASP.NET Web Application template. You then get to select the specific template you want in another dialog. If you don't want all the 'fancy stuff' select empty template but then check the the folders and core references for the type of web application you want (Web Forms, MVC, Web API). Is that what you mean? 
Please don't take this the wrong way but I wonder if you are the best sysadmin for this companies specific needs. Is there any reason why a full redesign is necessary? If the issue is an update mechanism then you should either use clickonce or one of the other libraries for performing updates (I've used wyupdate in the past with success). It sounds like what this company needs is a sysadmin more versed in the technology stack they are using. I don't know how eager you are to learn, but I see no reason that couldn't be you. Dive into the source code and online resources, it will be a fraction of the job redesigning from scratch would be.
 public static void Main() { GetChars(null); } static IEnumerable&lt;char&gt; GetChars(string input) { if (input == null) throw new ArgumentNullException(input); return null; }
Not much is going to change. You're pretty good to go for the exam considering it'll be another year before the exams are updated with the new os
Telerik has a free tool called "JustDecompile" that can create C# code from a .NET assembly. The king on the block used to be Redgate's "Reflector", but they've unexpectedly adopted a pay model that has caused their tool to be boycotted by many. Keep in mind that the output will not be as pretty as the original source. There will be no comments, but it'll at least give you workable source. 
That too. I just assumed that approach was obvious, and that the poster was asking for ways of decompiling it because that approach had already been ruled out.
One major oversight on their part, though, if they do not have the source: op explicitly mentions they plan on doing many updates to the app. Kind of hard to do that without the code (unless it has a nice pluggable architecture, which I'm seriously doubting given the rest of the story).
&gt;When I went there, the first 10 days I literally did nothing, and the only tasks I was given were fixing bugs. That's work. Part of that is to get familiar with the system. I was once paid $65/hour for nearly 3 weeks, without actually doing "real" work. Once management, and my familiarity with the system caught up, it got busy, though. &gt;I went to two different interviews while I was working and I found my ASP.net MVC skills are amateurish at best and I need to work on it more, If I wanted to get employed anywhere as ASP.NET MVC developer. Correct me if I'm wrong, but it sounds like you got a job right out of university. Of course it's going to be amateur type work - you're not skilled enough yet. &gt;In one hand I knew what I wanted, I don't want to fix only bugs in decade old code base, I don't even like Webforms but in the other hand been unemployed is not good either. Welcome to the real world: Where you do shit jobs to try to move up the ladder. It's all part of the game. &gt;after the interview he told me that I have the capability to think like a programmer but he want a junior who already know mvc to be productive at the job immediately, he told me I'm wasting my time working for a company fixing only bugs on softwares and I should focus more on developing my skills as ASP.net MVC developer. What did they expect from an intern just out of college? Too much, I see. You ARE a junior, afterall, but if they expected a seasoned MVC developer, it sounds like they fucked up on the job search. &gt;My friends and family thought I overreacted and the company was bound to create new products eventually, the thing is the commute was long and when I came home with massive pounding headache for riding public transportation , I didn't have the energy to work on my side projects or learning ASP.net. You did overreact, and so did they. They expected too much from you, but it sounds like you caved in, and offered more than you thought you could do. 
To clarify : the company (if we can call that) is small (one part time graphic designer, one part time technical support, one part time developer). I was the first full time developer, there is no ladder to move up or down, the owner work full time as Program Implementer for another company. Through his connection and network he bought the source code of two different softwares, which he sells on the side. My job was simply to maintain the softwares. &gt;That's work. Part of that is to get familiar with the system. I was once paid $65/hour for nearly 3 weeks, without actually doing "real" work. Once management, and my familiarity with the system caught up, it got busy, though. I didn't spend my week reading the source code, my machine wasn't setup for me because the technical support guy failed to show up during the week. &gt;Welcome to the real world: Where you do shit jobs to try to move up the ladder. It's all part of the game. I did work in retails and customer service so I know how to deal with shit jobs but is different when you start your career in shit job with no future. 
&gt; I didn't spend my week reading the source code, my machine wasn't setup for me because the technical support guy failed to show up during the week. Ouch. This happened to my on my first week at a Fortune 500 company. Boredom is bad.... but at least you got paid, right? &gt;I did work in retails and customer service so I know how to deal with shit jobs but is different when you start your career in shit job with no future. If you feel like you don't have a future at your current place of employment, leave as soon as you can! If you don't have a job lined up yet, I'd milk this position as much I could. They ARE paying you, right? EDIT: I just want to clarify: Just because you don't think you're going to "move up" in a position in your current employment, doesn't mean that you aren't "moving up". Keep at it - that experience adds up - even if it's not recognized by your current employer.
Well, no. I was supposed to sign the contract with the start of the new year. that why I planned to get out quickly. plus I don't know how to milk the job, I don't know webforms, I don't have the mentality of not giving a fuck (I actually called the owner 3 times to get my machine set up to work as soon as possible). I felt it was unfair to me and to him too, I don't know webforms at all, I got lucky because all his bugs were database related issues but if he asked me to develop or change any features then it would be a problem, I don't have a senior to watch over me if I screwed something up. If I wanted money I can get a data entry job, it will pay more and far less stressful.
Making it a webapp would be basically writing a whole new application. If they used some kind of API it might be easier to do - some of the code might be reusable, but just taking an application and turning it into a web app isn't just a simple conversion. It will take some development work and depending on how it written (my guess is hacked together if it was one dev and not a team) then it won't be an easy task.
LearnVisualStudio.net is brilliant, Bob Tabor is a really good teacher. Also try pluralsight.com if you can try and swing it from your work. If you have an MSDN subscription you might be able to get a wpf course from pluralsight for free
Reality check: if your job is to "simply maintain the softwares", then you are a maintenance developer and are doing exactly that. Consider this experience as walking into something you aren't familiar with, and just become familiar with it. Guaranteed, you will be in this situation multiple times in the future as a developer. Edit: Oh right, you already quited. Get another job like this, and work hard at it next time.
ClickOnce is full of giant pitfalls. It looks easy to set up, and is unfortunately deceiving. Unless you have a LOT of experience with it, it is very easy to end up with a situation where you have to either re deploy the app again, or even do a bunch of manual remediation on machines (removing install keys, etc). There are other more reliable and proven technologies out there.
If it's coded in WinForms, you might be able to use [this product](http://www.mobilize.net/webmap2) to convert the application from WinForms to Javascript and HTML, although if your developer is mid-development it may cost more to have him modify the generated code.
That does make a bit of sense, considering GP was purchased by Microsoft from Great Plains Software and folded into the MS family. It's so complicated and its functions so varied that AD securities don't really apply, and extending the AD schema wouldn't really make sense (IMO). I guess my wish was really more for the Single Sign-on aspect, which isn't so much an AD function as just simple domain authentication. In any case, as many have pointed out, a lot of these questions would have been better brought up earlier in the development cycle, years before I came on board. I appreciate the input.
Thanks for trying this out and sharing your code. I copied it exactly as your wrote it, used the same source text and ran it. The two offsets your code returned were 19 and 1868. Even if it were 1875, when I put the text into Notepad++, that is unfortunately still not the correct index for the second Steve. Here is a screenshot: http://i.imgur.com/IfYGzT4.png So confusing! Any more thoughts on these results?
Use [dotpeek](https://www.jetbrains.com/decompiler/) to look into the dlls. 
I guess I think that webforms or mvc are both just abstractions over and about http stuff. We can agree that MVC is a fine pattern and that webforms is too much obfuscation. But at the same time, modern webforms is a reasonable technology that is quite malleable. You have access to routing libs just like MVC, you could work on http handlers and get close to the metal. Webforms is also part of the larger .NET ecosystem - getting used to learning the various APIs is part of the work. Perhaps you're putting too much emphasis on particular technologies. I did this when I was starting out too, but these days I'm fine with most anything in .NET land. It seems like you made the right choice in quitting - it wasn't what you wanted - but you could've also capitalized and learned/researched on the job while getting paid. 
I hate to say this but I'm being honest. In my experience developers are rarely going to take development advice from a sysadmin. Especially when you're new to the company and have little experience in how the business operates. I know being the new IT kid on the block you want to run things at the best they can be done but don't underestimate the value of domain knowledge! It's very easy to have scope blow-out when you don't know how the company works. Think about how you can add value to the business with the smallest steps possible. Ask around if they even care about signing on. How about looking into some one click sign in solutions which could be used with other products in the organisation? If the mobility of the Web is a big deal, start by moving forms people on the road would use frequently and tie it into the existing software's back end. If you want faster upgrade, maybe some powershell scripts and a network drive could help you out. Don't walk in there guns blazing that everything should be rewritten as a Web app and use AD or you'll be laughed out of the room. I'm serious. 
Right but unless he has the original source, the language used is kind of irrelevant. Just decompile it to whatever language you're most comfortable working with and go from there. 
Hoo boy. You quit your first programming job because they put you on bug detail? My jaw is dropped. I'm trying not to sound too harsh, but yes, that's a huge mistake. A junior developer at any job is going to be put on "bitch duty", no matter what. It's the natural pecking order of things. You're the new guy, you will never get a say on what you work on. I put in my 2 years of bitchwork and then moved onto bigger and better things when they let me. It's probably too late to go back to the job now, but if there's one piece of advice I can offer for the future: ***NEVER QUIT A JOB UNLESS YOU ALREADY HAVE A BETTER OFFER ON THE TABLE ALREADY***.
Why don't you give this a try: https://www.remoteapp.windowsazure.com?
You won't get better at MVC working on webforms so I think you did the right thing. But as the other 2 replies said: it's not all greenfield projects. You learn a lot by doing bug fixes especially if you're a junior. You lose a lot of bargaining power when you don't have a job
It looks like the [richtextbox](http://i.imgur.com/gUJiq3v.jpg) removes the \r char and the [textbox](http://i.imgur.com/8utNnH7.jpg) keeps it and that is where the char difference comes from. 
&gt;(1) You cannot tell if a (.dll) was compiled from C#, VB, C++ unless you have in-depth understanding of the .Net compiler which I can guarantee you very few people have. =D I haz a rare skill? I bet it wouldn't make a good party trick would it... =( "Ok, the paper in front of you has IL code on it, take a note of the language on the post it note, and then pass the IL page to me. Without use of earpiece, microphone, nor a browser, I will tell you what it was created from!" ~audience walks off, carries on drinking~
Is it used for purchase order request? If it is, I'm assuming your company bought the MS Dynamics GP product but didn't bother to get the Requisition module add on. In such case, a custom app is built as a frontend for PO entry. You mentioned a bunch of xml so I'm also assuming that the app used eConnnect, its a module used to easily integrate custom third party app data with Dynamics GP. Check out eConnect, maybe it has some limitations that's why they resorted to just building an exe.
1) As other have said, could be either. I'd probably crack open the application in dotPeek and see if it references a bunch of visualbasic dlls. 2 and 3 sound like requirements/scoping issues. If your org didn't ask for AD integration, and it wasn't in the spec, then they won't implement it. There could be business reasons for the decision too - maybe your org doesn't have CALs for all users in dynamics (though I dunno if routing all requests through a single service proxy would be skirting the license), or users of the software won't have 1-1 AD accounts or something similar. An exe isn't necessarily a bad choice for this type of development, though I am pro-webapp, personally. Issues around updates are possible to mitigate in exes. It's probably a big thing to switch from app to web. It's probably a straightforward thing to bring AD into the application (regardless of whether the application is an exe or a webapp). Depending on the competency of the team that developed the software, there should be architecture/design/proposal documents or something that provides a rationale for the decisions that got made. Your company probably signed off on these at some point. I'd expect these to cover generally when each decision was made (things like windows app and application authentication are pretty major/basic decisions so I'd expect them to be in the earliest documentation/correspondence) A lot of this stuff is in the "how did it get here?" category. It sounds as though there's been either inadequate oversight, scoping or internal management of the project to get it to the point where you have an application which doesn't suit your organisation's needs. If the original project lead from your organisation is still around, maybe try asking them why those decisions were made. Your company will likely need to spend a bunch of money getting the developer to change functionality now. It's probably going to be a hard sell to get things changed (the AD thing sounds possible, the webforms thing sounds less likely) - my questions to the external developer would be something like: * When was the decision to use a windows app over a webapp made? By us or you? Why? Who from mycompany signed off on that (ie who you should talk to about this)? * Same as above for using application-level authentication instead of AD. * How much will it cost to change the app to a webapp? * Same as above for authetication. Then find out internally if you own the IP on the sourcecode and ask who was involved in the project at your company.
You are right, I realize now that I overreacted, my reasoning was that I didn't work in company with established hierarchy, is just felt weird that the boss work full time in another company and only send me tasks through e-mail. 
I realize that I always have to fix bugs as junior, but I talked with the employer and he made it clear that he wouldn't develop any new products, and the whole thing was something he do in the side. 
I realize that I always have to fix bugs as junior, but I talked with the employer and he made it clear that he wouldn't develop any new products, and the whole thing was something he do in the side while he is working full time in another company. but I got message from all other comments, I will never quit next time. 
This drives me nuts too. I guess if I want jQuery or Bootstrap or EF in my project, I'll add it myself, thank you. No need to have 412 frameworks in your project in case you might use them.
It's even worse with WebAPI. I want to make an API, not a web app, why do there have to be all those extra things? I mean I found out how to do it (solutions mentioned in this thread work for WebAPI as well) a while ago, but it's still weird.
I've not tried but I'm a Mac user and code most of my C# on *nix systems these days. Mono has support for WinForms and most of the regular .NET stuff works pretty well so my guess is it shouldn't be too difficult.
I totally agree with you, but like /u/codekaizen said, in an intranet SMB environment it is a quick and dirty solution. We use the WiX toolset now but I maintained ClickOnce for 7 years and the biggest issue I ever had with it is installing to the roaming profile in a TS environment and it was a bitch about dependencies and had not very (read: not at all) useful error messages. 
I think [Autofac](http://autofac.org/) and [TinyIoC](https://github.com/grumpydev/TinyIoC) both support Mono. 
There's no point in adding more fuel to the fire. I largely agree with what everyone else has said. You overreacted by leaving, and while fixing bugs doesn't sound glamorous, it's actually a fantastic way of learning, and is something EVERY programmer should go through. What I'm keen to address is your attitude to working as a developer. There's nothing wrong with WebForms as a technology. It has its limits, but at your stage any experience is good experience. There's also nothing wrong with maintaining old code. While you might not be working on something brand new, you might end up adding new features, or refactoring code to be more efficient. I too have worked in retail, and while I hated every second of it I won't look down on how hard it is. Software Developers aren't a special bunch. We're employees too, and sometimes you just have to sit there and plough through work that bores your ass off for clients that you often dream about punching in the face. A lot of people leave university and expect to be working on Google style problems. What you'll quickly realise is that shitty jobs can often be a good way to learn. I believe that unless you've worked for a shitty company with a shitty boss you'll never truly appreciate working in a good company. You'll also learn things you might not learn in a structured environment. People say life is too short to working somewhere you hate, but you're not tying yourself down to a role for life. Stay somewhere for a year, maybe more, and then bail when you feel you've got everything you can out of the job. It's what many devs do even at places they really like.
no problem, you can also use this to do a regex search foreach (ScintillaNET.Range m in scintillaTextbox.FindReplace.FindAll(new Regex("steve", RegexOptions.IgnoreCase | RegexOptions.CultureInvariant))) { scintillaTextbox.Selection.Range = m; textBoxResults.Text += "Match at pos " + m.Start + " in scintillaTextbox = [" + scintillaTextbox.Selection.Text + "]" + Environment.NewLine; } 
The class List&lt;&gt; uses the interface ICollection, IEnumerable and more, not sure there is a difference using either. I would solve it by adding another table to the database; BlogTags. Have atleast an ID and String (the string you store your tag). If a post with the string does not exist; create it, then add it to the blogpost. BlogPost need a property like this; public viritual List&lt;BlogTag&gt; Tags { get; set; } And maybe a property in BlogTag for reversal; public viritual List&lt;BlogPost&gt; Posts { get; set; } &gt;Another thing I would like to add to my blog creator is to suggest user what tags should he use (depending on what he used before). Fetch what other posts is used with the same tag (from BlogPost.Posts), fetch tags that where used together on other posts as suggestions to user. &gt;Do you know how is it done on sites like stackoverflow? StackOverflow Architechture (see mysterious Tag servers); http://stackexchange.com/performance
I thought that might become an issue, a while back when I first saw your site. Just out of curiosity, what caused it to break? And are you still vulnerable to it?
basically i run it in a separate AppDomain with restrictive permissions, and kill the thread if it exceeds some memory threshold or time. But the problem was that if somebody writes Thread.Sleep(a large value), the thread cannot be aborted until it comes back to managed stack (i.e sleep interval ends). This causes the thread abort to freeze and the process continues to run forever. I changed it to move to another process now, as process killing is definite. The servers are good, and probably I'll keep on losing money on that. But I am happy if someone uses it.
If you don't want to normalize tags, you can use Full Text Search indexes instead of Like queries. But, like most things, EF doesn't support this.
My favorite example is windows media center (not media player). There are a few youtube videos on it, but I'd suggest installing it for full effect.
This solves the "where do I put my DbContext.SaveChanges()?" problem and IMHO should be getting more attention. [github library] (https://github.com/mehdime/DbContextScope) Acknowledgement from Julie "Entity Framework" Lerman on [Twitter](https://twitter.com/julielerman/status/522797244760993793)
As the only living serious programmer who still uses VB, I am honored that someone took the time to make all of these kickass changes.
Agreed, he put a lot of research and thought into it, and what he came up with isn't bad, but it seems like it would be much simpler and cleaner to lean more heavily on DI and scopes/lifetimes.
How would use use DI and scope/lifetimes to a) allow sending of an email only after a db save is completed and b) "join" implicit transactions? I'm not sure you can. Of course, if you don't have those requirements something else might be preferable.
Personally, I swear by DI + well configured scopes with marker interfaces for multiple databases. I'd challenge the scenario you describe - failure to write to persistence is exceptional, and shouldn't happen. If its an issue enough to be writing requirements for it, I'd attempt to address this root cause. That said, legacy applications sometimes have this kind of crazy, and I'd probably solve it by moving the unit of work down a level of abstraction - use a chain of command pattern with the database portion being one command that commits its transaction, with subsequent operations fired afterwards. That would be my "legacy compromise".
Working in Legacy one of the areas you can make an impact on a system. Bar none. You can pretty much refactor until your hearts content, you can make wrappers to old calls in new fancy ways, cook up solutions to problems you've identified and present these to your colleagues. Because that's what good developers do.. It's rare you get to work on exactly what you want to all the time in this game. So what are your alternatives now? To be honest I do think you have made a mistake. But an important one. If you learn from it, then that's all good and you shouldn't worry about it. Sometimes you'll learn people don't know why things are a good idea or a better option... you've got to steer them and let them think it's them that's making the decision on something great, when really, that was your plan all along. You know where you get the real money and make the real difference in our game? Working at either end of the spectrum. Either bleeding edge, or with tech so old no one else wants to touch it. If you want the middle ground, the easy predictable stuff then be prepared to not be making much of a difference in your day to day duties. It's just the way the game works. Suggested reading; The Passionate Programmer;http://www.amazon.com/The-Passionate-Programmer-Remarkable-Development/dp/1934356344 Apprenticeship Patterns; http://chimera.labs.oreilly.com/books/1234000001813/index.html Best thing to do is put a period in it and move on. Good luck! 
You might be the only one around here that knows what EpiServer CMS 6 is...
No. It solves a problem for me so I thought it deserved more attention.
Thanks, and the passionate programmer looks like the book that I'm looking for.
&gt; Roll outs of updates of an .exe could be solved with a network drive. From a SysAdmin's perspective, this is probably not the best idea. OP should have the app packaged up into an MSI and deployed using infrastructure management; at the very least, via GPOs.
&gt; Dive into the source code and online resources, it will be a fraction of the job redesigning from scratch would be. It may be that diving into the source code is legally dangerous waters to venture into without knowing the details of the OP's contract with the developer.
I really don't like this idea at all. Deployments should be automated. Even in a small site/infrastructure, the publishing tools in Visual Studio are preferable (in my opinion). However, good luck to you! (I say this without sarcasm!) This looks like it could really help a small team, but it worries me that this tool could encourage bad practices.
Worked. I was wanting to post this in /r/WNC
vnext comes with its own webserver, so you should use that. 
That sounds pretty good. Let me see if I understand you correctly... Lets say I have a web application. By default I want to have a dbContext per request with SaveChanges() called at the end of the request. I configure my DI container to do this for me. This will be my 95% use case. Then I have a few crazy cases where I need more control over where SaveChanges() is called. I create a new marker interface IMyDbContextNewUnitOfWork which is implemented by MyDbContext. The commands (e.g. MyCrazyCommand) that require save changes called at the end of their execution have a IMyDbContextNewUnitOfWork injected instead of the regular MyDbDcontext. The DI container is configured to call SaveChanges on dispose of MyCrazyCommand. Is that right? Have you got an example?
Supposedly `kpm pack` gives you a package you can drop in the www root. I'm on OS X so can't verify.
Microsoft.AspNet.Loader.IIS (AKA "Helios") is the IIS &gt; vNext glue. * This server loads applications inside IIS and integrates with other IIS modules. * It bypasses the System.Web infrastructure and significantly boosts performance. * It benefits from IIS's support for Windows authentication, static files, etc.. * Note that an early version of this component was built to integrate with the Katana/OWIN infrastructure, but now it uses the ASP.NET vNext infrastructure. -- https://github.com/aspnet/Home/wiki/Servers
You're on the right track there. Normally I'd use a marker interface for a session that the calling code would explicitly control, so if you inject the naked context its auto-saved and auto-disposed, of you inject IMySpecialContext the calling code is expected to do this work. Alternatively, use session per request for everything, and for your 5% of wacky stuff just inject a session factory and use a using block. Because special cases are special and making them conform when they're a minority of use cases is probably more effort than its worth :)
I think he means that you can create a new scope, separate to the current one, your case might be something like: /* normal code */ using (var scope = container.BeginLifetimeScope()) { scope.resolve&lt;ClassThatHandlesCrazyCase&gt;(); } Because the scope is different the db context that ClassThatHandlesCrazyCase ends up with would be different to the one normal code gets. Could you give more information on what your crazy cases entail?
Considering that we know the final feature list of either, nor his specific needs, it is way too soon to start making claims like that. 
Not really. Frameworks come and go. It's more important you learn how to code than any particular framework. 
Phew. Well, I am definitely learning that. Thanks for putting me at ease.
Got it. Thanks for sharing.
&gt; e.g. #1 only send an email after db save is successful. As I said, a queue of some description would be best. Another, hacky, way to do it could be to generate and store the emails in a buffer with IDisposable. The mail could be sent to exchange in the dispose method. &gt;#2 legacy database I'm not intimately familiar with EF, is there really no way of controlling the order of this? Otherwise, have you considered not using EF at all? You might be trying to squeeze a square peg into a round hole and over complicating everything else to make it fit. Edit - just realized the first one won't work. Dispose will get called either way.
You'll learn lots of frameworks, and they all change over time. This career path if you choose it means being a life long learner if you are going to be doing it for long.
No ASP.NET vNext builds upon skills you gain learning ASP.NET (in particular ASP.NET MVC) 
It's pretty sweet. I've been using it to build a few smaller applications and the framework is nice. The things I love are auto compile. I can just save a change and go reload a page and bam C# changes are there. Project.json is much nicer to work with compared to web.config and lastly the new changes to Razor are awesome.
You said "the webserver coming is the one that's to be used" and that's just plainly wrong. It's just another option. And I meant better performance of vNext, not that you get better performance using IIS. But you won't get worse either.
Well, to be honest, I'm not sure. I've read on some comment that it was coming from the GitHub for Windows team, but I'm a bit skeptical now.
Thanks for your reply. I appreciate it. 
No I believe they have fully migrated IIRC. Could have simply been staging screenshots or something, but I'm pretty sure I saw @paulcbetts talking about it before he left GitHub.
&gt;Both versions of vNext import dependencies via NuGet packages rather than assemblies. Even when you build your own libraries, they can be published automatically as NuGet packages when you build your final output. What does it mean to build libraries and then publish them automatically as a NuGet package? Does anyone have a tutorial and/or blog post on how to do this? Also, does this mean the NuGet package will be in the public domain for anyone to use? What if I want to make a private NuGet package that can only be used by my corporation projects? 
&gt; auto compile. I can just save a change and go reload a page and bam C# changes are there. Project.json is much nicer to work with compared to web.config and lastly the new changes to Razor are awesome. When will this be production level? Do you think it will be this year with Visual Studio 2015? 
You can set up your own private nuget servers.
Yes, it works much the same but there's a crucial bug (For us, at least) - the resulting installers don't work on Windows XP. 
10 years from now asp.net vNext will be classic. :)
Where did he go?
I believe it was Slack...? Not 100% sure on that one, but whoever it is got a really solid developer. 
If you need an installer that can basically do anything WiX isn't a bad choice. I've had to write some very complicated installers with it and while it can definately be a pain to work with it is powerful. You can call scripts with it and execute .Net code during any part of the installation process. Also I've only been using it about 2 years now and it's had 3 releases during that time and if anything the community appears to be growing.
Why not letting the application itself deploy/manage the database installation on its own? That's what we're doing here. We deploy the application using ClickOnce, silently updates on start, etc. Good stuff.
That's also an interesting approach. Are you using libraries to help you doing this, or it's all custom-made? How do you check for updates and download/install the updates? How do you check with ClickOnce if it's the first time the user is starting the app and thus needs to create the DB? Do you use migrations to update the database schema?
Do you have a resource/documentation/link that you can provide me on how to do this please? Since assemblies are being depreciated, I need to learn how to create/host my own nuget server. 
SCCM can manage versions from a push perspective where a central authority can force a app update to clients. This works best if you control the target machines. Citrix or other app virtualization platforms can host a central authority application install that is containerized and executed in isolation for each client over a web portal. Updates would go to the single location. If your DB is shared between app instances, Microsoft this year changed the licensing for their release management product to be included with visual studio licensing. This would allow you to orchestrate the database deployments with the application deployments. SQL server database projects make database deployments much easier.
In Python this is: current = ((py,px),"")
It's in vs 2015 preview. The auto compile is meant for development. For production, you would do a different build.
I was just wondering if there was a better way. I use tuples of tuples pretty commonly in my programming and it's very hard to read the constructor here. 
I agree that exposing a Tuple publicly from an API is a bit sloppy-looking. For internal use I think they're OK, though if you only need to use the object in a single method, anonymous types are cleaner, especially since you get to name the properties. IMHO defining proper types is still preferable, I bet I've only used Tuples 4-5 times in the last 5 years.
Every time I start using a tuple, five minutes later, I'm creating a new class, because they're just not reader friendly. I then scold myself for being lazy in the first place.
&gt; I agree that exposing a Tuple publicly from an API is a bit sloppy-looking I'd just de-genercize the tuple: class MyAppSpecificTuple : Tuple&lt;string, int&gt; {} Whenever I find myself doing anything non-trivial with generics, I use inheritance to simplify the genericization.
Visual Studio Installer (vdproj or whatever) is awful and trades ease of creation for problematic installations for your customers and future maintainers. See also * http://blogs.msdn.com/b/robmen/archive/2006/09/18/761580.aspx * http://stackoverflow.com/questions/2635671/why-use-windows-installer-xml-wix-over-vdproj If you're going the MSI route for a product you expect to survive at least a couple of Windows versions, I would strongly recommend using WiX. Building a simple installer doesn't really require a large learning curve and you'll have a large community of people both on Stack Overflow and through the e-mail list. There's also about a billion highly qualified consultants who can probably get a solid, maintainable installer out in a matter of hours/days dependent on how much you're doing. I don't feel like Squirrel really solves any of the major issues without adding a myriad of problems right back in - lots of problems with missing files, errors when checking for updates within the app, less than thrilling experience working within the nuget framework. I think using NuGet's framework as an app delivery service makes a lot of sense on the surface, but the issues list definitely makes me shy away when you discuss a reliable installation/distribution strategy. I would strongly suggest adding NSIS since that's a pretty good installer that's more or less understandable to anyone who has coding experience. There's also the Express version of InstallShield - it's only $649 so it's not quite as price bloated as the Pro or Prem versions. You can find some listings of software in the Wikipedia page on the topic: http://en.wikipedia.org/wiki/List_of_installation_software Really, I feel like this space is something that hasn't gotten the level of attention it needs. Squirrel seemed like the solution, but I don't feel like NuGet is mature enough to handle the use case.
They switched the Atom.io installer to Squirrel recently: http://blog.atom.io/2014/12/10/a-windows-installer-and-updater.html Paul's still actively working on Squirrel: https://github.com/Squirrel/Squirrel.Windows/commits/master
GitHub for Windows still uses ClickOnce but they AFAIK plan to move over. Atom uses Squirrel
Why not use a [triple](http://msdn.microsoft.com/en-us/library/dd383822%28v=vs.110%29.aspx?cs-save-lang=1&amp;cs-lang=vb) if you're using three properties? Or for that matter, a struct? 
VB looks the same except you use dim instead of var and drop the semicolons.
I'm really sorry you still have to support XP. :(
I wouldn't worry too much about the youth of Squirrel. While it may be lacking some features due to it's relative youth, you also have the benefit of it being open source. So you can fix things, add features as you need them. And before you worry about the extra dependency, its not that much different in practice than being depending on a closed source product.
Yes that's exactly what I was thinking. You could get fancy and start adding categories/groupings for the fields. The other downside unless you start to make those fields values reusable, is that it gets difficult to tell which ones are the same for say reporting. (I.e. if you have a field called serial for two different asset types are they holding the same info? Or what if one asset type has 2 fields called serial, then just matching by name becomes a bit of a challenge. ). Reporting becomes a challenge too because it gets a bit more difficult to do reporting with tools like SSRS. You'll have to generate them on your own in the program. Or worse hard code field records to a view of some sort. This is what usually gets me, I finding that these dynamic systems are hard to build dynamic reports using traditional reporting tools. 
You have a circular dependency there which will cause issues to query your data. To simplify I would suggest perhaps having field type independent of the asset type. If you wish to constrain the types of field types by asset type then you can create a join table of constraints. Honestly I would advise you to not overthink your problem and stick to the requirements. 
This looks pretty neat! I work from home and do fairly regular side work and I've been looking for an automatic way to track my hours in each area.
You can, but passing it back into the view after failed validation will be much harder.
This is basically why ViewData and TempData exist. In this instance you have a lot of info to get to the screen to do important things like build dropdown lists, but the end result, the selected and/or entered values, is the only thing you want to see posted back. I've seen this done a lot of ways, but for me the most effective way to do it is to have a single ViewModel that contains the values entered/selected on the screen. For all the stuff that's not actually the value sent to the server, add those to ViewData. These ViewData elements can be pulled out in the View and used to construct lists of SelectListItems, etc. When rendering out your view, you can take the Model values and combine it with the collections, etc in ViewData to create the input form. This correctly brings up the question of validation. If you encounter server validation issues, you'll have to re-populate the ViewData elements for the View to be rendered properly. This isn't difficult. You can create a builder method that'll set the ViewData elements for a request, and simply re-use this. Of course any large lists should have some sort of caching strategy behind them, assuming they don't change often or aren't conditionally included, to prevent you from having to go back to a database to retrieve the lists. 
For dropdowns what I do is set a ViewBag property with the same name as the view model property I'm creating a drop down list for. For example, if you have a view model with a CustomerId property, I would set ViewBag.CustomerId to a select list collection of type IEnumerable&lt;SelectListItem&gt; and fill it with items. Then in your view if you use @Html.DropDown("CustomerId") it will pick up on the IEnumerable&lt;SelectListItem&gt; and generate a drop down for you. To make it even easier to reuse I have extension methods for converting IEnumerable&lt;something&gt; to IEnumerable&lt;SelectListItem&gt; with support for a default value and selected value. 
Take a look at ASP.NET Session state. By default, ASP MVC uses pessimistic locking per session. This means a user can only make one request to controllers at a time. A different browser would be considered a different session. You can disable this at the controller level with [SessionState(SessionStateBehavior.Disabled)], or let it know that the controller only reads session state without modifying it with [SessionState(SessionStateBehavior.ReadOnly)]. For further information, check out http://tech-journals.com/jonow/2011/10/22/the-downsides-of-asp-net-session-state
thank you, that explained it perfectly.
I agree, I didn't write the original implementation. I'm moving it out into a task, I just wanted to make sure I understood why it was behaving the way it was.
I actually would like to use them but don't, primarily because last time I tried it slowed down the build process considerably.
Could have been worse, with a recursive generic parameter.
I use them fairly extensively. It is still rough around the edges, but do provide a more structured way of performing defensive coding (eg ensuring attribute aren't null, ensuring return results, and more complex conditions). However, there are a few things that you need to be careful of. Firstly, the static analysis is very slow. It is so slow running your unit tests will be faster than waiting for the static analysis to complete, and catch most of the bugs anyway through the runtime behaviour. To manage this, I created a new solution configuration "CheckContracts", as well as new build configurations for all projects with the same name (inheriting settings from Debug). I then enable static analysis on that config, and disable on the rest. This allows fast build times normally, but allows you to perform static checking every now and again. This is useful as the static analysis will catch some bugs that your unit tests will miss due to not testing every combination of inputs from the unit tests. Primarily though, I just use it as a more structure runtime assert framework, as a way to both document and assert the expected values in input parameters, and the expected output result. If you just want it for that, go for it and just disable the static analysis behaviour. Lastly, it does complicate deployment, as you need to install code contracts on the development/integration machines (and not just point to a nuget package). 
WPF is completely style-able. So, in this age you might want to look at some of the "modern UI" styles that are floating around and being used by the community. Have a look at: [MahApps - ModernUI styles and controls](http://mahapps.com/) Here's an example of someone using MahApps to create a media player: http://github.com/punker76/simple-music-player [Dragablz - Chrome style tabs](http://dragablz.net/) - *disclaimer, this is my open source library* [ModernUI - Another Modern UI fx](https://mui.codeplex.com/) 
Yeah exactly. That's why I use my own implementation which is just ICloneable&lt;T&gt;. Though this produces another issue: If you now have an interface and an implementation where do you put it? I ended up putting it into the interface, so my class - when cloned - actually returns an interface. In my case it ended up working, but if you have multiple different implementations of an interface this can get you into trouble. Code: public interface ICloneable&lt;T&gt; { T Clone(); } public interface IBananaEater : ICloneable&lt;IBananaEater&gt; { } public class Monkey : IBananaEater { public IBananaEater Clone() { return new Monkey(); // copy all parameters manually } } public class Human : IBananaEater { public IBananaEater Clone() { return new Human(); // copy all parameters manually } } So now when you clone all items of a List&lt;Monkey&gt; you actually end up with a List&lt;IBananaEater&gt; and need to manually cast each item to monkey (assuming your convention is that each class actually returns its own type on clone, which is kinda ok for a convention). The reverse case (applying the interface to each individual implementation so that *Monkey : ICloneable&lt;Monkey&gt;* prevents you from cloning a *List&lt;IBananaEater&gt;* (which is needed often in our system due to a template based system).
Pretty much every generic CMS sucks on some level. Though I've heard EpiServer sucks more than most and you get to pay through the nose for the privilege.
&gt; @Html.DisplayNameFor(x =&gt; x.UsersLanguage ) thanks ;)
The job description looks like a 100k+ job description. I'm not sure why you would expect it not to be.
Is this in the US? Here in Canada (GTA) this type of experience would get you about 70-80k
This is a very common MS stack. (C#, SQL, EF, MVC, .Net, Agile). It shouldn't be to hard to fill depending on how sticky the word 'expert' is.
Because it's on Reddit...
In AB we make 70k as medior, this kind of position would definitely be over 100k.
Good point. But it looks like verbatim every job description I have applied to in the last 5 years... and they where all 100k+. You really couldn't get more precise Microsoft environment stack then what was listed.
This requirement is in California (SoCal) and a contract position for on-site work.
I don't know the rates at all but I'll be happy to point you to someone who does.
This is a long term onsite contract position in Cal. I'm assuming the rates will be favorable but I know nothing about them. 
I'm a big fan of LINQ, stored procs not so much. The ease of creating a new strongly typed object and popping it into a function call to save to the database is just too nice to pass up. I've been slinging web code for a little over a decade and get the trade-offs of LINQ vs SP, but the ease of development and code readability and maintenance is a big winner in my book. YMMV, I work mostly on sites for small businesses these days so query tuning is not nearly as critical as it is on larger sites. When you get into the millions of views range, you probably need to have more control over the SQL that is actually executed than you do with LINQ.
OK, will ask my friend for it and share it with you.
I meet these requirements and am interested in the position. If details about the company or compensation appear I would love to hear it.
I'm sorry but this is not my line of work at all, I can't answer.
That's why I mentioned GTA. Sad state of affairs here. 
Should be completely obsolete. It's to difficult to enforce all the copy down through child members. Say for example icloneable Did require the implementation a deep copy we still run into problems with the children. I expect that if I execute a local deep copy, I can do with the object as I please. There's no way to require that children of children also implement a deep copy mechanism. I suppose at a higher lever though abstract classes you could require this chaining of deep copy through the layers, but with private members well never really know what's going on and if we are getting "true" deep copy or mot 
The only valid reason I've ever heard to use sp instead of orm was that certain transactions, including updates, can lock tables and potentially cause problems. I believe it comes down to when you should be calling the savechanges method. 
Grand Theft Auto?
Lol... Greater Toronto Area
I have no idea about the rate yet so I can't make any assumptions. Thanks for your feedback though.
this (json.net solution) works if: * you mean deep clone, not clone-with-references * your object is json-serializable, i.e. contains no circular references * the thing you're cloning isn't really big
This is all true.
I'm really glad to hear that, thanks for telling me. Was the project as interesting as it sounds? We once had a neural network proposal to monitor a large city's electrical network. It's probably the project i'm saddest not to have won.
Thanks.
The code you posted "decrypts" a byte array by XORing everything in it with 29. THERE IS NO KEY OTHER THAN 29. Nothing you have posted has anything what so ever to do with the size of the file. Nothing you have posted has anything to do with generating a key for a file to decrypt the file. You either do not understand your problem, or you are not presenting it to us in any reasonable way for us to try to help you.
It does a while loop (num1) over the entire array (data) passed to it. This is all basic programming concepts. You probably shouldn't be jumping into reverse engineering encryption if you don't know those concepts.
&gt; MVC TIL 1995 still counts as "new". MVC has been the foundation of UI programming for *twenty years*. It's "new" in that in the past decade, web developers finally got with the program.
Today you learned there is a big difference between an academic paper and the real world. Many programming languages didn't have the feature sets to support MVC. The big move to MVC today is not because people spontaneously started using it, it's because language designers started building languages and features that make it easy for a programmer to follow it. Criticizing a developer for not using MVC in 1995 is like criticizing them for not using 128 bit encryption in 1995. "What, it was technically possible, why didn't you use it?" 
Well, [MVC](http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) is much older than 1995. There are many tech stacks you could have been using by 1995 that utilised MVC. /u/remy_porter isn't being disingenuous in his reply. MVC is far from new, and it's use is so widespread, I share in his disbelief that someone thinks this is a 'new' thing. Think about it - ASP.NET MVC is pretty much a .NET version of Ruby on Rails. RoR has been popular since 2005/6.
&gt; Today you learned there is a big difference between an academic paper and the real world The Go4 book (which is what I was referencing) is *not* an "academic paper". It's a fundamental guidebook for doing object oriented programming. While pattern abuse is itself an anti-pattern, basic patterns, like Observer, Command, MVC, and Facade are things everybody should know if they're doing OO (just don't over-apply them!). &gt; Many programming languages didn't have the feature sets to support MVC. This is simply false. Every OO language has had the ability to do MVC. Hell, even in the bad-old WebForms, best practice for anything non-trivial was to do MVC. Java's UI engines have been built around MVC since AWT (remember that?). And let's not forget Struts. I've done MVC in VB6 (awkwardly, since VB6 is barely object oriented). Everything in NextStep was built around doing MVC, and that continues into modern OSX/iOS development. I'm serious: if you've done any UI development in the past 20 years, you've either used an MVC-variant or you've probably fucked it up. 
I'll do anything for anything if there's money involved. I'd change my price to be a lot more accurate before any contracts signed. If it ends up being a lot cheaper to do than that, I wont bother.
It depends on the app. If it's something internal that you don't plan on getting massive traffic, then the DB is fine. If you're building a public site that may get a lot of traffic, some other type of blob storage is generally preferable. Storing in the database means backing up your data is easier, everything is in one place so it's just a single database backup. But it also means that your database server is now being taxed for something that could be done (and done better) elsewhere. Since database servers are notoriously difficult/impossible to scale out, any work you can offload from the database, especially where there's a preferable alternative, is one less thing your database server is doing, keeping it focused on what it has to do (storing structured data that needs some level of ACID compliance). It can be a bit surprising seeing exactly how CPU/IO-taxing it is to retrieve blob data from database servers, but the above really only comes into play if you're expecting a lot of traffic.
There are far more problems storing images in a db. The simplest / most performant / most scalable solution is to store your images on the file system. If security is a concern, put them in a location that is not accessible by the web server and write a script that handles security and serves up the files. Assuming your web/app server and DB server are different machines, you will take a few hits by putting images in the DB: (1) Network latency between the two machines, (2) DB connection overhead, (3) consuming an additional DB connection for each image served (depending on your implementation). I would be more concerned about the last point: if your site serves a lot of images, your web servers are going to be consuming many DB connections and could exhaust your connection pools. Db memory and storage source dramatically increases, it doesn't scale well. You can search on stack exchange 
A tip, if I may. Try writing the auto generated code yourself, using your own style. You're still falling behind if you're not digesting what's being created. You might think of a better way or find that the Microsoft way is overly verbose, or giving you lots of features you don't quite require. 
Thank you all for explaining. Things really get different if you have a high traffic site. Learnt a lot today, because of these answers.
This fixed it! Thank you so much. 
I think what you're looking for is called the Offline Installer: https://www.microsoft.com/en-us/download/details.aspx?id=42642 
All you need to download is one version of the .net framework. If you need to target 4.5.2 you don't also need to download 4.5.1.
I assume so but haven't seen any documentation on how exactly Chromium/Chrome actually implement the "search for username field" functionality. I ran into this issue a while back, spent a fruitless hour or two picking through Chromium code trying to find the logic, and in the end just guess-and-checked my way to a solution. I'm glad it worked for you and I wish I knew why!
Ugh...okay. Yet another different version. Keeping all these permutations straight makes me dizzy.
The built-in git client in VS doesn't support ssh remotes, I'm pretty sure. I wouldn't call that "full" git support.
express is essentially obselete now that community edition is out i wouldn't bother with it 
Probably not useful, but I've had four instances of VC with ReSharper running at once on many occasions, and never once have I had any of the symptoms you're talking about. I'd try disabling your plugins one-by-one until you find the culprit, because VS itself has been extremely stable for me. 
I can honestly say I very rarely get problems with VS 2013 (running Ultimate if that makes any difference). Sometimes I get DLL locking and can't run my project without killing VS and re-opening. But I think that's more my design than VS. Also, it seems to have stopped happening about 2 months ago (touch wood). On a related not I have a new monitor today. Android Studio caused a stack overflow in my head and I may have accidentally slapped my monitor causing it to break. I felt like a right tit afterward. Fuck Android Studio, but at least it's not Eclipse. 
Precisely 
I've been using VS since its first inception. If there is one thing I have learned, it's to wait awhile before adopting the latest version. Let everyone else find what doesn't work, or what's broken, while you get on with coding. Eventually, MS get their shit together and release a few patches and BAM, everything is cool...
At work, mine will hang when doing a new run of the application. It does not matter which app. I have tried it a few different ways with building or not prior to running the app. I have to kill the app and then I can immediately run it without issue. It's annoying, but yeah I still love VS.
Look at your activitylog.xml file. It will go a long way in helping you sort your behavior issues with visual studio. Now that sql and tfs both use vs as their core, there's all sorts of fun going on. That's before all our fun extensions and addons. Sometimes just vaporizing your component cache will sort things, sometimes resetting the dev settings does. But look to your activity log and try to sort out anything it's complaining about. 
http://i1.asp.net/media/4773381/lifecycle-of-an-aspnet-mvc-5-application.pdf
I uninstalled them, thank you :)
There is no compelling reason to have one dev environment. Use hyperv. Create a base image. Isolate your work at the system layer. Work smarter my friend.
I'm not sure if its the Visual Studio or Azure team to blame, but the latest update (4?) makes for an unstable pairing (infinite looping request to log in), with the workaround being to log out and back in again. So, that's annoying. I've been working with the Cordova extension as well, and that's been really unstable (many crashes while saving a file, which is always fun). I think that's still in pre-release mode, though, so I guess I'm a glutton for punishment :)
Yeah but OP is new to it all so isn't likely to have been paying attention a couple of months ago when it was all announced. He's read a tutorial that was written before the community release which says to download express, he's hardly going to disagree with that.
Yeah, I agree.. But only since you commented in it. :-(
I agree, I'd love to see how this was implemented. I could see us using something like this at the company I work for..
The focus is the concept, not the implementation, but the implementation is simple. That entire UI was achieved using data binding and value converters that blended color values. The colors were blended using standard color blending algorithms: [PDF Blending Equations](http://acroeng.adobe.com/PDFReference/PDF_1.6/blend_modes.pdf) 
I'll second this. Wix is simple to use once you get the hang of it*, and it robustly handles upgrades, versioning, dll registration, and other things. *Getting the hang of it can be mindbendingly frustrating.
If you use WIX I suggest looking at [WixSharp](https://wixsharp.codeplex.com/) It greatly simplifies the process of using Wix
Topshelf is a library that makes it easy to create and install windows services. I don't think that this really applies here.
The link is broken for me
Late reply... I'm assuming when you say session you mean DbContext. If you inject your DbContext your Service classes then have state. Do you think that's a problem? I've always been taught that your service layer should not have state.
Great question - yes - because you invoke your entire web stack, filters, owin components (like auth), serializers, IOC containers - all the composition that unit tests generally don't cover, where broad acceptance tests pay off. Basically, you get the full benefit of ATDD (acceptance test driven development) without the frequent tooling context switches and out-of-proc complexities.
Also, this is a compliment to controller level tests, not a replacement for them.
One of the design decisions for EF7 is to do much less magic than EF6, so it will no longer go and automatically create databases for you. This might seem like a step back, but in reality it caused more problems than it was worth. It would make a database automatically for you, and then when you ended up adding migrations latter down the track and/or handed the code over to someone else, your initial migrations would miss out on the actual database creation bits that happened transparently. A real PITA. You want to use the EF Commands. You'll want dependencies in project.json for`EntityFramework.Commands` and `EntityFramework.Migrations`. You'll also want a `k` command for `"ef": "EntityFramework.Commands"` This will allow you to run `k ef` to get access to the various EF commands. In there are options for creating and applying migrations. `k` just works for me in the Package Manager Console, but if it doesn't I'd just try and get things working via a regular command line.
Why write a new library, instead of improving JSON.NET which is heavily used by plenty of frameworks and libraries already?
Part 2: http://stephenwalther.com/archive/2015/01/13/asp-net-5-and-angularjs-part-2-using-the-mvc-6-web-api
Sometimes debugger auto changes context to ui thread. I've faced that issue, the issue was not using ui thread to do ui stuff. But it didn't cause rendering issue. I've seen weird rendering artifacts if there is timing issues caused because you hid some controls during load and resize and later made visible.
Yes, this is also what I've been doing. The problems I notice is that versioning and remembering what files are out there, as well as hating deployment. I would much rather have a PM install the software, then me having to manually do it.
It's working now, thanks
A few notes: The await in catch/finally blocks aren't doing the same thing but pretty close (the stack traces will be different between them and there is an extra variable in the C#5 version). If this counts as equivalent then you could do this for the exception filter example: try { } catch (AggregateException ex) { if (!(ex.Data.Count &gt; 1)) { throw; } ... } The null-conditional operator is also useful because it provides a way to succinctly access each level only once instead of potentially having multiple field and property lookups. Eg: C#6 var user = User.Find("byteblast"); var count = user?.Answers?.Count; if (count != null) { Console.WriteLine("Answer count: " + count); } C#5 var user = User.Find("byteblast"); var temp = user == null ? null : user.Answer; var count = temp == null ? null : (int?)temp.Count; if (count != null) { Console.WriteLine("Answer count: " + count); } (as opposed to doing a property lookup of User.Answer twice because it could potentially be null)
A question: is there really a reason for starting any new projects with Angular right now?
Hi. That is not _quite_ equivalent. Please refer to my response [here](http://www.reddit.com/r/csharp/comments/2samv6/c_6_equivalents_in_c_5_xpost_rdotnet/cnnq7pa) for an elaboration. 
I really appreciate your comment, thank you. A couple of people have suggest to me a similar approach to emulating exception filters but this is the problem I have: https://github.com/ByteBlast/c6-equivalents-in-c5/issues/3#issuecomment-69806326. I would love to know your thoughts. 
One thing you might want to mention is making sure you have nodejs installed and that you install grunt in the same folder as your Gruntfile.js before trying to run anything in the Task Runner Explorer. EDIT: Nevermind. I didn't realize you could do this through the Solution Explorer until after the fact.
I'll third this. Over the last 5 years, I've spent probably ~3 man-months evaluating installer builders for our internal projects. After all of that time and effort, I'm pretty happy with Wix. Wix is free and open, so you don't have to worry about encumbering your development environment to use it (I *hate* paid components in our software, because it complicates new developers, new checkouts, deployment, debugging, etc). For a while I was trying to find something with a UI, and for a long time I was using JGAA's War Setup which was a wrapper around Wix.. but after a long time of using it, I found that it wasn't really completed or polished, and had long-standing bugs that weren't being fixed. We used the VS Installer project back when it was still available, for most of our projects, and tried other things for a couple of our more complicated projects, and we were never satisfied with any of it. Now we've moved all of our projects to Wix and I absolutely love it. One nice side effect is that your commit logs now have meaningful changes instead of being opaque "I changed this one thing in the UI and a thousand things in the auto-generated files changed" garbage. Wix does take a little getting used to, and you'll end up spending a day or so building some 'templates' for your organization, but that's time well spent in my opinion. You should consider your installer code the same as you would consider any other code in your project.
As someone who emphasizes code readability as much as possible, I am not looking forward to how some people will abuse the "static without full qualification" feature.
Ah, true. Filters in particular I didn't think of. 
Seems reasonable. Another consideration...it would be nice to prevent SaveChanges() being called on the DbContext that intended to be only saved at the end of the request lifetime. You could have an child class like so that would prevent use of SaveChanges outside of disposal. public class ManagedDbContext : MyDbContext { private new SaveChanges(){base.SaveChanges();} }
Can I have ?. now? I reeeeeally hate the API I'm using now because I don't have it. 
Thank you so much! i fixed the problem :)
That's a good idea about trying a Thread.Sleep() and seeing if that works in release mode. That'll be the first thing I try in similar situations. In this case, the solution was unrelated to that. If you're curious, I updated the thread text with the answer. Thanks!
I really, really want to! Perhaps my Google-fu has been off, but I can't seem to find a video tutorial that takes me through building a basic app with a GUI...? I don't know, coming from the Winforms world, I have to admit I'm confused where to begin with Xamarin. Anything good on Pluralsight guys?
I'm not sure I understand completely what you are trying to do, but a ContentControl with DataTemplates defined for various content types (i.e. your different views) seems like it might be what you're looking for. Might be worth doing some research in that area.
Hell WiX is what they use for the actual Visual Studio installer.
No problem, mate. 
Delegate.Target. edit: since no one seems to get it, all delegates has a property called [Target](http://msdn.microsoft.com/en-us/library/system.delegate.target%28v=vs.110%29.aspx). For static methods this is always null. callerObject = callBack.Target No reflection required.
Thanks, but windows authentication is on....
theres a handful of lessons on pluralsight, theres a load on youtube too
How does this compare against [Jil](https://github.com/kevin-montrose/Jil)?
&gt; Hi, when I come to install and **old program** 
What happens when you're passed a delegate to a static method? 
Jil caches metadata pertaining to the object being serialised, so that subsequent operations are invoked faster than the first. JSON# doesn't use Reflection, nor does it require any knowledge of the object's assembly in order to serialise.
Jil caches metadata pertaining to the object being serialised, so that subsequent operations are invoked faster than the first. JSON# doesn't use Reflection, nor does it require any knowledge of the object's assembly in order to serialise.
Good question. If the project gains enough traction, I may consider merging the two. There are several JSON parsers that claim to be quicker than the others, but none that are designed with performance-optimisation in mind, that I can see. This is the purpose of JSON#, and IMO, justifies a separation of concerns.
You could make a call to callBack.GetMethodInfo() and you will get a MethodInfo object that should have all the info you need.
Sorry, but what does "Indie version" mean? Edit: never mind. My mistake
That probably won't ever happen. 
This version looks up the Answer property twice. Consider if user.Answer was this: public List&lt;Answers&gt; Answer { get { Debug.Log("Answer.get"); return _answer; } } The versions I have written will call `Debug.Log` once, while the nested version will call it twice. It is likely this is not an issue, but it means the line you wrote cannot be automatically refactored into the C#6 without ensuring this getter is pure.
Never heard of wix plug-ins are you talking about custom actions or wix extensions?
Too vague how? Software developed using Xamarin products is 100% owned by you, the developer, and 0% by Xamarin. Same as software developed using Microsoft .NET, Apple's iOS stack &amp; Xcode, or Google's Android stack &amp; Eclipse/Android Studio. 
The size thing can be an issue if the size of your app is really small, but the larger your app, the less overhead from using Xamarin there is. In other words, yes, it adds something like 3MB, but with most apps being &gt; 20MB, mathematically, the overhead is small for most apps. (obviously if you have a 4MB app and 3MB are Xamarin's runtime, then yea, it's a massive size increase). Odds are that most apps are going to allocate more space to images, app icons, and launch images than the overhead you'd get from Xamarin's runtime. 
Have you considered using Visual Studio to edit your source code and then just ship it over to a Mac to build? Not ideal, I'll grant you, but doable if you are really that tied to VS.
Both, I just categorically refer to those as plugins, because its the plugin pattern that's applied nearly everywhere else in software development.
It sounds like he was talking about Xamarin Studio, yes.
I gave Xamarin an honest shot some time ago. While I was initially turned on by the ability to use C#, Visual Studio, Resharper, etc, I found myself constantly hitting up the docs to learn how to create my UI. THAT is what killed the process for me in the end. I since moved on to PhoneGap/Cordova as I was still able to use C# to create my API layer + business logic, and then build an HTML5 app using the skills I already had. Hybrid performance isn't as big of an issue as it used to be either.
I guess the main thing to point out is that the bindings are one-to-one. So learning Xamarin Android is learning Android - you necessarily end up looking at tons of Java code and reading through the Java documentation. Not that the Xamarin docs are terrible. A year ago, I would say the Xamarin Android support was pretty bad. A lot of basic stuff was painful, especially debugging with emulators and heinous XAML support. But after a year of releases and the completely badass Xamarin Android Player (5 million times faster than Google Android Emulator), development is nice. Components are cool too. As everyone says, if you're a c-sharp coder, there isn't a better option. I don't think Xamarin Forms is good for much yet. Maybe extra basic apps.
http://blogs.msdn.com/b/christinematheney/archive/2015/01/14/microsoft-exam-70-532-developing-microsoft-azure-solutions-test-resources.aspx
Its a limited version. It gives you all the cool stuff as long as your code base fits to a certain size limit. So you can write a small app no problem but once your project grows enough it will prompt you to upgrade. I believe it counts third party dlls, so I had to write my own REST interface to Parse in order to fit within the limit.
If your button already fires an OnClick event successfully from your gridview, then all you need to do is pass the ID to the other page. [Here is the Microsoft Page](http://msdn.microsoft.com/en-us/library/6c3yckfw(v=vs.100).aspx) detailing that task. [Here is a page on CodeGuru](http://www.codeguru.com/columns/dotnet/passing-data-between-pages-in-asp.net.htm) by Peter Shaw that details out the options a little better. Personally, for a minor value like an integer, I'd use Session because it's fairly secure and hides everything from view of the client, unlike a query string option. Editing to add: Also, if the report is not interactive (clickable links to other reports, sorting, etc) you could render it out to a PDF programmatically and return that to the browser from the same page.
I know so little. I didn't even know this existed.
Thanks for responding to this - I didn't know that!
I would only ever use it if I had to create an application that worked on several platforms and has a lot of common code. I have created android apps in Java, that where a breeze to do. And I have created xamarin apps for android only, where it felt alien whereas java felt natural. At work we had a business component shared between android/ios/windows phone and wpf. In this scenario it was awesone with xamarin.
This is mainly what I do, small to medium internal apps. Most the other devs in my office lean towards SPs, the main reason they give is it is easier (at least in our environment) to make changes to an SP than to change the code, because we can update the DB without having to wait for the 3am maintenance window. BUt now that I think about it, the app code would still need to be updated because you have to update the "function import" stuff in EF, right? Personally I have tended to using LINQ, so I think I will stay there and just use SPs when working on a group project. Thank you all.
I'd try it but I can't because it's prohibitively expensive. Even more for VS support. Therefore, I don't use Xamarin.
That's for the forums, not any code you write. If you don't post the source code, how would we even have it? We have a policy of signing NDAs with customers if the need arises for them to send us their code to try and reproduce a bug in order to fix it, so even then it's not an issue.
Cool idea storing connection strings in text files! Such innovation, much wow!
It means the code that you write with Xamarin, you own the copyright to, not Xamarin. Standard stuff in the industry. It gets grey when you work for a company, or are enrolled in an education institute.
$25 per month isn't expensive. 
They invented asp.net and they'll ignore web.config if they want to.
I'll point out that web.config is a pretty fancily formatted text file and this strategy of deploying connection strings isn't the dumbest idea ever from a devops standpoint -- you can keep that text file in a separate secured repo and not have developers seeing production info. Makes it relatively easy to deploy the info to multiple boxes. Etc, etc. But no that did not make me more comfortable.
Pretty much spot on. I've seen a few projects that do this (ex: nopCommerce)
Of course the asp.net way is to encrypt your connection strings in web.config using secure key containers. http://msdn.microsoft.com/en-us/library/ms178372(v=vs.100).aspx
I ported an App I had written native versions for Android and iOS over to Xamarin.Forms recently, apart from a few teething issues with .Forms as its alpha, all went smooth. The biggest downside I would say is, if you don't already know the in's and out's of Android/iOS in their native form, jumping straight into Xamarin could leave you awful confused over some basics.
*.config and App_Data/* have about the same amount of protection -- they are designated http 403 unreadable by other config files. I'd also prefer configsource over this method.
That's an interesting method and I'd never seen it before. What's the advantage to this over just using the app pool identity to control access to production databases like I've seen on every job site ever? Is it just for cases where you *have* to embed a username and password in the connection string because your database doesn't support integrated security?
(L)ive (E)nterprise (E)dition (T)...reatment? I got nothin'
Wow! Thank you lots! Sweet!
I'll trap the null, and throw an exception. Orrrr........ not call back? Maybe create a new function for those conditions?
That's what I wanted to get - the "callerObject", without passing "this" to the function. I can use callingObject = callBack.Target ! http://www.reddit.com/r/dotnet/comments/2sdgxg/addresof_i_have_a_function_address_can_i_get_the/cnoipi5 I want to use dispatcher to avoid the check, and make the callback always thread safe: Dispatcher.BeginInvoke(DispatcherPriority.Normal,( (Action) delegate {..............} ) That way the callback is always running on the thread that the rest of the class is running on - I think. I still need to read this: http://www.codeproject.com/Articles/28485/Beginners-Guide-to-Threading-in-NET-Part-of-n
How did you come to know this less well-known property?
Thanks! Someone posted about delegate.Target too!
I've never actually used it for anything, but I as part of [Platform.Invoke](http://www.nuget.org/packages/Platform.Invoke/) I had to create a class that generated delegate types in runtime. AddressOf in Visual Basic is actually a remnant from Visual Basic 5.0 and was used to get the address of a method (VB5 compiled to native code) to be passed to Win32 library calls. In .NET however it returns a delegate for that method, so not really the address.
Do you work at Xamarin? Edit: from your other replies, it seems like it. I might be wrong.
Yes. I normally mention it when I post in Xamarin threads on reddit but forgot to this time.
actually, it's whatever format you want it in. you can even use INI format http://whereslou.com/2014/05/23/asp-net-vnext-moving-parts-iconfiguration/
Would you be able to give me an idea of how much discount is available to students?
It is any object. You use it to pass information from the caller to the callback. It can be null. It is passed in to the callback on the AsyncState property of the IAsyncResult parameter. So you have your delegate that you want to call using this pattern... var stateInfo = new Whatever { P1 = "result gets this" }; delegate.BeginInvoke(..., MyCallback, stateInfo); ... void MyCallback(IAsyncResult result) { var state = (Whatever)result.AsyncState; var caller = (AsyncMethodCaller) result.AsyncDelegate; var delegateReturn = caller.EndInvoke(...); ... } Perhaps this doc is helpful: http://msdn.microsoft.com/en-us/library/system.runtime.remoting.messaging.asyncresult.asyncstate(v=vs.110).aspx IMO tasks greatly simplify this pattern. You can create a task from a begininvoke/endinvoke like this: Func&lt;...&gt; @delegate = ...; var beginInvoke = @delegate.BeginInvoke; Func&lt;IAsyncResult, ...&gt; endInvoke = @delegate.EndInvoke; var task = Task.Factory.FromAsync(beginInvoke, endInvoke, null); Or some other appropriate value for the state info if you need it.
Oh. Suuuuurrrre, just take the easy way out! ;)
Note that encryption is done on the production server, not the development files. The whole point is that once you encrypt, it will work only on the machine it was done (i.e. the production machine).
But you could put encrypted production values in the production web.config transform file though, and keep the development settings in the standard web config. They would just have to be reencrypted every time they changed. Can they be seen or modified via iis manager?
Actually you put the unencrypted values in the production web.config, run the command line to encrypt them on the production service and from then on they no longer can be read other than by the .NET configuration API. The point is to protect against users that have admin access or against an intruder that manages to get elevated rights and grab the web.config.
If you're not injecting repositories then I don't think you need dependency injection. DI isn't the same thing as lazy loading either. I use DI to inject the repository into my controllers in MVC and I use the service locator pattern (DependencyResolver) when I need another entity's repository in a controller that has a different repository injected into it. Lazy loading in EF is really just a virtual property in your entity which won't be populated from your linq queries unless you include them. This is to prevent too much data from being retrieved for an entity.
NativeScript from Telerik will be out very soon and it's free. Gives you native UI and (almost) native performance.
Same story here. No problems at all. Had a few a while back when using alternative sourve control plugins. But that is a long shot.
We actually use redgate comparedb. We take a snapshot of the db at each build. When updating the db's at the customers (150plus) deploymentsite. We generate a schema change script with it, which handles it very well, to update from any current build deployed, to the latest version. It's transactional and has never led to unwanted dataloss. Happy customer here
I've seen that approach used, it was slow, error prone and didn't handle non trivial scenarios at all.
What about the chances of them pulling production values off your TFS server though? I know, it's highly unlikely, but if I was going to encrypt something I'm going to have it encrypted everywhere I can. Many cases your want to protect production credentials from your average contractor as well. If I had production access at my job it would be very easy for my power bill to be zero every month.
try adobe's brackets-shell. the easiest way to get started is just to download the brackets code editor, which is a hybrid app built on the shell, http://brackets.io/ there is a www folder in the brackets directory, which you can remove and replace with your html and js source. when you next start the shell it will run whatever index.html file is there as an entrypoint. im assuming you will be building a spa and using ajax to get data from your web api 
Lazy loading is the opposite of loosely coupling your models. Now every line of code that directly or indirectly touches the model is strongly coupled to your database and the EF context used to access it. I've been down that road before and it leads to nothing but headaches on non trivial applications.
This made things work correctly, but in my ignorance I'm not sure why: if (thisValue.GetHashCode() != thatValue.GetHashCode()) { areEqual = false; //we've proven false at least once, therefore set to false Console.WriteLine(thisValue + "!=" + thatValue); }
So I think ionic needs node? Correct me if I'm wrong, if not then i can use ionic then? Also, what kind of transactions do you use the app for? I'm planning to use my mobile app mostly to update records on my record in sql server. I would expect it will have to update about 144 transactions in an hour. It wouldn't be bad right?
This looks cool so far; it's definitely something I've wanted to learn more about. I do have to note that some of the text styling is not readable for me (pre has a background color of #F7FAFB, code has a text color of #f8f8f2; just way too small of a difference there for legibility).
Boxing converts a value type into a reference type by wrapping it within an object. This is necessary for reasons outside the bounds of this reply. The important thing is that it's not 1 when it's an object, it's instead (sort of) a class containing the value 1 within it. Object equality comparison is done by checking if the two arguments have the same address (are the same reference), which they don't because they're two different objects that happen to wrap the same value.
A lot of people have been reporting that, but a refresh seems to fix it. Can you try that and let me know how it goes? Edit: Fixed it, cloudflare being a little overzealous. 
Looks much better, thanks!
&gt; The string equality check passes however, I believe because strings are interned but I could be wrong. No you are absolutely correct. Strings are interned in C# for performance reasons and threading issues. 
Out of curiosity, what do you mean when you say interned?
Yeah.
So the .NET framework treats strings as a special case object. If you reference the string "hello" in one place in the program, and then independently create another "hello" string, both objects point to the same exact place in memory. The chief benefit you get from doing this is that strings can be forced to be immutable everywhere. From a C programmers point of view this may be a bad thing, but if you came from a functional language background you love this feature, because immutable strings allow you to make a lot of assumptions about them which can optimize your program- especially in a multithreaded environment, because now you can simply assume that every string you ever want to read will not be changed on another thread and corrupt your data or cause a buffer overflow. The downside is that you need to be more careful concatenating strings. Using the plus operator, especially for large strings and in rapid succession, will slow your program to a halt. Which is why knowledge of StringBuilder is so essential.
If memory serves ternary operators only return a value rather than executing an action. Try &lt;% if (Container.DataItem.ContentString.Length &gt; 10) { Number += 2; } else { Number++; } %&gt; Edit : Worth noting that &lt;%# equates out to a databinding expression which is why you're seeing the expression being written. Similarly a &lt;%= would equate out to a response.write from the classic asp days.
Oh? I was under the impression that ionic is built on AngularJS. I'm not sure what you'd need node for. You just add the JS references and start using it. Does ionic need to compile something? Perhaps I need to do more homework on this.
It's worth mentioning that Visual Studio 2015 has built in support and tools for Apache Cordova. 
Ah yes, I've hit this bug before. It's a C# issue, you never see it in VB or Java because they don't overload == to be both value and reference equality.
I'm developing an app in Ionic. The TL;DR; I'm going to kill someone at Ionic, then someone at Google/Apple for shitty HTML5 support, then someone that doesn't know what localization is, then someone at Apple again for making overly complicated IOS deployment, and the list goes on an on. The major fuckups from Ionic: - The GUI looks like something from 1995 (come on...check what they did with radio button list, my customers don't understand what it is), they don't follow any GUI guidelines, at least any recent or NONE at all - documentation is not good - it won't run on IPHONE 6, it just crashes with no explanation whatsoever (it runs on emulator, it runs on IPHONE 5...) - but Android works, but it's still fugly Browser HTML5 support: - Apple/Google and whoever makes browsers. Look, I don't live in USA. We have different date and number formats, so untill input type="date" and input type="number" don't include my formats hybrid apps are USELESS! It's not "just use type='text' and javascript the hell out of it", because if you don't use date or number, then you get normal keyboard and not date chooser or numeric keyboard on the screen. - And if someone says to me that they use number and date format from system settings, I will kill that person too, because they don't use it. Yeah, they did use date settings on iphone, but not the number settings, and Android, oh man oh man, I can't even choose to switch / in date to . and use , instead of . in numbers. So anyone wants to die? Yeah, and the sollution. Go native or don't do it. I chose that this project is my last mobile app project, because it's just not worth it. You like need to give the app for free or chage 1 EUR for it or it won't be used at all. And my customer doesn't want to pay like 3000 EUR for it. So no, appstores/playstores and everything is so messed up/overcrowded it just doesn't pay up. My solution: I make mobile friendly web pages (it's like hybrid app in browser -- the lastest version of browser that is, not the webbrowser component!!) with Bootstrap or something similar, you go there if you need something, click whatever you like, do whetever you like, and we're golden. If you need, let's say phone camera or something, then like I said before GO NATIVE, everyting else is in beta or whetever, it just doesn't work (but the speed of an app is OK, I have no problem with that). 
Thanks for posting this, I had heard that vNext was massively simpler under the covers but the amount of boilerplate that the 'new web site' project generated was daunting (as it's always been). It's good to see everything come together piece by piece.
I'm looking for a method that can be used on different objects, with different properties. Also, if I add, remove, or rename properties to an object, I don't want to need to revisit Equals each time.
Strings are actually not interned automatically. Not even statically known strings are interned load-time by default for performance reasons. The actual reason is that the compiler combines the two identical string constants compile-time.
Note that it also says: &gt; The .NET Framework version 2.0 introduces the CompilationRelaxations.NoStringInterning enumeration member. The NoStringInterning member marks an assembly as not requiring string-literal interning. And the C# compiler applies this by default.
this.Equals(that), since that method is virtual so it always calls the correct one.
Yes, == is a bit finicky in C# for sure.
Technical debt aside, in situations like this you should consider trying to refector. (I know you said no, but I need to stress it, sorry. You're making a big deal about hidden HTML, but is that the real problem? No.) Chances are, down the line, someone will be dealing with the same code and it doesn't sound like it'll get any less complicated. And that person might be less familiar with the codebase than you. Then they might come to you, if you still work there, and you'll have to deal with this shit again. And refectoring data in a .cs to display on a .aspx is about as simple as it gets. Without knowing the code behind, I can only tell you what I've done after years of dealing with repeaters. Assuming you're binding a IEnumerable or List, do a Select before/when assigning to your datasource. Have it return an anonymous object with properties that hold your aggregated data and other properties with values you need displayed. Then Eval the property names in the markup.
I have read many articles on design patterns and I tend to agree with you, despite my inexperience. Thanks for the insight, I will look into implementing DI in the manner you described. Is there a particular DI framework you'd recommend? I realize it's mostly opinion, but I've heard that Simple Injector performs better than other frameworks, while others such as Unity and Castle Windsor offer more configuration (not that I'll need it).
Yeah, I am not concerned with that as.. YAGNI.
From my research, it seems like that's the case. Thanks for your input.
I see. I thought lazy loading would be a nice feature, but I didn't think about the pros and cons of implementing it. Would you recommend eager loading, or some other variant?
Appreciate this. As a new dev I'm in the "heavily reliant on Visual Studio" group, so it's good to see a step-by-step look at using the command line for ASP.NET projects. Looking forward to the rest of the series.
I hate this. Ran into it several times and every time I forget this caveat.
t didn't work. Th number stays at zero. That's why I'm using the other tags.
Ah men brother 
Hmm guess thought I read somewhere you could pull front end repositories with the task runner. Have to look into the configuration options. Ever use it for unit testing?
Thanks, I'll change my code!
Don't use it then.
You use it if you like static typing. Which gives you compile time checks, rich IDE experience etc.
&gt; Serious question, why would I use this? It seems to me this would make debugging problems harder in just about every way imaginable. How does it make debugging harder? You can debug the compiled javascript, or you can debug the typescript with source mapping enabled. TypeScript has its challenges, but debugging is not one of them.
Some people are really into static types. I am one of them.
can you specify in what ways it would make debugging harder? TypeScript supports source maps, there is no difference between debugging pure JavaScript code and this
I've been waiting for this since I've heard of Docker. 
No, one of the so-called "Micro-ORMs". nHibernate is just as bad as EF.
I did and it was just trolling - it added absolutely nothing to the debate on this thread.
Authorization is the next logical step. Once a user is assigned to a role, you can query those roles through something like a IAuthorizationFilter http://msdn.microsoft.com/en-us/library/system.web.mvc.iauthorizationfilter(v=vs.118).aspx to prevent access to a controller or action. This is what the AuthorizeAttribute does http://msdn.microsoft.com/en-us/library/system.web.mvc.authorizeattribute%28v=vs.118%29.aspx. If you want to query the user roles in the view, you can go through the view context and using HtmlHelper extension methods. A very simple example would take in a list of roles, and one Func&lt;HelperResult&gt; for the HTML to show when authorization succeeds and another to show when authorization fails. Basically you are saying: if user in list of roles, then show first HTML block else show second HTML block. This article shows use of the HelperResult class: http://haacked.com/archive/2011/02/27/templated-razor-delegates.aspx/
Oooh that last paragraph makes me very excited. Await async has made asynchronous programming in .net so much simpler. It would be a godsend if I could do the same in js instead of the spaghetti code thats created without them 
There have been articles out there for a while, specifically running an OWIN server. But vNext certainly makes it easier.
Native promises in ECMA6 will hopefully make building async development much more standardized. Each of the 3rd party libraries do it differently (thankfully most people stick with jquery) so willbe nice for everyone to be on the same page. You know when we can stop supporting old browsers.
That might be tough. The benefit from open source and shared repositories are you get lots of experienced eyes on it with the hopes that the end result is some well written, efficient, and robust code. There's nothing preventing you from forking any number of projects on github and fool around with them. Coming from Java, c# is going to be a piece of cake. I'd start out with finding a few good codeproject or MS articles on net 4.5 and the new stuff. 
I've started using it for side projects, the structure it provides has made me stop hating javascript.
I thought that Microsoft was working on extracting an interface as IDataContext and IDataSet
Humanizer isn't an asp.net open sieve project, but might be an easy open source project to jump. As for asp.net...there really isn't much out there that aren't doing to be of significant scale. 
I've got my own data access library. Nothing like anything currently available but I can't talk about it until legal lets me open source it. Previously I was more than happy to use a simple reflection based wrapper around DataReaders to populate objects. These are called "micro ORMs" these days (a term I coined) with Dapper being the most popular (and still faster than mine, but I'll catch up).
Someone p/invoked something wrong 
It is very very unlikely.
As mentioned, it's unlikely that Microsoft will do it. However, what you're looking for already exists. Check out [Mono's GTK#](http://www.mono-project.com/docs/gui/gtksharp/)!
Xamarian Forms I guess if you're looking for .NET solution (not that I'm endorsing it, but Charles Petzold moved there, so may be hope?) but I doubt if MS would come up with any 
Isn't that pretty much what a browser is now? An incredibly cross platform application platform?
No documentation explaining what it is, what it does, or how to use it?
Basic look through, it appears to be a mvc project with some wiring already coded. Imo it's just for personal use, since it's got hardcoded string captions.
Hm, WPF isn't really that great I think. Also, the usual old problem is getting the native look across platforms...
OP here, part #1 was discussed here: http://www.reddit.com/r/dotnet/comments/2sl5gv/aspnet_vnext_ground_up_1_simplest_possible_thing/ If anything is unclear or hasn't been explained well I'd appreciate the feedback. Part 3 in a couple more days.
I ran: @powershell -NoProfile -ExecutionPolicy unrestricted -Command "iex ((new-object net.webclient).DownloadString('https://raw.githubusercontent.com/aspnet/Home/master/kvminstall.ps1'))" from the command prompt and it returned this: Using temporary directory: C:\Users\Will3\AppData\Local\Temp\kvminstall Downloading KVM.ps1 to C:\Users\Will3\AppData\Local\Temp\kvminstall\kvm.ps1 Downloading KVM.cmd to C:\Users\Will3\AppData\Local\Temp\kvminstall\kvm.cmd Installing KVM Copying file C:\Users\Will3\.kre\bin\kvm.ps1 Copying file C:\Users\Will3\.kre\bin\kvm.cmd Adding C:\Users\Will3\.kre\bin to process PATH Adding C:\Users\Will3\.kre\bin to user PATH Adding C:\Program Files\KRE;%USERPROFILE%\.kre to process KRE_HOME Adding C:\Program Files\KRE;%USERPROFILE%\.kre to machine KRE_HOME I then ran this: kvm install latest -p And it returned this: 'kvm' is not recognized as an internal or external command, operable program or batch file. Did I do something wrong? 
Hrm, can you confirm that: C:\Users\Will3\.kre\bin exists with kvm in it? 
Figured it out. It's: @powershell kvm install latest -p Not: kvm install latest -p
Does anyone else miss the days when creating a new mvc project was something simple with a single controller and a single view? It seems they've gone in the direction of either a completely empty project or a project with every popular javascript web framework possible.
I'm going to be covering off simple ways to bootstrap all this stuff up as we get further into the series.
Web browsers have little to no filesystem or device access on the client and the APIs provided are usually primitive compared to the native libraries (WebGL vs OpenGL 4.3 for example).
It could if you choose to do so.
You could use nlog, http://nlog-project.org/ Also, have a look at System.Diagnostics.Trace 
WPF has a great design but I think if wasn't properly supported. I agree with the native look but that's something some projects can live with.
I recommend NLog
Assuming you're using ASP.NET, and without seeing your code, try something like this: ``` return RedirectToAction("Posts", new {idOfCollection= idOfCollection}); ```
My understanding is that we'll get the best of both worlds eventually. Start with an empty project, run a single grunt command to get all this and more setup. Run another grunt command if you want some JS library with dependencies and so on.
I managed to find a discussion about this on [StackOverflow](http://stackoverflow.com/questions/3715981/whats-the-best-restful-method-to-return-total-number-of-items-in-an-object) I actually thought about doing custom headers, but the front-end guys thought it wasn't the best idea. It seems like there are two schools, custom headers or responseWrappers. I'd much prefer to do custom headers, but I fear that by having several layers, I'd need a responseWrapper anyway to pass this information from the DAL down to the BLL and then service layer. If I need a responseWrapper for them, I'm thinking I may as well use it in the service layer too. I just feel dirty.
There is an oasis standard for data transmission you can use for reference. I don't agree with everything 100%, but they thought through it enough to make a standard: http://www.odata.org/ For the layering problem, if you need to pass metadata between the layers you can either make it part of the contract or hide it. I ended up deriving a interface from IEnumerable&lt;T&gt; called ITotalCountEnumerable&lt;T&gt; and put it in my common cross cutting assembly. I implemented it with a derived class from List&lt;T&gt; that has a TotalCount property from the ITotalCountEnumerable&lt;T&gt;. That way, its still a list, but it has the extra metadata I need to pass between layers. This was the least dirty representation I could think of because I felt like I hid the metadata without changing the fact that it was a list.
Is the only reason you require a total number of records to indicate to the client that additional pages of data exist? We solved this problem in a bit of a more general way. First and foremost, our presentation layers tended to break pagination down into a few simple ways: First, Next, Previous, Last. Our API includes self links to any listing as well as any individual resource within the response payload, and it includes links for paging in listings that represent the above sets of records. A client doesn't construct the next GET request on the listing - they simply follow the 'next' link. So how does that link get there? It COULD be accomplished by a response wrapper, but we chose to essentially support pagination through hypermedia, as a separate API-specific feature. We avoid creating custom, boiler-plate response wrappers as well as polluting our domain model with unnecessary properties via an approach called content enrichment. During response creation, the API essentially 'injects' the response with whatever additional information we require. There's a good summary of it [here](http://benfoster.io/blog/generating-hypermedia-links-in-aspnet-web-api). Finally, the API itself supports robust filtering, sorting, and aggregation capabilities to help narrow down large result sets into something more palatable for a consumer. 
In this scenario isn't adding 'hasMore' basically the same as adding 'totalCount'? Each would require adding metadata to current response via wrapper or header.
I think the closest thing would be something like [Debug.WriteLine](http://msdn.microsoft.com/en-us/library/9z9k5ydz.aspx). Using this you can write to the Output window in Visual Studio while you debug your application. In production you could use [the EventLog class](http://msdn.microsoft.com/en-us/library/system.diagnostics.eventlog.aspx) to write to the Windows Event Log/Event Viewer. As mentioned in some of the other comments, there are also plenty of logging frameworks/libraries out there that you can use in production for more advanced scenarios.
Thanks. The hosting service does support 4.5.1 and I was able to run a 4.5.1 application on the host so long as I used the empty web app template when creating it. I marked the folder as an application in IIS. In all my Google searches, the only suggestion I found about the error that I ran into was that something might have been causing a potential request loop so the browser terminated the connection. I just don't know what it would have been. 
It's actually not a .NET exception but a browser error that says the page is not available with a error code of ERR_CONNECTION_RESET. It could be a request loop of some kind, I just don't know what.
In my opinion, Use x header for total results, use Link rel headers for providing next and previous page URLs.
The connection is reset by the server. It is not a browser error. There will be an exception in the event log on the server which says why the connection was reset. I bet your login page is set to require authentication. This would cause an infinite redirect loop as redirections to the login page cause yet more redirections to the login page. 
but ERR_CONNECTION_RESET should throw an exception in the server, IIS is the one resting the connection not Chrome. the most common scenario for ERR_CONNECTION_RESET is uploading large files or a timeout (not the case). the **Server.GetLastError** or IIS logs will give us more info. 
I've never done it myself, but I'm told that VB apps don't work out of the box. You have to install some other dependency or something.
How much are you willing to pay?
You pay per hour? Nah, can't recommend that.
This is correct. Hourly for the first 4 months, salary upon conversion. The salary would be 85 - 90k.
Looking for someone that has experience with VB, ASP and C# for a company that develops software for the mortgage industry. 
Well I know VB.NET, ASP.NET, C#, and have experience in finance. Unfortunately I'm not mid-level though.
Cool thanks for the reply anyways. You know anyone by chance? 
Oh no, I'm actually in Michigan but am looking to move to Texas eventually. 
Especially since its still in CTP, you have at least a quarter or two after it goes beta/RTM
Shooting yourself in the foot is one of the best ways to learn to not shoot yourself in the foot. Also, it's impossible for the VS guys to predict every mistake a newbie can make.
Thanks! This gives me something to research. This app is as default as you can get. It started as an C# MVC Application using EF6. The Controllers and Views were generated using the default MVC/EF scaffolding. The DbContext was generated the first time I generated a Controller. I was able to generate a string I can use to identify what sundomain they are trying to get to (using HttpContext.Current.Request.Url.Host) I'll look at the DbContextFactory option next. 
You think around April 29 - May 1? If they don't release it fully I would expect a bunch of demos and at least a Beta available.
Me too. I think it's a good move made by MSFT and will increase adoption rates.
Are you using SQL Server as the database? If so, you can create a synonym in database1 for the table in database2. When you create the DbContext for database1 you'll be able to access database2 via the synonym name, which will work like any other table/entity. If database2 is on a different physical system, you may also need to create a linked server between the databases for the synonym to work. Either way, you get access to two databases via one DbContext. 
You can post feature requests to the Dev team and allow members of the community to vote on them here: https://visualstudio.uservoice.com/forums/121579-visual-studio
Yes it's certainly possible! We have a live system in production that does exactly that, switches DB based on the subdomain. Cut down and edited a few of the relevant portions, might not be complete, should give you an idea of the approach. You'll find you'll need to implement a few moving parts to get migrations to work... Dependency Injection makes life super easy... (Using structuremap in this example) public class CurrentDatabaseConnectionSettings { public CurrentDatabaseConnectionSettings() { } public string ConnectionDomain { get; set; } public string ConnectionString { get; set; } } public class CurrentDatabaseConnectionSettingsProvider:CurrentDatabaseConnectionSettings { public CurrentDatabaseConnectionSettingsProvider() { GetConnectionSettingsForCurrentContext(); } private void GetConnectionSettingsForCurrentContext() { var productionMode = bool.Parse(ConfigurationManager.AppSettings["Database.ProductionMode"] ?? "false"); if (!productionMode) { this.ConnectionString = ConfigurationManager.ConnectionStrings["DefaultConnection"].ConnectionString; this.ConnectionDomain = ConfigurationManager.AppSettings["CurrentDomain"]; } else { //get the current domain var hostName = HttpContext.Current.Request.Url.DnsSafeHost; //make a connection string SqlConnectionStringBuilder builder = new SqlConnectionStringBuilder(ConfigurationManager.ConnectionStrings["ProductionConnection"].ConnectionString); builder["Database"] = "production__" + hostName.Replace(".", "_"); ; this.ConnectionDomain = "https://"+hostName+"/"; this.ConnectionString = builder.ConnectionString; } } } public class MyEntities: DbContext { public MyEntities(CurrentDatabaseConnectionSettings connectionString) : base(connectionString.ConnectionString) { Database.SetInitializer(new MigrateDatabaseToLatestVersion&lt;MyEntities, Migrations.Configuration&gt;()); } } public classMyEntitiesFactory: IDbContextFactory&lt;MyEntities&gt; { public MyEntities Create() { //needed to get migrations and migrate.exe to work.... var settings = ObjectFactory.GetInstance&lt;CurrentDatabaseConnectionSettings&gt;(); if (String.IsNullOrEmpty(settings.ConnectionString) &amp;&amp; (bool.Parse(ConfigurationManager.AppSettings["Database.ProductionMode"]??"false")==false)) { if (ConfigurationManager.ConnectionStrings["...MyEntities"] != null) { settings.ConnectionString = ConfigurationManager.ConnectionStrings["...MyEntities"].ConnectionString; } else if (ConfigurationManager.ConnectionStrings["DefaultConnection"] != null) { settings.ConnectionString =ConfigurationManager.ConnectionStrings["DefaultConnection"].ConnectionString; } else { settings.ConnectionString = "somedefault" } } return new MyEntities(settings); } } /*subset of the structuremap configuration*/ public static class IoC { public static IContainer Initialize() { ObjectFactory.Initialize(x =&gt; { ...... x.For&lt;CurrentDatabaseConnectionSettings&gt;().Use&lt;CurrentDatabaseConnectionSettingsProvider&gt;(); x.For&lt;MyEntities&gt;().HybridHttpOrThreadLocalScoped().Use&lt;MyEntities&gt;(); ..... } } } /* To use DbContext somewhere */ public class DoSomethingService:IDoSomethingService { private MyEntities _db; public DoSomethingService(MyEntities db) { this. _db = db; } public void DoSomething() { _db.Items.Where() } } /*inside global.asax.cs*/ protected void Application_EndRequest(object sender, EventArgs e) { ObjectFactory.ReleaseAndDisposeAllHttpScopedObjects(); } That's basically it, the lifespan of the DbContext is managed by StructureMap, and initialization of the correct connection string happens automatically. Easy. Clean. Works. 
The VS IDE is a general multi-platform development tool. As a rule it will not hide or disable the use of anything in the file-system under the solution/project as Microsoft cannot predict what the end developer may want/need. In earlier versions the IDE used to try to be more "helpful" (hiding things, generating code automatically without telling you, etc) and it actually became less useful as a result.
There are a number of great answers here already, just wanted to emphasize a couple of things. Make sure to hide that database selection code in a single place and call that from everywhere else. That's the point of the dependency injection IDBContextFactory approach. Your actual data access code should have no knowledge that it could be dealing with anything but a single database. It just calls IDBContextFactory.GetContext() and goes from there. Also, being new to EF, make sure to enclose that call in a *using* block to make sure that you're cleaning up after yourself. So the basic pattern of any of your data access calls looks something like this: using (var context = dbContextFactory.GetContext(url)) { // do stuff with context context.SaveChanges(); // if doing insert/update } That will save you countless headaches cleaning up connection leaks later. Just make sure that you don't have any queries that have delayed execution, make sure to do a .ToList() or something similar to force your queries to execute, otherwise when you try to access them they'll attempt to use the disposed context.
You might like to work with a database guy too. Its possible to create a lookup table and partition the database so that each division is in a different server and/or schema, and create rights for the users per that accomplish all of your stated requirements with no custom ef code. Not that it's not an interesting use case, but most of the code for that is built into sql server itself. This will save your butt for reporting too later on also.
Most git clients will auto create an .ignore file that excludes these.
we are not google neither are we psychic
http://lmgtfy.com/?q=.net+wikipedia
Sorry but I couldn't resist. I've been waiting to use that on somebody for a while. Try having a read through this when you have time. It's a little simpler and easier to understand. http://www.codeproject.com/Articles/20694/Net-Framework 
Think of .net as toolbox. In there you can find all sorts of tools for helping you achieving things quicker. For example image processing, working with strings, encryptions and lots more. asp.net is a framework(toolbox) for making it easier for you to serve content on the web 
That clears it up pretty well, 1 final question do I need to know more about .net to be a c# developer or should I just trust that it does what it does?
.NET is a platform. This platform has the official code libraries (.NET Framework) as well as official languages (C#, Visual Basic .NET, F#, C++/CLI).
You'll pick up knowledge as you go. I wouldn't worry too much about learning the guts of .net, just keep on learning to code and you'll get it via osmosis.
.NET is to C# what the JRE is to Java. The .NET Frameword is JDK F# is like Scala...
I personally see nothing wrong with response wrapper. The client asked for a "page of items from a collection", not for a "collection", therefor response including totalCount or other metadata feels ok to me as it's in line with what client asked for. Opinions about this differ greatly though.
-3 now...because he is correct. .Net is a stern reminder to software marketing groups, this is EXACTLY what not to do. Do not get too excited by things like the ".com boom", make your product name a bit more memorable than that. Because of this we have the wonderful joy of VB.Net and C#. Not C#.net, just C#, and a battalion of other inconsistent names (should this have a '.net' on it or not? Yes/No, no one has a freaking clue.)
Using [Response.Redirect](https://msdn.microsoft.com/en-us/library/a8wa7sdt\(v=vs.110\).aspx?cs-save-lang=1&amp;cs-lang=vb#code-snippet-2) like this: try { smtp.Send(mail); Response.Redirect("/Thank-You/") } catch { @*do nothing mang*@ }
There was a post recently by a Microsoft employee who said - unofficially of course - that we could expect it in the first half of 2015. I'll see if I can find it...somewhere on hackernews or stackoverflow or maybe /r/programming. EDIT: Found it, it was a post on SO asking about the vNext release date: http://stackoverflow.com/a/27560914
Oh I see, he's probably referring to deploying the project to a Web server using Web deploy or something 
Oh no, it was only VB.NET for versions 7 and 7.1. Now we're back to just VB. Same for Visual Studio.NET.
The JDK equivalent is called the "Base Class Library".
I'll try to give an honest, complete answer. .Net is many things. Collectively, it refers to the entire platform - the runtime, the compilers, the class library, the machine, the machine architecture, etc. Lets start at the bottom. Normally (but not necessarily), languages like C++ are immediately compiled to a specific machine and platform, for instance, to x86 machine code with the calling conventions that are used on Windows. What's a calling convention? Well, there's no concept of functions in assembly. Just memory, registers, and opcodes. So if you want to establish a function-looking thing, you have to tell people how they should do things. Maybe I have a function-looking thing that expects parameters to be 'stored on the stack' (the memory that the current stack pointer points to contains the values for the parameters). Maybe parameters are stored in registers - before jumping to the code that makes up the function, I have to stuff all of the parameters in specific registers. Some architectures have many registers (ARM), some architectures have few registers (x86); so calling conventions might be different depending on the hardware. In reality, different operating systems on the same hardware might chose to do things differently, depending on how the individual engineers of those operating systems decided to weight the pros and cons of each approach for that hardware and for the goals of their operating systems. This is why we say it depends on the combination of hardware architecture (x86, x86-64, ARM, MIPS, SPARC..) and the operating system (Windows, Linux, FreeBSD..). Okay, so that's a little diversion into machine architectures. So what happens in .Net? Well, one part of .Net is a made-up hardware architecture ("virtual machine"). The designers of .Net created a made-up CPU, with made up instructions, with made up behavior, etc. Maybe you're familiar with x86 assembly: to add 2 + 5, you'd do something like: mov $eax, 2 mov $ebx, 5 add eax, ebx; // result is stored in eax. Well, here's a curveball. The .Net machine isn't a register based CPU - it's a stack based CPU. It has one big stack *in* the CPU, and you perform computations by pushing and popping things to/from that stack. For instance, it might do something a bit like: ; // stack starts empty push 2; // stack now contains [2] push 5; // stack now contains [2, 5] add; // pops the top two things off the stack, adds them, and pushes the result on the stack // the stack now contains [7] Want to add 5 things in a row? Chain them: push 2; push 3; push 4; push 5; push 6; add; // pop 6, 5; push 11 add; // pop 11, 4, push 15 add; // pop 15, 3, push 18 add; // pop 18, 2, push 19 // the top of the stack now contains 19, our answer. Why did the designers of the .Net machine decide to do it this way? Because it's easier to *reason* about the behavior of such a CPU - which means it's easier to build tools that perform static analysis on compiled code and figure out, for instance, if every path through the code ensures that some variable will be initialized before its used (which Visual Studio does, today, as you type!). Not only that, but stack based CPUs are also pretty cool in their own right - they have much better operand density (fewer number of bytes to represent the same operation), and the lend themselves much better to optimization. But remember, this is just a made up machine. There isn't a piece of silicon anywhere that actually executes this. This is a *model* of a machine that's useful as an *intermediate* between some high level language (C#, F#, VB.Net, etc) and some low level platform (x86/windows, x86/linux, arm/windows arm/linux, etc). So there's one part of 'what is .Net': .Net, in part, is a description of a machine and it's behavior. Especially, it's a description of that machine's instruction set, which in this case is called *Common Intermediate Language*, 'CIL', also sometimes just 'IL'. CIL is the assembly language of .Net. We call the .Net machine a virtual machine, because it's not meant to be a description of a real CPU, it's meant to be a description of a made-up CPU that's useful as a platform-independent intermediate. Since every .Net language compiles down to CIL, they can all interoperate. And now I can have one library in C#, another in F#, a third in VB, etc, and they can all interoperate because they *compile down to the same instruction set and the same calling convention*.
Ok, so what's next? Well in order to make this CIL useful, I have to actually run it on a real CPU, and I have to do so efficiently. Technically, I could write software that parses the bytes of a program, figures out what the bytes mean, and has code to do what those bytes mean - my program would be something like "read 4 bytes and figure out what instruction it is; oh it's a push instruction, ok, let me read the next 4 bytes to see what it wants to push, and I'll put that on top of the stack". This is an interpretter, and it's slow. But it's easy to do - you do it every time you write code to read in configuration files. Instead, we prefer to have the CIL be directly compiled down to the native CPU's instruction set - transform all of those `push` and `add` instructions into real x86 `mov` and `add` instructions, and then let the CPU run that directly. This is most commonly achieved using a Just-In-Time (JIT) compiler - as the program is loaded, the parts of the program that are called are compiled to x86 machine code when they're first called. So that's another part of ".Net": a Just-in-time compiler that understand how to convert CIL to the native CPU. On windows on x86, we use a JIT that writes out x86 machine code that uses the calling convention used by Windows. But there's more to executing CIL than just converting the CIL to x86 and letting that run, there's stuff that .Net does that happens behind the scenes, almost like an operating system. C# and most other (all?) .Net languages use garbage collection, ie, the 'behind the scenes stuff' is constantly watching over the code as it executes, and every once it in a while it pauses execution, looks at what objects have been allocated, looks at what is still reachable by tracing the connection from each object to the next, and finding out what is no longer reachable. Then it unpauses the execution, and frees the memory of the unreachable objects. Well.. what software exactly does that? That's the .Net runtime. So we've just identified another component - the runtime, the operating system like part, that is itself code running on the computer, that watches *your* code as it runs on the computer. In other contexts, these are called hypervisors (like in VMWare), but the concept is basically the same. 
-2 because now you also have .NET Native (which is a really cool engineering concept but only really benefits ~~Metro~~ Windows Store Apps that have a poor startup time) which I think if I remember rightly builds down to native machine code so doesn't run in the .NET runtime...
Yes, MS SQL Server. I have an SQL Admin that I may ask about this. I appreciate that you posted this option. 
Thanks for the tips! I've run into a few EF gotchas already, I will make sure I avoid the situations you described. (and make sure I am using "using" correctly) 
Will do. The SQL Admin is also the Reporting person, so this might be a good option for them. Thanks!
Sort of high-level, but hopefully this helps. I apologize in advance if I have a few things incorrect to a small degree. Please feel free to comment and correct me. .NET is both an umbrella term (referring to the frameworks, tools, and supporting software) as well as a framework (specifically the .NET Framework). As an umbrella term, it consists of * The CLR (Common Language Runtime) which supports the execution of any .NET assembly * The frameworks such as WinForms, .NET Base Class Library (BCL), WPF, ASP.NET, MVC, and so on * Extension frameworks such as LINQ, ADO.NET, Parallel/Task, and so on. * Third-Party Libraries (Telerik for example) * Custom code (that's the stuff you write!) As a framework, you're most likely talking about the BCL and one of the first-party frameworks. This would include namespaces like System, System.Data and more. A framework is just a bunch of code that has common uses. For example, there is a Math class that incorporates common basic mathematic principles. Similarly, there are complex frameworks like System.Data that provides access to common database types. As a developer, you write .NET compatible code (code in a .NET supported language such as C#, MC++, VB.NET, et al) and then compile it into IL using a compiler (msbuild, which is inside Visual Studio). Once compiled, you'll have a DLL or an EXE; these are known as two types of an "assembly." Both are essentially the same, except the EXE has the ability to self-execute and boot up the .NET interpreter which compiles the IL into machine code "just in time" (JIT) for execution. From there, the OS takes over. The closest comparison to how this works would be Java, except it supports several languages out of the box. 
For all intents and purposes, if you are a C# developer, you _are_ a .NET developer. Even if you're on Linux and not using any Microsoft libraries.
Simples answer is to just do "File &gt; New Project" in Visual Studio and select one of the templates. You can do both Javascript and WPF/XAML based apps.
Thanks but there a bajilion project templates. I'm not really interested in creating a F# or Silverlight app/project. 
np. Thanks for the help! :)
Hey man, really appreciate the detail in your tutorials. I happened upon the (forgive me if this isn't exactly right) Microsoft.Owin.Testing package the other day and it got me excited as you can component test your controllers without having to host your web api project and send anything over the network. Kinda cool if you haven't tried it yet. 
I see, ta!
Note: OWIN is not gonna be used in asp.net 5 - but the concepts are adapted.
Uhm... What?
Are you sure about that? My understanding is that Katana, an implementation of the OWIN spec, is the base they use to remove dependency on IIS. Also this article talks about how they use OWIN http://whereslou.com/2014/06/10/asp-net-vnext-moving-parts-owin/ Could you give some links to where they say they won't use OWIN/Katana?
Jabbr Channel, David Fowler mentioned it a couple of times.
I get around this by using an ajax call to get a partial page from the controller. Stick the response on the page in JavaScript and only update by issuing the ajax call when you need to. Using a render will rerun your controller action every time the host page updates.
Maybe check out Apache Cordova? Not many people are using that, and you could create tons of mobile applications. Apps are always social right?
Thanks! .100 bits /u/changetip
You shouldn't be calling your DbContext object in the View - your Controllers should be grabbing data from the Models and passing those objects to the View.
WebAPI has already been using it and that is a big part of ASP.NET 5. So, unless they removed the OWIN support from WebAPI, I doubt this is true.
I think Partial Views is what you are looking for. Your question is a bit confusing..
Yup. I tried to make that point in the post. 
It was quick. However the time and date have been available for a while. We had our budget clearance and card prep'd. I had to refresh for about 30 minutes on 5 computers to get a stable connection, didn't expect it to be that bad.
I don't understand anything in that but either way it looks really cool and exciting!
&gt; Does ASP.NET MVC have a place as part of the stack when using something like Angular? Yes. You build your server-side in MVC, probably using WebAPI, and then connect your SPA client to it using whatever glue framework you like.
Well, generally, your .cshtml file will contain the static page elements that your Angular view depends on. It will rarely be dynamic (just HTML in your .cshtml)- but it can be. The advantage of doing some of your dynamic programming on the server side is that you can pregenerate certain page elements, thus avoiding re-layouts.
I was studying concurrency and starting out on .net, then i searched for actor model .net, found out about the project (which i had seen with scala) and thought "ohh thank you!! someone has done this on .net!!! \o/" Great work! (i hope to get good enough to help the project one day, still learning)
You shouldn't be calling page-specific functions from within your master template anyway. You should probably make the part of the page that needs that function part of the individual page rather than the template, OR make said function a global function that can be called from the master page.
Sold out in a few hours last year too, and that was without all of the hype from yesterday. One reason it sells out so quickly, is that they give out killer freebies. Last year we got an Xbox One and a $500 Microsoft Store gift card. I'm putting money on either them giving everyone Hololens or at least a huge discount on it when it is ready for release. I wish I was going this year, but I'm heading to Ignite instead the following week in Chicago - I'm looking forward to seeing how they combine all of my favorite technologies under one roof.
I've found that starting with MVC 5 was much easier for me - because I've done some web development in the past a lot of it just makes sense. Also, [this 3D Buzz video course is worth it's weight in gold](http://www.3dbuzz.com/training/view/comprehensive-aspnet-mvc/aspnet-mvc-basics).
I've used Pluralsight and have a paid annual membership, and it is great for many other technologies - but I've found their ASP.NET training (at least the beginner tracks) a let down. The trainers and editing they have for them just aren't their strongest. I'm hoping some of the intermediate and advanced ASP.NET/MVC courses are better (which there are several). I started with Microsoft Virtual Academy's [C# Fundamentals for Absolute Beginners course](http://www.microsoftvirtualacademy.com/training-courses/c-fundamentals-for-absolute-beginners) (which is free) and learned a bit - but was still struggling (I come from the systems engineering side, not development). However, I stumbled on the most amazing ASP.NET MVC course at 3D Buzz of all places and now it just "clicks." The course is [Comprehensive ASP.NET MVC](http://www.3dbuzz.com/training/view/comprehensive-aspnet-mvc/aspnet-mvc-basics) and is only $29 - and I'd pay 10x that without question for it, it is that good. I think the fact that a lot of the code is relational to HTML helped me ease into it more rather than diving straight into C#, and the guy leading the lessons is really great too.
Yeah, I love PluralSight - learned some great SharePoint stuff on there the past few months, but was disappointed in this course - http://www.pluralsight.com/courses/one-aspdotnet-from-scratch The editing wasn't great, it jumped around and was hard to follow at times. And the guy wasn't the best speaker on there, that's for sure. [Their C# Fundamentals course was okay](http://www.pluralsight.com/courses/csharp-fundamentals-csharp5), but I liked the Microsoft Virtual Academy more (which is free) The guy from LearnVisualStudio.net is the guy teaching the C# fundamentals course I mention above. His name is Paul Tabor and he has been hired by Microsoft to teach courses on Microsoft Virtual Academy and Channel 9 now (at last 6-7 at this point). He doesn't update his personal site much anymore because of it (I was a member of it in years past). 
Interesting... I know he (Paul) hadn't updated his site for several months but it seems like there's been more updates lately. At least it references 2015 on the home page, so that's something. :-) He's does a really good job with the videos. I occasionally have to do training videos and I really admire the people who do them well. It's a ton of work to get them right. (Mine suck, but they're for a very limited audience using custom software)
Sorry, I misspoke - his name is [Bob Tabor](https://twitter.com/bobtabor), not Paul (I blame coding at 3:00AM). It has been a few years since I was a member, so he could very well be updating it again. Reach out to him on Twitter if you want to know, he responds regularly. Here's his Channel 9 profile, where you can see the tons of work he did for Microsoft (this doesn't even include all of his MS Virtual Academy videos) - http://channel9.msdn.com/Niners/Bob_Tabor
Well they merged MVC and WebAPI, so it is called MVC, but you don't have to return a view, but could just return data, like with WebAPI. I still believe that they are using a OWIN implementation in ASP.NET 6 yeah it also says so in the article: &gt; ASP.NET 5 is indeed moving further in this direction. Katana itself will apparently be fully integrated into ASP.NET 5. OWIN will be available through an interop, but greenfield projects will be best off using the integrated middleware pipeline. However, most of what we discuss here will still apply, either directly, or conceptually (thanks to Rick Anderson and the ASP.NET team for the clarification!). 
Pluralsight is a great resource, as mentioned elsewhere. I really like the Microsoft virtual academy, sorry no link, phone. There is nothing like learning a product/toolset/framework from the team that writes the code for it. 
Note that MS haven't yet released (their version) of .Net Core, but as it's a subset of the full .net the mono runtime works in it's stead. It's not quite plug and play yet though.
No they did not. They "unified" the code base, but its all there.
Woah, this looks super cool!
Thanks! I am currently researching DI. 
We were kicking ourselves for not setting up some azure vms to access it from but we made it in the end.
I was referring to the official support (.NET Core), where I am correct. I should have specified that.
Isn't WCF legacy now? Anyhow run screaming.
I'm fairly new to MVC, but from what I understand is that you should combine these models into a single View Model and pass that. Someone please chime in if I am incorrect.
you are correct. op's model is directly an entity, which is a bad practice (even if you will see that in all the crappy tutorials online). the solution is to create a viewmodel class which suits every single need of the view, and use this viewmodel class as the model of the view.
Thank you for the expanded solution. Would you mind further explaining why giving a direct link is bad practice? Can a direct link be affected by malicious sql injection or something of that sort?
not really. but a direct link will only suits your need in very simple cases, and you will quickly understand that it's never that simple. Even a simple dropdownlist justify to use a viewmodel. You must never use viewbag / viewdata. the more precise is your viewmodel, the easier the view will be to code. Don't hesitate to put in the viewmodel, exhaustively, every piece of dynamic information your view relies on.
That makes sense. Thanks!
if i can suggest a book : Professional ASP.NET MVC 5 by scott hanselmann and his friends who actually designed asp.net mvc. The book is a very short read, 4-5 hours, but believe me it worth it as you will save DAYS. It's very relevant, you can understand how and why they did the choices they did with the technology, and it's not uselessly complicated. 
You'll want to make a ViewModel like so: So say you had two models, one for a login, and one for registration, and you wanted to put both of those one the same view, so you could create a nice fluent form without changing pages. public class LoginModel { public string Username {get; set;} public string Password {get; set;} } public class RegisrationModel { public string FirstName {get; set;} public string LastName {get; set;} public string UserName {get; set;} public string Password {get; set;} } Now if you were to try and get these models in one view, you'd get an error thrown your way like you are now. So you would want to create a ViewModel. A ViewModel is nothing but a single class that may have multiple models. It contains multiple models as a property. It should not contain any method. So your ViewModel to keep both of those models on one page would be: public class LoginRegisterViewModel { public string LoginUsername { get; set; } public string LoginPassword { get; set; } public string RegisterUsername { get; set; } public string RegisterPassword { get; set; } public string RegisterFirstName { get; set; } public string RegisterLastName { get; set; } } Now you'd be able to call that ViewModel on your view and submit all that data. Hope this helps!
You mean I should create another class that have two variables (IEnumerable and IPagedList)? Is it the best way to solve that? I mean: as I surmise, each of them stores the same data: list of items in database. Does it mean I download data from database two times? And why using entity as model is bad idea? Could you provide some examples?
Just ordered it from Amazon. Thanks!
I think if you delete the artifacts folder it gets better for a while. Personally, I have given up on the preview versions. I thought it was cool in June that they just grabbed a nightly and published it for the sake of sharing. But at this point I really wished they would do a bit more QA so that one could actually get something semi-serious done.
You should create another object that takes the values you need from each object and create a singular object out of that. This helps in the fact that you can often times make View Models lighter than the original model that you are passing around.
No WCF is not legacy, it is the big daddy of communication frameworks for .NET. Every other comms framework in the .NET world exists simply because WCF can be a bear to work with for the mere mortal. In an interesting twist, many popular frameworks actually implement WCF under the covers and are simply abstraction layers for people who just want the 3 most popular WCF features. WCF is a bear mainly because it does so much and it is responsible in a lot of ways for helping to bring us out of COM hell when it comes to integration. If you are doing a simple website with some JSON queries and a RESTful interface WCF may be too much. But if you need something flexible, scalable and when implemented correctly, future proof, WCF is the only way to go on .NET. A .NET developer that does not take the time to understand WCF is leaving a lot of potential work (as in jobs that pay) and time saving tech on the floor. 
Good points. I feel that models are neglected and people try to put it in either the controller or the view. The word controller does sound important but it should contain the least code.
As always : Pluralsight.com. Honestly, WCF is damn easy. The only problem is getting the bloody web.config right.
OP here, continuing on this series, here's the 3rd installment. Any questions fire away.
You need to add reference to mscorlib.
I'll have a look for that then. I know the references are there as this code is straight from MS's GitHub. Just to mention, I ran kvm upgrade and that sorted out some of my own sandbox projects I was working on but not this or any other of the aspnet repository solutions. Edit: Just to add, right this minute its 2.30am and I'm lying here trying to sleep, I'll try any suggestions in the morning proper
This is github code from the aspnet repository. All my own projects have been Starter Web projects
Not the kind of flexibility or scalability you think. I am talking doing things like communicating with custom hardware using a completely custom binary format and its own serialization. It may include encryption (custom encryption algos provided by a 3rd party, yup) or other fun crap too. It might nor even use a common or public network protocol (old mainframes run a variety of ways)/ You want the contracts and behavior of this to be just like your front end, with nice domain models and is served up via XML, JSON and a custom TCP protocol your industry developed 15 years ago. You want things to match because maintaining a custom protocol stack is a real bitch. the nice thing is WCF can do all these things, at the same time. Further it can communicate in and across app-domain boundaries in memory at EXTREMELY high rates and then be broken apart into SOAP/XML, JSON or that crazy binary protocol with a simple change to the web service binding and behavior definition found in the application config. Finally in the next couple years when the next hot protocol and format comes out that all the kids are excited about. Rather than sit on your hands and wait for someone to make a framework you will go to your existing application comms super-framework (WCF) write the necessary class implementations based on the spec, drop a couple simple configs and all of a sudden your entire WCF compliant service stack is available on the new hot thing. All the kids think you are "with it", you laugh and wonder when they will really learn how application integration works. These are things the likes of WebApi (or pick your beast) to one degree or another (WebOrb) cannot handle fully. Most I have seen leverage the WCF API under the covers and simply abstract the more complex configs for people who want something more turnkey. Not everyone needs the kind of things WCF does. WebApi and the likes are great for a lot of work, I use it myself but knowing WCF and when/how to use it is critical. 
(Not op), thanks for explaining to him and others, too many people think that wcf is being replaced, but in reality it provides the abstract layer needed for enterprise to use new tech.
Because its possible!
I figure a big point is for them to get feedback as well. But if it's too unstable you're not gonna have the patience to sit long enough to form an opinion about the features or discover bugs etc. 
Maybe it would be better for you to just wait then.
Does it build with msbuild?
No the assumption is that you have SQLServer LocalDB installed. That's the standard "development" database which is available with any of the Express SKU's. If you attempt to deploy such a project you'll just get a connection error. I should probably point out that localdb needs to be installed though...
I don't think that you need to point that out, I'm just a newcomer to the webstack :-) I want to start with ASP in my free time (not at work) and thought I'll just wait until vnext is coming around. So I'm following your series to kinda learn ASP.NET and MVC (6) ;-)
Not linq but possibly useful to you https://msdn.microsoft.com/en-us/library/vstudio/bb908822(v=vs.90).aspx 
select a from alpha join b from bravo on a.id equals b.alphaid into temp select b from temp.DefaultIfEmpty() where b == null select a Sorry for formatting, on mobile.
Please please provide a link. I'd really appreciate it. I'm still sitting at my computer trying to figure out PrintDocument and PrintDialog. Its driving me nuts.
It's been ages since I've developed and sprint / nhib stuff but does that require full trust? My suspicions are along the lines of something is requiring full trust and when you run locally you have it, but when hosted you're only granted partial trust in iis.
Did you look up the differences? That's the first thing you should do to evaluate the advantages.
Let's you use Microsoft's unobtrusive Ajax forms. 
Thanks. I implemented the DocumentPaginator class and wrote a custom code to print from any enumerable class of entity framework. Took almost 3 hrs of trial and error. Totally worth it.
1. Work out high-level functional requirements 2. Drill deeper in functional requirements and begin constructing an Object Model that will support data necessary to fulfill requirements 3. Build out classes that meet needs of step 2 4. Spend some time learning Entity Framework (EF) 6 (an Object Relational Mapper). Use the Code First approach to generate your corresponding SQLServer schema. There are plenty of online resources explaining how to do this. 5. Build some simple Unit Tests that demonstrate your CRUD is working between your Business Objects (C# Classes) and SQLServer 6. Add obvious Business Functionality into your C# Classes 7. Begin UI solution. A good start would be use ASP.Net MVC 5 (this separates out your Model (C# classes) from View and Controllers. There are many resources online that explain how this works. Implement a very simple set of screens that demonstrate CRUD from browser to SQLServer. 8. Start looking at important foundational application needs, application security, login screens, master pages, diagnostic routines, monitoring etc,.. (most of this will be included in online resources on ASP.Net MVC, tackle each topic one at a time, don't try and boil the ocean) 9. Work out your Production environment and release strategy. Do you host this on Azure? or internally at your company. As a side note, make sure you use a solid Source Code repository solution. If you are lucky you may have access to TFS, otherwise look to Git (either internal or hosted solution). Enjoy the experience... You can do this by June. Break out the steps above into 'weeks', give yourself at least the final month to revisit your codebase once and apply concepts (refactor) that you have learned throughout the experience. Also, as soon as you have something 'working' get early feedback and validation from your stakeholders. Good luck. 
You will need book buddy. WCF is too complex to try understand it through blog articles and StackOverflow.
&lt;RANT&gt;How is this more readable than straight sql&lt;/RANT&gt;
I dont understand you kids these days. If you looking to learn something slightly complex BUY A FUCKING BOOK. http://www.amazon.com/Professional-ASP-NET-MVC-Jon-Galloway/dp/1118794753 And get your employer to pay for it.
Opting for a functional approach (no side-effects) where possible would be cache-friendly. Or are you talking more about cache hits/misses?
Use pooling. Don't create objects on heap. For images you can hold byte buffer and use image apis to create images from that while rendering. Only two I've used so far.
Yes. Had the same problem. Fixed by installing 2013 community, 2015 preview, uninstalling 2015 preview and installing 2015 ctp 5. This was on win 8.1 though so ymmv.
Maybe you are interested in this book: http://www.writinghighperf.net/
If I was coding for MVC5, then I feel your observations are similar to my own. However, I am coding for the new framework. Please check this [link]( http://www.hanselman.com/blog/IntroducingGulpGruntBowerAndNpmSupportForVisualStudio.aspx) and it might guide you to try out something similar as me. 
&gt; why using grunt and not just NuGet? Grunt is a task runner, and NuGet is a package manager. Did you mean to compare NPM and NuGet? The packages for frontend web development on NuGet are rare and often not updated. Microsoft will embrace the usage of NPM and Bower instead of NuGet for frontend web development packages in the next version of ASP.NET 5 and Visual Studio.
WPF allows you to customize the UI better while still having the same amount of power. Animations are much easier and better. No .designer.cs files. Everything is much more improved and refined compared to winforms. (Besides the editor, that is some times buggy for me) Only reason to use winforms if you MUST use it for some legacy reason. WPF otherwise is the most obvious choice.
This is not better. Just because you can wrap it in an extension method it doesn't mean you should. And in this case it's very clear: **You shouldn't.** A name like `ThrowIfNull` says what it does.. It does not say what it's doing: Asserting something. Checking pre-conditions and post-conditions. Also, you're extending one of the base types (and in this case **the** base type) with extension methods. This is very bad behaviour too.
This is just one of many possible pre-conditions. You want to create an extension method for each possibility? That's just polluting the scope. Extending base types is fine - but this is just not a good option for extension methods. Are you actually accessing the object that you're calling a method on? No.
Do you want to use [SendKeys](http://www.codeproject.com/Articles/18366/Sending-Keystrokes-to-another-Application-in-C)? Did you try: string output = process.StandardOutput.ReadToEnd(); process.WaitForExit(); or the ReadBlock equivalent for fixed buffers.
WPF has large memory footprint (even worse under WinXP) and slow to start, yet it's a LOT more flexible compared to WinForms.
Microsoft still recommends both UI toolkits. Personally I would choose WPF unless I was really, really concerned about performance. 
I tried something like that. I had a StdOutputReader.exe and a StdOutputSpammer.exe Here is the snippet that worked to read the StdOutput form the spammer on the reader: private void bStartProcess_Click(object sender, EventArgs e) { if (!string.IsNullOrEmpty(tbProcessPath.Text) &amp;&amp; File.Exists(tbProcessPath.Text)) { ProcessStartInfo pStart = new ProcessStartInfo(tbProcessPath.Text); pStart.CreateNoWindow = true; pStart.UseShellExecute = false; pStart.RedirectStandardOutput = true; _process = new Process(); _process = Process.Start(pStart); new MethodInvoker(ReadStdOut).BeginInvoke(null, null); } } void ReadStdOut() { if (_process != null) { string str; while ((str = _process.StandardOutput.ReadLine()) != null) { WriteStr(str); } } } It's not a ready to use snippet but you can get the idea on how it works. If you want I can upload my solution.
I disagree. Now that I'm used to write XAML, I think it's way easier than to design an app with WinForms.
Sure, that's your preference. And for me, it's a lot more fun to do that too. But just because it's you preference, doesn't mean it's easier or quicker. Dragging and dropping controls is quicker and easier in WinForms.
Haha. That's why I said "for me".
It's Java but better designed.
It just works.
http://jwt.io
Abstraction
The built in Namespaces contain tools for almost all basic task you could encounter and are easy to memorize. VisualStudio Nuget Productive languages Fast executables Powerfull debugging tools Interoperability to C / C++ With Xamarin/Mono you get access to all major OSes 
I like that it makes programming accessible to people who didn't grow up programming basic on a computer in the 80's or writing assembler for research projects. This equals that, hide this, show that, press play (debug button) and bam an application.
* Cleaner &amp; more consistent namespaces than Java. * Better OOB dev environment than Java. * Better integration tools than other managed stacks.
It's probably six in one hand, a half-dozen in the other ... but, it is called the *.NET Framework*, and the use of the word "platform" can be a bit jarring when you consider: Microsoft is open-sourcing .NET so that the platform will be cross-platform. ;) I like the top answer to a similar question [on stackoverflow](http://stackoverflow.com/questions/2622609/net-platform-net-environment-or-net-framework-which-is-correct-and-whats-t): &gt; These are semantically identical. Usage depends only on the context where it is used: &gt; * You build code using .NET framework (equals .NET libraries) &gt; * The code runs on the .NET platform (in the CLR) &gt; * You need to install .NET environment (.NET framework redistributable)
The whole point of the CTP (aka, *beta*) release process is to get feedback from the users ... I honestly can't believe someone would troll a programming sub. That's just ... weird.
I would probably create a new api project for the public api without any authentication. This way you can control rate limiting or whatever else separately to your private authenticated data. With regards to authentication, webapi doesn't have any, you can look at implementing an httpmodule to handle authentication or could use wcf which supports forms authentication. Really wcf is designed for internal services and webapi public services.
He's a bad ass software engineer for sure. C# plus the full version of VS being free have made the Microsoft stack open to everyone on Windows/(etc... but easier on Windows).
Exceptional tooling. Hands-down the top thing it has going for it over other ecosystems.
One that I have which doesn't stop me but is annoying is related to registering a .net library during compile when not running as administrator. 
F#, Visual Studio, and how modern C# is. I'm also holding out for cross-platform capabilities :D
Msft is behind it. I dabble in node and I still get nervous sometimes taking dependencies on node modules that are maintained by two random guys who might lose interest in maintaining them at any point. Both models have value, but that stability is something in the pro column for .Net.
Nice project. Hate the name.
And just in time too! 
The documentation is really good and comprehensive with both descriptions and examples.
Probably regasm.exe?
Fortunately it is using the stdin/stdout, using the redirection operator does work.
&gt; C# (the most widely-used .NET language) is extremely expressive and pleasant. The designer of the C# language, Anders Hejlsburg, is a visionary IMO Unfortunately C# stays behind the CLR. It could advance much further if it wouldn't need to be a CLR compliant language.
Unsafe code will not help, unless .net is misrepresenting "ref int". You can use void* in unsafe c# code too, but it really doesn't matter. In this case void* / Port* / IntPtr should represent the same thing and are interchangable. One more thing comes to mind and thats whether IntPtr.Size in your c# part equals sizeof(void*) in your c project. I'm not sure why it wouldn't (I don't think you can dllimport 32bit lib from 64bit project or the other way), but check it anyways. It would explain why it SegFaults and doesn't throw stack imbalance exception (which it should in most cases when you are using wrong calling convention).
Interesting, what could it do better if it wasn't constrained to be CLR compliant?
Please explain
Out of curiosity, what was the bug that took so long to fix?
Visual Studio is the biggest factor in why I like .NET. I'm old enough to remember what it was like programming in a text editor with a command line compiler. It sucked. You spent so much time on non-programming related tasks. Debugging sucked. Memorizing and looking up library usage sucked. There is a trade off though. The complexity of Visual Studio makes the introductory time longer. With a command line compiler you can describe a Hello World application in a single paragraph without any screenshots. If you can get over the initial hump of learning VS you can get more work done faster. 
Turning uac allows it to register properly. I have tested turning it on and off. I turn uac off because another program I use does not play nice with it. I guess I consider this a bug because if you run visual studio as administrator it just fails to run. It also does not register unless you either turn uac on or I assume install visual studio under the administrator account.
The tooling is world class. Visual Studio is, far and away, the best IDE you can get. 
1) Tooling 2) MSDN 3) An $87 billion corporation supporting #1 and #2 
Even rosyln emits IL in the end. 
One more thought, does your existing app have it's own console window. E.g. is it a console app. I've had problems with some programs which use Console API's to request the handle to the current console, and then request the stdin/stdout handles from the console. If you don't have a console, this technique won't work and I've had app's hang as they effectively get a null pointer. What helped in some situations was using "cmd /c" to launch the app. cmd.exe becomes the parent process and does some extra stuff with handle setup which allows some apps to work with .Net process handle redirection which otherwise won't.
Microsoft offers a full stack and nothing is better then when everything is designed to 'just work' together. That doesn't mean the whole stack is perfect for each other but its damn near close. Visual Studio, TFS, C#, SQL, .Net, IIS, Windows - All these things designed with the others in mind. When you develop in a full stack environment almost all the choices are no brainers. You can dive right into implementation. 
I did just get the template to work using a less drastic approach than before. I removed the authentication section from the web.config file, the Account folder from the project, the Login section from the Default.aspx file and commented out the three lines in the Global.asax. The Global.asax was the last change and then the application loaded although without the CSS formatting. I haven't written code for capturing the actual errors yet.
Moving to a mobile app? Or staying as a website, just with support for mobile? Two very different goals. A mobile app is where phonegap or something like Xamarin would come into play quite well. Otherwise it's more just frontend changes to the site to support small devices. 
It has been cool seeing your posts in this sub and the evolution of your project. I have a project in the works where I use AvalonDock and Mahapps.Metro for my shell/tab management. I might give this a shot sometime in the near future and see if it works for what I am building, as the docking and window pullouts seem much cleaner/lighter than AvalonDock. I think my only problem would be that I need something similar to the Tools/Documents setup that AD provides. Do you support having multiple tab controls in the same window, and restricting tab items to one or the other? 
All those things are there for a reason though. It isn't bloatware, .NET is designed so you can use a large number of languages (hence F# and VB.Net) and sql integration is almost always used (SQL Server installation provides access to a localdb). Not having those things installed would cause more problems for people who need them and would need to install them after the fact. Besides, can't you customize installation and simply unselect what you don't want (not sure if you can)
The mobile app will behave slightly different with different layout and more focus on "submitting" buttons, but it will be the same exact site and concept etc. What is the average time involved? 
It depends on what you want exactly. If you want a mobile browser friendly version of your web site you could use a css library/framework like [Bootstrap](http://getbootstrap.com), which *could* help you speed up the process of converting to a mobile friendly web site. If you want a mobile app using PhoneGap/Cordova, you can still leverage your html and css. Look at a framework like [Ionic](http://ionicframework.com/). Doing an app this way would probably require you to change your backend to more of an API unless that's the way it is already. It sounds more like what you want is a mobile friendly version of your site. As others have stated, that is just reworking your front end to be responsive or have a subdomain like m.yoursite.com. I prefer a responsive design over a separate site. Good luck
Go with bootstrap if you just want a mobile version of the site. If you want an app, then Phonegap will let you do the stuff but did you build your site to just be another "app" that communicates with your API? Otherwise major rework. Bootstrap will also cause somewhat big rework depending on what you currently have, but is probably the cheaper option. If you designed your site as just an app that interacts with an API then you'll just be building the HTML/CSS/JS.
Thanks - very helpful. So some background: The major difference will be how someone "posts" to this, so buttons will act different and look different. Think this OOTB.NET is the best way to go with this? I have a limited budget - which will build out faster? Thanks in advance. 
PhoneGap/Cordova is indeed the best way to go about this. Developing an app with a framework like Kendo UI or Ionic + tooling like Telerik AppBuilder or PhoneGap Build will bring you solid cross-platform results in a fraction of the time you'd spend going fully native. Also AppBuilder provides far better Visual Studio integration than Microsoft, IMO. Disclaimer: I'm Product Manager for AppBuilder at Telerik!
I love c#. 
https://github.com/gregoryyoung/m-r From the horse's mouth, so to speak. :) This includes Event Sourcing, but shows the segregation as well. I use CQRS for just about everything bigger than a hello-world. SRP is a big player in how I like my systems, and the two overlap enormously.
&gt; ...It takes ages to install... Omg. Have you ever installed Eclipse?
Pluralsight is free until Friday-- http://www.pluralsight.com/courses/cqrs-theory-practice
Thank you! This is a great write up and it has helped me get the paradigms and concepts into my thick head!
Yep, absolutely, if you look in the InterTabController you can set up a Partition, to limit in which TabControl a tab can live. https://github.com/ButchersBoy/Dragablz/blob/master/Dragablz/InterTabController.cs Thanks for the feedback.
Cool, cloned the repo this morning. Hopefully I'll get some time to dig into it in the next couple of weeks.
I like what I see. I've been playing around with F# a lot lately and have fallen in love with how terse it is. Looks like some of features that enable that terseness could be coming to C#.
Have you used [MarkdownPad](http://markdownpad.com/)? I use it constantly for writing READMEs and anything in markdown, frankly.
That is awesome. We're hard at work on clustering right now. The water's warm, come on in!
This is the most common question we've been getting in the [project Gitter chat](https://gitter.im/akkadotnet/akka.net) and discussing with people wanting to start really using the project. Figured it was time for a proper writeup.
Do you also use Resharper, by any chance? I'm curious as to how Oz Code and Resharper work together.
Hi! I'm the CTO @ OzCode. I'm a very heavy Resharper user myself, and use Resharper and OzCode to develop and debug OzCode, and haven't run into any issues so far. :)
Visual Basic. I've been using VB since I started at my current job in '99 and moved on from VB6 to the present. The fact that MS continues to update it on par with C# is great. Many view it as a beginners language, but it's just as full featured as C#. Being more verbose, it's easy to read code, especially if you get lazy with commenting. Also, being able to to use the same language for WinForms, WPF, MVC web apps, and Web API's is awesome.
I'm sorry I can't be more specific, but pretty much everything in the [list of features](http://o.oz-code.com/features). Seriously, everything there is something that I've wished for in the past whilst debugging.
SignalR - please mention it's #1 use (web pages)
Pluralsight.com has fantastic .NET training.
I've been using [Stack Edit](https://stackedit.io) myself quite a bit.
LinqPad is incredible. I use it for query optimization and as a general .net scratchpad but also to run parts of a program as you can import Dlls to have access to their classes. Very handy tool. 
A bit off-topic for this thread, but I was wondering if you guys support debugging the Unity3D game engine with OzCode? (Unity3D uses Mono as its programming framework, but there's a Visual Studio plugin called UnityVS that allows you to debug Unity applications from within Visual Studio.)
Do you use resharper? I'd like to know how it is with both installed. Do they play nice together? I've basically got my resharper set up so I barely type anymore. I couldn't give up that productivity for debugging advantages. It does look good though. 
God damn. Now I've just purchased OzCode too! I remember when it was called something different, bugaid or something?
Thank you very much!
I love the short movies with the old-timey intros. Very well done! Top marks! You deserve every success!
I've been using OzCode combined with resharper for the last year, and they run great. AFAIK Resharper does not offer any debugging features right? I haven't ever see Resharper do anything at runtime, and OzCode doesn't do anything while not debugging
I agree with some of these but not all .net development is Microsoft only but this list is. Also a lot of these tools cost money but not every business is willing to pay for all these items. I would also disagree that community edition is the same as professional as it has more tooling just like the more costly editions. 
I work for the federal government and members of our network group are stupid.
Say no more. All IE?
Firefox is working so I've got that going for me. We might be able to get Chrome back some day. It's kind of critical as we're developing for "the outside" now and we're going to have to make sure our stuff works on other browsers and even mobile devices. Oh, and USB devices, besides keyboards, mice and card readers, are blocked so no testing on plugged in devices.
That sucks me think. I hope things improve. I can't stand the other browser tools. Chrome spoils you.
I've been a big fan of [Dillinger](http://dillinger.io), but StackEdit looks more promising. 
I'd like to use EntityFramework, but it still has too many missing features to replace NHibernate yet.
LinqPad is simply amazing! Not only can you import DLL's, but it integrates with NuGet as well.
This is awesome progress! I was confused over the small number of libraries in the beginning. I knew it was just the beginning, but small number + not knowing what to expect + not knowing how fast this will proceed = confusion and doubt for me. But this tells me a lot about both their speed (pretty fast!) and their goals (only 25% is done). Wow. How is Microsoft.Win32.* stuff handled? Registry is emulated in files on non-Windows? So many questions. I wish for as much .NET as possible cross-platform. This is so damn huge for me as a .NET developer, in terms of leveraging existing skills.
No offense, but this seems like blogspam. The article does not add anything and you might as well directly link to http://ww2.pluralsight.com/codeschool
You can also define your own context/provider to be used . So if you have built a data access layer, that you want to test/query in linq pad, you can define your context, select it (just as though it were object or dbcontext) and you're good to go. My only complaints is that autocomplete isn't the best and it'd be nice to get compile time errors prior to running...but those are minor quibbles 
Just use the books and their practice exams. I haven't attended a bootcamp class, so i cant comment on those. However, nothing beats actual experience. Source: MCTS and MCITP in SQL Server 2008
Oh the [joy of Visual Studio caches.](http://i.imgur.com/v1in6ez.jpg) Close visual studio and then run these as administrator: rmdir /s "C:\Windows\Microsoft.NET\Framework\v4.0.30319\Temporary ASP.NET Files\root" rmdir /s "C:\Windows\Microsoft.NET\Framework64\v4.0.30319\Temporary ASP.NET Files\root" rmdir /s /q "C:\Users\YourUserName\AppData\Local\Microsoft\WebsiteCache" rmdir /s /q "C:\Users\YourUsrName\AppData\Local\Temp\VWDWebCache" 
I think it was a dodgy file lingering in the obj folder. I'd tried clearing all those temporary asp.net files with no joy. I'd also tried deleting the obj folder. What finally fixed it was deleting ALL obj and bin folders for the entire solution and rebuilding.
There are many ways this issue manifests. Almost always, the underlying error is "file in use by another process" causing the build to fail to write the files out. Usually, that process is visual studio. Sometimes it's vshost, or w3wp, but it's usually visual studio. 
There is an entertaining little video on the OzCode site that showcases the features
Yes, you can use LinqPad to achieve a REPL-alike workflow even in the most tightly coupled projects. Invaluable for working in legacy code!
Back in the olden days in Unix we'd use a program called *expect*. (One of the only two things that TCL was useful for.) You'll have to do the following: 1. I'd recommend using the asynchronous methods for writing to STDIN/STDOUT on the process handle. It's easier to see what's going on a bit at a time. 2. You'll need to turn off I/O buffering on the read and write handles. Flush() might work on the write, but you'll need access to the read immediately or you'll block. Just because the writer (FORTRAN) sends a newline, doesn't mean that your ReadLine() will necessarily finish. Perhaps the code at [Expect.Net](http://blog.iwanek.eu/expect-net/) would help with the particulars. 
In the application pool settings, change it from on demand to always running. This will be in the advanced settings section for the application pool. The other thing to consider is using something like pingdom or new relic synthetics to ping the site once every minute. This will stop the pool from being spun down. This solution works even if you don't have access to the IIS settings for the site. 
WebAPI and MVC are the future of .NET development. WebForms are pretty basic (IMO) and shouldn't really take all that long to understand what is going on. I wouldn't attempt to memorize methods/classes. You'll naturally get to learn them by writing code. Like other commentors have said, you'll get more out of knowing concepts (and how to apply them), then knowing specific classes/methods. Practice is going to be your best friend. Try re-creating an application/website you like to use in .NET. Beyond the code, you'll also need to know version control. Git is the all the hotness right now, so familiarity with it would be good. Github offers free public repos. Bitbucket offers both public and private repos for free as well. 
When the site is only slow the first time it's loaded in .NET is almost always the first hit penalty. He stated that when accessed regularly there is no problem.
Because Reddit is the only place that ever hears about anything.
I'm liking the move towards community licensing for developer products. Makes being an independent developer a *lot* cheaper.
Thought of this too, but it's kinda hard to find one, even finding a tutorial to write one by myself seems tricky, most of these SDK links are either outdated, 404'd or totally dead.
or worse - on code project. But this looks promising - http://blogs.msdn.com/b/matt/archive/2009/09/28/converting-rtf-to-html.aspx One thing you theoretically have going for you is that you don't need to take random HTML and display it. That'd be rough. I'd use a hidden webbrowser control and copy paste magic for that I think. But converting RTF to HTML seems to be a solved problem. Ghetto as hell, I'll readily admit, but so are most WYSIWYG editors out there IMHO
Found and tried it right now - works in kind of weird way. When I'm passing a XAML code into it: XAML -&gt; HTML returns nothing, but HTML -&gt; XAML returns a FlowDocument without any formatting at all.
Ok, for all interested - I got it. Using [This](http://blogs.msdn.com/b/matt/archive/2009/09/28/converting-rtf-to-html.aspx) and [This](http://www.vbforums.com/showthread.php?719411-RESOLVED-TextRange-does-not-get-RTF-of-WPF-RichTextbox&amp;p=4400775&amp;viewfull=1#post4400775) I've managed to get RTF first and then convert this RTF to HTML code.
http://stackoverflow.com/a/15580293/643085
Quite often TFS can be the culprit as well, if the files got included somehow.
[neo4j](http://neo4j.com/) -&gt; has a .NET sdk. Easy to setup, easy to put data in and query. [Titan Graph Database](http://thinkaurelius.github.io/titan/) -&gt; Haven't used it but have heard good things.
If you've got an msdn subscription (licensing was pretty costly otherwise IIRC), have a look at the Microsoft Automatic Graph Layout library. 
I use the Pdf, Excel and Doc libraries. We have hit lots of bugs in them but I think they are still the best I've come across (this isn't a compliment). For example they have a tool that we use to convert excel, word and several other file types and put them all into a single pdf. Unfortunately it doesn't run properly in our web application (so we built a console app and use Process to run it separately). In addition to it spontaneously crashing (we think due to some deadlocking code, didn't look too far into it) it had a nasty memory leak. In another instance we have this process where we build a word document up with mail merge fields throughout it and then mail merge it with a dataset (syncfusion has this nifty nested mail merge thing which works decently) then convert to a pdf so that we can generate customized pdfs for each of our clients. To build the pre-mail merge document we have a base document and a set of templates located in another doc. In the templates we have merge fields that mark where we would paste other templates or text or so on. Here is a sample of the code where we are replacing a merge field at a particular marker with some string of provided text: static void Add(WMergeField locationMarker, string text, bool remove = true) { WParagraph paragraph = locationMarker.OwnerParagraph; int index = paragraph.ChildEntities.IndexOf(locationMarker); if (remove) { paragraph.Items.Remove(locationMarker); } if (string.IsNullOrEmpty(text)) { //if no text, remove the location marker container if (paragraph.OwnerTextBody != null &amp;&amp; paragraph.Items.Count == 0) paragraph.OwnerTextBody.ChildEntities.Remove(paragraph); return; } int lastindex = paragraph.ChildEntities.Count; try { //all text may contain html, but syncfusion doesn't handle malformed //snippets of html (for example "&amp;" instead of "&amp;amp;") paragraph.AppendHTML(text); } catch (NotSupportedException ex) { Logging.LogWarning("Couldn't append html: [["+text+"]]", ex); //since we cannot just let that exception happen, we catch it... //add the offending string as plain text so that at least something is there // var p = paragraph.AppendText(text); //perhaps we could add a comment, but this messes with markup (example bug: if paragraph is a heading then adding a comment removes the paragraph style) // var comment = paragraph.AppendComment("Added as text because it was considered malformed as html"); // comment.AddCommentedItem(p); } while (index != lastindex) { //work around not being able to add at any point in the document //instead we will append to the end of the desired paragraph and //then move everything that was supposed to be after the injected text //to be after it Entity item = paragraph.ChildEntities[index]; paragraph.ChildEntities.RemoveAt(index); paragraph.ChildEntities.Add(item); lastindex--; } } When it comes to pasting templates we have this gem, because TextBodySelection cannot cross TextBody elements and there is no way exposed (there is sort of one internally in the source) to get an entity collection between two of them: 
So... what is it?
Nice writeup! It's an area for concern when you have to reverse engineer things just to get a working solution, but honestly, I haven't met a toolset of this complexity where that was ultimately necessary. It's too bad FOSS hasn't taken over on .NET yet; that makes these kinds of things so much easier. Anyway, I will have to keep Syncfusion in mind in the future. Honestly, the last big .NET suite I touched was Telerik, and I've been camping out in Java-land the last 3 years, so it's nice to have options. Telerik had it's strengths, but I wasn't totally sold on them back then. 
I have done equivilant things to .NET itself: http://stackoverflow.com/questions/12700196/is-there-a-better-way-to-cause-form-submit-when-dopostback-is-called and Telerik and NHibernate and DNN and plenty of other cases. It is one of the reasons we buy source licenses for everything we use in our code. In this case we didn't have the source yet for that code (waiting for the approval to come down to buy it at the time; we were still using an evaluation version at the time I wrote that code and it hasn't changed since so the comment still sits there).
Well I mean if you already bought a license from Telerik or something then no point, but this is like when Telerik MVC was free. I think they just moved away from that and I dunno what they call it now. But as with other suites, can help make things look fancy. Though generally I prefer to just go with a simpler jquery plugin or something since based on Telerik back in the day, too much bloat.
I am getting some more clues about this: http://www.asp.net/web-api/overview/odata-support-in-aspnet-web-api/odata-v4/create-an-odata-v4-endpoint So there is no scaffolding for odata 4, you have to just write them. Doesn't answer the edm.date question but helps me to understand why I'm not getting a v4 endpoint. Oh and support for Edm.date is in the specification, so I think there is more support than just DateTimeOffset... http://docs.oasis-open.org/odata/new-in-odata/v4.0/cn01/new-in-odata-v4.0-cn01.html#_Toc366145496
IMHO this business model of control suites is on it's way out. It's just too easy to style, plug-in, open source, slap controls together in most modern languages/frameworks.
Whoa, I wouldn't call SQL Server or EF bloatware...
I am interested, thanks :-)
Just to add, .NET isn't 100% implemented in Mono; [here's the Mono compatibility list](http://www.mono-project.com/docs/about-mono/compatibility/). Xamarin studio on OS X is really good, but if you're going to be developing in Windows later on, knowing Visual Studio will be useful. If you just want to play about to learn the syntax, consider [dotnetfiddle.net](https://dotnetfiddle.net/ ).
vNext might not be ready for production use yet, but it should be released soon (hint - new Visual Studio is called Visual Studio 2015 for a reason). And it shouldn't be too different from .NET 4.5 for learning purposes. Just remember that some of new and fancy stuff won't work with C# 5 compiler, but there are around 10 such a constructs and for beginner they might not pop-up yet anyway. So if you can get it up-and-running by method mentioned in article and are more comfortable with OS X and your favorite IDE you can give it a shot. Though I doubt that there is better IDE for .NET that VS + R#, your mileage my vary :) 
Kendo has a solid documentation, but I hate their support and examples. 
Telerik kendo has visualization
If you don't use it (or want it) it's bloatware. I understand for you this isn't the case but it would be nice to have the option to install stuff as and when I need it.
If you can get your hands on a PluralSight account, take a look over there.
The Build conference is at the end of April. Expect the RTM then. 
You can choose which features to install during installation. You don't have to install the features you erroneously label as bloat-ware.
in your function calls, is step1 supposed to be a string? or some sort of object? I haven't used webforms in a long time so am not really sure, but I would guess it'd be easier to just bind them to different functions, but make each of the 2 functions just a 1-liner that calls your "true" function with whatever parameter you want. For example: Step 1: Next &gt; myFunction1() which would be { myFunction("step1"); } Step 2: Next &gt; myFunction2() { myFunction("step2"); } 
You'll be better off using DateTimeOffset in the long run, timezones are garbage!
I just installed VS2013 this week, and I didn't have to install the features I didn't want to install, including SilverLight, Phone SDK, Expression Blend, and SQL Express.
Gah, I forgot to add that I had a practice exam by measureup. I determined my readiness by being able to pass it twice in a row. What I noticed though, is that there are significant gaps between what the practice exam covers and what the actual exam covers. Some things that come to mind off the bat are preprocessor directives, garbage collection, and concurrent collections? There are even some things in the practice exam that aren't a part of the exam domain that I could tell. At any rate, I'm not trying to make excuses here - I guess I've just missed the basics.
Design patterns, SOLID design. Listening skills and demonstrating an eagerness to learn from the other developers will take you far.
The best thing, by experience, is gaining experience. Do projects that use these technologies, it doesn't matter what kind of projects. Read the documentaton (MSDN), try out things, just get your hands dirty. (This assumes you know the basics of software development.) Another good thing is to constantly read on Stack Overflow and the Programmers Stackexchange. Also make use of the CodeReview stackexchange. Listen to these people, they mostly know it better than you.
There is no way you can fail if you do these a few times before doing the exam http://www.aiotestking.com/microsoft/ You need to study to actually learn something :-) but also do a few tests beforehand.
How can you guys live without redgate? Even SQL search which is free makes my life a hell of a lot simpler. And the 2 compares are invaluable for deployments. Also viasfora is an awesome VS add in colors each set of braces individualy so you can easily tell where you are in the code
Are you going for graph layout in a desktop application or on the web (possibly on the client)? I second @OolonColluphid - MS AGL has worked very well for me in the past on the desktop. I've also used Graphviz by launching in a seperate process and then drawing the resulting coordinates by hand using System.Drawing. If you're looking for something for a web application, I love Cytoscape.js. Being JavaScript it can get slow for really big graphs, but has a variety of layouts and is easy to tweak. I just send the graph down and JSON and then use Cytoscape.js to layout and render.
We licensed MSAGL directly, however the issue is that there is no documentation. Are you aware of any good documentation for it?
Can someone point me to some wpf applications that look better than a win form? I think I just suck at it. I read that wpf looks better but it looked like winforms to me when I used it so I went back to winforms. 
There are blog posts here and there from when it used to be called GLEE: http://chalaki.com/how-to-program-msagl-glee-to-create-hierarchical-graph-layouts/519/ https://stevehorsfield.wordpress.com/2009/08/13/experimenting-with-glee/ But you're right, there isn't much. For me the best thing was to just pop it open in a decompiler like Reflector or JustDecompile. If I recall, the data structure and public API is pretty straightforward.
Don't stop learning just because you're now working. Learning new tech will help you in your current job. Learning new tech will help you in your future job(s). Dev's who don't continually learn go the way of the dinosaurs. I find Chris Alcock's [The Morning Brew](http://blog.cwa.me.uk/) to be a great resource for new articles.
WindowsFormsHost if you absolutely need some WinForms support in WPF. Fewer excuses!
Depends on the machine I needed to run windows 8.0 and 8.1 for different projects. I had different vms on my macbooc pro. No issues, but I do have 16gigs of ram and an sad. But sometimes I forget I'm in a vm. Almost no noticeable slowness.
Yes. Ask in a Wine forum.
You've got SO many layers there, hey bet you're glad you paid more for that mac now eh? Wine doesn't make 100% of software work On top of that you're layering steam and .net framework before your game code even gets a chance to run. Could be anything dude. 
I got the mac as a gift and this problem isn't that big. If nothing works I'll just do Bootcamp. Thanks anyway
Rebooting is a pain but probably the best solution.
Ok I will
Indeed!
I strongly believe that custom sorting/ordering is a UI smell that shows you don't understand the users work flow at all. This could be useful in the "scaffolding" stage though, where you really haven't worked this out.
The whole of paint.NET can be decompiled, and it's framework
You can still find its source on numerous websites from back when it was open source. 
That's pretty interesting to see. Could you give a list of the add-ons you're using? I'm especially interested by the advanced code snippets. Thanks
Thanks a lot!
You'll see two test runners at work here - the runner that's marking coverage on the left hand side is NCrunch, the test runner we occasionally flick to is ReSharper, and the test snippet is a resharper template that I wrote in 30 seconds, looks like this: [Test] public void $Method$_$Condition$_$Expectation$() { $End$ }
Boo! Reddit ruined my snippet
I use a connection string and sql.execute most of the time. But I also use [SubSonic](http://www.subsonicproject.com/) for an ORM.
It offers consistency and rapid application development. Something that corporations like.
There is documentation/demos. You didn't look very well. Plus, they have support to answer your questions within 24 hours. It's a suite of widgets(grid,charts,menus, etc) that offer consistency and rapid application development, exactly what a corp would want...
Sounds like it'll be "Micro" .NET on the Athens platform.
winforms seems completely useless at this point, except for maintaining legacy apps. Also, regardless what you think about Metro, doing XAML and MVVM is much safer because you can eventually just swap the UI framework to something else. Whereas winforms code is stuck on winforms forever.
No, I like MVVM and XAML for WPF
&gt; MVVM and XAML for WPF MVVM is basically platform agnostic. And XAML enables that via DataBinding and DataTemplating support. BTW, I've been swapping out my custom made WPF styles and templates that looked like the Windows 7 era (they were cool with lots of transparencies and gradients, but properly combined colors), to MahApps.Metro, and the change has been very positive. My (line of business) applications look even more professional now. So I'm not sure if you were talking about WinRT or what, but IMO, the Metro "design language" is pretty good.
We don't know that yet. Microsoft has already indicated that they want a universal binary. So maybe they might have a framework version compiled for ARM7. 
Agree. I really dig the clean, simple yet functional look of 'metro'.
What is the preferred MVVM framework these days? Is there one or have people been rolling their own? Any good tutorials on this that you could share?
This is great! I will definitely be using this soon! Thank you
SQLite is pretty standard for your use case. [A tutorial to get you started with it using EF.](http://bricelam.net/2012/10/entity-framework-on-sqlite.html) EF can be heavy handed though. If you aren't attached to the LINQ stuff and know SQL, [Dapper](https://github.com/StackExchange/dapper-dot-net) is a good and fast wrapper. Also, as far as toolig for SQLite, [firefox has a plugin that is my go to. ](https://addons.mozilla.org/en-us/firefox/addon/sqlite-manager/)
The only bad thing about ViewBag is that you are adding dynamic properties. Surely for a small string its fine like this. If you were using ViewBag to cart around a List&lt;MyImportantEntity&gt; and avoiding creating a proper Model class then yes ViewBag is a No-No. This usage of ViewBag is well within scope &amp; is the correct intentional use of ViewBag. I personally have never explicitly used the ViewBag in my 4 years as an ASP.net MVC developer, except for some projects that use the ViewBag.Title.
Totally agree. Once you had your ass bit by the ViewBag, you always remember.
I'm probably just behind the curve, but I've tried adopting WPF several times and WinForms is just SO much faster (for me). No fussing, just rapid development. That's the kind of stuff I do, so it's just worked better for me. I know I'll need to adapt eventually.
To be clear, I'm not trying to asset that I'm doing things the best way. Your tone is incredibly hostile. There are many times when I don't *need* data binding. I'm not talking about large complex applications, but rather small, simple utilities. I've done some time in WPF, and the amount of "wet" code smell that was generated implementing INotifyPropertyChanged stunk to high heaven. That's not to say you're wrong. You're probably right, and probably a much better programmer than me. But you're coming off as an arrogant jerk. Commonly, in my experiences, WPF has been a time waster, not a time saver. I know that this is not the norm.
Does this support Entity Framework?
You don't need entity framework. This is like code first pocos
Just make one of your filters act as a transform step. Think of filters as all of the operations that LINQ provides and pipes as the mechanism of transport. LiNQ just happens to be a serious of filters over an in memory set. If you had a distributed set of processes you would need other pipes to connect the data. Message queues, web service calls, flat file dumps etc. So in your case one filter takes dbaobjecta and maps it (transforms) it on to dbobjectb. 
&gt; but rather small, simple utilities. Yes, for toy applications the winforms approach might be usable, but trying to use it for anything serious immediately shows how it's retarded and useless. Regardless of that, you can still use WPF and use code behind for "small utilities". Nothing is forcing you to use DataBinding (which I do even for small utilities sometimes, mostly with ItemsControls). In contrast, winforms is totally useless if you need serious stuff. This means that whatever you can achieve in winforms, you can also do it in WPF in the same way, or a better way. Therefore WPF &gt;&gt;&gt;&gt;&gt;&gt;&gt; winforms. winforms is useless. &gt; I've done some time in WPF, and the amount of "wet" code smell that was generated implementing INotifyPropertyChanged stunk to high heaven But that can be automated. Last time I wrote any of that was before creating my own T4's that did it for me. In contrast, winforms' retardedness requires a shit ton of horrible code behind that forces you to lose your time writing it (and which btw is not testable and not reusable outside winforms, for example on mobile). &gt; Commonly, in my experiences, WPF has been a time waster, not a time saver You're probably approaching WPF with a winforms mentality just like everyone else does when coming from winforms to WPF. Just looking at the `ItemsControl` and what you can do with it and a simple `ObservableCollection&lt;T&gt;` makes winforms look like a useless dinosaur in comparison. Problem is people don't realize that.
Yes, filters aren't always order independent. Take a look at this example https://msdn.microsoft.com/en-us/library/ff647419.aspx
I'm still learning MVVM myself, but from what I gather Caliburn.Micro and MVVM Light are the two main frameworks. And then of course there is Microsoft's PRISM but I gather that is used in a lot of big enterprise applications.
if you have website monitoring that has URL capabilities you can have the tool access the website and look for a string of text within the page. This keeps the application running and the process running so it doesn't die off because it hasn't been accessed in a while. I'm not sure what the timeout settings are at GoDaddy's server hosting - but I'm going to assume it's low like 5 minutes. They're overselling servers and do not want processes sitting around idle. A tool that pings the website and looks for a string of text will keep the application running and prevent it from having to be recompiled on the first request. 
Sadly, I'm using godaddy, so I don't have access to IIS settings, apart from some simple things like CAS, Culture and so on. Luckily, I have Linux VPS that was just sitting idle, so I added a cronjob to wget my site every 3 minutes. It doesn't mess with google analytics and the site is always in the pool, resulting in almost instant access, always.
If you can scan the page and look for text or something like that you'll accomplish what you're looking for. Just keep the application running before the timeout value is hit. Not sure what that is but every few minutes is a good start. 20 minutes is the default
I choose ViewBag over ViewData for readability purposes. In my experience, the difference is not apparent between using the two. Here is a stack overflow article that goes into more detail (though it is from 2011): http://stackoverflow.com/questions/5078996/viewbag-vs-viewdata-performance-difference-in-mvc
&gt; What's stopping you from using a newer version of VS? Largely it's inertia. The book I started with was a 2010 edition of C# for Dummies, which got me started with VS2010 and I just kept using it. Otherwise there's nothing holding me back. &gt; VS 2013 Community Edition I didn't realize that was a thing. Definitely going to check that out. Thanks!
For the sake of simplicity, the sample is a skeleton of a project. I wrote the project this way so it is easy to understand how to integrate the two together. The UI in the sample sets a different string on each page depending on which script is loaded. I may do a follow up to this in a few weeks using an MV* framework such as VueJS to give a better idea of what this looks like in a larger scale project. 
&gt; VS 2013 Community Edition If I am not mistaken, the Community Edition allows for 3rd party add-ins like ReSharper; something the Express Edition didn't support.
It's a custom site built for us a few years back. All I need is another title added to the drop-down menu. They told me it could take 3-4 hours to accomplish this. As a layperson, it would seem to me to be a much simpler task requiring less time. Can I get a programmer's view on this?
What is not clear from that skeleton is what the actual usage case is. It's a proof of concept but it's a redundant one because Razor allows any JS framework to be integrated into it. A better demonstration would be to show why it's simpler/better/faster/more beneficial to opt for react as an alternative to any other client side combination especially backbone with bootstrap which comes integrated out of the box with Visual Studio while all the documentation and illustrating examples are geared for that combination. 
this is part of a message I got via email from the company: I took a look around and I'm not seeing any admin for that feature so I'm guessing that it's in the database so it would need to be addressed by a developer. We'd probably need a couple of billable hours to refresh ourselves with the site and then make modifications. If you have any database developers on your side, they may also be able to make the changes. 
The reply is basiclly the same thing I said. You are going to want someone who knows this stuff to take a look. Think about it like this. Pretend your website is the plumbing in your house: &gt; It's a custom plumb job built for us a few years back. All I need is another sink added to the kitchen. They told me it could take 3-4 hours to accomplish this. As a layperson, it would seem to me to be a much simpler task requiring less time. Can I get a plumbers's view on this? Like the sink you can do it yourself, though without exp there are chances you will screw things up. You not only risk not meeting your needs you risk breaking the system and making it function worse overall. If things like this really were simple for laypeople to do off the cuff then there would not be a whole industry of people who spend thier lives studying this stuff. Now if you see a need to edit this list in the future I would ask the person doing the work to add in the ability to update this list easily. 
Its still going to take someone a chunk of time to do the work. We he "Experienced developer" he means one that knows the framework and has context of how its built. 2-3 hours for someone external is about right. Another way to think about it is this is part of the cost dor outsourcing development; you'll always be paying someone to ramp up.
Backbone and Bootstrap perform different roles from RequireJS. In fact, you can use all three client side frameworks in tandem, and they work together quite beautifully. You are right, Razor is a template engine, so anything, including JavaScript frameworks, can work with it. The example shows how to use RequireJS to organize your JavaScript files in a way that allows a single configuration for dependencies and allows you to have specific JavaScript files per page without embedding per page script tags in the Razor page.
Ah, so this wasn't developed in house and you don't have anyone on staff that can do it. Well, your stuck. Yes, they are going to charge your their minimum billable set of hours for a change. That's just just how consulting is done. 
What is the drawback to dynamic types used in the ViewBag? I'm working through this right now &amp; not sure if I want to use a ViewModel instead. I'm trying to populate a dropdownlist on the page with a static model that is linked to the strongly-typed model on the page, but still an MVC newbie. 
Yep, community edition is awesome. I work for a very small business, so it saves us a couple grand a year. Or, it will, since this is the first version.
Do you have access to Visual Studio 2010? If so, the jump from 3.5 to 4.0 is not that drastic. You could most likely upgrade your application with almost no trouble. Otherwise, you could attempt to backport the library to .NET 3.5 or create a new .NET 4 web service to interact with that will handle the library for you and be consumed by your .NET 3.5 app.
I can see your own confirmation bias creeping in here, you are agreeing with the commenters that confirm your own view of the world. You have to understand that none of us know the internals of your site and therefore any pat diagnosis of "this is 2 hours" or "this is 2 minutes" are likely to be way off base. For what it's worth, I have been working in software for 25 years and consulting a good part of that. The time quoted to you by your consultant seems reasonable. You have to understand that they are building risk into their quote. They need to spend some time understanding how to deliver the fix properly so that there is no disruption. Yes the fix may be easy. But here's a short list of things they will likely need to figure out before they can make the fix (assuming that so much time has passed that they don't have direct answers to these questions): * how to access the source control repository; gain permissions to it * how to access the current site. * how to back up the current site code and how to restore it in case they make a mistake * make sure that the version of the code in the source code repository matches the version running on the site. If they don't do this, they risk pushing more fixes to your site than just the one you ask for. * find or create a QA environment so the fix can be tested Then, FINALLY they can determine where to make the fix, perform the fix, test it, and deliver it. Frankly the quote sounds very reasonable to me.
An experienced developer could probably find the right spot in the code/database and make the change in about 10 minutes. However, that's without any kind of testing or QA. I think it's unlikely that making that kind of change would break anything unless the code is *really* bad, but a) there's plenty of really bad code out there, and b) if you cheap out on testing you'd better be prepared to take responsibility if something goes wrong and not blame the developers. Plus, as others have pointed out, there's going to be a certain minimum amount of time to get access to the files/database they need to modify, and again to deploy the change to the live version of the site -- even longer if they're not familiar with your process for either of those things. It's like your car is out of gas and you want to pay someone to fill it up. Takes two minutes, right? Except the car is sitting in your garage, so you also have to pay them to drive over to your house and bring a can of gas. You probably want them to look up the make and model of your car too, to make sure they don't accidentally bring regular gas if your car runs on diesel or something.
OK, after reading everyone's responses, I better understand why it may take a couple hours to do this and we will probably just go ahead and let the developer who built the site make the fix. I really appreciate everyone's input. Helps me wrap my brain around something I really didn't understand very well. FWIW, my 14-year-old daughter is taking a college course in coding right now and she's good at it and I hope that she'll be much more savvy about this than her mother (and maybe earn a good living doing it one day)
I haven't tried it, but if you used .NET 4.0 to create a COM based wrapper around the new libraries, then used COM interopt from .NET 3.5 to access them maybe you could... Naw, that's crazy. Just upgrade your code base already.
Next part is coming this Thursday, stay tuned.
I'm confused about Xamarin and universal app. Is Xamarin a tool that fits above the universal app paradigm and then breaks the application down to native machine code for a specific platform we are targeting (Windows/Android)? 
The dotnet team hinted that vnext will be ready in 'early half of 2015' (it's somewhere in this subreddit). I got the impression that would be in a month.
Never heard of that and I seriously doubt it. The VS2015 CTP5 was released not long ago, and there is already a breaking change again that makes the latest version incompatible with the CTP5. The API is not even stable yet. I'd expect to be somewhat ready at the next Microsoft Build.
the times they are a changin
Note that if you can [rule out concurrent execution](http://blogs.microsoft.co.il/arik/2010/05/28/wpf-single-instance-application/) (and therefore likely don't need ACID protections), then simplest way is to skip the DB entirely and simply serialize the data to json and write to a file in your app settings directory. You can set up a static 'datastore' object that stores all persistent data using standard C# collections and serialize/deserialize that to a single file. This obviously doesn't scale as well as a proper DB (though you'd be surprised at how big a dataset JSON.net can quickly churn through) and you have to be very careful about making sure that crashes don't result in partially written files (hint: write to temp file then use File.Replace to overwrite the existing json file), however it makes storing persistent data really really easy. If you do want to go with something more DB-like, another option is "ESENT Managed Interop", which basically lets you leverage an embedded db implementation that is already baked into Windows. However, be careful if you go the route of its PersistentDictionary class. It offers a ridiculously simple persistent datastore but it also forces you to package your data in structs instead of objects and using structs to share data across your app is a recipe for unexpected bugs unless you thoroughly understand the semantics of structs and pass-by-value instead of pass-by-reference. 
3.5 is absolutely still supported and will remain supported for a long time: &gt;Beginning with .NET Framework 3.5 Service Pack 1 (SP1) the .NET Framework is defined as a component instead of an independent product. So, support for .NET Framework 3.5 SP1 is driven by the support lifecycle policy of the Windows operating system (i.e. when Windows is in the Mainstream Support phase or Extended Support phase, so are its components). When a product such as Windows reaches the end of support, so do its related components. .NET Framework 3.5 SP1 is supported on Windows Server 2003 SP2, Windows Vista SP2, Windows 7 SP1, Windows Server 2008 SP2, Windows Server 2008 R2 SP1, Windows 8.1 Update, Windows Server 2012, and Windows Server 2012 R2 according to the end of support date for each operating system.
I wish Microsoft would put more emphasis on getting ".NET Native" to work with desktop apps. Nobody wants a dependency that requires admin rights to install so, if you want to put out a desktop app with any kind of wide distribution you're basically limited to .NET 3.5 (if you're lucky), and that's not likely to change anytime soon.
Late last year I took some old VB.NET code from 2.0 (VS2003) up to 4.0 (VS2010) with very few problems. Updating 3.5 to 4.0 should not be a problem at all.
Brad, I just sent you an email.
Just jump up to VS 2013 Community edition, everything is basically the same and you can choose the framework you're dealing with.
Linux is a primary target for .Net 5 and ASP vNext
Build 2015. End of April.
I wasn't really, hadn't heard of it. But I'll look into it, thanks!
Thanks for making me aware of that book. I'm going to read it, but i'm not motivated for writing a review
No but it appears that it is building properly on Linux, which to me indicates that they're already working on it.
Here are my thoughts on .NET Core, .NET Native, and ASP.NET vNext https://odinsql.com/2015/02/net-core-runtime-coreclr-released-github/
Thank you for that.
wow - I wonder if they'll have an ARM7 version of CoreCLR as a separate install. Considering that they announced a version of Windows 10 for the new Raspberry Pi 2 on Monday. 
yes, it's on [NuGet](https://www.nuget.org/packages/Dragablz) now. I always intended to build it theme "agnostic" so there is the basic theme to plug it into a "vanilla" WPF project. Take a look at the demo [DragablzMeetzMahApps](https://github.com/ButchersBoy/DragablzMeetzMahApps) ...this will show you how to use the MahApps specific styles.
Yeah all the tab tearing and docking works with MahApps. Take a look at the demo [DragablzMeetzMahApps](https://github.com/ButchersBoy/DragablzMeetzMahApps) I'll try and get a blog post up soon at http://dragablz.net explaining how to hook in with MahApps as well.
As others have said, community editions will alleviate any financial concerns. If you want more, you can try to apply for one of their various programs to give you access to MSDN downloads. Examples being Dreamspark(students): https://www.dreamspark.com/ Bizspark(small businesses): http://www.microsoft.com/bizspark/ Both of these are free and offer full, professional, versions of the newest IDEs, as well as some other programs that you may be interested in. You may run into some issues where some addons may not work with older versions but if you like what you have and it works for what you do, then I don't see any compelling reason to upgrade.
I appreciate the input, but, in this situation, we're not going to completely rewrite hundreds of websites. I'm looking for something that will take a relatively small amount of time and have a big impact on reducing the vulnerabilities present in these old websites.
&gt; in this situation, we're not going to completely rewrite hundreds of websites. Then you will **always** be vulnerable and you will be chasing this beast for years to come. You'll fix your QueryString handling this week, the attackers will find a new vulnerability in it next week. You'll fix that one, they'll find the next one. Pretty soon, you've spent more time fixing that code **and mopping up after the attacks** than you would have if you'd fixed the code properly the first time. Or, you can invest the time to get it done properly **now** and not worry so much about it. If you (the organization, not you personally) have written these sites so poorly that there's no common method of accessing data such that you can fix that and propagate the fix out to all of them, a rewrite is probably needed anyway. There is no "quick fix" to poorly-thought-out security practices.
great, i'll play around with it later. this library is perfect for a POC I've been meaning to put together as a side project so quite excited about it
I cannot stress how correct /u/alinroc is. You really need to be using parameterized queries (either stored procedures or writing your own SQL...or using an ORM like EntityFramework, but that would be way too much rework most likely). This pretty much will guarantee to protect you from any sql injection attacks. 
If you don't refactor to use prepared statements now, you will be facing much more work and many more rewrites of cleansing the query string. That is the only right way to solve this problem, as other's have stated.
Although not what you're asking exactly, you can filter query strings in IIS. http://blogs.iis.net/wadeh/archive/2008/12/18/filtering-for-sql-injection-on-iis-7-and-later.aspx Be careful, though. A false sense of security can also be dangerous. You should be using parameterized SQL statements.
Switching to prepared statements will take a relatively small amount of time. It is a completely mechanical process that even the most junior of programmers can easily do.
Thanks for sharing. This was basically what I was looking for, but after reading the other posts, we'll hopefully be going in a different direction. Since most of these sites are on an old version of .NET it probably wouldn't work anyway.
Yup, that's the best way to do it, or use an orm like entity framework, please tell me this is what you are doing now for new sites!
Well tempered response. BTW: this isn't doing it the right way. You are past that point. This is just a way of getting where you need to be. Here are the "doing it the right way" options: * using parameterized queries for everything...which probably means rewriting all of you sql queries. Something like "select col1 from mytable where col2 = @param1" * use an ORM and remove all your embedded SQL. There is NHibernate, EntityFramework, * Keep your embedded SQL but use a microORM that helps with the parameters (Dapper is one). 
**USE PARAMETERIZED SQL STATEMENTS** string injectableSQL = "SELECT * FROM products WHERE id='" + id + "';" string safeSQL = "SELECT * FROM products WHERE id=@id;" Now, the question becomes... how the heck do I get that **@id** replaced? You do it at the time you're creating the **Command** object. using ( Command = Connection.CreateCommand() ) { Command.CommandText = safeSQL; foreach( string item in Request.QueryString ) Command.Parameters.Add( item, Request.QueryString[ item ] ); // create DataAdapter and fill a data set here } Using DbParameters, you're letting POCOs do the job they were designed to do. 
You haven't said whether you care about free or not. If you don't mind paying, there's VistaDB from Gibraltar. It supports EF supposedly, and is a single file deploy with your app.
Will you at least disclose to your users that you're basically revealing all of their data?
This was my thought - using filters to grab and sanitise any input before it gets anywhere near the code. Obviously it's not as good as re-writing every bit of code, but as OP states that's not going to happen. A very quick-and-dirty filter to look for anything too dodgy and returning an HTTP 500 instead would do the job here, if done carefully. 
Here is the second part http://www.newventuresoftware.com/blog/practical-analysis-new-features-c-6-part-2/