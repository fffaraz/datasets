But it's basically a wrapper for GTK#. Seems like another version of XWT which wasn't very successful either. I'd rather just use WPF on Windows, Xamarin.Mac (which is actually Cocoa) for Mac and then GTK# only for Linux (which I don't actually care about). If you switched out your Mac implantation to use native controls rather than GTK# I'd be interested. 
Another unnecessary abstraction. AutoMapper makes it looks like you're achieving more than you actually are. So many historical problems with misuse and misconfiguration. Every major Corp I've worked at has stripped it out for manual maps.
Grk# is horrible. I can't recommend it to anyone. 
.Net core is for server or headless workloads. There is no UI story.
Do you have the Kindle app on your android? If not, installing it and syncing it to your Amazon account should do the trick.
I think the saying is, there are tools that people bitch about, and there are tools that nobody uses. Automapper is definitely the former, and its justified... In many cases it turns out worse than the problem it solves. But, that aside, people hate it because it's everywhere.
It is horrible, but for a while (pre WPF) it was a good option, way better than winforms.
&gt; It also spelled out every field regardless of there being any special mapping associated. Hehe - doing it wrong. AutoMapper Manual Mapping Mode?
If you're doing a lot of mapping, it (or any of its competitors) can be a big time saver. I used to manually map entities to DTOs to ViewModel types, and that was complete waste of keystrokes. Whether you use AutoMapper or one if its competitors, take some time to learn the pitfalls to avoid.
You have fail backs, but can you write to multiple sources simultaneously? Can you tweak the level of logging on the fly at the config file? Can you say email all error/fatal events immediately? Sure, you could write that into your custom class, but why reinvent the wheel? Pull down the nuget, set up the config, and go.
I see divided opinions out there, which is obvious I guess. Thank you all for your input.
You're (or they're) doing it wrong. A better way is to write an extension method for each mapping for a particular class in a static class and put the mappings in a static constructor in that class. Something like: static class OrderMaps { static OrderMaps() { Mapper.CreateMap&lt;Order, PostedOrderDto&gt;(); ... other mappings involving Order ... AutoMapper.AssertConfigurationIsValid(); } public static PostedOrderDto ToPostedOrderDto(this Order source) { return Mapper.Map&lt;PostedOrderDto&gt;(source); } ... other methods for mapping to/from Order ... } Sprinkling your entire code base with calls to `Mapper.Map()` is pretty bad. This way when you come across `var posted = someOrder.ToPostedOrderDto();` you can just F12 in to the extension method and see the mapping. Also decreases your coupling to AutoMapper, DRYs up the rest of your code, and doesn't violate SRP.
I personally think that Log4Net is such a widely used and tested framework. Why not take advantage of it? If you are concerned abut swapping logging frameworks, you could create a class that implements an abstraction (ILogger) and implement your SimpleLogger that can later be swapped for another implementation using Log4Net. 
If the mapping between your service models and view models is relatively straightforward most of the time, then automapper is the way to go. But if there is a lot of manipulation of service model to viewmodel conversion then automapper will be an overkill since you have the write that conversion logic anyway. I have used both manual mapping and automapper. Like automapper as my current project has mostly direct mappings.
I'm paid to write code to solve problems. There are a multitude of logging frameworks for C# that handle logging exceedingly well. Why waste time solving an already solved problem?
Why? If you don't want to use log4net then use the built in trace logging which is part of .NET. Re-inventing the wheel is such a huge waste of time. 
&gt; modernity of the language itself which is less verbose or more modern if we cound the latest versions of each ... what's the main differences between the 2 .net is significantly ahead in this regard. Java just got lambda, .net got it in 2007. Java is about 4 years behind .net in terms of framework features. You will also find yourself going outside the core libraries far more frequently with Java. Java is a more verbose languge. In c# we can do; public string Property { get; set; } while on java you have to do; private String property; public String getProperty() { return property; } public void setProperty(String value) { property=value; } There are parts of java's core libraries that are awful, their Date implementation is terrible and you usually end up using a third party library to get .net style DateTime instead. VS is easier to work with then eclipse, runs significantly faster and much of the tooling is much easier to work with (eg nuget vs maven). Also language quirks like string1==string2 works with .net but with java you use equals, its frustrating switching between the two because they have such a similar syntax and its easy to slip between languages accidentally. &gt; which is faster ? On *nix platforms Java, .net (mono) is effectively as fast as Java for most benchmarks but Java destroys .net in a number of important benchmarks (notably regex), Windows .net has the edge in most benchmarks. Performance wise you are not going to notice differences unless you are building very CPU heavy applications.
I would say this. Abstract the mapping into an interface, at least, like IMap&lt;From, To&gt;, with a method like "To Map(From from)". Implement the mappings individually. In cases where it's straightforward, use automapper. If you find yourself creating a custom rule for more than one field, just implement the mapping manually and skip automapper.
Agreed. This is the correct answer. Like the request 2 years later on protecting new sensitive data when that role no longer has rights to the source object that you're writing from, you just change your custom mapper you've created. Simple, easy. Very straight forward when new team members come on board.
Asp.net MVC with entity framework. Right out of the box it has everything you need for these requirements. And its pretty much the "right way" according to M$ these days..
Not to hijack the thread, but can you explain what you mean by needing a good query caching mechanism? I've used EF a lot and never thought about that. Id love to know if I'm doing something wrong... :)
The correct answer is to not have separate models. Use one model for both and simply mark properties with DataMember where appropriate. 
I've always wondered why there are so many logging libraries for .net when the built in Trace and TraceListener classes seem to cover most use cases.
I've read about that. Was worried it might wind up being some sort of proprietary oddity like web server pages back in the vb6 activex days. Can it run on any host? Microsoft keeps mentioning it along with words like "office 365" and "azure." I'll just be running it on a regular old asp.NET host with sql hosting.
You are not alone. I was just thinking about asking this very same question today. 
How would you use an HTML helper?...
Good article explaining how to make a templated helper http://haacked.com/archive/2011/02/27/templated-razor-delegates.aspx/
Aside from the custom vs. framework debate, is anyone using nlog instead of log4net?
It sounds like your not doing anything log4net doesn't do, so i would use log4net. Having said that, I find the standard logging libraries to be way to simple a lot of the time. There's definitely value in creating your own logging framework.
Don't concern yourself with swapping logging frameworks if you're writing userland software. That's a waste of time and effort. You only need to worry about making your logging framework swappable if you're writing code for distribution as, say, a public NuGet package.
I will look into this. Thanks.!
My logger logs Form data, session data, post data, and the raw HTML via the response.filter ... (no json yet).... it logs crashes with line numbers and code sample if it's available.... I should open source it - no user at a financial institution can then say "That number was like that when the page opened." - we have the HTML sent out available to us. =D 
I guess the OP has no control over the service models and needs to consume them as provided. Otherwise I agree with you. Creating the VMs directly from the database would be the way to go. 
I know but it is not a bad thing. It is actually good that Java is trying to so things better. From his post it seems Java should not have great things like lambdas because C# had them first. 
I think mapping is a private implementation detail. There is really very seldom need for public methods for mapping objects. It's usually a one-hit thing done at each translation layer. So I usually just write a `private static Foo Map(Bar source)` which enables me to write `.Select(Map)` in most cases. You can F12 and step into the mapping code itself to see what is actually moved. Mapping code takes so little effort compared to everything else, so I don't really see AutoMapper as a very useful tool.
You stated reasons why you didn't like it, I pointed out that those reasons were due to poor implementation, and now you have different (but equally silly) reasons. If you don't want to use it then don't use it, but you've demonstrated that you don't know how to use it properly so nobody should give any weight to your opinion. Putting private mapping methods in every class that has to map from one type to another is going to lead to a lot of duplication in any non-trivial application. Saying that that's a worthwhile tradeoff because you can use method groups in Linq methods is ridiculous. Learn the SOLID principles and how what you're suggesting violates them.
Yes, I've used it in a few with great success
Especially since Mono will work to now use the official open sourced libraries where applicable, replacing their own code. Many parts of that project saw huge gains when Microsoft announced their project ([Mono 4.0](http://www.mono-project.com/docs/about-mono/releases/4.0.0/) and [Mono 4.2](http://www.mono-project.com/docs/about-mono/releases/4.2.0/) release notes -- see the MS source code sections). Mono seems to essentially become the ".NET Core + more" for cross-platform development. I have a feeling it'll remain like that. Anyway, the Mono project changes just the past few months can't be underestimated enough. The Collections namespace has been pretty much entirely replaced, the entirety of LINQ, Serialization, all cross-platform parts of System.Data, the ThreadPool, Tasks, the major IO.Stream classes, etc. As the migration now matures, you can more confidently than before use Mono for the cases when you need more completeness without nearly as many worries of "home brewn" solutions causing new headaches you didn't have before in the single platform world. I hope to see Mono become basically the official .NET with only added bits for parts that can't easily be made cross platform and extra care need to be taken, like UI. That would make it cognitively easy to picture when you might run into slightly less solid ground or not.
We picked nlog over log4net. I wish I could tell you why, I know I had a good reason for doing so, but I have forgotten since.
I can't speak for log4Net, but we use NLog and it's very flexible. It handles 99% of our use cases and for the 1% it doesn't, it's very easily extended. It's really well designed.
&gt; You stated reasons why you didn't like it, I pointed out that those reasons were due to poor implementation No, it still hides the mapping implementation, which is the entire point of AutoMapper. &gt; and now you have different (but equally silly) reasons More reasons to dislike a silly, almost pointless tool. &gt; If you don't want to use it then don't use it, but you've demonstrated that you don't know how to use it properly so nobody should give any weight to your opinion. That wasn't my code. I don't use automapper because it adds so incredibly little. &gt; Putting private mapping methods in every class that has to map from one type to another is going to lead to a lot of duplication in any non-trivial application. I am not an absolutist. I don't *always* put mapping in private methods. I use extension methods when *appropriate* which is *not all the time*. Most mapping happens once at the translation layer. If you map several places I would tend to think that the code is a little bit messy. &gt; Saying that that's a worthwhile tradeoff because you can use method groups in Linq methods is ridiculous You can use method groups with AutoMapper and extension methods as well, but it'll be longer. &gt; Learn the SOLID principles and how what you're suggesting violates them. This has NOTHING to do with SOLID, so stop being a condascending tool. On iOS platforms AutoMapper has a massive performance penalty as well since it cannot compile expressions and must resort to reflection.
Oh, tell me more about your experience here
We use Automapper extensively so that we can keep our separation cleaner. Mixing UI concerns into your domain objects is a surefire way to cause yourself problems later. By using Automapper, you allow yourself the flexibility to extend your view models later, or add behaviour into your domain models. Automapper exposes an interface called IMappingEngine which exposes all the mapping methods. Wherever we want to handle mappings, we inject the IMappingEngine and call the mapping methods from that. This allows us to keep our tests simpler as we can mock the interface. We then test our model mappings separately When you're dealing with a simple domain model and a simple view model, using Automapper can seem like overkill, but in my experience, these models rarely stay simple for long, and Automapper gives you flexibility and freedom There is an interesting article in the other thread (http://www.uglybugger.org/software/post/friends_dont_let_friends_use_automapper) about why not to use Automapper, but the first comment (from Jimmy Bogard - the guy that write Automapper) refutes or points out the errors in most of the complaints. The biggest thing is it's designed to convert out from the data layer to the UI - not the other way around. Automapper isn't designed to write to objects with behaviour in them
Look at Serilog. It logs structured data.
&gt;There's definitely value in creating your own logging framework You're the devil. No, there isn't. everyone who has used your logging framework agrees.
The class that writes to a file in log4net is 1000+ lines by itself. It also writes to the log file fine if you're tailing the log file. There's a lot of work that went into it. Don't reinvent the wheel, use good existing code.
Yes, but what exactly are you trying to achieve by sticking to OCP and LSP in this particular case? Principles and best practices are a means to an end, not an end in themselves. You shouldn't do SOLID just for the sake of doing SOLID, but because in the end of the day it either lowers costs or delivers business value in the context in which it is applied.
ELMAH is about as comprehensive as web logging needs to be, it comes with a lovely re-creation of the yellow screen of death, highly recommended. I personally find it better and easier to use than the failed request logging, which has a wealth of information.
We host an enterprise web application for a company. In production we turn off the debug responses. But due to some weirdness with the load balancer in production, we were getting an error we couldn't figure out. By turning the logging level down to get the debug info, we were able to pass it back to the network guys and tell them to fix their $h!t. I'm not saying it's something that happens every day, but having the flexibility to modify a single flat text file rather than pushing any new code or dlls is nice. And you have syntax errors in an XML file? Do you have code review processes to check diffs before production releases? Do you have a release manager who is in charge of making sure those things work?
This.
In a web based application, distributed across multiple web servers, with hundreds of users across the country, and I want the user to be able to pick the level of verbosity that's logged? Especially when I already have to deal with PCI-DSS and PA-DSS requirements around logging and activity tracking?
You catch form data, session data, posted data, and the raw html? Do you scrub out sensitive information before putting that anywhere?
We do. If there is an issue, uncomment the appender configured for debugging, send us the log.
we do, but it's part of the deployment process. we actually do quite a bit with serilog. In dev we write Debug level statements to Debug output (pretty handy with VS2015) and a local instance of Seq. Debug level statements include stuff like SQL statements and some pretty hefty messages that aren't really needed in prod unless we've done something horrifically wrong On deployment we use octopus deploy to adjust the config file. Point serilog to only our prod instance of seq and adjust the logging level down to info. Works well.
ok, thanks! I've been intending to add some logging functionality to a web app for a while now and have looked at a few things but never got serious about implementing. yet.
I think the best case is that they'll think "He properly configured log4net to provide this information" until they get into the code and realize that they have to use your framework which doesn't have the flexibility or documentation that existing options do. At that point they'll say "Oh, god, somebody here made a custom logging framework... again." I've rolled my own logging framework. Everyone has rolled their own logging framework. And, pretty much universally the only people who have appreciated that were the people who wrote it. Everyone else wishes they'd used something standard. It's not a reflection on your ability, or even the quality of the product you produce, it's about usability. Now, everyone is locked into your product. You missed a feature? Now they don't have it, or you need to develop it, or they need to. There's a bug? It's on you, or them. How many of those hours are you actually saving with your enhancements over something off-the-shelf?
Agreed. Came here to see if anyone mentioned .NET's built-in trace logging.
I can say, at least, that I wouldn't hate you for just using TraceListener. Then again, if your implementation were more than 100 lines of code, I'd start hating you again.
[SOLID](https://en.wikipedia.org/wiki/SOLID_(object-oriented_design\)) is the single-use/responsibility guide for new programmers/developers to learn. Experienced developers already know (or should have learned) it, but not as 'SOLID' That said, I've never heard of OCP or LSP before today. Conveniently, those are also listed in the wikipedia article too. **edit:** accidentally a ) in link.
What if it's a Windows Service running on someone's PC? No user interface, no login.
Dependency. Sometimes people go overboard pulling in so many additional references and dlls that you bolat the application because you didn't want to spend 30 minutes to an hour writing some code. Reinventing the wheel is bad but you don't need a 24" chrome alloy wheel with lighted spinners when a simple steel wheel will do.
OCP = Open Close Principle LSP = Liskov Substitution Principle
The OP is unsure whether he wants to use a custom log framework or Log4Net. By implementing an abstraction on top of whatever he want to use, he can come at a later time and swap the implementation using dependency injection, for example.
I like your idea :)
nothing special. Our systems under test take in an IRepository&lt;whatever&gt; and IUnitOfWork so we just fake them. I have a ReadOnlyQuery() method on the IRepo that we fake with the dummy data and then assert that Commit was called on UnitOfWork at the end of the process. More related to cleanish design of the domain objects than anything fancy with the testing. The only time this doesn't work that well is on tables with a f-ing identity column that we need to retrieve to complete the work. Ugh. We do have a separate set of integration tests that use an owin self-hosted instance of our entire web app wired up that will run a full end-to-end testing too. Between the two of them I feel pretty good about the coverage
Mapping is a translation layers responsibility. That's what they do.
If your mapping logic is in private methods then that class must be doing more than just mapping. Can you give a concrete example of what you're talking about?
Do you personally see a specific advantage for your current project? All too often people chase after theoretical benefits that never actually materialize in their project. Generally the best time to add an abstraction layer is when working without it becomes too painful. By waiting, you'll have a better idea of what kind of abstraction you really need.
I don't recommend using dependency injection to create your loggers. The correct way to create loggers is to have one static logger per class, named after the class which uses it. For example: namespace Foo.Bar { class Baz { private static readonly ILog log = LogManager.GetLogger("Foo.Bar.Baz"); There's a very good reason why it's done this way: this approach allows you to enable or disable logging on a per-class or per-namespace basis. Depending on your IOC container, it may not be possible to get the type of the class into which you are injecting your logger, so you can't do it this way at all. Additionally, using dependency injection to create your loggers means that you are only able to do logging in places where you are actually using dependency injection -- it won't be available to helper classes, extension methods and so on, for instance. Besides, you're also making your IOC container do unnecessary work in resolving your loggers with every request. Swapping out your logging framework is something that you'll do once in the entire lifetime of your project, if that. If that's the case, you're better off just rolling your sleeves up and going through your entire codebase replacing one with the other. You're certainly not going to do it at runtime -- if you are, then you're over-engineering something or other. Nor are you going to need to mock your loggers either -- you should be able to switch logging on or off in your unit tests exactly the same way as you would in production, and if doing so causes side effects in your tests then you're obviously doing something very esoteric with your logging *and* getting it wrong. Besides, if someone is implementing their own logging framework, it's a clear warning sign that they haven't understood logging properly, and the chances are somewhere in the region of 100% that they'll end up implementing an abstraction that only supports their original choice and not the alternative.
Unless you are the first one. You are correct though. Personally I don't use ef much. I have friends who do and they swear by caching to prevent this kind of thing.
Another very good article about the subject: http://www.reflectivesoftware.com/2015/07/26/unit-of-work-entity-framework-unity-ddd-hexagonal-onion/
&gt;I wrote this a while ago: https://shelakel.co.za/entity-framework-repository-pattern/ ContinueWith pretty much should not be used. &gt;In my opinion using EF is an architectural decision, not an implementation detail. You'd be better off hiding implementation detail using a service pattern. EF is a pure detail. Always hide it with a "service pattern". EF is no different than sql data adapter and a datatable (when it comes to designing a system).
I'm already running a business in the specific industry area and understand it inside out. It's not mine though. But there's a gap and an opportunity ready to exploit and one of the the tools to do so is faster and efficient turnaround on deliver of the services - ie reports required by law. A browser-based app that can do this is the key. Now it's just a matter of developing it. And someone's gotta do it
Using AutoMapper forces you into a habit of having classes for individual tasks and allows you to have much cleaner, more reusable code.
from msdn: https://i-msdn.sec.s-msft.com/dynimg/IC793846.jpeg
[This exists](https://msdn.microsoft.com/en-us/library/hh510202.aspx)
Well, is there a (higher than just you) need for the application? If yes, I'll develop it, find other customers and make money on selling them the app and give you a copy for free.
That looks awesome, thanks for the link!
No problem. The developer is active and responds to questions pretty fast, but if you're stuck on something I'd be willing to help out also.
Maybe [LiteDB](https://github.com/mbdavid/LiteDB). Single file NoSQL document db written in C#. I haven't used it yet, but it's on my list of things to try.
I guess there is but some would potentially be my competitors and by supplying them the app, it will kind of give them the edge that I'm planning on having by using the app. Edge = faster turnarounds on reporting, efficiency = low prices for services = more market share. 
Set classes will be very slow. I think that you will find that de-duplication can often be deferred to the last minute, and many operations guarantee non-duplicated outputs given non-duplicated inputs. You will probably be best off using lists, since you seldom will know the size of a relation prior to requiring memory allocation. What are you hoping to accomplish with your library? Is it syntactic sugar on top of linq-to-objects? Are you planning to build a relation optimization transformation layer? Will you be using relational algebra as an intermediate syntax? This is a tricky path to walk and you will have to make some sacrifices along the way. 
Just yesterday I found out about Firebird. It can be embedded and I haven't fully explored it yet but it seems to work with entity framework. 
well, that was worth starting a thread
Use Dapper instead of EF.
MVC (as well as ASP and EF) has a roadmap. So there is a release date in sight-ish
The first time I tried MVC6 I ran into the same EF issue. It was really hard trying to get off the ground without database migrations (DropCreateWhenModelChanges). The second time I tried MVC6 my project was a little more complex, an WebApi with a Xamarin Client. That failed because they have a new class library project type that is not PCL compliant. I could not share my model between projects (except by linking, I guess). I see mentions of Dapper. Dapper is really slick, but EF7 is going to be supporting new data providers (Mongo, ect). That`s a pretty cool feature that I would like to include. Anyhow, Im sticking with a slimmed down MVC 5 for now. Ive decided to keep most of my code in Web Api for ease of transfer and I am using a custom membership / session provider so I don't need to worry about any Identity legacy issues (Identity is much better than before, but, it is still a lot of code and not exactly a WebAPI (which I do) first class citizen.
Q1 2016 last I looked.
https://github.com/aspnet/Home/wiki/Project.json-file you can specify build scripts eg "scripts": { "prebuild": "prebuild.sh" }
Why would you have to port to MVC6 if your app works just fine in MVC5? People get too obsessed with having the latest version number for no real reason. 
Based on your simple requirements, my immediate thought was just regular ADO.Net datasets. You can keep them disconnected and 100% in memory and save/read the entire set to xml. It's not glamorous and has fallen out of fashion in recent years, but it is build right into .net and fits the bill. 
Is ASP.NET session state server still not supported in MVC6?
Use kataina/owin 
I'll move heaven and hell to get rid of fucking csproj files and if that means porting everything to MVC 6 then so be it. We started late last year on this project with MVC 5 but designing to ease the transition. 
I plan to build a relational language, in the spirit of brink back the database development of tools like Visual FoxPro/Acces for the same targets (LOP apps, data manipulation, etc). However, I don't want to limit all of this rely in a single DB engine (the big sin of Acces/Fox)... that is way I think the relational algebra will be a great foundation to build on. So the relational aspect will be mostly about: In-memory data manipulation and materialize information from data sources (like Sql Engines and text files)... so yeah.. like linq ;) 
I personally prefer this approach. Almost always faster than ef too.
Order is not guaranteed in SQL, oftentines the results tend to sort themselves by a clustered index when present, but there is no guarantee and order by clauses that are not part of the outermost query can get ignored. Hashsets require an effective hashing function which could be a challenge if you want the user to be able to load any type of data. I don't think I clearly understand what you are trying to do so I cannot give much more guidance
Try the empty mvc project. Ms likes to push their stuff on you. I would suggest using the identity stuff though. Edit: check out Microsoft virtual academy for classes.
MVC5 is not available for owin, so you're implicit telling him to use MVC6. Something that is still in a preview, has no go-live production and is still in production for at least another half year.
Mvc 6 is not asp.net 5 Mvc 6 is beta whereas asp.net 5 is in preview release
This is the only right answer. Never use beta stuff for production software. If it's a personal project you might as well learn what's coming sooner rather than later. Beta 7 is workable just fine. It will take a couple of days to adjust, but it'll be worth it! Only downside is that not everything is fully implemented/ported yet.
Maybe you should wonder if session state is the way to go :p I try to stay away from it as much as I can. Session state is one of the things that makes it hard to scale out an app.
5 any day. 6 is still too unsupported for good use IMO. 5 will probably serve all your needs to wait for 7 to port or even 8, depending on the project. 
&gt; Exclude files from the App_Data folder No, but I don't see that option either: http://i.imgur.com/nG1jw44.png maybe I should select "All files" on Items to deploy. On the first deployment I want to upload legacy data, and from then just update files and features leaving the data intact.
It's in the wizard which appears when you right click your project in solution explorer and click publish.
Is similar to http://www.try-alf.org/blog/2013-10-21-relations-as-first-class-citizen, a language where relations are the dominant aspect of programming. Also, similar to Visual FoxPro, where you build full end-to-end applications with a language that fit well with databases. Apart of the dbase family, the array languages like J and more exactly kdb+ are part of the same pack, where operations in groups are easy, normal, performant, idiomatic and the *default*. Pandas (python), Matlab, Julia are a bit closer too. Check this http://c2.com/cgi/wiki?CollectionOrientedProgramming Other way to see this is imagine if PostgreSQL was complete enough to, only using the integrated sql dialect, to build everything, including the GUI, Web apps, mobile apps, etc. Don't think this is *crazy*... this was true with the dbase family! 
It depends on how your printer is connected... If it's a network connected printer, you can open up a raw TCP connection to it, and pretty much dump the ZPL to the printer. This would be the way I'd suggest. But the server will need to be able connect to the printer you want to print to. This is the most scalable solution and what I'd use in a warehouse environment where you have a bunch of printers. However, if it's connected to the local machine via USB, you use this approach to print to the printer: https://support.microsoft.com/en-us/kb/322091 This will let you print in raw format to any printer windows can find. So it handles network shares, etc correctly. I've used that way before, and it worked well, but it requires a fair bit of fighting during deployment. That will let you print on the server. If you want to get it to print on the end-user's machine... I've written an ActiveX shim to do that, and had the users use IE, but I wouldn't say it'd be my first choice. For working with the label... I like JakDrako's solution.
Don't try to make your data layer technology swappable. It's far more complicated than just slapping IRepository and IUnitOfWork interfaces round Entity Framework -- you have to take into account differences in behaviour as well, and these can be very subtle and complex. In any case, on the rare occasions when you do have to swap out your data access technology, it will almost certainly be for something you never even knew existed, and your neat Repository will not help you. Besides, the idea that you can introduce a separation of concerns between your business layer and your data access layer at all is a complete fantasy. If you want to try, you first need to categorise your concerns as either business concerns or data access concerns, and herein lies the problem: on all but the simplest applications, most of your concerns fall into both categories. Take queries for example. Queries are business concerns by definition. Paging is a business concern. Sorting is a business concern. Any kind of aggregation is a business concern. Simply because these are business rules. However, unless you have a generic repository and your queries are very simple (like, against a single table), they will also be a data access concern. You can tell that something is a data access concern if there is any possibility that your mock objects will behave differently from a real database. Case in point: calls to `.Include()`. On mock objects, this will either be a no-op or else throw an exception, depending on how you're mocking it. On the other hand, against a real database, it changes the shape of the data that gets returned by Entity Framework. Another case in point: there are lots of queries that Entity Framework can't convert to SQL, so they throw `NotSupportedException`. On the other hand, with mock objects they behave as you would expect them to behave. The upshot of this is that you can't treat queries as either solely business concerns or solely data access concerns. You have to treat them as both. If you treat them as solely data access concerns, you'll get an anaemic business layer and (if you're adhering to the so-called "best practice" that your tests should never hit the database) poor test coverage. If you put them in the business layer, you'll end up with poor performance, excessive code duplication, and tests that pass when they should be failing.
I'm already way past the learning curve for EF =). I know how the database works quite well too. I'm also fairly confident that proper use of EF can be quite fast and efficient. For me, the main advantage in EF is how speedy the development becomes. It's just a pleasure to work with once you know what you're doing, in my experience. C# is among the best languages out there. Doing DB interactions with LINQ is just too comfortable. The only issue I have with EF currently is relevant to this post.. An ORM is supposed to make it easy to swap database technologies. I've seen a few posts about using EF with Postgresql, but I'm not sure if this is viable in production. 
I just don't want that limit, and am wondering whether or not I can avoid it. Why have it, when there are other perfectly good DBMS products? If there is a good reason, like incompatibility, then I will opt for mssql. But if not, then Postgresql is clearly a better choice, as it isn't limited in such a significant way. I haven't estimated how much storage I'd need, but I've worked with databases much larger than 10GB. The project involves scraping a whole bunch of data, and will continue to scrape/add data as the web app is up.
&gt; Why have it, when there are other perfectly good RDBMS products? Perfectly good? No, not perfectly good. For example, the query optimizer in Postgres is no where near as good as the one in SQL Server. So you are going to spend more time hand-optimizing queries. 
Were you using lazy loading? And anyway, I am considering Dapper. It just seems like it would make for a lot more work, with less elegant code. I guess I'll give it a go though. 
True. Plus, MSSQL Management Studio and other tooling for mssql is light years ahead of anything I've seen for Postresql. Would you say that MSSQL is better that Postres always, for everything though? It seems like for my small-ish projects, where I'm not looking to spend thousands of dollars or deal with the behemoth that is MSSQL, a free thing like Postres might be a better choice. I might be wrong of course. 
&gt; Would you say that MSSQL is better that Postres always, for everything though? Nope. If you need cross-platform, MSSQL is a non-starter. And if your database is larger than 1 GB, but you want it to fit into RAM without spending any money, MSSQL Express won't work. But lets be honest here. If your database is so small that it runs on your laptop, it really doesn't matter what you pick. Pretty much anything will get the job done, even a shitty NoSQL database like MongoDB.
I feel for my project, Postgres will get the job done. It is likely that the DB will grow past 10GB though. If I can get most of the functionality I had with EF, but with Postresql, that would suit my needs perfectly. So, basic CRUD, some sprocs, not a very huge schema, but sizeable. Postgres is powerful enough, and only comes in the [awesome edition](http://blog.codinghorror.com/oh-you-wanted-awesome-edition/).
&gt; Are there other ORM/DB pairs you would suggest? 
My current major project is an MVC 5 application backed by Postgres and I'm using EF6 code first with migrations. I've been working on this for about 10 months now and after jumping through a couple of initial hoops it is rock stable and does exactly what I want it to. And really, why shouldn't it? EF is meant to abstract away the data storage implementation, and that means the implementation isn't supposed to matter, right? First, there seems to be a little misconception in the comments about Postgres and "powerful enough." Postgres is not MySQL. It is the real deal and is made by database people for database people. It is powerful enough to run anything you want. Skype is backed by Postgres. That out of the way, damn near everything on the web that is written for or about EF is presented with the assumption that it is working with MSSQL. This gives the impression that it won't work with other data storage systems, or that it is somehow difficult. That isn't the case. In fact, the source of the "hoops" I mentioned earlier is actually the process of excising the assumption of MSSQL. So, the big hoop, which you only have to solve once, is getting your Web.Config correct, and making sure that your app is talking to Postgres instead of trying to find an MSSQL server that doesn't exist. Another hoop is the different conventions used with Postgres. Table and column names are snake_case. Nothing is ever pluralized. Capital letters are a no-no. And who the hell is "dbo," anyway? I wrote a few helpers that I plug into the model builder and the configuration classes that handle these things for me. There are two main EF providers for Postgres, NPGSQL (free), and dotConnect (paid, not too expensive). I use NPGSQL myself, but have heard dotConnect is very good. There are a couple of things NPGSQL cannot do on migrations, and you will have to edit your migrations manually to deal with that every now and then. Anyway, if you have any specific questions ask away and I'll be happy to provide any help I can. 
/u/concubicycle - i replied to the wrong comment, so I'm just adding your name rather than try to move a comment around. I would make the argument that if you are not using your database to store binary (i.e. files and pictures - there is plenty of discussion on this, however, I fall into the files don't belong in a RDBS camp if you can help it - full disclosure), that 10 GB limit is most likely a long way away. Additionally, it sounds like you are prototyping somewhat. You should mostly be concerned with proving the app works in the least amount of effort possible. Changing your data layer, if properly designed (you have a data layer) would be pretty simple if the need arises. If/When your app becomes successful enough that you are hosting &gt;10GB in your database, you are going to most likely be looking for ways to scale up your infrastructure (azure, amazon EC2, etc) which means you won't be licensing SQL directly, just renting space. Things to think about.
Consider trying something like RavenDB where you don't need the ORM. Also did you check out MariaDB. It's pretty good and a solid improvement over MySQL.
MVC (Model View Controller) - With this pattern you have Models which are basic objects with no code just properties, View's which are your presentation items (HTML, WinForms, etc) and Controllers which do the logic. In this pattern all the logic is done through the Controllers which typically do something with a model, create a view and then hand that view off. Examples: ASP.NET MVC, Angular MVVM (Model View ViewModel) - Here we have the Model and which are the same as in MVC but here we have ViewModels which are passed to the view and all the logic is in the ViewModel and there is no controller. Example: Knockout.js The difference is basically where the logic is sitting. in MVC you might have a PersonController with an Action DeletePerson that creates a person. Where in MVVM your DeletePerson would be called off of your view model. MVC is typically used when things are transactional and disconnected as is the case with server side web. in ASP MVC we send the view through the wire and the transaction with the client is over. Where as in Angular we're on the client side so we can hold on to objects and do a lot more logic in a non disconnected state. If you do programming for things that run on the device you see more MVVM. MVC is more of a web server/client pattern edit: Had my examples mixed up Angular is a client Side MVC library not an MVVM, Knockout is more MVVM 
Sorry if this question is stupid, but it is somewhat relevant: Afaik postgres is far from well-performing on Windows. So how would you recommend hosting a website I've made for fun (meaning that my budget is limited). Will having separate Linux (DB) and Windows (middle tier) dedicated servers from different providers mean increased latency? Again, sorry if I'm hijacking, feel free to downvote if you believe this is wrong...
Thank you so much! This gives me confidence. 
Thanks a ton!!
Simplified, I would consider MVVM for desktop apps and MVC For web apps.
&gt; Also did you check out MariaDB. It's pretty good and a solid improvement over MySQL. It's pretty much an exact clone minus the Oracle branding. Maybe a tiny improvement, but it doesn't fix any of MySQL's major flaws.
&gt; NHibernate No... please let this piece of shit die. 
Aside from generic "ORMs suck" type of issues, what's wrong with NHibernate?
That reminds me of Hibernate for Java. I was taking over from someone who was struggling with it for months. I took one look at that shit, ripped it out, and had a working WS/SOAP server using stored procs and JDBC in about a week.
&gt; Running your application without a database allows you to test every aspect of your application and achieve full code coverage with thousands of unit test. Ha! It isn't full coverage if you are lighting up the pathways that handle SQL generation in EF. Lots of subtle bugs can hide there.
It doesn't. Every request authenticates again. 
Magnificent reply, thanks for that :-)
OK sure -- apologies if I misunderstood you. I hope that my post does provide some useful additional clarification for the reader though.
In Visual Studio 2015, put a breakpoint before and after your method. It will tell you on the screen how long your code took to run, pretty damn cool!
Cool, octopus rocks. We love it and have been using it for a few years now
Check out effort for ef testing
I've got lucky - my company's using MVVM-C ! No ViewModel logic, everything in the controller. Takes some getting used to, but it's *neat*.
NTLM or Kerebos, Google them.
Indeed. The diagnostic tools in VS2015 are pretty sweet.
Everything is "out of stock" apart from the $100 credit - I guess that means they are all gone?
&gt; With this pattern you have Models which are basic objects with no code just properties I know this is common, and I do this myself, but in my mind this strongly contradicts MVC. The model is responsible for everything data. If you want to update the database, the controller should do it thought the model. If you want to retrieve data from the database, it should be retrieved from the model. However the common usage is that the controller populates the model with data from the database, massively increasing the controllers responsibilities. Should it? Couldn't the controller just create the model using a data repository, rather than handling this by itself? From the description of MVC it seems a lot like it should work this way at least.
Agreed. GP is accidentally promoting Anemic Domain Model anti-pattern and a return to Transaction Script. Controllers exist to coordinate between model and view, translating inputs into commands appropriate for the model and shuffling information back from the model to the resulting view. That's it. As for MVVM: it's what happened when authors of basic forms-over-data apps fell head-over-heels in love with two-way binding, forgot that Presentation Model was already a thing ;), and smashed their reinvention of it into MVC. It is suitable for certain types of apps, and they are actually pretty common, but it is definitely unsuitable for many applications (eg arguably those doing any more than CRUD) and is in no way a silver bullet. An architectural style called MVVM has been optimized entirely around the magic salve known as two-way data binding. It really has. If you were watching this over the last decade it was easy to see the progression. The problem is that two-way binding has a habit of falling down as projects get more complex, and when it falls down it falls down _hard_, and when your architecture is optimized around that not happening then all hell breaks loose. What happens then? A re-architecture back towards MVC extended with an added Presentation Model that interfaces with the binding mechanism.
You know that line has been a point of debate with me and colleagues for a while. The pure object guys argue that passing POCO's to services is not Object Oriented and that in an object world the actions should be on the objects. I kind of agree with you though, When I see stuff like that I think legacy. 
Yeah I think your 100% correct. I worked on a project where there was very little to no logic in the controllers (I think just validation stuff) and the data repo and business layer repo did all the work. Made for a very clean architecture once you understood the layers. Problem was some of the less experienced devs kept screwing it up
Imho, there shouldn't be any logic in your controllers. Just a call to business/domain/service layer. The testing should be done on those layers.
What do i **unit** test in an MVC controller? Absolutely nothing. Controllers are route drivers. It's pointless to test routing in a unit test. Controllers are tested by full stack automation invoking a browser or sending direct http requests.
A controller action rarely (if ever) should return different views. 
According to whom?
The entire MVC pattern. Use different routes for different views. The only acceptable situtation i see is having an action that returns the normal view to a web browser and returning a partial-html view to an ajax request. Of course i would likely solve this with an aspect and not if()return view else return partial-view.
Can you explain? That shouldn't happen and is a sign of bad design most likely.
:) Just caught that, go go gadget proof reading skills
You haven't addressed his original question.
Re-implement the "what" not the "how". You say it's some crud / text processing thing? Make sure you've got a bulletproof set of test cases first - and satisfy those. You could probably try to literally translate the majority of the logic as a first pass, then substitute in fresh implementations of system calls.
&gt; If you don't know exactly what this application is designed to accomplish, down to a pretty fine level of detail, it is very unlikely you'll ever make a suitable replacement. And don't forget the inevitable bugs in the original which people have developed workarounds for or adjustments in downstream systems to compensate for.
&gt; One thing you could try is compile the application using C++/CLI Without knowing any of the details that is where I would start.
That's a fair point. Looking over all the `.h` files, I've got the following classes: UnitMonthYear - Comments say it is basically a light-weight Date kind of object, except it has no need for `Day`-related functionality. Just month / year. AdoFunctions - only has 2 methods, OpenDataSet and ExecuteAdoCommand. Pretty self explanatory. CheckMessage - assortment of values and enums CheckNessageList - seems to be a doubly linked list of CheckMessage StringChecks - validation for strings pulled from user-supplied files UnitMain - bulk of application functionality exists here. Logging and file processing. It really is very small, but again it's mostly just the actual layout of separating functionality I'm fuzzy on.
Thank you for your reply, this was my #1 consideration for my website, too, and I guess this is what I'll go for at the end... Still curious as to what people use at this stage...
So, in most of the examples of POST actions that I have seen online, there usually is at least one IF statement looking at whether the posted model was valid. If it is, then great, do your thing. If not, then re-display the view. Also, if the meat of your action (the call to the business functions) raises an exception, shouldn't you capture/handle it in the controller action? I've seen people say its bad to have logic, but on POST actions, I can't see how people are avoiding it entirely.
 public ActionResult Create(MyModel myModel) { if (_myModelRepository.Insert(myModel)) return RedirectToAction("Index"); else return View(); } ^ The above is a pretty typical template for all the create/update/delete actions, at least in the controllers VS generates. How are you saying it should be handled?
WPF is way more flexible. Also, writing XAML by hand is much faster and efficient. It's pretty powerful as well. You can do a lot of stuff in XAML without writing any code. I would definitely recommend using WPF rather than WinForms.
&gt; Maybe it's just because it's unfamiliar to me but I really don't see any advantages of it. Yes. There, I answered it for you. With knowing WPF and MVVM, applications GUIs are so much nicer written than with WinForms in my opinion as the view logic and the presentation of it get splitten up more.
&gt; Never use beta stuff for production software. I'm considering using MVC6 for my production software, although it is waterfall and won't be done for at least 8 months.
There are headers, WWW-Authenticate: http://blogs.msdn.com/b/benjaminperkins/archive/2011/09/14/iis-integrated-windows-authentication-with-negotiate.aspx 
Winforms intertwines too much with your logic code for your application and updating layouts can be a major pain in the ass compared to WPF/XAML. I would know; I use Winforms primarily. I wish I were better with other two.
I avoid it by not using the built in views and instead using a js framework like angular. I've been doing that so long I forgot about this kind of stuff. I guess you could have some simple tests for the controllers but it won't be getting you much. You are only testing model.isvalid(). There is no need to unit test .net. many people did that before you. Edit: forgot about the error handling question. No you should not catch errors in the controller. I don't see how it is possible to get an error on the call from controller action to a dll. If that did happen there is nothing the controller can do about it. You should have a custom error page setup and use something like elmah or log4net to catch and redirect unhandled exceptions.
In this case you are testing .net code. You shouldn't worry about that.
Perhaps Azure? You can use a linux VM for the database and use their web stack, so the lag should be non-existent between them (assuming same datacenter). Obviously for a "fun" project you might be limited by either having an MSDN subscription or whatever is available on the free tier (web app fine, linux VM probably not).
&gt; The problem is that two-way binding has a habit of falling down as projects get more complex, and when it falls down it falls down hard, and when your architecture is optimized around that not happening then all hell breaks loose. What happens then? A re-architecture back towards MVC extended with an added Presentation Model that interfaces with the binding mechanism. Do you have any additional reading on this problem? We are beginning to get into a moderate complexity MVVM app, and this is a growing concern of mine.
It is a hard thing to accomplish. I would rather a dev write too many, or "silly" tests than none or too few.
&gt; You didn't answer anything. I'm pretty sure he meant that the "Yes" to you saying "Maybe it's just because it's unfamiliar to me" is his answer.
If it's easier to break down into pieces I would compile the functions into a managed .dll and then write the main application in C#. From there just import/rewrite the functions as you go, but that's just my naive thinking.
Unfortunately I can't point you towards anything specific off-hand. (Incoming wall of text, or something approaching it.) What I am tempted to suggest is that you take a brief step back to consider some nearby architectural alternatives and how you might go about ensuring your system could adopt them opportunistically when and where MVVM falls down. Map out how a part of your system might have looked pre-binding magic (eg. Controller reaching into Model, pulling information out, and shoving it (potentially after minor reshaping) into View. Controller hooking event (or otherwise receiving input) from View and shoving it (again potentially after minor reshaping) into Model.) Capture that conceptual model somewhere for a moment and then... Map out how that part of your system might then be modified to take advantage of some new-fangled data binding technology. A part of you will be tempted to bind to the existing Model types as they are, but the rest of you knows better. ;) And you certainly don't want to expose things from the Controller for binding just because the Controller is already there. You will likely then turn back towards the Model and find yourself really tempted to decorate the Model will more binding-friendly members. Nope, but Aha! You'll create a new type of model-like class which is fed (through any of many means) Model information but reshapes and re-types that information to be friendly to data binding technology! At this point you are looking at a reasonable interpretation of Presentation Model, depending on just how much of the smarts of this reshaping and relaying that it takes on. Stop and think of the different ways you might wish to shuffle the data between the Model and this "Presentation Model." Do the new types somehow consume Model types and extract their data? Do the controllers relay it through custom code? Does some other custom builder or mapper type do it? Do you try using some other non-UI data mapping technology? It was right around there where people started wondering "Can we do something simpler than this?" but forgot about (of at least de-emphasized) the "for some kinds of applications" part which should have followed. Anyway... You don't want to throw the baby out with the bath water. You want to look at each of the architectural styles suggested by the above stages of evolution, along with the MVVM style, and to then consider what additional abstractions and/or escape hatches you might want to put in place (or at least ensure that the path to them from where you are now is not cut off or prohibitively expensive) in your architecture so that you can use simpler MVVM styles where possible and have a clean (enough) way to step aside from that and use whichever of the other styles is most suitable for a given situation where MVVM isn't holding up. A few other things come to mind that you should ask yourself along the way, regardless of where you end up with various interpretations of MVC. A key one, in my opinion, relates to the shape of your domain versus the shape of data as a user might best interact with it. It is quite common, for example, for the creation and modification of data in the system to align closely with the shape of the domain model. It is, on the other hand, very common for various retrievals of data, particularly in sets and lists and tables and such, to not align at all well and to instead justify (or at least find a better sweet spot by going with) an entirely different shape of data. Or you might trip up and try to blindly use the domain model with naive ORM for everything and end up with horribly inefficient retrieval performance via the 1+N select problem. It's not the cleverness of the ORM tripping things up in this case, as much as people tend to think it is. The domain model often isn't the tool for the job here, and so neither is the ORM. More on that in a moment, but in short here enter terms like "view projection", "read model", etc. You will probably already find your considerations of the various architectural styles and related issues making you debate when and where to use the domain model at all, and what you might use in its place (to help facilitate binding, for example) if using a simpler mechanism to project more view-appropriate shapes of data directly off of more powerful and more performant database queries. When you find yourself at that point I strongly suggest a deep dive into the concepts of CQRS. Not because you should overhaul your architecture in that vein but rather because of elements worth borrowing or even just how learning about CQRS can change the way you think about solutions for various problems. Another suggestion, just because it came to mind as potentially relevant to your current considerations, is to familiarize yourself with Humble View pattern. It too may change how you look at solving some challenges and keep responsibilities organized within your solution space. I'm tired and have a hell of a day ahead tomorrow (after a few nutty days now behind me) so that's all that comes to mind for now. Hopefully it can be of some inspiration.
On the contrary, you do need to unit test your routes. Routes can be easy to get wrong, tricky to get right, and tricky to troubleshoot. Having said that, when you're unit testing your routes, you don't actually test the controller itself. You just test to make sure that the correct controller is invoked by the routing engine.
Quick rule of thumb when deciding whether to test something: ask the following questions: 1. Is it easy to get it wrong? 2. Is it tricky to get it right? 3. Is it non-obvious when you are getting it wrong? 4. Is it difficult to verify it manually? 5. Is it high-impact if you do get it wrong? If in doubt about the answers to any of the above questions, just translate "Not sure" into "yes."
&gt;GUIs are so much nicer written than with WinForms in my opinion as the view logic and the presentation of it get splitten up more. Not if you put some effort into doing it right. You can write pretty solid MVVM-style code with Winforms with the help of libraries such as [CSLA](http://www.cslanet.com/). I've done it. The question you rather should ask yourself is does the UI SDK of WPF give you user controls and presentation techniques that suits your needs better than those of Winforms? I'd say data-heavy apps with a lot of table presentation still are easier to write in Winforms due to the controls like datagrid and 3rd party controls like [GridView from Telerik](http://www.telerik.com/products/winforms/gridview.aspx). If you on the other hand need a sleek UI with animations and a more dynamic and flexible layout system, then maybe WPF is for you.
While to may not cover all the bases, have you looked at React?
This is the correct answer. Or if you are more comfortable with angular choose that. There is also a lightweight spa framework called Yo. https://github.com/jgauffin/griffin.yo 
I use tests on controllers to determine if the proper redirects are made after, say, a validation failure.
Most of this is only true if you know how WPF works. I'd argue it's a lot easier to make a complete mess of things with WPF than it is with WinForms, especially when you're only just getting into it.
This seems silly. Of course a redirect is going to work. You don't write tests against framework code. If you want to test your custom validation that is different. But that can and should be done outside the controller imho.
Hm! At first I thought React was just another way to imperatively update the state of the page, but upon closer inspection prompted by you, I see that you write classes that know how to render themselves and the framework takes care of deciding when and how to update the DOM. Interesting, I shall dig deeper!
What did you like about it? What other frameworks have you used in comparison that you liked better in Aurelia?
You can make a complete mess of things with pretty much anything if you don't know how it works. WinForms is no exception.
But how do I test if the redirect went to the right place?
I guess I don't have a good reason to NOT learn it, I just feel like either businesses are too slow to get with the times and will continue using win forms, or everything will just move tobthe web, so why bother. Figured I'd focus more on general application structure rather than actual technologies used.
+1 angular +1 knockout I've used both with ASP.Net MVC. I like Razor and binding as much as possible on initial page load. then use a VM to keep changes in sync without posting the whole page. little bit of jQuery to handle discrete posts and bam, it's tasty.
They don't, I imagine.
&gt; .NET Core is completely new written subset of the .NET framework Well, it's obviously not completely new. Almost all its code comes from the .NET Framework. It's a forked-off subset.
To be honest, I don't tend to use JS MVVM directly with MVC but rather with Web API. Some others in this discussion have mentioned performing an initial population of models in Razor to avoid loading an empty page and then performing the XHR to get the data, but I can't give advice since that's something that I haven't gotten around to doing yet.
You're probably going to get recommendations for each one that exists. I really like using Angular (v1) and Angular2, an especially TypeScript, looks like it's going to be amazing. I've also seen quite about about the migration path from v1 to v2 recently, so I don't really think that's too much of a concern for me at this point.
The answer is really that there isn't any that best complement WebAPI. The WebAPI won't care if it's getting serving to a web client or a native mobile client and if you're thinking that is should or would, then you're already tightly coupling. Take a look through http://todomvc.com/. It's somewhat trivial, but you'll get a quick look at how all these different JS frameworks implement the same goal. See what looks interesting to you. Personally, I'm most interested in Angular at the moment because we're using it at work, but I'd like to take a closer look at React and Aurelia.
very useful document, thank you...now i need to start using fiddler more. 
True enough. What I meant was that a framework like Ember partially derives its value from having a Node backend, so it's not as attractive a choice when you're using a .NET backend.
Where does Ember get value from Node? The biggest Ember based project I know of is Discourse, the discussion platform. That's backed by a Rails API. Unless you're talking about using NodeJS for build tools and generators for Ember? Generating files, views, templates, etc - then yeah sure. But that's the case for any front-end development these days. Tools like Bower, Grunt, Gulp, etc. 
&gt;I'm not sure I understand you here. Are you saying that all unit tests are worthless? All unit tests of a MVC controller are worthless. A controller is a functional piece with many dependencies outside of your code. Unit testing a controller is a false assumption that 'it works'
I would second Angular+TypeScript. I haven't worked with Angular 2.0 yet though, but TS works great with Angular 1.0.
I just wrapped up a project that used ASP.NET Web API + Entity Framework on the back-end, and Angular + TypeScript on the front-end. It was a fantastic experience, and possibly my new favorite dev stack. I strongly suggest you check out Yeoman to help you get your front-end setup quickly with Angular, TypeScript, and an awesome gulp file. https://github.com/Swiip/generator-gulp-angular Edit: Just wanted to really stress that there is no more "v1 to v2 debacle". There are now clear plans on how to upgrade v1 to v2. It will even be possible to have both frameworks working in the same app, side-by-side, even on the same page. [More info](http://angularjs.blogspot.ca/2015/08/angular-1-and-angular-2-coexistence.html)
Aurelia seems to be the only framework that has gone off the total deep end for making even the trivial mindboggingly difficult.
Looks interesting. I was wondering, is the license per developer, per project? 
Thanks for mentioning about the current state of Angular v1 vs v2
I don't know the full details but this application is on some old windows 2000 server and they want to get rid of the server. Guess they need to update the software to migrate it to the replacement machine.
That depends - do you want to use the views/routing/Razor provided by MVC? If so, we use Knockout and Typescript to great success. However, we are thinking of moving away from MVC to a complete WebAPI (which we already have feeding the TS/Knockout stuff). If this is the route you want to go, look into Angular or a more full-fledged framework. Currently we are looking at Angular V2 and React. Possibly Aurelia and Durandal as well.
I've been using Knockout since last year and I love it. I wish just there were more "battle-tested" custom binding handlers around; I had to modify the draggable/sortable handler due to performance reasons and I wasn't really comfortable doing so, you usually find out what's going to break the hard way. Still it was a pretty good experience, takes some time to getting used to but I'd really recommend it.
And if you are going to use React, take a look at [ReactJS.NET](http://reactjs.net/). 
Durandal is not really that active anymore. The creator of the project is now working on Aurelia as his primary project. Just an FYI
If I might ask, which version of Visual Studio are you using? And how is Angular in that IDE? I have been watching some videos on that, but haven't played with it yet in VS.
Thanks for all your feedback, I really do appreciate it. It helps a great deal. Especially since your answer seems to stress the TypeScript component, which I wasn't initially thinking about. So it definitely opened up my eyes, that its something I need to take a look at. Thanks again! 
FWIW, there are a lot of Kendo fans using Aurelia, too.
C/C++ and Java/C# are different tools for different tasks, you can't *effectively* replace one with the other.
Yes, that's exactly what I meant by "initial population of models in Razor." Sorry if I was unclear.
I just watched [this presentation](https://www.youtube.com/watch?v=z5e7kWSHWTg&amp;feature=youtu.be), and the performance comparison between Ember, Angular, and React was really quite impressive! I like the looks of React code, too.
Look at Haxe, it converts to whatever language you want. 
So, what's your point? Just like the first C compiler had to be written in a different language, the first VMs/CLRs also had to be written in a different language. That doesn't mean they couldn't be written in Java or C# nowadays, though. I don't see how that is relevant, though. What % of all software projects do virtual machines and language runtimes represent? 20 years ago, C++ was the dominant programming language, not so much anymore.
So, if you write a VM in Java, what VM are you going to run that byte-code in? Do you even understand how these platforms actually work? 20 years ago, the type of apps that are written in C# today were written in Visual Basic. The type of apps that were written in C++ (Norton Utilities, Office, PhotoShop, most video games) are still written in C++. I don't see that Java or C# has really had much impact on that.
What /u/grudolf said is correct. I would like to add that there is a nuget package called FrameLog https://www.nuget.org/packages/FrameLog/ that will accomplish exactly what you want to do and is EF friendly. I have not used it but I read up on it and it seems fairly easy to incorporate into a project. 
&gt; So, if you write a VM in Java, what VM are you going to run that byte-code in? It doesn't matter. If you write a VM on top of another VM, the underlying language becomes irrelevant since it's not going to be used for future improvements. As a practical example, the .NET CLR 2.0 could have been written in a language running the .NET CLR 1.0. Had that been the case, there would be no need to touch the C/C++ code underneath the .NET CLR 1.0 since all future CLRs would have been written in .NET 1.0. &gt; 20 years ago, the type of apps that are written in C# today were written in Visual Basic. Well, you're not completely wrong, but you're not right either. Of course, large applications with huge C++ codebases aren't going to be converted anytime soon, though Microsoft did initiate a conversion from C++ to C# in Visual Studio. Still, C and C++ have been completely wiped out from web dev (though not necessarily by Java and C# first, though) and mostly from mobile dev. I'm pretty sure it also declined in the enterprise space and for small to medium-sized software development. As for games, Unity is becoming more and more popular so I wouldn't say there hasn't been any impact.
Thank you.
Yes, I never said it replaced it *everywhere*, but VMs and OSes are poor examples because they represent niche applications.
Actually, the CLR is, in fact, written mostly in C#. The code is open source. It's almost all C#.
First, "garbage collection == bad performance" is a gross oversimplification. It's dogma that prevents critical thinking, especially once you start throwing around terms like "far superior". You can write high performance code with a garbage collector if you pay attention to what you're doing. Small programs may never even need to collect. As for large programs, play a few games written in C#, pay attention to the garbage collector, you'll notice it rarely if ever runs while you're playing. A smart developer may even hide the collection during IO. The amortized cost is quickly approaching equilibrium. Second, there exists versions of C# that do not rely on a garbage collector. .NET native is just the first step.
I don't know why he's being down voted. C++ has decades of cruft. It was conceived during a time of different requirements that has since passed. With languages like Rust, Go, and native versions of C# on the rise it's definitely a valid question in regards to new development.
The framework is mostly C#, sure. But the run-time still has an awful lot of C++. Have a look at https://github.com/dotnet/coreclr.
I never said GC is bad, it's just not fit for systems programming. I know there are cases where it's easier to work with GC, like deletion in graphs with cycles. But for systems programming RAII idiom is so much better as it works well not only for memory management but also for management of every other resource imaginable. C# doesn't have that language feature and it probably never will so you have to work around this by having interfaces like IDisposable, finalizers, using declarations. Code that uses all this stuff would not look as good as it would have looked in C++.
 public int Save() { var changeInfo = _context.Context.ChangeTracker.Entries() .Where(x =&gt; typeof(IAuditable).IsAssignableFrom(ObjectContext.GetObjectType(x.Entity.GetType()))); SimpleUser user = new SimpleUser(Guid.Parse("C06FCBB1-92BE-4103-BA8C-733630A12277"), "test", "test"); var transactionId = Guid.NewGuid(); IEnumerable&lt;AuditItem&gt; auditItems = changeInfo.Select(item =&gt; { var type = item.Entity.GetType(); var entity = (IEntity&lt;object&gt;)item.Entity; return new AuditItem(entity.Id.ToString(), entity, MapState(item.State)); }); var auditTrail = new AuditTrail(transactionId, auditItems, user); _bus.Send(auditTrail); return _context.Context.SaveChanges(); } The save triggers a domain event, a message was published on a bus and picked up somewhere else to log it (outside the application on a slower and cheaper database). Some parts are replaced just to give you an idea (user)
I'm starting to piece some of these technologies together, theoretically. I watched a few intro videos on Typescript last night and getting some idea. I've been messing around with Angular a bit already, but with dummy data for very, very easy demos. But where I do want to get to, is not playing with it in isolation. But more integrated. Hitting the APIs I have. And also within the IDE. I work for a small non profit, so we are only on VS 2012, so trying to figure out how to make it work in that. Most likely though, I may just run a demo of VS 2015 on a VM. And then try to get my employer to get us that upgrade. It really seems to be a better choice than trying to figure it out in VS 2012. 
well, for one thing, C# has self properties, so you can do this: public class People { public int PersonId { get; set; } public string Name { get; set; } public string ObjectId { get; set; } } And you don't need a default constructor. It's automatic.
&gt; In practice, that's simply not true. Take a look at the history of the .NET CLR. I know that's not the case, that wasn't the point. I'm saying that just because a platform relies on a language to run doesn't mean that language can't become obsolete. Technically, what would prevent Microsoft from writing the next CLR in .NET? And if they did that, how would C++ stay relevant with regard to the evolution of the CLR? &gt; Especially when there's no compelling reason to do so. Agreed. &gt; C/C++ were never really used for web development. Wasn't used for web 2.0 that's for sure, but since we're talking about 20 years ago, I'm pretty sure there were quite a number of CGI programs written in C. &gt; But IIS, Apache and most other web servers are written in C/C++ and will likely continue to be. I'm not disagreeing, but again these are more or less niche applications. Of course it's not going to disappear anytime soon, but if it's only used for legacy code and different varieties of runtimes, it's certainly not going to reclaim the top spot.
What is this?
A channel for https://telegram.org/
If it's capped at 200 people it isn't really appropriate for a this kind of thing surely?
I wouldn't call an OS niche considering every PC has one. In embedded development there are a lot of devices where C/C++/ASM are your only options, and there are a metric shitload of embedded devices in the world. 20 years ago, you didn't have a lot of options if you needed a practical programming languages, so it makes sense that C and C++ are on the decline in some application domains since other languages are now available which allow us to more efficiently solve some types of problems. Legacy applications are incredibly relevant, I know of multi-million dollar companies who rely upon rather archaic systems to drive all of their operations.
Thank you very much @twinsplynn! The license is per developer.
Ah, I just went to https://telegram.org/faq#q-who-is-telegram-for and saw &gt; Since Telegram groups can have up to 200 members
&gt; In reality however, these higher abstraction languages are less useful for writing things that are inherently lower abstraction, such as operating systems. You can write an OS in C# (since eventually everything translates to native code), however you are forced to work around certain limitations of the language to do so, making it inefficient and definitely slower. MS research already did this several years ago with Singularity. You can certainly design an OS around a managed kernel which both has tooling and speed, where your JIT sits is a design choice and a well optimized JIT strategy wouldn't impose significant performance constraints. Singularity used assembly for intercepts &amp; boot, c++ for hardware abstraction and everything else was in a c# derivative. Super interesting paper [here](http://research.microsoft.com/pubs/52716/tr-2005-135.pdf) regarding its design if you haven't played with Singularity before. TL;DR: Writing kernel code in managed code isn't a problem if your kernel is also written in managed code. Edit: Also see the FPGA's on the newer Xeon's, there are already CLR implementations on FPGA.
My _layouts and views basically set up their viewmodel based on the MVC model like so: viewModelInstance().model(ko.mapping.fromJS(@Html.Raw(Json.Encode(Model ?? new { })))); Where viewModelInstance is an observable with embedded functions and complex knockout bindings. 
I did it! It's as slow as buggary. Any thoughts? =( Imports SlimDX Imports SlimDX.Direct3D9 'this will allow you to import the necessary functions from the .dll Imports System.Runtime.InteropServices Public Class Form2 Inherits PerPixelAlphaForm Public bmp As System.Drawing.Bitmap = Nothing 'this imports the function used to extend the transparent window border. &lt;DllImport("dwmapi.dll")&gt; Shared Sub DwmExtendFrameIntoClientArea(ByVal hWnd As IntPtr, ByRef pMargins() As Integer) End Sub &lt;DllImport("user32.dll", SetLastError:=True)&gt; Shared Function GetWindowLong(ByVal hWnd As IntPtr, ByVal nIndex As Integer) As Integer End Function &lt;DllImport("user32.dll")&gt; Shared Function SetWindowLong(ByVal hWnd As IntPtr, ByVal nIndex As Integer, ByVal dwNewLong As Integer) As Integer End Function 'this is used to specify the boundaries of the transparent area Public Structure Margins Public Left, Right, Top, Bottom As Integer End Structure Private marg As Margins Public WithEvents t As New Timer() Public device As SlimDX.Direct3D9.Device = Nothing Public slimD3D As SlimDX.Direct3D9.Direct3D = Nothing Private spriteManager As SpriteManager = New SpriteManager() Public fuzzBug As fuzzBug = Nothing Private pseudo3D As Projection = Nothing Public ry As Decimal = 0 Private renderTexture As Texture = Nothing Private renderSurface As Surface = Nothing Private ss As Surface = Nothing Private tf As Matrix = Nothing Private renderBMP As System.Drawing.Bitmap = Nothing Private testBMP As System.Drawing.Bitmap = New System.Drawing.Bitmap(512, 512, System.Drawing.Imaging.PixelFormat.Format32bppArgb) Private Sub t_Tick(sender As Object, e As EventArgs) Handles t.Tick render() End Sub Private alphaBMP = New System.Drawing.Bitmap(512, 512, System.Drawing.Imaging.PixelFormat.Format32bppArgb) Private Sub Form2_Load(sender As Object, e As EventArgs) Handles Me.Load slimD3D = New Direct3D() SlimDX.Configuration.EnableObjectTracking = True Dim presentParameters As New PresentParameters() presentParameters.Windowed = True presentParameters.SwapEffect = SwapEffect.Discard presentParameters.BackBufferFormat = Format.A8R8G8B8 presentParameters.BackBufferWidth = Me.Width presentParameters.BackBufferHeight = Me.Height presentParameters.BackBufferCount = 1 Dim renderHandle As IntPtr = Nothing 'Handle device = New Device(slimD3D, 0, DeviceType.Hardware, renderHandle, CreateFlags.HardwareVertexProcessing, presentParameters) ss = Surface.CreateRenderTarget(device, 512, 512, Format.A8R8G8B8, MultisampleType.None, 1, True) device.SetRenderTarget(0, ss) 'renderTexture = New Texture(device, 512, 512, 0, Usage.RenderTarget, Format.A8R8G8B8, Pool.Default) 'renderSurface = renderTexture.GetSurfaceLevel(0) 'device.SetRenderTarget(0, renderSurface) device.SetRenderState(RenderState.AlphaBlendEnable, True) device.SetRenderState(RenderState.SourceBlend, SlimDX.Direct3D9.Blend.SourceAlpha) device.SetRenderState(RenderState.DestinationBlend, SlimDX.Direct3D9.Blend.InverseSourceAlpha) device.SetRenderState(RenderState.BlendOperation, BlendOperation.Add) device.SetRenderState(RenderState.SeparateAlphaBlendEnable, True) device.SetRenderState(RenderState.BlendOperationAlpha, SlimDX.Direct3D9.Blend.SourceAlpha) device.SetRenderState(RenderState.DestinationBlendAlpha, SlimDX.Direct3D9.Blend.InverseSourceAlpha) device.SetRenderState(RenderState.BlendOperationAlpha, BlendOperation.Add) device.SetRenderState(RenderState.ZEnable, False) pseudo3D = New Projection(Height, Height, Width, Width \ 2, Height \ 2) spriteRender = New SlimDX.Direct3D9.Sprite(device) tf = spriteRender.Transform spriteManager.pseudo3DRenderer = pseudo3D spriteManager.setDevice(device) fuzzBug = New fuzzBug(spriteManager, pseudo3D, pseudo3D.leftMax, pseudo3D.rightMax) imageTexture = bitmapToTexture(My.Resources.Snowflake) End Sub Public Sub New() ' This call is required by the designer. InitializeComponent() ' Add any initialization after the InitializeComponent() call. 'Dim margins() As Integer = {0, 0, Width, Height} 'DwmExtendFrameIntoClientArea(Me.Handle, margins) 'FormBorderStyle = System.Windows.Forms.FormBorderStyle.None 'Prevent the background flashing when window first appears 'AllowTransparency = True 'BackColor = Color.Black 'TransparencyKey = System.Drawing.Color.Black TopMost = True TopLevel = True t.Interval = 10 t.Enabled = True End Sub Public Function bitmapToTexture(ByVal bitmap As System.Drawing.Bitmap) As SlimDX.Direct3D9.Texture Dim stream As New System.IO.MemoryStream() bitmap.Save(stream, System.Drawing.Imaging.ImageFormat.Bmp) stream.Position = 0 Return SlimDX.Direct3D9.Texture.FromStream(device, stream) End Function Public spriteRender As SlimDX.Direct3D9.Sprite = Nothing Public imageTexture As SlimDX.Direct3D9.Texture = Nothing Public translateMatrix As New Vector3(50, 10, 0) Public Sub render() Try device.Clear(ClearFlags.Target, Color.FromArgb(0, 0, 0, 0), 1.0F, 0) device.BeginScene() ry -= 1 fuzzBug.update(0, ry, 0) spriteManager.updateSprites() spriteRender.Begin(SlimDX.Direct3D9.SpriteFlags.SortDepthBackToFront) spriteRender.Transform = tf spriteRender.Transform *= Matrix.Translation(-200, -200, 0) spriteRender.Transform *= Matrix.Scaling(80, 80, 0) spriteRender.Draw(imageTexture, Nothing, Nothing, New Color4(1, 1, 1, 1)) Dim ctr As Integer = 0 For Each sprite As SpriteItem In spriteManager.sprites spriteRender.Transform = sprite.matrix Dim cc As Color4 = CType(sprite.color, Color4) spriteRender.Draw(sprite.spriteTexture, sprite.centerV3, sprite.positionV3, cc) ctr += 1 Next spriteRender.End() device.EndScene() device.Present() Using deviceRenderSurface As Surface = device.GetRenderTarget(0) Using offscreenPlaneSurface As Surface = Surface.CreateOffscreenPlain(device, 512, 512, Format.A8R8G8B8, Pool.SystemMemory) device.GetRenderTargetData(deviceRenderSurface, offscreenPlaneSurface) Dim rec As New Rectangle(0, 0, 512, 512) Dim imageStream = Surface.ToStream(offscreenPlaneSurface, ImageFileFormat.Png, rec) renderBMP = New System.Drawing.Bitmap(imageStream) 'renderBMP.Save("C:\temp\temp.bmp") 'PictureBox1.Image = testBMP 'renderBMP 'SetBitmap(New Bitmap("C:\temp\dice.png")) 'renderBMP) 'Dim loadBmp As System.Drawing.Bitmap = New System.Drawing.Bitmap(My.Resources.cipfaLogo) Dim g As Graphics = Graphics.FromImage(alphaBMP) g.DrawImageUnscaled(renderBMP, 0, 0, 512, 512) 'BackgroundImage = renderBMP SetBitmap(renderBMP) End Using End Using Catch ex As Exception End Try End Sub Private Sub Form1_FormClosed(sender As Object, e As FormClosedEventArgs) Handles Me.FormClosed fuzzBug.bugTexture.Dispose() ss.Dispose() spriteManager.spriteRender.Dispose() slimD3D.Dispose() device.Dispose() End Sub End Class
thanks for this insight
Not a clue, my boss just said, "We want to get rid of this machine. Go rewrite this C++ app." Frankly I was surprised of these alleged incompatabilities, too. It might simply be that they plan to upgrade the app more the in the future. It's currently used exclusively by the company founder, but he's probably not around much longer I don't think.
C++ will never die for **true** real-time applications. With Java and C#, you just can't be sure exactly when something will happen, because you as the programmer aren't in control of it. With C++ you can. That said, Java/C# (/PHP/Python/etc) have well and truly taken over most of the main "general purpose" role. C/C++ have settled into a niche environment, but won't be going away any time soon. If you know C++ well, you will have fewer options on jobs (as there are fewer around) but you sure as hell won't struggle to find one, and you can pretty much name your price when it comes to salary negotiation... If you are doing general development you will almost certainly pick up C#/Java/Python or similar. If you are going to use C++ you will likely have a specific reason to do so. Why are you asking the question?
Testing that with a unit test is worthless. You still have zero idea it will work. What you're discussing should be tested against a real webserver and either a real web browser simulating a person or real http calls simulating an api request.
Not true. VS 2015 also has the light bulb.
Oh, I was thinking of the wrong feature.
Control - dot keyboard shortcut is way more useful than the lightbulb and works great in both versions.
The web api should return a 401 and an extra header with the url that can give an auth token. It's up to the js client to consume that response and direct the user to get a token (or just hardcode the url to redirec to). The extra header is not a standard that I'm aware of, you could instead implement something like Open Id Connect which I believe has a way to define token endpoints but that feels like an overkill when you're just wrapping someone elses oauth.
You get a `NullReferenceException`. Which reference is null?
Where is the session variable being set? In the code posted, you are just trying to access it. But where is it being set?
You're trying to set the value from session at a point in the page cycle where the Session object may not actually exist yet. Move the "Convert.ToString..." bit into Page_Load or Page_Init. public partial class RandomClass : System.Web.UI.Page { public String strEditID {get;set;} protected void Page_Load(object sender, EventArgs e) { strEditID = Convert.ToString(HttpContext.Current.Session["EditID"]); ... } }
I strongly recommend the Microsoft books for the exams. The quality can differ a lot between authors, however it is very closely aligned with the exam and they haven't let me down yet. For this exam I did read this one and enjoyed it: http://www.amazon.co.uk/Exam-Ref-70-486-Developing-Applications/dp/0735677220 Edit: Saying that it appears the exam has been upgraded ahead of the book. In situations I've not been able to get my hands on a MS book, I simply google the exam reference and search through blogs for MSDN links (lots of kind people post the links up). Microsoft Virtual Academy sometimes has videos that align closely with the exam too so I'd give that a look at also. 
I think these books are cheaper from https://www.microsoftpressstore.com/ (with ebook/pdf option)
Why not use a Datamatrix or PDF-417? Where will these be? Will they be printed on things?
I was under the impression that there is no native .NET container support as of yet until Windows Server 2016 is released (it is in technical preview). I believe the current Azure offering utilizes Linux and CoreCLR under the hood right? Or do they now offer .NET containers running on Windows in Azure?
The containers feature works in 2012R2 with a manifest hack (minus the HyperV container which breaks horribly) and some creative regsvr32's :) Needed to show it would work in 2012R2 since Microsoft are refusing to state if they will be porting it down or not. It doesn't work at all in 2012. The approach MS have taken is a little different to that Docker uses on *nix daemons so its worth jumping on TP3 to have a play. The resources they are pouring in to this its clearly going to be super important for Windows dev moving forward. 
You could also change the code to handle session storage and retrieval internally: public partial class RandomClass : System.Web.UI.Page { private string strEditID; public string EditID { get { object sessionValue = HttpContext.Current != null ? HttpContext.Current.Session["EditID"] : null; if (sessionValue != null) strEditID = Convert.ToString(sessionValue); return strEditID; } set { if (HttpContext.Current != null) { HttpContext.Current.Session["EditID"] = value; } else { strEditID = value; } } } protected void Page_Load(object sender, EventArgs e) { ... } } This approach has the following advantages: * The rest of the page need not know whether the data is stored in a variable or session. * Session will be used when available; there is a fallback to local variable. This is useful, if you want to mock your code behind class for some reason and run it through unit tests; because in a mock environment, the entire HttpContext will not available.
This is pretty good: https://channel9.msdn.com/Events/aspConf/aspConf/Async-in-ASP-NET
All you would need to do is declare your Action as `async`. That's all there is to it. If you don't `await` anything it won't actually be an asynchronous call, but that implementation detail is left up to the reader.
So really that simple? Well shit..
That's what I'm musing on.....
&gt; it pops the authorization page That's view stuff, either MVC or JS. WebAPI is for data transfer, not for displaying UI. If you need to show UI to the user, you'll need some process that coordinates between the UI and the API.
Yes and no. It's simple to write, but not so simple to write in a way that makes it actually better. For example, you can easily wrap synchronous calls with async/await but really all you're doing is shuffling about threads that are blocking anyway. As /u/celluj34 said, if you don't await, you're not really doing anything better and in fact can be making performance worse. It's also not a magic bullet, it doesn't always give you a big performance increase but that said, I would certainly consider making any new projects async by default.
Yeah, that was my concern. The alternative is to evaluate the entire image except for a 7x7 chunk in the corner where you are going to put the signature. 
[NUGET](https://www.nuget.org/)
If your code libraries have the same deployment and release schedule, they can be shared as project references. If you feel that your libraries have a different release schedule and will be shared with other projects, I'd suggest creating a local NuGet repo, publish to NuGet or create a MyGet repo and set references to the NuGet packages. You can put views and other artifacts in a library, the level of reuse tends to be application scoped unless you go out of your way to generalize the function ex: reusable grid control. General rule of thumb I use: one branching structure per deployable artifact. Apps are published to the runtime environment, dependencies are published to a NuGet feed.
Sorry I meant to say. I have seen NuGet but it doesn't have the comparison tables. NuGet is more similar to PyPi in that it is a directory of all packages but, as far as I can see, it doesn't give me a way to compare all the packages of a similar function. Unless I'm missing something! EDIT: It doesn't seem to have any obvious categorisation at all
Yes, I forgot about that. The compiler will tell you though, at least, resharper tells me
Yes, those choices will work. Go to 12factor.net to learn some patterns for cloud development. Azure websites + Azure Sql PaaS will give you a good foundation to scale up or down as needed.
That's sound, as long as you have a 100% free for learning/charity cases then i'm happy! So tell me, have you had many purchases yet, what seems to be the common use for it, all I can think of is Debug logging? I could also get a lot better value for money with PostSharp (eg: free for method interception): https://www.postsharp.net/purchase specifically:: http://doc.postsharp.net/method-decorator which uses method decoration to achieve the same functionality. don't get me wrong here, I think you have made a very good library, i think it may be just a bit too pricey is all imho. 
have a look to parts of Azure ecosystem though... not included exhaustly in the books but definitely are included in the exam Things like Azure Cache, SQL Azure, etc. ... the exam (at least for me) consisted of 58 questions splitted into 4 segments (3 study cases and 1 bunch of free questions) As other have recommended, Pluralsight and the Microsoft book are your friends
I seem to always run into double dispatch problems and code reuse problems when I have a bunch objects that maintain state and functionality. Eric Lipperts 'Wizards and Warriors' series explains it better than I can: http://ericlippert.com/2015/04/27/wizards-and-warriors-part-one/
@mogawowo thanks for your useful comment. The product is doing very well, in fact we were invited to the .NET Rocks! show http://dotnetrocks.com/?show=1198 to speak about it. 2.525 total nuget packages downloaded and hundreds of product subscribers in just 4 weeks seems a pretty good number to us. We don't pretend to compete with Postsharp and its up to the end-user to decide what better suits his needs. If its your conviction that Postsharp offers s better value for money then we respect it, there are certainly other people who think the opposite, but hey that's life. :) 
No but you can have a good guess, and yes you are correct. essentially a HTML/CSS/JavaScript WYIWYG load of bollocks creator.. I'm playing with signalR and it's all in the name of fun.
No problems here, just making sillyness of the fact that I included the evil comic sans... just wasted time here I'm afraid.
&gt;ComicSans = 666 hah! And I agree with the other posters, need to see what you're working with to tell ya if its right or wrong. Also, you could try this line of code to get the descriptions from the enum, where value is an int Enumerations.GetEnumDescription((eFontFamily)value);
I'm doing something similar with a large-ish government app. It is a WebForms based application, using stored procedure data calls, written approximately 10 years ago that we are modernizing. We are moving to an MVC5 app using Unity and EntityFramework with some AngularJS and BootStrap. In our case we are also implementing a number of new features &amp; pages, so our road map is to implement the new stuff using the MVC5-based stack. It's all in the same project with the legacy WebForms stuff (so that its the same app running under IIS, Session is the same, etc...). I created a separate MVC folder that houses the typical scaffolding for MVC inside of the Web project (~/MVC/Controllers, ~/MVC/Views/, etc..), just to separate the MVC stuff from the legacy WebForms pages. Since most of the MVC features are just built on top of WebForms, it plays along nicely. The transition between the 2 "sides" is fairly seamless. As we go forward (over the next couple of years), the plan is to slowly take chunks of the legacy code and convert it to the modernized stack bit by bit. We created a BootStrap theme/template that mimics the look and feel of the dated site, but the idea is that in the future it is easier to pop in a newer modern template without too much fuss. If you got the most experience I would just setup the folder structuring, routing, etc in the existing WebForms based project and create a sample controller/views as a proof of concept/demo. Demo it to your team and let them learn about it. We didn't do a whole lot of training or anything. I think most of the developers have picked it up though.
I guess reading is hard, because on every single build you get this message: 2&gt; Restoring NuGet packages... 2&gt; To prevent NuGet from downloading packages during build, open the Visual Studio Options dialog, click on the Package Manager node and uncheck 'Allow NuGet to download missing packages'. And like magic, if you click on Options and Package Manager you can disable package restore on build.
It's hard to explain really, it's a mish mash of things, Signal-R, MVC &amp; Entity framework, it's essentially a WYSIWYG page designer and as elements get moved around on the page it does backend stuff like this: var data = htmlElement.GetFromEntityData(id) var tf = new cssTransform(TransformProperty.rotateX,DegreeX: 50); var tf1 = new cssTransform(TransformProperty.scale, X:1.2,Y:1.5); var tf2 = new cssTop(100); data.cssValues.Add(tf); data.cssValues.Add(tf1); data.cssValues.Add(tf2); var rawhtml = data.GetElementHtml(); data.save(); Clients.Caller.updatePageItem(id,rawhtml, "header animated fadeInUp "); eg edit and add html elements to a page, save it and come back later.. trust me it doesn't make sense. 
Well, like all things in programming there are many ways to skin a cat. I don't think there's anything inherently wrong with what you're doing, but I would either store the font family values in the database and make some CRUD in case i ever need to update the values. If you want to limit the number of calls to the DB you could create an XML file instead. Since you are using EF it should be easy to interact with an EF CSS object anywhere else in code. 
You can probably scale this stack a lot farther than you think (or have been led to believe by tech sites) vertically (bigger better faster server), so you'll be fine. 
i don't think you understand. i *need* this.
If it works, it's a cheap way to get a lot of code coverage so you are free to concentrate on the more complex tests.
Well, that specifically is an issue with probotuf-net and not `DateTime`. And protobuf-net doesn't support NodeTime out of the box.
I almost work exclusively in legacy code. I'm the guy that gets called when offshore comes back or devs leave en mass due to badly managed code bases. Having run this over at least 5 projects I manage, the results are, sadly of very little use if any. Beyond null checking I'm getting naught. I hope this doesn't get seen as a replacement or excuse for not practicing TDD. I can write programs I wouldn't even know where to begin doing that. It's not giving me much more than asserting things aren't null which doesn't add much if any value in my scenario. I guess I was hoping for something I could just throw over legacy projects to give me some of the scaffolding rather than me having to dig through the codebase to find the seams and gradually add a test harness so that I can finally tear the thing apart to patterns and practice. Been using this for about 90 minutes and sadly this isn't the one :( Be interested in other peoples results though, perhaps managing a more modern or structured project. Like most things, I can see this adding *some* value in *some* situations. Perhaps in my excitement I forgot there is no silver bullet :) 
DateTime.Kind problems appear all over the place. They should have never added that property.
I'm going through the same transition and have found the Pluralsight videos to be extremely helpful. That will give you the base knowledge of how it works. I'd then suggest a small internal project to get some actual experience under the belt. I watched the following from Pluralsight. I started with the Scott Allen MVC series: http://www.pluralsight.com/courses/mvc4 http://www.pluralsight.com/courses/mvc4-building http://www.pluralsight.com/courses/aspdotnet-mvc5-fundamentals Once you have the basics this series by Matt Honeycutt was the best video series I've ever watched. It changed the way I work and gave me a great understanding of how much you can customize the MVC framework. http://www.pluralsight.com/courses/build-application-framework-aspdotnet-mvc-5 
I agree with this, getting started with scaffolding is a good way to begin to get familiar with MVC.
If you start adding Code Contract pre/post conditions to the legacy code, more advanced test cases can be generated. This aspect of PEX wasn't mentioned in this article.
Look into using BitBucket with CI tools such as TeamCity and OctopusDeploy (this will create packages with the "latest code" for deployments to any environment).
As computers continue to increase in speed and power, it's possible to write programs in high level languages that would be unthinkable a decade ago. When .NET first surfaced, it was fine for LOB applications, but I would have never predicted that we would get to the point that it would be used for high performance systems, like process control or game development. And yet, here we are. I am a longtime and currently career .NET developer, but right now Java still has a pretty strong foothold in the enterprise world. I think MS continues to make inroads there too, though, and the continued support and development of C# and .NET along with the spate of open sourcing makes this more likely to continue. All tools have their place. I don't think C/C++ will disappear, but as more developers pick up higher level languages, and more colleges teach them, they continue to grow, to the detriment of other languages. I started my career doing some pretty hardcore C++ development, but personally, I do like C# better. To each his or her own, I suppose. :)
You are going to get a lot of different answers here, I think. :) I envision the umbrella of "unit testing" as a slider. On one side are the "pure" unit tests, where individual units of your code are tested. These units are typically pure functions. This is the strictest, most restrictive definition. On the other side are "integration" tests, which test things at a higher level and/or how various parts work together. This is the loosest, most relaxed definition. I think you will find that many developers find themselves somewhere along this slider, but exactly where seems to be different per developer or shop. 
&gt; I'd really just like to be able to pull code for one of our repos (i.e. changes that someone else on the team made) and have the server for that repo automatically build the code and restart. Since we're going to be dealing with potentially a dozen or so services, doing this manually would be a nightmare. So what you want to do locally is replicate what you're doing on your central servers. Why not run those same tools locally, assuming you have the licensing for it?
Thanks for the insight. We're actually going to start using MVC on a new project, but there will be some overlap in functionality (user management, dropdown configuration, etc) which we can build first to compare. 
This sounds like a problem that would be best solved by an editable grid or list (bound to whatever your collection is), rather than by dynamically generating additional input elements. This should be pretty simple with something like Grid.Mvc or if you're using an MV* JS framework like Angular or Knockout.
Thanks! I can see this article is new and I'll definitely be following with interest :)
&gt; You'll need Visual Studio Enterprise. Are you saying you've tried it using another version of Visual Studio? 
I see what you are saying and that makes sense. However, it is also important for me to be able to make rapid changes and have those changes be (nearly) immediately reflected on my local web servers rather than having to go through a change &gt; build &gt; deploy cycle each time. EDIT: I think this is pretty close to what I'm looking for: [http://stackoverflow.com/questions/30289023/view-code-changes-without-restarting-the-server](http://stackoverflow.com/questions/30289023/view-code-changes-without-restarting-the-server)
I've never run the "full stack" on my laptop so I'm not sure how easy it would be (maybe IIS Express would work). Typically I work on one at a time, with the rest going to a DEV environment or I just run multiple projects from VS.
No, no you're not. You should name types (classes and enums) as well as properties and methods in PascalCase and not prefix them with anything Hungarian such as "e". The reflection you're using to get descriptions is going to get expensive if it is called often. You would be better off reflecting once and populating a dictionary and using that for lookups. You don't have an atomic data point in the description. You'd be better served making a different attribute type with separate data points if you really wanted to stick with an enum. Though as others have said, putting the data in a database or xml file would probably be better. Also, look into object initializers.
Try without the quotes around the object's property names
&gt; What are you talking about? MVC is currently the most popular architectural pattern for web applications. Popular != scalable. I was also referring to .net MVC not MVC in general, MVC does impose scalability limitations of its own but .net introduces its own to the pattern too. While OP might not be scaling to Facebook size there is a reason they abandoned PHP MVC. &gt; You will find large scale ASP.NET websites and one of them is Stackoverflow. Which doesn't use MVC but builds off it (custom compiler etc) and doesn't use the pattern universally precisely because of performance &amp; scaling issues. &gt; Classic ASP.NET is probably slower because of ViewState managemenet and bulky controls. You don't need to use VS. Controls with caching is actually super fast, OOB controls are loaded in to the tree on each request rather then the control tree caching. Edit: Just to clarify OOB MVC is significantly faster then web forms but web forms are easier to optimize and can be optimized to significantly outperform MVC.
depends on what you're using to build your microservices, you have the following options: 1) use the self-host feature ([link1](http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api), [link2](http://docs.asp.net/en/latest/conceptual-overview/aspnet.html#servers)) 2) host your own [web server](http://blogs.msdn.com/b/carlosag/archive/2008/04/14/hostyourownwebserverusingiis7.aspx) in your application 3) switch to mono and apache
Maybe your best bet is to take a step back and to think about the underlying problem the code solves, and consider if, maybe, there's a better way...
[This article](http://haacked.com/archive/2008/10/23/model-binding-to-a-list.aspx/) looks like it may cover what you need. Non sequential indices are covered near the end.
I'm one of the co-founders of Crypteron and would love feedback from the /r/dotnet community! The majority of security solutions out there focus on the IT department instead of the developer. Most developers aren't security experts so we think that folks can really benefit from this, including the free version. 
This is probably what you are encountering: https://stackoverflow.com/questions/2445874/invalid-json-primitive-in-ajax-processing
There is another option that I quite like. A lot of the Microsoft APIs ([Visual Studio Online](https://www.visualstudio.com/integrate/api/wit/work-items) being the one I am most familiar with) take a version in the query string: GET https://foo.visualstudio.com/DefaultCollection/_apis/wit/workitems/123?api-version=1.0 This means the canonical URI always stays the same and you don't need to do anything funky with the headers. It still has disadvantages, all REST versioning schemes do, but I like it more than the three in the article.
&gt; do anything funky with the headers What's wrong with headers? They're the cleanest approach.
try /r/forhire
That is a really silly reason. They suck because it makes stuff harder to test **manually**? Just write unit tests. And browsers are not designed to access REST resources like this anyway.
Thanks. No harm in trying, but it's a very broad sub ~~and not many subscribers~~ (edit: ok, 50K is a lot). I thought I was hitting the taget with /r/dotnet. I want to try Stack Overflow too, but can't find anywhere to make the request, aside from asking as a regular question and tagging.
I'm glad you brought that up. REST is like religion to some people. Some time people are referring to Roy Fieldings REST dissertation(which IS good) as the only truth. What makes sense for him and others doesn't necessarily makes sense in your project.
Try raising your jet provider version. Also, try using a local iis setup for testing instead of the visual studio setup. Next, what is the exact error you are getting on the remote server?
What are the permissions on the file or the folder that it resides in on the Production server? Is the IIS_USR group allowed to have read access on the file?
I did see that article, and that's probably what I'll go with. I was unaware that you didn't need sequential indices for simple collections. 
If it's any consolation, ASP.NET 5 is introducing Tag Helpers, so your markup will look more like markup, and you won't see @Html.TextBoxFor() anymore. http://www.mikesdotnetting.com/article/274/introducing-taghelpers-in-asp-net-mvc-6 The other thing you might consider doing is creating an EditorTemplate for List&lt;string&gt; to encapsulate the add/remove behavior, and having the added benefit of calling @Html.EditorFor(m =&gt; m.MyStrings, "TemplateName"), keeping in line with your other @Html.TextBoxFor() calls.
GAC = General Assembly Cache, which is located in the %windir%\Microsoft.NET\assembly directory. The Microsoft.Jet.OLEDB.4.0 assembly should be listed in the GAC. If you don't see it, then you may want to use a library like EPPlus to read Excel files instead of using the Access jet engine. If you do see Jet in the GAC, then check your project's target environment. I believe that Microsoft.Jet.OLEDB.4.0 is 32-bit, so your project should target x86 instead of "any".
Check the application logs on the server to see if their are any asp.net related errors there.
I had seen this before I posted here. I added the quotes that solved his problem, but for me they had no effect. I mentioned this in my solution comment elsewhere in this thread.
I request you to see this post and other content linked from it: http://blogs.msdn.com/b/visualstudio/archive/2015/09/30/intellitest-for-net-test-more-with-less-effort.aspx; they provide more context about IntelliTest. Also, I would be happy to help you apply IntelliTest to your code. I can be reached at @pvlakshm.
Sadly, I don't have VS Enterprise edition at work, so I'm out of luck with VS2015.
If you are looking for help, it is rather useful to actually show the error you get. Post the error, post your web.config, post the Fiddler log and post the IIS Failed Request Tracing log.
We noticed the same thing. Try adding this to web.config: ... &lt;system.webServer&gt; &lt;modules&gt; &lt;remove name="WebDAVModule" /&gt; &lt;/modules&gt; &lt;handlers&gt; &lt;remove name="WebDAV" /&gt; ... &lt;/handlers&gt; &lt;/system.webServer&gt; ... 
It's this a specific version or caused by using a specific project template? Cause I have made several web api projects and never had this issue. I've also never seen webdav on by default.
cool.. that's a decent rate 
Given everything else he is coding on is pretty up to date, I would assume he is using Web Api v1 or v2. Neither of these have a file extension per se, more MVC type routing.
Depends completely on server build. WebDav is added via Windows Components as a sub-item to the IIS component. If your server wasn't installed with the webdav component you won't need to remove the handler from your config.
I ran into this and went through all the WebDav removal to no effect. The only thing that worked was going into IIS =&gt; Http Verbs and noticed that POST was missing. I have no idea who thought that would be a good idea, but adding it back in did the trick. 
Yes, exactly, the service is an MVC Web Api that consumes XML http content. Nothing fancy and no pages needed, everything is automated. I had a more complex MVC WA earlier that I made from MVC Template in VS. It worked flawlessly in IIS 7.5 through Publish option, but had a lot of unwanted content, so this time I went with an empty web app and added only what was necessary. IIS Express is proof there's nothing wrong with it.
Thank you very much. I have confirmed this does work, even though I ended up taking another route.
Response header from Fiddler, log is empty: HTTP/1.1 405 Method Not Allowed Cache-Control: private Allow: GET, HEAD, OPTIONS, TRACE Content-Type: text/html; charset=utf-8 Server: Microsoft-IIS/7.5 X-Powered-By: ASP.NET Date: Thu, 08 Oct 2015 07:54:02 GMT Content-Length: 5587 Web.config has only one significant line (that is there jic) in system.web &lt;authentication mode="None" /&gt; IIS Failed request log: http://pastebin.com/Ww1vFnfu 
http://www.asp.net/web-api/overview/testing-and-debugging/troubleshooting-http-405-errors-after-publishing-web-api-applications
well you could use [rulesets](https://github.com/JeremySkinner/FluentValidation/wiki/b.-Creating-a-Validator#rulesets) or depending on your model implement classes, [here](http://stackoverflow.com/questions/13198471/fluentvalidation-multiple-validators#comment17971915_13200585)'s a great example of that also cardiak's [example](https://www.reddit.com/r/dotnet/comments/3nqazk/how_do_you_deal_with_copious_amounts_of_data/cvqk48m)
Your app pool doesn't have the correct permissions to access the folder, I can't remember the user but that is your issue. 
I've seen Visual Studio self-bork on a few occasions to a point where even uninstalling it and reinstalling it won't work (it'd be real nice if someone wrote a utility for completely removing it from a system, roots and all.) This may or may not be one of those cases. Have you tried re-creating the project elsewhere on the pc, ie, create a new blank project and add in the files from the original? It's stupid enough to sometimes work. Also make sure the files haven't been tagged as readonly by your versioning system (SourceSafe was really bad about this, at least for me anyway.)
I added IIS_IUSRS group, to which AppPools accounts belong by default, to folder permissions, but I still get 405 error. I added some lines to application's web.config to remove WebDAV and Extensionless ISAPI and adding the latter with POST verb, but nothing still.
I've tried and funny stuff is, I have multiple version of the project checked out. 4/5 are fubar but 1 works. Copy to other folders did not work, changing build setting (debug/release) also did not change anything. And I too am looking for a delete VS entirely.
Did you try the suggestions here? http://stackoverflow.com/questions/6518603/wpf-ioexception-cannot-locate-resource
There's 3-4 potential fixes in there beyond the accepted answer
Try the EPPlus library. It's my go-to for most office stuff
Take a look at Google Chart Tools.
Ah, makes sense now. Thank you.
Get rid of the fromname commands. Just set it to the sting of the hex. From name converts the word white to hex.
First - have you [been here yet](http://www.asp.net/web-api/overview/testing-and-debugging/troubleshooting-http-405-errors-after-publishing-web-api-applications)? &gt;There are several reasons why a specific HTTP verb may not be allowed, **but there is one primary scenario that is the leading cause of this error in IIS: multiple handlers are defined for the same verb/method, and one of the handlers is blocking the expected handler from processing the request.** By way of explanation, IIS processes handlers from first to last based on the order handler entries in the applicationHost.config and web.config files, where the first matching combination of path, verb, resource, etc., will be used to handle the request. Just for the hell of it, create a new site in IIS, give it its own AppPool and deploy there. There may be a configuration issue / some security reason that the default site is being "strange". If that works, then you may be digging into the machine.config files to find the site setup differences. 
Can you tell me where can I see which are the names of the columns in the GridView? I mean how ASP.NET reads them from the Excel file.
Sorry - I came back since you responded to my WebAPI comment below and threw in my ideas (without reading existing comments). [This MSDN](https://technet.microsoft.com/en-us/library/cc754617%28v=ws.10%29.aspx) should point you to where *all* IIS configuration files are located.
I'm not questioning the query at all. It looks fine. Do you have the connection block details? I ask because there's a difference between creating the connection object and opening it (see https://msdn.microsoft.com/en-us/library/ms676505(v=vs.85).aspx). I would actually expect to see a call to Open somewhere in the "execute query" block prior to the call to Execute. Similarly, you need to Open the recordset before you can read from it.
I try not to mess with machine level configs because now you're messing with settings for all sites hosted on the IIS instance. Pretty much all config entries can be changed on a per-application basis via the individual web.configs. You can remove entries for entire parent config sections using the "clear" element. Example: &lt;system.web&gt; &lt;membership&gt; &lt;providers&gt; &lt;clear/&gt; &lt;/providers&gt; &lt;/membership&gt; &lt;roleManager&gt; &lt;providers&gt; &lt;clear/&gt; &lt;/providers&gt; &lt;/roleManager&gt; &lt;/system.web&gt; 
:) start trek stuffs 
I think I've tried that. But I'll give it a go tomorrow. 
I copy the most similar project I can find in my library of past projects and start gutting it, then I fix whatever I remember not liking about it last time, update whatever packages need to be updated, then I start in on the new stuff. It is not a perfect system.
After creating the skeleton for the project, I usually try to plan out the database. It will probably change, but having a well thought out draft will save you time in the long run. I don't use any software to do it. Usually just a pad of paper. 
This is a terrible idea.
It depends how large the project is. If it's small with no requirements doc, then I just make it as I go. But if has a doc or a medium to large project, I'll start on the DB and build the app around that.
I know, at no point does it use Node.js, dynamic bytecode rewriting or a hand rolled communication protocol.
thanks, Pouint 1: Pascal case.. this is just semantics, doesn't make it wrong code. Point 2 : I agree. Point 3: Educate me, as ti what an atomic datapoint would be for a description. So to me an atomic transaction/data would represent the smallest value possible? ps it does hit the database, the enum is used to reconstruct the right object when it is pulled from db. I know of object initialisers, how would they help here please?
This may sound weird, but run CCleaner. Also there is another app that I can't remember the name of that goes after VS temp files. Try and google it because it worked for me. Both have worked for me before with similar errors.
I start with a database diagram in LucidChart. Then I sketch a rough ui layout with pen and paper. The actual implementation always changes as I go, but the above steps get the process started.
Thank you Jeremy. I'll be in touch via pm.
This may help - https://azure.microsoft.com/en-gb/documentation/articles/storage-dotnet-how-to-use-blobs/
So when hosting the app locally during development, I would still upload files to Blob?
I would also like to point out that the ReportViewer control would work nicely as well with an rdlc and the excel data as a data source.
You can or you can use the azure storage emulator Edit: link: https://azure.microsoft.com/en-us/documentation/articles/storage-use-emulator/
Thanks!
1) Speak to potential customers and get an idea of their requirements. Tools: Phone, Email, Surveys (Google Forms) 2) Get a commitment that potential customer(s) would pay $XX for product and or service 3) Plan 1 - 2 week sprints around features, that would help me to product / service to market in the shortest possible time frame. Rinse and repeat. Basically, I plan my development time around behaviors and/or features that people are willing to pay for. 
Open xml. Get the power tools too. Hands down best way to work with office. The power tools has a high fidelity html converter.
Sure, but there is nothing in it that matters. **The problem is IIS-wide**, but I don't know the cause of it, because I can't locate its applicationHost.config and other configuration files on the local disk. Here it is: &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;configuration&gt; &lt;system.web&gt; &lt;compilation defaultLanguage="c#" targetFramework="4.5" /&gt; &lt;httpRuntime targetFramework="4.5" /&gt; &lt;authentication mode="None" /&gt; &lt;/system.web&gt; &lt;/configuration&gt; &gt;Can you post your web.config for the web api site (minus sensitive information like connection strings)? &gt;Here's an example of how you would manipulate what verbs/handlers are used in your application via the web.config &gt;[Code] AFAICS, this just added two new statements to the Security section of the Response header. Did I miss anything?
I'd say it depends on a few things. First of all if you host it in Azure, file system isn't really a preferred option (although you can create persisted disks, and recently they introduced Azure File Storage). Secondly, how many images are going to be stored, and how often are they accessed. If these numbers are high, you risk overloading the database with read/write operations for which it isn't really optimized. Personally, most of our applications are hosted in Azure, in which case we use the Azure Blob storage. The available .NET libraries are simple and easy to use, and you won't have to worry about scalability and backups etc. I'm not really sure what the 'best practices' are here, but for me this set-up has worked well in quite a number of different use cases, so I'd recommend it for most applications.
Post it notes, post it notes everywhere! I am usually doing more projekt at onces, so for me to remember all that have to be done I use post it notes. Then I can prioitize them myself/with my boss/with our customer however they are needed. Edit:spelling
Thanks, but still no succes. :( 
Do you configure your Blob storage as a CDN?
Also, if using a repo have you checked that the files are actually there on checkout/pull on the local machine? I had a messed up git config before that wasn't pushing files properly.
In terms of development planning (ie ignoring financials etc), and assuming that I've already got a good idea of what the client/my boss wants the system to do 1. Split the project out into "modules" - basically the MVC controllers I expect to create, in reality. (eg login) 2. Split each module down into the tasks it has to achieve, and roughly map them to actions/funtions, or classes with functionality that lie outside of the simple MVC interactions (eg logins, registrations, password reset, forgot username, authenticate against the database blah blah). 3. Sketch out the views for the above tasks (ie login and registration forms) 4. If relevant, sketch out some preliminary designs for the overall design This gives me a reasonable idea of what I'm trying to achieve, and how I intend to achieve it. So the last main task is usually to go through and put things into a rough order of priority: ie - "core" functionality that I'll need to be able to demo the work or even develop it -"primary" functionality - things that aren't so urgent they have to be done first otherwise I can't continue work, but need to be part of the system - "optional" work - things that are nice to have or will need to be added later, but don't need to be present at release, for example - "nice to haves" - things that will be added if convenient (ie they fit well with another piece of work) but otherwise are last priority Depending on the complexity, I may also take one or two of the more complicated or tricky modules and plan them out in more detail: particularly if something is new to me or I feel I don't understand the reasoning behind it.
Those, I've tried as well, I was hoping that CCleaner would help, since i'm convinced it is an enviroment issue, rather then a project issue. We can make it work on vm's and other pc's, just those 2 pc's are fubar. We agreed that we'll reinstall the whole s***t and just be over with. Thanks anyway for the help.
Good luck lol, I hate crap like that.
Are you asking whether we serve the images from the blob storage as well? Then yes. The scalability and availability of the blob storage is quite adequate, at least for our needs.
Should be enough for that single page if those truly fit your need. You can always add custom validation methods in your models too - I believe it's called CustomValidator (not 100% sure?).
Why is there the perception that it is? I have little experience with it, so I'm curious.
Did you use /unsafe code in your C#, and turn off array bounds checking?
I am not too sure. I use it for a few of my projects storing files ranging for 2-3k - 100-200mb. And I don't even notice the cost. (I am storing under a thousand files).
This leads to untestable code. Also, two subsequent calls to S.esh return different objects in some cases. That's code smell.
Here's a pluralsight tutorial ($) that has a basic overview of incorporating Identity users and roles. In this solution it is setup for users to register themselves but need to be put into roles by an admin. It does cost money, but if you do the free trial you get access to the project files. The approach John Atten shows is probably a better long term approach and has more control, but was hard for me to get my head around. I now understand Identity better and how to incorporate it into controllers and views, but this pluralsight course got me there. [Master-Detail Workflow Apps with EF, MVC, Identity 2, and Modals](http://www.pluralsight.com/courses/master-detail-workflow-ef-mvc-identity-modals) 
I'm a total beginner and have been struggling (OK, struggling is my normal coding mode) trying to actually set up a site that uses Identity and most examples out there (and they are great) are pretty high-level and assume you know how to fill in the blanks. This is the first tutorial that actually shows how to set up the controllers and views for the different users. It does cost money but they have a free trial which you do get access to the project files.
I plan out my projects rather poorly! ;-) However, I had the fortune to work with an awesome software engineer that helped me see how a true pro thinks and goes about starting a fairly large project for a solo/small team. He literally started with a text file and wrote in order all the steps of the project. Then for each step he'd create the parts needed and move those comments into those new pages, and then comment deeper into the methods needed within those pages, etc. Then add new comments to the text file as new ideas/needs came up. By the end of the project the text file he started with was blank and all the code was highly commented with his original notes and new comments along the way. This was for a consumer lead system that handles over 10 million transactions per day and has a fairly robust admin/affiliate reporting website.
http://mvcdynamicforms.codeplex.com/releases/view/43320
I might be misinterpreting your question, but I think you might want to change your schema so that instead of having a table which contains the answers to questions where each question is a column, you want to have a question table where each question is a row, and the table describes if the answer to a question is a dropdown, a boolean, a string, etc. Then you'd have an answer table, which would say "person X answered with value Y on question Z". That way you can give admins a question editor so they can modify the questionnaires without having to recompile your app. The app would thus have to build the questionnaire interface dynamically by looping on each question object and then spitting out the correct UI (dropdown, checkbox, etc) based on the type of question it is. It's a bit more complicated, but ultimately a lot more robust. And you will learn at least 3 new useful programming powers if you get it working :)
[Identity Manager](https://github.com/IdentityManager/IdentityManager) Why write your own when one is already written for you and is open source as a bonuse.
I think you could have added entity framework without adding MVC through NPM and used that. There would have been some work in setting up objects, though DB first could have made it super easy and then possibly even easier in the future to update other queries since you would have already had EF setup. If you would have wrote the queries in EF the way of doing queries is slightly different otherwise you can cause it to generate some pretty long queries; so in that case i would pick what your most familiar with and what the company you work at has more of, so hopefully if another person there has to work on it they can understand it. Now I think stored procedures can be faster, they execute on the database server and if you optimize the query you should have less over head doing it that way. I am also not sure if you ever mapped your result to an object in .NET; but if you did I'm sure its a manual process using a SP vs with EF it would have handled the Object Relational Mapping for you. Usually the trade off for EF is, simplicity &amp; faster development time for slightly worse performance. I personally think it's much much easier to refactor common query logic using EF so maintenance is cheaper down the road, but you have to do lots of planning on the architecture in order to really see these benefits. I still use SP's in certain cases. 
Thank you. I figured it would end up with many to many relationship, but trying to communicate without knowing what half the stuff is called is a huge battle for me. Is this type of pattern/method called something? The part about building objects based on row values? Or as usual am I overthinking this? 
Sorry; instead of setting a page-wide variable that was fed by the session variable, I just called the session variable directly wherever it was needed, and then cleared it at the end of processing the final form submission (it also gets cleared on initial page load, just to avoid bugs). This was an edge case for the product I am working on, and is used in only one page in the entire project, so while this isnt an ideal usage, itll work for now until the client goes for a refactor or full redesign.
Last I checked it was more expensive than an equivalent amount of raw storage space (ie used for file system storage), but to be fair I can't claim I did an entirely exhaustive search on the options.
Something I came up with today for Blob storage to help anyone searching for this topic. public class ImageRepository { private CloudStorageAccount account; private CloudBlobContainer container; public ImageRepository() { account = CloudStorageAccount.Parse(CloudConfigurationManager.GetSetting("StorageConnection")); container = account.CreateCloudBlobClient().GetContainerReference("images"); if (container.CreateIfNotExists()) { container.SetPermissions( new BlobContainerPermissions { PublicAccess = BlobContainerPublicAccessType.Blob }); } } public string UploadImage(HttpPostedFileBase image) { if (image != null &amp;&amp; image.ContentLength &gt; 0) { string imageName = string.Format("photo-{0}{1}", Guid.NewGuid(), Path.GetExtension(image.FileName)); CloudBlockBlob blob = container.GetBlockBlobReference(imageName.ToLower()); blob.Properties.ContentType = image.ContentType; blob.UploadFromStream(image.InputStream); return blob.Uri.ToString(); } return null; } public void DeleteImage(string imageUrl) { if (imageUrl != null) { Uri uri = new Uri(imageUrl); string imageName = Path.GetFileName(uri.LocalPath); CloudBlockBlob blob = container.GetBlockBlobReference(imageName); if (blob != null) { blob.Delete(); } } } Simply call in your controller: ImageRepository imgRepo = new ImageRepository(); Or use DI. Also, dont forget to add in your Web.config: &lt;add key="StorageConnection" value="UseDevelopmentStorage=true" /&gt; //I'm using the storage emulator Using Azure.Storage &amp; Azure.ConfigurationManager nuget packages. I'm open to critiques.
Yeah, if I was more confident in my 'programming' abilities I would. But I could spend weeks farting around and not get anywhere. 
Shweet!
The only thing you can make less smelly is return null instead of a new instance. Besides that.. This is extremely smelly. This will tightly coupled code, and you'll rely on static properties to be set. Makes unit testing impossible wherever you use this class.
I generally found, once past very basics, Googling C# equivalent to Java &lt;class/etc.&gt; [ReSharper](https://www.jetbrains.com/resharper/) taught me a fair amount of more advanced areas as it often suggests alternative pattens e.q. LINQ to replace for loops 
I would suggest using a Flags Enum instead of a list of Enums.
Are you providing values for the enums? public enum TestEnum { test1 = 1, test2 = 2, test3 = 3, test4 = 4 } Are you using NET 4.5-&gt; ?
You can use Flags as /u/Kanegou suggested or values like /u/tommis suggested, but you also need to make sure you have a default 0 value, such as NoCategory = 0, otherwise you may get errors.
I enjoyed Adam Freeman's [Pro ASP.NET MVC 5](http://www.apress.com/9781430265290) when I was coming to grips with asp.net.
Rule 1, don't use EF migrations if you actually care about your database structure. 
[removed]
ASP.NET Identity is the way to go http://tektutorialshub.com/asp-net-identity/
Thanks I'll go check this out
No, I'm not explicitly giving each enum a value. But yes, I'm using .NET 4.6
what's the value of this over just using the owin test server? 
That's actually not a bad idea... You mean to have every tenant/customer have their own docker container running? The main problem is, I'm starting up new, and my main worry is paying for server resources (as I have 0 customers as of yet). Being able to run things (at least at the start) in a single Azure WebPages (without having to learn container-management) is weighing pretty heavily. Though, containers is probably the way to go eventually.
1. It leads to a very juvenile database schema that doesn't even remotely leverage the capabilities of the database server. 2. It generally defaults to giving the application way too many privileges in the database. 3. It assumes that you'll have one and only one application every touching the database, an assumption that never lasts over the long-term. 4. It doesn't allow the database and application to version separately, severely limiting the abilities of the DBA to improve the structure. 5. Unless extreme care is taken, it can be difficult to predict the exact steps that the migration will use, which can result in excessive down times. The only time I recommend EF migrations is when you are working with individual files that, instead of XML or JSON, use a relational database as their format. (e.g. SQLite, SQL Server CE, 90's era Access)
1. No idea what you mean here, as it does everything just using sql commands which you can customize as wanted. You can use views and sprocs as wanted. 2. Privileges are set by the user account you connect as, not EF. 3. Use a common/"shared" project for your data and business layers. 4. You can, but usually the database and application are mostly synced anyway. Any major DB changes will always break applications assuming tables are modified. 5. Uhh what? Have you not used it in a few years? It straight up creates sql commands that you see before you run them. https://msdn.microsoft.com/en-us/data/jj591621.aspx?f=255&amp;MSPPError=-2147217396 Look at the up/down methods to see how it creates tables or whatever other changes it needs to "sync" the application(s) models and database.
&gt; I swear, the amount of effort you people go through in an attempt to avoid using SQL is mind-boggling. But you can use straight SQL with it, so whats your point? I do often. Seriously when was the last time you used it? EF 3? Its been like that since **2012** or earlier. &gt;The up/down methods combine everything that's wrong with maintaining individual roll forward/back scripts with everything that's wrong with trying to express DDL operations in C#. What do you recommend?
I shudder to think of what nonsense led something to try testing a MVC view outside of the browser. 
&gt; owin test server Haven't used the owin test server before. In and of itself, Approvaltests doesn't actually host the server (was using cassinidev for that) so I would imagine that you could also combine it with owin test server. This is allowing test code to inject into the pages and give a way to verify (assert) the output. Also, I couldn't figure out if there was a way to test routes with owin. Was I missing this?
just trolling? or do you honestly not see the value in knowing that your views are rendering correctly? 
Well, views aren't supposed to have much logic in them. Maybe he is a purist and doesn't have any crazy logic trees going on. He could also be using coded ui testing. 
there's the simple answer. thanks. We've been 100% on webapi for quite some time now, and it never even occurred to me MVC wouldn't be supported.
You can go the html route with apache cordova
&gt; Also, I couldn't figure out if there was a way to test routes with owin. Was I missing this? You can't. OWIN does not support MVC 5.
thx, it helped me
Well uber is a multi national corporation not a hobbyist. Tinder went in series A in venture financing. So they had lot of money for just the business and one app. They were never hobbyist and the quality if their product at that level of finance is good. Other company publish dozen of apps with a smaller budget than tinder or uber. I'm not considering myself an elitist. But if you can't afford a ferrari you don't address them an open letter complaining that you can't afoord the car.
Coded UI is definitely an option if your UI is stable. We're generally doing new code, so we've found that just a manual tester is more cost efficient. My primary objection is that it is testing the wrong thing. If I add a compatibility hack to fix the rendering in IE that doesn't negatively affect Chrome or Firefox, that shouldn't result in a failing test. Yet it will if you are trying to unit test HTML generation.
Where do you think Uber and Tinder got started? They didn't walk into a VC office and say give me money. They started as independent small apps. And your Ferrari is just fucking stupid. 
Why is the ferari example stupid? Because is not software? Is a product like any product. And plese see the history of how uber and tinder started. The ideea that you don't need to be a business and think like a business is what drives quality to the ground.
While I agree with your sentiment towards writing database migrations in C# (or Ruby, or any other language that invented its own DSL on top of SQL), SSDT isn't any better. If you rely on a tool to _generate_ a schema migration script, while most of the time thing will work out, the edge cases will bit you hard. What you want is a tool that atomically applies sequential patch scripts (i.e. migration scripts). This tool can and should be used both in development and production environments to ensure predictable and consistent database upgrades. A lot of these tools exists, but I'll share my own implementation that's been used operationally for years: https://github.com/jdaigle/SqlMigrate.
I find that sequential scripts don't scale well when the team size increases. I've run into too many issues where multiple developers all claim that their script needs to be run first. It also makes code analysis harder because one can't see the complete picture without having to deploy the database. I want to know at compile time whether or not a column is missing.
&gt; Why wouldn't MVC be scalable? https://www.youtube.com/watch?v=nYkdrAPrdcw &gt; What does .NET introduce that makes ASP.NET MVC not scalable? Views have a significant first-use and memory cost due to the way they are compiled, when a view is run two JIT operations run; first to build the requested model tree and then to load the assigned model in to the tree (2nd op runs every request). SE use their pre-compiler to eliminate this problem. Caching doesn't actually cache output, it just stores an in-memory version of the model to prevent calls to the backing store and DMX still has significant work to do in order to render a view. SE use their own front-end cache to eliminate this problem, .net is used to generate resources for their cache rather then the cache being a component of their .net apps. &gt; Facebook surely uses MVC or at least some of it's characteristics. Facebook uses [Flux](https://github.com/facebook/flux). &gt; What is your source about Stackoveflow using custom compiler? [Their architectural overview](http://www.dev-metal.com/architecture-stackoverflow/). [This](https://github.com/StackExchange/StackExchange.Precompilation) is the current compiler they use. They also use their own [EF replacement](https://github.com/StackExchange/dapper-dot-net) for building models.
I was nasty to the guy that said hobbyists were destroying IT? Oh please will you ever forgive me. Also, I'm not sure you know what the word 'ignorant' means.
The word means: 1.lacking knowledge or awareness in general; uneducated or unsophisticated. 2.discourteous or rude. I stand by my statement that you are being ignorant. [Definition](http://i.imgur.com/7S0q3uS.png)
Have a look at [React Native](https://facebook.github.io/react-native/)
How does it handle database updates and rollbacks?
SSDT compiles your database schema into a dacpac. This is then used to perform a diff and update to the database. (SqlPackage from the command line or from VS or SSMS if you prefer a GUI.) To roll back, simply deploy the previous version of the dacpac just as you would deploy the previous version of your website or application binaries.
I don't really understand where people get off demanding price reductions, 'logicking' what is almost certainly a thoroughly investigated and modelled business price point in their own favour as if Joe Blow off the street has any idea what would get Company X more money vs their own sales and executive staff. There is an appropriate way to go about asking for this sort of thing - 'hey Company X, could you make a better/bigger student and or indie licence scheme please, I'd love to learn to use your product or use it to help grow my small business'. What is linked boils down to 'hey Company X, you are so stupid since you aren't targeting me personally as a demographic, here are some warm fuzzies without supporting numbers I think should convince you even though I don't know your revenue streams or corporate goals'. Which do you think is more likely to get a positive response? If I was making decisions based on this open letter my reaction would probably be 'lol. Poor people whining about shit and they want me to do what?'.
Good news everyone, we have found the cause. Now only need to find a fix. We set the culture at start-up and any different culture then "en-US" breaks
Great work. I have a similar set of helpers and libraries I use in typescript applications that mimic .net functionally 
I'm on my phone, but webclient is probably what you need. Specifically the download string method.
I would highly recommend mocking up the entire UI however you feel comfortable doing it (but no code not even html) and reviewing the mock-up with the users. I say no code because the temptation is too great to turn sloppy demo code into a mess of a production system. An almost working UI can also give the users the impression that the project is almost done. On planning you're basically outlining the work needed to support the UI and estimating how long each piece will take.
That is a righter answer but the next question is "how do you secure that?" You've still got things taking network requests for ad accounts at the end of the day. 
Anyone care to comment the on the tools suggested?
Also a message queue could work. 
Not quite -- the problem isn't securing the privileged account~ but rather securing the app so it can only do the things it should with the privileged account. ~ No need to bother with even holding the credentials, you can do a few things that never involve you directly holding nor needing to encrypt the credentials.
1) Content-Type: application/json header 2) Result Filters : http://www.asp.net/mvc/overview/older-versions-1/controllers-and-routing/understanding-action-filters-cs 3) Media Type Formatters
You just answered some questions I had, ie why do some requests (from a bank for instance) take up to 24 hours, or whatever. I imagine something similar to this. 
I nearly always have a browser tab opened with DHC, a Chrome extension. Very easy to use and very versatile.
That's what we do, yes.
Thanks!
I can't see anyone "against" MVC in the post you linked. However, you could read up on MembershipProvider and extend that, Identity, Entity Framework code/database first. Simply, yes you need to store your values in a persistent storage such as a database. Then you grab that data to display in your site. If you want to do some kind of CMS system on a deeper level I would believe there is some database design to do before you start coding. Thinking it all out. you can learn alot by going through examples on www.asp.net/mvc and a nice way to do some database work is Entity Framework Hope this help you out somewhat!
If it helps with some ideas... because of what we're serving up, uniquely-themed pages per tenant, we actually programmatically set the base tag in our template based on environment and tenant. The big downside of this is that means we need to fully qualify any urls for our app. The upside is that it makes it easy to serve all static content from the CDN.
It doesn't even need to take that long though. On the site I work on, we use queues for backend processing all over the place, yet they're timely enough that we can hold the user at a page with a spinner while we poll from the page to see if the processing is done. If the processing completes, we send the user to the destination page immediately, though occasionally it takes a few seconds and we do the email thing. 99% of the time it completes almost instantly and the user is completely unaware of what has been going on in the background. The real beauty of doing it like that though, is that we have internal web interfaces to all of the processing systems so we can trigger various processes off at any point in the chain whenever we need to. I've even created the odd console app here and there to kick off large batches of jobs if we've needed to do some kind of bulk update.
You are welcome! 
You could try Oxyplot. RectangularBarSeries class lets your represent almost any data with good performance. Additionally, it's free.
Why no MVC or Razor syntax ? No one should be using Web Forms.
I would recommend [ActiPro. ](http://www.actiprosoftware.com/) Download the evaluation, it should have a compiled control sample which demonstrates all the stuff they offer, from gauges (the reason I got it) to chart controls. In a fit of generosity, my company hooked me up with a license awhile back and I've been loving it ever since. 
Did you restart Visual Studio? :)
Yes
I guess there is money for prolonging a problem. That does not make it right. This tutorial would have a magnitude more relevance if it was using modern technologies.
Why does the JVM have so many languages? Mostly in my opinion (as a java dev) because Java is pants, and was out of date for a really really long time. So people built other languages on the JVM to maintain compatibility, but improve the languages they were using. .NET doesn't really have this problem, C# is pretty great, we also have a pretty good functional language
The big question that I and a lot of other .net devs would have is "what's the benefit to having a bunch of different languages?" - the only rationale you've given is "JVM has it." Java also has eleventy billion UI frameworks, but that isn't necessarily a good thing on its own. Groovy is really only Java + additional stuff that people want in the language (less boilerplate; string interpolation; some dynamicness; closures; traits) but that wasn't practical to push for in Java. Scala's another "we can do java but better" type language. Kotlin is a "we wanted something scala-like but it compiles too slow" language, so here's that and some of the stuff we wished was available in older versions of Java. Clojure is an implementation of Lisp, which was done to provide functional programming the Java ecosystem. Each of these languages is cool and serves a need in the Java ecosystem; but they're not an indicator of the "healthiness" of Java, instead showing the difference in how developers try to solve their problems in Java vs .NET. Further, some (such as Kotlin) are mainly practical because they let you run modern-looking/modern-featured code on devices running older versions of the JVM -- OTOH, .NET is typically up to date in most places it's deployed, so that's not a necessary thing. .NET developers are more likely to hold their breath waiting for MSFT to solve their problems. Java developers grew tired of waiting. The fact of the matter is that C#/F# are moving fast enough that it covers the main pain points that drove Java devs to want to implement their own functional languages and extensions to Java. If you look at C# code from 2002 and compare it to now, there's a crazy huge difference. If you compare Java, there are differences, but they're not quite as big as LINQ, async, etc etc. Even if there were a need for additional languages, these would lead to community fragmentation, which would lead to even fewer OSS projects and libraries easily usable "in your favourite .NET language". I agree that more choice would be "cool", but I don't necessarily think that it would help the .NET ecosystem, as the problems faced by .NET are way different to those faces by Java.
I actually do. But yeah, I do feel for you guys. I wish you all the best.
&gt; The big question that I and a lot of other .net devs would have is "what's the benefit to having a bunch of different languages?" - It lets us use libraries that aren't available in .NET. For example, the SEC* was thinking about mandating that all CMO calculations be available as a python program. (CMOs are crazy complex and each one has its own BS rules.) This means .NET would have to have access to python code if we're going to continue using it in the financial sector. *: Oversees stocks and bonds in the US. 
That blog is truly awful to read. This see-through background with the code that mixes with the actual text.. Makes my eyes hurt.
No, I get the same thing I struggle to read text at the best of times and I had to copy and paste your article into notepad to read it, the background text is incredibly distracting for me :(
Error code? "It doesn't work" doesn't help much.
The CLR has the following languages that are all active: From Microsoft (I'm sure I'm missing something): * C# * F# * VB.NET * C++/CLI * PowerShell From elsewhere: (have been updated w/in the last month) * IKVM * Boo * IronPython * IronRuby * etc. I'm not sure what adding another language would actually bring to the table though. I don't see much benefit to using a language on the CLR vs. just using the native implementation of that language. Sure, you would have access to .NET libraries, but that just leads to a lot of "weirdness" in that you are not using the same libraries that are typically built-in to the particular language or tying your code specifically to the CLR (which has historically been frowned upon).
Which is why I didn't scream when the SEC made that proposal. Speaking of which, I wonder if it passed. Since then I went into EMRs so I stopped following the news.
VS2013 doesn't support nuget 3.0 easily... I think you could try clearing out all your packages as if you have no nuget and try again. VS2015 doesn't seem to have a problem.
Till someone slips up and skims over the documentation.
Since it is also behind a firewall you have two layers of protection. Three if you verify the senders credentials.
To see the value of OData, consider its alternatives. You could write yet another "service" layer, which itself just exposes nonstandard subsets of the filtering, ordering, and projection capabilities which OData provides. As your SPA grows and goes through design evolution, the debt associated with expanding this service layer to cover new needs will become apparent. Now any time there's a UX change, you're updating the service layer as well as the UI, since change requests will rarely fall within the carefully limited capabilities of your service layer. Without a unifying query abstraction, the service interfaces will tend to diverge, pushing more complexity onto the client. OData isn't a cure-all, but it helps avoid a lot of very real and expensive problems. You still have fine-grained control over the query/modification capabilities supported by any of your resources, so it has none of the moral hazards associated with clients directly using ODBC.
I'm not trolling or rude, I'm just direct and blunt.
NancyFX. ReactJS.
This is another [link] (http://tektutorialshub.com/asp-net-identity/). It is a four part tutorials which explains the basics
It seems that you are not publishing your site properly, you may read this post - http://aspnetmvceuropeanhosting.hostforlife.eu/post/European-ASPNET-MVC-3-Hosting-Deploying-ASPNET-MVC-3-Application-on-Shared-Hosting-Environment.aspx This help for me.
If you are the author, props to starting a blog. Try to challenge yourself a little more with your topics. Another advantage is view models can have data from multiple domain objects. maybe offer some solutions to the problem your describing. Like automapper, valueinjector etc.
I don't get that error tho. I think it's because my hosting site haven't allowed the Roslyn compiler. 
Well that's pretty much the REST mindset. Personally I think it is a bad idea for most applications, but once in awhile REST makes sense.
https://github.com/iloire/asp.net-mvc-example-invoicing-app Demo: http://www.vitaminasdev.com/invoicing ASP.NET MVC3 invoicing app used in training purposes. Code first Entity Framework, custom membership provider, partial views/actions, HTML helpers, AJAX partial rendering, data annotation validation, custom T4 templates, NUnit testing, ...
That is the information to populate the list, why would he send it back to the controller in a post? All he would need is the users selection index.
I'm not aware of your form, but I would have thought everything the user fills out e.g. country, card expiration you would want to post it back to the controller to store somewhere? Then a view model would be ideal. I'm not saying Viewbag doesn't have it's uses, but most of the code I have seen containing ViewBag is terrible, 99% of cases with ViewBag is a code smell.
Sorry that is my mistake. In that case, then sure it's acceptable.
I bolted OData onto some services and found that the querying capabilities offered a great many innovative ways for customers to cause performance issues in the database. Strictly defined query and data export APIs only ever since that...
Don't use the MVC controller if you are using API only. Throw up an index.html in your root project and go from there.
What do you mean by templating? Using Razor a lot? Not particularly, as I'm doing so much with Angular. I guess what I'm wondering is if it would be better to do more server-side templating rather than rely so heavily on Angular.
Layout pages (Master Pages), I can't image if you're using any MVVM that you'd use much razor. I'm currently building a very large application made up of many Angular Apps, we decided to go strictly static HTML for the client and Web API for the back end. We also threw in GULP to help "build" those static files, we haven't found a good method to template boiler plate HTML with GULP, it's getting annoying having to manage global changes as well as starting a new app, but other then that just using static HTML has simplified development in other ways . P.S. anyone with experiencing using GULP to help with templates would be awesome.
Good to know, but I'm guessing you used the client to interact with an underlying API running on the full .net stack?
I'm still waiting for that .net port of Scala they were working on years ago.
This is pretty close to what we are doing. We have a single MVC controller that serves the base page, and all of our templates are plain html files. We use web api controllers for all of our data. We use razor for bundling on the main page, as well as to get some basic configuration info so it's there when everything starts up. We are using bower for front end packages, with npm and gulp for some front end build stuff (sass compilation, typescript definition files, etc.) I'd advise against nuget for front end packages, as it's not really geared for it. There's no way to control where files end up, and you have to include them in source control because package restore doesn't copy content files (which is a huge pain).
I've found using *.cshtml to be more useful than *.html. Specifically I can remove parts of the template when a user doesn't have specific permissions. That's how I use MVC with AngularJS. Outside of that, I basically have WebAPI living inside my "MVC" application.
There's a happy medium between returning a raw EF IQueryable and going with SOAP-style RPC. WebAPI allows you to choose how much validation or processing to apply to a query in an OData controller. Depending on your style of application, your command model might not match particularly well with your query model. You can represent these in an OData model as POST-only resources, and perform arbitrarily complex transactions in the controllers. EF can also help you with atomicity if you use row versioning. I'd say the main misconception to avoid is that an OData interface is necessarily a full-CRUD, validation-free, anything-goes free-for-all. Early iterations of data services might've been that way, but WebAPI lets you layer in more specific behaviors as needed.
Oo.... Maybe... It will better for you if you consult it with your hosting provider. 
use webapi for backend + angular for frontend no need to use classic asp.mvc razor engine
&gt; remove parts of the template when a user doesn't have specific permissions That's what ng-if is for. As long as your data source is being smart and scrubbing data that the user should not see, there should be no problem
Except I'd need to be able to interrogate permissions at the client side for that to work. I'd really rather not be bothered to digest identity claims just so I lean more on angular.
Just to say, that strategy is doomed to failure! If you want to restrict access to certain pages, use ASP.Net's authentication features, and require a username and password
Yes. Security on the front-end is not security.
One of the [breaking changes](https://github.com/aspnet/Announcements/issues/71) says that Authentication middleware now ignores ConfigureService options configuration, but the project template still configures cookies this way in Startup.cs I'm currently trying to upgrade a beta7 app to beta8, and I'm finding a lot of issues that aren't mentioned in the [list of breaking changes](https://github.com/aspnet/Announcements/issues?utf8=%E2%9C%93&amp;q=is%3Aopen+milestone%3A1.0.0-beta8+label%3A%22Breaking+change%22+)
ill go get the musicstore sample and deploy it to azure to remember how it all works
Could you elaborate on why it is doomed? I don't doubt you, I am just curious as I am trying to learn. I should probably also elaborate a little: We are starting a new offer during the holiday season and we don't want our competitors to see it (at least hold them off as long as possible). Our competitors copy us constantly and almost instantly, so we are trying to find a way to give them a different experience and increase the time it takes for them to copy us.
Do you have a smartphone in your pocket, with a data plan? The public facing IP address of your smartphone probably changes constantly, so your competitor could just whip out their phone, browse to your website, and see the offer. There's no way to prevent that.
Aww I see what you are saying about it being doomed. However, in this instance, we are just looking to delay them seeing are new offer and copying it.
That looks good, thanks for the reference.
I have nothing anything against PowerShell, RegEx, Closures, - on the contrary. Just use every tool when it's appropriate. But I do have something against TDD. I can't [quote](https://www.facebook.com/notes/kent-beck/rip-tdd/750840194948847) it better than Kent Beck, the reviver of TDD. Yes, your right, I'm a stodgy old man with a beard showing, but I've had a long day looking at pretentious code, that could have been a lot more simple.
Razor and MVC let us use .NET's built-in Cross-site Request Forgery protection: http://blog.novanet.no/anti-forgery-tokens-using-mvc-web-api-and-angularjs/
I don't see much use besides doing mini apps where needed and letting mvc do the heavy lifting view wise. My setup is to use yeoman, yo angular, then add a web.config to it to serve it in iis. I make a second project in vs as a web api for the 1st project which is the yeoman angular template. 
AND if you're POSTing requests then you need to respond to preflight requests
For CORs you need to also add the call config.EnableCors() ; to the WebApiConfig file in the App_Start folder. Also, have you added the Microsoft CORs package in Nuget?
Silly question, is this localhost testing with Chome? I've seen this issue with Chrome failing with localhost testing. You can try disabling Chrome web security (close all instances of Chrome first), or try editing hosts file to make a domain that points back to 127.0.0.1. Alternatively, you can look at the response headers and see if the correct CORS headers are being sent to the browser.
Yes I have set it up the right way. I have no trouble getting past CORS errors with regular web api but the odata is giving me problems.
As to the odata 404 issue: In the webapiconfig register function, does the call to config.MapODataServiceRoute come after the regular webapi routing?
Have you tried this Control Panel &gt; Programs &gt; Turn Windows Features on or off &gt; Internet Information Services &gt; World Wide Web Services &gt; Application development Feature Check the box 'ASP.NET' . Also in the Web Management Tools, remember to select IIS 6 Management Compatibility and IIS Metabase as shown below http://www.devcurry.com/2010/11/resolving-aspnet-40-has-not-been.html
Interesting. I hadn't looked into this at all. So I just set one blank page to ASP 4.0 and the another to ASP 4.0 Classic. I enabled 32-bit apps on both of those app pools, restarted the server and VWD, and tried to open both the blank pages, both gave me the error.
I'm using WebApi2, and set up a global handler for CORS preflight requests. If they're detected, then it returns a 200 result, otherwise it passes the request up the chain
I think you'll need to run regiis from the vs command prompt, if that doesn't work then you don't have .net in the right place potentially. Google for a stack overflow post explaining it's usuage. Happens to me all the time
Are you sure you have all the required IIS components installed? 
I'm about to that point, I just really wanted to conquer this. It has gotten under my skin. I just don't get why it isn't working. 
I have it set like this guy shows: http://www.devcurry.com/2010/11/resolving-aspnet-40-has-not-been.html
I have express, and don't think that comes with VS Command Prompt. I'm looking for a way to get it.
Hi, By a combination of the advice here and some work on my own I got my website published. Thank you all.
The challenge asks to create a simple CRUD web app. Would it be frowned upon to use scaffolding?
And with new ASP.NET Web Application, I've got erros refering to beta5 (the default that comes with VS2015). Lol! So I wonder if there is some way to download updated templates to BETA 8. 
What platform? If desktop, WinForms or WPF? If web, ASP.NET Website or Web Application? If Web Application, WebForms, MVC, clientside JS, or a mixture of all 3? Please be more descriptive.
Sorry, predominantly MVC
No worries, I'm redditing drunk. Sorry if I came off as a dick. ^^^Are ^^^we ^^^seriously ^^^not ^^^doing ^^^"phrasing" ^^^anymore, ^^^guys?
[Atom](https://atom.io) with [Omnisharp](https://atom.io/packages/omnisharp-atom)
Absolutely. Your results will be kept completely anonymous and will only be shared with companies with whom you opted to connect. Good luck!
We used sitecore, it supports multivariate testing.
I personally wouldn't use scaffolding. That's just me though.
I'm curious what type of "challenges" you have for .Net. Can you give an example? The reason I ask is because HackerRank is very similar to what you are offering. However - HackerRank's C# challenges are as irrelevant as it can get when it comes to business/web applications, especially if you are working with MVC/Web Api.
In one word, Tests. Start with unit tests, and move to integration/functional tests. Also documentation on the code you have written would be excellent, along with things like an installer, a readme, a video of installation, usage and code overview. Basically, think from a hiring manager's perspective: What features, if it were present, would make my job of evaluating this software easier? What features, if it were present, would make it similar to what we do on a daily basis in our regular jobs? etc
In other cases though there may be tests you can develop based upon an action: public ActionResult Survey(MyModel myModel) { if (surveyApi.Complete(myModel) == NeedsMoreInfo) return RedirectToAction("MoreInfo"); else return View(); } You would mock the surveyApi to test the action taken by the controller.
First off install Entity Framework through NuGet if you haven't already. Right click your project, add new item, choose ADO.NET model, enter your SQL server name/credentials, and EF does the rest. As for displaying your data just do some research on StackOverflow. There's plenty of resources to get you started.
It runs on ubuntu and thats a version of debian right. Did you intall with apt-get? http://waynethompson.com.au/blog/visual-studio-code-easy-install-on-ubuntu/
Ubuntu is based on debian but they are seperatr. Unfortunately those instruction don't work. I tried to convert them for Debian and got nowhere.
so you're saying the problem is that wpfAnimatedGif has a memory leak?
Does it have to be GIF? How about using something such as Photoshop to extract each individual frame and then using Key Frame animation? http://www.blackwasp.co.uk/WPFKeyFrameAnimation.aspx
Don't have any worries about it all. Just curious to what people are thinking about it. I do have guys on my team who are sweating it, they can't stand all the new front end stuff and would love to just go back to Web Forms.
That's a separate problem. The current situation with front end frameworks is a clusterfuck. I think this week's darling is Angular. Or was it Handlebars? My solution was to just say "screw it" and focus on database/middle tier development. Let someone else fight with JavaScript.
Yes. That's the only thing I can think of. 
Yes. 4.5.1 installed is required, but Omnisharp can parse any C#
Personally I love it. I started in the front end and moved to full stack. It always bothered me that VS was such a great tool for everything but the front end. They're fixing it now and I love it. 
Just to prove your point, the cool kids don't like angular, they like react now. Also handlebars is a templating tool. It might be more fair to compare that to $interpolate in angular.
&gt; I don't mean to sound like a jerk but I think any developer who can't embrace change is a hypocrite. Developers force change upon everyone else, so we must accept change as an inherent part of our careers. There's an important distinction here: if you're a hobbyist, researcher, or one of the rare few who get paid to innovate for innovation's sake, then maybe this is true. The vast majority of professional developers aren't in the job of forcing change, or anything so idealistic/philosophical. They're paid to develop applications to meet business objectives. If you're good at doing this, you'll evaluate possible technological/methodological approaches just as you would anything else: does it achieve something good for the business? Is the reward commensurate with the effort and risk? In practical terms -- unless you are a solo operator doing limited-scale, totally one-off projects I suppose -- this usually means being a moderately late and cautious adopter. You change when it becomes rewarding and safe enough to justify the effort. That's not being a hypocrite or whatever, it's just being good at your job. I'm reminded of this: http://thedailywtf.com/articles/Programming-Sucks!-Or-At-Least,-It-Ought-To-
Your comment makes no sense, can you explain it a bit more? MVC is nice because it doesn't try to fight http 
I wish I could up vote this more, well said.
Lol
There has to be an issue with how you're using it. The app I work on uses it extensively and it's never consumed much memory.
This is a good pattern. This separation allows you to be flexible and platform agnostic.
What do you mean by "...MVC...still sends a lot of bloat from the server?"
Sending the HTML and data in the response. That's fine for some solutions, but not necessarily the most efficient way to run. Yes, you can make an API controller, but then you're pretty much doing WebApi anyway
I just made a one window test app. To check it usage. All it had was a grid and the image tag in it. And I just used the code mentioned on the site to load the GIF. Starts with less than 50 kb, and as soon as I load the image, it shoots to 1.4 gb. Do you mind sharing a sample of your usage, I'll provide you with a snippet of my code as soon as I get home.
I'm also out at the moment. I'll look up the code when I get home.
Also, have you tried using a different gif to test it? Like a generic spinner or or something?
The node approach that they're taking is OK for the most part. My main complaint thus far is I would like it if I could actually have any test apps that I built not completely break between versions of the beta but whatever. But the general changes in MVC, as a whole, are meh. Nothing the average dev can't handle. You're probably never going to migrate existing code over to it because of the breaking changes, unless there is a compelling fiscal reason, but green field apps will be fine. And the learning curve isn't that great. It just looks like a messed up version of the current MVC to be honest. Now if you want the real pain, any dev who builds ORMs, IoC Containers, etc. are currently looking at the changes in .net core and saying "holy fuck"... Nothing works without a complete rewrite. I take that back, simple code works without a rewrite. Write anything complicated and it's a complete rewrite. On top of that, everything changes every couple of weeks currently. I had started going down the road of rewriting my various libraries to work on the new core libraries and it's like bashing my head against the wall. I've mostly given up until the RC is out next month. I've had more pain come from that side than the changes in MVC.
Yes. Tried a couple. I noticed that if the GIF had less than 100 frames, the app would use around 700 MB. Around 200 frames and it increased to over a GB.
Ah, ok, I never thought about it that way before. So if I'm understanding correctly, you get the most out of an API controller if you're coupling it with an SPA. Otherwise you have to get the View and data together using an MVC controller? Is that right?
Thanks
This is what we're doing, exactly. MS Web API, with Dapper as a micro-ORM to SQL Server (SQL Azure), a Redis cache layer, and then Aurelia on the frontend. Nice and clean separation. 
Handlebars is a {{templating}} engine for javascript.
C:\Windows\Microsoft.NET\Framework64\v4.0.30319
Cool, thanks
How do you like Aurelia? Ever work worh Angular?
You can grab the latest from the Windows SDK
Basically viewstate is a leaky abstraction. It tries to make managing state easier, but for anything non-trivial, you end up managing the viewstate instead, which tends to be worse than managing your own state in the first place. It's not a case study, but here's something I challenged a co-worker to do in webforms. &gt; Make a simple page that has a list. Each item has a text input and a label next to it. There are two buttons. One button adds a new list item. The other button assigns puts the contents of each text box in the label next to it. This must be done with postbacks, since working around postbacks doesn't tell you much about webforms. He gave up in 90 minutes. I've done work in both webforms and mvc. To be honest, I don't even know how to do this in webforms. It's probably possible somehow. In mvc, it's 15 minutes if you have one arm in a cast.
&gt; I'm far less concerned about having to build out new a JS-based frontend as I was about changing, well, anything with a webforms (or even MVC) monolith. MVC doesn't seem too bad, but yea WinForms is pretty rough.
&gt; I don't mean to sound like a jerk but I think any developer who can't embrace change is a hypocrite. There's a huge difference between embracing change that leads to demonstrably better results and embracing change for the sake of change. You may be happy with Angular today, but will you still be happy with it next year with Angular 2 is released with a completely different API? Or what will you do if you boss decides to rewrite everything in React? At a certain point one has to stop chasing the new and develop some depth in a small set of technologies. 
reg.exe query "HKLM\SOFTWARE\Microsoft\MSBuild\ToolsVersions\4.0" /v MSBuildToolsPath --&gt; http://stackoverflow.com/questions/328017/path-to-msbuild
I may have slightly misunderstood then :) but yeah, I'm somewhat surprised this is even an issue: I always thought ASP.NET's Model + ViewBag system was pretty intuitive and a neat solution.
I haven't built anything in Angular. The syntax and "syntactic sugar" of Angular turned me off, as well as the near-impenetrable documentation. I come from a history of .NET Web Forms and .NET MVC, and Aurelia is such a breath of fresh air. It's so much nicer to not mix the front end and the back end and let APIs be the data gateway. No mixing and matching of client/server code at all. No .NET bloat. Just C#, SQL, an API to get the data, and a modern JS framework to display it. I am really enjoying using ES6, too - which Aurelia supports via Babel. It makes Javascript a lot nicer, and I don't have to use much jQuery. I can use vanilla most of the time.
IMHO it's more about aligning with "modern" trends in front end development. It's in no way because it's easier or faster to develop line of business applications with MVC over WebForms. You will not get a lot of love on internet forums when you mention WebForms because of the younger demographic. Those of us who were around before .NET and into the early stages of .NET really appreciated WebForms and probably produced some kick-ass line of business applications that made the business we worked for very successful. Every framework has pros and cons to it. This is strictly my opinion based on my experience in the business is that the younger crowd is more interested in the science of computer science. The older crowd is more interested in the business aspect of their employment. Again only my experience working in the financial sector for about 15 years. 
Basically, yes. API endpoints are also fine for a regular MVC application if you don't need to update your view and just expect some data result.
Just came across this [article](http://www.thomaslevesque.com/2015/01/17/a-new-library-to-display-animated-gifs-in-xaml-apps/), hope this solves my issue.
How do I use this, or more specifically System.Linq, in an internal module in typescript? I can't require external modules in internal modules in TS.
I have those types of changes roadmapped into my stack and fully see them coming years in advance. I think if you confront change before it happens you can integrate it into your process and it won't be such a big deal. I'm quite hopeful that Angular 2 will solve the performance bottlenecks that Angular has.
I have a lot of replies to this bubbling in my head and am not sure where to start. I'll be back
To me it looks more like MVC is following in the trail that WebApi has blazed. That's great considering how similar they are, and how WebApi was basically spawned from MVC. WebApi = MC from MVC. But there is still a huge architectural difference that is based around whether or not you are using the 'V' part of "MVC".
I'm working with [SpecFlow](http://www.specflow.org/) right now. It's working well and I could see it being used to do responsive testing as well as regular UI testing.
Try http://www.seleniumhq.org/, it can looks for css selectors. Whichever css you switch out for mobile can be programmed into the testing flow.
I say hang in there. RC will be much more stable and be glad they are fixing bad design now before it gets baked in.
I'm not exactly sure your use case, but the latest #master branch has a dist folder with almost every module type. I'm working out the kinks so anyone can take full advantage of it. Currently the easiest way is with requirejs and relative pathing. I use this method with the qunit tests. If you are developing using NodeJS then you have two options that I know of so far, you can simply point to the dist/commonjs folder which I've ensure compatibility using Babel. Or (probably better) is to simply compile the source folder to the module type you need. I will likely be converting the default module type to something other than AMD in the near future. Until then, just set your compiler to build the source folder.
non-pastebin version of the model: using System; using System.Web; using System.Collections.Generic; using System.Configuration; using System.ComponentModel.DataAnnotations; using System.Data; using System.Data.SqlClient; using System.Text; using System.IO; namespace POReportModel.Models { public class POReportSearch { #region POReport Variables DataTable dtPOReport = new DataTable(); #endregion #region Search Range [Required] [DataType(DataType.Text)] [Display(Name = "Fiscal Year")] public int FiscalYR { get; set; } [Required] [DataType(DataType.Text)] [Display(Name = "Minimum Cost")] public int BeginValue { get; set; } [Required] [DataType(DataType.Text)] [Display(Name = "Maximum Cost")] public int EndValue { get; set; } #endregion #region Search Results public string Commodity { get; set; } public string PO_NO { get; set; } public string LINE_NO { get; set; } public string Description { get; set; } public string INST_ID { get; set; } public string FY { get; set; } public string Total { get; set; } #endregion Search Results #region Search Methods public String SearchByDates(POReportSearch model) { FiscalYR = model.FiscalYR; BeginValue = model.BeginValue; EndValue = model.EndValue; using(SqlConnection sqlConn = new SqlConnection(ConfigurationManager.ConnectionStrings["BizDB"].ToString())) { using (SqlCommand myCommand = new SqlCommand()) { sqlConn.Open(); myCommand.CommandText = "POReport"; myCommand.CommandType = CommandType.StoredProcedure; myCommand.Connection = sqlConn; myCommand.Parameters.Add("@Param1", SqlDbType.Int); myCommand.Parameters["@Param1"].Value = model.FiscalYR; myCommand.Parameters.Add("@Param2", SqlDbType.Int); myCommand.Parameters["@Param2"].Value = model.BeginValue; myCommand.Parameters.Add("@Param3", SqlDbType.Int); myCommand.Parameters["@Param3"].Value = model.EndValue; using (SqlDataReader myReader = myCommand.ExecuteReader()) { if(myReader.HasRows) { dtPOReport.Load(myReader); } } } } return CreateCSV(dtPOReport); } /// &lt;summary&gt; /// Writes a datatable in CSV format of search results. /// &lt;/summary&gt; /// &lt;param name="dtInput"&gt;&lt;/param&gt; private String CreateCSV(DataTable dtInput) { FileInfo f = new FileInfo(@"C:\Temp\POReport.csv"); using (StreamWriter sw = f.CreateText()) { //write the csv column headers string strColumnNames = String.Empty; for (int i = 0; i &lt; dtInput.Columns.Count; i++) { strColumnNames += dtInput.Columns[i].ColumnName; if (i &lt; dtInput.Columns.Count - 1) strColumnNames += ","; } sw.WriteLine(strColumnNames); //write the data foreach (DataRow row in dtInput.Rows) { string strRowData = String.Empty; for (int i = 0; i &lt; dtInput.Columns.Count; i++) { strRowData += row[i].ToString(); if (i &lt; dtInput.Columns.Count - 1) strRowData += ","; } sw.WriteLine(strRowData); } } return f.ToString(); } #endregion Search Methods } }
Good to know, thanks
Rosyln is a compiler sdk. Take C# and VB and turn them to intermediate language (IL). Also provides code hooks for custom rules and analysis. RyuJIT takes IL and optimizes it into machine language just in time for execution. Briefly looked at LLVM and a friend explained it to me as another intermediate language with broader support. If Microsoft can transpile MSIL to LLVM bytecode, they don't need to write core clr versions for all the other platforms they don't currently support. They could also take advantage of the LLVM JIT instead of porting RyuJIT. .NET native skips IL and goes straight to machine code from C# and VB 
&gt; Depends on the parameters You can try and add or increase `ConncetionTimeout`parameter in your connection string (see here https://msdn.microsoft.com/en-us/library/system.data.sqlclient.sqlconnection.connectiontimeout(v=vs.110).aspx) although as @wildwolfay5 suggested you may want to rethink your query as forcing user to wait that long degrades the experience.
Horrible title...makes it sound like MS is taking a big step backwards!
There are a couple of different ways you can do this. Look at the query that's being ran in your profiler (if you have one) and see how long that's taking. The other approach is to add indexes on fields which are being used to filter on. Also, it looks like you're calling a stored procedure? If so, what does that look like? Lastly. Don't open your connection until you actually are about to execute something. Let us know if any of this helps or works for you Edit: Spelling 
Although not technically the same as Azure Service Bus or RabbitMQ, I found that Udi Dahan's NServiceBus has a really good knowledge base and hands-on (http://particular.net/HandsOnLabs). I think NServiceBus takes a different approach than something like Azure Service Bus, but I was able to replace some legacy .asmx services and take advantage of basic "fire and forget" and "store and forward" advantages that you can gain from this. Particular also has some really cool monitoring and administration apps.
I learned C and C++ off some old deitel books and actually liked them. This one I have now is really bad, but it was free and I'm almost done with it. I get the impression they had a shift from effective smaller business to big useless corporation at some point, but I don't really know. What I would like to find is a series of books that teach languages assuming you already know how to program and also is light on words, heavy on examples. This deitel book is a rambler. Which books do you prefer?
I don't have indexes on the searched fields because I'm not sure if I can/should nor have I ever tried to. Seems like a DBA responsibility to put indexes in? That's what I got out of: https://technet.microsoft.com/en-us/library/ms191195(v=sql.105).aspx
I'm your web.config where it days "connectionString" append the following text in the quotes ConnectionTimeout=&lt;some time I minutes&gt; IIS reads the web.config file and should inherit this timeout 
I guess my other quess would be to check and make sure that your database is running. By opening up SQL configuration management and seeing I the instance as a stays of running. 
I use Bootcamp for this exact situation.
I agree, but none of that is really VS specific. That really should speak to your shops own waffling on technologies. But yes, the front end is crazy right now.
Yes, Bootcamp.
&gt; Which books do you prefer? In general, anything not written by Deitel and Deitel. For C# specifically, I give the people I tutor two books: Basics: http://www.amazon.com/gp/product/1430248602?keywords=Expert%20C%23&amp;qid=1445360278&amp;ref_=sr_1_2&amp;sr=8-2 Theory: http://www.amazon.com/Framework-Design-Guidelines-Conventions-Libraries/dp/0321545613/ref=sr_1_1?ie=UTF8&amp;qid=1445360352&amp;sr=8-1&amp;keywords=framework+design+guidelines
In my early tests I have basically just been pushing entity objects as messages ie. Product, User, etc. and then letting each subscriber kinda figure out what to do with it. Sounds like messages be structured as more discrete events. product_added, user_udpated, etc. How much data do you send along with the message so that it can fulfill the needs of every subscriber? Subscribers should not be accessing a shared data store (ie sql server) correct? For example, an "order created" event would obviously need the order info, all the products, and the user info to be able to send a confirmation email. What if you also wanted to include information about the manufacturer in the email, would you add all that to the initial message as well? As requirements and new subscribers are built it seems like the messages would end up being significant object graphs and tailored to the specific needs of each subscriber, this feels very tightly coupled.
An OrderCreated event doesn't need anything more than the OrderUID which will allow the order to be retrieved from storage (repository, database, nosql, etc) by any microservice that needs it. Carrying much more than that causes exactly the problem you describe and the messages/events get bulky and unmanageable. Edit: I may have oversimplified, but it sounds like you're just starting out. Rule of thumb is KISS and while you're googling microservices and CQRS, also take a look at [SOLID] (https://en.wikipedia.org/wiki/SOLID_%28object-oriented_design%29).
Perhaps you simply don't pay enough or are not attractive enough otherwise.
1. open your .edmx diagram under your EntityFramework project. 2. right click and select 'update model from database'. 3. under the 'add' tab select the Stored Procedures and Functions. 4. check the box next to your stored procedure 5. click finish 6. compile 7. in your .cs file type Context.MySproc( You will now have access to that stored procedure Entity via the database context just as you would with any other entity. 
Would you say only devs with senior level experience can be that choosy? I'm being similarly choosy at about 3 years experience and the hunt has been really slow going, considering just taking another crap job but in a new language or tech stack to just get my years of experience up before trying again and being picky.
100% this. I've never applied for a job myself. Average time looking for a developer job is literally days. Im not special, theres just not many developers and tons of recruiters
I've taken jobs that didn't fit my criteria and in most cases I've ended up regretting it. But sometimes you just gotta pay the bills. In the end, each job is what you make of it... so even if it isn't your ideal "I wanna work here forever" kind of job, consider what you can leverage it into in terms of new skills or networking. You can still be choosy, but maybe less picky.
With patience. The company that hired me said it took them a good while to find an experienced ASP.NET MVC developer. My guess is lots of people missed the early boat to MVC either due to lack of opportunity or by choosing to stick to what they knew. I myself was fortunate enough to finish a long WinForms gig and starting a brand new Web project when ASP.NET MVC was in beta 1. Chose that and stuck to MVC since.
try Meetup.com for groups that cover the technology stacks you are looking for and attend.
NserviceBus looks interesting, especially the tooling around it. The lack of visibility of what is actually in a queue is also a little unsettling to me. So accustomed to pulling data out of tables to understand what is going on when there is an issue.
If you already have a team ask for referrals. Unless you're looking for someone to mentor your team in MVC, I wouldn't outright dismiss web form devs. A candidate with strong general web skills (javascript / jquery / html) and really any .NET experience that's motivated to learn MVC will quickly become productive. Depending on the employment laws you're dealing with, you could offer riskier candidates contract to hire or a trial period.
Typically, you'll want to map the results of your stored procedure to a data type. Usually, EF just tells you how many rows were affected by the sproc unless you map the results to a datatype.
I basically get 1-4 recruiters calling a week. Typically I always take their call and see what they offer even though I am happy with my current position. If there is a company I really want to work for, I will get a substantial raise, and/or I will get a higher up position I will consider. 
&gt;Thanks, that is helpful. A couple of questions: &gt;1. The message contracts are just POCOs right? How do you structure your projects and repos so everyone has access to the most updated dlls? If a specific message class gets updated how do you keep from needing to redeploy every subscriber that is using that type? They are just pocos. When a service starts up, it sends a message on the bus saying "I am service ABC, I listen for message contracts X version 1.0.2, Y version 1.0.3, etc. We then had a service that would report all this information so a developer knew what projects their contract changes affected. When a contract was updated, all the subscribers needed to be redeployed. Not sure if there is a better way to do this. &gt;2. Is each subscriber its own isolated windows service, or do you group services together based on the messages they are ingesting? We are using azure so I We had services that were grouped by responsibility. Each service would have multiple consumers that could listen to a variety of messages. A service might be in charge of customer information which would include consumers to listen for GetCustomerDetails requests and UpdateCustomerAddress commands. &gt;3. Do you use a single topic that you pump every type of message into across the org, or do you have a bunch of different topics based on the entity? I sketched out a simple task in our system and it ended up being like 5 different queues and a handful of different services which seemed overkill, but I feel like I am thinking about it too much like a workflow. We used a single topic/service bus approach which made it easy at the beginning but maybe we would have been worthwhile doing multiple at a later stage. 
I forgot about that lol. It's been a while since I had to import a new sproc into EF. Thanks for commenting.
True, messages will get bulky but that's because they are the contracts between services. Microservices querying other microservices databases violates SOLID and in my experience will causes database locking that will grind the whole thing to a halt.
I just call context.Database.SqlQuery&lt;ReturnType&gt;("EXEC [schema].[StoredProcName] @param1, @param2", param1, param2, etc) and it returns an IEnumerable of the ReturnType. param1, param2, etc. are of type SqlParameter. If you're returning a single integer, set ReturnType to int, and call .First() on the IEnumerable returned.
That works fine for the most part. I thought the same until I started playing with React and MVVM frameworks. I'm still not sold on the whole SPA thing, like, at all, but react with a JSON serialized view model: totally sold that it is a better approach for read only applications. I just haven't found a two-way binding scenario that I like with it yet.
They don't need to (and shouldn't) query databases, they can use APIs. CQRS typically manifests itself as Messages for Commands, supported by Readonly APIs (e.g. HATEOAS/REST/etc.) for the Queries. This also gives you the bounded context control that means your service can look after its own data, and you won't need to maintain a central datastore. No more worrying about what will break if you change the schema or format of your domain's data, because only one thing is coupled to it - the service that owns and uses it and this will be the reason for changing it anyway. Message content should be kept to an absolute minimum in terms of data. If the related entity has an ID, send that and nothing else. Anyone that needs to know more can fetch it from the API. If you want extra loveliness, send URLs for full HATEOAS fluffiness in your messages to tell subscribers where to find that info. IME, Messages and APIs need to be versioned with SemVer, then published as binaries, and services that depend on them build from those - not from source. Anytime you need to break compat (i.e. a bump to the Major number in SemVer world) you will need to think about your approach to it carefully, which is a good thing.
This is usually a red flag. Not listing pay generally means shit pay. Edit - this goes for saying things like "competitive salary", list your salary and I'll decide if it's competitive or not.
How many years experience are you looking for and what's the salary range? Post the job description here. 
It really depends on structure of the services. If it's a back end for the same system then having it query the database is fine. If it's a logically separate system then it's something to avoid and the message should contain all the necessary data.
Yes it is.
We currently have four developers, looking for a fifth. Two local, two remote. We have about 30 employees in total.
Dear god no. Classic asp was garbage
Nice part about ASP.NET MVC is that it is a lot closer to classic ASP than ASP.NET Webforms. Classic ASP still exists - we do about 100k a month keeping legacy ASP sites hosted and running. But in no way is money to be made learning this. ASP.NET is definitely the way to go. Classic ASP is d-o-n-e for development for the past 6 or 7 years
Are you not the DBA on this? If not, they should worm with you as the developer to index those fields. 
Depends on your goal, part of my job is maintaining a classic asp code base and, for the (internal) customers who wish to upgrade, develop enhancements and support an ASP.Net web forms version. I've been begging my boss to use a newer technology and rebuild the application in a more structured manner and rely less on a single pattern being a applied throughout but for now, it is what it is. But I've recently begun looking for a position that's more tech focused and development centric so I spend a healthy amount of time learning about new technologies via podcasts and blogs. One of my favorite podcasts is Coding Blocks...they tend to focus on real world uses of what they're talking about and I use them as a healthy alternative to 10 msdn tabs open when I'm stuck.
I'm interested working remotely. Is it a full time position?
I would agree with this. Unless your specifically talking about MVC3+ I'd say your working on legacy software at that point. Now that MV5 is out maybe its possible to even say that MVC3 is too
Commonly this is the fastest and easiest way I've ever done it as well
Are you saying that when you zoom in on your application, you want the zoom to use the cursor (x, y) coordinated as the center of the zoom?
Checkout this site. I use it all the time when I I can't remember which connection string to use Connectionstrings.com/sql-azure. You'll most likely just want the standard connection string. Out that in your web.config Install EF NuGet Point your DbContext connection string to the name you gave it in your web.config
I've setup a local service bus installation for each developer then a test, uat and prod namespace on other servers for the other environments. One other things you might want to look at for ideas is Nimbus, it's an Open Source project that takes windows service bus and gives an API that's a bit nicer. They have a few demos in the github repository. https://github.com/NimbusAPI/Nimbus
Yes, they are doing away with database first and going with code first ef. You can still take a full db and pull in all the entities it just doesn't support the edmx files functionality. It really is a cleaner approach. 
No, I'm right at mid level and I'm picky. I guess it really depends on your market.
Yep. That's good advice and I do similar. I've also included all the required info on the message at times if I couldn't guarantee the data would be present for retrieval when the message was received by the consumer or I already had collected the information from multiple sources and didn't want my consumer to have to do the same effectively doubling the number of queries. Edit: to add to this. I usually prefer my messages to be as repayable as possible with point in time information
MVC3 has a clear and rather straight forward upgrade path though, classic asp and webforms don't.
Yeah! I'm just not feeling the equation. =(
Seriously!
Also that you know ASP.Net was originally called ASP+.
I started doing dynamic websites using classic ASP for the first couple of years, then made the jump to .Net. It was a pretty big jump, for me, and I picked up bad coding habits with classic ASP. Much better to start with .Net.
Please read this article and edit your question: http://mattgemmell.com/what-have-you-tried/
What's wrong with: 1. Get an Azure SQL database and an Azure web app 2. Change your connection string to point at the Azure database 3. Deploy ?
StackOverflow is great in certain areas (London, San Francisco, New York) - but it very location dependant. I looked at it for a while, realised there was nothing local and haven't really looked again since.
Anyone can be that choosy... it just depends how urgently you need a new job. I was very choosy about my last career move (at a similar 3 year level of experience) because I was in a reasonably good job already, and was secure, so I could afford to be. If you're currently unemployed or desperate to get out of your current job, maybe you'll have to be a bit less picky. If you're happy to stay where you are until you find a new job then you can wait until the ideal job comes along, if you want to move then it's all a balancing act. Personally I'd say wait for something you really want to do: it's worth it.
This is a very good point. MVC is a pretty simple approach to solving a few problems, and poses no real challenge to a moderately experienced developer.
Like PHP? lol 
With tears....I've seen angular used wrong I'm so many ways with MVC
If you had the time, maybe do it in stages? First WebForms to WebForms with WebFormsMVP to segregate code and ensure that existing business rules can be captured first. Then move to Webforms built in MVC to shift the code-behind from individual presenters to shared controllers, then to full MVC? I feel like with legacy applications the worst part is not the architecture change, it's all those business rules that were never captured and now lie obscure under the surface like an everglades alligator. Of course, some companies will literally sit a person down with the application, go screen by screen asking for all viewed behavior, then take those screenshots with the database/resulting data and throw away all the actual code. Apparently that view is based on the idea that if the customer never saw it, and there's no permanent data record of it, then it probably won't be missed.
I think it's getting a bullet with Azure.
i did, a long time ago because it was one of the quickest ways to connect to our legacy ibm machine. at the time, i had to make a decision to jump to .net or php (.net in its infancy) and unfortunately i made the mistake of jumping to php. we weren't 100% sure of the support for our legacy system in .net and had to make a decision quick. after many years invested in classic asp, then php... i FINALLY was able to clear some time until I caught myself up and learned asp.net. it was an uphill battle, learning a new ide (visual studio), razor, mvc, c#.. then how to connect to our legacy system itself with entity took forever, *but* i will say it's VERY worth it. VS as an ide is magical. the process overall, once understood, is amazing. im still working through design strategies and haven't grasped everything, but i just got my first small asp.net site launched and its great!
As has already been mentioned in this thread, you hit about mid-level of experience and you no longer have to look, everyone instead comes to you. I haven't been actively looking in awhile and I'm still deleting 5-10 messages per week from recruiters (which doesn't mean these are all good jobs, most are temp gigs in faraway states).
Why not use Azure?
It's pretty sensible, depending on exactly what you need and your usage. Certainly not "Oh wow, this is clearly aimed at enterprise" anyway, although there does seem to be a bit of a gap between the "Just having a play around" offerings and the "Enterprise" offerings... Worth a look, I'd say. You'll probably be surprised, depending on your budget. Here's the (British) pricing calculator, but a search for "Azure pricing calculator" should get it in your local currency etc... https://azure.microsoft.com/en-gb/pricing/calculator/
Just to clarify, because it isn't entirely clear from your post: Are you asking about hosting "multiple .net websites" as in the [Microsoft .NET Framework](https://en.wikipedia.org/wiki/.NET_Framework), not the [.net TLD](https://en.wikipedia.org/wiki/.net)? 
Yea, hosting actual sites. A few custom ones. And then done standard cms sites like dnn, WordPress, etc. So I need .net and sql databases as well as php and mysql
Okay, just wanted to double check. It almost seemed like you were just talking about the .net TLD and thought you might have been asking in the wrong sub, but never mind, you appear to know where you are. :)
I thought I was reading the date wrong - this article came out in August of 2014. Has it really been that long since 4.5.2 came out? [Wikipedia](https://en.wikipedia.org/wiki/.NET_Framework#Versions) tells me that, yes, it has been
I entirely understand. We have the same processes here for framework and tool chain upgrades. They certainly put a damper on keeping *up to date*. For instance I won't get vs 2015 until probably 2017. 
This title is misleading. What they're phasing out is particular versions of the runtime. So .NET 2.0 stand alone runtime and .NET 4.0 stand alone runtime for example. Applications compiled against .NET 2.0 will still run against the .NET 3.5 SP1 runtime included in Windows Server for example. Applications compiled against .NET 4.0 will still run just fine against the .NET 4.5.2 runtime. All they're saying is, you need to be on .NET 3.5 SP1 or .NET 4.5.2 as far as your installed runtime. You are not obligated to make any changes to your application or recompile. This is literally a sysadmin issue, not a developer issue, if you want to think of it that way. Edit: I get that there are only a couple CLR runtime versions. What I mean is the installed software package. If you have .NET 4.0 installed, you won't get any more security updates. You need to install the .NET 4.5.2 software package to get security updates. The same with .NET 2.0 not getting updates, you have to install 3.5 SP1 for your .NET 2.0 app, in order to get security updates.
I have a [Winity](http://winity.io/) server that I've been extremely happy with. I installed PHP/MySQL effortlessly and painlessly through the [Web Platform Installer](http://www.microsoft.com/web/downloads/platform.aspx) tool.
Shouldn't but for some reason, whether political or technical, there is always some amount of frustration involved.
I don't use dark theme and honestly vs 2013 looks like crap no matter what. I prefer 2008s look but I require 2015 featurs
buyshared.net
One of my biggest peeves with MS documentation is that they do this too much. Rote learning a series of steps is not the correct way to handle things like this.
There is so much I want to ask and bounce off of you. Don't suppose you'd be willing to answer some more? 1. How do you handle DB versioining with multiple supported API versions? For instance, if I add a new field that I need, but the old API can't provide it, how do you handle that? Breaking changes such as renames / table name changes? 2. I still don't see how the versioning can be handled in code... do you only ever have one "version" of a service running, it just handles messages and exposes versioned APIs for the previous versions as well? What happens when you have a breaking change (say I added address tracking in my UserService, a whole new breaking change, how do you handle that)? 3. What do you see as the advantages of MQ over straight POST calls through a webAPI? What has it made better for your team? And worse? Which ones did you look at and which did you settle on? 4. What have been major boons of doing things this way for your team, and what has it made painful? I want to make a case for doing more with microservices, but several members just see it as unnecessary complexity. We already have to have several solutions open at all times because of the smaller service / apps we have right now, and I think theres some fatigue over having to edit three things at once just to pass a new field through, etc. 5. You mentioned WCF... how far down that road did you go? I know it has support for MQ, lots of identity and security features, and of course the ability to bind over TCP rather than HTTP. Just curious how you thought it held up and why you ultimately moved away from it, if it was just the WCF complexity of configuration, popularity of REST / WebAPI approaches right now, etc.
I'm a fan of Arvixe.com for shared hosting. For vps i use host1plus.com with no troubles, and about 75% the cost of digitalocean.com.
There's not many *good* developers. Lots of developers. 
If you have to use classic ASP try my ORM. https://github.com/jeremychild/Clapper 
That sounds roughly right - more than a basic Linux VPS, but not daft money.
Then yeah, the simplest way is to get a SQL Server on Azure, then change your connection over to point at it. As for deploying the database: If it's code-first ASP.NET, you can just deploy. Otherwise you'll want to back up and restore the database. Personally I deploy to Azure using a git hook, it may be possible to use a hook to deploy the database, too - but I've not tried it.
I'm curious where people got this behavior. Even in the ASPT.NET MVC tutorials they create a property and assign the property in the DropDownFor, or what have you. instead of counting on select list giving you what you need. Is this a web forms thing to do this before?
What machine are you running? What machines are your peers running? What does the source code look like? What FE libraries are you using (if any) How are you consuming the field in the BE? Edit: Spelling
It could be anything, I have loads of libraries I've written in .NET for doing a multitude of tasks (data processing, file manipulation, image processing, printing etc). It may well be there are replacements in CPython but it would be beneficial to use the libraries I'm familiar with. Of course you can write wrappers for calling these using a REST service for example but that's adding another layer of abstraction and will also be slower.
There's a use case for `Selected=true` of `SelectListItem` - when you want to specify a default value on page load. `DropDownListFor` ignores that flag as soon as target property of the model is not null. People misunderstand this subtle aspect (not their fault, MSDN documentation is crap for the most part) and bash their collective heads against the wall for hours. Very frustrating.
I think there's plenty of us that just skipped early MVC because it wasn't very full fledged, the constant releases is also not good for enterprise software stability. I've read a book on every version release of MVC and it wasn't till about 3 or 4 that I finally found it had matured enough. This of course is just my opinion as a dev manager, 12 years in the business. As a tech it's fun to just chase the next hot new thing and leave the mess for the next poor schmuck but when you've got long term stability to consider you need to pick your frameworks carefully.
I still have nightmares seeing code like the below in a 1,000+ page classic asp "application": mysql = "select * from products" set rstmp = getFromDatabase(mysql, rstmp) if not rstmp.eof then prodnam = rstmp("prodnam") proddes = rstmp("prodes") ' ... etc end if all real variable names I've seen, and no indentation.
If you bought the Zebra from the company, see if they can give you the SDK. If not, PM me, I have worked with them in the past.
Is your machine or browser set up on a different Locale than the others? A DateTime value that would bind just fine for mm/dd/yyyy, like 10/22/2015 wouldn't bind for dd/mm/yyyy.
If you want a private server where you can control everything, I really like Atlantic.Net. They charge by the hour, no agreement, and if you want there's a Rest API that you can use to manage everything. For my needs, it beats out Azure pricing hands down.
Also, I am not looking for someone to solve this issue for me, just someone who can lead me in the right direction. I will most likely have a developer do this.
Ah, it is hidden. The AddEntityFramework() call in Startup.cs does dependency injection behind the scenes. Registers the DbContext into the service container, that way we can have it injected into our controller constructor.
I use Hostek (www.hostek.com). I have about 6 sites with them. They have great support and the prices are good. 
If a junior developer gets bogged down on little things that 15 minutes of Googling will answer for them, they're not going to get far as a developer.
What exactly were you looking for? The controller code snippet shows some DI: private MyDbContext _context; public HomeController(MyDbContext context) { _context = context; }
The reason it's happening is you have a page postback. This is a common thing in ASP.NET Web Forms. Your radio buttons are likely these -https://msdn.microsoft.com/en-us/library/System.Web.UI.WebControls.RadioButton(v=vs.110).aspx and they cause a postback when they're selected. Now that you know the _why_, there are several options you have (given your technology stack) - look into https://msdn.microsoft.com/en-us/library/system.web.ui.updatepanel(v=vs.110).aspx
Awesome, this looks like it could be the issue. We have a similar issue with input fields on our checkout page, could this be the issue for those as well? Thanks again!
This is becoming even more relevant to me now as I start applying for .NET jobs. Nice topic, btw.
That code is strongly tied to a concrete type, MyDbContext. It's been a long day, perhaps I'm muddling DI and IoC together. I would have expected to see both the controller and the unit test referencing IDbContext, or some interface, and a registration somewhere that says MyDbContext is available for IDbContext requests.
I would absolutely avoid injecting DbContext into the controller, its too tightly coupled and works against the purpose of DI. Instead, consider injecting an implementation of an interface to a data Repository object that returns DTOs and abstracts the underlying data source, in this case EF. private readonly IOrdersRepository _ordersRepository; public OrdersController(IOrdersRepository ordersRepository) { _ordersRepository = ordersRepository; } 
What you are describing is a good thing for sure, but I'm not sure it is either DI or IoC. DI means the dependency is being provided to me, in this case through the constructor. IoC means ASP.NET is creating my controller and calling the correct constructor with the correct parameter rather than me creating the controller. While you can use interfaces for DI, it isn't entirely necessary. It has the advantage of being able to replace the implementation of some component with an entirely different component (so long as it implements the same interface).
&gt; DI means the dependency is being provided to me, To me DI means the dependency is resolved at runtime, when you have MyDbContext as the parameter its pretty set in stone what the dependency is at compile time. Now, I'm not trying to criticize you, I promise :) I enjoyed your post!
I guess you missed the fact that you can do your own models.
Does anyone know the lifespan of DbContext when it comes via DI? I had some issues in old projects with huge amount of queries, and doing them all in the same instance of DbContext had worse performance than recreating the DbContext (this was with LazyLoading / Proxy off). 
The way I did it was to install ZebraDesigner. We already use it for printing our labels, so I just needed to configure and modify an existing design. But you can design a concept label there (the size + position you want it). Then when you print you select the option to print to file. it will generate a file which you can open with notepad and contains plain text. Next download the ZPL language manual and 'decode'. the commands, it is not that hard, but somewhat timeconsuming. There is a small library somewhere which you can use to send the string you generate to the Zebra printer. A complete library wrapper where you can just say new RFIDTag(&lt;width&gt;,&lt;height&gt;,&lt;contents); does not exist unfortunately.
I the first versions of MVC? First I've heard of it
My memory may be failing me but strongly type views where there since V1.
1. What is being sent from the client? 2. Try changing BookCreateVM to use an `array` or `IList&lt;&gt;` or `List&lt;&gt;`. `IEnumerable&lt;&gt;` can be anything, and the model binder might be ignoring it because it cant figure out how to resolve it. 3. Try making a [custom model binder](http://odetocode.com/blogs/scott/archive/2009/04/27/6-tips-for-asp-net-mvc-model-binding.aspx)
Pretty please look at MVC, rather than trying to work in "pages". I promise you it's easier, even if it looks harder to start with.
&gt;I[s] an application like an installation wizard? That's not what I asked. I just need to know what the terminology is so I can find information on how to do it. &gt;when you click the next button...what does it take you to? Is it a new "page" or "form" or "panel" within the window? I cannot "just start developing your app" beyond very basic things because I have not been able to figure out navigation, because I have no idea what it or the destinations are called in WPF.
If you don't mind sharing your source, you could perhaps put it on GitHub or BitBucket and we can take a look on there. People are understandably going to be wary about downloading a Zip that's being flagged as a virus...
MVC is vastly overcomplicated for this relatively simple application I am developing. But regardless of design pattern, my question is about correct terminology so I can actually find information and instruction.
Translated to English Windows SmartScreen prevented an unrecognized app from running. Running this app might put your PC at risk Thats just a warning message. You don't have to worry about it. Read the following link http://blog.aha-soft.com/windows-smartscreen-prevented-an-unrecognized-app-from-running/
If you want to get rid of this, you can digitally sign the installer and the app's exe with an authenticode certificate.
First, change the CategoryList into a List&lt;CategorySelectVM&gt;. Next, verify that the posted values have the correct names, i.e. "CategoryList[index].ID", "CategoryList[index].CategoryName", and "CategoryList[index].IsSelected". You can do this by setting a breakpoint in your Controller and QuickWatch "Request.Params". If the parameter names aren't correct, it's likely because of that loop, in which case you can create a Partial View, Views/EditorTemplates/CategorySelectVM.cshtml: @model CategorySelectVM &lt;tr&gt; &lt;td&gt; &lt;label class="control-label"&gt; @Html.HiddenFor(m =&gt; m.ID) @Html.HiddenFor(m =&gt; m.CategoryName) @Html.CheckBoxFor(m =&gt; m.IsSelected, new { @class = "checkbox-inline" }) @Model.CategoryName &lt;/label&gt; &lt;/td&gt; &lt;/tr&gt; And replace your loop with: &lt;table&gt; @Html.EditorFor(m =&gt; m.CategoryList) &lt;/table&gt; 
Great, thanks for the gold! (I'm pretty new here) :o
Any effort is better than no effort. My problem has more to do with model binding from a razor view than the selected indexes in a list (similar indeed). /u/zbapoc had the right solution - I welcome you to review as every programmer should always learning - whether familiar with the problem or not. :)
I've spent a considerable amount of time working with WPF and I'm not sure there is 'standardized' terminology for what you're asking. What I can tell you is I very rarely hear the terms Page, Form, or Panel when working with it. What I do hear alot of is 'Content' (can be buttons, images, forms, etc) which is what you populate a 'Window' or 'UserControl' with and 'Navigation' which generally means changing the content of the 'Window' or 'UserControl' by clicking a button or some other event. For me navigation generally means swapping out a 'UserControl' for another.
&gt; Another great place to use interfaces is when mocking in unit tests. Exactly! This is what I guess I was expecting to see when I saw EF and xUnit in the title, and my expectations led me to make incorrect assumptions :) Thanks for taking the time to reply. If you ever decide to post an article on mocking the data layer with EF and xUnit I would read the hell out of that! Though it's probably been done many times already, I suppose I could just hunt around.
Sorry the JS intellisense is worth the price on its own. No you can get along fine if you are just doing c# and vb but for us stuff the go to definition is worth it on its own.
All right, I'll shoot. How do I, without R#: Get the intellisense to show alternatives from namespaces I haven't imported? Break out a class into its own file with a single key stroke? Break out the selected code into a separate method?
That's what I'm torn on. I'm going to try VS Web Essentials and VS Refactoring Essentials for now: http://vswebessentials.com/ http://vsrefactoringessentials.com/
I'm interested in learning the answers to the first 2, but for &gt; Break out the selected code into a separate method? do you mean something different from refactor - extract method?
Here's a bookmark I've kept that could help you: https://www.reddit.com/r/csharp/comments/3medb5/resharper_10_early_access_program_release_2_with/cvegmiu JsLint.Net is pretty nice: https://visualstudiogallery.msdn.microsoft.com/ede12aa8-0f80-4e6f-b15c-7a8b3499370e And WebEssentials is awesome: https://visualstudiogallery.msdn.microsoft.com/ee6e6d8c-c837-41fb-886a-6b50ae2d06a2
Must have plugin if you don't want to use ReSharper: [Go To Implementation](https://visualstudiogallery.msdn.microsoft.com/0ed93222-83cd-4db3-92bc-a78909047156) Ever since 2015 I've not found much need for ReSharper. I do like the LINQ hints that ReSharper has but it feels more like a crutch and forcing myself to write-out my own LINQ is a better learning opportunity.
When I say version the db, I don't mean about sharing it with other apps. Totally on board with isolation of domain aggregates, and isolation of contexts, etc. I mean more like when my service has to handle multiple service contracts (v1, v2, v3, etc.) but the data store behind those service contracts is shared... So with your Gender example, if I have one app still on the old version, the data that it puts in the system for v1 is not complete by v2's standards. So when I pull from the v2 instance, I'm missing Gender for that. Do the calling apps just figure it out at their layer? What if app2 through the v2 interface is DEPENDENT on Gender but the record created by app2 through the v1 interface didn't provide it? Wouldn't that break app2? Or do you just say essentially "null is evil, don't do it" and select a DEFAULT for those? The more you describe this, the more I start hearing a lot of the things people espouse in functional development which I, admittedly, know very little about: are the architectural pieces here similar in approach? The null thing I just said, plus some of your comments about immutability, etc. just kind of rang a bell. I for instance, am looking at basically isolating domains into their own apps with exposed APIs. For instance, having a "Product Information" web app that has an interface for entering data, and exposes a REST service for querying (and originally commands, but I'm rethinking that now). Our ERP would ETL data into that on a regular basis as well and business users would enter more metadata into that system. Then any callers could query for that information. Another example might be an order service that would take an order from your original cart example. Often, that operation needs to be syncrhonous, i.e., when a user clicks the checkout button you need to expose when an error happens... Normally I'd say I'd POST over a call to the OrderService to submit it to the backend systems, but I'm unclear on how I'd do that with an MQ... post a "SubmitOrder" command with the order, and then await some sort of process message like "OrderSaved" or "OrderFailed" or similar coming back? How would one even write that in .NET with async? Thank you for taking the time to answer some questions for me. You cleared up a few concepts and gave me some ammo for convincing some people. I think our problem domain does warrant such complexity, and would help move some other initiatives forward too...
&gt; Break out the selected code into a separate method? `Ctrl-X`, `Ctrl-V`? :-p
You can create any code snippet you wish, in whatever language. It's just an XML template, the code goes in a cdata field iirc. Google is your friend.
Right, I know you can create snippets with static text and placeholders, but can you create a snippet that copies code from elsewhere in your class and uses it? This is one of the snippets I'm talking about: class Person { public string Name {get; private set;} public DateTime DateOfBirth {get; private set;} } Now, typing `ctorp TAB TAB` results in this: class Person { public string Name {get; private set;} public DateTime DateOfBirth {get; private set;} public Person(string name, DateTime dateOfBirth) { Name = name; DateOfBirth = dateOfBirth; } } I know how to make a snippet that would have placeholders for the constructor arguments and body, but not how to make a snippet that looks somewhere else in the code to read values. My Google-fu must not be as strong as yours because I couldn't find anything about it. Care to hook me up with a link?
I didn't realize you wanted it to go that far. I can't help you there. 
First: http://visualstudioshortcuts.com/2015/ Second: 90% of my work i use 3 short cut combos (though i should know more) 1. ctrl + q allows you to open any window. start typing the window name. (it also shows the short cut to open it like the immediate window is ctrl +d, ctrl + i) 2. ctrl + , (i could be mixing it up with . ) will allow you to open any file by name. just start typing the name (if you use sublime text this is like ctrl+p) 3. ctrl + . triggers auto-complete/help to open. 
This is nice. Thanks. But without CTRL-SPACE and CTRL+T I'd be lost
You certainly cannot match R#'s functionality in stock VS2015 and you can't event match it with extensions (although you can get very close). The main R# functionality I miss since I don't have it any more: - Go to implementation with automatic decompiling - Go to type member (let's you quickly browse and search for type members/properties/methods/etc) - More advanced code quality and refactoring analysis suggestions than what most new extensions (Refactoring Essentials, Code Cracker, etc) can provide - Way better code completion and intellisense (auto-insert parentheses, more intelligent intellisense suggestions especially related to generic parameters and interface implementations, requires fewer keystrokes overall compared to stock intellisense) - Automatic `using` statement sorting - Alt-Insert/Generate Code menu. In particular, generating constructors from class properties and fields. This is by far my most missed feature, huge time saver. - Easier to generate files from templates, and easier to create templates - Better XAML intellisense - Camel case keyboard navigation (using ctrl+left/right will break between camel cased words in a string instead of the whole string) - Unit test runner - LINQ refactorings 
Maybe you can help me with something else. I'm curious about why you posted this: &gt; You can create any code snippet you wish, in whatever language. It's just an XML template, the code goes in a cdata field iirc. Google is your friend. in reply to this: &gt; Is it possible to create ctorf, ctorp, and ctorfp snippets like in Resharper? when you apparently didn't even know what those snippets do. You state unequivocally that what I want to do is possible and even throw in some snark about googling (which sent me back to Google for another fruitless round assuming I had to have missed something). If you didn't know what those snippets do, why didn't you google it first? Is Google only my friend?
F12 will go to implementation. Shift + F12 will show you all the places that use that function. Intellisense will do all the work for linq queries as long as you are using FLUENT syntax (not the SQL type) i may understand LINQ/delegates a bit better than the average C# developer though because it's very much like javascript callback functions. ( underscore/lodash makes extensive use of them...especially when chained)
I work with a CMS called Sitecore Ctrl + Space is used to figure out how all their crazy commands work for their visual studio plugin. 
&gt; Break out a class into its own file with a **single key stroke**? You can't do this even with R# (it's mapped to ctrl+shift+r or ctrl+r, ctrl+o by default) but there's a similar [extension](https://visualstudiogallery.msdn.microsoft.com/5746c6ea-1f92-43f0-8bab-ec6f8573cd6a).
Not so risky anymore. It's almost like getting a new kidney
Thanks for asking this question. I haven't personally used one, but a neck/head injury makes sitting at a desk for several hours EXTREMELY uncomfortable. I've tried various office chairs ($100 to $1,000) and various monitor lifts/stands, but I'm still in a lot of pain. To get through it now, I take breaks and do mobility exercises and yoga regularly. I'm going to look into one of these workstations. I know they're going to be expensive. I thought I remember reading something a long time ago about a guy who made one on his own. I'll try to find that post. Here's an old reddit post where someone asked a similar question: https://www.reddit.com/r/AskBattlestations/comments/2e3md3/anyone_have_experience_with_zero_gravity/
Also http://www.usysware.com/dpack/ if someone is looking for some R# feats for free
Don't forget analyze stack trace
Yeah, I believe in the default VS it's even the same keyboard shortcut as R#. Ctrl+R, Ctrl+M. 
Sorry, I probably should have written "single key combination".
I'm using 2012 with Web essentials and r# and I get Intellisense across the imported js libs. I don't use it for learning, more for getting stuff typed quicker Can't comment on the gulp situation though Maybe it's a plug in conflict if you're struggling with load time. My machine (admittedly pretty powerful) can load even my beasty 70 project solution in under 30 second
1. caret on type, alt+shift+f10 2. There are free, lightweight alternatives for that 3. Edit-&gt;refactor-&gt;extract method
It won't decompile the implementation, but you will see all class with all member stubs and the most relevant docs (i.e. summary and exceptions)
Unless f12 changed in a new version (we're still in the Stone Age and just moved to 2013), you can actually type what you're looking for in r#'s case, and use shortcuts with just the Capital letters. For example, if you want to find SuperAwesomeSampleClass, you hit the shortcut (for me, Ctrl+t) and type "SASC" and it'll come up. You can also type "ctor" in a class and it will auto generate a default constructor at that spot.
I've learned JS without any intellisense, but if you got a very large messy (legacy) codebase it is pretty much essential. I normally use sublime, but I worked down in locked down environments where my only other editor is notepad or maybe if I am lucky notepad++ . Also VS takes about 2 seconds to start up on my laptop.
Yes I know all of this, but the JS intellisense is still better with resharper.
I'd love to see the default VS "Convert this complicated for/foreach into a LINQ expression".
IMO you would save a lot of frustration by just binding to a string and writing an extension method that you can use to convert the value into a nullable DateTime
Ctrl+, is the VS way. Not quite as fancy though
In case you didn't know, highlight code, right click, refactor, extract method. 
When you were standing, were you also using an anti-fatigue mat?
How am I being a dick? I'm just trying to understand what motivates people to do what you did in this thread.
VS has the ctor snippet, but not the ctorf, ctorp, and ctorfp snippets.
&gt; We have a sizeable legacy VB codebase and the default behavior for VS2013 is "Show the object browser". The object browser is borderline useless. Do you use symbols? If yes, it'll take you to the code.
Thanks for the link. I've been using Ctrl-K, T to navigate to implementation. FWIW, I used R# until 8.x, then realised that I used only a small subset of functions most of the time, for which there were suitable defaults in vanilla VS.
Gotcha. We have no control over our machine configurations (I work at a very large insurance company that does a lot of stuff in-house) so we always just get resharper with our VS distro. We *JUST* got 2013 like a month ago...before that was 2010.
Do you have a link to the type youre using?
Nope. I should try those mats.
The point is, this is not a difficult thing to do yourself
I have the Uplift 900. http://www.thehumansolution.com/uplift-standing-desks.html I also suggest an anti-fatigue mat. Makes a ton of difference in how long you can stand. 
What more do you need? I don't feel the trade off is worth it. R# is just too much of a hog. I'm not trying to insult you, I just don't understand how it makes you more productive. I'm not a fast typist and I can usually gets most of what I want typed out before intellisense picks it up. I feel that tools like this are more of a crutch than anything else.
How am I being pedantic? It's not like some small insignificant detail of what you posted was a little off, what you posted was just plain wrong. What you did is weird to me. I've seen other people do it too and I'm genuinely curious about what motivates people like you to do things like that. Is it out of a desire to feel/appear superior to the person asking the question? How is saying, "Google is your friend" at all helpful? It's 100% snark. Did you assume I didn't know about web search engines? They've been around for a while, I'm familiar with them.
Fair point, but that's still not very compelling for me personally ...oh, we are talking about built in refactoring here, aren't we. Disregard! 
I like how you avoid every question I ask. Tells the whole story, really.
I understand you're being hyperbolic with the 20 minutes to load up comment, but it takes Visual Studio less than 90 seconds to open a 200 project solution for me, while running in a vm. A lightweight text editor is certainly much faster than that, but VS doesn't take a terribly long time to load and if you're working on a project that just has supporting JS you're probably going to have it open anyway. You may not need that for what you work on, though
maybe it's time for met to reformat my computer, but VS2015 takes too long to open and is sometimes super slow to run. I might talk my manager to throw in for a 500gb SSD but otherwise my system is pretty good for development (16gb ram on a quad core processor) 
An SSD may help, absolutely. Your machine is no slouch, that's for sure. My work machine is similar specs, 16gb ram and quad core i7. That being said I do occasionally get slow downs, especially when I have 2 instances open and one is debugging a web service and the other debugging a Xamarin app. That will bring my VM to it's knees at times. VS is definitely more robust than what is required for just doing javascript changes, though, I would agree with you there. I'd definitely look at getting a SSD if you are experiencing bad slowdowns, it would help for sure.
I also want to talk them into getting SQL developer for me. right now I'm working with a network based SQL server. It's not bad, its just the CMS i work with has super slow start up times that I want to make super fast. 
I'm sorry your parents didn't hug you. It will be ok.
Rofl, that was great. Thanks for the laugh.
Yeah, just look for elements with AutoPostBack=true. If you don't need to do any server operations immediately when those fields are changed, then remove that attribute or set it to false. They will then only be posted back when the form is submitted. AutoPostBack=True is used for things like cascading drop down lists or for when the form needs to change based on user input while they are entering data. It's better to use JavaScript to do this if you can figure out how (you should learn, because MVC makes heavy use of it and it's a much better user experience). If you're not going to use JavaScript, then use Ajax via an UpdatePanel, so the whole page doesn't refresh when the fields post back. 
Not being an asshole just making an observation. You keep incorrectly assuming you know my intentions better than I do. You need to come back to reality. I answered every question you had. You keep insulting me and when I finally strike back I'm the asshole? You are delusional.
Hmm, I wasn't actually intending to mislead, didn't consider that. I wanted to dive deep on how to run the app in a Docker container and interact with it from the Windows host. Not sure how to name it more aptly?
It's obvious it was snark. You answered almost none of my questions. You resort to personal insults, which makes you an asshole. Here are just a few that you ignored: &gt; If you didn't know what those snippets do, why didn't you google it first? How is saying, "Google is your friend" at all helpful? Did you assume I didn't know about web search engines?
You can find help on this by searching for 'navigation' in MVVM. So it's more the verb that matters: the noun you arrive at can be different depending on how you implement it.
Dockerizing ASP.NET Applications. Anyway, Windows Server is supposed to be getting container support in the next major release. That'll be sweet.
I only insulted you after you insulted me. I was genuinely curious what would cause somebody to be so confidently snarky about something they were so completely wrong about. I see it's just in your personality to be an asshole, so I guess I have the answers to all my questions after all: assholes gonna asshole. I bet you'll be back though, the asshole appears too strong in you.
EF7 is still in development, but I understand data sources other than MSSQL are planned. https://github.com/aspnet/EntityFramework/wiki/What-is-EF7-all-about
Reading the entity framework documentation I found this: https://github.com/npgsql/npgsql Which brings postgres support to ASP.NET with full mono compatibility and it seems to be endorsed by Microsoft.
Awesome. I found this among the documentation: https://github.com/npgsql/npgsql Postgres and mono support for EF7 :D
That's good to know. I've been wanting to look into .net on Linux and used to use postgres back in my lamp days.
He's specifically asking for ASP.NET 5. Only Entity Framework 7 is supported in ASP.NET 5, which is still an early preview. MySQL and Oracle are definitely not supported.
I have found that it's only noticeable when you initially start VS, after that it's pretty smooth
That's incorrect, Entity Framework 6 works fine on ASP.NET 5. I blogged about it here: http://dan.cx/2015/08/entity-framework-6-mysql-aspnet However, some extra features may be coupled to EF7 (such as the built in authentication). I haven't tested those yet. 
Any reason why you can't just use `is`/`as`?
Can you be a bit more precis in your question? A lot of Gulp/Grunt is done before deploying to a production server. So Azure or any 3rd party should not matter.
Even if you were to disable client editing and still get all the values from the postback, there's no way you can trust them (basic principle of server-side validation - don't ever trust what client sends) as that POST can be easily manufactured by a malicious party. So posting the ID is really a way to go and if you don't want to load the whole entity so you can just drop it, there's multiple ways of doing it, say via stored proc or just plain SQL that gets sent via command.
Good to know. I'll have to look into it.
This is kind of right, although it's debateable whether support for MySQL/pgSQL etc are at the same level as that for MS SQL when it comes to things like using Entity Framework in ASP.NET MVC. That's not to say they can't be used, and I can't personally comment too much as I tend to stick with the MS stack when developing .NET, but "has a driver" isn't the same as "first class support"
To pull you up at the first hurdle... ASP.NET isn't cross platform.
Depends on the definition of "first class support" really. If you mean quick turn around for bug fixes and the like, then MS are lacking quite badly. The open source drivers for other DBs are much, much quicker to get solutions on that front. If you mean "someone to help at the end of a phone/email" then not even MSSQL gets first class support unless you pay heavily for it, and you'll be relying on the community just as much as you would any alternative. I'd argue that most connectors/drivers have as good as, if not actually better than, MSSQL driver support.
Thanks for sharing! Your ASP.Net 5 posts are great, very helpful.
My definition of "first class support" would be that it supports all Entity Framework features: code first, migrations, etc.
Well I know what I'm playing around with tonight!
This is changing with OWIN (Open Web Interface for .NET), which decouples the need for IIS and System.Web so you'll be able to run on something like Linux if you'd like. Unlike ASP.NET Web API, ASP.NET MVC 5 requires you to introduce OWIN to your application. ASP.NET MVC 6/vNext will be completely based on OWIN.
In an AD environment, I'd imagine all the actual authentication happens in the AD forest, so as long as the domains are trusted, and the cross domain groups/roles are allowed to authenticate, I don't think IIS really cares too much. 
This should work for you: //The path parameter should be the FQDN of your AD domain, e.g., LDAP://somedomain.com public bool AuthenticateUser(string path, string user, string password) { try { var de = new DirectoryEntry(path, user, password, AuthenticationTypes.Secure); var ds = new DirectorySearcher(de); ds.FindOne(); return true; } catch { return false; } }
You will likely find it very difficult to work remotely in a foreign country. Businesses don't want to bother themselves with dealing with hiring such a person. At a previous organization i hired a contractor who lived outside the US but he was a dual citzenship US citizen so there was nothing special about it. Your only main shots aside from shit jobs on like RentACoder are to run your own business and live in a country you can undercut the rates of citizens but you'll still struggle. When i hired people any resume i got from outside the US was just instantly deleted.
Ugh. Is there any chance you can use Web API instead? While WCF is a better technology overall, I've found that WCF+REST is painful.
&gt; You will likely find it very difficult to work remotely in a foreign country. Businesses don't want to bother themselves with dealing with hiring such a person. Any particular reason why? It's not like only people in the US can effectively communicate in English; if one is already not coming to the office, what difference does it make where he lives?
you fall into the "over-relying on it to do your job for you" category If you believe that you need that functionality, you can (and SHOULD) : 1) not use linq 2) learn linq better 
&gt; The entire .NET Core platform including Base Class Library is a set of fine grained NuGet packages. The modular design of .NET Core ensures that each application only needs to deploy what it needs. Yes, but it also ensures that all 50 apps on the system will ship with identical copies of 100 libraries, since they come with their own dependencies. It's gone full circle from self-contained binaries via shared libraries back to self-contained.
Yes there is. Use `is` or `as` or use virtual functions like they are meant to be used. Or even use `GetType` like suggested, but I wouldn't do that lightly.
Yeah it works, but it's not as good as ReSharper's.
What is the job market like for .net developers in Denmark? As a fellow European I am curious.
In our case, we like to work collaboratively all the time, we like having easy access to each other and, and it makes us like each other more which makes us work better together. We have one remote employee in another state, and most of the day we forget he exists, we rarely collaborate on anything. And when we work from home, the experience is so frustrating because VPN is so much slower than being connected to the server 50 feet away. We know you won't be as productive, and we know any technical support we give you will cost us time and money and likely another employee's time. Unless you're a superstar hotshot who's willing to work more to compensate for lost time that comes from simply being on the VPN, or you're willing to take a decent pay cut, you're not worth the trouble for us. 
It is funny in a way, but if I take the position of the person asking this question, I can also see how it can be annoying. 
Wow, I find myself to be twice as productive working at home. No random interruptions, plus over an hour saved by not having to commute. Even in the office, most of our collaboration is by chat. I prefer that so my concentration isn't broken continually by random questions or water cooler talk. I work almost completely locally on my laptop, except for pushing to github. That means the VPN is only an issue if I have to jump on a server for some odd reason. We have centralized logging and CI, so that isn't a frequent occurrence. I guess everyone has a different culture and work style. I only work from home once a week, but I can definitely see myself inverting that ratio with no problems.
The time zone are annoying to deal with. Even working remote requires some level of management and communication. Some companies have a problem with payroll of foreign workers. There are different tax codes and not all payroll companies can manage overseas employment. Then you have the usual factors of working remote that keep companies away, so the pool of jobs is already shortened. My best guess would be big companies with international footprints may be your best chance.
Thank you. Working on next post :)
Somebody mentioned stack overflow (that's where I found my current remote job) but working nomads is another good site that use
Yes. That seems to be happening a lot, a technology starts with a particular model then evolves to some other model and then comes back to original model with some modifications. It seems that Microsoft may be trying to tackle the issue that you mentioned in your comment (somewhat not exactly). Following is an excerpt from the [Introducing .NET Core](http://blogs.msdn.com/b/dotnet/archive/2014/12/04/introducing-net-core.aspx) blog post from the .NET Blog &gt; Were also working on enabling smart sharing if multiple applications use the same framework bits. However, the goal is to ensure that each application is logically having its own framework so that upgrading doesnt interfere with other applications running on the same machine.
It's very difficult to get a H1B visa. I tried in the past to hire someone from across the pond. The visas are gone so fast they are distributed by a "lottery". It seems like most of the H1Bs go to politically connected companies that allow them to import indian workers for cheaper than american workers. There's also organizations who really abuse it. Import indians, stick them 3 or 4 deep in an apt and pay them 15, 20/hr when they're billed for 60, 80, 120/hr. Then since they're on visa if they lose their job theyll be deported that they treat them as identured servants and ship them all over the US because they can't complain or risk being deported. Our immigration system is fucked up. It rewards criminals who illegally enter our country and makes it very difficult for people we want to emigrate to get here.
It leads to crazy tax implications and regulatory burden. It can also lead to issues where regulation prevents data from leaving the country. Language barriers can be issue but i'd say that's a lower concern. Lastly there's timezones. It's difficult working with people who are asleep during your regular business hours. Edit: someone downvoted this? These are first hand reasons foreign workers aren't hired. I was personally involved with it and these burdens are what had the orgnization decide to rule it out.
The problem is that we sometimes have to work with very large data sets that we can't really put on our local machines. I agree with you completely I am way more productive at home if I'm just working on code, but even running your application and having it connect to the database over VPN, when compared to being in the same building, feels like an eternity. But getting to know your team and becoming close to them is a really big deal, it makes me love my job even if I'm not entirely in love with the current project. One of my co-workers was my best man at my wedding, we're really close. Our hesitation is mainly from poor experience, I work from home one day a week, and wouldn't mind doing it more often, but I find it very important to connect with my teammates face to face. Our full-time off-site employees are always more difficult to communicate with, more difficult to support (hardware and software), and more difficult to mentor and teach. Maybe we're just bit hiring the right people, but it costs a lot of money to bring on a new employee, and it's a gamble that doesn't seem worth the cost. And I contribute to open source projects, so I'm not against the idea of working remotely, but certain projects need lots of planning from the whole team, hours of time white boarding and discussing engineering decisions, it just doesn't translate as well over a video conference. If we were working for a web design firm, churning website after website with little attention to actual engineering, sure, the whole team can be remote. 
&gt; I set the cookie inside my callback from Instagram that sends me the auth token &gt; At some point in the future, Instagram sends a request to my app with information about a photo that someone just posted. Cookies are sent by a client voluntarily, the Instagram API is not sending you the cookies you sent them back as it sounds like it is an API and not a regular web browser. Cookies are not saved on your server. If you want to save something on your server you will have to use something else. Cookies are for giving the client data to save and to send back to you in the future voluntarily.
On the other hand, my LINQ is pretty strong at this point, although there are some oddities I'm still learning and R# helps me there.
&gt; Some companies have a problem with payroll of foreign workers. There are different tax codes and not all payroll companies can manage overseas employment. So, is this pure speculation on your part? Because this by practice it's actually the opposite. Its insanely easier to hire from afar than someone local. Right now the IRS treats any payments via a 3rd party as MISC 1099s. Even if it's 'payroll'. I hire a US citizen who lives abroad. I send them money via PayPal, or wire transfer, etc. and at the end of the year I send the 1099 to PayPal for the full amount paid and I've done my duty as far as the IRS is concerned. I don't have to figure any employment taxes, etc. The full value of their wage is part of COGs. Now, for the us citizen, they are responsible to report their income to the IRS. IRS doesn't care where you live, if you make money and are a us citizen you pay taxes. But again, since we paid PayPal, a 3rd party, we are off the hook. ...And if it's a foreign national, well then it's even easier, if it can be even more. Update: I want to clarify. Even if I hire a US Citizen living in Alabama or Minnesota and I hire them from Upwork/oDesk I still only send the 1099 to oDesk. I don't have to do any W2 stuff whatsoever. 
It's totally up to the companies you go after. Google, Apple, etc. have a lock on the H1Bs. Small companies have pretty much zero ability to get these. If you get on[ CL in SF Bay area](https://sfbay.craigslist.org/) you will see jobs that will sponsor you listed. 
Look at the error list, the output and the highest level of the references node. Is the package restore failing or still in progress?
I'm not unhappy people downvoted you because what you are saying is not true. Source: I have been working remotely for the US as a dotnet programmer for, hmm.. almost ten years now. You are correct that some companies insist on face to face meetings. You are also right that there are many business who just can't be bothered with dealing with people that are out of the country. And some who refuse to talk to employees unless they are full citizens or already have visas. However perhaps this is because of statements like the above which circulate as truth but are, in point of actual fact, just false. The way it works for me is I send the employer (actually my client) a monthly invoice. It says please pay me this much. Then they pay me. Done. This does not create 'regulatory burden' for them because I am legally an independent contractor aka a vendor not an 'employee'. Neither of us have to pay taxes on this money in the US because I am neither a US citizen nor resident in the US. Of course I do have to pay taxes in my own country. But really that is not so complicated, surely. By the way I had the same arrangements with a company in the UK (working remotely) and with a company in Australia. It's not terribly difficult. Independent vendor sends invoices for "software development services". Over the years this arrangement has worked so well that we've added a couple more people over here with a similar arrangement. The lower cost of living here does mean they get to employ more senior, more 'in demand' programmers than they can get hold of in their home state for less cost. On the other hand the communication and timezone hassles are without doubt a hassle - which we try to mitigate by having frequent online meetings and by getting together in person for a few weeks every six to nine months. I believe it's a really productive arrangement for all concerned. And to be clear it carries no tax implications for my employer at all, certainly not any 'crazy' ones. No doubt if they had tried to make me officially an 'employee' (instead of just a vendor) in a remote country that would create regulatory burden but since neither myself nor my client are braindead idiots we thought, hey why not, not do it like that. 
What? I don't think we are reading the same thread. Not even going to go after the irony of your username.... 
Yeeupp. Exactly. 
OP never stated anything about starting his own business. Even if you start your own business, unless you have great connections or run underneath another sub-consulting organization you're still going to struggle to find work.
Well, I load the viewdata and cast it into an object at the beginning of the view so I don't have a bunch of magic strings. var _userData = ViewData["userData"] as UserModel; Unless that's a *magic string* too, lol. Deriving from a custom controller (which derives from the framework controller) is something I learned early on and I see it A LOT in tutorials. Is there any advantage to doing it your way vs this?
Of course it's not ready to be called RC1, the last release was Beta8. Breaking changes are to be expected given the beta status, and they've done a fairly good job of listing them in their announcements and issue tracker: https://github.com/aspnet/Announcements Mono may not have packages for Ubuntu 15.10 yet, so you may have to compile it and install it yourself for the DNX SDK to work correctly. Latest Mono packages: http://www.mono-project.com/docs/getting-started/install/linux/
I think its really cool! Can't wait to find a use case for it
Am I consuming the action filter? I thought I was just attaching an additional process after that action filter reported back that the user was authorized. It's not the job of my implementation to do any authorization, just pass some user data to the viewdata object so I can use it in the views. Your way you would use custom HTML Helpers to interact with this data, is that correct?
http://www.asp.net/web-api As obvious of a source as it may seem. If you understand the basic concepts and theory behind ASP.NET MVC and RESTful APIs in general, it's a good starting point 
I got my start using the tutorials from Mike Wasson over on the MSDN site. [HERE](http://www.asp.net/web-api/overview/getting-started-with-aspnet-web-api/tutorial-your-first-web-api) If I'm not mistaken, he's on the team that built it and there is a ton of content from him in the form of tutorials.
I've worked with big data projects with trillions of records. Almost all of my work was against sample data, because it's just much faster. When I had to work with the real data, I was running the actual code on servers, so it wasn't like my network connection needed to transfer massive datasets. I've never churned through websites, the stuff I build runs mission critical business processes for large corporations. It just so happens that I have no problem collaborating via the internet. In fact, I talk to my cube neighbor more via the internet than I do face to face. To each their own, I just don't agree with your blanket statement. 
Microsoft virtual academy is a great free resource. I find they are very 100 level but there are some good deep dives and advanced courses.
You could try something like http://www.toptal.com or hired.com. I've never done these myself, but I've recently interviewed someone from Argentina via toptal. There are plenty of other services like these because, oddly, there seem to be more software jobs than qualified local candidates to fill them.
By not using PRG, I avoid having to rig up a means of passing around the submitted form data and model validation messages to render the next view. At most I just need to use the anti forgery token helper provided by the framework to prevent negative side effects of double submission. And while it is true that GET and POST have differing semantics, one must removed that those are methods to be called on a resource. HTTP doesn't dictate that your resource must handle different methods in different controller actions or even in different controllers, or even different assemblies. Anyways, I was just chiming in to share what floats my boat. Beer, in case you're wondering.
Team foundation server should handle that. If not, visual studio has a handy task list and bookmarks I use.
They even said in the Jabbr chat that the internal build is broken for two weeks already. Constant breaking changes in different places. By far not ready for RC1.
If you are a student you can get 3 months for free with dreamspark.
India gets a bad rep but most of the visa abuse targets foreign students studying in the US.
I didn't make a blanket statement, I said in our experience and in our case. I know it works for others, it doesn't work for us. And no, I cannot test client code that runs through data on the server. We use hipchat, even when we're next to each other, I'm not against that in any way, I'm just saying I really enjoy knowing my co-workers beyond internet chats. 
I am checking this out now. Looks like I found a tutorial on Web API.
Hi, Thanks for your reply. To be sure I understand you did as follows: 1. Used ZebraDesigner to make a label. This is what makes your actual zpl code. 2. Downloaded ZPL manual to understand the zpl code in part 1. Right? In addition, I plan on using gen 2 UHF. Does this matter or I need to have this in the zpl or .NET code as well? 
Tried it and doesn't resolve the issue. Looks like I'll have to learn ASP.NET 4.5 in the meantime before 5 goes primetime.
Treat messages on any queue like an api. It's versioned and you never make breaking changes. If you have to make breaking changes you need to publish to a different queue 
We are allowed to use php, it's just more of a challenge to use asp. At least, I did not think it would be this of a challenge.
Damn, I must have seriously overlooked the CoreCLR stuff with the changes in beta7/beta8. It took me a good while to find any documentation that actually describes installing it without mono, [turns it out's on this page](http://docs.asp.net/en/latest/getting-started/choosing-the-right-dotnet.html#choosing-the-right-net-for-you-on-the-server). My apologies for being wrong about that. I have some more experimenting to do now. :) /kicks docs Going into an RC, I really hope Microsoft puts some effort into cleaning up their documents, all of this outdated information makes it frustrating to work with especially with the breaking changes. If they want people to reliably test the RCs, the documentation needs to be ready for it as well. Overall, I expect we will see additional RCs in place of more Betas. I agree that they should have had a few more betas, but they've got their release schedule to stick to for business reasons. At least they left room for more than one RC, but it looks like it might be a rough start for RC1 at this rate. While I understand desire for supporting bleeding edge environments, Ubuntu 15.10 and Beta8 were both released this month. I think it's unrealistic to expect compatibility so quickly, especially when distributions can have significant changes between LTS releases (kernels, libs, etc.). They're working on their first stable release, so targeting LTS environments makes business sense from an initial support standpoint. It would be too costly to try and support the non-LTS builds between them. I personally think it's more likely that support for 16.04 LTS will be added during the RC phases, or shortly after the 1.0.0 release. It's only 6 months away, and there is an incentive for them to support it. Compatibly improvements (but not support) for non-LTS releases can probably be expected shortly after the official release as well, and rolled into scheduled updates. The beautiful part about all of this being open source is that the pull requests will likely come in for all of this, but Microsoft still has a responsibility to make sure it doesn't break their ability support well established LTS platforms.
Actually, this assignment isn't that crazy. First off, there are *still* classic asp sites out there which businesses maintain because they work and they don't want to invest in rewriting the entire shebang from scratch. Second, classic asp is rife with opportunities to fuck up site security. I actually support a couple internal legacy sites and have found several vulnerabilities in them just by casual checking. Classic ASP also provides a lot of opportunity to fix those fuck ups manually which can be more useful in learning the "why" behind a fix. All that said, were I the OP, I'd have picked PHP. Classic ASP is a zombie framework which is still lurching around because businesses want to squeeze as much possible life out of their investments as possible. And businesses don't give a damn about security until after they have been breached. And even then they only care in so much as they can mitigate liability. With PHP, you could grab something like wordpress, install it, throw in a few plugins and then go about securing those plugins.
[**WATCH THE INTERN FOR FREE ONLINE 2015**](http://imgur.com/ayoZBup) [**Watch The Intern 2015 Online**](http://imgur.com/ayoZBup) [**Watch The Intern Online Streaming**](http://imgur.com/ayoZBup) [**WATCH THE INTERN 2015 FULL**](http://imgur.com/ayoZBup) [**watch the intern movie online free**](http://imgur.com/ayoZBup) [**watch the intern 2015 full movie for free**](http://imgur.com/ayoZBup) [**watch the intern full movie online free**](http://imgur.com/ayoZBup) [**Watch The Intern Movie Free Online**](http://imgur.com/ayoZBup) [**watch the intern full movie free online**](http://imgur.com/ayoZBup) [**WATCH THE INTERN 2015 FREE ONLINE STREAMING**](http://imgur.com/ayoZBup) [**Watch The Intern Full**](http://imgur.com/ayoZBup) [**Watch The Intern (2015) Full Movie**](http://imgur.com/ayoZBup) [**WATCH THE INTERN (2015) FULL MOVIE**](http://imgur.com/ayoZBup) [**WATCH THE INTERN FREE STREAMING**](http://imgur.com/ayoZBup) [**Watch The Intern Full Movie**](http://imgur.com/ayoZBup) [**WATCH THE INTERN ONLINE FREE FULL**](http://imgur.com/ayoZBup) [**how to watch the intern free**](http://imgur.com/ayoZBup) [**Watch The Intern Free Movie**](http://imgur.com/ayoZBup) [**Watch The Intern Full Movie 2015**](http://imgur.com/ayoZBup) [**Watch The Intern Online For Free 2015**](http://imgur.com/ayoZBup)
I also came here to say that it might be more practical learning to use PHP, as that may look better on a resume or during an interview. You are more likely to be securing a hacked together php site in a job than a legacy ASP site.
Thanks, great idea, checking it out. Say I just want to have like 1 page with a spread sheet, and approve/disprove buttons and "update" notes feature, like an XLS. What do you think would be the best way to build this? 
Thanks for the info everyone, I'm currently using php, which I'm already pretty comfortable with so it shouldn't be a problem.
I didn't think about it until I started duplicating your steps. Can you please be more clear on how you actually "encoded" the RFID on your label? 
Feature folders are great. I built something really small with them once and didn't see the point, but now I'm getting into something a bit bigger and MVC folder structure is really annoying. It's not easy to nav at all
Log4net supports the writing of events to a database. Configure that, then source your charts and graphs with the log info from the database. Use tableau to create your visuals.
Ok, thanks! It still seems that I would have to somehow encode the data and name in the string log message though, right? i.e. "Heater1:350" to log a new heater temperature. I suppose I could have a separate logger for each data item, but that would be thousands of loggers.
Take a look at the ELK stack - elasticsearch, logstash and kibana. We use nlog to send messages as UDP packets to logstash which is picked up by elasticsearch and graphed with kibana, but I'm sure log4net could do the same. Edit: a word
Ok, thanks! I will. How do you handle separating the log messages by data value? A separate logger for each data value?
Google Sheets FTW
Can you see the messages in the queue? First step is finding out if it's a sending or receiving issue If receiving are you checking for errors on deserializing?
If you aren't married to log4net you should definitely look at serilog instead. But honestly this doesn't sound like a "logger" but rather something that just be pushing data to something like splunk 
online store, CRM, CMS
Not really the right tool. I suggest storing time-stamped values in a database. That will be much easier to plot/analyze and perform significantly better. 
SaveAllTheTime looks perfect. I have a nasty habit of doing this (JetBrain's IDEs spoil me).
Good question. While I don't have the answer, I do believe the EF generates partial classes, so you can add whatever you want in a separate file without modifying the generated entity class.
I wish a lot of tools like this had a console interface instead of being just a plugin.
See my edit on the post. Someone would have to send me the database. Also, there's lots of straight text in the logs too ('moving motor to center position' type of stuff).
I don't use them except for login. I use angular or some other js framework and handle it there. Other than that there isn't much flexibility.
If you're logging data like this then serilog will give you some structure and standardization and an easy to parse format (http://serilog.net/). There are sinks built in or you can use an nlog or log4net sink. 
I've been using Git Diff Margin for a while now and love it. Very handy when you need to quickly see the line's original text.
[ReSharper command line tools](https://www.jetbrains.com/resharper/features/command-line.html) if you aren't familiar.
In Swift, you can write *code* in emoji :D
The time series databases look interesting. The question is getting the data out of the fab, which can be difficult, especially for nontext data
Serilog plus Seq or Loggly is amazing. Note, however that the official Loggly sink is extremely non-performant. However, there is a sink for the http bulk endpoint (5MB at a time) out there that achieves insane throughout that I can highly recommend.
As long as its consistent, you know all of the 'options' (and there is a sane number of them), and there is some form of consistent separation between items regular expressions should work nicely. 
/u/hmemcpy , for some reason your site is blocked for "malware" by my work proxy.
With Web API, it's possible to build true REST services and 'achieve' Level 3 of the Richardson Maturity Model. WCF however is SOAP-based, which is no higher than Level 0 in the maturity model. The benefits of each level in the maturity model don't necessarily apply to your situation, but it's a good idea to be aware of this stuff. If you're wondering what to pick for new development, you're almost always better off going with Web API and a RESTful approach. The only benefit WCF has is when you need to send large amounts of data, because then you can easily use binary channels instead of going over HTTP which would incur a hefty performance penalty (again, in case of large amounts of data). Though you might be able to get the same benefits using Web Sockets these days (haven't tried it yet, so can't vouch for that).
Sure, but without that abstraction (injecting implementations of interfaces into your controller) TDD, and in particular mocking, become difficult and sometimes impossible to do because your tests are now dependent on an implementation for you to write your test cases. TDD is often best accomplished by writing your tests before any implementation of the interfaces at all, e.g., implementing EF.
Same. Here's an archive. https://web.archive.org/web/20151029051557/http://hmemcpy.com/2015/10/7-open-source-visual-studio-extensions-to-make-your-life-easier/
Sadly, NoGit doesn't support VS2015. Spent a day or so fighting with that issue when I wanted to use the Git-TFS bridge. My productivity is just destroyed by TFVC when compared to any kind of DVCS. Without being able to tell VS to ignore that I had made the project a git repository, I had to give up on my quest for the time being.
&gt; WCF however is SOAP-based You can use WCF for RESTful endpoints. It's not nearly as convenient as Web API, but I built a WCF-based RESTful service before WebAPI was a thing.
WCF is meat cleaver with built in rocket launcher and integrated bottle of whiskey
It's a lot of hard work. You need to find a company you want to work for, research what they have open, see if it fits, customize your resume and cover letter, and apply. It is going to require time and effort, there is no getting around it. I personally dislike recruiters. I have had a few bad experiences with them and I will no longer work with recruiters. 
Thanks. i almost forgot craigslist. 
More like they shoehorned REST onto it while they built the web API so there was an official MS Supported REST option for the masses because everyone else in the world gave up on SOAP and went REST.
But WCF's main purpose never really *was* SOAP. WCF's purpose was to provide an endpoint agnostic programming model that does *anything*- and its main use case isn't SOAP, it's NetTCP. That's why full-duplex requests are such a big part of the system, something HTTP doesn't naturally lend itself to. SOAP is something layered on WCF, it's not part of the core, and it's pretty natural to *also* let WCF do REST.
I've used that and context.OwinContext.Authenticatin.SignIn(context.Properties, grantIdentity); There is seemingly no difference between context.OwinContext.Authentication.SignIn and Context.Authentication.SignIn. The response grants are set in the AuthenticationManager and the claims principal object is in the SignInEntry object. This is the case when it is working or not. Now, there is one thing, the System.Security.Principal.WindowsPrincipal is not valid but since I'm using Owin authentication, it shouldn't matter.(?) This is the most frustrating thing I've come across. Everything seems like it should be working but after I delete those cookies and try to hit an [Authorize] endpoint again, it just goes tits up. Should I maybe be clearing session data every time I start the challenge? 
Oh that's funny I was just going through the ASP 5 templates to take a hard look on whats in them, whats new, and how to configure the basics of DI configuration and use the new vNext tech they included like json configs, GULP, Angular and such.
Craigslist, LinkedIn, Stackoverflow Careers, alumni job boards offered by your college's career center, etc. Though back in the day Indeed and Dice were very good to find jobs leads on.
&gt; integrated bottle of whiskey Which you will be very thankful for after having to configure it for the first time...
Thanks. I'll work on a good cover letter.
Never used the spade part. I mainly use the cleanup part of it.
Damn. I don't even know who handles our work network. I was hoping you'd figure it out :P
I agree. Conferences keep everyone up to date on latest greatest tech. It also builds the community. Products I think are a little more sketchy. There are plenty of good products that we developers should know about. However, I don't care to see resharper marketing team in here once a week. (Not that they do!) Advertised websites - If the website is .Net related (aka blog/news/technical tutorial). I don't care if someone just built a brand new search engine in .net and they want us to check it out. At that point they just want a customer and are not spreading useful knowledge. Jobs - No absolutely not. I'm chased enough by recruiters I don't need another spam attack!
Probably a better question for /r/javascript. Are you using Razor? What does the URI structure look like? Either of those are a potential option in setting the information.
That code is COBOL and that place is hell.
I did not think of that, I will post over there as well, thanks! But to answer the other questions, I do not believe we are using Razor currently. The URI structure would work for the product title, but not for the other variables. The Uri parameters are just the product id followed by the product title.
I guess, even if you're not using Razor you could use &lt;%=PropName%&gt; in within the JS and it should still work since .NET will fill that before the page is sent to the client. 
If you ever figure it out let me know :)
It really depends. I worked on a project where we exposed business logic to a UI layer through WCF (We also used WebAPI between other tiers).... Once the business logic was set up it was so much easier than it would have been with WebAPI. Instantiate client object then just call the methods, instead of having to deal with URL's and what not.
COBOL: Completely Obsolete Business Oriented Language (I've had past jobs programming in COBOL and MUMPS, and I'm sure they're still out there running systems at large hospitals and banks!)
&gt; WCF provides a more classical calling schema where Web API is a more language agnostic approach, relying on data to drive functionality rather than classical Object.Method() calls Not even remotely close to true. DCOM is an OOP approach. WCF and Web API offer the same data access patterns. For either you can choose an RPC or REST style API. 
SOAP is just a format. It has no bearing on whether or not you are adhering to REST patterns anymore than choosing JSON or raw XML.
&gt;If you're wondering what to pick for new development, you're almost always better off going with Web API and a RESTful approach. Ha! REST is only appropriate for dumb data storage and retrieval. Most applications need an RPC model for their end points, even if those end points are exposed via HTTP and JSON.
The only extensions I've written were down in the object layer, where I never saw any intermediate protocols.
It should go in the model. Create a videoSource object that makes the calls to YouTube and returns video objects. Your controller should be for instantiating the view with the model.
Thanks for all the responses guys. We are moving into MVC 5 and looks like Web API suits our needs perfectly.
Very cool. I read through a few already. Hey do you know why the ASP 5 Web Api startup doesn't come with configuration setups by default? I added the one from the microsoft.framework.configuration. it just seems like something that should have been by default. 
Oh interesting, I hadn't noticed that. My best guess is that they're still working on the default templates, it seems like something that should be included. There's nothing inherently different about WebAPI vs Web Application in ASP.NET 5 that would preclude it AFAIK. Might want to file a GitHub issue about it and possibly even a pull request to fix it :)
PS - here's the Template sitting on GitHub. https://github.com/aspnet/Templates/blob/dev/src/BaseTemplates/WebAPI/Startup.cs
Not surprising that they want to milk the cow further.
I was testing out the ASP.NET MVC framework and ravendb for a work project. I made a gradebook for teachers. Spent maybe 15 hours total. A lot of it was just fiddling around with it to see what was possible and putting together how I wanted to interact with/use raven.
O'Reilly has a great book about WCF in my opinion. 
I think Azure provides some free hosting options. 
Just an idea. Your dealing with serialization so... maybe the type being considered for the weight attribute is different in both scenario. Float/double kind of scenario. The main question is more... is the precision good enough for you or not?
Entering those numbers in the Firefox dev console rounds them in the same way, interestingly. I was under the impression that both Javascript and .NET use IEEE 754 doubles and should behave 100% identical, but maybe there's a subtle difference or quirk when it comes to parsing or printing them?
Then you probably need to write a desktop application (WPF, WinForms, Universal), not a website (ASP.NET).
I think a universal app can be built with HTML so maybe it wouldn't be able to difficult to convert?
Agreeing with u/hdsrob. Have your public website and api hosted in iis on a central machine, desktop app for the clients to push their data to that central machine then everything else can view/consume that data.
Making every machine in the network a web server sounds scary to me. So many security and configuration problems, not to mention the resources required on every client to run the processes. This is an extremely bad approach.
What exactly is it you're trying to do?
its a small application for gamers, it doesnt require resources. Its just to monitor your pc hardware with a smartphone while gaming. I could also write an app for every kind of smartphone, that accesses the machine, but i took the simpler approach. so thats out of the question here :p
IIS does require resources. I still say setting up unsecured webservers with obvious machine-level permissions (to gather hardware information) all over the place is probably not a great idea. Either way, have fun and good luck. 
Well, it seems I misunderstood. We can use asp.net..
I added configuration initialization to that template add submitted a pull request. Thanks for the idea! I've been dying to submit something to the open source world but I haven't really had anything worth while. Hopefully it gets accepted because I'm not even sure if I did it correctly. 
Thanks for the really good advice.
Can you have another readonly property in your object that passes the weight as string? 
Wat
What's difficult to understand? I look in my C:\WINDOWS\Microsoft.Net\ folder and I see vbc.exe and jsc.exe and csc.exe etc. Do they think everyone wants to be a .net developer or something? What the hell.
Why does it even matter?
Not if you avoid the XML configuration and instead just set properties in code.
Why are they including developer tools for non-developers? what's their effing problem? do they have to make the downloads bigger for no reason? or is it because corporate loving people just want t ignore it. fssh.
Because it angers trolls.
I learned from the horse's mouth: http://www.asp.net/mvc 
What happens if you use decimal?
One and done. Close the thread! The only caveat here is if OP also needs to know about ASP.NET Web Forms and not just MVC.
I have spent a ton of time just trying to learn about what changes are in ASP 5 without reading a 600 page white paper. This guy is a redditor and his website about ASP VNext I found the most helpful. He also is a contributor to the open source project. http://dotnetliberty.com/index.php/asp-net-5-vnext/ 
that looks very nice, thx a lot! Owin is awesome, its perfect for my scenario.
I think it's fantastic. DI out of the box, with access to everything in the framework. Taghelpers. Middleware. Same execution environment in IIS and stand-alone, Windows and Linux. Separated wwwroot folder. Etc etc. Bundling will certainly be available as nuget packages from third parties. You can also try the VS extensions Web Compiler and Bundler &amp; Minifier if you don't mind committing the generated css/js to source control.
I watch his videos! Great source of information - very knowledgeable 
Link me and I will definitely pass on your feedback.
&gt; Go To Implementation. The feature many of you have been waiting for: just right-click on an interface or abstract method and select this command to navigate to the implementation. About bloody time!
I do love how they're actually treating DI like a first class citizen (finally). Definitely one of the better changes 5.0 brings
I thought they had a freebie tier. You project will live at http://whateverProjectNameYouComeUpWith.azurewebsites.net isn't that a free tier? Maybe I'm wrong.
I really think its just a free tier for testing/showing off prototypes, etc. When you want to actually get a custom domain name to point to it, you upgrade it from free to some tiny pay thing. You gotta jump through a few hoops with DNS, A records, CNAME and all that crap, but its pretty easy to setup. Yeah, found it. "You can open an Azure account for free - You get credits you can use to try out paid Azure services, and even after they're used up **you can keep the account and use free Azure services.**" https://www.asp.net/mvc/overview/getting-started/database-first-development/publish-to-azure Seriously, I think its just a feature of Visual Studio. Right click on the project and select "Publish". Go through the wizard and push it to Azure.
Ah thank you! I'll take a look :D
There needs to be a distinction in what /u/CaptainINcredible has posted. The .NET framework is currently at version 4.6. There is a beta of version 5, but it is a much different concept to project configurations/functional namespaces, etc than any of the previous versions (a lot of things decoupled from the primary libs and tons more, I haven't gotten to start to tinker with it yet). ASP.NET MVC is currently somewhere around version 5.2/5.3 with a beta 6 version that runs on the beta 5 version of the .NET framework.
Thanks!
Im not saying its not ready for production... my fear is that it isn't. :)
How is Identity strongly tied to Entitty Framework? I implemented custom user stores and sign in manager for my project and the only Entity Framework thing I've interacted with was the IdentityUser from the Identity.EntityFramework package, which I quite likely can drop the dependency on - I have to look into it.
&gt; I'm still not 100% convinced that they chose JSON because it was a good fit for what they need and not because anti-XML is "in" right now. They chose it because anti-XML is "in" and because JSON is "in". Not because it's the right format. JSON is an acceptable serialization format, but it is horrible for configurations. Just the lack of comments makes it unsuitable for project files... But now we are stuck with it.
Yeah, I probably worded things poorly. I just meant that all the doco, project templates, out-of-the-box stuff is EF centric at the moment. I did notice a few classes that were in the EF package, as you mentioned, but yeah, probably not too hard to swap that out either.
RESOLVED. The issue was actually with the ReSharper extension. Updating ReSharper to version 9.2 fixed this issue.
Whooopppppp! Although I've been using a add in for this which seems to work quite well (can't remember name, probably goto implemention or something equally imaginative)
Or ctrl+alt+B, if you use the alternative keyboard layout.
Yeah, but they'll spam call and email you to try to pressure you into buying something :/
I can't really figure out what the behavior is from this post, but I'd assume you can just make lower-case controllers?
In order to represent most doubles exactly, you'd need about 53 digits. That's 1 decimal digit per bit in the mantissa. For example, with 3 bits, You could represent 1/8 == .125, which has 3 decimal digits. Most decimal number can not be accurately represented as doubles. While, the differences are potentially interesting, you shouldn't rely on the insignificant digits of a decimal representation of a double.
Nice find
It really shouldn't. I actually went to php site today to buy some lawnmower parts and i found that it was case sensitive when I typed in the URL on my PC from my phone. You never know where your URLs are going to end up on the web. They could end up somewhere that outputs lowercase URLs by default and then none of those links on that site to your site will work.
I know exactly how you feel, if maybe for different reasons. Mainly, the whole Resource Owner Password Credentials Grant workflow seems a bit off for me. Is this what I need for a public facing angularJS app? Oauth docs have four available core workflows, but I cant seem to find anything relating the other three workflows being integrated with angularJS
I love the gulp/bower/npm integration, its nice to see those become first class citizens of the .net world
I love it! With beta 8 they have DNX watch command you can run that automatically compiles your code when you save a change, it improves my workflow so much! The bundling and minifacition is "just" moved to gulp/grunt as I think it should. I have never been a fan of the Microsoft way and the rest of the world way, so I love that they stake what everybody else is using.
&gt; Wow, haven't moved to mvc6 yet, still on 5. Because MVC6 is still a preview version. And gladly on asp.net 5 we don't have a web.config anymore.
&gt; With beta 8 they have DNX watch command you can run that automatically compiles your code when you save a change, it improves my workflow so much! Unless you want to debug your code.
According to W3, they should be case sensitive. I think it causes confusion, and so for convention I make my websites use all lowercase, and use 301 redirects to lowercase if upper case is used. Furthermore, there can be SEO issues when Google sees "About.html" and "about.html" leading to the same page, since it is viewed as duplicated content. There difference here being that a redirect doesn't hurt page ranking in Google while two URLs for one page does.
You can still import an xml source if you prefer it.
Official release to NPM: https://www.npmjs.com/package/typescript-dotnet
[Try again](http://stackoverflow.com/questions/17275030/how-to-map-a-value-type-which-has-a-reference-to-an-entity#comment25044662_17275030).
Here I thought I was clicking through to a tutorial... can you give a more detailed explanation for what you're looking to do? A step by step scenario maybe?
You have two different issues here: 1. Accept a file in MVC6. 2. Store the file in EF. For the first issue you need this: 1. Have an `input` field with the type `file` in your frontend markup. 2. Have an action that accepts a `IFormFile`. This is the interface to access the file sent from the frontend. 3. Use `OpenReadStream()` on the `IFormFile` to retrieve the binary data. To store it in EF you need: 1. Create / have an entity with a `byte[]` property. 2. Create a new entity, store the retrieved file data in the `byte[]` property. 3. Add the new entity to the `DbSet` and save the changes.
Looks interesting - performance is always a good thing :) Do you have any metrics for the performance gain?
* Authentication tokens (generated by *your* server) sent by the client when it registers itself to signalr hub. * Whatever creates the message raises an event to say "There's a new message for user 1234!" and the signalr hub subscribes to this event, upon receipt of message it can scan for all unread messages for user 12345. If there are any results - notification. Or if event driving is too much like hard work, have something periodically scan for unread messages, then notify the respective user via signalr. I have to ask... did you really actually try to think this through? This is, frankly, some basic stuff and to jump to the conclusion of "I don't think is possible on ASP.NET" is quite laughably bad.
That's slick - I was always interested in why ToString performed so badly, though I was evidently not interested enough to actually look into it :o) If you're interested in the IL parts, building the solution produces the edited Enums.il output (in the $SolutionDir\x64\Release directory). And I've never had a solution of mine result in someone patching an upstream product! You, sir, are a rock star.
comsume the api in a webApp
Only unrealistic ones. There are 10 tests under the categories "Baseline" and "Performance" that perform the operations over a huge number of iterations. It's not a good example of real-world performance due to the iterations (configurable as constants). They're done that way to give me a way to test micro-optimisations so that I could see sub-ns results expanded to make it measurable. The Console.Profile project does something similar so that I could attach a profiler to evaluate call times and such. The two main goals were to increase readability without adversely affecting performance (.Add/RemoveFlags/IsFlagSet over |/&amp;/&amp;~ operations and the other methods Enums.Method&lt;MyEnum&gt;(...) over the (MyEnum)Enum.Method(typeof(MyEnum), ...) pattern), and to give me no reason to ever write a switch statement/Dictionary in my code to replace the ToString() method for performance reasons (a problem I had in several areas of one personal project I use at home). The biggest gain occurs when converting an enum with a single value to a string after the initial "first call penalty" that happens per type where it caches values/names. After penalty, it's more than 20 times faster (and that may be conservative). The penalty is affected by the size of the enum. Under a profiler, AsString performed about as badly as ToString on first call for enums containing about 10 values. It's such a tiny amount of time to begin with that getting good, repeatable benchmarks the way I was doing them wasn't good enough that I'm willing to commit to those values, yet, so YMMV. I was still tweaking that as of last night and there's a lot that can still be done to reduce the impact so I'm hoping next weekend I'll have this down a lot, as well. The good news is that penalty is shared among several functions (at different levels -- Add/Remove flags does partial init, Parse, AsString, GetDescription[1] and others do full) so if you're doing a large number of enum operations (where micro-optimisation would be important), you pay it once and reap the rewards. My plan is to change the Console.Profile project to use BenchmarkDotNet (or something along those lines after evaluating options) to help get better numbers. For now, I used a few different commercial tools along with ildasm to evaluate line-by-line the post-init methods to ensure they were not doing anything obvious that would hamper performance. Methodology.md in the root of the solution goes into far more detail than I did here and might answer some more on this topic as well. [1] The penalty here is massive and needs work. In this version it will be slower for any circumstance where you have no intention of using every DescriptionAttribute value. I plan on breaking this out into two versions, one will humanize the camelhumps/underscore name, the other will cache the attributes. You still pay it once, so if you're using the values from DescriptionAttribute throughout your code, it's reasonable, but if you're pulling a few of them once or twice, it's unnecessarily expensive.
Just an FYI, I would give Microsoft Azure a look for your hosting needs. They give you 10 free websites to use. Also, the tight integration with Visual Studio, and use of things like Application Insights are helpful.
I'm literally using a default 2015 mvc project, no modifications to it other than adding trust to the web config. So my only entry is: &gt; {controller}/{action}/{id}
I actually didn't even think about wording it correctly. Sorry if that somehow offended you.
There's two pieces you need to download and install. I can't remember the exact filenames but one installer is for upgrading dnx, the other is for the Visual Studio extensions. They are both on the same download page.
 [Download Link](http://www.microsoft.com/en-us/download/details.aspx?id=49442) I am using that link to download DotNetVersionManager and the WebToolsExtensionsVS14. I ran the install on both but my ASP 5 WebApi beta template still looks like a beta 7. (ie using Microsoft.Framework.DependencyInjection should be Microsoft.Extensions.DependencyInjection)
Well my first question would be: (1) Why are your developers adding so many new packages? Usually once a solution is up and running new packages should be a rare occurrence. (2) Why are your developers allowed to add new nuget packages without conferring with your Technical Lead? I know us developers need some leeway in how they write their code but that shouldn't give them authority to add 3rd party libraries to your solution without running it by a tech lead. That smells of disaster, dead 3rd party projects, beta projects and so on! This stuff should be more tightly controlled. (3) No code review? If someone adds a new package it should be caught in code review. Then that package should be assessed against current versions. At the end of the day package management is a "manual" process. There is little you can do short of keeping an informed team and a well informed leader. I suppose if you where super anal you might be able to write a Unit Test that checks all of your project package.config files and fails if there are version mismatches between them. Then have a check-in fail on this test failure. Also, I think vs 2015 will fail at compile time if project A refs v1.dll and project B refs v2.dll then Project A refs Project B. You should get an error message that complains. So maybe a VS upgrade is in order?
Voat.co uses SignalR for inbox notifications. https://github.com/voat/voat This script may have what you are looking for (but I am not going to dig any deeper): https://github.com/voat/voat/blob/4baa0cce7ebd9f5e01ed3cd79235f37532d33673/Voat/Voat.UI/Scripts/voat.js 
That *nix systems still feel second class is my biggest gripe. A lot of the tutorials are Windows based and often centre around Visual Studio. I'm hoping this will change as the system matures and the (non Windows) eco system grows as I rarely use Windows.
I typically use the yeoman (yo) generator-aspnet templates when starting a new application: Instructions: https://code.visualstudio.com/docs/runtimes/ASPnet5#_getting-started The latest templates should be updated to use beta8 packages.
I think this is on the project level, and to prevent upgrades, right? If I could do this solution-wide that would be awesome, but as far as I know and can tell from the docs, it's per-project only?
migration is super smooth from an existing project from a technical standpoint. Getting the rest of the team to buy in is left as an exercise to those implementing it...
no problem. make sure to grab the Paket Extension to help but it really just wraps up the command line options - https://visualstudiogallery.msdn.microsoft.com/ce104917-e8b3-4365-9490-8432c6e75c36
I know it's so wrong to reference directly, and it makes me cry inside every time they do it because I have to clean it up (and it *has* broken production a few times). I've lost days of my time cleaning this stuff up for just a couple of projects. I never thought of NuGet being suffocated by the mass of multiple versions though. That could very well explain our predicament with NuGet stuff being extremely slow. One thing I left out is that we update packages from our own private repository frequently, since it hosts the outputs of a "common libs" Solution (which is consumed by other Solutions too). At first, this was manually done, and stuff broke frequently. I ended up automating it (inside a special TFS build), and it works fairly well, but it means more frequent updates, and I guess extra slowness. Honestly, thank you for your awesome responses. Everything is *very* helpful :) :)
Yes, I have noticed that nuget connecting to a local package store *is* slow as well. One thing I noticed about this was that it worked fine for 1 package 1 version inside a folder. But once we started growing that repository nuget started to crawl. I am not sure if its our network infrastructure (virus scan) or nuget short coming. 
Wow... I was afraid of this being the answer.