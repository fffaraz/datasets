In a webserver context, stick to `async/await` and avoid creating new Tasks with `Task.Run()`. Any background work should be done by putting Work Items into a queue to be processed. If it's OK if these work items get lost when the server restarts, then an in-process `BackgroundWorker` will do. Key point being that it is using a *finite* number of threads (and not ThreadPool threads) to do the work. If it is not acceptable for the work to be lost if the process is recycled, then you need to use an out-of-process queue and worker. ThreadPool threads are for small items that finish quickly (and thus return the thread to the pool). Finally, you will reach performance limits of your system at some point. The key is to maximize the scalability by avoiding limiting factor bottlenecks. The most common bottlenecks are ThreadPool exhaustion, waiting on slow IO (local hard disk, slow network/DB), and lock/mutex/semaphore contention.
If you are going to operate the DB yourself then you'll want to stick to a DB you know well. If you are using Database as a Service then you'll have more options (since it's maintained by the cloud vendor). There are also some good encryption options with DBaaS providers. &amp;#x200B; For example Amazon's RDS for Postgres has encryption features which are very easy to manage: [https://aws.amazon.com/rds/postgresql/](https://aws.amazon.com/rds/postgresql/) &amp;#x200B; npgsql is a well maintained library for Postgres and it works well with Entity Framework Core. It's my personal choice as a database with .NET Core.
.NET Core works just fine with Postgres. It's what happens at my job. We use \`Npgsql\` with no issues. It's been great, actually.
Calculate the actual licensing costs for MSSQL, it might shock you.
Then, do it for Oracle. 
Web applications developed in asp.net core will run on any platform, but not asp.net.
What really? You mean if I write an app in asp.net mvc5, it won't run on a Mac or Android
I can also vouch for the awesomeness of Postgres with EF Core. [Npgsql](https://www.npgsql.org/efcore/index.html) is great and easy to setup.
Oh that‚Äôs what you meant. I thought you were talking about development. Yes, your app developed in app.net will run on any platform. Sorry about that!
That's also a case with Android, where OS will crash any Http request made from the Main Thread. It's been a thing for a while now, since in ancient times some developers blocked UIs with network calls.
Not knowing the full curriculum of the course (and your general comfort with C# programming), you could get away with using .NET Core and Visual Studio Code in Linux. You can run NUnit tests in code and all of the ASP.NET Core stuff works just fine. Even SQL Server runs in Linux now. If you're somewhat seasoned as a developer, I'd say you would be fine working around it using VS Code. If you're pretty new to it, I wouldn't recommend it.
That's fair. 
Thanks for replying, this definitely helps my understanding. I'll keep this in mind going forward. üôÇ
Awesome, I will check this out. Thank you! üôÇ
We're using PostgreSQL for a side project and so far it works really well. Getting it working from a code perspective is pretty simple and as far as I can tell no different from using SQL Server, but costs way less which is why we're using it. MySQL was also considered but we wanted to have a s little as possible to do with Oracle.
I would like to take your argument a step further. Why even use a fully-managed RDBMS (open source or otherwise)? Do you actually have goals of moving to a hyperscale situation overnight where you think going horizontal on your persistence layer will save you? It is my professional opinion that virtually every successful business system starts as something that runs out of a single instance for a period of time (however short that may be). With that in mind, I would like to plug our top-secret production stack. This is actually what we use for critical business applications running at client sites today (and on our EC2 instances in AWS): Architecture Note: All persisted business objects (Users, Orders, InventoryItems, etc.) are exposed through simple repository interfaces that are then implemented by a separate class library which consists of the below for all of our current use cases. SQLite ([https://www.nuget.org/packages/System.Data.SQLite.Core](https://www.nuget.org/packages/System.Data.SQLite.Core)): WAL mode for ***surprising*** levels of performance, using PRAGMA user\_version to manage SQL migrations. I have saturated a NVMe drive's bandwidth on many occasions with bulk import/export against sqlite. The reason? Because it's just a damn file. Transactions are also extremely quick (WAL helps a TON), and if you are careful you can probably beat SQL Server in steady-state throughput and especially latency (sqlite is just a p/invoked method call away, no named pipes or god forbid network trips). Dapper ([https://www.nuget.org/packages/Dapper/](https://www.nuget.org/packages/Dapper/)): Data access layer, we figure if it's good enough for StackOverflow, it's good enough for us. It's incredibly simple to use for virtually anything you would normally want to throw EF at. That is it. SQLite is hands-down the best database engine I have ever worked with, and I've worked with Oracle, DB/2, MySql and MSSQL. The advantage you get with SQLite is that from a rapid prototyping and development perspective, you can throw an app together and get it onto a box in a matter of seconds. Using dotnet publish with SCD build options combined with SQLite for persistence gives you a hermetically-sealed business application that simply works *anywhere* without any concerns regarding runtime environment. I don't have to think if SQL Server or .NET Framework (or even Core Runtime w/ SCD builds) have been installed. I am getting to a point where I'm starting to wonder what the hell everyone else is still doing with docker, kubes, et. al. We are at a pretty incredible inflection point when it comes to building, distributing and running software, and you can realize 10-100x gains in productivity overnight, *but only if you choose your tools and methods wisely*. Once (If) the day comes where you actually need to scale off a single box, all you need to do is pick a database solution and then implement it against the same repository interface that SQLite was implemented against. If you planned ahead a bit, your repository interface would even include methods for explicit batch migration of the data to/from any arbitrary repository implementations.
Thanks for the comprehensive response. Im already a developer with about 4 years experience on my belt. I've been looking to validate my work place skills and have used the exam ref to fill in gaps I missed at the work place. The issue is that I've missed things such as azure as we have a private cloud we host in so we never really get exposed to that sort of stuff. It seems like the best way forward is to pick things im unfamiliar with on the ms syllabus and study up on it as you mentioned. 
Who were you DenverCoder9? WHAT DID YOU SEE?!
SQL Server can be significantly faster than postgres for complex queries, especially with larger servers. It has much better support for parallelizing queries and a more advanced query optimizer. For example, last I checked they're just barely adding support for inlining CTEs in PostgreSQL; a relatively basic operation. Meanwhile SQL Server can now inline multi-statement scalar functions, even if they contain queries. *** That said, if you are just doing basic crud operations then PostgreSQL may be the right solution for you.
Do not use the mySQL drivers from Oracle. They are crap and only pretend to support asynchronous operations. There is an open source project that re-implemented them which you should use instead.
And since we're on the topic, don't use the Sqlite drivers from Microsoft. They've got important bugs that they still haven't fixed after several years. I'm talking about returning the wrong data type of bugs. Fortunately Sqlite now offers official drivers for dotnet core.
MySQL really really sucks if you have to use joins. At one point it was so bad that people were performing joins in PHP instead of the database. These days it's a bit better but still not great. I must have customer demands it I recommend avoiding my SQL in favor of SQL Server, PostgreSQL, or no those are the only two I like.
Two weeks ago I published a blog post about individual settings using environment variables. As some of you suggested, there is a better way to achieve this using ASP.NET Core User Secrets. This new post should tie up loose ends :)
 I learned so much about testing in real world scenarios because we had flakey networks. If our network was actually working correctly, I might have been one of those knuckleheads who mocks everything.
&gt; Having to plaster ContinueAwat(false) everywhere is a sign to me that there's a smell there with the implementation under the hood. That really should be a compiler setting at the project level
In general I agree, but async await really should be your default. 
The vasts majority of these technologies have been designed for Windows. While Microsoft is slowly changing it's focus with it's .NET core initiative - the majority of these don't have 1-1 analogies in Linux. It's a course and I think it's fair to say that the instructor is going to assume you are running Windows. While you likely could maybe pass programming assignments running the code on Linux, you'll need to know what you are doing. And since you are taking a class to learn this stuff, Id suggest against taking the course or just pony up for the Windows license fee.
how would you use this in production environment, if you have to specify secrets id in project file?
User Secrets is a development feature and not something meant for production use. There are many ways to do this, but I can tell you how I do it. When running locally I use appsettings.json, environment variables and user secrets. I then deploy to Azure where I override all the settings needing other values using Application Settings inside Azure app services. Azure also have something called Key Vault if you need to store values that no one should be able to see by simply going to the portal. Hope this helps.
 The threadpool discussion is a red herring, async operations that don‚Äôt block on sync io shouldn‚Äôt increase the threads in the threadpool beyonds it‚Äôs maximum efficiency which won‚Äôt be much greater than 1 thread per cpu. If you have deadlocks in your code I can‚Äôt see how that could be caused by too many tasks, it‚Äôs caused by circular dependencies in your task completion dependency graph. How does your code queueing tasks prevent deadlocks that occur in the framework when it queues your tasks for you?
I stopped reading after I saw the code for dynamically creating the constructor. That shit is a massive code smell lol. 
Your class `Repository&lt;T&gt;` does not implement `IDisposable`. If the DI framework automatically disposes of stuff, I would assume it only does so if the injected class is actually disposable. It might be a good idea to start there, I guess. Make sure to actually call `Dispose()` on the `DataContext` when you implement the `IDisposable` interface. 
What's the equivalent of SSMS for SqlLite?
Your post has been removed. Self promotion posts are not allowed.
Your post has been removed. Self promotion posts are not allowed.
I tried, but apparently my formula is missing a right parenthesis somewhere.
Could you link me some of the courses you used? 
Im looking to move to Europe and hoping that the certification will assist with getting a job. I already have 4 years experience as a dev but would like to MS certified. Besides that its also a nice sense of achievement :) &amp;#x200B;
This a webscraping that i did in amazon website : https://github.com/SgtPeppers/cSharpBuilding/tree/master/AmazonWebScraping If you go to line 44 in program.cs you can see that i use a or("|") clasule, that is useful for dinamicaly website. Im not a english speak, sry my bad english.
Thank you very much. I am sure it will help. Your english is not so bad dont worry!
If you are going to make the HTTP[S] requests yourself in C#, and need to parse HTML - the HtmlAgilityPack is pretty good and actively maintained. https://www.nuget.org/packages/HtmlAgilityPack/
Yes I'm already following that path, but I lack experience in this area. I need to go further into documentation or some good tutorials for complex and dynamic html parsing.
Quick Synopses: * First launched in 2005, this free .NET CMS has over 500,000 installations. * Backend: C# .NET Framework 4.5, Admin area: AngularJS 1.1, Frontend: Can use Razor to output whatever you like (eg. Html) * This release is a code cleanup, removing all legacy code. It will be the last version running .NET Framework, Next major release will be .Net Core and as yet undecided Admin framework (Maybe VueJs, Maybe latest Angular) * Umbraco is developed by Umbraco HQ which has around 50 employees based in Denmark, mainly funded by Training, Gold Partner program (You pay 12,000 Euros per year, and get bragging rights to announce to potential clients you know Umbraco) and Cloud solution (Completely optional, has around 1% take up) * Reasons to use: Free, clean and simple content management, agnostic frontend or completely headless (output whatever you like). * Downside: Learning curve (Instead of keeping data in a database, you keep data in a cms, you have to learn how to interact with that). Usually requires backend skills/c# skills to adapt website to your specific requirements (This isn't WordPress, aiming itself at Frontend devs with limited backend skills). 
That developer has done you a big favour. He's exposed himself as an ignoramus and you no longer have to listen to anything he has to say.
You don't need Visual Studio to develop .NET - there are quite a few alternatives as it goes but I'd go with VS Code or Jetbrains Rider, they're both excellent. That said, .NET teaching can be too focused around the Windows tooling and you might need a bit of help finding the equivalent shortcuts in your preferred editor / IDE.
Is this a WCF service? This snippet is vastly more simple than the samples and tutorials I've been reading online, to the point where I'm almost suspicious that there must be a catch somewhere! Thank you, though!
I have a crawler running 24/7 on my Linux NAS. The HtmlAgilityPack is pretty good. My only advice to you is not to ever treat HTML as XML. XHTML never really took off, so parsing a HTML document as XML is not feasible. Hopefully the documentation here will be reasonable enough for you: https://html-agility-pack.net/
Im sure I will manage to finally do this. I Just need some Time with the docs. Thanks for ur advice 
Also, that line in the last bit that uses FirstOrDefault? You don't need ?? followed by newing up an empty item. FirstOrDefault will return an empty item if it doesn't find a match. Therefore you can also just return the result of the FirstOrDefault call to make that method a one liner.
Yeah, it's enough to suggest finding the time to make use of EF in my opinion. I worked for a manager who tried to reinvent the wheel all the time and suffered from not invented here syndrome. The amount of extra work we had to do was annoying to say the least. Sure, let's write and maintain your own JSON library instead of just using JSON.Net....
Just do as written in the guide. It seems the author likes to answer questions, so feel free to ask. [https://blogs.msdn.microsoft.com/luisdem/2017/03/19/net-core-1-1-how-to-publish-a-self-contained-application/](https://blogs.msdn.microsoft.com/luisdem/2017/03/19/net-core-1-1-how-to-publish-a-self-contained-application/) Or yet one guide [https://www.boxedapp.com/boxedapppacker/usecases/pack\_unity\_app\_into\_single\_exe.html](https://www.boxedapp.com/boxedapppacker/usecases/pack_unity_app_into_single_exe.html) Also, I have seen more tools that create an executable file from .NET Core on stackoverflow
Try DBeaver, it downloads any driver you need when you connect to a new database. I normally hate java apps, and have used (among others) ssms in the past but nothing beats dbeaver.
it reminds me of the dispute between Guido van Rossum and FP fans of Python community about "map" vs "for". 
I think the performance of SQL is understated. My previous workplace ended up swapping out PG for MSSQL to handle the vast amounts of search records and queries.
Yes, using vs 2019 preview 3. https://docs.microsoft.com/en-us/visualstudio/releases/2019/release-notes-preview#--visual-studio-2019-preview-3- New classification colors are available from the .NET Compiler Platform SDK (aka Roslyn). New default colors, similar to the Visual Studio Code colors, are gradually being rolled out. You can adjust these colors in Tools &gt; Options &gt; Environment &gt; Fonts and Colors or turn them off in Environment &gt; Preview Features by unchecking the Use enhanced colors check box.
hey there, &amp;#x200B; From my point of view, EF is over-headed, especially, if we r talking about get data and hundreds calls to db. So, fetching data from db via graphql is good idea and gives flexibility on front-end side. I'd recommend look at this one lib ([https://www.nrecosite.com/graphql\_to\_sql\_database.aspx](https://www.nrecosite.com/graphql_to_sql_database.aspx) ) that allows configure db connection and schemes based on db tables very easily. &amp;#x200B; &gt;helping with query params (search/sort/paging)? All these features are implemented there, so, you can use sort by fields, simple pagination (items limits and offset) and powerful filter which is really nice to set wide range of conditions. &amp;#x200B;
This is what we use at work: [https://sqlitebrowser.org/](https://sqlitebrowser.org/) It's incredibly lightweight and starts up in &lt;1 second on my machine. Loading databases is just a double-click or drag-drop away. Even has primitive plotting/raw text/hex edit features. Basically everything you wanted to do in SSMS but got your wrist slapped for.
I use resx files in combination with [Multi lingual app toolkit](https://marketplace.visualstudio.com/items?itemName=MultilingualAppToolkit.MultilingualAppToolkit-18308), but next time I would go with storing my localizations in a database. See https://west-wind.com/westwind.globalization/
been following this series.. it's awesome 
Thank you very much! 
I inherited this code, I copy/pasted from the code and removed the actual class/interface for anonymity. In some places they used .SingleOrDefault so maybe they considered them to do the same thing, I'm not sure.
Thank you, I will look into this. I appreciate your insight.
Understood, I was just trying to be helpful. üòÅ Pretty sad if they didn't know the difference between First and Single methods....
That's interesting. Like I said in the post, I inherited this. I did not write it. It is not working well. I want to fix it. The first three words of the post (past the greeting) are that I took this project over from someone else. So did you stop reading because you thought I was the one who implemented this or because you think I've walked into some mound of shit too big to fix? If you'd have kept reading you'd see that I'm looking for advice on whether this entire bit should be scrapped. I don't like it, I think it's a burden and I don't think it helps, but I also do not know everything there is to know about how contexts should be managed. For all I know this is some top tier 200iq shit. Sure I could rip it out and move on my merry way but I'm trying to gain insight on why this was done this way, if it is something worth trying to save or if I should scrap it. Which, clearly, I should. It's kind of frustrating that the only person who openly admitted to not even reading the post is the one who got an upvote and the thread altogether has been downvoted to nothing. I know votes don't mean anything but they kind of do, burying my question means fewer people will see it and someone out there who's been through this before might have seen it otherwise. Par for the course in this industry I guess. I wouldn't dare post to SO due to fears of unending ridicule but I thought this sub would be a little more human. Anyway, I still upvoted you (whether or not that means anything to you) because you clearly think that bit is unnecessary, and that reaffirms my original suspicion. I just wish you didn't throw your beer can at me on the way out.
This seems to be an issue with what I've walked into. I will push back to see if I can at least get 1 day to try to get rid of this in favor of EF. Thanks for your reply.
I fully appreciate it, thank you. I'm worried, this is the first project I've been put on and it's this rigged. I'm not even close to being a good programmer in my opinion but I can tell when something seems off... this whole thing has just seemed off to me.
The constructor is trying to be some sort of pseudo IoC container. It's just start using a normal dbcontext anywhere this repository is being used.
DotKnot, a multithreading framework. *Weave* the most efficient applications without worrying about the usual hassles of multiprocessing as the *Tailor* runtime will ensure your execution paths stay *untangled*. Or something.
[This](https://github.com/StephenCleary/AsyncEx/issues/107) is probably the most famous example of it that I'm aware of. Like I said, it's an edge case, not common.
You are the best. Thanks.
It's Microsoft's own fault. The refusal to allow for performance tests make it impossible to compare it to postgres in a meaningful fashion
The question isn‚Äôt if it‚Äôs worth it in a highly multithreaded context. The question is if it‚Äôs worth it for a site that gets 10k visits per day. The answer is.. why aren‚Äôt you measuring performance to figure that out? You can log request start and end time and just use that as a starting point. Convert a couple routes completely and see if the speed up is worth it for you. 
Don't focus on feeling like you're not close enough to being a good programmer. First of all, that doesn't matter because you have code to maintain and it's your job to maintain it. Just do your best. Second, you're at least food enough to know this is pretty shit code that needs fixed, so you're at least better than the person who wrote it. üòõ I'd work up something to take to the powers that be that can explain why this code is problematic, and that the best solution would be to switch to something like EF and ask to be allowed to do that. If they agree, great! If not, try to reach a compromise of you'll fix the existing code, but you want an EF conversion added as a tech debt item to be worked on ASAP.
To clear up. It can only be deployed to a windows machine. [ASP.NET](https://ASP.NET) core will run on pretty much anything.
That was my original understand too. Then I got confused 
Wouldn't the continuations execute on the thread pool if there was a `ConfigureAwait(false)` in there?
Is there something that‚Äôs missing or not working in it? Or is it just because it isn‚Äôt developed further that you don‚Äôt want to use it?
I never like using components that have stopped being supported. Enviable I'm going to run into a compatibility issue as the world changes around it changes over time. Better to take the hit now in switching it out than continue to build over it, making even more painful to replace it later. But my near term need will be switching over to .Net Core 3.0 when it is released. I'll need a graphing control that is compatible with Core 3.0. 
Someone did some work on it 27 days ago (closed an issue, commited some code), but other than that it's not looking promising. I'd love to know too as I've got some live WPF projects using Oxyplot. It's a fantastic chart, and I don't know of any open source replacement.
You need to work on your question. It is not very clear what you are trying to achieve. &amp;#x200B; If you question is how to use dbcontext from a different project in the same solution, you would just need a using statement pointing to its location. &amp;#x200B; If you want to access data you would just need to access it using the API end point? &amp;#x200B; &amp;#x200B;
The only thing I can think of is that Include does a left join on the foreign key reference and the references don‚Äôt match up so it returns nothing 
Shouldn't this be double quotes instead of single? .FirstOrDefaultAsync(l =&gt; l.Id == 'AA 005'); If that doesn't solve it, maybe you can share you project/solution and I can try to debug it. 
Scraping. One p. 
Sorry I added that in later. Ignore that. 
A free and open source .NET IDE option for Linux is [Monodevelop](https://www.monodevelop.com/).
Thanks man, that was really helpful
I used to use Monodevelop and Mono back when I coded to .NET Framework on Linux. Except for a few limitations [here](https://www.mono-project.com/docs/about-mono/compatibility/), it worked out find.
Sorry I understand how all the joins work but I'm not sure what you mean by they don't match up. They are both 'AA 005'
It might actually be, it will produce better search results;) good luck with your project!
Honest it's pretty good experience to work on some awful code bases so you learned what not to do, and when things smell and *why*. Then when so see and learn the proper way of doing it you're motivated to actually implement it, and know why it's done that way.
It was the first thing I could think of really, I can't see your database constraints so I can't see if your referential integrity is in place and if the values match completely. I don't know if you've done Code First against the db or matched the entities in .NET to match your existing DB. I'd start by matching your SQL query to linq query. select * from Inventory i join Location l on i.LocationId = l.Id where l.Id = 'AA 005' Failing that, I'd remove the include and query the tables directly in linq and see if it returns the single entities. &amp;#x200B;
I'm not seeing anything wrong in the code you posted either. Have you checked to make sure the FK associations were created properly in the DbContextModelSnapshot class?
Yep, this seems to be entirely in EF Core. I've checked my constraints and everything is fine. select * from Inventory i join Location l on i.LocationId = l.Id where l.Id = 'AA 005' Gives: Id Quantity LocationId ProductId Id(1) LocationTypeId Sequence 23373 12156 AA 005 290605 AA 005 1 30004
Yep. b.HasOne("Data.Entities.Location", "Location") .WithMany("Inventory") .HasForeignKey("LocationId") .OnDelete(DeleteBehavior.Cascade);
Try this: var location = await context.Locations .Include(l =&gt; l.Inventory) .Where(l =&gt; l.Id == "AA 005") .FirstOrDefaultAsync();
Be sure to use double and not float, too.
Okay, but knowing the "right way to do it" without unpleasant surprises is not always easy. Sure, we can always get it right by throwing more education and self-training on it, but most can't do that with *every* new thing that comes along, since a lot of new stuff pops up all the time. If a reliable source had said, "Here's the right way to set it up to get potential future benefits without creating bugs and confusion in the here and now", that would be really nice; but reliable sources of such advice are hard to find. Microsoft has an agenda and thus one must take them with a grain of salt.
I would first check generated SQL with SQL profiler, also check if in SQL server LocationID is indeed PK 
I've been using Jetbrains' DataGrip recently. It's very similar to dbeaver (more so than to SSMS) and also supports a wide range of database engines. dbeaver is pretty great too.
Hey bro I wasn't attacking you or anything I know it's not your code. I was just making a point that the dynamic constructor creation is a mess haha Nothing against you personally mate, I think as developers we take things a little too personally sometimes
Thank you for the answer. I investigated that if you set DefaultAuthenticateScheme to "Github", it will call Authenticate of Cookies and then will compare authenticationSchemeName with "Github"
They do that? aren‚Äôt you mixing them with oracle? Never heard of mssql not being benchmarkable before
To generate an api copy the source of your OpenApi And paste it in http://editor.swagger.io You‚Äôll be able to generate an aspnetCore api Server (or other java based , nodejs..) One of the advantage is that you‚Äôll have a clean documentation of your API . Have fun ;)
[Part 1](https://habr.com/en/post/439486/)
SQL Server has basically the same no benchmarking license that Oracle has. I don't know if they actively enforce it.
TIL https://stackoverflow.com/questions/12115397/is-it-against-license-to-publish-oracle-and-sql-server-performance-test odd it‚Äôs the first time i hear it as mssql is what i mostly use meanwhile i heard it tons of times about oracle wich i never used.
\&gt; Having to plaster ContinueAwat(false) *everywhere* is a sign to me that there's a smell there with the implementation under the hood. It is good practice to do so in [ASP.NET](https://ASP.NET) so that's not a code smell, unless you're saying the underlying handling of the synchronization context is messy which I don't disagree with, in which case you'll need to take that up with the language designers. Note that you don't need ConfigureAwait(false) with [ASP.NET](https://ASP.NET) Core.
I'm not surprised; Oracle is well known for sending in the lawyers. Microsoft is far more subtle and often has a third party deal with license violations. 
You can create a basic API relatively easy. &amp;#x200B; I started by creating a Web API application that provides the basic structure. Then, I have researched how to make database connections using Entity Framework. I generated my models using database first approach. You could research that, since you already have the databases. ( I am unsure if this is supported by Core atm) Entity Framework can handle a lot of logic for CRUD for you, I think it is an okay starting point. Eventually, you will want to improve data access by separating some of that logic and creating services/repository layers, which will allow you to mock data and perform unit tests.
I started using Umbraco about 2 weeks ago. It's great - I've really been enjoying learning about it, and how it can help me. I've always made bits of CMS-like functionality for sites that I make, but Umbraco makes it much more manageable. I think it's been a great experience so far - you just need to learn its ways of doing things, which can be a bit different - e.g. Examine search. I decided to back-port my personal [ASP.NET](https://ASP.NET) Core 1.1 site to Umbraco to make it more manageable. It's a lot of fun. It's great to see that it's still being worked on.
That's exactly what I'm saying but I must admit, I had no idea it was irrelevant in asp.net core, that's great news! Still, it's rather tedious and annoying that you still should do it in library code. As others have stated, it would be great if ContinueAwait(false) was the default or at least a project toggle.
Check out Aspnet boilerplates/zero and datatables editors.
Very cool. Thanks for sharing!
I just watched this yesterday. It was pretty neat. I didn't expect the project linking part. I don't have any desktop app to port but I'm interested in seeing if developers adopt this. I personally love the experience I've had with dotnetcore making web apps. It is great to see it expand. 
Not immediately, but as more and more applications either move to .Net Core or new one are being made, it will become the legacy component and eventually fade into oblivion (whether that takes a few years or a decade has yet to be shown). If you start a new app, you have almost no reasons to use .Net framework, because of the limitations presented in the blog post: - new features mostly going into .Net Core (both language and library) - slower file system access on .Net Framework - less frequent fixes on .Net framework (for the reasons mentioned in the blog post) - risky security updates on .Net framework (idem) So maybe it is too early to "spread" that .Net Core is replacing .Net Framework, but it is definitely going in that direction.
That's the point. Saying Microsoft said ".net framework 4.8 is the end" and ".net core is replacing .net framework" is absolutely wrong. The fact that those thoughts and feeling may end up being true is entirely likely. We need to stop projecting our feels and forecasts as facts and try to back them up by saying "Microsoft said so" to avoid an argument.
TL;DR; Change your csproj file structure and hope and pray it builds. Still a really good article and walk through on the exact changes that need to be made to port the apps over.
I don't remember anyone saying that .net framework 4.8 is literally the end. I think most people are saying this is the beginning of the end for .net framework. which I think is factual based on the evidence we have.
I see jobs for it here 
This is a great book. Highly recommend 
I see it it comment threads daily. Every time someone makes this claim and gets upvoted. .net core is great and is the future, but I do see it daily where people are saying Microsoft said they are done with .Net Framework. Especially when someone mentions .net framework, always someone seems to make this claim. I just want people to actually know what their talking about... not just repeating rumors.
You're welcome :). 
Hi are you the author? It would be good to have a read me on your github rather than just a link to the site. I do a lot of webscraping and xpath can get nasty with some html, so im pretty interested by this. I convert the html to xml using htmlagilitypack, though ive read SGML parser is better. anyways. I was curious if this is feature parity with xpath 1.0 or possibly 2.0? I never use 2.0. Are all xpath 1.0 functions implemented in this library? many can be actually be ignored with the string functions provided in .net so this is pretty neat. I might give it a shot and see if it gives a better dev experience than coding xpath. Another question, is there a way to convert a given linq xpath query into the xpath query itself? sometimes debugging why an xpath doesnt work can be difficult, so its handy to load up xml spy and build up the xpath slowly to figure out what's wrong. though i suppose with this tool, the immediate window or quick watch window can be used to do the same thing, which would be very helpful. 
I find AngleSharp pretty good as well and includes a lot of webscrapping tools built-in.
We don't have the same definition of "replacing" (since that the word you used in the title). Replacing doesn't necessarily mean it is a one time operation, it can also mean a long going process. So to the question "Is .Net Core replacing .Net Framework", the answer is a highly probable "yes".
Yeah, obviously. What I'm saying is a lot of people aren't saying "highly probably". They are saying "Microsoft said so" which they didn't. At all.
I will check out Entity Framework, thanks for the tip! I am currently at the very base of things, but will for sure try to incorporate unit testing as early as possible. I seems I have been able to generate a basic structure with the tools suggested by /u/FluoricSnek and the next step will be trying to somehow incorporate some controllers to fetch the data.
This is awesome, thanks to your suggestions, I have been able to create an aspnetCore project. Thanks a bunch buddy! I now have a basic structure to build upon. :D
Does AtomicusChart allow for displaying 3D models outside of a chart environment? Would i be able to throw in some 3D Models from some industrial designs and render it within WPF? There also seems to missing some documentation as to what Versions of .NET are supported.
[https://lvcharts.net/](https://lvcharts.net/)
Not enough control and it's not good UX wise. I mean, you run a UI based program and it opens a cmd .. BOOM. I hope you understand what I'm trying to say.
You forgot the next step... Google the error message but it's too early and you won't find anything for a bit.
basically everything here says 472 messes with SSL cryptography, and everyone here thinks i'm the one thats insane... pathetic.. https://docs.microsoft.com/en-us/dotnet/framework/whats-new/index#whats-new-in-the-net-framework-472
Until they release a cross platform GUI library, there is very little interest to migrate to .NET core.. The main hype for me was that it would work on Mac and Linux, but if it‚Äôs just for console applications I would rather use Python or C.. 
I think that's a great suggestion, I am addressing this on next videos. I already changed the title of this particular video to "{get; set;} and the Reflection problem".
Thanks for the explanation 
Just for LINQ alone I would say .NET core is better than Java. But then you have the other things: \&gt; Streams within Java is a disgrace. \&gt; Reflection is an absolute joke in Java. \&gt; Type boxing and unboxing on value types. \&gt; No way to embed the runtime into the distribution. \&gt; Dependency managers suck and the build tools arent as good. \&gt; DateTime isn't thread safe. (Really?) \&gt; Debugging in the JVM is weird, you have to drop the frame to go back up the call stack rather than just reversing the last execution call. \&gt; De/Serialization sucks in Java (Mainly a reflection problem). &amp;#x200B; The only thing I can say that Java does better than .NET Core is hot swapping code while debugging. The only reason I use Java these days is for the existing code bases that I like to work on. When they die, I don't think ill ever use Java again.
You can utilize partial views. &amp;#x200B; Have as many forms as you need in a modal and just hide them with jQuery. Then, when user clicks next you make the requested form visible and hide the previous page. Then, when user is done you simply submit the form using the ajax helper.
Interesting. I heard Electron is a memory hog, but the xplatform is interesting. Does anyone know a better library or have a material that explores the topic more deeply?
You can have a method that has two parameters (Furniture and IFormFile) just call that from your app since you said the user is sending over both image and text at the same time. In your form, have your Furniture elements then another element named "image" and call this new method.
So in theory this works. public IActionResult AddNewFurniture(Furniture newFurniture, IFormFile image) But I cannot test it in postman. How would you SEND those two things to this endpoint to allow it to accept both an image and JSON, kinda the root of my question here. 
Electron has certainly been associated with elevated memory usage. Slack (at least that's my perception) is the app that is most often mentioned in relation to high memory usage. &amp;#x200B; I use Visual Studio Code every day and I get very reasonable memory usage (I just checked and I have open a project with hundreds of files and VS Code is using less than 40 MB). &amp;#x200B; Electron will always be at disadvantage when compared with a native app, but a native app will only run where it's "native". &amp;#x200B; With Electron we can have things like VS Code in Linux and Mac and many other apps that I think we might not have otherwise.
I think you might be confusing people saying .net framework is done by thinking that means its done right now. If I write in a comment .Net Framework is finished I don't mean literally this second or this release I mean there is no future in .Net Framework. It's a nuance that can be lost in text posts vs talking to someone directly
Qt is "native" and also works in linux, mac and win. From my point of view Electron is an experiment that goes too far
Electron is a way of developing HTML/Javascript Web based UIs and distributing it as a desktop application. You can think of it as a mini web browser with only one tab. If your not already doing web based ui development I wouldn't recommend it. There are better xplatform ui libraries out there to explore. Additionally if your windows only just do wpf its significantly better. Now if you do a lot of web development and have experience with SPAs but want to release a desktop client then Electron is pretty much the only solution.
Qt is fine if you're working in an open source project, but if you want something commercial it becomes very expensive really quickly: [https://www.qt.io/buy-product/](https://www.qt.io/buy-product/)
 If you're using a newer version of visual studio code (within the last month for normal release, or last \~2-3 for insiders) it now uses the new electron 3, that uses much less memory. Slack probably simply hasn't moved yet.
&gt; (I just checked and I have open a project with hundreds of files and VS Code is using less than 40 MB). I'm baffled that people that know is even a thing Electron don't know Chromium uses a multiprocess architecture. VS Code takes more than 100MB instantly, divided into small processes (if you're on Windows, you can sort the processes by name in the Task Manager, so all the processes associated with an application will be grouped in a single entry, that way you can actually see how much memory it is actually eating). I'm not directing this to you, but ignorant Web developers are one (of many) reason why software developers hate Electron so much.
Good learning path. Some areas I haven't touched as a dev yet, will definitely try checking them out. Thanks for the share.
My XP is solely based on web development. I'm lookint into expanding my knowledge to something like desktop apps or PWAs, i.e., mobile development.
How many sessions are you talking about? If its very dynamic, using partial views will work. You do an AJAX request to a partial view, return the content, and display it via JS. If you want to keep it really simple (i.e. limited session types), you can have all the different form types listed on the razor page as hidden until user selects one. Then simply display it to the user. I don't use bootstrap modal but I found a great post with some helpful info on how to accomplish this: https://www.thereformedprogrammer.net/asp-net-core-razor-pages-how-to-implement-ajax-requests/ Do note that if you need client validation (i.e. JQuery unobtrusive), you'll have to get that working but its a little tricky.
I'm looking at that process tree and it's 34.8MB total. I also have full visual studio open with another similar sized solution and it's taking 1174MB. 
Is this still the case on the latest version of VS code? 
I really don't know what you're looking at mate, VS Code (latest version) takes 200 MB divided in 6 processes, without any folder/files opened.
You're right, I've just checked in Ubuntu and I see that. I don't know why I'm not seeing that in Windows. Maybe some option in Task Manager is hiding those other processes
I'm running the latest release. Anyways, it is not possible for the VS Code team to make VS Code take less than 100 MB because it is an Electron app and Electron uses Chromium (we all know the Chrome-RAM memes, don't we).
&gt; you can sort the processes by name in the Task Manager, so all the processes associated with an application will be grouped in a single entry, that way you can see how much memory it is actually eating Speaking of ignorant, totaling memory usage up this way completely ignores often large amounts of memory - particularly with Chromium - that is shared between processes and not actually being used independently by each process.
Irrelevant to the point, the point is that Electron is a memory hog compared to every other Desktop framework and by a significant margin no matter how much memory they share. Maybe I'm forecasting where you're trying to get, but no, I don't blindly hate Electron, if it fits your project, go for it, it's a tool like every other framework, I get its upsides but you have to acknowledge its faults.
You can test it. I forget the configuration off the top of my head, and I'm on my phone. Remember the object is from the body and the file is from the form. If I recall, you have to change the header to specify that it's multipart or the mimetype. 
No not confused. should I link you to some comments, upvoted comments, of people saying that? :)
Here is another good article about your question. [https://blogs.msdn.microsoft.com/benjaminperkins/2017/03/07/net-core-application-where-is-my-exe-how-to-publish/](https://blogs.msdn.microsoft.com/benjaminperkins/2017/03/07/net-core-application-where-is-my-exe-how-to-publish/)
The industry seriously needs a cross-platform GUI browser or GUI browser standard. Trying to get HTML/CSS/JS/DOM to act like desktop GUI's has created a yuge mess. JavaScript libraries can come close, but they are complex and often break, especially upon browser version updates. Everyone was hoping HTML5 would solve that issue, but it hasn't. Java applets were also supposed to solve it, but Java tried to be a full virtual OS among other things, making it too complex to manage and keep patched. (I don't know if Oracle just botched it, or bit off more than they could chew.) A good standard would *just* focus on GUI's, not try to be a Swiss army knife. "Productivity" applications still need desktop- and mouse-friendly GUI's. I'm not proposing we toss current HTML+ standards, but rather create a different GUI standard to enhance the HTML stack to be GUI-friendly.
Accurate measurements of ‚Äúmemory usage‚Äù are pretty hard to get, since it‚Äôs quite a broad term. 
Are there good dotnet core Qt bindings yet? I haven‚Äôt looked in a while. 
Is there any code between the comments?
Most of it is in the `OfferMessage` method. Everything else is currently proxied to the internal buffer block, but I included it because I don't know if that's the correct thing to do.
Before reading the code: Do you know that you can pass a filter predicate when linking blocks?
After reading the code: You realize that you're basically locking all usage of the block with a single lock? That's horrible for concurrency (which is what Dataflow is mainly used for)
Sending events from node/electron to dotnet looks simple enough, but what about the other direction? Say I've got a continuously long-running dotnet operation that fires events every so often, is there a simple way to send those events to the front-end?
I wasn't under the impression that you're meant to create your own blocks - rather combine existing ones. Filtering can happen in the delegates provided as argument to those blocks.
As I understand it that won't drop the messages that doesn't pass the filter. But mostly this is an exercise in learning how to create blocks.
So this brings me to my question of `ModelBinding`, the API will not allow for these two separate types to be passed as parameters, what does that `ModelBinding` look like?
Not right now but it's on the roadmap.
&gt;But mostly this is an exercise in learning how to create blocks. [You're not meant to.](https://www.youtube.com/watch?v=AFMv_nFIfvk) It's supposed to be feature-complete, save super-outlier special-case stuff you'll probably not encounter. Learn how to make proper meshes instead.
Yea, that seemed odd to me too. But that's what they do in BufferBlock&lt;T&gt; so I'm going to be single threaded either way. 
&gt; System.Threading.Tasks.Dataflow.dll includes the handful of blocks previously described, but the system becomes more powerful when custom dataflow blocks are developed by 3rd parties. There are a variety of ways to provide such functionality, ranging from implementing the interfaces from scratch to building blocks that layer functionality on top of existing ones. -- Stephen Toub 
Depending on what exactly you need to do it might be best to treat the HTML as a text document and parse what you need out of it with regular expressions.
 public IActionResult AddNewFurniture(NewFurnitureModel model) public class NewFurnitureModel { Furniture NewFurniture {get;set;} IFormFile Image {get;set;} } 
What I'm ultimately going to need is something that conditionally fetches information. Basically this pseudo-code: if ( cache.contains(x.TeamKey)) offer x to next block else buffer x fetchTeam(x.TeamKey) offer x to next block And I'm going to want to wrap that all up in an `IPropogatorBlock` so that it's reusable. *** But before I get that far I need to learn the basics of custom blocks.
&gt; runtime environment. I don't have to think if SQL Server or .NET Framework (or even Core Runtime w/ SCD builds) have been installed. I am getting to a point where I'm starting to wonder what the hell everyone else is still doing with docker, kubes, et. al. We are at a p I would be carefull with this - While I agree that far too many people are chasing the latest javascript/docker/kubernetes/whatever trend, and you can get surprising amount of performance and productivity by coming back to the basics. However, managed DB of a cloud provider will typically have ability to roll it back to any time withing last week, 1-week backups, etc. Are you sure you have all of that covered? Also be careful with the trade offs you are making, for instance we have a few containers accessing the database to perform various functions, you could call it "micro-services lite", that would not work with a file on disk. 
The open source version is LGPL, so you can use it in a commercial product as long as you make it possible for end users to replace the Qt libraries.
Awesome. Other than that, the project looks great. I look forward to trying it out.
Helpful yes, but this brings me back to testing it. How can I send an image + JSON to this end point? What adjustments need to be made in post man, I cannot seem to figure out how to hit this end point with the right settings? I cannot send `JSON` through `content-type=multipart/form-data` or ANY OTHER multipart type, do I need to send form-data, raw/type JSON, key value pairs? No combination I attempt will hit the endpoint, all return { "": [ "The input was not valid." ] } in postman
[He explains that here](https://www.youtube.com/watch?v=AFMv_nFIfvk&amp;feature=youtu.be&amp;t=3400), with kinda cool idea of the block feeding itself, creating a little webcrawler.
Looking over the options, it seems that only the `ExecutionDataflowBlockOptions` exposes `MaxDegreeOfParallelism`. Everything else seems to be single-threaded. 
They should appear in the details tab but possible not the main one.
Critique aside, I'm glad you posted about this library. Everyone should know about it - it's awesome.
you win this time :D
Recently at NDC Scott Hanselman demoed porting a WinForm app to NetCore 3 and then running it within Electron. Pretty amazing to think where we‚Äôve come!
I'm lucky Man :-)
Something's missing. On the whiteboard he has `LinkTo(target, predicate, true)` where `true` means "drop this message if it doesn't pass the filter". But when I look at the API for LinkTo (public and extension methods) I'm not seeing that last parameter.
&gt; I do a lot of webscraping and xpath can get nasty with some html, Is System.Xml.Linq not viable once the conversion to xml is done?
Have you found a page where to find Preview 4 / RC only changes? The one on the main page on shows what changed in total
 With [widdershins](https://github.com/Mermade/widdershins) you can produce static documentation from your OpenAPI 3.0 / Swagger 2.0 / AsyncAPI 1.x / Semoasa 0.1.0 definition. And [Mkdocs](https://www.mkdocs.org/) helps you create beautiful API documentation from markdown files. [Example](https://user-images.githubusercontent.com/781074/53407600-d283d200-39bc-11e9-8d8d-2b094fc4772e.png) 
[It's not this link?](https://docs.microsoft.com/pl-pl/visualstudio/releases/2019/release-notes#summary-of-whats-new-in-visual-studio-2019-rc)
oh jeez, i never thought of a native linq to xml, so obvious in hindsight. thanks for the tip 
I think you're meant to use the buffer.LinkTo(DataFlowBlock.NullTarget&lt;T&gt;()); Set up all your filters, then the nulltarget receives whatever is left and just does nothing.
English link without fragment: https://docs.microsoft.com/en-GB/visualstudio/releases/2019/release-notes
Freedom link: https://docs.microsoft.com/en-US/visualstudio/releases/2019/release-notes
Not well documented, but definitely the right answer. 
I'm still intrigued by the prospect of generating xpath from linq expressions though, would be cool to have an IQueryableProvider generate that so it can be used with the DOM model when necessary.
This also lists all changes made in Preview 3 , 2 and 1... which is fine for most but for those who actively tracked and tested changes its kinda double the work.
Indeed, little documentation here. I wonder how this is going to act when you set up constraints on the parallelism greediness on the linked blocks. Will the nulltarget swallow data it isn't supposed to? Hmm...
It's really not clear from that link, are we still in RC or has the final product been release now?
[Imgur album](https://imgur.com/a/XNfZzj5) with the same graphic for other programming languages - for comparison! | If you want more information on how these were created, it can be found [here](https://www.globalapptesting.com/blog/picking-apart-stackoverflow-what-bugs-developers-the-most)
As far as I can tell it's released
Sweet! That's my weekend sorted!
Had the same problem, I think it's not fully possible yet. Haven't exactly followed the github threads though https://github.com/postmanlabs/postman-app-support/issues/1104 there is another issue referenced in it. 
It's not.
It's preview 4 but also RC. This is not the actual release, but the RC binaries are set for distribution which is the most important part. Release will be April 2nd
It's not release. It's RC and Preview both. Release is April 2nd.
Yeah looks like you're mostly right, it's RC not the full release, though at least it's not a preview anymore: https://dl.dropboxusercontent.com/s/hrn5z9pj0xtnf2l/vs_installershell_q8nLOa2nRn.png
Nice, thanks for clarifying.
BufferBlock is constrained by an internal lock to be single-threaded, so I'm not worried about parallelism. See the updated post for how I have it setup now using `DataflowBlock.Encapsulate` and `DataflowBlock.NullTarget`. *** Also, thank you for your help. I would have never found the 'right' answer on my own.
What is the shortcut to trigger the "Search Visual Studio" in the left? I'm trying to find the name so I can change the shortcut but it doesn't seem to be obvious
Sounds like you understand the problem. It is definitely non-trivial to profile the entire application. I was planning on extracting it to a method like you said, but I was just going to leave it on as part of the stack trace because as long as the trace is consistent it doesn't really matter. I wasn't going to use a debug flag, but instead a data driven feature toggling library we have that allows me to turn the profiling on/off based on client. I also think the time spent doing the profiling is negligible compared to the amount of work actually being done by the method so I'm not that concerned, but I do want to make sure. Also I only care about calls where the cache is missed so in theory it shouldn't happen for every call.
It's both. That's the key. They split of the release channels to stop the madness where preview people had to uninstall preview because the temp license ran before the next preview. Usually we had to reinstall regular releases I'm between long preview ranges.
##r/DataArt --------------------------------------------- ^(For mobile and non-RES users) ^| [^(More info)](https://np.reddit.com/r/botwatch/comments/6xrrvh/clickablelinkbot_info/) ^| ^(-1 to Remove) ^| [^(Ignore Sub)](https://np.reddit.com/r/ClickableLinkBot/comments/9wy10w/ignore_list/)
is it any faster? i moved over to rider ide and not sure if i would go back now
Change namespace now available. If you have a wrong namespace (file does not match the namespace) you can have a code fix when pressing ctrl+. on the namespace. It will suggest the correct namespace.
Without ReSharper it is definitely. Been using the previews for a few weeks now.
 public async Task&lt;IActionResult&gt; UpdateAvatar([FromForm]ProfileAvatarUpdate model) { if (model.ProfileImage != null) { // etc 
Seeing the following options in the installer: VS 2017 15.9.7 VS 2019 16.0.0 Preview 4 VS 2017 15.9.7 Preview 1 Not seeing a separate install for the 19 RC vs the 19 Preview What am I missing?
Sounds like Preview 4 is just another name for RC. The preview release that became the first release candidate. 
I've read somewhere before that the new VS SDK coming with 16 does make it possible to extend codelens (e.g. adding other providers for woking items). Is there any info on that?
From the blog post, there should be 2 separate 2019 installs, one for the RC that will eventually become the production channel. And one for the previews. Right?
would make for a decent poster
Yup, that's how I understand it as well. You can update to Preview 4 **and** install the RC side by side - those two version should be identical. The RC can then be directly updated to the GA while the preview will continue in the preview track.
The MVC 5 stack is generally designed for plug-and-play teams of specialists who already known a particular layer, such as UI specialist (Razor/Ajax), navigation/routing/controller specialist, database/entity-framework specialist, etc. If you are a lone coder trying to take all the layers on from scratch, I feel sorry for you. It's probably the wrong tool for the job. *Good Luck.*
You can deploy 10 web apps on Azure for free
&gt; You can update to Preview 4 and install the RC side by side - those two version should be identical Hence my issue, I'm not seeing both those options in the installer.
I didn't see the RC option in the installer either. Just grabbed it from [https://visualstudio.microsoft.com/downloads/](https://visualstudio.microsoft.com/downloads/) 
Download it from: https://visualstudio.microsoft.com/downloads
You have to specifically download ([from here](https://visualstudio.microsoft.com/downloads)) it for it to appear in the Visual Studio Installer
[Expected release date: April 2](https://twitter.com/VisualStudio/status/1100835710251220994)
So basically resharper?
Here's a walkthrough of [how I accomplish this in ASP.NET Core / Angular](https://gist.github.com/JaimeStill/21a8bb06242e4a418e047ae20d1de674). Note that in this case, I'm just sending the ID in the URL path of the POST URL. However, if you really want to add a JSON object to the POSTed `FormData` object, you could easily do so with [**FormData.append()**](https://developer.mozilla.org/en-US/docs/Web/API/FormData/append), then just extract the object in the controller from `Request.Form`.
Yea I've been using thar function since like 2012
i still don't get what im missing here value types live on the stack reference types on the heap, yeah there is more to what a reference type structure is like on the heap and with a reference type when instansiating say a class, the reference to that object on the heap is also on the stack. furthermore copying a value type primitive or struct creates a new instance of that type. while allocating a reference type to another varaible just creates another reference on the stack pointing to the same object on the heap. this is a pretty long 2 part article. Is there a tldr of whats important that im missing here?
I think you're in good shape then. If you're truly paranoid you could always throw this into a bit of sample code (nested a few levels) and see what the perf impact is of walking that sample stack. Multiply that by how often you figure you'll see a cache miss and you have a rough estimate of the cost. If that's negligible in comparison to the rest of the system then you're set.
Sent you a pm.
My personal experience when consuming web APIs that handles images, is that they're always uploaded as [Base64 encoded](https://en.wikipedia.org/wiki/Base64) strings in the JSON. I've also done this exact thing in a web API I've implemented and had no issues with it. Base64 encoding/decoding is trivial and you'll be able to do it a few lines in most common programming languages. // Model... public class NewFurnitureModel { Furniture NewFurniture {get;set;} string Base64EncodedImage {get;set;} //string ImageType {get;set;} // Possibly add this, so that you know if the image is a jpeg, png, or something else. } // Controller action public IActionResult AddNewFurniture(NewFurnitureModel model) { // Decode the image, then do whatever with it. } // Example JSON body of a request { "furniture": { "someProperty": "Some value", "otherProperty": "Second value..." }, "base64EncodedImage": "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" }
**Base64** Base64 is a group of similar binary-to-text encoding schemes that represent binary data in an ASCII string format by translating it into a radix-64 representation. The term Base64 originates from a specific MIME content transfer encoding. Each Base64 digit represents exactly 6 bits of data. Three 8-bit bytes (i.e., a total of 24 bits) can therefore be represented by four 6-bit Base64 digits. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Is the talk published anywhere?
I don't know if you guys had problems with the earlier preview. release but when I tried to open sn edmx file it just crashed. 
it does!
That's a lifesaver especially if you're moving classes around folders to reorganize. Something that R# has, but I'm pretty glad it's a default code-fix now.
You CAN send binary data as a one of the properties in a json object. I've seen it done on a project, but I don't have time to find how we did it atm. May start with this https://developer.mozilla.org/en-US/docs/Web/API/FileReader Use the binary value of the file and set it as the value for the image property. Trying to replicate it in postman is a whole separate issue. Unless you can use js in postman...
So is WPF designer for .NET Core 3 coming with the release of Core instead? 
MVC is not a complicated stack designed for "controller specialists" (what the hell is one of those). MVC is the industry standard for enterprise applications, and for many developers just the preferred choice for any web application. MVC is also not the solution for abstracting data access (your database specialists/EF specialist) away from your view logic. That's what different layer's are fore (i.e. n-tier architecture etc.). While I agree that MVC can be a black box in certain areas, it is the industry standard and it is in no way any worse for that than web forms is (with viewstate, page lifecycle etc.).
&gt;hosted both the React frontend and Web API backend (written in .Net Core) in Azure, with SQL Server for the database Can you provide us some links to explain that? All I can find is how to deploy using the Visual Studio templates that puts React and Core together.
Not sure if this helps but you don't need to send JSON when it is a multipart post. just send each field as another multipart field. if you are using javascript to do this you just need to add each field to the FormData you are already using. Using the `NewFurnitureModel` given above, add the the `[FormBody]` attribute to the action parameter (`AddNewFurniture([FormBody]NewFurnitureModel model)`). THen just add each field with dotted notation to the FormData. For example: If i had a JSON object like: { "title": "my title", "metadata" : { "myfield": "field data" } } Then add it to the FormData as: formdata.append("title", "my title"); formdata,append("metadata.myfield","field data"); // then add your image - where file is the File object returned by the fs function/ file input formdata.append("Image", file); 
Easy fix, put everything in the same namespace.
Is this the best way to store images though? If you look at my `upload` method i'm saving the image to `stream`, can you do that with `base64`? Is it heavy to read that on the front end, do I need to encode when picture is taken and send? When I get the image do I need to decode it before displaying?
Really that's a pretty huge question and making decisions about design and architecture depends on constraints (so what do you have available). You could do this fairy simply on something like AWS Lambda (as you're already using AWS) [https://aws.amazon.com/lambda/](https://aws.amazon.com/lambda/) There's tons of other options of course. In 'local' solutions something like this may be done using a Windows Service (in C# / .NET look at something like TopShelf to handle windows services [http://topshelf-project.com/](http://topshelf-project.com/) )with something like this you'd likely also want to schedule when the operation takes place and for that you might want to use a scheduler like [https://www.quartz-scheduler.net/](https://www.quartz-scheduler.net/) &amp;#x200B; Sorry I can't give a done and dusted answer but as with most things in software the answer is 'it depends'.
Note that I am not talking about *storing* the images. My previous reply was only about actually uploading the images from the client to the server. Using Base64 encoding, you can transfer the image as part of the JSON data. This removes the need for multiple requests to upload the furniture data and the image and makes it easy to link the `FurnitureImage` to the new instance of the `Furniture` class. It also avoids the issues when you try to make a single action which accepts both an `IFormFile` and another object from the request body. -- Once the user selects an image, you read that image and Base64 encode it on the client. Then you upload it to the server. On the server, you decode it, which will result in you having a `byte[]` containing the image. You can load that into a stream and save it to disk or do whatever else you want with it.
You can post questions here. Stackoverflow has been a hit or miss for me, some solutions are just not good. &amp;#x200B; I started with 0 MVC 5 experience but never had strict deadlines. There is a lot of information to take in, and it may be overwhelming. Are you strong in C#? &amp;#x200B; Reading Pro [ASP.NET](https://ASP.NET) MVC5 and going through the few chapters with making the sports store should get you up to speed. There are chapters that go into detail of MVC (model-view-contoller).
Mostly through [The Morning Brew](http://blog.cwa.me.uk/) and a few other blogs I subscribe to (several of which also show up in The Morning Brew regularly). 
See a reply I made above. I link out to my GitHub repo with the project I mention. Basically if you know how to make a React app call into an API or to a web endpoint for some data, you literally just do that. :P
Same. Out of 30 or so jobs in my area for .Net, only one wanted core experience everyone else was still on webforms/mvc 5.
Totally understand. I will try to list the data sets and types of operations I will need to perform later today hopefully. I appreciate the response and I will look into AWS Lambda since it seems like it could work. Is there an Azure equivalent to Lambda? I have been interested in trying Azure utilities but haven't gotten the urge to start a new project or change over from AWS on this current project.
If server-side Razor Components already implements SignalR to send updated html, would there be a way to tap into that connection and start listening to other hubs too?
[removed]
I'm very interested. Storing the image is my main concern. it seems like storing a base64 encoded string to a SQL db is frowned upon because of the size of a base64. So naturally this route does not appeal to me, but saving to the disk, or stream seem like viable options, but what are the pitfalls? Will I run into scalability issues? Since the images will be generated from a phone app, is saving to the stream a bad idea? 
I like using Onion architecture, which is basically the repository pattern with a service layer between the repository and controller. For simple apps it might seem like you‚Äôre just writing pointless code that does the same thing as the repository, but if/when you start manipulating data, that‚Äôs where it really shines because you offload logic from the controller to the service layer. I do this because I try to make controller logic as simple as possible. But like everything, it‚Äôs simply a matter of opinion.
https://www.infoq.com/articles/repository-implementation-strategies https://www.infoq.com/articles/repository-advanced I wrote these specifically to answer that question. 
As usual "it depends". Applying design patterns usually helps you achieving good code maintainability and more. But this has a cost. Before you proceed you need to answer questions like: "how long will this project last? How long am I allowed to work on it?". And most important, regarding your question, how often your repository technology will probably change? If you're sure it's not going to change often, then opt for a compromise between patterns and spaghetti code (which doesn't mean to put repo cruds directly in the controller). A simple repository pattern (as you described) give you a good decoupling and it's fairly simple to implement. This will save you from a blood bath when someday they decide to migrate from repo type A to type B. IMHO: do yourself a favour, implement at least a simple pattern and try to be as SOLID as possible. Try to mediate with your boss on this. It's you that has to fix problems in the future, not him/her :)
Release candidate but not intended for production use? That's a bit disappointing. Source: https://docs.microsoft.com/en-us/visualstudio/releases/2019/system-requirements
There's some contention about the Repository pattern (also known as the Repository antipattern by some people) but if there's one thing everyone can agree on, it's that it *certainly* doesn't belong at the controller level. Repositories are really common and I don't recall seeing a very convincing alternative. As with anything else, I'd advocate relying on your sense of "smell" - if your use of Repo leads you down a path that starts feeling "off", find out why and reconsider.
+1 for the onion architecture. Never knew it had a name! Just to add to this - I‚Äôd only go from controller-repository to controller-service-repository when there‚Äôs enough logic to put in the service...seen many projects which use the latter for basic CRUD operations and all it does is add unnecessary abstraction.
My issue is the ID is generated in the DB. Currently the way I have the two endpoints, it works as desired, and if I had the ID available when sending them then I would be able to split up the calls to the endpoints.
EF implements repository as DbSet... Why reinvent the wheel?
Yes totally the equivalent would be Azure functions [https://blogs.msdn.microsoft.com/azuredev/2018/08/29/azure-functions-and-serverless-computing/](https://blogs.msdn.microsoft.com/azuredev/2018/08/29/azure-functions-and-serverless-computing/)
I'm a newb, what I see as a big problem for RIder is basically ZERO "Community". Jetbrains Rider forum is a ghost town. You're basically stuck with making a ticket and waiting to the following day for an answer. I'm US, pretty sure they're based in East Europe somewhere. Not a big deal really as it is a awesome product. Just wish it had some items that make it easier. 
They are talking about common performance mistakes that developers make, and showing in code how many developers trick themselves into thinking that their synchronous code is executing asynchronously. Despite the title, it does not imply that ASP.NET Core doesn't scale in general, or for specific versions. They are implying that ASP.NET Core doesn't scale when you make performance mistakes. No other web platform will scale with a synchronous or thread per request model either, as its mostly the cost of the OS threads themselves that affect the scaling. 
Download the RC installer. It's always been that way. 2019 preview didn't turn up in the installer until you executed the setup launcher at least once.
Excellent! Just a quick read over and it looks pretty straight forward to setup. Appreciate it!
Referring to my comment up the chain. New product manifests register themselves only when you execute the channel specific installer exe at least once.
Great info, thanks ü§ì
In practice I have found that without the extra abstraction between EF, unit tests are harder to setup and run. Not all code plays nice with the In Memory provider (it has different transaction semantics).
The binaries of the RC install can be legally redistributed whereas preview binaries can't.
Yes, but just for x64. Nvm you should look to Avalonia UI. Not as mature, but if the UI is simple, it could work
Because then you can't unit test classes that use EF. If you're doing a Q&amp;D app where testing isn't required, the EF has a lot going for it. If you're writing production code, EF has little value since you're going to create a Data Access Layer or Repository 
JFC onion/hexagonal architecture has fuck all to do with repositories, this is blind cargo culting
Simple 5 page application? Stick it all in the controllers. Bigger application or you want unit tests, you need a data layer (lets not call it a repository, it confuses people). Large application, you might want to add a sevice layer or jobs between the controllers and the data layer. Want to go nuts? Look at clean architecture. We should also start talking about view models and data transfer objects.
That's what I thought, appreciate the insight. 
At the bottom, I'm saying you if you need to associate the upload(s) with a record, you can append the JSON object to the form, then deserialize from FormData on the controller. In one POST method, you can do everything you need to do.
They are supposed to be going on the NDC site but I‚Äôve not seen anything yet and google isn‚Äôt turning anything up yet 
I agree with this. You end up with more code, but IMHO it's easier to maintain and test.
&gt; I use Visual Studio Code every day and I get very reasonable memory usage (I just checked and I have open a project with hundreds of files and VS Code is using less than 40 MB). I have an Angular 7 application running "ng serve" + a debugger through Chrome, and without any documents open - it's sitting at 1200 Mb of ram. It's a memory hog, but a very performant one I have to say :-)
Unnecessary abstraction today means less refactoring tomorrow. There‚Äôs really no project too small to follow good practices. 
In a previous project we also used the DbContext with an inmemory database for unit tests. It actually got in the way a bit sometimes and we thought it was actually a bad thing to do. - Sometimes we had to add test data to entities in the DbSet that was not necessary for the query, just for the item to be returned. That was rather annoying and made a test a couple lines bigger even though it did not benefit the purpose of the test. - You are essentially testing if a DbContext works. You should not have to do this; 3rd party libraries are dependencies and you can assume they are already tested by the developer of said library. - With InMemory, we made every test use a new database. This made the tests a bit slower compared to just mocking a DbContext. What we ended up doing is creating a IDbContext interface and used that in our DI container. That way we can just mock the DbContext and decides what it returns. We did have to add some TestEnumerable class or something to make it work (I could look it up if you want) but now we are no longer dependent on EF core to check if our tests run or not. For integration test we do use an in memory database.
Yup: https://github.com/qmlnet/qmlnet I'm the author.
Incorrect. QmlNet works on any platform that .NET Core and Qt works. https://github.com/qmlnet/qmlnet
Well, before your put your calculations on database componentes you must decide: backend will have complete dependence with your database componentes. Yes/No, if you said yes, then you have no choices with alternative data sources. If you said no, you may have options on future design changes. &amp;#x200B; Database its great for row complex calculations, but using databse engine create high coupling between your backend and your database. &amp;#x200B; If you want to include this project on your developer portfolio. I think, you must to separate your details ([ASP.NET](https://ASP.NET) MVC /Data base) from your policies (Business Rules and Core Calculations). That will speak better of your work for future employers.Hire a programmer who writes highly dependant code only will create problems.
Use a repository if it isn‚Äôt already provided. EF and EF Core already do, so no value added there. If you‚Äôre using some NoSQL clients, they can be quite low level so a repository adds value.
can you recommend any simple easy to follow examples of this?
We use the Oinion in all our enterprise applications. Its 100% the way to go for any non trivial software. 
this is what I do, at first i questioned it that it was too much unnecessary code, but after i used it, it started to make so much sense, I do alot of manipulation in by service layer, and i keep my repository only CRUD 
This, this, 1 million times this.
You could save the images in a predefined folder on the server, then add the filename to whatever database records that need to refer to an image. When you have to present the images, you've can do a few different things. If you want the user to be able to download the image, you can have an endpoint that takes an image name as parameter, then reads the image into memory, then converts it to a Base64 encoded string, then returns that string to the client. The client then decodes the string and saves it to the user's device. Alternatively, you could simply serve the image as a regular file. No encoding/decoding needed. Personally, though, I'd stick with using JSON to send and receive data, so I'd go with the Base64 encoded string. If all the images are meant to be public (it won't matter if everyone can access all images), you can save the images to a publicly available folder on the server. Once a client has obtained the name of an image (which is essentially its unique ID), you can generate a link to the image, like `https://www.example.com/images/the-name-of-the-image.jpg`, which can then be used to view and download the image like any other image on the web. This approach also makes it possible to create web pages that refer to the images, using regular HTML, like `&lt;img src="/images/the-name-of-the-image.jpg" /&gt;` Again, please be aware that doing this, all your images are 100% public, so keep that in mind. -- For some reason, I can't access the repository with the code I'm using to encode/decode images, so right now, I can't show you an example that I know works. I've tried to come up with a working solution from memory. I'm building upon the example from my other comment. This example lacks error checking and stuff like that, but it should work. const string ImageFolderPath = "images"; // A folder named 'images' that is relative to the current directory. public IActionResult AddNewFurniture(NewFurnitureModel model) { // Get the encoded image string from the model, then // convert to a byte array. string encodedImage = model.Base64EncodedImage; byte[] rawImageBytes = Convert.FromBase64String(encodedImage); // Generate a random name for the image and combine it // with the folder we want to save it in. // We assume the image is a .jpg file. string imageName = Guid.NewGuid().ToString() + ".jpg"; string imagePath = Path.Combine(imageFolderPath, imageName); // Save the image. File.WriteAllBytes(imagePath, rawImageBytes); // Tell the client that the image was saved successfully. return Ok(); } If you have a client written in C#, you only need a few lines to actually Base64 encode an image, so that it is ready for upload: string imagePath = ...; // The path to the image that the user wants to upload. byte[] imageBytes = File.ReadAllBytes(imagePath); // Read the image. string base64EncodedImage = Convert.ToBase64String(imageBytes); The string `base64EncodedImage` is now ready to be treated like any other value for an object that will eventually be converted to JSON and POST'ed to the server.
It seems that most people don't really understand what's the repository about. Repository is a public contract for your core (domain models), which tells the developer what can be done with these domain objects. Generic repository is a cancer and might be used as a helper class/interface that you can inject into the actual (proper) repository implementation (e.g. EF DbContext which provides UnitOfWork + DbSet represeting "generic repositories"). Think about the following scenario - it might make sens to define all of the CRUD methods when dealing with a Product model, but would you really want to have Update/Delete possibility for e.g. Orders? Is that really such a big cost, to create the IProductRepository interface which defines Create/Red methods only for Order aggregate? Why would you ever use the generic repository for such scenario? It's really terrible, when you watch all of these Pluralsight, Udemy etc. courses, and people tell you to create a GenericRepository&lt;T&gt; because you have to follow DRY principle and then they define domain models as aenemic classes, being simple dummy bag of public getters and setters - this is so wrony on so many levels.
I actually did some research after posting and found the old Qt# bindings, then eventually this. It looks great! I‚Äôm going to play around with it tonight. My first job was Qt in C++ way back in 2002, no QML anywhere, so I‚Äôm looking forward to it. 
The page I linked to, where you can download the RC, states "This release is not "go-live" and not intended for use on production computers or for creating production code", while the RC announcement blog post claims it has a go live license.
You forgot about .NET Standard :) You might want to take a look here: [https://stackoverflow.com/questions/42939454/what-is-the-difference-between-net-core-and-net-standard-class-library-project](https://stackoverflow.com/questions/42939454/what-is-the-difference-between-net-core-and-net-standard-class-library-project) &amp;#x200B; Basically, there's the 'classic' .NET framework (which was started somewhere around 2001 IIRC) and there's the new and shiny .NET Core framework. &amp;#x200B; ASP.NET is the web application framework and ASP.NET Core is the .NET Core implementation of the web application framework. &amp;#x200B; MVC is just a framework to build web applications according to the Model-View-Controller design pattern. &amp;#x200B;
Your principles should be firm, but your patterns flexible. Part of this is because it depends on the technology. Are you using lower level SQL like direct ADO.NET or Dapper? Use a repository because you want that logic abstracted. Entity framework? Just used the context directly. Is the controller doing nothing but crud and authorization? Just inject a context and do it. Is there a request that needs business logic too? Create an object or extension and use the repo there (or context, if that's your bag). And yes, this will probably offend a lot of people, but imo if one controller is straight crud and another is business logic, then I have no problem using different parents in them. Creating empty passthrough layers for the sake of 100% code coverage or dogmatic pattern adherence is a code smell, imo.
Hey, thanks so much for the link! I'll be sure to look through the material there. I'd definitely say my C# is my strongest point. I'm glad to hear this subreddit doesn't mind questions, I'm sure I'll frequent here a lot from here on.
But it has a _name_ and I read an article about it
Yes, totally agree with the *think about the why* aspect. One of the biggest problems was people created repositories around repositories. Wrapping a repository around EF for example. And then we had the *Anemic domain model* debacle where we went from POCOs/DTOs to being told this is terrible back DTOs. I've also seen developers over design too early. There's a tipping point where you're sufficiently invested in your design direction that it makes sense to formalise code into things like repositories, but until that point you just get the code working but disposable as you feel out the design flaws. We also split read and writes, often using two totally different technologies and do a lot more event driven work. Again, doesn't mean you can't use a repo, but it's generally easier to have a read one and a write one which are totally decoupled and don't follow CRUD. You don't need updates or deletes if you're storing an event history. This doesn't mean a repository is bad, as I said, EF is basically just a big repository pattern implementation, but it is one of those patterns people get coerced into adopting as a mantra without understand why or why it isn't appropriate.
You don‚Äôt need a repository or heaven forbid, service classes to do onion architecture.
Do you write tests reliant on setting up a mock container? That's a huge no no for me. DI is IOC not a decoupling mechanism which is where I've seen it go tits up.
Get on gitter if you have any questions.
Yep, although as a a load of people had said EF is an implementation of the repository pattern, the fact it doesn't have domain specific methods makes it a bit crap but still a repository. Funnily enough I mentioned aneamic classes further up. They're really not an anti pattern these days as we use streams, web apis, messaging, ES, etc. There's also the risk that an iron filled object is going to be mutating its own state which becomes a pita where you're operating concurrently. We've also gained things like LINQ, where extension methods rule the roost. You're absolute right though, crappy repos that operate purely at the crud level suck. It's funny how we went from doing everything as domain operations via stored procedures way back when, through CRUD repos back to domain operations.
I fell into the controller - service - repo trap, so many services that just pass objects between repo and controller without any logic between. Now I go controller to repo, if their is additional logic then I put a service layer. Cuts down on a lot of pointless classes and code. 
[y.Name](https://y.Name) is a string, and "letterTypeId is a string, but when you use the == comparison operator, you're essentially asking it to evaluate if [y.Name](https://y.Name) and "letterTypeId" are equal. Since they're not equal, the expression ( [y.Name](https://y.Name) == "letterTypeId" ) is false. You're looking at the value of that expression, not y.Name.
Thanks for your response, how would I write this so that it would equate to true?
Well to what string did you set it? It‚Äôs better to compare strings with the .equals() function. And what are you trying to do? Converting a string to a Int after that?
`y.Name.Equals("letterTypeId")` still resolves to false..
I didn‚Äôt say that would resolve it, it‚Äôs just better practice when comparing strings. Did you check where the value gets set in the model? Does it actually set it to ‚ÄûletterTypeId‚Äú? Maybe you got a problem with capitalized characters? Try equals(‚ÄûletterTypeId‚Äú, InvariantCultureIgnoreCase)
Well, if you're going to keep "letterTypeId" on the right side of that expression, you would need to add a [`y.Name`](https://y.Name) `= "letterTypeId";` Somewhere in your application. Probably closer to when you instantiate y. Without knowing more details about what you're trying to accomplish, I can't give you a super specific answer, but just remember when comparing two strings with ==, you they have to be the same strings for it to evaluate to true. If you need some more guidance and want more specific help, feel free to pm me.
Is y.Name returns "letterTypeId" ? 
It sets it in a previous API call, this what it looks like in postman &amp;#x200B; The part I can;t wrap my head around is that it does sometimes equal to true, but there is not a shred of difference in the way that they are returned. Basically whats happening here is I have a town of objects, each with a subset object with a name/value pair. What I'm trying to do is capture letterTypeId and it's value when name is set to letterTypeId. The spelling is never different and I have no control on how this is sent back from the API.
Nono! Lets say we have a GetAllProductsQueryHandler which uses a DbContext to retrieve all Products from the database. In its constructor it would just have a IDbContext dependency. In the unit test class I have a `private Mock&lt;IDbContext&gt; _dbContextMock` which I use: // Arrange var productsInDatabase = new List&lt;Product&gt;{ //products }; var productsDbSetMock = CreateDbSet(productsInDatabase); _dbContextMock .Setup(_ =&gt; _.Products) .Returns(productsDbSetMock.Object) var queryHandler = new GetAllProductsQueryHandler(_dbContextMock.Object) // Act var queryResult = queryHandler.HandleAsync( // query ); `CreateDbSet()` contains some code that makes it actually work :) Can't post that now but if you get my point and are interested, I can maybe post it :) 
Yes, it does return that, but occasionally decides to not honor it.
Are you looping the API call somehow? What does Name then actually have as a value if it doesn‚Äôt equal ‚ÄûletterTypeId‚Äú?
Maybe some space get through. Or Unicode character that look the same.
Here is a screenshot of what the value is at the time it hits this if condition inside the loop &amp;#x200B; &amp;#x200B; And here is the exact same pattern being followed a bit further down in the code and it is setting the exact same part in the return object to true &amp;#x200B; &amp;#x200B; &amp;#x200B;
You got a space in the second screenshot behind the value of your variable. Try this: y.Name.Trim().Equals(‚ÄûletterTypeId‚Äú) If that‚Äôs doesn‚Äôt work, trim it before. I‚Äôm not sure if you can chain those functions
That‚Äôs ok. But once you neee to do business level stuff you should put a service layer in. API üëâService (via DI) üëâ Repository (via DI) Also then good idea to start using DTOs in API, POCO in service and Entities (not EF or framework specific but data structure specific ) in repo. Then you‚Äôll be good till mid level enterprise. Once you get into large enterprise you need to redo everything using DDD and Eventsourcing because nobody will know WTF is going on the code anymore. Good luck. 
Holy banana shits. 
Thank you very much.
LoL...we‚Äôve all been there
Sure thing
Unless you don‚Äôt need to ever refactor it. That‚Äôs preoptimization and honestly I think it‚Äôs one of the worst sins you can commit as a dev.
Right but they do adhere to onion architecture.
Before I go over to the team that built this and give them a heaping pile of shit for having their API send this garbage back, can you imagine any reason why that trailing whitespace would occur?
Not without knowing more about the code that‚Äôs behind the API. If it is actually the API. Looks you got some kind of JSON format that gets parsed and mapped/binded to your model. Maybe there‚Äôs a little bug? Really difficult to say without the code in front of me
EF provides the repository pattern (via DbSet&lt;T&gt;), but it doesn't mean that it should be used without any further abstractions. Aenemic classes have their use, yet this is mostly related to DTO, ViewModels etc. our Domain models should never be aenemic classes - these should be feature rich, encapsuled objects which ensure that the invariants will be always in a valid state and so on - and this is where the well designed repositories might come in handy (along with other DDD tactical patterns such as factories, domain services etc.). As someone above mentioned Onion Architecture - if you want to follow this approach (or Clean Architecture, Ports &amp; Adapters etc.) you should start with well designed Core (domain) layer - and adding here infrastractural concerns such as EF Core which is just an ORM doesn't really help :).
No that's perfect :) I still come into contact with legacy tests where there will be a couple of hundred lines of mock container initialisation just to test a single service that might add two numbers. Usually the end test will be tautological and named something like TestProductWorks, with no indication of wtf "works" means, even if it did actually assert some functionality worked :) 
User input? If the other team that's sending the lettertypeid and value somehow used a data file to get the property name and value then it could just be user input
If you are a backend dev you need to deep in app.net core webapi. Not mvc nor razor pages. 
Most of the time trailing spaces are caused by user input. This happens often when a user copy paste things into a form field but I can imagine that if the other team uses data from a database to build up the complete response it can also be a cause. Or maybe part of the response is build using texts stored in files somehow.
Can you see the advantages of the repository pattern ? If not dont use it untill you reach a point that it clicks. The point where you say, shit repository could be amazing now. Otherwise you wont understand anything and would think that its just unnecessary.
Just to clarify, when you say "the same string" are you implying that they have to be the same reference? As in: new object.Equals(new object()); // false Because with strings that's fine to do: new string ('a', 1).Equals(new string('a', 1); // true Just checking what you were implying by "the same string"
Though that‚Äôs a good write up, my colleagues are naturally suspicious of async and I suspect I would need hard evidence to persuade them. Of course, if you‚Äôve come from JavaScript it sounds completely nuts to not use this for IO bound operations.
`String` overrides the `==` operator. It is equivalent to the static `String.Equals(string, string)` method. https://docs.microsoft.com/en-us/dotnet/api/system.string.op_equality?view=netframework-4.7.2
What's the alternative? If it's not a Repository, or a service, how does a controller retrieve data from the a data storage location (without doing its own SQL queries, as thats not onion)?
What's the alternative? If it's not a Repository, or a service, how does a controller retrieve data from the a data storage location (without doing its own SQL queries, as thats not onion)?
Simple dto / poco is there to separate data and behaviour. Im also against generic repo 
Simple dto / poco is there to separate data and behaviour. Im also against generic repo
You should be aware that in .net string comparisons should not be done by comparing using the equality operator (==) but you should always use the Equals method. Please read this: https://docs.microsoft.com/en-us/dotnet/csharp/how-to/compare-strings Especially the part about string interning is interesting. 
I have observed in order to push more developers to asynchronous paradigm .net and core teams are gradually removing synchronous variants of methods from few of their IO apis where asynchronous calls or methods are more appropriate. So soon it will become a norm to use asynchronous calls to perform IO bound operations.
I wrote [a comment](https://www.reddit.com/r/dotnet/comments/amtu5p/with_net_can_run_my_program_that_created_in_c/efpr15q/) on this a while back. &gt;In the .NET world, there are four different .NET implementations: &gt;**.NET Framework** The .NET Framework is the oldest, most mature implementation. This framework allows you to create a variety of applications for Windows only. &gt;**.NET Core** This is a new implementation of .NET. Unlike .NET Framework, it is open source and it also allows you to create applications for Windows, Linux, and macOS. The caveat is that it does not support building actual UIs. You can only create console applications or web applications. Note that when version 3.0 of .NET Core releases, you will be able to build desktop applications, but only for Windows. &gt;**Xamarin** This is used to build applications for Android, iOS, and macOS. If you're building mobile apps, this is what you want to use. &gt;**.NET Standard** This is a set of fundamental APIs that all .NET implementations must implement. It allows you to create libraries (*.dll files) that work on all editions of .NET, e.g. Framework, Core, and Xamarin. This means you can share code across programs built using the three other implementations. Keep in mind that different versions of the various implementations target different versions of the .NET Standard. For example, .NET Framework 4.5 might target .NET Standard version X, while .NET Core 2.1 might target .NET Standard version Y. -- The above doesn't cover ASP.NET, but the official ASP.NET website sums it up quite nicely: &gt;An open source web framework for building modern web apps and services with .NET. "ASP.NET Core" refers to the version that's made for .NET Core, while just "ASP.NET" is the version for the old .NET Framework. MVC is simply a design pattern and has nothing to do specifically with C#, ASP, or .NET.
Can you explain what you mean by you use an in memory database? Like do you load a copy of your actual database into memory? Presumably without any actual data since you can just mock what it returns
Haven't had the pleasure of working with those amazing systems yet! I recently started doing test driven development and it's wonderful. 
Anytime I've seen DI explained, the person explaining says it's useful for decoupling the implementation of a dependency from the class that has the dependency - so I'm wondering why you think it's not a decoupling mechanism?
PLS read about layered architecture: [https://programmingwithmosh.com/asp-net/layered-architecture/](https://programmingwithmosh.com/asp-net/layered-architecture/) We use at work Generic Repository and Unit of Work pattern in our Data-access layer project. Generic Repo gives your project a chance to be independent from EF because you use interfaces to the real methods. I also recommand to use Bussiness core layer where you can store your business logic. Then you can this logic from the controller and keep everything clean and separated. Dont forget about using ViewModels, Domain Models etc. 
To expand a bit on this: You'll be wanting to work with .NET Core and not the 'classic' .NET Framework. Core is where the new stuff happens and it's sooo much faster than classic .NET. ASP.NET is to .NET what Express is to Node. .NET and Node enables running your code on machines, while ASP.NET and Express let's you create web applications. You'll want to learn ASP.NET Core (not ASP.NET 5). The Core version is basically a rewrite of the old version with tons of improvement and more modern features. While MVC is classically used to build dynamic, server-side rendered websites, in ASP.NET Core it's also used to create web APIs. It let's you easily separate parts of your API into logical chunks and it sets up routing in an easy to manage way. 
I remember this company I worked for had a factory, and repository pattern in place it was extremely difficult to navigate the code. I like simple patterns. Once you start getting into an over architecture it is going to be a pain in the ass to manage. If you can't follow the code through your ide, you **might** be over architecting.
Really I meant the same contents, but didn't want to get into explaining it. Probably should have now that I'm reading my comment again.
these two extensions look awesome, im so sick of having to clone a repo to check out how to do something only to realise its garbage and then delete it. these extensions for those who didnt read the article https://github.com/ovity/octotree https://docs.sourcegraph.com/integration/browser_extension Any others good for navigating github?
Because for some queries EF rinds like a dog and you may want to use something like dapper
Thanks!
Thanks for the comparisons, that definitely helped.
Thanks!
EF Core has an extension (nuget package) called "InMemory" which means that you can create a database In Memory. Im in my phone, but docs are available. :) It's like a connection to SQL Server; this is just a connection to an InMemory database. You can just set up the name of it and boom, you are done. You can add entities and save changes and do all that; it will just work. This is very very useful for setting up integration tests because you dont need to depend on an actual database. In our case, for every "suite" of tests of a certain controller, we use 1 database. Afterwards it gets deleted a new one gets created :) 
Here I would say the opposite. Assuming your project will never grow, so eschewing good practices for the sake of speed. That‚Äôs a capital sin in my book. Spaghetti is spaghetti, no matter how small the plate
In addition to all the other reasons here, EF returns IQueryable, allowing consumers of your ‚Äúrepository‚Äù to write terrible queries that should be isolated to your data access layer. This forces consumers to know which provider the repo is using in order to write these leaky queries, introducing tight coupling. Consumers of your DAL should be working with abstractions and not care about how the data is persisted/retrieved.
Very well thought out and written. Thank you very much, I am going to dig deeper in this later this evening and see if I can properly apply it to my API.
Good read, thank you sharing this!
There is a massive difference between YAGNI and spaghetti code. If you have a service simply proxying CRUD operations from a repository to a controller because you *might* need it in the future, that‚Äôs over-abstraction and classic YAGNI. Removing it is not spaghetti code.
There's absolutely nothing "better practice" about using `.Equals` for string comparison.
This is entirely wrong, using `.Equals()` and `==` are both perfectly fine.
Re: *"\[MVC\] is the industry standard and it is in no way any worse for that than web forms is..."* Sorry, but almost everyone I know was able to pick up Web Forms relatively quick, but most found MVC roundabout and unnatural at first. (Okay, the early versions of WF were clunky.) Put another way, Web Forms seems to be easier to learn via trial-and-error. In MVC if you get stuck, the clues are more obtuse. Further, being a standard does not by itself reduce a learning curve. Sorry, I didn't mean to imply "controller specialist" is a formal or industry standard title. I meant that MVC is more geared toward dividing staff by sub-specialty rather than by say entity. The divisions may be shop specific, but *more likely* to be divided by technology than domain divisions/tasks. &amp;#x200B;
Never stuck it all in the controllers. That‚Äôs just awful practice. Even in simple apps, controller -&gt; service -&gt; repository is incredibly simple and keeps the logic separated. 
Being an industry standard certainly does reduce the learning curve due to the sheer amount of tutorials, guides and information. Chances are whatever question you have had already been asked, answered and solved. Just because it's a little more difficult than web forms (which I disagree with but it is subjective) doesn't mean you should use web forms. Web forms is all server driven, which is out of touch with the more common UI paradigms we currently have. Viewstate is an outdated concept. My final point on why a new project shouldn't be done in web forms is that web forms is dead. .net core, the clear future of .net, doesn't support it. Don't stick to an old tech, learn the industry standard.
Please do not put anything but stuff like redirects and passing off request data to the next layer in your controller
I totally agree.
How many .cs files do you need to modify to add a new input in the UI that gets saved in a new column on an existing database table?
Re: "Being an industry standard certainly does reduce the learning curve due to the sheer amount of tutorials, guides and information. " True, but Web Forms (WF) also has a lot of online material such that this factor is not a difference maker. Re: "Just because it's a little more difficult than web forms..." I'd say it's almost 3-to-1 if you are on your own. 2-to-1 if you have a mentor. If your experience is different, then so be it; nothing personal. Re: "Web forms is all server driven, which is out of touch with the more common UI paradigms we currently have. Viewstate is an outdated concept." JavaScript-heavy (JS) applications tend to be a maintenance nightmare as JS libraries change often and/or new browser versions or brands come along that are not backward-compatible. Granted, if your shop really wants "fancy" UI's that act more like traditional desktops, then JS-centrism is the way to go, but DO expect more maintenance headaches. Most of an app running on the server means you are dealing with one machine (type), but most running on the client means you are dealing with different brands/versions of clients that are not 100% equivalent. I'd rather babysit one machine than 50 machines. Re: "I also don't see how this would ever be a bad thing? Are you saying you shouldn't use MVC because it's good at separating logic?" But then you have to wire interfaces for sending stuff back and forth between such layers. It tends to create layer interface busywork/bloat. Separation can be nice for certain changes, but is not free. Sometimes shear simplicity trumps separation in terms of improving overall long-term maintenance. Yes, KISS \*can\* trump SOC. Re: "web forms is \[dying\]. .net core, the clear future of .net, doesn't support it. Don't stick to an old tech, learn the industry standard." Core may be a passing fad, nobody knows. MVC may be replaced by another Framework/Paradigm Of The Day; nobody can predict the future of dev; it changes more often than my underwear. I find it hard to believe MVC is the pinnacle of development because it's clunky and wiring-happy for smaller projects. There is too much production WF code for MS to force ending it in say less than 10 years without a revolt. Yes, it could happen, but is unlikely. Even VB6 still runs on Windows. &amp;#x200B;
Why do your example return IList instead of IEnumerable?
DI is really just moving the creation of the dependency, and consequently management of it's lifetime, from inside the dependent to outside the dependent. That does also mean the dependent can be ignorant of the concrete class, but that doesn't imply the two are decoupled. Coupling is a broader, more logical concept. Dependency inversion is a specific type of decoupling, for example. You could still have various forms of coupling between your dependent and dependency.
If I'm interpreting this correctly, it seems a design or tuning flaw in IIS rather than some inherent rule of universal computation. If a process isn't doing anything and there's enough cache, then the process can be cashed away temporarily, or put on the "slow train" core so that it doesn't bog down active processes. An IIS redesign may solve it. If the thread pool is shallow, make it deeper. If Async is almost free of side-effects, such as introducing no debugging confusion, sure, stick it in. But I'm not sure that's the case.
Yeah, you can see this in [.NET Core source](https://github.com/dotnet/corefx/blob/master/src/Common/src/CoreLib/System/String.Comparison.cs#L736) too
I wrote a web-site, mobile app, and windows service starting in 2012 (yes, a long while ago!). Back then I chose to use Repositories, a Service layer, Controllers (for MVC and Web API). Mobile App used Web API, Web App used MVC and Web API (with KnockoutJS for view models), Windows Service talked to the services for it's shared business logic. My domain entities also contain business logic. I was attempting to follow a more DDD approach. I used NHibernate for the ORM and Castle Windsor for IOC. Back then NH was much superior to EF, especially when it came to DDD. I also wrapped NHibernate's Session (EF's version is DBContext) in a Unit of Work. Recently I've been looking at moving across to EF core and [ASP.NET](https://ASP.NET) core etc. It's a whole lot easier having used the Repository pattern and a UOW abstraction. I also have pretty unit test code coverage in my service layer. That's made a lot easier by abstracting data access into repositories. ^((The far bigger challenge is changing IOC containers, with all the interceptors and facilities I have for Castle Windsor, but a new version recently got released, so maybe I won't).) Definitely consider the expected lifetime of your application - at least the maintenance lifetime. If it's not long, then don't over engineer it: put business logic in controllers, talk straight to the database, throw unit testing out the window. 
For the love of God don't put all your logic in a controller! What kind of idiot would instruct someone to do such a thing? 
My knowledge of IIS internals is limited but In my opinion IIS cannot do much in this scenario. If application code is assigned with a thread to use and application code blocks the thread to wait for some I/O to complete there is no way IIS can detect that thread is not doing anything useful other than blocked waiting. I don't think IIS have that sort of visibility. Even in the more recent version of .net core when you are using IIS or NGINX just as a reverse proxy in front of Kestrel Server you will face the same performance issues due to too many blocking calls or long running tasks. Moreover the above post used the IIS as one example. Blocking IO calls can cause performance bottlenecks for others Windows/Console applications Windows application developers doing it for a while (Avoid Blocking UI Threads for all computations and IO processing). Once I became comfortable with Asynchronous paradigm i found few really good tips to debug asynchronous code that really helped with most of the common debugging confusions so in my personal opinion adapting asynchronous programming in my application was a change that caused bit of trouble in beginning but it was well worth the trouble . 
Can anyone tell me why in the f**k .NET Core is like this? It just seems so backwards compared to what I'm used to with the .NET Framework. All of these workarounds just seem so dumb and I know I'm missing something.
If you return an `IEnumerable` you are telling the consumer, "I'm making no promises. Each time you iterate over this, I may make another database call". If you return an `IList` you tell the consumer "this is a in-memory list, feel free to access it any way you like". Returning an `IEnumerable` when you know for a fact that you will always have a in-memory collect is a very bad practice. It is misleading to the consumer, may result in poorer performance, and removes capabilities the consumer may need such as indexed access. 
If you use `First` the database will literally return the first row it finds. If you use `Single`, EF will tell the database to return the first two records (`SELECT TOP 2`). If two rows are actually returned, then EF can throw an error. *** If you know that there will always be one row (i.e. a primary or unique key) then the extra check that `Single` offers is unnecessary. So there's no reason to use it. *** If there actually is a primary or unique index on the column, then in theory the database will know to just give up and return the one row instead of looking for a second row in the `Single` scenario. So from that perspective, using `First` instead of `Single` is a premature optimization and there's no reason to avoid `Single`. *** In conclusion, if the column is indexes then `First` is potentially faster than `Single` by the cost of a single `if` statement, a trivial different. So as long as you are consistent, either is acceptable. 
As I mentioned, I'd like to see an example using machine code (perhaps simplified for illustrative purposes). It's not clear to me where the *real* bottleneck is, and still seems a matter of OS/server architectural decisions that favor or assume certain switching &amp; waiting pattern/styles. I will agree that if you want efficiency/speed here and now, one has to work with what server co's provide at the moment. But, I still want to know if the justification is based on some universal law/tradeoff, or is a result of specific architectural decisions. Thanks. &amp;#x200B;
&gt; EF provides the repository pattern Not really. If gives you the tools to build a repository, but even doing basic CRUD operations requires far too much client-side code. https://www.infoq.com/articles/repository-implementation-strategies
It is very hard to go wrong using repository pattern for most line-of-business applications. Entity framework simplifies it considerably. I suggest putting all your business logic in services in a library project where they are readily available. &gt; and implement all logic in the controllers This is wrong for all but the very smallest of apps. Personally I would never do it. Slightly off topic but still very much related - Jeremy Likness sent out a tweet the other day [with a link to a great n-tier article](https://twitter.com/jeremylikness/status/1100826909032763394). Its good to see n-tier getting the press it deserves again. N-tier has faded from the spotlight in the last few years as microservices have proclaimed anything that is not a microservice as a "monolith". Good news - n-tier when implemented correctly is anything but monolithic and using repository pattern fits in nicely. See [AdaptiveClient](https://github.com/leaderanalytics/adaptiveclient) for some interesting twists on what you can do with n-tier, repository, and dependency injection.
&gt; it's that it certainly doesn't belong at the controller level Well it depends, OP mentioned Laravel so it's very likely that they were using Eloquent/the Active Record pattern. It's pretty normal in Laravel land to implement CRUD functionality in controllers directly, because its models are based on Active Record by default. Some pseudocode: index($id): return Post::paginate(20); show($id): return Post::find($id); create(): tap(new Post())-&gt;fill($request-&gt;all())-&gt;save();
Well I'm just gonna wrap them all in Task.runs and wait for the results You can't beat me
Yes totally agree with you on that .That's another way to do it in some scenarios more appropriate than just plain async/await. But that adds the responsibility of writing thread safe code and race conditions.
Gotcha. I usually use .Single just as an assertion more than anything because there have been times where the unique index was removed from tables. I don't know why but it happens when many hands touch the code, I guess
This is a great reminder to always look for the simple stuff first. Nice. Username relevant :)
That's interesting. I usually return IEnumerable and then if the caller is going to access the data more than once, I expect them to do something like `new List(enumeratedData)` so they can do whatever they want with it. Does that seem like bad practice? Do you lose the ability to "stream" the data if you use IList?
Yes you are right the above post is applicable in case of .net and .net core ecosystem .I shared a post recently It' not exactly the Machine Code but here is a [Post About State Machine](https://www.nerdlife.me/Understanding-How-Async-State-Machine-Works/) that tries to explain using compiler generated code for CSharp code and demonstrate how code execution flow changes into promises and callbacks instead of sequential code execution when we use the asynchronous programming in C#. I am not sure if it will help or is inline with what your are looking for. Please share with us if you find any better answers to your above questions.
IMO, the real world starter app does a really good job of toeing the line with just enough pattern usage without over engineering things: https://github.com/gothinkster/aspnetcore-realworld-example-app Also, if you are using EF, you already have a pretty good repository pattern. If you find queries getting to complex, turn them into specifications.
The reason for this is because we have too many morons in the industry. They thought that a repository needed to have getCars, getCarsByColor, getSoldCars, etc until the repository had 50 different methods and 10 private methods doing a bunch of bullshit to prevent 5 lines of duplicate code. EF is a pretty phenomenal implementation of the repository pattern for most cases. EF + specifications will cover almost every case. EF + specifications + a bit of Dapper will cover all of them.
Not sure if it is ‚Äòonion‚Äô architecture but I like using MediatR pipelines for anything small. 2-3 simple pipelines will handle everything and I can just inject IMediatr everywhere making it pretty easy to refactor if I need to add validation or auth stuff to the pipeline.
One reason could be a fixed width database field and them not trimming the returned value for the trailing spaces to pad out the value. Still a heap but an explainable heap 
It's actually difficult to overstate the benefits of following the architecture correctly from the beginning.
Sounds like it'd be a model, a service/business logic class, and a new migration. And maybe a view model and a mapping depending on how view models are being constructed
When talking with a former colleague about an 1100 line stored proc he'd written that would eat up the sql server resources and took quite some time to run I mentioned we should break the stored proc up and then using async/await functions to call the new smaller procs and that it would greatly reduce the amount of time and resources it took to load the page. He looked at me like I was stupid and said it couldn't work that way. I thought maybe I didn't know what I was talking about, I've never implemented async/await but I've read a lot about them and this seemed like the perfect time to try it out.
You can definitely unit test classes that use EF
You did your homework you learned about the benefits of asynchronous programming. That already shows you are prepared to bear the discomfort of change to improve the way things work. I totally agree with you on your suggestions. Now it's the time to really put the learning in practice. I understand there is always some resistance when you try to change how things works. Because change always cause discomfort .But that's the only way forward. If i work with someone who is hard to convince for a change. I will create some small working Proof of concept that is similar to application scenarios where i can demonstrate benefits of a particular approach. Then do some research and find some documentation where some field experts recommend the given approach. Then go back to your peers and invite them to be the critics of your approach if they can't find something seriously wrong with your approach most probably they will support you because most people will think you just don't have an idea you have a execution plan. Anybody can have idea but having a plan to execute that idea actually makes the difference.
*Eyy, another year! * It's your **4th Cakeday** webdev21! ^(hug)
That's a good argument. 
&gt; I expect them to do something like new List(enumeratedData) so they can do whatever they want with it. Why make them waste the CPU cycles of copying a list into another identical list? &gt; Do you lose the ability to "stream" the data if you use IList? In a way you gain it. If I know that all lists are going to be returned as an IList, then if I see an IEnumerable I can infer that it is streaming and treat it as such. If you always give me an IEnumerable then... well I don't know anything and have to dig into the source code. And once I know the real type, I'm going to do a cast to get it. Which means that I've taken a dependency on an implementation detail. *** Note that IList isn't really the right answer either. For performance, a List&lt;T&gt; is faster and generates less garbage than a IList&lt;T&gt;, even if that IList is really a list. For public APIs, the .NET Framework Design Guidelines recommend returning a strongly named class such as CustomerCollection. That way it can evolve over time, adding methods and changing implementation details as necessary. *** So in conclusion, the general rule is to be as explicit as possible for your return types. A related rule is to be as abstract as possible for parameters, accepting an IEnumerable or IReadOnyList when possible. 
I've seen so many bugs from slight variations in calling a DbSet all over the codebase... I've seen very few bugs by implementing a single version of a GetMeAllTheActiveWidgets().
Thats pretty mean.
That's a hell of a lot of layers for a 5 page application... How does that saying go? If all you have is a hammer everything looks like a nail.
There is no such thing as 'The Architecture'... There are many many ways to structure applications.
Nope, that's still the wrong way of doing it. You should then just create a WidgetStatusSpecification and then you can do a repo.Widgets.Find() and pass in your specifications with a [WidgetStatus.Active](https://WidgetStatus.Active) or WidgetStatus.Inactive. With a bit of work, you can even chain specifications with an and/or. Much more flexible and the only way to properly encapsulate your repository. 
I use something similar for smaller applications. Instead of services for the complex bits, I use commands to encapsulate a single use case.
Yeah no. KISS
You are kidding right? You know one architectural pattern?
Everything looks like a nail?
&gt; Funnily enough I mentioned aneamic classes further up. They're really not an anti pattern these days as we use streams, web apis, messaging, ES, etc. &amp;#x200B; Its an anti-pattern. Your anaemic domain models are not view models or data transfer objects... 
Yaya, every rule has exceptions and every problem is different. I know I know. But it's a damn good pattern.
So how would you go about deciding to return an IEnumerable vs a List of some sort? Do you ONLY return IEnumerable for unbounded data or what? Because technically you can convert anything else to a list, right?
I'm trying to learn like everyone else.
The fact you think MVC means interfaces etc. To separate code is a bit weird. If you implemented dependency injection etc then yes, but no one is forcing you to do that. Regardless of that for long term maintenance of a large project, interfaces are a must to ensure you can properly unit test. You clearly don't like MVC, but my coomments were directed at newer developers and it's anymore valuable skill for them to learn. I'm not really sure how 'send a request, get a result's is clunky but each to their own. As for core being a passing fad, it's clear that it isn't. It's replacing framework for new projects. Framework isn't going anywhere but it's development is slowing down (Microsoft's words,, not mine). Sorry to say but you seem a little bit stuck in the past! Web forms, framework over core, a misunderstanding of the benefits of MVC (thinking it means specialists, dependency injection and 7 layers etc.). That's fine, but right now the future is dotnet core and MVC for large scale business applications. At the end of the day it's very easy to switch between web forms and MVC as long as you're competent, so knowing both is only a good thing for the OP, a newer developer. Thanks for the conversation.
db.Widgets.Where(w =&gt; w.Status == WidgetStatus.Active) Easy enough.
A little bit beyond navigation: Gitpod: https://www.gitpod.io/, where you can "open" (clone) the repo in the cloud IDE and have VS Code experience in the browser. But currently only syntax coloring support for C#. 
A lot of people here mention that EF already implements Repository and it's a bad practice to wrap it around in another Repository. IMO, these people do not understand why Repository pattern exists and what happens when you use DbSets(or what's worse, DbContext directly). Repositories are meant to implement Interface defined by the Domain that needs stuff, in order to abstract away implementation of Data access from your business logic. If your repository belongs, to a 3rd party, you're making your code dependant on that 3rd party source code. The whole point is to NOT do that. To achieve that, you define an Interface that has all the methods your business logic requires, then you create a separate project/module/whatever, that contains classes that implement this Interface. This way, your Domain, is injected with an Interface, it doesn't know where the implementation comes from and where it takes the data from. In a simple explanation, your business logic has abstracted away any namespace from 3rd party library. Now, what happens when you inject DbSets directly to your business logic classes is, they become dependant on the namespace Microsoft.EntityFrameworkCore(example). First bad thing of this approach is, when you want to update your EF Nuget, you have to rebuild and count on your tests(if you have any ( Õ°¬∞ Õú ñ Õ°¬∞) ) to catch any new bugs/changes introduced there. Core of your product suddenly needs to be rebuilt and redeployed, just because you wanted to update your DAL. Second thing, you injected DbSet directly to your business logic. Let's say that in many places, you're querying a particular entity with particular joins(you copy pasted the same shit over and over since it worked). Now you decide that it's not performant enough and you wanna use plain Sql for that query, or Dapper, or XYZ. You now have to go through your whole codebase to change that. Again, you rebuild, redeploy and pray to god you didn't fuck anything up in your business core logic. We have one project that we decided to do the easy way and we injected DbContext directly to Command/Query Handlers. They said the project was quick and easy, one time thing. It's not. It's now nearly impossible to untie it from Entity Framework and most queries had to be rewritten by hand to plain Sql due to much more complicated DB structure than what we planned for, because of performance issues. Testing anything also is huge pain. To be honest, I still haven't found any decent alternatives to Repo+UoW patterns that actually work well. I'm open to opinions though.
Nice one! Thank you for sharing!
If your class injects the DbSet/DbContext directly and you have to use InMemory providers and what-not, then these are not Unit Tests.
I agree, but most repositories aren't quite that simple and it isn't long before you have 5-6 different where conditions. Using specifications would allow you to have a NewAndPopularWidgetsSpecification that may hide a fair bit of logic and re-use it without having to copy paste code. Both are better than having a getActiveWidgets method on a repository though.
So use EF, profile your app, refine your EF code where possible and add dapper where needed. I usually end up with 1 or 2 dapper queries for every 50 EF queries. 
You use InMemory for integration tests. You mock the context for unit testing. This seems like another classic case of a developer just throwing linq on a dbcontext and wondering why everything wasn't perfect. ORMs are leaky abstractions by nature. You have to spend a decent amount of time learning how to use them.
What the fuck kind of system lets people consume a repository directly? Users should be consuming your data through an API, not directly through a repository. Anyone consuming your IQueryables should have the code checked out and should know what ORM you are using anyway. ORMs are always going to be a leaky abstraction. At least going with EF it is a well documented leaky abstraction with plenty of learning materials. I can't say the same for any 'home grown' ORM I have seen at various companies.
Things would have to get pretty complex before I‚Äôd decide to justify doing that work. 
Total bullshit. Any abstraction done without enough knowledge of the various implementations is worthless. I have seen too many ivory tower abstractions that ended up worthless. I would much rather see copy paste code 2-3 times and then an abstraction in most cases.
&gt;100% the way to go for any non trivial software Most software is at least 75% trivial. Onion architecture tends to add boilerplate to the 75% while not really making the other 25% much easier IMO.
If you mock whole DbContext then don't you need you need to know what exactly to mock? If you have a AddAddress() method, and you mock context.Addresses.Add() it's all fine. Until your method's implementation also requires something like User, to check for permissions or whatever. Suddenly your tests will fail until you mock context.Users. Now it' testing against implementation.
Eh. You're really just shifting complexity around. Some domains lend themselves better to more anemic domains, others not so much. I've dealt with rich domain models that were a pain in the ass and anemic domain models with services holding most of the domain logic that were fairly pleasant to work on. People have built great software with and without DDD. 
Layered architecture is fine as long as you are flexible with your layering. Vertical slice things and apply the correct number of layers. Most places are only going to need 1 or 2 layers. 
True. But I'm not sure this applies to approaches such as ES.
Yep. Although to be fair I'm sure we were all morons at some point early on :) I like the EF + dapper approach. I think I stole the idea from how StackOverflow do it, I can't remember now :)
Not when it's unnecessary. Building a working app is more important than all these extra layers. Too many devs focus on the wrong things. Adding more layers is the easiest thing to do later, *when you actually need it.*
Suspicious? Why? Sounds like they don't understand what async is and how it works.
Why would async complicate debugging? There is no difference in the debugging or exception handling capabilities unless you're doing very advanced task/thread manipulation.
Again, no. That‚Äôs how you get spaghetti code. Build a structure early, and utilize it. Having a few more files is worth the organization, and will ensure you don‚Äôt have a mess later on. 
I think when i said it really complicates the debugging it was a bit of exaggeration but it does pose few challenges because multiple tasks are running in parallel and their callback are getting triggered in unpredictable order and your debugger will be going all around in your application code and your watch values will be changing as the thread execution context switch happens . This [post about debugging ](https://blog.oz-code.com/debugging-multi-threaded-code-using-visual-studio-ozcode/) in visual Studio highlight few of the challenges of debugging asynchronous code. But I agree with you for few people this complexity is easily manageable and they doesn't even feel any difference in debugging experiences.
Yeah, it's a common misconception. If you think about what something like Unity is doing, it generates an object graph. So IService4 is injected into IService3 which is injected into IService2 which is injected into IService1. Contrived example. What DI allows you to do is swap out the implementation of any of those services, but regardless of the implementation the implementation of Service1 is dependent on implementations of Services 2,3 and 4. So you're not dependant on a specific implementation BUT you are dependent on having implementations of the entire hierarchy. Now imagine that Service1 is directly injected with Service2,3 and 4. To mock out a Service1 you'd have to mock 2,3 and 4. When you test that mocked scenario you have to setup the methods on each of the injected services to trigger the specific method on Service1. So you're tightly coupled to there being an implementation that correctly according to its contract. One alternative approach is to have Service1 respond to messages instead, you only inject the message transport abstraction. Service1 can now respond to messages that can come from Service3 or Service5 in the same way it previously could only respond to the injected Service3. It works the other way round too of course. Usually if you're injecting Service3 into Service1, you want Service1 to call a method on Service3. If you now have Service1 send a message you can have a Service3 respond to it or a Service5. Message here is more akin to the UI style messaging rather than enterprise bus style messaging. It encourages black box testing where you never spy on the internals and discourages white box testing. The negatives of this approach: It can make the design more opaque and difficult to visualise. YMMV, it takes a bit of thinking about, but essentially that's why DI is not a decoupling mechanism.
It is pretty cool, there are a lot of caveats though. The biggest is that writing tests is as hard, if not harder than writing the code it tests. Agile coaches never say this, tending towards really simple tests that show how "simple" TDD can be. There's a thing called "Test induced design damage" where your design becomes polluted and more convoluted because you've bent it to fit the tests. :)
* **.NET** = entire Microsoft development environment including languages, runtimes, compilers, etc. * **C# and** [**VisualBasic.net**](https://VisualBasic.net) = 2 main languages officially from Microsoft that compile down to the common intermediate language (CIL). * **.NET Framework** = older windows-only runtime that takes the CIL code and compiles it into native code to run your program. * **.NET Core** = newer cross-platform (windows, linux, mac) runtime. * **ASP.NET** = framework for building websites and web applications, runs on .NET Framework only * **ASP.NET** **Core** = newer framework, will eventually only run on .NET Core * **ASP.NET** **MVC** = a feature of the web framework that implemented the model-view-controller pattern * **ASP.NET** **WebAPI** = a feature of the web framework that let you build HTTP/REST APIs easily * Stick with .NET Core since that's the modern framework and .NET Framework will eventually be retired. * MVC and WebAPI are now merged into just [ASP.NET](https://ASP.NET) Core and you can easily return frontend views (HTML) or data (like JSON/XML) using the same controllers and even mix and match easily. * .NET provides a giant standard library and the web framework gives you a webserver (called Kestrel) and many other features like auth, mvc, model binding, output formatting, reponse compressions, middleware, filters, routing, and much more while being extremely fast. It's very different from the node.js world which has no real standard library and requires lots of 3rd party packages. When you're learning, stick to MS libraries first. * The Microsoft documentation is rather good now, read through the guides for the concepts first. A few hours will get you up to speed very quickly: [https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-2.2](https://docs.microsoft.com/en-us/aspnet/core/?view=aspnetcore-2.2)
Absolutely agree. It's always difficult to respond to threads like this because you either have to elide a butt load of detail or basically write a complete thesis on application design. It's great that you get people responding conversationally to add and direct the discussion :)
Not really. Setting up a good structure from the start allows for much easier expansion.,
Hey man instead of being incredibly condescending why don‚Äôt you try helping someone out?
But connecting to the db from a controller is not one of them. 
If you have very small project I can understand this question. But when you deal with giant projects LAYERS give you a chance to keep this code clean and help avoid mess.
Might be worth mentioning that devs should be careful with the Specification pattern. Be sure the specification ends up being part of the executing sql. Use expressions and not a delegate, otherwise you'll be running the specification after that all rows have been returned from the db. . 
What are you using as a data access layer? &amp;#x200B; Bear in mind, Entity Framework is literally an implementation of the repository pattern. &amp;#x200B; If you're using EF, you don't need to wrap that repository pattern in another repository pattern.
Are you sure that company is not null? Debug that in the controller. If its null maybe lets take a look about eager and lazy loading if you are using EF. [https://docs.microsoft.com/en-us/ef/core/querying/related-data](https://docs.microsoft.com/en-us/ef/core/querying/related-data)
Not everything needs to be made expandable when you can be very confident it will never be expanded. The industry would save so many hours if we stopped religiously sticking to design patterns and architectures where they're not needed If something took me an hour to finish then it's not going to take much longer in the future to restructure it, and if it's more likely than not that it will never be touched again why bother 
During your query use .Include or Select to load the data, it is probably null since lazy loading is off by default in .NET core
are did you structure your relationship? you need to learn about lazyloading vs eager loading. your company is null because you are not using lazyloading and you are not specifically including the company property.
This probably is a good article. I'm yet to read it. But why TF does every other website require JavaScript to be usable at all? Just leave the basic stuff readable for people who use NoScript and the such. It's just text. HTML is purpose-built to display text. You're welcome to add fancy JavaScript doodads and whatchamacallits to your site. But at least make the plain-text version accessible to people who don't (won't) run JS on every random website.
Why would you go with .net core over .net standard for a class library? Obviously.net standard can‚Äôt produce executables but the way I‚Äôve set it up is all my main code is in .net standard libs and the main executable is .net core or .net framework
Thanks. &gt; .NET provides a giant standard library and the web framework gives you a webserver (called Kestrel) and many other features like auth, mvc, model binding, output formatting, reponse compressions, middleware, filters, routing, and much more while being extremely fast. It's very different from the node.js world which has no real standard library and requires lots of 3rd party packages. When you're learning, stick to MS libraries first. This is definitely new to me, so thank you for explaining that. Node has a pretty bad standard library and like you said required many third party libraries to get started.
And the bindings for arm-linux-gnueabihf? Out of the box it just support x64. I think that you should give support for that in nuget. This way I could use your lib for a big project that will run on most buses in spain
Repository pattern is a unit of work to encapsulate all your database logic and model that should be exposed with interfaces, if you consider clean code approaches such as solid principles yes then repository pattern is a best practice! this way you can decouple the repository implementation from the rest of your application... today it can be using sql server tomorow mongodb or simply use case is to mock the dependencies to external infrastruture such as databases. 
Whoops, that's an error on my part. I meant to say "Framework", not "Standard". I've fixed it now, thanks.
A more important point in my opinion, consistency of code. Having a service for some logic and not for other logic is less readable and understandable for someone new into the project. 
It is most likely that you are not eager loading related data. But typically speaking passing domain models straight from EF to views is not recommended. Also try getting just the minimum data you need. I would transfer the ef model to DTO and then pass it around. Depending on how how you are using context, it is best to call toList() before the context expires
YANGI
YANGI
Wrapping the dbcontext in a generic class leads is an unfortunate mistake which leads to a restrictive view of what the db context can do and generally encourages a missunderstanding of how the dbcontext works... causing newbies to fubar stuff all over the place. In summary its a clusterfuck.
Sure, but this question wasn't EF specific.
Formatting and standards are necessary, you‚Äôre just simply wrong if you think otherwise. Saying you‚Äôll fix the structure if you need to later is how you end up with a mess of spaghetti code that is a nightmare to work with. Because in reality, you never do end up fixing it. You‚Äôll tell yourself that each change is just a small fix and you‚Äôll structure later. 
Formatting and standards are necessary, you‚Äôre just simply wrong if you think otherwise. Saying you‚Äôll fix the structure if you need to later is how you end up with a mess of spaghetti code that is a nightmare to work with. Because in reality, you never do end up fixing it. You‚Äôll tell yourself that each change is just a small fix and you‚Äôll structure later. 
You could say that about any good coding practice and you would sound just as stupid as you do now.
That's not a way to do it. You only wrap cpu bound functions in task.run. 
The sole reason for onion architecture is for testing. Integration tests are a pita to set up in a build system for automated deployment. If you can write only unit tests that don't need an outside db or API then you have the best way to test. Even if for a lot of your methods you are just handing around data, it does not matter - accept the overhead. You will thank yourself later when it comes to some shitty bug that could have been avoided by having good tests that can mock your repositories. The controller code should just be calling the service layer with what it wants to do. Put all your business logic in the service layer and take dependencies on simple CRUD repositories.
It‚Äôs a just a question. I am curious. I‚Äôm not judging. 
No it is not.
I just wanted an actual count because I‚Äôm genuinely interested.
Low level laguague is assembly
Only
C++ is actually a high level language. 
If you want to spend a day on an hour's job for something that's unlikely to be touched again, be my guest
assembly is an easy-mode high level language. the only way to truly write low level code is to write binary.
Microsoft forms maybe
Binary is the machine language
its a joke
I think that's a bit too light-weight. I'm going to need to be able to do some coding.
It really depends on what the SP is doing under the hood and what your goals are. It's quite likely that your version would actually be less efficient, but better at the same time. How can that be? Hypothetically let's say that this SP queries 10 different tables and aggregates the data in some way. You replace the SP with 10 individual queries by the app server, followed by an aggregate on the app server. You've now got 10 round trips to the database happening (even if they are done in parallel) and probably have substantially more data being sent across the wire leading to more memory usage on the app server. How is this potentially better? App servers are cheap(er than database servers), and it's relatively easy to spin up new app servers and throw them at the problem, whereas sql servers don't scale so easily. 
[Real programmers use butterflies.](https://xkcd.com/378/)
Who changes their technology in the middle of everything? Never have we switched from dunno SQL Server to Oracle. I don't get it, why even start with one and then switch to the other?
In this situation the SP was just one massive set of If statements and the .aspx page called it multiple times throughout the page load cycle, each one passing a date variable and another variable to trigger a certain If statement. Every time the SP was called it would pass that same date, do a calculation in the SP on that date to get the prior months data then filter through the If logic. The whole thing was a logical mess. Even without async/await I could have rewritten it to be more efficient.
The only high level language is User Stories. Fortunately, I am writing a compiler to turn User Stories directly into machine code. Support my kickstarter!
Author does make the point that low-level is subjective, so in my experience, I would consider low-level interacting with hardware instrinsically. This post is mostly calculations, which many high level and scripting languages can do. Shit I'd argue JavaScript could do what's done here with enough hair to pull.
In NOT-academyc contexts, it's not that rare to see certain technology switches. Especially when the company is driven by non tech people. The main reason I've always heard was "it's cheaper", or worse, "my geeky friend suggested this and he's a big shot in tech". And it's not only a matter SQL server or oracle. It can be from relational to non relational (for any reason). I'd keep on talking for ages, but believe me, crazy changes happen more often than you'd think :)
I needed to do something like this for a client recently. They had originally developed a form in Google Forms, but needed a way to accept payments. I re-designed the form using [Google Apps Script](https://developers.google.com/apps-script/) instead along with using [clasp](https://github.com/google/clasp) to gain access to some newer features and to develop locally.
The language itself is nothing more than a bunch of keywords and rules that form a syntax. What makes a language high-level or low-level is frameworks and libraries that are available for it. For example on Windows there is WinAPI (Win32) for C/C++ and there is .NET Framework for C#, C++, VB and F#. There are libraries to directly access disk (clean it, format it, etc) in WinAPI but not in the .NET Framework. It is possible to make .NET wrapper around WinAPI library. Actually the .NET Framework itself is a wrapper around the core WinAPI with added layers (like GC for example). So the question should be are there low-level libraries available for C# to directly access CPU, GPU, Disk, etc? C# wan't design for this purpose, C/C++ was already there. Although you might find 3rd party libraries out there for these things. C# is for business. In business you want the Framework to do the hard stuff for you (memory management, multi-threading, networking overhead, etc) and buy time as much as possible. The lowest-level thing I've done with C# was to program my Raspberry Pi to communicate with some sensors. But even there most of the difficulties are hidden for the developer, you just call some methods and magic happens behind the scenes.
Writing 1s and 0s is high level bullshit. You're only a real programmer if you build transistors from scratch and put them together to program an app.
Building physical hardware is high-level bullshit. You're only a real programmer if you use coding and algorithms so the drones don't crash into each other.
http://joeduffyblog.com/2013/12/27/csharp-for-systems-programming/
OMG do you know anyone who can do coding and algorithms?!?!?!?! They must be gods! Can they also do hacking? I want to get into hacking and I want to learn how.
Nice try, FBI
In my opinion, C# is somewhere in the middle. A higher level language might abstract more things at the cost of performance. For example, a language like PHP doesn't make you pick any variable types. A lower level language might make you code some stuff that is a bit boring, but gives you a performance boost when manually tuned. For example, C++ doesn't have garbage collection, so you have to use the new/delete keywords and do manual memory management. Pick your language based on what your needs are. Do you need super optimization or do you need to interact directly with hardware? Take a look at super low level languages like assembly, C, etc. Do you want to throw together a quick project and want it running quick? Maybe code it in Python.
Can you give an example of what you need?
This Matt Warren guy always posts about low level C#/.NET. Love it! It's a niche for sure.
I weave software using core rope memory.
**Frontend:** a pretty simple registration form for an event. While it's simple it requires me to be able to do an API call and use the results for an autocomplete, so all form builders I've found are out of the question. The API in question already exists. **Backend:** just somewhere to post the form to (preferably written in C#) where I can do some validation and some API calls before returning the result to the front-end. These APIs already exist. Like I wrote above I can do all this with a stand-alone MVC project or similar, but that seems like overkill since I'm essentially only going to need two files and a place to run 
You could do this pretty quickly with Microsoft PowerApps, but your client needs Office 365. 
I‚Äôm concerned you actually don‚Äôt know what spaghetti code is, and maybe that‚Äôs why you‚Äôre getting so defensive here. Can you describe what it is for me?
&gt; Author does make the point that low-level is subjective Somewhere several pages down. So the author indirectly admits to have chosen a clickbait title. I'd have expected better of Matt Warren.
I'm a big proponent of an MVC architecture that allows for "smart models" where you extend the base model with either extension methods or a partial class that performs model-specific functionality. Then in the controller you simply do the CRUD on the model, or call the model-specific functionality as necessary.
If you need a lightweight place to make a call, and you're already happy hosting in Azure, would an Azure Function work?
Our startup has come to the conclusion that users are an anti-pattern and decided to abstract them away using machine learning. There's probably room for a blockchain in there somewhere as well, watch out for our ICO.
Specifications?
Ya, I can see that side of it. I guess I have been working at large enterprise businesses doing large scale software projects for a while. 5 years of software that scales to tens of thousands of users an hour. It's not reddit but our projects are big. So my opinion might have been a bit jaded.
That‚Äôs thing, you‚Äôre really blowing this out of proportion. It might take you an extra 20 minutes, max. 
Something like Assembly would be low level - c# has to be compiled into machine level code, so I wouldn‚Äôt consider that a low level language.
Does it do it's own dynamic memory management and garbage cleanup? Then, no, it's not a "low-level" language. 
Code that is unstructured and hard to maintain, like the code you‚Äôre suggesting. By your logic everything should be put in one large function because you can just refactor later and caring about code quality is YAGNI, somehow. 
With machine learning!
And a blockchain database!
Okay, it seems you‚Äôre just unfamiliar with YAGNI. YAGNI says that we want to create the minimal amount of *quality* code to achieve our current programming goal. It forces you to ask yourself, ‚Äúdo I *really* need this right now?‚Äù So, if I have a REST API that has no logic besides CRUD operations and I‚Äôm getting ready to implement it with YAGNI in mind, I would ask myself: ‚Äúwhat does adding a service in between the controller and repository give me, and what does it cost?‚Äù If there is no logic unique to the service, there is no point in creating it. It will cost time and an increased cognitive load on any developer working on it as it‚Äôs an extra layer of abstraction. Abstraction makes sense when it *reduces* cognitive load and development time, i.e. what you were saying with having lots of code in the controller. If I need to do some mapping, then maybe it makes sense. If there is unique logic in the service, then it does make sense. Look at how projects are usually structured in C# when using an ORM. They usually just have a controller-repository setup as the ORM can handle, inside the controller, any simple queries in a concise manner. I could show you code if it would help you. Does that make sense?
Well, remember that you are only supposed to hold a shared resource (e.g. a DBConnection or DBContext) open for as little time as possible. So unless you need streaming (e.g. processing a million records), you should be tossing it into a list and release the database connection to be used by another request. *** One of my biggest annoyances is people who use DI to inject an EF DBContext. If you care about performance and scalability at all this is a horrible thing to do. What's really stupid is that Microsoft even says not to do it... in the same tutorial where they are showing it being done.
&gt; Now imagine that Service1 is directly injected with Service2,3 and 4. To mock out a Service1 you'd have to mock 2,3 and 4. Then don't. Test Service1 with its real dependencies. There are lots of good arguments for using DI. Writing shitty mock tests isn't one of them.
&gt; What the fuck kind of system lets people consume a repository directly? * O-Data * GraphQL * Any kind of report builder 
&gt; ‚Äã &gt; &gt; Bear in mind, Entity Framework is literally an implementation of the repository pattern. Bullshit. Entity Framework should be an implementation of the repository pattern, but its not even close. If it was a repository I could write `dbContext.Employees.Delete(27)` instead of public void Delete(int employeeKey) { using (var context = new CodeFirstModels()) { context.Database.ExecuteSqlCommand("DELETE FROM HR.Employee WHERE EmployeeKey = @p0", employeeKey); } } Or if you don't care about performance, public void Delete(int employeeKey) { using (var context = new CodeFirstModels()) { var employee = context.Employees.Where(e =&gt; e.EmployeeKey == employeeKey).First(); context.Employees.Remove(employee); context.SaveChanges(); } }
No.
&gt; A lot of people here mention that EF already implements Repository and it's a bad practice to wrap it around in another Repository. No it doesn't; that's a major misconception because it *should* have been a repository. Consider the simple example of updating a record: public virtual void Update(EmployeeClassification classification) { using (var context = new OrmCookbook()) { var temp = context.EmployeeClassifications.Find(classification.EmployeeClassificationKey); temp.EmployeeClassificationName = classification.EmployeeClassificationName; //REPEAT FOR ALL OTHER COLUMNS context.SaveChanges(); } } https://grauenwolf.github.io/DotNet-ORM-Cookbook/SingleModelRepositories.htm If you asked me to build you a repository, and I gave you one that required you to write all of that you'd be rightfully pissed. But because MS gave it to you, you give them a free pass. 
&gt; Well I'm just gonna wrap them all in Task.runs and wait for the results [Haaah](https://www.youtube.com/watch?v=lhckuhUxcgA)
C# is high level C/C++ is low/mid level ASM is low level At least that‚Äôs what it was considered when I started programming as a hobby back in 96 (VB3 - extremely high level). Then C# came out, it was always considered high level. 
I am not sure about this aspect. I'll keep digging.
An Azure Function for the backend is a good idea! I'd finally get to dip my feet into that too. What about the frontend? Or are you saying that I could somehow host that in Functions too?
In one of our projects we are using Dapper (so none of this DbContext stuff). Here is a line of code (inside a using block): `return await conn.Connection.QueryAsync&lt;Board&gt;(sql, p).ConfigureAwait(false);` &amp;#x200B; Now, the QueryAsync is returning a `Task&lt;IEnumerable&lt;Board&gt;&gt;` which is also what the method returns. I think what you're saying is that if I were to place a breakpoint inside a forloop in the calling method, this sql connection would stay open and the sql server would potentially sit there holding its results (until it times out probably). So the "bad" thing here is that it's rougher on sql resources but the good thing is memory allocation on the web server is probably less because it doesn't enumerate into a full list and instead you can look at 1 row at a time as it comes back. Am I understanding this correctly? &amp;#x200B; I feel like I'm in total agreement with your statements that it should be turned into a list right away (inside the using statement) but if performance is important, the idea behind it might start to break down and this is where I start to question things a bit more. &amp;#x200B; (The irony of returning a `Task&lt;IEnumerable&gt;` is not lost on me and with C# 8 I would probably convert these to use AsyncEnumerables instead if I were to keep this "pattern")
EF core sort of supports stored procedures. http://www.entityframeworktutorial.net/efcore/working-with-stored-procedure-in-ef-core.aspx If the database is locked down you're pretty much going to have to use stored procedures. Have you looked into dapper? Entity Framework is built around direct access to tables and is clunky with stored procedures. https://dapper-tutorial.net/stored-procedure 
Reading your comment I honestly don't know whether you're advocating against wrapping EF in repositories or not. My point was, that a lot of people here think it's useless sinc3 DbSets already are kind of a Repository. I tried to reason with that logic, as IMO wrapping any DAL in an abstraction is necessary in order to maintain a good quality code, that's open to changes. Not sure what was the point of your example in this case.
wow, having a bad day? it sounds like you just don‚Äôt like EF
&gt; So the "bad" thing here is that it's rougher on sql resources but the good thing is memory allocation on the web server is probably less because it doesn't enumerate into a full list and instead you can look at 1 row at a time as it comes back. Am I understanding this correctly? Exactly. &gt; ‚Äã(The irony of returning a Task&lt;IEnumerable&gt; is not lost on me and with C# 8 I would probably convert these to use AsyncEnumerables instead if I were to keep this "pattern") Well to be fair, the database is often slow at returning the first record but can continuously stream the rest. So `Task&lt;IEnumerable&gt;` can still be useful.
I strongly encourage abstracting the EF DBContext because it leaks so many implementation details. Whether you call that wrapper a "repository" or not isn't really any of my business.
Oh I always hated EF, but EF Core 2.x is especially bad. 
That was exactly my point? I don't know, maybe my first sentence wasn't worder properly then.
did this for many years, but using Transaction Script pattern like this leads to huuuuge services with massive amounts of methods. DDD is really complex to maintain, not sure thats a good next step?
So every single programming language can be considered low-level as long as there are libraries that can access and control hardware? That doesn't seem right to me
If you're just hosting a static HTML form, then you can host in in Table Storage with a CDN in front (which is still probably cheaper than a Web App) https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-static-website
&gt;Regardless of that for long term maintenance of a large project, interfaces are a must to ensure you can properly unit test. Many shops don't unit test. They should, but they don't. I can't change their policy, so one must optimize their own efforts around management's standing orders. But unit testing is indeed a factor to consider. &gt;You clearly don't like MVC You are putting words in my mouth. I already described where it does well and where it doesn't. It's a matter of the **right tool for the job**. &gt;Sorry to say but you seem a little bit stuck in the past! People like you ruin perfectly good tools by making everyone afraid of not keeping up with the Joneses/Kardashians. Let's stop the pump-and-dump mindset pushed by the big players so they can keep selling shiny new crap. &gt;\[MVC is a\] valuable skill for them to learn I don't dispute that, but one shouldn't rush into it under unrealistic deadlines like the OP appears to face. It has a big learning curve for most. &gt;At the end of the day it's very easy to switch between web forms and MVC as long as you're competent, so knowing both is only a good thing for the OP. It appears to me the OP is expected to get up to speed quickly on his/her own. That's too much to ask for MVC. If ramp-up speed is important, WF is usually the better option. The OP can learn MVC gradually between deadlines. Since you agreed that "knowing both this a good thing", these particular circumstances seem to then faver learning WF first. Let's review the logic: A. Knowing both is good (your words) B. Current circumstances favor Web Forms first because ramp-time has been given priority by OP's management. Therefore, OP should probably learn Web Forms first in *this* case. Where is my logic wrong, or do you disagree with the givens? I wish to give the OP a best recommendation based on sound and vetted logic. &amp;#x200B;
C and C++ are high level languages, they are abstracted from CPU commands (which are platform specific) and Assembly commands (which may not be platform specific). Just because you do not have a GC and need to allocate and free memory by yourself does not necessarily make a language low-level. https://en.m.wikipedia.org/wiki/Low-level_programming_language
Bah. Each program is a number. The lowest-level programmer simply plucks the right number out of the ether and runs it.
Too bad there is no benchmark after he he implemented those Twitter suggestions ..
No
I do always find it entertaining when an ignorant person who is clearly wrong tries to be condescending. Thanks for the amusement :)
I was clarifying my position because you wrote "honestly don't know whether you're advocating against wrapping EF in repositories or not"
This shows how the "waiting engine" works, but I don't see the "resource accounting" in a numerical sense, such as say comparing technique A to technique B, and seeing that the CPU has to execute "Section X" 25 times versus just 7 times for technique B under the scenario's load. Or, that A has to use 10k to cache a given set of requests while B only has to cache 4k. Concrete numbers and how they were derived is usually the clearest way to illustrate such.
Low level language is binary. /s That's what they used to bootstrap the whole computing revolution. Apparently there were a lot of people that could read binary like assembly. Wow.
Yes totally with you. I said this below, it's hard to answer a post here without writing a thesis on software design. Don't even start me on mocks. Mocks are an anti-pattern, but I'm classicist rather than mockist. But take my example in the context it was intended. I'm explaining that DI is not a way to decouple code. Using interfaces is interface segregation part of solid which is simply substitution and decouples the implementation from the interface rather than the components of the system. DI doesn't even require that you use an interface let alone constitute decoupling. You can inject a concrete class and have it new up all the concrete classes in the hierarchy. 
&gt; Most significant was that my Entities don't neatly correspond to tables. This seems strange... Your DbContext is an in-memory representation of your database. You need classes that correspond to your tables so you can read/write to your in-memory db thus your on-disk db. Now those don't have to be the ONLY classes in your app and in fact for most apps they are definitely not. You will want to create some presentation classes that give different/denormalized views of your data as necessary.
Full C# code [here](https://gist.github.com/mattwarren/d17a0c356bd6fdb9f596bee6b9a5e63c)
thanks for the response. I briefly looked at Dapper and found that it's a basic ORM. So it doesn't do change tracking. I found a Dapper extension that is able to do it, but I'm not sure if putting that all together will give me what I need \[including eventual separation from the db as a back end\]. I guess I should spend the time to try it out.
the issue is access control. I can't allow users direct access to the tables. This is why I had to create views that incorporate basic access control logic. In terms of columns the views and underlying tables are identical. But they can't be manipulated with exactly the same CRUD statements.
Specifications are dead simple. It‚Äôs not like they are more code or less readable. Different strokes I guess.
DDD is tough. Yes services grow large. Never any time to refactor. But what else as at least that works quite well for a long time and at least those layers can be easily dockerised and distributed. Until your SQL box melts or license fees kills you. Mongo as SaaS not cheap either. But DDD is easier than micro service mega disaster üòÖ
From a practical point of view, if you can directly access hardware then why not? After all, they all become machine code at some point in the execution.
Specification pattern. Basically your repository should have a .Find(...) or similar and then you can build specifications that you pass in to silver the data.
I looked into it a while ago but I didn't see any reference for what I want to do. Wouldn't it be really cool if all your client browsers were connected by default? Free chats and notifications and other collaboration tooling. Since SignalR is the workhorse of Razor Components, it's really weird that existing connection is not exposed somehow...
My guess would be user input error. Most of the implementations I have seen use some kind of serialization technique that transforms instances of objects to xml or json. When such a technique is used then the strange space in the property name will not occur. So probably they use some kind of string concatenation to compose a json or xml document and as source for the strings a database query. Things can get messy if the data in the database is inputted by hand where the user that inputs it types the property name and value. Note: I have seen far too many errors where user unit was not trimmed and the user copy pasted values from an email in Outlook or word. These applications tend to copy the space after the last word of the selected part of the text. 
The contention is because of the EF abuse of it.. :)
Shit mod.
I‚Äôm not being condescending. I‚Äôm still pretty sure that we mostly agree, you‚Äôre just misunderstanding what I‚Äôm saying, along with what YAGNI is. I would love to help you but I don‚Äôt think you‚Äôre interested.
The Microsoft provided ORM is called EntityFramework, and now the newer EntityFramework Core version. It's one of the best in the industry and lets you connect to many different databases with much more advanced functionality than just getting and saving a few records. Read more about EF Core here: [https://docs.microsoft.com/en-us/ef/core/](https://docs.microsoft.com/en-us/ef/core/)
Breaking up complex procedures can work but I'd look at the execution plan first and see where the bottleneck is. 
You could do it too if you spent enough time practicing it, and learning how to look at it, assembly is a 1:1 translation to machine language. Hex is useful. add %a, %b // a &amp; b are registers, % takes the values Just using single bytes for simplicity's sake, say the opcode for add is 0x01 and `a` and `b` are storing values of `0xA8` and `0xF2`, you get 0x01 0xA8 0xF2 ^opcode ^value in reg. a ^value in reg. b which in binary looks like 0000 0001 | 1010 1000 | 1111 0010 that's split up for clarity, broken down to the literal instruction would be 000000011010100011110010 That's not x86 assembly or anything, it's just made up for the purpose of explanation, but you get the picture. 
Great! Thanks for the info man, really appreciate it.
I love the idea of throw away containers using a fluent API like this. But it means we have to build a Docker image we want to use with this correct ? Is it possible to create a ‚Äúcluster‚Äù ? Eg. A DB , our app, and some other so they can inter communicate? 
I mean, it really doesn't sound like the OP is given a choice. This question wasn't should I use web forms or MVC, it's 'how do I do MVC'. I'm sorry but if you class asp.net MVC a new tool and 'keeping up with the joneses' then you are seriously lagging behind. MVC is not the new player on the block, it's the player that the industry and Microsoft have chosen as the current enterprise solution. If you think pushing people to use the industry standard instead of a tool that isn't even being actively developed by the company that own it is 'ruining perfectly good tools' then you're unfortunately fighting a losing battle, as it's already dead. You seem to have some rather large misconceptions about MVC, the current state of the enterprise software development world (excluding maintaining legacy products) and the future of .net. Don't learn an outdated tool unless you have to. Always learn the current industry standard because you need to.
definitely not microservices next.. but splitting layers into containers sounds a bit weird. i prefer one class/file per operation, cqs style
Ahh. Yes CQS. I tried that with Event Sourcing as. 1 year migration POC for a legacy system rewrite. Everybody was lost , BA , QA , Leads, Directors where like WTF during the whole thing. It was beautiful but nobody wanted to adjust/embrace it. And when I started talking about Actor Pattern as an Enterprise solution people suffered mild strokes. It‚Äôs a shame. But yea. Probably starting with CQS is a good long term solution especially for Actor models to ease growing pains. 
Does anyone know if the two bullets above are still issues?
Maybe it's just me, but it sounds like you have a problem with how you provide access. It *sounds* like they're AD accounts have been granted proc execution and you're passing through authentication. And again, this might just be me, but that doesn't sound like a great idea. Imagine if reddit or literally any other site, operated like that. It would be a nightmare.
Yes you are right thanks for correcting me. 
It's too bad this article has such a clickbait title, because it's actually a great read, but I can already see many people are simply commenting on the title.
&gt;if you class asp.net MVC a new tool I didn't. You appear to have misconstrued what I wrote. &gt;op knows web forms already, that's the issue, they've been told to use MVC for new stuff, correctly so. It's not the correct tool if management wants results pretty soon from somebody new to MVC. The OP posted because they *are* overwhelmed. That's common with MVC learning, and thus doesn't surprise me one bit. &gt; Don't learn an outdated tool unless you have to. If you are put on a tight deadline and Tool A has a quicker learning curve than Tool B, then you "have to" in order to fit the ASAP requirements imposed on you. &amp;#x200B; &amp;#x200B; &amp;#x200B;
The perception of what is "high level" is constantly moving over time. Back in era of first UNIX-es, C was considered as a very high level language and people were discussing if it's worth it or should they stick to direct assembly language programming. 
Currently, the library can only configure and run existing Docker images, locally as well as from Docker Hub. However, this behavior is easy to extend and I will do in the next releases. The library is very abstract and does not have to be used only for testing. Your use case sounds a bit like docker-compose? If you describe it in more detail, I can give you more information. Maybe we can work out another use case?
For now.
Scary thought lol
Front end and back end are not synced automatically. You either have to use a Post to the server or use an ajax call to the server. This does not happen automatically. Since you are using jquery you should read http://api.jquery.com/jquery.ajax/ And create a call function and use it when you need it, but the server side needs to be able to process that call so you need an open endPoint that accepts a Post. And creating that for just a single change on the front end seems overkill unless it's really important. You could use a framework that helps with this, but as a back end dev, i haven't used them, so i can't help you with that. 
Wow! I did not expect all of these responses. This kinda blew up.
That worked like a charm! Thanks for the suggestion!
"Binary" is not a language. It's just the alphabet.
When I was writing that I thought the same. Docker compose / kubernetes. Orchestration almost. But you know what. We C# guys love fluent API don‚Äôt we. I know you can put all the things in one Docker image. But I can see a use case for automation testing where maybe you would throw away the database several times while the core docker lives on. You could hook that up to Test framework. Maybe even spec flow ? That would be mind blowing. 
[removed]
Tortuga Chain was created for your use cases. It requires less code than EF or Dapper for CRUD operations; works equally well with classes, dictionaries, or DataTables; and has first class support for stored procs, views, and table-valued functions. It doesn't support deep object graphs or LINQ. For non- trivial queries you are expected to use views or stored procs.
Its just no. It compiles to an intermediate language that is then processed by DOTNET runtimes. 
event sourcing is waaaay overrated imo. cqs without es!
I assume this is web forms. Web form code has as a page state that tracks the state of the page. The long and short is that this gets passed between the server and user. When the change was made with JavaScript, the VB.net portion of the code was unaware of it and the server never knew about the change. Since this is a VB.net code base and is likely legacy - remove the JavaScript that is updating that value and migrate it to an appropriate event in VB.net.
We tried. The good: it looks like a Win10 app. The bad: for advanced users, it's slower than the Win32 app they were used to. At the time, there was no MS SQL library (they were trying to move folks to REST APIs). There's no way to print without a confirmation dialog, and adding a wrapper around the Win32 print library for our label printer made our app ineligible to be distributed via the Windows Store for Business. I'd strongly suggest WPF over UWP.
My application is internal for five users and is very different from Reddit. Users currently have direct access to the database. And access control is done in the database \[MySQL\]. This is why there need to be views and stored procs.
Haven't heard about it before. Will read up. Thanks.
Weird, the one corporate UWP app that I have created was also a label printer.
I hate to say it but on BIG applications it can actually be a good idea. It stops any accidental saving of tracked entities and provides a concrete separation between the datastore and the rest of the application. That said with the ease of use with the in memory database, I struggle to go to that level of architecture. &amp;#x200B; Based on you saying there are thousands of crud methods, I suspect that the level of granularity is too high. e.g. A simple example is a online store, crud operations on a Customer should include their address details, there shouldn't be one call for saving the user and another for saving their addresses. This will simplfy the access to the datastore. While this idea appears less performant, well, chase performance when you need to. &amp;#x200B; The 300 line long LINQ queries and stored procedures (especially on stuff like reports) is another problem.
Was the option to side load not available at that time? 
What do you consider to be a high-level language?
Yeah I've seen those in production. It ended up being just a different flavour of spaghetti when you start joining multiple specifications. I got rid of them and simplified the interface to a few basic methods. &amp;#x200B; But if it works for you, happy days.
Word mail merge too difficult to use? I swear, at my workplace it feels like we're recreating every single slightly more advanced feature in Microsoft applications, which we're already paying for, because we never bothered to googled it. Well, I googled it but I don't make decisions or control the money bag.
It depends on the size of the application. That said I am doing this now.... The nasty bit is remembering if an object has a service or if you can use a concrete repository (or the old generic repositories I am trying to remove). &amp;#x200B; &amp;#x200B;
Too far dude. His argument is valid as is yours.
A lot of it might come down to customization and special scenarios. 
I want to work with people that think like you.
I suspect it's a pretty common application for manufacturing. Which is why it's insane to me that the UWP devs have explicitly decided to implement everything to run a cash register (thermal printer, *certain* scan guns, etc.) and then neglected any other industrial use.
It was, and that was how we ended up distributing it. The company had previous issues with updating apps around the building, so we were interested in a turnkey solution like the Store.
Depending on what target version you were on, 17134 brought in automatic updates to side loaded apps. 
Maybe. I find it difficult to believe, though. More often than not if the feature isn't discovered in the first place then expecting people to find the advanced settings button is unrealistic.
What did you end up doing then? How was production deployment, and how's maintenance going? 
Good to know... this was in early 2018.
oof, why excel? 
While that may be true, the definition of a ‚Äúlow level‚Äù language is fixed and expressed well by Wikipedia: ‚ÄúA low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture‚Äîcommands or functions in the language map closely to processor instructions. Generally this refers to either machine code or assembly language.‚Äù
Well, first you put an uncritical engineer to design the general specs, which he wants to be as unconstrained as possible, next you put [this kind of developer in charge.](https://www.reddit.com/r/programminghorror/comments/8p93b5/code_reviews_no_we_dont_why_is_there_something/)
Is this web forms? If its just a loading image then just hide it with javascript by setting css display to 'none' in the button's onclick event. Postback completion ought to refresh the image to its original state on completion.
Use JavaScript for that. Have a html button that has onclick attribute which will fire and JavaScript function. You should know CSS too. Have a CSS class for hidden and displayed picture (display: block/none) that you‚Äôll switch between using that JavaScript function.
If I do that then where do I actually make the image visible? In the aspx file or the CS file?
Why would you suddenly have a mess later on? Anyone who knows what they're doing can easily add in the layer later on when it's actually important. Meanwhile, if you never touch the app again or only have minor changes, you save critical time.
I'm the lead developer so feel free to ping me if you have any questions or feature requests. 
Oh if you want to start with the image hidden then make its display none by default. Then in onclick make display 'block' or 'inline' whichever suits best
Got it. And when/where would I hide it? The click event takes about 10-15 seconds to complete and the postback seems to happen at the beginning ü§î.
Couple things.... EF has an attribute, something like `Ignore` that causes it do exactly that with respect to a column on a table. Also, you might consider wrapping your service layer in a REST/WCF/GraphQL/YouNameIt API layer. Just expose objects and methods that are suitable for your users. See [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient). See the [demos](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon) for fully functional desktop and web apps. Adaptiveclient makes your app transport-layer and persistence-layer agnostic. 
Thanks man, means a lot :)
You can use ajax, it will be a good option if performance is an importance. However there are many other options given how your project is setup and what type of MVC your using. Post some code of what you have already attempted, explain your thought process in attempting it
lmao, why not just use EFcore :/
Sideloaded it, it worked. The company got bought and then that office shut down, so no more maintenance.
Because if there‚Äôs any expansion at all, it becomes a mess. And while you say that ‚ÄúAnyone who knows what they‚Äôre doing...‚Äù, I‚Äôve seen way too much spaghetti code by good devs to think that‚Äôs anything other than absolute BS. If you write unscalable code today, tomorrow, when your manager is telling you to get something done quick, are you really going to tell him that quick isn‚Äôt an option, because you actually chose to write bad code previously? No, you‚Äôll just add to the mess. 
If im not mistaken the end of postback should reset the css just after page load event completes or just before. If not just make sure you set the .Visible property of the image to false in the page load event
No c# is far from low level. It is fairly high level intact as it effectively requires a runtime to work. C and C++ are also high level. Not sure how relevant it is these days but the level of abstraction away from ASM and machine code use to be dependant on the generation of the language. So 4g was higher level that 3g etc. Either way the definition of low level is essentially asm or machine code. So if it‚Äôs not asm or machine code it is unlikely to be low level!
Yep, you need to deal with all the [viewstate](https://docs.microsoft.com/en-us/previous-versions/dotnet/articles/ms972976(v=msdn.10\)) crap. tl;dr is that webforms tries to hide the browser from you completely. You change the value on the server, and viewstate propogates that back and forth between browser and server. You never really need to use JavaScript directly in webforms (but you can if you are [careful](https://stackoverflow.com/questions/1305954/asp-net-postback-with-javascript)\)
In webforms, they have two click events. OnClick(), and OnClientClick(). Make the OnClick() generate the file. Make the OnClientClick() show the loading spinner. I think that's all you need. When the OnClick() finishes the page reloads, so the spinner will be gone too.
Not to put you down or anything but there are countless attempts, by some of the greatest minds in computer science, none have every reached any meaningful success. Largely because to be as specific as you need to be, to create a language that creates meaningful applications that are non trivial end up being so verbose, that no one wants to write or read them. Vb is probably as close to this as anyone has come and been a suceess, and that is overly verbose for most developers I know. 
If you are use to creating traditional apps, or aren't willing to be heavily reliant on making rest calls, I would avoid UWP. It's highly frustrating because gaining access to the newer windows 10 API is very difficult outside of UWP. At the time we started development on our last product, you couldn't do PInvoke, can't create services, long running process where a problem as if the user wanted to remove focus and you were willing to hack in a media player call, your app would be suspended. Basically they are attempting to force mobile app model on a business world and those two don't line up. You can create the app in a traditional way and then use bridge to fake it into a UWP app, but there are a lot of limitations and headaches associated with it. After considering all of this, and MS price sharing model to access their store, we developed and distributed with a more traditional method, and created a shell app that effective told users to come download our app for extended functionality. It's worked well. 
&gt; Log.CloseAndFlush(); I always thought streams are flushed before they are closed.
I'm quite certain there were no Wikipedia back in 1970-ies when C has emerged and was considered as a high-level language. The point here is that definition is a part of the natural language, it can change / evolve over time. Also there is a question of interpretation. Even the same definition from wikipedia could have been interpreted completely differently back in 197x as C language clearly provided quite significant (by those days) abstraction from hardware, etc. All these "map closely" is a vague part of the definition, and it tends to change over time. 
We do WPF and it's great. Look forward to .net core 3. UWP has been kinda abandoned (kinda) since they dropped Windows Phone and that was the whole point of that framework.
Funny, it's actually easy to access UWP API in .Net in WPF or WinForms. You just have to manually add the reference.
That‚Äôs not what spaghetti code is... Bottom line is that that filtering logic is almost always domain logic and should be part of a business/domain layer. If you move it to your repository, you are either forced to have business layer repositories or you are forced to put business logic in your data layer. Specifications allow you to fully encapsulate your data access layer and your domain/business layer.
&gt; Front end and back end are not synced automatically. Not in a Razor or Blazor page way. You can get away with A LOT without even touching JavaScript if you're so inclined.
Looking forward to using this very soon!
Wikipedia captures a definition that has been around for decades. And C has never been considered a low level language from a binary high/low definition, only lower than others. 
I agree with you
Listen to this person, OP. Build a RESTful API service layer to sit between the desktop app and the database. That way you separate the concerns so your app no longer cares about the database - that's the APIs job. Additionally, you can better manage database traffic and do things like caching reads. And the desktop client is no longer tied to the database, so a database change wouldn't necessarily mean a desktop client change. And you might be able to simplify security (depending on your needs). Basically, this is a very good idea. We know the EF Core docs have some gaps, so please reach out to me at [@CamSoper](https://twitter.com/camsoper) or Cam dot Soper at Microsoft if you have any feedback that can help us improve that set of documentation. /Docs author
"*Spaghetti code* is a pejorative phrase for unstructured and difficult-to-maintain source *code*..." - Wikipedia My argument was that they are difficult to maintain, as I gather you don't have a problem hence my comment if it works for you happy days. &amp;#x200B; Your right though, filtering logic is domain logic, or is it? Could it be argued it is the data layers responsibility to answer the question? e.g. Give me all the accounts that have outstanding invoices. Should the data layer be just stupid crud? Your probably right. &amp;#x200B; But that said I don't have a problem moving business logic that is specific for the database into the database layer. Because. Because I generally don't separate out my database models (entities) from my business models. And I don't have anaemic domain models, so all my entities are my domain models and contain business logic themselves. So what I do there is bleed and I'm okay with that. &amp;#x200B; It gets worse, on big complex applications, I generally end up using stored procedures to handle the reporting engine. Now that is bleed!.... And horrifically dangerous because you lose the strongly typedness of C# when doing that. But sometimes you need to optimise and that's the cost.
Its not constantly moving. Low vs High level was a distinction made when "high" level languages like C were being developed. Most distinctions these days are made on things like: is interpreted? compiled to an IL? compiled natively? Does it run on VM? Does it need a run-time? Is it a declarative language? do you use it in the data, business logic or presentation part of the stack I've never heard people start calling C# or Java low level because they were working higher in the stack in HTML, CSS, Javascript 
I‚Äôd add the health probe middleware to this list: https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks?view=aspnetcore-2.2 Otherwise, great list of things to start with. 
In Webforms (aspx + aspx.vb), the server is only aware of what the server creates or modifies. It remembers this in something called the "Viewstate". This is a big long encrypted string stored in a HiddenField control on your page. The code-behind (via the Viewstate) has to have its claws in all your controls to know whats going on. &amp;nbsp; If, in code-behind, you set: pnlName.Attributes("data-use") = "true" Then on the next postback, you can read the attribute. If you change the attribute via jQuery (client-side), your webform's viewstate didn't magically change to reflect that. The viewstate is written to your page on every postback, and since your jQuery isn't doing a postback to change this, its not recorded. &amp;nbsp; So basically you need to find a way to set this attribute via code-behind. If you are setting the panel's attribute based on a button press or a menu selection, make that buttons click event (code-behind click) update the attribute.
just load the whole db! /s it's probably a good idea to use the explicit include IMO. Wrap the query in an extension method or function if you need it a lot. 
My main idea behind this project is to simplify integration testing. Start a pre-configured docker container, run your tests and throw it away. Even performance testing is possible, start 10 container and let them hit your API.
Btw you can replace the .Where with FirstOrDefault
I saw a bit of it now and it's funny how it goes back and forth. No mixing of code and html type stuff, to back to mixed again. 
Correct me if I am wrong, isn't a HubConnection is "virtual", e.g. you can create multiple HubConnection and they will all share the same underlying web socket?
OK I asked David Fowler (https://twitter.com/davidfowl/status/1101750108654190593) and it looks like they are investigating better support on this matter.
Those aren't good devs then.
It's YAGNI
That's exactly what you're doing with this and your past comment.
Passing down domain object too far down is not a good idea to start with.could you select all the necessary fields into a DTO. 
That's nothing to do with streams, That is a method from the logging library Serilog. It's a way to ensure your logging is fully sent to any sinks before the app is shutdown.
It'd doesn't even compile to machine code. It compiles to an intermediate code, ran by the .Net runtime. 
That works fine. There's a Include(...).ThenInclude(...) method to load children properties that might be better, but it's completely normal to use eager loading to get all the data you need in a single query.
I would create an Addresses DbSet and use a service to get it
What exactly are you adding as a reference? 
I've actually done that,. I was just looking for a better approach, I guess I'll just fetch the addresses in a separate query. The query that eager loading generated is a bit long, with a lot of joins,
did you mean make another call from the database for the addresses?
I don't think it's a size thing it's just not writing loads of pointless classes if you don't need them. Whey have a service full of functions that look like this. Void Foo(entity bar) =&gt; repo.Foo(bar); Until you need that service layer to actually do something useful you may as well go straight to the repository. It just makes everything a lot simpler not to have a useless layer that you don't need. Eventually you will possibly need to refactor to use a service but that should not be a big job. 
C++ compiles too but people calling it low level language now all the time. only pure assembler was low-level at the time and C was considered high-level. Now things changed. As they will in the future when we get another higher level langage than C#
Somehow my brain refuses to read code that is not indented. But that's probably my problem. Wanted to point out though there is a button called inline code that could make it look like this: _context.Companies .Include(c=&gt;c.Address.Province) .Include(c=&gt;c.Address.CityMunicipality) .Include(c=&gt;c.Address.Town) .Include(c=&gt;c.ContactInformation) .Where(c =&gt; c.CompanyId == Id).FirstOrDefault(); public class Company { public int CompanyId { get; set; } public string CompanyCode { get; set; } public string CompanyName { get; set; } public virtual ContactInformation ContactInformation{ get; set; } public virtual Address Address{ get; set; } } public class Address { public int Id { get; set; } [ForeignKey("Company")] public int? CompanyId { get; set; } public virtual Company Company { get; set; } [ForeignKey("Province")] public int ProvinceId { get; set; } public virtual Province Province { get; set; } [ForeignKey("CityMunicipality")] public int CityMunicipalityId { get; set; } public virtual CityMunicipality CityMunicipality { get; set; } [ForeignKey("Town")] public int TownId { get; set; } public virtual Town Town { get; set; } public string Street { get; set; } }
Awesome, man!
Yes. That's the preview 4 install. RC is not go-live but redistributable. 2 separate installs
My point was that copy pasting code from a specification to a repository method or vise versa doesn‚Äôt magically make it spaghetti code. And I think you would have a hard time convincing me that NewAndTrendingWidgetsSpecification isn‚Äôt domain logic. You are also violating open/closed principal by designing repositories your way. Also, how would you handle a complex repo with 50+ specifications, half of which are shared between similar but slightly different repos? Are you copy/pasting procs? 
I have to downmod you to ensure the Xkcd comic wins. You'll agree that should be the case once you see it.
Even then, it's more like the shape of the letters. The "words" are the instructions to the processor, which each consume some multiple of 8 of bits.
Liked the idea of exposing the error codes. Would just add some caching to it, of course. Nice post! 
Looks great! Does it work in Core?
Using include works just fine. what i said was more in context of when project becomes larger. Most often then your controllers or methods need data in different shape. For example, some methods may need company with only ContactInformation and other method may need just the addresses or address in printable format in which case you might not need all the fields like Id. It kinds of become very confusing then when your company object is partially hydarated (you wouldn't want to fetch addresses when your method only needs contact information). It is much better to have Data Transfer Objects which return data in the shape the caller needs. In your example, you might do something like this: from cp in from cp in _context.Companies where cp.CompanyId == Id select new CompanyDTO{ Code=cp.Code AddressDTO=new AddressDTO{ Street=cp.Address.Street, Province=cp.Address.Province.PROPNAME } } You can notice that I am not creating a separate object for Province here. This is a question for you, does your consuming method need Address in flatter format or deeper object model. There is nothing stopping you from creating more complex Address object. The benifit of doing this is that your queries to database are leaner (would really recommend to either log produced queries or if you are using MSSQL and SSMS, then use SQL server profiler). just something else I noticed. Try to avoid configuration like [ForeignKey("CityMunicipality")] in domain object. It is much cleaner to do it using FluentConfiguration. In your case, you don't actually need to mention it at all. It wil be done by convention anyway.
I suggest you to use [PdfProLib](https://www.officecomponent.com/products/html-to-pdf) HTML to PDF Converter. It is highly reliable and easy to use. With a few lines of code, your app is ready to save webpages to PDF documents. 
Good point. For me UWP was my entrance to IoT world with Raspberry Pi and Windows 10 IoT. I was too lazy to learn Python so I just sticked to whatever C# had to offer. .NET Core 3 has built-in GPIO libraries which is great, means more boards with Linux.
Not yet. As far as I know WPF itself is currently only supported in the preview version of Core 3.0. I will definitely look into that when Core 3.0 reaches some kind of final state.
In suggest you to use these for creating Excel, Word, PDF documents and files in .Net Core. You can easily manage the creation of Excel,Word,PDF files by using these: [PdfProLib](https://www.officecomponent.com/products/html-to-pdf) HTML to PDF Converter, [HTML to Word](https://www.officecomponent.com/products/html-to-word) Converter, [Office Excel](https://www.officecomponent.com/products/excel) library.
what of windows form? 
lazy loading will have been perfect in this case, since your virtual property is not a List
You can use this [Excel](https://www.officecomponent.com/products/excel) third-party library by [OfficeComponent](https://www.officecomponent.com)to export data in Excel from HTML using [ASP.NET](https://ASP.NET) MVC. I personally use this and i love it. It is very helpful and easy to use. With simple lines of code you can achieve your goal. 
Support for Windows Forms is not planned.
&gt; If you don't do this then a request which couldn't be matched by any middleware will be left unhandled (unless you have another web server sitting behind Kestrel). Not true. Unhandled requests return an empty 404 status. 
WinMD files. Check out https://github.com/ljw1004/uwp-desktop
No, that's not how it works. 
Yea. That is really cool. Gonna try it out Monday. Thanks again 
Yes it is.
Regarding TIP 12 Working with Null Collections The better thing to do is to avoid null collections in the first. Always fill your IEnumerables or ICollections declarations with a Enumerable.Empty&lt;T&gt;() or a empty array. 
Awesome.
When creating migrations, you can pass in dbContext name as one of the parameters - you must create migrations per each distinct dbContext, even when they end up in the very same database.
Yep, you are 100% correct.
Depends on what you want to do, I'm on the backend side of development for the company that I work for, but so far from what I heard from our desktop division its a pretty nice experience. Though they do most stuff as API calls to our backend than doing everything in process, since the internal app we use has both a phone version and a desktop version as to share most of the processes. They also use windows template studio to jump start most of their projects from what I hear. Though I gotta admit the apps they make is quite pretty compared to the old WPF apps we have. If I recall UWP's sandbox model is quite restrictive, so if your app requires more advanced stuff in process you might hit a wall depending on what you want to do.
Good job. Could I ask what was your motivation for this / why did you do it? It's great, and better still that you've chosen to share it with everyone. But why? Fame and fortune? :)
This guy right here is correct.
wasn't he joking
Ha, I hope so, would make me feel better about things. Sorry if I didn't get it. 
I created lots of winforms controls back in 2009-2013. I stopped doing it because I thought everyone had moved on to wpf, and winforms would be dead. Do you think there is still a market for custom winforms controls &amp; themes? Are you using it in a big project?
One I found out recently when learning xUnit for .net core was ConfigureTestServices(). I needed a way to setup an in memory database with EF and found out that if the environment is set to Test it will call ConfigureTestServices instead of the normal ConfigureServices.
Yeah file access is a rough area, and also the web viewer for uwp is pretty restrictive. Otherwise it‚Äôs not terrible. 
It basically started as a smaller collection of styles I made for an internal project at work. Just a small tool for few people in the team, nothing special. Later, when I saw a co-worker develop a small tool as well, I offered him to take over the design and showed him the styles I did once. He liked them so I started to extract them from my original project into a seperate library he could use. As he developed more features for his app, it required more features for the UI as well. I found fun in bringing thoughts I had in my mind for years to code and helping someone out at the same time. It became my playground where I finally could get rid of things that bothered me in WPF's visuals and where I could experiment with ideas I stumbled accross. By now, the project is far beyond what the tool requires but it still makes me happy to continue working on it. Opening it to the public is just a step to keep my motivation high. To give it sense again, so to say. Developing is fun, but it is even more when there are people appreciating what you have done. So fame is pretty close, I guess :) PS: In case someone is interested, [this](https://github.com/nkristek/Stein) is the tool I developed AdonisUI for. I still ask its author for feedback every time I bring in some new feature.
I am not using it in a big project, I think there's little market for it. but you're correct, everyone has moved to wpf (except me) . I stopped building desktop applications for a while. so whenever I have to, I just use windows form. 
Looks fantastic. Will give it a try when I get back to the office!
A very years ago I created styles for most of the controls in WPF and even created some custom ones like a color picker for my documentation generator (http://sharpdox.de). I still use the styles, with some color changes, for most of my WPF applications. I disliked most of the free UI libraries for WPF. Either because of the style or the code. But yours looks really great! Thanks for sharing!
When porting .NET code always have https://www.apisof.net on your second screen. Enter the type you're missing and scroll down the list. It says the name of the assembly. When you can't find it for .net core visit the respective docs page on docs.microsoft.com - some types will be missing in ef core but the old types' docs page might have guidance for your migration task.
Can anyone give an example in a real world application of #13? Basicaly .Net knows a condition is true when the check for being false is false and thus shortcuts, but when the check for being false is true he still does check whether the true is true, allowing to having both the state of being false and true. There must be a reason why they did this and it will be more understandable an implementation actually doing this. 
Oh this is cool! Thank you very much. I wish I had a second screen though.
I don't have any experience with this, but I recently came across nopCommerce: [https://github.com/nopSolutions/nopCommerce](https://github.com/nopSolutions/nopCommerce)
well that is sad.
That's what I found too (made a mistake in my question and instead of "NopCommerce" I've written if "eCommerce" is good for it - just corrected it). That's probably what I will go for if you guys won't suggest me something worth checking. Thanks for the reply
ObjectContext is considered to be deprecated. I won't get into the merits and flaws of that decision, but you really should look into replacing it with DBContext (and a boat load of helper methods). https://www.c-sharpcorner.com/UploadFile/ff2f08/objectcontext-vs-dbcontext/
I'm not responsible for the old code so I have no opinion either way, but if Context is deprecated I'll just get rid of it. It seems that EF7 (Core) can generate an Entity class anyway, so I'll just go that route. Thanks for the help though.
Good luck.
i discovered the same 2 days ago!
That actually means a lot to me. Thank you buddy. I really needed that.
Recently worked on a webshop using NopCommerce. Its pretty dope (:
Awesome, it looks and feels pretty nice. Disabling-all in TreeView reverts to light theme. Disabling also makes the selected item hard to read. Very nice.
This seems to be a common misconception. See this thread: https://www.reddit.com/r/Windows10/comments/a91pso/is_uwp_dead/
Didn't say it was dead. Said it was abandoned kinda. Microsoft put out an announcement saying that with the death of Windows Phone, that they weren't going to pursue pushing UWP, cancelled their Office UWP, ect. But no, it's not dead.
Thank you for your feedback! I can see both issues and will fix them for the next update. If you find more stuff like that, please feel free to send a message or create issues on github so I can look into them.
I've been testing Core 3 recently and there's a strong likelihood your UI library works out of the box with it. I've been running .NET Framework 4.7.1 apps targeting UWP written years ago and netcore 3 doesn't even flinch when it loads them up. I know that doesn't necessarily mean your WPF library will work OOB but I'd be surprised if it didn't! I'll try it out some time and report back on the results.
The info on the upcoming Windows version and its (un)support of win32 apps was on MSPoweruser, search there. Like said in the comments, Windows is not the only platforms UWP is for. XBox is already significant. It's not "kinda abandoned" since it's receiving updates. New controls were recently added, also several developer tools. Windows Template Studio is a great one.
In a future update I want to implement a custom window title bar that respects the theme colors especially for the dark theme. In order to do that, I would need to raise the minimal .NET version from 4.0 to 4.5 to have access to the WindowChrome class. I am interested in whether this would be a problem for potential users. I know from my own work that sometimes one can not simply update the version for reasons. I don't want to exclude people just for this relatively minor feature. Any opinions?
Ah found the guy who got on the UWP train.
&gt;some methods may need company with only ContactInformation and other method may need just the addresses or address in printable format in which case you might not need all the fields like Id. I've thought about it, and made a separate repository to pull Company, Address and ContactInformation, and currently having 5 or 6 queries in Database base on my sql profiler, I'll still try your suggestions. Thanks! &amp;#x200B;
Your post has been removed. Self promotion posts are not allowed.
Yeah having a cms be on dotnet core and be tied to MS SQL server kind of defeats the purpose of having a CMS on dotnet core. Getting off the windows stack is the big selling point, cheap linux hosting. PostgreSQL would be awesome, as would MariaDB. Those should be huge priorities imo.
Windows XP is the only thing blocking 4.5 adoption. Noone is supporting it now, I would go for it.
This is EF 6 info so I am not sure if it's related; eager loading too much data can cause the query to be very slow. It joins on every table you eager load and so the records multiply by the number of records in the next table. If speed becomes and issue I have resolve the first query with single entity relationships using include, then on the context object use the load method on the dbset for the child objects with a where clause limiting it to those just for that entity. Typically we select the items into a domain object from the object used for the dbset. But that means the EF entities are not the domain objects. Did you use that approach.
Take a look at the awesome .net core list. There are some e-commerce applications on there. I'd link, but on.my phone....
Thanks, I found it: https://github.com/thangchung/awesome-dotnet-core/blob/master/README.md Looks like great list of .Net Core tools, I will definitely check it when I'll be at home
Generally you wouldn't do this. In fact I find it quite horrible. Quick Google search said custom Boolean types may want this (DBBool)
Thanks for the example. I program in C, C# and Python, although I know it's possible, it doesn't reduce the awe :)
Implemented several sites. With your dot net skills it can do anything 
Thank you for this.
Your welcome :)
You don't mention if this is EF6 of EF Core, but if you are using EF Core 2.1, you could configure Address as an Owned Entity. This would eliminate the need to explicitly retrieve the Address in your code (at the expense of some configuration code) [https://docs.microsoft.com/en-us/ef/core/modeling/owned-entities](https://docs.microsoft.com/en-us/ef/core/modeling/owned-entities)
Odd timing. I was asked to investigate in how we could leverage AWS Lamda on Friday. Was going to do some of that today, and now somehow you and Reddit are forcing my hand. Thanks for the post.
People should also look into the serverless framework for an easy and consistent method of deploying AWS Lambda and API Gateway endpoints. I‚Äôve been experimenting with this tool for a few months now and have been very impressed. 
OK. Some quick reading of your first blog post. Great stuff and some great tips as well, like the tie-in for VS.Net. I'll be following your blog and I hope you continue to share your work! 
Sounds cool. Will have a look and perhaps do a blog post. 
Your welcome. You can do some amazing things with lambda :)
Hey OP. Others have given you pointers regarding the overall migration from .net full framework to .net core, but one thing you may not realize is that EF Core is significantly different than Entity Framework on full framework applications. If you link to the EF Core assembly, lots of functionality (for example, automatically creating many-many mapping tables behind the scenes for collections, several validator types, etc.) will disappear. The code generating functions of EF Core are also much more limited so if you're using database first to generate your schemas, this may have to change depending on what you're leveraging within the generated models. &amp;#x200B; You should consider the goals of converting the library first. There are many versions of ".Net Core" - for example, when you create an application, you can create a .NET Core 2.2 console application or [asp.net](https://asp.net) core app that targets .NET Framework 4.7.1 or higher as the runtime, and it will allow you to link to your existing library that is created in full framework within .NET Core with virtually no changes other than upgrading the library to 4.7.1. You can also target net standard 2, but use the Windows Compatibility Pack, which will still publish as a .NET Standard package but will add a dependency on the compatibility pack which makes it Windows only. This will let you use the existing codebase and .NET, but will cause the library to only work on Windows. Otherwise, you need to port to EF Core (or, rewrite it all in Dapper ;)). So really ask yourself if you just want future work to be .NET Core and you want to be able to link to this existing library until you can rewrite it (and you're OK with the Windows dependency) before you spend a bunch of time trying to migrate a legacy EF application to EF Core. &amp;#x200B; We had the same conversation for a lot of our internal applications, and the decision we arrived at as a team was to create a .Legacy project that was a .NET 4.7.1 library and any application that needed to use the EF logic would take that as a dependency and run on Windows, and then we isolated that dependency using decomposed services. That way we can have one service that needs to run on Windows and it opens us up to running the rest of our app stack on Linux. The long term goal of course is to truly port it to .NET Standard 2 and leverage Dapper - we have decided as a team that we don't need a full ORM like EF so we are moving to Dapper rather than EF Core - but the same could be true for you.
 [harman\_cheema](https://www.reddit.com/user/harman_cheema) has it right. This is one of the things we ran into over time with EF (populating dependent data by traversing FKs). The way you are doing it causes lots of round trips, which multiplies not just the overhead of planning and issuing a query on the SQL side but also the network latency, so in our case when our database and our app layer lived at different datacenters, especially in a failure scenario, these round trips caused significant performance penalties. We went the route of DTOs, and at that point it became much easier to just not use EF at all, and directly create the queries using Dapper. Something to consider if it makes sense for your architecture.
whoa I've never heard about this suggestion. is this common knowledge?
Is this a typo? .WithEnvironment("dayOfWekk", dayOfWeek) 
Great take on this. Really appreciated your POV and this helped solidify some thoughts I had. Saying any particular approach or tool is right denies the fact that all applications are different in size, scope, developer experience, need for performance/scaling, etc. and the tradeoff of dev time to production efficiency can differ wildly. We should be dogmatic and pure in our thinking and approach to problem solving, which necessitates flexibility on the pattern we use to accomplish our goals.
[It's generally considered best practice in programming to avoid working with nulls when possible](https://news.ycombinator.com/item?id=12427069) Always initializing IEnumerables with a default value avoids you having to do null checks constantly and therefore elimates boilerplate. Also you'll end up with way less null pointers exceptions in production. This advise depends of course on the sort of business you are in, if you are a game developer, then you don't want unecessary method calls and allocations in your crucial engine code. But for business developers like me, this advise has helped me a lot and it makes zero difference regarding the performance of the system.
This was a hard lesson we learned as we seek to make our code more easily testable. Calling controllers in our tests and making fake http contexts was... not good. The controllers should handle routing, returning data, applying middleware (eg. authorization filters) and not any business logic.
Do you ever have to work with nullable int? ? Or have to do IsNullOrEmpty? I fucking hate this sometimes. 
Thank you for the kind words. I will try my best to keep making quality content. 
Oh :) Yea, that's a typo. It's also in the [source code](https://github.com/HofmeisterAn/dotnet-testcontainers/blob/ec7684d4b2b8944dffcee32983edc28856929bcf/src/DotNet.Testcontainers.Tests/Unit/TestcontainersTest.cs#L167). I'll fix it tomorrow! Thanks for letting me know.
Don't force it into things the paradigm isn't designed for, but if it fits it's fantastic. Paying &lt;$10/mo for integrations is the bomb.
Thanks for the lengthy write-up. I was given the task to simply measure how possible porting our legacy code base for .NET Core was, as we're getting a new system that does not support .NET Framework. There'll be a more systematic redesign in the future, so I'll look into your suggestion, so thanks for that. Right now I'm trying to hack something together and leave .NET Framework dependent code somewhere behind an API over Http...
No problem!
For the custom serializer, looks ripe for some kind of code-gen in visual studio. Might be worthwhile to write that if you're expecting to do it for a lot of classes.
Good thinking I was also thinking that is lot of manual work. Do you know of any good tools that can help achieve this sort of automation ? T4 text templates is one tech that i am aware but can't think of anything else that can be used for similar purpose.
&gt; Replace temporary collections with Yield (My Favourite) Assuming you are iterating over the result
Codesmith is one I've used in the past. Not free though I don't think
&gt; By looking at the code what you think whether the Unit Test will pass or fail due to unhandeled exception? I'll bite. I'd guess that the IEnumerable is initialized but is empty since the exception is throw before the yield. What's the correct answer?
The Short answer is the code will not throw exception. Some explanation: The actual method " GetEvenNumbersWithYield" will not be executed until unless you enumerate the result from the line "var evenNumbers = GetEvenNumbersWithYield(numbersToFilter);". So if you do not call the "ToList" or "ForEach" on "evenNumbers" variable the function to "GetEvenNumbersWithYield" will not be executed and in that test we do not Enumerate the result of yield method so no exception will be thrown. 
Ah, that makes sense. Thanks for the explanation.
I get a 404 after a cert warning 
I am glad that i was able to help. Enjoy your day.
&gt;client.GetStreamAsync("...").Result is an antipattern in itself. Even if it's not extremely critical in the sample, you should be demonstrating proper use of await here. The yield example is OK, but maybe not completely ideal, since that particular situation would be much, much better for demonstrating LINQ (specifically Where), not yield. I personally find that yield is a keyword I use surprisingly rarely, when LINQ either won't cut it, or when it would be too complex/unreadable. If this were a code review I'd also add my customary niggle about sticking to 'var' or at least choosing either var or full type names and sticking to it. 
That is some really good feedback. client.GetStreamAsync("...").Result I will fix this as soon as possible . \&gt; The yield example is OK, but maybe not completely ideal Yes i agree with you on that but focus was to demonstrate the behaviour of Yield keyword used for returning collection from methods and could not think of better example at that time. \&gt;If this were a code review I'd also add my customary niggle about sticking to 'var' or at least choosing either var or full type names and sticking to it. Yes agree with you the var or full type names if we choose either stick not an excuse but I think it happened because all these examples i copied from my notes were written at different dates and I had a mixup because i think my preferences changed over time. But will request someone to proof read next time. &amp;#x200B;
1.. Use Microsoft.IO.RecycableMemoryStream. Also, why would you ever read the _response_ to a string? Did you mean the _request_? Often, say you want to log request bodies, it is not possible to avoid reading in the entire request. 2.. This is not common at all in my experience. It is not often at all that I have a synchronous, yield-able source of items that needs significant filtering. 3.. 4.. Nothing wrong here, but these are such incredibly basic things to know it's not of much value to blog about them. 5.. This is not a good way to achieve better deserialization performance. Hand written serializer for every class? YUCK. Please don't. Start with a faster serialization library than Newtonsoft. Using that when performance is a primary goal is a faux pas. If you are going to use Newtonsoft - take a look at the docs: &gt; To avoid the overhead of recreating contracts every time you use JsonSerializer you should create the contract resolver once and reuse it. Hell of a lot simpler than oodles of custom serialization classes. Or just use Utf8Json, or the upcoming .Net 3.0 Span-based serializer.
T4's editing experience sucks. Zero IDE support. I prefer Razor since that has IDE support and we're using that anyway so people are familiar with the syntax, but it does take a little bit of work to integrate into your build process, whereas T4 has better built-in support in that respect.
Same. I'll notify the owners.
\&gt; Often, say you want to log request bodies, it is not possible to avoid reading in the entire request. Often does not mean always and there are better ways to log request without reading all the request contents to a string and not a valid enough reason to reading entire request in to string. \&gt;This is not common at all in my experience. It is not often at all that I have a synchronous, yield-able source of items that needs significant filtering. So you are trying to say code example was not relevant ? \&gt; Nothing wrong here, but these are such incredibly basic things to know it's not of much value to blog about them. May be too basic for you but not for everyone. That's why i mentioned the titles in the post so people who already knows they can skip and even in the very first paragraph of the post so the readers don't complain about this but still .... In short I do not agree with you on the suggestion about not much value in blogging. \&gt; If you are going to use Newtonsoft - take a look at the docs: Yes the same documents also mention this : " The absolute fastest way to read and write JSON is to use JsonTextReader/JsonTextWriter directly to manually serialize types. Using a reader or writer directly skips any of the overhead from a serializer, such as reflection." And that what i used in example for serialization and de-serialization. And also shows examples about the manually serialise to improve performance. Criticism is welcomed if it is constructive just tossing in few fancy terms about **upcoming** technologies without any explanation and examples and pointless bashing never helped anyone or going to benefit the community. &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
*Woah!* It's your **1st Cakeday** Dcoolledge! ^(hug)
&gt; not a valid enough reason to reading entire request You _have_ to read in the entire request to do anything with it. You cannot read in part of it and then continue along the pipeline. But yes, reading it into a `string` specifically is almost never necessary. &gt; So you are trying to say code example was not relevant ? Not at all. It's completely contrived. 9 times out of 10 or better the source of whatever intermediate collection you might have is going to use I/O and should be async and cannot be yield'ed (yet, though you can implement async yielding enumeration, e.g. https://github.com/Dasync/AsyncEnumerable, though it's not of much use if you're not actually asynchronously enumerating the source). &gt; Yes the same documents also mention this : &gt; " The absolute fastest way to read and write JSON is to use JsonTextReader/JsonTextWriter directly to manually serialize types And it will still be slower than just using Utf8Json and not having a manual deserialization method for every single type you serialize - which would be a huge amount of absolutely pointless code. It's a really bad idea.
Based on your setup yoilu should be able to do the following: 1. Place your city look up in a form using the Html.BeginForm helper. 2. Create an ActionResultin your controller that your form can perform an Http Get to send the city name the user enters to the server side. 3. In the Action Result you can then use the city name to perform your API call to get the weather data. 4. Once you get the response back from the weather API you will want to convert the JSON data you get back from the API into a C# model that can be used to display the weather data on the page. This is done by doing what's called deserializing the JSON response using Newtonsoft. 5. Once you've got the data in your model you should be able to return the model by passing it into the return View call for displaying the weather data from the API on the page. 
Look into BlockingCollection&lt;T&gt;, it is ideal for these kind of producer / consumer scenarios. Although in your scenario where the writes would only need to be blocked every 30 seconds or so to let the reads happen (which is an eternity in computing time) you could also just use a simple Monitor.Enter, i.e., the lock statement in C#.
Couple of questions: I've seen "helper" in a few places, how do these work in the layout of an MVC project? Is it a model? I think I have the first 2 steps in place but how do I pass the actually input into the controller? How does returning the data work in a layout? I initially tried having a separate view tied together but putting return View("name of view") doesn't work, I get an error, I believe it's a 500.
Also, thanks for the reply!
\&gt;You *have* to read in the entire request to do anything with it. You cannot read in part of it and then continue along the pipeline. But yes, reading it into a string specifically is almost never necessary. It was a tip to avoid a very common pattern i have seen many times and other have seen in community where developers read entire response to string for just to deserialize and Even David Fowler himself criticised that Here is a tweet mentioning that \[[Tweet](https://twitter.com/davidfowl/status/1010403287521095680?lang=en)\] . \&gt; So you are trying to say code example was not relevant ? I received that same feedback from multiple readers so i agree it was a bad example but this " **whatever intermediate collection you might have is going to use I/O and should be async and cannot be yield'ed** " that collection was not I/O bound not sure what you mean by that still not clear. \&gt; Yes the same documents also mention this : So you agree you did not even read the full article before commenting first time. Even the title explicitly said Using [JSON.NET](https://JSON.NET) in tip title what i can understand you are suggesting an alternative Library to [JSON.NET](https://JSON.NET) if someone is using [JSON.NET](https://JSON.NET) everything in there is still valid and manual serialisation will have performance benefits in my opinion. So can i assume you atleast agree with me on this ? May be too basic for you but not for everyone. That's why i mentioned the titles in the post so people who already knows they can skip and even in the very first paragraph of the post so the readers don't complain about this but still .... In short I do not agree with you on the suggestion about not much value in blogging. Thanks for the constructive criticism :)
I don‚Äôt remember there being any speed improvements between them. They are both compiled to .net classes. Performance is negligible. What I found is razor is easier to read.
.NET Core 3.0 is getting a new high-performance low-allocation JSON APIs to remove the dependency on JSON.NET and improve performance: [https://github.com/dotnet/announcements/issues/90](https://github.com/dotnet/announcements/issues/90)
https://docs.microsoft.com/en-us/dotnet/standard/collections/thread-safe/blockingcollection-overview
Can you have multiple functions in the same project with Lambda?
https://github.com/neuecc/Utf8Json
Same server: no problem whatsoever. You would be creating a new site or virtual directory in IIS to host it. If you were putting it in a virtual directory under an existing site, you would have to be careful of the fact that existing Web.config settings would get inherited by the virtual directory. Same ASP.NET application: they would likely happily coexist, but you would need to ensure that e.g. the type of authentication that you are using would work for the old and new parts. Mixing OWIN and the older Global.asax stuff can get messy unless you have a good handle on the different points in the IIS request cycle that OWIN hooks into.
That Red Gate blog post you link to about the LOH is pretty outdated (10 years old), LOH allocation was altered/optimized in .NET 4.5.1.
If you can, try and make sure you run them under different application pools and bind them on either different ports or IP addresses on the machine in order to save yourself some potential headaches down the road. IIS will happily route them based on a server binding, but keeping them separate will help enormously if you are needing to debug
If you are able to use javascript, lets deal with ajax call. You can display some loader just before ajax call to the server and then when response is back you can hide it
One thing that might have made it feel faster is the razor page only has a single pages dependencies instead of all of the ones needed for all methods in a controller. I prefer mvc but do like that difference about the controller per page/Razer page setup. Lots of green devs put too many methods in a single controller and bring in extra dependencies in a lot of the projects I hand off and it just gets out of hand.
You could have a Singleton reference from your DI container which should handle most of what your asking for. Otherwise, and I hate to suggest this, a static Singleton that you populate in the startup methods. But you end up poluting the global namespace and is a frown upon design pattern.
You could set the button to fire Javascript that starts the loading icon, then fires a __doPostback that will initiate the file creation 
For ORM, I strongly suggest using Dapper: [https://github.com/StackExchange/Dapper](https://github.com/StackExchange/Dapper) We moved away from EF; we've tried some other micro ORMs like NPoco or PetaPoco but Dapper is out go-to micro ORMs. Also, we are in a process of ditching DevExpress - it is too bloated for us.
Thanks for sharing updated link if you don't mind can i use that in the blog post as a link for LOH. 
I think you have fallen into the trap of using your entities you created to represent tables as your domain business objects. The business objects in your domain don't necessarily match your data structures; and this is fine! You don't need to use the same objects; but most examples show this and most projects start this way. Create your domain of business objects, return those from your API as another user had said, have separate objects for your entities for EF that represent tables. In your service layer handle updating your entities from business objects values. Separate the crud operations using business objects and entities from your controllers you don't need to be using entities outside of the service layer/repo and data access projects. Take a serious look at domain driven design; it sounds like your database doesn't match this, don't try and force it and create multiple objects with different jobs; separate the concerns.
To be clear I'm not OP. Reading the whole request into a string is not a common pattern, unless you're doing something with the string, though even if it was, it's not a particularly slow operation in most instances anyway. You'll know roughly what kind of data you're expecting and if you get something you really aren't expecting there are ways to handle it. Yield doesn't work like you think it does. There's still an enumerable allocated and there's still elements created in it. The advantage is that you don't have to create all the elements up front, but that's only a benefit if you process. Three and four are important, but including them with a bunch of things which are **at best** situational and at worst just plain wrong does your audience a disservice. You're teaching new people things that aren't correct and boring more experienced people. It's fine to blog, it's even fine to blog stuff you're not an expert in, but if you're going to pass along tips it's probably worth understanding what you're talking about. In particular the JSON.NET advice is really bad. Making your code massively overcomplicated is **never** the right choice, and if performance really matters using libraries designed for that will almost always be the right choice. 
Keep the generation asynchronous to the UI. Instead of hitting the generation with a button click, have that click kick off a JavaScript function and an AJAX request. Then, every second or so, keep checking the status of the generation, again with JavaScript hitting an endpoint in your ASP.NET application, and update the UI based on the status you get back.
I started with this series back on mvc 4. They are great, the beginner level books are just that where the pro ones are really aimed at what you would do on the job as a developer. Though some of these things are a little dated at this point, but the book is still very good.
2. Or use Linq (which under the hood is the same) to avoid reinventing the wheel. ``` return source.Where(x =&gt; x % 2 == 0); ```
Not a full time developer, but I make custom applications for some research stuff, mainly computer vision and instrument interfaces. I tried to make a UWP app for machine vision. Problems were the camera library was built on .NET Framework so couldn‚Äôt be used, Emgu.CV, the main wrapper around OpenCV for C#, is not freely available for UWP. So obviously must use WPF Started another app recently for sorting and processing images then running some external scripts. Problem is I can‚Äôt call an external exe to run some of these scripts. I think what I have to do is create a console app as part of the project and make it a full trust process and then communicate the parameters to that which would then call the external exe. Too much effort so back to WPF. One more UWP app I‚Äôm writing now and actually trying to complete. I love the fluent design stuff. Problems I see is that the API surface is split across many NuGet packages, UWP XAML is missing some super useful things like style triggers, there are buggy styles eg control bar with text on the right if you add a separator the overflow button shows for no reason and margins stuff up, XAML bugs just throw errors with no debugging info (WPF at least you get a line in the console). All I can think of right now. 
Cool
It would be nice. We have lots for iOS Android UWP but nothing for macOS. 
If you need change tracking, why don't use EF Core for updates, and use micro-ORM for efficient SELECTs (if needed) and specific stored procedure calls. For detached mode it might be better to use real database like SQLite instead of inventing your own 'internal' detached storage.
Not a huge fan of throwing exceptions for validation but willing to be convinced. 1) pollution of intent: Often an incorrect email address isn't exceptional 2) Overhead : Building exceptions are costly, 3) Multiple fails : In the real world , users get pissed if you dont tell them everything wrong with a form submission 4) Translation : Error codes, error strings can be easily translated, but Exceptions ? not so obvious 5) Logging : I've inherited systems where these kind of "running exceptions" end up in logging and make dealing with live issues more complicated. Please convince me, as I like the idea you put forward
Razor Pages is a better model than MVC Controller for your typical GET/POST forms/pages because you can pass your extra data as separate properties instead of creating a giant ModelView to carry all your data or using ViewBag. 
Extension methods on basic types like `System.String` are serious code smell. This would have been **much** nicer as a simple method.
1), 2) Incorrect email address can be (and should be) validated earlier on Application Service Layer because this is data validation. I have written about this kind of cases here - [http://www.kamilgrzybek.com/design/rest-api-data-validation/](http://www.kamilgrzybek.com/design/rest-api-data-validation/). This article is mainly about validation business rules from Domain Driven Design perspective. Throwing exception is a trade-off which I mention in my article in "Comparison of solutions" section. I agree that exceptions should be thrown on exceptional cases, but I think the validation is good exception to break this rule. ad 3), 4) These concerns are solved using FluentValidation library and ProblemDetails middleware. I described this solution in REST API Data Validation article too. The message returned to the client is clear and structured. ad 5) These exceptions will not be logged because are caught by ProblemDetails middleware. Of course you can log them if you would like to. &amp;#x200B; &amp;#x200B;
Or as a simple constructor, which is already there: ```csharp var persons = new SqlString("SELECT * FROM Persons").ToList(); ``` Could you outline the reasons why this is a code smell?
&gt; Extension methods on basic types like System.String are serious code smell. Not sure I agree with this in all cases but in this one I certainly do.
Throwing exceptions is 20x slower than returning and uses up resources due to the stack trace. If you use CQS, you can force all your code through a funnel so you can handle any number of errors the same way once, regardless of how they are built up via an IResult being returned. It can also ensure validation rules are run. Showing 1 error at a time is not only annoying to end users, it also wastes a lot of server resources.
Well, yeah. Differently said: an extension method makes sense, if the method makes sense for all instances of the type. This is absolutely not the case here.
Turning a string into a list makes semantically no sense. It's code turning obscure by being lean on all costs.
All instances of the type could be an SQL string.
How do I deal with the response?
Exactly. Could be. And by far most are not. So the method is not applicable to most instances of the string type.
Hey man, &amp;#x200B; thank you very much for your input. It has really helped me get a new jump start in this new gig (which, btw, is the reason I took so long to reply this time, and I apologise). &amp;#x200B; Just for clarification, yeah. My objective is more or less the latter, with a little bit of "news" to keep you updated on new features, new use cases and the likes.
Also, a public static field makes parallel testing impossible. https://github.com/zspitz/StringAsSql/blob/master/StringAsSql/SqlString.cs#L12
&gt; behind a generic plain extension method. Technically, nothing happens in the extension method; it's all happening in the instance methods of `SqlString`. &gt; Important details of what is happening is hidden behind a generic plain extension method. If all I want is a list of `Person` from the database, isn't everything else an implementation detail? In many cases, and as long as everything works, I don't know or care about the connection, the reader, the command or the parameters; I just need the final result.
Ajax fire an event when the distant server sent the data or error happened. It is very easy to use (javascript): // If you want to send POST data, you can add it here var data = new FormData(); data.append("Key", value); // Otherwise, if not needed, you can just change the type to "GET" $.ajax({ type: "POST", url: "{yourURL}", contentType: false, processData: false, data: data, success: function (messages) { // The messages contain the return value what you got from the server. }, error: function () { // If an error happened, you can handle it here. } }); While this run in the background, you can display the loading screen/animation. The browser won't be frozen, as this method will run in the background.
Yes, I know: https://github.com/zspitz/StringAsSql/blob/master/StringAsSql.Tests/MSAccess.cs#L61 I don't want to add another parameter to the `SqlString` constructor. What would you recommend? 
Yes, try ConcurrentBag, you can read more here: [https://docs.microsoft.com/en-us/dotnet/standard/collections/thread-safe/](https://docs.microsoft.com/en-us/dotnet/standard/collections/thread-safe/)
Why should I use this instead of Dapper?
Why did you post the same thing twice? https://www.reddit.com/r/csharp/comments/awybsi/5_c_tips_that_you_may_already_know/
Because even with Dapper, the SQL statements get obscured in methods and other objects. Compare: ```csharp "SELECT * FROM Persons".AsSql().ToList(); ``` with: ```csharp connection.Query&lt;Person&gt;("SELECT * FROM Persons"); ```
Is there an IDE you can use to develop in .Net Core outside of Visual Studios you recommend? I need it at work, but can‚Äôt justify a license as it‚Äôs not really in my job description lol. I was previously using SharpDevelop. Getting a new work PC and need to request either it or something else.
Let me rephrase: Every instance of `string` can be marked as an SQL statement. Perhaps an implicit conversion from `string` to `SqlString` would be a more appropriate mechanism, but it would again introduce boilerplate which I am trying to avoid.
VS code powered by extensions. It's fantastic. 
 Visual studio community version is free. Rider is great and not very expensive. VS Code as mentioned in the other comment.
I think Community is not licensed in a way that you can installing on a work commercial pc. It‚Äôs intended for personal use. If that has changed that would be wonderful.
This is another library that extends string for http requests, it might help to see how it works. https://flurl.io/docs/fluent-http/
Agreed. Hell, depending on what sort of code changes I'm making, I prefer it over Visual Studio Enterprise.
Been looking to sink my teeth into a new pod. Definitely checking this out. 
I'm not sure I'd call that "obscured". I can easily see where there connection is coming from rather whereas in the first example I can't tell as easily how to find what it does.
It's mostly individuals but organizations can also use it if they fall into certain categories. &amp;#x200B; b. Organizational License. If you are an organization, your users may use the software as follows: * Any number of your users may use the software to develop and test applications released under Open Source Initiative (OSI) approved open source software licenses. * Any number of your users may use the software to develop and test extensions to Visual Studio. * Any number of your users may use the software to develop and test device drivers for the Windows operating system. * Any number of your users may use the software to develop and test your applications as part of online or in person classroom training and education, or for performing academic research. * If none of the above apply, and you are also not an enterprise (defined below), then up to 5 of your individual users can use the software concurrently to develop and test your applications. * If you are an enterprise, your employees and contractors may not use the software to develop or test your applications, except for: (i) open source; (ii) Visual Studio extensions; (iii) device drivers for the Windows operating system; and, (iv) education purposes as permitted above. &amp;#x200B; An ‚Äúenterprise‚Äù is any organization and its affiliates who collectively have either (a) more than 250 PCs or users or (b) one million U.S. dollars (or the equivalent in other currencies) in annual revenues, and ‚Äúaffiliates‚Äù means those entities that control (via majority ownership), are controlled by, or are under common control with an organization. &amp;#x200B; [https://visualstudio.microsoft.com/license-terms/mlt553321/](https://visualstudio.microsoft.com/license-terms/mlt553321/)
Also, I always thought a pointer to this temp array variable is what would have been returned in the traditional implementation. In the version without yield, we create a temporary array, store the items in there, and return it: are we allocating space for the temp array, then the returning function has another array just like it created for the return value, then we GC the original temp array?
One of the main points of a Repo that I'm not seeing in the responses to this question is that it improves test-ablity of the code. Writing unit tests is damned hard if the data access (and other integrations) are embedded in single layer logic. Also, much of what I see in the comments as "problems" with the Repo pattern should actually be captured in a "Unit of Work" data logic layer. Not always necessary, but again often helpful to improve unit testing. A lot of folks don't like unit testing, but I'm a believer. 
VSCode is one option. Rider is pretty great as well but isn't free (much cheaper than VS though).
Will this be equal parts .NET Core and framework or skewed a certain way?
I'm relatively new to Blazor and it does look promising, but could this approach be used (in future) to resurrect the "Lightswitch" business application toolset Microsoft had a while back?
I actually prefer VS Code over Visual Studio. It's lighter weight and if you grab the right extensions and learn the keyboard shortcuts it's pretty easy to use.
You should throw exceptions in the domain, since getting the domain in a bad state would truly be an exceptional event, but you definitely should have a validator on top of your domain in the application layer that gives better feedback.
You're still completely missing the point. Providing an extension method to a type that is only useful for very specific values of the type is code smell. Only a few instances of `string` are SQL statements, but your extension method is provided for **all** of the `string` values. &gt; Perhaps an implicit conversion from string to SqlString would be a more appropriate mechanism That would be even worse. A lot worse.
yeah , you run the risk of the "2 clocks at sea" problem, as changes in the business rules may not be equally implemented in both cases. Unless the validator is integrated , with (presumably) some kind of response object (as the article outlines and dismisses) there is a risk of either 1) Calling without validation or 2) Reusing the validator and creating a dependency nightmare &amp;#x200B;
Yeah, and the individual terms are "Any individual developer can use Visual Studio Community to create their own free or paid apps."
Thanks for listening! &amp;#x200B; Right now, we're really just sharing information that Microsoft releases (maybe other community things later on), so it somewhat depends on what information/releases/etc they give out in a given two week time-frame. We won't explicitly skew, though I suppose it may seem like it at some point. &amp;#x200B; As for the audio - completely agree. I've got some more stuff on order to hopefully tone down issues in mine, then its just a matter of getting better with Audacity, which I absolutely intend to do. Hopefully, there will be a steady progression of overall quality with each release.
The podcast was great. Thanks for doing this. You guys have a good dynamic going on. 
I don't see why the returned array would get copied, I belive the reference to the temp array is returned.
Looks similar to Python Bytes and is something similar I have been wanting for .NET for a while. I have added it to my list. Looking forward to listening to your episodes. I have added your public RSS feed to player.fm so it can be discovered by people using player.fm app.
In your first example, what connection is it using, do you have to set it up in DI? How do you handle having multiple connections to different databases? Does it return a list of person objects or something else you will have to map/cast yourself? The dapper code makes all these very clear.
That's what I thought too, so would yield be more efficient memory wise, or just better syntactically? 
I use JetBrains Rider. What I really like is that Rider indexes everything, if you need to find something, it finds it instantly. Also refactoring seems much more polished than VS refactoring.
Thanks for listening! &amp;#x200B; Funny you say Python Bytes, because that was, in part, a lot of the inspiration for this!
Sounds really useless, C# has lost its direction when it became easy to add features with roslyn, seems it‚Äôs just adding random syntactic sugar for things we really don‚Äôt need making the language overall more complex. This will be a hard pass for me, this feature can go straight in the bin, also could replace all of this favorably with a new linq operator
I think you can do this for sure. I think along the way you will notice that you need things slightly different I. The two environments. The trick I find is to identify the stuff that never changes and reduce it to the most basic concepts of the system. Then add the more specific stuff outside of the domain model. 
I would suggest WPF, UWP, or Xamarin instead of Avalonia. It looks like the last release of Avalonia was 0.7, so it isn't even 1.0 yet. I would lean towards Xamarin as this is exactly the kind of scenario it is designed for.
They are substantially less powerful and flexible than the index system in Swift, except for the ‚Äúfrom the end‚Äù feature. 
Actually it is very useful for me as it cleans up a lot of code, along with the ranges (I am writing an article on that later) it will be really helpful and make the code less messy. It's like x == null vs x is null, the latter being more readable
&gt; I've seen "helper" in a few places, how do these work in the layout of an MVC project? Is it a model? By this I mean the Html.BeginForm helper that comes out of the box with [ASP.NET](https://ASP.NET) MVC. Check this link out, it shows a very basic example of using this helper to send data on the page to a controller. [HtmlBeginForm tutorial](https://www.aspsnippets.com/Articles/ASPNet-MVC-HtmlBeginForm-Tutorial-with-example.aspx) &amp;#x200B; &gt; I think I have the first 2 steps in place but how do I pass the actually input into the controller? See the link above for the BeginForm helper, this is one way of passing the input on the page to the controller. &amp;#x200B; &gt; How does returning the data work in a layout? I initially tried having a separate view tied together but putting return View("name of view") doesn't work, I get an error, I believe it's a 500. What error are you actually getting? Its possible that its just unable to find the View that you created. Returning data in [ASP.NET](https://ASP.NET) MVC usually follows this pattern: * A C# class is populated with the data you want to display, this is commonly referred to as the Model and is the M in MVC. * The Controller, the C in MVC, is used to pass this Model to the View. * The View, the V in MVC, is then used to display the contents of this model using Razor syntax. 
Will listen, thanks for posting this!
Thank you. Maybe you could answer a subquestion for me? I see this in Startup.cs in the constructor: public Startup(IConfiguration config) { Config = config; } My question is not about the variable `Config`. Who/what is calling the Startup constructor, and what is it passing as the argument `config`? Who‚Äôs passing the argument to the constructor, or how would I find out? 
There's nothing stopping you from getting community edition and making tools at work. If you're actually given tasks on your workplace's commercial products then it's your manager's responsibility to see to it that you get a licence for professional edition.
What does this do that a simple extension method ElementAtFromLast wouldn‚Äôt provide?
What are you talking about, the C# designers have been super conservative with introducing features so far. It's obvious they're introducing this because they've recently introduced slices to the API. 
Are you kidding? I would have killed for this when I was writing bank software that dealt with parsing files. Though it may not be as useful as it could be with jackasses returning IEnumerable instead of List or IList.
Because they are two different communities and people who follow one of them may not be the followers other. My assumption can be wrong but I think this post was inline with posting rules for both communities please let me know if it is against the guidelines i will remove the post and i in future will poat in one of the communities onlys
I believe the dot net runtime is calling the startup for you, what type of project did you create and what version of dot net?
Your webhost builder (CreateDefaultBuilder) is adding the appsettings.json file. Check the source code. It is also calling Startup in the UseStartup&lt;Startup&gt; method.
Compression will help with the levels - it reduces the dynamic range of audio. Quick attack, slow release, with a limiter and some make up gain
It actually depends on the size of the array, as yield is a syntactic sugar for a new class.
Linq makes no guarantees how it would get that element. Maybe it would iterate the whole collection and then go back X from the end which would be terrible performance. You just don't know with linq. Usually Microsoft do add optimisations where possible, such as the Count () linq operator checks if it implements ICollection and if so returns the Count prop. But still you can't be sure without looking up source code and even then to be sure you have to know you have an ICollection. Anyway I could go on but my point is this, the indexer is clear that it is indexing into this collection 
I would use wpf or uwp for desktop gui. But there is a fundamental difference between desktop guis and web interface. The web interface uses a request/response model where you basically initiate the models each time a request comes in. The desktop gui initializes the models just once as long as the view lives. Responses to user input are handled on events and by utilizing data binding techniques. You will see that often the model classes implement INotifyPropertyChanged when they are used in desktop gui applications. In contrast the INotifyPropertyChanged does not really have a useful application for web applications. ASPNet Core's automatic model binding is very useful but suffers from overposting risks so you need an additional viewmodel layer to prevent exposing too many properties. There are many other differences that I can think of. All in all for a very simple tutorial project it's possible to use one model for both scenarios but in a real world application it's not really applicable. 
&gt; what connection is it using, do you have to set it up in DI? Yes; see https://github.com/zspitz/StringAsSql#setting-up-the-connectionfactory. Alternatively, you can pass in a connection object to the `ToList` method: ``` var persons = "SELECT * FROM Persons".AsSql().ToList&lt;Person&gt;(conn); ``` &gt; How do you handle having multiple connections to different databases? By [explicitly passing in the connection](https://github.com/zspitz/StringAsSql/wiki/Explicitly-passing-the-connection), as above. &gt; Does it return a list of person objects or something else you will have to map/cast yourself? That was a typo; `ToList` requires a type argument -- either a generic parameter, or a statically typed object from which the generic parameter can be inferred. See [here](https://github.com/zspitz/StringAsSql/wiki/How-to). &gt; The Dapper code makes all these very clear. True, but at the cost of somewhat obscuring the intent, which is to get the results of the SQL statement. The connection is almost an implementation detail, a means to that end. When you use some sort of database management tool, such as SSMS, queries from VS Data Explorer, or HeidiSQL, you don't have to choose the connection each time you execute an SQL statement; that state is stored between uses.
You're joking right? This is a great feature to use alongside the new `System.Memory` `Slice&lt;T&gt;` and other new changes.
&gt; how to find what it does I would ask, what is it trying to do -- what is the intent? Is it to pass strings down a connection object? Or is the intent to get the results from an SQL statement? If the latter, then the code should start from the SQL statement and continue from there, rather than start from the connection object. It is possible to [pass the connection into the `ToList` method](https://github.com/zspitz/StringAsSql/wiki/Explicitly-passing-the-connection), and when my application uses multiple connections it may be necessary; but if my application has a single primary connection, even that is unnecessary.
From a consumer point of view, it feels really strange to pass a connection to a ToList()-method.
Will System.Numerics types use this as default?
Perhaps because the standard LINQ `ToList` doesn't require any arguments. But `ToDictionary` -- which also produces an in-memory data structure -- does require arguments, so I don't think there's something inherently "wrong" about it.
On point 5, you could create a _DomainModelException_ that is extended by all other domain-specific exceptions. Any exception extending this class would be excluded from catch-all logging.
Not if you are a Roadway Designer Civil Engineer that could just as easily write most things for free in VBA or as internal plugins for our software. I know .NET and it‚Äôs more efficient and much better for a multitude of reasons, but I don‚Äôt NEED Visual Studios. I want it to increase productivity. Would also be nice for my own ends on lunch breaks lol
&gt;Reading the whole request into a string is not a common pattern, unless you're doing something with the string, though even if it was, it's not a particularly slow operation in most instances anyway. You'll know roughly what kind of data you're expecting and if you get something you really aren't expecting there are ways to handle it. That is purely your opinion. I have seen it many times and few other people in the community also noticed that it is a common pattern. I even linked a discussion where people advocating the use of it just for readability. &gt;Yield doesn't work like you think it does. There's still an enumerable allocated and there's still elements created in it. The advantage is that you don't have to create all the elements up front, but that's only a benefit if you process. I am not an expert at yield but yield does avoid the cost of upfront temporary allocations. It does involve the cost of creating Enumerator that's why i mentioned in the post "There are situations where using Yield will result in overhead that will overweight the benefits of avoiding some temporary allocations." because if the collection you gonna return is very small the cost of creating Enumerator will overweight the cost of creating Enumerator. &gt;It's fine to blog, it's even fine to blog stuff you're not an expert in, but if you're going to pass along tips it's probably worth understanding what you're talking about. You made a judgement too quickly man but i understand you have the rights to have opinion. But i am disappointed that people are very prompt in making judgements/opinions in our community without putting much effort in understanding what other person is trying to say. &gt;In particular the JSON.NET advice is really bad. Making your code massively overcomplicated is never the right choice, and if performance really matters using libraries designed for that will almost always be the right choice. In blog i mentioned if you have few classes that will need this sort of optimisation then you can consider writing custom serializer for those specific classes and avoid another dependency on another serialization library if JSON.NET is already in you stack. Because JSON.NET is already part of many projects as a dependency some Azure SDK or something But i agree i should have made that very clear in the post. Will update the post to reflect that. So thanks for this nice suggestion. It was a rookie blogger mistake may be. Just a request Please don't make your opinions sounds like facts because that is also disservice to the audience I hope you can agree to that.
I‚Äôve yet to use VSCode as I have Community at home. Can you do Form Applications still?
Then community edition is fine.
Go with Cypress. It's awesome. 
Just guessing but you might want included entities dependent on business logic in the service / domain layer I. E to load them or not. 
Yup... Calvin quiet, Matt loud. 
It‚Äôs not the struct that bothers me, it‚Äôs the language feature and operator
&gt; The connection is almost an implementation detail, a means to that end. So is the fact you are using sql at all, but your AsSql extension exposes that. If you wanted to hide implementation details then you are going about it in an odd way. I would also argue that if you are exposing the fact you are using sql then a connection is absolutely not an implementation detail.
No, Yield doesn't prevent q temporary allocation. The object you return is not temporary. Yield **defers** allocations. In particular yield can defer an allocation to never if you're not going to process the entire enumerable. You've written a blog, which is fine. You've gotten some stuff in it wrong, less fine, but pretty normal. You've then angrily defended ideas which are wrong. That's **not** fine. 
When writing business logic to execute via Web API, I condition the data as much as possible before finally loading the data in memory to send to the client. This is pretty much the only way I use EF Core. I don't like leaving DbContext connections open longer than needed / for more than one transaction.
&gt;No, Yield doesn't prevent a temporary allocation. The object you return is not temporary. According to my knowledge Yield allocate for Enumerator object instead of full collection so it avoid temporary allocation for the full collection that collection allocation is more expensive in most cases as compared to Enumerator object allocations. Not sure how it contradicts anything in blog. &gt;Yield defers allocations. In my opinion Yield returns one item at a time if you iterate using MoveNext()/foreach does not return full collection so avoid collection allocation. In particular yield can defer an allocation to never if you're not going to process the entire enumerable. &gt; That what is asked as question in post about a test with exception because you do not iterate the Yield result so method block never get execute so defer an allocation to never You just expressed the same thing but in your own way. &gt;You've gotten some stuff in it wrong, less fine, but pretty normal. Yes i got some stuff i agreed in last comment few things were not clear enough and admitted to that I will never make a statement that this is the best way to do thing or i am an expert at a topic etc. Everyone should make their own choices and mistakes i just share mine. &gt;You've then angrily defended ideas which are wrong. That's not fine. Apologies if that sounded angry i was just disappointed that without any effort to understand the what author was trying to communicate you already made opinions and declared things incorrect. If you would have asked few questions before making a decision that it's all wrong i should not write such posts. You may have found out that it was same concepts just expressed with different choice of words. But as i always say everyone have right to have opinions if "that's Not Fine" according to you. It is perfectly fine for me if people disagree with something.
Correct me if im wrong but the take away from this is that in .net core 3 we will get the following - The compiler will be further optimised to use cpu level instructions leading to faster performance of .net core. If true this is huge, it will blow the JVM out of the water and leave nodeJS even further behind in terms of performance - For development of .net core for IOT devices, would this allow further performance improvements? it would also add the potential to directly code with instruction sets on the device, allowing some features to be exposed on the device not possible with current IOT dev on .net core. - Does this give an alternative option to develop assembly level programming for hardware? Would this allow have any draw backs compared to c++ / c? in terms of what can be done? c you can program circuts at a low level (for a lack of a better term). - The parser https://github.com/EgorBo/SimdJsonSharp utilising SIMD instructions is essentially going to be the fastest json parser in any framework except c++ / c, potentially other langauges that target cpu instructions. follow up question - I havent been keeping up to date with the JVM, are they taking note of what .net core is doing and are they trying to keep parity? 
A yield returns an enumerable. Enumerables are processed when they are enumerated. **If** don't actually enumerate the entire list, you can avoid allocations, though they're not temporary, they're unnecessary. On the other hand if you enumerate the enumerator more than once you'll actually allocate the entire collection more than once. If you immediately call .ToList() your methods are identical. If you call foreach twice, the yield is worse. If you call .Take(2) the yield is better. And no, arguing you are right when you're not rather than learning is not fine, it's particularly not fine for you since you're not learning. 
This comment is discussion we had before now you started explaining more details about yield that I know already but still don't understand how they contradict the blog and support your criticism it's just something that can be added to the information already there. But still thanks for sharing. In my opinion I don't think i can get anything useful from this discussion because it's moving away from the actual topic just to prove something useless that will not achieve anything. &gt; And no, arguing you are right when you're not rather than learning is not fine, it's particularly not fine for you since you're not learning. I am not convinced with your arguments. May be instead of writing threads of totally unrelated comments how about you write a post with your opinions about the topic and share with community. I promise i will read with a open mind and that may help. But comments you posted so far sorry they are not helpful at all to get your point across. I hope you can accept this fact if people can't agree with your every opinion it's fine. If you can't accept it sorry for the discomfort.
They contradict the blog because you're saying: 1) That you should always use yield, which is wrong, depending on your use case, this is dramatically worse. 2) That you're avoiding the temporary list allocation, which you're not. You're deferring allocation to a later point and optionally you may not have to do some of the allocations. There aren't any temporary allocations. And again, you **don't** understand how yield works. It **doesn't** return one at a time. It returns sequentially, which is not the same thing.
You just say those two are wrong and I believe it no explanation just call them worse that's very convincing. It avoid temporary collection to compute and hold just the return results. &gt;And again, you don't understand how yield works. It doesn't return one at a time. It returns sequentially, which is not the same thing. You are just now playing with words it looks like you can't find some arguments what's next grammatical errors nice talking to you. More you explain less reasonable you sound and i started to doubt even if you read the post before criticizing now may be you read just parts of it jto argue in this discussion. Still don't agree with you. You can continue with comments but this one is last from me if you writeup a blog sure share link in community and if it will seem its worth my time i may read because your way to explain things still makes no sense too mee after putting so much effort can find better info somewhere else.
Can you add it to Stitcher?
[Dependency Injection Principles, Practices, and Patterns](https://www.manning.com/books/dependency-injection-principles-practices-patterns)
With regards to EF Core specifically, this book from Manning is pretty good:https://www.manning.com/books/entity-framework-core-in-action
I would build and push a docker image to docker hub and have your droplet pull it down using watchtower. 
Why SIMD is considered harmful according to RISC-V: https://www.sigarch.org/simd-instructions-considered-harmful/
you might be able to put a runner on your droplet and let it do the deploy
If you return an enumerable as a yield it will call the function every time you enumerate the object, not just once. It'll do that on a list too, but enumerating a list is effectively free whereas for any non trivial example enumerating a yield is going to be expensive. And again, it's not a temporary collection. C# passes by value of reference. The object you created in the method isn't copied, it's returned. When you're using it later it's the exact same item. That's the whole point, it's not temporary. And no, one at a time is not the same as in sequence. Every object is allocated as you iterate over it and at the end you have created the whole list. It's not just one object created and then disposed. It's a whole list. This shit is complicated. 
Pluralsight has lots of great content for ASP.NET Core
&gt;The compiler will be further optimised to use cpu level instructions leading to faster performance of .net core. If true this is huge, it will blow the JVM out of the water and leave nodeJS even further behind in terms of performance &amp;#x200B; The .NET compilers (Roslyn and RyuJIT) won't do auto-vectorization like some C/C++ compilers do. But some of the low-level parts of the BCL are being optimized with the new SIMD intrinsics. Span&lt;T&gt;, String, and Utf8String already have a lot done, and other things like SIMD-accelerated Base64 encoding/decoding are in progress. &amp;#x200B; &gt;For development of .net core for IOT devices, would this allow further performance improvements? it would also add the potential to directly code with instruction sets on the device, allowing some features to be exposed on the device not possible with current IOT dev on .net core? &amp;#x200B; .NET Core for IOT still runs on general-purpose CPUs, so the instruction sets for those will be standard ARM. But with access to intrinsics for ARM SIMD instructions, the BCL and other apps/libraries can be optimized so they run faster on cheap hardware. &amp;#x200B; &gt;Does this give an alternative option to develop assembly level programming for hardware? Would this have any draw backs compared to c++ / c? in terms of what can be done? c you can program circuts at a low level (for a lack of a better term). &amp;#x200B; It's an alternative to assembly to some extent, but it's more realistically an alternative to the SIMD intrinsics C/C++ developers have had access to for ages. We're just getting that capability now in .NET, so we don't have to P/Invoke native libraries for performance-critical code. &amp;#x200B; &gt;The parser [https://github.com/EgorBo/SimdJsonSharp](https://github.com/EgorBo/SimdJsonSharp) utilising SIMD instructions is essentially going to be the fastest json parser in any framework except c++ / c, potentially other langauges that target cpu instructions. &amp;#x200B; Yep, and it's possible some of that work might end up in the new Utf8JsonReader in .NET Core 3.0. The release of SimdJson came late in the 3.0 dev cycle, but it could still happen. &amp;#x200B; &gt;I havent been keeping up to date with the JVM, are they taking note of what .net core is doing and are they trying to keep parity? &amp;#x200B; I haven't been keeping up with them either, but I'd bet a shiny dime they're working on this.
Not yet, but that will likely happen eventually. &amp;#x200B; The abstraction level of System.Numerics makes it difficult to re-implement using the new SIMD intrinsics without hurting the performance they currently have. Hopefully devs will move away from System.Numerics and toward better-designed libraries using System.Runtime.Intrinsics underneath. As the author of the blog points out, he was able to speed up his raytracer by 7x over the old System.Numerics version with the new SIMD intrinsics and some architectural changes.
A what and what? And what? 
A service layer manipulating anaemic domain models is an anti pattern. As an added bonus, they generally throw away any concept of object oriented programming as well and we end up with a nice simple procedural application. Which as you say, sometimes can be quite pleasant to work with. But it's still an anti-pattern. &amp;#x200B; Currently working on a project that uses extension methods so they don't dirty their anaemic models. Now we are jumping through hoops to avoid OOP?
No, you refactor when needed to avoid ending up with spaghetti. If you can't see that I feel sorry for you.
Your designing an abstraction before you actually know what the problem is. 20 minutes max, and then a lifetime of pain for the project because you didn't quite get it right? &amp;#x200B; When all you have is a hammer.
Hey, hey! I was incredibly condescending and also helped him out... Clean Architecture is pretty interesting and about as far from onion as you can get.
I name it 'Minimal' architecture :)
1. When you don't know what your doing = Explicit / Lazy Loading 2. When you do know what your doing = Eager Loading 3. When you really know what your doing and everyone on your team does as well = Both &amp;#x200B;
Maybe, but Blazor is more about rendering interactive UI powered by C# code instead of Javascript. WebAssembly has more promise though and is already being used by companies to create really complex and resources intensive apps that run in the browser. Once Blazor is done, we might see more polish around the Mono to WASM compiler toolchain so you can build more kinds of apps.
Ok man one last time. Let see if when i said you can avoid some temporary allocation what i mean. If an example of Gist can help. You are over complicating it is very simple if i look at the decompiled code. 1. Here is the link for the [Non Yield Example](https://sharplab.io/#gist:30e4efaccb16b7e57744f491ed66c375) As you can see there is a collection to hold the results of the function execution and this temporary collection can be avoided using `Yield` keyword. In order to highlight the difference i create another gist that show the `Decompiled` code for the `Yield` example. And you can see in the Gist that `Yield` method is converted into a `StateMachine` that implements `IEnumerable` interface and Then calling method get the enumerator of `StateMachine` using `GetEnumerator` and Use move next to iterate through the values. So avoided an allocation for the Intermediate/Temporary list. 2. Here is the link for the [Yield Example](https://sharplab.io/#gist:3a7b041c4cc1768431a79f850e6e81ac) In the second example there was no collection needed to hold results but the state machine was good enough to iterate one of them. If you still cannot see the allocation that we avoided I don't think i have anything more to say. &gt;Every object is allocated as you iterate over it and at the end you have created the whole list. It's not just one object created and then disposed. It's a whole list. But I think the same variable from the StateMachine is used to hold the current state of machine (Result of single execution of function ) old one can be garbage collected. Check second Gist and state machine variable for current state same variable will be used to hold the Function execution result on every iteration. &gt;And again, it's not a temporary collection. C# passes by value of reference. The object you created in the method isn't copied, it's returned. When you're using it later it's the exact same item. That's the whole point, it's not temporary. Not sure what you understood by temporary. Totally out of context nothing to do with discussion. &gt;If you return an enumerable as a yield it will call the function every time you enumerate the object, not just once. It'll do that on a list too, but enumerating a list is effectively free whereas for any non trivial example enumerating a yield is going to be expensive. Yes i said very first time some times the cost of Yield overweigh the benefit of avoided allocations specially when you are returning a small collection you just rephrased even it was mentioned in post if you read post i think you missed it(May be not read the post completely). For some cases it can be more expensive than non yield. I added the Gist to the post also for future users. If you still believe i don't know how yield works and have no idea what i am talking about then i am happy to accept for sake ending this conversation. Because now to me it started to sound like "It's internet so I am the only one who can be right" and i don't see any value in this conversation anymore but you are rephrasing same things again with an illusion of that you are telling me something new. &gt;This shit is complicated. Does not look like from the Decompiled code. It looks pretty clear.
Your examples are an array of primatives, this masks a lot of what's actually going on. And again. **IT DEPENDS ON WHAT YOU DO WITH THE ENUMERABLE**. If you're using a list of object references you're going to create the entire list of objects and they're **not** going to be garbage collected. When and where you use enumerables is complex. It's got nothing to do with yield and everything to do with how enumerables work. Yield is not a bad thing, but it's not universally good either. 
What's wrong with IEnumerables instead of lists?
&gt;Your examples are an array of primatives, this masks a lot of what's actually going on.And again. IT DEPENDS ON WHAT YOU DO WITH THE ENUMERABLE. I was using primitive arrays to demonstrate very first time in Post example and gist show there are allocation benefits and you totally trashed the idea and commented that "You don't know what you are talking about". But now you are saying it is situational and we are talking about same example/situation. &gt;If you're using a list of object references you're going to create the entire list of objects and they're not going to be garbage collected, at least not immediately. Yes not immediately but qualify for the garbage collection can free up if needed by application and result in optimisation of memory allocations. &gt;Yield is not a bad thing, but it's not universally good either I never said it's universal if you read the post it starts with explaining these tips are one size fits all. But still your reaction was like i said it is universal and told everyone should think the same way otherwise they are wrong. I think it is evident with the help of Gist there is efficiency regarding the allocations in the above example by using yield and that was the whole point of the discussion in the post. Just want to request once again never judge too quickly and make statements like "You don't know what you are talking about" there are better way to start discussion around anything and above is not the right one. Because at the end i can see we both were right but you made a judgement too early in the discussion and never tried to understand what i had to say. It's already hard to find any inspiration to share your learning with community comments like these never help specially when it's just may be due to misinterpretation by the reader. I want to apologise if i used some rude words in the discussion.
I responded to how you'd reacted to others, which was aggressive. Once again though, you're not making a temporary allocation. That list **is** your return. It **is** the enumerable. It's **not** temporary. Now in some cases it may be the wrong choice, but it's not temporary. 
My instinct would be to stay away from pulling and building the code again, just because you're having to build the code twice and have to maintain that. I'd lean towards building once, and deploying the build artifacts. Docker would probably be the easiest pipeline there, but any sort of secure copy task that can put the files up on the server would work as your deployment if you want to go with something pretty simple.
&gt;A service layer manipulating anaemic domain models is an anti pattern According to Martin Fowler. I have yet to see him or anyone else provide very compelling arguments\\ though. Usually they misrepresent the anaemic approach by comparing a garbage implementation with a perfectly crafted DDD approach. I have seen DDD go off the rails and become a nightmare more often than the anaemic approach.
IEnumerable can‚Äôt be indexed. IList is going a bit far though - that has an Add method. There is IReadOnlyCollection though. 
I hadn't looked at it in terms of initializing the models like that. You're right, of course. I think I will drop [ASP.NET](https://ASP.NET) then and switch to a desktop GUI using one of the frameworks mentioned, plus a visualization in the terminal.
Thanks, I'll give it a look!
&gt;I responded to how you'd reacted to others, which was aggressive. When you jumped in you made it worse with your judgement and opinions and trashing straightaway. I am never reluctant to apologies if the OP says if i sounded rude or offensive i acknowledge i am new to community and may not understand the basic rules of conversations. What do you think of your reaction was it justified You look like an elite on platform? &gt;Once again though, you're not making a temporary allocation. That list is your return. It is the enumerable. It's not temporary. Again playing with words i literally showed you the code with the allocations that can be avoided and what i meant by temporary. But you still can't agree I don't have anything else to offer. &gt;Now in some cases it may be the wrong choice, Yes it can be wrong choices in some cases that true and mentioned from very first day in blog post which you may missed. You just focusing on choice of words. Not sure why you are rephrasing things from post to contradict it. 
I have done this simply to my raspberry pi using SSH and scp. It's not the nicest solution but it works well enough for my home projects. 
&gt;DomainModelException Yeah I would think that would be the way to go, but the other points are still a PITA. Honestly after years of inheriting other "experiments" I'd say a results object is the way to go every time
I responded the way I did because you're not listening to anyone. Your code doesn't work the way you think it does. Your yield code is going to allocate **more** memory than your non yield code will because it's going to build a state machine. Every single element in your list will be generated, all the information about what the next state is will be stored. The only difference is that in one case you're doing the work up front and the other you're doing it afterwards. The compiled code looks different because the compiler can optimise list operations, and it can't optimise your yield. In most cases you've saved no memory at all, in many cases you've used more memory. There are neat things about yield, but you haven't saved a temporary allocation at all because **the allocation isn't temporary**. You're comparing constructing an array with constructing all the elements of an array one by one. The end result is the same except you can't reuse the second one, and you've added FSM state storage. Yield is really cool, but it's **not** cool for the reasons you think it is. 
?? What is this overkill? What about none of this: Index secondToLast = new Index(2, true); int[] array = new int[] { 1, 3, 5, 7, 9 }; var x = array[secondToLast]; And more of this: int[] array = new int[] { 1, 3, 5, 7, 9 }; var x = array.Last(2); Or: int[] array = new int[] { 1, 3, 5, 7, 9 }; var x = array.First(2); Just give the existing LINQ operations an optional positional argument? These are clearly commonly used LINQ operations?
What's right about it? Hiding a list inside an IEnumerable offers zero benefit to the consumer of the method. Why return a list at all if you are not going to allow anyone to take advantage of it? You don't see the LINQ library passing around lists disguised as IEnumerables. No, it either has something that is actually streaming or it returns a concrete List or Dictionary typed as a List or Dictionary. 
Ah yes, I worked on several projects where they confused "returning a read only list" with "returning a mutable list temporarily disguised as a IEnumerable". I want to stress the word "temporary" because any late binding context, for example WPF, pierces the illusion. &gt; Why are all the read only lists being rendered as mutable by the UI? &gt; &gt; Because the developer writing the repository doesn't know WTF he's doing. 
&gt;I responded the way I did because you're not listening to anyone. So you are saying you was right for that kind of response? &gt;Your yield code is going to allocate more memory than your non yield code will because it's going to build a state machine. If collection returned is small in size it will overweight the optimization of using yield i already mentioned in article and now i am pretty sure you never read the post because so said same things as post atleast 4-5 times now or you are using stuff from my post to contradict it. &gt;Every single element in your list will be generated, all the information about what the next state is will be stored.The only difference is that in one case you're doing the work up front and the other you're doing it afterwards.The compiled code looks different because the compiler can optimise list operations, and it can't optimise your yield.In most cases you've saved no memory at all, in many cases you've used more memory.There are neat things about yield, but you haven't saved a temporary allocation at all because the allocation isn't temporary. If you can code. Let your code do the talking and write a blog post about it and share benchmarks otherwise all of this wall of text means nothing to me because all do you do is talking &gt;Yield is really cool, but it's not cool for the reasons you think it is. Sure i agree it's internet and you are the only person who can be right . 
I call it ‚Äúincompetent dev‚Äù. Don‚Äôt reply 3 days after the fact. 
You have to write the extension your self? And I trust the C# language doing it for me more than myself? Because me doing it means extra time, extra unit tests, extra maintenance
What resx file are you storing it in? And are you modifying the generated designer code? This will get overwritten when hinges are made.
Try to actually read what people are telling you. Your code doesn't do what you think it does. Your own example doesn't show what you think it does. All I'd post is exactly what you did, and you still wouldn't get it. 
pm me, I can show you how I deal with img loaders in my apps
Sure 
Try the same step with an object not a primitive. You'll see the exact same memory allocations, except for the list, which one more time **is not temporary**. You don't get to redefine temporary to make yourself right. The list **is** the IEnumerable. The yield **still** generates an IEnumerable. It just generates it through a FSM. Effectively your memory allocation difference is going to be the overhead of a list vs the overhead of the FSM and the list is **lower**. 
Sure
&gt; Why are all the read only lists being rendered as mutable by the UI? Because the UI is trying to make assumptions that didn't exist in the statically-typed environment that the repository was probably written for. IEnumerable *is* read-only in what it offers, and its underlying type is not the intent either.
This you see is why people are shitting on your blog, because you're not actually trying to learn or understand what you're seeing. Plenty of people have posted about this to your but you know better. 
Sure 
So the three types are: Lazy: Related entities are loaded when you try and access them. Eager: Related entities are loaded when you load the parent entities. Explicit: Related entities are only loaded when you say "Load!" Lazy is fine if you rarely need the related entities. If you almost always access the related entities there's an overhead because you're constantly going back to the database. It's also harder to handle errors. If you lazy load a related collection and the DB is busy, you now need to have db error handling code in any method that might trigger a lazy load. Eager is great if you know you're going to need the related entities. Just get the lot in one hit and you're golden. This is my preferred mode because I pretty much always need the related entities and error handling is easy. Explicit simply gives you control over if and when the related entities are loaded. Imagine you have a table with 100 parent entities and each has 1,000,000 related entities. You might want to pull back the 100 parents, look at them and then only do the expensive load on the parents that have related entities you need. You can wrap this all up so it looks like an Eager load, with the same one off error handling. You may be able to Eager load simply by using a better where clause, but sometimes you need to get the entity before you can decide if you need the related entities. Contrived example, you load the parent record, pass various fields to a web service that calculates a statistic, you then look at that statistic, compare it to the time of the day and then load the related entities if you get a match. 
Is it only for Intel and arm CPUs? 
As mentioned in another comment no need to build twice. Use SCP or SFTP to copy the built files to your target host, then use SSH to finish up the deployment, e.g. take service offline, copy to deploy folder, configure, start services. After you have that down consider putting a load balancer in front of your API and then do a green blue deployment so that the service is always online. 
I have class1.vb containing the designer and non-designer code, and a class1.resx file that contains the image data. I am modifying the designer code just to test whether it works...I need to figure out why the auto-gen code messes with the ImageList stuff.
I‚Äôd say that returning IEnumerable tells the caller ‚Äúthis supports enumeration. If you cast it to List and start doing other stuff, that‚Äôs on your head‚Äù. For a grown-up that should be enough. I mean in the end reflection let‚Äôs us call private methods, but we don‚Äôt say that marking your method private isn‚Äôt good enough to stop people calling it. Hackers can deal with their own fallout. 
The resx file is built by the designer and will be reset the same as cs file. Try adding a new resx file that isn't associated with the designer. It shouldn't get modified then.
Either use something like Cypress of Selenium. Run the host and then directly test against the DOM.
It's for cpus that implement the relevant instruction sets. Amd's x86 cpus support much of this too.
not sure if you are trolling or being serious
This sounds like a Job for `ConcurrentQueue&lt;T&gt;` &amp;#x200B;
IIRC there are WindowChrome classes floating around the net which support .NET Fx 4.0 ( [https://www.nuget.org/packages/Microsoft.Windows.Shell](https://www.nuget.org/packages/Microsoft.Windows.Shell) )
I don‚Äôt know if this is the answer you‚Äôre looking for but this pattern is called dependency injection. 
There is a big warning at the top of the designer generated files not to be modified as they are automatically generated and any changes you make can get wiped out. Go into your project Properties and use the Resources tab. Add your resources there. A dedicated .resx file for your use will be generated. In VB.NET code IIRC you use My.Properties.Resources object to access these resources from your code. You would do this in the form constructor after InitiailizeComponent is called.
Have you never heard of interface sniffing? Even the LINQ library does it, for example when checking to see if it can find a faster way to perform a count.
That's a technique being employed but it's definitely not what's being asked.
Detecting the capabilities of an object by seeing which interfaces it supports is not "hacking", it's standard practice. If you call .Count() the first thing the LINQ library does after the null check is try to cast it into an IList so it can access a Count property. If you actually want it to be read only, just make it read only. It's a trivial change to throw a ReadOnlyCollection wrapper around the list. It doesn't even require making a copy of the underlying array. 
The alternative is to build it self contained targeting Ubuntu or whatever your droplet is running and using SFTP to transfer it over. https://www.digitalocean.com/community/tutorials/how-to-use-sftp-to-securely-transfer-files-with-a-remote-server
Here's my thoughts: I would rename this: ISupportCaseProvider to ISupportCaseHandler - I am assuming this is the implementation of something which handles a support case?. I also renamed the CanGetSupportCase to IsValid. I would rename this: SupportCaseService to SupportCaseProvider - Since this is the class which is going to deal with giving you all of the specific SupportCases you wish to invoke. ``` public interface ISupportCaseHandler { bool IsValid() SupportCase GetSupportCase(); } public class SupportCaseProvider { private readonly IEnumerable&lt;ISupportCaseHandler&gt; _handlers; public SupportCaseService(IEnumerable&lt;ISupportCaseHandler&gt; handlers) { _handlers = handlers; } SupportCase GetSupportCase() { // psuedocode warning! - Not tested for compilation _providers.Where(x =&gt; x.IsValid()).GetSupportCase(); } } ``` The consumer of the SupportCaseProvider class should be responsible for the execution. The SupportCaseProvider should only be responsible for the retrieval of the SupportCases. 
Yeah, I figured but I wanted to give it a go anyways. I don‚Äôt get to have conversations with developers about this kind of stuff so I don‚Äôt what kinds of questions people ask. I‚Äôll have the right answer one of these days!
I use a single node docker swarm, with gitlab CI in two stages - My gitlab CI builds a docker image and pushes it to the private gitlab container repo - The deploy stage uses DOCKER_HOST to connect to the target swarm node and deploys the application stack. You could also look into doing Kubernetes, which DO has pushed out to some degree. That being said, I don't currently do unit tests (I need to, I know.) And I'm also using gitlab self-hosted, not the gitlab.com, with my own runners setup.
I will check you guys out. As a solo junior life can be rough sometimes heh.
Thanks for your input! I sort of like the idea of viewing the various implementations as handlers. That seems like a more semantic way of thinking about their purpose (to potentially handle a request for a support case). The decision to rename the "test" method to "IsValid()" is interesting, though. Any reason in particular for that? Each handler's state is valid from its own perspective, it's just that each request for a support case is targeted at a different business unit. Each handler is tied to a different back-end system used by the business unit in question (think one for Salesforce, another for Zendesk, etc). My "CanGetSupportCase()" method in practice would accept a few parameters, such as the business unit ID, for example. That way the handler can determine wether or not it is capable of servicing the request. &amp;#x200B; Anyway, I was just curious what the thought process was, that's all. &amp;#x200B; Thanks again!
No worries! I appreciate the input regardless. I'd hate to miss out on a nugget of knowledge because someone was afraid to join the conversation.
I‚Äôm always wanting to learn docker, and scenarios like you‚Äôve described interests me even more. Just haven‚Äôt grasped the concept yet. :(
Self contained is what I‚Äôve been using. 
Ah okay, that naming makes more sense. To be honest, it was more because I thought CanGetSupportCase is very wordy. Maybe Servicable, or Supported would be better? &amp;#x200B; IsValid comes from the Specification pattern but I adapted it slightly (IsSatisfied)
I prefer Linq2Db
LINQ isn‚Äôt business logic, I should have made it clear that‚Äôs what I was talking about. Obviously all bets are off in scaffolding code. All kinds of things are ok there. But in business logic, it‚Äôs just breaking KISS. Funnily enough the Count() example occurred to me writing that post. Business logic that was trying to cast things to different interfaces would be a pretty bad smell if I was reviewing it. That sort of thing is for tooling, the other side of a line between code that‚Äôs simple and expressive, and code that‚Äôs clever so that the caller doesn‚Äôt have to be. In business logic, if I return you an IEnumerable, and the you find out the object is a list, and you cast it to one and mess with it, and I later change the library so that the returning method now works with yield, your code will stop working (at runtime no less) because you decided to break the contract. That‚Äôs the kind of fallout I‚Äôm not interested in hearing complaints about, because I never promised you a list. 
No there's no 'open' SMTP services...because there used to be an they were abused massively. I'd suggest looking at either including \*everything but the username and password\* config for GMail and/or Write to disk (some sample code I wrote below). Same with FTP...that will need some config and it's best to let a user do this themselves (with sample / instructions). `private void SaveMessageToFile(MailMessage message)` `{` `var client = new SmtpClient("mysmtphost")` `{` `DeliveryMethod = SmtpDeliveryMethod.SpecifiedPickupDirectory` `};` `var mailDirLocation = Server.MapPath("~/App_Data/PremailerEmailHTML");` `if (!Directory.Exists(mailDirLocation)) Directory.CreateDirectory(mailDirLocation);` `client.PickupDirectoryLocation = mailDirLocation;` `client.Send(message);` `}`
Which ones are good for advanced development?
Any of the deep dives into a specific technology, Julie Lerman has multiple courses on EF Core 2, there are courses for things like Signal R, Razor and xUnit. Most of the DDD path uses .NET in it‚Äôs examples (The CQRS one is particularly well done)
&gt; Business logic that was trying to cast things to different interfaces would be a pretty bad smell if I was reviewing it. Yet that's what we frequently have to do when people unnecessarily return a list typed as IEnumerable. &gt; if I return you an IEnumerable, and the you find out the object is a list, and you cast it to one and mess with it, and I later change the library so that the returning method now works with yield, your code will stop working (at runtime no less) Why aren't you using yield in the first place if it is more appropriate? Changing the runtime characteristics can be a really bad thing. If I'm expecting a stream and you give me a list, you can blow my memory budget. If I'm expecting a list and you give me a stream, and I am enumerating it under a lock or a UI thread, it can hang the whole application while it does the work I thought was already done. Or if I'm using a repository, I may dispose it and the underlying connection before enumerating because in the pervious version that was safe. Only now its going to fail. *** You say "keep it simple". Well the simplest thing is to just be explicit about what you return. Casting everything to IEnumerable makes the client code more complex in exchange for the very questionable ability to change rather important assumptions. Or in other words, it's a premature generalization. 
I think I will learn docker and set it up. So what exactly would I need in my docker file? I currently have Nginx setup as a reverse proxy and I will also use PostgreSQL as my DB. So should I setup a docker compose with Nginx, PostgreSQL, and my app? Thanks!
async/await applies mostly to doing something in the code and then waiting for the result. Normally if you just loop in place waiting (the simplest way to do it without async/await) the thread can't do anything else. Using async/await allows the async engine to know it can take the thread and deal with it later, and run something else in the meantime. But your debugging experience is the same as if you had synchronous code. I am not sure how that would work if SPs, if at all, since each time you run code on a database its going to run on its own thread and it's not really going to be doing anything that needs async/await... all its operations should be on the database so the only waiting is for locks and so forth (and if you have to wait long for those that is a completely separate problem). I don't know if async/await would be useful here. I am guessing you mean async/await in the C# side, which would have 0 impact on the SP itself since it is not in C# and it is doing its own thing. So probably why the guy reacted like he did. It would only help improve things on the C# side.
Im asking because the official docs say System.Numerics types are SIMD-enabled. [https://docs.microsoft.com/en-us/dotnet/api/system.numerics?view=netcore-2.2](https://docs.microsoft.com/en-us/dotnet/api/system.numerics?view=netcore-2.2) So what does SIMD-enabled mean if they dont use intrinsics?
No, YAGNI isn‚Äôt a thing because you can never be sure your requirements won‚Äôt change. It‚Äôs almost a guarantee that they will for any project that‚Äôs going to see use. You condescending moron. 
I‚Äôm not suggesting you should do it yourself but that adding the extension method (by the BCL team, not by you) makes more sense than adding a whole compiler feature. Just like we don‚Äôt have a compiler operator for first last take skip etc
Well I can tell by your defensiveness and out of the blue attacks towards me that you‚Äôre either not very confident in your skills, or you‚Äôre overconfident and maybe a bit junior and haven‚Äôt learned about the nuances of architecture and how your approach can fuck you or just waste time. Plus, you‚Äôve completely misunderstood my point every time I‚Äôve explained it to you and honestly haven‚Äôt displayed any signs that you even remotely understand what I‚Äôm saying. So seeing as I‚Äôm clearly not speaking to the next Linus Torvalds, I feel completely comfortable telling you that I‚Äôm fine with you thinking I‚Äôm a moron and that I, now condescendingly, hope you become a better developer, or at least less of an asshole. You still need people skills as a dev, man.
More important than people skills is to have the remotest clue of how to develop. You have a loooooooong way to go before you can claim any sort of competence. I can take solace in the fact that you‚Äôll never pass the interview to work on any software I‚Äôll ever care about. 
This is great. Thank you!
&gt; Yet that's what we frequently have to do when people unnecessarily return a list typed as IEnumerable. Yep, sounds like a smell then. Code that casts a list to IEnumerable only for the caller to cast it back to list? How terrible. If the result needs to be a list then it needs to be a list. &gt; Why aren't you using yield in the first place if it is more appropriate? Are you seriously asking why the implementation of a method might need to change? Yield was just the first example that came to mind so that casting to a list would fail. An array instead of a list would still implement IEnumerable and would still fail on casting to a list. &gt; If I'm expecting a list and you give me a stream If you call a method that returns IEnumerable and you‚Äôre expecting a list, there are problems, either with your code or the library. Probably the library, if a list makes sense. Your other examples do make sense and are good reasons to be careful. &gt; Casting everything to IEnumerable makes the client code more complex in exchange for the very questionable ability to change rather important assumptions. Interpreting my post as ‚Äúyou should cast everything to IEnumerable‚Äù is pretty wild. Contracts should be explicit. Returning a list when one isn‚Äôt needed is giving the caller more than they need, for the questionable reason that your objects should always have as many capabilities as possible. Anyway I just realised you‚Äôre the same guy I had this same conversation with before, the guy who doesn‚Äôt want to think in abstractions. It‚Äôs fine, we‚Äôll never work together, don‚Äôt worry about it. 
Oh no, I won‚Äôt be able to work on your 5-sprint CRUD app with 7 levels of abstraction that are there because ‚Äúwhat if, in the future...?‚Äù architect astronauts designed the thing, which has to be mostly rewritten anyways because the project is pivoted in a direction you weren‚Äôt anticipating. I hope we can both mourn this loss together, you and I.
I'm not using Nginx but this is pretty close to your setup. This sets up an api and a SPA react application with postgres and watchtower. Watchtower will poll docker hub for new docker images and automatically install it. This only gets your running if you don't care about your database sticking around. I think most people should probably use a managed database otherwise it's kind of a pain to create a persistent volume and make all of that work. This is a start though. &amp;#x200B; [https://github.com/codenesium/samples/blob/master/PetStore/DockerFile](https://github.com/codenesium/samples/blob/master/PetStore/DockerFile) &amp;#x200B; [https://github.com/codenesium/samples/blob/master/PetStore/docker-compose-postgres.yml](https://github.com/codenesium/samples/blob/master/PetStore/docker-compose-postgres.yml) &amp;#x200B;
 You could check for empty Config values in the app.config/web.config and inject fake implementations of wrappers around the SMTP client etc. Then if someone wants to test it properly they provide there own details for SMTP etc. And it just works 
What version of VS/ VB are you using? IIRC, those mixed designer / code files were phased out in VS 2005 (.NET 2.0), so any sample code that uses them is going to be super dated.
Thanks I will check these out when I‚Äôve got time and implement it!
Thanks! I will check these out later when I can and implement it.
It means they *are* intrinsics. Some of those methods are hard-coded in the JIT to emit SIMD instructions. There are a couple of problems with that, though. * Only *some* are SIMD-enabled. For example `Vector4.Min` and `Vector4.Max` are, but `Vector4.Clamp` is not. * The abstraction of the types from the underlying SIMD vector types makes some of their implementations much less efficient than they could be. For example, loading a `Vector4` is a single instruction, but loading a `Vector3` is three instructions. Ideally, those intrinsics could be removed from the JIT and re-implemented in managed code making use of the new direct-mapped SIMD intrinsics, but even that is a challenge because of the abstractions and the fact that the JIT has special knowledge of those types currently.
On a less asshole-y note, the only reason I continued the conversation even though you were being a dick is because I used to be you, telling people that the hallmark of a good developer is having ‚Äúflexible designs‚Äù. Which is true, but a lot more nuanced than you‚Äôd think. I used to create these grand abstractions when there was no need. What if, what if, what if? I‚Äôve been there. Build enough to complete your AC for the current feature and have clean, testable code. Refactoring the skinny design you just implemented will be 100x easier than choosing the wrong abstraction and refactoring that.
that simdjson is crazy fast
I've played with Core 2.2. I've read that 2.2 isn't a LTS release. So I wouldn't use it for production (my opinion). I've been using .NET framework for like 10 years and it is battle tested. &amp;#x200B; I'm waiting on Core 3.0 and I'll start migrating over
I suggest you to use [Pdf library](https://www.officecomponent.com/products/pdf) by ComponentPro. It is higly reliable and easily useable in .net and .Net Core. It has built in methods to convert [html to pdf](https://www.officecomponent.com/products/html-to-pdf), pdf to word, word to pdf, image to pdf, [pdf to image](https://www.officecomponent.com/products/pdf-to-image), and much more. You can use it to generate pdf from Templates easily. I think it works best for you. 
&gt; I've read that 2.2 isn't a LTS release. So I wouldn't use it for production (my opinion). Then use 2.1, which is LTS. For new production web apis, I would go for .NET Core 2.1 or 2.2
SOLID is all that really needs to be said to your little outburst. Sorry you learned everything you know about architecture in the 2000s and can‚Äôt be bothered to stay current. Now we actually care about putting things together properly, not just rushing them out the door to save yourself a few minutes of much needed hassle. If you look closely you can see your grasp on your career slipping at an ever increasing pace. 
which will have old APIs...