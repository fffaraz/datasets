It's about concepts: projecting an entity graph to DTOs requires a mapping layer of some sort (either you write it yourself, or use e.g. automapper). The DTO graph isn't equal to the entity graph, even if they look alike: they're conceptually different. So there's nothing to 'avoid': if you cross boundaries through projections, you need tooling to do so; avoiding the tooling means you can't cross the boundary. It's that simple. &gt; Writing mapping code and tests is quick and robust. What's 'quick' in your book? A toy app has perhaps a couple of entities, screens and viewmodels. It's not that time consuming to write mapping code by hand for that. But if things are bigger, you will end up writing a LOT of code, which is plumbing code, i.e. better done by a computer than manually by a developer. No idea why one would opt for writing plumbing code if tooling can do that for you... 
Often, in a job interview, I ask the interviewee when it makes sense to use WebForms for a project. The ones that reply that it never makes sense usually dont last long. Engineering has no space for absolutisms. Despite all the quirks of the posts above, in a graph that has project size on its X axis and time to market on its Y axis, WebForms has the lowest point for small projects, say a simple crud site where each page is part of a master/details view. Obviously, when the project starts growing in size, the TTM skyrockets pretty soon and the entry price of MVC starts to make sense and pay off. But until that moment, a gridview, a listview and a repeater, or a formview paired to a datasource will kick any controller, its view and the model in the nuts every day of the year. But saying that WebForms has still a niche use case is like advocating for the superiority of WCF over any other out-of-process communication technology... ;)
I could imagine some scenarios where you might have somebody attempting a Web Parameter Tampering attack and blind automapping of data form the input parameters/view model to the back-end entity could allow this attack to be successful. https://www.owasp.org/index.php/Web_Parameter_Tampering Imagine you had an commerce site where new customers would create new instances of themselves (say a new Username, Name, and Address columns in the Customer table) but another administrative page would be able to update the AccountCredit field of that same Customer table. Well, if I knew to try it, I might try to set that AccountCredit field to $5,000,000 with my post data, instead of letting it initialize to the default $0 value. Now if you were manually only copying over fields that were valid for this operation, you'd be fine. But if AutoMapper were to copy over this overly-populated view model, then you could have a problem.
It doesn't scale, it does AJAX really poorly, and it has a really leaky abstraction that hides the truths of the web from a developer. Where we work, we've had a number of developers unwittingly introduce gigantic problems into the code without anyone realising it. Suddenly page sizes are up into the tens of megabytes and access times are over a minute as soon as it gets deployed into production. You can't do lightweight scalable web with webforms unless you tear so much out of webforms that you end up asking yourself: Why am I even bothering using it at all? It was a cool concept 15 years ago when 90% of developers only new WinForms, but ultimately, nearly every newer concept is far better these days. 
You will need a single list that contains both the XML data and image paths. The general steps you need to follow are: 1. Create a class that contains a property for each of the columns you want to display. 2. Look up how to read XML into a list of objects. 3. Iterate over each item in the list and add the image info. 4. Set the DataGrid source equal to the list.
Having done Web Forms development. If you are coming from desktop development moving to web forms development keeps a lot of things similar -- Drag and Drop UI development. Server Controls with properties and binding. If you are writing an intranet application -- for desktop use. WebForms is not a bad way of going. In the end it comes down how much effort does it take to develop, deploy and maintain vs how much benefits does it provide.
just use your devessentials: https://www.visualstudio.com/dev-essentials/ https://docs.microsoft.com/en-us/azure/app-service-web/app-service-web-get-started-dotnet?view=azure-dotnet
Thank you for the reply, I just got everything working without lists and classes etc which is what I was trying to do from the get-go. It is super simple once I looked at it. Thanks again everyone for the tips. New XML &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt; &lt;PotionInfo xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt; &lt;Potion&gt; &lt;Item&gt;Potion of Healing&lt;/Item&gt; &lt;Cost&gt;50 gp&lt;/Cost&gt; &lt;Weight&gt;0.5&lt;/Weight&gt; &lt;Details&gt;2d4 +2 hit points, takes 1 action.&lt;/Details&gt; &lt;Qty&gt;0&lt;/Qty&gt; &lt;ImageURL&gt;Images/i_potion_of_healing.png&lt;/ImageURL&gt; &lt;/Potion&gt; &lt;Potion&gt; New XAML &lt;DataGridTemplateColumn Width="84"&gt; &lt;DataGridTemplateColumn.CellTemplate&gt; &lt;DataTemplate&gt; &lt;Image Source="{Binding XPath=ImageURL}" Width="74" Height="63" /&gt; &lt;/DataTemplate&gt; &lt;/DataGridTemplateColumn.CellTemplate&gt;
After your edit I see what you mean now. It would be great if you weren't forced to read bad translation if you also know the native language the content was created in. There are a lot of countries where English isn't the primary language but is intelligible by nearly everyone. I'm guessing, depending on the quality of the translation service and quantity of idioms, people might find it easier to switch to English. That's probably a more common case than people ordering cheap Chinese electronics to play with Arduinos and Raspberry Pis!
Main problem with webforms is/was a hack job to bring desktop development/apps to the web (and easy transition for pre .NET devs which were main dev audience when it released) Basicly like hammering a round peg into a square hole, hit it hard enough it will go in, but it will never be a good/best fit.
Web Forms aren't *inherently* bad, but they do promote the usage of some outdated concepts. You can certainly create modern websites with them, even implement MVC on it (I've done so). The main things you need to keep in mind though are: 1. Disable ViewState. Try to keep your website as stateless as possible. 2. Don't use Web Controls, because they output extremely bloated HTML+JS. Instead output plain HTML. 3. Avoid using control event handling and instead implement methods that react to different request types (GET, POST ... etc).
It depends which parts of WebForms are you using. The more elaborate databinding and grid things? They're a complete trainwreck. The backbone of its databinding system is the FormView, which is a goddamned trash-fire from top to bottom.
Sounds like stuff is listed in the NuGet packages.config, but the references aren't actually set up to use them. I would use the NuGet Package Manager Console and reinstall the packages: Update-Package -reinstall If you still have some missing references after that, I would open the .csproj in a text editor, to see if you have any other non-framework references with a ReferencePath other than the packages folder (meaning they're not set up through NuGet). If you do, install them from NuGet - it will replace the non-NuGet references with NuGet ones.
Yeah your last point is a good one.
A slightly updated version of that OpenShift post, except via command line only, is available: https://access.redhat.com/documentation/en-us/net_core/2.0/html-single/getting_started_guide/#gs_dotnet_on_openshift
Are you required to use ASP.NET WebForms? You can accomplish using MVC in a manner that will allow you to more easily dictate the HTML that is rendered. If you must stick with WebForms, then maybe consider writing your own WebControl.
&gt; Are you required to use ASP.NET WebForms? unfortunately. I've tried putting each set of radio buttons in a div (e.g. div_q1) and using: For Each ctl As Control In div_q1.Controls If TypeOf ctl Is RadioButton Then If DirectCast(ctl, RadioButton).Checked Then ????? End If End If Next It seems to cycle the radio buttons as intended but I can't trigger that If conditional, even when checked
&gt; and it has a really leaky abstraction that hides the truths of the web from a developer. A few years ago, I had to explain to a "rockstar" developer that checkbox controls don't send anything if they're not checked (not even their name) and outright linked to the HTML 4.02 spec to prove it. He didn't know that because it's exactly the kind of thing that Webforms papers over. Granted, a lot of other frameworks paper over it as well, but at the time I was doing Java web development with old school Servlets and JSP, neither of which do that.
 Do you have the runat=server attribute in your inputs? E.g. &lt;input type="radio" runat="server" ... /&gt; 
I wrote a tutorial on my blog about a year ago [here](http://coderscoffeehouse.com/tech/2016/08/19/real-world-aspnetcore-linux-example.html). FWIW Digital Ocean are peanuts ($5 a month starting for a server) and they have built in ASP.NET Core support (you can create a VM with Core from a template).
Heroku has .net core containers, and a good guide
Yep. Turns out the issue was 'radiobutton' which instead had to be HtmlElementRadioButton or something
GteFinancial, a large regional credit Union is written in aspx. So, obviously, scale, security and look and feel can be achieved in web forms.
Like any tool, webforms can be used for good or evil. It looks like you're doing good things. If you're not evil with it, you'll probably be productive for awhile. Unfortunately Webforms doesn't age well, and I'd be surprised if you're not reworking them in a year or so. Gridviews are my personal hell. Our offshore shop embedded gridviews inside gridviews inside gridviews just to get an accordion control. I want to vomit every time I see them chug down the page with massive ScriptResource.axd files grinding up the bandwidth.
A good alternative to DO is vultr. They offer the same as DO but at half price. So $2.5 for the exact same specs.
Goodness me, that's cheap! I thought $5 was hard to beat, thanks for sharing :-)
Had to add a new page to an older WebForms app in the last couple weeks. Not only was it a pain in the ass coordinating development with another dev because there's poor separation of concerns (among other non-technical reasons), the whole thing was just archaic to build compared to more modern frameworks and patterns. The whole time I'm building it out I'm thinking, "This would be so much easier, efficient, and faster if we could build this out as a .Net MVC and/or React app." I've been blitzing some React learning lately and man oh MAN would I have loved to build things out using it and Web API instead of WebForms.
Even on intranets page size can be an issue - we have a page that can generate over a megabyte of viewstate. The postback on that is... painful.
Being a "large regional credit union" is no guarantee that they've managed to implement either scale or security. I will grant you that the look and feel is reasonably modern despite using WebForms.
The best tool is the one that works for you.
Except when you can't use that tool because the app is old and the new one has no direction in development so it's taking forever but you still have to add new functionality to the old shitty app which will then need to be ported over to the new app whenever people finally get around to realizing the old app is a pain in the ass to work with and takes far longer to add new features to without breaking other things because it's been pieced together for nearly a decade often by people who don't really care about standards and best practices and it just depresses me.
Do you actually need that much? You can disable it for various things that will not be needed.
&gt; Or do most people just create bad Web Forms projects...? Yes, that is exactly the reason it's 'bad'. You can certainly hack up a project in web forms relatively easily, and if you know what you're doing, you can make a quite reasonable project using web forms. The 'bad' thing about web forms is that the defaults are misleadingly 'ok' when you only use them for trivial things. ...but once you get to larger more complex situations, they fall apart, and if you've never sat down and suffered through a webforms datagrid problem, it might seem... totally fine to just drop it in. Have a read of: http://www.werkema.com/2014/11/11/asp-net-webforms-without-the-suck/ Notice under the 'WebForms: The Bad Parts' it's not that the features are *bad* (ok well, some of the controls just are flat out bad, but lets ignore that for now), it's that they are *abused*, and that leads of results which are bad. I mean, you could say the same thing for php; is it fundamentally bad? Or do people just abuse it, and *thats* bad? Or does the fact that it makes it easy for people to abuse make it *fundamentally bad*? I don't know... but I can tell you this: I've worked on web forms projects, and they have been, bar none, the worst projects I've ever worked on; that's why it gets a lot of hate. ...but hey, if you're the only one working on it, and you're keeping everything under control, I don't think you should somehow feel bad about the fact you're using webforms. You don't have to use angular4 and the telerik ui libraries to write your frontend; it's ridiculous overkill for some things. I'd say practically speaking, my personal preference is to *decouple* the frontend and the backend, because it makes it easier to migrate to a different backend and faster to iterate on and test the frontend. With webforms, that decoupling is very difficult; but tangibly, that's the only reason I would say 'dont use webforms' to an experienced developer, and it's totally just a matter of preference. From the sounds of it you know what you're doing. Don't worry about it~
Thank you for your answers. I'll be checking them one by one.
Webforms was a disaster that was meant to transition traditional ASP developers into a more OO design paradigm and using .NET. On top of some broken ideas that should have never been (ViewState? stop it, just no!). It sounds like your implantation may be closer to what MS intended developers to do. Most Early webforms developers transitioned from ASP and just kept writing/training code like they did in ASP land. This became an issue long term as component developers started building products based around a wrong-headed use of the platform. This makes using 3rd party extensions server side hit or miss with regards to how much effort you need to put in to just "drop it in". The truth is that ANY platform allows for bad code and patterns. The Webforms platform simply suffered from a lot of users that created bad code and microsoft did very little short of some training and really terrible docs (MSDN used to be a massive mess, it still kinda is) to try and give incentive teams to build code in any other way. 
Why hate inbuilt tables if the layout works fine? They'll be converted to named Grid cells next year when IE gets on board anyway. Better not to convert everything to Div's then back to Grid (like tables) again.
The vast majority of problems I've seen with web forms is giant monolithic pages that do the world. This is entirely a design flaw of the teams that built them. I think you could have some pretty darn clean web forms of your did it right.
Another (cleaner) avenue you may want to explore is to ditch ViewState (and the runat="server" attribute) completely. Just make a normal HTML radio button list. In the Page_Load event when IsPostback is true (i.e. not a GET request) and check for the value of Request["name_of_you_radio_button_list"].
You could always set the RepeatLayout property on the RadioButtonList to something other than table. https://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.radiobuttonlist.repeatlayout.aspx
&gt; The ones that reply that it never makes sense usually dont last long. Engineering has no space for absolutisms. Where have you been at all my career? God, I get so sick "always" and "never" use this or that. Every technology has a trade off - every single one, it just depends on which trade off is important to you. Here, have an upvote. 
Have you checked out IdentityServer4? I can't say for certain that it can meet all your needs out of the box, but you can certainly customize it to fit most needs.
I'm heavily invested in WebForms. I'd like to do more MVC, for my resume mostly, but WebForms makes sense with certain applications. I have complex applications with literally a hundred dynamic data driven pages, often with dozens of controls on each page, where time to market is everything. Yes, you have to be skilled at WebForms to scale it. Yes, you have a lot of power and can blow your foot off with 10MB viewstates. Know those things going in, offload Viewstate to a Redis server to minimize download size, put in debug statements so you're always aware of the viewstate size of every page while debugging. Better yet, turn viewstate off by default, and enable it only on controls you really, really need it for. Turn off Session state if you can. I don't like MVC's pattern that for one relatively simple page with couple of controllers,views, and crud statements to half a dozen tables ends up being a stupid number of files, which spirals out of control when you have a couple hundred pages, and even more database tables. Every tool has a job it's meant for - just know your tools, and know when it's time to switch. 
No, we don't, but the team responsible have other priorities. The size varies based on configuration, so only a subset of users have it *that* bad.
Nice! This might be exactly what I am looking for. I will check it out tomorrow.
The amazing thing is that this is like the smallest of issues that WebForms abstracts away. There's so many other issues that WF hides from developers that they can blow their feet off if they aren't familiar with them. Bottom line: WebForms does a bad job of pretending to work like WinForms and results in unscalable buggy code as a result
_db.Using will dispose an object that it doesn't logically own. If I call GetAll() twice, the second is accessing a disposed object, very bad IMO. The lifetime of _db here is defined by whatever owns _db (the instance, etc) - that and only that should dispose it. Note that dapper automatically opens and closes connections if it needs to, so you shouldn't need any wrapper code. The only thing you should need is a Dispose/IDisposable on the thing that has the _db field.
Don't be down. Most web applications have some form of SQL (or other database) interaction, so you are in a good starting place, that's where I came from. Have a look around for roles where you could be working on the back-end development to start with, where you will be working with both the .net and SQL, then try to learn the HTML/CSS once you are comfortable with it. 
Thanks a ton Marc, much appreciated! &gt; _db.Using will dispose an object that it doesn't logically own. If I call GetAll() twice, the second is accessing a disposed object, very bad IMO. `_db`creates a new connection for every query. I should use `GetNewConn()` for clarity. This is the repo that contains the _db field, with the full ASP strategy: [see other comment](https://www.reddit.com/r/dotnet/comments/6wdoyn/how_to_properly_register_dapper_on_net_core_2_di/dm87b7a/). Does it make sense in this case to use `Using`? I understand that no `Open` or `Close` is needed, since Dapper automatically opens and closes connections if it needs to. PS: I don't know if there is an "official post" with "dapper's gold standard" regarding this issues (specially for someone like me with a non-CS background).
Thanks man! Seemed to get one step closer at least. Now it says: The specified task executable location "alotoffolders\csc.exe" is invalid.
And this is the only correct answer.
I am actually looking to use IdentityServer4 for the OpenID/OAuth implementation, or at least the foundation. What I'm trying to figure out is how to handle scope/permission advertisement and the flow of a user in accessing an instance in the system, as it seems like there is more steps than typical. First authenticate, determine assigned instances, then authorize for an instance via oauth for it's unique permissions information. But this workflow seems odd and I'm concerned I'm incorrect on something, or thinking about it wrong. I'm more looking for help on the conceptual part. From there i think i can determine the implementation.
Then it doesn't work for you and therefore, isn't the best tool.
Well evidently it doesn't work for you then, but there are some people and situations it does still work for.
No, it doesn't work for the business because they don't work with or build stuff into the app, and don't realize how much longer it takes to add features they want as a result, but they still want those features post haste. An updated app is apparently being worked on, but little to no resources are devoted to it. Like it's existed for nearly two years with barely any progress made. Coupled with my raise being held up for five months because they want to implement a more formal process for handling raises, but can't be bothered to finalize that process in a timely manner, it's pretty much why I'm entertaining other options at the moment.
If you're building a pure CRUD app on top of a ljnq2sql DB, webforms is great. The second you deviate from that, you're in a world of hurt. There are a few reasons why this is so. First off, centering your logic around a view as opposed to your view around your logic is terrible design. Using the exact same page for all your operations on a given data set is almost never what you actually want, and it's really difficult to compose your views together. Second, two way binding is very difficult to work with if you deviate at all from standard patterns for the framework. All the free work evaporates and you end up rebuilding it all from scratch but without help. You end up having to maintain an intermediate state that works around all the difficulties you create. Third, because of the above facts you application state becomes very complicated and expensive and you need really heavy and complicated lifecycle patterns to deal with it properly. Bevause this is http you're also passing all of this crap back and forth on every call. The idea of composing your UI out of premade components is a good one. A lot of modern UI frameworks utilize it, but centering your design around UI with code behind is pretty awful ( that's one of the reasons angular 2 was such a wild rewrite) and trying to maintain state between two stateless systems (http without semantics and a browser without javascript) is bloody awful. As a point, pretty much all of this applies to the new razor pages which are equally crap.
Sounds like the job doesn't work for you either. 
Pretty much. 
I guess it is just a style thing. If the scope of the connection is discreet, I fail to see the purpose of a field - a local should be fine. To me a field says "has some defined lifetime", in which case ownership belongs to the parent object. But... meh. As for "non-CS background" - that would include me, so...
With both approaches, i keep getting this: An exception of type 'System.IO.FileNotFoundException' occurred in MySql.Data.dll but was not handled in user code: 'Could not load file or assembly 'System.Security.Permissions, Version=4.0.0.0, Culture=neutral, PublicKeyToken=cc7b13ffcd2ddd51'
A control adapter or a custom control are also options, but the repeat layout is probably sufficient.
If anyone has a suggestion for a better subreddit too, I'd love to hear it. I've also posted on security.stackexchange.com, but nothing there yet.
There seems to be a method... https://stackoverflow.com/questions/16194140/how-to-invalidate-cache-data-outputcache-from-a-controller
I don't know about this specific error, but instead of using the official MySql dll I used the `MySqlConnector`. Without changing a single letter of code, it solved some issues that were giving me quite a bit of headache. BTW I don't know if you saw it, but I summoned the Dapper creator (Marc Gravell) via Twitter and he gave his interesting insight to my answer. You can see his answers to my comments.
After watching a few Vimeo and YouTube videos on the subject, it seems no one really talks about what is required on the DAL to retrieve the data. That is what I am really interested in is how to source the data to the graphQL sever. 
Normally, you would set up a policy to retry on a transient fault. You could retry immediately, or do an exponential backoff on it. I like this article: https://docs.microsoft.com/en-us/aspnet/aspnet/overview/developing-apps-with-windows-azure/building-real-world-cloud-apps-with-windows-azure/transient-fault-handling
I'd say there's nothing wrong with putting retries into an SQL data layer. Exceptional issues can occur and its all technically possible. However there is no standard way, it varies business to business. Some retry 10 times, some never do. It would need to be coded in manually (or configured). However, if someone said to me "our network guy asks can you put some/more retries in", I'd think "wait.. why is the network even failing **occasionally**?" Occasionally being the word (rather than exceptionally)
Thanks a lot! Learning tons with you guys. :)
Have you thought about why it fails? SQL is pretty robust. If multiple clients keep getting connection failed, then retrying multiple times will only increase the load on the server / network and cause more failures 
Your last statement is exactly why i posed the question. I haven't been able to find anything from a network prospective that indicates a network problem (believe it or not i actually looked instead of brush it off), and it just struck me as odd there were no retries, but I simply don't know what best practice is for these sorts of things, and i wanted to try and understand things at least a little bit from the devs point of view. 
Thanks, that's good advice. I bought a couple of Udemy courses during this past sale on C# and ASP.NET. After I'm done w/ those I'm thinking of subscribing to Pluralsight. I'm guessing a potential employer would want to see some kind of portfolio, or ask me a bunch of knowledge-check questions so hopefully I can sound smart enough to get a position, then expand my skills w/ on the job experience.
AutoMapper supports mapping to/from anonymous types, as well as DAL projections. Maybe you could look at building up an anonymous type from your GraphQL query and projecting your DAL queryables to that?
I understand and I hope my reply doesn't seem dismissive of the question. To answer directly : its absolutely possible (and recommended!) to add a tolerance to an applications connectivity. Typically its done to a certain initial spec (i.e. how often do we think this should happen? can the app even run without a database?) and not often changed after. There's no reason why you can't ask for this to be altered (as its technically possible) as its typically a simple matter. The next question should be can this application be updated/changed, would ops be okay patching the software. Code changes are easier than change control, if you see where I'm coming from. You may need to weigh this against further network investigation (i.e. which is more hassle)
I would recommend a retry pattern for both the connection to the database as well as the data CRUD operations. The connection can fail for transient reasons, including the DB failing over in some HA configurations and DNS issues. The CRUD operations can fail due to deadlocks and etc. Disclosure: I work for Microsoft on SQL Azure.
absolutely not dismissive. i was trying to poke some fun at the finger pointing we've all been through in our careers. I really appreciate the response. and helping me to be able to speak intelligently on the issue. 
This kind of just feels like a regurgitation of the .NET Core docs (https://docs.microsoft.com/en-us/dotnet/). I would advise any developer to get this kind of info directly from the docs so you aren't frustrated when things change.
I'm gonna install this tomorrow or the day after and see how it handles my usual 2gb+ IIS memory dumps. Since the engine is the same this shouldn't act any differently.
&gt; It doesn't scale, it does AJAX really poorly ... This is my biggest problem with asp.net webforms. Mostly it seems to come down to the fact that parts of it like "session" are limited to 1 action at a time per user: https://stackoverflow.com/questions/24149120/why-is-my-asp-net-site-acting-single-threaded So if you have 1 relatively long running action, it can make the whole site appear to slow to a crawl. 
Too bad it's only in Windows Store. My employer blocks Windows Store for security reasons so I can't even download it. Development tools should never be behind a wall like that. 
&gt; My employer blocks Windows Store for security reasons ..what? I mean, the store isn't the most useful of places, but a security concern it is not.
I've never heard of GraphQL in dotnet yet, but OData is there for a long time. OData is a different thing, but many problems there will also have to be solved for GraphQL. It might be helpful to dig OData libs to find answers for some questions.
Yup. "Reasons," not realities. They whitelist apps for non-IT workstations, which is their reason for blocking Store. Unfortunately don't apply different network policies to IT workstations.
Sounds like you *might* be missing this package: https://www.nuget.org/packages/Microsoft.CodeDom.Providers.DotNetCompilerPlatform/
At the bottom of the [GraphQL dotnet page](http://graphql-dotnet.github.io/graphql-dotnet/getting-started) there is a link to an [entity framework example](https://github.com/JacekKosciesza/StarWars)
From the comments: &gt; We’re planning on including it in the SDK/WDK in the future. We’re using the store for the time being because it lets us get updates and fixes out much faster than any other option.
Authentication is not Authorization. Try to keep the two distinct. A tenant is basically a resource you want to protect. Roles are really bad at managing tenants because they tend to be global for the application. Ex: admin role, is that admin for the app or admin for a tenant? Do I need an admin role for each tenant defined like {tenant}admin? The ThinkTecture guys put a lot of thought into this and in the new asp.net created Resource Based Authorization. You can see more info [here](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/resourcebased). [Policy based Authorization](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies) gives you a bit more control, and is the same basic concept Basically, instead of roles, you have attributes, the user (from authentication) and a policy. You map attributes, for example tenant Id, to a user through an attributes mapping. These attributes are often called claims. You can assign attributes to resources as well. The key is in the policy that governs access. Now that you have policy, you just need one more piece to get your usecase taken care of. Context of the current tenant. You can call this TenantContext and assign it to some stateful variable like session state or cookie state. As a user switches tenants, change the TenantContext. All of your queries and app logic should use the current context to determine how the app behaves. As to how the front end talks to the back end, this depends. You could do impersonation of the current user to the back end API, or you could have the front end applications be authenticated with an application id of some kind. The same thing applies, policy controls access so you will need in either solution to pass the user ID to the API. With impersonation, it is part of the user context. With appid, you will need to pass it on the API request.
I think it also depends on the culture as well. If they know english they would rather have english then poorly translated in their native language.
I don't believe I confused the two in my post. I want to authenticate the user to the API, and then fully-authorize them access to the different instances/tenants. Basically: * OpenID : Authenticate to the API (simple login) * GET : /api/instances * OAuth : Authorize to /api/instances/{id} (grants JWT with permissions that user has based on their Role within that tenant) As for the roles, that is what it will be called for the tenants' instances. Each tenant will be able to create a role, and assign permissions (basically claims that will grant access to different resources in that instance). The claims will be thing like 'report : read', 'report : create', 'report : delete', 'report:edit'. The specific naming of these is yet to be assigned, but hopefully this makes things clearer. I actually already plan on using claims to represent these permissions on a per-tenant basis. As in, the JWT generated will have claims for the tenant that they requested from OAuth, as well as the individual claims for the permissions. I am definitely going to stay away of the context/stateful variables. This is a REST API, and we currently do not use Session, and will not. The tenant information will be hydrated by some token and passed to members that want it via an identity object via IOC. But all of this is outside the scope of this question. This question comes into play specifically asking about the handling of the tokens (should the Authenticate [openid]/Authorization [oauth] be two separate tokens - I've not seen this before, and don't like it), and what should the flow be for a user logging into the application. Does this make more sense?
Yeah, I thinks that's part of the reason I made this post. The site is clean and loads fast. Bootstrap is fantastic for people like me, and most pages are just a couple of gridviews, or a gridview and a couple charts. I'm not really a fan of those types of pages you have to scroll through, especially for a small web app. 
Thanks man, I've been thinking about starting a separate project, and refactoring some of the code to be used with WebAPI, but have been a little hesitant. You say decoupling frontend and backend is difficult with WebForms, but that kind of hand holding is kinda nice with a small project like mine. What would you do if you wanted to experiment with WebAPI? I messed around with Ajax a little bit implementing a JavaScript chart, so that's the only possible way I know on how to connect front end to backend. Or is this where one of those fancy JavaScript frameworks shines?
&gt;OAuth : Authorize to /api/instances/{id} (grants JWT with permissions that user has based on their Role within that tenant) That is the part I was referring to when I mentioned mixing authorization and authentication. There is no need to re issue a jwt token at that point. The user is already authenticated. If they have a different userid/password or social account then you would need to reauthenticate. The context is meant for the client, not for the api service. You are correct, REST apis should be stateless. Think about the tenant being the key to the implementation. Instead of granting claims for the user accessing the app, you are granting claims for the user accessing the tenant. This will require the claims to be mapped between the user and the tenant. You can do this with a simple table that has UserId, TenantId and Allow/Deny. You don't need to pass this info to the user for any reason. Just enforce it when they attempt to access the tenant with the given user context you created during authentication.
If I want the JWT to have the permissions/claims that grant them access to do things, and I don't know what permissions/claims they have for the instance upon authenticate, then how would I grant a JWT with the correct set of claims for a tenant during authentication? Unless I grant claims for every permission that they have for every tenant and tie those claims to the tenant IN the JWT. I don't want to do that as a JWT should be small/slim, and the bulk of extra data needed to map that correctly is bad. And this now sounds like you've mixed authenticate vs authorize. I authenticate to the API. Now I need to grant them an authorization token for an instance they have access to, with the permissions they have for that instance. That can't be done in one step/without reissuing, unless the server is looking up that information every call (costly).
Thanks for that, at the time the first version was built, the department was using a bunch of tasks created by MacroMaker to send out 30MB excel sheets through email. Then I tapped into one of the databases and built a shitty Web Forms site, then we moved to a new system and I built the project in a more sane manner. Now I'm at he point where the site runs great, but I want to move to a more relevant front end / back end just in case I need to move on from this job. I'm not really interested in moving to MVC, what would you recommend? Web API + some JS framework, like Vue.js?
Yeah, I've been kinda looking forward to Razor Pages. I just haven't had a chance to start a project and remove all the crap they bundle with the new project, I wish it had Razor Pages empty project option.
I've never actually used the drag and drop features, I've always just hand coded the controls. I think my grid view only actually uses the run at, ID, and CSS class. 
If you want to issue a new token for each tenant, the user will need to re-authenticate each time they need to switch. You mentioned in your original requirements that you want the user to stay logged in. There is no hierarchical authentication, that is why you feel the need to implement a second form of authentication for each tenant. Authenticate the user to the app/api, authorize the user to the tenant. If you want the user to have access to all tenants where they have been granted access and you want to do this with tokens, you would have to provide the user with a token containing all Tenant Ids and rights to those tenants. If you can lax the constraint of using a token for authorization your authentication token can just provide the user's information (userid, first name, last name, etc). You will already be going round trip to the database to get the information for each resource. The cost of joining this to the permissions tables will be trivial if you have indexes setup properly. SELECT * FROM Tenants WHERE TenantId = @tenantId vs SELECT * FROM Tenants t Left Join TenantUsers tu on t.TenantId = tu.TenantId and tu.UserId = @userId WHERE TenantId = @tenantId In your api, if a user tries to access a resource they don't have access to (basically tu.TenantId is null) return a http 401 result. 
I work as Software Architect and when I get to actually write code it's either infrastructure or backend so I'm not the best person to ask about frontend. That being said... If you want to study something to keep yourself updated, I'd suggest you to look into a backend built on an ASP.NET Core application (maybe hosted on AWS Lambda) and any JS framework for the front-end. The technology you use matters up to a certain level. My team still uses a lot of Knockout.js even if it's not cool anymore. We are slowly getting more and more into Angular. The one thing I am adamant about is using TypeScript.
Why would I need to reauthenticate each time if the authorization token contains the user's ID, and since they are both issued by the same service/server, and is signed by a private key, the information is known to not change. I don't see why a user would have to re-login, if you already have a trusted token telling you who that user is. It seems like you're trying to pigeonhole authentication and authorization into one call. From what I've seen, OpenID is about giving authenticating the user. Telling you who they are, and what service they belong to. OAuth is about authorizing client's to a service/API, typically giving or receiving a list of scopes/claims that the user has. I'm not trying to authenticate them to an instance. Just know what permissions they have. The user could just as easily attempt to authorize to an instance they don't have access too, at which point they'd get a 403, not a 401. And the method a user is associated to a tenant isn't always directly like that. It can also happen as a consequence of another relationship, one I'd rather not have to navigate every time. More what I was talking about earlier not wanting to go to the database is just to verify that they can/can't do an action. If the token has report:read information in, then i will pull the report id (verifying their access to it via correct joins). But if they don't have the report:read claim/permission for that instance, then there is no need to ever go to the database. 
I wonder if it includes what's needed to effectively debug .net applications... 
Seen it... I was expecting something deeper, especially about the compact shim... Pretty good video if you dont know so much about .net standard tho
You're not alone, took me nearly a full week to sort out a migration to core cookie auth. And still not happy with the way it works.
I took a look at graphQL-Net today and it looks like it might do what we are after but still not sold on anything yet. The biggest challenge we face is the disassociation between our entity framework model and object model we are offering up to our webapi layer. Given that we are also looking for proven patterns for data access on the backend. GraphQL-Net seems like it may be able to help us bridge that gap. 
This is the sort of thing you should work on with your DBA. Connection pools, retry strategies, and connection errors in general are all big concerns for them and you're going to make their lives heck if you do it wrong. Also, the strategy employed will vary according to the database product you use. In short, yes, a simplistic retry scheme for 3-5 retries probably won't do any harm, but it definitely won't prevent issues and it may not even work around the root cause of the problem; which is where the bulk of the resources should be spent.
Ok, I'm clear on what you are asking now. I have never seen anyone do OpenID Connect for authentication and OAuth for authorization as distinct flows. In fact, OpenIdConnect IS OAuth2, it just specifies more specific rules around how authentication is handled. from : http://openid.net/connect/ &gt; OpenID Connect 1.0 is a simple identity layer on top of the OAuth 2.0 protocol. It allows Clients to verify the identity of the End-User based on the authentication performed by an Authorization Server, as well as to obtain basic profile information about the End-User in an interoperable and REST-like manner. To do what you were asking, you would need a distinct OAuth2 endpoint for each tenant. You would need to forward the JWT token from your authentication to the specific OAuth 2 endpoint for the tenant. This would represent the "subject". The OAuth2 server would then verify the JWT signature and would would issue an Authorization token for the target API. Then the client would use the resultant OAuth 2 token for subsequent API requests. Basically the OpenId Connect token would give you access to the OAuth2 endpoints and to the base API but you would need the client to manage secondary tokens to access the tenants. You asked earlier if there were suggestions on a different way to do this, I personally would cut out the secondary OAuth 2 endpoint and manage authorization based on Policy based access control instead. This will eliminate the number of tokens the client needs to manage and greatly simplify the api flows.
Yeah I've been there. I got that working with my new app and that wasn't exactly a breeze. It's like wading through treacle.
I generally never do db connection re-tries and always fail if db can't be accessed on first attempt, because if you can't open a network connection on first try, that something that absolutely needs to be resolved first. Also generally I experience this issue only when getting firewalls opened up to access the database, can't recall a time where network connectivity to db was a regular recurring issue. What's really important is that error message. If the error is because of too many connections, timeout, or deadlocks or something like that, it's an application level issue.
Depends on the application and how you classify your database. In contrast to most comments here, I'd say that _typically_ I wouldn't retry on a connection failure of an application's main transactional database, if that database is local. Database connectivity is something that needs to be completely solid - if a connection fails, it's exceptional. If the database isn't reachable 100% of the time, then the setup probably needs to be fixed. I'd compensate for the error (possibly retry the whole operation, or show a nice error) depending on the needs of the system, but behind the scenes it would still be something that would need to be logged and investigated. I'd never accept _regular_ database connectivity issues as normal. The approach should be the same as any error of an essential service - what do you do when you write to a local disk and it fails? Transient but unexpected issues like failovers should follow the same pattern IMO. What your app does when exceptional events happen should be dependent on requirements. If the database is peripheral, external to the organisation, or expected to be flaky, then by all means, I'd put it behind retires or a circuit breaker. But that's because you don't have control over it and can't trust that it'll work all the time.
If I could describe the situation I would say: overengineered mess. I found out the best way to make it work is 'dotnet new mvc --auth Individual'. This generate a template site which work out of the box with user password. As you, I consider identity a regression compared to membership. 
yeah, that's where stuff like angular2 really shines; you can just drop in components like https://valor-software.com/ng2-charts/ or https://swimlane.github.io/ngx-charts/ ...and to talk to the backend, you might use like https://github.com/2muchcoffeecom/ngx-restangular/ It's pretty nice; you basically point your frontend website at your rest endpoints, and just like `Users.one(id).get()` or `Users.getList()` from your javascript. If your interested in just checking it out and seeing if it works for you check out https://github.com/angular/angular-cli npm install -g @angular/cli ng new Project cd Project npm start Will get you up and running with all the default scaffolds and you can mess around~ For WebApi I really recommend going with one of the frameworks; if you end up writing the ajax requests by hand in jquery, its really more 'building scaffolding' than 'making website'. (There's plenty of awesome react stuff out there too, I don't really have a strong feeling about which is better / worse, but angular comes with more 'talks to server' stuff built in, where react is mostly frontend components and 'do whatever you like' to talk to the backend)
I'm not _sure_, but I don't think so. The project's sole dependency is `Microsoft.AspnetCore.All`, but that package has a lot of dependencies that one probably doesn't need for "hello world" - EF Core, for example. You can see its dependencies [here](https://www.nuget.org/packages/Microsoft.AspNetCore.All).
The are installed with the dotnet runtime so it only picks up what it needs. If you build it standalone/self contained where it includes the runtime with with executable then using `Microsoft.AspnetCore.All` then it would be huge and you should reference the packages directly. Also if you do self-contained you probably also want to use the [Using the .NET IL Linker](https://github.com/dotnet/core/blob/master/samples/linker-instructions.md) to go even smaller for output.
You're going to be starting at the bottom if you go down this path. Your SQL experience works in your favor but it won't make up for your lack of experience developing web applications when it comes to finding a new job and comparing you to other applicants. If I were you, I'd look at moving sideways into another BI tool. SSRS really is the pits when it comes to BI, there are better options out there. Download Tableau and take a udemy course in it. It's much more enjoyable to use and it's in really high demand right now. In terms of prospective employment I think it's a more natural fit. Good luck
https://docs.microsoft.com/en-us/microsoft-store/windows-store-for-business-overview
Don't disagree with the lack of concrete information but like most other things I've found with .Net core once you get it, it sort of clicks. It took me a solid weekend to simply get the basics down. A lot of this, and i think this was ultimately a problem with classic framework where they gave you all this stuff by default and people were using it without knowing how and what it does. Remember that with .Net Core, DI is key and when you add Authorization and use Identity you have a few options for implementing the various pieces such as sign in, role lookup, role server, etc. We have a weird setup because we don't use EntityFramework so we needed to implement the user manager and the role manager. Essentially when a user hits the login controller, we set the principal information and use he signinmanager to sign in (you have the opportunity to implement this as deep as you want). The identity framework will then handle all the behind the scenes stuff to set the encoded cookie and allow you to use the Authorize and HasRole attributes on your controllers and methods. The following link is pretty useful in showing the differences: https://docs.microsoft.com/en-us/aspnet/core/migration/1x-to-2x/identity-2x One gotcha with the default implementation is that if you roll your own identity providers you need to implement the security stamp validator. This article talks about it and gives examples: https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?tabs=aspnetcore1x. This burned me pretty hard. Last, if you're on a load balanced environment, they moved away from using the machine key to decode the encrypted cookies across servers. The data protection API handles this and there are examples of this here: http://www.tugberkugurlu.com/archive/asp-net-core-authentication-in-a-load-balanced-environment-with-haproxy-and-redis Edit: I realize (because I was in the same boat) that this isn't exactly what you wanted but am on mobile so couldn't put together. Basically if you're not using entity framework and want to use Identity, you need to implement your own user and role store and managers. These are then handled by an encoded cookie thst uses the Applicationcookie properties. Additionally, if you want to make sure you correctly authenticate users you need to keep track of their secure stamp in your user store. Finally, if you need multiple servers to share auth cookies (so not using sticky sessions)you'll need to centrally store the data keys for decryption. Microsoft provides some default implementations for this but I doubt a simple login server would need this. We scale anyway from 6-20 servers and although I'm one of a handful of users logging in, it's just easier doing it this way. 
You can make it smaller, but it will cost in clarity: * You can change ctx to be be 'c'. * You can specify Microsoft.AspNetCore before WebHost and save the using statement * Program class can be renamed to a single character * It still has whitespace... I think your claim seems pretty fair though.
I don't know...does a "Hello World" app really need extensive clarity? 
Due to the same awesome reasons I kinda moved from asp net, now I'm building a system with nancyfx core, identity server 4, and pushed logic crunching in console apps using an mq for communication... you can push the jwt along with the message if you need the identity every where ... not the best way to do it but it seems to be working just fine atm...
For .net I prefer [Multilingual App Toolkit (MAT)](https://www.microsoft.com/en-us/translator/mat.aspx)
Please be aware that those are for core 2, there is a separate branch for core 1 
Thanks for your answer, I recognise a lot of the stuff you mention. The data protection API caught me out (again, I think there were breaking changes when I was implementing it). As mentioned I have got the basic authentication and authorization stuff working but I it's JWT implementation I'm now struggling with. I don't think there's anything 'weird' about not use EF, in fact your point highlights another issue. I don't use EF, I prefer Dapper but find you're forced to use EF - at least all the examples go down this road.
Yeah, Nancy might be worth a try. I'd not given it much thought but it looks a lot simpler.
Definitely. I meant it was weird in the sense that their examples do not specifically outline this. 
Not particularly useful (really just a short press release). Would have been nicer to actually go into some detail for the APIs that are applicable to typical enterprisey development.
Correct - I was looking at hitting it from the perspective of using OpenID Connect to get the ID Token, and then using that to get the access token from an OAuth endpoint - such as listed on here [connect2id.com](https://connect2id.com/learn/openid-connect): &gt; Token exchange — The ID token may be exchanged for an access token at the token endpoint of an OAuth 2.0 authorisation server (draft-ietf-oauth-token-exchange-03). There are many real world scenarios when an identity document is required to obtain access, for example when you check in at a hotel to get your room key. Token exchange has uses in distributed and enterprise applications. Would I really need multiple endpoints for OAuth though? Couldn't I just utilize the realm concept, with each tenant being a separate realm? Last, as per the policy based access control, I guess I'm unclear on how that would benefit me, but maybe that is because I already have an idea to implement it in mind via the route I've been on. My plan had been, that once the user authenticates, then authorizes, that their JWT would contain either a permissions claim array, OR utilize the scope claim with permissions like those previously mentioned. The resource server would then simply have to check that they have the scope/permission for the action. Thus allowing fine-grained access control per method, without ever having to go to the database to see if the user can even do the thing, or store it in a stateful way. For PBAC, how would I accomplish checking if the user can access a method on the API without hitting the database, if I didn't already have the claims/attributes to evaluate that policy in the token? Is it that I'm looking to do basically the same thing, but nomenclature has been confused between us? Looking at the information on the MSDN for [ASP.NET Core PBAC](https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies), it basically utilizes the claims on the identity object to approve access to the resource. It would seem that the only difference in my implementation would be that I am not using ASP.NET core, so would just implement this with a class deriving from AuthorizeAttribute, and implement the logic to utilize the requesting identity from the authorization bearer token header, and assert that it has the scope/permission there. 
I looked up the rfc for realm, that would work. You only need some kind of partitioning information. Url, Http Headers, Form body, scopes, etc. The important part is the authorization server understands this info. Policy based access control is great for situations where rights can't be assigned at one time and need to be allocated on demand. For example, if you have a list of 20,000 documents and a user should only have access to 19,998 of them. You wouldn't list out 19,998 documents in an access token. The thing I like about it is that it simplifies access control and you don't end up with a bunch of if/then statements littered in with your business logic. If your resource authorizations are fairly flat, any access control solution will work, including tokens or policies.
I've used Let's Encrypt (win simple) a couple times. https://github.com/Lone-Coder/letsencrypt-win-simple
I use Let's Encrypt for at least 10 Azure hosted web apps including my blog and renewing certs is very very easy....
They don't seem to be familiar with the concept of "pit of success". 
https://github.com/vrbyjimmy/EcdsaAcmeNet is pretty good too. Runs as a service and uses simple XML config files.
I wish I'd seen something like this earlier, would've saved me hours of trawling half complete docs and out of date examples. Still nice effort all the same!
Generally its also slightly a teaching aid; making it unintelligible doesn't make it approachable
As good as previous windbg versions, but not any better.
I'm the dev lead for the project so let me know if you have any questions.
This could be good to play around with. We've been having issues with the current compiler being to aggressive and transitive dependencies, used in DI, are being excluded and breaking at runtime. We've had to put at stupid defence to these packaged in our startup. ( ReferenceList.add (typeof (class)) ) 
I've spent some time looking into all this. Here's what I found to work for me. If using IdentityServer4 - Resource Owner Password Grant/flow/whatever they want to call it. If using Identity Core with EF - roll your own JWT token gen (not hard). Set life of the access token to something like 10 minutes. With either of these solutions, you also want to generate refresh tokens (guid). Set life to 2weeks or more (really depends on what you are building). Store the access token (JWT) in local storage in the brower (your client side app) and then store the refresh token also in localstorage, but have it saved in the DB for revokable access. Another option is to store these tokens in a cookie. If you do this, you need to configure your API to send anti-forgery tokens/cookies with each request that is submitted from a form. Also, make sure your app only works on https. There's really no right way to do this as it's a massive clusterfck. Regardless, hope this helps.
Just tested some of our free apps now, and they have HTTPS - so it's not that "unfortunately".
I've got a working copy now, thanks partly to [this](https://wildermuth.com/2017/08/19/Two-AuthorizationSchemes-in-ASP-NET-Core-2) post from Shawn Wildermuth. I'm going to cobble together a simple demo with Cookie and JWT authorisation without EF, SQL Server or IS4 and stick it on Github. It's because all examples incorporate the other technologies that it gets overly complex just to get something basic up. Thanks for the response :-)
Why not save the values you need into hidden fields within a form. Make that form POST the data to the 2nd URL.
SOLVED: See https://github.com/filipw/dotnet-script/issues/114#issuecomment-326280083
Finally got it built and running. But man, what the #### has he done wrong? I had to add a few dlls manually through add reference and then download maybe 6-7 from NuGet. Why didn't they download with the project? They did it seemed like, but lots of them did not work. Grateful for your help, internet friend. Kudos.
Was actually solved by deleting the minified css and let it recompile with gulp. Updated .csproj as well, seems like something was corrupt!
"1.298 installs" why?
One thing to consider is if the client side framework is Angular 2+, built in XSS and CSRF protection is activated out of the box, so it doesn't matter whether you use cookies or localstorage in that scenario. I can't speak for other frameworks though. :)
I’m personally scared of new languages from Microsoft with little traction. F# seems like a second class citizen and I’m worried it will get abandoned in the future. In that case, the massive incompatibilities between pure F# and C# code will be a headache in a transition. Even if it’s not completely abandoned, just some thing where some part in tooling will be...
And MS is not exactly making F# easier to motivate with more and more functional programming conveniences entering C#.
[This](https://www.hackster.io/Ra5tko/running-native-net-core-apps-on-raspberry-pi-arm-0bb717) article explains how.
It is worth checking out Keycloak
Good question, they should be doing `1298.ToString(CultureInfo.InvariantCulture)`!
We just Assembly.Load all assemblies in the directory matching certain criterias. 
WebForms is fine -- for small projects. Once you get to something sizeable it is very easy to fall down a rabbit hole of "I want it to behave like [x] but WebForms doesn't *quite* do what I want". A couple of tweaks later and you've bastardized your code and abused the WebForms pipeline enough that maintenance becomes a headache. That's where MVC shines. You can do anything in MVC that you can do in WebForms, but it is so much *cleaner*. There's less abstractions between you and the HTML, so it makes reasoning about the end result much simpler.
So if i understand that correctly, I don't build it to the raspberry Pi but my entire application gets put into a DLL when compile and then I put that DLL in there? Seems like I should just be able to build right to my pi. Also, would this still see it as an application I can start/stop and move to backround/foreground in the IoT console if i am simply moving over a DLL?
How do you get the assemblies into that folder? We've not found a good way using Nuget to get the dll into a folder. I could be wrong but I thought we were having trouble scanning a folder that was not in the assembly in NetCore. I think it was disabled because it is a security concern, but I could be wrong about that. 
I think you do it the same but you use/inherit from identityuser instead (on phone can't recall exact naming) 
Oh hell no
Very odd :-) I fixed it by adding in an http to https extension. Thats working now. Thanks for replying. 
I'm curious, why would one choose this over .NET Core? Edit: It also looks like it hasn't been updated in over a year.
Just FYI - if you want to be more cost efficient with Pi Zero, you will need to look at running mono because .NET core doesn't run on it due to ARM version.
Thanks for the heads up! I do have the pi 3b. I think, if anything, I'll just make some fun little projects I'll just load to github. Need to think of some good use cases I would have for having an always on system.
I worked with this for a few years. It was super awesome. It's good GHI took it over. However, it seems the .net MF devs have moved to Core.
I'm running a dotnet core app on my RPi 3b, but Im using debain stretch. When i publish the app it creates a folder full of the dependency dlls as well as my app's dll. I copy that to the pi, then "sudo chmod 755 [APP's Exectuable path]" one time for permissions. Then im good to execute however i please.
Now that the Allwinner based SBC's have full mainline kernel support I wouldn't bother with the Pi Zero.
Set copy local to always in reference preferences.
https://www.nuget.org/packages/Ensure.That/ Argument validation package.
I've got my own static library that does this type of stuff with arguments, but I'm thinking this might be better than that. Thanks!
.NET Core doesn't fit into a ARM Cortex M-4 running on bare metal, or a Netduino (Arduino with .NET firmware).
I'm pretty new to this, can you point me towards a model that is within the Zero price range and can run .net core?
I gotcha, I was planning on doing it through Windows IoT, but that is a great alternative, thanks!
The docs.microsoft.com stuff is [open source](https://github.com/dotnet/docs/tree/master/xml), perhaps you can use that?
[Consider some of the FriendlyARM boards.](http://www.friendlyarm.com/index.php?route=product/category&amp;path=69) It will depend upon what particular features you need, maybe the Pi Zero has some IO not found in some of those cheaper boards, or perhaps you need ethernet and would prefer to have it on the board in the first place. I'm running .NET Core on the Nano Pi NEO without issue. Only downside was that proper ethernet driver support in the mainline kernel was missing but last I checked that was supposed to be merged into 4.13 which should be out pretty shortly.
I don't see that as an option. http://i.imgur.com/7WTZXyH.png Note: We are using 2015 - .netcore 1.1 (project.json still)
Thanks, I will check it out. I need to use a GSM connection rather than LAN/WiFi.
What does this actually do?
It's a bit of a self-plug but I use this CLI Arguments parser in many projects at work and have been overall really happy with how it came out: https://github.com/Nick-Lucas/EntryPoint 
Does this help? https://stackoverflow.com/questions/43837638/how-to-get-net-core-projects-to-copy-nuget-references-to-build-output
It allows to replace this: public void SomeMethod(IFirstService firstService, ISecondService secondService) { if (firstService == null) { throw new ArgumentNullException("firstService"); } if (secondService == null) { throw new ArgumentNullException("secondService"); } ... with public void SomeMethod(IFirstService firstService, ISecondService secondService) { EnsureArg.IsNotNull(firstService, "firstService"); EnsureArg.IsNotNull(secondService, "secondService"); ... and includes many other checks and combinations.
Oooooh. Never realized how much I needed this in my life. Thanks for sharing!
not a nuget package but an extension, shift + f2 to add a new file, instead of having to go through the visual studio menus to add new file and select the type you want, speeds up dev https://marketplace.visualstudio.com/items?itemName=MadsKristensen.AddNewFile
That's pretty cool. Bookmarked for later ;)
I loved the reply. :-)
I do a lot of desktop dev, so I made https://www.nuget.org/packages/DeviceId to help with uniquely identifying my users (for licensing, analytics, etc.) It gives me a nice fluent interface to build a unique identifier for my user based on things like motherboard serial number, MAC address, processor ID, etc. It's pretty flexible so I can extend it to add other components (GUID stored in the file system, etc.) string deviceId = new DeviceIdBuilder() .AddMachineName() .AddMacAddress() .AddProcessorId() .AddMotherboardSerialNumber() .ToString(); 
What do you mean by hosting? Visual studio solutions are irrelevant after the bits are built and deployed.
Visual Studio Code is apparently doable on a Pi 3, and the dotnet commandline tooling for building projects works per that article.
Oh I know, but I mean, is there a problem when publishing a solution with both the app and api in it as separate projects?
Apples and oranges. .NET core is for building modern computer applications, .NET MF is for low power embedded development.
If I'm not mistaken, publish is setup by default to publish a single website. You can tweak publish profiles to get it to work with more than one. Personally, I always design with one solution and branching structure per unit of deployment. Each unit of deployment gets its own build and release. Push common code into versioned nuget packages. This keeps things simple but a lot of people don't like it at first because it's a little more work when getting started.
For anyone doing WPF or WinForms dev, I strongly recommend Ookii Dialogs (http://www.ookii.org/software/dialogs/) It provides some awesome dialogs: * TaskDialog - A task dialog proving a superset of the standard message box functionality. * ProgressDialog - A dialog that displays a progress bar. * CredentialDialog - A dialog that accepts credentials from the user. * InputDialog - A dialog that accepts arbitrary text input from the user. * VistaOpenFileDialog - A Vista-style OpenFileDialog. * VistaSaveFileDialog - A Vista-style SaveFileDialog. * VistaFolderBrowserDialog - A Vista-style FolderBrowserDialog. The packages themselves are: * https://www.nuget.org/packages/Ookii.Dialogs/ * https://www.nuget.org/packages/Ookii.Dialogs.WindowsForms/ These packages are especially useful every time I have to do WinForms dev, because [the default WinForms FolderBrowserDialog is not very good](https://mking.io/blog/a-better-alternative-to-the-winforms-folderbrowserdialog-using-ookii-dialogs).
[Polly](https://www.nuget.org/packages/Polly/). Might not qualify as little known but sure there are many who haven't heard of it and it's great.
Check out the PayPal .Net SDK. It is fairly straight forward to use: https://github.com/paypal/PayPal-NET-SDK
Only one other dev at my company had heard of Polly, but once I explained it twice to everyone, they all agreed that this was a much better solution than "run this in a tight loop and hope one of these calls succeeds". Polly isn't terribly revolutionary, but it's so easy to use and makes you really think about error handling in ways I didn't before.
Optional, brings some much needed functional programming types.
Thanks. I'll have to keep that in mind when we upgrade to csproj files. We're still stuck on project.json files for another few weeks. 
So, kinda like code contracts?
Related, I just started using [FluentClient](https://www.nuget.org/packages/Pathoschild.Http.FluentClient/) for a REST client in .NET core. It is much easier than working with the HttpClient class directly.
SlowCheetah - https://www.nuget.org/packages/Microsoft.VisualStudio.SlowCheetah/3.0.61 - To transform config files other than web applications
Nothing wrong with that at all. I prefer that actually as long as the solution isn't outrageous in size.
Check this guide. https://docs.microsoft.com/en-us/aspnet/core/publishing/iis After you install the hosting bundle you will need to restart the w3svc which will cause a short downtime for other services on that host machine. Otherwise it shouldn't interfere with existing sites.
If using something like AWS Beanstalk, deploying client and api separately allows you to scale one or the other independently. Maybe you can get away with a single micro instance for your client app but your api/database spin up several instances based on usage at the time of day. 
Why not use contracts?
Yup, it's a simple matter of adding a reference to System.Diagnostics.Contracts.dll and then doing something like this: public void SomeMethod(IFirstService firstService, ISecondService secondService) { Contract.Requires&lt;ArgumentNullException&gt;(firstService != null, "firstService"); 
 public void SomeMethod(IFirstService firstService, ISecondService secondService) { Contract.Requires&lt;ArgumentNullException&gt;(firstService != null, nameof(firstService)); Added nameof operator to replace difficult to refactor string literal.
Not sure how little known they are but I like: https://www.nuget.org/packages/ExpectedObjects/ to do duck-style comparing in tests https://www.nuget.org/packages/Flurl.Http/ is a great fluent webclient that is also testable.
You don't need vagrant AND docker - use one or the other. You can install Docker on your host machines without need for a separate VM. 
We are a team of 7 right now running Dotnet Core in production. We use whatever we want. We save the code in Git and we have our own enviroments. We have some basic rules: Has to run in Docker and project has to include a dockerfile that works. At the moment we have 5 using Windows and Visual Studio and 2 using Mac with Rider IDE. We have been using Dotnet core for little less than a year now. It's been a fun long go from prototyping to release. You really don't need to overthink that much about localhost development. Let people decide for themselves if they want to develop on Mac, Windows or Linux. I use windows my self, simply just because I love visual studio with Resharper. 
I didn't know that. Thanks. Question though: The library I linked has tidier API. Any benefits in moving to contracts?
Static code analysis comes to my mind (fxcop) 
[removed]
I have used entry point as well and I really like it. The only annoying part is path handling requires double backslashes to work.
https://github.com/nlkl/Optional
You can migrate painlessly to VS2017, for sure. .NET Core 2 works just fine in 2015, but Docker support is a lot better in 2017. Cannot comment on how Win7 runs with Docker. Edit: Totally wrong about .NET Core 2 on VS2015. My mistake!
Consider using SteelToe library. Implements full on hystrix client with circuit breaker dashboard, collapsers, etc. (I think hystrix package is prerelease and should ga within a month) 
Thanks. 
My company is looking into the same thing but we are still evaluating options would love to know the answer to this.
Have you looked at Identity recently? As of .NET Core 2.0 they broke the role stuff out. Also you may want to look at using a configuration file and/or passing DbContextOptionsBuilder instead of sharing the DbContext directly. Review the 2.0 stuff before proceeding; they've changed a lot.
Sounds like that might be a feature of your shell. Before arguments are sent to the application the shell handles and strips escape characters. The normal way around is to use / or do what you did and escape the escape ( \\ ). If you think this actually is an issue with EntryPoint then please open up an issue on the repo, and I'll investigate, but I'm reasonably certain it's a limitation in how .Net sits on the shell. 🙂
You can configure things so the contacts are removed from the code for release builds.
Can you give me an example of what you use this library for?
_Can you give me an_ _Example of what you use_ _This library for?_ &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ^- ^CowboyProgrammer ------------------------------ ^^I'm ^^a ^^bot ^^made ^^by ^^/u/Eight1911. ^^I ^^detect ^^haiku.
You're awesome, bot.
Disclaimer, I am the author of the following libraries: https://github.com/NimaAra/Easy.Logger - A modern, high performance cross platform wrapper around Log4Net. https://github.com/NimaAra/Easy.MessageHub - A high performance, easy to use cross platform implementation of the Event Aggregator Pattern.
Not having hyper v support in my opinion makes win7 and docker a no go. If you have windows 10 available then it's smooth sailing.If your just doing prototypes on a dev box then docket with virtual box may be an option. I mean worst case spin up a win 10 vm to prove it out.
r/vscode --------------------------------------------- *_Upvote to allow me to help others. Downvote to remove. PM for your subreddit to be ignored._*
I believe .netcore 2.0 is only supported on VS 2017, so it's a no go there. Docker toolbox for anything lower than windows 10, uses virtualbox instead of hyper-v so you're ok there. You could always just use the dotnetcli and vs code.
Retry logic, circuit breakers. Like retry downloading of a file over http 3 times, with a custom increasing backoff (try once, wait 1 second if failed, try again, wait 5 seconds if it still failed, try again), if 100 total failures in the last minute, stop trying to make the requests and queue them up for later. The library doesn't handle all of that, it would be a mix of your code and configuring/calling into the library, but their API is very clean and makes it easy to place your custom code where you need it. 
Win7 Docker is a pain. There's enough *gotchas* to seriously slow you down. We're hitting this just now and I'm about to upgrade to Win 10 solely because of this.
Yeah, the issue is that I dislike having authorization based on the user's roles (i.e. if (role == "Admin") ), rather my framework would allow a developer to assign resource and action combinations to roles. For example, I could create a role "Editor", and assign the action "Edit" to a "Post" resource, allowing for more fine-grained access control, similar to how they work in AWS IAM. I think the way I have to go about this is provide an interface that gives access to the DbSet for each type, so the developer would have to implement this interface and they can provide the DbSet's to their liking. 
I believe it's gonna be a problem to run Win 7, spin up a Win 10 vm, then install dock inside that, which uses hyper-v. It feels like running a hypervisor inside a hypervisor may lead to problems. I know that vmware didn't support that when I tried some years ago. Got the funniest error message. 
&gt; but Docker support is a lot better in 2017 This is important details. Thanks. 
Yeah, I installed it and immediately got errors. I haven't used docker or containers before so it's not at all obvious what the cause to it is and what to do about it. Seems like Win 10 would be easier. 
Thanks! This definitely puts me more at ease.
Thanks, that makes things much simpler.
Yah you are probably correct
another question: The link for hosting bundle send me to version 2.0, would this work with an app made with aspnet core 1.1?
&gt; Digital Ocean I'm looking into these. They support ASP .NET core 2.0 out of the box?
I don't know for certain, but I would expect that to work. You could also just update your app to 2.0 and see if it still builds.
Good idea.
I'm not sure if this the right place for it, but if anyone could give me some critiques on one of my Nuget libraries that would be awesome! [Shell.NET](https://github.com/phil-harmoniq/Shell.NET) I often found myself needing to execute bash commands and decided to make a simple wrapper. I'm wondering if perhaps I'm over-engineering my problem.
will try that, thanks.
Forgive me, I may be missing something but there are no examples of how to properly implement the Instant Payment Notification within an MVC project. Thanks. 
I've been using them for a while, they're pretty good. I preferred to set everything up myself but they do have templates for ASP.NET Core (so yes, they support it out of the box). Edit: Sorry, I missed your question fully. I just logged in to check and I can no longer see images for .NET Core which is a bit odd as they were there before (see [this](https://www.digitalocean.com/community/tutorials/how-to-use-the-net-core-with-powershell-one-click-image) tutorial on their own website) but they've stopped doing it. So it's a no at the moment for out the box 2.0. If you have a rudimentary knowledge of Linux though it's not too hard to setup (apt-get install etc).
What? Just use Docker and have your Windows users use Docker for Windows (which runs a Linux VM automatically, transparently for you). We used this in a team in Azure to develop something that ran on Linux. The windows devs could use VS Code on their Windows box, and then run a Docker container to build it and execute tests, etc. Worked quite well.
Awesome! Thank you for the reply. I'm solid with Linux but didn't know .NET core worked well with Linux yet! Does it matter what distro I choose from the site?
Very underrated imo given how good it is Refit: https://github.com/paulcbetts/refit
Does it? I know I was having issues with .NET Core 1 on VS2015 before I switched to VS2017, but that was more with it having dated tools at the time.
I've only used Core with Ubuntu myself but all the main distros are supported so as long as you're not picking anything too obscure you should be okay :-)
I had the exact same issue, its an annoyance but the other really nice features make up for it tbh. If there is any sort of solution or workaround for it it would be great to know. 
Awesome! I'll grab one of those hosts and get started. Thanks for your help!
What specific exception is thrown and what's it's message?
Haven't gotten there yet because I'm waiting to get the permissions for the server. I'm working on that. Once I get that I'll get back to you.
&gt;my mindset is that a parameter query assigns the value to the parameter based on the @Tag in the statement, and the @tag in the parameter. it apparently just adds the parameter in sequential order. which was causing it to be added to a field that couldn't accept date time, but the catch string i made to catch .net params was working the way i thought it worked. The Access Database Engine (ADE) does not support named parameters. Parameters are added in sequential order even when you specify a name. Unfortunately the ADE isn't exactly the greatest library in the world.
It should work. Try following the suggestions in this bug report https://github.com/OmniSharp/omnisharp-vscode/issues/1117
How are you consuming these feeds? Something to check might be that the server is resolving the purl.org domain, as that's where it's drawing the schema from
* Use session to store a bool: Should the user get this javascript? * Write middleware which populates the session value if it doesn't exist * Clear the session value on user logout If you do this, any action in your controllers or view page can read session to know whether this user is allowed access to the js or not. Middleware initialization order: 1. app.UseSession() 2. app.UseMyCustomSessionPopulatingMiddleware() 3. app.UseMvc() 
Thanks! This will definitely makes things a lot easier.
&gt; The Access Database Engine (ADE) does not support named parameters. something that i wish i knew ten hours ago. i usually avoid access like the plauge, but this project called for it. 
Yep, it caught me out as well. Very annoying.
Wow, looks awesome. The dialogs included with WPF are a bit rubbish.
[Dynamitey](https://github.com/ekonbenefits/dynamitey)
Thanks! Will session persist until the user logs out ? 
Not necessarily. For example, in ASP Identity a user can have their login remembered, which can keep you logged in after a session expires. 
Yeah that will be a problem. Thanks, I'll dig in. 
This + shift-f3 for New Folder = goodtimes
I thought it was Cmd and "+" not Cmd and "." so I misread. Thanks for the reply :-)
The .NET Core 2 Visual Studio tooling requires Visual Studio 2017. It does not work with 2015.
Unfortunately .NET Core 2.0 doesn't have tooling for VS 2015. I understand that upgrading both OS and VS is hard, however I'd still recommend starting the process - Windows 7 support ends 2020 I believe and Windows 10 seem to be pretty much "The Windows" meaning there might not be next major version anytime soon and it brings you Windows Subsystem for Linux among other things. Upgrading to VS 2017 will bring you the .net core 2.0 support and the last update (Update 3) is quite stable from my experience, so again - I'd really recommend starting the upgrade process. If you need temp environment for a proof of concept you could also try virtualization and use trial windows + trial vs, however I'm not sure whether docker would be happy in virtual env (might be somewhat slow if working at all).
Note there will always be a little bit of view state, each control can save a little bit of control state. 
CMD + '.' is the keybind for solving any lightbulb suggestion in regular visual studio. Auto solving missing usings is one of those light bulb suggestions
The session lock is not specific to web forms but is general to the default session provider of ASP.NET. Out of band a new session provider is available which does not lock.
Is not a build engine.
&gt; Unfortunately .NET Core 2.0 doesn't have tooling for VS 2015. Good info. Thanks.
OP wants to compile a csproj at runtime. I don't see why Roslyn isn't a valid solution. Please let me know why instead of a useless comment.
Also note that VS2017 and VS2015 can both be installed on the same machine. You don't necessarily need to use VS2017 for all your projects.
MSBuild handles so much more, like setting up proper command line parameters, ensuring only building if out of date. Also a good chance that any csproj will contain more than just using C# files to compile and require handling by some MSBuild logic. 
Fair enough. I guess it depends on OP's needs.
This sounds as though you are trying to automate your DTO layer instead of actually writing. This also sounds as though you are to lax with DTO validation.
No, I'm trying to avoid having two full sets of models one of which is nullable. Essentially we have fairly complex data models coming in from a web browser multi-step funnel. The models at the *end* of the funnel are fully validated and non-nullable. However, we have a requirement to save the state of the model as the user progresses through the funnel. These models are exactly the same as the final set of models, except the must support null values for uncompleted steps. 
Thanks :-)
&gt; These models are exactly the same as the final set of models, except the must support null values That would be a DTO. If your Model is so complex that automating models makes sense, then there that is a clear indicator that your model is missing abstraction and not properly thought through. Speaking as someone working on a 7 Digit Lines codebase at work there is no way automating the creation of DTOs does not break 7 gazillion development idioms. Not everything should be nullable. If your customer somehow manages to send a fully null'd transfer object then this should mean an error. "Foreign key" ids should not be nullable. This is what i meant with shitty validation. Making a property nullable requires a semantic decision. Also... if your DTO supports nullable and your model doesnt... you better ask yourself what the fuck happens in the time when the DTO is still not valid but also persistent on the server. You are designing with fury into the realms of temporary/parking tables. Not good.
Sorry for the lack of clarity - these have nothing to do with the database. There are no foreign key ID's etc. What I'm trying to do is avoid re-implementing the abstraction of our API models (view models, not DB models). These models are very complex. Say you have 5 forms on a website - each has 30-40 boolean values (we use more then bools, but for simplicity sake). When the user is completed the form, all of those booleans are set to true/false and can be submitted to the 'complete' endpoint on the server. Easy. What we need to do is also save the intermediate steps. So from 0 booleans filled (all null) to 40 booleans filled (completed). These nullable models are used by a totally different set of services within the application. This leads to us basically duplicating the model layers for these intermediate steps, which I'm looking to automate to reduce overhead. 
How complex is the source object? Does it just consist of primitive types, or are there custom structs, enums or nested types to handle? Also, do you require the generated type to be in the same assembly as the source type? Depending on your requirements you may run into a sort of chicken and egg situation with T4: you need to build the project to reflect over it and know what code to generate, but you can't build the project before the code generation is done. I've heard positive things about the Roslyn-based Scripty, but I haven't used it myself yet.
There are a few enums and a few nested types, but its nothing crazy. I'll take a look at Scripty. I could probably do it via string modification in something like Powershell to be honest. 
I switched from ST3 to VSCode due to better integration for dotnet. I recently started using VSCode at work for Python and React work as well. I know some people complain gin it's mem usage but I don't really have any issues with it. Tried Rider in EAP but don't feel like paying and VSCode is good enough for me since most extensions I use are pretty good. I also enjoy just having my terminals in Code and only use git from command line. 
Then why don't implement viewmodel persistence? Because that seems to be what you are actually trying to achieve?
Yes, that is what we are trying to achieve. The true view models are handled on the client via a React front end application. We could look at serializing the models are just JSON and storing them, but then we have to invalidate old data anytime the models change which is not ideal. I'm totally open to other suggestions. I'm just looking for a way to reduce boilerplate "copy/paste" code here. **Edit:** the other thing I'm debating is just making the models nullable so they can be reused for both "pieces" and then validating the nulls have value on completion.
I tend to use VS Code for front end (JS/Angular/React) on the mac side and Parallels Windows VM + Full VS for the back end. I know that this slightly dodges the question but it is a setup I really like. Particularly as its easy to switch on the fly and I have a better feel for how the two devices/VM's will interact in prod.
You can use shift + F2 to create new folders too. Just add a / on the end to indicate that it's a folder. Or better yet, if you add a file in a folder that doesn't exist, it will just create the folder for you and place the new file there.
I picked up Rider after using it during EAP. If you liked resharper, it's a must IMO. I have iTerm2 configured to slide down from the top if the screen - that's pretty handy. GitKraken works nicely on mac as well, that and the command line gitting get's me through the day.
Did not know! Thanks!
Crazy idea: what if you used the exact same model for both but had a generic helper method on a base class that would retrieve the value or throw a validation exception if it was null based on a private property. So base model would have a private property "isView" that is true when it's a view model and false when you're using it as a DTO. Then extension method would be called from all of the property getters and either return Value or throw. This way you wouldn't have to duplicate code and could still validate the fields that need to be validated when checking model state. This is assuming, of course, that nullability for then entire model is a black and white operation. 
Yep, I'm thinking something along the same lines. 
&gt; it's a massive clusterfck summary of the state of web development in general today 
Get homebrew and brew-cask. Setup a file that contains all you brew/cask dependencies, so you can setup a dev environment in no time. Set up your aliases right, so you save time when working on the command line. Do regular backups! Visual Studio for Mac is okay-ish but I prefer Rider. Use VS Code for everything else. I also use Parallels, but only because my employer pays the yearly fee. I wouldn't do that on my own because I don't agree with their licensing model. Oh and use the-fuck (https://github.com/nvbn/thefuck) :)
I switched to a Mac some time ago before .NET Core was a thing. I used Xamarin Studio for MVC development and it was challenging but doable and it's obviously a lot easier now. I've been using VS for Mac since it was released and though it lacks some of the polish of its Windows cousin it does a pretty good job. That said the past couple of weeks I've been using VS Code with a view to switching over as I'm doing more front end stuff and I'm not sure I need a full IDE (I actually found VS on Windows too bloated for my needs). Other tools I use ... * iTerm2 - get it, you'll love it.The terminal experience on a Mac blows away what you'll get on Windows imho. * zsh - install it for a much better terminal experience. It also has lots of great plugins. * Sublime Text - even though I use VS Code I still use ST for basic text notes, it's just blaziingly fast to load if I want to create a quick text file. * SQLPro for MSSQL (not free) - for when I have to do MSSQL stuff * Postman. * Atlassian SourceTree for when I need Git gui stuff (not often but handy sometimes). I also have Parallels installed for Windows which I now almost never use. I mention it because it's the best tool out there for running Windows with great seamless integration. FWIW I have a few things on my [blog](http://coderscoffeehouse.com) about my experiences working with .NET on a Mac.
I've not heard of GitKraken, how does it compare to Atlassian's SourceTree?
Hello, that's my first post on reddit! I want to shere with you my open source library for subdomain routing in ASP.NET. That's my Minimal Value Project which I think is enough for use. Any participation in project is welcome. Let me know what you think! There is still much work to do but as I said before it can be used for a basic stuff related to subdomains.
Well done my friend if you want some help with that let me know. Probably I'll test this on my project next week. Till then good luck with development!
Nothing wrong with seperating out your api layer into a separate project, particularly if building and publishing to a single iis site. If instead you are looking to host the api in a separate process, e.g. self-hosted owin instance / inside of a windows service, you would also have to configure cors to accept/allow taking requests from your other client(s).
Nice! I'll be testing it out on one of my projects
[Nuke](https://nuke.build) ([GitHub](https://github.com/nuke-build/nuke)) for build automation with C# DSL and full IDE support. I'm the author, but since I'm also using it, I think it's okay to post here :) FYI, I've been using MSBuild, PowerShell and Cake before and also looked into Fake.
You can find it on nuget too https://www.nuget.org/packages/Microsoft.AspNetCore.Routing.Subdomain
Can you tell me a particular use-case for something like this? IMO if your site / api needs to concern itself with different sub-domains than it's implied that its handling different business roles. I would be inclined to split them up and deploy them separately (easier maintenance) and set up sub-domain routing further down at the load balancer.
Can't wait for the day when MacOS's .NET development experience is on par with .NET development on Windows. As a dev with a Windows laptop and a Macbook, there's no incentive to right now to do .NET on Mac when you have Windows on hand. VSMac is getting better and better though.
I am using that for cities, so I have localised results in search engines. There are also blog services which put your nickname as a subdomain. I think there are plenty cases where you may want to use subdomain as a 'internall' page.
Hi, you'll want to change the nuget package name unless you're from MS. Otherwise it will be removed. 
the-fuck looks great and I'd really like to use it but I simply can't stare all day at a terminal that says 'the-fuck'.
You can have a different alias, e.g. orly
It's an electron app that looks pretty slick - I haven't used sourcetree extensively, but I found gitkraken to be much more intuitive to use.
Thanks, will check it out :-)
Hello, I'm not from MS. Had no idea package name can be issue. Will change that soon.
Found the blog post. Read here for more details: https://blog.nuget.org/20170417/Package-identity-and-trust.html
This looks pretty neat and I like the license; I've opened some PRs. :-)
What problem do you have that you're looking at identity sever to solve? Asking whether it's the right tool for the job without telling us what the job is isn't going to get you a useful answer.
I mean, Identity Server 4 is great. I'm currently in the process of revamping and re architecting my company's 9 products to use oidc, using identity as the base. It's a very flexible library, yet it effectively prevents stupid mistakes.
Identity Server is a fantastic tool to use when you want to have single-point authentication, authorization, and claim management for many different external applications. Having all of these things handled at a single location is much easier instead of having one-off solutions for each and every sprung-up API or otherwise. I don't know if it's something for a university context since you're speaking about only a single-use assignment, but when you have N number of apps that could use identity management, it's nice to have it all in one place.
Awesome! Thanks for the contributions.
Cake is nice to replace a custom build script written in PS or bash but is missing one very important feature: file dependencies. Every other build system I worked with allows to define dependencies on files, to avoid regenerating up to date artifacts multiple times. With cake you can only declare dependencies on tasks, and you get the whole dependency chain executed each time you run it. For big projects this is a lot of cpu and time spent for nothing.
Shows how Microsoft is slowly losing plot.
Personally I think the whole authentication and authorisation model for ASP.NET is a mess - difficult to understand, hard to decouple dependencies, almost impossible for newcomers to get up and running quickly. I put [this](https://github.com/matthewblott/simple_aspnet_auth) together as a simple boilerplate for user management with ASP.NET.
I'd wager SQLite with EF Core would be much faster for CRUD operations and just as easy if not easier to implement here.
I agree this is shit. But Microsoft generally are really good at the moment.
In most cases, json serialization would be sufficient, much safer and actually require less code. And non-minified json is ok to read too. 
Yup, I recently had an interesting use case. A 300 GB csv that is distributed daily by the mexican government. It's basically a ton of records indicating SSN | Boolean Field | Boolean Field. It contains a record for every tax paying entity in the country. Initially we loaded it up in our main database, but since we had to update daily, it would slow down our service dramatically while we were updating. We tried multiple approaches (update record by record, use views and just dump to a new table and regenerate the views), but everything slowed down the main service too much for 30 minutes to an hour (by our standards anyways). Instead of spinning up a new database on a different server (we're on prem due to government regulations and cost), we just create a new sqlite file with the daily data and distribute it to each of the frontend servers, then switch over the open database to the new file. It works wonderfully, both generating and reading is ridiculously fast. We're seriousy impressed with Sqlite performance for basic CRUD.
It is somewhat confusing, but until I started reading about IdentityServer I think I was good lol. Thanks for the link!'
I think you are right that for the most part, it is quite overkill. However, I figured it was a good time as any to at least try and "learn it" even if I'm not using it at its 100% . Thanks for your answer!!
It's not that we were looking at IdentityServer, we found it while looking through Users, Claims and Roles in ASP.NET Identity and were confused as to what it could offer us. The problem we are trying to solve is authentication and authorization, and while we believe it could be solved with ASP.NET identity alone, maybe by using it along IdentityServer there was some benefit we were not aware of Thanks
&gt;A fine idea in fact. Why use more moving parts than needed? Where is the series of animated gifs to illustrate this point?
I would like to bounce ideas off anyone. Would there be a way to dynamically add and remove routes? Edit: Was thinking that a database context is created before the routes are defined. An override method could be created to use to query a db for the values. I also need to look at how this package would handle the Route annotation. Then again I would set up this as a service to run on top of separate projects. The next logical thought is why not use something like nginx? 
That seems like am amazing dataset! Could you post a link, or even to /r/dataset for some Karma? 
Title aside, this seems more about CsvHelper than an actual persistence strategy. I've been looking at it recently, but wondered if anyone had any experience with it. Back to the title. For "dirt simple" persistence in a situation like what's described, I'd go with json columns in SQL. Have as few tables as i could (don't get to relational) and call it a day. The key is that all the persistence logic is tied to an interface and there's no dependence on it from the rest of the code base, so i can swap out to something better when it makes more sense. I'd much rather deal with json serialization and a db call than setting up that mapping code and persisting a CSV file, but that's just me.
If you're going to use json anyway, why not just use a nosql db like mongo or something? Putting json into sql columns feels really hacky.
Any more hacky than writing CSV files? Because at a stage like what they're discussing, i don't want to deal with standing up a noSql store and the administration/maintenence that goes along with it. In my environment, i can create a new db on an existing instance and it will automatically be handled by our backup and maintenence jobs. I also won't have any dependency on a nuget package for persistence. If later down the line, a full document store is what makes sense for the project, that's when I'd migrate. 
Maybe you can check out [IdentityServer 4](http://docs.identityserver.io/en/release/quickstarts/1_client_credentials.html) or [OpenIddict](https://github.com/openiddict/openiddict-core/blob/dev/README.md). IMHO these libraries are quite good for inplementing token based authentication and can be integrated in ASP .NET Identity without too much effort.
No worries, I hope you find it helpful :-)
I got so sick of the complexity of ASP.NET authentication and authorisation I put together this super simple boilerplate project [here](https://github.com/matthewblott/simple_aspnet_auth).
If you don't know what you need identity server for then how do you know whether identity server is what you need? Do you understand? You shouldn't just use libraries because other people use them, you should use them because they relieve some pain. If you're not experiencing some pain in your authentication system then why are you looking for a fix? How will you know when or even if you actually fix it? How can you expect others to tell you whether it will fix your pain when you can't describe the pain? Long story short, cargo-cult programming is bad. 
I agree it's tough to grok. Do you know of any framework that gets it mostly right? .Net or otherwise.
I've used it a couple of times over the years. Once for returning a CSV based action result and currently to upload to S3. It offers an easy to use API and has some great options for type-conversion and reformatting.
If your csv column names match your POCO property names no mapping is required, fwiw. If they don't, I think you'd be better off putting the time into configuring something better suited, but if they do this looks like it might be faster than involving a dbms for this particular simple use-case if you're not already using one for the project.
I guess the challenge is that for most of the CSV files I use it wouldn't make much sense to throw it into a POCO. Its more than I need a list of the headers, and a list of the data for each row. It'd be a pain in the but to have to set up POCOs for these cases, and since some of the headers are not valid property names, I would have to map them as well.
I understand what you mean, and maybe I didn't express myself as clearly as I thought. My question is: what could identify server offer us in terms of authentication and authorization that's not provided already by asp net identy?
I'm... not as disgusted by this suggestion as I feel I should be. This seems decent for small sets of flat records.
Rails is pretty easy with Devise.
I published Toggle Raspberry Pi GPIO Pins with ASP.NET Core 2.0: https://carlos.mendible.com/2017/09/01/toggle-raspberry-pi-gpio-pins-with-asp-net-core-20/ Hope it helps...
With .NET Core 2.0 and corresponding SDK you can now build for ARM (i.e. dotnet publish -r linux-arm) and it will work. I've created a sample toggling GPIO 5 and it works like a charm: https://carlos.mendible.com/2017/09/01/toggle-raspberry-pi-gpio-pins-with-asp-net-core-20/ Hope it helps...
Your events are broken. Have a separate thread that does nothing but subscribe and unsubscribe from the event continuously, and another thread that triggers the event. It'll cause a NullRef exception. void OnCounterChanged(int oldValue, int newValue) { if (CounterChanged != null) CounterChanged.Invoke(this, new EventRaiserCounterChangedEventArgs(oldValue, newValue)); } What happens if CounterChanged becomes null after your null check? Your code should either be: void OnCounterChanged(int oldValue, int newValue) { var handler = CounterChanged; if (handler != null) handler.Invoke(this, new EventRaiserCounterChangedEventArgs(oldValue, newValue)); } or void OnCounterChanged(int oldValue, int newValue) { CounterChanged?.Invoke(this, new EventRaiserCounterChangedEventArgs(oldValue, newValue)); } Depending on which version of C# you're willing to require.
Absolutely! My company only started using it earlier this year as the number of microservices and APIs has more than doubled alongside new partner integrations into our system. It's pretty flexible as well and has methods to use all sorts of standards! 
Hi, I wasn't thinking about adding such functionality. The idea was to create subdomain routing inside mvc app where you want to catch subdomain as a route value, create forms and urls across app preserving standard MVC routing rules. Please check Samples project on github source, there you will see what exactly this lib is doing.
Excel doesn't do JSON.
I'm curious how similar the datasets are day to day. If you compared the last two csvs and created a patch for your db offline, if the diff is small enough you could apply it fairly quickly.
Thank you for input. Changed id and project names. Now you can find nuget package under https://www.nuget.org/packages/AspNetCoreSubdomain. Will contact support about the old package
&gt; what could identify server offer us in terms of authentication and authorization that's not provided already by asp net identy That's the exact backwards approach. What need do you have that asp net identity doesn't provide? If you can't answer that question first, then the answer to the question you asked is irrelevant to your project. Edit: I guess the answer to your question in the context of your project is, unless Identity is falling to meet some need you have, then identity server 4 can't offer you anything you need that's not already provided by Identity. Hope that's helpful!
Excellent! Thank you much for the follow up. I'll give this a try this week. 
Great answer. Thanks 
Excel is also really bad at CSV a lot of the time. Got some config files in CSV and if you open it in excel and save it it wipes a whole column of data because it tries to format 20047495 as 2E7, so saves the CSV as 20000000. Hae it happen to me all the time. Every time you open it you have to manually set the formatting to something sensible before saving or it'll destroy that column. Easier than dealing with JSON though!
I wouldn't consider this an embarrassing thing to not know, but you could end up impressing if you have some knowledge of ORMs (Entity Framework, Dapper, etc). Some jobs will care about this and some won't.
Csv would be worse, yes. No advantages over json in sql for performance or security, and even compatibility is fine because anything can read sql and pars json. I guess I would just spin up a document db in Azure and be done with it. If you need a database and you need it backed up, you might as well do it right. If you aren't using document db at all, I guess I'd look into adapting the data into SQL relationally. If you have a document db running anywhere, you should be able to add to it and have it backed up the same as your other dbs.
I've used EF Core, but not very heavily. I've setup EF with SQL Server running on a docker container and run a few migrations configuring the DB context, but that's about it, all pretty basic stuff. Anything in particular I should know about EF? That's really the only ORM I've used. 
I would say that understanding the basic concepts of interfaces, abstract and virtual classes and the difference between them, generics, and the basics of Linq would all be helpful. 
Of course specific stuff to know will depend on what kind of job you're looking for. But in my opinion, a basic understanding of EF IS great for a junior level programmer. It sounds like you are well prepared!
I started tracing the repo and it looks really good. As soon as I get a chance I am going to run the samples and try to implement it on a project I am working on currently. The ideas were more about the next evolution of the library and not meant as negative criticism. 
Explain the difference between value types and reference type. Is string a value type or reference type. What is the difference between a thread and a Task. What is AppDomain. Explain encapsulation. Explain contravariance.
Great idea. I have an application at work that would benefit a lot from this "create a patch" approach, thanks!
No worries. Didn't sound negative. Let me know if you like it or what is missing, if any issues found feel free to open new one on github. I am so happy by interest of that lib though.
I was chatting with a mate earlier who is currently interviewing. He's sent the candidates a spec and asked them to build something, I agree with this approach as it far better demonstrates someone's ability than what they remember - that's what Google's for. If it's the sort of interview where they ask you to name 4 methods on some specific class and they think that's more important than your Maths degree then you're probably better off elsewhere. One final thing, do you have a Github account? If you've done stuff put it there and send them the link. I'd be more likely to hire someone if I can see what they've done - pre Github days I'd take a laptop and several CDs I'd hand out with my code and then ask 'shall I show you what I've been up to?'.
Great suggestions in the answers -- FWIW, when I've done technical screenings, I'm really just trying to determine the applicants skill level. So there's nothing special to do to prepare, and no wrong answers. If it's a senior position, the questions I ask are much harder and you have to get them right, so don't try for this unless your fully prepared.
I mean, I didn't invent it, in fact I'm sure that idea is older than I am, but you're welcome anyway. The only way this actually works is if you're working on a "total replacement" of the dataset like OP is. Otherwise if you're doing any other kind of scheme it probably won't work. EDIT: and as I alluded to earlier, it's only good if the difference is small relative to the speed of processing a change.
Even if you're just going for specific type of .net dev job I'd say at least be familiar with the main technologies/tools/libraries so you can at least describe what they are for. Things like WPF, XAML, WCF, entity framework, core, standard etc.. Been doing interviews recently and very frustrating how little people applying know outside of their one little area which is typically just asp.net
Yes, same exact situation as OP, different dataset. I'm currently diffing the current state of the DB against the latest file, but it would definitely speed up the process if I just diffed the latest 2 files in memory. This avoids an expensive SELECT query I otherwise have to run. So yeah, thanks for the idea even if it isn't yours.
Plus, it's the first word in the URL. 
.NET shops often ask things about SQL, like what the purpose of an index is, nonclustered vs clustered indices and when to use both, difference between different kinds of JOINs, when to use GROUP BY.
Is there a reason why you're using Win7? I'd strongly recommend against it since it goes EOL in 2020 and has already ended mainstream support in 2015.
NP. Glad it's helpful. My next suggestion would be finding a way to reliably automate this procedure so you don't have to touch it at all.
It's already automated via Azure Scheduler, just not performant, which currently doesn't matter but will as we start to ingest more and more of these files :)
If you can tell me the actual way that tasks work, I'd hire you on the spot. Up until two weeks ago, everyone I knew at work all had slightly different understandings, and none of us were right.
Holy shit, I laughed my ass off at this. So true. In a world where "everything should be async" almost no one actually understands the mechanics, or understands just enough to get it wrong. As an example, when is it appropriate to use ConfigureAwait(false), and why? I've seen so many explanations that even I am fuzzy on it.
The other comment to you here says EF is a good item to learn for junior level work. While I agree, you should also know what reasonable ADO.NRT code would take for that. For instance, we do not use any code gen nor ORMs for our API. Other teams do some, and then other teams use Dapper.net.
I agree that a GitHub account with a wide expanse of personal work is the best possible situation. I really do NOT like the take home test approach to screening though. It kind of already sets a precedent that the company's time is worth more than your time, and that's a bad way to start a relationship. I much prefer just having people do a Euler problem real quick in front of you. You get a better picture of their thought processes, how they work under pressure, and you aren't asking them to spend 8 hours of their own time to try and impress someone who's going to take 15 minutes to look it over. You have more of an option to avoid those types of screenings as your career progresses though.
I am probably going to start a package for dealing with the problem of CSRF validation across load balanced web applications. The only way core can take care of it is through a UNC path or Redis key store. Either way you implement it with those options absolutely sucks. I want to handle key rotation and remove any infrastructure requirements. This was documented here, https://github.com/aspnet/Antiforgery/issues/143. If you are interested in looking at it when I get something halfway done I would love input. 
They are very similar, and that would actually be a great idea. So I guess you would compare both the latest data set against yesterday directly on the csv, and a build a batched query that only updated the data that needed to be updated, instead of trying to do the whole process inside the database.
Would be great if someone could post a great link with a good rundown 
It's actually not really interesting. We do processing for tax paying entities for the Mexican government. For each entity that tries to use our API, we basically have to lookup a couple of fields in the appropiate SSN record to check if they are authorized to execute that particular operation, what their current tax paying scheme is, etc. 
You rarely work under conditions such as "solving a complex problem live in front of someone you don't know to get a job to feed your family". That is the definition of a sadistic relationship to me. And you learn nothing of how that person work in real work conditions. If you ask someone to do 8 hours of work and only plan to look at it for 15 minutes, you definitely are a sadist.
I'm the opposite. I love take home stuff. A good small project can show you how they structure code and their design skills way better than white boarding. Plus it helps reduce the risk of dismissing someone just because they were too nervous to perform at their best. Bonus points if they turn the project on either unit tests. 
&gt; solving a complex problem Euler problems are not complex or stressful to solve. I'm not talking about the "write a b-tree algorithm without aid of Google or IDE" type interviews, which I also dislike. I'm talking about bare minimum kind of stuff here with all appropriate tooling, like "can you write a for loop". This wipes a SURPRISING amount of candidates, at least in my midwest market. The purpose of an interview is to evaluate capabilities and fit. The best way I've found to both demonstrate my own capabilities and evaluate others has been to simply have them solve a simple contrived problem and following up with some basic questions. These don't take more than 15 minutes to do. I don't think its any more stressful than the kind of work someone is asked to do to solve issues during deployments, etc. Our jobs are kind of stressful by nature, especially with 100% uptime expectations and such. Not being able to handle some simple interview stress is itself a red flag.
That's fair, this is a pretty opinionated topic, and design eval is a good point. As your career progresses you can pick and choose interviews based on those preferences, I'm just naming the things I avoid and hate putting others through. I feel bad asking others to spend unequal amounts of time on the interview process, it just feels wrong. You can figure out most of what you need to know about someone in 2 hours with some basic live coding, tech questions, and personality evaluation for team fit. I'd challenge that you get much more out of a much longer / involved evaluation except for specific high value roles. Hell, if things don't work out in the first 90 days, either party can just go their own way if it doesn't work out.
That's right. That way your production db never slows down or is switched. This is also assuming that there are no online changes to the dataset as well (essentially read-only outside of your daily update). It also requires that there are no significant changes to the dataset day to day (no reordering, or that there is a keyable data in your dataset).
Stephen Cleary's blog has a ton of very in depth information on async await. Intro: https://blog.stephencleary.com/2012/02/async-and-await.html ConfigureAwait: https://blog.stephencleary.com/2012/07/dont-block-on-async-code.html Best Practices: https://msdn.microsoft.com/en-us/magazine/jj991977.aspx
Warning, indentity server's docs are out of sync with the most current version, 2.0, so basically it's unusable until they update those because of all the breaking changes.
Covariance/contravariance has always been an academic exercise for me in the 15 years I've been developing software 
I haven't used Auth in ASP.NET Core 2 yet, but I've heard it was changed up a bit between Core 1.1 and 2.
The .NET specific jobs (at least in my area) seem to be mainly ASP gigs with a few Xamarin postings here and there, so I've been focusing a lot on just ASP and keeping up with Core. How is the market for WPF now that UWP seems like the way forward?
I was explicit with the recruiter that I'm at a new grad/junior level because I wanted to be honest and let him know that I've got a lot to learn. As an interviewer, are there any "gotcha" questions you like to throw out or filter type questions to weed out some candidates?
Awesome, I'll take a look at that! Edit, I see a link for .NET Core 2.0 SDK but I don't see one for Raspian/Arm? Are you saying I would just connect to it through my operating system and deploy a new distro of .Net Core? Sorry if that's a silly question.
Yeah. If you're not sure of an answer, say that you don't know but the explain how you would go about finding out. If someone in an interview I am doing can admit they simply don't know, that's a checkmark (rather than guessing)
I was never really certain how this things work but his blog post seems to be really helpful and easy to understand. Thx for sharing.
I suppose this all depends on the difficulty of the problem. If by Euler problem you mean something from [here](https://en.wikipedia.org/wiki/Project_Euler), where an example problem is something like "If we list all the natural numbers below 10 that are multiples of 3 or 5, we get 3, 5, 6 and 9. The sum of these multiples is 23. Find the sum of all the multiples of 3 or 5 below 1000", then yeah, I can see that as being reasonable.
**Project Euler** Project Euler (named after Leonhard Euler) is a website dedicated to a series of computational problems intended to be solved with computer programs. The project attracts adults and students interested in mathematics and computer programming. Since its creation in 2001 by Colin Hughes, Project Euler has gained notability and popularity worldwide. It includes over 600 problems, with a new one added every weekend (except during the summer). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
Hello there, for a junior level position I think CS fundamentals matter most, not things that are .NET specific. I'd say the weights in the interview should be at least 75% CS fundamentals (algorithms, data structures with important special cases like relational database theory). I'd advise against trying to learn all of the .NET ecosystem. No shop uses all of it and you'll learn a lot on the job. If the employer is sane, for a junior position they will prefer someone with strong fundamentals and weak ecosystem knowledge compared to someone with weak fundamentals and strong ecosystem knowledge. If they are not sane, maybe it's a good thing they hired someone else :-) Have you read (at least partially) something like https://mitpress.mit.edu/books/introduction-algorithms? How's your SQL? How's your C#? Do you know how C# anonymous methods are implemented? Do you know how much memory a heap-allocated value uses? How much memory a value type takes? What's boxing and unboxing? Can you draw box and pointer diagrams for a set of classes/structs? Can you read IL (at least a little bit)? What happens if I an exception is thrown in a finally clause when the finally clause is entered because of another exception? When should you create immutable objects? (nice answer and rationale provided by Joshua Bloch in "Effective Java" :-) ) As an example, why are String and DateTime immutable? What's the difference between readonly and const? Can you write a recursive factorial/Fibonacci and also draw an approximation of the stack for each invocation of the factorial method when computing factorial(5)? Can you solve the 8 queens problem in C# using recursion? Without recursion? How much slower is accessing a field using reflection compared to just accessing the field? What's the approximate time cost of an uncontended lock? When are static fields initialized? How much memory is available to your process in a 32 bit process? What's a memory model and why do we need one? Do you know how to use the Visual Studio debugger? Can you use VS to jump to a class/method definition? Can you use VS to find all the places a method is referenced? (just a set of random questions, probably most juniors wouldn't know at least some (most?) of the answers, but trying to find the answers will lead down interesting paths) For someone with a math/physics background https://mitpress.mit.edu/sicp/full-text/book/book.html might be a lot of fun (not directly related to C#/.NET, but Cormen + SICP + a good relational theory books would put you head and shoulders above most CS graduates :-) ). It also helps to know other languages in the long run. Really know, like knowing Java + JVM details (a lot of .NET is derived from the JVM, and the literature for .NET and JVM stresses different things, so reading JVM books benefits .NET people and viceversa). Best of luck with your interviews! 
How about solving a problem similar to the domain the work is in? I've never had to solve any Euler-type problems at work. (And I suck at maths!)
&gt; `Task` and `Task&lt;T&gt;` are basically interfaces. How is that a useful thing to say? They are classes, not interfaces. Saying that they are "basically interfaces" doesn't really explain what they are and is just confusing. &gt; If you have an instance one of those types, then it represents the status of some operation that may be in progress, may have already finished, may not have even started yet, or may never even start at all (maybe some more states too, going from memory here). You forgot that the operation may have failed with an exception or it may have been cancelled. When working with tasks, you should never forget that those are possible.
Return type covariance can be pretty useful. Too bad C# does not support it.
I think that you should know that EF and EF Core are different things, that you shouldn't confuse the two and that you should always make it clear about which one you're talking about. The same applies to ASP.NET Core vs. ASP.NET and .Net Core vs. .Net Framework vs. .Net Standard. Communicating clearly is a very useful skill.
This. Personally I hate people hovering over me while I'm working, it stresses me out.
Configure await will tell the threadpool that it doesnt have to guarantee a return on the same thread. You use that in classic .net when hosted in iis because just like with winforms there is a concept of a 'main' thread there.
Yep, my mate actually put in the notes he sent that he didn't expect the to complete everything. One thing he's looking for is how the prioritise, i.e. showing good agile skills and deciding what's the most important stuff.
I'm afraid i will fall into your coworkers category, and also my answer might be superficial, but basically a Task is a Thread abstraction where .net framework manages a threadpool for you. That means that a thread is obtained and the code in the action is scheduled for execution. There are a bunch of apis that allow you to cancel a task or chain task reaults together. Using await inside a task may even result in your code block being executed by multiple threads (.configureawait).
People still buy programming books in 2017? 
Sometimes it's easier to look something up in a well-thumbed book than search the internet. 
Yes. Maybe basic beginner topics can be gained from the internet, but for more in depth topics being able to read through a book that covers many aspects of a technology or methodology is important. It's also a much easier reference that videos.
What about the docs and continuously updated ebooks? True, .NET doesn't evolve as fast as other stacks but still, paper books get outdated quickly, I find. 
A book on Angular or React; yea that'll get out dated. A book on architectural design principles; that will have a much longer shelf life. Personally I've always found it tough to dig through documentation. First off, a lot of documentation just isn't that good. Second, documentation tells you all the possible things you can do, but not how to approach it or which of those things are the most useful 
You can use any compatible windows, linux or mac device to develop and build the app with the sample command I provided. Then you just copy the binaries to the RPi and run it... No need to install .NET Core or the SDK on the RPi
Looks cool, will check it out when I get home! Thanks!
How did you configure iTerm2 to slide down from the top of the screen? I have conemu configured like that on my Windows laptop but I'd like it on my mac too :D
Tbh I just followed https://marcjenkins.co.uk/drop-down-terminal-with-iterm2/ - it's pretty awesome!
That's right, I forget that .NET Core is self contained so you don't need to install frameworks like you do with previous versions. Good to know I can use Raspian so I can have it run in the background with other stuff. I was under the impression it only worked with Linux or IoT. Thanks!
Thanks, I hope you find it useful :-)
Or create a library of repositories, and use them with IoC. In a situation like this, just use the embedded docDB repository. I use Mocoding Easy Document DB. Just as easy. Then, when you're ready to move to a better, long term DB, you just switch your repo out in your app container for a Sql or MongoDB repo, and none of your surrounding code has to change. 
Does the entity framework book cover unit testing?
Translation: EF implements its own repository pattern of a sort, therefore implementing another repository pattern as a wrapper is redundant, therefore bad practice? It sort of makes sense, but only if you can swap out your data providers with out having to refactor any code dependent on EF. Plus, you have options other than rdbms that could be usable under an additional, more generic layer of abstraction. 
Yes, it does. It covers the Entity Data Model and Object Services to WCF Services, MVC Apps, and unit testing. 
The problem with using EF as a repository is that its a pretty shitty API. It doesn't encapsulate dependency life-cycles, making you manage the database connection. You also have to write a lot of boilerplate code unless you want performance to suck. Here's an article with some code examples showing what I'm talking about: https://www.infoq.com/articles/repository-implementation-strategies Beyond that, you can do some really interesting things if you add a repository layer above EF. For example, I like to inject the current user into the repository so that the repository can automatically set the `CreatedByUserKey` and `UpdatedByUserKey` fields. More suggestions: https://www.infoq.com/articles/repository-advanced 
I have to agree the Entity API seems rather odd, especially for me coming from Java / Spring background, maybe its just not what I am used to.
That's why I invented Chain. I wanted an ORM that handled my standard use cases without needing a repository layer wrapped around it.
Reasons to add a layer : * Testability/Mockability * Move to different ORM * Multiple implementations (or even just a single complex implementation) (pull from cache, etc) * Generic reusable functionality * Encapsulate database logic in a layer that isn't your business layer (Where are you going to store all your queries? Make a Repo for each entity and you now have a convenient spot) 
I'm going to play devil's advocate here * You should refactor your code or redesign your tests so you don't need to mock repositories * Probably not going to happen * Again, probably not going to happen * Yes, but only because EF sucks * Maybe, but stored procedures are usually a better fit for this
&gt; * Encapsulate database logic in a layer that isn't your business layer (Where are you going to store all your queries? Make a Repo for each entity and you now have a convenient spot) &gt; * Testability/Mockability I would say those are the two main reasons. I've never been part of a project where the ORM implementation changed, or the underlying database changed. 
* This is HOW you refactor your code so you don't need to mock EF. If you are suggesting just not testing that layer/logic at all, I strongly disagree. * true * You haven't worked on very big systems if "can we toss this data in redis" hasn't come up yet. * so we agree. * "Put everything into a stored proc" is so 1990s. Are there good reasons to do it sometimes? Sure. But saying thats the default position is lame. Why does LINQ2Entities even exist if this is the "right" answer?
Corporate inertia? :)
I've been on two projects that switched from NHibernate to EF. But I do admit its a rare case. 
Grey fox and grey wolf on the same thread? What are the chances :)
&gt; Why does LINQ2Entities even exist if this is the "right" answer? 1. Because a lot of people don't know SQL and can't be bothered to learn. 2. Because a very small number of people actually do need to support multiple databases. (Think CMS platforms such as WordPress.) 3. Dynamic query generation. (Which unfortunately EF is bad at without 3rd party libraries because it doesn't have an `Or` method.)
Could you elaborate on point #3 there? Because I've certainly written dynamic queries that involved an or. (Unless you are considering PredicateBuilder 3rd party?)
Here's a wonderful article about the topic http://mehdi.me/ambient-dbcontext-in-ef6/
Certainly. First, what you can do with EF var query = context.Customers; if (lastName != null) query = query.Where(c =&gt; c.LastName == lastName) if (firstName != null) query = query.Where(c =&gt; c.FirstName == firstName) Each `Where()` method is translated to `and`. But what if you wanted to search using `or`: var query = context.Customers; if (lastName != null) query = query.Or(c =&gt; c.LastName == lastName) if (firstName != null) query = query.Or(c =&gt; c.FirstName == firstName) Last I checked, this is possible but only if you use a third-party library.
&gt; If you are suggesting just not testing that layer/logic at all, I strongly disagree. No, I just prefer to isolate my business logic as much as possible in models that can be unit tested without mocks. If anything has to touch a repository, then it gets a test that touches a real database. I've seen far too many designs that pass mock tests easily but fail horribly once actual storage is taken into consideration. *** But I don't intend to argue about testing techniques today. As I said, I'm just playing devil's advocate. Personally I'm still going to use a repository (or more likely a service class) around my ORM.
I'm not in the right state of mind to read that right now, but at first glance I found it to be interesting. Hopefully later today...
Couldn't you simply refactor this down to var query = context.Customers; if (lastName != null || firstName != null) { query = query.Where(c =&gt; c.LastName == lastName || c.FirstName == firstName); } I don't have direct access to an IDE, and I know there are some issues with one being null and the other not, so obviously that needs cleaned up, but overlooking that isn't this a solution you are looking for?
Wouldn't you just do... query = query.Where(c =&gt; c.LastName == lastName || c.FirstName == firstName); 
[Expert C# 5.0](https://www.amazon.com/Expert-5-0-Framework-Experts-Voice/dp/1430248602) is very good.
No, you would have to use this: if (lastName != null || firstName != null) query = query.Where(c =&gt; c.LastName == lastName || c.FirstName == firstName); else if (lastName != null) query = query.Where(c =&gt; c.LastName == lastName ); else if (firstName != null) query = query.Where(c =&gt; c.FirstName == firstName); Which is ok if you have only two columns. But I was working on a database with 40+ searchable columns. That would have required 2^40 different else cases.
Nope. If lastName is blank or null, it's going to match on that when you really want to ignore c.LastName entirely. Depending on how the database is setup, this will either give you the wrong results or just be a major performance hit.
[Pro .NET Performance](https://www.amazon.com/gp/search?index=books&amp;linkCode=qs&amp;keywords=9781430244592) is very good.
 query = query.Where(x =&gt; (!String.IsNullOrEmpty(lastName) &amp;&amp; x.LastName == lastName) || (!String.IsNullOrEmpty(firstName) &amp;&amp; x.FirstName == firstName)); ?
That'll work for quick and dirty code, but will really suck if your table is large enough to need indexes.
So assuming you are doing strings or nullable types wouldn't it be much, much more efficient to implement a method at that point that takes two parameters: field value and search term? I'll just kind of pseudocode this since I'm unsure what types certain objects are: GetFilteredList(fieldToFilter, valueToFilter) { if (valueToFilter != null) return context.Customers.Where(c =&gt; c.TryGetColumnWithStringName(fieldToFilter) == valueToFilter); } this then makes the original method a lot more extensible by allowing us to now type: var listOfFilteredItems = new List&lt;T&gt;(); listOfFilteredItems.AddRange(GetFilteredList("LastName", lastName)); ... which follows the SOLID principles much better than 40 else cases. 
Yup. There is nothing like having *ALL* of your programming books on a thin eReader. 
With third party libraries you could probably do that, but I don't think EF has that capability out of the box. And SQL certainly doesn't allow you to reference a column via a string parameter. (Though it would be awesome if it did.) 
I didn't even notice that! Lol
Yeah ASP.NET is definitely more popular. As to UWP vs WPF I'd say WPF is still the better choice unless most of the time unless you're really making something to target mobile/windows store. I'd say at this point they are still pretty different tools for different jobs. You also give up a lot of audience with UWP by limiting to Windows 10, especially if working on any programs used by businesses. I'm not suggesting learning in depth how to use them all, more so that you should be able to intelligently described what they are and their uses. For me at least it doesn't come off that well if someone doesn't really know technologies that are very closely related to the one they are interviewing for.
Actually mine loosely means "horror wolf". It was supposed to be greywolf, but some idiot told me "grauwolf" was mis-spelled and I believed them. Nearly two decades later a couple people told me the truth of the mistake. On the plus side I got a totally unique username because grauenwolf isn't a real word.
&gt; I've never been part of a project where the ORM implementation changed Several .Net projects before EF was really good enough (looking at you SubSonic). That said, I'm not sure it would've even been possible to encapsulate the ORM in a way that was still reasonably usable but also wouldn't require major reworking and testing to change later.
Sorry a few days late, but the exception being thrown is System.Net.WebException
Doesn't that apply to Fake and Sake as well? It is inherent to the fact that all those "build systems" are executed, not declarative like MSBuild. Things need to execute, and at that time it is already too late.
&gt; http://mehdi.me/ambient-dbcontext-in-ef6/ Is still still relevant with Entity Core 2.0 and Entity Core? The article was written for Entity Framework version 6 so curious because I've seen some examples in .NET Core of doing something like this to have .NET manage the injection and lifecycle of the DbContext. // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices(IServiceCollection services) { services.AddMvc(); services.AddDbContext&lt;SandstormDbContext&gt;(); // Add repositories to DI container services.AddSingleton&lt;IProjectRepository, ProjectRepository&gt;(); }
Meh, that's a poor reason. Time for your manager to go to bat for you. Express to him that there are serious security concerns with using software that is approaching EOL and that it will hurt more when concerns are exploited. You might read up on some papers explicitly about vulnerabilities in Win7 to demonstrate these kinds of issues and when they became a problem for people.
I like to see code that the candidate has created, and I'm likely to be impressed if they have something substantial and it doesn't look like a class project. The code itself should be efficient and clean, and I'm likely to ask a lot of questions about the choices that were made.
Pretty much need VS2017 15.3.x to develop in net core 2.0. 
One will observe I asked for the message, also, which is arguably more important than the exception But in this specific case, I suspect the sibling commenter is correct: it's trying to resolve an XML Schema from the Internet and getting the finger. It's a good idea to instruct your XML parser to disallow chasing those links anyway, to nip XXE vulnerabilities in the bud
See, that's what most people think (and I first thought that there were threads somewhere, too). If you're using async in the usual way in an ASP.NET application (or really anything with a UI thread), [there is no thread and it's not parallel](https://blogs.msdn.microsoft.com/benwilli/2015/09/10/tasks-are-still-not-threads-and-async-is-not-parallel/). Now, you can use `.ConfigureAwait(false)` and get it to run tasks on multiple threads (ASP.NET Core doesn't pass a [synchronizing context](http://blog.stephencleary.com/2017/03/aspnetcore-synchronization-context.html) along like old ASP.NET, so it does this by default). You could also run into awaited network I/O being parallelized (I ran into that bug when 100 SQL connections were being awaited together, which led to connection pool exhaustion).
 I don't want to go through the hassle of living on the edge anymore and want something that just works. It sounds like you have already made up your mind, which is fine. .NET Core has been nothing but a PITA for me so far but I'm hopeful that it will get better. If you can get off the ground with Java and actually get something running then that seems like the clear choice.
I was talking about tasks, not async/await. When i said configureawait i meant roughly Task.Run(async () =&gt; { Console.Writeline(Thread.CurrentThread.ManagedThreadId); // will be one id await SomethingAwesome().ConfigureAwait(false); Console.Writeline(Thread.CurrentThread.ManagedThreadId); // MAY be another id }); I am aware that async does not mean thread, you asked about tasks :) 
personally I hate read programming books on the kindle...
I think the content of the article itself is still relevant. It talks a lot about the reasoning behind things. Whether his ambient DbContext package is still relevant, I couldn't tell ya. I don't know enough about how the .NET Core DI container manages contexts behind the scenes. 
I work in an agile environment. I created a set of repo nuget packages for my work, and in the past year I've had cause to migrate several projects from various orm/dals. Eg.; Client wanted to do azure doc db but then hated the pricing model. We switched our repo registration in our app container to a different type (one line code change), and voila, we pivoted the entire project to mongodb. Tl;Dr: repos are awesome!
I've been using .Net Core for several months now and have almost nothing bad to say. There is certainly a lack of info, and not many answered questions on stack overflow, but I've found it very easy to use with a slight background in previous MVC implementations.
It doesn't happen often that you completely change the db, but I've found it's nice to be able to make application specific repository implementations for caching or session stores. With the repository pattern your services don't care about your orm or datastore, so you could override your GetWhateverObjects method in a repo to pull it from the cache if its a frequently accessed entity in one application and less so in another. This is way better than having to have a service method like GetWhateverObjectsFromCache that's exposed in your service layer which may or may not work depending on the context.
You can always use Mono. Works just fine for console apps, and very simple WinForms apps.
Generally, I would argue against implementing a repository pattern on top of EF - wasted effort. That said you can still have a Unit of work pattern with EF. In one project we did - that I was quite happy with - we went for: - A models project with POCO objects for the models (used by EF but visible 'above' the Services layer also) - A database project with very little in it, except the migrations the DbContext etc. This is where you would put a repository pattern if it was felt it was needed later. - A services project which is the only place (other than the db project) that references the actual EF binaries. Services have the actual business logic and use EF directly (via the DbContext). We don't reference EF outside of this services layer, so that we can be sure all db access goes through services, and can later refactor to use a repository pattern or a different ORM later, should that be desireable. - etc .. (Under 'etc' would be the actual MVC project with controllers using the services, API layer, and of course tests projects etc). As for Unit of Work our services can optionally be passed a Unit of Work object which encapsulates the EF unit of work so, in the case of updates across multiple services calls it still implements as a single update. I think we have the page controller create the Unit of work by default so the unit of work for mvc pages is per request. Trust me this works just fine and generally, we don't have to think about it. We make a point to *not* reference EF from outside of Services project. That way we *cant* end up with EF code all over the rest of our code base, eg in our controllers, which I think would be a very tempting anti-pattern that would tend to break encapsulation over time. tl;dr No. We've been very happy not to have a repository on top of EF but we still have encapsulation, good testability (using localdb/in memory db) and we can move to a different ORM at any time by just swapping out our db layer project. 
You could probably do that by having multiple roles and putting multiple attributes on the functions that require the multiple roles. But its not as clean a solution.
The first if expression should join with a logical and, lest the other two expressions be unreachable.
I've been working exclusively on Linux for about 3 years. I was previously using Mono which was pretty stable tbh (though had some challenges) and have now migrated to Core. There's a few things about my experiences on my [blog](http://coderscoffeehouse.com) if you're interested.
It would be nice to have this functionality baked in, but you can be clever when dealing with nulls to achieve the same effect. context.Customers.Where(c =&gt; lastName == c.LastName ?? lastName &amp;&amp; firstName == c.FirstName ?? firstName);
And how's that supposed to work with index selection in the database? 
Are your issues with Core or with Linux environment? Setting up a basic running site on a Linux server isn't typically as easy as running something through IIS express via VS. I haven't really had many issues with Core so far, but my project is small. 
I'm a .NET developer with lots of Java experience. I went back and forth on what to use for my personal projects for a while. I landed on .NET Core and am happy with my decision. .NET Core 2.0 and the relevant tooling has gotten WAYYY better since I first picked up .NET Core. There are a few libraries I'm waiting for (c'mon MassTransit!), but most other libraries either have a .NET Standard version or have a great alternative.
It's not a test of your limits or math capabilities. It's a test of your basic logic skills and fundamental programming abilities. I.e., any experienced software developer should be able to do the ones I'm talking about in their sleep. Tests based on those are fizzbuzz like complexity. If you can't do it, it's a huge red flag. You'd be surprised how many people fail at the fundamentals these things demonstrate.
You should definitely get around to reading it. I'm not in complete agreement with the author's conclusion, an ambient DbContext, but I agree with his assessment of the different issues surrounding using a DbContext within repositories. Essentially it comes down to how you manage the DbContext lifetime and how you control/manage calls to SaveChanges.
heads up - check out Rebus. i was tinkering it with it the other day on another redditor's recommendation and it was pretty simple to get going. Azure Service Bus still requires full framework, but i think all the other adapters are fully .net standard compatible. i've been using NServiceBus and was having similar issue with waiting for the update. EDIT: for clarification, Rebus's azure service bus extension requires full framework since the Azure Service Bus SDK for .net core is new. Azure Service Bus itself obviously does not require full framework.
Bringing indexes into this is moving the goalpost. Nevertheless, ymmv based on DB, DB provider, and how these indexes are created. I spend most of my time in Oracle, where NVL is optimized to use the index if a value is present. ODP.NET, iirc (and forgive me if I'm mistaken as I've seldom used its query generation features outside of proving they work), will generate NVL calls from the null-coalescing operator. The query as generated by this call should happily used your indexes. I seem to recall SQL Server being considerably more picky, however. So this may generate poor results. Again, ymmv.
Some numbers to get you started: Language - 10%. General principles - 30%. Libraries and general ecosystem - 60%. Percentage of internet percentages invented on the spot: 63.876%
I've been bitten once by array covariance (`object stuff[] = new string[5]`)
ASP.NET \ ASP.NET Core.
The goal posts never moved, I started this by saying it only matters if your table is large enough to need indexes.
With .NET Core 2.0 as long as you're not porting anything it feels very much like a cleaner cross-platform 4.6, not sure what the complaints are about.
Don't pick java. It's not the right tool for the job... in fact, it's barely the right tool for any job. You may want to consider kotlin + intellij. Are you building a desktop app? In that case, don't pick .Net core either. That distribution story is still pretty rubbish, possibly (unbelievably) even worse than java (uberjar is at least, semi-practical). If you're planning a web app / service and don't need to distribute it, .Net core is probably good enough; the documentation is mixed for 1.1, 2-preview and 2.0.0 and there are a few core nuget packages you'll probably want, but won't be able to use; but its pretty good, pretty stable. The tooling is still a bit rubbish (Rider doesn't support core yet), but its almost there. I think we're at 'the worst is over, time to get things done' stage with .Net core now... but it's still a bit rough. &gt; java... is basically the same language... haha~ Says someone who has never ventured down that path. No, really no. https://zeroturnaround.com/rebellabs/how-to-avoid-ruining-your-world-with-lambdas-in-java-8/ http://blog.danlew.net/2017/05/17/why-kotlin/ https://kotlinlang.org/docs/reference/comparison-to-java.html https://arctouch.com/2017/05/kotlin-vs-java/ I'm not particularly trying to push kotlin here, but you'll notice a very common theme from these articles; don't pick java. You'll notice most things under 'What Kotlin has that Java does not' are things C# also has... ...but heck, whatever you do, don't pick java. Seriously. &gt; has plenty of jobs Fair. ...but ...are those jobs you actually want? Php also has 'lots of jobs'... but I certainly don't want to go be a wordpress developer.
Now that you mention it, I do see you said that - in a sibling thread after someone else provided a similar, if more verbose, counterpoint. It looks remarkably like the same moved goalpost.
So according to this article, the only reason to install yeoman and it's .net core SPA templates is so that it can tell you to use the bog standard dotnet command to create a new angular/react/redux project and not use yeoman at all. Waste of time.
It's crazy that .NET platform has no decent open source wiki implementation.
It's true the "answers" ecosystem is quite young, but this is a void filled really quite well by the documentation, which is fantastic! There's no issue I've had since Core 1.0 that couldn't be answered with .Net Core docs or answers from classic .Net. 
Mono XSP may be an option. It's ASP but for mono and runs on Linux in conjunction with Apache.
&gt; In that case, don't pick .Net core either. That distribution story is still pretty rubbish, possibly (unbelievably) even worse than java (uberjar is at least, semi-practical). &gt; What do you find rubbish about the deployment options?
Publishing a stand alone executable involves picking exact platforms, manually editing the csproj files to select the runtimes, then generating a folder with hundreds of files in it. https://stackoverflow.com/questions/44074121/build-net-core-console-application-to-output-an-exe
With .NET Core 2.0, platform selection is a little bit better, because you only need to select between a single Windows, Linux, or macOS identifier, rather than a bunch for each. You don't need to change your csproj file directly -- you could just call `dotnet publish -r win-x64`, for example. The number of files could still be a concern, but perhaps the [linker](https://github.com/dotnet/announcements/issues/30) helps?
oh, that's nice to hear. I stand corrected. ...but yeah, I still think a publish for a trivial 'hello world' web app that generates a folder with 350 files and a 'App.exe' buried in there somewhere, weighing in at 80 MB still kind of sucks *compared* to other solutions (specifically single file binaries you get for go, or the uberjar you get using, say, clojure). 'A bit better than using electron' isn't really the gold standard to aim for. I think the linker stuff does look good, but it needs to be baked into the default pipeline, not some weird extra preview package you download to strip dead code.
Don't know why you got downvoted, guess you triggered the hive mind
&gt; This process works very well if the client is a browser, it’s a different story when we the client is a mobile app. *sigh~* &gt; On mobile it depends on the platform you are using but they all have facilities that allow you to save the token (e.g. Android’s SharedPreferences). The irony is that the reverse is true; android actually natively supports cookies using https://developer.android.com/reference/java/net/CookieStore.html :P Of course, there are totally legit reasons to use JWT... but if you can't articulate them beyond 'something something web **and** mobile app something something reasons...' you a) clearly haven't used them in that capacity and b) you're not using JWT for the right reasons.
Why the change?
That's great that android has a cookie store. Haven't done much android development. Do you mind elaborating on what are the right reasons for using JWT? I'll update the article with them. Thanks
been a long, long time but try adding Unicode = true to the connectionstring. They might still default that to false.
There are two main reasons you might want JWT; one is practical, one is more theoretical. Practical reason: You don't need to share a session database across multiple services and servers. Lets say you have an SSO server, and a number of other API micro services. Do you federate the backend session database for them? It's a massive headache. This is the practical reason for JWT: you can auth in one place, and in other places, statelessly, you can verify. Particularly, this lets you **secure** your single sign-on server, without requiring the same strict security management for your other servers that simply consume tokens. You could also argue that its stateless and it scales and what not, but honestly, I question that. People use websockets at massive scale, which is like, the entire other extremely of a massively stateful connection, and it scales fine. Practically, I doubt this is actually an issue that people realistically encounter except at like, google scale. On a more theoretical basis, it's well articulated here: https://softwareengineering.stackexchange.com/questions/141019/should-cookies-be-used-in-a-restful-api Which is to say, that REST services shouldn't really be stateful. An App talking to a REST service shouldn't need a persistent session; it's not having a conversation with the server, it's just fetching or putting some data. So *aesthetically* its pleasing for a single isolated request to self-contain its own authorization token. ...of course, this is on the basis that your **app** has its own state management, including its own database, and it makes periodic requests to services, eg. downloading a news feed; which is categorically different from a typical SPA that will expect the server to be its database and talks to it constantly. Basically, it boils down to: Are you re-implementing cookie sessions using JWT for your web app? If yes, then don't. Unless... you have a complex server configuration that makes having a shared session database hard, or undesirable for separation of concerns reasons. 
There's also the added benefit of being immune to CSRF attacks. The browser will not send any tokens in the header by itself unlike with cookies.
Most (modern) ORMs ultimately (can) still expose IQueryable, so if you have your repository expose the Set (or query) as IQueryable, its fairly easy to move between those ORMs. 
Sql translation varies wildly. IQueryable is not a straight forward replacement between orm's. Not to mention it's only the read side.
I've heard about the session argument before although in asp.net, if you are using the cookie authentication middleware, the cookie contains the claims, encrypted. Not a session id.
Well, it obviously depends on what features you are using. I don't think anyone is promising a 100% transparent transition. But getting rid of the easy stuff automatically is still a huge time saver (for very little up front effort/design, that you get other benefits for anyway) If you are using Linq syntax for your queries, even if the sql generated differs, the result shouldn't. Complex cases might need hand touching, but the 80% should work. Add/Update at the entity level works pretty universally imo. If you are doing some sort of complex insert, you are probably doing that outside of the ORM anyway tho?
Thanks for this, I'll add a few sentences about it.
What are the "right reasons" to use JWT beyond a security implementation? EDIT: Never mind, I see the same question below answered.
Nice, just what i needed.
Dapper is the only one I've used so far. Nice and simple and suits my needs (but I was never keen on EF pre Core anyway).
JWT seems a bit insecure opposed to a HTTP-only cookie.
clicked on this topic to provide the same answer; IMHO Dapper does away with the need for ORM's. Just find a good POCO generator - or [use this one](https://visualstudiomagazine.com/articles/2012/12/11/sqlqueryresults-code-generation.aspx) - and never look back.
EF Core. It just works and I am very productive on it. Unless I am making a high performance sensitive application, I don't think i would want to switch to anything else.
You might get 80% if you use class per table, Active Record, anemic domains, but you may be surprised how quickly compatibility issues appear even with simple joins and projections, and using IQueryable willy-nilly is almost inevitably going to result in a code base with lots of scattered DB &amp; ORM specific code that is not easily switched to a different data store or ORM. That said, attempting to hide IQueryable entirely usually ends up worse. How many different LINQ translating ORM's have you used? Have you ever worked on a 5-10 year old app using NHib or Linq2Sql or Subsonic or Lightspeed or DataObjects or an old version of EF etc. and worked through replacing it with a recent version of EF? I've been building apps in .Net professionally for 15 years and have worked with all of those ORM's and more. Switching them has never been a simple task.
What is the cost of this after the trial? I cannot find it anywhere. 
However the downside is that if your site has a single xss vulnerability then it is 100x worse since that means they can grab user's jwt and use it everywhere on your site.
Both have their insecurities. If you use HTTP-only cookies you have to watch out for CSRF attacks and if you use JWT in localstorage you have to watch out for xss attacks. If you still feel HTTP-only cookies are more secure though you can absolutely store the jwt in an http-only cookie and then you can get all the benefits of the jwt.
ODP.net is for .Net Framework. This is for .net core. Oracle hasn't released official support for .net core.
Well said! Also particular helpful when working with scopes &amp; claims. BTW, through JWT signing and cert pinning on the client side you can add an additional layer of security.
&gt; Unless I am making a high performance sensitive application In which case you could just use Dapper on your already generated EF classes. That's my preferred configuration these days.
It is not decided yet. The goal is to have something quite affordable. Suggestions welcome :)
Dapper is an ORM, so I dont understand that statement.
i have heard really good things about it, i need to check it out 
Dapper was created by the StackOverflow team and it's what they use. It's considered to be a Micro-ORM and differs from full blown ORM's in that it piggy-backs on ADO.Net, it doesn't attempt to generate your models, doesn't track schema changes for you and doesn't use auto-generated models to create dynamic SQL for you. And above all else it's magnitudes faster than the majority of full-blown ORM's out there. 
It worked for a few thousand users so far ;-) What issue are you getting?
What would the benefits of a JWT inside an HTTP-only cookie be? I was under the impression that any authenticated request effectively must have: 1. https (whatever latest TLS version and HSTS config) 2. high entropy authentication token of some sort (some form of secure chain to track a request back to the user entering valid credentials) 3. any action that alters server state must be a POST 4. a form body value that prevents a CSRF from happening (double submit cookie, form value that matches unique session value [variant of DSC], encrypted token pattern) As far as I can tell, using a JWT specifically to replace some other authentication cookie only succeeds in potentially making the process of creating an auth cookie more complex (and thus potentially more vulnerable to some attack vector).
This will be VERY interesting to try out as it gets closer to completion. Very nice!
Is there anything specific you miss in the current version? :)
I will need to take a look, I am one of those that tends to avoid anything beta simply due to limited time. If I do I will let you know for sure!
 I still find it to be fantastic. I enjoyed it in 1.0, and now that it's fully-featured, I don't have a reason not to use it. It's simple, incredibly fast, and gives the benefits of using C#. I have multiple projects that use it with Angular to deliver a superb end-user experience. Super easy to maintain and is one of the most supported frameworks out there, I don't see myself using anything else for a while.
Given that 2.0 was just recently released, anyone who has been using it for a while has probably experienced some pain as things changed. If I didn't have years of experience with the full .Net Framework, I would probably have a much more positive view of Core. I've been working in a script generation project that is running on 1.1. I forgot it was running in Core until I added a reference to a real .Net dll so that I could use nameof(Entity.Property). VS-2017 let me add the reference, but generates errors on any entity properties that are nullable. 
I have been building an application for a large benefits company over the last 7 months and have been using EF Core from 1.1.0, 1.1.2, and in the last week 2.0.0. I've found that for most things, it works great (although I always use projections instead of directly pulling my full entities). I do have to say **beware** if you have many-to-many relationships. The requirement to have a join entity makes querying very difficult and often performance poor. There is an [issue on GitHub](https://github.com/aspnet/EntityFrameworkCore/issues/1368) to remove this requirement, but it is not on the current [roadmap](https://github.com/aspnet/EntityFrameworkCore/wiki/Roadmap). I have gotten around this issue by writing stored procedures for the more complex or heavy joins, and dropping down to ADO.NET to retrieve the results. I have multiple projects in my solution that abstract away the details of how I'm returning my DTOs. Recently I worked on an ATM locator as well, for a credit union, and used Dapper in an ASP.NET Core MVC application (only using Web API). The data is directly loaded through bulk insert in SQL Server, so I didn't use Dapper for any CRUD operations. I did build out stored procedures to do geospatial proximity searches, and calling and working with the data from Dapper has been a breath of fresh air. I also haven't been spoiled by SQL generation, which has helped me get back into productively using SQL directly. I personally prefer to use Entity Framework (Core) where it fits, which may be personal bias since I have been working with it since v4.3.0, but when presented with more complex persistence I would definitely reach for Dapper. Having common SQL generation is a great time saver, but as soon as you start needing complex relationships or need the full availability of your database features, a micro-ORM takes away managing connections and mapping so you can get down to your optimal solution.
I remember reading about SemanticMerge. This looks great.
I prefer Dapper myself as well. Very performant, simple to use, and setup. If you are familiar with SQL in general it makes life easy.
Well, we'll be renewing betas until we launch the commercial, and we expect to launch something really affordable. Feedback will be welcome :)
Historically Dapper would have been referred to as a mapper. An ORM would be something like EF or NHibernate that worked with deep object graphs. In late 2009 I coined the term Micro ORM in an article on InfoQ. https://www.infoq.com/articles/MicroORM By 2011 Dapper had adopted the term to describe Dapper. https://www.infoq.com/news/2011/04/dapper-released https://www.infoq.com/articles/ORM-Saffron-Conery
I still find it kind of amusing that Dapper calls itself a Micro-ORM. When I invented that term I was thinking about mappers that were only a couple hundred lines of code. Stuff that you would paste into an application and customize as needed. Still, it is "micro" compared to EF.
Please dont do webforms. Do it with MVC. Core or not core. But pick MVC. 
For full ORMs I *think* the only options are EF Core or LLBLGen. There are plenty of micro-ORMs.
I'm actually trying it out (and will be using it over the next few days). I hope this is viewed as constructive criticism because I really like the idea of this and both GitKraken and SourceTree sometimes frustrate me with miscellaneous issues, so I'm actually receptive to just trying out something different. A couple of notes of things that I'm running into: * I tried "Stash All" and it doesn't appear to do anything -- Stashing changes is something I do fairly often, so that might be a blocker for me using it as my primary. I may be missing something on how to use that in gmaster? * Right now everything feels like it's far too "big"/wide. There's a _lot_ of wasted "whitespace" around everything and it makes me feel like, for the first time in recent memory, that my widescreen monitor is just not wide enough. Fortunately I can collapse everything to get some width back, but it'd go a long way to figure out how to use at least width a little more effectively (then maybe I wouldn't feel like collapsing the Semantic Outline, for instance). * I would really like an alternate view of branches/commits on the branch explorer view. All of these big dots mean absolutely nothing to me. I have to see what the commit message is by hovering over or selecting one, so this is a huge usability downgrade from GitKraken or SourceTree. I think GitKraken has the best commit view right now, so if you could just blatantly rip them off, that'd be greeeaat. * A tree view would be extremely nice, and showing the changes per folder in that tree view when you select it would be super nice. I just got done with a big refactoring/upgrade job and GitKraken's tree view made the thousand-file changes actually bearable and understandable. At one point SourceTree threw it's tree view into the garbage, so that's one of the reasons in the past I've had to throw SourceTree into the garbage. It's a _mandatory_ feature when you need it, so at this stage I know I would need at least another backup git client if that's not put in. Not saying that's a deal-breaker for me to use gmaster as a primary (since it's not every day that I _need_ it), but it would make a sale to me much easier since it's not the cost of your tool plus the maintenence cost of another company's tool. * I definitely would like hunk and/or per line staging of changes or an alternative feature for handling. Some notes of my personal workflow that I've always thought could be improved: Sometimes I do too much work and I want to make a smaller commit to make it more smaller/atomic (especially to make sure a semi-tricky change gets visibility in the commit log for later review). The only downside is that I can't (trivially) check to see whether partial staging compiles and works as expected (if I want to maintain that each commit is buildable and passes all tests, for instance). It actually would be nice to do something akin to the _exact opposite_ operation that typically git clients support: maybe I want to select lines to keep (and stash all the rest, but also be able to easily add a few lines from that stash as I decide I need them as I author my commit). For instance, I might stash all but line 25-30 of a file (let's say I added a method), but later unstash line 35 as well (maybe it's a field declaration I'm using in the method I wrote). That way I can actually compile and run the commit I'm about to make. I'm already essentially doing that in my current workflows, it's just something that I think might be common enough that it should be treated first-class and made easier and more straight-forward. None of my coworkers go through that much trouble on their commits, because it's actually kind of a pain. Throw in some semantic smartness and maybe I could select the method itself and gmaster knows enough to add the method in it's entirety... and maybe the tool eventually could suggest committing referenced fields that changed relevant to that method? Obviously, there's lines to be drawn there (I don't want/need it to be equivalent in smartness to a full IDE, since I'm planning on compiling that _before_ I commit it and I'll discover those things at that point). Anyway, it's an interesting tool. I'll definitely keep playing around with it.
Seems like Asp.Net Core Razor Pages would be a good fit for you; [here is a blog post someone posted a few days ago that covers it.](https://stackify.com/asp-net-razor-pages-vs-mvc/) Good luck!
Can gmaster track refactorings across files? I remember that was an issue with SemanticMerge when it first came out. Also, any plans for open source licenses once it transitions to a payment model?
In subreddits dedicated to a specific language/tech/framework/etc if your answer isn't always "Our X is _far greater_ than Y and Z" you're practically asking for downvotes (although my comment does contribute to the thread so it is quite clear people on this subreddit do not understand the use of downvotes) but obviously a ranking on the internet for a comment I made means nothing in the grand scheme of things.
Mvc core 2. There is little reason to use MVC 5
I swear by LLBLGen 
But it's not micro, at all. ORM is literally Object Relationship Mapper. That's 100% what Dapper does, and all it does. It's not a Micro ORM, it's just an ORM. EF, NHibernate, etc, are just extended ORM's.
I would think about how critical this application will be to the business, and how long-lived it will be. If this is something that isn't extremely important, not very complex, or you will have the opportunity to rebuild after time to learn another technology, I would say use what you know (that is assuming the little experience you have allows you to create a simple application). The future seems kind of bleak and uncertain for WebForms, and it's fun to hate on it, but Microsoft is not likely to just drop support for it. While WebForms does allow you to build responsive websites, you will still need to make use of CSS (or something that builds to CSS) to create the proper media queries for different device sizes. Depending on what web controls you use, their pre-generated styles may cause issues on smaller devices. Since you mention PC and tablet, you will probably be fine. I would suggest something like Bootstrap as a framework if you aren't very familiar with web design/front-end development. If this is going to be mission critical or a long-lived project, I would urge you to learn ASP.NET Core MVC or full .NET Framework MVC. There will be a steep learning curve (especially since online documentation is still in flux if you choose ASP.NET Core MVC), but you will have an easier time with maintenance once the base is built.
Many people consider one or more of these to be essential features of an ORM * Create/update operations * Mapping *from* objects back to the database * Support for relationships between tables, expressed as relationships between objects * Lazy loading * SQL generation * Saving changes in batches rather than one row at a time If you said something like Dapper was an ORM in 2008, people would laugh at you. 
Please yes. 
I don't understand the *sigh*
JWT is the new 'web scale'. Everyone says to use it, and it seems sometimes like no one actually understands why. I've literally had this conversation maybe 4 or 5 times in the last 2 weeks: - blog, person, whatever: 'This is how to use JWT and manage tokens... ' - me: 'Why?' - them: '...because mobile and SPA and... JWT... is good... right?' - me: 'Why?' - them: '...? Cookies are bad' - me: 'Why? Everyone uses cookies. They're robust proven technology. They do exactly the same thing as JWT. Why are you using JWT? You're manually re-implementing the browser cookie store. Why?' - them: '....' - me: 'Well, I don't think you should be using JWT because [there are downsides](https://paragonie.com/blog/2017/03/jwt-json-web-tokens-is-bad-standard-that-everyone-should-avoid) [to doing that stuff yourself](https://developer.okta.com/blog/2017/08/17/why-jwts-suck-as-session-tokens) if you don't have a good reason.' 
 And then you laugh back and slap them with a dictionary.
For a new project .net core is great. Much easier to keep your project organized and clean. Two thoughts 1) We used entity framework -- not sure I would use that again -- for 95 percent of the time it works great but for the times that it does not it is a real pain. 2) We choose to use Razor with Jquery which was fast, easy and reliable. I sometimes wonder if we should have used web api with Angular or React as we have some UI components that is stretching Jquery.
&gt;A major change in ASP.NET Core 2.x is that the cookie middleware is absent. Instead, the UseAuthentication method invocation in the Configure method of Startup.cs adds the AuthenticationMiddleware which sets the HttpContext.User property. https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?tabs=aspnetcore2x
Are you doing an SPA and an API or rendering pages with angular components? I assume angular 2/4.
A is an **awful** use case. Really, don't do that. Just simply validate your pre-conditions in the method beginning, don't obfuscate your code and move something that is logically at first, to somewhere down the method. And the implementation is awful too: - You're needlessly introducing a closure to validate `request`. That's garbage absolutely not needed. - Creating the `Lazy&lt;T&gt;` is pointless and produces unnecessary garbage too. Either you have only a `Lazy&lt;T&gt;`, or a `Lazy&lt;T&gt;` and a `StringBuilder`. If you just replace it with the `StringBuilder` directly you have one allocation less. - The `Lazy&lt;T&gt;` you create is using the default `LazyThreadSafetyMode`, which is `LazyThreadSafetyMode.ExecutionAndPublication`. Since you're only using this `Lazy&lt;T&gt;`in a local method, there is absolutely no need for this additional locking. If you really want to use the `Lazy&lt;T&gt;` here, at least use `LazyThreadSafetyMode.None`. - A console message is no adequate use-case for condition validation. It's probably meant as an example.. But no matter what you do: The proper solution is to throw `ArgumentException`s. - What's the point of `nameof(validationResult.errorMessage)`? Just write `"errorMessage"`- it does not matter if this is renamed. It does not represent the name of the argument that failed validation. Case B is a good use case for local functions, but the naming of the inner method is weird. It's completely differently named from the parent method - do they do different things? No. Just name it as the surrounding method and add `Impl`, or call it `Aux` (coming from functional languages). Case C is borderline and needs to be heavily evaluated. Even if the local function is only called from one method, it often makes sense to place it outside the method anyway, especially when the method becomes longer. And I think really that both `LogInvalidDataMessages` and `HasValidValues` are poor candidates for local functions. And really? `Dictionary&lt;string, (bool isValid, int errorCode, string errorMessage)&gt;`? I shudder when I see code like this.
Wow, super detailed feedback, thanks!!
One other thing to consider: creating a new project (web application model-view-controller) with changing the authentication to "Individual User Accounts" does actually build out an excellent template. User logins work excellently, and it even automatically hashes the password which is created. Adding roles appears to be trivial - That being said - I might like this a lot more than using Identity because I'd have stricter control of the user logic. I'll play around with it, see if it works for my needs.
I've been waiting for them to get SQLite support in .NET Core right for years. I don't understand why it isn't on par with System.Data.SQLite yet.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Might want to avoid Microsoft.Data.Sqlite for awhile • r\/dotnet](https://np.reddit.com/r/csharp/comments/6yljgh/might_want_to_avoid_microsoftdatasqlite_for/) [](#footer)*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))* [](#bot)
Yes, it can do it across files :-) And yes, we'll have free lics for OSS
I had a problem with that package getting it to work on my pi and core 2.0. in the end moved to some random pcl package that seemed to work. Have you seen a review of the various ways of connecting to sqlite? Pretty tempted to go with CSV files like Hanslman suggests!
To get Microsoft.Data.Sqlite to work at all make sure you install the package by that name. If you use Microsoft.Data.Sqlite.Core it will miss some dependencies. &gt; Have you seen a review of the various ways of connecting to sqlite? Nope. All I know is that System.Data.SQLite works with all of my test cases and Microsoft.Data.Sqlite fails about a third of them. 
I hate the fact that we need case B. I know its too late to fix it now, but it didn't have to be that way.
This looks awesome. Now I just need it for Mercurial.
Interested to try it out but the Licence Agreement check in the installer won't detect our proxy. Let me know when you have an installer that recognises proxies (or doesn't need to) and we'll give it a go.
The most fun part was making direct API calls to sqlite3.dll to perform a database backup. Good times.
Thank you, this look like an interesting article.
Thank you for the detailed piece of advice. It won't be a long lived project or too-critical, at least I don't have to face it that way. The lack of documentation for the fresh Core framework does trouble me indeed, but then again it also feels cool trying the new thing. Do you have an opinion of on the "ready" DevExpress Web Application templates? I didn't think much of them, until I encountered a company that builds all its (desktop) apps based on these templates. If i don't opt for WebForms, do I still need a bootstrap-like framework to handle the resposiveness?
https://linq2db.github.io/
The most obvious one is multi-tenancy, e.g. xxx.mysass.com
Good suggestion. Unfortunately it didn't work when I tried. :-(
I see you've posted in this some of the other sub reddits I'm a part of. It's a bit annoying that you don't post any info about what you are aiming to build. Is it a SPA? Something with many anonymous users? Complicated database look ups or pretty straight forward? Are you comfortable with SQL Server (though it is not needed for ASP.NET Core). By the way, ASP.NET Core can use non .NET Core run times. It's a bit confusing, but what you're really going to be programming with and googling questions for is ASP.NET Core. To answer the question, I am using ASP.NET Core on several projects. Some are high usage, but are in development. They are healthcare related and are very complicated with databases. We have 10's of thousands of users and don't really need to load balance much. Just 2 web servers handle everything. It's pretty speedy, but the reason for us using ASP.NET Core is SQL Server. Due to the nature of our app, we have a lot of joining with SQL. Everything is Angular 4 on the front end, which makes our web back end simple. The pain points are with using libraries that are not .NET Core supported. This can be mitigated by switching to use .NET 4.X. We don't need to host on Linux so I haven't even looked into it. The pain points were in early versions of ASP.NET core when trying to figure out how to customize authentication. Now we use Identity Server 4 for Oauth2 and it works well enough. If you are trying to bring new programmers up fast, it's a very good choice. I think that Elixir would be rough for new programmers, though I have not used Elixir for anything beyond tutorial projects. Typescript + nodejs would be pretty simple. I have used Nodejs on some internal company sites and it was very easy to use. The good part about .NET in general is that tooling is very top notch. Visual Studio is easy to use. Logging is pretty well covered. And of course SQL Server works amazingly. Perhaps you can ask more specific questions related to what you are trying to accomplish. 
OK, answesing myself here for anyone's information... I remembered that I actually happen to have a fairly clean Windows 10 virtual machine here to try this on. I copied the contents of _C:\Program Files (x86)\Microsoft Visual Studio\2017\Professional\MSBuild\15.0\Bin\Roslyn_ to the VM. I typed this into a `hw.csx` file: using System.Windows.Forms; MessageBox.Show("Hello, world!"); Then did: csi /r:System.Windows.Forms hw.csx ... and it worked! The CSI folder could probably be cleaned up a bit as there are other tools and maybe their dependencies inside too, including VB support, but I think I've got what I need now as the only dependency other than its assemblies in its own folder + exe.config + csi.rsp seems to be .NET Framework. And our clients already have this. Great, I like "xcopy installs" as we won't need administrator rights. Total folder size is just 16 MB. BTW, default assemblies referenced / used (as in keyword "using") by CSI is inside the file `csi.rsp`. System.Windows.Forms wasn't a default reference (but you could add it) and so I had to explicitly "Add Reference" with /r. See other useful options with csi /?.
Link to the issue on Github? Test case to reproduce the issue?
This feels incredibly wrong. 
Still not sold on them.
Hm, yes, that's odd. It's the obvious open source compact database.
I found it here: https://github.com/aspnet/Microsoft.Data.Sqlite/issues/422
Websites that don't have clear pricing information BEFORE you download make me nervous. No thanks! I'll stick with SourceTree for now.
Thanks for this writeup. Yes, I think I posted it in 5 channels. Felt kind of spammy doing it, but it's given us some pretty good summations, so probably worth it. We are currently working on a monolithic Rails application which has become quite difficult to maintain. We are going to either be building a new product soon, or slicing off bits of the current application and moving it into services. To answer your questions: - It's not currently an SPA, but slices of it are built like one, and we are probably moving to one - We have some complicated database lookups, but it's mostly straightforward and most of our complicated ones really shouldn't be - I've used SQL Server extensively, and MySQL to a lesser extent, as well as Mongo (never going there again, if I can help it), only dabbled in Postgres I've used .NET extensively for ~16 years or so, JavaScript off and on for 16 years, Ruby for ~2 (and I strongly dislike it), Clojure for a few little projects. I've done some Node (including in production), dabbled with Elixir and Go. The rest of the team has a lot less experience, .NET, Ruby, JS as well as some PHP. But they are plenty capable of picking up any of these stacks. Also, I already know that all of these stacks can solve our problem, but what I don't have is a broader sense of how the stacks work out long-term, what problems people run into that I might not have found, etc. Ultimately, we're going to probably spend a sprint building some small slice of our app in each of these stacks. That, combined with these threads will probably be the biggest factors in deciding our next move. We're leaning towards TypeScript + Node, as it's probably the best balance of perf, familiarity, and tooling. But I strongly prefer Clojure for a whole variety of reasons. When I left .NET ~3-4 years ago, it was before the OSS culture shift had happened. That culture shift is the only reason it's even on the table for consideration by our team. 
.NETCore makes .NET great again! Took me a small learning curve to be efficient with ASP.NET Core, but no regret now, I love it.
I was very wary of these to begin with, but the more I see them, the more I'm OK with them. They basically prevent the need to create extra little helper functions and pass a whole bunch of parameters around, or alternatively replace a lambda function. A isn't a very nice example. B is actually pretty good. Previously, the "correct" way to do this would be to split it into two functions - one "GetEnumerator" for doing the validation, and the second helper function as the actual enumerator (which gets compiler magic'd behind the scenes). This combines them into one, and I'm not sure of a nicer way of doing it. C is OK, and in more complicated examples it can help prevent the creation of many helper functions which need a lot of cumbersome variables passed in (since the local function gets the outer scope). It also avoids lambdas, which need to sugar up a wrapper class to hold all of the local variables. Local functions are more performant than lambdas, and more convenient than helper functions.
Reading this comments first makes me not want to read the article for fear of picking up an STD 
What's the alternative? The syntactic sugar on the enum means that it's lazy by design (as it should be). There needs to be at least a function to do the parameter validation, and another one to represent the enumerator. The alternative is using two functions, the first being a `GetEnumerator` that validates and returns the second enumerator, just like this local functions example (but separated out).
Wow thanks for the info. I've seen complaints about Node and callback hell as well as difficulty getting things to not stop the main loop. But that's all probably mitigate-able. I think that having good logging is crucial and you can do that with all of these frameworks. As for leaving dotnet, I understand why. I have my issues with its abstractions such as what you get with auth. Asp.net Core has more of an express inspired setup with middleware that looks the same. However, you still need to get past the first hump with initial setup. With node, you can skip having to create your statically typed models, which might speed up dev time a bit. The cross platform dev tools for dotnet are not even close to Visual Studio 2017 yet so that may also be a consideration if your team is non windows. 
Case B, why wouldn't you just use Where(). The calling side would look like this: booksForDisplay = books.Where(validate); instead of this: booksForDisplay = BooksAvailableForDisplay(books, validate); 
I actually haven't done WebForms development since around 2011, other than some simple maintenance. I had a brief look at the DevExpress ASP.NET templates, and if the controls they provide fit your needs, it looks like it could be a huge time saver. It looks like you can get DevExpress ASP.NET Bootstrap controls (although I'm not sure if this is an extra cost) which work responsively. Again, I don't have experience with these templates, but here is a demo of the [ASP.NET Bootstrap controls](https://demos.devexpress.com/Bootstrap/). If you decide to go with ASP.NET MVC (Core or not), you will still need to build your CSS for responsiveness. The web application templates for Core and full framework MVC do include Bootstrap, so you may be able to get by on this with a few tweaks for project specific items. I do a lot of full-stack development with Bootstrap's grid layout and media queries as a base, and that is often good enough for a LOB application. There are often complaints that Bootstrap sites look like Bootstrap, but I've built applications and CMS sites that have been heavily modified and look nothing like the base styles. I would urge you to use ASP.NET Core MVC as there is definitely a better roadmap for the technology, but if you are trying to get up and running quickly without having to worry about being tied to the technology, ASP.NET WebForms will be fine.
Yes, Microsoft.Data.Sqlite.Core specifically omits dependencies so that you can substitute other forms of the native SQLite library. For example, sqlcipher.
It does not seem that your problem is with .NET Core 2.0. Check out Angular 2.0 tutorial on how to fetch AJAX data. They should have it.
Just use an embedded DB, like Mocoding Embedded Document DB. It uses whatever serialization you want... json, xml, even yaml. Frankly, I think the CSV file suggestion Hanslman made is silly. 
SPA, and ya angular 4. We build apps with ionic so it's one common workflow, and we have reusable components and services for controls where applicable. One API manager, one JWT manager, etc. Can be consumed both by the app and website, and desktop app if we ever decided we needed it.
Remove the thought of consuming your C# app. The C# backend and Angular front end are two completely separate concepts, that are not reliant on each other. It's no different, even being in the same project, as writing one that consumes someone else's, or an off server API. When you create their SPA template, look at their weather page. They show making a get call, and that should be enough to show you the way. It's the same with get, post, whatever. 
We are not yet there, I'm afraid. We are testing if the technology makes sense first. We'll try to have something ready ASAP.
One folk shared this to make it work. https://twitter.com/aateeque/status/903725478548303873 If you keep having issues, reach us by email: support at gmaster.io
It's a bit of a learning curve for me. I got to used to old fashioned MVC and couldn't do without Global.asax for initializing things on application start such as loading items from the DB to persistent cache, etc., and to also be able to call the cache or db context outside from the controller. That's my fault though as I was not following the rules for dependency injection which I am still very new at.
An async method is executed immediately until it reaches the first `async` statement. They could have designed the iterator method to likewise execute immediately until the first `yield return` statement instead of deferring it entirely. Though on further thought, that would have probably led to its own problems so I retract my claim. 
Thanks. I intended to include that link but had forgotten.
If you are doing just straight reads and writes of a single table, CSV is going much faster than parsing JSON or XML. That said, you need to have a lot of data for it to matter.
I did not notice that. Great that is what I needed to get started. Thank you!
In this case I am interconnecting with an existing system so must find a working solution. For my own stuff simple is better. I will look at the things you suggest. I am never going to use CSV :-)
CSV also has trouble with data hierarchies. Not to mention that you either have to use a clunky third-party parser or roll your own, and if your data structure changes, you'll probably have some work to do. Plus reading CSV sucks, excel sucks. Why not just look at a simple json file? Much easier to read as a developer. [Reasons to not use CSV](https://blog.datafiniti.co/4-reasons-you-should-use-json-instead-of-csv-2cac362f1943). There is a third party tool called CSV helper or somethign which will let you use syntax similar to newtonsoft.json when reading your CSV, which helps, but you still have the short comings of CSV. Why not just use the repo pattern, and have an embedded document db repo concrete to use for simple prototypes? When the prototype gains traction and you need to pivot to something better, you can swap out your repo for a SQLite repo or even a HTTPClient Repo that calls an api for you, with 1 line of code change. I actually do this all the time at my work, and I can set up a prototype to use our Embedded Document DB repo just by pulling down a nuget package, and defining my models. Or, if you really want to use CSV you could create a CSV repo instead of using a document DB repo; but still, csv sucks. 
It's a real PITA - not at all easy with docs that would overwhelm a beginner (and still confuse seasoned developers). I put together a really simple boilerplate project [here](https://github.com/matthewblott/simple_aspnet_auth).
I love that you can use fluent configuration and POCO persistence models thus making it easy to swap out ORMs at the infrastructure layer with EF Core.
 Every windows since windows 7 comes with .net installed, (7 - 3.5, 8 - 4.5, 10 - 4.6 AFAIK), so no need to do such shenanigans as to copy csi. Also if it's for console apps, you can use .net core which is self contained. Or just use powershell ( which can execute .net code from dlls ) 
Is X: a mapped network drive? You'll probably not be able to access drives mapped for one user while impersonating another.
If you need to deal with data hierarchies then yea, its probably the wrong tool. But don't give me this bullshit about needing a third party library. .NET has had a robust CSV parser since version 1.0. 
This is the correct answer. A mapped network drive is local to the current user profile, when you impersonate a new user you lose the mapping.
Solved, thank you.
Pray tell, what built-in csv parser are you referring to? Do you mean the TextFieldParser that is found in the VB library? [This one?](https://msdn.microsoft.com/en-us/library/microsoft.visualbasic.fileio.textfieldparser.aspx?f=255&amp;MSPPError=-2147217396) Good luck using this in .Net Core. but also the usage sucks: using (TextFieldParser parser = new TextFieldParser(@"c:\temp\test.csv")) { parser.TextFieldType = FieldType.Delimited; parser.SetDelimiters(","); while (!parser.EndOfData) { //Processing row string[] fields = parser.ReadFields(); foreach (string field in fields) { //TODO: Process field } } } Gross. I'd much rather use simple Json: var data = JsonConvert.DeserializeObject&lt;List&lt;MyModel&gt;&gt;(json)); So if you want good syntax sugar and you don't want code bloat, you're either rolling your own or using CSVHelper. Here's a CSVHelper example: var csv = new CsvReader( textReader ); var data = csv.GetRecords&lt;MyModel&gt;(); Which is better, but it's still CSV, which is ugly to read if you have to actually read it yourself. Anyways, I stick to my suggestion to use simple serialization like JSON, unless you have a really good reason to use CSV. 
A lot depends on your use case. If you are loading bulk loading files into a database, you can't afford to convert them all into objects. It's much better to write a simple adapter that exposes the TextFieldParser as an IDataReader.
In a situation like that, sure. That would be a good use case, one of the few that I would accept. In a situation like that I'd use SQLBulkCopy. https://msdn.microsoft.com/en-us/library/434atets(v=vs.110).aspx
How big is the app that your employer wants you to rewrite? What is it currently written in? What is the main reason he wants you to rewrite it? Especially if you don't have much experience with it, it might not be a very good idea
Since you seem to have a firm grasp of core 2.0 + angular, I have a question that's been nagging me. What is the typical workflow for running an asp.net core 2.0 app with angular? Angular is typically run using nodejs, but debugging the backend is done using kestrel. Do you have to startup nodejs, then start debugging (f5) each time you want to test new code? Wouldn't nodejs be running on a different port than the debugging session? Or does hitting f5 just start up nodejs to work with the front end? What if you want to run your server using apache, would you also have to run nodejs, or is it pretty each to switch to apache to run the angular app?
It's not very well written. There's not a lot of explanation and full of ads. Maybe add some more explanation? And remove some ads?
Agreed, CSV is gross, you can always put JSON into db also
Is it a solution with multiple projects? Do any of the projects have a migrations folder? [This is my goto site](https://coding.abel.nu/2012/03/ef-migrations-command-reference/) for when I brain fart an EF command.
That sounds awesome. I want to set myself up with a similar system for a passion project I want to start. Do you know of any good starting points you could share? 
I'm not great with the yield keyword, but wouldn't you lose that ability in your substitution?
I have a trigger in my csproj to kick off the node stuff on build.
Because System.Data.SQLite contains native code.
So does Microsoft.Data.Sqlite. The difference is that Microsoft.Data.Sqlite allows you to plug in the correct native library based on what OS you're running on. In this way it can offer a .NET Standard version.
These examples are gross. I can't justify the use of local methods for these use cases. To me, usages of these local functions would just muddy up a code base. I'd much prefer to just do the logic in the method... unless it will be called by other methods in which case extracing the logic to a private method makes sense... but these local functions do not feel valuable to me. 
I'm not really sure there's a need to be an asshole about it.
&gt; Don't pick java. It's not the right tool for the job... in fact, it's barely the right tool for any job. You may want to consider kotlin + intellij. I've developed with Java / Spring, C# / .NET, and Kotlin / Spring. To say Java is barely the right tool for any job is a really bold statement. I'm not interested in starting a flame war over technology stacks, but at the same time your comments are misleading to the OP. JVM and .NET are both solid technology stacks and suitable for building small applications to enterprise software. 
I need it to implement my production Fibonacci number generator
[hahaha](https://media.giphy.com/media/vWDrezW0rMjmM/giphy.gif)
My only grip with .NET Core is EF Core Migration. I am scared every time I make the changes to the model and generate the migration. 
Have you tried using the #r directive to reference assemblies?
The only issue I see with this is there is no tooling yet (SSMS) for SQL Server on MacOS or Linux (I dev on Fedora). Otherwise, there is no reason you can not connect to a remote SQL Server Database. I will note that you don't need to use SQL Server and .Net can also handle MySQL, PostgreSQL, MongoDB, and more. I have successfully deployed ASP.net Websites with a MySQL DB myself.
So even if the SQL Server is hosted on a cloud, I can't connect to it on my Mac's VS Code since there's no SSMS on Mac? I was just wondering if it's possible to host it remotely and connect to it instead of running a localdb, because it seem like the same idea. I prefer SQL Server because I already learned some of the basics on Windows environment with Visual Studio and SSMS running localdb, was wondering if I can replicate that on MacOS.
You might want to check this VS Code Extension? https://marketplace.visualstudio.com/items?itemName=ms-mssql.mssql
Why do you need to load the report into the iframe? Are you trying to load just a report or is it a page that has buttons, links, other controls and a report viewer? What technologies are we talking about? 
The framework will allow you to connect to azure sql server on Mac. They were just saying you won't be able to administer it because Mac doesn't have ssms. The poster below mentions the mssql extension for vs. It looks like ssms lite inside of vs. You can just use the shared sql in azure for testing/development, you don't have to set up a vm and install sql server. 
You can run SQL Server locally via Docker on your Mac, if you'd like. See [here](https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker). No VM or Windows necessary.
VS 2010, .Net 4.5...basically the application I work on is internally facing, only available to members of a marketing group. However we have a publicly accessible application that they would like to display a handful of the reports from the internal application. The portion of the HTML that would need to be in the iframe would have buttons, links, etc and those buttons and links would need to be functional because they allow users to update the data displayed within a report.
If it's publicly facing there's probably authentication before they can update report data? When editing data are modals used? I would see if just having a link to the report that opens a new tab would be sufficient. If you don't have modals you can probably just put in an iframe with the source set to the report page. You might be able to do jquery Ajax to just grab the response and spit into a container div, but not sure that would help if you go to new views for data editing. Worst comes to worst you could grab the code for the reports and import into the internal app. Edit: you might run into cross domain security issues. Not 100% certain, but I think as long as the iframe contents don't try accessing the container documents Dom you should be ok. Haven't worked with iframes in a long time. 
ferociousturtle we literally could be brothers, I have nearly identical dev background as you. &gt; We're leaning towards TypeScript + Node, as it's probably the best balance of perf, familiarity, and tooling. But I strongly prefer Clojure for a whole variety of reasons. Clojure the language(lisp+immutable) and interactive development is a thing of real beauty. I honestly LOVE it. However clojure the core team, tooling, community, docs and it's direction leave me DEEPLY disappointed and frustrated after 6 years of occasional/often use. It's a shame and from my perspective the window for it's longer run viability is closing. Trying to raise concerns seem only to fall on deaf ears as those in the cognitect bubble seem blind to the wider perception of clojure/cljs because their consulting business seems great... for them. Clojure is a poor choice for a team unless you don't mind being led on a journey where you have zero visibility or input, community concerns are largely ignored, and are committed to literally building and maintaining the toolset from top to bottom on your own. Typescript + nodejs is simply a better choice when considered as a whole and honestly the one I would make on a risk/reward business perspective for a team. Good language design and leadership(anders), solid tooling(vscode,definitelytyped) and huge community (with growing momentum) that will only improve the typescript landscape over the next decade. 
I'm using Entity Framework Core and SQLite support is OK. Only place it sucks is it can't migrate tables if it would have to alter them since SQLite can't do that. You have to destroy and rebuild the table as a workaround.
You know, that's probably why it sucks so bad for me. They are focusing on the features needed by EF, while I'm dancing with more complex SQL.
It's a pretty big app, but we are also a small shop. I am the only C# developer and my manager is the main front end/Angular developer. So he is wanting me to learn it to assist him with some front end work in rebuilding our app. The current app is written in ASP.NET 4.5 and angular 1.3
&gt; Case B is a good use case for local functions I've never used `yield return`. What's the difference between the original (parameter validation omitted for brevity): public IEnumerable&lt;Book&gt; BooksAvailableForDisplay(IEnumerable&lt;Book&gt; booksAvailableInStock, Func&lt;Book, bool&gt; validate) { return ValidBooksForDisplay(); IEnumerable&lt;Book&gt; ValidBooksForDisplay() { foreach (var book in booksAvailableInStock) { if (validate(book)) { yield return book; } } } } And this? public IEnumerable&lt;Book&gt; BooksAvailableForDisplay(IEnumerable&lt;Book&gt; booksAvailableInStock, Func&lt;Book, bool&gt; validate) { foreach (var book in booksAvailableInStock) { if (validate(book)) { yield return book; } } }
&gt; To say Java is barely the right tool for any job is a really bold statement. Everyone has an opinion, and I have absolutely no hesitation to stand by mine: **don't pick java**. I've also used java, and while its certainly true to say: - Many systems are built in java - Many people use java and have used java to build successful products - Java has an excellent ecosystem and is a robust platform to build enterprise software ...but that doesn't mean it's fit for purpose for all the things its used for, or without problems, or without a certain amount of 'WTF are you doing with EE oracle?' or that people who use it like it. The reasons why are *very clearly articulated* in the articles I linked to above. You don't have to agree, and to be fair, 'barely the right tool for any job' is probably a bit harsh... but I'm not going to apologise for saying don't use java. I think there's plenty of well articulated reasons why kotlin was created, by probably one of the best programming groups on the planet, who are **heavily** invested in java, and why google has officially adopted kotlin as a tier 1 language for android. ... so you know... it's a solid tech stack, sure... but I wouldn't recommend it.
I wanted to look more into developing a resource-based access control, not just role based. Role based gives general access, but resource based gives granular control over who can access what resource. You'll be able to do queries from a certain users context such as `_securityManager.validate(User, "application:memberships:premium", Actions.Access)`, or `_securityManager.validate(User, SomeObjectInstance, Actions.Edit)`
You left out an important part of the original code: The `null` check. You should design your methods to throw `ArgumentException` or `ArgumentNullException`s on invalid arguments. But when you use the `yield` keyword your method is deferred, means it's only executed when the iteration is executed. That means also your `null` checks are deferred. Take `.Where(..)` for an example. It requires you to pass a non-`null` delegate as an argument. If you, for whatever reason, accidentally pass `null` you'd want to throw **immediately** - and no at some point later when you started iterating the enumerator.
Ah I see, that's pretty cool, you could technically hack that in with roles but it would be an absolute monstrosity of redundant awful code to do it.
X-Post referenced from [/r/csharp](http://np.reddit.com/r/csharp) by /u/smatsson [Migrating from ASP.NET Core 1.1 to 2.0: Custom authentication](http://np.reddit.com/r/csharp/comments/6yemm4/migrating_from_aspnet_core_11_to_20_custom/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
I understand about the `null` check, which is why I wrote "(parameter validation omitted for brevity)". My question was about the local function - is there any difference between returning the inner function versus just having the contents of the foreach loop?
I do not understand your question. Can you elaborate? The only purpose of the local function here is to have the pre-condition checks without deferring.
I think I understand now. Let's say I restructured the code, as shown below, to omit the local function and have the foreach loop directly in the BooksAvailableForDisplay() function body. I had assumed that the validation would occur and only the foreach loop execution would be deferred. But if I understand you correctly, then the entire function execution, including validation, is deferred. Is that correct? public IEnumerable&lt;Book&gt; BooksAvailableForDisplay(IEnumerable&lt;Book&gt; booksAvailableInStock, Func&lt;Book, bool&gt; validate) { if (booksAvailableInStock == null) { throw new ArgumentNullException(nameof(booksAvailableInStock)); } if (validate == null) { throw new ArgumentNullException(nameof(validate)); } foreach (var book in booksAvailableInStock) { if (validate(book)) { yield return book; } } } 
Returning an Action or Func is really the only sane use case for local methods, I use them pretty regularly in Python to return a callable of some form for various reasons.
That is correct. The whole method is deferred.
OK. Thanks for wading through my mental mire.
We should do it. By we I mean you. 
I'm new to SQLite so this might not make sense, but why not dump the source database and execute that to clone it to a new one? I also have only used System.Data.SQLite so I may be missing something. 
Because of the locking (shm and wal files) you can't just copy a DB file while it's running. The risk of corruption is ridiculously high. I wanted a backup without killing my program, so this was the only safe way. 
The jump from angular 1 to 2 (or 4) is quite big. I'm not sure about how much work the backend would be. But I think you do need to think about how much value a rewrite would give you, and if that is worth the work. And not just do it for the sake of keeping up to date (although that definitely has value)
You are lucky, I have exactly what you want! A star is welcome :) https://github.com/lugrugzo/AspNetCore-Stack/
Yes, I'm just not looking for compiled code and would prefer to script in C# over PowerShell although I'm aware there is PowerShell. I don't think a REPL environment for C# is shipping with any version of Windows?
No, but I'm sure they work as well.
You can host it in Azure and connect to it using any dotnet core application, but you might find the latency too horrible to work with. Whether you host it locally or in the cloud you should almost always be able to connect to it with SSMS If you're on Mac, check out DataGrip by Jetbrains. Pretty good tool that should at least get you started. It can connect to a multitude of different databases so it's an all round decent application
I really wanted to use windows containers, but setting up multiple containers into a service and getting the networking right is painful compared to using linux. NAT sucks. Another problem is you cannot run both windows and linux containers. You have to switch between them, but the vast majority of useful, canned containers (like docker registry) are not windows. If I want to spin up my app + elk stack + docker registry it has to be linux containers.
If you figured out how to do that, why not send a PR for [#17](https://github.com/aspnet/Microsoft.Data.Sqlite/issues/17)? ;-)
One external contributor has made [a big impact](https://github.com/aspnet/Microsoft.Data.Sqlite/pulls?q=is%3Apr+author%3AAlexanderTaeschner) on the product. Notable contributions include: * Data change notifications: SqliteConnection.Update * User-defined functions: SqliteConnection.CreateFunction() &amp; CreateAggregate() * Custom collations: SqliteConnection.CreateCollation() * Prepared statements: SqliteCommand.Prepare() * Query result metadata: SqliteDataReader.GetSchemaTable() It's open source. Why not jump in and help?
+1 Super painful to work with. Everyone upvote [#329](https://github.com/aspnet/EntityFrameworkCore/issues/329) so it gets prioritized appropriately.
Since I'm building an ORM that uses it, I'm already contributing a lot of time towards testing it. And that's probably the best use of my time. Instead of wandering around code that I don't understand, I can quickly find and document usage patterns that they weren't expecting. 
Gladly. SQLite is the *only* use case where I actually want EF to modify table directly.
It turns out to be SQLite weirdness leaking into the way batching was implemented. Open readers can be affected by subsequent statements. You can see the same behavior without batching: var selectCommand = connection.CreateCommand(); selectCommand.CommandText = "SELECT * FROM Employee;"; var reader = selectCommand.ExecuteReader(); var deleteCommand = connection.CreateCommand(); deleteCommand.CommandText = "DELETE FROM Employee;"; deleteCommand.ExecuteNonQuery(); // reader returns the first row, but nothing else since they were deleted Be careful when mixing SQL and DML in batches. The team is deciding whether to change the batching behavior or not. Changing it has other negative consequences...
Azure has SQL Server ready to go for $5.00 a month. Just go on there, hit Sql Server, and deploy it. It manages everything with it. 
&gt; contributing a lot of time towards testing it Testing 3rd party code instead of your own :S Unless my employer was dedicated to it, I would probably have abandoned it https://www.youtube.com/watch?v=yo3uxqwTxk0
Oh I did abandon it at one point. But if I ever want to support .NET Standard I need Microsoft's version. Also, it is much better than it was when I started. There was a point where it couldn't even return the correct data type. I forget the details, but it was something stupid like an integer column coming back as a string or decimal. 
Is the Microsoft one marked as thread safe?
He'd had over 24 hours to make that edit, three months ago... And he originally posted it all the way back in September, and *still* hasn't updated that article to include a disclaimer. After 9 months I think it's reasonable to call out meaningless platitudes!
Nice code. Is there any easy way to share the login with multiple applications (token in this case?). Say I have App1, App2 that require authentication and be redirected to generic LoginApp which does the authentication.
Look at identityserver 4 
 Just learning both asp.net core 2 and angular separately is all I can say. Don't learn them together, as they are completely independent, they just work nicely once they are brought together. I don't know your current knowledge on either language so it's harder to go farther than that, but ya, just take them as two separate things starting out, it's a lot easier to find resources that way
 I use visual studio code for the most part, so I usually do everything console based. When I'm first fleshing out an app, I use the command "dotnet watch run", it will use webpack for your client side and dotnet watch for server, lets me get an example running incredibly fast. In terms of debugging, I debug my typescript directly in my browser, not my IDE. It lets me keep debug breakpoints on my client interactions at the same time as my C# back-end, so I can walk through on both sides without it trying to jump back and forth. And I've hardly used apache, so I apologize I can't answer your last question :(
If all you're running is ASP.NET Core, what's the benefit of doing it with Windows Containers instead of just the usual Docker / Linux?
aren't you supposed to use dependency injection for everything in Core? public UserController() { _userService = new UserService(); } would become public UserController(IUserService userService) { _userService = userService; } Furthermore I can't help but write one-line methods these days like this, because I'm a sucker for sugar public UserController(IUserService userService) =&gt; _userService = userService;
Why would you do this instead of implementing and overriding types in aspnetcore identity? https://www.nuget.org/packages/Microsoft.AspNetCore.Identity/2.0.0
&gt; You're needlessly introducing a closure to validate request. That's garbage absolutely not needed. I believe it'll be a struct closure in this case, and so won't generate garbage. The compiler can create a struct closure because no delegates are creates and thus no escape out of the main method is possible. It's even possible it'll eliminate the closure struct entirely, since it would only contain a single field.
&gt; Though on further thought, that would have probably led to its own problems so I retract my claim. Yeah, it could lead to other iterators being consumed too soon, I guess.
Yeah, in this particular case. But you can imagine much more complicated scenarios.
B is kinda neat, though, given the limitations in yield return.
&gt; Returning an Action or Func is really the only sane use case for local methods Certainly not, but in that case they are just named lambdas.
&gt; Local functions are more performant than lambdas As long as you don't create a delegate object for them, then it's exactly the same.
As far as I know in Identity, its developers are encouraging us to use claim based authentication but I want to use role based. Also wanted to see how things works in platform.
Yeah, it totally doable.
You might need to run on the full .net framework which needs server core.
True, I'll give them that
Identity supports roles. The biggest difference between the two as far as I can tell are that roles are retrieved by your application where claims are dictated by an authorized party (the issuer) and thus are available as part of the security principle (the authorization bearing object). So the userid is a Claim (as is your random token), it can be used to find the roles via a db query. If you wanted to use claims everywhere, you would add the claims when logging in to the security principle.
I think you can run both, it's just that only one client can be run at a time -- that is, both the Windows and Linux daemon stay running 
Occasionally industry terms mean more than their literal interpretation. Do you also believe JSON can only be used by JavaScript?
NHibernate is still a valid, if painful, option. It still has better mapping capabilities for certain types of mappings.
Oh right. That... makes local functions somewhat less useful.
That comparison doesn't even begin to make sense, do you know what JSON is? It's Javascript Object Notation, its literally based on a subset of the Javascript language, it 100% makes sense why it's called JSON, and even using it in other languages, it's still the JSON standard. Wtf?
Thanks for detailed answer!
Please stop. Razor pages are an abomination. Using razor doesn't stop it being webforms. Don't bring back webforms. Don't sell it as a way to get going quickly, that's how we ended up with all the webforms crap in the first place.
With 2.0 being released most of the pain points went away (especially when it comes to referencing non core packages). The Identity membership system still feels a bit wonky and there are some bugs related to using IdentityDbContext in conjunction with your own EF contexts. Also, if you need SignalR, it's still not available, alpha said to show up next week though.
Ideally the form would get data from the classes and then perform operations on its controls. If your class takes the form in its contructor it now can't be used on its own (it is tightly coupled to the form)
I don't do VB so coming up with a good example is tough, but I recommend looking into the **humble dialog** pattern. The idea being you create a controller that does all the logic, updates lists, performs the commands and button clicks, no references to any ui framework at all. then you have a IMyFormView contract, you inject it in the controller constructor and the controller calls set functions on the view contract to tell it to update itself, it also uses a ``SetController`` function to pass itself to the view, now the view can call ``ctrl.SomeButtonAction();`` etc. in short the View logic is in a controller class, the view itself literally ONLY deals with taking received data, displaying it in form controls and assigning selected values. that is it. It is similar to MVVM , though it differs in that the contract between the controller and the view are tightly coupled but that the view contains no real logic of any kind. 
&gt; This summer we had the awesome opportunity to be part of Microsoft’s Explore Program, a 12-week internship for rising college sophomores and juniors to learn more about software development and program management. As interns on the Visual Studio Web Tools team, our task was to create a web application template as a pilot for a set of templates showcasing new features and best practices in Razor Pages, the latest ASP.NET Core coding paradigm. ie. A set of *interns* tasked specifically with 'go use razor pages for something' made this. So... you know. Don't super critize it; it is what it is. I just feel vaguely sorry they ended up building something so kind of uninspiring. To go intern in the web tools team and walk out having made a 'hello blog' app you could have done with a few tutorials seems a bit disappointing. 
You can easily create a template for this to be used in dotnet new and/or VS2017. See https://blogs.msdn.microsoft.com/dotnet/2017/04/02/how-to-create-your-own-templates-for-dotnet-new/ and https://www.youtube.com/playlist?list=PLqSOaIdv36hQdvGVkMXDJJ4Uh-nGj3NRK
What were they supposed to do with the zombie corpse of webforms? 
Ideally (IMO) the way to interact with form controls from within a class is to create a Property in the form that does the heavy lifting. For example, let's say you've got a form with a textbox called txtUserName. You've also got a class that (1) pulls the name from a database and populates the textbox, and (2) pulls the name from the textbox and sticks it back in the database. The easy way to handle it would be like so: Public Property UserName() As String Get Return txtUserName.Text.Trim() End Get Set (ByVal value As String) txtUserName.Text = value End Set End Property The benefit to doing it this way is two-fold: * First, it allows you to have a simple way to interact with the control from multiple classes - they all just have to set Form1.UserName to whatever, or read Form1.UserName into whatever. * Second, if you change the control (say, from a textbox to a combobox or something) you only need to update your code in one place - any class that interacts with the form will keep seeing it as just a regular old String, regardless of what it is/does in the form itself. In essence, it treats the control as a "black box" where the class doesn't care one whit about the structure of the form, all it knows is that it expects/returns a string. At least that's my 2c. Other people might have different/better ideas, but this is generally my go-to if I need to refer to a form's controls from an external class.
&gt; https://blogs.msdn.microsoft.com/dotnet/2017/04/02/how-to-create-your-own-templates-for-dotnet-new/ Thanks, I'll set one up :-)
Great, let me know if you need any help. 
FYI see my reply about how you can create a template from this https://www.reddit.com/r/dotnet/comments/6ykyji/comment/dmry7xr?st=J7DO7IYR&amp;sh=b50d5c9e
Cheers, from the link you provided it looks fairly straightforward no?
Yeah it's super simple
I thin what you are looking for is User.IsAuthenticated
Good！thanks
Sure, you can. But what gain is there over a standard private or private static method that produces the IEnumerable via LINQ instead of yield statements, which few even know about tbh. Why is a local version truly needed? I think this is a feature that might have use but has yet to find a use.
How is B good when we have existing solutions for lazy-validation like: ```return booksAvailableInStock.Where(Validate);``` Does something happen to this when compiled? Do local functions of this nature compile significantly different IL than LINQ's where? Because it is [pretty similar code too the .NET Core Where implementation](https://github.com/dotnet/corefx/blob/master/src/System.Linq/src/System/Linq/Where.cs#L60) yet the LINQ equivalent will be significantly cleaner and simpler.
Makes sense! So the classes should be independent from the form. I guess I should dabble on how to set Properties.
Thanks for the sample code. I read a lot about Properties but I have little idea how to implement it. I guess I'll refactor my classes.
Well... yeah. Fair call; but perhaps they could have been guided in a more practical direction. Like, you want to make a blog template using razor? Is that really useful to people? Probably not. ...but maybe if you make your project: "Here's how you turn your existing `Project` in a `dotnet new ...` project template", that could have been cool? Like, *even if* you make something that's not bad (and to be fair https://github.com/VenusInterns/BlogTemplate isn't that bad, it's just a bit messy here and there, and uses (*shudder*) [BindProperty] too much), its pretty dubious how useful that is. ...but I bet you a bunch of people are interested in how to take an existing project and turn it into a template. I know I'd like to hear more about that part of the process (the video covers it to some minor degree which appears to summarize as 'it wasn't that great an experience'). Maybe you could have made a set of simple templates from a much simpler set of example sites, and shown how to templatize existing projects. Even if you're using Razor, I think that would have been valuable. 
Yes, but they've been tasked with convincing people who are using .NET Core that you can build interesting things with web forms reborn. That's the project requirements.
The whole point is that you don't want the validation to be lazy, otherwise you could just put in the enumerable. It should throw as soon as the enumerable is created. You can go and make a Where extension that does this validation for you, but that just moves the problem into an extension method.
What is in ConvertToHashEntryList?
That example B should also be lazy evaluated too though? It uses a yielding IEnumerable just like the Where extension.
Two things: 10 seconds sounds like a lot for a small project. Are you doing anything else but compiling changed `.cs` files in the reload? Because that's all you *should* do. I use both macOS and Windows, and I don't really see a big difference in the experience. The time to compile should be roughly the same, given the same hardware. Only difference I can think of is if you're using IIS Express as the host instead of Kestrel? That might save you from "connection refused" errors. But technically, the same things need to happen on both platforms - the C# code needs to be recompiled.
I hate to be *that* guy, but do you really need EF-core? It's clearly not ready for prime time. 
Include is just `.Include(o =&gt; o.OrderDetails)`. You specify the collection to eager load, not a specific instance in that collection. In EF6, you would use `.Include(o =&gt; o.OrderDetails.Select(d =&gt; d.Vinyl))`, but I'm not sure about that syntax in core.
You might consider creating an issue on [the GitHub repo that contains `dotnet watch`](https://github.com/aspnet/DotNetTools).
.NET Core is an implementation of .NET Standard. It's just like .NET Framework. You can implement an HTTP-based API directly on top of .NET, but it's going to be painful. Web API is a framework built on top of .NET in order to make API development easier. In ASP.NET Core (related to the work done for .NET Core, but not the same), Web API has been unified with MVC to be one product. At this point, for greenfield work without other dependencies, I can't think of a single reason not to use .NET Core (if you needed some particular DLL or something that wasn't available, that's a different story). ASP.NET Core is definitely going to be the most hyped route, as it is Microsoft's chosen one. You'll find lots of tutorials and examples. I'd encourage you to take a look at something like [Nancy](http://nancyfx.org/), though, which involves a lot less ceremony and looks more like what's happening in other programming languages. I personally enjoy the more bare-metal feel of Nancy over something like ASP.NET Core for a simple API.
I can't find the code for it in StackExchange.Redis by looking through GitHub, but I would have to assume that it reflects over the object passed in to generate a `HashEntry` per field and/or property. A HashEntry is basically just a key-value pair, and a hash in Redis is really just like a Dictionary in C#, but with the ability for values to have different types. I wrote some code at work to do the same thing dynamically; it would reflect over a type once, generate the code to serialize and deserialize it to/from hash entries, and then execute that code to store things into Redis. It's a thing of beauty, and way more efficient than any kind of JSON serialization (which was our original approach). You can also manipulate hashes on the server without bringing them locally, so you could just increment one field or pull back only the hash entry you need.
https://www.visualstudio.com/vs/community/ &gt;For organizations &gt;An unlimited number of users within an organization can use Visual Studio Community for the following scenarios: in a classroom learning environment, for academic research, or for contributing to open source projects. &gt;For all other usage scenarios: &gt;In non-enterprise organizations, up to five users can use Visual Studio Community. In enterprise organizations (meaning those with &gt;250 PCs or $1 Million US Dollars in annual revenue), no use is permitted beyond the open source, academic research, and classroom learning environment scenarios described above.
You can use Prefix to profile entity framework (for free). Helps verify the weird queries it creates and if it is doing any n+1 type queries. https://Stackify.com/prefix/
For .NET Core you can use MVC to return JSON, XML or probably even custom media formatters. There are other wire protocols like msgpack, protocol buffers, bson, etc. Web API is gone https://stackify.com/asp-net-core-web-api-guide/
Dapper for querying data. EF for updates.
Also consider using the new Razor Pages instead.
It will depend on; Where and how you want to host it What format will the API accept data and output data in Amount of business logic handled by the api
Try adding --no-restore. This adds a nontrivial chunk of time to each call of dotnet-run. https://github.com/dotnet/announcements/issues/23 
Web API is not gone. There was a roughly 80% code duplication between MVC and WebAPI and MS merged them. They do still have some unique functionality.
Right, it's part of MVC now.
With core 2.0 I spend a lot more time waiting for builds, and I'm not to sure why. 1.x releases built way faster... What's more annoying is that with 2.0 it appears to compile all dependent projects whether they've changed or not.
In this kind of pattern, would `Core` models be able to annotate things that apply to `Web`? Such as: * `[JsonProperty(NullValueHandling = NullValueHandling.Ignore)]` * `[StringLength(45)]` * `[Required()]` ...etc? Similarly, should the mapping for the `DAL` project be performed inside the DAL, or outside of it?
I was dealing with this kind of crap yesterday with .NET Framework. I need a way to say "fuck it, just use whatever version is provided". Changing app.config for every minor update is ridiculous.
&gt; I can't think of a single reason not to use .NET Core Really? I was under the impression it still needed to mature a bit. Something about some functionality not available or something. (I'm not trolling here, I just don't follow it closely. My info is outdated possibly.) Soon, (a week or two) I'm likely going to be upgrading an ASP.NET Webforms site. Its a business site CRUD app that has user accounts, special media files available only to certain users, forms that users fill out, a way to turn data into pdfs. It uses Entity framework, and MS SQL at the persistence level. We might want to do a front end in Angular 1 or VueJS. So ASP.NET core is the way to go with this site? 
Maintaining a multitargetted netfx / netstandard15 SDK for the last three months with the utterly flakey support in VS17 has been my own personal hell. Going to push everything to standard20 shortly and hope to high heaven it's tolerable.
.NET Core 1.0 was slimmer on features, but still usable for a lot of line-of-business work. .NET Core 2.0 is out now, and implements significantly more features (32k APIs vs 13k before). The foundation is solid; it's basically just the .NET Framework made portable (the way it was supposed to be). The biggest problem right now is external dependencies. If you need WPF, GDI, Windows-specific functionality, or some NuGet package that hasn't been updated (although a lot have been updated to take advantage of .NET Standard, which .NET Core implements), that's where your problems will come in. But for a simple HTTP API, you should be solid. Entity Framework Core is supposedly significantly different; I avoid EF like the plague (personal choice; I don't like heavy ORMs). I don't know enough to tell you much other than that you can use it. As for SQL Server, I'm using it in console apps (.NET Core 1.1) right now without any issues. If you're going to do a front-end in Angular or VueJS, I'd think about what you need in the way of a back-end. If you want a heavier back-end, I'd say ASP.NET Core 2.0 is a decent fit. If all you really want is a thin API for a heavy front-end, then I'd suggest looking at Nancy first, but ASP.NET Core is again perfectly fine.
Cool. Thanks. I'll look into it. The unknown right now is the ability to create .pdfs I'm going to research how doable this is for Core 2. Also, we might have to fax these .pdfs somewhere. (I know... Fax. Trust me I think it's stupid too.) 
I just recently switched from multi-targeting to netstandard20. It did consolidate my references, but the .NET Framework project is still deploying tons of .dll files.
That sounds pretty much on par with my experience. I haven't found any work around other than splitting the code into multiple assemblies, but if you find one, I'd love to know about it...
if you accept let them buy you either a VS prof. licence or better a MSDN subscription - don't accept if they are not willing to do so Good luck
We are always open to hearing about new feature requests. Could you raise this as an issue on the GitHub repository?
Map inside the DAL for sure, that way only your DAL ever needs to know about your database models. As for data annotations, you *can* do that in core, however you might also want to consider annotating your viewmodels (in WEB) instead. I'm not particularly precious about which approach you use, there's pros/cons to both. For larger, more complex projects the separation definitely helps but for smaller projects there's likely little to be gained.
dotnet watch is slow, even on SSD. I don't know why they can't just run the C# code on interpreted mode for development.
.NET Core 2 finally has BinaryFormatter so serializing objects to Redis is viable. https://github.com/dodyg/practical-aspnetcore/blob/master/projects/aspnet-core-2/features-session-redis/src/Program.cs
 When you run dotnet watch run, does it show you're in Development mode or Production? If Production, I noticed the times were much, much longer than Development (and typescript changes never updated, even though C# ones did)
... what? do you have any reasoning for this? I use EF Core 2 in multiple production environments.
Personally I'd stick with MVC however NancyFx is a good lightweight alternative. 
It's missing a lot of features. Some can be worked around, like the missing many-to-many mapping, missing support for seeding data (although that was always quite a mess, even in EF-6), others are absolute showstoppers for production systems, like "supporting" group-by operations by silently loading the whole fucking table and doing the grouping client side. https://docs.microsoft.com/en-us/ef/efcore-and-ef6/features
Hi coconutoctopus I wrote a serias of articles regarding this question - look at it here: https://www.infopulse.com/blog/tutorial-creating-basic-asp-net-core-angular-4-application-in-windows-10/#part1 https://www.infopulse.com/blog/tutorial-creating-asp-net-core-angular-4-app-in-docker-container-connected-to-sql-azure-database/
I feel the need to comment on this, because it's not even slightly accurate and I feel it will confuse new people more than it should. /u/winteriver ASP.NET previously had MVC and WebAPI. Then with ASP.NET Core (which can run on full .NET and .NET Core) it became merged, and now controllers can can either be traditional MVC type controllers or they can return XML/JSON as part of an API. The parent comment to this is suggesting MVC should be used for API's which is not accurate at all, and implies that making API's with ASP.NET is no longer possible now. /rant
Package versions are not assembly versions. This is a necessary distinction, because packages contain things that aren't assemblies, and therefore you have a reason to distribute a new version of a package without modifying any of its included assemblies. Furthermore, a package can contain assemblies for multiple target frameworks, so you might want to distribute a new version of the `netstandard2.0`-targeted assembly without modifying the assembly for the `net35-client` target. - 4.1.1.0 and 4.1.2.0 are assembly versions. - 4.3.0 is a package version. Assembly versions are not the same as assembly file versions. - 4.1.1.0 and 4.1.2.0 are assembly versions. - 4.6.24705.01 is an assembly file version. I don't know where 4.1.2.0 is coming from. Edit: OK, so 4.1.2.0 comes from `...\MSBuild\Microsoft\Microsoft.NET.Build.Extensions\net461\ref\System.IO.dll`. I'm guessing that that's a dirty trick that they used to cover for the fact that prior to .NET 4.7.1, I don't think that any of the "full" .NET Framework versions actually implement .NET Standard 1.3 or higher, fully, out-of-the-box (minimally, `net461` does not implement `netstandard1.3`, `netstandard2.0`, or anything in-between). Once you're able to target `net471` + `netstandard2.0` and above, assuming that Microsoft isn't going to start lying again about .NET Standard support by platform, I expect that these warnings should start going away.
"ASP.NET Web API" as a framework is gone. Building a REST or "Web" API with ASP.NET Core MVC is very much available. 
Automatic Binding Redirection?
Although it is not free you might check out service stack. I can't say enough good about it and it works everywhere Net, core self hosted etc. 
Server-side groupby is the single thing stopping me from ef core. Lets pull millions of records into memory on the web server so we can sum a column in a handful of them. I get that you can work around it, but it's still a trap waiting to be stepped on. 
Interesting. I'm not familiar with Redis; but I just read up on it. What do you use it for? 
Yeah.
I ran into DLL hell with a .NET Framework exe project referencing .NET Standard 1.5 libraries, all of them referencing DryIoc. The result was that some of the components of DryIoc were not copied to output, and once I referenced them from the main project all hell broke loose because my .NET Standard 1.5 libraries created the DryIoc Container which then failed to load the Framework version of the DryIoc packages (Attributes and Mef) Workaround/solution: Change the platform of the nuget packages in the Framework csproj to netstandard1.x OR use .NET Standard 2.0 (which wasn't out yet at the time)
As a session store and caching. Pretty much anything that you want to spare your DB for. It can also be used as a simple message queue.
The first time that I developed a larger .Net Core project recently, I bumped into all these issues. It felt like I was chasing my tail since I'd make a seemingly unrelated change that would cause something else to break in terms of dll versions. I constantly bumped into situations with dlls that felt like catch-22 scenarios. I burned a lot of hours trying to make it happy and still don't even understand why it works the way it did. I love .Net, but it was a very unpleasant experience.
Please tell me more.
&gt; Package versions are not assembly versions. &gt; Assembly versions are not the same as assembly file versions. They are when I publish a package on NuGet because I don't hate you.
Xml storage and lots of hard-coded strings :S
Are you comfortable with the IDE? Are you using VS for Mac or VSC?
There is absolutely nothing stopping you from seeding... Even the official example does it https://github.com/aspnet/MusicStore so I don't understand that point at all On to of that, you can always do many to many relationships, and it was defined how to in 1.1... http://www.learnentityframeworkcore.com/configuration/many-to-many-relationship-configuration And group by will be (partially) supported in 2.1 https://github.com/aspnet/EntityFrameworkCore/issues/2341 But I really don't see why it's such needed functionality. There are so many work arounds... 
Not quite as bad as this, but anyone know how to stop Visual Studio from copying dll references to the output path of this one project? I have a .netstandard2.0 core lib, (and previously) .net core 2.0 main executable and plugin systems. Referencing the core lib on the plugins winds up copying the .netstandard2.0's references to it's output directory. Also, a minor nitpick with VS output paths for anything not .net framework: the appending of "netstandard2.0/" automatically to the build path if the build path doesn't end with "netstandard2.0/" is annoying... if you forget or delete that slash too, you end up with "netstandard2.0/netstandard2.0/" as the suffix. :/
Bah, it's not black and white. This leads to assembly redirects galore and the inability to fix stuff by just dropping a fixed module in.
No, but it does make things 10% less painful.
After I graduated I recieved two very different offers, one offer where I was supposed to be the only developer and the other offer was with a team of 4. I went with the team, and I have learned so much from working in a team environment that I wouldve never learned from working alone. Regarding VS Community, at my workplace theyve been using community since always, and just recently switched to professional. The only difference I notice is that you can see who made changes &amp; when above every method.
Yes, it is good.
I'd encourage you to check out Rider too. Its fantastic. https://www.jetbrains.com/rider/
Most dashboards are using bootstrap Or materialize CSS libraries for the overall look. The rest is simply JS libraries for charts etc like chartjs or one of the 100 other big ones .
No it isn't the same thing. 
&gt; &gt; Regarding VS Community, at my workplace theyve been using community since always, and just recently switched to professional. The only difference I notice is that you can see who made changes &amp; when above every method. I think that comes with using Team Explorer or something. We use SVN in our org with no VS integration and I don't get anything like that in VS Pro.
Yeah. OOP design is based around the idea that a class (such as a form) be as self-contained as possible. No code outside of the form class (and contained control classes, of coure) should be directly interacting with the form or its controls. What I suggest you do is come up with a list of things other classes will need to do with the form then figure out the best OOP way to do it. For example maybe you have a tray icon which can be used to close all the windows to quit the application. In this case Form class already exposes a Close method so you don't need to do anything there except maybe override OnClosing in your form to make sure it is ding anything it needs to do there instead of, say, in an "Exit" button click event. Let's say I have a class which needs to pull data from the form, like user input. I would create a property on my form class which pulls the data from the form controls. Then the other class just needs to access the property and doesn't need to know about the exact types of controls on my form or how to pull data from them. In other cases it may be appropriate for the form to expose events or subscribe to events from another class. In this way the classes do not need to be dependant on each other and can be split up for other projects. For example, if my form closes I may want to open a second form. I can override the form OnClosed function and open my second form. But if I do that, I may be unable to use the first form in another project since it calls directly to the second form. What I can do is have an instance of my second form subscribe to the first form's Closed event and show itself. This way the first form doesn't need to know about the second form. Sometimes both classes shouldn't know about each other at all in proper OOP, so you can create a bridge that links the two without them needing to know about each other. In this case, my Program class can subscribe to the first form's Close event and open an instance of the second form. Now neither form needs to depend on the other. An excellent example is the OpenFileDialog/SaveFileDialog/FolderBrowserDialog classes. Although these classes are not actual forms, they are a good example of how you could potentially make a form class. They have properties that control various aspects of how the dialog looks and behaves. They have events that fire when the user triggers certain high-level actions (like selecting a file). And they have methods that you can call to run the dialog and get the result. You never need to directly interact with the controls in the dialog to use it.
[BeyondAdmin](https://wrapbootstrap.com/theme/beyondadmin-responisve-admin-app-WB06R48S4) uses boostrap with some extra JS built in. It might be what you're looking for. It has ties into .net for quick MVC helpers, I believe.
The benefits are that you can use a jwt as a stateless form of authentication (not having to hit the db every request to authenticate a user). As well as maintaining the benfits of using a cookie as opposed to localstorage. I am a bit confused about your list though. 1. you can set an http-only cookie to also require https. One common security complaint about jwt in local storage is that if you have an xss flaw anywhere then that means the attacker can grab the token and your website is totally compromised. If you store it an http(s) only cookie then you only need to worry about csrf attacks (you should still worry about xss though.) 2. Jwt is a secure auth token. That is the whole point of jwt. 3. Post requests still send cookies. Which includes the jwt cookie 4. yeah. The cookie creation isn't more complex, it is the same complexity. To create a cookie you just do createAuthCookie(jwt); as opposed to createAuthCookie(oldAuthMethod); here is a good link to [stormpath](https://stormpath.com/blog/where-to-store-your-jwts-cookies-vs-html5-web-storage) if you want to read more. 
Telerik is an example of a company who makes UI components/toolkits specifically for ASP.NET MVC. http://demos.telerik.com/aspnet-mvc/ 
 [AttributeUsage(AttributeTargets.Property)] public class CheckYearAttribute : ValidationAttribute { public override bool IsValid(object value) { if (value == null) return false; var enteredValue = (DateTime)value; var age = (DateTime.Now - enteredValue).TotalDays / 365; return age &gt; 18 &amp;&amp; age &lt; 60; } }
You could try something like Wisej? Or cshtml5 Or https://iridiumion.github.io/SharpJS/
[`nameof`](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/nameof) is indeed very useful but not without risk: never attempt to resolve [`nameof(nameof)`](https://i.imgur.com/dLMhKgM.jpg) as it is akin to crossing proton beam streams.
see the sidebar in /r/csharp 
The expression-based formatting is neat; seems like it would go really well with structured log data! Many .NET libraries now implement message templates for structured logging (implementations list at the end of https://messagetemplates.org/) - have you considered giving it a shot?
[Why not?](https://www.reddit.com/r/csharp/comments/6pw5dy/puzzle_when_is_nameofnameof_a_valid_c_expression/)
Slight modification to make it more accurate [AttributeUsage(AttributeTargets.Property)] public class AgeInAllowedRangeAttribute : ValidationAttribute { public override bool IsValid(object value) { if (value == null) return false; var dateTime = (DateTime) value; var now = DateTime.Now; return dateTime.AddYears(18).Date &gt;= now.Date // 18 or over &amp;&amp; dateTime.AddYears(60).Date &lt; now.Date; // one day less than 60 } } 
What exactly are you missing from the official docs? They are a pretty great starting point.
why not post some code then there is something to start with
When it gives you the little alert to install the plug-in, click that.
if i remember correctly, you have to run "add-migration &lt;name&gt;" first before "update-database" if its using ef. could you try the add-migration command after building the project 
If you are using the validation in the video, note that it is moderately restrictive and somewhat limited. A much more flexible validation tool is [FluentValidation](https://www.nuget.org/packages/FluentValidation/). In your model: [Validator(typeof(YourMethodValidator))] public class YourMethodViewModel { // All other model items [DisplayName(@"Your Birthdate")] [DataType(DataType.Date)] public DateTime BirthDate { get; set; } } In your Fluent Validation class: public class YourMethodValidator : AbstractValidator&lt;YourMethodViewModel&gt; { public YourMethodValidator() { RuleFor(x =&gt; x.BirthDate) .Must(BeOldEnough).WithMessage("We are sorry, but you are too young.") .Must(BeYoungEnough).WithMessage("We are sorry, but you are too old."); } private bool BeOldEnough(DateTime birthDate) =&gt; birthDate.AddYears(18).Date &gt;= DateTime.Now; private bool BeYoungEnough(DateTime birthDate) =&gt; birthDate.AddYears(60).Date &lt; DateTime.Now; } Note: I have a habit of using the correct data type through the entire model lifecycle, as it ensures that if I need a DateTime to be submitted, an actual DateTime will actually be submitted. You cannot submit a random string using a DateTime container, as it will just get stripped out as being unsubmittable. Plus, it will tell the `@Html.EditorFor()` in your Razor pages to actually create a DateTime form field instead of just a plain old text field; preventing anything *but* a DateTime from being entered in the first place. You can extend many form fields in this manner (phone, eMail, password, etc.), by adding the correct `DataType` to the model element (notice how I reduced the type from a full `DateTime` down to just a `Date` data type). Plus, on Mobile this means that the correct interface will then come up (data pad instead of full keyboard for Phone).
I had never heard of structured log data before to be honest. Though it looks super useful. But yes, it should be fairly easy to implement. I'm going to get right to it. Thanks for the feedback!
I know the designer is really flakey but I still get excited when I see design time data changing on the designer. 
How do you think I should do it though? As it stands, the format string basically contains function calls. Where does the name/key of the value (which currently is interpreted from the format string at runtime; giving it a kind of "context") fit in to the format string (aka, template)? Or perhaps worded a bit better: How should I go about naming expressions within the format (template) string?
I know what you mean.. I usually shut the designer off due to its sluggishness, but I would love to be able to use it again.
I really hope that with this update the Xamarin Forms designer finally works.
Never understood why T4 didnt get more love. For anyone that is using T4 and resharper: https://github.com/MrJul/ForTea
It is not sufficient to just do the binding and SSL settings because that means people can access both HTTP and HTTPS. You can use method outlined in the docs (from the app site) or use the URL Rewriter module of IIS.
If you want simple ASP.NET Core token based authorisation try [this](https://github.com/matthewblott/simple_aspnet_auth).
* Read how HTTP protocol works * Learn how DB index works * Don't give up
UWP only, WPF getting the finger from Microsoft.
I've already done that, I've even searched for a core specific extension 
They did mention that WPF didn't have most of the issues that the UWP designer has. WPF will only get bug fix updates going forward.
With the IIS HTTP to HTTPS binding and redirect, your web application (web site) ONLY has an HTTPS binding. You create an empty site bound to HTTP that redirects to the HTTPS site with a relative redirect. Your site will never serve you web application from HTTP in that case.
The feature you are describing is code lense and many people turn it off. A proper usage of git specific tools is way more efficient.
Trusted auth? Add the web/DotNet user to the SQL server so it can log in using built in windows/Kerberos auth.
Here's my take on that... Web.config files are not served to clients by IIS, so trying to access it via URLs will fail. So, if anyone has sufficient access to actually read your web.config files, you're in much deeper trouble because your server itself has been compromised. Of course, you can do things such as [encrypt your connection string](https://www.codeproject.com/Tips/795135/Encrypt-ConnectionString-in-Web-Config), but in my experience, I haven't seen it done much.
You can bind to both (80 and 443). You put a rewrite rule in there that takes non-https traffic and redirects it to https. 
Thank you all for the replies. I've talked to the HR Company and said that if i am to accept it has to bem in these terms. The company will talk with the IT team and said that they will call me in the next day. Let's wait and see.
1. Machine.Config 1. Trusted Auth 1. Run your application pool as a SQL user 1. Encrypt them 1. If they are reading your web.config files you're already fucked. 
As others have pointed out, if they have access to your web.config, you've got bigger problems. I think having that in your web.config is fine, assuming your server and site set up is done correctly (app pool users, proper folder perms, etc.)
This does require that the servers have knowledge and trust of each server's domains.
The problem I've found with web config isn't so much what happens if someone compromises the server, because as you said, you've got bigger problems at that point. Rather, the problem is around leaking credentials via source control, especially if open source and hosted on sites like github, but also within an org where devs must not have access to production data. Of course, one should have a proper process that only sets the credentials at time of deployment, but there's a known problem with credential leakage via config files (not just .Net) on Github, and sometimes just knowing the credentials for one service leads to exploits in other services because the same credentials are reused (again, bad practice, but most devs are bad)
Yes it does. And it only provides a mild increase in protection - an attacker who can, for example, upload an ASPX page, can still access the database using the same trusted connection. They can no longer steal the creds and use them from other computers that might be able to see the database though. Also, not learning the creds has benefit if those same creds are used to access other systems (but shame on you for reusing creds in that case). I would not consider this the biggest problem, unless you have solved a lot of other problems. For example, does the app have least privilege in the database? Is the database limited in its ability to be a launching point for further attacks? No linked servers? No outbound connections allowed to other data center servers? The app user is not an admin or dbo in the database? The app has no privileges except login and execute on a limited set of stored procedures that perform the app's job? If you really harden the database and implement that least privilege interface from the app into the server, the creds pretty much don't matter. You may be introducing more risk by connecting their domains at that point.
I won't extrapolate as this problem gets confusing enough once you go down the rabbit hole. https://identityserver.io/ or something similar. Whatever you do, I highly recommend NOT rolling your own.
Number 5 is pretty much the crux. 
If your IIS and db server are in the same domain, use integrated authentication. If not, encrypt connection strings (to hide user/pwd). If you want to protect yourself from an attacker who gained haccess to web.config, encrypt connection strings in either case.
Store your connection string as an environment variable.
It should be fine. Ive been using server and pass in my config since the 90s. I do kind of cheat, I encrypt the password at rest in the text.config and decrypt it on call. 
Its safer to never add the binding to port 80 for your site in the first place though. What would the advantage be of using a rewrite rule instead of a redirect?
True but I have a trusted connection and it lives in my machine.config becuse I like the false sense of extra security it gives me... 
A rewrite doesn't send the traffic back out so it's transparent to the end user. The negative of that is that it doesn't change the URL in the user's browser. The only time I've used a rewrite is to map the root folder to a subfolder for vanity reasons. The requests get rewritten to the subfolder but the end user sees it as root. A redirect will send the initial request back out and if you point it back at the server then it does that but with the new URL. As far as it being less secure, I could totally see that being a concern or perhaps not compliant with some of the stricter regulations but I'm not sure that you're exposed so long as the rule applies to all traffic and the URL rewrite module is functioning properly. If you have data that is that important then you should be using a proper proxy/load balancer to handle all of that for you. In our case we have Netscalers (behind firewalls) that take requests on both 80 and 443, rewrite everything to 443, terminate SSL on the netscaler and the re-initiate communication with the IIS servers by SSL-only using private CAs. It's end-to-end encryption and all communications to IIS is on port 443. Whew! 
Just use Windows Auth for all of them. You won't need to change the way the third-party app works, you have control over how the others work. Assuming you're using IIS you can configure it there. There is excellent tooling and plenty of documentation on it. Then, when you need to disable a user, you do it in AD and it's done everywhere.
In Windows land, the site should run as a dedicated domain user, and you grant that user access to the database. Then you have tight access control. If you're not in windows land, or your servers are not part of the domain, you need to store the credentials somewhere. Web.config is a reasonable place as long as the access is restricted. If you need audit logs, you should store them in a secure audit logging "vault" - then you have complete logs over when it was accessed, and potentially ability to store some context data as well. This service must be tightly locked down, otherwise it's just more work for no added security. But no matter how you do it, there must be a secret somewhere. People don't like it when it's in cleartext in web.config, but in any reasonable threatmodel, it's not different to storing it the dll. If I remember correctly, you can use the windows subsystem to encrypt a connection string with the running users certificate. That should be secure from prying eyes, but it requires some non trivial setup.
The issue is that we have thousands of users that aren't part of our domain. Only our internal users use AD.
Well, it's a new project, I want to evaluate the different options, VS for Mac seems a bit heavy so far (new MBP), VS Code is fine so far, but I think it should be tested while developing a full project and not just on a playground.
I am now using a small watchdog script that checks if the .dll is changed (after I build from the IDE) and calls *dotnet exec bin/Release/project.dll*, it feels that the build from IDE + dotnet exec is much faster than *dotnet watch run* I will post a small benchmark and the small script with results later. 
So my experience is a bit different. VS for mac has better auto complete. VSC is not nearly as good as Visual Studio. Keep that in mind when doing cross platform dev. 
Integrated Auth is the way, The trick these days with later windows is to add the windows login of the app pool, in the SPECIFIC format e.g. IIS APPPOOL\DefaultAppPool You'll notice it will change right away to the app pool name. No idea why M$ made these new service users so damned hard to find.
Hey people I just made my first medium post ever and I decided to make it about NET Core. I investigated how to create an MVC app and in this post, I wrote little steps for this from scratch :) Cheers
*On Linux, from scratch. Nothing to do with [Linux from scratch](http://linuxfromscratch.org/)
Thanks, we also have JetBrains licenses, so we will probably give a shot to Rider.
Guess I was confused by "internal applications"
This is a good guide for beginners under IdentityServer 3 (different, not better :). https://app.pluralsight.com/library/courses/oauth-secure-asp-dot-net-api/table-of-contents
I've built some relatively complex workflows in Sitecore, similar to what you listed. You can add custom actions that hook into custom classes where you can do whatever you need to do. I know you can read the user who submitted the content, their role(s), send notifications and approve/deny content programmatically. Sitecore is a big, bloated mess of a product though, but it does excel in customizations.
You might also want to check out something like SCSM. Not sure, that might be too specific to something else.
yes, sorry, I meant a project from 0, not that I use linux from scratch :)
Pretty sure umbraco can do this. I'd be surprised if sitecore and kentico (the enterprise focused platforms) didn't offer it.
Why do you need IIS?
Orchard. 
Don't do that. It is not possible to restrict access to environment variables.
Integrated security. A domain service account is preferable, but setting up a domain controller might be overkill for some applications. If you have to put the credentials in the Web.config, either use encrypted credentials or limit access to the Web.config to the service account used as the identity of the application pool.
I've mixed feelings about Orchard. On one hand it seems reasonably flexible and capable, on the other hand it seems to me to be poorly documented and from a developers perspective I'm not keen on how you write extensions to it. Everything is very dynamic and there's multiple ways of achieving the same thing, with some ways being deprecated. I'm really not a fan of Orchard to be honest, but the one website I did work on with Orchard actually turned out pretty well. Unfortunately I don't have any other significant CMS experience to be able to have a well informed opinion. I would personally be taking a look at Umbraco as my first stop.
You shouldn't need to watch cshtml files. Mvc watches them for changes by default and recompiles them without having to restart the application. Presumably, css or js files wouldn't affect your compilation, so those shouldn't be necessary either.
When I looked it up I found this explanation of the Microsoft documentation: "The UseIISIntegration method looks for environment variables that ANCM (asp.net core module) sets, and it no-ops if they are not found. This behavior facilitates scenarios like developing and testing on macOS or Linux and deploying to a server that runs While running on macOS or Linux, Kestrel acts as the web server, but when the application is deployed to the IIS environment, it automatically uses ANCM and IIS. " But mainly because it was a proof of concept :P
And you really like to confuse devs who delete settings from the web.config *but this shit still connects wtfff*. :)
I could be misunderstanding your question - but I think it's important to differentiate between the output format (where I think your project is using expressions), and the capturing format, e.g.: ``` log.WriteLine("Hello {Name}!", "World"); ``` Message templates fit in where you'd normally see .NET format strings, like in the `WriteLine` call, above. Rendering out the resulting event, e.g. to display at the terminal or in a file, isn't really addressed by message templates, so you'd probably continue along similar lines as the example: ``` log.Formatter = new LogicalFormatter("Name is ${Name}. Oh, and by the way: ${message}"); ``` (Where `message` might be the fully-rendered message, and `Name` would just be the captured name. It might be worth checking out https://serilog.net to see more of the implications of a structured logging pipeline, like enrichers, filters, and structured data capturing. Lots of interesting angles left to explore, too.
I can't agree more. Rolling your own is an exercise in misery. I'd suggest going one step further and investigate Auth0. Get authentication completely out of your hands.
Have a look at Sitefinity, there is section in it dedicated to defining complex workflows like this.
 A recent response in Umbraco forums is that they only support a simple submit-approve workflows natively and there's no plan of extending the functionality. The closest I found was this [Plumber plugin](https://github.com/nathanwoulfe/Plumber), but it is still in early beta.
Our default home route is the react app. So when you hit / you get react and react router takes over and nothing ever hits ASP again except explicit Api calls. Our fallback route for anything that is not /api/ is also / so links within react router work on new sessions but we still get proper error handling and response codes from the Api calls. 
Good to know, thanks.
Why not roll your own? Database storage with the appropriate flags; either in-line with the data, in a separate one-to-many table, or both. Primary content table would probably work off a self-referential relationship with itself, so that content versioning could be worked out. Sounds pretty simple to me.
Actually it was our first choice and the former dev team started using Orchard but for some reason, they dropped it and started searching for another CMS. I agree it isn't very well documented although it is the only one that has various specific sections dedicated to the workflows module, but all the examples and videos are very simple demos so I don't know if it can be customized for more robust scenarios.
Right but the goal is that the site admins, which aren't devs, could have the ability to create or modify these type of workflows as they wanted without having to consult the dev team, just as they do with other functionalities offered in every CMS. So it is not create a specific workflow that's set to stone, but rather find a CMS that exposes a workflow framework that the user can interact with.
&gt; the goal is that the site admins, which aren't devs, could have the ability to create or modify these type of workflows as they wanted without having to consult the dev team So manglement wants to mangle? Ouch. Looks like you don’t have much of a choice, so my condolences.
https://en.wikipedia.org/wiki/Shunting-yard_algorithm
**Shunting-yard algorithm** In computer science, the shunting-yard algorithm is a method for parsing mathematical expressions specified in infix notation. It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). The algorithm was invented by Edsger Dijkstra and named the "shunting yard" algorithm because its operation resembles that of a railroad shunting yard. Dijkstra first described the Shunting Yard Algorithm in the Mathematisch Centrum report MR 34/61. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.27
There's a small example using the Sprache parser combinator library here: https://github.com/sprache/Sprache/blob/develop/samples/LinqyCalculator/Program.cs#L18 The combined parser/evaluator: https://github.com/sprache/Sprache/blob/develop/samples/LinqyCalculator/ExpressionParser.cs The links on the Sprache README should cover enough of the background to DIY the whole thing, if you want to go that far :-) Good luck with it! 
Does Hyper-V require Windows 10 Professional or higher?
Try ANTLR, it has a C# version IIRC. For a lower level approach, try https://compilers.iecc.com/crenshaw/ (in Pascal, but the code is very simple so it shouldn't be too hard to translate). Have fun!
I went to the "Internet options" (-&gt; tab "connection" -&gt; button "LAN settings" -&gt; put checkbox to "Use a proxy server for your LAN blabla" -&gt; enter address and port) and put proxy there, it worked for me.
Indeed a bit sad since WPF is an excellent candidate for building store apps using the Centennial bridge.
Thanks everyone!
right, so let's say i do have that and then i have a router like this ReactDOM.render( &lt;Router history={history} store={store}&gt; &lt;Switch&gt; &lt;Route path="/" component={IndexPage}&gt; &lt;/Route&gt; &lt;Route path="/products" component={Products}&gt; &lt;/Route&gt; &lt;Route path="/shelves" component={Shelves}&gt; &lt;/Route&gt; &lt;/Switch&gt; &lt;/Router&gt;, document.getElementById('app.inventory') ) when i hard code the url on the address toolbar it would try to hit the controller of that link. 
Oh, OK! I think I understand now. So what you're saying is that I could support structured templates *in addition to* the already implemented expressions. Yeah, I don't know what I was thinking you meant before. :)
&gt; Oracle says they'll add support once .NET Core 2 becomes a thing, hopefully by the end of 2017. damn it, all until that line sounded promising :) That exactly what i need it for, because my app on Windows Server is too damn slow, IIS Worker gobbles CPU so I wanted to move it to ASP.NET Core on Linux. But `end of 2017` shattered it
You can handle all 404 requests by redirecting them to your index.html page. Then make sure if your React app doesn't have that route, it shows a nice 404 page
how do you redirect them?
From my understanding, the modifications in the documents I linked in the post are meant for the Kestrel server? Using an IIS hosting setup, it is basically just a reverse proxy to the Kestrel server, the Kestrel server port is never visible to anyone in the network. So far, am I getting it right? So if I setup IIS with SSL, and redirect all HTTP traffic to HTTPS, this should be sufficient? Even though the connection between IIS and Kestrel is only HTTP, but that only happens in the machine, and no user has access to it. 
Hi, can you read my comment above, am I understanding it correctly? Edit: [Direct link](https://www.reddit.com/r/dotnet/comments/6zl66f/net_core_2_mvc_app_how_to_host_on_iis_with_https/dmxuy53/)
You can definitely roll your own in umbraco
Certainly with Orchard you can bend it to your will, but I found it to be quite development heavy, as developers we did ~~most~~ all of the non-content changes to the system. I was often frustrated by the development experience as well, sometimes finding bugs in Orchard and often finding my code wouldn't work for some minor reason that's not detected at compile time (often Module.txt / Placement.info problems, but also typos in C# code that aren't detected because large parts of Orchard are based on C#'s `dynamic` and strings); it it a very dynamic environment, which leads to a great deal of flexibility... but it comes at the cost of ease of development IMO. I often found it to be incredibly frustrating as a developer. I wouldn't count Orchard out (as I mentioned before I was involved in the development of a website that ended up being a good product), but I would definitely be looking at the competition. Something like Umbraco appears to be much better supported in terms of documentation and users/forums and that does count for a lot. 
At my current shop we only go to docker for deployment. Haven't hit any issues yet, and we've on it full steam for a few months now. We have a Dockerfile which copies the output of the build into the container, and runs the dll with "dotnet". This gets done on our CI server, and pushed out to aws.
Dotnet core has a spa services middleware that you can add via nuget, but you really just have to redirect on 404. Look it up, it's easy to find ok stack overflow. Sorry I'm on mobile now :)
[removed]
There aren't really any shortcuts if you are doing IIS as a reverse proxy to Kestrel. Make sure you understand everything in [this link](https://docs.microsoft.com/en-us/aspnet/core/publishing/iis)
What you can take from that is if you're not using Windows then you don't need it. Probably best to remove it from a tutorial that's focused on Linux as using code that references IIS is confusing.
I put together [this](https://github.com/matthewblott/simple_aspnet_auth) really simple boilerplate solution for authorisation.
in case you haven't kept up on the "community stand-up": If you are a .net dev, or want to know what Microsoft has been doing for the last few years ".net/asp/everything core", the community stand-up has been a great resource. Please check it out. https://live.asp.net/ apparently moving to https://www.youtube.com/channel/UCiaZbznpWV1o-KLxj8zqR6A/videos
And why did we need this?
No idea. Maybe they are planning to use it in some SignalR Core samples when they release that (presumably later this year)
* SyndicationFeed does not work Cross Platform. * RSS/ATOM is the standard for syndication so they are used a lot.
What year is it? Joke aside I'm sadden that RSS feeds have been dying in the past years in favour of Twitter. They had a much better signal-to-noise ratio than Twitter. 
&gt;I'm sad [Here's a picture/gif of a cat,](http://random.cat/i/076_-_ESQXAkp.gif) hopefully it'll cheer you up :). ___ I am a bot. use !unsubscribetosadcat for me to ignore you.
Actually RSS is more popular that ever thanks to the explosions of Podcasts 
I'm also sad, they're so good for keeping you up to date with specific things and they're nice and simple
Thanks bot but I'll be fine. Thanks for the cat still.
[removed]
You can check out EpiServer CMS, maybe even contact them about you specific needs.
Sub'd, thanks for the heads up.
That's right, I just updated the post, thank you very much for the feedback my friend!
Yeah I followed that link to set up IIS as a reverse proxy to Kestrel. Just that it doesn't have any information regarding HTTPS on the IIS side. Any tips or tutorial?
This is similar to the sorrow of moving from Usenet to reddit. Nostalgia-ugh...
This is a .NET Standard 1.3 project. Reminder - You can check which platforms a .NET Standard project will run on here: https://docs.microsoft.com/en-us/dotnet/standard/net-standard 
I recommend that you do development locally but also a bit of app testing in a docker container. We ran into issues with getting timezone information. As the timezone names are different on Windows than it is on Linux.
Thanks for the insight. Mind if I ask what CI server you are using?
I use https://github.com/aspnet/JavaScriptServices which has a MapSpaFallbackRoute helper method.
Dang, that's unfortunate surprise. Thanks for the advice!
I don't think they would even recommend it for .NET Standard 1.6. It doesn't look like it's ready yet. I'm also pretty eager for a stable version.
Sitecore
SignalR will be released with ASP.NET Core 2.1 ( https://github.com/aspnet/Home/wiki/Roadmap ). Also they will release alpha version for 2.0 soon: https://github.com/aspnet/SignalR/issues/429
But podcasts have *also* been dying in the past few years as well... Most people seem to have migrated to uploading YT videos of them talking whilst the screen displays images related to their topic.
.net framework, core, etc should be classes, not interfaces. They are implementations of the various standards. But that's just my opinion.
Somewhat unrelated to the fact that SignalR isn't released yet, that error message is telling you that you haven't installed the .NET Core 2.0 SDK. You won't be able to target netstandard20 in your other projects until you do so.
Will the code be compatible if someone writes for the existing .Net SignalR or is this a case of same-name but very different api?
I feel like I'm going back in time because I'm using AOL more than ever due to their RSS reader.
Nobody I subscribe to is that stupid.
&gt; u won't be able to target netstandard20 in your other projects until you do so. .NET Command Line Tools (2.0.0-preview2-006497) It is installed :)
ya it's just you. you are missing the whole point of this example.
Are there multiple implementations of .Net Framework 4.6.2? 
Across the board: I'm really eager to start with dotnet core, just waiting for the right maturity level in the tools and ecosystem.
The dictionary is pointless, static members of generic types are unique to their generic type arguments, i.e. `Singleton&lt;int&gt;.instances` is not the same as `Singleton&lt;string&gt;.instances`.
I think it'll be at least a partial rewrite, based on presentations etc that I've seen, although it might be more on the client side than the hub side. It's going to be .NET Standard 2.0 compatible though, which means that you should presumably be able to use the new SignalR in both .NET Core and .NET Framework 4.6.1. I'll be interested to see how much effort will be involved in migrating, because we have used SignalR quite extensively at work for moving real-time instrument data around.
When you create an instance of a generic class the compiler actually creates a new class from scratch so like /u/tweq said it's pointless to have a dictionary as each separate class will hold their own Instance property.
I've written one with a Pratt Parser before. It supported variables and everything. It was great fun to write!
My feeling was that it would require a total re-write. I did some experiments with it; I don't envy anyone who has to rewrite stuff that works well. 
.Net Standard 3.0 :S
I've been cracking at this for a couple hours now and I figured it out. The template enables debugging by default. It's located in the webpack.config.js file. If I comment out the plugin: //new webpack.SourceMapDevToolPlugin({ // filename: '[file].map', // Remove this line if you prefer inline source maps // moduleFilenameTemplate: path.relative(clientBundleOutputDir, '[resourcePath]') // Point sourcemap entries to the original file locations on disk //}) The compile times are around 300 milliseconds. https://webpack.js.org/configuration/devtool/
Having made the switch to mostly DI, I don't use many singletons anymore. This page discusses the various options for singletons and can be used to create a Singleton&lt;T&gt; wrapper if you need to create lot of them. http://csharpindepth.com/Articles/General/Singleton.aspx
As pointed out elsewhere here, the dictionary will never be used effectively as each ```{ type &lt;generic-type, ...&gt; }``` permutation is itself a distinct type. With that said, why use the dictionary anyway? Static initializers are guaranteed to be thread-safe so why not just do something like this? static class Singleton&lt;T&gt; where T : new() { public static T Instance { get; } = new T(); } And with that said too, why use a singleton in such a way? This has value as an academic exercise but it does have a code smell to it. :)
(cross-posted from /r/vscode) Also provides a bunch of other intellisense features in MSBuild project files.
Nice. This brings back some nice memories. I had made something similar a few years back when the company I worked for was transitioning from Installshield. It was based on MvvmLight, a DIY IoC container, a simple message passing system and an abstraction around all the facilities provided by Burn (this allowed us to unit test the installation UI).
Thanks. May I ask what were you unit testing? I really want to cover at least some code with unit tests, but in my experience it's very hard to make any meaningful tests with Burn since the integration is harder than anything else in the project due to the lack of documentation and the weird API. The way I see it is that making a sane abstraction around all the facilities provided by Burn would require quite a lot of effort and integration testing, or am I mistaken?
2.0 final has been released.
Rider is great!
Nice work. I've been working on a commercial version of something like this, also in WPF.
Heavy smell. Singletons should be used where it's appropriate, and should be distinct. Creating a class that can create singletons from everything just opens the code for abuse
Also, the `new T()` will be turned into `(T)Activator.CreateInstance(typeof(T))`.
Terrible title, nice video. Gets into the subtleties of the string interpolation to parameterized query feature of EF Core 2.0 which many people including myself think are basically just landmines in your code.
If you are doing this to learn, great. If you are doing this for real know that there is very little value to spending the effort and extra junk that will bite you when using singletons is often not worth it. Most of the time you would be better of with just a normal class and only using it once. This really only becomes an issue if you are creating some generic type loader kind of thing where the user can pump any kind of data and have you create all kinds of objects willy-nilly. If you are making a singleton because "ohh global access to data or feature", don't. Really, its enticing but you will hate yourself when you are more experienced and fully understand why that is a terrible idea. There are exceptions of course, though chances are you are not dealing with one of those. 
It's jenkins - with a few additional scripts to help generate the task definition. I'd be interested in any ci that isn't quite as ad hoc though!
Thanks. I currently use jenkins on another stack, so it's good to hear that with some tweaking it can work.
&gt; The current .NET SDK does not support targeting .NET Standard 2.0. Really? I would have thought that the [.NET Core 2.0 SDK](https://github.com/dotnet/core/blob/master/release-notes/download-archives/2.0.0-download.md) would support .NET Standard 2.0 as well. 
Well, jenkins is just shell scripts, so if you're enthusiastic enough, you can do just about anything :)
Just write some HTML with the check box values in a div or something. But it in a string and send an email.
Like from what I found here http://www.blinkingcaret.com/2017/03/01/https-asp-net-core/ Browser &lt;---- HTTPS ---&gt; IIS &lt;---- HTTP ----&gt; Kestrel Is this not a legitimate setup?
Will that also display the checkboxes in the email? Do you have any handy links I can look into? I'm kind of new to web dev.
You can display HTML checkboxes. But it's not going to make it clickable. You'll need to send them a link to your form. If you Google C# email and HTML, you'll find plenty of examples.
OP had 2.0 preview installed instead of 2.0 final
Yup, and it looks like they don't have any updates on [the page](http://www.oracle.com/technetwork/topics/dotnet/whatsnew/index.html) that links to the PDF I linked to above. :(
Great thanks for your help
quick question on this. do i have to serve a static html or can i still use cshtml? https://docs.microsoft.com/en-us/aspnet/core/client-side/spa-services I tried to follow this (i'm using .net core 1, so i adjusted accordingly). If I tried to add anything past the "/api" url eg. "/api/items" , it would not direct me back to wherever the default url that I said to redirect to... 
You can serve any url. It's a browser based thing. If you need a hand feel free to pm me and I will send you our routing configuration. 
I will try again later today when I get home from work. Install latest 2.0 :) Hope to get started working with some websockets again.
That's good to know. I haven't had time to play about with it yet. My takeaway from watching presentations etc. was that the actual writing of hubs wasn't really changing, but the mechanism of how it was hosted was, which is why I thought that the client-side stuff would be affected more than the server-side stuff. That said, it's currently a bit of a mess on the server-side when e.g. you're hosting multiple SignalR instances on sub-routes of your web app, and you need to send messages to clients via static methods (which is a situation we ended up in), since Microsoft.AspNet.SignalR.GlobalHost isn't much help when you have multiple instances of SignalR registered. So hopefully this is something that is a bit easier to do in ASP.NET Core SignalR.
"Only your reverse proxy server requires an SSL certificate, and that server can communicate with your applicati..." https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?tabs=aspnetcore2x I found this
Burn and Wix is a layer over Windows Installer, which in turn is incredibly complicated and it's lack of documentation is impressive. I recommend getting a good book on Windows Installer (I think I used "The Definitive Guide to Windows Installer") and have a look at Rob's blog (http://robmensching.com/blog/), that way you get to understand how sequences and custom actions work, what goes on under the hood, etc. We created a message passing system in our installer application. We hooked up a number of events on Burn engine and the Wix installer engine to detect state changes within the Burn / Wix systems. We tested the messages that were triggered whenever burn and windows installer changed any of their states. Example: When burn engine entered a particular state (can't remember what it was), we had a test to ensure that the first dialog is rendered to the user. We had tests that verified that we were showing (burn) bundle installation details - download states, install progress, etc. We had a test to verify that the progress dialog was displayed to the user when the bundle deployment started. We had tests to check that the various bundle stage types were interacting properly with the bootstrapping application. We had tests that checked that file copy / move operations where reported properly to the user. These tests were easy to do because we created a message passing system (MPS) - we'd hook into a bunch of events in Burn, translate them and push relevant messages onto the MPS. Our unit tests would simply push the same messages onto the message passing system. Verification would be done by hooking up to the viewmodel manager and other system services to check the state of the UI / properties set in the view model / etc. We also had other unit tests that checked UI behaviour. We had a serial number entry view so we tested: 1. the verification of the serial number 2. if the serial number is wrong, we checked that the error state was set to true and a description of the error was specified 3. if the serial number is correct, we checked that the Next button would be enabled (we had the next button bound to a property on the view model that enabled / disabled it) and that the IsValidSerialNumber property would be true. Hope this gives you an idea. Sorry I don't have more details, unfortunately I haven't used Wix since switching to a web development shop. 
I am kinda getting disappointed with .NET Core, there is still no VS, only crappy VS Code. This looks more and more like WinPhone. And the same shit [for Mono](http://www.mono-project.com/docs/database-access/providers/oracle/) btw: &gt;Oracle produces a client for Windows, but they do not support this on Mono. I am considering to do new projects in Nodejs, or may be I should just start using fucking Python? This is impressive: https://trends.google.com/trends/explore?date=all&amp;q=.NET%20Core%20tutorial,node%20js%20tutorial,python%20tutorial,ruby%20on%20rails%20tutorial
The title really makes it sound like EF Core 2 introduces some crazy vulnerability. Thank you so much for confirming that it's not that. You saved me the urgency of watching this ASAP.
[removed]
I've been very impressed with whats out there now (2.0) I was following from the early "vNext" days, and its been a rough ride but smoothed out quite a bit.
Well, there is fear that this particular feature could cause a vulnerability. Definitely worth watching and then I'd highly recommend avoiding this functionality. Definitely could lead to problems by someone that doesn't know how it works refactoring your code.
I'm gonna be working in dotnetcore like... 110% of the time as soon as my references allow it. I love the direction it's been going, bumps and all. As an F#er though things are a bit delayed. Apparently VS Code is generally robust for F# on dotnet core, even as VS support works itself out :)
I worked for a company that tried to build it's own CMS. It was a disaster that ended up with us having to edit databases directly to add new content. There are a ton of CMSs out there, no point in reinventing the wheel.
Yea, I was thinking about adding this functionality to my ORM. Now I'm having second thoughts and I haven't even seen the whole video yet.
I think the only safe way to do it would be to have the ability to ONLY take a FormattableString and not a string data type like they are doing. Nick Craver has a nice demo of all the headaches https://github.com/NickCraver/EFCoreInjectionSample
Currently I use a method called `DataSource.Sql(string, object)`. I don't think I want to get rid of this because sometimes we pass constants to it. I guess I could add a `.SafeSql(FormattableString)` method. But how to ensure people use the correct one? 
I didn't used it but a coworker recommended it to me : https://reactjs.net/ 
I always welcome contributors on my project: https://github.com/tidusjar/Ombi
That's actually not useful. ReactJs.net needs a build made for its server side rendering also, so I'll still be struggling with a webpack configuration. Plus, it's not as nice as javascript services for most everything else.
I have done something similar: https://hackernoon.com/server-side-rendering-of-deep-links-with-react-and-net-core-882830ca663 I mostly talked about using my own library, but there should be enough info on getting react SSR working with JavascriptServices. Plus, there's working example.
The full FX starts to look like web forms in comparison to core
Too little, too late as far as the configuration is concerned. My old team migrated to JConfiguration and never looked back.
What do you mean no VS? I've been developing in .NET Core in Visual Studio for a few months.
For easy registration, this is one way to do it https://github.com/dodyg/practical-aspnetcore/blob/master/projects/dependency-injection-3/src/Program.cs Learn ASP.NET Core 2.0 and EF Core separately. It will cut down the amount of concept you need to learn.
&gt; JConfiguration Never heard of it. Did you mean ***I****Configuration* ?
Nope. It's JSON configuration using JSON.Net.
I've been doing https://openedx.microsoft.com/courses I keep having version problems with the labs though.
If you can get a pluralsight subscription, there is actually some good material there.
i had the same problem trying to follow asp.net core 1, the pluralsight courses wouldn't work if you followed the code unless you explicitly used the versions in the npm they referenced in the courses.
yep got one, but they dont have anything on .net core 2 from what i can see
is there a big difference between EF core and EF? figured it would be easy enough to pick up given i know EF
I almost panicked before realizing it's not .Net Core. Whew! Not that .Net Core Configuration has no issues.
looking for a code sample for listing users in a role while showing extended profile data like FirstName and LastName.
but not on Linux? Full VS is available only for Windows, right? My goal is to keep switching to Linux as much as possible, because IIS sucks, Win Server sucks and slow, developing in Windows sucks also because the community keep switching to Mac/Linux and tools like npm etc just don't work that well. 
I have never used EF but EF Core is a subset of EF so you should be good to go. The tutorial for MVC on the ASP.NET Core official site is good enough to get your started. If you want to get to the underlying component of ASP.NET Core, here's 106 examples to keep you busy https://github.com/dodyg/practical-aspnetcore/
* EF6 is end of line * Purchase https://www.llblgen.com/. The product has been around for 14 years now and always updated. It's amazing. I have been a happy customer since version 2.0 and now it is at 5.2. * Stick to RDBMS. Your system will generate tons of reports and nothing beats RDBMS on this.
Thanks for the tip. But it looks like it's more about code generation. I will be writing POCO classes myself in a code-first fashion. So I don;t really want to deal with generated code. But thank you for the info.
It is actually mostly DB first. You design your schema and prepare your migration code separately (which is a good practice). Then you just generate the table mapped C# entities and relations using LLBLGen. It has the best LINQ support around and you can customize a lot of things from the editor. And if you need to, you can drop to plain SQL directly on the same lib.