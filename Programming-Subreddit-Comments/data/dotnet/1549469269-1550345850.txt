[removed]
Would the 'Current Block' option in the search do the trick? It's listed with the other search scopes -- 'Entire Solution', 'Current Document'..etc.
That's either an extremely brave or uneducated person to start a greenfield project using web forms
if you are getting null exceptions, is that not an exceptional case? If I look up a name, and the name doesn't exist, that is an exceptional case. 
depends if null is an expected value or not. If null is not, and something went wrong, then it is an exception. If null is a proper value to return, then yes, you should check. I guess some places will now throw an exception rather than return null, which I think is a better solution in general. If you look up a name and it doesn't exist, you throw an exception and not return null. 
Try View Call Hierarchy instead then.
very nice. Here's a .net SAML resource I've used that might be a good addition: https://github.com/jitbit/AspNetSaml/ 
right, nullable values gave a feature to a value type that didn't exist, the ability to say "unknown" basically. It is expanding what we can do with values. I don't see this expanding the use of references, but more restricting them. For example, say you have it not-null because it doesn't fit the domain, and then you hit that edge case where null does make sense. That can have a massive ripple effect across the system, either through mapping or changing of the entire pipeline. I like flexibility because as software changes, not needing to change everything is a feature. I use null to indicate something hasn't been initialized yet, there are times when I thought my system would start up initialized, only to hit a data cap that required a lazy loading approach. Now I needed nulls to keep track of what was loaded or not. These aren't big things, but just things I can see happening that this "feature" will actually cause headaches for. Meanwhile, nullable I feel didn't cause any headaches, it was a new feature that integrated well into new code, and the bridge between old code and nullable was nice (such as ability to just assign the object the value)
&gt; If I look up a name, and the name doesn't exist, that is an exceptional case. Then you should throw an exception before assigning null to a non-null name. Your exception here would be NameNotFoundException, not NullReferenceException.
The way I view it, is if I received null, I know something has gone wrong. So when I get null back, I'll code something to log / present an error to the user. 
I‚Äôll try that out
No since I want sub methods to be searched as well
&gt; I don't see this expanding the use of references, but more restricting them. All types system exist entirely to enable adding **contraints**. This allows you to add a non-nullable constraint to ref types. &gt; For example, say you have it not-null because it doesn't fit the domain, and then you hit that edge case where null does make sense. That can have a massive ripple effect across the system, either through mapping or changing of the entire pipeline. I like flexibility because as software changes, not needing to change everything is a feature. If that's what you want you should just be using `dynamic` everywhere or use a dynamic language. Who needs type constraints anyways? &gt; I use null to indicate something hasn't been initialized yet, there are times when I thought my system would start up initialized, only to hit a data cap that required a lazy loading approach. Now I needed nulls to keep track of what was loaded or not. You should be using constructs designed for this. `Lazy&lt;T&gt;` comes to mind, but there are several other constructs that exist or can be built yourself very easily. Relying on nulls and null checks everywhere is very amateur. &gt; Meanwhile, nullable I feel didn't cause any headaches, it was a new feature that integrated well into new code, and the bridge between old code and nullable was nice (such as ability to just assign the object the value) Start with your project allowing nulls as before, then slowly move your code over to this new pattern. As for assigning on object the value you can do that in exactly the same way. In the end if you don't want to use this or if you've written a giant code base with amateur-league anti-patterns all over it then I guess this feature isn't for you. The rest of us will happily use it and let you stay in the past. 
Thanks for the tip. I've created pull request and we will see ;)
Good. Because had it been a greenfield project, I would have broken out the [Cat 5 ‚ÄôO Nine Tails](https://laughingsquid.com/cat-5-o-nine-tails-ethernet-cable-whip/) and given you a good flogging with it.
Nice one canes\_93, could you create a PR for that?
Interesting, I'll have to give it a look. I hear ads for Syncfusion on developer podcasts, which is all I know about them.
I don't know a .Net one.... but I was pretty fond of Decisions in the past. &amp;#x200B; [https://decisions.com/](https://decisions.com/)
i was just talking about something like this with a coworker yesterday. very nice! i think you'd get way more interest being .net core and maybe the frontend written in angular (most .net developers can do angular very easily). 
Windows Workflow Foundation, which newer versions of SharePoint are built on. 
Perfect
Try using first, see if you remain so enthusiastic. We ended up just storing workflow states in a plain old sql tables, accessing via a service shared by several apps each tailored to user roles.
Thanks, will look at it, but the issue is not with the button but with browser refresh. 
done. Thx!
You point out the multiple simultaneous steps at the first couple - but then stop calling that as a negative.... I know Teamcity doesn't have that concept which is a total bummer. 
TeamCity has the pitfall that you can program your build in TeamCity instead of a build script, which leads to hard-to-reproduce issues and a experience where TeamCity is required to do the work. At my work we have checked-in build scripts (written in C#/Cake) that can be run easily from command-line but are run in the same manner by TeamCity as well. If any build issue happen, it is often a no-brainer to attempt to reproduce it locallly.
Sounds like OP wants to essentially search the 'potential' call stack for a call to a particular method, starting at some method in the code base. I think Call Heirarchy might be of use but not sure. Also maybe Resharper has a tool.for this?
Good point üëç
Thanks. There is the TestFlask manager which helps creating, viewing and modifying scenarios, steps and recorded invocations which is already written in angular 4. [https://testflask.github.io/test-flask-manager/](https://testflask.github.io/test-flask-manager/)
I'm not sure what you mean by "built in support", but the agents work on windows and are controlled by build kites SaaS platform. 
By built-in support, I mean some sort of knowledge about how to build a .NET project. All of the mentioned products have some kind of template to help create a build, test and deploy flow, tasks specific for .NET, etc. But as long as Windows (for full .NET) is supported as a build agent and the server can execute scripts on the agent, building is supported of course.
Might take a look at stateless on git
I see. This reminds me of ElasticSearch C# API, which supports both approaches (declarative and excessively functional) to satisfy both camps. But hey, whatever works.
Could you post your button click event handler? I‚Äôm assume you‚Äôre calling the work required from there and not the page load event handler?
Yea I would chalk a lot of our issues up to that, as well as the fact that we use specific configured machines as build agents (instead of docker hosts/containers) and the worst offender: it costs $$$. The folks before me LOVED making 20 step build configs. It really sucks that they configured so many repos that way... but we are slowly fixing it.
Patently not true. Mono can probably run a legacy ASP.NET project without too many changes. 
Thank you! Merged.
Sub-Method?
Is azure devops free for corporate use or is it just for open source?
I am using it for my open source projects.
Is this in a razor view? You'll need to import tag helpers in your viewimports file. Then you'll get the intellisense and color. https://docs.microsoft.com/en-us/aspnet/core/mvc/views/tag-helpers/intro?view=aspnetcore-2.2#managing-tag-helper-scope
I've never played with [Magick.NET](https://Magick.NET), but it is a wrapper around the ImgMagick libraries, and this is fairly easy using the "convert" command ... &amp;#x200B; `convert -size 200x -pointsize 10 caption:"word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12 word13 word14 word15 word16 word17 word18 word19 word20" c:\temp\white.png` &amp;#x200B; &amp;#x200B;
Mocking, moq is a good library for this. Moq the interface and set it up with your dates and the expected result is method has been called. Or similar 
You can trigger the recurring job from the dashboard if you have access to that (assuming asp.net web app with the hang fire dashboard configured) - there's a button to trigger it immediately on the recurring jobs list. If not you can do it in code from the jobs Id eg `RecurringJob.Trigger("my-job-id")` Docs: http://docs.hangfire.io/en/latest/background-methods/performing-recurrent-tasks.html
I'll rephrase. Is there a way to search within a given method if it or any other code called from within that method for a specific method?
You could use Durable Functions for this. Would be a bit "roll your own" but it's designed for workflows.
Worst comes to worst, git blame someone else :)
Mono can perhaps handle it. Your statement is entirely false. 
Something like this may work (if you want a specific height), using System.Drawing: //using System; //using System.Drawing; //using System.Drawing.Imaging; //using System.IO; var imageWidth = 100; var imageHeight = 100; var text = "word1 word2 word3 word4 word5 word6 word7 word8 word9 word10 word11 word12 word13 word14 word15 word16 word17 word18 word19 word20"; var imageFileName = @"C:\MyImage.png"; var fontFamily = "Segoe UI"; var fontStyle = FontStyle.Regular; var fontSize = 8.0f; var fontSizeScaleFactor = 0.1f; var font = new Font(fontFamily, fontSize, fontStyle); using (var img = new Bitmap(imageWidth, imageHeight)) using (var gfx = Graphics.FromImage(img)) { gfx.FillRectangle(Brushes.White, gfx.VisibleClipBounds); var stringFormat = new StringFormat(StringFormatFlags.LineLimit); var size = gfx.MeasureString(text, font, gfx.VisibleClipBounds.Size, stringFormat, out int charactersFit, out int _); while (charactersFit &lt; text.Length) { fontSize -= fontSizeScaleFactor; font.Dispose(); font = new Font(fontFamily, fontSize, fontStyle); size = gfx.MeasureString(text, font, gfx.VisibleClipBounds.Size, stringFormat, out charactersFit, out int _); } gfx.DrawString(text, font, Brushes.Black, gfx.VisibleClipBounds); using (var fs = new FileStream(imageFileName, FileMode.Create, FileAccess.Write)) { img.Save(fs, ImageFormat.Png); } } font.Dispose(); &amp;#x200B;
Im not following at all. Could you give a concrete use case?
This is client-side. I don‚Äôt believe swashbuckle has support for generating client code from openapi xml/Json
I want from the "Start" method search and see if it or any sub methods implement uses of the method ImportantMethod. The chain of methods from Start might be very long, therefor I dont want to traverse all manually. I cannot either to find references on "ImportantMethod" since it has thousand of usages more than the one you see here making it almost impossible to go through them all. public void Start() { DoWork(); } public void DoWork() { ImportantMethod(); } public void ImportantMethod(){ // compute }
Unfortunately not. Web Forms. Although I believe we may be switching now. 
Not even close to true.
This is super amazing.
Cross platform framework in c#. Duckduckgo is your friend...
Part of the new evolution of ASP.NET and Razor pages.. 
Pretty easy to google, but basically Microsoft took .NET and [ASP.NET](https://ASP.NET) apart, made it modular and left out anything that slows it down. The result is a lightweight, cross-platform and faster version. They serve slightly different purposes and the runtime is different.
Well, if you are already familiar with .Net or Asp.Net - you know that Framework itself is closely bounded to Windows and its staff. .Net Core is rewritten from scratch .Net Framework, primaly it was focused on web and cross-platform easier development, so first redesigned framework was Asp.Net. Now it becomes more and more popular, because Microsoft announced more and more development in Core, but Framework itself will be only 4.8 and no more (you can read more about it in official forums). Now Core 3 is coming out and most of Net Api is available, production ready, stable, performant, etc
Someone will be better than me but wouldn't it be easier to create a datagrid / table and customize it to your likings? you'd be able to get it to look basically exactly like your excel graphic could extend it for when you click in cell or colorize it or anything like that for extra information
Core is a cross platform framework so write once run anywhere (check out the net core runtime). ASPNetCore builds on top of core adding APIs for web development. 
It's a super amazing repost.
Serilog has a ludicrous range of buckets etc, so I tend to use that.
NLog. It gets the job done so I haven‚Äôt thought to look at anything else. I originally chose it because it was flexible enough while still being super easy to configure. I looked at log4net but the configuration for that was too complicated for my liking.
I wrote a control like this, with the difference that the time axis is the vertical one. Here are the steps (you may need to look up how every individual step works): 1. Create a custom control, which usually goes into a seperate project (the template I use is "WPF Custom Control Library"). 2. Create a dependency property called "Bookings" or similar on it, so you can bind the collection to it. 3. In `Generic.xaml`, create a layout, with a grid in it, and name it `MainGrid`. 3. In `ApplyTemplate()`, try to get the instance of this grid by using `GetTemplateChild("MainGrid")`. If it's null, return, otherwise, continue. 4. Add row definitions, columns definitions and content based on the stuff in the obervable collection. The cool thing is, with additional dependency properties like StartDate and EndDate, or similar stuff, you can dynamically modify the scope of the control. Just make sure, that ApplyTemplate() is only called once. This is really bad design on the WPF part, but who knows, maybe I'm doing something wrong here (basically, I always check if all depencency properties are set and all template children are not null). Feel free to ask if anything is unclear.
Built in Logger with App Insights.
i still just log to text files lol
Debug.WriteLine(). Not even kidding.
Thanks! I just converted this to VB and tried it out and it does exactly what I hoped it would. Dim message = "This is a song that never ends, yes it goes on and on my friend." Dim imgWidth = 200, imgHeight = 100 Dim fontFamily = "Arial" Dim fontSizeScaleFactor As Single = 0.1f Dim fontSize As Single = 11.0f Dim fontStyle As FontStyle = FontStyle.Regular Dim font As New Font(fontFamily, fontSize, fontStyle) Using img As New Bitmap(imgWidth, imgHeight), gfx As Graphics = Graphics.FromImage(img) gfx.FillRectangle(Brushes.White, gfx.VisibleClipBounds) Dim stringFormat As New StringFormat(StringFormatFlags.LineLimit) Dim charactersFit = 0, linesFit = 0 Dim size As SizeF = gfx.MeasureString(message, font, gfx.VisibleClipBounds.Size, stringFormat, charactersFit, linesFit) While charactersFit &lt; message.Length fontSize -= fontSizeScaleFactor font.Dispose() font = New Font(fontFamily, fontSize, fontStyle) size = gfx.MeasureString(message, font, gfx.VisibleClipBounds.Size, stringFormat, charactersFit, linesFit) End While gfx.DrawString(message, font, Brushes.Black, gfx.VisibleClipBounds) Using ms As New MemoryStream img.Save(ms, ImageFormat.Jpeg) Dim bArray As Byte() = ms.ToArray() imgTest.Src = "data:image/jpeg;base64, " &amp; Convert.ToBase64String(bArray) End Using End Using font.Dispose() 
We have been rolling our own custom logging lib since 2014. But it's a very specialized field and we don't require much. Some sinks for databases, files and event log. String interpolation. Log levels. Could build that shit in half a day again.
in my code i like to use liblog. this provides me with loose coupling to any logging library and it installs no packages/dependencies. other than that i use serilog as the actual logger. that is set up in the app entry point. liblog automatically picks that up. and i just wire up third party libs to serilog too (asp.net core logger, nhibernate logger, etc..). logging into elasticseach (at old job) and seq (new job) is also highly recommended. seq is trivial. elasticsearch you have to set up correctly (mappings) so that your payloads don't mess up your index (log UserId as string one payload and as an int in another).... so best to set up a standard set of optional fields you can log, but can't log new ones.
I've been using MetroLog. It's pretty basic but that's part of why I like it. https://www.nuget.org/packages/MetroLog/
I‚Äôve heard good things about Serilog, we discussed moving all of our logging to that as well. 
On Core the built-in logger will do most of the time. Logging to file is not included though, but the community has already provided some implementations for that, e.g. [this one](https://github.com/adams85/filelogger) (by myself).
Just transitioned from log4net to Serilog, and is very happy with structured logging. We're currently evaluating Seq to search and visualize the logs.
&gt; [DatabaseGenerated(DatabaseGeneratedOption.Identity)] This says that the database needs to generate the ID. So what does that table look like?
The `DatabaseGenerated` attribute doesn't set up your database to automatically insert a value; it just informs EF that your database does that. You still have to configure the database to perform the data generation. How you do that depends on what database you are using. So, what database are you using and what does your `AddNewFurnitureImage(FurnitureImage)` method look like?
| FurnitureImagesId | FurnitureId | PictureInfo | ------------------------------------------------------ No rows have been entered yet. But EF core generated this table. It was not manually created.
I'm using SQL Server. I have two other controllers set up this exact way . I used the EF core 'Code first' approach and have made no changes to anything in SQL server for my other methods to properly insert records. public void AddNewFurnitureImage(FurnitureImage newImage) { _ctx.FurnitureImages.Add(newImage); _ctx.SaveChanges(); } Again, this method above is mirror copy from another method that properly inserts records while allowing the database to generate a unique id.
1. It's a bad idea to trust EF to create your database. Long term it's just asking for trouble. 2. To help I need to see the real database table definition. It should be in SQL form when you ask the database. 
Sounds like your DBContext is missing something. You need to configure identify columns in it if I'm remembering it correctly. 
Serielog does the job. 
While I'm sure there is good reason for it, this is a learning exercise. I'm taking the Code first approach for EF core. Are you looking for the table definition of `SELECT * FROM sys.tables WHERE object_id = 1557580587` returns? What specific information are we looking for?
I thought so as well. But I have no issues POSTing from other controllers/methods/entities set up that exact same way. There only difference is added this controller/entity later on in the project rather than at the very initial migration/setup
If this is SQL Server, right-click on the table and pick "Script table as" =&gt; "CREATE to". 
Go ahead and paste in your whole DBContext class. That should reveal something.
It was almost on the same page as the other one.
Can you post the definition for one of the entities that works?
 public class HouseInfoContext : DbContext { public HouseInfoContext(DbContextOptions&lt;HouseInfoContext&gt; options) : base(options) { Database.Migrate(); } public DbSet&lt;House&gt; Houses { get; set; } public DbSet&lt;Category&gt; Categories { get; set; } public DbSet&lt;Furniture&gt; Furniture { get; set; } public DbSet&lt;FurnitureImage&gt; FurnitureImages { get; set; } } &amp;#x200B;
 public class Furniture { [Key] [DatabaseGenerated(DatabaseGeneratedOption.Identity)] public int FurnitureId { get; set; } [Required] [MaxLength(50)] public string Name { get; set; } [Required] public string UID { get; set; } public Category Catgory { get; set; } [Required] public int CategoryId { get; set; } [Required] public Decimal? Cost { get; set; } public string PurchasedFrom { get; set; } public DateTime? DatePurchased { get; set; } public House House { get; set; } [Required] public int HouseId { get; set; } [Required] public int Turns { get; set; } public List&lt;FurnitureImage&gt; FurnitureImages { get; set; } } &amp;#x200B;
 USE [FHIStorageDB] GO /****** Object: Table [dbo].[FurnitureImages] Script Date: 2/7/2019 11:50:06 AM ******/ SET ANSI_NULLS ON GO SET QUOTED_IDENTIFIER ON GO CREATE TABLE [dbo].[FurnitureImages]( [FurnitureImagesId] [int] NOT NULL, [FurnitureId] [int] NULL, [PictureInfo] [nvarchar](max) NOT NULL, PRIMARY KEY CLUSTERED ( [FurnitureImagesId] ASC )WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON) ON [PRIMARY] ) ON [PRIMARY] TEXTIMAGE_ON [PRIMARY] GO &amp;#x200B;
If the table was being setup correctly then it should have been [FurnitureImagesId] [int] NOT NULL IDENTITY (1,1), So whatever the problem is, it's being manifested in the way EF generates your database.
I believe you've fallen victim to a convention that isn't being applied. As mentioned, sometimes the database provider will automatically configure `IDENTITY` columns. The rules for this aren't documented very well, but I think one of the rules is that the property has to be named `Id` or `&lt;classname&gt;Id`. Anything else, and the automatic `IDENTITY` setting won't work. The key in your `FurnitureImage` class is `FurnitureImage**s**Id`, which doesn't match the convention. Maybe try changing that to `FurnitureImageId`, or just `Id`.
I second that. Use LibLog internally to not pull in dependencies and then plug in a real logger (log4net mostly) at runtime. Works best for me!
ApplicationInsights: went from homegrown to appinsights and haven‚Äôt looked back. It can get pricey if you‚Äôre not managing your log retention but has been great for anything from resource monitoring to exception logging.
Yup. This looks to be it. My other tables are set up that way. And scrapping the table and rerunning it manually solves the issue.
I appreciate your help u/unndunn, but the issue lied in my database. The table for whatever reason was not scripting that column to allow for ID creation.
Serilog. 
Recently switched from log4net to NLog - I found the configuration to be more flexible, it was easier to tie multiple records together in a multithreaded, high-load application, and the asynchronous calls improved performance.
Hi Mr @grauenwolf, can I ask you how well Tortuga plays with Oracle? I have not had anytime to do the investigation required, but Dapper has little support especially once you start using the extra add on packages i.e. Contrib. As the author of Tortuga I thought you might save me some initial investigations :) 
This was eye opening in terms of what people use!
You're welcome. Now if I may, once you are done with this set of tutorials, I suggest learning how to use SSDT to manage your database schema. It is going to be a lot more predictable and exposes a lot of functionality that EF won't give you. https://www.infoq.com/articles/SSDT-Intro
Log4net with the help of New Relic or Kibaba in production
NLog, simple and gets the job done. logger.Error(ex.Message);
Right, and I'm saying the reason it didn't do that was because of the mismatched property name.
NLog. It does everything we need from a logger so I haven't looked at alternatives.
You may! Once I round out the corners of the CRUD actions I will be (attempting) deploying this as a REST API with the intentions of building a phone app via ionic to read and write to the API. This is all very new to me given the scale. I work as a .NET developer on legacy webforms app's, but for the most part EVERYTHING is automated weather it be through bamboo, octopus, bitbucket or has already been set up. Understandly why, this can be painful
It took the property name as FurnitureImagesId once the table was scripted correctly
Oracle support has been on our to-do list for years, but we have been distracted by other projects. If you would be interested in helping with the research and testing, then I would be more than happy to focus on it. 
It takes practice, but you can automate (most) database deployments with SSDT. I won't lie to you. You do have to use the tool for awhile to understand what kinds of changes it can make automatically and which require you to hand-modify the migration script it makes. (But really, that's the same case for EF migrations.)
That's just proof of how fast Core is!
Seriolog &gt; Splunk
It‚Äôs not like exactly once delivery was promises
Serilog
Stop using locks.
Now even more excited what 3.0 is bringing.. ;)
I suggest to use [Office word](https://www.officecomponent.com/products/word) library for .Net . It is very easy to use and highly reliable and compatible with c# .Net
Serilog's structured logging works really well with something like [SEQ](https://docs.getseq.net/docs/using-serilog) or [Loggly](https://github.com/serilog/serilog-sinks-loggly). The [literate console](https://github.com/serilog/serilog-sinks-literate) sink is really easy to drop in and get some easy-to-grok console logging. You can even use their JSON style logging with their [Microsoft.Extensions.Logging provider](https://github.com/serilog/serilog-extensions-logging) `logger.LogInformation("JSON Object goes: {@Here}", someThing);` attaches someThing as a structured/json property `Here` on your log event. SEQ/Loggly/etc... pick it up and process it as you'd expect.
There's two types of people in this world: people who advocate Serilog, and people who are wrong.
How's performance compared to regular dapper and raw ADO.NET?
I recommend to take a look into mediatr: https://github.com/jbogard/MediatR/wiki
Who cares
Love the simple approach keeping all the data in a single DB, separating the reads through views. How would you evolve this solution if you wanted to selectively use event sourcing for some aggregate? I‚Äô 
Serilog to ELK
It is being used in the linked code.
* I would go ahead and do the reads through EF, it seems like too much duplicate effort maintaining two data layers. If performance is a concern, you can implement a cache layer. * It seems like too much information is being leaked out by requiring all orders to maintain a collection of all products in the system, maybe the order total could be a lazy-load value that queries the DB for pricing information when you need the total instead of making every order drag around the collection of all products? * I would have the unit of work not be a part of the customer repository - as the system matures, the likelihood that you end up in a situation where a unit of work can span multiple entities will increase. * I would recommend throwing in some automated tests. &amp;#x200B; Good job though, thanks for posting that!
AWS logger (Cloudwatch) with built-in logger.. and it's shit, I mean AWS part
I haven't looked at Serilog but I like [Elmah](https://elmah.github.io/a/mvc/)
TBH I find the "it's a repost" posts more annoying than it being a repost... But maybe I'm just not exposed to enough Reddit that reposts irritate me that much. Exception to that might be when I browse new on r/gifs and there's 30 of the same gif. 
Seq is so worth it when using Serilog. It makes troubleshooting prod issues so much easier, the querying capabilities are great and the dashboards give you a quick at-a-glance view of whether your afternoon is going to be ruined or not!
We just recently moved our services over to Serilog from log4net. Would definitely recommend, and it's not as painful as it seems at first, just fairly time consuming depending on how much logging you're doing.
There is almost no difference. Dapper uses [ADO.NET](https://ADO.NET) internally, in general it only adds better abstraction and mapping for executing SQL statements.
buzzsaw and Knuckleboom loaders I think
&gt;I would go ahead and do the reads through EF, it seems like too much duplicate effort maintaining two data layers. If performance is a concern, you can implement a cache layer Would you still call it a CQRS system at that point?
&gt; Serilog Seq seems pretty expensive
ad 1) - It is true for simple queries. If query is complex, Entity Framework can generate quite inefficient query. Caching not always is the applicable solution and adds complexity. It depends on multiple factors, but in general I prefer has full control over my queries ad 2) - Yes, I agree - you described an alternative solution. It could be a DDD DomainService called PricingService passed as parameter to aggregate method. It depends really how big collection of products could be. ad 3) EF implements UoW, for me it belongs to repository which is per aggregate. It could be moved to separate class, but I don't see the need to do this. ad 4) True, unit tests are must-have for complex domains. I will add some examples soon. &amp;#x200B; Thanks for great insights! &amp;#x200B;
Apparently you shouldn't use literate anymore. Its readme.md states it's being "absorbed" into the default console sink.
From theory, it would not be the problem because each aggregate should be treated as an atomic unit. Aggregates should not reference each other. But from practical perspective I guess mixing two approaches could be tricky. I think it could be better to this separation by Bounded Context boundary instead Aggregate boundary.
In my view it still is; if I am using commands and queries as my interface to get in touch with the data model, that's the most essential part of CQRS for me. My understanding of CQRS is a little primitive though, I never used Mediatr or any DDD libraries to enhance the experience - it was all just the abstractions I chose to define.
We have output debugging and used custom file writing logs, log4net and now serilog. We have our own ILogger interface and we simply create an adapter to the logging tool we want to use.
for real. love mediatr
Yes
Log4Net
Thanks!!
Or don't. I actually encourage not using Mediatr in almost all situations. All it effectively ends up doing is convoluting your code base with Request and Handler classes. Behind the scenes all it's really doing is using reflection to ascertain which Handler to execute, which isn't the mediator pattern at all. The only valid use case I can see for using Mediatr is if you will have a one-to-many relationship between your Handlers and the Requests they can handle. Otherwise you are just obfuscating the Repository Pattern to say you're using CQRS. In-memory CQRS can be handled numerous other ways without the bloat of Handlers, Requests, and an external dependency. Most obviously with delegates and multicast delegates. TLDR: Dont use Mediatr. Source: I'm using Mediatr in a current project and I have to create a new Request class and new Handler class for every API that interacts with the database. In the code base I'm forced to implement it in it's not even a poor man's CQRS, it's just an obfuscated Service and Repository Pattern with an extra dependency. Additional info: https://stackoverflow.com/questions/50663501/mediatr-when-and-why-i-should-use-it-vs-2017-webapi
I agree. They do have a single developer license that you can use in production but their first priced license at 15 users is a bit steep. They say you can call them for startup or small team pricing. Loggly is even more expensive and their user limits for the cheaper plans are a pain in the ass for any medium-or-larger team. So yeah, the distributed logging backends aren't perfect but Serilog is still a nice logging framework, and there are tons of other sinks I didn't mention.
Thanks! I don't know these stuff about hangfire. Looks like I have to read the docs about it. Thanks!
I personally prefer code-based migrations, with a tool like https://github.com/migratordotnet/Migrator.NET SSDT looked fine and all, but I never felt like taking the time to learn it when code based migrations were more familiar and handled everything I needed well anyways (perhaps SSDT has more advantage in larger organizations).
No problem. We use hangfire quite extensively for our system so I've had to do some digging into it. Their docs are a little lacking and mostly only provide a "getting started" guide which barely scratches the surface. For more resources/assistance they have a [tag on stackoverflow](https://stackoverflow.com/questions/tagged/hangfire) and their [source &amp; issue tracker are up on github](https://github.com/HangfireIO/Hangfire) &amp;#x200B;
I think I can kinda see what you mean. Sometimes I have queries that don't require any arguments and I have some commands that don't return any results. It'd nice to have more options. However, most of the real world handlers I have to write require both a request and a response.
Serilog is awesome, as evident from this thread. I want to put in a plug for [Logary](https://github.com/logary/logary#logary-v5) here. You'll need a license for commercial usage (though it's a recent change. The previous versions are free) but it's a high quality library with a lot of good options and it compares very well with Serilog: https://github.com/logary/logary#comparison-with-serilog
Realistically, with the rise of microservice architectures CQRS and Event Sourcing are natural progressions for maintaining a single source of truth among distributed services. CQRS is great in the correct right application. Using an external library like Mediatr within an individual service is superfluous when .Net provides all the underlying necessities to make commands and queries.
\^This. But millennials don't fucking get the concept of tool generated proxy clients and the volume of fuckups it saves you. We use REST where it makes sense but 99% of internal systems are WCF net.tcp/http and it works.
Amazing guide! Thanks for all the info, I'll try it soon and will let you know how it goes.
No its mostly grinder clones and bullshit. Humor aside (I work for a major bank). Lack of WCF is one of the major reasons we are not looking at this beyond web front ends. Keep in mind they only just now release a version that supports in process IIS hosting. Prior to that you had a huge performance hit unless you exposed kestrel directly. Who the fuck is going to do that? And then there is the fact that all QA is done via telemetry. You crash and they might pour through it in the next 10 versions. No thanks.
Hey thanks for the reply. I can't remember quite right, because it was some time ago since I last tried but I think I tried to do it via datagrid, but to no avail. I'm quite a newb so it might be just as well done like you said. I'm planning to look deeper into it in the following days. However, I think this is getting too custom for the datagrid(?)... And I'd like it to automatically support some cool features which I'm not quite sure of yet (things like being able to edit guest spans by holding and dragging, etc.)
EventFlow is proving to be quite nicely designed general purpose CQRS solution: https://github.com/eventflow/EventFlow
You'll probably find a fair number of shops that thumb their noses at ORMs, just because ORMs allow devs to do things thats shouldn't be done with SQL. I think it's less common, but it's not really "rare" either.
The UI for App Insights is great. I love telling people to look up their own logging inquiries! 
There's always SharePoint \*shudders\* or Nintex
I've spent much of the past 4 years writing integration frameworks between all of these systems and creating APIs, performing data migrations, etc. Most of the data models generated by these SaaS providers is pretty garbage, so there's a lot of enjoyable development work streamlining everything.
I enjoy Razor a lot more than the old ASPX templating engine, but to be honest I prefer to take it completely out of the equation nowadays. I generally write a REST API that provides all the data I need to create the front end in React. For very simple sites, I will just stick with Handlebars templating.
I hate whiteboard interviews. When I interview people, I keep it pretty free-form. I will first confirm that their skill level is as advertised by asking them some technology questions. Last guy I interviewed was a DBA so I asked about clustered versus nonclustered indexes, the benefits of creating views, and some other stuff that I would expect a candidate with 3-5 years of experience to be able to answer in depth. I like to give them a hypothetical issue and have them walk me through their troubleshooting process - do they check logs? are there tests they are running? are they comfortable asking for help if they find themselves out of their league? &amp;#x200B; At that point I'll just sort of switch to talking shop. I want to hear what their favorite projects have been and why. What's the one technology they would pick up if they had a month to learn anything right now? I work at a small company so people on the IT team need to be pretty autonomous, quick to provide back up to each other, and self-motivated. I'm really looking for passion, open-mindedness and teachability more than pure technical skill.
I'm actually working on porting axon framework from Java to .net. Hopefully will get it done in a few months. If you are not familiar with it I suggest you look it up - it's well documented and have commercial entity behind the project
Thanks! I will check as soon as I'm at a computer. Their mobile site is unusable
The downside with the cake script is that you get very little visual feedback on TC about what's going on... What steps does this build have, what packages did it consume and produce, and so on. I'm kinda more a fan of the Azure Devops way of scripting the build config, where the config can be defined in a source controlled file, but is parsed as a "first class citizen".
&gt;This release updates Blazor to use Razor Components Huh, I thought they branded server-side rendered Blazor as Razor Components? That is already an overloaded term for the exisiting "components" entity in Razor?
Another vote for Serilog. We use it for all our logging needs on [elmah.io](https://elmah.io). We log both to our own service as well as to Elasticsearch with Kibana on top. I've created logging integrations for pretty much every logging framework for .NET. I must admit, that while I prefer Serilog, NLog has become pretty much as good as Serilog the recent years. There is also Microsoft.Extensions.Logging, which is a decent choice, especially when logging from .NET Core and ASP.NET Core.
Do you suggest avoiding CQRS? If not, what is a good example implementation you consider?
&gt; But millennials don't fucking get the concept of tool generated proxy clients and the volume of fuckups it saves you. Yes :-/
serilog + seq
Why is the domain full of anemic models (full of public getters, setters, and no invariants and business logic whatsoever). This isn't how you implement the Core layer (speaking also in DDD terms)
I've been using Mediatr in a couple of projects and I love how clean it makes your code. It allows for greater application of the Single Responsibility principle which I think is important. If you've got db access methods doing multiple things like updating and inserting you're probably doing it wrong. That's where CQS helps you to think about splitting them up. You don't have to use Mediatr - of course you could do this yourself but it does make it easier if you want to apply CQS. 
Mediatr isn't CQRS though, it's CQS. There is a difference.
I've worked with CQS architecture in enterprise apps and personal greenfield ones. I can personally vouch for it because it makes you think about the Single Responsibility principle and thus making your system cleaner and easier to test. All Mediatr is doing is finding the handlers to match the command/query objects - it's not doing anything special. It's just a good architectural practice in my opinion.
Likewise, it's was a great conference!
When working with document stores, you more or less sacrifiy consistency for availability and scalability. This tradeoff is just what you bounced into - since a join operation allows you to join in consistent data. There are probably multiple ways to handle this. If you want eventual consistency for this data, you may want to update the copied entries, when source changed. In DDD terminology, you would have those two entities in different aggregates and use domain events to propagate the changes. If mongodb does not support multi-document transactions, this means, that you will have a time inbetween this update, where your data is inconsistent. It depends on your business, if that is ok for you, or not - but probably, since you opted into mongo, it will be ok. If you want strong consistency, you could go ahead and not copy the data and have an id betweeen the documents. You would at least need two queries then to get to all your data, since you can query your products with something similar to an IN statement in SQL. I hope this helps you somewhat.
CQS != CQRS, right? The way I think about it, command = system mutations, query = stateless, denormalized reads. 
What about opening all suspect files, then doing a text search through open documents? Other then using some other IDE, not sure what you want is possible.
I completely agree. That's the bulk of my frustration with it's use in my current application. When I left the project the "microservices" (various web projects deployed as a single unit onto a single host - don't get me started) were each using a Service/Repository Pattern. When I came back the Servics layer was replaced with Mediatr where the handlers all call the Repository methods. I think there could be a use for Mediatr with managing a microservice state in a Flux/Redux like fashion, or using it to manage messaging for a message bus, but I just haven't seen one that couldn't get by without it.
I'm more conservative, so I'd consider `long` for any situation where you have the remote chance of exceeding 100-200 million. When you get that big, you're within spitting distance for running out of IDs. 
I also write Xaml rather than bother with the designer. The designer could be a great tool with some changes though. For example, Javafx‚Äôs fxml format is somewhat similar to xaml and their SceneBuilder tool does layout and editing control properties really nicely, so much so I never bother writing fxml.
They are not the same thing and I must say I had to read up on this to make doubly sure. From what I understand: CQS is the principle that commands and queries should be separated. CQRS is the practise of applying separation of commands and queries (e.g Mediatr) In my experience with CQRS in enterprise we would store the commands and queries as transactional events and apply them to the system. This is what confused me. I thought this was CQRS but it isn't. Although it does use CQRS.
That's all well and good. However, not everything can become a microservice. Not everything should.
We‚Äôre about to implement a backend with EventFlow soon. The initial PoC was really clean, so I have high hopes once we get down to the nitty gritty details.
Yes, by default you're right. We've built some wrappers that show the task progress and feed them to teamcity in specially crafted messages. It works just like you would expect then.
No wcf story or path forward without massive amounts of code and writing a new host? No sale. There is a shit ton of was hosted non http talking wcf running in the real world that works and more importantly scales. Explain to me like im 5 why suddenly this should all go away because you think the world should run on rest?
Hi, could you elaborate on the reason to wanting to burn services and repository patterns? :)
No wcf story? No sale. There is a lot of old and new was hosted non http stuff running the world out there. Not going to convince many companys they need to abandon it for possibly slower rest based http that hasnt been tested since ms abandoned qa testing their new shiny.
I can understand where your frustrations come in. Making such a big change like swapping out that service layer is something I'd question unless it was causing such great pain or was preventing further changes to the system. I tend to stay away from big breaking changes like this. I think that's why adopting an architecture like CQRS is best done at the start of any project rather than a long way into it. I've only ever experienced the former so I'm talking through rose tinted glasses. I've got a couple of greenfield projects both using CQRS to separate the commands and queries but that's as far as it goes. Could I have done that without Mediatr? Yes. So right now it's just a case of getting that separation between the two to help try and adhere to the SOLID principles. I did work with an enterprise app that was using transactional events but to me that isn't CQRS. That's event sourcing. At one point I did confuse it to be CQRS though..
Come to think of it, what you've described sounds more like someone implemented CQRS and Event Sourcing. The latter is usually implemented ontop of CQRS. Are you sure you're not confusing the two? CQRS are its very definition is a very simple idea.
&gt; But EF core generated this table. It was not manually created You mean you generated it from the migration builder ? See here: 
&gt;It's a bad idea to trust EF to create your database This is why i started using Dapper in my personal projects. Plus, i get to write pure SQL. 
Yes, generated from Migration, that statements wording was misleading.
Ok then read through the github issue and you'll see how the migration build should have built the table. Please Note: that the migration build **is** a first class citizen for EF Core and Sql Server so not sure why it didn't autogen this with an auto increment annotation.
Woooow, just looked up an overview of dapper. Dapper seems far easier to use if you have SQL experience. Will have to keep this in mind for my next project.
I just re-read your first comment in this string of messages and you were correct right out of the gate! Guess I just needed to be slapped in the face with the answer. Thanks again!
My thoughts about this design: \- commands and queries should be immutable, they are not (public setters) \- this is Anemic Domain model, entities don't have any behavior \- entities are not encapsulated (public setters) \- Application layer implements both application and business logic (because of Anemic Domain model). So it is Transaction Script pattern. \- Persistence for me is a part of Infrastructure, I would not separate it. \- The critical part, domain model is not tested at all 
Hasn't that been stale for years?
Custom logging library currently. We're transitioning from RabbitMQ to fileBeat as the transport layer. Will fully switch to NLog when we're done with that transition.
I quite like the idea of PostSharp, but if I introduced it into one of our projects it would commit us to dozens of licenses. We have a core team who drive a product, but we can get pull requests from various other teams, all who would need PS licenses to contribute? 
No worries. Just glad you got it working. üëç
Yeah, pretty much. But it's solid and it's almost effortless to throw it into your application to start logging unhandled exceptions. Customization is fairly easy too. For my next project I'll take a closer look at Elmah. While it can be a pain to use stuff that isn't currently supported there is no shame in using a perfectly good tool that a little old. I also just pull the source into my project, if necessary, to expand the capabilities or fix an issue.
Part cross-platform development, but also increasingly because it's simply better performing than .NET Framework on Windows. Microsoft can also move forward with a higher pace with it. So use .NET Core if you can, use .NET Framework if you must.
You only need the license to compile the code. Other teams can still make changes to your source and send it to a build server, or they can consume your projects as assemblies or NuGet packages. There are definitely ways to get around the problem. It is true that you will need licenses if everyone wants to test and develop large-scale solutions locally but you only need a small handful of licenses to provide PostSharp benefits. For example, we have a "Helpers" library here which contains methods for accessing databases, registries, event stores, and all those other mundane tasks. PostSharp wraps those with threading models and logging models so that a developer including "Helpers" in another project doesn't have to ask themselves how those concerns are handled.
It's a great post, don't apologise for adding detail :) I'll give it a dig again. I'm doing a lot of transitional work at the moment, so even if licensing is bad we might only need them briefly. Thank you for the information :)
Still far from the end goal. Stayed tuned for more!!
Yeah we were put off by the licensing too.
Thanks, I appreciate it! :)
That's completely understandable and has been the major obstacle for me as well.
It's bad enough it overshadows any benefits of the library. I spent a good amount of time removing it from our code base. If you look closely most of the nice features are available in the regular framework. 
&gt; Other teams can still make changes to your source and send it to a build server That's making it much harder to change the code and I'd say it more than negates any benefits PostSharp might have.
It doesn't need to be harder. NuGet packages can be distributed on the local network the same way as they're grabbed from Microsoft. Unless you're talking about not being able to modify the NuGet from a different project. In that case you're probably used to working on monolithic solutions where none of the projects have clearly defined boundaries. Having those boundaries defined is how you get the benefits of AOP. If you don't define them clearly then you cannot get the advantages of AOP. That's just how it works by design. 
Even with clearly defined boundaries, you sometimes need to change the code beyond that boundary. I thought I made it clear I was talking about that by quoting that part of your post.
Very cool stuff!!
Nlog is also better in performance than log4net
Splunk is paid if I'm not wrong?
A relic from closed-source enterprise land that will soon be completely dead and hopefully replaced with an open source solution. 
There have been open source AOP libraries for at least a decade now. The problem is that these solutions are not easy to architect. I've tried both methods and PostSharp is what makes it feasible to save time. 
I was addressing that same point yes. 
Pretty sure. Im not 100% on there not being a free tier though. 
That‚Äôs a fair point. I guess my issue is that an increasing amount of the .NET ecosystem is open source, and anything like this is completely incompatible. It‚Äôs a great write-up though, and has inspired me to take a look at some simpler patterns to solve the issues presented. 
Have you tried browsing the source code from that same archive page? https://web.archive.org/web/20120822054619/http://code.google.com/p/sharepod-lib/source/browse/#svn%2Ftrunk It's kind of a hassle, but I think you could recreate the project file by file.
How about a description and/or link
I have. The nested folders were loaded over JavaScript, and as such weren't scraped by Wayback Machine. Sadly those unscraped files are the most important!
Sorry about that üòÖ. The project and documentation links are now added
If the situation is really desperate, you can try disassembling with dopeek. 
I used [ILSpy](https://github.com/icsharpcode/ILSpy), but the results weren't that great. Since my goal is to update this for .NET Core (and make it cross platform) losing original symbols and stuff complicates it a lot further.
Honestly, just bloat. Almost everywhere I see the service/repository patterns aka n-tier architecture I see just massive classes that do way way too much.
I my experience dotPeek is better, specially if the pdb were available. But YMMV. 
I only have the dll, but I'll download dotPeek and give it a try! Thanks for the tip.
Thanks. The first link doesn't work for me but the 2nd does. I suggest you have a standard elevator pitch and links that you include in every post, including blog posts. The less work people have to do to find out what your project is, the more likely they are to follow it.
hope you get lucky
We had the same issues, before our codebase started being used by the whole company, we had to remove PS. It would commit us to paying for dozens of licenses forever.
You're missing a trailing e on the github link. [https://github.com/stsrki/Blazorise](https://github.com/stsrki/Blazorise)
There is a backup of google code, I tried finding it for you on it, but I didn‚Äôt have any luck. Try looking at this. https://code.google.com/archive/schema
Yeah that archive was made in 2016 when Google Code was closed. I think the project was removed from Google Code in 2013, so it wouldn't be a part of the archive.
Can you upload the dll, and link to it?
My goal is to update it for .NET Core and remove all the Windows-specific bits, so I can't use the .dll itself. (Or did you mean can I give you a link to download the .dll?)
.NET Reflector will decompile it. The source code won‚Äôt be pretty but it will be functional. 
The goal is to update it for .NET Core, so I don't think it'd even be functional :(
I'm 100% on board with you on that. Open source and the direction the .NET ecosystem is headed is not compatible with PostSharp's current model and that of many other tools which haven't adapted yet. Now that the .NET community of developers has a taste of what can be done if we embrace open source then they're not going to tolerate closed source solutions forever. At least that is my hope. Thank you sincerely! It means the world to me I was able to get this written up and for anyone to read it.
The latter, I would like to look at it to see how hard it would be to convert.
[Here you go!](https://drive.google.com/file/d/18RENnhYH1oIIelNtdw9g0x5H-Hfa_nVM/view?usp=sharing)
That one has a license that is incompatible with your goal, so you will need to find a different dll that has a more open license. Other then that it has also been obfuscated. Its not hard to deobfuscate it, but getting it to compile, and then making everything readable is going to be tedious.
Yeah, which is why I'd prefer the source since that's LGPL.
Unfortunately I was in a hurry and edited links on my phone. Good idea about creating some standard post templates. That way theese errors will be less likely to happen. Thanks
[removed]
Slightly dated, but I found a good article on it... https://msdn.microsoft.com/en-us/magazine/mt185569.aspx
Your post has been removed. Self promotion posts are not allowed.
Nlog and Stackdriver 
Posting a question like this sometimes throws up a suggestion for an alternative library or solution that I'd never have considered so there's always value in a discussion like this :)
The biggest part I like about MediatR honestly is the pipelines. In a enterprise project I'm working on which made it easy to add extensions on the app (logging as we don't do event sourcing on that project) that isn't necessarily part of the business logic domain. Just create a pipeline behavior and add it to where you'd want it to fire. Pair it up with `FluentValidation` made it easy to sanitize requests before even reaching the handler. It made the domain cleaner and more focused on business logic instead of having to sanitize input there. Though this was a ground up project so it was designed with MediatR's style of CQRS from the start. I can't say anything for refactoring an existing app for it as most of the time that'll be a huge undertaking which adding Mediatr might become a nuisance if improperly added.
Assuming you are talking about *HttpContent* extension method, there is no guarantee that the network operations have fully done by the time the method is called.
It's possible to [instruct][] the `SendAsync` method to resolve when the headers of the response have been received, with that `HttpCompletionOptions` parameter. If that‚Äôs done, then reading the rest of the response should be done asynchronously. [instruct]: https://docs.microsoft.com/en-us/dotnet/api/system.net.http.httpclient.sendasync?view=netcore-2.2#System_Net_Http_HttpClient_SendAsync_System_Net_Http_HttpRequestMessage_System_Net_Http_HttpCompletionOption_System_Threading_CancellationToken_ 
oh? I assume this is the best way to do it?
Depends on the use case. If the response body is being read unconditionally, nothing is gained for the complication of managing the lifetimes of the moving parts yourself. It's only been a meaningful difference a handful of time in my C# career. Unless you meant that using `ReadAsAsync&lt;T&gt;`is the best way? In which case, yes. Definitely, for any deserializable response. If it's an image or something, reading the bytes directly is probably better? My brain's in weekend mode, so I can‚Äôt be certain on that second part. 
No, I'm just receiving an object from a different service I controll myself. In my head it would make sense for it to be faster to read the header and then start deserializing as the body comes in than to wait for the body to come in and the deserialize. But most likely this method does just that!
This is probably one of the worst videos I saw out-there on youtube ... We should have a rule that any low quality content is not accepted on this sub-reddit !
The graphical aspects of this are definitely doable. Both grids and Stack Panels would be good starting points, each with their own pros and cons. The biggest question would be what behaviors do you intend it to have? Should it be paged like a week calendar, or a single page that's infinitely scrollable? Where is the data coming from? DB backend? Should the schedules be editable? 
Well this is something that you don‚Äôt see everyday
This is blatant self promotion, which is one of 3 rules for the sub, so yeah...a mod should remove this post.
Did you try to drill down into the code (Resharper is great for decompiling on the go) and find the awaits?
No! This question came to me during the weekend. I'll do that on monday :)
...Windows Vista!
HTTP request has the opening line, some header lines, and then optionally a blank line followed by the request body, if sent. For highest performance, web servers theoretically can start routing the request before the OS is done reading it off the network and all the hardware buffers and sending it to your app. Hence why HTTP body in C# is presented as a stream. You don't know what the stream represents, it could be all pre-buffered in RAM like normal variable but it could also be hardware buffers or still out there on the internet.
You can‚Äôt run .NET Framework projects on Linux. If you want to work with this setup, you‚Äôd have to switch to .NET Core. 
There is a chance it will work with Mono, but I don't think Mono implemented everything that ASP.NET Framework has, so it also might not.
I‚Äôm not sure either, but I wouldn‚Äôt see the point anyway. Since they are just working on a project for fun/school, they are better off using the latest technology. 
Make sure it is AspNetCore, otherwise it won‚Äôt fully be compatible and you are gong to find many other problems sooner or later 
Worked immediately after we switched to Core, thanks a lot. 
Really depends on if you are really returning a task. See the async void section https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md#asynchronous-programming
While it's true that there's a ways to go before client-side Blazor is released, server-side Blazor (aka Razor Components) is going to be part of ASP.NET Core 3.0. Most of the knowledge developers gain using that will be transferable to client-side Blazor when (hopefully) it hits 1.0. In the same vein, knowledge gained from learning about client-side Blazor (from this book potentially) would be transferable to Razor Components. Most of the stuff I've read on the topic recently suggests that Microsoft thinks Blazor is promising, but its progress has been slowed down by the immaturity of Web Assembly. I understand how some people are skeptical, but personally I'm tempted by the opportunity to get in on the ground floor of an emerging technology.
Aspnetcore isn't guaranteed to be compatible either. Until version 2.2 it can run on net Framework as well.
I can‚Äôt wait to sing my teeth into Blazor
By default, nothing. Once the `Task` objected is GC'ed, the ` TaskScheduler.UnobservedTaskException` event will be raised. If `&lt;ThrowUnobservedTaskExceptions&gt;` is configured, once the task is GC'ed it will crash the app similarly to other uncaught exceptions. One approach is to use `Task.ContinueWith` to attach a handler that will log exceptions from an otherwise unobserved task.
Honestly excited for us high level guys to have a say browser side :). Been following this project for ages. Hoping for the best 
Blazor will be even better once WASM can talk to the DOM.
Isn't this technology out? What are you waiting for?
A real release.
Not using angular but webpack and the asp webpack nuget have been working fairly well for me in a mixed Vue, MVC, Core 2 project. 
I meant compatible between operating systems running the same version of netcore. They should develop using only netcore, not the full framework.
That's not an answer on his question...
Why not both?
You state you want to cache the data when it arrives, but if that is after your 2 second limit, that won't be happening anyway. Does that change your mind on how you would handle the task? If caching is involved this seems like something you would want to use a singleton or caching mechanism for and load the data from the view asynchronously via Ajax. Then the mechanism for this can be cached and reset itself as needed, and if it takes more than 2 seconds who cares let it finish. 
Which extension are you using for analysis on null reference exceptions?
The method being called likely does the cache population - not when control returns to the unawaited calling location.
Back in the .NET 4.0 days (if I remember correctly), any ignored exception thrown by a Task would crash the AppDomain. It's no longer the case though.
Not even just Blazor. But once other languages can easily manipulate the DOM, I think there will be a web revolution. I'm excited for it!
Top answer is a good answer. But, you should pass CancellationTokens. If you get hammered with requests you‚Äôll have lots of dangling, self running, pointless Tasks eating up resources.
The old .NET on Linux is a bit like oil and water. I got so fed up trying to find solutions I started blogging my experiences so I'd have something to reference. [Here's](https://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html) a detailed setup of how you get the old ASP.NET running on Linux. It can be done but it's a bit of effort!
The full .net framework is only 100% compatible with Windows. Mono is an option, but doesn‚Äôt implement 100% of the full framework specifications.
Multitargeting with a condition on OS. Easy. That way you run using net framework and core on windows and .net core on linux.
Agreed, Razor Components themselves are a understated but important upgrade over the old MVC views. Running these on the server-side also provides most of the nice frontend experience without webassembly.
Even if that was easily doable and manageable (which I don‚Äôt think is), I wouldn‚Äôt go that way unless there is a very specific reason for or if they just want to have fun. How do you deal with dependencies? How about specific code for each OS? I might be missing something here. Please elaborate on the reasons to follow that path.
Visual studio tells you when code is incompatible with one Targetframework or the other. If you can't find a way you can ifdef around certain framework differences. This is no obscure feature. Multitargeting is a prime feature of the new csprojs and key to migrating from framework to core.
Ended up just doing it with Webpack. Last time I used it must've been like 2 years ago, it was a hell to setup back then, but it isn't so bad now. 
It depends on how big the response is. If it's a large file then you can first check the headers and decide to do something with the content after, or even cancel the whole request. If you're dealing with small responses then it's not going to make a difference. You should focus on the most readable code first unless performance is an issue. &amp;#x200B;
Ah, sure. In that case (migration) that is an option. But if they are starting and doing for fun, going straight to netcore would be way more elegant and easier to maintain later.
r/AZURE
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/azure] [Token Purger app, where to start](https://www.reddit.com/r/AZURE/comments/aozzko/token_purger_app_where_to_start/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Honestly, if you have the authority in your organization to deploy something to it, this sounds like something that could be done right on the SQL server with a SQL Server Agent job that runs a single UPDATE or DELETE query in a stored procedure. Then it's backed up with the SQL server, and restored after a disaster recovery. Decoupling components isn't *always* the best way to go. But it also could be that I haven't thoroughly understood your requirements. But as to the question as asked: I've done several projects with Service Fabric and after a steep learning curve IMHO it's an absolute joy to work with. However unless you need this app to be rock-solid in terms of reliability and scale to the moon and back again, it's **massive** overkill. I'm not an expert on them, but an Azure Function triggered by a timer could work fine. Would definitely be a better fit to the task than SF. (And cheaper!) A console app might actually be easier to test and debug. Just make sure to containerize it, so you easily deploy it anywhere: AWS, Azure, Docker on premise, or just about any cloud - seems like they all support containers now. Other than the SQL Server Agent solution, this is probably the one I would pick.
I'm going to talk it over with my team and see what we can do on the SQL server Agent job side. If not, it's certainly not out of reach to deploy a docker container with this capability in house. Thanks for the insight. Cheers!
Ideally, WebAssembly is meant to write performant operations that require integer and floating point operations, like resizing an image, do cryptographic stuff, etc. People has the very wrongful idea that WebAssembly is meant to replace everything related with the UI stuff that Emscripten compiler does. 
Good, Gulp is obsolete imo and I wouldn't use it at this point. I wouldn't use razor if you got a choice. Why would you mix 2 front end frameworks (razor and angular)?
I'm not using angular. Just razor pages and standard html / css / js. The reason I had do setup webpack myself is because I'm not using angular or the like - which has live reload / hot module replacement out of the box.
Use ts over js, so much better
It depends on the libraries it uses but I had some .net codes that have been converted to Core by just renaming the ‚Äúusing blahblah;‚Äù parts at the top and some initializers in the code. If this case would be valid for you, then you won‚Äôt need to know the variable names too. You should give reflection a chance.
Look for this guy that it was not comfortable question for him. I think that he knows that if someone good use this technology new very strong competitor' ll show on the market.
I‚Äôm not using any, since I haven‚Äôt found one. That‚Äôs one of the things I‚Äôm missing from resharper.
Your post has been removed. Self promotion posts are not allowed.
 
Amazing! In which way does this change the IL? What makes it faster compared to C#?
Thats what I dont know. How can I generate the IL?
You can decompile it with something like dotPeek or ILSpy.
Just because that‚Äôs what it‚Äôs good for now, there is. No reason you shouldn‚Äôt use it for Everything JavaScript can do eventually.
Are those two for loops strictly identical? I don't know F# syntax well enough to be sure, but something doesn't look quite right there
You are pretty much asking people 'chocolate or vanilla', 'emacs or vi', etc. I like vue.js myself.
I don't think there's such a thing as "more popular with .NET Core", since the API and the client are separated (even though you can create a single project that includes both). But generally speaking, React is more popular than Angular and there are way more jobs involving React.
I like react because I think its less verbose. But my much more experienced colleagues say angular. I‚Äòd recommend: Create an example/tutorial app in react, angular and vue and decide for yourself. 
From my observation Angular is more popular with .NET Core. I think the reason is Angular is strongly connected with TypeScript and TypeScript is quite similar to C# in many aspects and it is created by Microsoft. I have to point out again that is my observation and I don't know any statistics. I won't tell you which JS framework is better - "it depends".
This 100%. It shouldn't matter: 1. While you could argue that Angular was/is more popular with pure .NET Dev's, 2. React has a huge pure JS following and you're likely to find more React dev's than Angular ones depending on your market. **TLDR**: Look at local job market for ultimate choice but I'd be surprised if there's a "wrong" choice ultimately. 
Personally I'm a fan of React, but I *suspect* you're going to see more Angular, because .NET tends to be more common in the corporate world, and it seems like Angular is targeting/more popular with corporate users. In the long run it doesn't matter all that much which you pick since you're building an API with separation between the front end and back end. 
how many iterations were you timing it for/averaging over?
But that won't happen if the task isn't awaited and the 2 second time limit expires. I agree that the method being called likely does (or if not, should) do the caching, that doesn't take away from my point. 
Vue. But I prefer Dotnetify + Vue
If it isn't statistical noise, remember that F#'s bindings are immutable by default, and the compiler may be able to make some additional assumptions/optimizations as a result.
I say skip asp.net MVC and jump into Asp.Net Core. Nowadays people are writing single page applications (client side rendering) vs server side rendering, i.e. Asp.Net MVC. Learn some back end Api and then pick up React or Angular for the Frontend 
Why not try Blazor? http://Blazor.net
If you are not obligated to learn MVC 5 (e.g work), why not learn ASP Core ?
https://sharplab.io/ might work
To add, visual studio has a great angular aspnetcore template to get started 
I'll start by saying ive really enjoyed using blazor. I just finished building a sizable side project with it and loved using it. That being said, if OP is diving into frameworks/js libraries to try to gain experience for getting a future job, id highly highly recommend against learning it. Blazor (client side) is still listed as experimental and not production ready so i realllly doubt you'll find any companies using it. Theres also the chance (although very unlikely) that microsoft decides to throw client side blazor in the trashcan as there's been no official commitment from them to continue with it. 
I will get my first job next month, it is a very small company and most of the work is being done in MVC there, though I will learn Core on my own.
Then take the course on Asp.Net MVC 4. The asp.Net MVC 5 has differences in the middle wares such as OWIN, but you‚Äôll learn that stuff with Asp.net Core as it uses Kestrel. Skip the Asp.Net MVC 5. 
Doesn't matter provided you have some prior experience, but to a newcomer the combination of technologies may matter because of the ease of access to learning resources. I don't know anything about React at all, but I know there are a lot of introductory books and tutorials that deal specifically with Angular clients and .NET Core APIs.
Yup. It literally doesn‚Äôt matter what the front end is created in. I‚Äôm a fan of Vue as well, nuxt specifically.
From personal experience I can say that getting started with typescript is extremely easy when you are familiar with C#. 
I do both but personally I tend to favor React over angular when I have the choice: easier, lots of documentation, you can use typescript as well Hey, azure devops, office portal and outlook are written in react ;)
Err.. asp.net core is also MVC. You are very specifically talking about single page applications. But honestly at that point you're nearly HTML and JavaScript. But actually the trend is *actually moving back to server side rendering* as of the past month or so.
You're not wrong to do MVC. MVC is also in asp.net core. There's some big confusion on terminology here for some reason. Lots of companies are moving to MVC and are just now getting into it. Asp.net Core means you can use Linux and avoid IIS on Windows. But if you're workplace is already IIS dependant, use MVC and embrace all that it has to offer. Following online web dev trends is ridiculous when trying to do things in the enterprise world. It's great if you're indie or in a startup. But that's about where it ends.
For next few months I would have to work on MVC 5 only (MVC 4 not being used where I work) hence not possible to Skip MVC 5.
I work on Enterprise software for a big financial firm and we‚Äôre not doing any new development on Asp.net MVC as we‚Äôre developing SPA in the last few years...
What I am saying is, if you learn asp.net MVC 4, then 5 is and won‚Äôt be hard to pick up as the basics are already there. MVC 5 introduced middlewares to help you host your web server outside of IIS on windows servers. 
I have never been a fan of React because it is made by Facebook. Forget privacy concerns with them as a company. The problem is their flagship product, facebook.com, always is in perma-beta. It always has critical bugs in core features. And it always has. Google and angular are not perfect, but they operate at a level above.
&gt; Which tools are you using for SPA? I have advised to learn as much JS,CSS and React I can learn.
Exactly, so long as you decouple everything use whatever framework you want -they will all be dead and replaced in a few years anyway!
My team in particular uses Angular. I have used react in the past. As long as you know JavaScript, CSS, html you should be good. If the company you‚Äôre joining is using MVC 5, I am for certain they‚Äôre using jquery still...
Ok, this is good suggestion. Should I learn membership and security modules too from MVC 4 or skip them because Scott Allen in his MVC 5 course says that except these two topics rest of the content from his MVC 4 course should still be watched and in place of Membership and security modules MVC 5 has introduced new identity components.
Learn the basics. Skip the security stuff as what your company does might be totally different from the tutorials. As with all security, nothing is ever straight forward and easy. 
I also work for a big financial firm on enterprise software. It's been a pure fight to get off of web forms.
That's probably work for getting it running on Windows, but not for cross-platform support.
They‚Äôre are strategies in place to leverage both. We have our monolith application that can bootstrap SPA‚Äôs. At the end of the day these SPA frameworks just compile down to JavaScript...main.js, vendor.js, styles.css. 
Yup, that‚Äôs why I always have two separate projects for my apps. Putting them both in one solution is a headache to be, IMO.
Any specific reasons behind the use of Jquery in MVC 5 development? I have good understanding of Javascript and want to avoid Jquery.
When working on server side rendering views, there will be the need to manipulate the DOM and jquery framework was a popular way to do that. You basically show and hide divs / or update data fields using CSS classes based on the Ajax response from the server. 
MVC is fine.. you don't have to jump onto the Core bandwagon, it isn't THAT much better for simple web applications.
But there are still more mvc5 jobs than core jobs. Additionally, by learning mvc5 and then core, one can better understand the reasons behind solutions in .net core architecture. Again, the technologies are closely related, so the candidate could relatively easy add another line to their CV
Ember! There are dozens of us‚Äîdozens! For real though, whichever you‚Äôre comfortable with; using .NET core doesn‚Äôt preclude what you use on the front end.
As someone else already said, you need to learn this through practice and the most important thing is to learn how to use reference, because you cannot learn everything. However, if you want a place to get started, you can take a look at [this course for free](https://www.udemy.com/windows-presentation-foundation-wpf/?couponCode=REDIT100), for limited time, you will all the basics that you need and a bit more with that.
Either one is fine. I prefer Vue.js
JetBrains' dotPeek is free to download from their website.
Vue
I prefer Razor server pages w/a lightweight SPA (pager) and binding (vue) lib opposed to the DIY world of Angular. I like not writing session, security and other components. Precompiling pages, resource bundling, minification, and other release optimizations have all been there. Tree shaking with webpack was superior in Angular for a while. I imagine the community has something for that now. Angular is a great platform for those not already in the .net fold, and TypeScript is neat. But I don't find anything technically superior to what we were doing in MVC + VM libs. 
Honestly I don't have a valid technical solution, I normally just use a loading spinner when I do AJAX calls to let the user know that something is indeed happening and they (hopefully) exercise patience. Some operations are idempotent (result will be the same no matter how many times the form is submitted), so in some cases it won't matter. I like the idea of the hashing, but you have to consider what are the chances that the data you are collecting is so commonplace that two people could legitimately have similar submissions. I wouldn't rely on user agent data though, it can be forged or omitted. You could have someone generating a different UA with every request for whatever reason (privacy, they were previously debugging their code, etc). You could do it with a session ID, but you wouldn't want that in the form because someone could do some damage with that value and impersonate a user. You would want to check the value of the session ID set in the cookie, but for anonymous users this doesn't help you unless you have a mechanism to create a session for everyone.
React is more stable imo, remember the whole Angular 1=&gt;2 transition clusterfuck?
I love React. It makes the functional programmer part of me very happy and it just clicked with me a lot better than Angular did. Regardless of what you choose, build management in JS is a pain in the ass. Not gonna lie, I don't know shit about WebPack or how to set it up myself. I generally use [create-react-app](https://github.com/facebook/create-react-app) to create the basic layout for my React app as it provides you with the package chain and a working build system. Instead of using Typescript, I use ES6 with [Flow for static analysis](https://flow.org/). There is a plugin for VS Code which will integrate the warnings from Flow into the IDE.
There are a lot of Web Essentials VS extensions that could do everything you ask for, but I dont like that route because it doesn't work for automation. You can find someone else's build script for JS and use it for yourself. Gulp may not be a popular solution anymore, but I still use a Gulpfile I wrote 2 or 3 years ago to handle basic build tasks. It may not be the fanciest, but it works.
The reason there are many different solutions with very different results are because what you're actually trying to prevent can be very different and will heavily influence how you implement your protection. So for example of the request is just an an update in a CRUD system then updating twice isn't really going to be an issue. However, you're probably implementing the system to stop people accidentally doing it or to stop them doing it if it's maybe taking a bit longer than a second. I would say I can't think of many reasons you would want to stop people double submitting that couldn't be undone via something like dev tools. If people are that determined you're not going to stop them. But without knowing the reasons, hard to say. What's the problem you're trying to solve?
Maybe pass the cache request off to a queue running in a microservice that will handle everything so the MVC controller doesn't have to worry? You won't get the data back right away obviously, but it sounds like this is already a situation you have accounted for.
There is no problem I'm trying to solve at the moment, this took my attention while reading some blogs. And I didn't find any clear step by step way to solve this. But indeed this will depend for each project. Each use case will have its own way of securing it I believe.
All of these dont work on Mac Os sorry. Atleast not streamlined. I might boot into windows and check it out on VS Studio
Debounce on submit button on your front end. Introduces some delay on submission but might help take care of people who think double clicking is how you submit things. Obviously not a be-all-end all, but might help along with other techniques. 
IMO the Angular team really screwed up with the naming. Angular 2 is not a second version of Angular 1 (angularjs). It's a completely different framework and would've been better off with a unique name.
Imo, that clusterfuck was necessary for the Angular framework to move forward.
Because Google tool over the angular.js project and completely redesigned the framework from the ground up. All of the releases since Angular 2 have made it easy to upgrade version and have had no major changes, and this is the future of the platform. Angular.js is completely irrelevant to Angular today. 
Do you have any good books to suggest?
For some smart reason whysoever our initial desktop app developers for a Visual FoxPro application decided it would be a great idea to start at 1,000,000,000 and auto-increment in steps of 2. Reasoning for the 2-step increment is doing manual edits inside the database and keep it ordered. Because people don't want a datetime field, because that's not supported from our seniors. Once a big customer of ours hit the 2,4mrd mark after about 16 months, they were offered to have their data "archived" to make space for more data.
https://learnxinyminutes.com/docs/typescript/
Thanks üôè
Disable the submit on click and only reenable if an error occurs.
If you are frontend developer you don't even need to know what programming language uses the API. Anyway you should take endpoints of the API and implement adapter in angular/react/vuejs or whatever you choose eventually.
I think iOS had some issues with this when I last tried to implement it. 
Asp.net webforms?
Yuppers. 
Ha, I knew it. That's because webforms uses browser detection based off user agents and when it doesn't know about a browser, basically all modern browsers at this point, it sets the browser compat to basically IE4. MS broke their browser detection several times when they changed the IE user agent strings. I remember having to write new browser definitions for webforms. 
Thats interesting. If I can get the assembly decompiling working Ill keep an eye out. I dont think its noise. If it was, both would move around the save average. But F# is consistently better performing. 
&gt; But I prefer Dotnetify + Vue What's the advantage of Dotnetify over a more regular REST approach with, say, vue-resource?
Just an FYI, dotnet core has a create react CLI version that is dumb simple to use and works great... I feel like react works better as a companion than angular does. But, I'm more of a Vue fan myself üòÅ
Personally I prefer Angular. It basically boils down to whether you want a more full out-of-the-box front end framework (Angular), VS a leaner front end library that will probably need to be used on conjunction with other libraries (React).
Isn't this the way the asp.net default build does it?
What default? Is there a default Docker file? I didnt think they provided one with asp.net
scott blogged about this a while ago and even went into running tests etc https://www.hanselman.com/blog/BuildingRunningAndTestingNETCoreAndASPNETCore21InDockerOnARaspberryPiARM32.aspx 
Why are you using non-relational database with relational data? Pro tip: If there is possible use-case under which you might want to join, you have relational data and you ought to use a relational database. &amp;#x200B; The only use-case for non-relational databases are non-relational data such as whose structure you do not know. But in your case, you obviously do.
Ember + "jsonapi dotnet core" works really well
We actually use WCF with Angular :) 
Bookmarked it. Was looking for a .netcore video and found all low quality or old ones :D
Haha :D.
5. Use ConcurrentDictionary https://docs.microsoft.com/en-us/dotnet/api/system.collections.concurrent.concurrentdictionary-2?view=netframework-4.7.2
https://github.com/dotnet/dotnet-docker/tree/master/samples/aspnetapp
For angular I have read 'Angular 2 Cookbook' [Amazon Link](https://www.amazon.de/Angular-Cookbook-Discover-solutions-challenge/dp/1785881922) Its definetly a great starter if you have none to almost none real experience with Angular 2. I liked the fact that every chapter can be read as a single unit. Chapters do not have to be read from chapter 1 to chapter x. Of couse this book works with TypeScript code examples. I cannot give any advice on TypeScript books, since I havent read any yet
you are doing god's work
Thank you :).
Bought it!
Hey, sorry for the late reply. I'm not yet absolutely sure about the functionality. To some extent, the user should be able to modify/add/remove guests by just clicking/dragging/right-clicking specific cells. On the other hand there's a point after which the input stops getting more intuitive, but I believe I can find that pointy mainly by trying it myself once it's done. However, my first concept should include LMB click selecting existing guest or cell where no guest exists, LMB hold to select multiple cells/guests. And then I wouldn't mind some buttons on the side to play with the selected cells (remove, add new, change details...). The paging should be infinite scroll with being able to search specific dates/months. But infinite scroll is a must for overlapping guests. The source needn't be anything complicated. My first attempt was just na observable collection that was saved in an XML in appdata. Local database would be another alternative, however online DB is overkill. And schedules should be editable. Preferably straight in the calendar. Specific guests shouldn't be very complicated themselves. Mostly name, datetimes from/till, and some misc info like contact, count... And perhaps some additional notes (to self). The program itself is basically a notebook for organising accomodation services, purely informational for the user/host.
If you are truly decoupling your UI from the backend, it shouldn't matter. I have been building WebAPI's using dotnet core, and frontends as a separate project using Angular. Since they are separate, I could just as easily done the frontends in React. I say, pick which one you want to learn and go that route. There is no right answer here.
Looks like that \_context.Keys class has non-nullable DateTime property which is being initialized by NULL value from DB.
You must be doing something within your view model constructor against that nullable datetime column.
If it‚Äôs to cut savings, don‚Äôt go that way. Creating this dynamically is way harder than just doing manually. There are multiple approaches to doing this. Do you need indexing on the fields, if not, storing json in a column could solve it. Going for a full document db could also be away. 
Check the stack trace. It is trying to get a DateTime value from the SQL query result but it is getting NULL which is not a valid DateTime. You can see from your screenshot you indeed have a NULL value. Either you need to use a nullable DateTime (so "DateTime?") as your type, or the column must be "NOT NULL" and a default value specified (such as the 0 value for DateTime).
Yes, thanks! Changed DateTime to DateTime? and it worked.
is 0 a valid value for DateTime? What date is that then?
Changed it in the Keys class to DateTime?.
The stack trace shows the exception happens inside of EF. So it is a case of his data model not matching his database schema (specifically, EF is expecting a non nullable column but the column is nullable).
You can pass the `-r &lt;platform&gt;` switch to `dotnet publish` on the command line, or choose a self-contained publish in the Visual Studio publish configuration dialog. Keep in mind this still has to bundle all the .NET Core libraries that your application uses as well as the runtime, which could total up to 100mb. In addition there is no supported way to combine them all into a single EXE. The EXE will be a renamed dotnet.exe with your project still compiled as a DLL (you can just double click the EXE to run it though).
Good call.
I mean the default value, which in C# can be referenced with the `default` keyword (or `default(DateTime)` for older versions of C# or to remove ambiguity). IIRC for DateTime it is also available as the constant DateTime.MinValue.
It's mainly to cut down on the copy-paste work / development time for myself as I'm doing this part-time, allowing me to set-up a clients database and front end in a matter of hours rather than a few days. What I currently do with other clients websites is basic actions: * Table display (using .NET Datatables, server side) * Create / Update / Delete (making the object invisible) * Creating documents based on data available in the application I've been looking into this for a while now and my main concern is loading times. I'd assume I could cache all the 'Objects' and their 'Fields' and based on that generate a front-end on the fly. The only thing that would then have to be loaded from a database would be the actual data in those fields which seems fairly straightforward to me. Updating the cache in the Objects/Fields would only be done when relogging or something alike to keep the calls to the database to a minimum. The current basic structure I have in my head is the following, but it seems too simple in my mind: **Object (**inherits from BaseObject)**:** * Name (mainly used for menu's etc.) * List of Fields **Field (**inherits from BaseObject): * FieldType (Number, Text, Choice ( ; seperated choices), Boolean, ObjectRelation (Guid of a different object to map a relation)) * Properties * List of Data **Data** (inherits from BaseObject): * HasValue * Value **Properties (**inherits from BaseObject)**:** * Visible * Editable * Formatting * ... **BaseObject:** * Guid * CreatedAt * UpdatedAt * DeletedAt * CreatedBy * UpdatedBy * DeletedBy * ... Another option is using ExpandoObjects but I'm not sure whether those work as I'd expect them to when using them in a database context.
I am also gonna recommend Vue. I believe it is the perfect mix of features/simplicity. I created this template that binds .NET Core and Vue, you can use it as an example or starting point. GitHub: [https://github.com/danijelh/aspnetcore-vue-typescript-template](https://github.com/danijelh/aspnetcore-vue-typescript-template) Medium: [https://medium.com/@danijelhdev/multi-page-net-core-with-vue-js-typescript-vuex-vue-router-bulma-sass-and-webpack-4-efc7de83fea4](https://medium.com/@danijelhdev/multi-page-net-core-with-vue-js-typescript-vuex-vue-router-bulma-sass-and-webpack-4-efc7de83fea4) If you find this logical and makes sense to use then use it. If you're building service oriented product then consider splitting your API from your UI part.
In my experience they‚Äôre all some variation of multiple choice. There are some where there are blanks in a code example and you have to move choices from a small pool into the right spots. Or different examples that include sorting or grouping examples from a set of code samples. But you always have a set of choices - I‚Äôve never had a ‚Äújust write code‚Äù question.
For on premise installation: Grafana + AppMetrics + any metrics storage, for example Prometheus. What is especially good with this setup is that it very popular (except AppMetrics) and can used to monitor other services and host VM/server.
If you have active directory, MVC 5 is super easy to set up and you can just utilize current security groups. Individual accounts can take a bit longer to set up since a lot more work is involved.
What's appealing about the warp-packer app he mentions is that it features a tree-shaker tool that will remove all methods not needed, which reduces the file size by about 80% (in his example).
Whoops. I don't know why, I thought this was a self post asking for how to do it. Somehow I totally missed the link LOL. Sorry.
Generally you would create a model that inherits from IdentityUser and add your new columns to this inheriting model. Your DbContext class would inherit from IdentityContext&lt;CustomUserClass&gt;.
bump. I decompiled it. Have a look if you like
In Azure, I use Application Insights with web tests to hit my health check endpoint.
Unless I'm mistaken, you need to sit 70-486 and EITHER 480 or 483. That's to get the MCSA, you need to do another exam to move up to the MCSD. I did 486 this month and I'm taking 483 in March. I wrote a very short blog post about my experience with 486 at https://blog.squidbox.co.uk
I though emit was for generating IL and Cecil for modifying compiled code. I could be wrong though.
But all it actually seems to do after that is put everything into a self extracting zip file and extract it to a temporary directory when it‚Äôs first run. Still cool, but not ideal for distributing small apps. 
Hi, I stopped taking the quiz near the beginning when you asked about what broke the build. It didn't seem to appreciate how branching models work. I know it's unfair to ask you to explain each question, but at this point I didn't feel you understood the overarching theme of CI. 
euh when a build fails we fix it?
Keep us posted, I'd be very interested to know
Hi, thanks for your interest! Are you referring to the different build failure scenarios? 
I've written some code gen stuff for Unity in the past. It uses the mono compiler in a post build step. I can dig it out and share it with you if you can wait a week or so. As an example - I specify an interface and then generate an implementation based on a known pattern. This could work for your network code. If you only have a few messages to implement I would suggest just hand crafting them. I've wasted way too much time trying to be smart and/or write frameworks instead of game code. 
On a related note: is there any way to add custom transpilation steps to Unitys pipeline? I would like a procedure for my Unity plugin that will rewrite fields and properties with a certain attribute to have custom boilerplate `set` and `get` mechanisms.
I decompiled both versions. They are linked in the original post
99% of the time the build fails on the merge/pull request and then gets fixed on a branch, allowing others to still push code to master. Seems like a non-issue to me?
I'd have to go back through it. Give me a sec: &gt; In which role do you typically get in contact with CI (e.g., as developer, tester, product owner, etc.)? This needs fixing. You should know the roles, it's not a free form field. Next page: &gt; In your experience, for which kind of build failures does it take long to find the relevant information that describes the build failure? You don't understand the domain if you need to ask this. &gt; In your experience, for which kind of build failures does it take long to understand the problem and to plan a fix? Ditto. This is unreasonably broad for a question. How are you planning to process the answers? Beyond that the questions lose relevance. "A dependency?" I don't want to shoot you down too quickly, this sort of research is really good, but your approach is really shitty. Spend more time understanding the domain so you don't have to understand so many free text answers. Who is processing that?
Seriously, it's not rocket science. Is the broken build blocking someone? Yes, go fix it. No? Get done as soon as reasonable.
I'd much prefer to have a AOT compiled to native code single exe. I tried CoreRT some time ago and it worked. Don't know about it now.
this, i really hope they focus on AOT compilation, GO is eating their cake
exactly. we might be doing it a little different, I don't know. We have feature branches that merge back into develop. PR's are made on these feature branches and that's when you spot a broken build. Whatever breaks the build gets fixed, for example a failed unit test or a missing reference. No rocket science indeed.
You're not committing code correctly if one problem with a specific task or feature is delaying deployment of all of your other work. Short of two features/bugs that are dealing with the exact same code, all of your work should be segmented so your master or production branch only ever has code in it that has passed QA and Testing. There are tons of ways to do it, but the way we handle it is each task/feature/change gets it's own branch off of master. Once it's been vetted, it gets merged into master and then deployed. 
Thanks. The try-finally in the IL of the F# looks wierd, I've never seen that before in IL, especially since you did not implement that.
Would you mind sharing what version of Blazor is being used in the book?
Aside from running the executable on a non-Windows platform, can someone kindly ELI5 the advantages of doing this over a .NET framework EXE?
Wait, if your CI is on branches, what happened to the Integration part of continuous integration?
CI on branches deploy snapshots for developers to test their work and the CI (did I forget to include files in source control, in build scripts, etc?). Think short lived instances. Once the pull request is accepted and merged to develop - then the integration occurs and the build goes to a longer lived environment with stable build artifacts.
CoreRT still works. It's glorious. It will be nice when it reaches 1.0
I see your point of having closed questions. However, it is our intention to keep them open so that we do not guide the participant towards a specific set of answers. Our survey is intended to be more a discussion on the problem. To extract insights from the free text answers we will perform [open card sorting](https://www.usability.gov/how-to-and-tools/methods/card-sorting.html).
Yeah, sometimes a simple solution is the best. Keeping the user from submitting twice is fairly easy to do with a little JavaScript.
I already said in another comment the easiest way is just to block submitting twice from the browser side. But this isn't a 100% solution (maybe you want to allow JavaScript-less browsing, or want to allow the user to click again if they are having connectivity problems). As you said using GUIDs, cookies, form fields can be useful. However you are concerned with users clearing these things. Ultimately, remember the user has full control of their browser. If they want to submit twice and make it look like they are completely different requests from completely different PCs, they can. Don't worry about it. Focus on the actual problem: accidental button clicks. A cookie or hidden form field (this is probably the better of the two options) with a GUID will work fine in those circumstances.
Makes sense. Be careful though. It works well at a loose level, but wild card input like this is going to confuse the process. If it's working for you, go for it though :) and if you get the time, blog about how it worked for you :) Honestly it would be really interesting as I'm a bit fixed on this and shouldn't be :)
Yeah this is the approach you want to take. System.Drawing is how you generate or manipulate images. There are other libraries as you found, mainly for cross-platform .NET Core support or whatever (I am looking at SkiaSharp for using it in Unity personally), but .NET has support built-in so if that works for you go for it. The only changes I would make would be to use a Try/Finally block around the code where the font variable is used with Finally disposing the font (this is basically what Using does automatically but since you're changing fonts Using won't work). Also, remove the space after the comma in your data uri, that is not correct. There are also probably better ways to find an exact font size if you're looking for the "best fit". Such as bisecting your search area instead of incrementally searching. But I wouldn't worry about that unless you run into a real situation where the incremental approach is unacceptably slow.
The article is about using warp instead of ILMerge, the point is to get an exe without dlls sitting next to it.
Not supported though. I don't have the smarts to fix it when it goes wrong !
Checkout [https://github.com/VirtoCommerce](https://github.com/VirtoCommerce)
How do I deal with build failures? I get the developer submitting the pr to fix it before I merge it. 
Have a look into Malimbe, a library I created for similar needs (reducing boilerplate): https://github.com/ExtendRealityLtd/Malimbe
So, to be clear, you‚Äôre talking about something more like mini-waterfall with integration only at the end of each feature, rather than CI.
ExpandoObjects won‚Äôt work, I‚Äôve tried it. If you want something similar go for a dictionary with string/object and save it to a json string in the db. It won‚Äôt give you separate columns though. You can put an attribute on it to get same functionality as ExpandoObjects; take a look at the JsonExtensionData attribute for API‚Äôs etc. Json string won‚Äôt give you indexing on the fields in the db. MS SQL has a json column, it doesn‚Äôt give you full indexing either, but it will help. Orchard also did a great job on a data structure that allows fields to be added dynamically. If you go for your own solution, you briefly described, I would recommend you switch to .NET Core and EF Core for performance or use Dapper in old .NET. EF6 is really good and stable, but performance can for sure become an issue. I hope Serenity helps, good luck! 
Can you explain? We branch per story. Dev and complete the work. Developer is required to deploy to a container (done via the build server on a branch of code). When they are satisfied with their work, they submit a pull request that the team reviews and QA does the same container build to verify the end functionality. When QA and the team are satisfied, the pull request is accepted. If other stories are completed before the pull request is completed - developer has to integrate that addition into his branch before the pull request is accepted. I mean, each story in agile is technically mini-waterfall
Cool thank you for the heap of information!! 
Ok, so the point of continuous integration is to mitigate divergence between each developer in the team‚Äôs copy of the code by integrating and testing often - typically at least once a day. This is conceptually close to the idea of trunk-based development in extreme programming. What you‚Äôre talking about is basically an opposite position to that - isolating each developer from the others until the last possible moment, beyond even the QA phase. Basically storing up the pain of integration for another day. It‚Äôs continuous something, but an automated build pipeline does not make it continuous integration.
Developers have to pull develop into their branches whenever new changes are approved in pull requests.
You‚Äôre still fully isolated from other WIP. Cue stacks of pull requests where the first to pass triggers a chain of merge conflicts for those unlucky enough to come after.
WIP is exactly that. It's not finished, and it should be held in isolation from other work. If story 1 gets completed before story 2, then it can be released as soon as it is done. Develop must be maintained in a constantly shippable state. If story 1 is contaminated with changes with story 2, then neither can ship until they are both done.
I just remembered this thread was meant to be about CI, and now it‚Äôs about gitflow and why you think CI is bad instead. That‚Äôs pretty funny. Look up XP some time, anyway.
Most tree shakers don't take into account runtime assembly loading or reflection based type access. There's plenty of that in .NET codebase, so I would be very careful in using this. Fody/Costura will collapse everything into single exe but won't trim the size - unless you're sure libraries you're using are not loading things reflectively I would use Fody instead. 
If it's an HTTP API, and not much in the way of background pollers or something, then NewRelic APM is excellent. It is fairly pricey, from what I can gather.
That's one of my make or breaks for using C# outside of the day job. I need to be able to distribute binaries without the runtime.
I think the ideal scenario would be to have something that, by default minimizes for file size, but has directives that allow you to tell it to ignore minimizing certain sub-sets. For example. Let's say you create a program that allows for 3rd party plug-ins. You could black-list the public API from being shook out, but still allow it to optimize out the true unused code.
Just to be clear the new csproj format still isn't supported with .Net Framework's ASP.NET inc. MVC. Relevant bug: https://github.com/dotnet/project-system/issues/2670 And while people have been able to get it working, it breaks some stuff. I wouldn't really recommend it until the official SDK supports it (assuming that occurs). 
Yeah super strange. Maybe F# is just really hacky in its IL compilation
If I can one day hit publish and get folders for whichever platforms I choose, that would be ideal. If not, I'd be happy enough with the ability to publish to a .exe and not have to worry about which version of .NET the user has. Overall, though, I just want to think about it as little as possible: let me publish and go, and I won't care how it's done, just that it's done
goreleaser is quite amazing. 
Your post has been removed. Self promotion posts are not allowed.
Yeah. Only thing missing. As a more extreme example imagine being able to compile a native exe for the Ps4, or the Switch. 
I've completed 486 in December. I passed with 80% but it was very difficult and there was a lot of studying involved. I previously failed that exam twice when it was covering the older material. I'm currently studying for 480 (the HTML one) which is much, much easier. My test is scheduled for Thursday. MindHub sell bundles that include 30-day access to practice questions and a voucher for the exam, which includes an additional retake exam as well. They've worked well for me. Be aware that 486 (The ASP.NET one) is now covering mostly ASP.NET Core stuff so a lot of the online reading material and practice questions no longer apply. If you're looking to learn, there's no official textbook, your best bet is to simply read Microsoft's documentation.
You are not required to use EF. I believe it's just set up as the default, but you can replace it if you want.
Using it is not required. ASP.NET &lt;= 2.2 will still include the DLLs as a dependency, but 3.0 will fix that.
Postman has a monitoring feature which might work for you. 
More info https://www.infoq.com/news/2019/01/AspNetCore-App-3.0
Not required. ADO.net is still available. There are a few changes but nothing too extreme.
Cool will give it a look. Basically you need to transform your Healthchecks into metrics and then use those metrics in grafana. Do you know if app metrics works with asp.net core 2 built in Healthchecks?
No. Thank goodness. 
Interesting. Have you tried it?
Yea where I work had new relic and they are going away from it üòÇ
Cool that's an interesting way. Will have to look at that but was hoping more for a dashboard. Probably can be done by exporting data and doing a pbi dashboard
This is definitely a bit of self advertising, but if you want to see a fairly significant ASP.Net Core web application with no Entity Framework, check out my project, Fantasy Critic. https://github.com/SteveF92/FantasyCritic All my SQL code is in the FantasyCritic.MySQL folder. I use Dapper and love it.
Thank you very much for posting this. Having ruled dotnet core out, this is exactly the kind of mono walkthrough I've been looking for.
EF Core is great when used correctly.
I took a quick peek. Dapper supports joins; this would cut down on the number of round trips to the database for you.
Any functions in particular you think could be improved?
Sure, in `GetLeagueByID` for instance you do 3 distinct queries, when you could be doing just one by either using a nested select, or ideally using a join to the related tables. The syntax for using Dapper with joins takes a little getting used to, but here's an example: https://dapper-tutorial.net/result-multi-mapping
Dapper is great. 
You're right, I can definitely take it down to at least two by querying for 'LeagueEntity' and 'LeagueYearEntity' at the same time. However, I struggle with the middle query. Right now it's in a separate class that is strictly for user management. If I write code to look at my user table in the main repo class (the one GetLeagueByID is in), do you think that violates some separation of concerns? This is something I seem to run into a lot.
Thanks for the tips. I've always read that Using blocks negate the need to manually dispose during "Finally" because when the code is compiled, the MSIL (or whatever it is called now) actually puts it in a Try/Finally itself.
Side note/unrelated. I think you misspelled a class in the FantasyCritic/FantasyCritic.MySql/Entities directory. You have a class/file named PublisherEnity, I think it is supposed to be PublisherEntity?
Yep, the default \`Console()\` now sink does everything that \`LiterateConsole()\` used to do, and thanks in part to a very long flight (Oslo to Brisbane!) it also supports themes :-) [https://nblumhardt.com/2017/06/ansi-console/](https://nblumhardt.com/2017/06/ansi-console/)
Dapper also supports just straight SQL if you want too
For simple request 
Not true at all. In my experience, EF is actually better for complex scenarios. Performance has been the main reason we have chosen not to use it in certain areas but we always start out with it and switch when it becomes a problem.
What do you tend to use?
Did you try search for alternatives like [https://github.com/mono/ipod-sharp](https://github.com/mono/ipod-sharp).
Ok, you are doing right
No. You are not required to. EF Core seems to be more about tracking changed in entities of certain sets. Dapper works better when executing and mapping the results of queries. I would combine them both in my projects.
Maybe it's not very common, but it has issues with extern alias
Interesting. 
I'll address my experience with them point by point: * DI - I've honestly never found a need for them. The main reason I use DI in traditional application is to decouple classes, but the functions are usually so small that there is almost nothing to decouple. That said, there is an argument for a more complicated Durable Function to use DI in which case you would have to roll your own. Thankfully though, it looks like this is actively being worked on: https://github.com/Azure/azure-functions-host/issues/3736 * Never used EF with functions so I can't comment. * I think there is, at least last time I checked. https://docs.microsoft.com/en-us/azure/azure-functions/functions-openapi-definition * This is somewhat supported, but you need to be using the Azure version: https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-signalr-service I would say that if it's not working for you, and if what you're building is more of a traditional API then I would go ahead and just build an API. I've never found functions overly suited to it, unless you're sticking something like API Management over the top of it.
Never do a sub-select if you can at all help it. Sub-selects often increase runtime by an order of magnitude if not more. I once took an SQL script that was pushing over 2 minutes down to less that 20ms by converting all the sub selects into the appropriate joins. It was a mind-blowing level of improvement.
What's the advantage of using Dapper over SqlCommand?
```&lt;PackageReference Include="Microsoft.AspNetCore.Blazor.Browser" Version="0.5.1" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Blazor.Build" Version="0.5.1" /&gt;``` Is what I found. They recommend taking the latest though.
The whole post was written under a misaphrension - the issues wasnt proxying, it was an extra / in the url, which IIS express didnt mind, but Kestrel did &amp;#x200B; Original post was &amp;#x200B; Hi, I have a project that works fine in production - its an angular dotnet core running on Linux. It has a MVC controller for the data backend, and the angular stuff When running it in the debugger, it runs fine in IIS Express, but I need to hit it from external IP address to test incoming requests from the cloud in the debugger. When I run it under (PRoject Name) instead of IIS Express (i think that brings up Kestrel), it seems to run the angular stuff on port A under ng in a command prompt, and then run another web server (Kestrel???) on port B which proxies requests to port A. The problem is, its proxying ALL requests, including those to the non-angular stuff, ie the requests to the API. This obviously results in a bunch of 404's since the angular side has no clue about those endpoints. Anyone know how to get around this? The other option is to install IIS proper, and debug under that - does that play nice with .net core?
I'm not using ASP.NET Core built-in health checks so don't know if it's supported. Check [App Metrics ASP.NET Core Health](https://www.app-metrics.io/web-monitoring/aspnet-core/health/) documentation.
From a quick scan, looks like a lot of really great info. I'll give it a full read when I have more time. Thanks for posting!
No, there are plenty of other ORMs and Micro ORMs out there, which support .NET Core (often through Net standard), like &lt;shamelessplug&gt;LLBLGen Pro: https://www.llblgen.com&lt;/shamelessplug&gt;. Or one of the (micro) ORMs: Chain, ORMLite, Linq to DB, NPoco, RepoDB (See: https://github.com/FransBouma/RawDataAccessBencher/blob/master/RawBencher/BenchController.cs#L59-L78 )
one more: https://github.com/nreco/data
Do you have https://xyz.azurewebsites.net in the list of replyURLs?
What does the ReplyUrl look like in the redirect to the login.microsoft page? I had a similar issue to this and it was because my ReplyUrl had the http instead of https route for my site which isn't allowed as a valid ReplyUrl outside of localhost
[https://github.com/SteveF92/FantasyCritic/blob/master/FantasyCritic.Lib/Interfaces/IFantasyCriticRepo.cs](https://github.com/SteveF92/FantasyCritic/blob/master/FantasyCritic.Lib/Interfaces/IFantasyCriticRepo.cs) &amp;#x200B; Wow! Good old typical repository pattern :D Maybe some Query pattern (Mediatr) or simillar will be better :)
You should check out [Exceptionless](http://exceptionless.com). It‚Äôs an open source exception monitoring app similar to Elmah and Raygun.
It will map query results to classes for you by name. There's also less boilerplate to setup a query. 
See this thread [](https://community.oracle.com/message/15050608#15050608)
See this [thread](https://community.oracle.com/message/15050608#15050608)
These guys make one: [https://www.devart.com/dotconnect/entityframework.html](https://www.devart.com/dotconnect/entityframework.html). It's not free but not prohibitively expensive either. I've used them back in the EF non-Core days and their support was top notch. &amp;#x200B;
I had never heard of Mediatr until now, but I looked it up and I'm curious. I'm going to consider it. 
There is not a lot of work happening on ELMAH. But it works out of the box and extensions are being developed as external NuGet packages. Questions are answered on StackOverflow and new blog posts seem to pop up from time to time. There is a 2.0 branch of ELMAH, that might be released at some time. ElmahCore (ELMAH for ASP.NET Core) seems very active, though. I wouldn't hold back using ELMAH, but there are some good cloud-based alternatives out there today.
It's ok, but I don't like the inflexible logging options. Performance wise, I'm actually not sure. I'd prefer a provider that takes the raw strings, or better data values and lets me format them. I don't need half the stuff you stringbuild most of the time. Something that looks like obfuscated JSON would be better. "{1: "20", 2: "30"}" We use it in finance a bit and it's easier to read by humans than you'd expect. It would also give us more control over what we log and to where. For debugging I'd also want to know the number of some specific types. I don't know how the memory profilers do it I'm afraid, so no help there. All that seem reasonable? I only went over the code briefly, so I may have made some errors.
This is one of the reasons why I've provided the GcLogBase class: you could derive your own class from it and listen to the same GC event as EtwGcLog ([https://github.com/chrisnas/ClrEvents/blob/master/src/GcLog/EtwGcLog.cs#L70](https://github.com/chrisnas/ClrEvents/blob/master/src/GcLog/EtwGcLog.cs#L70)) to generate the format you prefer. Feel free to read the other articles in the series to get more details about the ETW events you might be interested in monitoring for your debugging scenario. I hope this helps
I can not be the only one, that wishes oracle will disappear, and people quit using it. It is a money suck.
Actually I will be looking at the ETW stuff. Have you looked at App Insights? MS have the ILogger interface now, which is their concept of a logging abstraction, so that's worth looking at. I'd still skip the string builder. Spit it out as an object or struct and let me worry about what I log. Does that make sense? Fuck, I've been playing Portal 2 and typing this feels like I'm looking into a portal out of a 90 degree portal on a wall :/ I had to finish it, so it's now hard wired into my psyche :) 
Some additional points on the section regarding hashing and encryption: # Hashing Hashing is not encryption. Hashing is a one-way function, and multiple inputs *can* (must) hash to the same output. * The list of algorithms includes MD5. ***Do not use*** MD5 for any security-sensitive use. MD5 has been broken for years, and can be cracked trivially. * Desirable hash algorithms produce wide variability in output based on small changes in input. * Hashing is used for passwords specifically because of its one-way nature. When the user logs in, running the password through the same hash algorithm should produce the same hash value if their password is correct, and should not produce the same value if the password is incorrect. In this way, we do not need to * ***Always*** use a salt value with your hash. A salt is a unique value per instance (user, for example), prepended to the password value before hashing. This ensures that, even if Alice and Bob have the same password, their hashed password values do not match. This also reduces the possibility of a collision attack (different input that hashes to the same value) since the attacker does not control the entire input to the hash method. * The salt should not be an "intrinsic" value of the user, such as user id. Why? See next point: * A new salt should be generated every time the password is changed. * If possible, use a strong function such as `bcrypt` to hash passwords. bcrypt has a number of features that make its use desirable: * Supports a range of hashing algorithms: Blowfish, MD5, sha1, sha256, sha512 * Performs configurable, iterative hashing. ("cost") * The idea with this is that slower algorithms make it harder to brute-force passwords using a rainbow table or similar attack. As machines become more powerful, this cost can be raised to cause brute-force attacks to remain impractical. * Salting is automatic and incorporated into the algorithm. * The output includes the algorithm used, the number of iterations, the salt used, and the final hash. # Symmetric Encryption Symmetric encryption is useful when you control both the encryption and decryption endpoints. Because they share a common key to manage both encryption and decryption, this key *must* be kept secret. * Symmetric encryption does not provide a guarantee of identity. * Symmetric encryption is faster than asymmetric. * Symmetric encryption is used in a TLS (SSL) session *after* the handshake. The way this works is that the initial session handshake uses asymmetric encryption to establish communication. For each connection within the session, a large nonce (number used once) is also exchanged and becomes the key for symmetric encryption of traffic exchanged between the two parties *for that connection*. # Asymmetric Encryption Asymmetric encryption uses a public-private key pair. The public key is provided for anyone to use, but the private key is kept... private. Content encrypted with the private key can only be decrypted with the corresponding public key, and vice versa. * Asymmetric encryption is the slowest algorithm. * Asymmetric encryption provides a guarantee of identity. (of the private key holder) * Anyone is allowed to know Alice's public key. * If I want only Alice to read my message, I encrypt it with Alice's public key. Even if Bob sees the message, he cannot read it. Only Alice can read it, because she has the corresponding private key. * If Alice sends me a message, I decrypt it with Alice's public key. I am certain that only Alice could have sent it (not Bob masquerading as Alice) because it is signed with Alice's private key. * Asymmetric encryption is also used for third-party validation. (OAuth, etc.)
Thank you so much
I have tried them before and they are really good, but wanted to know oracle status too before taking a decision for the future
Sure you are not üòâ
Too bad the language feels like the 80s. No reflection, no generics, no versioning. I like it better than c++ but it still feels light years behind C#. 
That would be awesome, though I know that Mono has been ported to the Switch and it supports MonoGame.
Afaik MonoGame uses a transpiler to convert C# code to C++. Don't know at what level, probably directly from IL to C++; Probably they had to rewrite a portion of the base runtime in C++ too.
Interesting, I did not know it worked that way.
Early days of Oracle drivers for EF non-core was a bit buggy. I remember changing table schema in Oracle, and having to remove all tables from EF designer and re-adding them to show updates changes. Ugh, it sucked.
Here's a good read on the subject: https://www.scarydba.com/2016/10/24/sub-query-not-hurt-performance/ The truth is that _it depends_. It's easy to shoot yourself in the foot with it but you don't need to strictly avoid it either.
If, in your domain, a League has a Manager, then including a Manager in a repository method for League is not a violation of separation of concerns. If you were using EF it would do this for you under the hood.
Worked at a company that used Oracle. Was a fucking nightmare to know which package we needed, and to make things worse we couldn't just install shit on our machines ourselves. Had to submit a request, then manager approval, then IT approval, then they would schedule it to be pushed to your machine. Someone identified the wrong version that we needed to try moving from WebForms to MVC (in an ideal manner that is), and once we found the right version IT refused to push it saying they didn't want to waste their time verifying everything again if we couldn't get it right the first time. üòê Had we been on any other major RDBMS shit would have worked with EF or Dapper+Simple.CRUD, no muss no fuss. Fuck Oracle for shitty support, not following standards, and being a shitty company in general for a while now. They're a big reason we opted to use Postgres over MySQL in a side project I'm involved with 
Yeah I don't know the details myself because these tools aren't open source. Aren't free either. 
I never said strictly avoid it. There are some queries where a join just isn‚Äôt possible, or it adds massive complexity to the query. Yes, in those cases use a sub-select. All I am saying is if a join can do the same job, use a join. Especially if you have several levels of nested sub selects across several items that need returning. And IME, joins cover 99+% of all use cases. Sub selects should be exceedingly rare and highly specific.
they are on the right way to repeat this failure again with ef core 
I have that as a replyurl as well and it hasn't worked.
This is the redirect_uri: https://mysite.azurewebsites.net/.auth/login/aad/callback&amp;client_id=a1234 I added it as a reply url and it isn't working.
Banking sector?
No, telecommunications. You've heard of them.
?
We can‚Äôt even get them to give us proper support for products we pay obscene amounts of money for each year, because we aren‚Äôt throwing cash at them to use their cloud products. The plastic faced muppet can go jump off a cliff on one of his private islands...
I agree.
My problem with EF is to debug/solve problems because I don't know what query it is generating to execute on the DB.
And everyone else is trying to understand just wtf you asked.
I just wanted to make sure it was not a requirement when using .net Core. Not a big fan of EF so if it was a requirement I may change my mind to use .net Core.
I _strongly_ agree.
I think you can see what the query is though? https://stackoverflow.com/questions/1412863/how-do-i-view-the-sql-generated-by-the-entity-framework
Yes...?
I think you may have it configuration issue. Can you make sure your appsettings have /signin-oidc for the callback path value. &amp;#x200B; Then in AzureAD App Registrations please double check that you have the following for ReplyUrls: Localhost should be: http://localhost:{yourportnumberhere}/ [http://localhost](http://localhost):{yourportnumberhere}/signin-oidc &amp;#x200B; And then for your Azure site: https://mysite.azurewebsites.net/sign [https://mysite.azurewebsites.net/signin-oidc](https://mysite.azurewebsites.net/signin-oidc) &amp;#x200B; If you go to your site without being logged in yet and you are shown the login page from [login.microsoftonline.com](https://login.microsoftonline.com), the URL should look something like this in your browser: [https://login.microsoftonline.com/{yourAADTenantId}/oauth2/v2.0/authorize?client\_id={yourAppsClientId}&amp;redirect\_uri=https%3A%2F%2Fmysite.azurewebsites.net%2Fsignin-oidc](https://login.microsoftonline.com/09c89fa1-6eaf-4086-a1be-7a28b8e60f35/oauth2/v2.0/authorize?client_id=e481d134-ddce-4e35-a936-0d3144ff4600&amp;redirect_uri=https%3A%2F%2Feasi-web.azurewebsites.net%2Fsignin-oidc&amp;response_type=id_token%20code&amp;scope=openid%20profile%20https%3A%2F%2Fmyq7cdc.onmicrosoft.com%2Fd5b06e8e-4bf1-4750-b36c-a544d25932f2%2Fuser_impersonation&amp;response_mode=form_post&amp;nonce=636856002778854543.NTc4YzA1YzEtZDE4NC00NTdhLWI4OTctMjA4ZTlkOGQxMjQyYmZkYjA3ZWMtMmNkOC00MDkyLThmMDgtZjFiNGU3OTQ4ZDNh&amp;state=CfDJ8CMfDmTYStFMuKkN5YmgX-3PPXzpWVcG7a2IfzNKkevpZXGUKQWc0NVevmxLGPnArkdlrysqhvMdXy1bl1K8O1BS5ubSKqn9RCsvQFuOr04EkN3gDFFH4I1H7aGrQo_m30n2lfcxlDgLb1WtEkoCnNI-ZYWmCYG7BNaFUFq4IUbQkxnD-7nOOB-AuftNqEhqTZzKhJX1vTFZAMqxmWBerihFBz3yALfaVmKBt0bEgHfrjzwYtNabCUqlLAtk4Hzg2laJM8JYVC7bBW6F1VlVZAjhClHb6XhSLSw-NFFBFivrEtCNR6i_z2NSzufhYyDBnaXS5_KJwdwGhiuWoVIl4VKHzKDGdmh-nOR3igXHgUpa&amp;x-client-SKU=ID_NETSTANDARD2_0&amp;x-client-ver=5.3.0.0) Make sure that the redirect\_uri matches your sites host name, protocol, and call back path in this case /signin-oidc 
&gt; Never do a sub-select if you can at all help it. This sounds like a strict avoidance to me, and I generally take issue with this sort of statement without context because it might lead someone to make choices that are contrary to their best interests based on the cargo cult. That being said, I agree with your followup and generally wouldn't consider using one unless the enclosing query has a low row count and the query isn't run often. And if I'm being lazy.
r/ihadastroke
&gt; if you can at all help it That‚Äôs the modifier that flips the statement away from strict avoidance. Strict avoidance, like a pure vacuum in Nature, is something generally abhorred. Absolutes rarely work well.
Well you must try vb.9 it's even better! 
Absolutely 
And here I was thinking I was being more efficient using struts over classes (where possible) all these years. 
I'm going to take a stab at translating this just for the fun of it. I make no warranty or claim to the accuracy: "I'm trying to understand why VB 6 and Visual FoxPro are faster than any .NET language when accessing an SQL server." Am I close, OP?
[https://www.smith-consulting.com/Products/Hotel-Booking-Software](https://www.smith-consulting.com/Products/Hotel-Booking-Software)
https://fullcalendar.io/ - JavaScript based so server language agnostic (unless you are not talking about a web application)
Found this one at the same time as you posting it here. I think we will go with this one. 
The quick brown fox jumps over the lazy dog
This is Shakespeare.
You are correct. I meant that you can see the generated query but it is not something you can just open the code and see it right away.
"In early 2000s, why did Microsoft **SELECT** the .NET Language **RUN**time to replace efforts in VB6 and FoxPro" I know this isn't what he meant, but its more funny to make puns on 'SQL Process' and 'qickly' because I have no idea 'whut the fox say'.
If I add up all of the numbers...it would be the net amount?
VB supports smart quotes? Damn, I know far less than I thought.
My very brief experience with Oracle was similar. Maybe I'm naive, but I can't imagine what kind of data Oracle could handle that SQL Server or Postgres can't. Using Oracle is a nightmare so unless you meet whatever that criteria is, avoid avoid avoid.
You joined the two tables in the GetFurnitureByHouseId() function but never used that info there.. You need to make a GetFurnitureModelByHouseId() method instead and move the construction of the FurnitureModel type into that function. Something like this: `public IEnumerable&lt;FurnitureModel&gt; GetFurnitureModelByHouseId(int houseId)` `{` `return` `from furn in _ctx.Furniture` `where furn.HouseId == houseId` `join furnImage in _ctx.FurnitureImages on furn.FurnitureId equals furnImage.FurnitureId into imageGroup` `select new FurnitureModel` `{` `Id = furn.FurnitureId,` `...` `FurnitureImages = imageGroup.ToList()` `};` `}` &amp;#x200B;
At one point they were good, and I believe their support on the Java end of things is better (I could be very wrong about that). The place I was with chose them years ago and it's at a point where migrating isn't really an option given the amount of data in their system. It would be an absolutely monumental overhaul of their system.
`using` is now styled similarly to the `use` keyword in F#. I‚Äôd be OK with C# just becoming F#.
Class will indeed automatically reorder fields to use the most efficient memory layout but don't forget that class instance will also use 8-12B of reference-keeping overhead (double that on 64-bit OS). If you want to pack a tight data array in the most efficient way, use structs - you also gain the ability to use them in a way that doesn't touch the heap. 
r/rareinsults 
[removed]
I wouldn't say classes and structs in this case are about efficiency. There's also the header that takes some space, the reference itself adds overhead, heap vs stack allocation, etc.
If you use [managed driver](https://www.nuget.org/packages/Oracle.ManagedDataAccess/) you don't need to install anything but I guess it wasn't available when you used to work with Oracle
Well he doesn't really paint VB as bad, just that it's syntax and flow is more different from C# than a lot of people claim
What's problem with my question bro?
That's exactly what I asked. 
There is book ["Reactive Messaging Patterns with the Actor Model"](https://www.amazon.com/Reactive-Messaging-Patterns-Actor-Model/dp/0133846830), also just to share nice general [talk](https://www.youtube.com/watch?v=GgM8pp2et0A) by Vaughn Vernon about Actor model. And follow up to OP question: "*Can anyone recommend some simple library to implement DDD events?*" AKKA .net or Orleans are very powerful but seem like overkill for this purpose... 
[https://github.com/jbogard/MediatR](https://github.com/jbogard/MediatR) for DDD Event.
Did you see this great NDC talk: [https://www.youtube.com/watch?v=7OVU9Mqqzgs](https://www.youtube.com/watch?v=7OVU9Mqqzgs) ?
I have this book, but it about the actor model, I need more practice example
Thanks, very much. I will try to look at this video. I have a problem with the English language, therefore, I am searching primarily text resources than video.
If you don't want to push your settings wny not just use [User Secrets](https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2&amp;tabs=windows) which is made exactly for this purpose?
Thanks for the post. This is not Hot Reloading. This is Live Reloading. Hot reloading is when the app state is preserved and only the required parts of the UI change, without refreshing the whole site. You are simply refreshing the page. I would argue that Hot Reloading in Razor Pages is not possible at all, since it is server-side rendered.
Wow, thanks for explaining the differences! I always assumed they are one and the same. And yeah, hot reload with changing the UI, but keeping the state might be near impossible without internal engine rework. Maybe with Blazor/Razor components that becomes possible.
It is still more memory efficient to go with structs if you know how to align the fields correctly, as you don't experience the object header/vtableptr overhead of an object. The article probably should have explicitly stated that in order to avoid confusion.
While the method of having multiple config files per environment is great. Storing secrets in them is asking for trouble... Something like this: [https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2&amp;tabs=windows](https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2&amp;tabs=windows) Would be better for storing the secret
Agreed, locally should be using secrets manager. Is incredibly easy to use.
A poor man‚Äôs live reload could simply make another request to the page, parse the HTML and replace specific sections of the page that need to be updated. A better version could request the body section of the page rendered with an empty Layout. An even better version would only request data it needed, probably without the HTML, and inject that into the page. The browser would say ‚ÄúGive me just the labels and price data, then I‚Äôll combine that with an HTML template to update what is on the page.‚Äù
Serilog + Seq Really happy with that combination. Seq‚Äôs app framework makes it easy to integrate with other applications like Slack, Teams etc
Totally agree. The post shows what I believe is the easiest approach to understand, when coming from a classic ASP.NET world. In the next post in the series, I will write about user secrets, which is a better solution in most cases. Thank you for the feedback. Appreciate it!
As a developer who has built and subsequently abandoned an actor model library, I would like to point out that you're on the _fringes_ of .NET at this point when looking at using the Actor model. Seriously, the less than one percent. So anything you find is going to be at that same level, the edge, and often tied to a particular use case. Finding other people using the same library will be a challenge. That said, good luck to you. Orleans was made to solve a specific problem (pretty sure it was Halo, or something something XBOX specific, but I can't remember exactly), and as such it has a very specific design and the tooling around it to "make it work" with C#. Lastly, realize that by moving into that less than 1% of C# developers, you're going to have one hell of a time finding engineers who can figure out what you've done after you leave the project - leaving your employer a bit miffed at you for choosing such an approach, compared to a traditional development architecture.
Thank you thank you. I wasn't making that proper association to the FurnitureModel. Seems so simple now that someone else points it out, thank you again!
hey! i was just at the LEAP conference and saw John Azariah speak there. His topic there was quantum computing. Cool dude, definitely giving this a watch.
Thanks, looks like exactly what I need! It is also used for CQRS implementation! But I did not get how MediatR does this. MediatR `IAsyncRequestHandler` specifies return type, does not it violate command query separation? As I got it commands should only change state. 
[Akka.Net](https://getakka.net) uses the actor pattern too. I used it at a job extensively. Here's their documentation: [What is an actor?](https://getakka.net/articles/concepts/actors.html) Also, Service Fabric has a type of stateful service employing the actor pattern. Their [Introduction](https://docs.microsoft.com/en-us/azure/service-fabric/service-fabric-reliable-actors-introduction) might help you. Orleans, as others have mentioned, is a great framework for this pattern as well. When I used [Akka.Net](https://Akka.Net), we were between the two and ultimately chose [Akka.Net](https://Akka.Net). 
I am not too familiar with how PowerShell modules work. But the error calls out Oracle.ManagedDataAccess which is NOT one of the packages you installed. I would recommend you verify the assembly is contained within the package. If it is installed the issue is likely that you need to tell PowerShell where the assembly is located so it can load it (there is no GAC in Core IIRC). I don't know how that would work.
Please modify the post to state there is another approach as well.
It is already mentioned in the last part. And when the next post is published, that part will be updated as well.
Right, great! üëç
This cool, my only frustration will be remembering ^ is indexing from the end but represents line start in regex lol
I prefer having my secrets in the development and production versions of my appsettings files and not commiting those to source control.
yeah, can be confusing initially :)
I'm currently using razor pages and using modals for edit/create/update. The modal content is a form that is fetched from as a partial view that is returned by a page handler. Request is done through AJAX so its definitely possible since partial pages ignore layout as you mentioned.
Very short for an "ultimate guide", and also wrong at some places, e.g. &gt; Asynchronous code in .NET uses only three return types: And async&amp;await / Task is not the only way to do asynchronous programming.
Good article, it's always helpful to remember that multithreading != asynchrony! I had been playing around with some async stuff the other day and I'll readily admit I am a newbie this stuff in C#. I had fired off a request to a web site to get an access token so I could try playing with their API, and was trying to figure out why I was never getting responses with my access token back, since I knew my credentials were valid testing it out on their documentation. It turns out my application was ending before the response was ever received, due to using async stuff and not properly awaiting for the response. Based off your metaphor, it sounds like my chef in the async kitchen put in the customers order in the oven, realized he had no other work and clocked out for the day, instead of waiting around for it to be done since he had nothing else to do.
This is good too and more in depth [https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md](https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md)
You were. A class is always on heap so there's time spent for that and heap management has its space overhead.
This. 
good catch. lots of us used (or are still using) BeginFoo/EndFoo patterns. and let's not even start talking about async-over-sync and sync-over-async. 
check out the code at [http://www.cqrs.nu](http://www.cqrs.nu) , it contains a complete CQRS solution, with event-distribution, testing and write/read-side. It's all open-source and written in C#.
Nice to hear, please create issues in the EventFlow repo if you have any problems. The same goes for any "WTF moments".
I will, no doubt. My main focus right now is multi tenancy. Ideally I want the tenancy to determine the actual data store used to persist events, and some mechanism preventing cross access even in case of programming error. But, the more we debate it and try things out, the more I‚Äôm leaning towards streamlining installation and upgrades instead, so every tenant have their own set of sites and services.
I would suggest you can tweak your model to make things even better. There is no need to explicitly join tables in queries or make foreign keys (at least for simple cases) EF can do it for you. First step would be to include a Furniture field in the FurnitureImages class. EF will automatically set up a foreign key to handle the association the same as if you had done so by hand (you can still keep FurnitureId field; EF is smart enough to realize that is the same reference). Then I think since you want to go the other direction (take a Furniture and get all the images) you can put a ICollection&lt;FurnitureImage&gt; field on Furniture and EF will automatically populate it if you have an include clause on your query that calls out the field (I don't know how that particular syntax works. I use function chaining which does it via .Include()). I forget the exact details, you should check MSDN on the exact syntax it wants for the field. You could also use lazy loading which removes the need to even do any sort of join or include in the query altogether, as extra queries will be issued as needed in the background when you try and access the member holding the image list. Though that's probably not the best performance-wise. Lots of cool improvements you could look into. Maybe for now, maybe for future projects to make things a little easier.
Just to be sure I follow, you are going for a more "micro service" architecture instead of a monolith with a rather complex set of configuration options. If so, then I totally agree, just for the being able to do a rolling upgrade instead of a big bang when you wan't to update your system.
That's a great list of tips! And I love the concise format with examples.
The system will have multiple services of various sizes. For on-prem or own cloud customers that fine. But for saas customers they have to be efficiently handled for installation and upgrade, and we need to ensure some degree of underlying mechanism separating the customers access to data. Ideally a user gets a token/key on login, that unlocks their data. But I can‚Äôt encrypt things because that would make things rather complicated when write and read sides are separated and you might need to replay events.
ehhh 
&gt;Per your feedback, we have moved the solution name information from the status bar control back to the top of the IDE. It still looks like shit and doesn't stand out nearly as much as it used to.
Getting it working for now is the goal I've set for myself. Refactor when I have a better understanding of everything. I have seen what you had mentioned about letting EF generate those foreign keys, and I believe I have actually implemented that but I fear I don't fully grasp it. I'm far more comfortable with relational keys because I think it juts makes more sense in my head
Using MediatR with your understanding of CQS the mapping is the following: Query -&gt; `IRequest&lt;T&gt;` Command -&gt; `IRequest` (being that `IRequest` is actually `IRequest&lt;`[`Unit`](https://github.com/jbogard/MediatR/blob/master/src/MediatR/Unit.cs)`&gt;`) But don't be holistic with the "*commands should only change state* ", for example, ask yourself, when creating an entity (which is a mutation of state) is it inconceivable that we receive the ID of the newly created entity? On [this comment](https://www.reddit.com/r/dotnet/comments/a3qig2/mediatr_tutorials_exist/eb8ykun) I provide a presentation with a sbu-topic called "*gray areas*", which presents multiple scenarios to either strongly adhere with the CQS principles, or just be a little bit more pragmatic in the solutions. By personal preference I tend to code with a more pragmatic approach, as long as it conveys simplicity, readability and maintainability, instead of being holistic to concepts just for the sake of it. 
Fantastic 
Please correct me if I'm wrong but I believe that SynchronizationContext doesn't exist in .NET Core but I guess he's talking about .NET framework.
The SynchronizationContext exists in .NET Core. And my comment stands regardless whether he talks about .NET or .NET Core.
Haha I enjoy that extension of the analogy 
Most of it is just personal preference. Personally I like how the type-name is on the right of variable name in VB. Without syntax coloring from an IDE, it's hard to spot the variable name in C# inside a big list of declarations. The self-matching of block enders is also nicer in VB. With C# the square brackets are easier for your eyes and the IDE to mix up because they are all the same. VB gives the compiler more clues about nesting intent, making for better localization of problem flagging. Wrapping a line is easier in C#, but arguably too much wrapping is a sign of poor coding. Most statements should be no more than about 100 characters. 
 ng-swagger-gen is an awesome npm package for generating Angular http clients for Swagger apis. Eg: add swashbuckle to your asp.net core webApi project, run ng-swagger-gen, and your client side calls are all taken care of!
Orleans was not made specifically for Halo or any other specific application, it's a general purpose framework.
‚ÄúIt has been used heavily by a number of high-scale cloud services at Microsoft, starting with cloud services for the Halo franchise running in production in Microsoft Azure since 2011. The core Orleans technology was transferred to 343 Industries (https://www.halowaypoint.com/) and made available as open source in January 2015.‚Äù https://www.microsoft.com/en-us/research/project/orleans-virtual-actors/ 
Yes, Halo uses Orleans but it was not made for Halo or any other specific application. I should have mentioned above that I am on the Orleans team.
Actually, it doesn‚Äôt! https://blog.stephencleary.com/2017/03/aspnetcore-synchronization-context.html
That was my thought too. Not sure why they didn't use negative (-) instead?
The SynchronizationContext exists in .NET Core. You can use it. Just that the ASP.NET Core framework does not set one (unlike the old ASP.NET one). Nothing prevents you from setting one. It will also be used in the WinForms / WPF applications (.NET Core 3).
We've started using SQL+ and it is almost a combination of them both. You write a stored procedure and it generates an entire class library at the click of a button. It is such a nice tool I can't imagine doing a project without it. I imagine it will make the other technologies obsolete. &amp;#x200B; The site is here www.SqlPlus.net
What is wrong with this approach? How is it different than using secrets manager?
The not committing part is what gives me cause to pause. 
You don't commit secrets manager either. What's the problem?
Anyone watch it yet? Anything worth mentioning?
Just in case anyone doesn't already know, you can replace this: ``` public static void Main(string[] args) =&gt; MainAsync().GetAwaiter().GetResult(); static async Task MainAsync() ``` With this: ``` public static async Task Main(string[] args) ``` If you can use C# 7.2. Much cleaner!
They showed a setup where the page is rendered on the server and sent down first, while the real blazor app loads, to make it appear to load almost instantly. Also showed a proof of concept of AOT compiling your project to WASM, which made it run similar to native speed. And also showed a proof of concept of debugging in Visual Studio. He also showed a blazor app running in electron, but I'm not sure if that's new or not. Seems like you could probably do that now. 
More like an introduction to asynchronous programming in C#. Sometimes humility doesn't hurt.
And if it doesn't work, make sure to add the following in your property group of your csproj: ``` &lt;LangVersion&gt;latest&lt;/LangVersion&gt; ``` [Source](https://docs.microsoft.com/dotnet/csharp/language-reference/configure-language-version?WT.mc_id=social-reddit-marouill)
Thanks for sharing!
Ah yes, C# 7.2 must be enabled too!
You right. However, I want to use Actors for my research project - IoT for my university campus. 
I really hope this works out
I really hope this all works out
If you're generating clients for javascript, I'd recommend using other more modern tools: The Swagger team has Swagger-Codegen: [https://swagger.io/tools/swagger-codegen/](https://swagger.io/tools/swagger-codegen/) And that project has also been forked into OpenAPI Generator: [https://openapi-generator.tech/](https://openapi-generator.tech/) These both generate far better Typescript/JS code if you're creating frontend clients to access backend APIs.
People from r/brandnewsentece would be pleased with that title :D
Pretty excited for AOT. It runs just fast enough for my purposes interpreted, but it would be really nice to just be effectively instantaneous.
Couple of questions: - Are you creating a mobile app or a web app? - What technology are you using to authenticate and interact with this external service? Web API, something similar? 
I watched it live. Basically if you've seen Steve's general Blazor talk before and you've been following it, not that much has changed. He shows the basics and then runs the fibonacci in C in his browser as per. But he does go in to some additional detail within the last 20 mins or so of things such as debugging that you may have not seen before and goes in to a bit of detail regarding server Blazor vs client Blazor and addresses issues regarding performance.
I‚Äôve been aware of Blazor for a while, but that was the first real ‚Äúdive‚Äù into that I‚Äôve seen. Pretty exciting stuff. The big question I have (and hope someone has an answer) is, ‚ÄúIs it ready for production, yet?‚Äù
Unfortunately, it's still mostly experimental. Not even an alpha or a beta.
Razor Components (the component based way to construct views) will be part of the upcoming [ASP.NET](https://ASP.NET) Core 3.0. That alone is a major upgrade to creating frontend views in a more modern approach similar to React. Blazor with it's WebAssembly powered client interactivity does not have a release date yet but is developing rapidly and will come after.
WPF and UWP. I have not seen a UI framework that's as pleasing to work with as WPF is. When it comes to making apps without any bells and whistles. Once you want to add some animations or styling... Oh boy...
&gt; lamented the complexity of frameworks like specflow Wut?! 
I would strongly encourage you not to do this. If you should be authenticating with another service then that service should be providing you with a way of authenticating. Think Facebook, google, etc that provide an authentication mechanism. You would then be given a token from the service which you can use in your requests to perform updates etc. This would absolutely be the correct way to do this. Storing usernames and passwords for other services opens you up to all sorts of liability, eg GDPR. &amp;#x200B; if you absolutely have to do what you have stated then i would use .net cores identity [Password Hasher](https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.identity.passwordhasher-1?view=aspnetcore-2.2). Its built in and its whats used on a new scaffolded project with authentication enabled. This might get you started - [https://andrewlock.net/exploring-the-asp-net-core-identity-passwordhasher/](https://andrewlock.net/exploring-the-asp-net-core-identity-passwordhasher/) But please, think about your obligations as a programmer to protect peoples data. Are you in a position where you could approach the developers of the 'external app' to build you an authentication mechanism that prevents you from storing any data. (Do they already have one?) &amp;#x200B;
Maybe something with seed? Try seeing [Cryptographic hash algorithms](https://en.wikipedia.org/wiki/Cryptographic_hash_function#Cryptographic_hash_algorithms) or use [Bcrypt](https://en.wikipedia.org/wiki/Bcrypt) \- there is also [repository](https://github.com/BcryptNet) for .NET ( :)
This. Just to add, though, any token based authentication that the other parties provide needs to be rotating. There's little added benefit in using a username/password combination to get a token, only for that token to be passed in the Authorisation header and stolen, or decrypted from your database. The best practice is for an OAuth token type flow, where the caller (your system) receives a short lived access_token (1h), and a longer lived refresh_token (90d). When the avcess_token expires, the refresh_token, along with a private client_secret (known only to your app and the provider) is sent, and a new token generated (usually resetting the expiry on the refresh_token).
this is really cool!!
What specifically are you trying to do? What version of SSRS are you running? You're a little light on details. 
NSwag also has this support now as well, for the UI too rather than using Swashbuckle 
to not be like Python exactly :v :v 
Password hashing is one-way. If OP stored a hash he wouldn't be able to un-hash it to authenticate against the external service. But like you said, don't store passwords in the first place. Use OAuth and store the token. 
for .NET documentation, I always use sandcastle. There is also a standalone GUI version of sandcastle on Github [https://github.com/EWSoftware/SHFB](https://github.com/EWSoftware/SHFB)
Yes of course. Doh! Haha, thanks. However, OP shouldn't ever be storing a password with 2 way encoding/decoding. So, i think we all agree. Don't do it.
I think that's why you got downvoted, because your solution doesn't actually solve OP's problem.
As others have said, you should not store passwords if you can absolutely avoid it. That being said, I'm going to assume for your use-case that you cannot avoid this. Take everything here said under the guise of "You should probably not do this, but if you must....". Encryption is [_fairly_ straightforward](https://www.codeproject.com/Articles/769741/Csharp-AES-bits-Encryption-Library-with-Salt), the thing you need to do in order to do this "properly" is to salt the key you use so that nobody can brute force your encryption key easily. The process is simple enough: Encryption keys for something like AES are of a fixed size and supplied as bytes, but rather than using the raw bytes of a string or hardcoding those bytes, you want to use an initial string to *derive* the encryption key bytes (Using something like the Rfc2898DeriveBytes method). This way you can use strings of arbitrary lengths and complexity. Secondly, you don't want to use the same _raw_ encryption string/key for every single password. You want to _salt_ the input key with additional random data so that the _derived_ key is unique for every single instance. This means if you encrypt the same password twice, you'll get two different results - but both will still reverse to the same value. Think about your database - if you had 1,000 passwords and 10 of them were identical, an attacker might be able to assume that the original password is "password" and from that they can work backwords to figure out your encryption key - meaning your entire database is compromised (though for the record, if you're storing passwords - even encrypted - and that database leaks, you can assume it's been compromised regardless). That's what salting is for and that's why it's extremely useful - it's perfectly acceptable to store the salt alongside the encrypted data. The final, most important thing you need to consider is your actual encryption key/string - this is what you _need_ to protect and keep safe. Do *not* store it in the database. Do *not* store it in the app. Store it somewhere super safe - such as a keyvault. But now think about who and what has access to that keyvault. Consider your app - the thing that's going to consume the decrypted key - if it can talk to the database directly and has access to the encryption key, then your efforts to protect those passwords are utterly irrelevant as anyone who can dump your database can almost certainly pull the encryption key from the app as well. You'll want to consider a mechanism for your application to request a decrypted key from an intermediate service - that service should not be accessible anywhere and it *must* authenticate your app and it *must* authenticate the user to ensure they only have access to the credentials they've supplied. As you can see, this is _not_ trivial at all if you want to do it right.
&gt; var fields = GetType().GetFields(BindingFlags.NonPublic | BindingFlags.Instance).ToLookup(x =&gt; x.FieldType); &gt; fields[typeof(given)].ToList().ForEach(x =&gt; ((given)x.GetValue(this)).Invoke(context)); &gt; fields[typeof(when)].ToList().ForEach(x =&gt; ((when)x.GetValue(this)).Invoke(context)); &gt; fields[typeof(then)].ToList().ForEach(x =&gt; ((then)x.GetValue(this)).Invoke(context)); Wow so easy and simple. *BARF*
You can schedule reports to be delivered from the standard SSRS UI... The only caveat I think is that if you need data-driven subscriptions (i.e. dynamic list of recipients) you have to have enterprise version.
I mean doing this really depends on your situation, like: - What database are you using? I know SQLServer supports encrypted columns, though I don't know much about them. - Is this running on a service, or some end-user application that's using an API to get these credentials? - Obviously storing usernames and their passwords in a reversible way is definitely not ideal. Like others have said, is there any way around this? Some kind of SSO like oauth? Doing this has a lot of considerations, like where you're storing the passwords, what is responsible for decrypting/encrypting the passwords, where the keys are stored for this process. To preface this: **This is not by any means intended to be a comprehensive guide on how to do this, or what to use.** My point with this is to illustrate some of the considerations there are when dealing with encryption, and to really push the point that you should make sure this is your last resort (ie: You can't do SSO.) I'm by far not an expert in cryptography, and you should ensure that you do your own research, and read about and understand all of this before you try to implement it. If I absolutely had to do this, I would probably derive a key for each user, probably using their own password for your service with something like PBKDF#2, or something like Argon2 or scrypt which are designed to add both a CPU and memory cost (where PBKDF#2 only adds CPU cost.) The use of these would depend on your use-case, hardware, and users. Obviously if you have many users hitting this service, it's going to scale the load on your systems. This would get you a key to use for the algo of your choice. I would encrypt both the username **and** password of the third party application. These combined would require user input for your system to be able to access the third-party system on behalf of the user, and make it so if an attacker were to breach your database they would have many uniquely encrypted credentials to crack instead of a bunch of credentials encrypted with a single master-key (**do not reuse encryption keys or nonce/IVs for this!**) This way you leak nothing about your users third-party account (not even the username.) These operations would be performed in memory and not cached. User is on your service, inputs his password to authorize your service to use the third-party service, your service derives the key and decrypts the third-party service credentials, uses them, then discards them from memory. If your user changes their password on your service, you would obviously need to re-encrypt their credentials. For encryption you'd have to decide on an algo, and if you want to authenticate the encrypted data using a MAC. IE: AES+HMAC-SHA256, CHACHA20+Poly1305. This would ensure the encrypted data has not been tampered with. These all have their own considerations that you should research.
lol. I get where you're coming from. What's interesting about this comment is when I was talking about the complexity of specflow I was talking about actually using it, not the internals of it. I guarantee if you look at the internals of any testing framework code much more complex and ugly than this exists. 
Why do you care about the internals?
I think it's probably very much just my opinion, but just the getting started page is more complex than I'm interested in. https://specflow.org/getting-started/
very useful, thanks!
There are a few tools for Swagger as well: springfox, swagger2markup-cli, and swagger-spec-to-pdf npm package. 
Thanks! We have CU113.0.4411.0 [KB3208177 ](https://support.microsoft.com/en-us/help/3208177) January 18, 2017 SQL Server 2016 Service Pack 1 (SP1) And apparently it's not enough for the users. Plus we have customized delivery schedule (our pay periods have a specific schedule, we have reports that needs to be delivered within our pay period schedule -- it's not built into SSRS).
We don't have dynamic list of recipients but we need customized delivery schedule that fits within our pay period schedule (it doesn't adhere to any pattern except that it goes by our customized schedule that's in our db).
That is subjective, I don't consider it complex at all. It's a VS extension and then some screenshots to show the next steps. Do you expect your non-tech savvy types to read your xUnit test cases? I'd argue it's wholly less readable, despite it being the main point of your concern, for people not familiar with code.
Why don't you just schedule the reports to save to pdf every day, then simply run an agent job or windows scheduled task for a simple console application that emails them out. You don't need to rebuild the scheduling logic again for this simple use case
The lack of dependency injection out of the box in both v1 and 2 is so ridiculous. 
You can output whatever you want, the SHFB api is extensible for any type you can come up with. IIRC you can do a couple HTML versions, CHM and PDFs by default. 
I use Swashbuckle to auto generate swagger.json files. Doesn't go to pdf though.
NSwag produces poor bloated JS code. That's why I recommend the other tools that are more modern and used by a bigger community.
Huh. I always saw VB.NET as a vestigial language, but now I can kinda see why it's still around. Neat!
As others have said - this is a bad idea. If you do have to do this then I have an alternative suggestion. &gt; Have a username and password to your service. The user's password will unlock their "password safe". You can do this by allowing your user to authenticate with your service, per normal, with a salted hash, but at the same time use another key to hash the password and store that hash in the claims on their identity. From herein, you can use that hash to encrypt and decrypt the users information. The beauty about this is that you need both elements (your key and their password) to decrypt the data.
What about error handling? Do i have to wrap the client calls into trys and handle all possible network/http errors myself? What if the API has an alternate return type for non 200 responses?
I found them but don't know how to use them as they focus of java but i'm using .net core
I'm like you so wait the answer with me then
Swagger2markup is Javascript based. 
Just as important: **.NET CORE 3.0**
Woot. The VS team has been killing it for the last 5-10 years.
They should just scrap the whole year thing. "2019" has so few changes it could easily just be a version 16 and have it auto-update on top of version 15/"2017." They should consider Resharper's business model, and sell people subs with a perpetual fallback license for a specific version if they cancel. As an aside: Anyone else in the .Net Core MVC space looking at dropping Visual Studio for something more performant? I've been playing with VS Code. It just has a handful of missing things then I'm all in. It isn't even about money. Either way we'd pay for MSDN. It is about how seemingly big and lethargic Visual Studio has become as a piece of software. Even with no extensions/safe mode I'm seeing CPU ramping, delays on intellisense, characters buffering, and all around jankiness. Plus as soon as you add even Microsoft's own extensions (e.g. Performance Power Tools, IntelliCode, etc) performance goes down in flames, third party extensions just forget about it... I get that Visual Studio's guts are like 20+ years old at this point, and that's impressive in a way. But at some point they have to take stock and improve things, otherwise it is an inherently doomed product. 
Oooo, official Blazor support! Now known as Razor Components it seems That's neat!
What the future holds for those in the ecossystem of this subreddit is very promising. I wonder if there's anyone out there still blaming MS for everything. Speaking for myself, I'm not even using M$ anymore. lol
Can the start screen with the project list get a search? 
The only half-grudge I still hold against MS is their CAL licensing model which is a poor fit for small and mid-size businesses. 
Nah, that's still ~6 months out. "Fall" is what they're saying.
Only server side though from what I can see.
I switched over to Rider professionally a while back. At first it was just for the performance but now I genuinely prefer it to Visual Studio. It feels sad seeing as I've used VS for so long, but at the same time hopefully it'll push Microsoft to finally sort out the performance problems in VS.
Umm.. VS 2015 was delayed half a year with never ending previews (due to the broken installer and overhauled solution engine), and the end product was a performance mess. 2017's focus was "don't screw it up" and it was a very very conservative release. They were even hesitant to announce the date until just before release. TBH 2013 was the last "solid" release, having a balance of features and stability (after 2012's UI engine fiasco, thanks to stephen sinovsky's UI influence into products that had no good reason to be there) The fact they're targeting more than a month out sounds like a financial reason, or they finally turned the boat.
You mention VS being slow and you also mention having Resharper installed. I find the two to be linked. Resharper runs terribly on my dev machine, I am seriously considering not needing it after 2019 drops.
Price of SQL enterprise is my main gripe.
All of these guides and help documents tiptoe around the glaring deficiency in c# async: async constructors. We all know it's "async all the way down"..... except it can't be. 
I haven't seen these issues with Visual Studio, when I am dealing with a monolithic project or with someone who has loaded a lot of heavy extensions. The issue there is that VS is trying to run a lot of different analytics to help with development tasks. If the solution is huge, that can result in a LOT of processing. You can actually disable many of these features when dealing with projects that are too big that where analysis hurts more than it helps. VS Code isn't doing a lot of this stuff (and if you don't care about those features... VS Code may be a great fit). Side Note: Ryder may be an alternative that you could try, but I have heard mixed reviews on performance.
2015 did feel rough. 2017 has been smooth for me so far.
Negative - Core 3.0 is still 6 months out or so, and [Blazor](https://msdnshared.blob.core.windows.net/media/2019/01/blazor.png) will be post-3.0. We'll be getting Razor Components, but there's still a long way to go with Blazor (ie the actual Browser+Razor rendering whatnot) itself.
Rider is way better than VS. 
I also mentioned VS being slow running in safe mode with no extensions. 
What's bonkers is that even the $15K/2 core they charge is relatively inexpensive compared to the companies they replaced (Oracle, IBM, etc). Not saying I disagree with you, it is expensive, just funny how far things have come. Many companies are still giving Oracle $100K or more just in licenses. 
Why not April 1st... oh. Seriously, VS/. NET Team, thank you. Your goodies make many many devs' lives better and Blazor promises to rid my world of Javascript for good.
As my old maths prof would say "It is trivial. It is easy."
Source?
i saw it on twitter, I can't confirm. but there's alot of jubilation about 2019
Pushing my place to postgres/Aurora tbh. 
You could do significantly worse. Wish we were on Postgres. 
Yeah, it's definitely subjective. I'm just saying, creating text documents and adding attributes and making sure that strings match exactly isn't something I want to deal with. I generally have low expectations of business people reading the passing and failing test results. My previous framework, which I mentioned, did output HTML results with all the gherkin and scenarios and everything. For companies I've worked at, the benefit has been low. The benefit of readable tests for documenting code for future development is significantly higher, so this just optimizes for that. I have nothing against specflow or what it's for, I love the concepts. I just think the ROI is lower than I want it to be.
Mess with it ! The code will be the same and server-side debugging is a better experience anyway. Get some Components whatnot going and later on, flip a switch and it runs on client rather than server. Razor Components will be usable with MVC/Pages as well, so it's worth it to mess around with, even if we don't get the WASM bits for a while.
I was just thinking the other day when will VS2019 drop, and concluded that probably summer soft rollout again when everyone is on vacation and stuff that might brake are therefor limited. Interesting development this is!
[removed]
These are great questions which leads into why and when to use a document DB. Full disclosure, I love document DB's and think when used correctly are the best reflection of how modern applications work. To cut to the chase, the scenario you give, yes you would use the IDs of where you would want a relationship. This is not necessarily a bad thing, just because document DBs allow you to remove some relationships doesn't mean all relationships are bad. You are sacrificing some integrity of relationships (at least by default), but it's totally fine. On top of that, the speed sacrifice is none or very small compared to a traditional RDBS. I might be taking your example too literally, but I would for the case of invoices store the ID of the related orgs, but also store all of that info to display (company name, price, etc) at the time of the order. Just like a physical invoice, I would have a copy of what it was, at the time of the order, and not have to keep a version history. If you wanted to show a version with the latest info on the company by ID, you would have to do the lookups and generate the invoice object at runtime. I hope this helps! A lot of people think that you can't have any relationships in a document DB, but it is perfectly allowed. I also want to let you know about [Marten](https://github.com/JasperFx/Marten) which is just like a document store on top of Postgres, that lets you do relational and document operations. I find it a lot nicer than Mongo personally.
This should work: https://marketplace.visualstudio.com/items?itemName=munyabe.ToggleComment
What's the use case for async constructors? Where do you miss them?
Rider indeed is really good, but some missing features are essentials for me, like edit and continue / docker-compose debug.
Dependency Injection. The common workaround is a static builder method which becomes the constructor. If you use DI writing factory methods around those is so tedious.
Installed ReSharper just for that single keyboard shortcut...? What's wrong with Ctrl+K+C/Ctrl+K+U? Works just as good, you can always just configure the keybindings.
Is this also the C# 8 release?
You can't configure a single keyboard shortcut to toggle a comment. You know, the way every other IDE in existence works? Anyway, I found another extension. ToggleComment. I should have searched a little harder first before posting
Maybe you haven't look deep enough in VS settings? It's there.
What makes you think it is 6 months out? In the past their preview 2 is usually really close to a full release (like a month or two).
Didn't have too many issues with 2017 either. 
You are correct, I failed to read the part that said &gt; .NET Core 3.0 is scheduled to release in the second half of 2019 And yeah, only server side, but I'm still very much excited! Pretty solid steps towards that end it feels like!
At the bottom of the article in the "Wrapping Up" section it says &gt; .NET Core 3.0 is scheduled to release in the second half of 2019 I missed that part too
Please, enlighten me. What's the shortcut called?
From this link: https://blogs.msdn.microsoft.com/dotnet/2018/11/12/building-c-8-0/ It says &gt; The current plan is that C# 8.0 will ship at the same time as .NET Core 3.0 Which, according to OPs link: &gt; .NET Core 3.0 is scheduled to release in the second half of 2019.
Thanks
I was able to eliminate the error by creating a symlink to the assembly in the project's `/bin/Debug/netcoreapp2.1` directory: $ ln -s ~/.nuget/packages/oracle.manageddataaccess.core/2.18.3/lib/netstandard2.0/Oracle.ManagedDataAccess.dll Oracle.ManagedDataAccess.dll This is not ideal, but works until I find the proper fix.
Good find. Thanks!
I still don't see that disclaimer anywhere on the Launch Event page, but I guess I'll take your guys word for it.
Cool, thanks for explaining!
2017 has been pretty solid, but 2015 was flaky as all fuck, especially with the sign in engine. And I never dared try any TypeScript dev in it, as VS seemed to do it's own thing with TS - your solution would never work properly machine to machine.
It's only 2019. As for fair, you can use community or Code, but I sympathise. 
Why isn't it fair? You don't need any IDE to develop for .net core. There is the free VS Code, depending on your situation even VS 2019 community... If you have a MSDN subscription you can also just use the new version. Rider will also be upgraded to work with it. It's a glorious time to develop on core. Why do you feel left out?
because 2017 is not too old. some of us live in places where data is expensive and also slow. downloading visual studio is not always an easy task. I'm currently using Enterprise 17 and would expect that dot net core 3 should work on it. I have visual studio code also but i prefer using the visual studio IDE for non javascript work. 
I appreciate your feedback, but I'm still lacking *specific* examples of what Razor does significantly better than the prior approach (and couldn't be added to it).
It's a somewhat valid point.. but to be frank, if you only use it for C# work you can boil down the downloaded packages to around 1.7GB in the current version of C#. If that is a major problem I'd be happy to ship you a full image on a USB drive a few weeks/months into the 2019 release cycle. Where are you from? 
Why? I'd like a concrete example, please.
yeah I only do c# (desktop and web) . wow, thanks. I live in lagos, Nigeria
This. The messaging is confusing on this bit since everyone reads blazor as a client technology. I kinda wish they didn't co opt the name for the server side thin-client verison. I'm not even sure what problem it is supposed to solve.
Right, this is far simpler to look at IMO than your syntax. private int result; private Calculator calculator = new Calculator(); [Given(@"I have entered (.*) into the calculator")] public void GivenIHaveEnteredIntoTheCalculator(int number) { calculator.FirstNumber = number; } [Given(@"I have also entered (.*) into the calculator")] public void GivenIHaveAlsoEnteredIntoTheCalculator(int number) { calculator.SecondNumber = number; } [When(@"I press add")] public void WhenIPressAdd() { result = calculator.Add(); } [Then(@"the result should be (.*) on the screen")] public void ThenTheResultShouldBeOnTheScreen(int expectedResult) { Assert.AreEqual(expectedResult, result); }
Send me a message to remind me in april/may/whenever you feel it is stable enough to try it. I'll pull you a full layout and send it to you.
I think you've missed something. The reflection is in a base class that you would never look at. I've got it in a private nuget package that I reference. Each test just has given when and then delegate fields. 
More confirmation, from last week: https://www.youtube.com/watch?v=WmDXgO0f-MQ&amp;t=48m17s
I know there's already been a reply, but this is where i got it from (aired 2/5) - https://www.youtube.com/watch?v=WmDXgO0f-MQ&amp;t=48m17s they go into a bit more timeline stuff as well
Why would you ever need asynchronous dependency injection? Assigning values is processor bound work.
It's probably because the F# version uses an iterator (it generates an `IEnumerator` and calls `MoveNext()` on each iteration) for its loop whereas C# uses a standard loop.
This looks like a problem I had. It didn't copy the nuget dependencies to the output folder. Add this to your csproj: ``` &lt;PropertyGroup&gt; &lt;CopyLocalLockFileAssemblies&gt;true&lt;/CopyLocalLockFileAssemblies&gt; &lt;/PropertyGroup&gt; ``` 
&gt; There is the free VS Code, depending on your situation even VS 2019 community... What about people that requires a licenced VS because the company where they work can't use the community version?
&gt; The common workaround is a static builder method which becomes the constructor It's not DI itself that is async, it's that it relies on the constructor which cannot be substituted with a builder method.
VS code is still free. If you work for a company that needs a licensed version you are most likely covered by subscriptions. They aren't fixed to versions. I can't speak to volume licensing - but I believe it is also not fixed to the version only user count. If your company somehow acquired licenses that are fixed to a specific version there might be upgrade paths - but, to be honest, given the history of VS, that was kind of a dumb move in the first place. Newer features like new major language versions of C# or stuff like the roslyn compiler have never been made available in older versions. So I don't see why they would do it for C#8... should not be a surprise.
Looking through it now. Focusing on the loop since that's the only real difference between the two: 1. F# creates an iterator, whereas C# uses a more traditional loop (compare and branch) 2. Local variables `i1`,`i2`, and `i3`: It appears that F# explicitly converts them to int32 (`conv.i4`) when they're used as indices, whereas C# appears to load them directly. 3. The boolean OR short-circuit is implemented as two jumps in the F# version vs 1 in the C# version
Strange, I very rarely see hangs or thread blocking events. Most of my solutions are less than 10 projects with half of them in git, the others in TFS. You may need to look at cutting down background processes or upgrading the machine. 
&gt;VS code is still free. Yes, but Winforms/WPF/Xamarin is not supported &gt; Newer features like new major language versions of C# or stuff like the roslyn compiler have never been made available in older versions. So I don't see why they would do it for C#8... should not be a surprise. I Agree-
Did you uninstall it from add or remove programs?
Still no x64 version? I assume there's some underlying reason but it's painful. I hit out of memory crashes every other day and as memory increases intellisense and other features turn off/stop working so I usually have to restart it frequently anyway...
I've installed .net framework many times, and this has never happened. If it did happen to me, I'd re-install windows. Even though this started happening at the same time you installed the framework, consider the possibility that it didn't cause it.
its not there, if it was i won't be having this issue. i added it via the online installer (NDP472-KB4054530-x86-x64-AllOS-ENU) adn it did not appear in the add/remove programs section either, is also disabled (4.5) via the windows feature section but its still enforcing it somehow
why should i re-install windows when everything else on my PC is fine? why doesn't Microsoft instead create tools/software that can be rollbacked if not REMOVED. i get tried of ppl telling me to reinstall my OS, thats like saying if your child is sick, kill it, go make another one.. its how silly it sounds to me.. 
Not really. VSCode is destroying them. Every time I start up VSCode it feels like a relief - the UI is so much less cluttered and weird than VS, it‚Äôs faster at almost everything, it looks prettier, the syntax highlighting is better, the theme and extensions are better and there‚Äôs more of them, and the integrated terminal is sweet (why doesn‚Äôt VS have that). I think most people would agree with me, look at how many people upvote and comment on VSCode posts even on non MS subs like /r/programming. 
Dot net framework does support un-installation. I know this because I've done it. I don't know why it's not working on your computer. I have computers with operating systems, and I have children. I see a difference between the two, but if you don't, then I wish you luck.
In what application you get those tls error? .net framework tls settings are irrelevant to say chrome who talk directly to a native api. 
What was the motivation for the openapi fork?
&gt;why doesn't VS have that Yeah it doesn't ship with one proper, but there are extensions [like whack whack](https://marketplace.visualstudio.com/items?itemName=DanielGriffen.WhackWhackTerminal) if you want one. 
I‚Äôve used that extension, it‚Äôs extremely slow and buggy. 
I used the format command with dotnet cli. It is amazing already. It is similar to `gofmt` and `black` for python. ``` $ dotnet format Formatting code files in workspace '/home/user/projects/dotnet/Command2/Command2.csproj'. Formatting code files in project 'Command2'. Format complete. ```
got a question, when to use multi threading vs async await? My understanding is that generally you want to use multiple threads to take advantage of CPU bound work and split it off into different threads, so other cores / cpus can do the work with Parallel foreach for example While async / await is used for handling processing that is not CPU bound e.g. wait on a webrequest. Another question, and ive read conflicting info about this. With Async / Await. My understanding is that the thread that handles the execution of the operation being awaited could be the same thread or a different thread. When it returns it goes back into the main thread. Is my understanding correct? Some people say that its the same thread, while others have my view.
Do you need access to the raw data, or are you able to use aggregates for reporting purposes? In my applications, I will take data over X date and push it to blob/file storage, storing it in json/csv depending on the type of data. This is after I have done aggregates on that data for the reporting purposes and keep only that in my database. Example: If we wanted to keep track of the exceptions coming in, I‚Äôll group by the type, create a count, perhaps data on how often they happened per requests, basically the reporting data. If you want to do long term analytics, you are able to use those archived files. I run Azure Data Lake Analytics job each month to determine how activities of my site change over time. We could do this with aggregates as well but the raw data allows for very specific circumstances. Hopefully that helps, and others are able to share other possibilities.
Resharper is absolutely the issue. And resharper is just not needed. Almost all the features have been brought into vanilla Visual Studio.
Not to be a negative Nancy here, but I work on some extremely large projects and have never come close to running out of memory. Are you using a bunch of bloated plugins or something?
.NET Core is not available for Visual Studio 2015? That's not fair. Etc etc It's been always the case with Microsoft and Visual Studio. New features are not added to the old versions. It's not unfair.
Async constructors make no sense. Do you have an actual use case and example where such a thing would be needed and could not be solved with a factory?
Or just use environment variables.
I still need access to the raw data. If we could aggregate it, I could handle that. 
Have a look at SQL server stretch. I haven't used it myself but am considering it.
Okay. I‚Äôll look into it.
The problem is that resharper causes most of the performance problems of visual studio. VS gets better with each release, resharper compensates by getting worse. Everyone just defaults to using resharper though. 
This is a development subreddit for people who write apps using .Net. It's not really a support subreddit. Many of us aren't equipped or inclined for technical support. Unlike, say, r/techsupport. Not only that, but you're running an operating system version that's out of mainstream support and is now in the extended support section of its life cycle. And you're playing Don Quixote against functionality that was deployed almost two years ago. How so? .NET *4.7 and higher* deprecated older versions of TLS by default because they're insecure. So it's not .NET 4.7.2 that introduced this, but .NET 4.7. Which was released around the end of Q1 of 2017. So unless you avoided those updates for the past two years, it's probably not .NET 4.7.2 that caused the problem for you. For programs actually affected by TLS deprecation, you could update _those_ tools as well to newer versions, or use [various other documented means](https://stackoverflow.com/questions/44751179/tls-1-2-not-negotiated-in-net-4-7-without-explicit-servicepointmanager-security) like editing the exe.config for your program to add the `AppContextSwitchOverrides` (or even change the version of the .NET Framework runtime it uses). So those tips are there in case you *actually* have an app affected by this issue. Before you waste time on that, though, read on. Disabled versions of TLS in .NET 4.7+ might not be relevant because TLS settings are only going to affect *encrypted* channels, like tools using HTTPS. Which would most certainly _not_ be a port scanner. They just hit the ports unencrypted. So that, and most likely other things, are completely unrelated to this. Windows Defender will block common "hacking" tools, though, as it has for 5+ years. Check with your antivirus settings. It's also relevant to point out that the TLS issue would only affects traffic to sites that don't support a TLS standard released in 2008. Your browser having issues navigating to Microsoft.com isn't related to the .NET Framework because: 1. None of the browsers rely on specific versions of the .NET Framework. 2. All remotely recent browsers play nicely with the new ones. 3. The site supports the new TLS standards and wouldn't be blocked anyway. It's only small and niche sites that won't support it, and most of that won't bother a regular consumer going to microsoft, youtube, google, etc. So it wouldn't touch your browser. Unless you had some weird .NET-based extension installed or inserted in your networking stack. Which would probably be malware. Most people hit by TLS deprecation issues are developers or custom business apps. .NET compatibility issues in popular mainstream apps are pretty rare. Most people you know would only be mildly inconvenienced by this if they insist on using one really old tool or site. Your problem sounds more like you have a proxy MITMing your traffic and faking the secure connection with an old version of TLS because the proxy was written over a decade ago. There's a fair chance that's happening, given you're running a packet sniffer, VPN software and keep talking about network tools. And like to run old software and delay updates. You don't seem to have a clear idea of what is actually wrong with your computer and are taking dangerous steps to "fix it" (such as deleting important files). Given that the behavior you're blaming for everything should have kicked in sometime over a year and a half ago, you're probably barking up the wrong tree. Something is massively wrong with your system and it's not .NET. Or at least it wasn't when you first installed it. Who knows what's happened to that poor computer since then. „ÉΩ‡ºº‡∫àŸÑÕú‡∫à‡ºΩÔæâ Your current diagnosis is wrong. And guessing by the blood pouring from my eyes and soul after reading your novel, there's a fair chance that you're making things worse. You should probably have a professional diagnose your computer. Try HijackThis (it's free) to get a better log of non-standard configuration settings. There are sites where you can post them and ask for help. If you don't have money to take it to a store there are other options. Make a nice dinner and prepare a thoughtful gift for a gullible friend. Hit up online support forums. Beg politely and don't claim that legitimate software is a virus because you're just going to irritate people. Just state the facts: system details, what happened, what else happened around then, and what you've tried to fix it. Or just reinstall windows and be more careful about your browsing/downloading habits. Most pros just reinstall windows when something goes wrong like this because tracking down the issue is more annoying and time-consuming than just starting over. Use a configuration site like ninite or a tool like chocolatey to speed up your app installations. Good luck with whatever's going on there. I've done my good deed for the day and can contently get massively drunk now.
Hold on, at least make sure he's a prince first.
&gt; When it returns it goes back into the main thread. Maybe. If you have a UI thread (WinForms, WPF, ASP.NET (before core)) then you need to return to that thread before interacting with the UI (forms, controls, response object). If you do not have one of those contexts, then it will grab a random thread pool thread. **** .ConfigureAwait(false) says "don't worry, I don't need to return to the UI thread just yet". Always use it in libraries, never use it in your direct UI calls.
&gt; While async / await is used for handling processing that is not CPU bound e.g. wait on a webrequest. Almost right. You can also use it to move CPU bound work off the UI thread. 
&gt; You mention VS being slow and you also mention Resharper I love how 25+ people upvoted this. Just goes to show that people don't read a reply after a certain length at all. Let's look at the ONLY single reference to Resharper in my entire post: &gt; They should consider Resharper's business model, and sell people subs with a perpetual fallback license for a specific version if they cancel. Hmm so I "mentioned" Resharper in the context of its fairly famous subscription model and how it could be applied to MSDN. Then went out to expressly say that Visual Studio: &gt; Even with no extensions/safe mode I'm seeing[...]. Plus as soon as you add even Microsoft's own extensions [...] But 25+ of you, and three replies at the time of writing, obviously didn't take the small amount of time it would have taken to read my post. You should be ashamed of yourselves. 
Read posts you're responding to before responding next time. 
Identity is fine for an API
it was triggered by a few tools i have whic hwas made with net framework (older versions i think), infact the issue was gone fora while but 4 days ago i installed a software called IvMS which came with one of those visual c++ runtime libraries and the moment it was installed i noticed another software i used for security cams (dahua cams on smartpss) started logging out of cams, i figured the bug was back so i loaded my faceboook which is what i use to test and it started hanging on TLS errors, somehow this bug triggers ssl issues which forces TLS issues on browsers but kinda disconnects other tools from the internet which were created by or uses net framework tools but this time it was much worse, as i could not go on any sites on https and it slowed down my access to http sites too (since a lot of sites on http used links like say images from external sites on https or google stuff which are on https), my firewall and defender are not blocking anything.. i'm not saying net framework 472 actually killed my net but it made some changes to the registry which is indirectly causing it... so its sort of a bug..
as i said on win 8.1 pro which came with 452 so i updated straight to 472 assuming it would allow any new tools created on the framework from running on my pc..the mistake on my part was to not do it via windows updates as i did it after i got a bug on a tool using netframework which wanted 4.5 or higher, at that time i did not realise that in the optionalfeatures, 4.5 was not enabled, i had to enable that ( i only found out a week alter after the issue) , i figured 472 was an update to 4.5 so i added it not realizing it wasn't.. you would think when they update from 4.5.2, to 4.6.2 to 4.7.2, you woudln't need an older version as the updates will carry on previous versions, apparently not when it comes to this software.. i know this isn't tech support but i have asked multiple people on other tech sites, none ever actually got back to me and i'm aware that on reddit, ppl actually do help so i popped up here looking for any reddit on netframework and i figured this was the one where i could find "real" experts who know or might know what may be causing this.. sorry for that..
Have they already committed on Blazor?
 https://www.youtube.com/watch?v=WmDXgO0f-MQ&amp;t=49m22s &gt; we do plan to ship it, we just don't have a concrete ship date yet, we don't have a full roadmap as to when that will land.
&gt; Always use it in libraries Unless your library actually needs to resume on the captured context. If you're writing a library that needs to get at the HttpContext in an MVC5 app, for example, you'll need to skip the .ConfigureAwait(false) sometimes.
But the workload you can accomplish, assuming you've not saddled yourself with a bad database design, is huge.
They have an FAQ about it: [https://openapi-generator.tech/docs/fork-qna](https://openapi-generator.tech/docs/fork-qna)
Good point.
jwt bearer token on identity. Easy to imement and use. Just google it.
i'm just starting out and want to learn about this topic, thanks
Great. Let me know what you think about the book?
My most immediate thought was to look at your network devices. And I am thinking NordVPN and the Internet Sharing's IP subnet could be causing you issues if your cams are on the lan. ``` Adapters List Enabled Remote NDIS based Internet Sharing Device IP Address: 192.168.0.153 Subnet mask: 255.255.255.0 Gateway server: 192.168.0.1 DHCP: 192.168.0.1 DNS Server: 8.8.8.8 8.8.4.4 1.1.1.1 9.9.9.9 TAP-NordVPN Windows Adapter V9 IP Address: 10.8.1.44 Subnet mask: 255.255.255.0 DHCP: 10.8.1.254 DNS Server: 103.86.96.100 103.86.99.100 ```
Maybe say where it is. This isn't a very useful comment.
Are you basically looking for a data warehouse? If so, I'd recommend looking at a star schema. https://www.datawarehouse4u.info/Data-warehouse-schema-architecture-star-schema.html This isn't my area of expertise but I've been at multiple companies where they have gone down this route. And it does work fast. 
lol , im not a prince. i know its a joke but not every nigerian on the internet is a scammer 
I don't know about the contents, but I love the cover
hehe thanks. Did you read the content or just didn't get to that part yet?
No harm meant, good luck :)
What's wrong with auth0? We use them, their documentation can be a bit spotty but otherwise pretty good
So still no designer for winforms/WPF for .NET Core 3.0? Or would that come with a Core preview update instead? 
In my case it was Rider. Once installed, Inever looked back and missed VS, which I have to open only when I have to deal with Windows Forms (yes, I have to, my bad)
This is in my opinion terrible advice. Having experience with both Node and .NET Core i would say it depends on your usecase. Are you creating a banking application? Then C# is absolutely worth the rewrite. But the overhead of typed languages makes prototyping more tedious. Javascript is great for places where data type integrity isn't crucial (like frontend) Typescript falls in between by being terrible at both
Resharper was the main culprit for me. Turning it off is a significant improvement on large projects. However, Rider has been my daily driver for over a year now so I worried I'm out of touch.
FWIW I'm not shitting on your framework, I think it's a great move. I've personally attempted to make a Describe/It style framework for (n|x)Unit that failed spectacularly mostly because I don't have the time to dedicate to it, but whatever. SpecFlow is a happy medium for me. It allows me to write, word for word, without any compromise on syntax, what my stakeholders that have 0 knowledge of computers and code can read and write, and _most important of all_ I can point at and say things like "So what does this phrase/word/sentence mean to you?" and lift it verbatim into an executable format. That's a goldmine right there. If you're not already aware of it, the [Pickles](http://docs.picklesdoc.com/en/latest/) publishing tool may be of interest to you. I use it for publishing the progress of my work and its somewhere stakeholders can easily look at to see how things are coming along without needing to interrupt me.
i actually got NordVPN in late November, a whole month after this bug started, its the only thing making it possible for me to be online (even though it crashes every few minutes now due to some related ssl bug) that subnet mask is used by every computer out there i believe 
Yes likely. I do use Resharper so I'm sure that's a huge part of it. I also have Xaml Formatter and the Wix Installer extension. This morning when I got in I closed all the source windows in VS and close the app itself. I then relaunched my solution and waited for it to fully startup. It was using 1243.4MB without me even starting work. After about 10 mins it dropped to 728 and then got back up over 1000 as I started to do work. One day i'll have to disable resharper and see the differences While resharper might be the main reason, it's still 2019 and x64 is pretty much the standard...
Thank God for that. I have literally no idea what's been happening in .NET since he went on paternity leave.
Why do they make no sense?
You can check out my link [blog](https://www.alvinashcraft.com/), but I agree that Chris does a phenomenal job of picking out the highlights each day. It's great to have him back.
Hello, Thomas. Thank you very much for putting this out there and providing it for free! My team is currently developing a .NET Core WebAPI using ILogger with Azure Application Insights. While I haven't read your book, I'm sure it would have been helpful when we were scouring the Internet for tutorials. It is worth noting that downloading the book from leanpub requires that you set up an account and provide an email address.
You're welcome. I think you are right about Leanpub. Don't think there is much I can do there, I'm afraid.
&gt; you would think when they update from 4.5.2, to 4.6.2 to 4.7.2, **you woudln't need an older version as the updates will carry on previous versions***, apparently not when it comes to this software.. **It does**. 4.7.2 runs like 98% of apps from prior versions with no change. It's a drop-in replacement for applications who were following best coding practices and have been keeping their system infrastructure updated responsibly. You can even jump straight from 4 to 4.7.2, though it's unlikely that you have a computer that wouldn't have already hit an update in between. &gt; if i used a tool which was being blocked such as that smartpss tool i mentioned above , it well do a "syn_sent" but then it would hang up and not wait fora reply, i also noticed a lot of DNS issues in my event viewer basically meaning it was causing my DNS to become unresponsive as i got a lot of "arpa" related errors in the event viewer.. for the first 2 months i did all the DNS/DHCP related cleanup from flushing to arp table to ssl certificate clearing.. **None** of this is stuff the .NET Framework update would cause. The .NET Framework is essentially a big standard library and runtime intended to accelerate application development. It's just a toolkit used to write new user applications. Most core windows processes do not use it for performance reasons. Why's that, you say? It's a *managed runtime* with *garbage collection* (GC). That would mean if it were used for core operating system parts like the network stack you'd see periodic slowdowns during GC runs without some serious engineering efforts to mitigate that. That's why most major operating systems (including Windows) are written in non-managed languages like C or C++. Especially for code that's used a lot like the networking stack. .NET updates don't affect software that doesn't use the .NET Framework. So, the **only** way it could have any possible effect would be if some .NET-based **malware** has inserted itself into your network stack. Which, once again, would not be caused by .NET itself but by the malware. And, in that case, **finding and removing the malware** would fix the issue. Since you love using network tools, try checking the headers and certificates on all the different sites for evidence of MITM. Other than the site name, it might look like it's filtering through the same "server" or that it's been signed by the same certificate authority (CA) if you're experiencing a MITM attack. Your network might have an old proxy server configured, which could cause similar behavior without being hosted on your machine. Check proxy settings in your Internet Options. You might have even done it to yourself if you have a really old version of a proxy tool like Fiddler or Charles running on your machine to snoop on encrypted web traffic. You're seriously wasting time on your witchhunt for .NET 4.7.2 - yours and everyone else's. Which is probably why no one wants to contribute on your snipe hunt. You're extremely adamant about blaming something that knowledgeable people would recognize as completely unrelated to your issue. I mean, it's not quite a moon-landing level conspiracy theory, but it induces the same amount of eye-rolling. Go back to the drawing board and re-evaluate your issue from the ground up. You've missed some key data that would have lead you literally anywhere else.
That's a great resource, appreciate your effort with it! This and morning brew are my go-to aggregators for .Net news.
First, from a performance perspective the pointers get larger, so data structures get larger, and the processor cache stays the same size. That basically results in a raw speed hit (your mileage may vary). So you start in a hole and you have to dig yourself out of that hole by using the extra memory above 4G to your advantage. In Visual Studio this can happen in some large solutions but I think a preferable thing to do is to just use less memory in the first place. Many of VS‚Äôs algorithms are amenable to this. Here‚Äôs an old article that discusses the performance issues at some length:http://blogs.msdn.com/joshwil/archive/2006/07/18/670090.aspx Secondly, from a cost perspective, probably the shortest path to porting Visual Studio to 64 bit is to port most of it to managed code incrementally and then port the rest. The cost of a full port of that much native code is going to be quite high and of course all known extensions would break and we‚Äôd basically have to create a 64 bit ecosystem pretty much like you do for drivers. Ouch. 
In MVC its worth to start reading code from controllers Use GoToAll (ctrl+T) and Search All (ctrl+shift+F) 
I wish it was that simple. There are several projects under this solution. One of which is the one which has all the controllers, models and views and lots of other stuff which I don't understand. Most of them code is just accessing some resources or repositories from other projects. I try to go to views and see if I can understand the .cshtml files but most of them have just one or two lines in that(I think they're accessing js files). 
That worked, thank you. Is there a way to have selected assemblies, `Oracle.ManagedDataAccess.dll`, for example, copied to the output folder (rather than all of them)? Given that my assembly is run via PowerShell, it seems like the `System.*` assemblies are unnecessary. I'm assuming that I need to include these files when I distribute my Cmdlet: * `PsOracleCore.dll` * `PsOracleCore.psd1` * `Oracle.ManagedDataAccess.dll` &lt;-- can NuGet install this automatically on the target machines? * `PsOracleCore.deps.json`? * Others?
https://visualstudio.microsoft.com/xamarin/
Honestly, just ask. If they get defensive about that, it‚Äôs probably because they barely understand it themselves.
It's called Xamarin
I think in short: async/await mostly for making UI experience more smooth. And multithreading for backend logic like connection pooling and other stuff.
I think in short: async/await mostly for making UI experience more smooth. And multithreading for backend logic like connection pooling and other stuff.
No I mean Runtime that you install on Android just like the desktop ones. For example when you write a .NET Core console app it gives you an .exe or .dll file and you can run that file on very machine that has .NET Core Runtime on it. But you can't run it on Android because there is no Runtime for it.
&gt;No I mean Runtime that you install on Android just like the desktop ones. For example when you write a .NET Core console app it gives you an .exe or .dll file and you can run that file on very machine that has .NET Core Runtime on it. But you can't run it on Android because there is no Runtime for it. &amp;#x200B;
&gt; Android is an open source OS It isn't. It is Google platform to get more revenue from the Google ads. https://en.wikipedia.org/wiki/Android_(operating_system) *The license does not grant rights to the "Android" trademark, so device manufacturers and wireless carriers have to license it from Google under individual contracts.* 
thanks to add to the ui aspect which pertains to desktop, async / await is useful in the web api layer. i do async all the way, most blocking is usually done at the db or calls to other apis. so it makes sense to do it there. i had an interview and i was asked about the task parallel library. i never had used it, because im a web dev. ive only ever seen it used once in a web api, and i think the benefits were not worth the over engineering.
most people here are talking about the UI thread which would be desktop apps. What role if any do you this parallel for each or multi threading plays in the web api layer if any? or mvc for that matter? can you think of any use cases where it would be warranted? generally i would think high cpu bound work should be put onto a message queue and handled in say an azure app function. that is, if the work done is going to really slow down your site. 
I don't think its over engineering. If TPL makes user experience better, then it's a must. As a good developer you should take care of it.
thanks, can you think of examples in the web api, or mvc, where it would make sense to use TPL? i cant think of any... if there is blocking work thats going to degrade the web experience perhasp it should be processed in an azure app function via message queuing. the result when returned can be stored in the db and there can be a signaling process to see when its done. it would depend on the work being done of course. if it can be executed in say 5 minutes and lets say for example its doing a lot of expensive cpu work that can utilise the TPL to get it down to 30 seconds, i think that would make sense. but scratching my head for a scenario where it would come up.
I'm not a web dev. I mainly do backend development. Why do signaling if async/await can 'callback'?
It is useful to think of ASP.NET as having a specific UI thread per request. ASP.NET Core does not have this limitation. 
&gt; What role if any do you this parallel for each or multi threading plays in the web api layer if any? Oh have one site that kicks off a dozen or more database queries simultaneously. After a 1 second timeout, I return the ones that have completed merged into a single list. (It was for type ahead searches, so speed is more important than accuracy.)
The problem just the amount of time to develop it. Dotnet guys said that was a waste of time doing it at the moment cause we have netstandard2.0 and xamarin. Also Xamarin now gets a lot of source from coreclr and corefx
Irrelevant to the question but it is. Anyone can view and change the source and publish it with their own branding. The Android brand and Google softwares are not hough.
A surprisingly large amount of VS is already managed code luckily. Each release it seems to be more
XNA won't be forgotten.
i use smartsniff to read packet filters, (recently got wireshark and netmon3.4, neither has been helpful) i actually don't have fiddler or charles, i checked the certificates many times, apart from the "expired" 2 certificates from Microsoft from the 90's, every other is well thumbprint and legit..i didn't say .net actually killed my net but one of the updates did make (enforce) changers somewhere related to TLS/SSL or some other cipher which is overriding the one i set for system default via registry (not group policy cause is disabled).. my pc is virus/malware free, i check for those weekly, the proxy settings are fine, there isn't any..not sure why ppl blame the vpn (nord)as i said the issue was there for a month before i got nord to make the internet usable, if you guys keep claiming net472 isn't the issue, i believe you but then tell me how to get rid/uninstall this shit software (i'll stick with 3.5 for now) so that i can be sure completely? or recommend tools for me to search for this "specific" bug cause its surely not on registry
&gt; tell me how to get rid/uninstall this shit software Yeah, none of us knows how to uninstall it off the cuff because one very rarely needs or wants to uninstall it. Especially as developers who are used to having multiple toolkits installed simultaneously. It just adds additional functionality that doesn't take up any resources other than disk space until you use it. You can google for it as well as we can. First result page when [searching for "dot net uninstaller 4.7.2"](https://duckduckgo.com/?q=dot+net+uninstaller+4.7.2) returned a [quora link](https://www.quora.com/How-can-I-uninstall-or-remove-Microsoft-NET-Framework-4-7-from-Windows-8-1-OS with relevant links on removing and repairing the .NET Framework. Assuming that you can't just remove it using Add/Remove programs. If that doesn't work, keep searching. If you still can't find a solution, reinstall Windows. You've got options, even if they're not your preferred options. &gt; i'll stick with 3.5 I stopped trying to keep my OS in perpetual stasis since I left Windows XP, and it's not a pursuit I find remotely engaging or rewarding any more. You're riding this train alone. God speed.
You might have better luck posting in the LINQPad forums.
I have not seen this previously, so thanks for that. I also did a quick compare to the Dew Drop ([https://www.alvinashcraft.com/](https://www.alvinashcraft.com/)), and it appears that The Morning Brew does not have much overlap. Good stuff.
I suspect this is largely due to the security model used by Android. When you debug a Xamarin app it does actually use a shared runtime, however this depends on the runtime and app being signed with the same key. A shared runtime published to the app store would either be completely useless (since your app wouldn't be able to call it) or it would have to fundamentally undermine Android security and permissions.
Breakpoint in the controller, debug and F10/F11 through the code? 
Yes this is spot on. Mobile apps are sandboxes and permission controlled. To add, Apple don't allow runtimes of any kind on mobile devices, so mono is compiled to native as part of each xamarin ios app. Iif you're designing a cross platform mobile SDK, it makes sense to take a consistent approach to each platform. Mono runtime is embedded in the app itself regardless of the platform. AFAIK Mono is now endorsed by Microsoft as being _the_ cross platform implementation of the CLR, which is also why the Blazor team are using it for their web assembly shenanigans.
async/await is about not blocking. That's it. Whether that matters to you really depends on what you're doing. If the thread is the UI thread in a desktop application, then you don't want to block the UI for other operations. If you have a webapp with lots of requests then you don't want to block threads while waiting to read data from the network. If you have a console app that just reads a single file to process and there's nothing to do until that file is opened, then not-blocking doesn't make any difference. &amp;#x200B; Parallel is about doing multiple things at the same time. async/await is only parallel if you have other threads available to do the work, which is fundamentally limited by how many CPU cores you have. The work may be sent to another thread or not and depends on how the thread scheduler manages to get the work done. If you're waiting on something like a SSD or network card, then you're waiting on external hardware. &amp;#x200B; 95% of the time, use async/await to get the simplest code and fine-grained ability for the scheduler to split up work. Use Parallel.foreach when you definitely know you have things that can be processed in parallel and have the capacity for it. Remember that code within that foreach can still use async/await to keep from blocking which is why it can get complicated, so stick with async/await to not block on anything for the best results most of the time.
There is an android sdk...and it has to be installed with jdk and mono.net for xamarin to work...look ma...my certification paid off!
Thanks for the tip, I went ahead and posted my question there. 
Do you know how to run python program? You might if you use cygwin and I saw a few python install in your pyriform logs. Install sslyze with pip and test your connection to website you think you have connecting. For example, this is how it look when I do it on google.ca sslyze www.google.ca --regular And the output AVAILABLE PLUGINS ----------------- EarlyDataPlugin FallbackScsvPlugin HeartbleedPlugin SessionResumptionPlugin SessionRenegotiationPlugin CertificateInfoPlugin OpenSslCipherSuitesPlugin RobotPlugin CompressionPlugin OpenSslCcsInjectionPlugin HttpHeadersPlugin CHECKING HOST(S) AVAILABILITY ----------------------------- www.google.ca:443 =&gt; 172.217.13.195 SCAN RESULTS FOR WWW.GOOGLE.CA:443 - 172.217.13.195 --------------------------------------------------- * SSLV2 Cipher Suites: Server rejected all cipher suites. * TLSV1_3 Cipher Suites: Server rejected all cipher suites. * OpenSSL CCS Injection: OK - Not vulnerable to OpenSSL CCS injection * Certificate Information: Content SHA1 Fingerprint: d8b5d840bc62a0e8a8d71980f404077272409ee3 Common Name: *.google.com Issuer: Google Internet Authority G3 Serial Number: 5381731771420787293 Not Before: 2019-01-29 14:58:00 Not After: 2019-04-23 14:58:00 Signature Algorithm: sha256 Public Key Algorithm: EllipticCurve Key Size: 256 Curve: secp256r1 DNS Subject Alternative Names: ['*.google.com', '*.android.com', '*.appengine.google.com', '*.cloud.google.com', '*.g.co', '*.gcp.gvt2.com', '*.ggpht.cn', '*.google-analytics.com', '*.google.ca', '*.google.cl', '*.google.co.in', '*.google.co.jp', '*.google.co.uk', '*.google.com.ar', '*.google.com.au', '*.google.com.br', '*.google.com.co', '*.google.com.mx', '*.google.com.tr', '*.google.com.vn', '*.google.de', '*.google.es', '*.google.fr', '*.google.hu', '*.google.it', '*.google.nl', '*.google.pl', '*.google.pt', '*.googleadapis.com', '*.googleapis.cn', '*.googlecommerce.com', '*.googlevideo.com', '*.gstatic.cn', '*.gstatic.com', '*.gstaticcnapps.cn', '*.gvt1.com', '*.gvt2.com', '*.metric.gstatic.com', '*.urchin.com', '*.url.google.com', '*.youtube-nocookie.com', '*.youtube.com', '*.youtubeeducation.com', '*.youtubekids.com', '*.yt.be', '*.ytimg.com', 'android.clients.google.com', 'android.com', 'developer.android.google.cn', 'developers.android.google.cn', 'g.co', 'ggpht.cn', 'goo.gl', 'google-analytics.com', 'google.com', 'googlecommerce.com', 'source.android.google.cn', 'urchin.com', 'www.goo.gl', 'youtu.be', 'youtube.com', 'youtubeeducation.com', 'youtubekids.com', 'yt.be'] Trust Hostname Validation: OK - Certificate matches www.google.ca Android CA Store (9.0.0_r9): OK - Certificate is trusted iOS CA Store (12, macOS 10.14, watchOS 5, and tvOS 12):OK - Certificate is trusted Java CA Store (jdk-11.0.2): OK - Certificate is trusted macOS CA Store (12, macOS 10.14, watchOS 5, and tvOS 12):OK - Certificate is trusted Mozilla CA Store (2018-11-22): OK - Certificate is trusted OPENJDK CA Store (jdk-11.0.2): OK - Certificate is trusted Windows CA Store (2018-12-08): OK - Certificate is trusted Symantec 2018 Deprecation: OK - Not a Symantec-issued certificate Received Chain: *.google.com --&gt; Google Internet Authority G3 Verified Chain: *.google.com --&gt; Google Internet Authority G3 --&gt; GlobalSign Received Chain Contains Anchor: OK - Anchor certificate not sent Received Chain Order: OK - Order is valid Verified Chain contains SHA1: OK - No SHA1-signed certificate in the verified certificate chain Extensions OCSP Must-Staple: NOT SUPPORTED - Extension not found Certificate Transparency: NOT SUPPORTED - Extension not found OCSP Stapling NOT SUPPORTED - Server did not send back an OCSP response * Downgrade Attacks: TLS_FALLBACK_SCSV: OK - Supported * OpenSSL Heartbleed: OK - Not vulnerable to Heartbleed * Session Renegotiation: Client-initiated Renegotiation: OK - Rejected Secure Renegotiation: OK - Supported * TLSV1_2 Cipher Suites: Forward Secrecy OK - Supported RC4 OK - Not Supported Preferred: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 128 bits HTTP 200 OK Accepted: TLS_RSA_WITH_AES_256_GCM_SHA384 256 bits HTTP 200 OK TLS_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_RSA_WITH_AES_128_GCM_SHA256 128 bits HTTP 200 OK TLS_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK TLS_RSA_WITH_3DES_EDE_CBC_SHA 112 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 256 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 256 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 128 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 256 bits HTTP 200 OK * TLSV1_1 Cipher Suites: Forward Secrecy OK - Supported RC4 OK - Not Supported Preferred: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK Accepted: TLS_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK TLS_RSA_WITH_3DES_EDE_CBC_SHA 112 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK * ROBOT Attack: OK - Not vulnerable * Deflate Compression: OK - Compression disabled * TLS 1.2 Session Resumption Support: With Session IDs: OK - Supported (5 successful, 0 failed, 0 errors, 5 total attempts). With TLS Tickets: OK - Supported * SSLV3 Cipher Suites: Server rejected all cipher suites. * TLSV1 Cipher Suites: Forward Secrecy OK - Supported RC4 OK - Not Supported Preferred: TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK Accepted: TLS_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK TLS_RSA_WITH_3DES_EDE_CBC_SHA 112 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA 256 bits HTTP 200 OK TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA 128 bits HTTP 200 OK SCAN COMPLETED IN 8.49 S ------------------------
Shenanigans? Is that the place with all the goofy shit on the wall?
i see, thanks for the well thought out response. it cleared things up for me. one question based on what you said. you said that cpu bound work can be awaited on, thats awesome. so instead of having to use the TPL i can do AwaitAll, or AwaitAny and execute cpu bound work which will go to other threads if they are available. (correct me if im wrong) This all came about because i had a job interview, they asked me if i had used the Task Parallel library, i said no and they asked if i had much experience with dealing with multi threading and resolving deadlocks. also a no. i think that was the kicker for me not getting the job. So i was curious why they are using the TPL, because async all the way as you said is just easier. That said, i dont know what they do with the TPL, so it might be warranted. However they asked me if i had experience with ADO.net and i said yeah, years ago before ORMs became the norm, first it was nhibernate. now i use entity framework. They said to me, we tried using entity framework but the queries produced were not optimal. This seemed like a bad smell from the company, it seems like using EF was too hard for them, or perhaps they "tried it" when it was in a very early release, even so, nhibernate was the go to for some time because it was not feature parity with EF, and correct me if im wrong, it still isn't. I'm talking regular entity framework not EF core. EF core is still not even on parity with regular .net EF. So, perhaps i dodged a bullet with this company. I mean, if your queries suck coming out of EF, its generally not going to be all your queries, so you can still execute raw sql. so i really dont understand why they would be 100% ado.net. yes its bare metal as can be. but this company sells office supplies B2B, so i cant imagine they have any high volume real time transaction processing systems. The interview was quite strange as well, they prefaced it saying they want a specialist senior .net dev, im like ok. but most questions were pretty stupid. Some examples - whats the diff between a reference type and value type - whats boxing / unboxing - what does immutability mean, so i just said some types arn't e.g string they can't be changed so assignment doesnt work etc. no need to repeat my answers to such basic questions. then they simply ask if ive delt with threading and deadlocks, like what the hell?! The angular questions they asked me same deal, lots of basic stuff like what do pipes do in the template, and do those pipes effect the value in the controller. how to debug a slow page.... then they ask me what does the async pipe does. so yeah, my questions here probably sound pretty noob, i just have never had to use the TPL, its always been async / await. i figured it was more for desktop application development. So it seems perhaps they are doing things the wrong way, but yes it depends on what they are doing. but as a B2B office supplier? had my head scratching. sorry that became a rant :P I wanted the job mroe so because its 10 minutes from my house lol.
&gt;so instead of having to use the TPL i can do AwaitAll, or AwaitAny and execute cpu bound work which will go to other threads if they are available. Yes. This works fine. `Task.Run``()` is an easy way to schedule work on main threadpool. Asynchronous programming has been in C# for years and has 3 main patterns. The latest pattern uses `Tasks` and is embedded directly into the language with `async/await` keywords which makes it very easy to use. Before we had these keywords though, the TPL (Task Parallel Library) was released to help make parallel and concurrent operations easier when working with the Task based pattern. It's still very useful for more complex scenarios when you need precise control, and includes things like TPL Dataflow which lets you create complex processing workflows by connecting different "blocks" of logic together with multithreading built-in. TPL can also handle all 3 async patterns although everything has moved to just Task-based by now. &amp;#x200B; The .NET documentation is pretty in-depth and I would recommend starting with the overview on asynchronous programming patterns: [https://docs.microsoft.com/en-us/dotnet/standard/asynchronous-programming-patterns/](https://docs.microsoft.com/en-us/dotnet/standard/asynchronous-programming-patterns/) &amp;#x200B; And how the TPL works: [https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/task-parallel-library-tpl](https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/task-parallel-library-tpl) &amp;#x200B; I also recommend looking over the .NET Guide and reading through all the sections in general to get a good overview and catch up on any areas you might not know well yet: [https://docs.microsoft.com/en-us/dotnet/standard/index](https://docs.microsoft.com/en-us/dotnet/standard/index)
Other than having your own interface on it, [MailKit](https://github.com/jstedfast/MailKit) is what allows us to switch around what we're using. 
again thanks! ill have a read, busy with angular atm tho. sigh, too much to learn never enough time...
I've used both sendgrid and Amazon ses. Sendgrid for ease of use and support but SES for cost.
You can create your own shortcuts. I have two for when I tab on ‚Äúcwl‚Äù it puts in the code for Console.WriteLine(‚ÄúHello‚Äù) and ‚Äúcw‚Äù will do the same but without the hello. This is useful for quick debug messages but obviously can be expanded on 
Highlight + F12 things you don't understand and don't forget your navigation arrows in the tool bar. 
I have too many favorite keyboard shortcuts to list, so I would just recommend looking up the list. You can right-click on the document tab and select New Vertical Tab Group. This is handy, for example, when you want to have a cshtml file and the code behind file open side by side. [Related to the above tip] You can unpin the side panels to give yourself more screen space. In code view, you can drag code blocks to the Toolbox panel to store code snippets. I find it handy for Usings frequently referenced. 
With Amazon SES it is definitely not completely abstracted. The code you write to hook into SES is not something you can re-use if you decide to use something else. Im not sure about sendgrid. 
With both options you can access it via SMTP. You don't need to write any proprietary code to use them. The weepy I access is probably more secure.
Take the time to create your own keyboard shortcut. I personally like to use ctrl + r for refactoring (ctrl + r, ctrl + r = rename; ctrl + r, ctrl + m = move), ctrl + t for anything related to tfs (ctrl + t, ctrl + p = pending changes; ctrl + t, ctrl + b = builds), ctrl + p for Goto file (like in chrome devtool), ctrl + w for "Close this file", ctrl + alt + w for "close all but this", f12 for "Go to definition" and shift+f12 for "Find all references". It will make your life so much easier to move around the code and link all the information together in your head.
&gt; [Related to the above tip] You can unpin the side panels to give yourself more screen space. I also use an extension that hides the menu unless you hold alt. VS takes too much vertical screen real estate so it's nice to get some of it back.
Ctrl + , baby "Ctrl + ," all the way
Must underrated!
this is a great question! i like: \- the ability to auto format: CTRL+E,D or CTRL+E,F . btw, format on save extensions makes it even better \- the ability to return to a place you visited before: Navigate backwards CTRL + - (i'm ashamed to say i didnt know it before)
thanks for being the only one trying to help, after trying to install this via cygwin and it failing 3 times i installed nassl after which i got this error, just not my year :( is there another way to test it? [img]http://666kb.com/i/e1b4mnt31aqws91i2.png[/img] 
If you need to do some async work in the constructor... Just don't. Put that work in a public async method and call that method right after instantiating the object. 
i have gone thru all those link weeks ago, nothing worked..just moments ago i noticed one of the folders updated with the dfsvc.exe.config file, which is net framework4 working when i have it disabled then? why is it still working? http://666kb.com/i/e1b4sss4ikxfpfu8q.png
Get a mouse with forward/back buttons on it. Perfect for browsing through code.
 Can't remember a shortcut / don't want to dig through menus ? `Ctrl+Q` will save brain cells and time, while reminding you of said shortcuts and menu locations.
I'm a backend dev using the TPL in a background work library. Whenever someone logs in to one of the many websites my orgsnization has, our service is called. The service makes a db call, checks if the data is up to date and if its not, a call to another backend service is made to update the data. Before the data is sent to the consumer, a task is fired off that registers that this person requested their data (for audit purposes) and updates their data in the db. We dont want to wait for that so we just return a response and the work completes in the background.
Get an educational ReSharper license (free) you‚Äôll love it I promise. It helps with writing code and refactoring. Get well familiarized with keyboard shortcuts and watch tutorials on how to effectively debug with VS. Also catch up on how to use your teams version control system (GIT, TFS, etc) inside ofS. Learn the version control CLI usage and integrated plugin. Best of luck!
I already have a commercial license for ReSharper at work. Installed him. Don't know all the functionalities of it though. 
Thank you
Or ctrl+f12 when it is part of an interface that has been injected in the class. It will lead you to the implementation instead of to the interface
CodeMaid + .editorconfig changed me life when it comes to formatting code :O
Interestingly you can install [termux](https://play.google.com/store/apps/details?id=com.termux&amp;hl=en) and then a Linux distro and then install Mono or .NET Core on it! I guess because Android sees the hole thing as an app and doesn't care/know what's going on inside it. So I'm wondering if it would be possible to make a .NET Core or Mono apk that is capable of running .exe and .dll files inside itself.
Didn't know that! Thank you!! :)
One thing that helped me learn ReSharper was printing out a shortcut cheat sheet and referencing it as I developed. The cheat sheet will show you what tools are available to you and you will quickly hone in on the ones that are most important. Most useful for me are: Improve: Alt-Enter Refactor: Ctrl-Shift-R Fast useful search by class or file: Ctrl-T For me, using ReSharper improved my C# skills a lot. Pay attention to the warnings and suggestions it gives. You don‚Äôt necessarily need to change every warning and suggestion presented to you, but if you take the time to understand WHY ReSharper is suggesting the changes it does, you will hopefully find yourself learning a lot about the language!
But shouldn't you have used async and await for exactly that, since it's mostly IO bound? I mean using cancellation tokens and cancelling after 1 second just returning the result of the tasks that RanToSuccess (not on a PC) 
I've never tried, but it should be possible to load dll files and run them from a Xamarin Android app. If you look inside the compiled apk you'll see it's using dll files anyway. The linker would be problematic though, so you'd probably have to turn it off and have a much larger apk file (otherwise you could be missing necessary framework code). The underlying sandbox issues would still be there too, and if your dll wanted to do anything, your host app would have to have that permission.
- open terminal - ping theapi.com - profit
will this return the IP address? e.g ping api.google.com/g/search?
Sneakers is the greatest film of all time!
ctrl , is incredible useful for finding classes in a large project.
Is there an option to open whole directory structure in the Project tab of a file you have currently opened? It's the one thing that IntelliJ had, that for the life of me I can't find in VS.
What do you mean? 
DOCUMENTATION!
Care to explain what it does? Thanks in advance
Of? 
You‚Äôd just ping api.google.com, and yes, the IP would be in the result. However, if NAT was setup correctly in the router/firewall, then this wouldn‚Äôt be necessary. I can‚Äôt think of a need to whitelist outbound traffic.
A button or a shortcut to show where current file lies in Solution Explorer.
the network team say it's for security purposes. i keep telling them it's a problem, especially if the owners of the api change their ip address. i don't really know much about networking but i think there should be a better setup
try binding extra mouse buttons to copy and paste. enter too if you have buttons to spare.
I'm not sure. But if you hover over the file tab at the top, you'll see the whole address of that file. 
Sounds like a half-arsed implementation. TCP/IP should be fine having a blanket incoming block, and still let you perform this request; established traffic is exempt from this block because the firewall sees the outbound SYN packet and the matching incoming SYN/ACK, and ‚Äòwhitelists‚Äô the connection until there‚Äôs a FIN. I guess your network team have a reason for not doing it this (well established) way, but I can‚Äôt see what it is.
`prop` &lt;tab&gt;&lt;tab&gt; One of the best! Any love for `ctor`?
You simply need to define an interface and use di to pass it down to your classes. Then changing is as easy as implementing the interface and changing your registration in the di framework.
Learn the basics of VIm and install vsvim. Can't use an editor with vin emission these days. I've even typed :wq into Word before now...
Ctrl+ something opens up the way for many useful shortcuts. It is good to read about what your IDE can do.
Oh my god I've been using VS for like 10 years and didn't know this. Thank you, sir!
Same as Ctrl + T
In this case it will open a small box for searching for file, classes, properties and methods in your solution. Which means: it can act as a really fast way for navigating your projects. P.S Don't forget about alt + f12 for peak code D.S
I think your reply has just increased my VS productivity by about 10%. Thank you
The order of the words is a bit wrong so it sounds funny. 
Is it acceptable if I know some but not all?
* Ctrl + T Super-easy way for navigation. Get familiar with this - productivity skyrockets. * Shift + Space: newline without breaking your current one no matter where your caret position is. * Ctrl + Space Same as above, but the newline is created over your current line. * Ctrl + W I rebound this myself to Window.CloseDocumentWindow, will make it function like Chrome - closes a tab. * Ctrl + LeftClick Same as F12 - Navigate to definition. * Ctrl + R, Ctrl + R Rename a variable or class name; happens everywhere it is used.
Neat little feature that I discovered way too late: Go to `View -&gt; Other Windows -&gt; C# Interactive` to get a window with a C# language shell where you can quickly test code snippets (or how a certain API behaves) without building. I found myself using this a lot for testing how to format dates or little details you constantly forget and have to quickly try, like: "Wait, does `Path.GetExtension("path.ext")` return the extension with or without the dot?" Lots of potential to save you a little time here and there you would otherwise spend googling or building and stepping through code with break points...
I rebind this to CTRL + P to match other code editors!
These are called code snippets. You can also make the ‚ÄúHello‚Äù portion a replaceable section that will get highlighted after you hit tab the first time. Then you can type to replace it and/or hit enter to ‚Äúcomplete‚Äù the snippet. These are some of the ones I used to have, to give you some more ideas: https://johnnycode.com/2010/10/18/some-useful-visual-studio-code-snippets/
The immediate window while debugging. Pause on a breakpoint and type any code in that window to have that code run in the context of the current breakpoint with access to local variables. With ReSharper installed, Shift+Alt+L will select the current open file in solution explorer. Ctrl+Q to open quick launch. Search for and execute VS commands. With a blinking cursor anywhere in a line (nothing selected), Ctrl+X will cut the entire line. Also, Alt+Up and Alt+Down will move the current selected lines up and down in the source file. Right click anywhere in the text editor of a file and the bottom context menu item has source control commands for that file. 
You can also use forward and back on the mouse!
CTRL + K, CTRL + D does the whole document!
And Shift + F12 is ‚ÄúFind All References‚Äù
You can right click a project in Solution Explorer and there‚Äôs a context menu item for Open Folder in Windows Explorer. 
I have 2 old blog posts about just this! - https://johnnycode.com/2009/12/07/5-visual-studio-productivity-tips-every-developer-should-know/ - https://johnnycode.com/2011/09/12/5-more-visual-studio-productivity-tips-every-developer-should-probably-know/ TLDR: The good ones that haven‚Äôt been mentioned (AFAIK): - Bring up the context menu: ctrl + . - Cycle through the clipboard buffer: ctrl + shift + v - Collapse all code regions: ctrl + m, ctrl + o - Expand all code regions: ctrl + m, ctrl + p 
I was about to say the same use ReSharper it boosts your productivity big time once you learn 10-15% of stuff it can do
People should do what they want but ReSharper often becomes a crutch for people. A paid crutch. When switching jobs, working at home, etc, if you don't have resharper, you will probably be lost and uncomfortable. This is the reason I personally don't like using ReSharper and choose not to use it at all.
Click the folder icon in the Solution Explorer window to get a folder view. The two right-left arrows in the Solution Explorer syncs the explorer to your active document.
Or ctrl + t
You can also type in abbreviations. Like instead of CaseProgramActivityRepository, you can type in caproacrep, or even CPAR. Works anywhere you have intellisense too, so you can also do it in the editor or the watch window.
Ctrl+K+C and Ctrl+K+U For commenting code blocks. So useful for when you're testing stuff, different ways to do things, etc. And you still want to build/test while debugging so you can see your previous code. Just make sure you get rid of all the unneeded comments before committing.
Umm...because you haven't learned how to yet? This is too broad a question and OF COURSE you can send emails in a multithreaded environment.
Totally not what I meant, I know this feature :) What I want is to locate current file in Solution Explorer, without expanding projects/directories manually.
You're trying to send the e-mail asynchronously by using a `Task` in a fire-and-forget way by not `await`ing it. The code has a bug in it causing it to throw an exception which gets thrown into the ether because of the lack of `await`. ‚Ä¶is my best guess because you've provided _literally no information about the problem_. 
If you have ReSharper, Shif+Alt+L. If not, there‚Äôs a plug-in called Find in Solution Explorer. 
You probably have to set Cultureinfo.InvariantCulture somewhere.
Or learn keyboard shortcuts and keep your hands on the keyboard (just the regular: F12, CTRL+F12, F10, F11, CTRL+-, CTRL + and so on will do as a start)
BOOKMARKS, omg so good! ctrl + k,k to set ctrl k,n for next. get your speed up, this is the main reason i dont use vs code. Perfect for those, i need to change here but i need to check this first, moments, especially in big projects. Also resharper, beware tho, once your used to it you cannot go back to vanilla VS!
That‚Äôs wild, I can see how that‚Äôs true, but I don‚Äôt know I don‚Äôt feel dependent on it.
Yes, developing in VS and developing with ReSharper are two different things really. I would say that if you are working in a close-nit team then it's best that the whole team either use it or don't.
You could generate a shortcut as a file and the user could download it and save it to their desktop, or you could instruct the user to create a web bookmark (ctrl d, perhaps) and then talk them through saving that as a .url file on their desktop, or if it's a .lnk shortcut you want, you can instruct the user to create their own. There's no other way, as the browser (or javascript) won't reach out into the user's operating system and start messing with stuff.
You have to look at SQL Plus dot net - you basically write your SQL in the database where it belongs, and then generate your service layer code from your SQL routines. It might be the tool your looking for. [https://vimeo.com/317582635](https://vimeo.com/317582635) &amp;#x200B;
Appreciate the response, thanks. I had a feeling that was probably the case but figured it's worth a shot!
I'll check out the extension, thanks!
One of the things that helped me most was playing around with VS's built in ASP.NET MVC Templates. Calling them template's doesn't really do them justice as you can scaffold a fully functional MVC site with a bootstrap frontend and entity framework database in a few seconds. File -&gt; New -&gt; Project (or ctrl+shift+n). Then pick "ASP.NET Web Application (.NET Framework)", which I assume your using, it's under Visual C#/Web. You then get a wizard to choose the template. Pick MVC and Change the Authentication to individual user accounts - this will set up ASP.NET Identity and add in all the code to manage user accounts. I found that particularly helpful as it's a good example of how the various different components of ASP.NET MVC work together.
They are called "Constructor" for a reason. 
Any great tutorials you found extremely helpful? I could use some of that right now. 
For the MVC basics I found Microsoft's virtual academy tutorials useful, they're a little old now and about to be retired but at the time I preferred them as they were less specific/goal oriented than those on pluralsight and helped me more for getting a handle on the general ideas. https://mva.microsoft.com/en-us/training-courses/introduction-to-aspnet-mvc-8322?l=nKZwZ8Zy_3504984382 That said I've not looked at any beginner stuff on pluralsight for a while so it's probably better now. If you sign up for Visual Studio Developer Essentials (free) at my.visualstudio.com they'll give you limited-time free pluralsight and LinkedIn Learning access (and some other freebies like Azure credit) which is nice too. If you're new to C# as well then I can recommend the fundamentals courses on pluralsight - they're less specific but they're good for giving you an understanding of the language itself.
**Edit-and-continue.** You're stopped at a breakpoint, and see something wrong in your code. Don't stop the application, edit your code, and re-start - just edit your code while execution is paused at the breakpoint, and then continue with your code changes in effect. https://docs.microsoft.com/en-us/visualstudio/debugger/edit-and-continue?view=vs-2017
Ctrl+k Ctrl+d reformat your code (sometimes Ctrl+e Ctrl+d)
Use SMTP, it is already the universal interface for delivering email. &amp;#x200B; If you need to use the HTTP API for their advanced features, create a interface that accepts the common parts and write the interface for two different providers at the same time to make sure it is abstracted well.
I can read, group by, and count about 5 million rows with no issues on a fairly small SQL server instance. The trick is you have to have the right schema, varchar filtering, bigints when the value only goes to 100, guids versus PK hurt performance just because of the raw memory bandwidth you need to shuffle the data around. I had some vendor Ids stored in a historical table as a varchar(100), sped up the query by just changing it to a bigint which ran in 1/60th of the time. 
Shift select works like a normal continuous select. Alt select will allow you to select non-contiguous items. It's awesome if you have several lines of similar code and need to change the same piece on all of them. Especially if all of the change text is in a perfect vertical line.
This. Can‚Äôt believe Ctrl + M, Ctrl + O isn‚Äôt near the top. I use this all the time.
Yeah, that because python 2.7 will be abandonned at the end of the year. Use the 64bits version of 3.6 or 3.7.
Disable it for a day, for science
One way is to add post build actions (project properties in VS) and xcopy the files. A more robust way is to use a script that builds and deploys the project in a single run. I made a [generic script](https://github.com/marektoman/deployment-tools/tree/master/scripts) that does that. You can modify it to copy the files selectively. You'd also need to remove the `CopyLocalLockFileAssemblies` validation check. As to what files are necessary, that's something that may differ per version and machine, so I rather copy everything even if it means redundancy.
The Goto Symbol box is insanely helpful.
Run to line of cursor Ctrl+f10 (instead of setting a breakpoint and hitting f5 while debugging) Ctrl+shift+f10 set next instructions while debugging. This has changed the way I work everyday.
You need to set the culture info in core, https://stackoverflow.com/a/44888965/1274441 The model binder doesn't use the display format for model binding.
Example : place cursor on the line I want to debug. Ctrl+f10 executes to that line and places you in the debugger. Change something. Place cursor/carret on some line I want to re run. Ctrl+shift+f10 Will set the instruction pointer to run from that line.
In chrome at least you can drag the icon next to the URL directly onto the desktop to create a shortcut. I think it works for ie too. Maybe a little animated gif showing that ?
Yeah, I also have a WriteLine and ReadLine snippets
While this is useful, I want to point out to any beginners reading this that it should not replace a unit test. For example, if you want to make sure your date is formatted correctly, first write a unit test to check the output, then write the code to generate the output. Keep modifying the code until the test passes. Congratulations, you've just done TDD (test driven development). I HIGHLY recommend beginners get into the habit of TDD early instead of having to retrain themselves to do it like I've had to do.
Ctrl+m+l will expand/collapse all regions. 
There is an option under tools/options to keep solution explorer selection in sync with currency opened file. 
Why can‚Äôt you provide the necessary information for us to help you 
[removed]
When you use an anonymous type inside a method (\`var cfg = new { ... };\`), but later you need to use it outside as well, just add a name to it \`var cfg = new Config { ... }\` and pres \`ctrl + .\`. The type gets generated with all properties having the correct names and types.
Map those actions to keyboard sequences. You're not keeping your hands on the keyboard when you do Ctrl+F12, if you don't have monster hands. I have mapped Ctrl+&lt;letter&gt; to allow navigation without the mouse: left (or right) one char, goto left end of line (or right), go up a line, go down a line. I'm old enough to be a fan of the old Wordstar navigation, which became kind of standard for other 80's apps (Turbo Pascal, dBase, the DOS editor). I've implemented most of it, except the define-block, copy/move-block feature because AFAIK that can't be done with the keyboard.
&gt; change their ip address Yeah, this approach is fraught with problems. Also, .net does have a dns class you can use to look up the ip. I forget it right off hand, but it should be very easy to Google.
what i found on Google - Dns.GetHostAddresses i hate this approach 
The ability to drag the (stack pointer I think it is called? The little yellow arrow that tells you what line you're at while on a breakpoint/during step actions) backward while debugging. So if I accidentally setup over a line, or if something unexpected happened, I can rerun the line and step in to the call. 
The top right drop down for a code window allows you to see all methods and properties in a class and navigate to them if you select them. Exception settings allows you to set the code to break on any exception even if it will be caught, which is useful when debugging. You can split the current file that is open to two vertical windows so you can view and edit two sections of the file at the same time (I hardly ever remember to do this when it would be useful). Try not to set a file which will copy to the output directory to copy 'always'. This causes the entire project to rebuild every time even if nothing has changed. Set to copy 'if newer' instead. 
Yes and no. You can kick off a bunch of tasks, manually or with custom version of `Parallel.ForEach`, then `await` the whole lot of them using `await Task.WhenAll` or `await CustomParallel.ForEach`.
Woooowww
That quick refactor shortcut is amazing for lots of things. Unused using? Ctrl+. Referencing a tie that you don't have a using for? Ctrl+. Typo on a type name? Ctrl+. Just wrote a method call to a method you haven't written yet? Ctrl+. Major time saver.
I don‚Äôt think those codes are equivalent. I mean, why would you use TryParse and then throw an exception rather than just Parse, or why would one change that kind of code to ‚ÄúI don‚Äôt care if it parses‚Äù? In the end it‚Äôs just either TryParse (which everyone knows since it‚Äôs included in the framework) or a custom method people have to look up what it does and it‚Äôs quite clear which is usually better. 
You can move it forward as well to great effect. Testing code that branches on user permissions? Move debug cursor forward. Nope. I'm totally not an admin. Pay no attention to the man behind the curtain.
[https://vimeo.com/317703281](https://vimeo.com/317703281) &amp;#x200B;
No, hate it. The methods are too close to the originals and we already have new features to declare a variable inside a "TryParse". I get what you're trying to achieve, but it's counterintutive as it stands. Now, it could work better if you made the method names a bit clearer. ToIntReturnsDefault(); , but I stiil can't really see the value. The whole point of TryParse is that it does not throw an exception, so throwing one means you're using the wrong method. This is really what's wrong here I think. Sorry for the randomness, I was sort of thinking and reading :)
You know you can just write: var maybeValue = int.TryParse(str, out var val) ? val : (int?) null; Right?
Related: You can put some basic expressions in the immediate window when *not* debugging, and Visual Studio will host your assembly in a process and run the expression specified. Only useful for isolated code (mostly static methods with no state dependencies).
I code all the time without it, when I‚Äôm using other editors besides VS. I don‚Äôt really think it‚Äôs that big of a deal. Why would you make your life harder than it has to be? Use tools and resources to get code out faster and add value to the business faster. I‚Äôve worked in other languages and didn‚Äôt have an equivalent tool, I was just fine, I missed my tool because it made refactoring easier. I definitely can see where you guys coming from though .