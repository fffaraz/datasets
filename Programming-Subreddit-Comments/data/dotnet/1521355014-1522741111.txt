I, for one, would expect a very simple reference-based hash [what object.gethashcode does](https://referencesource.microsoft.com/#mscorlib/system/object.cs,96), not the god damn hash of the underlying type (!?!?).
Not sure how your database looks or really much else about your project, but if you're using entity framework, you can just run a query that looks something like: ```var appointments = context.Appointments.GroupBy(x =&gt; x.DependentId);``` which should return you back a list of your dependents, then a sublist of appointments for each one. Of course, you'll have to be a little creative and substitute your own names and such to make that work, but that's the general idea. Then the easy way of iterating through your data would be just to construct a `foreach` loop on `appointments` and doing something with the data that way. The thing you originally grouped by (`DependentId` in this case) would be accessible by `appointment .Key`, assuming `appointment ` is the name of your `foreach` variable. So that would look something like: ``` foreach(var appointment in appointments){ var dependentId = item.Key; foreach(var dependent in appointment){ //do something with your appointment here } }``` Let me know what other info you need
Delegates implement value equality (probably so that removing event handlers is easy to implement), so they also need a custom hash code. This implementation produces bad (though technically valid) hash codes, but delegate equality is quite complex and a better implementation may simply have been too slow in practice.
@the_other_sam Still 2 questions. Do I put the database stuff in Services ? how to you transfer data from one layer to another. I thought you have to use a DTO for it. 
I gave up resharper quite a few months ago, if not a year at this point. I absolutely love how fast VS is without it. I've used Roslynator for most of the missing features that RE# used to offer (https://github.com/JosefPihrt/Roslynator).
Looks great. I'll take it for a spin tomorrow
What about a replacement for vses terrible test Runner? 
You have to use something like SQLite. There exist packages for using SQLite with Entity Framework. https://sqlite.org/index.html https://www.nuget.org/packages/Microsoft.EntityFrameworkCore.Sqlite/
Save yourself the bother with SQL Server CE. It's already been deprecated too, if I can remember correctly. There's a replacement library called SQL Server Local Database or something like that. But that's irrelevant. Just use Sqlite. Package a pristine copy of the database with your application. First time a user needs to save something, make a copy of the original file into the user's AppData directory or wherever and write to that newly created file.
Anyone have a comparable replacement for dot cover?
If you have already created the database in SQL Server and want to use SQLite, as others have suggested, SqlCeToolbox is a handy export tool available as a VS add-in. It's a fork of Microsoft's EF Core Power Tools adds SQLite export. The github page is here and it's available through Nuget: https://github.com/ErikEJ/SqlCeToolbox 
Thanks, but I want to be able to create the structure. The database is part of the source code. So I at least need a set of scripts if I decide to create the DB manually. I used to have a bunch of code which ran scripts. I forget which repository it is in :(
Brilliant! Thanks!
So hooked on C# that is has washed away any of the past hate I had about .net (the Windows only years). Unfortunately I don’t have a new project to work with .net core. So for now it’s just tutorials. 
Check out [AxoCover](https://marketplace.visualstudio.com/items?itemName=axodox1.AxoCover)
As others have said just use sqllite. it is already in the core ALL dll. in Startup.CS public void ConfigureServices(IServiceCollection services) { // Confiure DB Connection and EF. services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt; options.UseSqlite("mylocal.db")); In Program.cs you can create the DB. public static void Main(string[] args) { //BuildWebHost(args).Run(); var host = BuildWebHost(args); using (var scope = host.Services.CreateScope()) { var services = scope.ServiceProvider; try { // Database Context var db = services.GetRequiredService&lt;ApplicationDbContext&gt;(); db.EnsureCreated(); } catch (Exception ex) { var logger = services.GetRequiredService&lt;ILogger&lt;Program&gt;&gt;(); logger.LogError(ex, "An error occurred while seeding the database."); } } host.Run(); } 
So what's the actual solution here? Make a simple POCO with a property of type delegate? Basically a wrapper, but implement all the HashCode stuff in the wrapper?
Ncrunch? Not free though
Oh, never done that before. Would you say thats slightly easier in this particular scenario?
One thing I miss from Resharper is auto closing of quotations, parentheses etc. One I find myself stumbling on every time! Vanilla VS only auto closes brackets. It’s more the tiny quality of life features that I miss more. 
Thanks I will give those suggestions a try
Thanks! Actual code helps a great deal. I'm coding a client, though, in stand-alone c#. But those methods give me something concrete to google!
i think this is a good resource on Web API 2: https://www.apress.com/us/book/9781484201107 I don't think it has anything on deploying it to Azure - but, there are plenty of other tutorials for that. 
There is a shortcut you can use to put the closing parens in. It's shift+0
My favorite thing about ReSharper is automatic disassembly when inspecting classes and methods not in my solution. Are there any alternatives for this?
Why dotnet command are used? I though dotnet commands were only for .netcore and not mono.
Here have some more, you can create and populate a database. Ensure created will only create your DB once on startup. in my code below I use a Seed Script to do things like if (!db.users.Any()) - No users { AddUsers(); } You can Add roles, users, entity rows etc etc anything really. try { // Database Context var db = services.GetRequiredService&lt;ApplicationDbContext&gt;(); // User Manager var uManager = services.GetRequiredService&lt;UserManager&lt;ApplicationUser&gt;&gt;(); // Role Manager var rManager = services.GetRequiredService&lt;RoleManager&lt;IdentityRole&gt;&gt;(); // Environment var env = services.GetRequiredService&lt;IHostingEnvironment&gt;(); // SEED DB Seed.SeedTask(env, db , uManager, rManager).Wait(); }
Just be warned that SQLite is a bit hairy to work with with the EntityFramework if you use ADO.NET to create your database model classes. I have had loads of issues trying to get it to work and it usually takes a bit of fandangling to get it to work. There is a third party add-on for VS by a guy who goes by ErikEJ on GitHub and StackOverflow. He has some guides for getting it setup that has granted me the best results. I'd give more info but I'm on mobile.
Recently went to a .Net conference where some Microsoft employees gave a presentation on .net core 2.1 and visual studio 2017 upcoming releases. Looks like they're redoing the test runner that they're coming out with in the next preview of 2017 (I think 15.7.1?) that is significantly faster, more responsive, and finds new tests with reloading every other test. They tested it with a solution of 10,000 test I believe? (This was last week) and had a side by side comparison. The preview loaded, compiled, ran, added one new, compiled, and ran again before the previous preview build even finished discovering all the tests. I still prefer R# and dotCover or nCrunch for continuous testing, but at least this shows that Microsoft is making strides to improve Visual Studio's oldest pain points. Maybe we'll see continuous testing in the future? One of the presenters flat out said that they're trying to break away from R# and implement the features you would be getting natively into Visual Studio.
That is dotPeek and it's free regardless of whether you use Resharper. [Here](https://www.jetbrains.com/decompiler/)
NCrunch
Now that looks like a real alternative. Thanks I'll check it out.
https://stackoverflow.com/questions/12021198/sublime-text-style-ctrl-p-equivalent-in-visual-studio-2010/43946151#43946151
That does sound like a strange deployment plan, more typical is using group policy to remotely install your package, talk to your IT department. https://support.microsoft.com/en-nz/help/816102/how-to-use-group-policy-to-remotely-install-software-in-windows-server
Thanks!
This has existed in Visual Studio [since a couple of versions back](https://blogs.msdn.microsoft.com/benwilli/2015/04/09/visual-studio-tip-3-use-navigate-to/), but it still was not easy to find the [documentation for it](https://docs.microsoft.com/en-gb/visualstudio/ide/navigating-code#go-to-commands). If you are like most of the ReSharper users I encounter -- I had been a user myself for many years -- you are probably shielded from the many improved standard features of Visual Studio by ReSharper :-) 
&gt; Just can't get used to having to wait for the computer... ^--- This is the main reason I [stopped using ReSharper]( https://www.reddit.com/r/dotnet/comments/85a78y/a_month_without_resharper_has_my_productivity/dvwb7y2/).
Watup! Im currently leaning to build Xamarin cross platform apps. if any1 wants to share tips and knowledge contact me on metcalfecode@gmail.com I need some help haha 
Bruh but how will i uppercase 0 now, bad program
This is what I am finding :( The MS page says that SqlLite does not support the database creation. I want to stick to official tools if I can. The good news is that I am going to encapsulate all the DB functionality in a class, so I think I am just going to use SQL Express for now and make users install it if they want to use the app. Well, user. It only has one, at the moment, and that functionality is almost obsolete now that Twitter does threading natively :)
Yes it is faster, but Visual studio still is slow as sh*t.
Wait for the next VS update. They are finally working on it. 
I use Ctrl + , and it handles finding files and classes just fine.
So do you think a global filter would work to enforce the user be logged in, and then add AllowAnonymous on a few selected classes would work? Also,m I never know when to use Middleware. What would be a good purpose of it?
Not to mention the constant pop ups about being unable to reach the license server every time you switch networks. It wouldn't be so bad if it didn't freeze Visual Studio for a few minutes every time Resharper is re-enabled.
Oh, that's pretty cool. Didn't realize this was a thing already. Unfortunate it's enterprise only, though. Personally I've only used Community for personal projects and my job only provides us with Professional. I feel that this will be the case for most people.
&gt; using the context, having it dispose, then doing it all over again when I needed a new one. I was thinking along the same lines as you and wrote something similar. As I worked on it and the idea evolved realized I could leverage my DI container (Autofac) and create a client that would handle newing up my context and/or resolving the correct service implmentation based on whatever connectivity I had at the moment (LAN,HTTP, etc). Now I just inject IAdaptiveClient and I have access to all my services (which all share a single instance of DbContext). Or if I only have HTTP connectivity AdaptiveClient will inject a client that wraps my service with a call to a web api server. https://github.com/leaderanalytics/AdaptiveClient Has a link to nuget and also several demo projects. 
Nice
I agree that there are lots of areas where EF can improve but I think the need to create/dispose/inject the context is tied to the architecture of .net framework and c#. Not really a flaw in EF. Having said that I would love to see EF become a stateless ORM as grauenwolf has suggested.
I personally haven't seen any performance issues since installing it. 
It does both for me. Either check settings again, or get in touch with me, if you like.
yep that works. that would fall in the category of 
You use C# for code running on the server, like with PHP. (In the future C# (and maybe PHP) will run in the browser as well using WASM. There are already functioning experiments.)
Interesting, thanks for sharing. It sounds like a similar pattern to what I’m using in this project if only a bit more broadly targeted, while I’ve got something separate to handle httpclient lifetimes and requests.
in this case, a poco may be easiest. make the hashcode a random number, and have the equals be Object.ReferenceEquals (the guts don't matter). that would spread it out equally.
Why does an EF context need to be disposable? Because it holds an open database connection. Why does EF need to hold an open database connection? Outside of a long-running transaction (i.e. longer than a single call to Save), I can't think of any.
I have an ASP.NET Core MVC project. It has API methods and React for the frontend. I use MVC part only for server side routing and transforming JSX to JS. My code resides in github and it is connected to travisci and heroku. I think heroku is the cheapest so far and older than those fancy cloud platforms. Heroku can serve the api and spa at the same endpoint. According to first comment, this is a monolith structure but I don’t think so. Because I can just move my wwwroot folder to anywhere I want. It does not have anything in common with the api. If you want to know more about it, I can write more in detail.
Instead of going without R# I've just been going without Visual Studio, but I've always loved IntelliJ so not having to bounce between two separate platforms (I also do Kotlin and Python development day to day) when I need to work on a C# project has been nice.
It's been improved in the last 6 months too. When it first came out it only searched for occurrences of what you typed in, where as in ReSharper you could do `upur` and match `UserProfileUpdateRequest`. It now matches the same as ReSharper making it much more useful than it was initially.
Aaaaaand I have a database. Must have mis-read the Microsoft documentation. Thank you for your help! There's [still a lot of limitations](https://docs.microsoft.com/en-us/ef/core/providers/sqlite/limitations) but this will do for my purposes.
Do heroku support net.core???
There is an unofficial third party build pack for .NET Core, yes.
I didn't know this had been improved. This was the reason my last attempt to ditch R# had failed one year ago. I'll give it another try.
Maybe this helps you: https://www.reddit.com/r/dotnet/comments/85a78y/a_month_without_resharper_has_my_productivity/dvwealu/
Where I can find that build back?
While I agree with you that you can make R# faster than it first appears, it's a fruitless effort. As I posted in another thread about R# a few days ago, I run it on systems that are more than capable of handling it. Still, some of the slowdowns are just unacceptable. They're a result of Roslyn doing one thing, while R# does another. I appreciate JetBrains' products. I think it's great that they're offering Rider as a well-rounded IDE for those who don't want to run Visual Studio. But, I'm *never* going to switch to Rider. It's not going to happen. Co-workers would never sync up with it and I would lose numerous *other* add-ons that make .NET development easier. Maybe someday they're re-write their analysis engine to use Roslyn and R# won't be such a hit, but until that happens: my options are either to live with R# or not.
I'm giving it another try as well, I tried it when 2017 was first released and wasn't impressed. Something that I find myself using a lot is the ["Search Everywhere"](https://www.jetbrains.com/help/img/dotnet/2017.3/match-elements-in-any-order.png) where it essentially does the CTRL+Shift+F search for the text and populates results as you type. 
Do you mean .net or Asp.Net?
Just out of curiosity: - What do you mean by system that are capable of handling it? - And what other add-ons do you have installed?
I've never understood the appeal of auto-closing quotes, parentheses, etc. I have to hit the End key to continue typing a line, which is more of a hassle than just closing the quote or parentheses myself. Am I missing something?
It seems they added decompilation, see [this](https://www.reddit.com/r/dotnet/comments/85a78y/a_month_without_resharper_has_my_productivity/dvwi273) post
That works fine for small projects. I work mainly in a solution with around 2m lines of code and thousands of files. The ReSharper file search is instant while the built in VS file search can't take 10 to 20 seconds to show results after typing. 
Why not?
Check Dotnetify, a framework for MVVM with react for the View and C# (real c#, not transpiler to js) for ViewModels.
Honestly the difference between the two are a blur if you are completely new to the Microsoft development environment. I think I've reached that it is C# with the asp.net framework I want to learn with web development in mind.
I replaced both VS and resharper with Rider some months ago, and it’s been great. Still a few features missing but much faster and *much* better UI. 
Yeah, or DotCover
Implement INotifyPropertyChanged in ToolData?
I mean, $160 as a one time purchase isn't too bad.
Once your license expires you are stuck on the version of the .NET framework that their version supports. My license expired and the latest version supported 4.7.0, then my team updated some of our projects to 4.7.1 and NCrunch wouldn't run tests from those projects. 
A slow startup for me is fine so long as my coding and solution navigation speed is compensated, which it is for me.
I feel like most people don't take the time to learn ReSharper, they just install it and expect magic. It has so many features that once understood, improves productivity quite a bit.
CTRL + T has the option to look for files and members separately, without R#
Oh, I wasn't aware of that. That kind of sucks =/
Sweet! I gotta try it out!
Does all that code really need to be in the same solution?
Sure. It's really just one website and its dependencies. 
I was a bit miffed because I doubt there was anything between 4.7.0 and 4.7.1 that meant I wouldn’t be able to use the extension.
Having client and server code in the same project is absolutely not equivalent to a monolith. For a relatively simple app, it makes sense for them to share a project.
Adding additional analysers versus R# which has to do all of the same work as Roslyn and then the extra analyses they do. 
Did you ever find something for this? I don't have any issue with rolling my own, but if someone has already done it, then hey, I'll happily contribute to building it out with them...
https://elements.heroku.com/buildpacks/jincod/dotnetcore-buildpack
Even with all those things, an i7-7700K, a Samsung 960 Pro and 16GB RAM, VS + R# runs out of memory a few times a day for me. And working in XAML files also starts going wrong after a few hours, causing VS to hang for a few seconds to a minute when switching between files. 177 project solution (yes, we're working on splitting it up). I still don't get why they don't just move R# to a separate process, that would solve so many issues.
My choice of language is C#. .NET is then the obvious choice. .NET Core is really fun and nice to work with. It's still lacking some stuff but development is happening rapidly. I'm super excited for what's to come
If you have worked with asp.net before its not as steep as you would think though. And the stuff that it offers, cover much of what we use 4.6 for today. Just faster. 
The 16GB won’t help you much. It’s a 32bit process, so allocation is limited. Regarding out-of-process, it’s a thing... you can follow this issue https://youtrack.jetbrains.com/issue/RSRP-468407.
I know.
This seems similar to this recent question: https://www.reddit.com/r/dotnet/comments/856v7r/net_pros_and_cons_from_noob_to_novice/
I wouldn't exacly call .NET Core new since it is a continuation of a very mature techlology with lots of mind share amongst developers. Speaking as a mostly NodeJS developer, I would use .NET core as a back-end if my app was strongly dependent on a RDMBS like MS SQL Server. I don't think anything comes close to Entity Framework when it comes to DB abstraction.
From the way you asked you should probably go node or python. They're much better for the small learning developer and will have a much better ecosystem there to speed you up. There are still significant merits to picking up c# on top of that... Or if you do the node route, typescript (though not at the start).
C#. With NodeJs and Python, you are fine until you need to maintain it.
It's slower and seems to need to have to index every time it's invoked, meaning if you type what you want and press enter it will open the wrong file, but if you'd waited a second longer the one you actually wanted would appear. Resharper has all this in an index already.
&gt; Python has a very specific origin audience and is not without problems in larger projects. The larger project point I can see, it's developing tools to help with that (optional type hints, kind of like Typescript). But what origin audience are you talking about? Python's used for tons of stuff. 
But most of the stuff is very limited in scope. It was originally used for parsing text, and it is now used a lot in data science - but it lacks the vertical stack for full enterprise scale projects. Which means you never use "it" but "it and a lot of other things". And wide stacks are problematic (hiring wise). Being used for a lot of small isolated things is not the same as being a platform that allows you to handle a ton of different requirements at the same time. Which is where dotnet (not dotnet core - dotnet as the ecosystem) shines.
I got a special deal on my SQL Server license for my co-hosted server. I'm seeing that end, doubling my rates. That's fine, but I'm still wary of the back-dues. I'll check to see if I can get the original wording.
I started mine as I got my bizspark nearly 10 years ago.
I think Visual Studio is still untouchable by other IDE's, it just works out of the box, you don't have to mess around with config files, packages, you press new project, hit F5, your site is already running. Apart from this, each stack has it's advantages/disadvantages.
I went to conference many years ago at Google to listen to Guido talk about the future of Python and Python 3.0. You could hear the sighs and feel the dread in the audience even back then. And this is why you have a choice of Python 2.7 or 3.6 even today, like 12 years later. Python isn't a very good language, but it does have a good ecosystem. That doesn't mean I want to play in it. And then there's node. x = 1 + '1' // 11 x = 1 - '1' // 0 No thanks. You say .net core is sophisticated and has a steep learning curve, but have you really delved into the npm/yarn/webpack/babel/gulp/grunt/eslint world? That's a mess.
&gt; but it lacks the vertical stack for full enterprise scale projects. Which means you never use "it" but "it and a lot of other things". That's news to me. I'm sure it's news to lots of people as well. What exactly do you mean by that, cause from my perspective it's not only possible to build large scale projects with Python, it's been done countless times (open stack, Instagram, etc). 
You say "more sophisticated" like it's a bad thing. Some of us are building software that will be in production for 10+ years and are much less concerned with speed of development than with correctness and maintainability.
Yes you are right. I would describe it as a monolith if the client and server code are deployed together.
&gt; NodeJs/Python both seems to be much older .NET has been around for decades. That's like judging Python as a language based on the [PyPy](http://pypy.org/) implementation, while CPython has been around for much longer. &gt; whereas .NET was fully proprietary framework How often do you really, truly look at the source code for the Python/Node interpreter? Yes I appreciate that it's now open source, but it's entirely possible to write open source code on a closed framework. Also, I can't find the official post but [this SO question](https://stackoverflow.com/questions/734892/asp-net-mvc-is-now-open-source-is-this-a-good-thing) implies that ASP.NET, the primary web framework for .NET, has been Open Source (with a license and everything) since *2009* - 9 years ago.
NodeJs + typescript, tomorrow is nodejs+flow, or es7, or whatever, every year your code gets outdated by a new tech stack or new set of features that other languages have had for centuries. I'm not even talking about frameworks and the open source drama of express and what not. If you need to transpile the language it self with something like typescript, why dont you just pick a good language to begin with?
&gt; Except they do not. They use Python as one langauge. Or do you tell me that the Instagram ANdroip [sic] App is written in Python? I never said they're strictly python operations - openstack *might be* but there's probably a fair amount of C when needed as well. I only said they're things that come to mind that have large python code bases. Also, you could write a android app in Python, it just requires some finagling. I know Beeware has projects for this and I think their method is compiling python source to jvm bytecode. &gt; Plus, just because dotnet allows yo udo mostly use one stack for a large enterprise app, does not mean enterprises do it. Plenty of them are run by apparatschiks, ignorants with an agenda "open source only and freedom for apes" or thigns just are built over time. Keeping the stack small requires more effort than most large companies are willing to extent. What. There's so many typos and rambling here I don't see the point you're making. I feel like your making a snide remark about open source though. &gt; But please, you tell me openstack or instagramm are ONLY writte in Python, proove it. Term here is ONLY. At least 95% including mobile apps. Get off your high horse and realize that not everyone uses exclusively one language for everything just because you do. And again, I never claimed they were exclusively python operations. 
Because I'm not an old VB 6 programmer longing for the days before we had multi-threading and robust type systems. With Node you can't even do really basic stuff like "return this data to the user while you perform complicated calculation X on a separate thread" without a bunch of clumsy work-arounds. And don't get me started on the quality of their tooling and libraries.
When I had resharper I didn’t need to hit enter it just added the closing quotation etc and I continued on typing. 
Yeah I’ve noticed it seems to do it sometimes but not always, I’ll get used to it until I eventually get a license for resharper. 
&gt;Python isn't a very good language I mean, that seems pretty subjective, no?
You didn't make a point though, your middle paragraph is so full of typos that it's impossible for me to decipher what you mean. If you think I'm so how opposed to using dotnet core, then you're sadly mistaken. I actually really, really like dotnet core and C# in general. But that doesn't mean I'm going to use it for everything. &gt; Wish you would have learned basic logic. [k](https://media.giphy.com/media/xT77YaaaHg19gao6OI/giphy.gif) 
If its the same people who changed the publishing dialog windows from 2015 to 2017, I hope they learned some lessons. 
&gt;stop fighting the IDE it was designed to work with Its odd, but slowing down VS seems to have worked out well for them.
&gt; x = 1 + '1' // 11 &gt; x = 1 - '1' // 0 That's disgusting. &gt;npm/yarn/webpack/babel/gulp/grunt Yet this is worse. I tried to set up a decent web dev environment in an offline machine for internal web app development, and after a day or two of messing with these, I wanted to die. Meanwhile, I can just nupkg a NuGet package and all its dependencies, copy it over, and be going in minutes. 
For reference: public struct ReferenceEqualityComparer&lt;T&gt; : IEqualityComparer&lt;T&gt; { public bool Equals(T a, T b) =&gt; object.ReferenceEquals(a, b); public int GetHashCode(T x) =&gt; RuntimeHelpers.GetHashCode(x); public static ReferenceEqualityComparer&lt;T&gt; Default =&gt; default; } ... and in the constructor for the concurrent dictionary: new ConcurrentDictionary&lt;EventHandler, ...&gt;(ReferenceEqualityComparer&lt;EventHandler&gt;.Default)
If it was in widespread use, most of that 13MB would be cached. And 130MB sounds like a normal Chrome tab. YouTube.com is 150MB for me.
Check out System.Linq iEnumerable https://msdn.microsoft.com/en-us/library/bb503062(v=vs.110).aspx?cs-save-lang=1&amp;cs-lang=csharp#code-snippet-2 BTW, the reason you're having a hard time is because it's terrible code. Just because someone can jam everything into a one-liner doesn't make it good. 
I've always been told that a new assembly/dll should be created if the logic is easily reused else where and stands on its own. So for example if you had a namespace that was responsible for reading a writing a special file type and you wanted to support this file type in other applications, then it would be reasonable to place that into it's own assembly.
 &gt; and it works This does not look like valid c# but maybe I'm wrong. Perhaps you could try something like this: string.Join(Environment.NewLine, dt.AsEnumerable().Take(100).Select(x =&gt; x.Field&lt;string&gt;("ColName")).ToArray())
Not that it makes a huge difference, but I'd go with making it a class instead since 99% of use cases will box it anyway and actually cause more allocations that way. Also probably constrain `T` to `class`. It doesn't even really need to be generic since `IEqualityComparer&lt;T&gt;` is covariant.
It's VB.NET, not C#.
The brackets allow you to use reserved keywords as identifiers in VB. I'm pretty sure they are pointless in this context. The bit after the `Select` is a [lambda](https://docs.microsoft.com/en-us/dotnet/visual-basic/programming-guide/language-features/procedures/lambda-expressions).
It might be true, but I my case I had used ReSharper for a very long time. The way we developed software changed, which meant that how I used ReSharper also changed, until I didn't need anymore. Not saying that it's a bad product, just that it was no longer something that I needed anymore. 
Move the shared code into a class library. I usually put all my business logic in its own project within the solution so that it can be shared with any other projects in the solution.
This is probably more system resource dependant than I realized, I run a nvme SSD with 32gb of ram and a quad core w/ HT pc. I don't ever see lag.
Separate assemblies are compiled in parallel and give noticeable boost to compile times since unchanged assemblies are not rebuilt.
 Those package repositories are usually considered much better than NuGet. I would suggest trying them again. Even microsoft (take their global tooling) is constantly playing catch-up to npm's systems.
electron is only for native apps for desktop, it doesn't have to do with wasm.
By who? And where can I obtain their intoxicants? Yes, Microsoft is taking inspiration from Node. But for the most part, they're doing things the right way, rather than whatever the fuck the node people are doing.
You could sign up for a free year with AWS and then host your website there. Not sure about your database but you might be able to host it on the AWS server too.
 You do know that Yarn runs off of npm, correct? It's a tool on top of it to make interfacing between developers easier. And... what? Normal version pinning has been a thing since node existed. "@angular/core": "~5.2.0" &lt;-- point to any updated minor version (5.2.1, 5.2.2, etc) "@angular/core": "^5.2.0" &lt;-- do any major release (5.3, 5.4, 5.5) "@angular/core": "5.2.0" &lt;-- only do version 5.2.0 Do not change no matter what. "@angular/core": "latest" &lt;-- keep up to date.
 And what issues do you have against npm? And npm != node
Saying NPM doesn't equal Node is like saying NuGet isn't .NET; while technically true, in practice its a meaningless distinction. As for NPM itself, where should I start? Maybe when they managed to destroy servers? Or something less dramatic like shoving multiple GB of packages into the Windows roaming profile? Have they figured out that you can include the version number in the folder name yet? Or does it still panic when you need 2 versions of the same library?
 How critical is your site? Do you need SSL? There are some questions you need to ask yourself. For basic use, if you use .net core, you can host on linux, and you can upload your site to say digitalocean, which is only $5/m for small sites. You will want to host your DB in a high availability environment, such as Azure.
&gt; Don't remember if there were still some proprietary pieces or not in the Core implementation. Nope, which is a real pain in the ass when you need stuff like access to geospatial types in SQL Server.
 &gt; Have they figured out that you can include the version number in the folder name yet? Or does it still panic when you need 2 versions of the same library? What? Everything is based off your package.json file, just like nuget. If a version exists in your node_modules that shouldn't be there, it's removed. I'm confused by this. By &gt; As for NPM itself, where should I start? Maybe when they managed to destroy servers? Or something less dramatic like shoving multiple GB of packages into the Windows roaming profile? I assume you mean this: https://www.techrepublic.com/article/series-of-critical-bugs-in-npm-are-destroying-server-configurations/ ? If so... a pre-release version breaking things, that's not new. Not good, no, but... it's a pre-release version. Why would you install a pre-release on your server.
I don't get that. I love TypeScript but don't care for Angular.
Those pinning approaches do not pin the dependencies recursively. You have to use npm shrinkwrap or whatever the new version of that is now that came out in 5.0. 
&gt; &gt; What? Everything is based off your package.json file, just like nuget. What if your collection of projects ("solution" in .NET terminology) has multiple projects and some of them need different versions? While I admit that it isn't a great situation to be in, it is a very common one.
I am currently a .net Architect / Developer. I am the leader at my company which has 7 developers. We have code from 15 years ago still running... I have been doing ASP.Net since 1.0, 18 years. The first thing I would think about is what is the life cycle of the web application? Do you expect to be working on this web application 3, 5 or even 10 years from now? Or are you going to code it once, deploy it and forget. This is the key thing you need to think about. If you are going to code it once and deploy it, use Angular, Vue or whatever framework you are most productive with. If you plan on working on this five years from now, you may not want to use a full framework like Angular and stick to more traditional MVC rather than SPA. When I architect applications that are designed to be enhanced / used for an extended period, follow two rules. 1.KISS (Keep it simple stupid). Never add complexity because the cool kids are doing it. If you don't need async, dependency injection, logging etc... Don't add it. 2. Keep code out of the presentation layer / client. You just don't want lots of code in Angular or Vue or even in the C# controllers. Because Front End code always changes. There was web forms, then Flash / Silverlight, Sharepoint, JQuery, Knockout.js and now Angular. But a C# class library written 15 years ago still works. If you want it to last or plan on working on it in the future, stick with the Core Tech. C#, SQL, HTML, CSS and JavaScript (or TypeScript). TypeScript is the exception because it is too good not to use and it makes the JavaScript so you can go back to JavaScript if TypeScript goes away. So yeah, go ahead and re-write it. But be very careful what framework, if any, that you use. Make it simple. Make it work.
or wrap your c# code with powershell cmdlet classes
Powershell Core
Pretty useful as of January 10th: https://blogs.msdn.microsoft.com/powershell/2018/01/10/powershell-core-6-0-generally-available-ga-and-supported/
Off the top of my head I don't know, and a cursory googling doesn't bring anything up. Experiment and find out yourself?
Yes, but can't use Electron without NodeJS. When wasm get ready, it will create new solution for native apps for desktop which does not touch NodeJS.
Unless there's a nuget package for PowerShell Core's System.Management.Automation namespace and classes (there isn't a .NET Standard/Core library that I can find), there's not really an east way to build a .NET Core application using it. Whereas OP's library is already using .Net Standard 2.0
Depends on your situation. None of them is perfect for every situation.
but both are totally different things.
Ok... Not only does that no make sense, but you're just googling things now. Shrinkwrap hasn't been around for almost 6 years.
That's the same as nuget then, lol. NuGet is project based, not solution based. You can use the solution to manage individual projects, but they can all be out of sync (and that is because of VS anyway, nothing to do with language/nuget), and have multiple versions of a single package across your solution.
It also means using PowerShell.
See now I am confused again. I was under the impression that asp.net was a framework that interpreted your code when compiled
&gt; since unchanged assemblies are not rebuilt. ..unless they depend on assemblies that are rebuilt. Changes in "central" assemblies will often mean rebuilding most of the code.
Yeah, but PowerShell isn't great. Especially if you're used to Unix-style shells.
Thats actually a pretty old article. 2015. I wonder did they ever change it
The last commit to the project was 1 year ago.
https://www.smarterasp.net
So, wait. Your users can log in, copy the URL with a session string in it, paste it into another PC, and be logged in? - Why is any kind of session info in the URL to begin with? - Do you have control over the website code? - Do you know what framework the website is written against, are you sure it's .net?
The system before that ran unchanged for almost 20 years.
1. This is standard ASP.NET SQL State Session management. URL's end up looking like https://our.domain/(S(wauazwvqvcdwrktpo5gg1ztn))/SomePage.aspx 2. Yes, we are looking at upgrading some *old* implementations 3. Straight up c# aspx 
Wow, you are incredibly daft. Can't even read and comprehend the first word of the post. **Intel**
A master of comedy as well, impressive.
Yeah I can't help with that site. Most modern frameworks I've used, including non-dotnet frameworks, don't store session information in the URL. That practice died off years ago. I would recommend upgrading it to asp.net core, if you can. Or, move the data to cookies at least. I know asp.net core creates a signed (and IIRC encrypted) cookie. I would definitely recommend getting them out of the URL though.
Working with the customer now: their 'policies' didn't allow cookies when these things were first built. Trying to bring them into the 2010's now ;-)
Yes. This is exactly the problem. I was hoping for a softer, gentler fix.
Why are you even unit testing DateTime? Just curious.
I read he was emulating it on a Raspberry Pi for the last few months of his life but I don’t know if that’s true or not.
Intel was sponsoring in later years so even if he did it would have still said Intel.
Why are you doing cookieless session IDs?
Old school policy at the customer. Working on convincing them that isn't best practice anymore.
looks like it was a stopgap to test the emulator which would run on a PC. He wasn’t intended to keep the Pi on board long term but died before they could make the swap to the new hardware. https://www.sfchronicle.com/bayarea/article/The-Silicon-Valley-quest-to-preserve-Stephen-12759775.php
Time depending algorithm call datetime.now. he wants to test one specific path in his algorithm so he wants to isolate that specific path. How do you achieve this? By setting the system.clock to the required timeframe obviously. Or... You intercept that call and replace the return value with specific time value he needs to run the path.
I'd look into [Hangfire](https://www.hangfire.io/), but there are other things like Quartz that would also do the job.
Plenty of RAM won't improve anything, unfortunately, given that VS is 32bit. It's like putting 10 people in a small car :) 
You should see how fast ReSharper (Rider) is without VisualStudio :)
I haven't seen code so I can't exactly visualize it. I might change my methods to be tested to take a TimeSpan and/or DateTime parameters. And maybe construct a DateTime object depending on business logic needed. I don't know exactly but i would refactor my code if it would be useful. 
It helps when you’re running multiple instances of Visual Studio at a time, R# increases the memory footprint a lot even without solution wide analysis enabled.
I suggest you write your own personal website. Use it to learn stuff. That's what I do. I tear down and rebuild my site every year or so. It keeps me on my toes. 
 I guess what I’m struggling with on the user sessions is it seems like he native class for IdentityUser is complicated. More complicated than I need it to be, but I also don’t know how to build my own user class from scratch and everything it needs, like password hashing and all that. I’m looking for some source, book, video series, whatever... that deals with user sessions in details everything I’ve found has either been too basic, or assumes I know more than I do. 
How did you make the images in your readme file?
Try this: https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?tabs=visual-studio%2Caspnetcore2x 
BTW if you figure out how it works please let me know LOL :)
Yeah I've gone through this. This is actually what left me in my current state of confusion, as the prebuilt template for using authentication creates an ApplicationUser model that inherits from IndentityUser, which I don't want to use. I could modify the ApplicationUser model to not inherit, but I don't know what I need in my own user models. I need to figure out how to do all that myself, and not use something prebuilt, because I don't understand whats going on if I use that. I also don't understand how the cookies are being passed from the server to the client, \(get that its done at sign in, but how are the generated, and I think they are stored in a dictionary on HttpContext\) and I don't understand whats happening behind the scenes when it receives the cookie with the user request \(I think it searches for that cookie in the HttpContext and returns the user key.\) It's handling too much stuff that I don't know whats going on, and nothing I've read dives into those details. 
That's right. We use different end point routes for different environments.
Or just use an interface to make your code testable like regular people do.
With Visio (2013)
Consider this as a test class. Its the exact example from post. It test a function doing something different depending if the current time is &lt;12am or after. the PoseContext then isolates the whole thing to take control of function invocation. This can't be done with stuff like Moq, because those Frameworks all more or less are based around the idea of taking control of the entire class by copying the mocked class, creating an anonymous class at runtime and then inject mocking orchestration where the regular methods IL would have been.
Why should you have to wrap every single third party that doesn't provide an interface?
I was actually looking for something like this a couple weeks ago because Fakes doesn't support Core. Good job!
I've been walking through this, this afternoon [https://github.com/blowdart/AspNetAuthorizationWorkshop](https://github.com/blowdart/AspNetAuthorizationWorkshop) it seems to give me a better understanding of whats going on.
&gt; Time depending algorithm call datetime.now. Well, it shouldn’t. It should call some time provider interface that can be stubbed off in test. 
IdentityUser is the EF model used by the identity DbContext, you have to inherit from it. Cookies are sent with all requests not just on sign in. Also the session is stored on the server. The cookie is used to identify which session the request belongs to.
For testability? Just like you wrap http client and entity framework and system.drawing?
Sentences 2 and 3 I understand. Lets talk about 1. I get that IdentityUser is the EF for the DbContext, but do I have to use that model? Can't I create my own user model for EF to map to my DB? I guess it doesn't matter, I can always ignore fields in the DB I don't plan on using, but I feel like writing my own I'd understand what I'm doing better. I was able to add additional properties to my Application user, beyond what is inherited from IdentityUser, I was able to add a username to my view model, add a form group for that property, and change the controller logic to make the username in the IdentityUser model a custom user name. So I know how/can make some rudimentary changes to my ApplicationUser model/entity, but then I guess I don't know what I do with it beyond that. If I were to have a controller to create something; lets just say its an application for keeping notes. pretend my DB has a table for users and a table for notes. the fields for notes are the actual note text, and the UserID. So I have a user who registers/logs in, and then in a Notes controller, AddNote method, I'd have to access the UserID for the user who's currently logged in and save it in the note table, along with the actual note content. Then in the Notes controller, a ViewAllNotes method would pull the current UserID, use Linq to pull all notes with the same UserID as the current UserID into a list\&lt;\&gt;, and display that list of notes on my view. from a 10,000 foot view, is this all following? how would I access that UserId? would it be part of HttpContext?
I'm probably just not thinking this through the same way that you are. So I apologize in advance. But based on what i see, change the method to have the DateTime as a parameter and then create two unit tests. Pass in one DateTime where the hour is less than 12. And then the other test with an hour over 12. Would have full coverage with repeatable tests having the same result every time. So I'm basically stating that I wouldn't use DateTime.Now in my code. Again, maybe I'm just seeing it differently. 
Thank you. I've been trying to state that in a different way. I was wondering what I have been missing. 
Yes, I restored it from a .bak file to the new server.
You can also heck the refere header and if its not the site url reset the session token. But really should fix the issue
I like to wrap up DateTime in what I’ve seen called the ambient context pattern when I find that I need more than an interface. It looks a bit like a singleton, but it’s fairly easy to make thread safe and use in cases where DI is not possible. 
What's the value in setting the field to 0? Why not just check if the current time is &gt; the end date when loading the page, then display according? If you want a timer on the page itself, do it in JavaScript. Obviously you want to make sure bids for an auction aren't being accepted after the end date, so you'd check the timestamp against the submit on the post method. 
While we're talking about DateTime, let me remind you not to use it. Use DateTimeOffset unless you know you need DateTime instead... and you probably don't. [Future you will thank me.](https://docs.microsoft.com/en-us/dotnet/standard/datetime/choosing-between-datetime)
That DateTime has to come from somewhere, though. If the calling code truly doesn't depend on the date, it would make more sense to have the greeting service know about it instead.
What login is it trying to use to connect to the database?
It's using the account your application is running under.
So, my domain account that I'm logged in to the server with? 
It depends on how you configured your application. You mentioned IIS so I assume this is a web app. In that case you'd have to check IIS. The "Integrated Security=SSPI" part of the connection string tells the SQL client to connect using the current credentials.
Yes, it's a web app. I've tried to copy every setting I could find from the old server in IIS to the new. Is there anything in particular I should verify in IIS?
Use the sql profiler. It will tell you everything. 
In IIS, you can check the app pool for your application to see which account it's running as. You can also use the Task Manager. Start Task Manager. Find w3wp.exe process (description IIS Worker Process), Check User Name column to find who you're IIS process is running as. Make sure you can login to the DB as that user. Also, that user should NOT be a sysadmin of that database.
If I understand your question correctly, you'll want a Details (or something) method on your controller that takes an book's id. It queries the database for the book with that id and returns it in a view. public ActionResult Details(int id) { // Code to query database return View(model); } You'll create a Details view. You can link to it from your list view with something like: @Html.ActionLink("Details", "Details", new { id = item.BookId }) But, you might want to do a tutorial that implements basic CRUD so you have an example of this and understand it better. Example: https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/introduction/getting-started 
To understand ASP.NET Core Identity you will need this book https://www.apress.com/gp/book/9781484231494
I know a lot of people use Zendesk due to its VSTS [integration](https://docs.microsoft.com/en-us/vsts/service-hooks/services/zendesk?view=vsts). That said, in a previous place we just made our own integration using the VSTS web hooks which are actually pretty good. Zendesk is pretty expensive and making a portal isn't hard if you want something simple.
Creating a simple action that returns a view based on a book id it was passed in worked just fine.
When populating db with some dev data I like to write a one time tool that imports data from json or csv and pushes it to the db. If you're using entity framework you could deserialize the json file into your models and add them via your dbcontext. Or you could use plain old ADO.NET and write some sql to do this job.
In three words: undefined is undefined.
Lol, like that one time where a npm update broke thousands of production servers by changing permissions on system files.
2nd option seems to be working. I have a question though. Can I somehow make the field ImageLocation in my db get set to images\BookImages\Title+.jpg?
i'm suggesting creating a wrapper class in your c# code that implemenrts the powershell Cmdlet class in System.Management.Automation.dll then you can use dynamic auto complete from powershell.. i dont think this would work directly from bash, you would need to start the powershell REPL first most likely.
AFAIK System.Automation.Management is only available for Windows, as well.
We use [Intercom](https://www.intercom.com), they have a great product called "[Articles](https://www.intercom.com/articles)" (keeps changing) that makes for a nice support portal. Here's our portal: [link](http://support.cab.md/)
That's because NullReferenceException uses interfaces _Exception and iSerializable which play nice when throwing exceptions. 
if you are using cookies this may be an important topic : https://docs.microsoft.com/en-US/aspnet/core/security/anti-request-forgery 
Excellent write-up here. I'm quite surprised at the performance of using Span with `stackalloc[]`.
Lol, like that one time I already discussed in this thread of it being a [pre-release] that people told their [production] [build-system] to update to
Who the hell does `throw null` anyway? Does that even compile?!
Then would you argue that all interfaces are extra, unnecessary code? I think that's a straw-man argument. Wrapping DateTime serves a purpose, just like wrapping any other static call. Why do you think we have ILoggers now? Is Console.WriteLine not good enough?
[removed]
You shouldn’t encrypt your passwords, you should hash them. If you’re in charge of that, find a solution that handles it for you. Don’t run your own
The likely cause of the poor performance with `stackalloc` was a shortcoming in the jit that has since been fixed. See [coreclr/16220](https://github.com/dotnet/coreclr/pull/16220). Will verify when I get the chance. The fix should be in preview2.
Ok, I changed the account in Default App Pool -&gt; Advanced Settings -&gt; Identity from ApplicationPoolIdendity to Network Service and now at least the pages are loading correctly. When I do try to log in to the application, it's still giving an error: "An OLE DB Provider was not specified in the ConnectionString" Do I have the syntax correct in the connection string? Should it be ProviderName? Is there a difference? 
Very nice. I have an app that's doing a lot with byte arrays, where performance *really* doesn't matter, but I want to convert some stuff to Span usage just to learn how to do it. I think these low-level tools are going to go a long way in terms of .NET Core being taken seriously.
I'm not sure about that one, "Provider" is correct. Try removing the ".1" from the Provider? 
&gt; When populating db with some dev data I like to write a one time tool that imports data from json or csv and pushes it to the db. If you're using entity framework you could deserialize the json file into your models and add them via your dbcontext. Can you explain a little how would you approach this? I know how I can deserialize json file with JSON.net, how would I add all the files into the db with entity framework?
Welcome to the fun world of A&amp;A (authentication and authorization). I just got done setting up a Windows AD Domain that syncs with an AZURE AD instance as well as using an AD B2C instance for customer logins so I have had to slog through a lot of what you will need to learn. A lot of your choices depends on your need, since it sounds like a small web application I would look at using either federated login (IE google users/fb users use their own logins, you dont worry about security since you dont do login) or use an example that uses an AD, LDAP or encrypted DB storage for your user info. leverage OWIN for the mechanics you need for user interaction and the meat of your security concerns. tl;dr Try to avoid writing security as much as you can and lean on professionally developed and tested solutions that you can put your logo on. 
Watch some pluralsite videos on authentication.
Take a look at IdentityServer 4. It offers the centralised, federated single sign on you’re looking for.
Thank you, I'm new to .net core development and have been looking for a password solution like this 
If you don't mind using a hosted solution and a prefer a 'let's not reinvent the wheel approach' I'd recommend taking a look at Auth0. The free version is more than enough for most small companies as long as you don't mind user credentials being stored in their databases as opposed to your own (an enterprise offering). It also runs in AWS. It is very easy to integrate with any webapp and the documentation is extensive. It comes with a hosted login portal that you can modify using their editor and add/remove features such as Facebook/Google login at will. 
It is a great question, if you trying to gauge the developers SQL experience level. If you write stored procedures or reports, you are going to see the problem right away. 
Learn the core first. HTML, JavaScript, CSS. 
Not worth it. I hate magic databinding and mapping. I open up legacy project. Do find all references to a model’s property to see where the value is coming from and it shows zero references! MAGIC. For real, just spend an extra two minutes and set property value in code. Make debugging easier for other developers and your future self.
I hate AutoMapper. I despise it. For real. If you have some legacy code and the QA guy says the wrong value is on the screen. You do a find all references to the model property and no references. It just makes debugging harder. Debugging other people’s code is the tough part of the job. AutoMapper makes it tougher. Just my opinion. Use it if you like it.
I *really* wish Visual Studio for Mac had CodeLens (among other things)... I've been spoiled by VS Code at home and Visual Studio at work, and I keep finding little things in Visual Studio for Mac that put me off
This is kind of a ridiculous question. What is your goal? 
Will chime in that Auth0 is a personal preference and very cheap compared to how much time I spent having to manage it ourselves. If you are doing something very simple and basic, it might be better to roll your own with identityserver, but we prefer the ease and reliability of Auth0; never going back.
You're doing more work here than is required for adding the password to a config and gitignore the config. You're not getting anything based on the scenario described in the article.
You can't really store a list of something in a database without a little creativity. Before I go much further, was this ever working with your dictionary of items before or did you just add this particular property to your entity?
&gt; System.Data.SQLite.Core (1.0.108) I'm just starting out with Core 2 and SQLite, so this is my "I'm no expert" disclaimer. As far as I know, support for SQLite is baked into Core 2 and you shouldn't need to install that package. Microsoft.Data.Sqlite (2.0.1) and Microsoft.Data.Sqlite.Core (2.0.1) are included in the Microsoft.AspNetCore.All package. Is there any reason you're not using those?
First thing I do is turn off CodeLens :)
I wondered about that.I have come from using Xamarin, and was using it there. Thanks for that :-)
I would say its slightly easier than using git ignore, the article doesn't detail it's simplicity very well. My biggest issue with it is it doesn't work with test projects it only works with websites... Which makes it useless for anyone doing any form of integration tests.
How so? There's like 5+ steps. Gitignore is one step. No libraries, no extra configs, no editing project files, and no commands to execute.
 1. Add Nuget UserSecrets Package. 2. Right click on Project -&gt; Manage User Secrets. 3. Add settings in text editor.
Why can't we just get rid of this thing and suggest using Rider or Code on Mac?
Competition isn't a bad thing
API testing is becoming a necessary aspect for enterprises of all shapes and sizes. So, it becomes critical to add API testing to your QA processes. Failing this, a business can face a situation where they may find themselves struggling with expenses due to performance issues and delays in production. If done right, API testing leads to reduction in costs, time and efforts. With quicker feedback and results, API testing enables a business to reap maximum ROI. To identify the best scenarios for API testing and how you can achieve the most comprehensive test coverage, join this free webinar, 'The Future of API Testing - Trends and How to Propel Your Testing' on Tuesday, March 27, 2018 at 11:30 AM PST. Register FREE with the following link: https://info.qasource.com/signup/the-future-is-api-testing
Yes but ~~MonoDevelop~~ ~~Xamarin Studio~~ Visual Studio for Mac lacks so many features and is rather unpredictable I still don't get why we put so much effort into it instead of just scrapping it for good and suggesting the use of Code or Rider which are considerably better and more stable. This is just my personal opinion, I don't speak for my employer.
I don't think so. I wrote thousands of sprocs and reports and I only used select into for inserting into new temp table. For inserting into existing table I use insert into ... select, so I don't much care whether it is possible to use select into for table variable - i've never used such syntax for this case, I have however solved the insertion many times without any problem...
other problem with stackalloc is that it zeroes allocated memory
&gt; I feel that for .net devs angular with typescript is a more natural choice Yes, and AngularJS is a great complement to .NET, enabling very rapid development of modern web applications. And many [Angular developers ](https://www.itechart.com/development/web/angular/) I know proved it. When you choose .NET MVC and study theoretically Angular, you should have good future.
If it was called Visual Studio (optional "for Mac"), so why I couldn't install extensions from the Visual Studio Marketplace? 
I'm not sure what your point is. You can just use Rider or VS Code if you wish.
VS for Mac is slowly getting better. The # 1 thing i'd like fixed though is the debugger. I'm covered for front end stuff with VS Code but when I want to do some serious debugging I still need Studio but the debugger still isn't as polished as it should be - nowhere near as good as its VS on Windows.
I feel like the author is just perf testing different serialization methods, rather than the communication itself The author could have configured wcf to use message pack to get a fairer comparison 
Ok my next question to you is what’s the difference between a table variable and temp table? The question had a table variable not an existing table.
Or are forced to...
There are comparisons between ASP.NET Web API and ASP.NET Core MVC, but you make a good point. The problem is that there's no ready to use way to configure WCF to use MessagePack. There's https://github.com/schulz3000/msgpack.wcf, but that uses msgpack-cli, not neuecc's MessagePack library. So I'd have to include a benchmark for ASP.NET Web API and ASP.NET Core MVC using msgpack-cli as well. I'll see if I can make this happen somehow.
I'm with you, I dislike vs for mac, that xamarin app, for a few reasons. I was using rider until recently when it became too slow to use. It has a bug on my Mac that they haven't fixed yet, whenever I start it all my apps become slow, there's a lag with scrolling, etc. So now I switched to VS Code. 
Hmm, I don't think WCF is that hard to work with to be honest. Our WCF interface is a simple .NET interface, and since we don't care much for the flexibility of WCF and settle our applications to assume a specific form of binary, encrypted, transport model, we've simply hard coded the setup of that in our WCF server application. This being done in a DLL shared by the server and the client. So on the client side, we have a single line of XML to simply specify a host name and port. Done. But yeah, the WCF defaults are pretty insane regardless if you look at it from a security or performance perspective, and it's hell to deal with the WCF .config files in XML, not fun if something in your scenario makes you forced to deal with the full extent of WCF flexibility and thus complexity. I'm not sure this is very common though. How often does a web service need to be deeply configurable down to the transport model from XML?
... by an evil warlock.
I wonder which is faster, WCF with netTcpBinding + binary or Asp.Net core + MesssagePack? My bet is on WCF.
I made a switch from SQL Server to MySQL and then later MariaDB and that went well.
I don't know if you're the OP, but if you are, I believe the comment thread (for ref 1) you're thinking of is here: https://www.reddit.com/r/csharp/comments/7y7co5/best_practices_for_messaging_between_2_services/dueb72h/
Yes, we moved a few moderately sized (few hundred GB each) from on-premise SQL Server to Azure SQL Database when we moved from self-hosting to the cloud. We didn't have any real difficulties that I can remember, especially now with first class real support for it in tooling like SQL Server Management Studio it should be a breeze more or less. Some things you need to keep in mind are for example that picking the correct scale/size for your DB can be a bit of trial-and-error and making backups yourself (like, not using the built-in snapshot Azure functionality) is more involved. 
You will need to handle Transient connections. Basically your connection can randomly be dropped, so you have to handle reconnects.
I did about a year ago and it's been great. There are some features that I'm assuming you could do in SSMS like point in time restore but they are super simple in the web UI. I don't need to get too advanced on geo-replication so no comment there. Before I was doing web apps on two VPS's from Rackspace for about $500/month. I switched to Azure and gained way more disk space, better reporting, and was able to make my release pipeline CI with hotswapping the web app for $300/month. The only drawback is SSIS/SSRS. To do those you need a full blown SQL server. Luckily we have one internally I can leverage but to do it only in Azure you'll need a virtual machine and a SQL license.
I choose ASP.NET Core whenever I don't care about security, message encryption, authorization, propagate transactions across several layer of services, I only want to use HTTP/HTTPS, I have time to manually create by hand HTTP requests and types to serialize/deserialize payloads and responses, I don't need to use queues to create asynchronous operations and I feel like reinventing the wheel for everything from error propagation to URL conventions. But yeah, it's faster.
Isn't the article about showing how WCF compares to ASP.NET Core?
How is the process of backups more involved?
How would I handle something like this if I'm using Entity Framework?
Just tried it out, this is cool! 🎉
One might use WCF to create a communication layer between a database (creating an interface to that DB) and then the Model of your MVC website might consume the other side of that WCF connection. At least this was the model we used for a large scale website we wrote 6 years for some project. Performance was good. You might then make a REST API that handles authentication and authorization on of top this, largely as a pass-thru. And use those other technologies that the author listed. WCF vs. ASP.NET Core? I feel like you're comparing two different technologies built for different purposes. Like a nail gun vs a screw gun. They both drive fasteners, but have much different use cases.
No difference that I know of
this is exactly how I combine REST APIs with a WCF-based architecture. REST APIs as entrance point, WCF for the real stuff :P
There’s a link in the post: https://github.com/heemskerkerik/WcfVsBenchmark
I haven't tried using it because my understanding is that its not for production use. 
For EF 6+ You can configure a `SqlAzureExecutionStrategy` to automatically retry when transient errors occur. https://msdn.microsoft.com/en-us/library/dn456835(v=vs.113).aspx
No, I do *not* want to sign up to read the article. Please stop spamming your website on this sub.
It’s funny b cause the built in XML serialiser is terrible. That’s why Newtonsoft wrote his Json package. So how on earth can XML on WCF perform faster then XML from a WebApi? Probably the pipeline ??? 
Newtonsoft’s serializer is great at being able to manipulate JSON in many ways, but it’s pretty bad at performance. This makes sense, because its serialization process goes `object -&gt; JSON DOM -&gt; string -&gt; bytes`. WCF’s XML serializer skips a few steps. 
This might be a good start: https://docs.microsoft.com/en-us/aspnet/core/security/ Sure it relates to a ton of concepts but does a good job in linking to explanations and examples. That said, security and data protection are quite the hard candy: good luck!
I get your point, and it was never the point to compare them as equals. I do think it’s more like comparing tangerines to oranges, because they can all be used to host APIs. 
Best news all day. Been waiting for this for a while. 
I'd be interested to see how these figures stack up against `ServiceStack`.
I don’t think it is, but thanks for mentioning it. 
This has been something I am struggling with, I just managed to get Jenkins working with build and deploy for the existing apps and have been considering containerizing them but everything is 100% azure based and the APIs we are using mean we would not be able to change platforms easily other than to deploy on IIS which is already supported natively and the webapp instances/web apps themselves are already in environmental containers you can control. I can see this being useful if you need to scale and do it across different providers with different systems and deployment models that you dont want to leak into your application configs but even when I have worked at product companies they all will just say "this works on X only".
&gt; Hmm, I don't think WCF is that hard to work with to be honest. It is until you learn to avoid all of the XML config crap. 
&gt; because they can all be used to host APIs. This becomes clear once you dig into the benchmarks and details. I think it's your broad title that begets straight comparisons for the rarest of the rare redditor that doesn't actually read the article. :)
[Same thing on proggit](https://www.reddit.com/r/programming/comments/86aqqm/is_wcf_faster_than_aspnet_core_of_course_not_or/)
Actually I did have to add a SqlAzureExecutionStrategy as mentioned in the other comment.
Yeah, I was expecting knee-jerk reactions with a clickbaity title like this 😊
Nothing makes asp.net core inherently insecure. But it takes more work to get the same level of hardening I can achieve in WCF with an attribute. TLS is about transport encryption. WCF supports oob message encryption. Asp.net core doesn't. Can you make flow the authenticated user across several layers of asp.net core-hosted services? Sure, oob? Not really. Why is propagating transaction across layers a bad idea? Especially in a non-trivial graph of distributed services, the ability to rollback faulty operations seems pretty important to me. About the contract types, in WCF you can generate client and payloads with three clicks. Something that only with swagger we have achieved in the REST space, and still is not the same level. Don't think only about the server, consider also the client that has to interact with it. Consider the ambiguity of the http verbs, whether or not a certain parameter is passed in the URL, in the query string or as payload, the ambiguity of the status codes. About the queues, sure asp.net core can push to different queuing technologies. How about listening to a queue? The point is that a WCF service is transparent to all these details. Your architect defines a contract, you implement it. Amen, it just works. Another developer consumes your contract and they can start working straight away. It might be wrong, but I pay my developers to implement features, not to reinvent the wheel everytime we need to create a new service. Simply WCF removed the time my team spends doing plumbing.
Not usually, unless extremely simple most CSproj files contain dependencies you cannot just open in VSCode. You may want to look at trying Rosylin or some live debugging but that only works if you are on higher versions of .net. If you are running 4.5 and earlier you are stuck until you upgrade at work. Welcome to real software development, we make the intro tools simple because trying to teach you the big iron out of the gate overwhelms people. 
It really depends on the project. On some projects I've used VS to host the webapi only, and then used VSCode to develop a web app that used the localhost JSON webapi to do it's thing. That's really the best setup IMO, but not all projects require that much UI that it's really needed. But you can still try out Browser Link: https://docs.microsoft.com/en-us/aspnet/core/client-side/using-browserlink?tabs=aspnetcore2x It's built pretty much for what you want, but I usually find it annoying and disable it. It might work for you
You are going to have to provide better requirements of what you are trying to do.
&gt;But it takes more work to get the same level of hardening I can achieve in WCF with an attribute. What attribute is that? I've used WCF for a long time, but I'm not aware of a `HardenAttribute` or something like that which magically ‘secures’ your API. &gt;TLS is about transport encryption. WCF supports oob message encryption. Asp.net core doesn't. I guess it makes sense to encrypt your messages when you're sending them across channels that do not have the luxury of TLS. But if you're comparing apples to apples, TLS is more than sufficient for transport encryption. &gt;Can you make flow the authenticated user across several layers of asp.net core-hosted services? Sure, oob? Not really. It takes one line of code. Good enough. &gt;Why is propagating transaction across layers a bad idea? Especially in a non-trivial graph of distributed services, the ability to rollback faulty operations seems pretty important to me. Nowadays, the way to do that is through sagas, not through DTC, which slows throughput down to a crawl. &gt;About the contract types, in WCF you can generate client and payloads with three clicks. Great, if you're only interested in interoperability with other Visual Studio-built applications. If you're interested in interoperability with pretty much anything, REST is a much better solution. Look, I'm not saying WCF is inherently bad, its use case is just very narrow. If you want to create applications that are easy consume from various platforms (anything that can do HTTP, and nowadays that's pretty much anything), ASP.NET Web API or ASP.NET Core are both much better choices. Yes, you can make WCF do JSON and XML, but then you're just shoehorning WCF into something it's not really meant to do.
Dang. Really excited for this, want to give feedback, but kind of don't want to install VS Preview and DNC Preview just for an experiment. Could be worth it, though... at least DNC versions install side by side...
VS Preview also installs side-by-side with your regular instance. It's designed not to interfere. The VS installer lets you update or remove either instance independently. Give it a go :)
I'm not sure they are. 1. there is long term backups for sql in azure as an optional extra 2. default backups are there for 14 days without configuration with 1 minute increments 3. sqlbak.com supports azure db backups 4. you can just script a backup the main thing I realised is how poorly we had some of our indexes - at the start we need over 500 DTUs, but after a week of using the sql query tools/optimisation this dropped to 50 dtus just by rewriting some sql and applying indexes. i wouldn't go back now. 
Alright, you sold me. `git checkout -b blazor`
I'm thinking about starting with the 5 DTU basic server, first. How will I know if I go beyond it? Is it just laggy?
you can view the DTU usage from the portal. If you're at 100% DTU usage you'll get timeouts and the likes. How many concurrent users are on your site?
I would say a few hundred at a time, typical web crud stuff... a little bit of SSRS, but I think I can find a replacement for that.
Not really interested in the ASP.NET portion, but I'm happy to see any progress on .NET WebAssembly. Hopefully sometime soon it will be feasible to run games/other graphical apps on top of WebGL.
Looking forward to playing with it during Easter, this is a new UI technology I wanna be in front of for a change!
This is very, very interesting!
The big appeal for me is guarentees about cleanliness of my systems. I reload my PCs once every year or two because as much as we all wish that apps cleaned up after themselves, there are ALWAYS things that get left behind. Especially on a Linux system where there's a culture of putting all the dependencies in central locations, and the likelihood of having stupid shit left over from old installs is basically a given. With Docker, that is just not a thing. All the dependencies are in one package that if I delete it, everything is gone, no questions. That is especially awesome on things like my Synology NAS that I can't just reload every year or two. I just run everything in a Docker container and I don't have to worry about any leftovers, or odd shit later on because of version mismatches, etc. It also makes all my updates easy because it explicitly segregates persistent data. Sure an app can and should do this anyway, but its explicit in this setup where I KNOW where it's data is because I HAVE to set up the persistent volumes. It's part of the Docker definition. And if I blow away a container, upgrade it, etc., and just point it at the same place, theres no worry about a poorly written upgrade script leaving left overs, etc. for the most part. An upgrade is just a delete of the container, build a new one with the new version, and start it, guaranteed to be clean. And yes, if you want to run it all on a cluster on a commodity host, you can do that, although frankly I'm not doing that. Packaging all the dependencies is a huge win for me too. I know the article says "I've never run into it" but I certainly have. This completely eliminates half the issues with "runs on my machine" vs "doesn't run on the server", which is huge for me personally. Normally I'm pretty adverse to complexity for the sake of complexity too, but I can't disagree with this article enough. Containerization in general is awesome for me.
Let me know in five years time if they're still supporting it. Then I might be interested.
Great stuff but one downside is it looks like they've incorporated probably the worst design decision of C# - the adoption of PascalCase, literally going against every other 'C' type language out there.
I can't wait to try this. Keep up the good work guys!
Instead of asking such a generic question, why don't you come up with specific questions that you might have regarding the implementation of it? You'll likely get more answers being specific than not.
PascalCase = public, camelCase = private. It's consistent to what it's always been. Unless I'm looking at something else.
Except written in C# instead of JavaScript.
is javascript dead yet?
 &gt; How the hell do you think people scaled out before Docker came along? It’s not a new problem. Yes, it is, actually. Were there any scaling problems before? Sure. There are now MILLIONS of users that have constant access to the internet. The more users that join, the more your website has to scale. This is definitely a new issue, and that's why these technologies are coming along to help. &gt; it turns out that the major bottleneck in the system was our database server. In other words, we could have spun up a million Docker instances, it wouldn’t have fixed our performance problems anyway. It sounds like you didn't take the actions you needed to. If the database was THAT much of a bottleneck, what caching servers were you using? None? In-Place-Memory? If your DB was THAT much of a bottleneck you couldn't upscale a few more instances of your website, fix that. Boom, upscaling now means a whole lot more. Now, I will say, people a lot of times think just shoving an application into a container is going to fix everything. It's not. But it's a step TOWARDS fixing the issue. It sounds like you were willing to spend the time (there is absolutely no way it took 2 months to containerize an application, even learning docker and porting would take less than a week for a single person. To put it in perspective, my current company is on 600,000+ files, with 489 projects as of this second, and it took me about 4 days to dockerize our app.) making a container for your app, then didn't fix the issues that were displayed by doing that. It's not a magic wand, and you didn't spend the time to learn why you use it. That's not a fault of the technology. I seriously hate the "I had a bad experience of a technology, here is why no one should ever use it"
 I agree completely. I do agree with manictor that starting an app in a docker container is usually much better than migrating a current in terms of seeing the results and purposes for it, but still. On top of all of this, automatic scaling. Kubernetes can scale without you even thinking about it. How could you not see that as a positive feature?!
send me the codez plz. thx
I use nuget for small projects and bower and sometimes npm for bigger frontend projects. I don't care what they tell me!
You probablyPrefer php_case.
NuGet works for ASP.NET, but for ASP.NET Core nothing happens. I think that's why they said not to use it.
As mentioned in the post, like: https://github.com/dotnet/standard/blob/master/platforms/net461/System.Core.cs
There's more to it than that. I need the typescript definitions for one.
How do you actually use NPM? A lot of people just say `npm install jQuery` but that's not even close to the whole story. That doesn't put the files in the right folder, nor does it add the TypeScript bindings. 
Sure, maybe 15 years ago. These days we have PSR, and I appreciate the style guides laid out by them a lot more than I can say I enjoy the C# standards. While we're (semi) on the topic, what the fuck is with XML documentation? Completely hideous to see XML to document a method.
ah right, well come to think of it I have not used nuget, or jquery come to think of it, with a newer project.
Bower is deprecated in favour of npm. But they (bower) weren't exactly thinking of visual studio, so you're right, there is more work to do when using npm in a dotnet core project. You probably want to explore using gulp or another task runner (though vs2017 supports gulp nicely) to copy the files to the right place (from `node_modules` to somewhere in `wwwroot`). If you want type definitions I believe these are under the `@types` organisation on npm these days, so you'd do `npm install @types/jQuery` as well as installing jQuery itself. If you want something more complex (probably not) you can look at [this repo](https://github.com/UniversityOfNottingham/dotnet-boilerplate) which contains near enough what I use on dotnet core projects, but it is opinionated around my team's standards, so enforces the use of SASS and bundles up modern modular JavaScript through gulp instead of the asp net core bundler. It does however drop in and "just work", and includes jQuery and bootstrap 4.
`npm install jquery` `npm install --save-dev @types/jquery`
It’s not really C# so much as Microsoft. Pascal case has been ubiquitous in Windows development. I also like to think that languages don’t force capitalization conventions on their users.
That fixes the typescript errors, but it's still not the whole store. That puts jQuery in the node_modules folder, which isn't where you actually need it. Yes, I can manually copy the files from node_modules to wwwroot/lib, but that's a very error prone process.
Look at a task runner such as gulp to automate the copying
&gt; &gt; &gt; You probably want to explore using gulp or another task runner (though vs2017 supports gulp nicely) to copy the files to the right place (from node_modules to somewhere in wwwroot). I certainly could, but that raises a very important question. Are ASP.NET developers really manually writing these file copy actions into their build scripts? As in that is the expected way to do things? 
That's how they do it for node js, and I think quite a lot of asp net core was modelled after node. Also means you can source control these tasks so they run the same for whoever, or on a build server. Of course not everybody is using that kind of setup.
Fucking JavaScript developers. Can't even get the god damn basics right. Rather than an actual package manager, they just have an auto-file downloader and expect you to write the rest of it. Sigh, thank you for your help and the explaination.
Is React/Vue achieved via Web Assembly?
Honestly? I just handle them manually and they go into source control. I just really don't feel like bringing in another dependency like npm for something like this. Nuget works great and makes total sense in the .NET ecosystem. But even when front-end deps were on nuget for MVC I'd still move them around in the project after they'd download and I'd add them to source control. I set up a project the other day that didn't have a ton of front end dependencies, just jquery, bootstrap, lodash. I added them manually, got them in git, set up less/ts compiling and bundling with web essentials and it honestly feels pretty great. I guess if you have a more complicated workflow bring in npm/webpack etc. But if you're just looking to bring in something like jQuery, use a cdn or add it to your project manually.
What did everyone do to deal with no more SQL Agent, rewrite jobs in powershell or whatever the Azure scripting language?
And here I am still working on web forms apps. 
Don't use nuget for front end packages. Just don't. Use node or if it's really just jQuery, I like the manual CDN reference. Alternative answer: "don't." You don't really need jQuery these days.
I'm not sure this going to work? I'm used to taking a 15 minute coffee break every time I have to run *npm install* or *ng new* to create a new project. Running *dotnew new* basically takes no time at all. I also just bought a new 512 GB SSD drive to hold the redundant copies of my 100+ MB *node_modules* folder that every project requires, even if it's a simple 'app' that displays *Hello World*. What am I going to do with all the free space that Blazor doesn't require? Farewell Typescript, you were a better Javascript... but that isn't saying much. 
Burn me once with Silverlight shame on me
Your typescript compiles to JavaScript. The definitions will give you type checking and compatibility in your project. A reference to the jquery cdn + the definitions is all you should need for it to work on production.
What's your opinion of them? I loved the gist of the RAD approach, but it made me sad to see it's pretty much being deprecated. (Just getting into web stuff)
Look at the bigger picture: jQuery isn't the only library that a project may need.
Why do you feel that way? It seems to me it'd be better in the web tier so you're not making the call to the web API (assuming they're different physical machines) at all. I did something similar with storing models in Redis and implemented it before the API call to get the model. That way if it's already in Redis I was never making the call to the API. If it wasn't, I'd get it from the API and store it in Redis for future requests. Each item was very small however.
But it's there only one you asked about.
It depends on where the costs are. You cache to avoid costly operations. If the cost is in getting and serializing the object, and it doesn't change often, that's an obvious case for caching in the api. If the cost is in transferring, and the client is guaranteed to be a browser, cache headers are a good choice, otherwise perhaps a client side cache. Or even a combination of the three. They all have reasons to be used. Need for cache invalidation is another concern to keep in mind.
It's what I'm most comfortable with. There's a lot of things that are frustrating though, mostly trying to not write boilerplate code and deal with quirky asp controls. There are some upsides, like Ajaxcontroltoolkit comboboxes (which I have yet to see a combo box that is plug and play like that anywhere else except Access and winforms). Once you get a mature workflow down, it's hard to look at anything else the same, but I eventually want to switch development of our new dev projects to MVC. I am already using entity framework for some of our projects, which makes things cleaner for me. I'm about to graduate in may, so after that I'll really switch my focus over to MVC/EF and perfecting that workflow. I would say learn webforms to get your feet wet, but just view it like you're passing through.
Without jQuery, would Asp.net Core's model validation at the frontend still work?
You and the Blazor team are changing the world for the better for so many front end developers. Thank you, thank you thank you. "I just want to tell you good luck. We're all counting in you."
This is fundamentally quite different from Silverlight. WASM is an establish standard.
Possibly not. I have not used it, but then again,I don't do much pure mvc these days, moved over to web api, where validation is a different beast. At any rate, definitely don't use Nuget for this! It's really not well designed for front end packages. It's great for back end stuff, but it gets really messy for front end.
Read up on the package.json file. And gulp. Download a boilerplate and dig around in it. 
And skip jquery unless you absolutely need it. 
&gt; Phrases like "I’m sending 100 very simple objects (a single property that is a GUID) to the API and it’s sending 100 items back." are kind of open ended about whether that means 100 requests or 100 items in one request. I’m sending 100 items in one request. Doing 100 requests per iteration would not be very useful, as I’m trying to see what the impact is of larger requests. 
No, it's a jquery plugin that requires `window.jQuery` specifically to be declared.
Is that maybe an old config setting? Microsoft have changed the way you get type definitions several times in typescript's life, with `@types` being the latest. Otherwise not sure.
Sorry if I was unclear - it was more of a reply to comments here and the common meme that WCF is hell always, than a comparison between ASP .NET Core and WCF. I think WCF is more obboxious to work with if all you want is a web service; then ASP sure seems better.
I don't know about you guys - but as I read I didn't feel like I was reading a dry academic post about the downfalls of Docker. More the potential downfalls of poor process/decision-making and using technology for technology's sake. &amp;nbsp; Maybe that's not the point of the post after all, but that's my takeaway.
Many simple things can be manually included in your project, things like jQuery, Bootstrap, or AngularJS. However, you need to manage their dependencies on your own too (Bootstrap might need some jQuery to perform some things), and any addons you install on top of them (version compatibility). Bower (or similar tools) would manage this for you, but given that it is only a tutorial, feel free to go to the Bootstrap website, download it, and include it in your project. Unless you do something that really needs NPM, I wouldn't use NPM for a tutorial. And given that the tutorial included a Bowser step, then surely you can do it yourself.
Wow. I think this is my most unpopular ever reddit comment.
The simplest solution in your situation would be to use the external CDNs as described here: https://getbootstrap.com/docs/4.0/getting-started/introduction/ Just add the `&lt;link&gt;` and `&lt;script&gt;` tags to your `_Layout.cshtml`. Boom, you're done. No bower, NPM or other package managers necessary. Many other client side packages have similar solutions. If you need to do anything more complicated, with build steps etc, you are much better off following documentation and tutorials specifically about those technologies. You would probably be looking at tools like `webpack`, `babel` and so on. In my opinion, having these client side packages integrated in the ASP.NET templates and, to some extent, tooling, was a mistake from Microsoft. It really has nothing to do with ASP.NET and it has been a great source of confusion. Also in my opinion, you should in general be wary of tutorials and guides that mix different technologies. In the long term, it's much better to learn the different components you need separately and then figure out how to make them work together later. To make a (very) exaggerated comparison: you'll be much better off learning how to drive at a driving school and then finding the best navigator app - instead of printing out that one quirky, outdated, incomplete blog post that describes all the different steps of starting and operating that particular Ford model you have, while also listing the turn-by-turn instructions for driving between Paris and Brussels. 
How do you handle local development with an Azure SQL Database? Do I just need to create additional db's in Azure?
I agree. Redis should be used to store data that is cacheable, static and requires faster retrieval. One more thing that can be done is to partition the huge payload into smaller chunks if possible and store the chunks in Redis and reconstructing huge payload back when needed. 30 to 50 seconds is kinda huge which means JSON payload is really huge which is a case for chunking. 
PascalCase &gt; 🐫 Case. Sorry. 
C# doesn't even care how you write your code. Use Camel Case all you want.
My point is that 'faster' for a web service can't be measured as the latency when handling small, non blocking, single threaded requests. Latency != performance.
As a backend dev, can someone ELI5 what Blazor is and how it benefits you front-end guys? Genuinely curious
In terms of developing, i just have a local sql server running on my dev machine. for staging you can just create a new database. We use elastic database pools - if you have the requirement for several databases they make more sense than for paying for single databases as you can host 50 or more databases in one pool. 
So, simple answer, use a cdn or manually add the files to your project. A workflow I like that's a little bit more complicated is this setup: https://imgur.com/a/68U3D Front-end dependencies go into wwwroot/lib/, they get added manually and they go into git. All front end code/css goes into wwwroot/src/ and gets added to git. I use Webessentials compiler/bundler to compile-&gt;bundle-&gt;minify everything to /wwwroot/dist/ and serve a single minified css and js file. You can use grunt/gulp/webpack etc. but I just really love the simplicity of webessentials and how nicely it plugs into VS. Another nice touch is to ignore the compiled/bundled front-end stuff in gitignore so you're not tracking changes in 5 different files if you change a less or sass file */wwwroot/dist/* */wwwroot/src/less/*.css */wwwroot/src/ts/*.js */wwwroot/src/ts/*.js.map 
You can make WebGL and even WebVR games with Unity3D using C#.
It depends. If your framework is able to handle a million concurrent requests but each one takes a minute to handle (for just the framework) I wouldn’t say it performs well. But then again, maybe so. It depends on your requirements. 
Let's hope so ;-)
&gt; It depends. If your framework is able to handle a million concurrent requests but each one takes a minute to handle (for just the framework) I wouldn’t say it performs well. I agree. If you try to do 11,000 concurrent requests to an endpoint where each request runs a simple table query, then ASP.NET Core can handle it with no problem and each request would average about 11ms (according to techempower benchmarks, using a quad vCPU VM). With WCF's thread-per-request model on the same size VM, it would burn through most of the available processor just trying to create enough threads and doing thread context switching. I imagine you would eventually get long delays and timeouts trying to handle the same number of incoming requests using WCF.
Finally, a decent programming language to replace javascript. Javascript was never meant to do the things people are doing with it today, and doing anything useful in it requires a long chain of dependencies, hacks piled on top of hacks, no type checking, really WEIRD behaviors like copying a variable to another doesn't really make a copy, but instead makes a pointer to the original variable. Some things exist, like typescript, to try and fix a lot of javascript's problems, but again, they're just hacks that are just putting a band-aid on top of the problematic language.
A reboot seemed to fix it. All up and running now!
the future is now!
lol you made some great points there
Thank you so much! I naturally noticed that I should probably not utilize all that extra stuff because it would be a source of confusion. Your driving analogy works very well also! 
Mostly because it's not even true. C# doesn't care what case you use. 
No they both are built using javascript 
You know how everything nowadays is a single page app with 12mb of javascript libraries, talking to an API backend? No more server side rendering. Well, this is the same thing, but in C#/.Net using Razor syntax for the front-end SPA framework. 
As someone who was formerly hardcore into big fat ORMs and NH specifically, but has since migrated to lighter weight solutions (dapper), what's new in the NH world? Looking through release notes for 5 it seems like they added async support and core support, but maybe not a lot in the way of fixing "the reasons why I left" if you know what I mean.
So, using Dapper, are you manually coding your inserts/updates? Because for basic CRUD a full-blown ORM works best IMHO, while for reporting and large-scale data retrieval custom queries using Dapper for instance works best.
I don’t mind using dapper for reads but updates are such a pain in the ass especially when it’s across multiple related entities. 
Not really interested in a Micro vs Macro ORM debate, I understand the pros / cons and differences. I understand that session.Save(obj) in NH does the Update/insert sql for me. So does Dapper Extensions. I'm not really a fan of absolutes, I would even consider using both in the same project honestly (though I've yet to find a compelling reason to do so). I was thinking more along the lines of has NH does anything to reduce the time spent troubleshooting macro orm specific issues. Mapping issues, N+1, things like that. I know how I used to do it, things like fetch/fetchmany etc. My daily workflow when I used NH pretty much mandated also using NH Profiler to get acceptable performance, I don't miss that at all. 
If you start without debugging then you just have to rebuild to see your changes. It saves time from having to restart iis express. Its about as fast as you can get.
Oh I’m not trying to start a fight either! It tends to be when I’m working on side projects that I get bogged down in that stuff - CRUD work is tedious and feels like a waste of time I don’t have when I’m writing SQL by hand. 
After spending two hours with this benchmark, I managed to get "SmallWebApiJsonNet" for item count 0 and 10 down to a mean of 459.4us and 544.9us, no idea of the details below will post correctly. I want to verify a few more things before I post this myself up to GitHub. Initial test results were around 920us for the SmallWebApiJsonNet test with item count of 0, may be my Ryzen with a different instruction set being generated for the RyuJIT. Changes made Singleton HttpClient per best practices Removed async code to reduce overhead (need to retest this theory) Removed "PostAsJsonAsync" to "PostAsync" with StringContent passed in with the _itemsToSend serialized beforehand. (biggest difference, why is unknown, PostAsJsonAsync is an extension method) _url is preformated in the constructor, not each instance removed "typeof(T).Name" and only ran the "Small" benchmarks. Removed the "Func&lt;IreadONlyCollection&lt;T&gt;&gt;" to reduce overhead (WCF test does not have that), only invoked JsonNet tests afterwards. Moved to services.AddMvcCore versus services.AddMvc (Source Twitter, link later) BenchmarkDotNet=v0.10.12, OS=Windows 10 Redstone 3 [1709, Fall Creators Update] (10.0.16299.98) AMD Ryzen 7 1700 Eight-Core Processor, 1 CPU, 16 logical cores and 8 physical cores Frequency=2929687 Hz, Resolution=341.3334 ns, Timer=TSC [Host] : .NET Framework 4.6.1 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.2600.0 Clr : .NET Framework 4.6.1 (CLR 4.0.30319.42000), 64bit RyuJIT-v4.7.2600.0 Method | ItemCount | Mean | P95 | Gen 0 | Gen 1 | Gen 2 | Allocated | ------------------- |---------- |---------:|---------:|--------:|-------:|-------:|----------:| SmallWebApiJsonNet | 0 | 459.4 us | 478.9 us | 39.0625 | 4.8828 | 0.4883 | 79.9 KB | SmallWebApiJsonNet | 10 | 544.9 us | 582.4 us | 45.8984 | 3.9063 | - | 95.18 KB |
That's certainly true, and Unity is great. Still, it would be (even more) awesome to have the option to use my own graphics library and game engine on top of WASM.
We have a multi-tenant application here at my company (using ASP.NET Core as well). You need to work with policy-based authorization.
My team still writes out the sql in stored procs and we use ado.net. It takes longer to code, but it is nice. We write test methods on all our data access before coding the rest of the application. We can test / debug without using the UI. Been doing it that way for five years now with much success. Our queries are all indexed properly, we are only selecting the data we need and our applications are fast. More than a few times we have been able to fix bugs by updating a proc not having to do a release. It just doesn’t take that much longer to code. An ORM really does help make simple crud quicker, but you would be surprised how quickly you can still code with stored procs and how much easier it is to optimize your database queries. Because the database is almost always the bottleneck.
Is this better than EF core 2.1??
I think you can add the [NonAction] attribute on top of a method to exclude it from the api.
On a side note, i wouldnt have the bank and its customers sign into the same system. Give the customers a seperate system with limited functionality. It will increase your security and as a side bonus your ability to scale the app.
It is mature and proven, at least on the full framework. It can map almost any database schema to an object model.
This
A filter, simple as that. Can be applied globally, on a controller base or on an action. Can accept arguments to limit what actions you want filtered. Here's example code: using System; using System.Collections.Generic; using System.Linq; using System.Net; using System.Net.Http; using System.Threading; using System.Threading.Tasks; using System.Web.Http.Controllers; using System.Web.Http.Filters; public class DisabledHttpMethodsAttribute : ActionFilterAttribute { public IReadOnlyCollection&lt;string&gt; DisabledHttpMethods { get; } public DisabledHttpMethodsAttribute(params string[] disabledHttpMethods) { DisabledHttpMethods = new HashSet&lt;string&gt;( disabledHttpMethods ?? throw new ArgumentNullException(nameof(disabledHttpMethods)), StringComparer.CurrentCultureIgnoreCase); } public override void OnActionExecuting(HttpActionContext actionContext) =&gt; Verify(actionContext); public override Task OnActionExecutingAsync(HttpActionContext actionContext, CancellationToken cancellationToken) =&gt; Verify(actionContext); private Task Verify(HttpActionContext actionContext) { if (DisabledHttpMethods.Contains(actionContext.Request.Method.Method)) { actionContext.Response = new HttpResponseMessage(HttpStatusCode.MethodNotAllowed); } return Task.CompletedTask; } } return Task.CompletedTask; } } And then using it: [DisabledHttpMethods("TRACE", "PATCH")] public class YourController : ApiController { [DisabledHttpMethods("BAR")] public string Get() { return "foo"; } }
I've used Dapper and EF in the same project once. Sometimes it just makes sense.
God, I hope so. Tho is the one thing I *actually* am looking for the FE community to rush ahead on
100% agree. My previous team migrated away from EF to straight ADO.NET/SPs as you describe, and the reduction in friction and increase in simplicity/perf were incredible. People look at me like I have 3 heads when I suggest that an ORM is maybe not such a great idea for every project. Not sure why everyone wants to go the ORM route aside from lack of comfort with SQL...
Thanks. Nice to hear other people agree. You are right. People look at me like have three heads too sometimes. 
I used NHibernate and EF in the same project once. Sometimes you just want to watch the world burn.
This MS article may help some, https://docs.microsoft.com/en-us/azure/sql-database/saas-tenancy-app-design-patterns
Are you guys creating SPs to perform updates as well? Multiple SPs to handle relationship child data? I'm curious what your approaches have been, especially for datasets with a large number of columns and child relationships.
Stored procs for everything. Some procs join up to 10 tables with maybe 30 columns. For a parent child relationship you would use two procs if you needed many detail rows. The key is only select exactly what you need. View the execution plan, make sure it is indexed. Every full stack dev I work with uses SQLManagement Studio and checks the execution plan. We use with nolock if the query is for a grid and there are many rows so we don’t block inserts or updates.
Hey thanks! This was really useful. Definitely got lots to learn. 
My project was created only a few months ago. Not sure if any changes have happened since then.
Yes, but that’s rare. Even 30 columns wouldn’t take that long to code. Copy the column definitions from table schema paste the @ front of each one. It might take an extra hour over an ORM, but no big deal. It is tedious work, but worth it IMO. 
Gotcha. Thank you! Appreciate your time and willingness to share!
Anyone knows why Value Tuples are distributed as a nuget package and not as part of the .net framework? 
I think it wasn't included in the full Framework until .Net 4.7. If that's not correct, then a correction will be appreciated. Edit : This SO post seems to have a little more information: https://stackoverflow.com/q/38382971/5063152 
They are in .net standard 2.0. Take a look at https://stackoverflow.com/questions/38382971/predefined-type-system-valuetuple´2´-is-not-defined-or-imported?utm_medium=organic&amp;utm_source=google_rich_qa&amp;utm_campaign=google_rich_qa
Although he asked about .NET Framework. 
so they are rewriting angular in c#
You can use Let’s Encrypt certs for generic SSL, doesn’t have to be HTTPS-specific. What issue are you seeing with applying LE?
Classes are heap allocated. Value tuples are passed by value and stored online. Heap allocation is sometimes undesirable for things like games. 
&gt; Value tuples are passed by value and stored online. They're stored online?
Fixed :p
Thanks for answering with such a simple explanation 
really good read, thanks for sharing
A rule of thumb, when parameters are less than 16 bytes, structs are faster.
My understanding then is that using a struct over a class, when the parameters are small would be more efficient?
This article conflates C# 4 tuples (reference types) with the newer value tulples. 
You're welcome! I learned this back around 2000 when I was on call 24x7 for a website that was "never allowed to go down". Also, with modern compilers and runtimes, you're not losing any significant amount of performance (usually none) by making everything easily readable by the humans. The machines are capable of doing their own optimization.
In the cloud, actually 
You really should, its so easy to get going. I'm working on a project right now.
Don’t get me wrong but web assembly movement is at conceptual level of Java applets or web forms (world again is trying to reinvent the wheel). We should focus on existing problem which is “how to fix java script” and make it faster (again its just my opinion)
... And you basically wrote resume of that article..
Plenty of people are doing that. No reason MS can't do something like this.
Instead of arguing about js frameworks, we will just be arguing about blazor frameworks :)
I'm not sure I agree, while I get it may look similar WebAssembly doesn't work like Java applets and is nothing like web forms. WebAssembly hooks into the existing JavaScript VM that already exists, applets added there own. Applets also allowed calls into native code which WebAssembly does not. I'm by no means a hater of JS but I think if Blazor gets off the ground the question may be more along the lines of "we could do more with JS but why bother?" I'm saying that as a .NET dev and I completely understand not everyone will want to move to .NET or even away from JS and each to their own. In all honesty, I don't see Blazor or any Blazor like project killing JS but I think the JS world may have to accept that competition is coming. 
Sounds great for people who won’t bother to learn angular or react, and choose to stick with razor, and also have no need to support older browsers - so basically non-enterprise web developers. It certainly won’t be the death of JavaScript, because it will only fill a small niche. I realize it sounds like I’m taking a dump on blazor, when I actually think it’s a very clever idea (aside from using razor). It just doesn’t sound like the right fit for anything I’m working on right now since we are supporting IE 11.
&gt; We should Who is we?
Maybe I’ll play around with it if I have time, but the elevator pitch has not piqued my interest...
How is it that you believe you can predict what the people conducting the interview are going to ask about?
It looks nice and fun, but isn't this just trying to solve a problem that's already being solved by several JavaScript frameworks and modules?
These two posts explain it pretty well: https://stackoverflow.com/questions/3881238/c-ssl-with-socketasynceventargs https://stackoverflow.com/questions/5345947/socketasynceventargs-encryption-ssl-vs-custom-pre-transmission-encryption I'm not too keen on any of the answers, though.
I’m full stack developer at my daily job. To accomplish my goal in the project right now it’s difficult to choose proper tool (with all of the js frameworks craziness). And yet in few years there will be more options because other languages will join the web assembly race (I like idea of having other languages in the browser. ) but it’s only my opinion.
I partially agree with You but other languages will create their own solutions which will have other problems ( so fixing bugs per language- new framework)
I think you can extend the AuthorizationFilterAttribute and override the onauthorization method. https://msdn.microsoft.com/en-us/library/system.web.http.filters.authorizationfilterattribute(v=vs.118).aspx
That's true, but Javascript is the worst language. 
Why? 
That would be terrible. No search engines, no font resizing, no help for disabled people, no semantic web, no Ctrl-F, no downloading of pages, no individual stylesheets.
&gt; with UIs done via WebGL. Jesus no. Moan about Javascript all you want, but HTML/CSS is fine. If we start replacing HTML/CSS, we're going to end up with a lot more issues than we'll ever solve. WebGL rendered content can't be made accessible by default, so nobody will. It'll become a non-standard mess. It'll be like the bad old days of flash all over again.
Great feedback.
The semantic web was a stupid document centric concept. The modern web isn't documents, it's UI. Standards can be established for exchanging metadata for search. Font resizing, accessibility, and text searching are common features in UI frameworks and there's no reason to assert that they won't exist. Custom stylesheets is an absurdity that needs to go.
Yes, because desktop and app UI has never had accessibility. This is simply bringing the power of desktop and app UI frameworks onto the web.
&gt; desktop and app UI When rendered using OpenGl, they don't. Try using a screen reader in a game and see how well that works for you.
Ummm, you're aware that almost all modern UI sits on top of GPU APIs, right?
Yes, I am well aware but they also use native controls the OS understands and thus screen readers and the like can "read". Those "native controls" in the web world is the HTML/CSS you want to get rid of in favour of some "webgl" rendered UI. Or if that doesn't work for you, I can equally argue that modern browsers render pages via a GPU API and thus your point is moot. Either way it's a bad idea.
You're basically arguing that the way all non web UI is built is a bad idea. Being locked into a single language and single UI framework is a bad idea. No competition has left us with the festering pile of shit that is JavaScript, HTML, and CSS. This will open up competition between langauges, frameworks, and companies to create better and better UI while still being compatible with any browser.
It seems like it would be ideal to hook up custom auth flows. I followed [this tutorial](https://ignas.me/tech/custom-authentication-asp-net-core-20/) and added custom claims so that I can use the default Authorize attributes as necessary.
&gt; You're basically arguing that the way all non web UI is built is a bad idea. No, I'm arguing that *non-standard* ways of building a UI are bullshit. Huge difference. Nothing is technically stopping you from doing what you want *today*. You could build your website in Unity and host that instead, if you were really so keen. Hell, even Unreal Engine has a web target so there's competition as well. There's a reason nobody does it, though - you take for granted a lot of what the browser does for you and the second you throw that away, you have to do it all yourself. Try writing a 3D engine that's responsive. Try making that engine work with screen-readers. Now make it actually performant so it doesn't die on low-powered mobile devices (or eat their battery). We've been down this road before - with Flash, Silverlight, java and so on. The fact that they were plugin-based isn't the reason why they failed, it's because they just don't work. It's easier for you as a developer developing on your one machine, but it utterly falls apart the second you try and make it work on all machines.
No, they failed because they were plug-in based. They proliferated because they provided more options for UI. Flash provided options for more animated UI and Silverlight provided a desktop like experience (including accessibility). Security, install base, and cross browser compatibility killed them. All of which are solved by WebAssembly. Game engines like Unity and Unreal are poorly optimized for UI because that's not what they were built for. In addition, Unity uses complex transcompilation from C# to C++ to JavaScript that is prone to issues and unsuitable for complex business logic. Many UI frameworks don't use native controls, but still interface with accessibility APIs, etc. And many are being ported to WebAssembly and WebGL. In 5-10 years, the browser will be nothing but a VM. 
Glad you enjoyed it!
Because believe it or not, there is a limited number of topics you need your candidate to know and thus limited number of good questions. Sure, you will never cover 100%, but most of the questions get repeated again and again based on how the technical recruiter is creative. Http for example - I want my candidates to be able to build web apps, therefore I am asking about http protocol - get, post, request, response, headers, F12... It's predictable. I could ask tricky questions, but do I want to? No. If the person reacts, we are good. He understands the concept and is capable of learning the details when needed.
&gt; It'll be like the bad old days of flash all over again. You will bet, that is what everyone is looking forward with WebAssembly.
My favorite quote "The problem with every JavaScript framework I've ever tried is JavaScript". 
&gt; No, they failed because they were plug-in based. They proliferated because they provided more options for UI. Flash provided options for more animated UI and Silverlight provided a desktop like experience (including accessibility). Jeeze, where to start with this. &gt; No, they failed because they were plug-in based. They failed for a whole variety of reasons, not *just* because they were plugin based. Flash had a huge marketshare, most browsers at one point had flash installed - but it still didn't succeed. Nobody liked using fancy websites built entirely in flash - not because it required a plugin, but because it was a poor experience. It tended to have a fixed viewport (i.e. wasn't responsive), it didn't scale well (i.e. accessibility was non-existent) and so on. The fact that it was a massive security issue as well was more the final-nail-in-the-coffin rather than the downfall of Flash. &gt; Flash provided options for more animated UI It sure did and *boy* did people abuse that. In fairness to flash, it wasn't really meant for building entire websites &gt; Silverlight provided a desktop like experience (including accessibility). Because it* only worked on windows* - because it hooked directly into the same OS controls as desktop apps. That's why it had desktop-like experience. For a similar experience today that works cross-platform, you're going to have to hook into the same "native" controls of the OS - but that's not possible without plugins. Luckily we have an abstraction layer that works on all OS's and Browsers - the DOM. i.e. your HTML. &gt; Security, install base, and cross browser compatibility killed them. All of which are solved by WebAssembly. The poor user-experience is what killed them. All of the points you've listed contributed to that poor experience, but they were by no means the only reasons. Flash sites were *huge* by comparison to standard sites. Back when plenty of users were still on 56k modems and the average site was maybe 200kb, a Flash site would be 3MB+. Wasm is neat and the ability to condense code down will mean it's a lot better than flash, but when you start bolting entire rendering engines on top of that - engines that, let's be realistic, are duplicating the functionality of a modern browser, you're going to end up with a huge payload. All just because you don't like HTML. &gt; Game engines like Unity and Unreal are poorly optimized for UI because that's not what they were built for. Yet, the reason they wouldn't work in a browser will apply to *any* engine - large, unsuitable and at best duplicating the same functionality your browser already has. &gt; Unity uses complex transcompilation from C# to C++ to JavaScript that is prone to issues and unsuitable for complex business logic. There's nothing wrong with the *logic*, javascript is more than capable of complex logic and wasm isn't going to change anything in this regard. Honestly, I don't think there is as much of an issue here as you're saying, the only difference between unity today and unity on wasm is one of performance. &gt; Many UI frameworks don't use native controls, but still interface with accessibility APIs, etc. Want to name some? &gt; You can already run Xamarin in the browser, albiet in an alpha state. Ah, you mean Ooui/Xamarin.Forms. Hint: Xamarin uses *native controls* on the platform it targets. It doesn't implement its own renderer, Web/OpenGL or others That's one of the appeals to it. Even with something like Ooui, it's ultimately just building *the same DOM* under the hood that HTML creates. In fact, look too closely and it's [still HTML/CSS](https://github.com/praeclarum/Ooui/wiki/Styling) in the end. Ooui is a fantastic project, but make no mistake it's not going to replace HTML/CSS any time soon. You might get 90% of the way there but you will ultimately have to drop down into HTML/CSS at some point. The same applies to most platforms Xamarin runs on - you can get most of the way there, but occasionally it's just a little too abstract for a platform and you need to customise it. For the web, that's always going to be HTML/CSS. There *will* be a point when you can build a website in something like XAML or some other markup - but it's going to get translated to the same DOM we have today. It's going to be styled using the same CSS we have today. Webassembly is **not** replacing any of that.
&gt; Flash didn't have any real competition because it relied on a plug-in being installed. This sentence doesn't make any sense. Flash had *plenty* of competition - the good ol' browser itself was a competitor, silverlight was a competitor, all sorts of other plugin-systems were competitors. However, the fact that it relied on being installed doesn't somehow make it both uncompetitive and the only competitor? Nevermind the fact that it was bundled with Chrome, the most popular browser out there. &gt; Without competition it floundered and fell behind, just like current web technologies. ....I'd love for you to elaborate further on both of these points. How, exactly, does "no competitions" cause something to "fall behind"? Fall behind what? The non-competition? And how exactly have "current web technologies" floundered and fallen behind? Again....fallen behind *what*? What amazing new tech is surpassing *the web*? &gt; Game engines are a poor fit because game loops are optimized for raw performance and not for event driven UI. Neither of which has anything to do with my point - they can't replace the browser DOM unless they perform *everything the browser does*. That's your issue, not the fact that they aren't event driven. Also, you're [just plain wrong,](https://docs.unrealengine.com/en-us/Engine/UMG/HowTo/EventBasedUI) anyway. &gt; Frameworks would be distributed by a CDN and cashed by the browser. That first load is going to be a *killer*, though. Want a &lt;200ms page load time? Forget it. Never mind that the more CDNs you use, the more chance you have of failure. You *should* be bundling your own assets and hosting it on your own CDN endpoint - which means that resource is specific to your site (and thus one less failure to worry about) - which also means that framework will have to be downloaded and cached by the time a user hits your site. &gt; Many UI frameworks don't use the OS native controls, but still have modern UI features. I've asked you once to name them. I'm asking again. &gt; Again, competition is good. I don't understand why people continue to argue for monopolies. Just because a space is monopolized by a committee that doesn't make it somehow better. Nobody's debating competition, what we're debating is your understanding of what web assembly is - and more importantly, what it is not. It is not a gateway to replacing the DOM. The DOM exists for a reason - it's the standard that ensures all browsers interoperate together. It's the standard that ensures crawlers can read your site - so Google can index it properly. Standards *are a good thing*.
I look forward to followups on this.
Yep, it's hard to beat a declarative UI like HTML/CSS. &gt; UIs done via WebGL Like [this one](https://pbrfrat.com/post/imgui_in_browser.html)? Looks fun 'til you try to refactor your UI over time. 
But Blazor is C#, only one language and only one frontend tech. Javascript is so fragmented, I hate fragmentation, you learn a js technology and the next day it is obsolete or there are a new framework to learn everyday 
But Blazor is Blazor, javascript is knockout, React, Angular, Vue, etc.
Not having too many classes just for only some methods? 
I don't mind it but it's by far not the worse language, having done a fair bit of VBA/VB6. It's certainly better than it used to be and is actually introducing new features to improve itself.
People say this all the time and it's completely bollocks. React, the most popular frontend framework is 4 or 5 years old now. I can't even think of a new framework that's been released in the last year. Unless you're looking for obscure experimental 'one guy in his bedroom' "frameworks" there really isn't a new on ever day. Or week. Or even year. And when there is new stuff it's almost always evolutionary rather than revolutionary.
When you work in a company with an entire IT department which can manually setup a new VM and scale things and do all the scale scripts for you sure... be manual. Else, just use docker.
Do you want to download a .NET application every time you open a website? Cause that's what this is. Everything that makes .NET .NET has to be downloaded on every page. Webasm has no native GC implementation, no native UI framework other than DOM interop which is most of what makes JS suck in the first place. The reason you don't need plugins is the whole thing needs to be downloaded every time.
Javascript is fine, it's just event based and you don't know how to use it properly. 
The DotNetAnywhere experiment compiled to sizes than weren't at all disconcerting. Mono is huge, but no IL trimming efforts has been done yet, so that will change. 
How is HTML/CSS fine? How is it possible that there are people who *still* believe it's okay? --- HTML and CSS are not fine. They're clunky, they're old, they're inefficient, they're not consistent across browsers, they're not intuitive, and they're surprisingly complicated to understand under the hood. If HTML and CSS were fine, we wouldn't have all these different frameworks that exist trying to replace them (React being the most notable one). Why don't we just make a separate markup format for user interfaces and get away from HTML. HTML was intended for documents, not user interfaces.
Unity for UI is not even that bad, actually. Speed test app on macOS and Windows is a Unity app. Not sure about Android/iOS apps... 
Dotnet anywhere had a really limited feature set and 300kb would already be a fairly large library by Web standards and that 300kb was before a single line of functionality has been implemented. There's certainly a place for webasm, but this is a huge price to pay just because the developer doesn't know how to use JS properly. 
That's like saying C# (or Java) is a huge price to pay because the dev doesn't know how to use C++ properly, which in turn is a huge price to pay because the dev doesn't know how to use assembler. 
IIRC the Mono runtime compiles to 4mb before any kind of optimization. Mono is **huge**. If they get IL shaking its bound to look more manageable. 
I'm not really sure I got the why out of reading that... to me there was no compelling reason _against_ it other than it costing the authors organisation money and bringing dubious immediate benefit (though that was mainly because of the failure of not understanding their own systems) The fact that the author or organisation didn't entirely understand the longer term implications of containerisation isn't really a good reason for others to avoid the tech... Is it just a piece saying that you should think before making major architectural changes to your systems, or am I missing something else here?
The pandora box is open, it doesn't matter what the FAQ says.
I sure do, we are old friends all the way back to 1996, shared lots of parties together, but rather not having to deal with it as much as possible. 
I don't use ASPNET web forms anymore, but I must say I miss some of it. The RAD approach was really adapted for quick prototyping and could be learnt by mediocre developers as well. The learning curve for modern web dev is higher.
it's funny because blazor feels like it will excite enterprise developers the most.
Well, good luck with that. You're using a technology for a purpose it wasn't designed or intended for. That could go either way.
No it isn't, but JavaScript in 2018 will gladly accept all the warts written since 1996, and I do know quite a few of them.
&gt; There has to be some better alternative. Perhaps, but WASM isn't it. Nor is creating your own renderer - not if you want performance, integration, web crawling and any number of things that that the current standards work with. You think webgl is any simpler under-the-hood? That different GPUs/OS's don't have inconsistencies, never mind the browsers? &gt; all these different frameworks that exist trying to replace them (React being the most notable one). This has nothing to do with our argument, but I'd *love* to know what makes react special compared to all the other frameworks out there. In any case, if you think the flavour-of-the-month framework situation is bad now, you're going to hate it if they all start doing their own rendering - everything's going to be inconsistent, nothing's going to work reliably across a multitude of platforms, things are going to *break* constantly and users are going to hate it. It won't work.
So it'll accept most of them, so what? You can write shit code in C# too.
But they all start with "ASP", and they're all made by Microsoft. The illusion works for me!
I've got round this before by using Ajax requests to load sections of views. You need controller actions to return each section. In your main view template for the page, use html.renderaction() to bring in the view parts returned from those actions. Once the page is loaded and you need to refresh one of the parts, use Ajax. If you need to get more complex then you should think about doing a single page application instead.
Except every framework is like learning a new language and things constantly change. With C#, you'd finally have some stability. 
&gt; Sounds great for people who won’t bother to learn angular or react Or the one that is created tomorrow, or the next day, or the day after that. Meanwhile, we have a language that has been around for 18 years and is used by millions. It's statically typed, wonderful to work with and doesn't go from version x to version y as a total re-write every year.
&gt; no downloading of pages What is a "page" these days? With things like Angular SPAs, there are no pages, there's a lot of magic going on with hiding divs, animations, massive imported libraries to that use a single function. If I click "next" on your SPA and am on page 12, then click "download page" what exactly am I downloading anyway? It'd be better to just screenshot it, like I would do on my desktop app.
Except JavaScript has basically been pretty stable for the last several years and the frameworks to do front end we apps in C# webasm don't exist, so they'll be in massive flux. You've basically got React and Angular at this point, going through web pack and babel. Even Angular hasn't had any massive breaking changes for a couple of years now.
Angular 2 is 566Kb, so you shouldn't be using it according to your own logic.
We've migrated everything we were running as a job into an Azure Function.
So let's imagine your scenario for a moment. Websites are code and implement their own UI from scratch. Then people start defining standards for keywords, for search engine access, for being able to link into sub-pages, for being able to copy and paste text, simple ways for web developers to apply styles to all elements of a certain type. People might create a markup language, and a stylesheet language as to separate content from styling during development. What's the point? We have all that already with the DOM, HTML and CSS. Having a standard is a good thing. Sure it's not perfect, but nothing ever is. The idea that throwing everything out and starting from raw pixels is the way to go is stupid. 
Minified and Gzipped it's about [111 Kb](https://gist.github.com/Restuta/cda69e50a853aa64912d). 566 Kb still gives you room to keep your app easily under 1 Mb, and angular can cached. Just the runtime for Mono is 4 MB, not counting the libraries or your own code. Not even close.
So after checking out the changes to the new test Runner, I am not convinced. The runner from what I could tell doesn't update test status during the run, and vs build still seems way off in comparison to resharper build. Very unfortunate 😕
&gt; java applets and silverlight had I agree, but in this case there is no vendor lock-in. WebASM blows my mind in this regard, and I'm a "full stack" developer for many years. I don't know where this is headed ... but it's headed somewhere as all browsers support it. JavaScript is a massive headache and now we can run binaries in the browser. "View Source" will be a mess. Where does this head in terms of standardization? I'm surprised they got .Net in there, I thought JIT compilation under WebASM was on the "roadmap".
https://github.com/exceptionnotfound/SampleCQRS Excellent example with related blog posts explaining the various parts. For the CQRS part, I'd highly recommend going with MediatR: https://github.com/jbogard/MediatR Gets rid of all the boilerplate and the built in pipelines make implementing concepts like decorators and event sourcing a doddle. Recently used it in an enterprise API project and wonder how I got by without it all these years.
For a quick and easy solution, try https://github.com/filipw/Strathweb.CacheOutput Supports client and/or sever side caching by decorating controller or controller methods. As for choosing between where to cache, that really depends on what type of application you're building; saas, multi tenant etc. If you're serving the same data to multiple different people, e.g a list of countries, then server side caching makes sense. If it's more client centric, e.g a user dashboard then maybe client side makes more sense there. That's a soft guide line to get you thinking, more than a hard and fast rule.
&gt;Nor is creating your own renderer - not if you want performance, integration, web crawling and any number of things that that the current standards work with. I disagree. You could conceivably create a renderer with a framework around it to add all those features. And, it would definitely be more performant than rendering HTML. &gt;You think webgl is any simpler under-the-hood? Pure WebGL, Yes. I know how WebGL works, but I don't actually know how a web page renders. I probably know more about how a video game renders a single scene than I know how a web page renders. &gt;That different GPUs/OS's don't have inconsistencies WebGL is a standard specification, it's up to the GPU manufacturers to provide the implementation. If they mess up somewhere, it's on them (that or the spec could be wrong). &gt;This has nothing to do with our argument, but I'd love to know what makes react special compared to all the other frameworks out there React uses a virtual DOM to track and maintain updates to the actual DOM. By batching DOM updates and only re-rendering what has changed on a page, it gains a huge amount of performance and makes it harder for developers to write non-performant HTML. For example, there's probably man developers out their messing with offsetWidth/offsetHeight or adding things to the DOM in the for loop who don't realize what they are doing could be trashing the layout of the page and causing the whole layout to be re-evaluated from top to bottom. It'd be much better to actually create an container element, for loop adding things to the container, then set innerHTML to that container than to do a for loop. But is that *actually* obvious, no? The question is why does React need to easiest to do this? Why can't HTML just do it for us? That's why HTML is shite. &gt;you're going to hate it if they all start doing their own rendering - everything's going to be inconsistent, nothing's going to work reliably across a multitude of platforms, things are going to break constantly and users are going to hate it. It won't work False. WebGL is already a standard across the web. It's a single spec that browser vendors have to implement rather than the huge number of different CSS and DOMElement properties. Which means, implement it once, and then web developers can pull frameworks/packages/whatever to implement the CSS features they want and they'll run on all the browsers. No more waiting for the next version of CSS or for browser to adopt some new feature WebKit added. Plus, you can do much cooler things if you use WebGL for rendering that aren't easy with HTML. Such as rendering to texture, applying effects, and then rendering it back into the page somewhere.
&gt; I disagree. You could conceivably create a renderer with a framework around it to add all those features. And, it would definitely be more performant than rendering HTML. What are you basing this off of? Even hypothetically if this were true, for your "framework" to be useful, you'll have to define some kind of document model. It'll almost certainly be a tree-like structure with nested elements so you can create partials and reuse components. Congratulations! You've (re)invented the DOM. &gt; Pure WebGL, Yes. I know how WebGL works, but I don't actually know how a web page renders. I probably know more about how a video game renders a single scene than I know how a web page renders. &gt; WebGL is a standard specification, it's up to the GPU manufacturers to provide the implementation. If they mess up somewhere, it's on them (that or the spec could be wrong). So, two things: 1) You don't know how a web page is rendered, but you're claiming you can *do it better*. Don't you think ignorance is a bit of a pitfall here? Shouldn't you at least research what you're talking about before decrying that it's terrible? 2) For someone claiming they have a good understanding of how a game renders, you've clearly got little experience dealing with graphics API's. They're a *mess*. Here's [a really famous post from 2014](http://richg42.blogspot.co.uk/2014/05/the-truth-on-opengl-driver-quality.html) - see if you can guess who each of the vendors are. Now consider that this is just desktop focussed, for what you want to work you'll need to get all the mobile chipset providers onboard and make everything perfect. Or accept that you - as the framework developer/designer will have to deal with [all the same issues](https://github.com/crosswalk-project/chromium-crosswalk/blob/master/gpu/config/gpu_driver_bug_list_json.cc) that browser vendors deal with today. Today, web developers don't have to worry about things like buggy GPU drivers. It's nicely abstracted away by the browsers who deal with all that. The second you bypass that, **you** have to deal with it. &gt; &lt;React Gubbins&gt; &gt; The question is why does React need to easiest to do this? Why can't HTML just do it for us? That's why HTML is shite. Are you complaining that HTML is shite, or that the DOM is shite? HTML is a *markup* language, it's not meant to represent some constantly-changing document or an application. HTML doesn't "do this" because it doesn't need to - it's just a *representation* of the DOM. React's Virtual DOM has nothing to do with HTML and everything to do with preventing unnecessary DOM updates. A virtual DOM is one way of doing this, but not the only way. It's also not some amazing panacea, either - a Virtual Dom will only increase performance when you can batch a load of updates, like with a decent SPA. It hurts performance and consumes more memory if you're not - this is why, like with everything being discussed, there's a time and a place for each approach. &gt; False. WebGL is already a standard across the web. Ahem. True. Except I'm not talking about WebGL, I'm talking about the next part: &gt; and then web developers can pull frameworks/packages/whatever to implement the CSS features they want and they'll run on all the browsers. All those *frameworks/packages/whatever* are going to be inconsistent. You might fall in love with framwork A because you love its declarative syntax and it has a really neat fast WebGL renderer but guess what - every single package/library/etc. created up until now **won't work with it**. That's the fragmentation I'm warning you about. The second you step away from the DOM, you lose literally everything that came before it and everything that comes after it, should it not be designed explicitly for your framework of choice. In today's world, you can pick whatever framework you like - react, vue, angular, all them bad boys - and pull in any additional libraries you want. Find a nice datepicker? Throw it in. It'll all work. In this "new and improved" world of yours where frameworks handle all the rendering using their own markup, all that disappears. You end up with a completely closed ecosystem, one that's a nightmare to maintain, won't work across nearly as many devices, is slower and just a worse experience for the end user. But hey, *your* life will be easier - you won't have to learn HTML.
Faster how? In reality all the talk of speed increases that I've seen have been vanity metrics. 
The entire Blazor Hello World app shown [here](https://youtu.be/MiLAE6HMr10?t=2408) was 326kb. That's not gzipped, not minified, includes bootstrap, and uses a version of the .NET runtime that wasn't designed with this use case in mind. And yet, [commonly used javascript frameworks](https://gist.github.com/Restuta/cda69e50a853aa64912d) are already heavier than that. Of course, you're missing something much more important: caching is a thing. Once this framework is production ready, I would expect the runtime wasm/js files to be hosted on Microsoft's CDN.
I agree. However, some of our code behind files are upwards of 800 lines. To me, that seems huge for a single page. Mvc just breaks things down to manageable levels, and I'm worried that eventually Web forms will retire.
&gt;It'll almost certainly be a tree-like structure with nested elements so you can create partials and reuse components. Congratulations! You've (re)invented the DOM You've invented *a* DOM and there's nothing wrong with that. &gt;You don't know how a web page is rendered, but you're claiming you can do it better. No. I was speaking from my point of view as your standard developer who doesn't dive into the internals of ever web browser in existence to understand how a web page determines how to render a page. &gt;Shouldn't you at least research what you're talking about before decrying that it's terrible You don't have to do research to know when something is bad, especially if you work with it every day. I don't need to become a cobbler to know that my old shoes are hurting my feet. &gt;For someone claiming they have a good understanding of how a game renders, you've clearly got little experience dealing with graphics API's. They're a mess... Today, web developers don't have to worry about things like buggy GPU drivers. It's nicely abstracted away by the browsers who deal with all that. The second you bypass that, you have to deal with it You're right. They are a mess. But the key point here is that it's "abstracted away by the browsers". Are you implying that this new technology can't also have similar abstractions? Maybe it's not pure WebGL, but a common procedural language intended for shaders that when compiled maps functions over to supported WebGL behaviors. Like a VM for graphics processing. &gt;Are you complaining that HTML is shite, or that the DOM is shite? They go hand-in-hand for the most part, so I was really referring to both of them being shit. Personally, I wish I could define my own elements and their behaviors explicitly rather than being stuck to the one's provided. In that way, it'd be more like XML and it'd be more extensible and it'd also probably lead to more semantic markup code. React gives you this functionality in a fairly nice, backward compatible way. &gt;it's not meant to represent some constantly-changing document or an application Maybe that's a problem. Because that's what a good number of web page are these days. Maybe that's why it should be replaced with something more suitable for the way people are using it? Ever think about that? &gt;All those frameworks/packages/whatever are going to be inconsistent... every single package/library/etc. created up until now won't work with it. That's the price you pay when creating new technologies that aren't necessarily dependent on legacy. Forever an uphill battle. But just because it may be hard to get people to adopt, doesn't mean it's not worth trying. &gt;In this "new and improved" world of yours where frameworks handle all the rendering using their own markup, all that disappears. Maybe I wasn't clear, but I wasn't trying to imply that these frameworks handle all the rendering using their own markup. I was really intending to describe a similar eco-system to javascript as you've explained in your post. Just that this eco-system is much more low level. Maybe people writing their own DOMElements which you can import individually (like a DatePicker), or a framework which is a collection of many DOMElements (StackPanel, Grid, etc.). React's eco-system already kind of has this going for it (semantic-ui-react vs reactstrap), but it's stuck behind the limitations of HTML, DOM, CSS, and browser vendors, still since developers can't get low level access imo. I, as a web developer, would prefer a single technology that could accomplish all these feats: * An even more extensible markup language. * Lower Level access with WebGL/Shaders (or another procedural abstraction of that) * The virtual DOM and modularity of React. * The near-native performance of WebAssembly. * Uses a static and strongly typed language. People who don't want to muck around with all the low level stuff can use a framework (just like when people use Boostrap, Semantic, Foundation, or whatever instead of writing their own things from scratch). I feel as a web developer, what I'm asking for is the equivalent of video game developers asking for Vulkan or DX12.
Do you work in the same place as me? My place of work has the same monster sproc mess instead of EF. We have sprocs that output sql that is then executed. (NOLOCK) everywhere instead of setting transaction isolation to read uncommitted. It's a maintenance and data integrity nightmare.
BTW, I haven’t done any side / demo projects in a while, but here is a data layer I used. Student-Assessment-Web/KindAssessment/KindAssessment.DataRepository/Db.cs. Also, you can see the other project I put on github used EF. I can go either way. 
Very cool, nice write up. 
I put together a super simple solution for ASP.NET user management [here](https://github.com/matthewblott/simple_aspnet_auth) as it can be overwhelming. Tutorial on my blog here ... https://coderscoffeehouse.com/tech/2017/09/05/simple-aspnet-auth.html
Latest versions of JS added lots of features. Each of the JS frameworks have you do things specific ways, so it's like learning a new language each time. then you have npm installing 30k files through dependency hell and each month the community goes "look, new shiny". 
Simple question bro, if you don't know that's ok 😉
I think you didn't really get what I want. For me, .Net Core Identity is perfectly okay for managing the authentication procedure. Identity allows for custom user and group/role stores, and I'm looking for a solution under Identity that uses the underlying OS' own user management, so that a OS user can authenticate against the service running. E.g. if your Linux host has a user called "john" with the password "ih8passwords", then he can log in easily to the .Net app too with the same info.
&gt; Here's a really famous post from 2014 - see if you can guess who each of the vendors are. OK, I'll bite. * Vendor A is nVidia * Vendor B is AMD * Vendor C is Intel Honestly, if this article was a few years more recent, I might have switched A and B because in general, AMD's drivers have been getting better while nVidia's have been getting worse.
I believe you are completely correct!
**Executable compression** Executable compression is any means of compressing an executable file and combining the compressed data with decompression code into a single executable. When this compressed executable is executed, the decompression code recreates the original code from the compressed code before executing it. In most cases this happens transparently so the compressed executable can be used in exactly the same way as the original. Executable compressors are often referred to as "runtime packers", "software packers", "software protectors" (or even "polymorphic packers" and "obfuscating tools"). *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Well, the first version is here: https://www.bettercoder.io/JobInterviewQuestions (on the right hand side there is a Get questions by Job Ad URL button) It's still just a PoC and far from perfect, but the quick and dirty version is up and running. The following needs to be improved: - No AJAX pages supported right now - currently only data from HTML that is directly present in response is parsed, so if there is some AJAX on the page, it won't be processed - List of classification keywords needs to be increased (Mostly .NET related kwds are present now) - Classification needs to be improved - Weights of classification need to be tweaked - Tagging of the Job interview question database need to be improved so the classification have the right data - Performance - no caching, indexing (shame on me, no time for that so far) - UX - it's quite bad right now (but not a big deal for PoC) So give it a try and let me know what you think... 
So it's up and running, see my post above in this thread.
I don't dislike Javascript or think its a pain in the ass to work with when its used with Node, but using the coolest parts of JS with .NET is a pain in the ass. Blazor clean implementation of what React/Angular and Node does.
Javascript pages that use ancient scripts are proof of their strength. Even when mixed in with new JS frameworks, you can usually just package that awkward JS page up and use it in your application while being sure it'll still work on every browser, even if it is old as hell. What if most of the internet used ASP.NET instead? Web forms scattered around, mixed in with MVC and now Blazor? I like ASP.NET and I like the up and coming JS ecosystem too. I agree with the other used, the JS of today is not the JS of yesterday. 
Yeah.... you may, uh, want to rework the title of your article to not use the phrase "red pill." We know you mean the matrix, but it actually sounds like you got pissed at Blazor, bought a tank top, and started negging girls half your age.
I got excited. And then I found out [WebAssembly is not supported on IE11](https://caniuse.com/#search=WebAssembly). IE11 also [does not support asm.js](https://caniuse.com/#search=asm.js).
&gt;We know you mean the matrix, I did not think of that but did think of the other meaning you brought up. I was very confused
Iirc there's a video where they showed blazor working perfectly in IE11 with a polyfil
Yes it will retire, I am a bit sad by it. MVC controllers can get pretty big though, but they are very testable.
asm.js is fully compatible with regular js; its just if the browser has asm.js support it can run it in a more accelerated mode. So WebAssembly (and by extension Blazor) can be run on downlevel broswers
Do people outside of weird legacy enterprise application still use IE?
Can you be more specific about the scenario?
Wow. Gross. Thanks for the heads up. Is this an American thing? Not come across it. Might have to add a note to the post.
Yeah, even if you are a complete M$ fanboy, there's still Edge.
It’s mind blowing what they doing there. I love it though. Just looking at the browser downloading dll’s and then executing them in Chrome. Fucking priceless. Yea I know it’s pretty limited but hey , I got a MVC site hosted in my browser. To be honest I’d rather use this for SPA than Angular. Thus is more like self hosted singe page app. It’s mint. I hope it matures to release. 
It's vb6 clients connecting to com+ services on local host 
I didn't like the look of knockout when i first saw it. I was wrong. I didn't like the look of react when i first saw it, again i was wrong. I don't like the look of blazor. It's not the c# to webassembly i dislike, it's the web framework itself. Maybe I'll like it once it matures.
It wasn't a new paradigm, but it certainly comes with its share of issues.
Not for Windows 7 or 8, which still have the majority of the market share OS|Share :--|:-- Windows 7|44.62% Windows 10|28.98% Windows XP|6.93% Windows 8|16.26% 
I hope you're wrong again. I want to like this.
So, one thing Im kinda missing, is Blazor supposed to be used for SPA's, or just regular MVC style? I looked at some demo's but it still isn't entirely clear to me.
Wow ... exciting stuff! Currently architecting a solution that needs to be delivered by March 2019! Trialing React and Redux at the moment Would love to use this ... what are the chances of Blazor being out of preview by March next year? Too risky to be an early adaptor? 
Com+ is still a thing? Wow.
You're really mixing oil and water here. Core is about as compatible as Java.
Thats why i need to find an alternative in the .NET Core world
Has someone made some sort of polyfill to run WebAssembly via plain old JS? If not, IE11's support for regular JS doesn't say anything about its ability to run WebAssembly.
is is for SPA's
&gt; You could conceivably create a renderer with a framework around it to add all those features You pretty much just described the whole foundational architecture of [Google's new cross platform mobile solution named Flutter](https://www.reddit.com/r/FlutterDev/). Granted it's only for iOS and Android but there are plans to eventually bring it to the desktop also.
Ugh, don't get me started. Older companies even to this day start "new software" projects using decades old and deprecated technologies. It's legacy the second the first line of code is written.
Ugh, don't get me started. Older companies, specifically in engineering, even to this day start "new software" projects using decades old and deprecated technologies. It's legacy the second the first line of code is written. And to justify the money on "new" project it has to be pitched as a keystone piece of software to automate tasks, and now you got the ball rolling where any other new feature must now be able to communicate with this poorly implemented pile of excrement and everything developed onward is half-assed with passive-aggressive protest in all your code. God help you if there's no code reviews or unit tests, which of course there aren't as that level of knowledge from management would've avoided this situation in the first place. This is what happens when your "developers" have 30 years experience about how things were done 30 years ago.
I'm not sure what you're asking. Are you asking us to imagine the ideal Agile team, or what we do if we work in an Agile or "Agile" team? 
&gt; This benchmark test was testing HttpClient, an extension method, a Javascript seralizer, string formating, reflection (the type name call), and a lambda function, when the point was to see which server framework is faster. I think that's a fair test, if that's the way code is normally written. I won't comment on this article specifically, but in general we have to be careful in our attempts to write "fair tests" that we don't over-optimize the code to a degree that wouldn't actually be done with production code. 
Hello komtiedanhe, Thank you very much for the reply. Would like you to answer what we do if we work in an Agile or "Agile" team?. Thank you very much.
And what was your results? 
I do see your point. I think some aspects of the original benchmark are as someone would write them similar to if a developer were to write a naive algorithm versus a well thought out approach that is tested and verified. Too many blog post decree that something is faster than another when education on best practices, and performance examples are few and far between.
For the ORMs? My tests are out of date but here you go: * Chain: 3.4160 ms (0.2764 ms StdDev) * Chain w/ Compiled Materializers: 3.0955 ms (0.1391 ms StdDev) * Dapper: 2.7250 ms (0.1840 ms StdDev) * Entity Framework, Novice: 13.1078 ms (0.4649 ms StdDev) * Entity Framework, Intermediate: 10.1149 ms (0.1952 ms StdDev) * Entity Framework, Intermediate w/AsNoTracking: 9.7290 ms (0.3281 ms StdDev) https://www.infoq.com/articles/repository-implementation-strategies Looks like a 25% reduction in run time for basic CRUD operations between typical EF and using the more common performance tweaks. (And of course even the best EF test case was CPU bound rather than I/O bound because EF really does suck that hard.)
Considering how much of an impact it makes on the performance, I feel like it's an important point to make. It's likely was just overlooked in the original article.
To be clear, I didn't mean my comment to be a criticism of either article. Rather, just something to keep in mind when writing or reading performance studies.
Thanks! Much appreciated. Will take a look tonight 
&gt; I think that's a fair test, if that's the way code is normally written. (Author of the original article here) That was exactly the intent of my benchmark. Sure, I could pre-calculate the hell out of everything, but that’s not what happens in real life. WCF doesn’t have to, so the Web API code also doesn’t get to. I will admit that the `typeof(T).Name` will have negatively affected the results. But remember, my initial results were that WCF was responding in 1/3rd the time of Web API. That’s not due to micro-optimizations. And something that I clearly omitted from my article was a benchmark using something other than HttpClient, and the conclusion that Kestrel itself is really fast. I did some benchmarking with fully pre-calculated and pre-serialized payloads, and they blew WCF out the water. 
Great post! If you're looking for fairly comprehensive docs, check out [Learn Blazor](https://learn-blazor.com/).
What does it pre-calculate? It can’t precalculate the contents of the field that contains `itemCount`, because that’s a dynamic value. It might pre-calculate how and where to write that value, but the same goes for generating the URL. 
What we did at my company is we built a .net app that runs on clients and made a com exposed dll that communicates to the hidden .net app with wcf. The vb6 apps communicate through this dll. It's made everything a lot better for us since now we can leverage .net for new dev while keeping the legacy apps around. 
Take a look at https://referencesource.microsoft.com/#System.ServiceModel/System/ServiceModel/ChannelFactory.cs You can see it iterating over the type to create the endpoint details. itemCount in this benchmark case is part of the seralized body while in the Rest Service base is moved to the URL query string. The "fix" for the benchmark was not a micro-optimization, the biggest change for the timing was to stop using PostAsJsonAsync which is not in the .Net Core CLR and is now the new norm for posting using HttpClient unless you grab a nuget package. The micro optimizations created the smallest gains of around 60us. "But remember, my initial results were that WCF was responding in 1/3rd the time of Web API. That’s not due to micro-optimizations." This is misleading, the HttpClient+Seralization was taking 2-3x more to return data from the WebAPI than WCF, not Web APi was not responding well, it was responding in a good time, the client was the slow part. The initial results accidentally were benchmarking client JSON serialization speed not Web API speeds.
Asm.js isn't used to polyfill WebAssembly; it's used to polyfill the .NET runtime if required. Blazor's output includes a JavaScript-based .NET runtime that is used if the browser is not able to run the WebAssembly-based .NET runtime.
I'll fill it out, thanks! 
I think it would be fair to include both clients and servers since you need to have both to get a working system Given that there is a big trade-off in complexity for the 2 stacks, I think it would be fair to also include options that are not supported by both stacks such as Wcf with nettcp. 
This might not seem like a big deal. For those of us using Oracle it's a big deal! Right now all we have is the Oracle.ManagedDataAccess for .NET Core, there is no EF Provider yet... (Estimate is end of Q3/2018?). Not sure if I would use EF with Oracle... NHibernate works pretty well and is reliable. I imagine most people using Oracle are using NHibernate, and to port existing code over to .NET Core should be pretty simple. I wrote a tutorial about using .Net Core with Oracle and NHibernate at: http://www.dshifflet.com/blog/20180326.html 
.Net Framework has CCW, its been many years since I looked at it. https://docs.microsoft.com/en-us/dotnet/framework/interop/com-callable-wrapper
I used your Ooui and managed to get Zork working. http://www.dshifflet.com/blog/20180325.html Pretty cool.
Thanks. I did see these. They were useful, but obviously didn't have everything I needed. Looking for contributors?
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/latexandloaf] [My response and benchmark to "Is WCF faster than ASP.NET Core? Of course not! Or is it?”](https://www.reddit.com/r/LatexAndLoaf/comments/87clbb/my_response_and_benchmark_to_is_wcf_faster_than/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
With the pace of things so far, 12 months feels like a long time. It may not be out of preview yet, but it'll be close I'd say.
If you need an ORM that can read Access, Tortuga Chain has you covered.
I'm confused by your inheritance question. Are you talking class inheritance or table inheritance or both? For class inheritance, ORMs such as Chain or Dapper couldn't care less. They're going to fill your objects just fine either way. For table inheritance, I was under the impression that Access didn't support that.
Foreign data is best handled by a view. That way the ORM doesn't know or care where the data came from. The only ORM that I know of which doesn't support views is EF Core 2.0 and eariler. (Seriously, avoid EF Core at all costs.)
NUnit is my preference. You could probably pick any of the three and do quite well.
I've always used MSTest for it's native TFS compatibility. The TestCategory and ExpectedException have always worked for me &amp; my teams on NetFx or dotnet. TFS 2015 on-prem has some issues that require a little more manual configuration and coverage is still lacking.
Any specific advantage of NUnit over the other two?
It sounds interesting but for now I'll stick to WPF on Citrix for remote access.
Not that I know of. I just prefer the syntax. 
FWIW, at my work all new projects use xUnit while legacy projects are stuck with NUnit.
I too prefer Nunit. I've found that NUnit syntax is just nice, and it's been around longer so there are a lot of things that support it. I use Specflow too for BDD, and I've found that the NUnit providers work better for it than the XUnit providers. I've heard great things about Xunit too, so don't get me wrong. These are just reasons I chose NUnit over the two. As for MSTest... meh. 
xUnit by itself is useless for me. They are too dogmatic about "one assert per test" and do stupid stuff like not allow you to include messages on some asserts. xUnit with a custom assert library is workable. I use it a lot when I need data-driven tests. But it still has other problems when it comes to running tests concurrently. MSTest supports `Assrt.Inconclusive`. This is essential for integration testing when you need to know the difference between a failed test and a test that can't be run because a prerequisite is broken. I also use `Assrt.Inconclusive` a lot when working with legacy code bases, as it allows me to flag tests that need to be written as distinct from tests that are actually failing. In conclusion, they all suck when you need to do anything more interesting than simple unit tests. But they don't suck bad enough to make me take the effort to build something new with full VS integration.
Yah. MSTest2 is delivered by nuget now. No VS required. You can even run tests on Linux and MacOS. Bottom line: it doesn’t matter one tiny bit what framework you use. Just write tests in whatever makes you happy. 
Asm.js came before webassembly. Asm.js is a JavaScript library for executing "native" instructions. It is rather slow. WebAssembly was a reaction to asm.js. Webassembly is a native format for the web. Asm.js can be used to build a polyfill for webassembly. Asm.js, as you said, is not itsself that polyfill. Correct me if I am wrong.
Is it .NET Standard compliant?
The only thing worse than changing into something that ends up not being viable is failing to change when must to stay viable.
Yup, it's the sportsStore application from "pro asp.net mvc core 2". I thought it might be a bootstrap related error because the example uses 4.0 alpha and I'm using 4.0 (I cause a few bugs so far because of classes that have changed) using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using SportsStore.Models; using SportsStore.Models.ViewModels; namespace SportsStore.Controllers { public class ProductController : Controller { private IProductRepository repository; public int PageSize = 4; public ProductController(IProductRepository repo) { repository = repo; } public ViewResult List(string category, int productPage = 1) =&gt; View(new ProductsListViewModel { Products = repository.Products .Where(p =&gt; category == null || p.Category == category) .OrderBy(p =&gt; p.ProductID) .Skip((productPage - 1) * PageSize) .Take(PageSize), PagingInfo = new PagingInfo { CurrentPage = productPage, ItemsPerPage = PageSize, TotalItems = repository.Products.Count() }, CurrentCategory = category }); } } 
Everything here looks OK. You can simplify your anchor by leaving out `asp-route-productPage="1"` since that value is defaulted on the action. Are you including all tag helpers in your _ViewImports.cshtml file?
ViewImports.cshtml @using SportsStore.Models @using SportsStore.Models.ViewModels @addTagHelper *, Microsoft.aspNetCore.Mvc.TagHelpers @addTagHelper SportsStore.Infrastucture.*, SportsStore I have only one tag helper file so far: using Microsoft.AspNetCore.Mvc; using Microsoft.AspNetCore.Mvc.Rendering; using Microsoft.AspNetCore.Mvc.Routing; using Microsoft.AspNetCore.Mvc.ViewFeatures; using Microsoft.AspNetCore.Razor.TagHelpers; using SportsStore.Models.ViewModels; using System.Collections.Generic; namespace SportsStore.Infrastucture { [HtmlTargetElement("div", Attributes = "page-model")] public class PageLinkTagHelper : TagHelper { private IUrlHelperFactory urlHelperFactory; public PageLinkTagHelper(IUrlHelperFactory helperFactory) { urlHelperFactory = helperFactory; } [ViewContext] [HtmlAttributeNotBound] public ViewContext ViewContext {get; set;} public PagingInfo PageModel { get; set; } public string PageAction { get; set; } [HtmlAttributeName(DictionaryAttributePrefix = "page-url-")] public Dictionary&lt;string, object&gt; PageUrlValues { get; set; } = new Dictionary&lt;string, object&gt;(); public bool PageClassesEnabled { get; set; } = true; public string PageClass { get; set; } public string PageClassNormal { get; set; } public string PageClassSelected { get; set; } public override void Process(TagHelperContext context, TagHelperOutput output) { IUrlHelper urlHelper = urlHelperFactory.GetUrlHelper(ViewContext); TagBuilder result = new TagBuilder("div"); for (int i = 1; i &lt;= PageModel.TotalPages; i++) { TagBuilder tag = new TagBuilder("a"); tag.Attributes["href"] = urlHelper.Action(PageAction, new { productPage = i }); PageUrlValues["productPage"] = i; tag.Attributes["href"] = urlHelper.Action(PageAction, PageUrlValues); if (PageClassesEnabled) { tag.AddCssClass(PageClass); tag.AddCssClass(i == PageModel.CurrentPage ? PageClassSelected : PageClassNormal); } tag.InnerHtml.Append(i.ToString()); result.InnerHtml.AppendHtml(tag); } output.Content.AppendHtml(result.InnerHtml); } } } 
Even Microsoft is using XUnit in their new tutorials. I’d prefer it over MSTest and NUnit.
I'm at a bit of a loss. I just recreated what you have, minus having the action parameters as segments in the url, and it works fine. If you hover over the links, what URLs are being generated?
Sounds like overkill to me. Generally speaking I have three projects: * The website itself * Services &amp; models * Automated tests for services and models If I don't trust the people I'm working with then I have five projects: * The website itself * Models (and other things that are tested in isolation * Automated tests for models * Services (i.e. things that touch the database, message queue, etc.) * Automated tests for services and models The reason for the five project break-down is to ensure that they can't slip database access into a model or rules engine that is intended to be free of external dependencies.
Philosophically, each layer builds upon the layers below it. So you can grab any project, and the stuff below it, for use in testing or other applications. I really don't like it when people organize their project like a jigsaw puzzle where you need a DI framework to glue the pieces back together. This makes debugging and testing unnecessarily hard. Which in turn often leads to crap tests full of mocks and untested production code.
NUnit has the best and most extensive documentation, as far as I remember.
You need to be more specific on your questions. Are you using .NET or PHP? MySQL db or MSSQL? To host your site, you just need to upload all your files via FTP to your root folder. For db, you can restore it via Control panel.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/bash] [MSTest or xUnit?](https://www.reddit.com/r/bash/comments/87h0i5/mstest_or_xunit/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I want to have the ability that the user can add additional informations to an entry for example if a printer was created, that login informations for the webadministration could be added even if not present in the model (stored in a collection of entries)
I've forgotten to mention that access is only our frontend for the data on the sql server
I'd like to use efcore but given that it only supports TPH it's a dealbreaker for me... I used it for a few smaller projects and did queries on views (ms sql query on a so called settings table which also stores the tenants) which worked fine
Oh, it looks like no url are being generated. That's weird.
Do you need all the migrations? It may be easier to delete them all and create a single migration.
Hi Thanks for the reply. Theoretically I could start anew but in reality shouldn't it be perfectly reasonable to use these migrations that are already there?
xUnit seems to be the cool kid on the block but I favour NUnit as you can't do ordered tests which I often need when doing integration testing. None of the other frameworks offer this.
There might be a __RefactorLog table in your database that's causing issues. If it exists I'd backup the database, delete that table, and try again. 
Hi, no table like that exists unfortunatly. I just did was /u/allinighshoe suggested and removed all my migrations and created a new initial one :(
It might be the changed namespace thats causing problems. What you can do is a diff between the edmx in your new Test migration and the edmx in your last migration. Every migration you write has the edmx file stored in the target string (resx file). This file is used to generate the Sql Statements. You can read the contents of it with this Code Snippet https://stackoverflow.com/questions/15709849/can-i-get-decode-an-entityframework-model-from-a-specified-migration Lookin at the edmx is often the easiest way to understand why ef migrations sees changes you didnt noticed.
did you try going directly to cost management/billing rather than clicking the popup?
Yeah it should. All I can think is moving the classes has somehow broken it. I've had things like this happen before and the only fix was to flatten them.
Thanks for the help dude I’ve basically done a new initial migration :) 
Glad I could help :)
Thank you very much !!!
During development I will frequently clean up my migrations and start from a fresh initial migration. 
What benefits other than fixing an issue I just had would that get you? The way I’ve always seen migrations is that you.l could go to any point of time in your code and migrate the database to it and not have to worry about mismatching tables/missing columns etc. 
Still doesn’t explain why you need a data layer model used instead of a domain model created from the data layer.
Subreddits like /r/TheRedPill also refers to the movie as far as I know. I am assuming that the sort of references you are thinking of.
Well you killed this conversation... Okay, thought of one, seperating your integration tests from your unit tests so you can run all the unit tests easier and isolate your fragile integration tests. Its a stretch but its all ive got.
I mostly use mstest2 along with FluentAssertions. Not suggesting that anything else is worse, just find that the combination of mstest and fluentassertions works perfectly for me. 
You can do ordered tests in xunit but it's not out of the box. That said it's quite trivial to implement, I've used ordered tests myself before in xunit but that having reliance on test order should be quite rare.
&gt; The way I’ve always seen migrations is that you.l could go to any point of time in your code and migrate the database to it and not have to worry about mismatching tables/missing columns etc. Personally I tie my my db migrations, web code and production deployments together. Let's say I'm making a change and during development I create 5 new migrations. When it comes time to deploy this to prod, I've got one changeset I'm checking into source control which covers all my DB and web changes. If I was to roll that changeset back on production, I'd have to go back 5 migrations on the database to get the web and database code to match up. So in this situation, what's the benefit of 5 migrations? I can't roll my web code back to a point where it will reflect the database at migration 3/5 because it was half way through my changeset, so I'd prefer to just bundle them all up into one migration.
Thanks for explaining it clearly. This is what I do as well.
I'm not entierly shure what a domain model is, but I'm allways willing to learn new concepts ... (wouldn't be a good dev if otherwise) As I understand it NHibernate supports a domain model (I'll have a look on this topic) I would higly appreciate it if you could explain me briefly whats the difference between a datamodel and a domain model is.
I have been able to call a webjob but without AAD OAuth, why do you need to use that?
Interesting stuff, took a quick look at the usage and the source. Just my 2 cent, few things I would change. I found that BaseMappable&lt;TIn, TOut&gt; where TIn is redundant. This is what I propose: public interface IMappable&lt;TOutput&gt; where TOutput: new() { TOutput Map&lt;TInput&gt;(TInput mapTarget); } // This is your BaseMappable.cs public abstract class MapTo&lt;TOutput&gt; : IMappable&lt;TOutput&gt; where TOutput : new() { public TOutput Map&lt;TInput&gt;(TInput mapTarget) { ... } } public class FooValueObject : MapTo&lt;FooViewModel&gt; { public string FooName {get; set;} public int FooAge {get; set;} public bool FooIsCool {get; set;} public string FoosPassword {get; set;} } public class FooViewModel { public string FooName {get; set;} public string FooAge {get; set;} public bool FooIsCool {get; set;} } var myFoo = new FooValueObject() { FooName = "Dhruv", FooAge = 30, FooIsCool = true //false? }; //done var fooViewModel = myFoo.Map(myFoo);
I'm not the author, but they do have a [github site](https://github.com/software-architects/learn-blazor) if you want to make pull requests.
The correct answer is because when you generate a migration, it takes a snapshot of your model and stores it in resx file with same name as your migration in binary format, and this is what is used to compute the delta. If you change namespace it will screw up those snapshots - you gonna have to regenerate the whole thing. This is also the main reason why it's a huge pain in the ass with ef 5 &amp; 6 to do merge conflict resolution when two developers did schema changes and then committed code. EF Core went away with resx files to address this issue. More info: https://msdn.microsoft.com/en-us/library/dn481501(v=vs.113).aspx
What VC needs at this point is the functionality that's in DnSpy - the ability to decompile and debug assemblies where you don't have the source code.
It's still on an experimental stage so there will be alot of changes per version upgrade. I'd advise to wait it out I guess for now. The current roadmap for the team if I remember is a new version every 2 weeks till May for Microsoft to assess the viability of the project. I myself hope it gets committed as a MS project, hope for the best.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/azure] [Need info regarding updating Azure B2C users via Graph API](https://www.reddit.com/r/AZURE/comments/87k3pf/need_info_regarding_updating_azure_b2c_users_via/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
So then it is like a repository pattern, only about 10-100x more capable and flexible? Sounds good to me. As long the user can sort of customize it i see no problem using this. Thank you btw for your explaination, served me better than 20min reading online
In visual studios you can now decompile an assembly when you go to definition. It's pretty useful at times. I believe to debug you need the pdb to match the IL to the source, but I believe you can generate this when you decompile it.
I'm finding xUnit better that MSTest, but I already noticed the lack of Assert.Inconclusive. I sometimes use it instead of an assert to check preconditions that are not the primary concern of the test. I find myself using Assert True/False a lot because it has a message param.
Uh, i = i+1; and i = i++; are very different operations under the hood. In the first case, you are adding 1 to i, then assigning the result back to i. The compiler can easily optimize this into an increment instruction. Which is why you see this: 02D0047D inc dword ptr [ebp-40h] In the later case, `i = i++`, you are using the postfix `++` operator. This will store the previous value for i before incrementing, then return it. Then you do an assignment, (redundant I might add and gives you the wrong value) static void Main(string[] args) { int i = 0; i = i + 1; int x = 0; x = x++; Console.WriteLine($"i: {i}, x: {x}"); } will give you `i: 1, x: 0` since you don't care about the original x value, the more accurate comparison would be static void Main(string[] args) { int i = 0; i = i + 1; int x = 0; ++x; Console.WriteLine($"i: {i}, x: {x}"); } which then gives the following in the disassembly: int i = 0; 02DB047D xor edx,edx 02DB047F mov dword ptr [ebp-40h],edx i = i + 1; 02DB0482 inc dword ptr [ebp-40h] int x = 0; 02DB0485 xor edx,edx 02DB0487 mov dword ptr [ebp-44h],edx ++x; 02DB048A inc dword ptr [ebp-44h] 
I like xUnit because your test preconditions are more intuitive - just in the constructor of your test class. 
So in theory I could just have a windows service POST to my own WebAPI, but how would I communicate in reverse to tell the windows service (from my webAPI) that it needed to send something out via XMPP?
Usually I use WCF for that. You do have lots of options though. You could use message queues, named pipes, raw TCP, even a REST server hosted by the Windows Service.
But can it run crysis? Which of them is better to use in what conditions? 
You are correct, good spot! I will make the refactor tonight. If you want to make a pull request you are more than welcome too!
Here are a few that I've found; https://www.recaffeinate.co/book/ http://aspnetcorequickstart.com/
For integrating with XMPP I have found the Matrix library easy to use. It had wrappers for all the calls and xml objects. It handles all the connections and callbacks. Super easy. It does require a license but worth the time saved. The owner/dev Alex is also super responsive. https://www.ag-software.net/matrix-xmpp-sdk/ 
$1,700 is a pretty steep price for a small dev. I don't think this would be an option at all... Partly why I was looking around for something open source on nuget or github.
Only if we would use a more modern approach of development... (we have a lot of denormalized data
Um, this may or may not be helpful, but Logitech HarmonyHub uses XMPP as it's underlying protocol for local communication with that device. I wrote my own library to talk the fairly limited subset of Jabber that they use: https://github.com/i8beef/HarmonyHub It is nowhere near a full implementation, but I couldn't find an XMPP library to really piggy back on when I was doing this. If you end up needing to write your own client to their stuff, this COULD provide you with a base to get started from. I make no promises that everything is perfect or even done well here, it just works for my needs.
&gt; just difficult to maintain schema changes when it is done by anything outside of .NET. DBAs, rogue developers, data "architects", etc. Obviously every company is different, but keeping everything in sync was a nightmare for me. You should never use EF Code First if there's any chance someone is going to change your schema outside of your VS project. If you've got other developers changing your database on you, how do you keep it in sync using DB first? Do you regenerate your model continuously? I feel like you'd be having other issues with DBFirst right now as well if someone changed a column or dropped a table on you as well right?
&gt;You should never use EF Code First if there's any chance someone is going to change your schema outside of your VS project. Yes, that is what I implied/said. &gt;If you've got other developers changing your database on you, how do you keep it in sync using DB first? Do you regenerate your model continuously? I feel like you'd be having other issues with DBFirst right now as well if someone changed a column or dropped a table on you as well right? Well DUH. Don't ask stupid questions. Everyone knows that stored procedures and views are the ONLY proper way to communicate with a database. Why on earth would you want to select, update, delete tables??? Brah, just create a stored proc /s And yes, during a sprint, I will regenerate the model. To be clear, I'm not doing any Database First. I'm doing Code First from Database. 
&gt; Everyone knows that stored procedures and views are the ONLY proper way to communicate with a database. Why on earth would you want to select, update, delete tables??? Brah, just create a stored proc /s Running a select or update isn't going to change your schema. If you're cool with dropping tables your web application relies on then good luck with that. 
Strange, after going via this link, the link from popup works again. Damned web applications ...
You didn't understand my comment whatsoever. That is ok though. Thanks
I'm dealing with that now. I can't even convince my client that they need a "users" table and that putting UserDisplayName in every data table is a horrible idea.
Interesting, thanks for sharing that. That's mostly the client end of XMPP right?
What keeps a microservice from eventually becoming a monolith and running into the database bottlenecking? Genuinely interested, not trying to be snarky. 
Technically each service should be operating in its own data store. In practice, it is difficult to break most systems into independent stores that can not have any relations. 
Correct. At it's core it's just a TcpClient, a background worker reading the stream via an XmlReader, and Task based stuff to tie it all together. The TaskCompletion stuff is probably the most complicated stuff here actually, and this is built specifically for early failure (i.e., there's no recovery of a client closing, etc., my wrapping service expects to fail on disconnect, and I rely on the Windows Service host to handle restarts of the whole service on connection failures, etc.). I don't really know much about the real protocol, but this might give you some ideas.
Very difficult. I guess you could have jobs that sync up the interdependent data between the different stores in off hours... There would be no way getting around this in a CRM 
Yeah. And another downside of that is now your sync job and every downstream consumer of the data has a hard dependency on the microservice. So any change to the structure has to have a coordinated deployment, which now is significantly more difficult than deploying a change to a monolith. 
i = i + 1 is better because I read it somewhere and lots of people agreed with the author
Truth is many projects move towards microservice architecture as a form of premature optimization, where those problems can be solved by a well architected monolith with loose dependencies. For all the advantages of microservices, there are significant overhead that comes with it including: 1. contract management - ensuring that service and caller api align is now either a manual job or requires putting in place something like consumer driven contracts 2. diagnostics, debugging a monolith is so much easier, you would require special tools to do distributed tracing and it won't be hooked up to your IDE in most cases 3. deployment - if you practice ci/cd you need to create and manage a pipeline per microservice. that's overhead in itself 4. data relationship management is a whole can of worms, that requires careful orchestration when you need to create interception views across multiple services. in practice most people end up with reporting database which is your classical relational schema, and data is migrated from each microservice using some kind of ETL or data stream jobs. Which you're required to write and maintain. Microservices have their place, but their are not a magic bullet many are making them out to be. 
I prefer i += 1 actually 
There's a better way: Event streaming. But it requires a much different kind of design. 
For most organizations monolitic with SOA should be enough. If necessary just start splitting off some services into microservices. But, given this is tech it'll probably be "if it's not microservices then it's shit."
What, when an event is fired you have it make changes across the multiple data stores?
No, I disagree. I believe it's good that they keep iterating and improving like this, instead of stagnating due to being held back by technical choices with downsides that only become apparent after some time in use (in this case after a couple of maintenance cycles). You want .Net Framework if you want that kind of commitment and stability. 
That's good to hear! I've got great plans for the library so it should be around for a long time.
You could encrypt the data and save it in a config file or local database instance
Yep, thanks. I have since come across and hijacked some of the code from the official .NET Core secrets manager (https://github.com/aspnet/DotNetTools/blob/dev/src/dotnet-user-secrets/Internal/SecretsStore.cs). I see they don't bother encrypting it though, so, for now, I will not either.
Can you provide a link for a post that goes into detail as the op went? 
I really don't understand why MS screwed the pooch with versioning so quickly... if they have 1.0.0, hotfixes should have been 1.0.0.1, 1.0.0.2... then when they release 1.0.1, everything would move up in lockstep... instead they have this mess where everything is slightly off now. If they wanted to keep with only 3 positions, then what they have been releasing as 1.0.0, 1.0.1, 1.0.2, should have been 1.0.0, 1.1.0, 1.2.0. This 2.1 release is having a bigger dog and pony show of a release than 2.0 release. It should be 3.0.
You may have already seen this but "[The Secret Manager tool doesn't encrypt the stored secrets and shouldn't be treated as a trusted store. It's for development purposes only.](https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?tabs=visual-studio)"
The original Jabber-Net is dead but the one on Nuget is alive. https://fornever.github.io/Jabber-Net/ The last update was 2 months ago. The problem you're probably going to run into on finding a library is that most are GPL, which can be problematic depending on your business model. And #2 is yes most likely. They sound like they want you to set up an xmpp server which then communicates with your API and then returns the result.
Thanks for sharing! Suggestion to decouple the entities / models from the mapping domain. Instead of using a base class, try a Interface like IMapTo&lt;Tout&gt;. Then, write extension Methods for that interface to do the mapping. That way, you can have multible mapping targets and do not couple Mapping details and models / entities.
Woo! Woo! Happy to see this library keep pushing forward.
We have a generalised table for values and one for its labels to store some kind of data and a table which stores so called settings, including tenants, good luck building solid relations with that...
Agreed. The speed of churns will slow down eventually. Right now they are just catching up.
Great library. I've been using beta 2 in production for a while and it's been flawless. Can't wait to try out beta 3!
sybase ase as well. though of course SAP won't create a .net standard/core driven until 2030...thankfully a team rolled their own and its on github/nuget. woe to my fellow developers stuck with sybase ase. literally on my list of things to ask on my next job interview. never again!!
nunit is the most used (see nuget downloads) and i believe best performance wise. xunit seems to be the "hip" one, but doesn't really offer anything better. its just semantic differences. nothing wrong with mstest either. but i'd suggest picking one of the above.
Fair enough. Thanks for the explanation. :)
A lot of the libraries including jabber-net seem to be really focused on client-end stuff, can I actually build a server with jabber-net?
Macos has a key vault you can make use of. Doesn't help for windows and Linux, but it is an option 
Fuck you
You can get it and encrypt it and keep it in memory, however the reality is that none of this is safe from someone running your app in a debugger, since your app needs to be able to decrypt it to use it. There really is no "safe" place to store this kind of thing, there is only "safer" 
I’m hoping these instructions will help me on the UWP app I’m trying to build and trying to use EFCore with. I’m almost resigned to using barebones SQLite without EFCore after several days of failing to make anything work together. Thanks for posting even if it doesn’t work for my case.
Yep, got that too. Why do they do that to us?
 That's not semantic versioning. The first number is breaking changes. These are usually re-writes of parts of the system, api changes, etc. The second number is additions. The third is patches, so bug fixes, etc. This is 2.1, and only 2.1. There are no breaking changes. You can upgrade your app, and use it just like how you would on 2.0.
I wish I could offer you an alternative ORM, but I'm still waiting for Microsoft to fix the SQLite driver for .NET Standard. It works for the simple cases, but my ORM uses batch operations and Microsoft's SQLite driver chokes on it.
why what happen 
That's my point, they should not be dependent... and that is very hard to accomplish for most businesses since most business entities are inherently related to each other.
Where does the key for the encryption get stored? Or should it require a user supplied password every time?
&gt; &gt; &gt; Apparently the godforsaken order you put netstandard2.0 and netcoreapp2.0 matters. Fuck. I'm glad you warned me.
 Over half the article is writing the exact same thing over and over again. &gt; MSIL stands for Microsoft Intermediate Language. We can call it as Intermediate Language (IL) or Common Intermediate Language (CIL). During the compile time , the compiler convert the source code into Microsoft Intermediate Language (MSIL) .Microsoft Intermediate Language (MSIL) is a CPU-independent set of instructions that can be efficiently converted to the native code Microsoft Intermediate Language - MSIL &gt; What is MSIL in .Net Framework ? &gt; MSIL stands for Microsoft Intermediate Language. We can call it as Intermediate Language (IL) or Common Intermediate Language (CIL). During the compile time , the compiler convert the source code into Microsoft Intermediate Language (MSIL) .Microsoft Intermediate Language (MSIL) is a CPU-independent set of instructions that can be efficiently converted to the native code. &gt; During the compile time , the compiler convert the source code into Microsoft Intermediate Language (MSIL) .Microsoft Intermediate Language (MSIL) is a CPU-independent set of instructions that can be efficiently converted to the native code. During the runtime the Common Language Runtime (CLR)'s Just In Time (JIT) compiler converts the Microsoft Intermediate Language (MSIL) code into native code to the Operating System. 
Essentially I have a UWP app and a class library that will contain all of my models. The .NET standard is so the UWP project can reference it; the .NET core app is so that I can use EFCore tools on the class library. You can't use EFCore tools on a UWP project, either, so I have to have the class library as far as I can tell.
I understand, that sucks. You could also create a console app that references the class library where your models live. Use the tooling from there. That way you don't need to be editing .csproj files every time you want to run migrations or update your db. 
I’ll look at that, thanks. The fact that SQLite apparently doesn’t support Code First has sort of dampened my enthusiasm for using EF here further though so I may just go the plain SQLite route.
As has been mentioned, you're never truly safe. Debuggers, memory dumps, etc can all potentially expose this. Generally the trick is to find "good enough" security that doesn't hurt the user experience If you're okay with the user inputting a password every time, you could generate an SSH key on initial config and use that password to secure it. The password will give you access to the key file to decrypt what you need. If that's too much of a hassle and you are okay with relaxing the security a little you could do the same without the password, but anyone can then use the key file if they get their hands on it. This is where security through obscurity could come into play to somewhat help (just don't rely on it entirely) eg if you save the key file in the application dir as something inconspicuous like "JasonParser.dll" most people wouldn't realise it's the encryption key unless they go digging through the source At the end of the day, it really depends on who your users are, and what kind of security trade-offs you/they are willing to make for the sake of convenience
Shouldn't the fact that you edit basically mean that you already ported implementated some sort of accessor to verify them? Couldn't this be a regular integration test over your framework?
You'll need an asp-for on your checkbox too not just the label. The model that backs your checkbox list should have the name, value, and selected properties. Something like: @for (var i = 0; i &lt; Model.Filters.Length; i++) { &lt;li&gt; &lt;input type="checkbox" id="@Model.Filters[i].Name" asp-for="@Model.Filters[i].Selected" value="@Model.Filters[i].Selected" checked="@Model.Filters[i].Selected" /&gt; &lt;label for="@Model.Filters[i].Name"&gt;@Model.Filters[i].Name&lt;/label&gt; &lt;/li&gt; } See https://stackoverflow.com/questions/40555543/implement-checkbox-list-in-asp-net-core
Have you used the Model View Controller pattern before?
I'd rather test for access on action name rather than http method. Thoughts?
Seems a little more difficult than a regular .Net Framework application.
Why don’t use a OneNote Notebook? 
Can you use Azure? https://azure.microsoft.com/en-us/services/key-vault/
I have, a bit. So in previous projects i just made the drop down list in the view in the div tags which passed to the controller and everything was gravy. The model would have a string that relates to that drop down and thats the value it gets. Its my first time generating the list at runtime so i think ive managed to confuse myself... My issue comes from not knowing what to do with my data. Do i run my query in the Create portion of the controller and modify the model from there? How would it know which option ive picked since the variable in the model is itself a list? Or do i run it in the constructor of the model? Both of those seem rather process heavy. And again would it know what option ive picked from the generated list? As you can tell, im a massive novice at this. Im also on mobile atm so i hope ive gotten my point across. 
To your point, any pattern, framework or library can be abused. You make a great application with almost any language or framework. You can also make a terrible application. Razor pages are supposed you use a Model class that is returned from view. It isn’t the framework’s fault if people choose not to do that. 
Oh well. Please do a follow up post with your solution when you get it figured out.
This guy posts a lot of copy-pasted junk blogspam, just look at the other stuff from this domain.
&gt; 1. contract management - ensuring that service and caller api align is now either a manual job or requires putting in place something like consumer driven contracts Most of us have used some Remote service, that has been gracious enough to provide APIs on Github that handle how they hope/expect to be called. *Take 5 minutes and do that for yourself.* Contract management becomes more clear. New methods get new endpoints, unless you can deprecate. This is what you'd be doing in a normal versioned API anyway, right? &gt; 2. diagnostics, debugging a monolith is so much easier, you would require special tools to do distributed tracing and it won't be hooked up to your IDE in most cases Tracing can be more difficult, yes. But I wouldn't say debugging a monolith is *easier*. If anything it gets more difficult after a certain level of complexity, because FFS I don't care about the 5 operations before the one that broke, *I just care about the one that broke*. &gt; 3. deployment - if you practice ci/cd you need to create and manage a pipeline per microservice. that's overhead in itself How much overhead? In my experience, not much. Doing this with FAKE + Jenkins for me was never that bad. Octopus makes this even easier, you can set up your dependencies such that you make sure to deploy dependent projects and avoid 'forgetting' something that would have *had* to change due to API surface change. &gt; 4. data relationship management is a whole can of worms, that requires careful orchestration when you need to create interception views across multiple services. Yeah, but if you get to the point where the Data Management for microservices is a can of worms, You're probably at the point where it's a choice between dealing with that data management problem, or dealing with scaling issues of your backend. &gt; Microservices have their place, but their are not a magic bullet many are making them out to be. They're not a magic bullet, but they have a number of very favorable qualities, some of which may be more psychological than truly 'practical'.
Not 100% sure what your goal is, but assuming you want to populate the datalist once you go to that page. Just a general note: do not put anything 'slow' into a constructor, use a method for it. In your case, you'll want it on your Create method of the controller. [HttpGet] Public Task&lt;IActionResult&gt; Create() { var list = ConnectWise.GetTechnology(); // This is where you would put the SQL query. GetTechnology would return an IEnumerable&lt;string&gt;. return View(list); } If you need parameters for create, put it in the controller and pass it to the GetTechnology method. If this is for production, I highly recommend you use the Dependency Injection container built into Core and hide the ConnectWise implementation behind it. This would something like public interface ITechnologyRepository { IEnumerable&lt;string&gt; GetTechnology() } public class SQLTechnologyRepository : ITechnologyRepository { public IEnumerable&lt;string&gt; GetTechnology() { SQL stuff here } } 
Spot-on. Some comparative benchmarks suffer from similar issues. Unless the various methods being compared produce the exact same result, the comparison is invalid.
Yeah, for now I am just doing something similar to what the .NET Core secrets manager is doing. It is not encrypted though. Will use the .NET Core data protection APIs later on the ensure the information is encrypted Would have been nice if there was some sort of library which took care of all of this for me though :)
[removed]
Please take a look at our solution: https://github.com/okhosting/OKHOSTING.ORM/ Its simple, small and works like a charm on PCL, total control of object-table mappings and stable for some time now.
NHibernate was released recently for .NET Core. http://nhibernate.info/blog/2018/03/17/nhibernate-5-1-released.html An example of using it is here... http://dshifflet.com/blog/20180326.html This uses Oracle, but all of the tests use Sqlite In Memory. For Sqlite you are going to need to implement a ReflectionBasedDriver if you are using Microsoft.Data.Sqlite. Something like: https://github.com/dshifflet/CodeExamplesFromBlog/blob/Nh_Test_Changes_for_DB_Creation/OdpNetCoreExampleNhWithTests/tests/Northwind.Tests/MicrosoftDataSqliteDriver.cs Maybe you might have more luck with NH? If you aren't doing a lot of updates or inserts Linq2DB might be an option? That site has examples for that also. Good Luck! 
Can't style the scrollbar without hijacking.
[NPOI](https://github.com/tonyqus/npoi) can read and write Excel files. I've used it for Excel stuff \(albeit it was a while ago\) and it worked great
Yeah, I suppose I haven't been clear. Basically, when I hit Create New I want to be able to select some info from a drop down. The data in the drop down is to be populated from a database I query. That is super easy, my issue is with getting the data to the view to make it selectable in the drop down in the first place and have it show the proper value on the Index page. Thanks for getting back to me with that info, I suppose I'm confused about how Views work...when I see return View(list) I assume it's just going to take me to the output of the list variable..
the key is to not really put any logic into your controllers. the controller should be provided (IoC) services that it needs to fulfill its requests. the action simply maps the request model to the model required by the service(s), calls them, and returns results. the action also should perform the validation and exception handling and return the proper responses. 
No mention of [HSTS](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security) which should be considered as part of any implementation of HTTPS enforcement. Basically, HSTS (a response header that is only valid when sent over HTTPS) informs the browser that all requests should be sent over HTTPS only.
I assume this is because the article is focused on APIs and most clients will not care about HSTS in that context. In any other situation you are correct though.
IActionResult&lt;T&gt;?
Was looking for this as well. Why is OP's method the "correct" method?
In the context of the implementation (HTTPS for APIs), I'd agree that HSTS is an optional requirement, since there is no guarantee that the consuming client will support HSTS. However, since APIs are often directly consumed by the browser (or rather inside JS), HSTS should still be considered as part of *any* HTTPS implementation.
IActionResult alone is fine 
Good news! I got the drop down working, thanks for your help. The issue now is that it's not adding the data to the database, its complaining about IDENTITY_INSERT being off so I'm looking into that. Thanks again for the help! :)
You'll want to deserialize that json into an object in your code. Then you can interact with it. I'm not sure what you'd want to do this in LINQ specifically though.
I'm on option 1 because I like to have the actual return type, but I will need to return a validation error in some cases so I might need to use option 2. What would be in Boilerplate.AspNetCore? 
That's where HttpException is defined, as well as the exception filter needed to convert the results. https://stackoverflow.com/questions/31054012/asp-net-5-mvc-6-equivalent-of-httpexception If that's all you need, you could also just create it yourself. It's not hard to create new exceptions and exception filters once you see an example.
I'm sorry, i think my question was a little bit confused. I need to select values of an database table and create a json file with this data. I'm trying to do this but my code seems to be a little wrong. List&lt;ProductViewModel&gt; products = await (from prod in db.product join prodPhoto in db.product_photo on prod.id equals prodPhoto.product_id where prod.id == id select new ProductViewModel { id = prod.id, name = prod.name, description = prod.description, rating = prod.rating, price = prod.price, //This field needs to be dinamic, depending on the amount of pictures in the database photos = new HashSet&lt;ProductPhotoViewModel&gt;() { new ProductPhotoViewModel { path = prodPhoto.path } } }).ToListAsync(); 
I absolutely don't understand how returning 400 is any better than 302. Browser still sends complete request, but instead of useful and in most cases automatic redirect you have basically the same behavior but with errors instead.
Maybe some missing dependencies or the users pcs don't have the required .NET Framework version? If you can test it out on a couple of machines you can probably rule that out.
Just serialise the object
At work we just use Api gateways.. 
I'd recommend dropping your start at 1 requirement and use GUIDs.
If the call fails completely then it's more likely to get attention and get fixed.
For normal companies, as long as you know how to write CRUD applications and having an understanding of some *basic data structures* like dictionary vs list vs tree vs hash set, you will be fine.
You could either store the last number used by the user in a column in the user's table, or you could just do something like: Int newid = context.users.first(x=&gt;x.id == whatever).max(x=&gt;x.serialnumber) +1 
gotcha. I understand those data structures. Thanks!
So parsing the content from URL is a bit more complex than my time allows to fix right now. I have however added option to show questions based on job ad text that you can just paste job ad to the textbox and see the questions then. So may be try to have a look now...
Why build out an auto-generate at all? I'm guessing at your application requirements, but why not simply number the 'Note' based on the number of entries in the database table? Like /u/TlovesA said - use a GUID as the PK and use a 'CreatedOn' (or whatever you want to call it) DateTime and then order it and count the row index.
I threw this together VERY rapidly using restsharp and jwt.net Chances are I've got the jwt payload misnamed or something but it should at least give you a better basis. https://pastebin.com/pQkKxzX0
There are websites that will define C# classes from sample JSON. Add those classes to your project, query the data and populate the classes with the data. Then use something like newtonsoft.json.jsonconvert.serializeobject() to convert it to json. You can use Linq for your queries if you are using entity framework I guess. 
Depends on what VBA you are talking about. VBA inside of Excel? Only thing thayay perform better is creating an office plugin and using C# rather than VBA. But really, it's a long going model for Office. If you run into issues, solve them as they come up. You'll learn more from experience than from random opinions on Reddit.
I would learn about some of the more common design patterns. I never ask about algorithms. 
Question, I'm new here. is jwt.net for authentication or what exactly? I came in here just to see what was up. Thanks for referencing rest sharp, just took a look and that seems really simple and helpful.
Just FYI you can do that natively in Visual Studio by going to Edit &gt; Paste Special.
The documentation you linked uses a function: JWT.encode which creates a [JSON web token](https://jwt.io/). Their API requires some data (the payload) to be wrapped inside of that token and the token itself to be passed as a header. JWT.net is providing the functionality of stuffing the payload into the JWT format.
What happens if the spreadsheet is opened on a PC without the dll?
Yes, 100%. Even if you're not focusing on a big tech company and applying to .NET jobs, tons of companies of all sizes ask whiteboard-style algorithm questions, either on an actual whiteboard or on a laptop with a text editor and/or compiler. This is especially true if you live in a major tech hub like SF, Seattle, NYC, etc and are applying to tech focused companies. If you apply for a small web dev firm, you might see less of these questions, but just assume that you'll have at least one interview that focuses entirely on solving algorithm-heavy problems. You don't need to know every algorithm in the book, but here's some advice: 1. Make sure you understand Big O. A lot of interviewers will ask you to optimize your solution and will certainly ask what the time complexity of your solution is after you're done. 1. The major data structures to study are: lists, hash tables (i.e. dictionaries), hash sets, trees/graphs (especially Binary Trees). 1. The major algorithms to study are: Quicksort, Mergesort, Binary Search, DFS, BFS. 1. I would also make sure you understand how to write recursive code. The most straightforward implementation of DFS involves recursion, btw. Use leetcode.com and interviewing.io for practice. Seriously, I would spend 90% of your time (and if you're starting from scratch, expect to spend at least a few months) preparing with data structures/algorithms. Specifically for .NET interviews: I would spend 10% of your time on SQL + C# + ASP.NET trivia.
A more robust way to implement your requirements: Define a unique index on (UserID, SerialNumber) When adding a new note, do like this (assuming SQL Server and EF): while(true) { try { note.SerialNumber = (context.Notes.Where(n =&gt; n.UserID == user.ID).Max(n =&gt; (int?) n.SerialNumber) ?? 0) + 1; context.SaveChanges(); break; } catch (DbUpdateConcurrencyException) { } catch (DbUpdateException ex) when ((ex.InnerException is SqlException sqlEx) &amp;&amp; (sqlEx.Number == 2601 || sqlEx.Number == 2627)) { } } This way you guarantee that SerialNumber is always unique and handle concurrency errors when the user is trying to save two notes at the same time (unlikely, but additional checks won't hurt). Also note how getting max value handles potential absence of other notes for the user. The index will likely speed up the max value query.
I changed companies 6 months ago and had 8 or so interviews. 7 of those 8 involved some kind of algorithm question. So yea, knowing the most common sort/search algorithms would be useful as well as the Big O complexity. However I always felt the question was not just about knowing an algorithm but more about knowing how to solve a common problem using the algorithm or not
Exactly this.
Sure, I think there is also an argument to be made about how this is better from an architecture standpoint as well. It allows for simple enforcement of rate limits, management of keys, controller access to a VPC, etc. That said, it obviously adds cost, but in any enterprise situation it's worth it. I'd even argue in any non enterprise situation, usage is probably low and worth it... 
Depends on the interviewer. Who the fuck really knows..
M8, can you stop?
[removed]
This is true now but I think it won't be with 2.1 and global tools.
Hopefully global tools will make all of this much simpler
Thanks for the explanation. Just as a note, I'm not OP, just was curious. Appreciate the help!
Why reinvent the wheel? Use an Identity column in the database and let SQL Server deal with it. Append a new row for your record, ask SQL what it is (SELECT @@Identity) and use it. 
In general I would say no, don't prepare. It's not fair to you or them since it is a false representation. That said, it's something you should know if you are making a career change. It's not something you need to learn right away, but it's something you should always be studying in some fashion. The tools come and go, and you'll learn to be proficient in them and crank out software. What always stays the same are the fundamentals, like algorithms, data structures, design patterns, etc. The real CS side of things. You can be in the industry for 20 years, but if you only follow the latest tools and tech instead of the core fundamentals, after 20 years you'll find you don't know very much other than a bunch of old, unused technologies from the 90s. 
This sort of logic should be handled in the database. Appreciate you have posted it in a. Net group but using app code to handle database stuff like this is just asking for performance issues later on when your db has grown a bit in size (hence why dbas generally hate entity framework). https://stackoverflow.com/questions/5463384/sequentially-number-rows-by-keyed-group-in-sql
Sql server is perfectly capable of handling concurrency and unique key violations on its own. Any additional concurrency implementation (outside of special cases of which this is not) is akin to super gluing a brick to a ferari. You are adding weight and no value. Your answer is a perfect example of why object oriented coders always end up with poorly performing databases. Every single mct/mcp qualified dba/sql person I have ever spoken to thinks that EF is awful. I have a fairly big sample data size aswell given I work with a significant number of them on a regular basis. 
OP wants auto incrementing to function independently for each user. 
Big sample size with mostly similar perspectives. EF is not for DBAs it is for app devs to get their 80% value for 20% effort. That said, I really hate giant ORMs. I default to dapper calling stores procedures most of the time. 
Do you need it just authenticate the user using AAD, or do they need more permissions (like the ability to query AAD i.e.people picker). In general, here's a good list with code samples: https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-code-samples For authentication, it's just a couple lines of code and registering a new AAD app. No need for secrets or anything. Check this one out: https://github.com/Azure-Samples/active-directory-dotnet-webapp-openidconnect-aspnetcore 
In my opinion it is a good thing to get rid of the horrible viewstate and page life-cycle. I wish I could do that whit my project :p. You would be using webforms just to load some content and serve te page. I wouldn't go so far as sending all your dynamic content through Web api. But I would certainly avoid postbacks if possible. 
The official asp.net docs is where I learned and they are great
Alright, right now I've only learned the basics of HTML, CSS and Javascript. Do you think I should keep going through the Odin project to learn more or should I focus on asp.net through PluralSight?
One approach I used is using a hidden field to transfer some state or other data to the client and the use a library like knockout.js to create a dynamic interface. Then you can use regular update panels and postbacks to persist the data.
Definitely focus on .net core. That's where all the innovation and features are nowadays
I think performance is very important to know, for your own architecture designs, but I have never had a performance question asked to me. Data structures, problem solving, and showing ability and drive to learn more ranks rather highly.
I would argue the percent of devs who understand how to effectively use EF is not much more than the percent of DBAs... I keep my query content/logic in sprocs because I don’t want my app to know or care about those implementation details. To an app a DB should be a service, not infrastructure (generally).
I'm having trouble understanding what a framework is/works. So asp.net isn't a language I need to learn, more like a tool? So I still need to focus on a front end language instead of trying to understand asp.net? Also how does Telerik UI factor into this? 
This book: https://www.amazon.com/Pro-ASP-NET-Core-MVC-2/dp/148423149X
I will just be authenticating against it. It might be nice if I was able to retrieve the user's group.
There's nothing inherently bad about using javascript to build up your DOM, as long as code remains maintainable and the additional overhead is acceptable. But do try to get as much of the content as you can embedded into the initial page download as you can so that it'll be ready in time for the first onload event. Sites which pop up a skeleton page and then go unresponsive for another half a second while the dynamic content and resources load in can get aggravating for a user, especially if the user has to navigate back and forth between pages frequently. If you have to, stuff whatever data you would have gotten back in your initial AJAX load into an inline script and have your client-side render engine chew on that while the rest of the page is loading. As far as client-side rendering goes, JQuery is nice and convenient when you have a handful of arbitrary one-off DOM elements to manipulate, but it becomes more work than its worth for full-blown model binding. For that kind of thing you want a full-blown templating engine. Look for lightweight libraries first and work your way up to big ones. By the time you get to the crazy complex ones like Angular there's so much setup and configuration that it's not really worth it unless you're going all in with a single page application.
I'll need to checkout knockout.js. I'm happy with Bootstrap, jQuery and jQuery-UI for now as it does what I need, but I'm interested in other ways (e.g. React). One issue I'm seeing is authorization as that is all going through post backs. The guy before me decided to inherit from Page and implement a lot of session and authorization stuff in that class, but that'll need a post back. I'll be needing to check all the security stuff in all the web methods, which are static. Interesting design-wise.
It seems that answer to my question is divided but I'll play it safe and learn DS/Algo's. I'll feel more confident that way (instead of worrying about whether or not I'll get an algo question), plus I live in NYC too so my odds of not getting a DS/Algo question is probably a bit lower.
Thanks. I feel moderately comfortable with design patterns and really enjoy learning them.
Wow, that's handy
I'm not completely caught up on the .Net Core with MVC6, which it looks like is what you're using, but I do know what would be causing your list not to bind in MVC5, and I'm assuming the answer is the same here. Instead of using a foreach loop in your view, you need to use a for() loop, with an increasing index. Then, when you make your inputs in your view, you would make the inputs for YourProperty[x] (assuming you used x as your loop variable). If you look directly at the html output of your view in your browser, if you've done it right, you should see the names of all of your list-related elements as having [0], [1], [2], etc as part of their names. These indices are needed to get the list to bind on the post. (If MVC6 is different in this regard and has a better workaround, I apologize, but again, MVC5 and below works as described above).
When I started learning c# and MVC I used Bob Tabor's tutorial videos. I got a year membership to his course materials and it was money well spent. devu.com
Yes, you are correct! I am just learning MVC, this being my first webpage, and I'm just trying to play around with it at the moment. Hmm, that makes sense. Would &lt;a href="@HTML.ActionLink()"&gt; be the correct way to call it? Also, how would I distinguish @HTML.ActionLink(), between Showcontent1, content2, and content3? 
Maybe I am misunderstanding you, or maybe i wrote my question wrong,but my view doesn't seem to have the problem. The syntax issue I'm running into is when I go into my 'POST' Add method in my RecordController, the parameter I pass in needs to be changed to List&lt;AddRecordViewModel&gt;, but once I do that, I can no longer use addRecordViewModel.Sets, or .Reps etc etc. It seems that since it is now an enumerable, the properties cant be accessed. Does that make more sense? I'm not great at explaining my problems through text :/
Thank you, I bought it and am 6 chapters in. This books really covers a lot well, without resorting to running the very basics again. 
C# is the language. ASP.NET is the framework that provides web functionality using C#. When you use ASP.NET you can program the front end using Razor (templating language derived from C#) in cshtml files. Telerik provides components that provide more interaction and features for ASP.NET views, usually with lots of JavaScript in them.
Have you tried reinstalling VS?
Another question: Does the language I choose for the interview matter? This is probably a silly question but I know python too and had planned to learn DS/Algos in python. If I'm expected to provide solutions in C# then I'll probably learn the subject in python first, then re-implement them in C#. I wouldn't want to do this unless I had too though.
Telerik documentation and support is on key, great company to work with imo. Microsoft visual academy for anything mvc methinks. Learnvisualstudios.net was also a great resource for me. 
No, but I think I know what happened. I remember that I compile the roslyn compiler, I think that the compiled roslyn added the c# 7.0 features to VS2015. My projects are compiling fine, but the editor shows errors in code using c# 7.0 features.
Partial views. You need to implement and look further into partial view returns on a "index" or host view page. JQuery to your controller and return a partial view to fill out the rest. On mobile so can't immediately provide links but can offer you a Google route through partial views returning viewmodels.
I'm up for a 6 month refresh myself...
A lot going on here :). First off, you mention lambda - So, if you want your api to run on aws lambda you'll want to use asp.net core. Just wanted to put that out there first. Also, you'll then want a separate API project that will run on lambda. You can then put your mvc app in a container calling the lambda endpoints. Now, as far as JS. Yes, JS and the various frontend frameworks can be annoying (keep in mind we're trying to use a language beyond how it was designed for). If you want to get into frontend, you have to learn it. I would highly advise learning the fundamentals on javascript through books/videos/etc before a framework. The frameworks just build off of that. 
No, but I think I know what happened. I remember that I compile the roslyn compiler, I think that the compiled roslyn added the c# 7.0 features to VS2015. My projects are compiling fine, but the editor shows errors in code using c# 7.0 features.
If there isn’t a lot of content you could render all of it on page load and hide them. Then create an event handler that will toggle the hidden content depending on what the user clicked.
Thank you for your response. I did forget to mention that I am indeed using .NET Core 2.0 for development. Regarding JS, I agree with your point, however, for the time being, I don't have enough time nor enthusiasm, to get into the whole JS development. Also, I am quite optimistic about Blazor and webassembly and for the time being, my job requires strict back-end development from me so the whole front-end part is for educational and personal projects (for the time being). As I stated earlier, I am not much familiar with MVC in ASP.NET (Core or the original) but I do know that it is famous for being able to create nice web-sites with the Razer engine. So, I was wondering whether it is doable/feasible to use MVC as a standalone front-end app which relies on a external API for back-end and data exchange.
You can absolutely have a separate API service. In the olden days of asp.net, there were two separate project types - mvc or web api. Now with asp.net core they combined that, so in the same project you can do both mvc and api controllers. Now given that, it doesn't mean you have to. You can still create a separate asp.net core project that will just house your api. Then create another project that will handle the responses from the razor pages in it's controllers, and those controllers in turn can call the api project, or you could even call the api project directly from the razor pages through some javascript. 
Are there any project example on GitHub, that you can link, which follows this approach? Also I was wondering what's the general standard of designing an MVC model. Is it alright to call API from controllers directly or is it more optimal (design-wise) to have a separate service/layer do the calling and keeping the controller completely isolated from actually contacting the API. I am a beginner so feel free to fill me in the details of general convention and standards of writing MVC codes and designing the architecture. 
If you change it to a List you need to loop through all the items in the list. I'm not understanding what you expect to happen. You can do list[0].Property = ... do you understand what a list is?
OK, yea, now that you say it out loud, that makes sense that I have to go through each item. I'm going to have to take a step back and think about how my newRecord needs to look keeping that in mind. Thank you!
No problem bro.
Ah then I think I won't be able to accomplish much since I have 0 experience in C#
It's a lot like Java. I'm sure you won't have any problems learning it. ;)
Hey Macaveli! Thank you for the suggestions. I'm not sure I quite follow. Is this something along the lines you were aiming at? UML https://imgur.com/a/mVitl
Yeah I missed that part.
Here is a great video explaining versions of .net https://m.youtube.com/watch?v=3EZytmpo0yk
Have you tried clearing your MEF cache? There are extensions to do it this. Not sure if that is your issue though. That was my go-to thing to delete when vs2015 bugged out, it's gone with vs2017 along with a lot of other annoying issues.
Never assume. Always test. Depending on the change, it could be ok, a minor problem, take down the system, or even cause corrupt production data.
There are five principles and four cornerstones of object-oriented development. The first principle is the single responsibility principle. No class or method should have more than one purpose. Second principle is the open closed principle. Every class should be open for extension and closed for modification. The third principle is the interface segregation principle. Keep your interfaces small and separated. The fourth principal is the Liskov Substitution Principle. Every object should be replaceable with an instance of a subtype. The final principle is the dependency inversion principle. This means that a high-level component and a low-level component should not be mutually dependent. The five principles are supported by the four cornerstones of object-oriented development. The first Cornerstone is encapsulation. The second Cornerstone is polymorphism. The third Cornerstone is inheritance. The fourth Cornerstone is abstraction. The essence of encapsulation is the act of information hiding. This is a design time action that the programmer consciously chooses to implement in his code. The essence of polymorphism is the definition of "contracts" through which data flows. Inheritance is defining Behavior once and then having many different types of objects use that behavior. An abstraction is what both inheritance and polymorphism Achieve. Abstraction is deep learning -- something that humans are really good at and machines are not. It's the core of what makes us valuable as programmers. This ability to look at multiple different classes of problems and find their commonalities, factor out the commonalities, and then compose those commonalities into something which describes an entire class of classes of problems is abstraction. 
&gt;create an event handler that will toggle the hidden content How would I go about doing this? 
Hmmm, I've looked into it and still a bit lost on how i'd use Partial Views, and "link" it to a certain list item, and call the action to call a few. Care to explain how this would work? 
You can tell EF to not verify the database migration status... I'm on mobile so don't have the full code handy but it's just a one liner in the constructor to set the database initializer to null or something similar. That will allow you to migrate the database separate from the project as long as they're aren't breaking changes... So you can *add* fields to tables but you'll fail on accessing data (runtime) if you modify or remove columns.
No, by no means decline the job. Take it. I know it sounds overwhelming, but you can figure this out one little piece at a time. My advice would be to start by following a simple online tutorial on how to build your first ASP.NET MVC project. Don't worry about stuff like code first or database first, or ASP.NET or .NET core. Find a good a thorough tutorial, and start building. It will be frustrating at first but you'll get it with enough effort. Good luck. C# is a lot like Java, so you shouldn't have any trouble there. 
If you're not interviewing at a big tech company, I would spend about 1/3rd of your time reading about new technologies, 1/3rd of the time building stuff, and 1/3rd of the time studying data structure type questions on sites like leetcode. If you're applying for a big company, make that 2/3rds leetcode/reading "cracking the coding interview" style books, and 1/3rd of the time doing everything else. Basically, you need to know how to use data structures to solve data structure problems, full stop, if you don't want to be embarrassed and waste perfectly good opportunities. If it's a mid tier company, they're probably going to be easy whiteboard questions, so if you know your basics you should be fine. If you're applying to Amazon or Facebook or Microsoft, you'd better be at least somewhat fluent with the hard ones. Reading about programming always helps me - at least an article a day. That way, when someone asks me about Docker, even though I've never used it, I can speak intelligently about it. That goes a long way. 
[removed]
Have you tried creating a database project in VS? You don't need red gate to do a db compare.
Try my project https://github.com/dodyg/practical-aspnetcore 
You mean Razor?
I kind of like them. Been playing around with using them and Vue to enhance them on the client when I need anything fancy. Although I kind of feel like I'm implementing a smoother version of webforms. My current been is around IDE tooling. Things like their main OnGet I can intellisense my way to success. And when you get out wrong there aren't any complete l compiler errors. And then R# complains about the unused members. Not the end of the world obviously, but the dev experience I could see frustrating long term.
Looks like Web Forms 2.0 don't quite understand "why" it was needed.
This is all I do: app.UseHsts(); app.UseHttpsRedirection(); And that's if it even gets to my app; I have Azure configured to only allow HTTPS as well.
I found this post by Jimmy Bogard very helpful recently. https://jimmybogard.com/migrating-contoso-university-example-to-razor-pages/ If you've read any of his stuff you'll know he's keen on using MediatR (which he built) to separate business logic from controllers. He comes to the conclusion that, if you're used to building vertical slices for your features (for example by using feature folders so your controllers, logic, models and views are all co-located in the same place) then Razor Pages offers the same deal but with a little more cohesion...
Funny. I had the same thought. It feels a bit like a revisited webforms. Im not sure if there is any good reason to choose this instead of MVC. 
Agreed, it almost like they are providing a "path forward" for Web Forms developers, similar to how VB.Net came to be. It breaks the clean MVC cycle, and mixes .Net code in which may be foreign to many front-end developers.
It's faster for hacking shit together, and makes sense for websites that are heavily page driven. It's not really fair to compare it to webforms because it's really still MVC, just in a shorthand form.
Webforms 2.0 now featuring mvc like models... I've been told by co-workers it's helped them understand asp.net core more easily. On a side note I feel they should've come up with something better. C# &amp; .net have been loosing ground for the last 5 years ..go look at the stack overflow stats for end of 2017.. Microsoft are getting better but how late it is that they are, so many bad descions .. Everything is behind ..default templates using bootstrap 3.. recommending less instead of anything else (scss) for the longest time etc.. I genuinely want c# and dotnet to do well and enjoy asp.net core (deployed 2 production apps).. but I really wonder about it's place and relevance, here in /r/dotnet for example people often rag on all the js libraries saying it's this week's 'speical', 'dime a dozen' etc but the truth is there are only a few of relevance react, angular and Vue .. they've risen to the top for a good wile ..years.. but hey complain and don't learn it. 
I have totally written off the whole ASP.NET MVC, Razor Pages, etc. in favour of API's.
I don't like them. Feel like we are sneaking too close, to mixing logic and layout again.
We use React for all UI at work and we don't have any if the history or routing issues. I think those used to be a problem but these days most of the major pain points of an SPA have been resolved, or at least reworked, in a pretty intuitive way. I personally, doesn't mean I'm right, don't feel traditional server side MVC apps have much of a role anymore, at least if you want to get into anything seriously considered a web app. They are good for quick dev to just get shit done, like you said, but anything customer facing feels like it should use some form of SPA to get a good experience in my opinion. Again, depends on the site and what needs to be done, but that's just me.
Just making sure it was what i was thinking. I really like them, i like how you can use C# and Microsoft technologies to dynamically create the HTML page instead of the traditional PHP.
Except we actively try to move away from this kind of stuff in PHP.
Love it. Most of my server side projects these days use razor pages for html and APIs for everything else. It’s the better version of mvc. 
I'm curious, you use Web APIs, what do you use for your front end? I recently started my first web project at work (first for our company!) And so I have no mentor in web development. I'm doing a .ASP.NET core 2 web api project with an angular front end and I dont know if this is a clunky way of doing things.
And which front-end do you use with your API's?
You can't control layout from razor page code-behind as it was possible in webforms.
Just pointing out it’s different from just Razor. Razor pages is a bit different from the typical use of Razor in an MVC project. 
The idea of exporting the object to LINQPad is not only to visualise it's data but also to manipulate them! If you have big dataset or complex object and you want to investigate your data by let's say write a custom linq query you can
I think razor is a huge mistake. I avoid it entirely now; prefering apis, client side rendering, or custom html renderingserver side when necessary. Why? - Tooling is poor. They've been working on this for years, but it's still worse than plain C# tooling. It's gotten a lot better, but it's still not a first-class citizen: you can't easily "extract method", stick it in a nuget package, pass it around between threads, save values for later reuse, serialize to your db or whatnot... it's just second class all the way. - It's complicated. Because it's a a second-class citizen, lots of standard C# or .net functionality has been reinvented... usually poorly, or in ways that inter-operate with the rest of the ecosystem poorly. Stuff like "partial views" or "dependency injection" in razor are just fancy words for "methods" and "arguments"... done poorly. I think it's interesting to compare razor to JSX here: they superficially look similar - yet JSX is lightyears ahead on this. Instead of living in some parallel restricted world, JSX is actually *equivalent* to plain method calls, arguments and object literals. JSX interoperates with normal JS just fine; and you can use it trivially for non-html tasks too, if you wish. razor? ehh.... - It fails in unexpected ways. Because it's not actually statically compiled *reliably*, you can get really idiotic errors in production. - Deployment is hard(er), and slow(er). Because it's typically dynamically compiled, you need to pay more attention to the order in which files re-appear. If you do this wrong, you can get razor pages that try to reference dlls from the previous deploy ephemerally, or the other way around. Also, deploying many files is often slower, and you're depoying a lot *more* because you're deploying a whole compiler (or two). Precompilation is now an option (but wasn't in .net core for quite a while, so may not be everywhere yet), but I'm not sure you can *disable* dynamic compilation simply. - It's not inspect-able. If somebody passes you a fragment of razor (itself non-obvious because of aforementioned second-classness), then it's not at all trivial to tell even the most basic things about it. Does it contain a `script` tag? What languages is it using? Are there any links in that fragment? Is it empty? All of that is *possible* to tell, but it's not at all trivial. Why isn't an html fragment a fragment? - it utterly fails to leverage the whole point of being integrated with C#. C# is a static language. You may or may not like that - but if you're going to pay the static price, why not at least have the benefits? Razor doesn't even fully prevent plain old syntax errors, let alone fancier types of checks that it could. And it's not like these would be useless: non xhtml html has quite a few tricky corner cases that can cause mis-parses. Do you have a link anywhere inside a link? Whoops. Can I place that view component in a `&lt;p&gt;`, or does it transitively contain a block-level element? Razor could, but doesn't help you with that - not statically, and not dynamically. And because it's not inspectable, you can't abstract away this kind of stuff either. So it's an important part of any html chunk you're going to include, but it's unchecked, unverifiable, and not supported by the type system: not exactly ideal. Fortunately, everybody documents everything they write with detailed notes, right? To be clear: I don't think it's a huge problem to use it - mostly because most html just isn't all that complicated but it's just a big ball of spagetti I'd rather do without. And at least for me: why not simply use client side rendering? Bonus points: you can mostly ignore the weird html parsing rules too, since you can update the DOM directly, and thus a button-in-a-button is merely bad UI, not your-document-is-confusingly-broken. 
Angular works great with any backend API. React or Vue will do just as well but I find Angular is good if you're coming from .NET. it's MVC structure is nice and typescript draws a lot from C#. The real question to ask is does the content fit. If you have a lot of user interaction, data updates (but not necessarily whole page updates) then it fits well with the "SPA" type application I'm a browser. Typical example - dashboards, email or 'app' like sites (chat, management tools etc) If it's more content delivery oriented like, tbh, most the internet - say a blog, company publicity site, news site, forum, shops etc etc then SPAs like angular and react make less sense. The time to load is longer because you load a lot of stuff up front before you can render anything. More traditional backend stuff like Razor or ASP/PHP laravel, or Django are more suitable.
Post xaml not a picture. Grid content should fill their parent so it must be your column definitions. Use * instead of auto most likely. 
OP was talking about Razor Pages, not Razor syntax. The weird webforms-like thing. Which I see some merit in but it falls under meh for me. You do have valid points though.
1) Razor is intended to dynamically construct your pages. It's for taking data and displaying it by constructing HTML inside loops. Usually, any processing would be done in the Controller, and then Razor in the View is pretty much just for generating HTML. 2) Partial views are used to create re-usable chunks of View, just like classes are used to create re-usable chunks of code. I use partial views all the time for certain "controls" and "containers" in the view. They're an architectural thing; a design choice for re-usability. They're not analogous to "methods" or "arguments". 3) I have seen incorrect red squigglies in Razor views from time to time. 4) Referencing dlls from a previous deploy? What? I don't see how this is a thing. When you re-deploy, you should be replacing necessary files. Razor is all about dynamically constructing a page. If you don't need that, then don't use Razor, but it's quite a common requirement in line-of-business apps. If you don't do it on the server, then perhaps it's something you would prefer to do on the client. 5) I don't understand your complaint here. 6) Razor is about C#, not HTML. Razor does not really know HTML. Razor just allows you to script the construction of HTML (AKA dynamically generate HTML). You still need to keep track of the HTML. 
MVC has become a bit of a nebulous term in .NET nowadays. With Angular and React you’ll handle some things client side that would traditionally be handled by the server (routing for example) but you’ll still need to hit a backend API for things like data retrieval which MVC can be used for. In .NET Core, the WebApi Controllers that you would use for this are part of the MVC assembly so I can understand the confusion.
Thank you 
:( Thanks, i've tried that and it just results in the whole: Misused header name. Make sure request headers are used with HttpRequestMessage, response headers with HttpResponseMessage, and content headers with HttpContent objects. That and AppendWithoutValidation
[removed]
[removed]
ASP.Net (Q&amp;A guide 2018)
&gt; https://jimmybogard.com/migrating-contoso-university-example-to-razor-pages/ thanks for the link. Good article. 
How bout this. The good news is now u have a choice of MVC or MVVM. The V and M are essentially the same in both for SOC purposes. It's just a matter of emphasis/approach whether you go the Controller route or the Presentation Model (ViewModel) route. Both approaches should get u into a nice SOC pattern and achieve requirements of the project. One or the other might sometimes have a better alignment with the nature of the project. You say tomato, I say tomata :-) 
Even on old school VB projects in the 90s, a team could achieve SOC if they put business logic in a Model and not in the code behind (and had a data layer, etc). 
Not parent but I use a razor view just to provide a `#root` for React to inject itself. For AngularJS, the `@Html.ActionLink` and `@Url.Action` helpers definitely come in handy, but the views can get cumbersome quickly going down this road.
It's not my API or Framework man, would be nice if this company followed standards. It sure would help me out....lol
I have a similar problem, but was looking at the new Swashbuckler Core repository, as I was planning to move .Net Core
Thanks but no dice with this. No errors, but isn't successfully added.
For backend, Blazor is coming and it's gonna be a game changer. 
I like Razor. I use it as much as I can. I use JS UI framewok only when I absolutely have to.
https://github.com/dotnet-architecture/eShopOnContainers
Check out Umbraco https://github.com/umbraco
Are you trying to set an attribute via your own applications code? And then later retrieve that value via your code? If so, you can look up Azure AD Graph API extension attributes. Ignore anything for apps.dev.ms.com. Ignore anything for V2 apps, you want to use a V1 app registration at portal.azure.com and create an extension attribute. This is not a B2C specific operation. 1) create an app reg in portal.azure.com 2) on the app reg, click required permissions, windows aad, under application permissions - read and write directory data. Save. 3) click grant permissions 4) on the app reg go to keys and generate a key - save this key for later Follow this to understand how to register and use an extension attribute https://msdn.microsoft.com/en-us/library/azure/ad/graph/howto/azure-ad-graph-api-directory-schema-extensions#RegisterAnExtension In code you'll want to use ADAL to get a token for https://graph.windows.net using the clientId (app reg application id) and the key you generated. Then you can make the operations as per the article against graph api for the logged in user.
Serenity https://github.com/volkanceylan/Serenity
Umbraco is built on regular ASP.NET though, not Core as OP inquired.
Try Orchard Core.
Err, you see there's this thingncalled Maven central 
Of course I prefer in the long term to have APIs that just return JSON and a VueJS client. However, I like Razor's syntax a lot for when you need really quick prototyping :) Most of the time I design my controllers to return viewmodelresponse objects anyways so all I really have to do when molding an MVP/Prototype in to an API is just change the response type to something like Ok(theObject) and it does it like magic now that .NET Core synonymously uses Controller vs regular .NET that requires APIController on ur class :)
Dude, the date.
Do you get off to spamming that comment or something. This seems like a troll or someone who has no idea what they're doing 
Ah fuck 
Am excited about this
That's certainly a very interesting idea and one that I've already read about. I'm moving the project from .NET 4.0 to .NET 4.6.2 with the next release, so this should be possible. However, I guess it would involve a bit of a learning curve, so for now I'd like to just implement what needs to be implemented, if it won't turn out to be completely horrible and unmaintainable code. It is most certainly a valid option for the future, though!
I will be messaging you on [**2018-04-04 13:15:27 UTC**](http://www.wolframalpha.com/input/?i=2018-04-04 13:15:27 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/88p8k1/professional_open_source_website_in_aspnet_core/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/88p8k1/professional_open_source_website_in_aspnet_core/]%0A%0ARemindMe! 3 days) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
We've been mostly talking data coming from the server to build up the client, but what about POST'ing data back to the server through AJAX? Is there stuff to consider here? In my mind it will work well, because I'll just receive an object with the data in the web method and can deal with everything from security checks and persisting data accordingly. But it doesn't seem ideal from a design perspective somehow.
Survey says! overkill lol
Very very false. Hell the difference between core 1 and core 2 is big on its own
 &lt;Grid HorizontalAlignment="Stretch" Grid.Column="0" Background="{StaticResource NavPanelBackBrush}"&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width="*"/&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Grid.RowDefinitions&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="*"/&gt; &lt;RowDefinition Height="5*"/&gt; &lt;/Grid.RowDefinitions&gt; &lt;Label Content="Control" Style="{StaticResource NavPanelFont}"/&gt; &lt;Button Style="{StaticResource NavPanelButton}" Click="OnNavButtonClick" Name="CreateClassButton" Grid.Row="1"&gt; &lt;Grid&gt; &lt;Grid.ColumnDefinitions&gt; &lt;ColumnDefinition Width="*"/&gt; &lt;ColumnDefinition Width="4*"/&gt; &lt;/Grid.ColumnDefinitions&gt; &lt;Image Source="Content/ButtonIcons/CreateClass.png" Margin="10" Grid.Column="0" Width="30"/&gt; &lt;TextBlock VerticalAlignment="Center" Grid.Column="1" Margin="20 2 20 2" TextAlignment="Left" FontFamily="{StaticResource LatoBold}" FontWeight="Bold" HorizontalAlignment="Left" Text="Create Class"/&gt; &lt;/Grid&gt; &lt;/Button&gt;
False. There’s a pretty big difference between core and normal. There’s a big difference between core 1 and core 2.
In regards to organizing a project???
Oh, never mind then. Mistakes happen.
Well I guess file structure probably not, but things like where you put you package references, how you register services using DI, how you map routes... there’s lots of differences
Scott Hanselman just pushed a new article about testing in 2.1. https://www.hanselman.com/blog/EasierFunctionalAndIntegrationTestingOfASPNETCoreApplications.aspx 
Do you feel like dotnet boilerplate is a good example of project organization for large projects?
I guess that’s a good point. It probably depends on how large we’re talking, but i think it’s a good starting point to get an idea.
!Remind me in 3 days
I am in charge of hiring a couple Angular developers at my company. I sent a piece of text for the job posting. When I saw what they sent back, I realized later that someone added Asp.Net MVC and the ability to read C# to the posting. Despite explaining to the recruiter, who I was asked to go through, they didn’t understand the problem with that. Let me be clear, our job doesn’t require any C# or back end knowledge. Maybe the postings you read are a similar situation.
Yeah good point, missed the ‘core’ part in the OP
!remindme 3 days
1) the excuse that razor is for views and thus bad at code is pretty lame. It support almost all code; it *is* code; it should not be necessary to reduce cohesion by spreading view logic out into the controller. If you want clean code for maintainability reasons, separate code by functionality, not technology. Razor inhibits this; encouraging poor separation of concerns. 2) you state that partial views are not analogous to methods or arguments. Obviously, we can quibble about exactly which code feature partial views are best analogous to, but that's not (my) aim here. As JSX demonstrates, views *are* code, and there's no need to make the mapping at all complicated. A really trivial mapping works fine. 3) you blame visual studio for razor's poor tooling. Yet it's hardly surprising - because razor is such a deep intertwining of C# and html syntax, it makes the surface area for tooling that much more complicated. So it comes as no surprise that it's not just visual studio's tooling that is of lower quality and completeness when it comes to razor - the same thing goes for jetbrain's stuff, and almost certainly for most other tooling too. You want additional static analyzers? Good syntax highlighting in external systems? Things like semantic merge? All of that is likely worse or even absent for razor - and it will *always* be worse. It's telling that even after years of investment, even razor in VS is worse that C# in VS - and that's just the quality. I've got my doubts about the perf too. 4) When you deploy, you can get into situations where not all files update atomically. You may update all the files, but if that update takes a non-zero amount of time, that can cause observable consequences. Obviously, that's not ideal - but semi-dynamic like razor suffers worst. (I don't think this is a serious drawback by itself, but it's symptomatic of an overly complicated solution). 5) If a control generates html, the surrounding context may need to be tweaked for that html to work. However code cannot easily "read" from the generated output and alter it or adapt to it. Razor necessitates a single-pass approach to generating output, and that's often a lot more complicated. 6) Razor is not just about C#; it's clearly tuned for html. You need to be quite careful to e.g. generate XML with it, even though the syntaxes appear similar. And in any case - if you're making a tech that's obviously tightly coupled with HTML - why not have the boons in addition to the banes?
I knew I'd probably be downvoted for this lol. I just need to return to the books and documentation to understand the full idea behind how this all works...
yeah man, you're a bit confused but no worries. Please read through this example and try it yourself. I wouldn't worry too much on the css/html stuff. just try to understand the code first, enabling migrations and updating the database. After working through this tutorial let us know if you have some follow up questions. 
Button Click="OnNavButtonClick" Name="CreateClassButton" Grid.Row="1"&gt; &lt;Grid Width="{Binding ActualWidth, Mode=OneWay, RelativeSource={RelativeSource FindAncestor, AncestorType={x:Type Button}}}" &gt; Button content doesn't stretch very easily. But you can bind the grid width to the actual width of the button. If you want padding you will either need to nest a grid or use a value converter.
Excellent! Thank you. I didn't know if I had to permanently choose between code-first or database first and how to access the code-first in-memory implementation with SQL server management studio. I'll give it a go first though!
In Core, you define the data store in ConfigureServices method which is located in startup.cs. Good question by the way. 
&gt; I knew I'd probably be downvoted for this Every post in this sub (and /r/csharp) gets downvoted.
I couldn't help but see you as Dwight Schrute as I read this. The startup and DI options are of course different, but 90% of the code bases between a large MVC .Net project and a large MVC Core project will be pretty much the same in terms of controllers, views, models, custom classes etc. It feels like you are out to be contrary for the sake of it.
Yeah that's originally what my code was but it brings Content-Length along with it
 // Returns true if the transaction is successful. public static bool TryTransaction() =&gt; ConnectionFactory.Using(connection =&gt; // Open a connection and start a transaction. connection.ExecuteTransactionConditional(transaction =&gt; { // First procedure does some updates. var count = transaction .StoredProcedure("[Updated Procedure]") .ExecuteNonQuery(); // Second procedure validates the results. // If it returns true, then the transaction is commited. // If it returns false, then the transaction is rolled back. return transaction .StoredProcedure("[Validation Procedure]") .AddParam("@ExpectedCount", count) .ExecuteScalar&lt;bool&gt;(); })); 
That worked perfectly, thank you very much!
Try using [onchange](https://www.w3schools.com/tags/tryit.asp?filename=tryhtml5_ev_onchange). Hope that helps.
ProducesResponseTypeAttribute is perfect for that!
That's my point, you can use IActionResult alone; you need to add the attribute.
ASP,Net MVC (Q&amp;A guide 2018)
[removed]
[removed]
NopCommerce. Last I knew they hadn't switched to EF core, yet 
As a .Net webdev who does a fair amount of Angular. I actually like how it's quite clean than littering {{}} everywhere. It's quite a good way if you want to prototype something quick and don't need that much interaction (e.g. Blog) that SPAs aren't designed for. Though i'm quite excited as it seems Razor would probably become an SPA itself as well with Blazor.
I would suggest looking at how identity server 4 does it
All Types of Q&amp;A guide on VB.Net
In most cases, 3 is the way to go. Auth user to 3rd party (in this case, Discord), save the tokens to the user's (server)local account, and authenticate the user separately with your own claims. This adds a little bit of extra security, plus allows the user to change browsers without losing the Discord auth session.
You shouldn't need a view for rendering XML. You can return XML directly from the controller building it using a model or returning a static resource. There's a few XML serializers and a few different return types you can use depending on your usecase. [This article talks about returning XML a bit](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/formatting#adding-xml-format-support) [Same idea, different format](https://andrewlock.net/formatting-response-data-as-xml-or-json-based-on-the-url-in-asp-net-core/) [XMLResult](https://stackoverflow.com/questions/134905/return-xml-from-a-controllers-action-in-as-an-actionresult) (wouldn't recommend this unless you really need this over the previous solution) 
If you like front-end you could expand to other languages / frameworks. I started as a .NET back-end developer and now mainly do Angular front end / Ionic. For me the HTML/CSS is not that fun, but you can get pretty mental with the services etc. Plus the frameworks are pretty similar so you could develop web / phone together.
So I'm going to split my response in half here; the technical half, and the consequence half. Technically: I don't agree with a bunch of your points. E.g. razor is far from trivial syntactically. You may as a user only be aware of the @ with some brackets, but the integration is much deeper than that - razor really needs to parse both the html and the C# to work - because you don't need that bracket after the @ everywhere, because you want syntax highlighting in both halfs, because the parsed html is used to e.g. provide arguments to your controls. As to deployment: yes it takes seconds. Which, you know, really isn't that short at all - I don't implement with anything I'd call high-traffic websites, but I can still tell you that you'll usually get a user requesting something in that time-period. As to html helpers: this gets to the heart of the matter - razor is poorly astractable. As long as that's OK, and you're basically always writing raw html sprinkled with some scripting, that's fine! But the moment you want a more dynamic output - a user-configurable or data-driven UI, say, or a semantic UI with radically different implementations depending on device, or even just the ability to render the same conceptual content differently in different places of your app - then razor falls short. So your example is a good one, and it's why I don't dispute that razor generally works fine - because most sites are fine with trivial html output. Then again - if that's all you want, it's just not that hard to find alternatives. So it's a shame that razor gives up so much and is so complex for nothing in return. 
If you really want to get into it try dotnet core. I was like yourself spilt between 50% office and IT support and 50% Dev. Not having a Windows machine at home I took up dotnet core and it's excelled my general c# at work significantly. I tend to take things I don't know or get stuck at in work home and find examples on the internet that I can recreate.
Since you already have 50%/50% in frontend/backend I would recommend you maybe learn to make distributed systems like microservices that works with your Frontend in a restful manner. I personally just 'ranked up' from junior, and will now primarily work with a react frontend and .net core microservices hosted in Amazon. 
Thats not what he is talking about. He means 50% UI side amd 50% system/database side. 100% dev. 
Authentication is harder than it should be with dotnet. They push Entity Framework on you kinda hard. At my work place, with our existing database, it wasn’t an option, so I would recommend learning SQL well. You don’t need to put a lot of effort into getting good. And it gets you to a solid full stack position regardless of being front end or not. So what would I practice to keep up with my back end ability? Create an auth using cookies, JWT, and read a bit about Identity Server 4. You don’t need to implement that last part, but it helps to understand it. Then create an app that uses one of those methods. It’s okay if you use Entity Framework. From there, create a database and get used to writing your own queries. In my opinion, SQL is the more important skill set when calling yourself full stack.
Very cool! I did something very similar for Table Storage last year for the same exact reasons.... I found the sdk cumbersome and I really disliked the fact that I couldn’t use POCOs.
This isn’t technically dotnet specific, but read The Pragmatic Programmer (pdf) and also read Professional Enterprise .NET. These are both great books. The Pragmatic Programmer is a great book for anyone really, and helps with mind set. PE .NET is a bit more in depth.
I will look up that. Never fully understand what microservices are, can a api be a microservice? Thanks.
Yes of course. Read a book or something on distributed systems. 
I have basic knowledge at React and Angular that you get from Udemy courses :) At the moment I will mostly work with jQuery unfortunately and ofcourse HTML/CSS. That's why I want to focus on backend stuff when I have time.
Pluralsight is a wonderful resource when it comes to C# learning. Look on there too see what's available and suits you. And also, practice practice practice. Try streaming that Excel file in and async handling the parsing line by line.
Thanks. Will look them up :)
Actually I'm most familiar with .NET Core. But most current applications at my company is MVC 5. 
I was in the exact same position as you are now 3 years ago. Here is what I did, and it did work for me. At that level, performance doesn't really matter. It does matter but it comes second to the following: * Enthusiasm * Willing to learn * Problem solving * Fearlessness to pick up challenging tasks This is 70% of what a Junior developer should have. The rest is self improvement. Let's start from the basics. * Clean Code * Clean Coder * Head First Design Patterns Read these 3 books asap, especially the first two. Keep in mind that you should never read language books (unless it the latest C# in Depth from Jon Skeet) because languages get outdated and opinions change. Things like clean code or design patterns don't really change. Ask for a PluralSight account from your company and start watching. There are so many interesting courses in Pluralsight. It's not easy to do distributed systems or microservices in your spare time but watching the related courses will really help. Again that's what worked for me, and it's not a one size fits all but i hope it helps you as wel..
Read the Tao of Microservices. And watch videos on YouTube. (Such as the ones about and by Netflix) It will take some effort, but should give you the basics. I would also recommend that if you are working on front end, really LEARN javascript and the stack. Learn a framework like Vue, React, Angular. (And why you would use each.) Same goes for advanced CSS and responsive design and the various libraries. No, it's not dotnet, but it's far more valuable and far more complicated, just because of the number of options. Dotnet is just one backend, there are many more and some better at certain tasks. 
That's not what I am saying. I am saying that my experience with dot net core has helped strengthen my general dot net experience so it was merely a suggestion to help keep learning outside of work. 
Rather than worrying about specific code, learn concepts and patterns. E.g. for authentication figure out the way that modern web apps are protected, read up on transport auth, mutual ssl, basic, ntlm, kerberos, cookies, federated auth, oauth2, saml. Be able to explain ‘what auth are you using’ in a lot of detail. Apply the same across the board-mutli threading, thread synchronisation, concurrency, race conditions - all these apply to the whole stack, really (with a small exception of js being essentially single-threaded) Learn the difference between signed and unsigned numbers, stack and heap. Do some profiling to be able to optimize things, so you can spot n* queries. This universal knowledge will help you in any language. Above all, learn to think and analyze problems. Never give up ‘because it is too hard’ or ‘ i didn’t know what to do’. As a developed, you are in the most advantageous position as google and stack are at your service
I was in a very similar boat to you a couple of years ago. I ended up moving to 100% backend coding because I felt there were more opportunities there that appealed to me (DevOps, Networking, SRE, Infrastructure, etc). The biggest thing that I learned that helped me along the way was getting very familiar with docker. I learned about containers, Linux, networking and generally felt like I became more well rounded in terms of programming. I also was able to find opportunities that didn't pigeon hole me as a .NET developer which I personally feel is an important aspect for my career long term. Beat of luck!
Aside from all the obvious frameworks and libraries, look into improving your craft. Clean Code, SOLID principles and Design Patterns may all seem a bit overkill when you are a novice programmer working on smaller projects. But I can assure you that they become extremely important when you have a codebase that has a couple of man-decades of work into it, and that has to be maintained for years to come. Start out by reading Robert Martin’s book on Clean Code. Then maybe Mark Seemann’s Pluralsight course on SOLID. And then practice... in every project you do. Later, learn about DDD, Event Sourcing, etc.
Actually no. Microservices should be the last thing you should learn. In most cases it won't be applicable to your job. And if you attempt to "microservicized" your currently application, in most cases you will be applying it wrong. - Learn HTTP protocol, like learn it properly. - Apply Statemachine with your project (Stateless library is good) - Learn on database index works - Figure ways to automate certain things in your project Whatever you learn, don't take on microservices. 
&gt; razor really needs to parse both the html and the C# to work What you're describing here isn't Razor syntax. Syntax is what we type to use Razor, and as far as I know that just extends to "@" symbols and brackets. Razor does need to be intelligent to fill your point, but that's the functionality of Razor. Similarly, syntax highlighting requires Intellisense. &gt; But the moment you want a more dynamic output - a user-configurable or data-driven UI, say, or a semantic UI with radically different implementations depending on device, or even just the ability to render the same conceptual content differently in different places of your app - then razor falls short. I agree with what you are saying here, but this is more a complaint toward HTML helpers than Razor itself. The only time I use HTML helpers is in pre-generated views, and if it's something I'm only using myself, like something in an Admin area, I'll probably leave most of a pre-generated view, which may include some HTML helpers. If I'm writing something myself, I always prefer to write raw HTML, because like you're saying, it's more powerful. 
Auth is always a huge pain in the ass for me. It's like they make it a pain in the ass to secure your app on purpose.
It’s not really just learning particular skills or frameworks. It is more about providing value to your employer. I “ranked” all the way up from SQL report writer to Developer&gt;Senior Developer&gt;Architect. The key is to take on very important projects and make them successful. Also take ownership of existing applications, improve them. Automate processes at your current position. At my current position I took over an existing application. I removed the msmq and Biztalk portions of the application. I simplified the code, reduced technical debt, made enhancement easier and saved them money on licensing. Complete your work ahead of time, follow KISS and YAGNI principals. Document your achievements with each release. Keep a Word document with all your accomplishments to use at review time. Every sprint and every release you need to add value to your employer. If you are not then look for new work. Focus on learning core tech. Learn how to use GIT and do Pull Requests. Learn how to optimize SQL. View execution plans. Lean HTTP. Make sure you are a whiz at LINQ queries. If you just watch 2-3 hours of pluralsight videos per week, you will be good. Make sure you are using the latest version of Visual Studio. Just keep coding. If you need to find new work to rank up, search for open jobs. Ask recruiters what skills are in demand. Make a sample project with in demand skills and put it on GitHub. It depends what is needed in your area. Some companies might need someone who can do MVC and others might want Azure skills. Others might need people to code the original.net framework. 
- Learn on database index works. - Learn how to index database. - Learn how to write queries that utilise database index - Learn how NOT to query the whole table(s) into memory.
Hi dev-f4 its encouraging to hear that you want to improve as a developer. This willingness to learn will take you along way. My recommendation would be to read books like C# in depth by Skeet. Will definitely improve your base understanding of language and technologies involved. Make to sure play around with the code in something like LinqPad (best darn little tool I've run across in years). My other recommendation would be just to read a lot of code and try to learn from it if your not familiar with it. One thing I have been doing recently is looking at the top nuget downloads. Going to their git repo's, downloading and exploring the code base. Its a great way to get introduced to a lot of code and various programming techniques. Not to mention, many of them implement various design patterns that you can learn from (if you can can identify them... make sure to accept that quest :) 
What a *darn* shame.. *** ^^Darn ^^Counter: ^^497934 ^^| ^^DM ^^me ^^with: ^^'*blacklist-me*' ^^to ^^be ^^*ignored*
That's alot of topics to investigate deeper into. Thanks alot man :)
Per https://en.wikipedia.org/wiki/Syntax_(programming_languages): &gt; Computer language syntax is generally distinguished into three levels: &gt; &gt; Words – the lexical level, determining how characters form tokens; &gt; Phrases – the grammar level, narrowly speaking, determining how tokens form phrases; &gt; Context – determining what objects or variables names refer to, if types are valid, etc. Clearly, that includes stuff like tag-helper calls, which look like normal html, yet need to be parsed. Even a superficial perusal of the [Razor syntax reference](https://docs.microsoft.com/en-us/aspnet/core/mvc/views/razor) should make it clear there's hell of a lot going on beyond that. Razor is clearly complex to parse than either html or C#, and because in parsing the sum is generally more complex than its parts, probably more complex than html and C# together. That matters, because it's the *cause* for the poor tooling; it means you should *never* expect that tooling to ever catch up. Even if razor were barely any more complex, it suffers from being a relatively small niche - there are simply hell of a lot of uses for a plain html or plain C# parser, and fewer for razor. As to the criticism surrounding dynamic output - html may be complex, but getting a html parser nowadays is trivial, certainly in C# - it already exists. Not to mention that you don't even need to do that - if you generate object graphs that generate html instead of html immediately, then you never need to parse a tag-helpers output - it could be an object like any other, and thus inspectable when necessary. Instead, Razor chooses to represent html by a side-effect, making it hard to compose. So this really isn't html's fault at all; it's Razors.
Yeah, I realize, but it pertains to anything using the razor syntax. It's usable. But despite the hyped up marketing, it's just not very good in my opinion.
How are you accessing your card reader? Can you share some code?
Unless the card reader emulates a keyboard, I don't think it's possible without a plugin/applet/desktop service running on the client computer. 
Any chance you can provide this?
 public class SchoolDBContext: DbContext { public SchoolDBContext() : base("SchoolDBConnectionString") { //Disable initializer Database.SetInitializer&lt;SchoolDBContext&gt;(null); } } http://www.entityframeworktutorial.net/code-first/database-initialization-strategy-in-code-first.aspx
I think you're probably seeing the pattern here based on all of the replies. The modern development stack is so vast, so complicated, and so specialized that it's impossible to know it all. I've been developing on .NET since 1.1 days, and have been a "senior" developer for probably 8 years now, at least. That said I don't know more than half of what is mentioned in this thread alone. I'm a jack of all trades and a master of none sort of developer. I can do anything from WPF to Windows Services, to SQL, to MVC, to Webforms, to WCF, etc. But I don't know nearly as much about any one of them as programmers that spend most of their time in any one of those areas. My only suggestion is that if you want to stay on top and really concentrate on technology, get employment at a development first shop. A place that will advance at the blazing speed of .NET, and if they don't, move, and keep on moving. Otherwise you'll become a dinosaur in a very short order.
You should definitely look into how language features you're not familiar with work, and get a general overview over what exactly the framework can do. For example, .NET has had System.Linq since 2.0 and I, being self-taught, never knew about it. But it is stupidly useful for doing things with any sort of collection (arrays, lists, dictionaries, etc), and you will find yourself writing iterator code that is usable with Linq. It's probably not the best choice when optimizing for performance, but 99.9% of the time it will be just fine. You can avoid doing long-winded foreach loops usually. Other things I would advise you to be sure to know, from my own experience of things I wish I had found out about sooner (plus a couple extra things about your specific requests): 1. Learn how to use XMLSerializer and DataContract[Json]Serializer to serialize and deserialize XML/JSON into .NET classes without having to write more than a few dedicated lines of code to do it apart from tagging methods and classes to influence the serialization. 2. Learn how generics work. If you find yourself using the object type at any point, it is probably a good class or method to turn into a generic. You will get better compiler/intellisense support and error checking. 3. Learn how to use reflection, and then try to do things without it (as it will be much faster if you can) but it's still a good idea to know how to use it if you need to. 4. If you can, install ILSpy extension for Visual Studio so you can take a look through .NET decompiled if you need to know implementation details. Alternatively if you are coding for .NET Code all the source code for that is on github. Learn how to set up Visual Studio to automatically download and step through it when debugging (it's in Debugging settings) if you need that at any point. 5. Learn what the operators ?. and ?? do. 6. If you are doing desktop applications, I like using classic Windows design with WinForms but you should be sure you know the XAML stuff as well (I haven't quite gotten around to this myself). 7. For working with Office files there are three approaches I think you can take: 1. Require the file to be EXPORTED in a standard format (eg csv for excel) which can be read in easily by a .NET application (CSV has a standard format which is a lot easier to code for than EXCEL). 2. Add the appropriate Office Application COM as reference to your .NET project. The API is messy and you will need a lot of Google-fu to figure it out. Worse, the Office app will run in the background, and won't automatically close if your app crashes. So this is a slow method and requires Office installed on the target machine. 3. Require .???x file format and use the ZipFolder (or whatever it's called) to extract the files to a temporary location. Write code to parse the resulting XML for whatever you want. Can be tricky due to how flexible Office files are; Excel for example will have attributes to skip columns rather than defining each empty column allowing you to just skip tags to skip columns. 8. Leave authentication to security experts. Use existing solutions whenever possible. I personally like how .NET Core lets you leave Windows Authentication to IIS so you don't need to do anything; it just works. Similarly I use Shibboleth for IIS which transparently applies authentication without my app needing to be aware it even exists (Shibboleth is not a .NET specific thing). 9. Learn how to use the using keyword in all it's uses: 1. Importing namespaces 2. Importing static class namespace (`using static System.Math;`) 3. Importing a namespace with an alias to avoid collisions. 4. Unrelated to the previous uses: Automatically disposing an object with a .Dispose() call. You should always call this and using makes it automatic as well as ensuring it happens if an exception is thrown. 10. On a related note, learn how to use try/catch/finally. finally can be used to ensure some code is run even if an exception is thrown. You do not have to catch with finally! 11. Learn how to make/use Attribute classes in general. Goes with Reflection stuff to some degree since that's how you typically end up using them. Of course this is usually only useful if you're doing something with reflection in the first place. 12. Learn how Action/Func delegates work (eg allow you to declare arbitrary function signatures with generics without having to define your own delegates). 13. Learn how lambda functions work (eg (param, param) =&gt; { code } or () =&gt; some expression). Goes well with System.Linq. 14. Learn Entity Framework. It's like the XML/JSON serialization stuff, but for databases! Very cool and useful. 
It's not too complicated. The idea is you are not tied to a specific database. So you can support SQLServer and SQLite out of the box without any code on your end at all (the only exception is you need to tell EFCore which database type your connection string refers to).
I admit I am not too familiar with code-frirst and database-first terms but I suspect that is for more traditional SQL coding and not EF. With EF you are basically making both the code and the database at once by constructing code classes which reflect the database structure.
The Head First book mentioned above is excellent. A lot of it boils down to depending on abstractions instead of concrete implementations....inversion of control is your friend.
How do you know when you're ready to make that jump? I've been programming for 5 years and I am still uncertain whether I could hold my own doing contract work.
Well, securing .net apis (full and core) is really easy lately. Securing secrets in desktop apps is still a compromise. But overall .net has a lot of apis to make things secure
Look up udp. A basic communication tool need the ip and port of another client and send data via tcp or udp. Most tools use udp because it's faster but you have to implement your own error correction. Tcp is fine for chat tools but for video and audio you should use udp! Now you can capture video - encode it send it via udp and decode on the other side. Next step could be to setup a server to make things easier for the user (like in skype you only need the name and one of the the skype servers initiates the connection.
Ok I've been playing around with this code but haven't had any luck in getting a response. I just keep throwing errors because I'm getting nothing back
Whatever you do, try doing TDD. You will have a lot of understanding.
I'd suggest using a solution like KeyCloak instead.
Also Dependency Injection
Any server-side code can only access server-side devices. From what I can tell, the devices are going to be on the remote machines used to access your site. Sensibly enough. So, yeah, you're going to have a harder time with this. It's not a [unique question](https://stackoverflow.com/questions/15807038/architectures-to-access-smart-card-from-a-generic-browser-or-how-to-bridge-the). But there's not really a canonical way to do it at the moment. Options: 1. Use a java applet/flash plugin anyway and add support pages on how to install/enable for their sites on various browsers. Both have been deprecated in most browsers for security reasons and disabled by default. Some well-known sites have exceptions. 2. If you absolutely need vanilla browser support, you're going to have it very rough at the moment. You'll have to use the new draft APIs and add support pages for when users inevitably use unsupported browsers or versions. This can also depend a lot on the hardware used. Possibly relevant APIs: [WebUSB](https://wicg.github.io/webusb/) (Chrome 61+), [Web Serial](https://whatwg.github.io/serial/) (Not seeing that it's implemented, though Chrome has an older serial interface), and [Web Bluetooth](https://webbluetoothcg.github.io/web-bluetooth/) (Partial support in Chrome 53+). Near as I can tell, none of these are finalized, and you're going to be finding lots of issues and seeing things break. 3. Look for vanilla browser plugins (PPAPI preferably) and example JS. You're still going to have cross-browser issues. Check the first stack overflow link for some suggestions. 4. Mandate a browser (or something like a CEF wrapper) and bundle the plugins for the site. This can lower your support burden by standardizing what's available. But it's also terrible unless it's an in-house site. Basically, this will probably change in another year or two. So, buckle in and be prepared to either rewrite or to support an aging non-standard implementation.
I've converted most all of our low-level libraries at work to .NET Standard, and we've been (slowly) converting back-end applications to run on .NET Core. I can't say I've seen any immediate benefit (beyond Dockerization, which is really cool), but I'm hopeful for compounding performance wins. I've been benchmarking critical portions of the code to see what the difference is between the full framework and .NET Core, and especially in those applications where we were using the legacy JIT, I've measured quite a difference.
Yes, enterprise web microservices and frontend, it's great! Azure hosted so far with plans to move on prem later
Fully agree with this advice. I think Angular with ASP.NET Core is a killer skill to have. CLI&gt; dotnet new angular
I'm using it to build an amazon affiliate site using their API. I use Hangfire scheduling to update product prices, availability, etc. I &lt;3 net core
I built my personal website and blog with it, it's been a pleasure and the web server is *really* fast. 
I'm interested to know what you used for your front-end. Razor or Angular or something else?
So AspDotNet Core allows you to use normal .Net. I just put a new system into production with .Net 4.6 built on AspDotNet Core for the APIs, and Angular 4 for the front end. Merging these technologies allowed me to make use of libraries that are not available in dotNet Core yet, but still use the Core architecture. It's running on IIS, served through an nginx gateway, with a SQL Server backend. 
My enterprise is so far behind the times it is criminal. DNC has allowed me to open my home to Linux and Linux compatible devices. Then there’s Blazor...
At work: We use Win10 + VS2017 + ReSharper. My team is currently tasked with decomposing some of our legacy monolithic .NET services into micro-services running in Linux containers. As a hobbyist: I'm used to a unixy environment. I have a MacBook Pro and a desktop running Ubuntu. I &lt;3 .NET Core! Throughout my programming journey I've dabbled in a few different languages (Python/Javascript/Bash/Java), but I genuinely find C#/.NET Core a joy to use. I use VS Code, which is really nice once you get it set up for C# development. I have a few .NET Core hobby projects on my GitHub that I work on. :)
 If you don’t mind me asking, why the move to on prem?
How do you go between using VS + ReSharper to VS Code? I found that using Code felt like trying to run with a wheelchair.
From a business perspective it's much better to be using tried and tested *older* software than to be on the absolute cutting edge. New software tends to be buggy and requires frequent updates.
I am using .NET core with Oracle and NHibernate http://www.dshifflet.com/blog/20180326.html You can even port old stuff over pretty easy to WASM like text adventure games. (Admittedly, Mono is involved.) http://www.dshifflet.com/blog/20180325.html
This is good advice for what to do at a low level. If you are looking to do this in .NET and not delve deep into reinventing the wheels... Look at SignalR and/or WebSockets. There are a lot of examples for text based chat clients. Another option is WebRTC , you could do it without a server and even handle video and audio. There are several examples out there. 
I’m using it with Entity framework and all the other fixings to develop a web application that functions similarly to Airbnb. It’s been my first big freelance job and it’s been super rough but also incredibly informative and invaluable development practices. I host it on Azure for testing for right now. 
I'm probably an odd case, but my first introduction to C# was with .NET Core 1.0. I never had any experience with VS+ReSharper until I got my job about 6 months ago. I'll admit, ReSharper has some really fancy refactoring tools, but beyond that I feel most things I can do in VS I can do in VS Code with the right extensions/settings. I also get the benefit of it being much quicker/lighter than full Visual Studio. A bit of a shameless plug, but you should try out [this Vue/.NET Core template I made](https://github.com/phil-harmoniq/vue-template) that includes a workspace settings, debug configuration, common .NET build tasks available through the Ctrl+Shift+B build menu, and extension recommendations. I'll fully admit that getting it to work like this takes some work, but once you know how VS Code handles workspaces it's trivial.
React/redux + .NET core and postgres to build a data visualization platform. 
So you are suggesting, that instead of reading docs on how other do store tokens, he/she should use a full Linux server made to be a full authentication server? And that's on the /r/dotnet sub, where'd you expect to be suggested dotnet solutions?
I can't speak for how good they are in terms of best practices etc. but two open source dotnet core/netstandard libs I've personally looked at (and in one case contributed to) which have unit tests with xUnit are: [Geocoding.net](https://github.com/chadly/Geocoding.net) and [ShopifySharp](https://github.com/nozzlegear/ShopifySharp) If you want a REALLY large project to use as a reference point, [Entity Framework Core](https://github.com/aspnet/EntityFrameworkCore) is open source and I believe contains xUnit tests. After all, what's better than an official Microsoft framework for learning best practices within the dotnet ecosystem? Also possibly worth mentioning is that recently Scott Hanselman has made a few blog posts about [unit testing in dotnet core + VS code] (https://www.hanselman.com/blog/AutomaticUnitTestingInNETCorePlusCodeCoverageInVisualStudioCode.aspx) that you may want to check out. Hope this helps. Good luck!
My current job is working on a project built on .net core hosted on Linux in AWS containers and lambda. I highly recommend .net core and don’t see myself going back to windows and definitely not to .net framework. That isn’t to say .net core is perfect, but it’s pretty damn good except for when having to integrate with pre .net standard stuff.
Until all your good developers leave because they need to keep up to date.
Very similar here. * .Net core 2 API / entity framework backend with Visual Studio 2017. * Angular 5 front end with Visual Studio Code. * JTW Auth because Identity Server looked like a pain in the arse :) Also * Test driven for the core business logic and behaviour driven tests using in memory database. 
&gt;document.getElementById("demo").innerHTML = "You selected: " + x; For this line, how could I "call" another html file? 
[Create &amp; Fake](https://github.com/CreateAndFake/CreateAndFake): A class library that handles mocking (like Moq/FakeItEasy), test data generation (like NBuilder/AutoFixture), and validation (as in Asserts). Been working on it for the past year, and just hit 1.0 with it. Use VS Community on Windows, works great.
Web micro-ish services running on CentOS, with a central OpenIdConnect server. It has also been great for client daemons running on Linux ARM. We have long-running services running under systemd, doing web request, GPIO, and other hardware integration, and it works very well.
I'd agree with you if this sub had new posts every few minutes, but new submissions don't happen very frequently. Besides, I kinda like helping out people with their questions
The questions reduce the overall quality of the sub when people mostly go there to see some interesting news or projects. Forums like /r/csharp are total trash because of this, but I do think /r/dotnet deserves a forum of some quality (in addition to /r/fsharp of course)
Okay actually threw (this)[https://pastebin.com/BSQ8ircT] through a compiler really quick since my last version was just on paper. I get a 401 back which makes sense since I don't have any token_id or secret, but it could also be an invalid JWT(I won't know unless I actually set up everything to get API keys). However there is one area of concern, assuming you have proper api keys. 1. strftime('%Q') "Number of microseconds since 1970-01-01 00:00:00 UTC." this is potentially an issue since my Datetime class only goes to miliseconds. (thus the * 1000) This might lead to potential inaccuracy and a 401 if they don't like the token. If this is the case you might need to find a more accurate way to get this value. 
The various ASP.NET Core projects use XUnit. You can look through those repos: https://github.com/aspnet For an example of how you'd use this in a typical app, you can perhaps have a look at the test in for the MusicStore apps: https://github.com/aspnet/MusicStore/tree/dev/test 
 So this thread wouldn't be allowed? You only want people self-promoting? That's a good way to kill a sub
Clean code covers all that