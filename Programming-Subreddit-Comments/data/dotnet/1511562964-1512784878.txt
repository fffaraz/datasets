Yeah, the more I think about it, this might not even be dealt with in managed code. This could easily be in the platform specific runtime code.
No, it doesn't break the whole purpose of DI. The purpose of DI is to fully decouple components, and you're presumably still accomplishing that. Also, most DI containers allow type registrations to be overridden in configuration. Perhaps you want a default IType but maybe you want some other modules/assemblies to override it or otherwise change it by configuration. Furthermore, I always say every interface should have two concretions; your primary implementation, and mocks for your unit tests.
I think rules should be reactionary rather than preemptive. There doesn't seem to be anything wrong with the content posted to this subreddit, so I don't personally see the need to prematurely address an issue that hasn't manifested.
It does and works very well. 
The second rule of /r/dotnet is: NO. JOKES. ALLOWED.
Reasons, not rules, make us strong. 
This sub doesn't get enough posts to need explicit rules. Implicit rules - no spam, being civil - can be enforced ad hoc by the community and the mods.
I suggest taking a look at Fasterflect... From my experience it's been far faster at performing reflection, and also better at creating generic functions delegates, setters, getters, etc (since it's done via Emit, not delegates) https://github.com/buunguyen/fasterflect 
Definitely worth looking into. I'm already caching a lot of reflection objects because it's so damn slow otherwise.
I think a partial view for each form would solve it. Then pass a RegisterViewModel to the RegisterPartialView and a LoginViewModel to the LoginPartialView
This sounds like it will work! Thanks. I'll try it out as soon as I get home to my computer.
This is presumably in response to the guy who was looking for tech support for his random crashing application. I don't think we need rules, though we could call out a little more clearly somewhere that this sub is for devs.
Do you have the source code to your application, and are you in a position to be able to post it publically? 
By the way, nice job on the blog. Checked out some other posts. Keep it up.
&gt;I'm a bit unclear with what you mean by "bridge the two databases" though, do you need to connect to both? Yes we need to connect to both. We need to access the student's data (list of students enrolled for the semester, student's personal data etc) from the registrar's database.
This entire post would have been exactly what I would have written. Good one. 
That does not exclude posts asking for help. There is no such rule as you describe. I'm all for strict rules and would actually agree with yours, but they need to be defined by the mods, not randomly made up by users.
Sounds like an interesting concept. Would be good to have some examples listed in the README to understand some of what the library can do without reading all the text/src
What if the difference is deeper than just the roles? Mentors might be signed in via a Active Directory domain whilst students use Facebook to authenticate themselves. 
It still has test recovery problems though.
Your talking about authentication which is different from authorization. You can have different methods of authentication but you can always complement with authorization using roles , claims and policys. 
* I like all the hooks available to it. * ASP.NET Core receives all the investment right now and in the future. * It gets faster and faster. * It has projects like Orchard Core which contains a lot of awesome functionalities. * You can make it based on Full Framework for now and just switch to cross platform with minimal pain.
My philosophy on this is to ask whether the data is useful outside the context of this specific application. If it isn't, Code First (or Code Only). If it is, DB First.
Resharper is all good till you are working on a big project. This is when it starts to glitch and slow down ALOT. Been experiencing some run time exceptions with the latest 17.2 update too.
&gt; Implicit rules - no spam, being civil Those are not implicit, those are explicit Reddit wide rules.
I'm still a little bit confused about the "module" system. I've got some api integrations with Google that I think might be a good fit as a module, but am unsure where to start.
I'm not really that into tech and programming so I don't really have an idea of how to get the source code, sorry.
That was party of why I created the template project. If you have any suggestions on what you would like to see as examples I can add them. We're you thinking about the help function or how it handles errors?
I like the module system. The module system lets you easily set up the hierarchy of your solution and the dependencies and configuration for each library/module. It can be overwhelming but download the ABP source code and try to follow along the logic from the point the website starts all the way through configuring each module that is included. That really helped me understand how it works above and beyond the documentation, which is pretty good itself anyway. 
You wanted an explanation for how a person might know this is a programming related subreddit. I gave you an explanation.
 this is example code for seeding using identity public static async Task&lt;bool&gt; SetupSecurity(RoleManager&lt;IdentityRole&gt; roleManager) { try { // Check if roles exist then Add them if they dont. if (!await roleManager.RoleExistsAsync("Admin")) { var role = new IdentityRole { Name = "Admin" }; await roleManager.CreateAsync(role); } if (!await roleManager.RoleExistsAsync("User")) { var role = new IdentityRole { Name = "User" }; await roleManager.CreateAsync(role); } //TODO ? maybe //await AddCalims(); //await AddPolicy(); return true; } catch (Exception ex) { Console.WriteLine(ex.ToString()); return false; } }
You should add a .gitignore file to exclude you bin obj and other non code folders from your repo.
Rx rules rather than imperative rules?
It works! Though for some reason now my client side validation isn't firing, but at least I have server side validation. It's probably completely unrelated. I'll come back to it later and see if I can figure it out. Thanks!
Okay, be sure you have included the jquery validation reference (I assume that's what you're using ) 
I read the README and I'm wondering, if it isn't very similar to ManyConsole? 
Why use attributes when a controller is a startup class anyway. Use interfaces.
Easy. 1.Use identity server 2.connect to active directory 3.Profit There's no reason to build on a dying ecosystem.
Yeh just a simple Hello World example in there would be good to hook people into checking out the code/sample project. Obviously the sample project can show off the fancier stuff! I will check it out on Monday üëç
Yeh just a simple Hello World example in there would be good to hook people into checking out the code/sample project. Obviously the sample project can show off the fancier stuff! I will check it out on Monday üëç
There is a huge difference between "not the newest thing" and "dying". 
The full framework is dying. Period. There's a reason the UI designer updates are only for uwp UI code not for wpf. There's a reason Microsoft is pushing out more software for all os's than they do for the exclusive windows platforms. The full framework is dying. One azure container at a time.
The last update for WPF was in October. The last new feature for WPF was in September as part of .NET 4.7.1. The framework isn't "dying", you're just not paying attention to it anymore.
Ohh yeah they backported a uwp feature to wpf. A neglible one on top of it. Wpf is my main development domain. I pay lots of attention.
So what do you want? A annual breaking changes like EF?
No. But I don't want to invest in a system that's going to get more cost ineffective day by day when it takes no cost to actually use a new one with better support and a healthier ecosystem. That is OPs use case. No past code. Fresh start.
Maybe give us some examples of your code?
Depending on the situation, switch and if-then may actually compile to be the same thing... In Debug mode. In Release, having numerous cases to a switch statement (more than five ish) can result in the compiler generating a hash table for the results. This will increase the efficiency if the switch compared to if-then as your cases increase. If you o my have a few cases, use whichever logic is most readable and easiest for a future developer to understand.
Here's some boilerplate for an API call: public class ApiClient&lt;TArgs, TModel&gt; where TArgs: IUrlSerializable, TModel: class { private static string UrlRoot = "http://api.contoso com" public async TModel GetData(TArgs arguments) { var client = new HttpClient() var request = await HttpClient.GetAsync($"{UrlRoot}{arguments.UrlSerialize()}") var json = request.Result.Data; return JsonConvert.DeserializeObject&lt;TModel&gt;(json) } public interface IUrlSerializable { string UrlSerialize() }
Starting point would be to install the "AWSSDK.Polly" package from Nuget. Have you done that yet? You might also want to get it working from the command line: http://docs.aws.amazon.com/polly/latest/dg/setup-aws-cli.html Once you have that working as a proof of concept it should be straightforward to port the logic to your application using the SDK
If your switch statements are messy or hard to read, then you may not be following the single responsibility design principle and some refactoring might be in order. Try to remove code logic inside the switch statement and move it to functions/methods where possible/sensible.
"Healthier" is debatable opinion. The tooling support for .NET Core is pitiful compared to .NET Framework. Even things we have taken for granted for over a decade such as FXCop aren't available yet. And the library support is still years away from being on par. Furthermore, when comparing ASP.NET classic vs core, there's not really a lot of difference. Missing libraries aside, the effort to port from one to the other is quite minimal. And it's a flat cost; you pay the same no matter how many controllers you have because the changes are mostly limited to DI, handlers, and other cross cutting concerns. So again, I'm not seeing this as a clear cut choice.
P.S. If you don't actually want anything for WPF, then stop whining that you aren't getting anything.
Ah the SRP, used by everyone who wants to say something but doesn't have anything to offer. Allow me to fix that for you. &gt; If your switch statements are messy or hard to read, consider other options. A table driven approach, where the data or action is stored in a dictionary, is often cleaner. Especially if the data frequently changes, as it can be loaded from a database or configuration file. You see, just telling someone to "refactor" doesn't help in the slightest. You have to actually give them options. And forget about SRP. If you can't argue your case without that platitude, you haven't thought enough about what you're trying to propose. And if you can make your case, then its a superfluous buzzword that distracts from it.
Big/ugly switch statements are a code smell indeed. The traditional way to get rid of them is the introduction of polymorphism, but! My guess is that this will be too much for you/your code, so forget it. The simple way to make the code a tad more palatable is to just refactor the code from your cases into their own functions. The sole benefit of this is shortening of vast swaths of cases and code in them, but it's a start üòÄ.
If they provided an example then I would have offered more. Sorry, I won't say anything at all.
Sometimes it helps using the state pattern
If your such provides some logic based on data, strategy with factory might be a good option.
Don't worry, it's not you. 1. You were being helpful 2. They were extending your answer. 3. They didn't need to be shitty about how they did it 4. This is reddit, unfortunately toxic replies are the norm.
My personal rule of thumb is to use switches for distinct set of values. Enums fit this perfectly, though not exclusively 
*jump table, but yeah pretty much.
Depending on what you do in your switch statements, you can convert them into Dictionaries. 
Use F#'s "matches" and you'll feel great.
I'm not aware of a really simple and elegant method. There are some options of course. You could create your own tag helper which sets the id attribute differently. See https://docs.microsoft.com/en-us/aspnet/core/mvc/views/tag-helpers/authoring You could have a bit of JavaScript which on page load, goes through all of the input/other elements as necessary and updates the id attribute. Note you'll need to call this again if you update any content through AJAX using partial views. I'm not sure what happens if you just try specifying the id attribute yourself when using the tag helpers; might be worth a try? In JavaScript (or if you're using a library like jQuery) you could override the selector function you're using to .toLowerCase() either side. Personally though, I'd leave them as they are though. Partly because for form fields I'd rather have the id and name attributes matching than have different casing for the id field. If some of the above techniques are used, it does make the code slightly more difficult for another MVC developer to pick up (only slightly), e.g. if they create a new viewmodel property called MyVar, they might initially write JavaScript which uses document.getElementById("MyVar"), which will work if this function has been overwritten, but not if a custom tag helper is used. Swings and roundabouts really. 
Don't forget downotes without explanations too. Making you wonder if you've got something completely wrong, or there's a much better way which no one is telling you, or if it's just people feel like downvoting
According to this : https://docs.microsoft.com/en-us/aspnet/core/mvc/views/tag-helpers/authoring &gt;Pascal-cased class and property names for tag helpers are translated into their lower kebab case. Therefore, to use the MailTo attribute, you'll use &lt;email mail-to="value"/&gt; equivalent. The last line sets the completed content for our minimally functional tag helper. Seems that the main issue is that the method in question is breaking .net convention by not using pascal case.
Do you have any examples? If each switch case has dozens of lines of code within it, then a simple abstraction would be writing functions for each and then calling them from within the case. If you wanted to remove the cases completely you could create a Dictionary which uses the case condition as the key and a function to execute as the value. e.g. var whatToDo = new Dictionary&lt;string, Func&lt;bool&gt;&gt; { {"Adam", dealWithAdam}, {"Bob", doSomethingWithBob}, {"Charlie", handleCharlie} } var result = whatToDo["Bob"](); Just remember to handle the case when the value doesn't exist in the Dictionary (the 'default' in switches). If there's only two or three you might want to just stick to the old if .. else if .. else flow. If you've got a code sample we might be able to offer views on whether they are 'messy' by other people's standards. Personally, I take ReSharper's suggestions, which leans towards switches over if .. else if .. else, and I think it looks a lot cleaner and easier to read.
I leave them alone. It's just that it screws up our javascript naming standard.
You can use other assertion libraries fine with xunit...
Windows containers/vms are more expensive than linux. That's what I meant by the cost. Other than that fxcop has been replaced by the nuget analyzers package system which allows for customization which is much more healthy than full framework fxcop people just enabling it without knowing what the fuck these rulesets are meant for. That's the reason it didn't get ported as a whole to netcore
The not wanting anything part was in reference to OPs question. I have lots of feature request for wpf. Mostly bindability issues but that's besides the point. I can clearly see that you have no desire in conversation but really just want to grind on me for something.
Do you have your project set to compile views? That can add a lot of time to a build. 
Ctrl+f5 if you don't need the debugger 
 &gt; Big/ugly switch statements are a code smell indeed. I'm not downvoting you. However there are plenty of use case e.g. state machines, parser and reducer functions.
Also try disabling js debugging in VS. Takes forever. Tools - options - debugging - enable JavaScript debugging.
That's cumbersome though. I was looking for an easy default. There must be a way to hook in the id generator.
Yea. Right now it has just been me in there so it wasn't a huge priority, but I can go a head and add that today as well. 
Not sure. Is that a NuGet package?
I was kind of modelling it off of Microsoft's MVC. I don't see a huge benefit to either, but I though at least this way it might feel like something people have used before. 
Thanks. I updated my comment and added MSDN documentation backing it up.
Well, the thing with attributes is that they rely on reflection. You have no compiler safety. The more stuff happens inside the attribute code the more you have to look at your stack Trace and surrender to the magic.
Disable JavaScript debugging, and Browser Link. That speeds it up a lot for me!
Yeah, sorry, should've linked it before: https://github.com/fschwiet/ManyConsole
That's true of MSTest as well. And since that's like 90% of what people care about, there isn't really much of a reason to choose one over the other.
&gt; I have lots of feature request for wpf. Mostly bindability issues but that's besides the point. That I'd love to hear. What I'm tired of is having this conversation: &gt; WPF is dying. They aren't supporting it at all. &gt; &gt; Well what do you think it's missing? &gt; &gt; Um... well... it should be faster... and WPF is dying!! I've had that same conversation so times with so many people that I've grown rather frustrated. 
&gt; Windows containers/vms are more expensive than linux. That's what I meant by the cost. That depends on your hosting company. I'm not paying anything because azure offers free websites for small projects. &gt; Other than that fxcop has been replaced by the nuget analyzers package system Yea, that's the theory even for .NET Framework. But I haven't actually seen any of the rules get ported over.
I've had this feeling as well. I'm currently reading 'Clean Code' and the author mentions that a good rule is to have only one low-level switch that is used to create polymorphic objects (I think this is factory method?)
It will be browser link disable it
which specific ones are you missing?
&gt; &gt; WPF is dying. They aren't supporting it at all. &gt; &gt; Well what do you think it's missing? &gt; &gt; Um... well... it should be faster... and WPF is dying!! &gt; I've had that same conversation so times with so many people that I've grown rather frustrated. that is funny, because thats not what i'm saying at all. makes me think you dont actually listen to people you are so frustrated about. WPF is well supported and will still work for years as a great framework for xaml bases / mvvm UI software. WPFs performance is second to none aside from the serialization backend, but that stringified backend is what makes WPF so great.... But UWP is like that and then some. Things can get great support but still be dead. See VB.NET for example. VB.NET still gets lots of features ported from CSharpLanguage and the dotnet framework. That doesnt change that thats just by microsofts grace. Microsoft has no vested interest in supporting VB as a language anymore. Would you design a new project from scratch with WinForms, because WinForms got the same features that WPF has (aside from XAML as the transcending language of course). I'm all for not changing production code to a newer framework just for the sake of it. But if you're required to design from scratch, the "this is older therefore more stable therefore better" fallacy needs to go.
There's avalonia 
&gt; Would you design a new project from scratch with WinForms No, but I actually have met a lot of people who still do. They figure it's never going away (and to be fair, MS is still adding stuff like high DPI support).
Um, pretty much all of them. I have seen a few random rules that people have added, but never a comprehensive set to match what FXCop offers for the full version.
As it's not been mentioned also try c# dev in vscode, it's quite nice with the dotnet core cli, and a refreshing change from VS's slowness
I get around it entirely by replacing instances of directly referencing the name with the @Html.IdFor() Razor method. Only problem is the Javascript has to live on the same page as the view, so it can't be in a separate file. 
Glad someone else mentioned this. So true. I‚Äôve tried everything to speed it up to no avail. VS2017, VS Code, VS for Mac and Rider, same result: 10-30 second page loads. On Azure my web app is blazing fast. 
Do you happen to know if SQLite is similar to LiteDB? With regards. 
Yes, I have added AWSSDK.Core, AWSSDK.Extensions.NETCore.Setup and, AWSSDK.Polly Nuget packages. I tried to follow the AWS documentation to get setup and add my IAM credentials but I'm having a really hard time following everything and I don't know if it's configured correctly. Compared to the courses I've taken on .Net from Pluralsight and Microsoft, I find AWS documentation confusing. To test Polly I copied the controller action I found here: https://chrisbitting.com/2017/04/07/using-amazon-polly-from-net-c-get-mp3-file/ I'm getting an error when I run this on the first line, "Amazon.Runtime.AmazonClientException: 'No RegionEndpoint or ServiceURL configured'" and I have no clue why. I've tried to Google this but I'm not getting a clear solution. I would be happy to pay for a course if there is one that covers this sort of thing in detail.
This should cover what you are missing: https://stackoverflow.com/questions/43053495/how-to-set-credentials-on-aws-sdk-on-net-core
I just want to highlight this, it was hugely helpful for us. 
Fair enough. But nothing actually happens in the attribute. It is just for the reflection. Other than that it is a POCO
Just set up a new dev machine at work. Using VS 2017 on Win10 I've found that disabling JS debugging as well as using IIS instead of IIS-Express has been my best experience thus far.
I've had to do the same with NIEM in the past. Generally I would just use xsd.exe to create my own business objects and edit when needed. NIEM was complicated enough to not really work with most tools out there, the references were too recursive and the whole schema was too large. 
I'm resigned to this being the solution, but holding out hope that there's something out there. I have about seven exchanges to do, and they have already updated each schema and changed code lists. 
Make sure you have enough RAM that you aren't bottlenecked on disk access for GAC assembly loading.
I never realize Html.IdFor() exists!
SQLite is Relational. LiteDB is a document DB.
I've dealt a lot with NIEM (and GJXDM before it). The number of xsd files in an exchange package can be insane. I usually create a script to feed everything into xsd.exe and name the resulting file niem.cs. I then do a bit of manual massaging (change arrays to collection types, tidy up attributes). I segregate this into an assembly with associated helpers, etc. I use AutoMapper to convert to/from my domain model. The rest of the solution generally does not care about NIEM. When updates are made to the IEPD, I update and rerun the script, then use Beyond Compare for merging. Occasionally, the changes are simple enough (adding a few values to code lists) that I just make them manually. Of course, this can be as painful as it sounds. Fortunately, I've never had to do this more than a couple of times for a project. I've been doing this for years and would love to hear a different solution.
It does not. Code first is about generating tables from existing C# classes.
That's called "migrations". Here are the features of EF Code First from one of the original announcements. &gt; * Develop without ever having to open a designer or define an XML mapping file &gt; * Define your model objects by simply writing ‚Äúplain old classes‚Äù with no base classes required &gt; * Use a ‚Äúconvention over configuration‚Äù approach that enables database persistence without explicitly configuring anything &gt; * Optionally override the convention-based persistence and use a fluent code API to fully customize the persistence mapping https://weblogs.asp.net/scottgu/code-first-development-with-entity-framework-4 Way down at the bottom they talk about your options for actually creating the database: &gt; 1. Manually create and define the schema ourselves using a database tool (e.g. SQL Management Studio or Visual Studio) &gt; 2. Automatically create and generate the schema directly from our model classes using the EF Code-First library 
Migrations is about upgrading database after changes to your classes.
Strategy Pattern. Between Strategy Pattern, Command Pattern and LINQ, you can write REALLY clean and safe code in C#. Most of my projects have zero switches and almost no ifelse statements. Sprinkle some dependency injector (my fav is SimpleInjector) on top of that and some Generics (meaning generics from .Net and some of your own making) and you are setup to have a lot of fun with this. Writing generic converters, validators and utilities can be super nice when combined with strategy pattern. Careful of letting REST and api contruction dictate how you build your inner layers so that you aren't stuck with only simple crud. Web development mindset can greatly stagnate how you think about what is possible in C#/.Net. 
It looks OK. Most of that can be achieved with dapper plugins, however, which keeps dapper itself fairly lightweight. Are there any performance comparisons between the two?
Did you try [XmlSchemaClassGenerator](https://github.com/mganss/XmlSchemaClassGenerator)?
There's a lot more there than meets the eye. For example, the ability to do an upsert and read back the object in a single SQL statement requires a deeper understanding of the database than you'll find in a typical Dapper plugin. Another is automatically dealing with audit columns (e.g. InsertedByKey, LastModifiedDate) and soft deletes. It can handle soft deletes without any modifications to your business logic. You literally just add one line of code at start up. A big emphasis is reducing the amount of tedious and easily forgotten boilerplate. Another is making security easier. You can tell it to always skip writing some columns during insert/update operations. This effectively makes those columns read-only without having to manually copy the editable fields from a DTO to an entity.
Here are some unofficial numbers. We're faster than Dapper in many cases despite having to do runtime SQL generation. https://www.reddit.com/r/dotnet/comments/7f63ps/today_i_learned_the_joys_of/ And there are further performance enhancements planed. Since we read database schema at runtime, we can eliminate expensive IsDBNull checks when materializing objects. These numbers of course don't include Chain's unique features such as inserting multiple rows with a single SQL statement and a table-valued parameter.
Not so much his post as the reactions to it. &gt;call out a little more clearly somewhere that this sub is for devs That could be a rule ;) 
I actually had all of that configured but I was still getting an error, "No RegionEndPoint or ServiceURL configured.". I thought maybe I was missing something with the credentials but you sent the same instructions so I figured mine was correct and it was something else. I came across this: https://stackoverflow.com/questions/20289257/amazon-route-53-client-exception-on-creation which solved my issue
Let me think back. I worked on a project where a Java team and our .Net team had to share models so we used XSLT schema files to generate the object models between both teams so we could stay in sync. I loosely remember having a VS plugin where I could right click on an xslt file in my project and generate a C# object model class. Also, you could run the xsd.exe to generate the files needed prebuild. 
xsd.exe doesn't handle the substitution groups in NIEM.
- docs.microsoft.com/en-us/aspnet/core/ - github.com/dodyg/practical-aspnetcore 
Normally I would say no because TPL Dataflow is based on background threads. ASP.NET hosts can be recycled at any time by the host and evaporate the data flows. But if you are using a flow entirely within the context of a single request it might work. But really, you'd be better off just using normal and astnc functions. TPL Dataflow is better suited for scenarios where the producer and consumer don't know about each other.
bro asp.net basics :P not advance tho thanks 
&gt; TPL Dataflow is based on background threads. Please stop repeating this information, it is wrong. TPL gives you limited control at best as to when and even if a background thread is used. Most of the time the Async code written using TPL or Async/Await will not spawn extra threads. Even if for some reason you do spawn extra threads its only a major issue if you have very bad code (harder to do with TPL but still possible) coupled with long running sessions or a ton of state. 
The microsoft doc are pretty good for beginner. At the middleware level, asp.net core is super simple. Try Razor pages. It is super simple. https://docs.microsoft.com/en-us/aspnet/core/mvc/razor-pages/
Is there anyway to get the output of a TransformBlock once `await bloc.Completion' is done? Or do I have to resort to ActionBlock get it.
.Core doesn't seem to include comprehensive localization support, which I find unfortunate. The resx Xml is not really that hard to write
I think the problem they are trying to solve is similar to what I was aiming for, but the solutions are pretty different. It looks like you still need to specify a lot of the details your self using this and it still passes around the string args. With CommandAndConquer you don't pass anything around. The package will handle all of the wiring to make sure you get where you need to go and to validate all of your inputs. 
A lot depends on whether you are using a pull or push block. Thankfully that's not something I've actually had to think about.
I don't know. My flaws always end in an action block that write to the database or file system. 
The real use case for TPL Dataflow in ASP.NET is in the Background Queue processing: https://stackoverflow.com/questions/36942716/why-use-async-with-queuebackgroundworkitem
TPL DataFlow looks like an elegant way to implement pipe and filter.
 https://mva.microsoft.com/en-US/training-courses/asp-net-core-beginner-18153
*Alex Jones intensifying*
Caller attributes are nice. I probably wouldn't use the file attribute unless I was troubleshooting dynamicly loaded dlls, but it's nice to know that's a thing. 
This is why I have always said that in a sense, Net Neutrality is a bandaid fix for a bigger issue. I think we need a law to say that you can't throttle or flat out censor loading websides below what the customer is paying - double dipping both the consumer and the service they are accessing is bullshit, and so is the possible censorship that could come out of this. If I pay for 60Mbps, short of technical issue or problems That being said, NN is a bandaid to a bigger issue: Corruption and deals blocking regions to competition. Net Neutrality isn't preventing competition, as I've heard other articles say. I guess better wording than bandaid would be incomplete. Without Net Neutrality, we aren't going to suddenly get competition. We are instead going to give the current existing ISPs more power. That's not going to help Local ISP 123 get pole access, permits, and all the other stuff making becoming an ISP prohibitive. As an example, lawsuits from existing ISPs against Google and the cities like Nashville, combined with slow as hell pole access, is what made Google halt their fiber operations - not Net Neutrality. 
I would consider looking at Akka.NET over TPL dataflows, especially for any kind of long lived tasks. I've tried building some things over TPL dataflows, and found they had a number of limitations for practical applications. Don't remember what they were now, but I found akka much better suited.
Xsd.exe does handle substitution groups for schema 1.0 however. They are deserialised similar to choice if I recall and tricky to get exactly right. You may need to xslt the difference away.
There isn't one for the class that contains the calling method, so the file one seems to be the only way to figure it out via the attributes.
MVA is not "pretty good", a lot of smalltalk, not to the point. The information density is pretty low.
Yeah, a few days ago i read a somewhat-strongly-worded post from someone who dislikes Pluralsight. Consider the context of who/what i was responding to - a beginner seeking a free, quality ASP.NET tutorial. MVA's offering ("ASP.NET Core - Beginner") is two weeks old, and i think it's fair to say packing a shit ton of info into a beginner's tutorial is the wrong thing to do. Scott seems to slow things down and play dumb sometimes on purpose, to let things sink in a bit. Sounds pretty good for a beginner looking to learn about ASP.NET to me. To make the blanket statement that their content is full of smalltalk and low-info-density is just silly. There is a LOT of content there. It's kinda like... Pluralsight-Lite. That said, my calling the site "pretty good stuff" is subjective. Tutorial X from Site Y may not suit Person Z's learning style or experience level. /rant-ramble
Sorry, I haven't seen that video series. But I have seen a few others, the 70-483 for example. Its the series for the exam, but it was a pain to watch. "How are we going to name this variable? - let's call it foo. - no let's call it bar. - right bar that's a good name." This is really something from the videos, the wording is a little different but the feeling is the same. 
Bit of a story about my first attempt at getting something running on on old Pi.
Mono != .Net. The title made me assume you made .Net Core work on an old Raspberry Pi, since I believe they only have runtimes for arm32, we're still waiting for a compiler/runtime for arm64 (I may be wrong about that, I don't follow that side too much). Congratulations on getting Mono running none-the-less. Always fun tinkering with things :)
Thanks, it was a lot of fun getting it running. And yeah, I meant to imply any flavour of .NET would do. 
You can run core things on raspian, but you have to compile them on something else. https://github.com/dotnet/core/blob/master/samples/RaspberryPiInstructions.md
Funny you are in this thread, Dave, I was just about to give you an honorable plug! OP: one option (throwaway lunchtime also mentioned) that my team took is in between web forms and the rewrite needed to go to .net core. You can convert a Web Forms app into a Web Application project and start converting pages slowly over time. We took this route mostly to add a REST API. I essentially just followed [this article by Dave](https://www.davepaquette.com/archive/2013/12/30/so-you-inherited-an-asp-net-web-forms-application.aspx) (poster above me). 
https://stackify.com/net-core-2-1/
this only works on arm v7 though. 
I've recently been going through a similar saga, starting from a Zero W. I wanted to use .NET Core at first, so moved up to an old Pi 2 that I had, but ended up back at mono anyway. Since I'm going the Mono route, I guess I could go back to the Zero W. The project I'm doing is way overkill for anything that has an OS, but also trying the "What do we have on hand" model.
Great link! Ultimate evidence that i suck at googling!
Can't you install Windows 10 on them and run UWP (.NET Core) apps?
Why didnt you use ifconfig on the raspberry to get the ip?
A great example of the friendly solution for the specific problem. And a good post also!
Hi. Cool didn't even know that they had that service. I just made a small console app you can try out: https://pastebin.com/6n6zWFNZ Made with DotnetCore, but should work in normal dotnet aswell.
Windows 10 Core is only supported on B and newer models.
Hey mate, thanks for the taking the time to reply, I appreciate it. Going to play around with this now!
I tried another approach using `optional` library ``` //Load the rss listed at opml subscription file var syndication = Option.None&lt;List&lt;ComplexSyndication&gt;&gt;(); (await RenderPipeline.OpmlReadingAsync(subscriptionListFile.ValueOrFailure())).MatchSome (opmlXml =&gt; RenderPipeline.OpmlParsing(opmlXml).MatchSome( (opml =&gt; RenderPipeline.GetSyndicationUri(opml).MatchSome( (async uris =&gt; (await RenderPipeline.ProcessSyndicationAsync(uris)).MatchSome( syndications =&gt; syndication = Option.Some(syndications) )) )) ) ); //Read the template file and render the rss content var output = Option.None&lt;string&gt;(); (await RenderPipeline.TemplateReadingAsync(opmlFile.ValueOrFailure())).MatchSome (template =&gt; RenderPipeline.Render((template, syndication.ValueOrFailure())).MatchSome( o =&gt; output = Option.Some(o) ) ); await output.Match( some : async doc =&gt; { context.Response.Headers.Add("Content-Type", "text/html"); await context.Response.WriteAsync(doc); }, none: async () =&gt; { context.Response.Headers.Add("Content-Type", "text/html"); await context.Response.WriteAsync("Error"); } ); ``` 
Well, fingers crossed but if they merge my DbProviderFactories PR, it will at least bring back that old friend from the past :) 
I am doing short lived tasks - I'm just trying to figure out what's the nice way to pipe and filter.
Don't you like the new options system?
Why is this here?
Hi, I've been using for since the early versions of .NET Core 2.0 (including building both ASP .NET Core projects and .NET Standard 2.0 libraries) on both Linux and Windows without (many) issues. Not sure what is the exact problem you are facing, but maybe the following sample will help: #tool nuget:?package=GitVersion.CommandLine&amp;version=3.6.5 var target = Argument("target", "Default"); var configuration = Argument("configuration", "Release"); var solution = "Solution.sln"; string version = null; Task("Clean") .Does(() =&gt; { foreach (var project in ParseSolution(solution).Projects) { DotNetCoreClean(project.Path.ToString()); } DeleteFiles(GetFiles("./artifacts/*.nupkg")); }); Task("Version") .IsDependentOn("Clean") .Does(() =&gt; { version = (GitVersion(new GitVersionSettings { UpdateAssemblyInfo = false })).NuGetVersionV2.Replace("unstable", "preview"); }); Task("Restore") .IsDependentOn("Version") .Does(() =&gt; { foreach (var project in ParseSolution(solution).Projects) { DotNetCoreRestore(project.Path.ToString()); } }); Task("Build") .IsDependentOn("Restore") .Does(() =&gt; { foreach (var project in ParseSolution(solution).Projects) { DotNetCoreBuild(project.Path.ToString(), new DotNetCoreBuildSettings { Configuration = configuration, ArgumentCustomization = args =&gt; args.Append("/p:Version=" + version) }); } }); Task("Package") .IsDependentOn("Build") .Does(() =&gt; { foreach (var project in ParseSolution(solution).Projects) { if (project.Name.ToString() != "tests") { DotNetCorePack(project.Path.ToString(), new DotNetCorePackSettings { Configuration = configuration, OutputDirectory = "./artifacts/", ArgumentCustomization = args =&gt; args.Append("/p:Version=" + version) }); } } }); ```
Awesome. Glad to hear someone else is doing it without issues. I'm gonna give it another whirl when I get home using your gist as a reference. I haven't tried netstandard, just netcoreapp, so that might be the issue. Fingers crossed. Thanks for the reply!
What exactly do you mean with new options system? 
I meant that you have a EF DbContext and you pass in the constructor an object that sets up the usage of a specific provider. Long story short, what DbProviderFactories allow that we cannot do already?
I've sometimes had a similar problem that turned out to be incorrect .Net versions. If I am creating a .Net 3.5 project and try to reference a DLL built for .Net 4.5, that reference will not work.
Yea it's really cool. In the field that I work in, we have people script narrated powerpoint slides or audio only media. Then they record and it gets processed by our multimedia team. They cut out "umms" and "ahhs" and other mistakes. Sometimes if there's too much background noise (if author decided to rent mic and record at home), or if for some reason content is incorrect then it may need to be re-recorded. Then it goes for captioning. The transcript and captions are for accessibility requirements. What if I could cut out the recording/post-production process and allow changes to happen more easily? In our cases the voice does not matter and the talking head never visually appears anywhere. The audio typically ranges from 3-6 minutes. There are limitations with AWS Polly: http://docs.aws.amazon.com/polly/latest/dg/limits.html To get around them, one can use batch processing: https://aws.amazon.com/blogs/ai/create-audiobooks-with-amazon-polly-and-aws-batch/. I'm not really happy with this solution since it could break 1500 characters at a random period that may be there by mistake, abbreviation, ellipsis or other situations I can't think of. So far I have done the following which works in a .Net Core application. You will see that I've combined 2 voices in one file: public IActionResult Temp() { string ssmlInputText = "&lt;speak&gt;&lt;amazon:effect vocal-tract-length=\"+5%\"&gt;Hi, &lt;amazon:effect vocal-tract-length=\"-5%\"&gt; I'm Mathew!&lt;/amazon:effect&gt; Welcome to this text-to-speech tool powered by artificial intelligence. I will read any text that you write.&lt;/amazon:effect&gt;&lt;/speak&gt;"; string ssmlInputText2 = "&lt;speak&gt;&lt;prosody rate=\"medium\"&gt;Hi Mathew, I'm Salli! I can be a part of the conversation, perfect for scenarios such as interviews or podcasts.&lt;/prosody&gt;&lt;/speak&gt;"; //https://chrisbitting.com/2017/04/07/using-amazon-polly-from-net-c-get-mp3-file/ AmazonPollyClient pc = new AmazonPollyClient(Amazon.RegionEndpoint.USEast1); SynthesizeSpeechRequest sreq = new SynthesizeSpeechRequest(); sreq.TextType = "ssml"; sreq.Text = ssmlInputText; sreq.OutputFormat = OutputFormat.Mp3; sreq.VoiceId = VoiceId.Matthew; SynthesizeSpeechRequest sreq2 = new SynthesizeSpeechRequest(); sreq2.TextType = "ssml"; sreq2.Text = ssmlInputText2; sreq2.OutputFormat = OutputFormat.Mp3; sreq2.VoiceId = VoiceId.Salli; var SynthesizeSpeech = pc.SynthesizeSpeechAsync(sreq).GetAwaiter().GetResult(); var SynthesizeSpeech2 = pc.SynthesizeSpeechAsync(sreq2).GetAwaiter().GetResult(); using (var fileStream = System.IO.File.Create(@"c:\aws\"+ DateTime.Now.ToString("yyyy-dd-M--HH-mm-ss") +".mp3")) { SynthesizeSpeech.AudioStream.CopyTo(fileStream); SynthesizeSpeech2.AudioStream.CopyTo(fileStream); fileStream.Flush(); fileStream.Close(); } return View("Temp"); } I will have a view show a textarea box that allows 1500 characters to meet polly restrictions. Users can select a voice/language and use ssml. Users can add more boxes and preview each box individually (need to figure this part out, thinking I'll have each file processed in s3 then played back and all preview files are deleted after 24 hours). Textarea boxes will be an array to be compiled into one file. Then they can click download to have all of the files processed into one mp3 and save in s3. Then mp3 downloads for them. I have no experience with AWS or creating/playing mp3's prior to this. So I don't know if this is efficient but it's how I'm picturing it in my head right now. I'm sure there will be room for improvement.
I just realised my post on SSL isn't using SSL lol. FWIW it's on a server I have limited access to but I'm moving it this week (where it will be using SSL!).
Huh! I just started getting the same error in a project with similar structure that has been running fine, until today. The only thing that has changed is me using visual studio to the latest version x.x.5. VS bug?
I switched from ".net standard 2.0" to 1.6 and I could include the namespace but unfortunately it said that .net core 2.0 is not compatible with standard 2.0
I haven't tried to build the project before the update so that could be it
I will try building my project on my laptop which is still running a older version and see if that works. However I left it at the office when I went home so I'll get back with the results in ~12 hours. üôÇ Hope it's just a VS problem... I mean it can't be us... Right? üòâ
You moved ApplicationDbContext? I don't know which method you used to create it, but there is quite a bit of configuration and generated code when using the EntityFramework. Instead of moving *automagic code* to Project.Data, can you rename *automagic project* to Project.Data and move your library to that project? Or you can try configuring old Project.Data yourself using nuget http://www.learnentityframeworkcore.com/efcore/how-to-get
Dont have more textboxes. Simply run with 1 textbox and split them into several requests. If the split is in a word simply backtrack until next . You will most likely never have a request that hits 1500 chars, but instead 1450 - 1475. Hope this makes sense.
I've thought about this but what about abbreviations (st. Johns), ellipsis, typos? If the sentence gets cut off incorrectly, it doesn't sound right when Polly outputs it. I think the best option is giving the user control, it allows for multiple voices and previewing small chunks if the script instead of everything everytime. Sometimes we have interview type scenarios with multiple speakers. Since this won't be a public facing website, I can train the one person that would use it. Also I'd provide instructions and over time improve with user testing.
What are some of the requirements you need it to do? I am actually writing a VERY VERY lightweight content(and content only) management system in .net Core right now, but not quite ready for the real world yet is why I ask. Always curious about what people want to not only offer suggestions, but to improve on what I am building.
I haven't tried it myself but you could also check out orchard. http://www.orchardproject.net
Well one of the main requirements is that it have a robust/thurough API/framework to make it extensible. One of my main problems is with other systems, and not just .NET it takes so long to learn their way to get things done that it would be much faster to just do it yourself. Another requirement would be that it handle content linking/references between content easily...like wikis, but also with contributing user linkages 
&gt; Another requirement would be that it handle content linking/references between content easily...like wikis, but also with contributing user linkages You mean like linking to another page without using an anchor tag in HTML, but just making a reference to an object in the CMS, and the renderer would make that an anchor tag for you?
Paid product? I used EpiServer a few versions ago and one the things I liked about their approach was that it felt like they enhanced the platform I was working on (Asp.Net) instead of creating a new platform which happens to be on Asp.Net. 
Hmm..looks interesting. Built on new asp.net MVC, which I like. Looks easy to install/configure even on cloud/azure/hosted servers. Thanks! I'll check it out.
well, i'd be fine having to put the tag in there, not HTML necessarily but almost like a hashtag or @ or whatever that refers to a named entity within the existing content taxonomy. But just making a reference could present you with some options ideally.
Well I was hoping for an open source one, but not opposed to paid for the right now. I like what you said about it not being a new platform entirely that just so happens to run on asp.net. I'll check it out! Thanks!
People can port code to netstandard orm can preregister factories, and ef isn‚Äôt good example, it doesn‚Äôt use dbproviderfacory instances but almost all others do
Excellent tutorial! Nicely explained and demonstrated.
Here's a little more detailed list... https://github.com/dotnet/corefx/milestone/12 
Try deleting bin, obj and .vs folders. Sometimes changing namespaces when switching Git branches causes issue for me.
It seems strange to me that there isn't an IMiddleware interface. Anyone know the reasoning for that?
I think since Invoke supports additional parameters, it is more flexible than a ridged interface.
I've successfully extended Piranha CMS. Give it a look see.
But it will be nice to have, it's tiring to always google "asp.net core middleware" to remind myself the signature.
That's the point since the signature can change without renaming the method. I remember Scott talking about it on one of the podcasts.
There *is* one, it's just not well explained how it gets used and in what circumstances. Also, my experience right after reviewing a few overviews of middleware that looked pretty similar to this is that there's still more to it. E.g. my customer middleware matches the constructor and invocation conventions illustrated, yet for some reason it still isn't working. Depending on what I fiddle with, the ASP.NET framework either bitches that I didn't register my type with what I assume is the service container, or the app process completely crashes when I try to start it (i.e. no exception info is provided).
I'd prefer Orchard, if I need a more complete solution. The thing is, it's anything but lightweight. So if you need to develop something based on a lightweight CMS, C1 Orckestra would be a better alternative. If you are into dotnet core, Platformus seems to a better candidate, which is actively developed and in early stages. So that you can assume it as a boilerplate. 
https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.hosting.iwebhost?view=aspnetcore-2.0 It represents a configured web host. The interface is `IWebHost`. Kestrel is the underlying technology used to accept requests used by this configured web host. It feels abstract to you, because it's an abstraction.
Not a **single** mention of the chain-of-responsibility pattern?
The most interesting frameworks were * VisualMutator * NinjaTurtles But as you said, incomplete or abandoned. I forked VisualMutator in the hope that one day I would have the time and will to try to continue and port. Haven't succeeded at that yet.
Check out contentfull, we love it.
The ASP.NET Core version is amazing (https://github.com/OrchardCMS/OrchardCore) although it is still not yet 1.0.
I'm having some experience with SiteCore. It's incredibly powerful and extensible, I even used it to control billboards, but it's very much aimed at the enterprise market with the associated price tag. Just to be clear, what do you want to achieve? A site for a small business has completely different requirements as one that gets 1000000+ visitors a day.
Big problem here in NZ with SiteCore though, as it is very hard to find developers with experience in it.
You could always try Peachpie which brings WordPress to .NET (and there's no bigger CMS than WordPress) ... https://www.peachpie.io/
We only use SiteCore. https://www.sitecore.net/
Episerver, i've been working wirh it for almost 3 years now. Its pricy but damn it beats the shit out of sitecore!
&gt; I also often here people saying ASP.NET Core is now "self hosted" in its own process. What does that mean when we say 'process'. Basically the website runs as its own service in Windows (or OS of your choice) instead of running as a part of IIS. 
I was kind of expecting to read a little bit about this too. Or at least see a reference to it. 
Tried it and didn't like it for the following reasons. * lack of IDE and suggestion support * Extremely difficult to troubleshoot if something doesn't work * Poor documentation on both the first and third party tools * Steep learning curve if you've never used it before. This makes the project more difficult to pass of to others. * Creates another (what I thought to be) unnecessary layer of tooling to maintain. I've never really had a project that was so complex that I couldn't handle it with other scripts. * Requires other dependencies to be installed on my machine, which makes me feel like I'm using NPM, which I can't stand sometimes
We do at our shop. I've been around long enough to not drop everything for whatever is new and shiny at the moment. I always ask, what is this going to give me that I dont already have? How many more moving parts (that may break) is this going to introduce? How much stuff is happening by magic, under the covers, that I cannot get to if needed? 
I tried it but it didn't work, I even tried to reference just an empty class created in the library(without moving any files) and it still didn't work.
And now my post on SSL ... is using SSL :-) https://coderscoffeehouse.com/tech/2017/11/24/aspnetcore-ssl.html
Are you by any chance using resharper ? I update mine today and everything works now
Holy crap you're right! And you'd think I'd have noticed that. https://www.exceptionnotfound.net/chain-of-responsibility-the-daily-design-pattern/ I'll have to work that in. Thanks for pointing it out!
One of my favorite deep-dive books is [CLR via C#](http://a.co/iyJx2q8) by Jeffrey Richter.
This book is a great resource for learning how C# and .NET work. It's what I read when preparing for interviews to make the jump into .NET. Supplement it with a Pluralsight subscription for higher level concepts and technologies like OOP and WPF. It's well worth paying for a couple of months if it expands your skill set and lands you a better paying job.
We keep them separate so that we can do independent scaling &amp; deploys of front/back end
One solution with two projects?
I prefer to keep the two separate. I typically have an API project which houses the web services and a Web project which serves the browser side scripts and content. FYI - you can have two startup projects in your solution which gets negates the need for opening two projects/visual studio instances (I'm assuming you mean two solutions). I don't see how the ports are a big factor.
Great points, which I 100% agree with myself! In fact, to the extent I created my own little _"rant"_ around it, which you can read here - https://gaiasoul.com/2017/10/17/old-is-the-new-new/
Separate is the best for flexability. 2 projects in 1 solution. One of the projects is the API and the other is for the SPA. Set up your solution so both projects start up when you debug. The template is fine for the SPA project, but I've built it as just a plain angular site. Since it sounds like you are not familiar with multiple projects in a solution, I have one more suggestion. Separate your business login to its own class library project in your solution. Each layer of your application is a separate project in your solution (Data, Business, Web, Core, etc) 
Wow thank you I am 5 years old and now understand LINQ
Very good article, thanks for the link!
What's your thought on the Angular template? I feel like it's pretty hard to grasp with that huge webpack file and just swapping from bootstrap 4 was a hassle.
If you want to get to know C# inside and out, I recommend [C# In Depth by Jon Skeet](https://www.manning.com/books/c-sharp-in-depth-fourth-edition). Look Jon up on Stack Overflow.
What is this nonsense?
when you are 5 you will understand
How much scaling do you need for delivering static files? That should ideally just be part of a CDN.
&gt; as its own service Don't confuse this with Windows services. That is not related. It just means it's a simple console application.
Yes it does work. My bad. I was trying to get it to work in a file using Serilog. Serilog.AddFile() apparently doesnt support scoping. So i have to go try the other suggestion
How do you easily run them without having to go between VS CODE for the client side and VS 2017 for the server and running both every time ? 
ohhh.. set up them both to run at startup.. though I never could get a regular angular project to run in VS2017 (using f5).. I always have to goto command line and npm start
Ive done the server side separation.. But things don't act like I am used to when I add a 'web site' to a solution.. F5 would not just work anymore.. but Ill revisit that
Webpack is magic to me so I just leave it alone.. You swapped out bootstrap 4 for something else? This nice thing about a separate project is you can start with a nice template using whatever client UI you like and go from there 
Sorry, I worded that poorly. I struggled with swapping from bootstrap 3 css to bootstrap 4 scss and it was very confusing. Same with including font awesome. I‚Äôm just scared to committing to the template since I‚Äôm not too familiar with webpack. It‚Äôs be nice if there was a template with just the minimal requirements for it to run!
It would be nice if there was a template that spilt it up for me and could F5 right away but I guess well figure it out. couldn't be that hard
Good luck! I‚Äôd love to hear ehat you guys come up with.
Create a solution containing all application solution projects. Use this solution to edit the reusable class libraries. This way you can be sure that any change won't break compilation in any application. Then you can have separate solutions for each application if you want to run them at the same time. Though any change to the common library's requires that you restart all applications. With that said, this is probably not a good practice.
Yep, You nailed the two problems there. There are a few things you could try doing to mitigate: 1. Implement some sort of CI process that builds and tests each solution routinely (on check-in) and notifies you when there is a failure. 1. You could distribute the 'Common' solution as NuGet packages and have each of the consuming projects install them. 1. Have just one solution put everything in there and pick the startup project based on what you are developing. There are obviously pros and cons to each approach.
Yes, on number 1, we have bamboo plan on bitbucket server check-in. We also have code review process as well. But we would rather have something configured so that all maintenance goes through a "Common" solution, and have the projects as readonly in the "Application" solutions, leaving the CI plans as sort of a last line. It wastes time when it isn't caught early. Anyways, most of this lines up with what I was thinking about doing, just wondering if there was a better practice. 
Uhhh
The problem with this is that when someone wants to check out from our repository, they have to get everything. All solutions. And nothing is separated past code check-in. Thanks for your suggestion.
This is pretty cool. I've been using LINQ since pretty early on, but I have always suspected peformance problems but never cared because it allowed me to do so much in the app layer when I'm too lazy to dick around with SPROCS etc....and I use it for small usage apps..nothing enterprise...
I'm 4 years old and what is this? 
No, you can't really make the project 'readonly' like that. The best you could do is distribute the compiled binaries, preferably by NuGet. You may also run into a problem eventually where you want to introduce a breaking change into your common solution, but don't have the time/need to go through the hassle of updating all the consuming projects to account for that change. Using a package manager would let you use different versions of the assembly in different projects. Then you can update the projects on your own schedule.
It also doesn't work with docker support which is disabled through the template selector but it's possible to add it after the fact... Which then reveals that the module folder is so loaded with packages that they lost track of what's in there what their quirks are. Because just webpack alone requires the container to provide node to be installed to be able to precompile the bundle. Node is not the actual binary anymore. The binary is called nodejs thus webpack doesn't find the binary
RemindMe! One Year
I will be messaging you on [**2018-11-29 21:17:01 UTC**](http://www.wolframalpha.com/input/?i=2018-11-29 21:17:01 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/7ge3sx/explaining_linq_to_a_5_year_old/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/7ge3sx/explaining_linq_to_a_5_year_old/]%0A%0ARemindMe! One Year) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Nuget package the shared project. The interface is a subproduct that indicates product needs to owned, maintained and versions separately. You can do shared projects but the ultimate answer to all shared project troubleshooting is that shared project are code smell/lazy management &amp; communication
The only time I encountered significant issues with Linq was when I was helping a friend work on a project in Unity Game Engine. I automatically used linq bu had to replace most linq with basic for loops.
I just use visual studio code for both. dotnet watch run the server, ng serve the client. Edit both and have edits happen for front and back end at the same time.
interesting..do you know the reason?
I'm 36 and I'm more confused now than I was before. I've heard OF LINQ, and I've had coworkers write it into my apps before, but... It's all black magic fuckery to me.
Nice picture. Now do Aggregate.
Its just an easy way to manipulate collections. Just mess around with Select and Where with a list of basic objects. You'll understand pretty quickly. 
The main loop in the game runs often and lots of linq means lots of enumerators getting created. https://developer.microsoft.com/en-us/windows/mixed-reality/performance_recommendations_for_unity
Thanks for the nice words :)
I also dabble in Unity, so this is good to know..thanks
Because my router is on the other side of the house and I didn't have a second HDMI monitor to plug in.
It‚Äôs very overhyped IMO, although mainly its name and everyone acting like it‚Äôs something totally unique. It‚Äôs basically a bunch of methods on sequences and collections. 
I have completely separate things. 1. Web-API project and class libraries and tests in .NET solution with swagger (err OpenApi) generated wrapper site 2. angular CLI project folder with tests They fundamentally are two projects for us. We are careful to not introduce breaking changes in the the swagger spec. Clients may run multiple versions of the angular project (we are willing to do custom one-offs for a client of the angular project with the understanding that this is a dead end thing and they will not be upgraded automatically with front end changes [for example we may deny them features like new page change animations that do not exist in their SPA instance] when this happens). It helps not to think of the SPA and API as ports of each other or something like that: * The SPA is an app that interacts with a number of defined APIs (mostly just ours and google analytics) to present data and facilitate user interaction with that data. * The API app is a collective set of functions that protect the data, store and manipulate it, and operate on it (notably it has no UI that a user interacts with). Disclaimer: I am in a domain where we provide application instances that our clients brand for their users. I don't have a one to one API/SPA configuration. The API we have is available for our clients to use directly if they desire as well (well it would be, none of this is shipped yet; the shipped solution is still webforms based and the API and client site are the same brownfield thing with a bewildering array of options and features to allow us to continue updating it while not changing any particular instance look-n-feel).
That's what I was looking for!
Damn, you have a great .NET group!
I once had a Linq query like this: places .SortBy((a, b) =&gt; (a.Latitude - b.Latitude) * (a.Latitude - b.Latitude) + (a.Longitude - b.Longitude) * (a.Longitude - b.Longitude)) .First() I assumed it would perform terribly, but in release mode the entire thing compiled down to a very tight loop over a flat array (no indirection) with SSE instructions to vectorise the subtractions, multiplications, and even the loads and stores (both fields in one instruction).
... Is SortBy LINQ? I thought the method was OrderBy
Sorry, yes, it should OrderBy!
That's cool...and I wish I was a good enough programmer to fully comprehend what you are saying :) 
2017 is LEAPS better than 2015 in performance. 
Wow this makes programming managed code in unity sound like a nightmare. All those hoops to avoid gen 0 collections don‚Äôt seem worth it, why not just use an unmanaged language at that point.
LINQ also has a lot more function calls I believe. The compiler may in-line them, but last time it profiled it sure didn‚Äôt seem like it. Foreach loops were faster but literally 99% of the time it doesn‚Äôt matter. Just be judicious and don‚Äôt put silly .Where clauses in a tight loop where a dictionary or something would make more sense. Prepare data first, then loop on it wherever possible.
The problem is cartel, the reason why there is cartel is possible is because of lobbying / corruption. Now instead of fixing lobbying people what to make another law to fix the problem caused by lobbying.... where such a law is decided by the same people who got lobbied and corrupt in the first place.
Yeah it's not unique, as every modern language has collection operations. On the other hand, they wouldn't be everywhere if they weren't useful.
Because ... malloc??? 
I've used a simple Database project in the past. It works just fine
 &amp;nbsp; *I've used a simple* &amp;nbsp; *Database project in the* &amp;nbsp; *past It works just fine* &amp;nbsp; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;*^-sloloslo* 
If you mean SSDT in Visual Studio, that might be a problem :) I'm developing in VSCode in Linux. I'm very comfortable, perhaps too comfortable, in Windows and Visual Studio so I'm trying to branch out a little. Unless, of course, there's a way to do a database project in Linux now that SQL Server is available.
I do mean SSDT in VS. Sorry
Stick to FluentMigrator
How does skip while work? 
Sorry for getting back so late! Got held up with... meetings instead of programming at work yesterday. I tried running the project on VS 15.4.4 and everything worked great, just as before. Yes! Thanks for the tip, I will try updating ReSharper and see if it solves the problem for me as well. :)
**Reference vs. value lists** Sometimes lists are kept as an array of references (memory addresses, pointers) to the items contained therein. The memory might look lik ehtis: `some address; some address; some address; some address ... name; latitude longitude .... name; latitude; longitude ... ... other stuff ... name; latitude; longitude ....` This is how lists of objects work in C#. It adds an extra memory lookup when you loop over the items which can be a problem when they are scattered in memory. With lists of values, like a list of structs in C#, the structs sit side by side in the list and can be iterated over quickly, without external lookups: `name; latitude; longitude; name; latitude; longitude; name; latitude; name latitude; longitude` **Vectorising arithmetic** Modern processors have large registers (very quick to access, but scarce memory) that can hold several numbers on which an operation, like addition, multiplication and so on, can be done in one action. Hence, the operation is done over a vector of values. **Vectorising loads and stores** The runtime identified that the latitude and longitude values live side-by-side in memory, and used one instruction to load the range covering both addresses into a register, rather than loading them separately. Hope this helps!
Skip over the object when it meets this condition. Once you encounter an object that does not meet the condition, take everything from that point (even if subsequent items meet the condition, doesn't matter anymore)
From what I can tell it iterates through the collection and skips each element that matches the parameter. So since the parameter was circle, it skipped the first two elements because they were circles and then returned what was after that. 
For the groupby, wouldn't the result put squares before circle, because the first square comes before the first circle in the input collection?
Wow I never understood LINQ forever and now it all clicks together. Now if only you could do the same for Garbage Collection. 
You could take a look at DBUp. Its not top bad either
Very nice article. If anything, I would argue against that `GetAll` returning an `IEnumerable`. But since it wasn't the point of the post, I won't argue about it. üòú 
My only complaint would be that Where() and OfType() are exactly the same in this diagram. I guess that's the consequence of using types and values interchangeably.
Many of the ‚Äòstandard‚Äô repository interfaces define `GetAll` as `IEnumerable`, that‚Äôs why I defined it as such. But I very much agree. I would even say that a `GetAll` method is wrong; you should define specific methods geared towards your use case. If you need to find all of the orders that include a specific product, define something like `GetOrdersWithProduct`, instead of getting all of them and filtering afterwards. 
I usually let my team define a "find" method that accepts a list of filters. But the method returns a finite list (often a 'IReadOnlyList`. But it seems we agree that returning `IEnumerable` is leaking your abstraction and letting the user of your repository decide the querying strategy rather than leaving it to the repository itself.
The get methods should return finite and read only lists, not open and potentially infinite sequences. 
I agree, and my statement still stands. They don't contradict each other.
I never said you were wrong ;) `IEnumerable` better than `List` :P
[Flyway](https://flywaydb.org/) is pretty good, pure SQL. 
Out of curiousity, is this getting getting a bearing from two latitude / longitude coordinates?
Yeah that's what I felt as well, there are so.many.packages. Which doesn't have to be bad but a template is supposed to be somewhat simple not bloated with so much stuff.
I wrote it from memory and made a mistake, it should only take a point A in de lambda. B is external. It finds the location closest to B. It's a reverse geo lookup. I'm aware that lat/lng aren't Euclidean but for our situation the error is negligible.
Now I think about it mine was a stupid question, a few sin and tan functions short haha
In your `ConfigureServices` method When setting up the `TokenValidationParameters` you seems to be setting the audience incorrect `ValidAudience = Configuration["Tokens:Issuer"],` Also in your `appsettings.json` you have this right? { "Tokens": { "Key":"yoursecret" } }
Thanks. Yes, good catch but that's not the problem, Issuer and Audience have the same value. I was going to dig down into what they actually represent after I got a basic example running: "Tokens" : { "Issuer" : "http://localhost:5000/api/", "Audience" : "http://localhost:5000/api/", "Key" : "0123456789ABCDEF" }
I'm not 5 years old and no longer understand LINQ
In the `TokenValidationParameters` have you tried to set `ValidateIssuerSigningKey` to true?
It's not stated anywhere, but I had an issue until I increased my secret length to around 20 chars. Check the output window for messages.
I need to get better at watching pluralsight. I know a lot about C#, but I don't know as much as I'd like. I'm not super familiar with WPF (I've just never needed to use it, and now probably 100% of my work is .Net Core running on a Linux system.) But when I go to watch the videos, I just get bored :/
Thanks. I added it but still same error. I'll create another project and try to do it from scratch, maybe I'm missing something but I'm blind to it. 
I don't know if Unity supports other languages, but C# without linq is still far better than javascript
So, I agree in general : We need to fix the root of not just the NN problem, but all the problems. Which is lobbying, "donations", and in general corruption. And I agree : The biggest problem with becoming an ISP is not technical, it's legal issues via city deals with ISPs to own regions, pole access laws being a pain in the ass, lawsuits against competition trying to move in, etc. I mean, even without that you will still have a monetary problem your average Joe isn't going to be able to overcome without a lot of capitol to throw at buying equipment and running fiber. Unless of course we're talking about city and state run infrastructure leasing out to ISPs. That being said, I think NN is a bandaid fix that's keeping bigger problems at bay. NN isn't hindering competition or innovation. It's not stopping new ISPs from popping up. I have friends in other countries who have NN concepts and have way more competition than we do here. So to me, getting rid of NN is only going to make things worse, without fixing the issues that will make them better first. Even if they do fix the core problems with all of it, I still think there might be a place for NN. I mean, there's still a free speech issue. Companies shouldn't be able to throttle my blog out of existence because they disagree with it. &gt; You just can't fix a broken system by adding a broken law made by the same people who broke the system in the first place. I say the same thing to friends of mine, actually. I knew some people who supported Bernie back in the election (though their secondary choice was Trump - the fuck?) I explained to them: Sure, let's give it the benefit of doubt and say there is an ideal plan that would allow this country could support universal health care and basic income or whatever. The problem is, we won't get the ideal plan. We will get a plan formed by the very people who are corrupt and slowly causing the system to get fucked in the first place. We'll get a plan that does more harm than good, in the name of good. If you have rusty and corroded pipes producing dirty water in your house, it doesn't really matter how clean the water coming in is, it's still going to get discolored and dirty.
I don't see how this explains a damn thing but I guess the rest of this thread feels differently.
I agree, I don‚Äôt think it does. Just a bit unsettling that that article was talking about how gen 0 collections cause glitchy images. Maybe it‚Äôs not that bad in practice. It doesn‚Äôt seem feasible expect people to write code that avoid any GC collections in C#. 
Follow this guide: https://blogs.msdn.microsoft.com/webdev/2017/02/14/building-single-page-applications-on-asp-net-core-with-javascriptservices/ When you type in `dotnet new angular`it will create a single project with both Angular &amp; .NET Core
What's the point of this? If you open the VS Code extension tab these are the top few recommended extensions anyway...
Samples? I haven't run into any. VSCode/Angular CLI/VStudio with multiple projects works for me. It doesn't make sense to me to smush everything together into one. 
It should work without any problems how you describe it. Although you need to have running both projects at the same time... The Angular code normally depends on the .Net core WebApi controllers.
Yes. I think that is the way to go.. I was just trying to find some walk throughs that go that direction.. since all the ones I find go the combined project route
Yes. I am just trying to get it to a point where I can press F5 and they both run.. Have not been able to get that to work.. 
It's quite simple. Assuming you know your way around .net, you'll need node, and angular cli for the front end portion. Angular cli can easily serve up the front-end during development. Integrating with the back-end happens later, and you should know how to cross that bridge when you get there.
And handle CORS, but that's simple
Ugly work-around: 1. Create a Console application that calls `Process.Start("ng serve")` 2. Set both projects to run when debugging. (Right-click on the solution. It's in there somewhere.)
Not sure if you've tried this, but I would imagine if they are in the same sln with two different projects, you could use the "Set Startup Projects" option you see when right clicking the root sln in visual studio? You can choose "Multiple Startup Projects" and select both your angular project and .net core project. They should both start when you hit f5 with that setup.
I posted what I use [a while ago](https://www.reddit.com/r/dotnet/comments/6c6bq1/noob_questions_about_spa/dhsui14/). It's not in two projects, but the underlying bits should still work and I think you can just move the 'ui' folder outside of the .NET project, modify the guplfile and change the `cwd` path for the `serve-ng` task. After setting this up, and if you set up the hook in the Task Runner Explorer, the angular development server should start as soon as you open your .NET web project.
I find authorisation and authentication in .NET is over engineered and hard to follow, god knows what a beginner makes of it. I put together this really simple template for basic user management needs (and it includes a web api) ... https://github.com/matthewblott/simple_aspnet_auth
I was about to say that one - they talked about it on .NET Rocks, it looks pretty cool (open source too)
Advertisement.
Ha. I had used your project to test if the token generation works and it did. It's not the over engineering which I find annoying but the lack of thorough documentation. Not a single example of JwtBearer on their docs site? Come on. They release new .net versions every few months, the internet is full of .net core samples which are already dated. I had the same experience with Spring framework for Java, coming from .NET I was so annoyed by all the tutorials which got me to dead ends and the documentation which was never complete. 
It sounds like you are wording your request backwards. You don't want them in separate projects. You want them in the same project. That way, when you hit F5, you run both of them at once. The default template for Visual Studio 2017 does this for you. It even attaches to Chrome and allows you to debug from Visual Studio. You can also do this with the dotnet command. If you want to create a project like this yourself from scratch, it would be very difficult. There are webpack tools for visual studio and spa services. I have not dug into those because I prefer using Visual Studio Code for my angular needs.
Hard to find any examples online but the best setup I've seen is the js equivalent of using the dll output of another .net project * leave your .net project as the startup project * set the angular project as a dependency so it builds first * set the angular project's settings to build the angular project and output it's files to a folder in the .net project then the .net project can serve the angular files because they're a local resource like any other js files hope I explained that right 
We split them. We basically serve the API and the static built files from the main .NET project, and the Angular project when built with gulp, npm, etc. just copies it's outputs to that project. Then you could setup the main project to call the build script in the other directory as a pre-build or something... but we usually just manually build JS project and then run the API project. It got annoying waiting for the stupid NG project to build every time you compiled the main project. Unfortunately, the front end stuff is all going this route where trying to integrate it back into a VS project just wasn't worth the time. Use the tooling for JS development so you follow what the rest of the community is doing, and just have it spit it's output as a blob into a location your VS project serves the file from is my recommendation. You end up having two do two steps to build, and keeping a command prompt open to run the stupid Angular / npm / gulp stuff, but it ended up being better than trying to deal with keeping VS working with the JS project, because they change the commands / build pipeline every other week.
I have an Angular 4 project that connects to a legacy .Net 4.5 (not core) project. https://github.com/savagelearning/machete https://github.com/savagelearning/machete-ui I have several build configs for angular, and build it inside the old asp.net MVC project, then commit the bundle files. 
This sub has no rules ;)
Yes you might be right. I added CORS.. I guess the rest is done exactly the same as the combined examples.. I was thinking there was more
The web site does work with f5.. I need to type 'ng serve' 
Interesting idea. I'm surprised no one has demonstrated how to set this all up with a simple F5.. I guess everyone is using VS code for the front end.. 
no.. That is what I am trying to avoid.. People prefer to separate the projects from what I read.. But I lose the simple F5 way to run things
The angular project is a 'web site' so it doesn't compile .. I think.
maybe its easier to keep the client side in VS code .. I am thinking the way javascript is you are not shutting down the browser all the time.. Often you are just making changes and refreshing the current instance of the browse... I think right now I'm not sure what to expect and was thinking I MUST have them compile/run at the same time becase that is how I am used to but maybe in practice that is not so important. 
These days people who do both C# and Angular seem to be getting rarer and rarer. It's just too hard to keep up with the changes on both sides.
Windows? What OS?
If you‚Äôre using vs code you can define tasks that need to run in tasks.json, then you can make a task group (I think that‚Äôs what it‚Äôs called, I‚Äôll double check once I‚Äôm home) that can be attached to your launch.json. That will allow you to hit F5 to run/debug however many projects you have at once.
Perhaps over engineered is a bit strong but most CRUD apps in my experience just want something simple to start with and it's not obvious how to get going. It can seem over engineered because you get pointed to all sorts of third party implementations of the underlying technologies (OAuth for example) when trying to hack something together. As I said, it's not welcoming for newcomers - try and do something similar in Rails or Django and it's a lot simpler. Agree on the docs, they're a bit of a mess. Sometimes they'd be more helpful if they don't exist as the out of date stuff can send you in the wrong direction for hours. Interesting on Spring, I have no experience on it and wondered how it compared. I like the look of Scala and was tempted but just don't get the time.
DBUp doesn't work correctly with .NET Core and runs migrations out of order. We're using this one in production and it works fine: https://github.com/canton7/Simple.Migrations
Check the tutorial and repo I created a few weeks ago: https://github.com/lugrugzo/WebApiJwt It contains Identity, Asp.NET Core 2, EF Core, MySQL.
Is it set as a start up project, but it doesn't start?
It's not so important, in fact I work on an app that is set up similar. We do use VSCode, and just serve the javascript app up. It lets you make changes to the backend without restarting the angular client, or even point your angular client to another environment while you work on it. It's convenient.
It doesn't compile to binary, but there is a build process with an output. Or at the very least just copy over the js files
That's what we do. More because everyone wants to edit in something different. Silly front end people ;-) But yeah, we treat as two separate projects. But we do distribution of just the WebAPI project... During builds it just copies the output of the client project into static files in the WebAPI project, and that's what we actually host. Our local iis points at the WebAPI project directory, and serves out the compiled client files as static files. We never use the ng serve crap.
The web server for your client can just keep spinning throughout your session. It only serves the files, and should rebuild on the fly and hot-reload, so there's no reason to ever shut it down.
 Not really fully developed, but I've been writing a program for someone that I just made open source, it doesn't do a ton yet, but it should be pretty obvious how you could extend it: https://github.com/Dispersia/Survivor-Quest uses .net core 2.0, angular 5, identity server 4, and ionic for the app (the app is where there's actual controllers for now, not the website, but... it's the same as a website)
Oh my GOSH dude I just got this to work myself today. Absolutely ridiculous. You're not alone. [HERE](https://stackoverflow.com/questions/45686477/jwt-on-net-core-2-0) is a highly upvoted SO question that helped me. This was after days of piecing together a half-dozen different examples.
Contoso University on the MSDN is a great tutorial. Wes Doyle on youtube is also putting out some good ones that are up to date for the most part. https://www.youtube.com/channel/UCfniixfhHqpIGbU7z2JCNJw/videos?disable_polymer=1
I hope you are right, for NN I can't say I see any downside upfront as far consumer and competition is concerned. (outside as a matter of principle) 
Have you tried any of these? 1 and 2 is probably what you want. http://markheath.net/post/four-ways-to-deploy-aspnet-core-website-in-azure I am starting a new Asp.net Core 2 app and want to put it on Azure too so hopefully one of these will work. I am not ready to deploy yet, though.
Check out this article. It allows two separate projects to run. It might help you achieve what you're looking for. I believe I was able to still set break points and debug. https://medium.com/@levifuller/building-an-angular-application-with-asp-net-core-in-visual-studio-2017-visualized-f4b163830eaa
I cannot comment about the site you reference, but you could try the Microsoft Virtual Academy for free... https://mva.microsoft.com/en-us/training-courses/introduction-to-aspnet-core-10-16841?l=JWZaodE6C_5706218965 
I'm a big fan of Pluralsight, and a member since 2010. Courses by Shawn Wildermuth, Scott Allen, Gill Cleeren are good. It costs money but it's worth it.
[removed]
If you are using Visual studio, its super simple, just right click on the solution-publish and you use the wizard to create required resources and generate config file. 
keep them separate. one for the api and one for angular 
Ok I understand now. I was confused because it‚Äôs a very unusual thing. I also tried to do this once. I didn‚Äôt get very far but I did some research. It looks like hitting F5 is really based on running webpack behind the scenes. There is this SPA services and Webpack project you should look into from Microsoft. I‚Äôm not sure it‚Äôll be easy to do.
https://docs.microsoft.com/en-us/aspnet/core/
another MVA option, only a couple weeks old - https://mva.microsoft.com/en-US/training-courses/asp-net-core-beginner-18153
You could keep An eye on https://mutationtesting.net in the coming year :)
I answered an SO question on this a little while ago - https://stackoverflow.com/a/47221994/969613
I'm also a huge fan of Pluralsight, and all the autors that u mentioned. But acces to pluralSight can be free - if u search Microsoft MSDN program subscribscions, they give u free 3-months pluralsight access (over this year i made 3 or 4 microsoft accounts just for this puropose, call me an onion).
I've tried that, it oublishes fine but gives me the error HTTP Error 502.5 - Process Failure when going to the webpage.
Have a look here: https://github.com/aspnet/Hosting/issues/844
I've looked at that, but it seems to be mostly to do with it being core 1.0, they mention web.config and project.json which don't exist in my solution.
Didn't you hear? Udemy has great online courses
Enforce HTTPS at the web host level not the application level, I.e. IIS, NGinx, Apache
I would create a database project in your solution which has all your tables, stored procedures, functions, users, permission scripts, etc. And then when you publish that project it will know how to migrate those changed
Thanks I think I‚Äôll go with that one. 
search for kudvenkat videos on youtube, he has around 160 videos series for asp.net and 100 video series for MVC, explains in great detail! 
Not if you want SSL throughout your data center.
I would honestly go to the aspnet githib and go look through some of the examples that they have, and pull down some sof the frameworks like the mvc one for example. You'll learn a lot from those. Otherwise what do you mean by "more advanced stuff" ? What type of projects are you looking to work on? UWP, API, Web, Mobile?
Well... I think this more dotnet patterns than angular one. The whole article is tldr; serverless, microservice or monolith. Take your pick. More interesting: Should you use redux / ngrx/platform? If so, pure redux with the redux ecosystem (eg. undo), or ngrx with observables? Should you use container components and display components? Is "foo | async" with observable properties a pattern or anti-pattern? Are 'wrapper' components a good solution to the lack of HOC in angular? (eg. for drag and drop) Module introduce a bit of overhead for components, but the new material implementation has each component in its own module (https://material.angular.io/guide/getting-started#step-3-import-the-component-modules). Good practice, or ridiculous? *Real* decisions like should you use the dotnet core scaffold with angular/c# debugging in vscode, or ng-cli which is 'best practice'? 
What does that have to do with the data center? That should be behind your application which should have already have had SSL enforced. And your Data center should be behind a firewall with a restricted number of IPs and system permissions. 
3 ways shown here, the 2nd way seems the best: https://channel9.msdn.com/Shows/Visual-Studio-Toolbox/Angular-and-NET-Core#time=20m45s
Yep.. Watched that video but he didn't walk through the client side.. he just pasted some web site there.. Wish he put the source code somewhere
Webpack is the most complex part to me but that seems to be the magic that allows f5 to work
Webpack is not too hard to set up. There used to be a small tutorial on the angular site for webpack. The issue is then installing this extension. I think it's not a bad idea. Angular-cli uses webpack underneath so the way you work with your front end would be roughly the same. I've been meaning to try this out for the sake of having a debugger automatically attach, but have always opted for Visual Studio Code. It just felt lighter to me. 
if you use the angular cli that will create a bare bones site (ng new ...) then he just adds that as an existing website. once you do the port changes in proxy.config it can run as separate projects
I can't edit c# code without stopping the project in VS2017
sounds good. Wish there was a video on it. When it comes time to deploy Ill be doing two separate deployments since that all I know. I think the main point you are saying is if I can make the client site build copy files into some folder under the web api project then all I have to do is publish a Web API project .right ? 
VS2017 and windows
I'm talking about the web client not the backend. üôÇ
Yes, that's what we do.
SSL everywhere. Its a security thing. Don't trust your own network these days.
I just watched this entire playlist. Very well put together - thanks :-)
Thank you for your kind words... :)
I personally use Pragim/kudvenkats videos: https://www.youtube.com/watch?v=-pzwRwYlXMw&amp;list=PL6n9fhu94yhVm6S8I2xd6nYz2ZORd7X2v He is to the point, makes some mistakes for learning purposes and is clear in his explanations of different topics. 
Nice! Thank you this helped me with this topic in a way some other sites failed to do so. Say it with me now, Delegates, Delegates, Delegates, Delegates! /ballmer 
:D glad to hear that!
+1 for pluralsight. Deborah Kurata knows her stuff but she can be a bit monotone. My girlfriend hates when I listen to 'the robot voice chic'. Also a big fan of Shawn Wildermuth and his 'earls'.
How do I make my own attribute where the value is resolved at compile time and written to IL?
If you're implying that the ADO providers that have been around since the beginning of .NET are insecure and can't be used to safely provide and access data then you should probably notify Microsoft of that. Does that mean that I just use this tool and others like it all willy-nilly without trust? No. But I'm fairly confident that I don't need to make my own data driver and provider for accessing information. furthermore if the concern is towards API related datastore services like firebase, document db, mongo, reddis, etc. Those all require SSL/HTTPS to be used for access. If they don't/didn't people shouldn't be using them and should knowingly understand the risk they're putting others in.
This is the article that should be used when talking about assisting with securing data information BTW https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/protecting-connection-information
That's ok for connection strings, but that isn't the only sensitive information over the wire. Use SSL everywhere.
IIRC it needs to be 4.6.2
Glad to see the Linux support. This is neat!
Does it work when you run it locally when you change target from IIS to dll aka kestrel ? Maybe you changed something in your startup. Here is stepbystep guide that I used for core 2.0 to deploy to azure try starting fresh and see what you missed https://docs.microsoft.com/en-us/aspnet/core/tutorials/publish-to-azure-webapp-using-vs
Make sure you have the .NET Core 2.0 SDK installed. It's a requirement for targetting .NET Standard 2.0 in .NET 4.6.1 projects. See here: https://docs.microsoft.com/en-us/dotnet/standard/net-standard
That kind of makes my brain hurt, but I'll give it a try. Thanks.
Any chance you can move up to 4.7? https://blogs.msdn.microsoft.com/dotnet/2017/10/17/announcing-the-net-framework-4-7-1/ 
This is WPF VS 2017
Not anytime soon with our huge legacy apps. It's eventually possible but we'd need to review everything first. I did see this in your link: &gt;Applications that target .NET Framework 4.6.1 through 4.7 must deploy additional .NET Standard 2.0 support files in order to consume .NET Standard 2.0 libraries. followed by: &gt; If you use Visual Studio 2017 15.3 or higher, the .NET Standard 2.0 support files are automatically copied to the application‚Äôs output folder. I'm using 15.4.3 so I should be good there, in theory.
You can do both. I wrote a tutorial on it a few days ago ... https://coderscoffeehouse.com/tech/2017/11/24/aspnetcore-ssl.html
FWIW, I added it as a project and project reference to the solution and I still can't use an interface that's defined in the .net Standard class library... Installing .net core sdk 2 didn't help either.
I tried and rebooted but still no go. But thanks for the suggestion.
Sadly, because of the youth of the Electron.NET framework, Linux support is very patchy at the moment. But it's on the road map. Linux is my daily driver and the main reason I'm early adopting and trying out so many alpha-release cross platform UI frameworks. I love C#/dotnet, but the lack of decent cross platform UI frameworks is frustrating.
NVM I got it working. Using a Bool check. manager mainload2 = new manager(true); mainload2.Show(); this.Close(); Then checked for it on InitializeComponent
I dig it.
What version of Visual Studio do you have? I think you need at least 15.3 to use .net standard 2.0 even with frameworks that support it.
Are you using `PackageReference`? I had difficulties referencing `netstandard20` libraries from .NET Framework using the old system. https://www.hanselman.com/blog/ReferencingNETStandardAssembliesFromBothNETCoreAndNETFramework.aspx
you're missing a B in lambda
Exactly!..fixed it..thanks :)
Nice GUI. Man, I hope it takes off, it'd be nice to have a good looking and flexible cross-platform .NET GUI.
This actually seems like it would be very useful. I wish I knew about this in my previous project, we actually ended up doing something less efficient. Will give it a look for the next one.
For work, small or big application, I put business logic on database stored procedures. That way, for changing requirements, I just have to change the stored procedures without (for most requirements) touching the UI. This is super important for us as most of our clients are banks with strict access policies where receiving access permission to database is hard, but getting access permission to both database and hosted application is even harder. For personal projects though, I like to make a service layer as a C# class library and put all business logic there. I love C# and prefer working on it more than sql scripts.
I like to create a separate dll for business logic that way testing is easier. Also I'm a fan of the command/query pattern with a mediator.
True. I used to use my personal account long before MSDN was an option. Now I go through my work's MSDN premium account. Katura is awesome if you can handle the monotone part. Her angular course which is #1 on the site right now, is GREAT! She does a great job of explaining the building blocks.
Same here. I hate using SQL for business logic but at work it's necessary due to the deployment issue you mentioned. It's bloody nice to just tweak a stored procedure without having to deploy code.
c# in a nutshell there are some great chapters. a lot of ones that are not worth reading e.g. reference types vs value types
Does it support PUT with a header override for firewalls that don't support PATCH?
I can‚Äôt access the video
kindly check if you can access youtube or not :)
I can, for other videos, It just fails to load this video with the message ‚Äúan error occured‚Äù. I‚Äôll try it later on my laptop, maybe it‚Äôs a mobile issue
Deploying code is trivial with good CI
For what type of application? - Simple CRUD - Rich Domain - Real-time, offline-first, cross-device
See https://www.reddit.com/r/csharp/comments/7h1de5/visual_stuidio_project_errors_after_windows_10/
A bit more enthusiasm would help. 
will keep that in mind :) trying to do something like egghead.io
I think many people had been using the oData hack, me included, so it's good to see someone working on this. I can see the reason for JSON Patch, but for me it just doesnt feel practical and I prefer the simple approach.
Even when it is only a small project, the extra work to put the logic in a separate business layer is nothing compared to the convenience you gain when writing unit tests.
Don't you have to deploy the stored procedure?
Create a service class and use dependency injection
Technically yes, but it's way less intrusive than a UI change as it's essentially transparent to the end user.
I'm using Asp.NET Core 2 Razor Pages for curious. If you have question I'll try to answer. The main reason for me developing this is I want to extend my SQL and Asp.NET Skills.
If you had a good CI. Most, if not all of the large enterprises I've worked for had a CI process that consisted of copying code from one folder to a folder on another machine.
Do they not have an API layer? I can't think of a good reason to keep the frontend code and backend code in the same application. You should only need to deploy the API after making a change, why are you touching the frontend?
I‚Äôve seen that best used here. https://youtu.be/WTVcLFTgDqs 
API? In a corporate environment? Where time to market (user desktop) takes precedent over any thought of actual software design? LOL But seriously, what I said above...
Oh man, that‚Äôs awful, and not really CI at all. TFS on-premise and online has a great ci tool. So does teamcity. I recommend using something like that 
I usually have mvc-&gt;service layer-&gt;data layer. MVC is only for displaying information in my mind. Then if you need an API, you just put up an API front end that just calls the services. Easy peasy.
I try to make it mvc -&gt; domain &lt;- data in every app (except for the most trivial ones, or demos that show some different aspect and would get unnecessarily complicated). The whole app then centers around the domain logic, which has abstractions on both sides (usually interfaces) so it is easy to unit test without the data layer and the mvc layer. But I also build a test rig for most of the applications that allows me to setup the complete application, including the web server and database for each test, and then tear it down. That allows for stable, isolated tests, that really prove that the app will work in production, especially that my container registrations are correct. 
I hope .NET Core and C# used in many console tools! Like golang and node.js.
You know, I can‚Äôt imagine if I‚Äôll ever come up with a use case for this, but damn if that isn‚Äôt cool as fuck.
This is the grandest feature we never needed.
Oh wow, this is neat! I'm going to use this in a console tool I'm working on.
I agree that a use case of it is rarely. But some tools (e.g. Electron.NET) are built on .NET Core CLI extensibility. And on Linux/macOS, a .NET Core app runs via `dotnet` command. In my opinion, better friendly `dotnet` command experience is important for the future and cross-platform. Of course, my library may be needless in most experience. :)
Nice effort. I appreciate anything that helps get us away from the SQL Server straightjacket. One question, have you tried it with MariaDB? A lot of companies are migrating to MariaDB because of Oracle's licensing model (Google is the most notable). MariaDB supports WordPress but I found tooling like MySQL Workbench doesn't work very well with it.
Oh, I‚Äôm not saying it isn‚Äôt useful, only that I doubt I‚Äôll ever personally find a use case for it. I‚Äôm using MVC and doing a lot of corporate intranets; not much call for command line output there.
Hey, thanks for doing this and adding to tooling for console apps! I still like using and building command line tools and utilities (e.g. we would put maintenance logic or admin logic outside the core of our RestAPI back end, and have completely different dev cycle on those). I have not thought of spinners (our dev ops / admins are less appreciated :-) then the end users) - but thank you for doing this - I am sure that it will become useful! Kind Regards Alex
Looks very easy to use! I hope I can find a use case for it. Thank you for sharing!
Let's imagine 2 ASP .NET MVC applications with an identical feature, but in one the business logic is implemented in a class library and the other in a sproc. Looks and functions exactly the same, the HTML/JS/CSS is identical. Now please describe to me some hypothetical change to the business logic that would result in a UI change in the class library version of the application but not in the sproc version. I just can't come up with one.
Wow. What platforms does this run on?
I can definitely see me using this as I write quite a lot of cli tools. Good work! 
I havent used 2017 but is it possible youre missing something? Core is probably for developing libraries for use in applications, directly or in third party apps, cant you develop a WindowsConsole app for what you want, or does that not exist anymore?!
Building a Windows Console using .NET core compiles it as a .DLL file.
Some excellent use cases for this would be any CLI utility: .NET "curl" client? Loader for each of the requests. Okay, so sure, it's purely cosmetic, but it's still better than not having it.
It builds it as a dll to use a shared runtime; the runtime is relative to the exe (dotnet) so it needs dotnet in the path to find it. Also the dll is cross platform as the dotnet is the thing that varies between OS. To create a standalone `exe` for Windows use 64-bit `dotnet publish -c Release -r win-x64` 32-bit `dotnet publish -c Release -r win-x86` By default it will put all the files in `\bin\Release\netcoreapp2.0\win-x64\publish\` This includes all the runtime (so can xcopy deploy); but is a lot bigger for it Discussion on how to best go about create exes that use shared runtime: https://github.com/dotnet/cli/issues/6237#issuecomment-328245453 
You must be doing something wrong if you have a console application being compiled to a DLL. Maybe try using the dotnet command line to build the application and see if you get anything different? I have a decent sized core application and while it does include the DLLs it needs in the output folder (because it can't rely on the full framework), there is still an exe to actually run the application.
It depends on what you target. If you build ‚Äústandalone‚Äù, you get an executable targeting the destination platform. Otherwise, you get a DLL.
&gt; not really CI at all I think that was the joke. Additionally, the enterprises I've worked in do not grant dev access to production - deployment of code is handled by the support team, which is outsourced and more or less done by hand.
I feel like that would be an easy sell: for certain updates the user no longer has to go through the effort of upgrading the software and rebooting (or whatever is necessary to get the new code running). It also hugely reduces risk because the user app will (I assume) no longer contain database credentials with access to execute sprocs and/or edit database data. 
Based on the parent's other comments, I think the app is a desktop application, not a web application. Any change to the C# code is therefore a 'UI' change because the user would need to upgrade the application on their PC before using.
This is so cool. Any plans to release it to NuGet?
&gt;One question, have you tried it with MariaDB? I didn't try it but I'll made code base extensible so it may be easy to port to other sql databases too. 
Core is a stripped back .Net which is supported cross platform. Additionally it has a proper CLI SDK like all other modern languages. It's not just for library development, that's .Net Standard.
I'm already thinking of ways to incorporate it into my Nuke build script.
If you have limited knowledge of ASP.NET Core, stick with plain vanilla asp.net core MVC. - https://docs.microsoft.com/en-us/aspnet/core/ - https://github.com/dodyg/practical-aspnetcore 
- One project for ASP.NET MVC - One project for ORM - One project for everything else
&gt; The first problem with monoliths is that they are hard to scale. This is bullshit. You can scale monoliths and it can carry you very far for most systems. &gt; Because there‚Äôs so much code in one place, larger monoliths can be hard to understand. Lol. It's the opposite. Microservices is harder to understand because now you are working with distributed architecture. Seriously. Monolith is good enough in most scenarios. Never forget this https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing
**Fallacies of distributed computing** The fallacies of distributed computing are a set of assertions made by L Peter Deutsch and others at Sun Microsystems describing false assumptions that programmers new to distributed applications invariably make. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
This is the problem with video tutorials. You can't just update parts of it. Everything needs to be redone when there are some changes to the tools or technology. 
I understand but MariaDB is (supposedly) fully compatible with MySQL which is why I posed the question.
Deploying an updated DLL requires a recompile and thus intriduces more risk overall to the app, regression issues, session state drop, etc. On the other hand, a change to a stored proc is completely transparent to the presentation layer.
I think one of the main reasons that .NET Core outputs to a .dll is because you can *literally* take a .NET Core .dll and run it in any environment without changing a thing.
This thread is specifically about ASP .NET MVC applications. It's right in the title.
&gt; Deploying an updated DLL requires a recompile and thus introduces more risk overall to the app, regression issues, session state drop, etc. On the other hand, a change to a stored proc is completely transparent to the presentation layer. in your mind, SQL isn't compiled and can't cause regressions? Can we stick some goalposts in the ground and leave them there for the duration of the discussion? Anyway, back to that hypothetical change you were talking about where changing the business logic in C# would require a UI change but changing that same business logic in SQL wouldn't. Could you lay it out for me please?
 &gt; &gt;And why would a change to a SQL business logic layer be transparent to the presentation layer but a change to a C# business logic layer not be? &gt; In the case of an ASP.NET Web Forms app, which is what I'm working with, you can't just push out an updated DLL during production hours as it will cause an app pool recycle which is bad when you have active users. This could result in loss of work, for example when someone is filling out a form and they hit submit, it will bomb out. Not so when updating something in the SQL layer. 
I'm down.
Me too.
I've got a successful asp.net core site under my belt, AMA.
Sounds cool
That‚Äôs fine. You can not give dev access to production but still have CI for production. You trigger CI with a Pull request. Your support team can manage pull requests from develop-master, or whatever branches you want (qa branch?). CI can work for any project at any business, you just have to try it. I know military contractors that use CI for military applications on a global scale, using Jenkins as a distributed build system. Thinking you can‚Äôt use CI because of company structure or things is just silly and naive.
I'm building a production api in .net core currently and have learned alot and also still need help. 
I build my .net core api into a docker container, and then push the self contained container to azure.
This also happened to me
What would be a good name for the channel?
So do you create Models in the MVC part if your data is handled by a separate layer?
So do you really use the model part, or does your controller call the service layer directly?
Generally speaking, the controller calls the services. I also like having viewmodels for the mvc side and map them to entity models from the service and data layers. Gives me more flexibility during large scale UI changes, which can happen frequently in start-up land.
This is a thread specifically about ASP .NET MVC apps. Again stop moving the goalposts. Also, your web forms experience probably isn't relevant to this discussion. Neither is your windows desktop app experience. So, can you PLEASE walk me through the scenario you've laid out (ASP .NET MVC app where a change to a C# BLL necessitates a UI change when the same change to the same logic written in SQL would not. That is what you have claimed exists and is common enough to influence your decisions, so it really shouldn't be hard for you.
Sounds cool.
I have just started at a firm where ASP.NET is heavily used - this would also be massively beneficial to me!
Favourite method of adding localization to inject in webapi DALs to localize based on accept-language header? 
Pardon the question, but what‚Äôs a ‚Äúgitter channel?‚Äù
Any good read on how to best do authentication, especially for a stack with client app (Angular/React)? I'm also not sure how to handle permission. The way I do it right now is by purely using roles but I think it's more appropriate if I also can do it with the Claims.
If UserManager was written correctly, it would be thread-safe and would only hold a database connection open for the duration of the method call. UserManager wasn't written correctly. I can tell because it implements `IDisposable`. Assume that anything that is `IDisposable` is meant to be created as-needed and disposed immediately after. Don't try to use it as a singleton or otherwise share across threads. *** Warning: `HttpClient` is `IDisposable` but actually is supposed to be used as a singleton. This is an exception to the rule and `HttpClient` is considered to broken by most people. But we use it anyways because that's all we have at the moment. 
Authentication for client apps look at identityserver4 or openid connect server. For a simple implementation, look into openiddict which is simpler implementation of openid connect server. You can continue to use roles, or look into authorization policies for more complex scenarios 
MVC apps have the same issue. Why don't you go take a fucking chill pill. You sound like a real asshole. I'm done with you.
Sounds good to me :-)
 https://docs.microsoft.com/en-us/aspnet/core/fundamentals/localization
https://docs.microsoft.com/en-us/aspnet/core/security/authentication/social/ https://docs.microsoft.com/en-us/aspnet/core/security/authentication/windowsauth?tabs=aspnetcore2x
Why do you want to defend core, if you don't even have a good reason? Why not choose what does the job, have a good community and is well tried. And you can install pretty much everything MVC5 through NuGet on core and get the same experience, but with a bit more hassle?
https://forums.asp.net/
/u/matthewblott has a great article and boilerplate project to demonstrate some ways to do this, I used it as a basis for auth in my apps. https://coderscoffeehouse.com/tech/2017/09/05/simple-aspnet-auth.html 
Models are part of the domain. The data layer handles data persistence (how to save and load the data), but the entities (models) are in the domain layer.
Seen that one already. Lacks proper implementation. Just excerpts. 
Thanks, appreciate your positive comments :-)
They don't have to, but I'm not surprised you don't know that. It's always easier to take your toys and go home than it is to admit you don't know what you're talking about. Bye!
What's the recommended way to make an Edit page for a collection? For example, a form to enter all the songs for a particular album (as strings), hit Save, and have them saved to the database all at once?
Currently working on enterprise level software in the medical industry implemented in dotnet core. I'm always up for a fun chat as well.
The default should now be MVC Core. MVC5 is no longer being developed.
You've taken a stance on something and you aren't even sure why? 
CoreChat? DotNetDiscuss? CoreQandA? Pretty sure anything gets the point across except for Core McCoreFace.
I will say that Core is starting to reach a level of maturity that allows it to work fairly well in production environments, but it's still far from perfect. If the team is more comfortable with the more traditional .Net stack, I'd say go with the standard .Net framework. I'd especially go with the standard .Net framework if you're writing applications that will need to weather the test of time, wont be open sourced, and don't need to be cross platform. Core is a great technology to start using and implementing, and it makes a ton of more tedious tasks (like dependency injection) far more convenient, but you need to assess your business needs before making such a huge jump. This is coming from someone who uses Core on a daily basis in an enterprise setting. There is a learning curve, and it's non-trivial. There are still tons of bugs and many times they aren't quite documented yet. There are sweeping changes to a variety of systems such as authentication patterns, and your team may find themselves sinking dozens of hours into some of the most trivial things. For now, unless the business requirements call for it, I'd say stay with the standard framework and gradually introduce core more and more as time goes on.
The easiest would probably be a simple post back with the controller accepting an `IEnumerable&lt;string&gt;` parameter. On your front end, you could implement a form that uses something like knockout or jquery to manage the collection. The only important part of the implementation is to ensure that the name of the controls matches what the controller is expecting. For example, if your collection is called `songTitles`on the controller, make sure the name of the input is also `songTitles[]`. Note the braces, they're important.
It's the having to use knockout or jquery that bothers me. They spend all this time teaching about Razor views but if you want to do something really common you have to throw it all out and use Javascript instead. I was hoping there would be a Razor way to do it.
The razor syntax does have a pretty vanilla implementation of jquery implemented that allows you to do some basic javascript using it, but I personally find it rather clunky. Don't get me wrong, I LOVE razor syntax for a plethora of very basic CRUD actions, but if you're needing anything moderately dynamic, I default back to javascript/typescript.
Do you have any proof of that claim?
Avoid .net core unless you really, really need it. It's still no where near as mature as .net framework. Thankfully asp.net core will run on .net framework.
As of 2.0 I no longer believe this to be true. We've been developing a Core 2.0 project for 3 months now and have yet to find a single instance of something missing that we need. Core is ready for prime time, IMO.
Check the repository, there's been no activity on MVC5 since work on Core began.
Asp.Net core is .net standard library that can run ether on .net core (cross-platform) or on full Windows .Net Framework. MVC 6 doesn't exist, it's ASP.NET Core MVC. 
Sounds like a good idea.
https://dotnetthoughts.net/using-scaffolding-to-create-aspnet-core-applications/
I would make content data driven, set up a SQL database to drive the content then.
https://gitter.im
Hashing and salting.
What's your architecture look like? Do you have an API and a website or just a website?
Try using yhe hexadecimal values using a ColorConverter (i forgot the actual class name)
Shit, I just started working on one the other day
.NET Core is a slimmed-down version of .NET for cross-platform software. AFAIK it's meant more for backend shit (i.e. ASP .NET websites) and not necessarily for distribution to end users. There's no support for any GUI framework. You should stick to plain .NET if you're developing for Windows, especially if you want a GUI. 
It's just a website. Plain old razor templating.
Seconding this. Haven't transitioned my apps over to Core, but all new stuff is in Core 2, and everything has been going pretty well.
Ok, I buy that.
As long as you don't try to tell me EF Core is production ready I'll accept your opinion. I'm dreading Monday when I get to find out my next EF Core based bug. Last week it was setting a non-nullable FK to null when I told it to delete a child record.
I will say, however, that I really miss having code analysis support. Really it's the tooling, not the runtime, that I see as being substandard.
aspnetcore.slack.com
Wow! Very cool. I had no idea this existed. Thanks for the heads-up!
Be future proof without technical debt. Greenfield should be ASP.NET Core -- it even supports full .NET 4.6.2 to use with Entity Framework 6.2. Also, webpack + typescript + npm gives you modern web components -- SPAs are better than MVC Views + Jquery.
ASP.NET Core 2.0 + EF 6.2 work fine together.
I mean, sure. But Entity Framework is a horrible clusterfuck of its own. We've learned to live without that nonsense, long before Core was a thing. EF was one of the worst things .NET ever made. Only to be surpassed by Windows Communication Foundation. 
I'm pretty happy with WCF now that I know what parts to ignore. The problem with it is that the documentation sucks so you have all this flexibility you can't use but still have to pay for. 
The only reason we built a new app with core was so that we could host it on linux/docker. We built primarily a React SPA so where not to keen on using Visual Studio but rather Code (it has worked better for TypeScript in our experience). So core's command line build system made it heaps easier to work with. This also extended to configuring our CI agents a lot quicker.
&gt; SPAs are better than MVC Views + Jquery. Only when done right, and that is very difficult and takes time. So many shitty SPA out there that can't even handle proper browser navigation.
Ask.Net Core can run with the traditional dotnet framework. I personally haven‚Äôt found these issues with authentication, but I have noticed a change in a recent version so yes it might be a slight issue for some. We are on ASP.Net Core and we had that discussion. The truth is, MVC 5 is not the latest version and I prefer to be on the latest version. Dependency Injection is useful. Middleware is very nice to use. Deployment is not much harder if your on IIS. 
It is data driven however, it's driven from upto 6 different sources and it picks the source to go against through reflection. Each source is defined against an interface so they all respond the same however the implementation of each source is different.
We‚Äôve been on Asp.Net Core for 8 months now on one project (not in production yet). We are doing a huge redesign in Asp.Net Core for our main app. It‚Äôs stable and it‚Äôs awesome. I just want to make it clear because this is confusing for a lot of people including the grandfather post to this one... Asp.Net Core can work just fine with .Net versions that are not Core. We are using 4.7.1. Asp.Net Core is in reference to running the web server on Kestrel, which is invisible to the normal process of using IIS, with the exception of setting the App Pool to No Managed Code and the issue of stopping the web server when deploying new code (via CI or CD). 
Your `UserManager` is **not** a singleton. It's written correctly and is created on a per-request base. The middleware is created **once** per application life-time and re-used for all requests. `Invoke` is invoked for every request. You're storing the `UserManager` passed to the `Invoke` method in the middleware directly, then perform some async operations (which can take some time), and within your async operations you access the `UserManager` in the middleware. And that's a mistake. When you now get a second request simultaneously you overwrite the stored `UserManager`, and suddenly your first request is using a different `UserManager`. The solution is simple: Don't store anything you receive in the `Invoke` method in the instance scope. Instead pass the variable along.
`UserManager` is written correctly and works just fine. The context that the `UserManager` is using gets passed as an argument (actually wrapped in a `UserStore`), so the creator of the `UserManager` is responsible of handling the life-time of the context. It's meant to be per-request. That's also what is happening, he's receiving a new `UserManager` per-request. The issue here is only that the middleware is a singleton, and he overwrites the per-request `UserManager` with each request. When you have simultaneous requests he will be using the wrong `UserManager` for his async method. Request 1 takes a bit longer, request 2 comes in and overwrites the `UserManager`, request 2 is finished and the `UserManager` is disposed, request 1 continues and is using the already disposed `UserManager`, resulting in a `ObjectDisposedException` error.
DotNetCoreStudy? The channel needs to be - Welcoming to beginners - Oriented towards learning and building capabilities in addition to problem solving. - Develop a checklist of what you need to know to be proficient in ASP.NET Core and help each other towards completing those list. In addition to gitter, we should create a github repository and use its cases as a forum. We can grant commit access to everybody so people can upload their samples. We can build a FAQs there as well. .NET Core is a new world. There is so much to explore. Let's make it as fun and efficient for all of us together.
Razor + jQuery will last longer than most JavaScript frameworks out there.
&gt; Only to be surpassed by Windows Communication Foundation. Lol. Have you ever tried Windows Workflow? 
&gt; Does it support PUT with a header override for firewalls that don't support PATCH? For now it doesn't. But it is a great idea to improve it. Thanks
I've created the gitter channel (https://gitter.im/DotNetStudyGroup/aspnetcore) and github org (https://github.com/dotnetstudygroup). Please add your account here https://github.com/DotNetStudyGroup/aspnetcore/issues/1 so I can add you to the organization.
If you desperately want to avoid javascript you can have an add to list button which POSTs to the server, makes the array longer and then rerenders the page.
https://github.com/aspnet/AspNetWebStack &gt; Latest commit 7dad200 3 days ago Not much work, but some.
Installation of visual studio is much easier and secure. One of my colleagues creates the ISO offline installer directly from the layout. Its total size is about 22 GB and 100% virus free I guarantee it. If anyone interested he may visit the below URL https://goo.gl/bDJ6ug
thanks, i'm gonna look onto these.
If you got questions, we just created new help channel minutes ago (https://gitter.im/DotNetStudyGroup/)
&gt; It uses .NET Standard, so it should be usable across .NET, such as with F#, VB, etc. That‚Äôs not what .NET standard is at all. Those languages all compile to CIL, .NET standard is about API compliance
That is true, I think that was two thoughts that merged in my head. I did some further research, and .NET does support cross-language libraries without .NET Standard. I stand corrected. I updated the Post :)
Is there a reason you dont use system.xml? You can even load Schemas and Check the validility
The specification is large and complex. I am looking for the fastest way to implement the protocol. Handcrafting it would be fast but it will take forever.
By the way why you use an underscore and "this" together on a class member. In my opinion one of these two is enough :D
I cannot really see the *kick-ass developers* there. From the titles and length of the videos all they cover is pretty basic stuff. Rather for beginners than for kick-ass devs.
What? Could you explain what you are trying to do with an example. Are you trying to generate an xml File or are you trying to parse a xml file? What is the size of the xml file? How many nodes do you work with? Did you already try to use the system.xml class?
In visual studio there is an option to paste XML as classes. You simply copy all the XML and pasted it. It will paste it as classes and then you can use that as a starting point.
Have you tested HtmlAgilityPack's correctness? Because I found that it didn't generate the same DOM tree for a ‚Äúmalformed‚Äù document (e.g. non closed tags) as any other browser. I use AngleSharp for parsing HTML, but maybe it would be redundant for your project since it also includes programmability, and even the option to execute JavaScript.
The Open NDC Specification has a large number of XSDs (https://imgur.com/a/T1Xjg). It has 53 different XSDs. One of the XSD is over 1 MB. So the scope of the XML is large (many type of xml documents). I don't know yet if they have MBs of documents per transfer. So what I am looking for guidance is what is the most efficient (in terms of implementation) of handling these types of large XML specifications. 
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/voyz6Fr.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) 
According to the GitHub readme for HtmlAgilityPack: &gt; The parser is very tolerant with "real world" malformed HTML. The object model is very similar to what proposes System.Xml, but for HTML documents (or streams). This library is pretty much a 1:1 of the Python-Mechanize library in it's current state. The dependency on HtmlAgilityPack runs reasonably deep, but I don't think it would be too difficult to swap it out for a different underlying library. 
The Readme says one thing, but the real usage doesn't match what browsers do (HTML5 defines precisely how to parse malformed HTML, to the point that any valid string is also a valid HTML). I raise this point because the utility of this library could be hampered if it's not seeing the same as, say, Chrome or Firefox.
The thing is my starting point is XSD, not the XML documents itself.
I have 15.4.3 (VS 2017) so as far as I know, it's supposed to have everything it needs to run .net standard 2.0.
Actually I find this very useful. I have several projects in mind for this already :) Then again, most of my work in .NET involves asp.net core, or console apps.
Personally I would try to separate the bulk of the business logic in a .NET Standard lib that way the logic can be used in both .NET Core and Framework. Then wrap the GUI around it. If / When we get a good GUI for .NET Core, then you can wrap a new one around it or at least have less to port if you want to. 
Hmm. Thank you for pointing this out. (The whole experience so far has been a frustrating failure, so I guess I get to go test using PackageReference with our old legacy solutions now...)
I'll see what I can do in this regard. Perhaps I will rip out the dependency on HtmlAgilityPack, and create an IHtmlDocument, IHtmlNode interface so that you can use whatever Html Parser you wish. Maybe I'll just switch to AngleSharp, seeming as it has better error correction, and for [performance](https://github.com/AngleSharp/AngleSharp/wiki/Performance) reasons. (Although Html Forms don't require CSS).
Xsd.exe can create classes for you with the proper SerializableAttributes.
I've had a look myself and I think the answer is ... no. JetBrains' Rider is pretty reasonable and (I think) includes all the .NET tools they sell separately as VS plugins.
// TODO come back fix thing *never comes back fin
System.XML is quite fast. I have used it to handle 3-4MB size xmls without any performance issues. 
Has that tool been updated since .NET 2.0?
XmlReader is super fast. What I want to avoid is hand crafting the code myself because the XSD is huge.
Don't use reflection for this -- huge performance hit. Use an enum and a factory class.
To expand, reflection at initialization time is fine, reflection at request time is a perf problem.
https://docs.microsoft.com/en-us/dotnet/standard/choosing-core-framework-server
Tested so far with production-like request-frequency and there is no performance hit whatsoever. This isn't an API that will handle 300k requests on a single instance :P
There are readily available tools to auto generate c# classes from XSD. Would that not suffice for you?
Even better, Google "xsd.exe visual studio". From a command prompt you put in the xsd name (make sure any dependant xsd are in the same location) it will make one large class file or individual depending on your preference.
What about the Xamarin Profiler. Should have been installed with VS for Mac AFAIK.
You can use something like ILSpy or dotpeek to decompile assemblies/code into IL and make it a little nicer than working directly with ilasm or ildasm. It may also interest you to look at how some of the "sugar" works for C#. For example 'using' gets broken down into a try-finally-dispose. Async stuff is crazy, lambda expressions get converted to classes with properties etc. ILSpy has settings to see some of these items.
OCD :)
/u/nirataro, check this comment from /u/timmyotc if you're looking for a way to generate it. https://www.reddit.com/r/csharp/comments/7hbpgy/novice_trying_to_convert_a_json_file/dqppeyo/
Dunno about an interactive IL compiler/editor, but [sharplab.io](https://sharplab.io/) is really neat for seeing how individual language features are lowered and compiled, and unlike most other .NET decompilers dnSpy can patch assemblies without going through a manual decompile/recompile process.
 I personally don't really understand the point of learning IL if you're just a developer. Also, it sounds more like you want to learn MSIL/IL, not ILAsm, correct? IL is the language (Intermediate Language), not the actual assembler, (Intermediate Language Assembler). The reason I don't find it useful is because even though sure, you will learn some interesting things, like under certain conditions code can be compiled in different ways (some cases if turns to switchs, etc), you're not really going to know much better how your code is working, it's not the true assembly, which you would need to find after JIT'd that really matters. But, if you're just curious, the best tool by far: https://github.com/0xd4d/dnSpy ^ allows for any executable to be decompiled, turned from C#/IL (most can do it) but you can see the steps it took to get there, as well as editing the assembly in a common work environment and using any language you so choose.
This is not viable for complex XML document specification. The use case I have has about 2MB worth of XSD.
I tried - XSD.exe - Xsd2Code None are working for this scenario.
I did mean MSIL, you're right Dispersia. dnSpy looks promising!
Thanks. Looking at the sugar and stuff I think will come secondary, but looking at lambdas sounds sort of fun.. I've wondered how they worked for a while. I have ILSpy downloaded, and an extension for VS2017 to stylize and provide intellisense for writing MSIL now.
/u/tweq, this is almost exactly what I needed. Thanks!
Did you get an error with xsd.exe?
I got an error when using the code generated by it. I tried it earlier this year. I will put up a reproducible project soon.
Probably not, but neither has System.Xml. 
 Do note though, dnSpy would be more for a learning resource of looking at programs are currently compiled together, it wouldn't teach you how to write the actual language - but if you've ever written any flavor of asm before, you'll understand the structure pretty fast.
I never heard about any useful tutorial about the MSIL, but I read the book ".NET IL Assembler" and found it extremely useful, it precisely describes every element of the MSIL and its structure. 
Yep, you're right!
Does it runs JavaScript? Or is it only a page downloader coupled with a HTML parser, with nice API? Might be able to replace selenium testing?
I am trying https://github.com/mganss/XmlSchemaClassGenerator at the moment and it blows up on two XSDs. 
hasn't this been done with data adapters since data grids since the early 2000's?
Look at the API for that class again, then tell me one reason why it should be stateful and thread-bound? https://msdn.microsoft.com/en-us/library/dn613290(v=vs.108).aspx 
You can't do that from the controller. It breaks the very meaning of the MVC pattern. You pass that information along to the view somehow (either via the Model or the ViewData) and act accordingly in the view.
This is useful info. I‚Äôll probably try to find a pdf online somewhere in a bit.
Because of the constructor, where it accepts a user store that is used to access the data. The alternative approach would be to pass this store as an argument to every method, which would just needlessly bloat the API.
The user store should be thread safe as well. Open a connection, save or load the data, close the connection. There's no reason to hold a connection open for the life of the object. 
Whether or not it should do that is an implementation detail. I'd rather use connection pooling over constantly opening and closing connections. And even that should be left as an option. And with the store it's the same case as the manager: It accepts the entity framework context as an argument, instead of getting it passed to any method. The life-time of the context is handled outside of the scope of the manager and the store. This is important, because otherwise things like transaction scopes would be impossible, or worse: be part of the manager/store API. But the entity framework context is indeed not thread-safe.
every one of those `.this`s are unnecessary. maybe you should compulsively remove them instead :p
ASP.NET Core or regular ASP.NET?
 dotnet new vue That's all you need to know :)
Virtual DOM has proven to be effective in libraries like Angular, React and Vue.
&gt; I'd rather use connection pooling over constantly opening and closing connections. So would I, which I why I am recommending this. When you call `SqlConnection.Close`, it doesn't actually close the connection. It just releases it back to the pool. This means that connection pooling in .NET works more efficiently when you hold the connection object open for as brief of time as possible. If you instead hold the connection object open for the lifetime of the request, you are denying the use of that connection to other requests. This can have several negative outcomes including: * Opening an excessive number of connections on the database, straining its resources * Exhausting the connection pool entirely You won't necessarily see this in short-lived requests, but if you have any with an lengthy CPU-bound section that can quickly cause problems under load. &gt; This is important, because otherwise things like transaction scopes would be impossible, or worse: be part of the manager/store API. Now we're talking about acquiring locks in the database. So it is even more important that transactions only exist for the shortest possible duration. 
Thanks ! Could you please provide links or sample code to achieve same, either via passing the model or viewdata? 
Use jquery 
I don't even see why we're talking about keeping the connection open. That is not even the issue here at hand. The EF context is actually even giving the connection back to the pool after any operation. &gt; Now we're talking about acquiring locks in the database. So it is even more important that transactions only exist for the shortest possible duration. Sure, it should be the shortest possible duration, but that duration must be controllable by the developer, not the library. And that's again where the state comes in and the ability to pass instances along. You completely move away from the actual point instead of understanding the reasoning behind it. You also provide no alternative solution approach.
I've used hangfire for three years doing basically exactly what you want. I have a job which runs every 15 minutes to send out text message reminders using Twilio. It's been flawless, I would highly recommend it. Another option might be to use something like AWS Lambda with dotnet core. You can schedule the function to run every 15 minutes, set and forget. 
Top shelf will just make it easier to write a windows service, so won‚Äôt give you any benefits if you can‚Äôt use windows services. I believe hangfire runs in a webapp (with the nice admin UI) so that‚Äôs probably a better bet. Be aware that if you don‚Äôt take precautions, IIS will shutdown your webapp if it doesn‚Äôt get any traffic. You‚Äôll have to make sure that doesn‚Äôt happen.
Yeah, I haven‚Äôt had to solve this problem for 10 years.
This is exactly what was I hoping to hear! I will take the hangfire route. Much appreciated!
The last point is important. I use pingdom to monitor the site availability which keeps it alive.
The alternative approach is to open/close the context on each database request. Besides making the code threadsafe, it would likely perform better too because EF bogs down the longer a context is used. All the internal tracking it does is expensive.
The context still needs to be passed in. The context still will contain state. You could drop the exception on an disposed object that way, but I'd rather have an exception than unintentionally using an object past the expected lifetime (which happened here).
You'd need a property on the model like `bool ButtonDisabled` that you conditionally set to `false` when necessary. In your razor template, you check for ButtonDisabled and display an enabled button or disabled button accordingly.
[Firebase](https://firebase.google.com/docs/cloud-messaging/) has notification platform too. Though hangfire sounds more like what you are looking for.
Has anyone had any experience with the author and his courses?
No, you pass in a context factory, or connection string, so the context can be created when, and only when, needed. 
Which is a perfectly fine possibility, but yet again just a design choice. The factory needs to be created too, the factory will contain state too. Or the factory is designed as a singleton, which results in limitations again. Creating a context does not automatically open or use a connection, that only happens during a database request. You haven't provided a single reasonable argument why it's written incorrectly, or why using `IDisposable` automatically results in incorrectly code ("xyz wasn't written correctly. I can tell because it implements IDisposable"). It's just not your preferred style, but not wrong.
I like to use Azure Functions for stuff like this. VS 2017 has really nice tooling for it as well.
Yes, that makes an Edit page for ONE object.
 &lt;button @(Model.IsDisabled ? "Disabled=""" : "") /&gt;
&gt; The factory needs to be created too, the factory will contain state too. Not mutable state, which is the problem. &gt; You haven't provided a single reasonable argument why it's written incorrectly I've already listed many of the problems associated with keeping contexts and connections open longer than absolutely necessary. What more do you want, an essay on why immutable objects are better than immutable objects? 
The core of what you want is to store some sort of bool value into the viewData, and then handle it in javascript on the view layer. Very common scenario and tons of examples for it. If your condition is in the model, /u/jewdai has an even better solution. 
To give a little pseudocode example to tack onto /u/delphi_edict's answer, one standard way is to pass a ViewModel from your Controller to your View. So your controller will create a ViewModel object, populate it, and then pass it to the View. It would be something like public ActionResult Index() { ViewModel vm = new ViewModel(); vm.EnableButton1 = //some sort of logic to set it true or false vm.OtherAttributes = //set anything else the view might want to use //for example... vm.TabName = NameOfTab.ToUpper(); return View(vm); } ViewModel would be a class you define specifically. Then, in your View, you would call the code. Assuming you're using Razor... &lt;div class="section"&gt; @if(vm.EnableButton1) { &lt;button&gt;&lt;/button? //blahblah markup } &lt;span&gt;more example markup here&lt;/span&gt; &lt;/div&gt; But make sure to not forget to declare a reference to the ViewModel at the top of the view file. MSDN will have a lot more example documentation on how to use viewmodels. Try [this](https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions/mvc-music-store/mvc-music-store-part-3)
Noice!
&gt;How do you process XML nowadays? We don't. We use JSON.
Yes, you need to move on from windows forms. I'm in the same boat. Learn something like Angular with a .net core wepapi backend. If you must write windows applications, then WPF/UWP is the current technology. All use declarative UI markup via web tech or xaml. Drag and drop UI's are legacy technologies.
Very informative, thanks to the author.
 What does this offer over the official documentation that has more test scenarios and explains usage parameters?
It doesn't run JavaScript, it allows you to fill in Html Forms and then use POST or GET to submit them, using UrlEncoded, Multipart or Query strings. It also has some basic state storage. Check out the project on Github, Its fully commented, and a LOT easier to understand than the Python version I interpreted.
you can configure the IIS site to be always on the app pool.
wat
Why would you want to handle it in javascript? I created a template engine once though that let you use javascript and razor together to make a .js file loaded from a script tag. Never got to use it in a project though as... no one uses MVC anymore for new projects it seems. Everything is Angular/React/Knockout/Vue(!) now it seems. =/
Bear in mind with this though that this will either show the button or not show it, not disable the button. You can't do &lt;button disable=@Model.MyBool /&gt; either though which is annoying, so you'd need to have an if on your bool and have a normal and disabled button in each condition
You need to put code in backticks. Correction below: &gt; You can't do `&lt;button disable=@Model.MyBool /&gt;` either... You could have an else which followed the if for the true condition which shows the button normally and allow the if to be the button with the disabled attribute (or class of they are rocking Bootstrap).
&gt; **Performance.** In this update we continued to improve performance. Solution load times for large C# and Visual Basic projects is nearly cut by half. The time to switch between debug and release is significantly reduced. It is faster to add, remove, and rename files and folders in .NET Core projects. Project templates should now unfold much faster than before. In the most exceptional cases, you can see up to a 40x improvement in unfold time. 
&gt; Handcrafting it would be fast but it will take forever How can it be both?
Enterprise uses MVC quite a bit still for internal apps.
This would be my go to concise method for purely MVC.
If all you're doing is sending an email notification, I would deploy an AWS Lambda function to do this work for you on a schedule. Don't even need to write it in C# - node, python would work well for this task since it doesn't sound very CPU intensive
It's still a best practice to reflect once, then cache the result. 
Is this what you mean by pass the variable along? Seems to be working. public async Task Invoke(HttpContext context, UserManager&lt;ApplicationUser&gt; userManager) { UserManager&lt;ApplicationUser&gt; _userManager = userManager; ... try { user = await GetUserAsync(_userManager, context, "from_refresh_token"); } ... } private async Task&lt;ApplicationUser&gt; GetUserAsync(UserManager&lt;ApplicationUser&gt; _userManager, HttpContext context, string getUserFrom) { //Do stuff with _userManager }
You can script scaffolding using powershell or cmd.
You are right, sorry. I rushed through my example a bit. 
I can't blame them, nevermind the better integration that github provides, Bugzilla really could use some love.
Why in the world would you not want to handle the enabling or disabling of a DOM element in JS?
When an opensource solution really ousts github, by all means jump on that. But until then? Just use GitHub, it's just plain good.
Thanks a lot, it seems pretty cool. I have several project on which I want to run HTML tests, I initially planned to use selenium, but selenium is a pain in the ass to run everywhere because of other browser dependencies, and some other native dependencies. Your project seems a very good replacement.
If you're running azure i can't recommend to go use Logic Apps. It requires no development skill to get tasks which can be scheduled to run. Ie. Nightly stored procedure. Post a message in a chat room when a job or task is computed. Put messages out to logs when an exception occurs I your app, or send out automated emails. I ditched Hangfire for this tool this year and I never looked back. I promise you wont either. 
The component makers like telerik and devexpress have done a good job for drop in replacements (kendo is pretty nice)
One way I think of a view model as a fa√ßade that adapts the model to the view. If you have some set of services that you need to query across, or data you want to aggregate or filter, the view model is the place this logic goes. The upshot of doing it in a view model is that it is more testable - instead of expecting HTML or JSON to come out, you can test objects. It is also more abstract - you can use the same view model to render HTML as JSON in some cases. Of course in the web world, with domain logic behind its own fa√ßade, the view models can be pretty thin, but the cost of the extra layer is often recouped in testability and the abstraction of the view.
At the end of the day I think it is a judgement call. In an MVC application you will need something to act as the "M"/model/data for the view. You have a number of options: * Do stuff directly in the view. This might be suitable for some simple scenarios, but I think it's more common to do things in the controller. You can't unit test a view as far as I'm aware. @{ var aValue = new SomeService().GetAValue(); } @Html.TextBox("AValue", aValue) * Use the dynamic ViewBag object. This is super convenient and good for simple scenarios, but you do lose some type checking. You can unit test this. When POSTing data you'll need to use a lot of controller params or a dictionary which can be ugly and doesn't offer type checking. // Controller ViewBag.AValue = someService.GetAValue(); // View @Html.TextBox("AValue", aValue) * Use an existing class in your domain. This can convenient because you reuse an existing class. However you may require a view model anyway if you need multiple domain classes for a particular view. You also may require a view model if you find yourself needing a lot of view-ey properties e.g. "ThingIsVisible", "ThingIsEnabled" which don't matter to your domain, but do on a screen. You also may struggle if you need to serialise these objects into JSON or XML as they can be very heavy. You may also be uncomfortable sending certain things to the client where anyone with dev tools can have a snoop. // Controller return View(someService.GetSomeObject()); // View @model MyDomain.SomeObject @Html.TextBox("AValue", Model.AValue) * Use a dedicated view model. In my opinion the best of all worlds. You may end up with a lot of classes, but they will be simple and in a very appropriate namespace that explains what they are e.g. MyMvcClientApp.Models. You also separate your concerns nicely i.e. no UI stuff in your domain and vice versa. You can create properties for everything you need and map or just compose a bunch of existing domain classes and things that are missing. If working with JSON or XML you also have full control over what goes to the client. Finally it's the most MVC thing to do if you ask me. So square peg square hole. If you need help you won't be doing anything weird and should be able to find it easily. // Controller var myObject = someService.GetSomeObject(); return View(new MyViewModel { AValue = myObject.AValue }); // View @model MyViewModel @Html.TextBox("AValue", Model.AValue) There may be other ways, too, but ultimately there are options and you should be able to find something that works for your use cases. T 
I use top shelf to manage and configure my hangfire service 
Is it possible to execute the unsorted query and sort in memory, or do you have too many rows for that to be feasible? Alternatively, could you query the table you need to sort and include the other table?
Any chance you can make one for MS SQL next to expand your skills there?
I use service fabric these days
Good instincts - but not quite! First, if you're reusing models between pages, it may make sense to make some sort of base class and take advantage of inheritance. You can inherit this base class with the common fields (e.g. a "Form" base class with a Name and Date), and then extend it for your particular view (e.g. a model with, I dunno, Job Title). Secondly, you have interfaces. Interfaces can be really powerful for combining different classes into one readily accessible interface. For example, if you had multiple database contexts, you could just reference one interface class, throw it into your controller's constructor, and access accordingly. Saves you from having spaghetti code - you have one "library" of database controls, instead of having your database access spread like mayonnaise through five hundred sub classes. tl;dr: Inherit and use interfaces. 
XLinq. Build XElements by hand.
Pass a boolean through the model to the page itself. Make the visibility or active state of the button controlled through that boolean.
Oh that i do. Cache the result that is :) 
Historical debugging, woohoo!
Most of the time I just declare the view model at the same file as the controller. The View Model classes are pretty much throwaways classes to make your life easier to deal with the UI. 
- learn JavaScript and typescript cold - learn the various module system and build system for JavaScript. This can be confusing. - learn CSS and HTML - learn how HTTP protocol works - then try View library like Vue or React - then if you are curious, go with more integrated library Whatever you do, don't start with JS framework 
I mean take forever to develop
A lot of B2B standards are in XML
Aaah i see
I'd but right now I only have a macbook with limited resources. I'll look what can I do.
After having struggled with this for a long time myself I came to the conclusion that there is no one size fits all here unless you're OK with the numerous view model classes. I now use view models only when necessary and very specific to the use case. After all it makes little sense to create a view model class that is a simply a copy of a domain class structure for purposes of consistency. It just adds more work and headache in the long run. Here's when I DO use them: - When annotating for client side validation (so I don't have to muck up my domain classes) - When I need instances of multiple classes to render my view - When the view has a very different shape than my domain classes - When the view has logic I won't need anywhere else 
You're right that view models could easily get unwieldy, but there are ways of organizing them that would keep things under control. For instance, keeping a folder structure for the view models that corresponds to the view folder structure with a naming convention that's clear. But, in my experience, having used MVC for many, many years, view models aren't always needed. You'll find that a view will often naturally be associated with a model without the need for a VM. It's when you need to say add a few dropdown lists along with your model that you will need a view model to represent both the model and it's drop-down options in the form of List&lt;T&gt; or something. View models are effectively data transfer objects. The advantage of MVC is that it is a pretty non-imposing architecture. You're free to do as you please in a lot of ways, which is both good and bad. It also allows for some sloppy patterns to emerge very quickly too. That's where your experience will come in to play in making good judgment calls about how you do things. 
Good, bugzilla is fucking awful.
There are certainly a lot of classes in projects these days when done "right". You have models, viewmodels, controllers, services, repositories, and interfaces for all of them. But what burden is one additional c# file? If you have 5 extra classes for something that used to take one, but all of your features are separated and organized nicely into folders, and the extra files all follow the same patterns you might actually find that it is far easier to work with in the long run.
Somewhere you need state to allow control of the transaction scope, if necessary. And again: You can do it the way you propose, but you don't have to. It comes with it's own set of features and limitations. This doesn't make this solution incorrect, it's just another approach. But this conversation has become, or perhaps was from the start, weary. You either don't understand, or you're simply refusing to acknowledge that other approaches than your own one are valid too. You must be fun to work with.
Yes, exactly. But you can skip the entire `UserManager&lt;ApplicationUser&gt; _userManager = userManager;` line, just pass along `userManager` directly.
What unit testing framework are you using? In which version? Take a look at the supported frameworks: https://docs.microsoft.com/en-us/visualstudio/test/live-unit-testing
Hi /u/r2d2_21, I extracted the interface for the Html Parsing, so that you can use whichever Html Parser you want, although by default it comes with the HtmlAgilityPack Parser. I added a AngleSharp Extension for Mechanize.NET, which you can get for NuGet, and then initialize it in the Mechanize Browser constructor: ``` using(var browser = new MechanizeBrowser(new AngleSharpParser()) { } ``` When I was researching this, because I hadn't heard of AngleSharp before, it appears you can Post Form data using AngleSharp itself, however, it seems more complicated than this library. It is up to you for what you use, HtmlAgilityPack is more lightweight in some regards compared to AngleSharp, AngleSharp gives you more freedom and customisability, and the ability to run Javascript, but for a larger file size. 
This version has the c# support for spans, but doesn't that require framework support too? Is a release imminent?
I installed xUnit and nUnit. I guess that is the problem but uninstalling them haven¬¥t work so for (get-package | uninstall-package -removedependencies)
Install only one of them, not both. And you need to additionally install an adapter, depending on the framework: - xunit.runner.visualstudio, at least version 2.2.0-beta3-build1187 (prerelease!) - NUnit3TestAdapter, at least version 3.5.1
Nah I need to sort the query in db. I'll just change the table to include the important row, it's a small db so no big deal
Every time I wanted to report a bug bugzilla mentally blocked me from doing so. I've been hoping for that switch since months.
Even if you got past the mental block it didn't seem to work properly!
The purpose of a view model is to project your database model, which has generally been normalised for performance into a model which makes sense for your use case and to your user. There are a number of reasons for doing this, but the primary one is that it enables you to have more flexibility in the future. If you've projected your data model into something sane and denormalised you can trivially change your change your app to deliver a JSON object to a JavaScript front end or a mobile application. Views can't really have multiple models in any event. If you need that youh need to compose your models into some parent object at the very least. Even in C# working with deep complex object relationships in a view is fairly painful and error prone, dealing with them in JavaScript is just horrific. This is particularly true when you're pushing data back into a database. View Models also loosen your coupling with the underlying database. If you use your data model directly in all your views then it will be very difficult to change the structure of your database without having to rewrite all or most of your app. Now this isn't so much about creating a one off class for every model and you certainly won't be creating any kind of sub classes as part of this. The purpose is to transform the model you built for your database into the model that it actually belongs in in the first place. The model that actually solves the problem you set out to solve. If your app is resulting in a combinatorial explosion of view models then it's a poorly designed piece of shit and you have no grasp of how your users are actually interacting with their data. Your viewmodels should represent the way your users understand the data in your application. Sort of by definition your views should also map this way because they are *how* your users interact with data in your system. Your data model represents the way your particular persistence layer most efficiently stores, retrieved and updates the data in your application. If you're building a non trivial application and the two are the same you're almost always doing something wrong.
Try the samples here. It will get your feet wet. https://github.com/dodyg/practical-aspnetcore 
Thank you, though I see only couple samples for ASP.NET CORE 2.0 or isn't there that big difference between 1.0 and 2.0 version (except more cross-platform feature support and the main initialization/settings)?
I was in the same boat as well. After 7 years of winforms I started web development, mvc + knockout, then angular and everything web development involves. I would say at the moment this is the way to go. I tried xamarin mobile development for 1 year, did the certification on Xamarin University, and still I prefer web development. But if you think it is not for you, take a look at xamarin, you might find it interesting.
I guess I don't understand what's wrong with having lots of ViewModels classes. They're just POCOs.
Most things transfer over https://docs.microsoft.com/en-us/aspnet/core/migration/1x-to-2x/
For real, just go to the ASP.net website. They go through a TON of examples in their documentation. And they have a lot in their github
WARNING! DO NOT USE DEVEXPRESS UNLESS WHEN MOVING FROM DEVEXPRESS WINFORMS TO WPF. It just isn't worth it. The licenses are costly, it breaks 99% of the nice tooling around wpf and process cost around their components is fucked up. Free packages like mahapps metro deliver the same if not better quality, cost nothing and use the regular WPF toolset. I don't know about telerik but I assume it's the same thing. Throwing money at problems isn't going to fix them. You can do ribbon UIs yourself. It's just as much cost to develop them yourself than to integrate that foul stuff in your process and paying for it all the way
But that obviously creates a maintenance problem, and it seems, from continuing study, to be a real minefield for creating mismatches. Of course, a plethora of extensions to VS exist to try to make it easier, but that just shifts the burden to another complex tool with its own inevitable problems. I've seen this cycle so many times - a good idea develops into an incredibly complex one in actual structure, tools are developed to hide the complexity, and now the complexity is in the tools, which increasingly will be a burden. I'm expecting this to go the same way, over time.
This most closely matches what I am continuing to take away from the subject. I understood immediately the data transfer aspect. 
Of course, but in my experience designing both web and desktop apps, the cases where a single, simple model will match a view is not large. Dropdowns and other list structures are common in many forms (at least in the rather scientific/technical apps I'd been working on the last 10 years). Others here say that using a lot of viewmodels means one did a shitty job, but that's both non-technical and unhelpful. If one's experience is with simple forms, great, but, for example, where an app is primarily 200 specialized reports, I see this design going into explosion mode.
I didn't need an OO basics lesson. I've been using inheritence and interfaces since the 80s, along with a huge variety of class combination techniques that exist in more experimental OO systems. The point is, the design pattern and how it's implemented in the .NET world encourages and, apparently if followed strictly, results in one not having a large, single class, but many viewclasses. Obviously, this is dependent on the nature of the views - where they mostly correspond to a simple element in the model, it's all good. Where multiple are needed (and this would be almost standard in the apps I've been working on for years, which tend to be reporting heavy, with reports being extremely numerous (hundreds) and very few would not need custom viewmodels. The read-only aspect is a good one to have pointed out, however. At least in the course I'm taking, though, Viewbag in particular was shown to have some problems that made using it less desirable, although I'll have to go back and review to find out what it was. Perhaps it was just the read-only aspect. If you'd like to chat, presume I'm very versed in all the OO stuff, understand concepts like "read only", etc., and just get to the meat of it.
 On a side note, I haven't seen someone mentioned it, you're also talking about the MVVM pattern in the OP. The MVVM pattern is tried and tested, if you want a good look, look at a large angular 2+ project. Every view, even control, requires a viewmodel. As long as you structure your folders correctly, it's extremely clean. We have over 800 views in our application, all doing MVVM, and it's still easy to find and manipulate what you need.
This has been the most helpful reply! It does corroborate, but in a factual way (as opposed to the "lots of viewmodels= you did a shitty job" unhelpful stuff), and exposes what I was sensing was the frailties of the design pattern, at least when confronted by the reality of actual coding. It's like, to use this, you have to violate the pattern's contract - and I find it amusing, as in my career, I've seen so many of these paradigms swoop in to save the day, be embraced by pundits and managers as "the right way to do it", and then we all paid the price way down the line when the real world scrambled all the lofty ideals of the theory and its application. If I may, specifically, for each of your points: &gt; Do stuff directly in the view Obvious violation, but super easy, simple, and at least in this case, going through a service does decouple one from direct model access. But shreds the design pattern, and I'd guess each such instance is a potential problem. &gt; Use the dynamic ViewBag object. You did a great job pointing out the frailties of this. But it exists because the strict design model doesn't cover all the "shapes" it's intended to. &gt; Use an existing class in your domain. This can be convenient because you reuse an existing class. However you may require a view model anyway ... This is in fact the de facto case for views in my experience - that they will require cross domain data. Perhaps its the field I've worked in the most for the last decade - scientific data mining and reporting (and the database work that goes with that). Almost no view, and definitely no report in the hundreds designed, would not need a Viewmodel, ViewBag, or similar. &gt; Use a dedicated view model. This is pretty much my understanding of the best-case way to handle this, and while organization certainly will help, it's the very heart of the problem with proliferation that I see likely in the sort of apps I've developed. Indeed, ViewModel is abstract, although I don't see the point in bringing that up. ViewModel seems to me not to be "a role that a class plays", so much as being a source, mapping, or projection, along with other classes, forming a newly "virtual" model. To me, the fact that this exists is a huge, gaping red flag, indicating a fundamental deficiency in the "pure" pattern. I am currently studying and while partial views have come up, it's just a little early in the course. I suspect it's soon.
Fast spans require framework support, managed spans just require a nuget library.
Ah ok, neat
But it doesn't "adapt the model to the view". THE model being the word - it exists because *the* model doesn't work because the view is cross-domain. I do appreciate the testable part, and I'm sure every one of my concerns are being dealt with successfully on a stupidly huge number of projects every day. As I read the thoughtful (and some not so) replies, I think mostly what I see is that, as with so many other "paradigm changing" methodologies that have been embraced over the years in programming, this one has the same issues as every other one - it just doesn't really (perfectly) fit the real world except in very simple cases, and stretching the pattern to actually work for real applications requires a lot of things that break the purity of the intent of the paradigm. This isn't to say it's wrong or bad, just that this is one of a long series of things intended to simplify the work, which doesn't really do that (at all). But I do see how, in a world of highly automated tools for coding and testing in particular, we can leverage the complexity into something positive. I just expect, again as with so many such developments in the past, this will hit a critical Jenga point.
I actually replicated the entire scaffolding script myself. It makes me a database and all the Razor pages using reflection.
WPF, Web, or mobile. WPF will have about the same learning curve as web because of its large differences to the WinForms model, but will allow you to continue using the better development ecosystem and leverage your existing Windows experience. Mobile would be similar if you're doing "native" mobile development. Web is in many ways a complete shitshow with regard to the development environment and constant flux in tooling and all the rough edges. However, there's a ton of work out there for it and it can be leveraged for mobile and desktop apps. 
&gt; Whatever you do, don't start with JS framework I think this was debated heavily over on Coding Blocks.
While I appreciate the tips, none of this addresses my issue... :(
Good to know. It's obviously the tools that are keeping the chaos at bay. 
These 3 are free on the Microsoft Virtual Academy-- https://mva.microsoft.com/en-US/training-courses/aspnet-core-beginner-18153 https://mva.microsoft.com/en-US/training-courses/aspnet-core-intermediate-18154 https://mva.microsoft.com/en-US/training-courses/aspnet-core-advanced-18155
&gt; Indeed, ViewModel is abstract, although I don't see the point in bringing that up. ViewModel seems to me not to be "a role that a class plays", so much as being a source, mapping, or projection, along with other classes, forming a newly "virtual" model. To me, the fact that this exists is a huge, gaping red flag, indicating a fundamental deficiency in the "pure" pattern. Remember how ViewBag was mentioned earlier? That's basically how *every* other MVC framework has its View layer. Meaning that the View layer has absolutely no type checking and you find out **at run time** when there's an error. Razor allows you to specify a single class as a strongly typed model, hence the use of ViewModels for pages that take more than one database model*, or that you pass additional information not found in the model. *Note: `IEnumerable&lt;ModelName&gt;` can be passed for collections of a model instead of creating a ViewModel.
I was going to agree with you but I think you're onto something. I didn't find the MVVM pattern to be very interesting until I started using Prism Unity IoC container and dependency injection. Like you said, tools are created to hide the complexity.
WPF has nowhere near the learning curve of web UI's. Visual Studio takes care of most of the work.
And in many cases, when the view does closely match the model, but you need a few extras, inheritance can help avoid duplication.
It takes care of a lot of the work, but not all of it. If you're going to do WPF "right" then it's a lot different than how most people do WinForms. If you're just going to pack every bit of logic into an event handler in the code-behind, then you're correct. It's not that different. 
What's the "right" alternative?
MVVM with heavy data binding. As little code behind as possible, the magic is in the models.
Okay. I've been told I've been using WPF "like it's WinForms", and having little WPF experience, I couldn't really argue the point. But the code-behind is only updating UI elements and binding data, all the logic is in another project entirely. So I guess I'm not too far off track.
A few small changes here and there (looks like to support NetStandard2) but you can pretty much see the exact point in time when main development stopped: https://i.imgur.com/wMRuNNr.png
One model for each view isn‚Äôt something I would call an explosion. On the contrary, it‚Äôs very predictable. You never have to guess. On the other hand, models that serve many views tend to lose focus and become difficult to maintain over time. Sooner or later, one of the views needs data that is irrelevant for the others. This creates complexity, not a high number of small view models.
I have this problem with just about any XSD from HL7. 
Jank.
 Just to explain, nUnit is the test service. NUnit3TestExplorer maps the tests that nUnit does to how visual studio expects tests to be ran. Visual Studio tests do not work the same way the other frameworks do, so the test adapters map it from how the testing framework works with the way visual studio wants.
Great! I'm so glad I could help. Feel free to shoot any further questions you have my way and I'll do my best. If it's of any comfort, I have definitely been frustrated at times with the framework/pattern. "Do I really need a class for this?", "Which controller does this belong in?", "Why do I have to keep typing [some thing] over and over?", etc. At the end of the day I take comfort in a few things: * ASP.NET MVC is the most actively maintained .NET web framework. * It's nice to have *a* pattern/framework. It can be nice being told what to do. I can think less and build more. I also read a design patterns book once that talked a lot about the benefits of having "shared vocabulary". I agree with that a lot. If you search "asp.net mvc how do I [thing]" you'll have a mountain of resources and you'll understand what they're talking about. * When I look at code in other modern web frameworks (Python's Django, Node.js's Express) it looks super similar to what I'm doing i.e. map a URL to a function -&gt; create a data structure -&gt; render that structure. * Historically I've been frustrated more by designs with fewer, more general types. I typically prefer more numerous, specialised types. This is just based on my preference and experience though. Good luck on the journey!
I have a few tutorials on my blog [here](https://coderscoffeehouse.com).
I can only offer help with 2 of your bullet-points. [I am currently working on a ASP.NET Core + Vue/TypeScript/SCSS template.](https://github.com/phil-harmoniq/vue-template) However, I haven't been able to find any information for getting pre-rendering working. If you (or anyone else) has any advice, I'd love to hear it!
Hangfire is nice, a bit clunky in .NET Core and nobody seems to give a shit, [see my issue](https://github.com/HangfireIO/Hangfire/issues/1007). Also, don't use the Sqlite provider, it is completely bugged.
Thanks, I will take a look at this tmr. I had assumed that pre-rendering so that everything can be saved statically is not too difficult (time will tell). Have a look at [this link](https://vuejs-templates.github.io/webpack/prerender.html).
I think I understand your question more readily. There's a concept called "Partial Views". You can basically reuse common parts of views, and inject them into a View. I typically find it easier (albeit a bit more cumbersome) to generate a new View per page, with one layout page holding scripts and navbars. Is it redundant? Sure. But I can generate it with three minutes of grunt work instead of an hour of thought. You could, of course, change the routing if this gets too cumbersome. You could have a master "subpage", and swap out different partial views ("sub-sub pages") depending on the routing. Routes are usually controlled on the action method as an attribute. And while you do know OO, Asp.Net's design pattern isn't as traditional as a standard OO project. It's very opinionated - and you sometimes can't just instantiate things like you'd normally expect. If you're stuck, try refactoring in a way that emphasizes interfaces, dependency injection, and simpler data structures. A lot of my refactoring typically revolves around "Oh, damn, I can't pass this from my view because it's too complicated." (Since the C# we write is really translated into JavaScript when it goes on the view. The models are JSON objects, and are subject to constraints.) 
I started a brand new project the other day. It was a .net core console application to simply get files from an Azure blob storage. I got tripped up with the async functions. functions like CreateIfNotExists() have been changed to CreateIfNotExistsAsync() and ListBlobs() have been changed to ListBlobsSegmentedAsync(). And the [getting started with azure blob storage](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-dotnet-how-to-use-blobs) article that is ranked highly on Google made no mention of the async functions. 
Sounds like a good change, but the documentation should be updated.
No, the complexity is already there. You are just realizing it. The complexity does not come from having additional view model classes. It's that you have lots of unique views of your data. Coming up with some god object to represent the data across many views isn't necessarily efficient or effective. Embrace that each view is unique and just create a view model per view unless they are practically the same.
The problem with starting with the framework is that JavaScript ecosystem is brittle. If something breaks or you misconfigure something, it's hard to figure out what went wrong.
My personal take on this is to not throw it all into one project. I would handle the AWS Serverless + Web API - in other words your REST API - as one project. The Vue SPA is a consumer (or client) of the REST API, and therefore a separate application. Remember, that there can potentially be other consumers (or clients) of the API like mobile apps, Bots, etc. In the same way that you would not make the API and Mobile app a single project, I would not make the API and SPA a single project. So I would suggest that you create the API using the appropriate AWS template in Visual Studio - probably the AWS Serverless Application (.NET Core) template with the ASP.NET Core Web API Blueprint. Then create your Vue application as a separate project using the Vue CLI. This way you can keep taking advantage of the excellent tooling provided by the Vue CLI which you will lose (AFAIK) when you use the ASP.NET Core SPA templates.
but... shouldn't a proper Async-Method also run without a deadlock, as long as you don't write code in it that forces a certain SynchronizationContext ( eg. within a library always use .ConfigureAwait(false) ) Example: public static void Main() { var storage = CreateIfNotExistsAsync().GetAwaiter().GetResult(); } public async Task&lt;IStorage&gt; CreateIfNotExistsAsync() { var storage = await azure.GetStorageAsync().ConfigureAwait(false); if (storage == null) { storage = await azure.CreateStorageAsync().ConfigureAwait(false); } if (storage == null) throw new Exception("err"); return storage; } I always code my libraries this way, and only had problems when people were able to provide a Action/Func Parameter that executes on the same thread.
This is already a thing, though. Does it offer any advantages over the xsd.exe tool that comes with .NET/VS?
Xsd.exe has not been updated since .NET 2.0 if I am not mistaken. This one has more features and actively maintained. 
No point in doing 1.0, the latest is the best place to learn. They change things and it gets frustrating when the code you find in tutorials doesn't work anymore. I've now read your post. Do you want to create a desktop linux app? Then yes, maybe xamarin is the way, I've never used it. If you want to create a web app, web restful api, just use asp.net core mvc, you can host it on linux. MVC is the convention of how the web request is handled, it arrives inside a Controller derived class, at one of the methods of the class, based on how you set up the routing. Default is http://url/ControllerName/MethodName The method reads the request, does something and then returns a result. The result can be either HTML (which is based on a View, a html file with the extension .cshtml located in the Views folder, which typically uses the Razor templating system, basically dynamic C# code within the Html code, or custom Asp Html tags, which get rendered into plain Html) , or it can be Json/Xml, and in that case you refer to it as an API endpoint. All these action methods and controllers can be annotated with c# attributes to describe authentication/authorization/security/http verbs/security. Angular is a front end javascript framework. It's basically some javascripts which do automatic data binding of data to Html elements. For example you build the html front end as a static html with Angular javascript, and then call the backend for data, like an API. When the javascript objects you defined in the page get updated, then Angular will take care that the right HTML fields will get the values. You use Angular typically when you want to build a SPA - single page app. There are other frameworks for that as well, React, Knockout JS, Vue. But that's not the only way to build a frontend, and actually the MVC convention was usually more geared towards a multiple page web app. One advantage of separating the backend and frontend, by having the backend be just a API, is that you can reuse the API for a web frontend, for a mobile app, etc. It's never black and white, and there's many ways to do the same thing. If I were you I would skip learning Angular at this point. Then you have the data access, if you use a database it's easy to use Entity Framework Core. You basically create some POCO classes as your data objects, which will get mapped to tables, a DbContext derived class which represents typically your database and contains fields with these POCO classes. You can then use the CLI tooling which comes with .Net to do entity frameowork Migrations. You say something like "dotnet ef migrations Add myCurrentDbSchema" and it generates the database schema described by your POCO classes. Later if you have changes to these classes, or add anything new, you create a new migration and it amends the database with the new columns/tables. Of course, you have to supply the connection string to the database somewhere. Built in into .NET Core is a dependency injection system. For example in the constructors of the Controllers you can just add interfaces as parameters, like IConfiguration, Ilogger, etc. When the runtime needs to instantiate the controller, it will look at which concrete class you mapped to which interface, and the scope of it, somehwhere in the Startup.cs file, and inject it into the constructor. For hosting, .net core comes with a built in web server, but because it's light weight it's recommended to run it behind another web server. For example IIS on Windows server or Nginx on Linux. I'd recommend their Docs site, look through their samples on github. Sometimes their documentation is not complete, or you see many ways to achieve the same thing. If you have some money to spend then maybe get a Pluralsight subscription, plenty of video courses there. But I think learning is mostly up to you, there's plenty of free resources around. 
Paste special, paste xml as class ?
Asp.Net Core is essentially a rewrite of asp.net MVC. They are close but essentially two different frameworks. 
The Readme addresses this. Things such as nullable properties are improved.
You have to compare .Net Core MVC vs .Net Framework MVC. You should read more about frameworks on .net ecosystem in order to clear confusion about them. https://docs.microsoft.com/en-us/aspnet/core/choose-aspnet-framework
Ah, I‚Äôll give that a try, thank you :)
SvcUtil may also be an option
Yeah, this. I'd love to see the whole family of XML tools in C# modernized - the Xml Serializer is similarly ancient and the Data Contract serializer is a poor substitute since it sacrifices a massive amount of functionality.
No they‚Äôre not. Also.Net Core is MVC 6 merged with Web API. MVC is a middleware you use. It also has all of their latest development so I would expect Asp.Net Core to be where all the bug fixes and feature development to go. The main difference I see is that Core uses dependency injection everywhere. It‚Äôs hard to get used to at first, but it‚Äôs very convenient. Also, middleware makes certain things much easier. The down side is that auth works differently. It‚Äôs really not that hard, though. Read about both and make the choice.
I honestly have no idea. My point is that I wasn't expecting to have to deal with undocumented async behavior on a getting started article from Microsoft. It works on my home pc which is all up to date, but totally fails on an older work pc which has .net 4.0.* installed on it leaving me with no real way forward.
This is good advice. In fact as soon as I started combining the two, this was my first thought - Do I really want everything in a single project? My conclusion was the the same as yours. I have therefore created a single solution containing two separate projects, one for the REST API back-end, and a second project for the SPA/Vue front-end. The AWS Serverless Web API project is a good match, but the dotnet CLI uses ASP.NET MVC and Bootstrap, so I will convert that to a pure Vue/CSS/HTML front-end. AWS Serverless does not support MVC nor Razor (Razor may appear when .NET Core 2.0 goes live). Hence there is no way to pre-render MVC pages to static HTML from Lambda. In any case it seems there would be a better separation of concerns if .NET Web API is used at the back end, and only Vue/CSS/HTML is used at the front. [Here's a diagram of the AWS architecture](https://i.imgur.com/7yAOeFD.png)
I heard Core was open source. That‚Äôs got to be the main difference.
Came to say this as well. It's the most useful and easy to use tool; and already built into the IDE. 
as u/d-signet pointed out, [copy and paste special](https://weblogs.asp.net/morteza/Paste-Special-a-less-well-known-feature-in-Visual-Studio) already does this.
Let's not forget that .NET Core 2.1 drops in early 2018
Both MVC5 and Core 2.0 has Razor View Engine (Core 2.0 has more feature such as Tag Helper). That's all the similarity. Everything else is different. 
Ahah great post. I really love the quote &gt; We can't fix stupid.
That only works for trivial cases. Try these http://www.iata.org/whatwedo/airline-distribution/ndc/Pages/schema-download2.aspx and see how copy and paste handles it.
That's a toy functionality.
I once had a customer who, even after being told otherwise, claimed that a MSCS could run without MSDTC. Rather than debate it, I said "That would be awesome, could you forward me the documentation that explains how." It never came up again. I would suggest that you provide a brief explanation of parametric queries and ask for precision and documentation about how not being "a prepared statement" creates sql injection problems with parametric queries. 
I thought so too before experiencing it for myself. If you logically separate each feature into a module and separate modules into layers so that all dependencies go in one direction (down to lower layers) so that each feature could be deleted without affecting another feature on the same or a lower level, then you can realize the benefits. Layer 4: Client Layer * Website * Mobile App * AnotherWebsite Layer 3: Minor Features * Navigation * Page Content * Page Metadata * Search * Media (banners, carousels, responsive images, etc.) Layer 2: Major features * Analytics * Personalization * A/B Testing * Authentication Layer 1: Core * Unit Testing Helpers * Helper methods and laugage extensions * Serialization framework * Dependency Injection framework 
&gt; The down side is that auth works differently. It‚Äôs really not that hard, though. Any good resources (besides official docs) for implementing custom auth with encrypted cookies? I found it terribly convoluted and difficult to convert an existing mvc app over to core and am still not happy with the situation.
Let me check... Okay with Asp.Net Core 2, there were some changes to how cookies work, so I'll post that. [migration article](https://docs.microsoft.com/en-us/aspnet/core/migration/1x-to-2x/identity-2x) will cover moving from ASP.Net Core 1 to 2 and should be enough to make something. It doesn't say if the cookie is encrypted, but I assume that it is. I would have to create an example myself to test this out and it's been kind of a long day. 
Hi, thanks for your feedback. The playlist will be updated on continuous basis with more in depth videos. I hope, soon you will get the feel of being a kick-ass developer :)
\*While hosted on Azure. Kinda important to mention.
Very, very cool. Hope this expands outside of azure and asp.net apps.
Bro, as others have said the cold hard truth is that it only gets more complicated from here. Yes, compared to WinForms, web development is a hot mess. There is no other way to put it. So bring a heaping serving of patience if you really want to make the switch to web development. Sounds like you plan to keep your current job and just want to start developing new projects on the web. [This is a really good course to learn the basics i.e. HTML, CSS](https://www.udemy.com/web-design-for-beginners-real-world-coding-in-html-css/learn/v4/overview) And after that [check out this course on Asp.Net Core 2.0 MVC](https://www.udemy.com/learning-aspnet-core-mvc-beginner-to-developer/). It should help get you started. There is a [book on Asp.Net Core 2.0 MVC also.](https://www.amazon.com/ASP-NET-Core-Razor-Pages-Beginners-ebook/dp/B0772SL5VJ) Might find some good courses on PluralSight. If you need to do Single Page Applications (SPA's) then look into eventually learning Angular 5 and TypeScript. Plenty of courses covering that on Udemy. But it sounds like MVC would work good for you to help ease you into the world of web development. Best of luck! 
I mean, these only look complicated because there are imports. Is that what you're getting at or am I missing something?
*And when using Visual Studio Enterprise
Visual Studio is in my very biased opinion by far the best Ide out there - particularly the 2017 edition. Even the free community edition is fantastic. IIS is not a great webserver. It will do the job, but if you're doing greenfield development and can do with the less mature ecosystem of .Net Core, I would look at running Kestrel. In either case, using nginx as a reverse proxy and/or load balancer. You can't usefully compare C# to PHP or node/js. It comes down to your preference for typing - dynamic vs static. C# does give you threads which is definitely useful. As to the rest, it's not an issue. You can use MySQL, postgres, Mongo, or whatever you choose with .Net. There's drivers for them all.
I thought this sounded fun, so I'm reposting.
joelving hit my biggest pro on the head. Visual Studio is hands down the best IDE on the market. That, alone, is enough for me to want to use a language supported by it whenever I can. Beyond that - it's difficult to compare languages without comparing something specific about them. C# is an strongly typed object oriented programming language and feature wise it will be very similar to any other language that is as mature as it. For the most part, you're unlikely to notice much of a difference. That said - the .NET stack is much different now than it was under Ballmer. Many of the frameworks are now open source, and they've made several development tools free. If you're going to be connecting to the cloud - the Azure service integrates pretty seamlessly and a lot can be developed against it for free or nearly free. We stick primarily to the .Net stack here mainly because there are far more local developers familiar with that than others. 
&gt;they are sort of set on a .NET / Windows solution could u clarify what u mean by this i.e. desktop app, web app. As others mentioned going .Net does not automatically mean u have to use WCF, WPF and IIS. All of those are older frameworks that I would avoid.
Thanks for the response! Kestrel seems interesting, I will do some research. As for C#, I'm very comfortable with a static typed languages. That's not really a concern - it's more the platforms / frameworks and how they interoperate and deploy that has me worried.
Thanks! I can appreciate that some of the stack has gone open source, seems like a big step forward. I also have worked with Azure before (though deploying Linux) and the overall experience was very good. Unfortunately I'm not a big fan of Visual Studio, but I could probably get used to it.
Sure thing, it is for a desktop app though there's not a ton of reasoning behind that. Could probably convince them to do a web app. Those are the frameworks that seem to be preferred - what would you suggest as alternatives?
copy and paste special works from sample documents. It does nothing if what you have is XML Schema.
C# as a language gets better in a hurry.
WCF is by far the best remoting technology you can wish to work with. I know this is a controversial statement, but I'm fairly convinced about it. To the point that, used in the proper way, it ceases being a remoting technology and it becomes tout court a fundamentally new .net. For those of you looking for references, try reading the books by Juval L√∂wy and attending his Architect Master Class. It's a mind opener! 
Because I said so
Not a fan of VS?! Blasphemy! I'm curious which IDE you prefer.
Yeah, this already exists...
AKA, the "You have to bend over for them just to hear the price" edition.
The most important part definitely
Yes, it would!
Isn't that what the web service discovery tool is for? It will generate all of that for you. And it looks like that's what all this is. 
Your app needs to support .Net Standard 2.0. So proper framework 4.6.1 or Core 2.0.
You can run app insights outside of Azure with an agent on your server, hey you go to portal for telemetry. I wonder if this would work in that scenario.
Man. I've used prepare statements a ton in golang, but I honestly had no idea they were a thing in .NET. Furthermore, learned a ton more about some sweet under the hood design. Thanks for the awesome and informative post. 
MVC WebApp or Web Api with Angular/React/whatever frontend.
What are the newer frameworks you would recommend then? (Still learning here :)
I am sort of in your camp with one foot...but I'm interested as to why you like WCF for remoting so much?
Can you do that from a file?
Asp.Net Core for rest services and UWP for desktop app. WPF and WCF are fine also but they have been eclipsed so to speak. Sounds like your decision makers have experience with those already. Definitely would not be the end of the world if you had to go that route. For web front end though I would go with Angular 5 using Typescript.
I use Hangfire in production. It has its pros and cons but I'm not sure it's necessary in your case. As others have said, probably a simple AWS Lambda (some of the other cloud providers offer similar services).
Doesn‚Äôt everyone already debug in prod and just *pray* for minimal impact anyway? No Azure or Enterprise needed!
For desktop, you can do UWP, but there are some other things to consider like stability. .NET Core is growing so fast it can be a tad bit risky for some project types (not all.) [This decision flow-chart](https://purple.pizza/where-to-start-in-dotnet/) _might_ be helpful to you.
And a fine point you make, indeed. 
That's exactly how ASP. net worked before this model was popular. 
Sounds reasonable to me. 
Thanks. Razor is literally opening the next section of my course. 
If it's archtected correctly you can re-use modules in not only several projects but different types of projects. I have one where I re-use core components in a winforms (desktop app), WebApi, windows service.. essentially all I have to do is create the UI. Php and other such languages don't have this flexibility.
You are mistaken. Asp.net core is still in it's beginning and UWP is being phased out as it's unpopular and no one wants the Windows Store. WPF is still developing and Winforms is going nowhere.
Don't listen to this guy. .net core isn't a replacement tech and UWP (the stupid Universal Windows Phone platform) is terrible. Microsoft has abandoned the Windows Phone. WPF and Winforms are not outdated and infact WPF is still expanding. Visual Studio itself is WPF. Stick to MVC for ASP for web and learn how to use WPF and WinForms. All three are widely used and definitely required for enterprise level development. There's a reason Microsoft isnt releasing any business level applications using the tech the other guy is mentioning.
$$$$
I love Visual Studio and also VS Code. The best IDE's around. Also .net is very full featured and reliable. I have used it for 9 years and it has always been great really. That being said I have recently within the past year been into Javascript and have built one react app using IIS and its amazing. I love the simplicity of react with Javascript and how powerful it is together. The difference between js and c# and dot net framework is that dotnet has everything available but is cumbersome. Javascript in react means you just have to know Javascript and that's it. If you want some then you just look for an npm package. It's similar with dotnet and nuget. But dotnet and Visual Studio have pretty much everything you need right there. Just my opinion. 
kestrel sitting behind a load balancer like aws's elb works like a charm. locally you can use something like haproxy, which is easier to set up than IIS. 
// TODO Read article
the biggest feature c# has that many other mainstream languages don't is the first class support for async/await. Makes life so much easier.
well, the part of the stack that is open source is not the same thing as the .net that is built into windows. the former is .net core (always has core in the name), the latter is what sort of referred to as "full framework" (stuff without core in the name). The full framework moniker is a bit of a misnomer, simplistically it just means it has access to windows UI stuff. But starting with .net core 2.0 and .net framework 4.6.1, both implement a shared api surface (called .net standard 2.0) so code is extremely portable between the two (and if you're using .net core, extremely portable across windows, mac, and Linux). 
Because that‚Äôs a great idea /facepalm
I was surprised at how fast managed spans are. The perf gains from using even managed spans vs copying stuff around is still totally worth it.
Is there an sdk release coming as well?
Where have you read that, I'm unable to find it? That would make the feature actually useful if you're not using Azure.
NHibernate migrated to GitHub too, from the monster that calls itself JIRA.
If you are deploying to a Windows Server OS, then using IIS as a reverse proxy is preferred using [.NET Core Windows Server Hosting bundle](https://aka.ms/dotnetcore-2-windowshosting)
If you are deploying to a Windows Server OS, then using IIS as a reverse proxy is preferred using [.NET Core Windows Server Hosting bundle](https://aka.ms/dotnetcore-2-windowshosting)
WCF is typically overkill for simple REST APIs. For everything else, I agree.
This looks like it's just logging the stack and variable values at a given point. It's not debugging in the sense that it isn't live and it isn't something you can step through/back/change while it's happening.
You can definitely use app insights outside of azure. (I mean, the data is sent to azure, and you view the app insights data in azure, but the app itself is running on prem in IIS)
Yes but this functionality appears to be tied to Azure App Services. Isn‚Äôt that something different?
The problem is that REST APIs should be used only at the most extern layer of an architecture. Either to power your frontend (web or native app) or for B2B integrations. Internal systems should take advantage of the superior tools offered by WCF or equivalent (if anything like that existed)
Yea the article says "Additionally, you can capture snapshots automatically when exceptions happen in your app by setting up Application Insights" but I think they are saying that you still need to run it in Azure, just that App Insights would automatically log the snapshots.
WCF was introduced around 2006 and already supported many things that nowadays we have to implement by hand every time. I'm thinking of aspects like transactions, state management, security (signing and encryption of the messages) and so much more. All of these features are implemented in a way that they don't pollute the business logic of the application. You will never have a parameter transactionId in your contracts, yet you can have a transaction flowing through 5 layers of services and rollback without an issue. Most of the criticisms towards WCF are about the verbose configuration in the app.config file. But you can have everything set up in code without missing anything.
I think that is an old-school way of thinking. When you "go cloud", everything uses an API management layer (e.g. Kong) that is almost always REST-oriented (SOA and REST are almost opposites here). I see most developers avoid non-REST approaches these days.
My company platform sits really comfortably on the cloud, yet we use the pattern I wrote above. Good luck handling transactions across several layers of REST APIs without either polluting your domain with metadata or reinventing the wheel by putting the metadata in the HTTP request headers. Nothing impossible to do, but why should I spend that time when Microsoft did it for me with a team 1000x bigger than mine? (WCF was worked on by a team 1.5x bigger than the team that worked on the first version of .NET Framework). WCF gives me transactions, message signing, authentication, authorization, versioning of the contracts, routing (with some more work), pub/sub, duplex, discovery... Trust me, keep the hipster REST APIs at the edge of your architecture. You'll live better. üòâ
[x-post from /r/AZURE]
How?
It was meant to be a joke. 
Haha, you got my hopes up there!
Microsoft using their enterprise dev tools as a profit center has always baffled me.
can only speak for myself in my company but we use a almost 100% pure microsoft stack (sql server, AD, ...) and here you get many things for free (say getting the user principal from AD) that would be a horrible task otherwise. Couple that with the power VS gives you (you get newbies up to speed incredible fast because they can use the excellent IntelliSense, debugging support etc. to explore the framework space and how things work) and it's basically a no brainer if you are in this kind of environment. If not for that I would probably choose JVM or other options.
Vs remote debugging tools. I hope you don't break anything on prod now. 
I will try not to, or at least, when I break stuff, that's what I'll say "I tried not to"...
I agree, they have been better lately with the community edition. On the other hand our management is demanding we move our .net stuff to Linux servers, so maybe tooling will be a better long term bet.
Every developer has a fully functional test environment for their code. Lucky/good programmers have an entirely separate production environment.
No idea. WSDL tool is never mentioned anywhere as a tool to generate C# classes from XSDs.
Not sure how I feel about this. Yes PROD specific bugs exist. However, having enough logging, tracing and error reporting in place should be enough for you to diagnose any problem. This feature just promotes sloppy exception handling and lazy logging practices. If things go wrong we can just debug in prod leads to: catch(Exception) { return false; //todo log this }
Try having a Location, Sales Header, Sales Detail, Agent table and put the sales between the agent and the location. That way an agent has either no sales, or sales in which you can find and calculate the amount that each location receives per sale, and also the cut of the agent etc... having location linked to the agent is something I wouldn‚Äôt do because logically its the sale that has location details required not thr agent. Apart from this i have no idea what you wanna do or what kind of app (win/web -used tech) you‚Äôre building so cannot suggest anything further.
Yea readind this article more, it is a feature of app services, not app insights. Since they do Azure Compute things to make this work, you need azure. App insights can run on prem however
Yes, it's for cross compatibility. See https://github.com/dotnet/corert for ahead-of-time compilation.
Have you looked into Self Contained Deployments? Also take a look at how to publish stuff to specific runtimes e.g. `dotnet publish -r win10-x64 -c release` That will produce a .exe
See here: https://github.com/dotnet/cli/issues/6237 by default .Net core compiles to a portable app that has to be run from the framework. Exe are a windows thing so unless you specifically target windows it wouldn't make a lot of sense to output one. Welcome to the irritating world of cross platform development.
Do like microsoft with some of their new products and make an Electron App(VsCode). Why it will be faster, and it wil a lot more pleasant to develope. .NET with REST or GraphQL backend. And FrontEnd with Angular That way you can easily expand the client to the web and mobile aswell.
.NET is the best option if you are developing Windows-only desktop applications, internal business applications in a Windows-only environment, or mobile apps. But if you're being forced to use it, it doesn't really matter, does it?
The whole point of core is that it is cross-platform. By making an .exe you would be eliminating that cross-platform functionality and, thus, eliminating the entire point of core. It's not irritating. Find a different way to deploy your app. There are many great ways to accomplish this.
Check this out https://github.com/mganss/XmlSchemaClassGenerator/issues/48 Now this schema http://www.iata.org/whatwedo/airline-distribution/ndc/Pages/schema-download2.aspx can be generated!!!! 
&gt; can I compile to .exe without creating a framework independent version of my app? If I publish a standalone build it is around 60MB for mere KBs of console app. So is there any way to do it how oldschool .NET did it? Old school .NET application requires the framework installed on the PC which is quite big (I remember 26MB for 2.0, hundreds for 3.5+). For Windows 10, 4.6 is pre-installed (newer .NET will be installed in newer Windows 10 builds). I believe .net core will be pre-installed soon on later Windows 10 for UWP apps. On web applications perspective .net core can easily published to azure web apps which with .net core runtime installed, you only need to publish few kb's to the server, instead of old .NET having seperate big dll files. Cross platform frameworks like JAVA requires you to install java runtime too, I guess for .net core just not officially released it's runtime yet since Microsoft is pushing their Azure Platform as a service 
His question has nothing to do with ahead-of-time compilation.
exe in this context just stands for executable, and that is not a windows only thing. And this is possible just fine with the self contained deployments feature.
He is concerned about distribution size
You should be specific that your answer is regarding the second question, not the first. :-)
&gt; It's not irritating. Is it not? Unless I wanna ship 60MB of runtime with my 100kb console app, it seems quite irritating to make cross platform console tools. &gt; Find a different way to deploy your app. There are many great ways to accomplish this. Can you please point me to some? I have searched and all I found was either to use dotnet x.dll or compile a standalone version that is too big for my use case. In a way mono did this quite well for what I am looking for. You have your exe, on windows you launch it as normal and on linux you just do mono app.exe and all was great. Is there any reason why core would drop this idea and instead go with not so handy dotnet app.dll ? Current console app development in core is not very handy for the reasons I mentioned above. Do you have any info on if they plan to introduce similar feature in the future of core? I'd be very happy if they did. Best workaround I found right now is to make a batch/shell script that calls the dll but that feels so dirty I have to take a shower each time I do this. üéÖ 
java doesn't really make .exes either. You run java on the .jar file. It's the same idea. 
https://www.aspose.com/ (unless you were interested in something free?)
Yeah I have seen that, that's one of the first things I found while searching for this issue. It doesn't really answer as to why it is that way. For example mono can run executables on linux just fine (for the most part at least) in the same as dotnet with `mono app.exe` and on Windows you just use them as normal üåô. It seems strange to me that dotnet would ignore this functionality, especially seeing as one of the strengths of core is console application development. Sure I could just publish a self contained build but that thing is ginormous for a simple console tool.
I helped code the predecessor for WCF using .NET 1.0/1.1 while at Siemens for a telco system we were making (OpenScape). It was a huge project (54k C# files). We worked closely with Microsoft and the lessons (and some code) we learned out of it went directly into WCF later. So yeah, I understand WCF, and I use it for anything SOAP/Remoting/some REST. But the world is a messy place and you have to do all kinds of things to make it work. WCF doesn't easily interface with many hand-rolled REST APIs on the internet (some work, some don't).
They only list .NET. Do they support .NET Core?
You can ship also `myapp.cmd` with content `dotnet myapp.dll %*` which should solve all your troubles on windows.
Mono runs Windows applications on Linux. With .NET Core you're writing a native Linux application on Linux and a native Windows application on Windows. If you want the crappy speed of Mono, you can use Mono.
&gt; you just do mono app.exe and all was great You do realize that the other 60MB (actually it was more like 500MB in total) of your 'Mono console app' was installed when you installed Mono? Disk space is cheap. &gt; Best workaround I found right now is to make a batch/shell script that calls the dll This is actually a good idea - lots of applications, at least on Linux, do this. It also allows additional environment configuration if needed (although for your console app there may not be any)
&gt; You do realize that the other 60MB (actually it was more like 500MB in total) of your 'Mono console app' was installed when you installed Mono? Disk space is cheap. I do realize that. And it was good because user downloads runtime once and then all apps using that runtime can get away with weighting kilobytes. Disk space is cheap but I don't think a tool that does 1 or 2 functions should come with a 60MB download size. Seems like bad practice to me. If I didn't care about resources I'd use electron. No idea why the downvoting as I was just seeking a solution to a problem üò∞ 
This is what I do. Not the best way I feel but it's the best for core it seems. I made this thread with hopes of finding a better alternative but seems like there isn't any, at least currently. Thanks for suggestion.
I've switched over to Linux as my main OS on my work laptop. I've been developing all of our stuff in .NET Core. It's not bad, VSCode is nice, but it's taking some getting used to over Visual Studio.
&gt; Disk space is cheap but I don't think a tool that does 1 or 2 functions should come with a 60MB download size. Frankly if you're providing this tool to random people (e.g. freeware/OSS) I'd much rather have a standalone executable, even if it's a little large, than be forced to install all of Mono just to run it. If it's for internal use and all of the employees are expected to have Mono already, I can understand that, but having a script call `dotnet app.dll` is hardly much different - if your app is in the user's Start Menu/equivalent they won't even notice it's not an exe. &gt; No idea why the downvoting Maybe you're better off asking whoever downvoted you.
If you're made of money [PrinceXML](https://www.princexml.com/purchase/) is great. Out of all the converters I've tried, it did the best with CSS. It's a standalone executable, but if you already have the HTML and don't need to configure the PDF with additional code it would work well in a pipeline.
Please don't use database first and edmx files. If you want to design in the database, use the wizard to regenerate code-first after making db changes.
pdfsharp / htmlrenderer. You can override the default css values or disable them completely. 
I did something like this recently. Used headless chrome to render html and tell it to print to pdf. It even has it as command line feature. Can't beat that rendering engine and its free. Wrapped the whole thing as microservices but can't share as it's for a client
PC, .Net, Visual Studio, SSMS, and batch files. Yes, I'm old.
Dll is also a Windows thing. Shared objects/libraries are definitely also present on other platforms, but isnt Dll to SO basically what Exe/PE is to Elf?
Why is EDMX bad? I find it quite useful.
PC, .NET Framework, Visual Studio, mostly Powershell. SQL Developer for Oracle. Holding on .NET Core until there is a sound story for native GUIs, feature parity with EF 6, support for Oracle drivers.
C# dll use il code so can be interpreted on any platform. An executable has to be executed (lol) in a platform specific way.
Maintenance. We have inherited multiple projects using edmx and they all had problems if you tried to change anything. Each time we had to a choice to redo the whole edmx or change to code first.
* .Net * VS (I do use VS Code if I need to touch a non .Net project) * SSMS... the other thing was just released and it seems to be the VS Code version of SSMS " there doesn‚Äôt appear to be a Right-Click create database option. " * CMD
So its not a regular dll then, just re-use of the extension?
I have a question for you, I have a deployed application that uses Code First, but I have a lot of problems with foreign key relationships getting generated properly. Do you ever have this issue? If so how do you get around it? I was contemplating reverting back to database first but your comment has piqued my interest.
PC / .NET / VS. Mostly building desktop applications and Windows Services. I do have a Mac for iOS development, but it's not something I'd want to use on a daily basis. 
Dot net languages do not compile to native code they are interpreted languages.
So its not actually a dll then.
I use Visual Studio on a PC, but ssh to a Linux server running Docker for deployments with ASP.NET Core.
Not sure how much this will help you, but to give you an idea of our situation: I've been writing all of our new software and sites at work in ASP.Net Core. I have an on-premise gitlab setup for VCS and CI. It was relatively easy to setup gitlab to do the workflow push -&gt; build binaries -&gt; build docker image -&gt; push docker image git gitlabs private docker registry. Personally I love the direction Microsoft has been starting to take with .NET Core.
Well no it's a .Net assembly. I don't see what you are getting at?
&gt; problems with foreign key relationships getting generated properly At first, every single time ;) Practice, practice, copy paste. Sometimes I need to add custom stuff. protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Entity&lt;Thing&gt;() .HasRequired(x =&gt; x.Category) .WithMany() .Map(t =&gt; { t.MapKey(nameof(Thing.CategoryId)); }) .WillCascadeOnDelete(false) ; } If you change the database and then use the wizard, the code it generates might have a lot of stuff in OnModelCreating. 
I don't think it's reliable to ask like this. We use what we need to, not what we actually want.
Tell me what you want.
Cool, good to know. I get scared a lot when I update the database because I feel like the relationships don't get mapped correctly and I keep changing the DB schema. It's a live application so it's a little hair raising.
I think he means native binary is what DLL is supposed to mean? 
They aren't Win32 DLLs, but they are PEs. From a pure technical perspective, I believe you can use Managed EXEs as if they *were* DLLs, even cross-platform, so the main difference is that EXEs have an entry point (which is ignored on non-Windows envs). I could be totally wrong but I believe that even .NET Core DLLs contain PE-related headers that are simply ignored when used cross-platform. If you do package it into a native executable you will lose the cross-platform nature of the binary, but also lose the extraneous PE data. A truly superior solution would probably be to replace DLLs with a .class equivalent that doesn't contain any Windows cruft at all, but it probably isn't worth the cost of adding a new file format. P.S. Managed DLLs are not "irregular", just different. It's possible, though unusual, to [mix native and managed](https://docs.microsoft.com/en-us/cpp/dotnet/mixed-native-and-managed-assemblies) in the same DLL.
Is mono that slow? I've not looked into it at all since vnext but I remember the benchmarks being good.
Hmm seems most of the time the answer is "both": * Laptop/PC with Linux Servers * Both * Both * SSMS if working with MSSQL else whatever the best tool ist * Both
Still generally faster than, say, Python but except in certain cases it's generally slower than the same executable on Windows.
Make a copy in dev so you can test changes. When I'm not sure, I check the database after update and then rollback and delete the migration if I'm not satisfied with the results. 
This isn't really true, strictly speaking. All of the assemblies in the standard library come pre-compiled (to native code) when you install .NET Core -- same with .NET Framework. There is only minimal JIT work at runtime for those assemblies. I understand there's some differences in opinion around what it means for a language to be "interpreted", though.
I wasn't talking about the standard library though.
Yeah but we are on the dot net sub so I don't see what the point in making that statement is.
PC / .NET or Core depending on Project / VS 2017 / SSMS
does that work on .Net core?
The same tech is available for any .NET library, although few actually use it.
Windows Forms? Geez‚Ä¶ I guess he‚Äôs still gunning for Windows XP support with his products.
Some workstation grade Linux distro like fedora and i3 with vscode and/or atom is my ideal Dev environment. In practice I'm still working on asp.net 4.5 projs so have to stick with win10 and full fat vs professional.
Yes. Your just wrapping chrome which will save it to some file. You then just read that file when chrome finish rendering
Headless chrome is an executable. The Process class will take care of it.
Pc, VS w/ssdt, ssms, powershell, notepad++ (for quick file access) 
Thanks for that. The reason I asked is I've been doing .net since pretty much day one.the very first VS.net in about 2001 and I used to use WCF when it came out..but then it became a shameful thing(not by my choice) and I haven't been using it for a decade but thinking about going back now that I make decisions for myself with regard to tech. Has it changed much? Do you have any good resources for best practices and cookbooks/patterns with it?
.exes have either a main() (/subsystem:console) or a WinMain(/subsystem:windows). .dlls have DllMain(), but no main/WinMain method. To use DLLs in other applications, the methods need to be exported via __declspec(dllexport). .NET Assemblies are DLLs (have a DllMain method) and EXEs (have a WinMain method), but neither exports symbols via declspec(dllexport). Instead they use a .NET manifest. More info [from microsoft](https://support.microsoft.com/en-us/help/815065/what-is-a-dll) 
That's cool, didn't know about headless Chrome, so I looked it up and here is documentation and an example to print to PDF: https://developers.google.com/web/updates/2017/04/headless-chrome
+1 for distributed transactions
.... Using our $20/month/1000 users closed source proprietary product Saved you a click!
If there were exes then it wouldn't be cross platform. The entire idea behind Core was to make it platform agnostic. Granted the fact that Ldap support will only be available for Windows, which pisses me off, but most cross platform is the way it should be. 
No. Please don't do this. If you want executables use 4.7 not Core. 
Everyone hates it but it is still widely used in the industry. Also if you are aiming at beginners dropping a button onto a wysiwyg editor and double clicking it is much easier to explain then say xaml. WPF has a steep learning curve and it is unnecessary when creating simple applications. For more complex applications it is an obvious choice.
* Macbook Pro * I flit between VS Code, VS for Mac and ... er Vim (yes really). * Powershell never (hate it). Bash (I don't like it much either but it's a legacy of Unix so it has an excuse to exist). * I'm trying to get away from MSSQL but unfortunately I have so much legacy tied to it I can't so I use [SQLPro](https://macsqlclient.com/).
You could always run 4.5 under Mono if you're feeling adventurous. It's what I was doing before Core was released but it does have its pain points. [Here's](https://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html) how I got it running in any case.
Which MBP do you have?
&gt;No. Please don't do this. If you want executables use 4.7 not Core. ...why?
15 inch Retina (Mid 2014), 2.2 GHz i7, 16GB RAM
My ideal .NET environment these days... * MBP 2016 16GB * .NET Core * JetBrains Rider. I've always used the IntelliJ keybindings in R#. So pretty much heaven IDE for me. * For SQL stuff, it's back to JetBrains for me w/ DataGrip. * ZSH 
Because it makes no sense using core for Windows only development. Exe files only run on Windows. Use the right tool for the job. 4.7 is fantastic for Windows development. Core is great for cross platform development. 
Presumably it's because .NET Core is supposed to be truly cross platform, and the .EXE file extension is a DOS/Windows construct. I'm pretty sure you can still create what is otherwise a command line application DLL and execute it with 'dotnet run' whatevers.
Nice! I'm toying with the idea of a 2017 15". Do you feel the size is good? I don't need crazy portability - just coffee shops and couch.
Pretty poor example. Missing code and important explanation as to why he has done it this way. 
If you're shipping for multiple platforms, then you can just produce multiple standalone deployments. Also, .NET Core is great for Windows-only development. It's significantly faster and more advanced than .NET Framework in many, many ways. Everyone should prefer .NET Core unless you are using an unsupported technology (and then you should consider using a newer technology that works on Core).
Same than me 
I exclusively use Core for my personal and professional development. At work we deploy to Linux and leverage Docker for numerous reasons. I agree everyone should prefer Core but only when it makes sense. Also the last point of using a newer technology is not 100% true. For example, you are developing an enterprise application where you need to interface with active directory. You will not be able to interact with AD because System.Directory has not been implemented. The dev team is implementing it and it will be released soon with a catch. Forget about it working on Linux or Mac. There is no support and it is in the ethereal future pipeline. Core is not the tool to use. I know there are 3rd party libraries out there, but they do not implement all of the necessary methods needed for effective use such as create, update and delete functionality. My main point is Core is for cross platform development and that is the reason it was created. Having multiple standalone deployments for the same application creates chaos and technical debt. 
PC .NET Visual studio I also use this for SQL, but will occasionally pop into SSMS for specific reasons. I do rarely use visual studio code for html and other text files if I don't open them in visual studio. I use powershell and command prompt
Know who has a really good example of this? [these guys](https://github.com/aspnet/Security/tree/dev/samples/JwtBearerSample)
Or you know..just look at this example https://github.com/aspnet/Security/tree/dev/samples/JwtBearerSample
PC, Azure, Visual Studio Community 2017
[here ya go!](https://docs.microsoft.com/en-us/aspnet/core/index)
I'll just leave [this](https://docs.microsoft.com/en-us/aspnet/core/index) here
Sorry, I'm finding it hard to follow your argument against using Core for an application that only needs to be supported on Windows. Is it that, if you want to use APIs that are only supported on "fat" .NET, then you have no other option? But that's true for any tool: if it doesn't satisfy your needs, then there's no argument. I thought the original claim was that you should favor "fat" .NET over Core **when both are possible solutions**, as this originally came in as an unprompted response to a suggestion to publish an existing Core app as a self-contained win10-x64 executable. I thought the JIT in the latest version of Core is doing certain optimizations these days that "fat" .NET hasn't gotten around to implementing yet, e.g., de-virtualizing virtual calls? &gt;My main point is Core is for cross platform development and that is the reason it was created. Is that true? I genuinely don't know what sparked this development, and I'm striking out on trying to Google it. My intuition says that it's probably not **just** that one factor, but that combined with the benefits of developing each modular piece on its own, and getting input from the community to pick up some of the low-hanging fruit that just doesn't make sense to invest your own very limited developer resources working on implementing, like a constructor for `HashSet&lt;T&gt;` that accepts the initial capacity, or some further LINQ-to-Objects optimizations, or a `KeyValuePair.Create&lt;TKey, TValue&gt;` method, etc.
&gt; My main point is Core is for cross platform development and that is the reason it was created. Not really -- there were a lot of reasons. Granted, cross-platform development was a major one, but it's ignoring a lot of others. &gt; Having multiple standalone deployments for the same application creates chaos and technical debt. This is how pretty much all software is delivered. Actually, it's comparatively much easier to accomplish with .NET than it is with, say, C++. With .NET, you can publish for any other runtime quite simply (just use a different `-r` option), whereas with C++ you will often need to use incredibly complicated cross-OS build tools or require that each OS builds its own application.
SP2017/VS2017/SSMS/Sublime/VSCode
I agree with your question and your sentiment 100%, and I think it's funny how everyone in here attempting to answer your question is instead actually attempting to change your opinion instead of actually giving you a good reason and solution. There have been a few good responses as to why a .exe is not provided with core, and a few good workarounds with the shell script wrap, but for the most part everyone in here is basically saying "you're an idiot and you shouldn't feel the way that you do" and I think that it's a bit toxic. I hate it when you ask an honest question and all of the answers either move the goalposts or try to change your opinion. I also wish core would compile to an exe on windows for the same reasons that you do: easy to launch, and you can drag/drop files onto it an have them run as a parameter. And I agree that core is using the dll extension wrong, or at least strange.
It‚Äôs called Peek Definition, and it is Alt+F12. VS feature, not R#.
I have watched a couple DotNet Conf videos on ASP.Net Core and gone through the tutorial before on Microsoft's website but I am absolutely loving these training videos. I think it's because I've done a few examples before that's really helping but I finished the beginner and am halfway through the intermediate and man, these are really filing a lot of gaps and holes in my understanding. I can't recommend highly enough! It's so nice to hear someone talk and pick apart all the actions you take and setup.
Virtually all multiplatform software gets distributed with self-contained executables targeting different operating systems. Visual Studio Code. Spotify. Google Chrome. A billion others. Why is it such a bad thing if .NET Core apps follow suit?
* PC or Mac? **PC** * .NET or .NET Core? **Both, but prefer Core where possible** * VS or VSCode? **Both** * SSMS or SQL Op Studio? **LinqPad :) Or SSMS if necessary** * Powershell or Bash? **Powershell**
I went ahead and joined the gitter/commented on the github issue.
I had occasion to try using Aspose for HTML to PDF a couple years ago (company already had a license for other functionality). Unfortunately, I found that it has complete garbage HTML rendering, so I had to find a different solution. It was a bit like trying to tame IE6, except it has all its own stupid quirks (and they are so much worse, even intermittent), and almost no one uses it so you can‚Äôt just Google for solutions. There‚Äôs plenty of better options for less money or even free that just work. See other comments in the thread. Headless Chrome sounds promising.
I would start from here: http://www.oreilly.com/pub/au/741
Hey /u/cq2020 if you're running this on a modern Linux, the preferred way of writing applications for users is [Desktop entries](https://developer.gnome.org/integration-guide/stable/desktop-files.html.en). I just tested the following (on core 1.x so your exec line may be different than mine) and you can both double click on the .desktop file to execute the console app and drag a file onto it to execute with the file in the program's args array. Create a file named `testapp.desktop`: [Desktop Entry] Type=Application Name=Test Console App Exec=dotnet run -- Terminal=true Your exec line will probably look like: Exec=dotnet do.dll -- Maybe that will be useful for you...
Any good resources that ELI5 docker and how to use it?
Oh man, I dunno if Docker can be ELY5.
.NET Framework executables are specially recognized by Windows. When the Windows loader sees an .exe compiled for .NET Framework, it knows how to invoke the runtime to load the program. .NET Core is not designed this way, it does not have any special integration with Windows. Therefore it either needs to use a loader a la Mono and Java, so it's `dotnet app.dll` just like `mono app.exe` or `java app.jar`. This enables a few interesting things, for instance, the ability to have multiple .NET Core runtimes installed and easily switch between them, and a simpler installation process for the runtimes themselves. Alternatively, they added the capability of AOT compilation where it integrates a platform-specific loader into your executable.
I am kind of happy to not be the only nostalgic guy about WCF... So many times I banged my head on a wall reinventing the wheel in REST of what WCF was already doing... Then the cool kid tell you "keep it simple, you don't need this feature". So you shrug, and reinvent the wheel for the feature actually you need... then 1 month later the cool kid come back "wow have you seen this new technology made by google/facebook/amazon? it can do automatically what you did last month"! Yes I know... this technology was a backed in standard in WCF 6 years ago and not tied to any specific vendor...
It's the Dev tools really. I don't like win server, but at least through managed hosting and octopus deploy it's mostly hands off after the initial configuration. But mono develop/Xamarin Studio really doesn't hold a candle to visual studio, and that's before Resharper. 
Yeah. There is another one called wkhtmltopdf or something like that aswell. Same principle. Based on webkit.
Render html to pdf with webkit engine. Without having chrome, I believe. I have used this a lot: https://wkhtmltopdf.org/
Both desktop and laptop with Windows; at the work .NET Framework, for my own projects .NET Core, VS17 as IDE, VS Code as a text editor (instead of Notepad++) and for ES6 (mostly React); SSMS; native bash shell supplied by Microsoft.
Docker documentation is very good. Also you can find many tutorials on how to use it with template projects.
Awesome. We have achieved 100% growth in just 4 days!
What kind if problems? I can imagine issues might occur if there were significant drift, i.e. the model hasn't been updated in a long time. Not trying to defend edmx, I've just never had a problem if things are kept in sync.
&gt; Use the right tool for the job. 4.7 is fantastic for Windows development .Net Core 2.0 is also fantastic for windows development. I'd say depending on requirements I'd always now choose .net core over .net framework.
Yeah I hear you on the dev tools though conversely that's one reason I moved off Windows - if I stepped outside of MS tech I found the experience was better on Mac / Linux (the other reason was Windows Server and too many blue screens of death!). That being said VS for Mac and VS Code are both pretty solid on a Mac with healthy competition for Rider (which I don't use but it does have Resharper and the other tools built in).
Yeah the size is about right, I could've gone with a lighter 13 inch but glad I didn't. I have occasionally worked in coffee shops and found the portability okay, battery is a good half day with extensive use (tip, use Safari instead of Chrome if on battery).
If you rename a core compiled dll to exe say app.dll -&gt; app.exe and run said executable on windows it launches, it just spits out errors that it can't find some assemblies. More specifically ```Unhandled Exception: System.IO.FileNotFoundException: Could not load file or assembly 'System.Runtime, Version=4.2.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a' or one of its dependencies. The system cannot find the file specified.``` So if a core binary can already be launched directly by simply being renamed, why can it not automatically detect the right path for the assemblies it is looking for? I assume that when you run it the normal way using `dotnet run` it reads the runtimeconfig.json file to detect what version of core to use. Then how come it can't detect that by running directly? It seems that the feature is almost there, just not fully realized for some reason‚ùì &gt; .NET Core is not designed this way, it does not have any special integration with Windows. If this was true then why does renaming it to an exe work? If you drop in the missing dlls it even loads some of them, though I couldn't get all of them to load like that.
ThinkPad .NET Framework Visual Studio 2017 PowerShell for build and deployments I also build on the command line from time to time instead of in the IDE to make sure the build is done the same way as in CI.
Does one need to understand Amazon Web Services or Azure to work with a docker containerised app?
Mine: * Windows 10 Enterprise * Dell XPS 15 (32GB Ram, 1TB SSD, quad core i7) * Two 1440p monitors connected to laptop (one via HDMI, the other via USB-C). Also using third 4k screen on laptop. * Visual Studio 2017 Enterprise * .net core 2.0 * Powershell * SSMS * Visual Studio Code I do most of my dotnet development with .net core these days, and have taken to jumping between VS Code and regular Visual Studio as I feel like it, but still use regular Visual Studio most of the time. I also have a mac so have been playing with dotnet core on that. I like to make sure all of my code builds on the mac, as a sanity check.
The statement above is one I can get behind!
Make sure you‚Äôre compiling your project with the latest language features. Go the build tab on the properties page of your project and then click Advanced and ensure you‚Äôre using the latest mi or version or explicitly 7.2. Also you need to have VS2017 15.5 installed. 
Totally agree. All my machines at home run Linux and it'd make my Dev life so much happier if I could use the tools I like at work. But until we make the push to dotnet core it's not an option. The windows Linux subsystem does make the whole thing a lot less painful though. 
Yes, the C# 7.2 features are enabled, and the others are working fine, but the System.Span type is missing. Upgrading VS or setting the language level won't fix this.
Code can be found in the linked GH repository and as described in the article the overall sample aims to be as simple as possible.
Should be in the system.memory namespace. That's where it is in core fx, https://github.com/dotnet/corefx/tree/master/src/System.Memory/src/System
Neat. It seems you're on the right path using System.Memory. This article (http://adamsitnik.com/Span/) seems to indicate that this will be how it works and currently native support is only offered in .net core 2.0. If you need something more recent, aspnet team has ci builds available at https://dotnet.myget.org/feed/aspnetcore-dev/package/nuget/System.Memory which are more recent than the official one in nuget. Personally, it feels like they are gearing up for a minor release and i'd expect we'll see a full release in January.
A thing to note is there is still work being done to bring Span overloads to .NET Core 2.1 (and I assume .NET Standard 2.1). That will greatly improve the usability of Span. https://github.com/dotnet/corefx/milestone/12
I was wondering the same thing, turns out current builds from the Core repo are available [on myget.org](https://dotnet.myget.org/feed/dotnet-core/package/nuget/System.Memory).
Yep this is what helped me lock down my app. 
&gt;if things are kept in sync. I don't know the history of the project before we got them. In one case, adding a simple property caused a bunch of pluralization problems with entity names.
https://www.youtube.com/watch?v=gJLIiF15wjQ
I am done banging my head against a wall. I have said my point of view. Use whatever you want. If you want to use something that was meant to be x-plat to create Windows only applications then do it. It only goes against the core principle of the framework.
My point was related to the original question and the answers provided: that DLLs somehow are better than EXEs for cross-platform. From my understanding, DLLs are equally tied to the Win32 platform as EXEs so I dont get why DLLs were supposed to be superior. DLL is a Win32 concept, and I wondered why its better for cross-platform to call DllMain rather than Main and why its better to ignore all the Win32 specifics of DLLs compared to ignoring all the Win32 specifics of EXEs. It seems quite the same for me.
Thanks!
OK I just wasn't sure why you were asking. It won't call DLLMain it will call the managed entry point (usually Program.Main) therefore when running your dll with the dotnet command there is no native entry point required. However if you compile to an exe it must first connect to the DotNet CLR with native code before calling the managed main. I apologise for the misunderstanding. Let me know if you have any other questions.
:) I see a lot of people in here that said they were interested but I don‚Äôt think they‚Äôve revisited this thread since you added the gutter and github setup (Thanks for doing that!) and it‚Äôs fallen down in the list now. Maybe a new thread would help! 
Add your name to this case and I will add you to the github organization https://github.com/DotNetStudyGroup/aspnetcore/issues/1
Maybe this (https://www.telerik.com/support/kb/winforms/details/wrapping-a-net-user-control-as-an-activex) will help you. Didn't tested it yet (still have to finish the control that I want to use in Access) Tell me if it worked
Thanks very much for the suggestion. However, it did not work. After I removed all of the controls from the sample project's user control that came from a library I don't have and converted the project from Visual C++ 6 to Visual Studio 2008, I got the same result as before.
&gt; If this was true then why does renaming it to an exe work? It clearly doesn't, from your example. It's possibly looking in the GAC for the runtime, which isn't there because .NET Core doesn't have a GAC.
You do know that C++ is also a .NET Framework language, right?
&gt; I am 12 year old danish elite K
no c++ is language for men and .net is language for little girls
.NET isn't a language. It's a runtime for many languages. You can write code in C++ that targets .NET. I have.. many times :)