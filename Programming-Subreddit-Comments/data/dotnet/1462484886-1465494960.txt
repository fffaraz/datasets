Its not smarts you are missing, its confidence. Also im a software architect and i would say the role will involve some degree of politics. All leadership positions will. 
How does this compare to IntelliTrace?
Go to local meetups. That's where you can meet fellow devs at least.
make sure you're on linkedin! have up to date skills and maybe even a resume up there as well!
This is so me, it's not even funny. I've been everything from a junior dev to Director of Technology over the last 18 years in IT. I think I might be happiest doing what I do now: Dev Team Lead. I'm considered a leader and have quite a bit of influence with both my manager and team, but I have no direct reports nor do I have to deal with management politics. I feel like I am respected by all departments for the depth and breadth of my knowledge and ability to get shit done when other people can't or won't. I get to jump in and code (and enjoy keeping my chops sharp), but mostly I work to set my team up for success: I track down and solve weird problems, keep our web environments running smoothly and am the point of contact with our systems engineers, DBAs, project managers and quality assurance team. I have my hands in everybody's pie, but ultimate responsibility for nothing. I have no power and kind of like it that way. I still get the respect I deserve (and am well-paid), but I don't have the headaches of dealing with personnel crap or bullshit middle management politics. I get to **just work**. Money isn't everything. There's a lot to be said for quality of life and low stress. 
I wouldn't worry about it, a ton of business problems can be solved without a binary search tree!
Our team lead definitely isn't the best coder on the team. But, he likes handling some of the management stuff, which I'll gladly let someone else do. 
That is good to hear.. MY fear is a tech lead should be the best coder there... But I really like talking to people about real things.. I am just terrible with small talk
I know. I just bring it up since it was my worst interview ever.. I was thinking I am just doing web stuff right? Not compression algorithms? 
I do see a lot of 'Sr. Developer' jobs.. Maybe that job title really is what you do in many cases... OR sometimes maybe it is just another developer.. I am strange in that I enjoy meetings when we talk technical and high level ideas about how to do things. And it seems every developer I met hates meetings. I my job now I am never in any meetings I like your job. I want it
I am hoping to get more the inside scoop on a few specific places because when I open myself up to random people I get recruiter spammed... and like I mentioned I don't want a new job NOW... I want connections for the future.. so I'm trying to figure out how to go about doing that
no doubt. I should just go interview for the hell of it if there is atleast a 2% chance I will want the job... But only for jobs that give me atleast a 50% payraise... Should not be much pressure on my interviewing if I already have a job.. Sometimes I even like interviews if they are not the kind that put me in front of a whiteboard for an hour.. I like talking big picture
that's the cool think about linkedin, at least for me. it's not really "random" people, you start with very close friends, and they may know someone who works for big company x 5 miles down the road. and you know them through your cousin, and so on, etc. i'm a very reserved guy. programmer for 20+years. lots of contacts and those contacts have lots of contacts. when something comes up. ... well. you know how they say it's not what you know, it's who you know? good luck.
Yes this is the best thing except the the lack of .NET devs at these meetings :(
I imagine the perfect way is somehow know the HR people at companies that have .NET devs and then check in with them once in awhile... but they all use recruiters.. and I don't know an effective way to find them
Actually the best thing is if there was a /r/dotnetwashingtonDC but still not everyone who codes is on reddit.. Though maybe I should just post a message to hear from DC area dotnet people
i put up a linked in. an old friend from my last company contacted me said they had an opening and would forward me to HR. i got a call (literally less than a day after putting a resume on linked in) from the hr guy and had 1st interview on the phone that day. next day went in, drug test, following week 2nd in person interview. hired. "they"'re out for .net and java programmers (i do both). put your name out there and they'll find you! 
Have you had a look at Filehelpers?
And **again**: This is about the **.NET Core** framework, not the .NET framework.
Apparently you can cluster a bunch of node instances together and then bolt-on message passing between them. Its as hacky as all hell, but it kinda-sorta simulates share-nothing multi-threading.
I find a ton of work through people that do not know many other .Net developers. Being that one .Net guy they know is great once their Web App needs to be expanded to the desktop and nobody knows how to do that because all they know is PHP. TopTal is a sort of connect programmers to clients thing where you go through a pretty heavy selection process first, once you're you can basically pick from a lot of remote jobs or jobs in the area you're looking for.
TopTal sounds interesting ill try it. Though when you have to compete with millions of developers from India I am guessing your worth will go down right? 
I been to a few. I don't like the 'presentation' ones.. I want to just socialize.. I can watch presentations at home
I work for Microsoft near DC. We have a fairly large amount of people in the area doing development, mostly government consulting but not exclusively. Send me a PM if you want more information. 
lots of big companies do a drug screen when you get hired.
if you want to concentrate in backend, then learn C# the language, WEB API framework and then entity framework. The world is moving towards (or has already moved) to a some backend that send back JSON and some JS framework FE. I love using EF to talk to RDBMS and I think ORMs is what everyone is moving towards
TL;DR, using InteliTrace you have to know what you’re looking for beforehand. With Time Machine for .NET you can look at whatever you think you need at the moment in real time. In a little bit more detail -- IntelliTrace when running without the Visual Studio debugger records only method entry and exit points and drops parts of parameter values when they exceed a certain amount. To record the history of variable values you need to hook Visual Studio into it and add the exact variable you are interested in to your watch list (or evaluate in the debugger or have trace points defined against them). Time Machine for .NET record locals all the time and doesn't really need a debugging session at all. It also overlays recorded data directly onto the source code making analysis much easier and faster than IntelliTrace. Also, it runs on all Visual Studio 2015 editions (even Community ed.). 
The friendship Heights stop I am guessing. 
Are they all for consulting or do any of those building build MS products? 
But if the local jobs pay better why not do them ? Do you really enjoy the freedom of being remote that much ? Do you work alone on projects? 
Ok. So you work some for TT and some directly with Euro clients. Both are remote. So why work for TT at all if you have hire pay through Euro clients ? 
just gotta pass a test once. ;)
That fine as long as my hire date is 3 months from the job acceptance date I suppose.. but in 2016 the whole thing annoys me.. But I'm guessing this is a gov secret clearance jobs that do this ? 
Recruiters is a whole other topic . I've concluded they are a needed evil ... Atleast Ive used them so many times I know all the buzzwords to tell them so we can move to the next step
I've been able to stay in the private sector so far, but it does take a bit more scrutiny to find those jobs.
There is no 5.0 per se, it will be Core 1.0. (Confusing because at one point there was a 5 and they just renamed that to Core 1.0)
Whats your history in .NET in 50 words or less ? Any words of wisdom ?
They are rare. But the environment is normally a lot more interesting .. and for me I really prefer job that is interesting over just getting the max $$$. I like to code! 
Hold on I have an essay to write.
There was ASP.NET 5.0 which is now called ASP.NET Core 1.0. This is unrelated to the .NET framework. The next ASP.NET will run both on .NET Core 1.0 **and** .NET Framework 4.6.*
If it has to run on Windows 7, you can't use WinRT or UWP. You're stuck with WPF. The best advice is to do as much of your business logic in a class library and use MVVM for the UI project. That way, when you move to UWP, you won't have to completely rewrite your business logic code.
I managed to avoid it somehow the past 14 or so jobs . And now that it is legal in DC.. isn't that kinda strange to ban people from a job now for weed ? If you use it legally
Yeah, I understand it's not random, but it's something so far off the algorithm questions one would normally expect. After the interview, I read about it at https://www.pluralsight.com/blog/software-development/idisposable-for-dummies-1-why-what "The .NET C# team went a step further by providing a construct in the language called “using” that will allow you to declare, use and call the Dispose() method in atry/finally block." Unfortunately, I haven't used C# enough to know that, or care about using it in the first place. I believe Java added something like that in Java 8, but then again I was used to calling something like File.close() after I finished using, and I was used to mostly automatic garbage collection. 
Asking about IDisposable is a *much, much* better interview question than some shit about algorithms. And your response here, about "automatic garbage collection," shows that you still have serious gaps in your understanding. The code you would write in an enterprise environment would likely be buggy as a result. This wasn't a "gotcha," they caught you out on a gap in your core education.
I know getting a first job can be tough, and I wish you well. But be careful that you don't take a job you are not qualified for. It'll make your life hell and you employer's too. If the questions are reasonable and you can't answer them, then maybe the job wouldn't have been a good fit anyhow.
Noooo, stop saying garbage collection, that's such a dangerous misconception, that this is about garbage collection. Go read *Effective C#*, it covers this and a bunch of other things like this
&gt; I don't see any monumental features missing in the full framework. Windows.Graphics.Holographic would be a nice to have.
+1 for Effective C#
Winrt/uwp is a limited set of .net, and has limited third party support. Look at the store, it's full of small simple things in part because it's so hard to do anything else. I've spent the last 2 years building a big LOB with it and it sucks. Limit interaction with files. Limited wcf protocols. Printing sucks. We've been considering rewriting to get away from RT. It might work for you but I'd think hard before using it again.
I have interviewed a lot of candidates for different positions. I used to use a test that a few of my coworkers cobbled together. There are tons of sample tests out on the internet and review them as part of your prep work for the position. I have a MCSD and I still review my tests before interviews that I take. When i am on the other side of the table, and I know a lot of my friends do the same thing. **The score of the test does not matter.** I know that a lot of people have test anxiety, what I am looking for is confidence and a basic knowledge of the subject matter. * I want to know if you don't know something, how will you find out more information about it (don't just say google.) * I want to know why you chose something over something else. (like why you would choose quicksort over bubblesort) As with most tests. **SHOW YOUR WORK**. Talk it out like you are a game show contestant. Let them see your thought processes (that you actually have a thought process) and you would do fine. Also, if you have a phone screen test and stumble on something. Expect that something to come up again in person.
It looks like they are taking features from F#, I feel like pattern matching could be pretty big.
WPF is a better option for greater platform compatibility (as others have mentioned). Also, there are a lot of great libraries and articles related to WPF. If you get stuck, finding a solution should be fairly easy.
That's a nice list.
Nice name. Coincidence? Not all user groups are necessarily like that. Depends who's organizing it. You could always suggest a different format. Maybe some lightning talks, or an open spaces night to switch it up? 
You're not expected to know everything, you're expected to know some things and learn others. For example you might, just off the top of my head, be expected to know that the IDisposable interface is for disposing of unmanged state. Things like file handles, network connections or COM objects. Stuff that the garbage collector doesn't handle very well. This is pretty basic stuff need to know how to use to make even basic .NET applications properly. It's also something you haven't actually spent even thirty seconds looking at because you still think it's a destructor, which it's not. Unlike the algorithms you were expecting I've used it more than a dozen times in the last week. As to what you should learn. Pick a language, a single language. Learn that language to the point where you're not an embarrassment to whatever university you graduated from and then apply based on your skills in that language. It's easy for a Java developer to learn .NET, or even for a C/C++ developer to do so. Converting from a managed language to one that's not is harder, but still doable. You don't have acceptable skills in any language though and you don't seem to know how to learn new skills. You're also either massively overestimating your experience or lying outright.
Not least of which Visual Studio which still uses WPF in order to write UWP ;) 
I agree with everything you said except for "you don't seem to know how to learn new skills" to an extent. Unfortunately I can only compare myself to my classmates, and from my experience those classmates had worse skills than me. For example, some would not know what a pointer was, and how it worked. But they managed to get a job. I've also interviewed at some other C# shops, and they never asked me this domain knowledge type questions.
&gt; We also need to make it easy to work with projects across these application models. In order to do this we are working to merge the capabilities of .xproj/project.json and .csproj project systems into a single project system based on MSBuild. Slightly scary this... what are they up to? 
There is already a lot of good advice here but I want to echo the importance of talking through your thought process. Even if you've never looked at the IDisposable interface, you could probably say, "Well, I've never studied this interface directly but based on what I know about interfaces, I think that any class that implements the IDisposable interface can be disposed". It is likely that this type of answer would be sufficient with most questions like this. It shows that you are honest (because you admit not knowing exactly) and that you can apply knowledge between different situations. 
Unexpectedly soon, wow.
There a happy hour themed one I goto but its not dotnet. pointer to unknown outer pUnkOuter
Search C# interview books on Amazon. Study it until you can answer 80%+ correct. This will also be a great way to get an all around better feel for the language. But don't just memorize words, really try to understand the aggregated concepts.
I hope relying less on xproj/csproj and more on project.json
Why? The `project.json` is **awful**. The major reason for this is: **It's JSON.** As we all know, JSON does not support comments (because it's a **data-interchange language**). Now look at the content of a `project.json` file: You can include globs, exclude globs, then include single files again. If you ever have to use this, wouldn't you want to document these cases? You can also add tool references or even scripts to be executed at specific moments: That's definitely stuff that should be documented too! Too bad, you can't, JSON does not allow it. So you end up either having no documentation for this, or you have a second file next to the project file. And when you have a second file, odds of the changes drifting apart are increased. Note: I know that the JSON specification technically says 
Hi, I'm actually in the process of working out how I'd like to handle settings in a WPF app right now. I stumbled across this solution on Code Project about a week ago which I was planning to strip down and update to meet my needs: http://www.codeproject.com/Articles/25829/User-Settings-Applied I haven't quite gone through all of Jot and the source for it yet. If you are familiar with that project or have some spare time could you compare and contrast Jot with what it is doing? Thanks.
Oh definitely. I typically see 15 to 20 downloads every time I post a build to Nuget, even for the first placeholder version that doesn't actually contain any code.
Because then you mix up **data** with **comments**. Also, this "hack" is not compatible with array elements. And it likely will also violate some schemas.
.. and the problem is?
You shouldn't be in charge of variable naming. 
&gt; Json doesn't have a schema The `project.json` has a schema that is being evaluated by VS. &gt; and any array can be part of a larger object that has a comment. You can't comment individual elements in an array.
I bet if you put a comment attribute anywhere in it that it will ignore it. You can add a comment attribute on any element in an array. 
Show change history displays the entire series of changes along with the messages. Annotating a document displays commit messages relevant to each line of code. I think that is adequate for my needs. Commenting reasons for changes in files is a bad practice. Files end up with too many comments and obfuscate the meaning of the code. They are also prone to errors and becoming stale. Like I said I understand your issue, but in the case of a project file I think this is a bad practice in general. For structures that you have more control over I still say you can add comment properties to the structure to fit your needs. 
Small error, sbyte is equivalent to byte in Java; Java doesn't have unsigned byte types.
If you think you have issues with IIS idling off your site. Just set up a script to load the home page every 1 minute. You could also setup logging in application start and end. I do this on all my sites anyway, log out every app start and end. Can be very helpful in determining a stack overflow that kills your site silently 
There will be a .Net 4.6.3 or a 5.0 or whatever they choose to call it. The important part now is that it will implement `netstandard1.6` ([just as 4.6.2 will implement `netstandard1.5`](https://github.com/dotnet/corefx/blob/master/Documentation/architecture/net-platform-standard.md)). New functionality will largely be in NuGet packages but they simply can't do everything. For example, let's take `string`. If we add a new method to the `string` contract for netstandard 1.6, it has to be pushed to desktop as part of those libraries. We want to do things like this with `string.Format()` [with generic overloads](https://github.com/dotnet/corefx/issues/1514) (which avoids allocations due to boxing). However, adding these just to CoreCLR doesn't work. They have to be available on the desktop CLR as well in order to expose them in the contract as being available everywhere for that .Net generation. The way types work in .Net Core on a desktop CLR install is the core types for all the things like `string` to `DbConnection` are all there. The *members* are there. So .Net Core **forwards** the type to the existing implementation already on the box, without all of the DLL weight. The implementation for `netstandard1.5` would be your .Net 4.6.2 install, or: you include the entire runtime with the application. For the purposes of forwarding, the important part is **it's at the type level**. You cannot add individual members, because when forwarded they won't be there. There has to be a new desktop CLR release backing those members you just compiled against, unless we want every app to distribute all the bits. I realize it's not a simple thing to grasp at first, and honestly I wasn't cognisant of nearly all the situations where this restricts API design until we went through it for the .Net Core ADO.NET bits. I hope that helps - please ask about anything that's unclear.
Seems like a nice enough library, it does most of what Jot does: make settings persistent without requiring you to write code that copies data back and forth. I prefer Jot, I think it has a way better API and does more, but I'm obviously biased and I only read through the article without giving the other library a proper try so take it with a grain of salt. 
Microsoft technology has some of the most powerful instrumentation and health features in all of the world but so little of it is provided to you out of the box with no active effort.
I don't know the overall costs sorry, I just tell the IT guys what I want and they take care of licensing and so on. The company uses Jira (and has done since before I started) for all issue tracking, so I have never looked into its costs, but the pricing for Confluence and Greenhopper is very reasonable. Obviously it all depends on your team size though and I'm afraid I can't give a number. What I can tell you is that those three products all integrate very nicely, although we haven't looked into integrating them into our VS environment as you can do with TFS and other such products. Plugins do exist though so I'm sure you could set it up. My experience with other plugins for their systems have all been good and I would expect their VS/TFS plugins to be of the same quality. (Now I think about it I am actually going to research this myself - we are all located at the same site so our need is less pressing than yours, but it would still be a positive step for productivity I am sure.) IDK how the Atlassian stuff plays with TC either, but I know it will work with Bamboo. As far as TC and VS go they work well together. We're using TFS and have no problems, but Git would work fine OOTB too. Everything in TC seems to. TC will also integrate with Octopus Deploy, though we haven't set that up yet either. It publishes our NuGet packages as part of the build, which is very convenient. I get what you mean about the MSFT stack only devs. We use C# and ASP.Net MVC and not much else from the .Net libraries, but that is partially a function of our architecture as well as personal choice. A lot of places only use standard .Net libraries, which would drive me nuts!
You can also configure IIS to be "always on." This is easiest in Azure (it's just a switch) but totally possible on a vanilla IIS install with a bit of config. More info at: https://www.simple-talk.com/blogs/2013/03/05/speeding-up-your-application-with-the-iis-auto-start-feature/
My company has products written with C#, VB.Net, Java, JavaScript, and some older languages that are being phased out. Atlassian stuff has worked really well for my company * BitBucket (Git) for source control * JIRA for big tracking * Confluence for documentation and how-tos And they all integrate with each other. Do yourself a favor and use Git - it's more flexible than TFS and once you have practices in place it is just as simple to work with. Look up GitFlow and follow it's guidelines. Visual Studio Git integration works well enough, but our team uses the SmartGit client since not everyone has projects in VS
I'm about to setup a simple dev/bug tracking system at my workplace. Development is done in VisualStudio (C# Desktop), Eclipse (Java Desktop), and Excel VBA ^^. We will use git with Gitlab as Development and Issue Tracker. All you need is a server or NAS (like Synology with Package Management) and you are good to go. Also for small business the free Visual Studio Community edition will be sufficient I guess, you need to check the license terms though. 
Community edition is only good for up to a team of 5. He has 10. That means no community edition licenses for him.
I thought maybe he could use 5 CE, then only be on the hook for 5 licensed, but maybe he cannot do that? 
IANAL but I'm pretty sure that as soon as you go above 5 guys, you have to buy them all licenses. There's other restrictions as well (like revenue).
I assume you mean you've got a web api and you want to create an in process client library which will hide the http calls from your clients code. That being the case, a class library would suffice.
&gt;I'm not a huge fan of most .net developers I have come across. It seems that most of them are stuck in this Microsoft only world that always seems to be a little behind. They don't care to look at what other environments do right. There are of course many good .net developers, but they are either already taken or hard to find. I just have to ask how much have you actually seen this to be the case. The reason I ask is I read this all over the internet but in the real world of applying for positions it seems that those in charge of hiring has the exact opposite expectations. At least when you're applying for places where software is a support position and not the focus or product of the company. The impression is that if you haven't been programming in .NET for atleast 5 or more years you don't have enough experience and you're cut short. I'm transitioning careers from CAD Design to programming. I've done a lot of programming as a hobby and side work, (C#,VB.NET,VB,Java,Javascript,HTML,PHP,C,C++) for years but just recently went and got a A.S. degree in software development. Basically the reaction I've gotten is I don't have enough particular experience, or to much experience in other areas. (I've done web work but don't want to be a web guy. Prefer to do applications programming.) But because of my web and drafting experience I get they are looking more of an 'experienced .NET programmer'. I'm older as well and thing that has some to do with it because I've known people at a few of the places and know that they've hired someone early 20's with a year of experience despite what was said to me. In your position ads do you have expectations for the developer that you're looking for. I really hope you're not asking for one thing hoping something else walks in the door as most places seem to do. Not getting on you personally just this attitude to disparage .NET developers to the point of abandoning .NET work even when you're a .NET shop I don't understand.
Class Library would get my vote. Lets you abstract whatever type of call you're doing (another DLL, website, etc) and provide a simplified way for the consuming code to call into it.
At least winforms is a desktop framework designed for desktops. Webforms is a web framework designed for desktops.
OP said shared hosting, probably can't use always running 
We develope in C# and .net and use tortise SVN for version control and gemini (it's subscription based per users) to track our stories. Honestly, if you really know C# and the costs aren't going to be prohibitive, that's the route I'd go. It's been my favorite combination of tools to develop so far. My bigger question, what's the potential for this thing to scale, in terms of devs needed to work on it. Is it something that could end up with 100 guys working on it, or do you expect to be around the 10 dev mark indefinitely?
&gt; Is there something as straightforward as gridview on the MVC system? Depends on what you're after. There's nothing "straightforward" about a gridview, even if WebForms came with one that was *straightforward to use*. However, the nice thing about MVC is that it works with all standard, modern web technologies. But remember, MVC is very much for your *backend*, what you want is something to go on your frontend to make the backend data look nice. A lot of the time, you'd be happy enough with a @foreach in your Razor view.. Take a look here for more options: http://stackoverflow.com/questions/177275/grid-controls-for-asp-net-mvc Don't let the fact that Webforms makes it easy to drop a gridview on your page, it's easy because it does it in an utterly terrible way. Switch to MVC and it's trivial to use the latest and greatest web frameworks out there.
I don't disagree with your points regarding the cost of migrating an old tool at all, but /u/ocklack specifically asked me about gridviews and I wanted to offer some advice. In my company, we still use Winforms for our applications and it's in a similar position to Webforms (Old, past it but we're invested) so believe me, I do appreciate what you are saying. &gt; This is actually my least favorite thing with MVC. I think Razor syntax is one of those things you either love or hate. I understand your frustrations and you're not wrong about the "code soup" being nasty if it's abused. Looking at aspnet core, they've gone to great lengths to try and remove a lot of the need for that with tag helpers so fingers crossed it only gets better from here. &gt; Web forms may of had more overhead, but the code on the front end was cleaner. While I use MVC, i really hate mixing my code in html like that. I do think there's an element of personal preference on this one. Certainly a modern site is almost expected to be a SPA and it's hard to make that work with Web Forms - but it goes back to what you were saying about already having a large project and migrating over.
They're not, the idea is that since so much of your app is client driven and your server is lightweight it's fine to have it running on something like nodeJS. Plus if you're doing something like React, it buys you the option of doing server rendering with client bindings for your UI templates.
+1 on integration between parts of your software toolchain. As an Atlassian, I feel comfortable acknowledging that MS has done a nice job considering the flow between workitems, source code, and builds. And err's point is that you want that flow to fit with your team's way of working. So, it might help to understand the key points in the Atlassian toolchain: * Work is managed in JIRA Software (the recent successor to JIRA + Greenhopper which was known as JIRA Agile). TFS and JIRA both have major features like drag-and-drop prioritization and visual management boards (some call them kanban boards). * A developer will begin the integrated flow by creating a "feature" branch from JIRA, which will use the issue number (issue is same as TFS workitem) and title as part of the branch name in Bitbucket. All commits on a branch are thereby related to the issue, so there's no need to continually make the association. * Bamboo builds on every change the developer pushes to Bitbucket. The green/red results are shown for every commit in Bitbucket, and results are summarized in JIRA. Everyone on the team can keep the "pulse" by watching for build and push notifications in HipChat. * Within a couple days, the developer has a set of commits ready to share with everyone. He/she opens a pull request, which is opportunity for peer review, and JIRA automatically moves the issue into "code review". * A couple people make sure the code meets team conventions. After commenting and resolving problems, the developer merges the code, which automatically moves the JIRA issue to "done". * While developers "rinse and repeat", JIRA keeps track at a release level to make sure all issues have code review and builds. That way a release manager can make a more informed decision about whether the release is really ready. Atlassian pricing is transparent. Basically everyone purchases through the web site so there's no intricate discounting schemes and you don't have to talk to a sales person to know it. Atlassian products aren't going to help you find good developers. But I think they do help good developers have freedom to pick and choose the right tools. There's a broad and vibrant ecosystem of 3rd party integrations, including a number of 3rd party VS/TFS/.NET integrations. With Atlassian's Connect framework for it's cloud-based products, you can even write tight, web UI integrations in .NET (or whatever language you want).
&gt; I wanted to add a message to an assertion the other day, turns out it's not supported by design because the authors don't think it's a good thing, Again, undocumented extensibility. Apparently there is a NuGet package that drops the source code for Assert into you test project so that you can fine-tune it however you want. http://xunit.github.io/docs/nuget-packages.html
Aren't multiple reasons for failure in a test a no-no anyway? If you're trying to use asserts for state info, xUnit has logging for that.
Still better than MSTest and NUnit.
True, but that's not saying much.
No, that's just a stupid thing that bloggers say to prove that they don't know how to write tests. Here's an article showing how to write an actual unit test for a first-name property. http://www.infoq.com/articles/Comprehensive-Unit-Test Using the "one assertion per test" methodology, we'd end up with 216 tests for just one property.
You should know, CEFSharp for WPF vs Winforms uses a newer core and allows transparent controls however has a different render loop. I've had to use both and there are differences between the two platforms. Just FYI.
shouldly's support for what you are looking for [Test] public void ShouldSatisfyAllConditions() { var millionaire = new Person() { Name = "Homer", Salary = 30000 }; millionaire.ShouldSatisfyAllConditions ( () =&gt; millionaire.Name.ShouldBe("Mr.Burns"), () =&gt; millionaire.Salary.ShouldBeGreaterThan(1000000) ); }
&gt; Is there an actual question? Are you just ranting?
Congratulations, you managed to achieve the same amount of test coverage with over 600% more lines. Do you get paid by the line?
I didn't say anything about one assertion per test. One reason to fail is different. The idea is that one thing was changed and one failure of any asserts should be enough to report the reason for failure. Using asserts for logging or state capturing is not what I would consider correct testing practice.
C#? OK, lets do this. Challenge accepted. - Visual Studio Community Edition (while you have 1-5 dev's at least). - GitHub for source control. (BitBucket is free, but GH &gt; BB). - AppVeyor for your deployment. (AV &gt; Octopus (awesome but hardcore) &gt; TC (urgh already!) - Trello (free) for your Kanban board. - Slack (free) for team communications. - Azure Web Sites if preferable. Otherwise VM's (urgh). - Split your team into two parts : front and back. With back, try and have small API websites (urgh .. but the fancy pancy word being thrashed is 'microservices'). Front : go with Node.Js &amp; React. - Use Cloudflare for DNS and then reverse-proxy to your real website. ** NOTE: use Trello for most cards while GH issues should barely be used .. and if so .. on a small scale for tech issues and even then, keep the issue small. Don't mix and match boards - keep em all in one main spot -&gt; trello. More tech stuff: - Use NancyFX. NancyFX &gt; Mvc &gt; WebApi - Use Azure SqlServer &amp; Azure Redis. If you're game, use RavenDb instead of SqlServer - Redgate's Sql Compare for DB migrations if using Sql Server. - Use Dapper not Entity Framework. - Use deployment slots with AppVeyor as part of your Continuous Integration. - xUnit - TortoiseGit (git &amp; git shell integration) &amp; Git Source Control Provider (VS extension for git integration) - making git easy to use on Windows. Enjoy .NET - it's starting to get fun, now :) /me waves @ .NET Core, etc :) TL;DR; - avoid TFS (run! run! run away fast and hard!), Jetbrains (except R#/dottrace/dotmem..) and Atlassian. EDIT 1: added more stuff and updated TL;DR; EDIT 2: Added Cloudflare
If you have two assertions then you necessarily have two reasons why it can fail. And we're not even talking about hidden assumptions such as the constructor actually working so that you actually have an object to test against.
I think his question is: "Where is the fucking manual?"
What's wrong with NUnit?
Posting your own articles for your point does not really count.
Has anyone also tried http://www.awesomium.com/ That's what I've used in the past and wonder if this is more reliable/faster.
It is my argument. I wrote it because I got tired of explaining to people why "one assertion per test" is such a stupid idea. 
You don't say what sort of business/app you will be building? Will it be an internal business application or a SAAS? I believe that this has a large impact on your choice. Also I would suggest, unless SEO is important you use an API first architecture + SPA/APP, and away from server generated html ui. With that there really is no need for back-end to constrict your choice on the front end. In the realm of line of business apps, it's hard to beat the MS stack because of long term compatibility, which is a huge benefit. I would choose dotnetcore/linux/c#/asp.netmvc/EF/postgresql and use git for source control and vscode for the editor. Probably Angular 2.0 + typescript on the front end html + Xamarin for mobile. Dotnet core really is looking to have a very bright future. For apps in the SAAS realm the benefits of the MS stack are not so clear. You should look into golang on the server side, perhaps even something on the JVM like clojure+clojurescript if you'd like a front to back solution that is really solid. Front end choices would be pretty much the same, Angular 2.0, React, etc... Database choice again depends on what sort of app you're building. Postgres would be the smart choice for sql based needs, but maybe your project is perfect for DynamoDB, or something like Datomic if time and history are important. So in other words, once you have some basic needs defined, choose the most appropiate DB/language/runtime, then add on all the other stuff from there, and build a team that can learn and adapt around that. 
You can develop with .NET and not be dependent on MS these days. I only use VS for legacy stuff and use Xamarin Studio daily - the new beta 6.0 version looks great and supports all the latest language features. There's also now good support for pretty much all the major editors you can to name and JetBrains have their own .NET IDE out soon. You don't need to use VS Online either, TeamCity will cover your CI needs (which is free for smaller projects). As others have said avoid TFS at all costs - even MS don't use it internally anymore.
Doesn't **count**? Of course posting a write-up of your argument *counts*.
&gt; Unit testing is hard, I know. Wait till you try a test that requires a database. Or worse, a web service that you don't control.
I'd choose C# over JavaScript by myself. But if you're familiar with HTML, CSS and JavaScript, why are you choosing for Xamarin instead of React Native/ Apache Cordova/ whatever other framework? Unless you see it as a challenge to learn some new stuff as well. And first of all, I'll learn the MVVM paradigm together with XAML. Writing a GUI is so much easier in Xamarin (imo in general as well) when you have an understanding of those. Can be overwhelming to learn at first though. 
It really depends on what kind of programming you're doing. Personally I've had to implement reference-counted disposable wrappers dozens of times, so I couldn't imagine working _without_ knowing IDisposable.
I'd say you're probably outside "most people" if you've had to do that dozens of times (in fact, more than about once...). I'm aware of IDisposable, but in my day to day work that means no more than knowing it lets me use things like COM objects, file handlers etc in a Using statement without having to mess around
No problem - I just updated the post with some information for the WPF version if you're interested.
Not my article, but incredibly detailed and thorough explanation of using AppDomains. It's a complicated topic and if you need them, this is a great resource to get up to speed with them.
I used it several years ago. It worked well then, better than CefSharp at the time. However looking at the web page it has hardly moved on in ages and looking at CefSharp the FAQ has details on how to do all the things which you didn't used to be able to do. So, without running anything, I'd say CefSharp all the way
The fact that you need to send around text messages just to have a clue as to what actually broke is the reason why multiple asserts per test is an awful idea. There's also the fact that a fairly minor change can require you to completely rewrite your gigantic test.
Nope, I understand exactly what you meant by that. There's two sides to your app, the "Frontend" (Which is what your post describes) and the "Backend" which is some kind of API handler. That makes complete sense to me now, it's how I *thought* it would be done.
As opposed to a "fairly minor change" needing to rewrite a dozen or more tests such as in /u/Jestar342 's example? And in what way are error messages explaining the reason for the assertion "awful"? 
That attitude is why 90% of its functionality is undocumented and only discovered by chance.
Guessing you comment the line opening a filestream?
Pull request!
So you have members of your team randomly upgrade packages all of the time for no reason? Sounds like a workflow problem. We have time set aside every sprint to upgrade 1 package, in one commit, with one meaningful commit message. So, if I create a branch, I upgrade a package and shit breaks, I know exactly what broke it, every time, in isolation, if I can upgrade it, I upgrade it across all my projects, no code is checked in without all unit tests passing and a code review. Everyone knows about my decision because it's written down in the sprint board, if anything odd happens with that package during QA we can see why, it's in the sprint board. We don't release broken code. It's either fixed or reverted (via a squashed feature branch). I don't know about your work situation but I don't have any time to manually trawl a csproj file which may contain out of date or inaccurate comments anyway about package versions when my workflow automatically accommodates for upgrading packages.
I would be leaving that company ASAP because at some point in the not so distant future your job opportunities will be severely limited.
Perhaps, but I'm more inclined to fork it so that I can remove many of their limitations. (e.g. MemberData must come from a static property of the same class, making reuse hard)
Again: Take randomly any other example. Someone worked and spent time with something, and that work is partially visible as the result.. And the rest? The thoughts? The reasoning? Gone, if not documented. And no one will remember a sprint board or something mentioned if it's one or two years later. But frankly, I think this conversation became pointless. I document my work, you don't. Fair enough.
Oh don't worry, I am working to change that :)
Ideally one project.json would be ideal. CS.PROJ has served me well but it's full of noise and another layer of syntax you have to learn. Other than it holding references to packages, I don't think I've ever needed to use any AfterBuild or BeforeBuild stuff or use comments in my csproj file as I've explained below, it's completely unnecessary to me. There could be reserved sections of the project.json to deal with the extra stuff if need be. 
So you genuinely document everything around a feature including reasoning and recall on that years later? Our sprint board is persisted on Trello. So, if I really want, the reasoning is all on there, archived away (despite the fact, a year on, even 6 months, the reasoning behind implementations are no longer relevant to the new feature). For example, I wrote an import service for XML files 6 months ago, last week I enhanced it to include CSV file formats, I know what it is, I know everything I did to get there because it's all on the card, needless to say, I don't even need to look at the card. My work is documented, just not in code. No offence but I'm guessing you're at least late 30s? Basing this on the immediate response of "This is pointless because you don't agree"
No one is gonna look at the Trello card in 3 years, when you have left the company already and someone else will expand something in the project. And no, I'm not in my late 30s. But I've worked with enough people who shared your attitude regarding this subject.
&gt;Wait till you try a test that requires a database. That's not a unit test, that's an integration test. Unit tests shouldn't cross boundaries or depend on external systems.
No shit. And that doesn't change the fact that unit tests are by far the easiest tests to write.
It depends on the code you are testing. If you're writing tests against legacy code that use a ton of static object/method calls that will be harder to test. Also, no need to be rude. We can have a discussion (or even a disagreement) and still be civil.
I find it very hard to be civil with people who act like unit tests are the only tests worth writing. Almost every time I hear "That's not a unit test" it is an excuse for not writing anything besides trivial unit tests. Also, just because you use a database doesn't mean its an "integration test". Actual integration tests involve working with black box servers from other teams, often other companies, that you have little or no direct access to beyond the public API. Real integration testing is really hard. By contrast, merely talking to a database that was custom built specifically for the application is easy. Not trivial, but if done correctly not hard either. If you don't have functional tests hitting the database within the first week or two of your project, you aren't doing TDD correctly. (And don't get me started on the idiots who think integration testing is when you test a Customer class with a CustomerAddress class instead of using a MockCustomerAddress.)
Integration testing isn't just hitting your database. That's just a normal, everyday functional test. Integration testing is when you need to tie in your application to your vendor's flaky payment gateway. Integration testing is when you start directly controlling the robotics on the factory floor instead of using the in-memory mocks. Integration testing takes coordination and planning between multiples teams and is by and large a difficult process. Having your application hit its own database is something you should be doing as soon as you have a database to hit.
&gt; "I did this because there is a file size limit in AWS - May 2014". Go online, checked the docs and lo and behold the file size limit has changed. How useful is that comment? If anything it's utterly destructive This information is neither useless nor is it destructive. It provided an explanation **why** something was done the way it is. And you knew that you could check the documents whether the file size changed. If the comment wouldn't have been there? Who knows why the code was there. You would have had to guess what's the reasoning behind that line of code. Technically you understand the code, of course, but why was it there in the first place? Read the comment, aha!, and you could check the documentation and see it's not the case anymore. &gt; This attitude of "noise = I'm contributing" is prelevant with older develops who are used to writing non self-documenting code hence my assertion that you're an older developer. That is not what I'm talking about. And I've seen plenty of this "self-documenting code", which in the end is just a lazy excuse to not document it properly. You're not documenting the code, you're - again - documenting the reasoning and the thoughts behind this code. Information that you can not express in code. "self-documenting code" is one of the most misused phrases of our time.
&gt; I don't think I've ever needed to use any AfterBuild or BeforeBuild stuff or use comments in my csproj file as I've explained below, it's completely unnecessary to me. It's always interesting that the need to document is unnecessary for those who never did anything more complex.
I know why the code is there because I can read code. I can read that it's calling AWS with a certain file size because that's what it's doing. No one needs to know about the rational for every single line of codes existence, that's made obvious by the context it exists in I.e this code in an import service exists to execute upon import. When reading it, knowing that at some point AWS had a file size limit means nothing to me when modifying it. I still have to look up at the current docs and check that's still the case, then go back and edit that comment if it's not true. This is the case with almost every comment encountered. Not commenting isn't lazy, it's just arranging your code in a format that barely needs further explanation. I was also being generous with that example, most people don't date their comments nor write comments anywhere as good as that. The only time I've written a comment for why I've done something is because of a framework or library constraint or because of a customer specific requirement. Anyway this isn't really about code comments, it's about comments in the Cs.proj file and from this I can only assume you justify via a detailed rational your inclusion of every external library. So perhaps you were right, I simply cannot debate this with someone who does that.
Wow, again great features! This thing feels like it's built by 10 fulltime developers... (as in the past) i have some feature requests (first world problems): I am an Android Developer and i use TailBlazer to show ADB logcat; Unfortunately after each log line it inserts an empty line (probably because of linux line endings). I don't want to look at empty lines that cut my screen real estate in half... I cloned your code dug into the code and build in an empty line filter (in a very messy way) and use that since v0.7 I am too busy to really flesh this out into something i could contribute... also seeing your style i guess you would approach an empty line filter very differently. So the feature request would be: -&gt; Empty Line Filter (maybe a button somewhere) Another annoyance is Window Positioning i have an cmd script that starts the adb logcat monitoring, pipes that into a file and then opens TailBlazer to show the log. Unfortunately each time the window shows up at a default position and i have to move it so it is half maximized on my second screen... this somehow makes the start of a new debugging session a little bit less enjoyable. &amp;nbsp; thx for this great tool
Thanks for the positive response. Can you raise an ignore blank lines option as an issue on github? Also you will be pleased to know v0.8 preserves the windows position.
ok done. https://github.com/RolandPheasant/TailBlazer/issues/115 good to know the window now sticks to it's place
Why is this tied to Xamarin at all? Would be nice to have a single-file object database tech that just works with a file. The only Xamarin hook should just be swapping in a different file system.
Look at OpenTK, SharpDX or SlimDX. They all have a control that can be embedded in either WPF or WInforms that gives you a 3D surface.
What's it do? It seems like a manual way of adding one text file to another? I'm a bit of a noob.
Looks pretty cool.
I document my stuff. You don't. You don't understand my rationale. Fine enough, discussion ended.
WPF also has built in controls for that, like Visual3d. But indeed, WPF is the way to go.
It's a file tail utility. An alternative to BareTail
Is there any reason why you didn't update straight to RC2? RC2 has breaking changes from RC1.
Wait didn't the roadmap posted a few days ago say rc2 was coming soon? That put me under the impression it wasn't out yet.
It's available in the form of "nightly" builds. Getting it to work with Visual Studio Code is a bit tricky, and to work with the full Visual Studio is probably not possible at all.
That is what I am having problems with integration test. In unit test, you have an idea of what you are expecting and that is not the case in integration tests. In your integration test example would that mean that I would need to assert each scenarios in a single test or divide the test to each different scenarios? Actually, if I look at it in a BDD way, integration test makes sense. TDD and IT is kinda shaky.
Check out Sony's and Naughty Dog's GitHub repos, they have stuff like this which is kinda like Unitys Editor: https://github.com/SonyWWS/LevelEditor/wiki
Its not out yet, should drop on Monday. Nightlies are always available from the myget feed or github directly.
Thanks for the great, level-headed comments YuleTideCamel! I agree as well integration tests are code level tests. To take it a bit further I think as soon as you cross any boundaries (network, IO, etc) it qualifies as an integration test. Integration tests don't have to test the full stack. They can test a single integration or multiple. Great discussion!
No arguments here, /u/CaRDiak has it pretty spot on. Integration testing is making sure everything fits together like you think it does. To remove the programming from the analogy, think about a piece of Ikea furniture. Unit testing is checking every single part is the right shape and quantity. Integration testing is making sure those parts go together and make a whatever you bought. It is completely possible to have 100% coverage with unit tests and still have a product that does not pass validation and verification of requirements (ie, it doesnt work).
This was actually a couple weeks ago I did this and I didn't quite feel comfortable with upgrading to RC2 yet. Its not even on Nuget yet; I'd have to pull from thier MyGet. From what I've seen though RC1 &gt; RC2 isn't too big of an effort, mainly namespace changes and some fairly straightforward API changes. I'll probably upgrade to RC2 as soon as it's "released".
Exactly :). I thought RC1 is a good balance between bleeding edge and still some stable enough tooling to actually still be productive. I'll be happy to move to RC2 once it's"released".
&gt; The decision to make such breaking changes might be a bit painful for large entrenched projects, but it's definitely the right move to make. For hobby and non-business critical apps. If what you're working on is important it's much better to wait for the release. Maybe even a few releases.
Just watched this video on EF core. https://www.youtube.com/watch?v=x3cymeg9Lpo Seems like it will work well. 
As for Excel support you are right, there are not many libraries ported yet, so we decided to build a Java service with Apache POI just to generate our Excel data from our database. This far from optimal, but is suitable for our requirements for now.
Thanks, I'll give that a shot when I get home from work today. Sorry for hijacking your thread!
Thank you for sharing!
Integration tests can be thought about at the test case level. If a tester was testing your system they would start with a use case, add some inputs and check for some expected outputs. This would be one test case. One test case might end up calling many different methods across many classes. The integration test would test at that level. This applies for public API's. If you provide a library with some public methods, a user of that API would expect certain outputs from those methods based on inputs. Each public method might call many private or internal methods and classes. An integration test would test the public API. If you can build integration tests for all your test cases, you would essentially automate all your manual testing. This might not be strictly true if you can't automate anything above the domain layer. There might still be some manual testing to make sure the UI is acting as expected.
I am using Asp.Net Core RC1 with DynamoDB and Ubuntu in dev right now. Its been kind of a pain at times but most of that is surrounding our build process and deployment process. Its not very difficult to get it running, but with all of us doing our dev in VS on windows, commiting to a git repo on an ubuntu box running open project, building on an ubuntu box running jenkins, and then deploying to an ubuntu box running ubuntu, things can get kind of out of hand. I would suggest that if you have the extra time or patience, at least in the beginning, find a way to self host the app and just run it in mono. DNX is proving to be a pain in the dick to get running in the background because of how much it does in user account its installed under. You can definitely tell that these are windows folk working in a linux environment, Some of the design decisions are silly. If you have any specific questions about my experiences so far I'd love to share. 
I would be wary of using ASP.NET Core in production just yet. It's still not released, at least wait until then before you start using it.
A coworker was testing out some of our code in .net core and managed to get Word support using [Aspose](http://www.aspose.com/.net/word-component.aspx), but I'm unsure if he used .net core or mono for the runtime. Either way it worked. If Word works Excel might too.
Man, ServiceStack looks awesome but boy is it expensive. Still hoping my current employer goes that route though. :-)
You're freaking crazy to be using this in production. RC2 shatters the world, nothing works together anymore. I can't imagine how much breaks after RC2 to RC3 or whatever they call it. 
RC2 next week. RTM by end of June.
As stated [here](https://blogs.msdn.microsoft.com/dotnet/2016/05/06/net-core-rc2-improvements-schedule-and-roadmap/) RC2 is (almost) go-live release, so I believe there won't be critical breaking changes in the APIs. We only expect minor changes in CLI infrastructure, configuration and tooling. And also, we will ship in 18 months so .NET Core will be definitely RTM by that time. Our only concern is EF 7 (EF Core) Posgresql support.
Yes RC2 breaks all existing tooling. To label what's next as RTW is a fucking embarrassment. You don't go live after changes so significant everything is broken. This isn't even preview quality, it's a joke. And cruel joke at that, one that's having been played on us for years 
Yea, it's a trade-off and probably up to personal preference, but I prefer stable tests. I view a test failure as a product bug and that attributing test failures to flakiness is a slippery slope into distrusting tests overall. I've anecdotally seen people make a change, see a test fail which specifically tests what they changed, but they were so used to thinking "well tests just fail sometimes" that they checked in the change anyway. Also, generally breaks between different services happen out of band. As long as you're testing thoroughly, in theory any breaks should be with a dependency in your dependency, not with your app, so the non-mocked tests would seem to just fail randomly. Of course, that's some strong assumption on your app having really good testing against the contract you're mocking, and practically speaking bugs will slip in from time to time, but if you have good live site monitoring and alerting, you should catch those quickly. Anyway, you'll probably need to find the right balance for you. I've seen it done both ways and from personal experience I feel like mocking everything works best, but I'm sure that point is debatable, especially for projects of varying scale and complexity :).
And they have just announced the other day, they are throwing out the planned project-file format... So the tooling is going to change when we get to RTM
I still use NHibernate because of the reasons listed here http://enterprisecraftsmanship.com/2014/11/29/entity-framework-6-7-vs-nhibernate-4-ddd-perspective/
They have been changing the tooling and the build process, yes. But that doesn't mean it's not fit to run apps in production. It's been rock solid in runtime. If you are a small team with people who are enthusiastic about learning how the new stuff works as soon as possible, the quirks in the tooling isn't a big deal.
[removed]
It’s ASP.NET 4.5.2 or 4.6.1 with MVC 5. There was never an MVC 4.5.
Like I said, I like what I see, and I don't even disagree that there is a place for commercial frameworks (as you point out, Telerik, etc. exists and people DO buy those). But frankly, as a developer, I am always going to look at free, loose licensed solutions over proprietary ones. Don't get me wrong, they've made something really interesting here, and they deserve to get paid for their work, but because I will never get to try it out without paying money, I am equally as unlikely to recommend it to an employer because I have no experience with it. Kind of a chicken and egg problem there. That's why I think some of those other commercial frameworks have unhindered free versions for OSS and non-profit work, to encourage organic word of mouth by developers who use the free version and love it. It's cool, I just probably won't ever get to use it. :-)
Orchard
Considering this is dotnet subreddit he is probably looking for something else than php. 
Does this really need to be a mobile app? A responsive web app would make more sense than mobile unless the users are disconnected or they're hoping to sell in the app markets. Using Xamarin, I would still have the content in html and display it using the browser control. That would be more flexible and provide easier formatting options than trying to format the content using XAML controls. You could create a web app for maintenance and use something like TinyMCE to give them a WYSIWG editor. The menuing, navigation and quizzes you could code using XAML.
I would not use something critical like that as a service, especially not if it's closed source.
I'm trying to split the difference with [Chain](https://github.com/docevaad/chain). While I hate Table=Class style ORMs like EF, I do see the advantage of not writing trivial SQL for insert/update operations.
I'm a fan of LLBLGen, you can have a pretty good influence on the queries that are generated and it's performance is good. I can see why not many people are using it, probably because of its pricing and lack of publicity.
N2 CMS https://github.com/n2cms/n2cms
Interesting idea.
I worked with LLBLGen in two places and it worked fine. It used to use these very long Win32 style calls where you would add like 5 nulls as arguments out of 8 but later I only worked in a visual tool that mapped to everything. The visual tool was very busy but you could ignore 80% of it. I always had the feeling everything just worked, also mapping to stored procedures and stuff like that. EF is slow as molasses and has serious problems mapping to stored procedures. I find myself mapping hand written queries more often simply because EF poops itself when things get difficult. But that has problems with nested objects. I don't remember having these issues ever with LLBLGen, but the steep pricing (as opposed to *free* like EF and Dapper) is what keeps me from using it.
Never heard of it
Code generation driven by something outside source control (e.g. WSDL / RAML / localisations from Google Translate) is, as far as I'm concerned, something that should _only_ be performed interactively. Otherwise I'd worry about my build process not being repeatable. GUIs tend to lead people away from trying to automate things during their build process so maybe that's a good option.
There's also raygun.io, New Relic, Application Insights / HockeyApp, and even Rollbar (depending on your platform / requirements).
&gt; Otherwise I'd worry about my build process not being repeatable. Yes, that's the situation that I must avoid.
+1 for Umbraco. We've been using it for a few years now. 
It's funny you bring up his coding style. I really want to contribute to this project, but his style and naming conventions are super unconventional. It makes it hard to try and contribute.
Have you tried changing the return type "ActionResult" to "FileStreamResult" and your return to "return new FileStreamResult(stream, contentType)" as shown below to see if that works? public FileStreamResult ViewStream(int id) { // Your validation logic here var source = string.Empty; // create url from id here... var req = HttpWebRequest.Create(source); var resp = req.GetResponse(); return new FileStreamResult(resp.GetResponseStream(), resp.ContentType); } Otherwise, more details about the error would be useful, as well as the type of stream your IP camera returns (gzipped or not? etc).
The UML diagrams are just for viewing. They don't have any effect on the classes/database to my knowledge. The edmx is what you are looking for as far as "use a diagram to manage database tables and relationships". But its was POS and got killed off by Code First development. 
I use SQL Server Data Tools for my database modeling. It gives me precise control over my database objects, and it compiles to DACPAKs, which are really easy to use in continuous deployment projects. My UI devs write their models in RAML, which is turned into C# objects using a code generator. (Custom for now, but I hope to offer an open source alternative.) What's really cool is that my ORM is based on runtime mapping. So I just expose views with all the columns they may want. Then the SQL generator automatically selects the subset that matches the properties the UI team actually asks for in their RAML. It beats the hell out of automapping database entities to view-models. 
I've looked at Dapper. Because everything with dapper is string based isn't it tough to tell if a change might break existing queries? I work on enterprise apps, and really want to minimize the amount of code creation and database scripting required. When breaking changes are required I would like compilation to pick up any inconsistencies. For this I've had no issues using a model first approach using LLBL. You might say great keep using it! Well no one else does, and I'm starting to look for a new job. Every position out there mentions entity framework. I thought I'd give it a go and it immediately it feels like a step backwards.
&gt; Because everything with dapper is string based isn't it tough to tell if a change might break existing queries? So, I understand what you mean but, I want to back up a bit ... so what EF and I assume LLBL give you is a drop-in repository for you to use. My use of Dapper has been to replace the EF context with my own repository. I generally do not use anonymous types with Dapper - I'm mapping to strongly typed classes so that I do get the kind of compile-time "what explodes when I change this?" kind of checks. As for breaking things unintentionally ... well that's what my collection of regression tests is for, because I will break obvious stuff over and over without them. To be clear, using a micro-ORM is a ton more work up front. Chain and PetaPOCO contain functionality to help with some of the boilerplate for the actual querying but you'll need to create your own repository-type thing. One thing I do seem to disagree with /u/grauenwolf on is EF migrations. I actually really like EF's code-first migrations, I'm consistently surprised that it's able to do as much as it does. I've had to get in there and tweak stuff, and long-running updates are a real problem when updating from Visual Studio, but the benefits have caused me to keep EF around. Although, I'm now looking into SQL Server Data Tools based on /u/grauenwolf's other comment.
We've been using a custom ORM for the past 3 or 4 years. Recently we took the lessons from it, rewrote the code from scratch, and released it as an open source project: https://github.com/docevaad/chain It fits between Dapper and EF on the speed/power curve. It has SQL generation for basic scenarios, but for anything complicated you are supposed to use views and/or procs rather than LINQ. Our theory is that LINQ cannot represent more than a fraction of SQL's power, so there's no reason to bother trying. But that also means we aren't bound by one-to-one mappings between classes and tables. This means we can do interesting things like update multiple tables with one class. 
&gt; Chain and PetaPOCO contain functionality to help with some of the boilerplate for the actual querying but you'll need to create your own repository-type thing. I know that's a problem and I'm working on a solution. But before I share what I have, what features do you think a repository needs to offer?
&gt; RAML is new to me I'll have to add it to my growing list of technologies to learn. RAML is very immature, especially the .NET tooling, and there's no guarantee that it will win, but we desperately need something like it. 
Just in time for the project structure to go back to what it was.
&gt; While I hate Table=Class style ORMs like EF Hmm.. I haven't tried chain but I do like table-mapping. Really, it's the one thing about ORMs I really do like. Most of the rest of the features in ORM just get in the way though if want to use real SQL. I've been on exactly one project where we used an ORM to have a retargetable backend between SQL Server &amp; Oracle. The abstractions were valuable there and we gladly suffered with the idiosyncrasies of the ORM in order to simplify the code base, and we STILL had to have dual code bases for situations where complex sprocs were written for the sake of performance. Beyond that, it's just a pain. Also, are there any ORMs that don't support table mapping? It's kind of a core feature isn't it? I mean, you can use table without without having true ORM, but ORM itself really isn't possible unless it first supports table mapping. 
Let's say you have a class called CustomerProfile. It gets some of its data from the User table and some from the Customer table. This is how you do an update in Chain: ds.Update("User", profile).Execute(); ds.Update("Customer", profile).Execute(); In EF you have to map the `profile` object back into separate `User` and `Customer` objects. Then you need to open a context. Then you need to attach said objects to the context. And then finally you can actually perform the save. (Or more likely, you just give up and use a fat object graph with more data than you actually care about.) *** For another use case, lets say that you just want a customer's name and email address. In EF that would again require manually mapping between Customer and our new CustomerEmail class. In Chain you just take this code: var result = ds.GetByKey("Customer", customerKey).ToObject&lt;Customer&gt;().Execute(); and change it into this code: var result = ds.GetByKey("Customer", customerKey).ToObject&lt;CustomerAddress&gt;().Execute(); That's it. Just by changing what return type you want, it will automatically update the SQL being generated. Heck, lets say you literally just wanted the email address. Easy enough: var result = ds.GetByKey("Customer", customerKey).ToString("EmailAddress").Execute(); *** This is what I mean by not having table==class. As long as they have overlapping columns/properties, any class can be used to operate on any table. 
&gt; The only thing that's occurred to me so far is "injectable!" but what am I injecting....? In my mind you shouldn't inject something into the repository, but rather just mock out the repository itself. &gt; I'm very curious to see what you're working on - I've been very impressed with what I've seen of Chain so far but I haven't had a chance to actually use it yet. Thank you. And if you do get a chance to play with it, by all means let us know if there is something that you don't like.
&gt; (On the other hand, my repository approach might not look generic enough for some people - I'm not concerned with abstracting anything about the db server away, we're all-in with SQL Server.) My current thought is that the repository shouldn't be a thing, but rather a pattern. Perhaps a C# class template than you drop into place and then fine-tune on a table by table basis. But I'm not settled on that idea.
That's what makes Chain unique. At runtime it will ask the database what columns are on those tables and then generates the mappings accordingly. You can override this somewhat. We honor .NET's `Column` attribute and we have our own `IgnoreOnInsert`/`IgnoreOnUpdate` attributes to prevent people from accidentally stomping on the CreatedByKey column. 
Personally I prefer code first for EF, but the EDMX model is based on the behaviour of DBML from Linq2SQL which is why it works the way it does. It can be customized, fairly easily but it's not really intended to be. Database first is basically designed for the group coming from the MIS side of things that do their work starting in the DB. There are tools and VS extensions to build code first models from an existing database which will give you the control you want. Code first is harder, but much more powerful.
Thanks for commenting. Do have concrete experience doing this? If you don't mind me asking? Thanks
Don't mind at all! Yes, I have concrete experience doing this - I work for a company that creates custom software for businesses based on their needs. We've created many cross-platform mobile apps using Xamarin for our clients and one for our own internal use.
A generator inside a nuget package is a good compromise here. Not technically under source control, but still repeatable.
&gt; So you are using a code first approach then? If that's the case maybe I can see how this would be more manageable. I'm still not clear how scripts are generated based on changes to code. How would it know that a column is renamed? Yeah we only sort-of do code first. Our entity model classes get mapped to database columns using the fluent syntax, this is only necessary when the column name on the db table differs from the property in the class (which we usually dont do, but is sometimes necessary). When a new table is created or a new column added to an existing table or really any change to the schema we handle that separately with visual studio. Most people dont use it but there is a SQL Server Database Project in visual studio. You create a create table script for each table and you can do a schema compare against a database. This schema compare will generate a script to perform the migration. You can either output the script to a text file or execute it right from within VS. When developing a new feature we will make the change in the Database project and do the schema compare against our local db and run the update. Then we will make the change in code. It has served us well, the only downside is we have to manually update every customer database which can take about 4-5 hours with over 400 databases. But really thats not too bad, and if we go to a more batched release cycle it will be easier.
Sorry I'm still a bit fuzzy on the details. It is because whenever I search on the net almost all suggests (at least on my understanding) that in an integration test, you need to test it how it will work as whole. But thinking about it some more I kinda see what you are saying. That I should mock the IO still. I guess what I have been reading up is actually saying not to mock the dependent objects not IO.
EDMX has nice tooling for small databases, but it is not the recommended path anymore. The recommended path is generating POCO ("code-first") classes from a database, then adopting the code-first story from there, or continuously reverse-engineering when needed (like LLBLGen Pro). Try generating code-first classes with this: https://visualstudiogallery.msdn.microsoft.com/ee4fcff9-0c4c-4179-afd9-7a2fb90f5838/view/Reviews/2
I usually default to explicitly associating my properties with column names. Write a couple of extension methods and knock out those entity configs in a couple of hours. IsNonUnicodeFixedLengthCharWithLengthOf(12);
Yeah as the other guy said, there is no "free" version. Xamarin is now all free and open sourced.
&gt; Most people dont use it but there is a SQL Server Database Project in visual studio. You create a create table script for each table and you can do a schema compare against a database. This schema compare will generate a script to perform the migration. You can either output the script to a text file or execute it right from within VS. To be a bit more precise, a SSDT project compiles into a DACPAC file. That file is then used to create and optionally run the migration script. This means you don't need direct access to the target database. You can just compile your DACPAC (preferably using a build server) and hand off the result to someone for deployment on any number of unknown servers. &gt; It has served us well, the only downside is we have to manually update every customer database which can take about 4-5 hours with over 400 databases. In theory you can just run a batch file that calls SqlPackage in a loop, once per target database. Again, this uses the DACPAC as an input file.
I think the best way of explaining it isn't about where you're going to go first to solve problems. If you're going to go build a view our stored procs and views as your primary tool, go database first, if you're going to try to solve problems in code first, go code first. If your DB is shit b and you can't change it, go code first.
Interesting, I agree DNN is more user friendly than Umbraco for getting started but I ditched it for Wordpress (see my comment below). I really didn't get on with DNN at all (although this was a few years ago).
My biggest gripe with WordPress is it's extensions and compatibility. We just found that paying 30 or 60 dollars for a module meant that you got support, and that they tended to work better with newer versions of the cms. That, and there still isn't a storefront I like for WordPress. For a couple hundred bucks you can get a pretty awesome storefront for dnn that we ran a lot of storefronts on. Some pretty decently sized ones too. But for most websites any cms will more than likely be plenty. Another reason was we tended to have less support calls from non technical end users when they were on dnn as opposed to WordPress. It just seems to be more intuitive to the layman. That's a big deal when you're managing a ton of sites. Although I'll admit the newer versions of WordPress are a lot more user friendly. But dnn has had edit buttons right on the page or over the content for like a decade
Thanks for the clarification, I only just started using SSDT projects at me new job and don't really know much about them. &gt; In theory you can just run a batch file that calls SqlPackage in a loop, once per target database. Again, this uses the DACPAC as an input file. I'm pretty sure this is exactly what my boss does when he does a deployment, I've yet to have to do a DB deploy just hotfixes for bugs in code. 
Good idea but that would require to much to rewrite. Now every endpoint is independent from each other. A few uses jquery only and some angular.
&gt; **hopeful**
Why the twitter link and not just link straight to the blog?
Pardon the ignorance but why not? 
"once" When RC2 comes out I will have production code running on this stack. The title is really negative.
Create a generic controller and [take over the ControllerFactory](http://www.quickmeme.com/meme/3qkrts) to instantiate a typed controller instance based on the route info. 
It can very easily violate separation of concerns. Also, how you secure an API is different to how you secure a web page, meaning you're risking problems. You'll just end up with twice as much code in your method, so you may as well separate it out to where it belongs.
I'm a pretty big fan of [Stackify](http://stackify.com/)
You're right. The best way would probably to move the web api methods to the mvc controller. What do you think?
Controllers are just thin wrappers anyways. If they do anything more than attach the current user and pass along the request to a service library for testing than I'm really suspicious. 
Can you clarify for me please - how would the models, and controllers/routing work if you completely took out MVC? I don't intend to use spa just yet but interested in the concept of using a full html5,CDs,JS UI. 
Is your WebAPI used by anything other then your MVC controllers? If not my recommendation would be to get rid of the WebApi project and keep the MVC project. 
I absolutely LOVE dapper. Directly improved my quality of life. I've never looked back at entity framework.
The routing and controllers would be moved to javascript. Web API would just serve you JSON data for GETs and accept POST / PUT / DELETE for changes.
We thought to do the same thing as you! Even more, we wanted tighter OWIN integration so that we can use `app.Map('/subpathfoo', ...)` and have our "api" serve up html from inside subpathfoo, all the while keeping its own routes clean from having to know this. In the end to accomplish this we used [Nancy](http://www.nuget.org/packages/Nancy) with the [Razor view engine](https://www.nuget.org/packages/Nancy.Viewengines.Razor/), a great combination IMO.
The wording used in the post title and needless twitter redirection sums up the quality of the what you'll be reading if you follow this link.
Is it possible to have a JavaScript based front-end framework like this without abstracting incorrectly? We are moving into a period of terrible design (code-design, not graphic design) We have slowly moved from secure, well structured, manageable, maintainable, searchable, strongly typed, server-side applications .... to a mess of JavaScript framework based applications with everything except the DAL sitting in the browser. No matter how well you try to minimise it, the more code you move into the front-end, the more code duplication you have got (unless you build it so that the api assumes that all input and output is well structured, validated, and secure of course and pass it almost straight to the Db....but nobody would be that stupid, right? .... right?)
Get rid of WebApi and only use MVC. There is not a single thing you need WebApi for that MVC cannot do easily. The converse, WebApi totally fails when it comes to views. 
So you would create a webapi project instead of an empty MVC web application? Does EF, Identity etc just work as normal and would you use HTML,CSS,JS in the wwwroot folder? Yes and yes.
Please provide more context and some code examples please.
Thanks. I will look up load balancer.
Sorry but that is incorrect. Google still has issues crawling angular sites and I don't think any others do. That is one of the advantages of react. There are sites dedicated to helping with seo for angular
You can also do this in a WinForms app where you get presented with the login first then have some automation take over. 
Yep go with "code first" even with existing databases. The EF migrations will help you keep the database updated
Interesting where did they say that?
If I didn't find EF to be so frustratingly bad, I wouldn't have started down this road. 
Let's play a game. Consider the question in this post: https://www.reddit.com/r/csharp/comments/4j8z22/entity_framework_dtos_update_all_associated/ My complete answer including transaction management takes 5 lines of code. Show you EF alternative.
I have no idea. Lol. Though I have been programming for at least 2 years now I still have a lot to learn. Maybe I would try asking him when when work starts again. My boss is a seasoned developer himself but now mostly focuses on the business side so I don't think he himself is not up to date with the current technologies.
Just going to point out the obvious here. But you have the JSon ignore attribute on the child property... I hate to sound snarky, but beers... What do you think ignore means :) 
If you have files namely MP3, css, or js files that you need to provide sans login you can add a separate section to your web config that allows anonymous access to those files/paths Typically you would create a public folder that holds these assets and allow access to a single point. 
Also, are users set by default when they login using the cookie? And should I create a static telemetric context to use for my entire application or create a new instance in each class?
Do you have an example of doing this with routing? I'd be very curious to see it...
Not really... Your tied to the dnx and dot core can't do services so it's a gimmick c land useless 
Yeah, I realized that was the problem... Unfortunately, if I remove it, it acts as if I am POSTing a brand new child along with the Location part, which causes everything to fail. I think in this case the acceptable solution is to pass childId as the URL parameter, or have it in the headers. 
What do you mean by that?
If you call db(your context).Children.Attach(child) it'll be added to the existing graph... Aka an update not an insert... 
Looks pretty nice. Been giving it a try and I'm stuck with something I hope you can help with. I store my globals settings in a Singleton class that implement INotifyPropertyChanged. That way I can bind the XAML directly to this one class system wide. It seems to work ok. I can't however get Jot to work with this setup. I can only get it working with Windows that have a Close event to do the persist. I can provide a code sample if it helps.
I'm pretty sure you have to login via the WebClient class and do some sort of POST(maybe UploadString) in order to get authenticated into the site. Once signed in properly you can use the WebClient to download files. Use Fiddler as well if you have trouble getting through the login.
You should be POSTing data in the body of the request, not headers or query string.
Man, from your question that screen shot means nothing. Terrible question asking aside; you'd want to make another table called something like Emails, then have a One to Many relationship between the email and user account table. There is heaps of resources on how to do this, try hitting google for some 
Not that the OP is very clear at all about the requirements but he says mutiple, which indicates to me that there could be more than one or two, therefore this would not suffice
I feel like this is completely by passing the entire MVC framework to begin with my suggestion is that you make pages, or page objects, in the database which can be called by passing a URL parameter in a controller. otherwise you mine as well rebuild your own wrapper around the HTTP stack altogether to do what you're asking. for example... {controller}/{pageId or pageName} then based on the URL parameter you would serve up the page that's essentially how every single CMS (content management system) does it. sorry for the formating, on mobile
it all depends on the type of authentication. OAuth, OAuth2,, Basic, all of these are implemented differently. first I would ask whom-or look up in their wiki,API docs-what authorization they are using. then you can work from there.
What I mean is that if you're not populating the pages with a model server-side, on the client you can just link to an HTML page. While I write this I see a possible inconvenience: the view is wrapped around a layout that actually needs server-side data somewhere. In this case, of course, you can't do anything about it. EDIT: WRT routes: you could set routes client-side, e.g. with Angular :)
You can add a new ado.net data model and you can select code first from database and that'll automatically generate the POCO classes for you. Edit: Tutorial Here: https://msdn.microsoft.com/en-us/library/jj200620.aspx
I think this is what you might be looking for, [Quick Tip: EF Core 1.0 Reverse Engineer POCO Generator](http://www.danylkoweb.com/Blog/quick-tip-ef-core-10-reverse-engineer-poco-generator-ET)
Create an ActionFilter that validates the model: http://www.asp.net/web-api/overview/formats-and-model-binding/model-validation-in-aspnet-web-api
Thank you very much. This is something I considered as well - I just thought that there may be a simpler solution that I am missing (without adding DTOs). This is probably the approach I will take. Question, do you usually put the DTO code in the class containing the Model, or do you create a separate file? Or you simply put it in the controller? Is there a "best practice"? Also, just for my information, with the DTO I'd have to do something like this: Location.Latitude = LocationDto.Latitude; Location.Longitude = LocationDto.Longitude; Correct?
&gt; team's absolute fear of EF. This is an indication they do not understand the fundamental concepts, and even do not understand how to profile an application. Even if you totally screw up, just trace it on the database and see what it's sending, then fix it until it's doing what you wanted it to. &gt; and inline SQL functions Oh boy. Someone needs to mention that if so much as a column should be renamed or a datatype changed on the server, you're going to have to release an entire new build. Or, create a stored procedure, change stuff around all you want, and don't release anything. Magic, new stuff that just "happens" and the user doesn't download anything. I like the second option in that case.
&gt; No. EF is notoriously slow. Is it? I've been using it for years, in all sorts of situations. In any case, you can get very readable code that is also performant. If it's slower, it depends what "slower" means. If we're talking about 15 ms slower than going through the raw ADO.Net route, in a situation that's run once a minute, that's not "slower" in a real sense. I've seen some EF cases where the structure of the code itself caused the underlying framework to do the wrong thing, but that is what profiling is for. It immediately becomes clear what needs to be looked at.
+1 on this. This is exactly what we do at work.
&gt; Luckily this is a SaaS web platform, and our databases are pretty much finalized (at least the legacy ones), so there's not a lot of changes to the overall schema, but updating the DAL feels pretty messy with all that SQL scattered throughout. Ok, valid enough. My head was still in the days when I had to deal with this on the desktop. It's similar with the web though, in that if you have hard-coded stuff, it's going to require compilation (with .Net obviously). And depending on how complex your project is, that means passing tests, environment deployments, all the other associated fun stuff. You should still be doing that if the database schema is changing, but in the best-case scenario it's set up so no tests will fail as long as whatever you did on the DB is all working. Just be careful with the mind-state that "not a lot changes to the overall schema". Until the day it does, and then everyone wonders how they didn't see it coming!
I understand completely, and that's one of my biggest fears with using straight SQL. At least with EF, you have a centralized set of models that can be easily corrected. With SQL, you're setting yourself up for failure in potentially hundreds, if not thousands, of different areas.
If they don't want to use EF, look at Dapper. I love how EF, LINQ and IQueryable work, but Dapper and EF can work side by side for different use cases. I'd suggest using Dapper instead of ADO.NET directly.
I rarely see slow queries out of EF -- when that happens, I'm probably writing something incorrectly.
&gt; create a stored procedure, change stuff around all you want, and don't release anything Most of the time I go down that route, requirement changes include changes in the sproc parameters, so I'm releasing a new build anyways.
I'm already using .NET Core in Windows Store Apps for a time. Maybe .NET Core isn't a substitute for classic heavyweight .NET, but it isn't supposed to be so. .NET Core from a start was intended to be modular and lightweight - which is very important for mobile development. Using of Silverlight on Windows Phone was a huge mistake. Silverlight lacks even most basic things like ASCII encoding support, most standard cryptographic functions and was in many places incompatible with classic .NET at source code level. Using of .NET Core on mobile now is much better and this framework is compatible with classic .NET in most parts so fewer code should be rewritten and most open source libraries can support .NET core easily. I don't think ASP.NET move to .NET Core is a top priority as most enterprise developers would use classic .NET as it support enterprise features out of the box. .NET Core is much more open source world centered, but hardline Microsoft-world enterprise developers (like me) don't want to move here unless it's a manager choice. Classic .NET and classic Microsoft infrastructure are mature technologies to which too many effort and money was invested at enterprise. I think .NET Core isn't much of evolution but an attempt to conquer more market share by invading into enemy's territory. On the other hand, mobile world is already an enemy territory dominated technologies like REST/JSON, OpenID, SQLite and so on. Microsoft attempted to create its own embedded database MS SQL Compact, but nobody care as SQLite is a default choice on mobile. REST/JSON looks ridiculous if compared to powerful WS/SOAP (which support transactions, message routing, message level security, automatic service discovery and so on), again nobody care at mobile world and prefer to use primitive and straightforward JSON and hype this 70-s years level approach as modern and cool. When I read opinion on technology from typical web/mobile developer I want to make face palm as they hype technologies which seems to be appropriate in early 80-s at best. But it's reality and Microsoft is forced to play by this rules on "enemy territory". At enterprise things is different, but lightweight web and mobile centered world is dominated by hipsters.
You can create base generic controller and implement base crud method in it. That way just a fact of creating a controller for particular type will get youself all set. one requirement for that will be to use generic interfaces on ObjectContext rather than DbContext for instance. We've implemented similar thing with EF v3. Another approach, which will save even more time, is to create OData endpoint. Thansfully it is much more stable thing now, than it was 4 years ago
Is this for an internal project or internet facing?
Yes. No custom domains and a few other restrictions.
Well that appears to be exactly what I need then. 
I think this is for a possible incoming project. So yeah.
Hey, sorry bout the late reply. Tried out the different return type, no go. Here is some more info on the output from the IP cameras. Looks like they are returning their content-type as: multipart/x-mixed-replace; boundary=--BoundaryString HTTP/1.0 200 OK Server: Motion/3.2.12 Connection: close Max-Age: 0 Expires: 0 Cache-Control: no-cache, private Pragma: no-cache Content-Type: multipart/x-mixed-replace; boundary=--BoundaryString
&gt; a giant string OWASP has a few things to say about that.
How many tables and how many fields are we talking about? *Note: Please see the edit at the bottom, if that works, this can all be bypassed* It might just be best to do it by hand. I generally resort to this if it's basic POCO stuff and doesn't involve EF for whatever reason. I mean ... it usually just boils down to how fast you can type: public int id { get; set; } public string FirstName { get; set;} public DateTime HireDate { get; set; } etc etc ... I was actually surprised I was unable to find a pure class generator for SQL that fits this scenario, as everything is "all about EF!". Have you looked at [SqlFu](https://github.com/sapiens/SqlFu) or [Mapster](https://github.com/eswann/Mapster)? Although if the team is already worried about a solid, robust, performant framework from Microsoft, then dealing further into the unknown may not even be an option. I don't have experience with those either, but they both support .Net Core and at a glance seemed to be able to create classes from the database, and more than just SQL Server. If I am missing part of the question I apologize, I think that's what you're getting at. I'm curious on what you choose to go with, as although I always think I've "seen it all" at this point, this case I just brute-force. If there's a better way, let me know. **Edit**: I looked further into those two suggestions, they do not seem to be able to create classes from the database. Of all things, I found this script and ran it against some tables to see what it comes up with. It's actually pretty decent if you are dealing with simple stuff! Heck it even will attach attributes like [Required] and [MaxLength]. May need to be slightly modified to do what you need, but ... here it is: https://gist.github.com/joey-qc/6710702 In a lot of cases, it generated code that was almost identical to me having typed them out manually, mostly because it's extremely straightforward on creating C# POCO classes from the associated tables that deal with common data types. I ran it against a demo table of vehicles ... it came up with this: public class Vehicles { [Required] public Int32 ID { get; set;} [Required] public String VehicleYear { get; set;} [Required] [MaxLength(50)] public String VehicleMake { get; set;} [Required] [MaxLength(50)] public String VehicleModel { get; set;} [Required] [MaxLength(50)] public String Color { get; set;} [MaxLength(20)] public String LicensePlateNumber { get; set;} [MaxLength(20)] public String VIN { get; set;} [Required] public Int32 VehicleNumber { get; set;} } Minor modifications would be needed in this case, mostly just renaming the class to "Vehicle" from "Vehicles" to keep it in line with coding guidelines, and possibly changing the types from the CLR types to the C# associated keywords (i.e. Int32 -&gt; int, String - &gt; string, etc). Obviously, modify the script there and not the result. Fire away.
Why don't you just host it yourself on IIS ? Use a dynamic DNS updater and just point a prospective client to your own web page. This has worked for me for years. I love that I have absolute control over what comes in and goes out. And I can laugh at the scriptkiddies that try to hack my site with PHP tools and Wordpress tools. (which happens a lot more often than you think) Don't be worried about your internet connection either. I live on a farm and use a 3G connection (HSDPA) which is plenty fast enough for a ASP MVC site, I know because I wrote an MVC site for a German company and they tested it from my site and they were very happy with the load times. (you need to get all your ports unblocked if you go 3G..) I use just an i7 laptop with IIS 7 and a 3G router. I use the standard Windows Firewall to block anyone I want and a good "Robots.txt" to keep all the robots out that want to eat your data constantly and for no benefit.. I keep just Google as my indexing bot. I don't need worldwide hits, I just want a place to showcase my work and this works great for me. I use a paid version of DYNDNS service which is like $ 20 or $30 a year and you get plenty of domain name choices which gives your site a more professional feel. Nice touch to this, my laptop has a slot for a GSM SIM card, so I can actually travel in the car, with my laptop, while the web server is on line. Just a thought...
I've only been with the company since last week, so I've only touched a very small subset of our databases, but overall, I believe there's approximately 5 databases, roughly 30 tables per database, and 20 fields per table. 
What databases do you need to support? I ask because I've got an ORM that can offer a Dapper-like experience, but currently we only cover SQL Server, PostgreSQL, and SQLite.
And from their docs: [DBFirst with EFCore 1.0]( https://docs.efproject.net/en/latest/platforms/aspnetcore/existing-db.html)
Again, EFCore1.0 fixed a lot of the problems.
MSSQL and *shudder* Oracle.
Based on past experience, I recommend including version numbers in your proc names. Increment them whenever you add a new mandatory parameter and you regain the ability to deploy your application and database separately.
Oracle is on our list, but we're planning on hitting MySQL and Access first. Our design queries the database for its schema, then combines it with reflection to generate database-specific SQL at runtime. So once it's done, we'll be able to offer you database-agnostic insert/updates, even upserts, without the EF-overhead. https://github.com/docevaad/Chain/wiki/A-Chain-comparison-to-Dapper
Unanswerable question. Depends on your skills, location, expectations and lots of other things.
&gt; .NET Core isn't a substitute for classic heavyweight .NET, but it isn't supposed to be so That was not what they announced. They told us we will be able to run .net of linux, the caveat is this is not .net. This is just misleading. Now what will happen to the teams and companies invested in .net for enterprise applications, who are a large portion of MS developers, I'm afraid we'll find us sidelined by better options as there's no plans for anything new or updates for .net 4.x. May be we should have selected Java when we had the chance.
&gt; Question, do you usually put the DTO code in the class containing the Model, or do you create a separate file? Or you simply put it in the controller? Is there a "best practice"? Not sure if I can point you to a best practice there. I would probably put them in a folder called ViewModels. &gt; Location.Latitude = LocationDto.Latitude; &gt; Location.Longitude = LocationDto.Longitude; Yes.
Classic .NET will be long time supported and developed, Microsoft stated it clearly. So, for enterprise developers better option is to stay with classic .NET. Just understand that .NET Core and whole bunch of "Core" technologies is not aimed for enterprise. It's an initiative for take market share at places where Microsoft's positions are relatively (or absolutely if we will talk about mobile) weak. At first time Microsoft named "core" technologies within .NET version history (named ASP.NET Core as ASP.NET 5 and Entity Framework Core as Entity Framework 7), but later Microsoft rebranded it as a "Core" family. It's a clear sign that Microsoft do not abandon classic .NET but considered a clear strategy for parallel development of modular and cross platform Core stack and monolithic enterprise ready classic .NET stack. For obvious reasons they used .NET Core in Universal Windows Platform because it's aimed at consumer market and heavyweight .NET classic would be too excessive for mobile. Also, there is very different requirements for release schedule and backward compatibility in enterprise environment and consumer environment. Enterprise do not love "release quickly and more" strategy because enterprise schedule of development and support also slow and costly. So, this division to enterprise and consumer branches is very reasonable. There will be no silver bullet to kill all prey. Practice approved that full scale .NET have not become popular with consumer apps (despite great effort from Microsoft) but gained firm ground at enterprise. So it will be two branches of .NET.
That is a post full of incorrect assumptions. Anyway... If I notice loading a detail screen takes 4-5s, I dig into it. And if I can take some time off that, I will. That's not premature optimisation, that's just fixing slow code. Turns out 3.5s of that was EF executing a query. I split it up, now it's less than 400ms per query... on the database, EF still adds a second to it. I still think that's a slow query, but apparently I can't convince EF to generate a proper goddamn query. And it's not even that large an object graph! Ridiculous. 
Depends on location, in my area there are at least 50-100 openings at any given time
this is the best response. OP, we would AT LEAST need more context like this. 
my first question really is why the company is already using .NET Core. when the team themselves has announced NOT to use it for production yet. I understand what OP is asking. I'm just curious what company OP works for and does, that would allow him to do this
yes and no. if it's a release candidate, then it's a candidate. basically that means they've created a well tested, integrated system which could theoretically be pushed out as production, yes. But it needs to be peer reviewed. meaning is the codebase clean. easy to use and read. is there enough documentation, does it have all the necessary features. and I'm sure a lot more that we don't even know of.
Saying you're specialized in .NET is like saying you're specialized in driving vehicles that have wheels. Which language(s)? Which platform(s)? Do you know any SQL? Do you have any DevOps skills? What do you know about various development methodologies? Do you have any broader SDLC knowledge? And that's just the tech. Where are you located? What type of software do you want to create? What size company are you looking for? Do you have biz domain preferences? What are your career aspirations? What are your expectations for the type of company you want to work for? You need to think about all of these things before wondering whether there are opportunities available to you. 
We are using both and it is a delight.
I took a .NET class in college 
Hey! That scenario seems straight forward enough, can't see why it wouldn't work. Wanna send me a code sample and I'll take a look? My email is antonio.nakic@gmail.com 
yeah.... that's not a specialization though
There's also nothing wrong with using a few hand-written queries here and there when profiling shows doing it the EF way is inefficient.
How are you running your application? Is it a web site or web application project? Is it in IIS or local dev server?
You degree won't mean a ton to good hiring managers. What experience do you have? Either job related, school project wise, or just on your own?
It is a web app project on a local server
Does it happen in multiple browsers?
Yes. It happens in IE, MS Edge, Firefox, and Chrome
Just tried it. It didn't fix my problem. Thanks anyways though!
Clean and then build
* The below assumes you are debugging from within VS. I've seen it happen in older versions of VS that sometimes that IIS will keep running, even if you've stopped debugging. The IIS will keep your cached version in there. Check Task Manager and make sure that IIS is no longer running after you've stopped debugging. On older versions, it would be named something like w3p.exe or something similar.
I'm really sorry. I use c sharp. Yes, I do know MS SQL and Oracle. I do have really good knowledge of SDLC. I'm in Edmonton(the economy is bad) right now. I don't have any preferences as of now, I just want to get a job.
Wow! The economy here is bad.
I did have IIS running earlier, but I shut it down. To be sure of it, I just restarted my computer and doing so did not fix the problem. 
Nope! To quote Phil Haack himself: &gt; This is partly by design as routes are generally considered application code, and should have associated unit tests to verify that the routes are correct. A misconfigured route could seriously tank your application. See http://haacked.com/archive/2010/01/17/editable-routes.aspx for another approach from him.
Unfortunately, my recorder doesn't record anything except video games. However, I made some screen shots! http://imgur.com/icr7c1J http://imgur.com/v7SA5M7 //note that the textboxes that were created aren't present and yes I'm new to asp.net
My friend up in New York City said recruiters are constantly hunting for full stack MS developers (HTML5/Javascript/ASP.NET MVC/SQL Server). Lots of openings also in Dallas, TX. 
Ok, you have Visible="false" set on the TextBoxes. If you set that property they are not rendered.
For an integration test you generally only mock/substitute dependencies not under your control. For example, I wouldn't mock a database because the project controls the db structure. You wouldn't want to use an external credit card processor though.
Thanks. Why didn't I think of that? What you said ties all the other comments nicely. Of course I would need to mock those that I don't have access to!
Thanks. I downloaded the remote simulator and I'll install it tomorrow. It's really late here. 
The iOS one only shows on the Mac. Nothing comes up on the PC. As for the android one. What emulator are you using? The android emulator for visual studio?
I couldn't tell you from the job seeker side, but I have hired two junior developers in the last couple of years and have some insight into the hiring side. In order to get in the door, have something you can show. Your resume will obviously be thin, but if you have a link to a site where you can show some of the things you've done/can do you have a much better chance of getting a call. You don't need to build a massive enterprise application, just something you have some code behind that you can talk about. If you get an interview, be prepared to discuss decisions you made in building what you built. They don't have to be the perfect solution, they might have even turned out to be bad decisions, but discuss them and what you learned from them. Don't be afraid to admit you don't know the answer to a question. But be prepared to offer ways you might go about finding the answer even if it's google. I cannot tell you how many people I interviewed (Houston area) that wanted to be a programmer but hadn't written a single line of code, didn't have any idea how logic loops worked, etc... Your competition isn't fierce. 
Competition is good, and I'm sure you'll find a market for your product. My last company wouldn't let me sub to NewRelic because of the cost for instance. 
Take a look at a nuget package called ClosedXML. It allows you to output a data table into an excel workbook and setup the formatting etc. 
It is very easy to end up with a mess when mapping multiple solutions to a project structure. What if you add a new project to solution 1 and add it as a dependency to a project you are also referencing in solution 2? You now have to change solution 2 as well. The option of nuget is probably the best one here because it allows you to share project code without forking or creating a complex project reference structure across multiple solutions. It takes more discipline and will probably result in some architecture discussions with your team, but the benefits outweigh the drawbacks. You can host a private feed on a network share, you don't have to publish the packages to the world.
It's all async. It just takes too long to load for what it is - that is the issue. I know it can be a lot faster.
Right in the middle of the page is "Comparison Matrix with Microsoft IntelliTrace" It's pretty hard to miss.
Yes, I think you're spot on. It's not a problem in general though, as you can switch frameworks instantly if you want to in the designer (e.g. you started your project using our own ORM framework and then want to switch to EF because &lt;reasons&gt;, you can do so), but people tend to think that if MS is behind it, it's future proof. In the past 14 years I've now worked on LLBLGen Pro I've implemented support for many Microsoft frameworks, APIs and systems for our runtime and designer, and many of them are defunct today simply because MS moved on and kicked the framework to the curb. EF6 will follow that same route. It takes ages before they fix an issue (I still have an open issue with them regarding code first, it's open for many months now) and clearly the EF7/core rewrite takes all their time. Maybe when developers learn that they can't really rely on MS regarding ORMs (Linq to SQL should tell people that already though ;)) they'll come around, but that'll take time. In any way, we're still going strong :)
&gt; I have never run into any issues with EF and number of tables, though I'm sure at some point you would hit a limit. At work we are currently at about 170 tables per database in our project, all of which are on the same EF context, with no real issues. Have you measured startup time? EF's context with 80 tables or so (AdventureWorks) already takes several seconds the first time it's used. With 170 it takes many more time. 
If you are genuinely looking for a framework for data transformations, look no further than Dataflow in the TPL. https://msdn.microsoft.com/en-us/library/hh228603(v=vs.110).aspx http://www.michaelfcollins3.me/blog/2013/07/18/introduction-to-the-tpl-dataflow-framework.html It allows you to code data transformations into "blocks" that inherit all sorts of goodness, primarily the ability to run data pipelines in parallel in a safe, abstracted manner without having to ever even think about creating a thread yourself. Assuming you are already familiar with the basics of the TPL (Task Parallel Library), you can get results in very little time. 
**Announcing .NET Core RC2 and .NET Core SDK Preview 1** https://blogs.msdn.microsoft.com/dotnet/2016/05/16/announcing-net-core-rc2/ **Announcing ASP.NET Core RC2** https://blogs.msdn.microsoft.com/webdev/2016/05/16/announcing-asp-net-core-rc2/ **Announcing Updated Web Development Tools for ASP.NET Core RC2** https://blogs.msdn.microsoft.com/visualstudio/2016/05/16/announcing-updated-web-development-tools-for-asp-net-core-rc2/ .NET Core RC2 is supported on the following platforms: * Red Hat Enterprise Linux 7.2 * Centos 7.1 * Debian 8.2 (8.2, 8.3, 8.4)+ * Ubuntu 14.04 (16.04 support is coming at RTM) * OS X 10.11 * Windows 7+ / Server 2012 R2+ * Windows Nano Server TP5
Wow. Ok...I had no idea this massive transformation had taken place. I guess I am definitely going the Xamarin route. Thanks!
Expanding on this a bit, I was messing around at work today, and I believe I may have been mistaken when I referenced that the project was using .Net core. I checked our references and it looks like it's using DNX 4.5.1, so I'm not entire sure here.
I guess you're looking for more downvotes now? Sure, have one!
Yeah it failed to install on my OS X machine, after sorting it out - tools are crapping out - "Can not find runtime target for framework 'DNXCore,Version=v5.0' compatible with one of the target runtimes" aaaand the documentation is still for ASP.NET 5 and not Core.
Just so I'm clear what the guys on the .net core team are saying; we're pretty much good to start using .net core to make production projects now right? Also, I assume the answer is no, but is there a way to have a common class library between a .net core project and WPF project? Since they're on different .net versions you cant directly reference them, so is there another way?
They said the same about RC1. But RTM should not have any breaking changes, so I guess it's pretty safe to start now and end it with RTM already released.
I love the new choice. I am glad that I can choose between Windows or Linux for my .NET apps.
[Shouldn't be too hard](http://lmgtfy.com/?q=.net+developer+edmonton)
Mass Transit is a Open Source messaging framework written in .NET that utilizes RabbitMQ or Azure Queues to process messages. https://github.com/MassTransit/MassTransit. They use the concept of Sagas to handle workflow: http://masstransit.readthedocs.io/en/master/usage/sagas/index.html. If you are OK with Java, another option would be Pentaho. Kettle is the data integration front end to Pentaho http://community.pentaho.com/projects/data-integration/. 
The error is giving you a clue. &gt; Session state can only be used when enableSessionState is set to true, either in a configuration file or in the Page directive. Please also make sure that System.Web.SessionStateModule or a custom session state module is included in the &lt;configuration&gt;\&lt;system.web&gt;\&lt;httpModules&gt; section in the application configuration. I didn't see your configuration/system.web/httpModules section in your examples above. Documentation on what it should look like located here: https://msdn.microsoft.com/en-us/library/9b9dh535%28v=vs.85%29.aspx &lt;system.web&gt; &lt;httpModules&gt; &lt;add name="Session" type="System.Web.SessionState.SessionStateModule"/&gt; &lt;/httpModules&gt; &lt;/system.web&gt; 
I fear the classic EF. EFCore though is good.
Is this slack only for professionals or are beginners/intermediate also welcome?
All are welcome
Awesome! :)
You must set it each time. Use an initializer to ensure all your items gets in the same way.
I don't follow. Shouldn't NuGet pull these down?
I've been a .Net developer literally since beta (started a major enterprise product on the beta, but had to wait for 1.0 to get it out there, then 1.1 ... etc, now 4.6.1). WinForms, WPF, WebForms, ASP.Net MVC through 5. This is by far the biggest change I've ever seen come out of Microsoft. At this point, I'm not even sure is ASP.Net MVC is even a valid technology now. Everything seems to be moving towards client-side AJAX stuff backed by API calls (such as Web API with Angular, etc). Just taking the start, in RC1 the framework called a class library, now it's a console application that has to write the code that was in the framework? Or something like that. Haven't gotten through it all yet! 
To be honest I would not want to be writing production apps with .Net Core until the final version is released (and even then, there will likely be growing pains.) Just going over the announcement post there's been some big changes between RC1 and 2. Definitely play around with RC2, make some trivial apps to get some experience with it, but if it were me, I would definitely NOT be doing anything production-level at this point. 
Also when you get there thank Shane Boyer (spboyer) he is the one who got the ball rolling.
&gt;The release contains the RC2 of the .NET Core runtime and libraries. These libraries are everything that ends up in your ‘bin’ folder when you deploy an application. The tooling included (command-line tools, project tools, and Visual Studio tools) has been declared as a preview 1 release. Tooling, which is likely the culprit of the errors you are seeing in Visual Studio, is only at Preview 1.
Gotcha, cool! I'm just playing around at this point, not using this for anything important. Oddly enough, I don't get this issue on my other laptop, which has pretty much the same setup.
Stay with MVC. The Javascript world loves to produce a million different frameworks that essentially do the same thing. You want the fundamentals of MVC. Ignore this guy. Source: 10 year dotnet dev.
I think he's referring to the fact that messaging (discrete transactions) can take place at the client level with AJAX and Web API calls. Meaning, normally with MVC, the View would capture information, and pass it back to the Controller; which in turn would subsequently update the model information that was obtained from the View. On the other hand, instead of sending that information back to the Controller of the application (which could even potentially result in a full page post back to the server), the updating of the model in question is handled by a Web API call with AJAX, effectively eliminating the model concern (other than Authentication) from the Web Application itself.
Does .Net Core have F# support?
LLBLGen Pro is a great tool and I've had no issues with it. I've been using it since 2.6(starting maybe 9 years ago) for a fairly large 200+ table enterprise app. If it were up to me, and I was starting a new project, I would definitely stick with it. However, I'm now going back into the job market and I need to know EF for my resume. It seems to be required for most .NET jobs. Considering it's now at version 6 I was surprised that it immediately felt like a step backwards from LLBLGen Pro. I'm not sure if Microsoft even uses their own tools sometimes. They usually demo the simplest examples, and then completely glaze over any real-world scenarios. Like having more than a dozen tables using the model first approach. They are also apparently abandoning this altogether. I actually really like the model first approach. When done correctly it handles database scripts as well as entity generation. This can result in far less work for the developer. I think we are even getting close to the point where developers won't need to worry about the particulars of a data store at all. That would leave more time to focus on the actual business needs. At least I hope we are moving in that direction. Yeah it's hard to go up against the Microsoft marketing machine. Maybe you could at least update some of the common sources for ORM comparisons like this one: https://en.wikipedia.org/wiki/Comparison_of_object-relational_mapping_software 
The CORE team has been saying that core is ready for production since RC1. Any senior developer will tell you, don't put your business at risk by being an early adopter. Have fun, play, practice, learn but don't depend on it.
Nah Damian always told that "production rdy" is up to the persons values. RC1 was a supported release. (Not sure if that's still the case tho) with a - more or less - stable API. 
Where I can read about the 'dotnet' command. All their documentation is about ASP.net.
Wait, don't ignore this guy (that guy being me). I absolutely, 100% agree with you!! I have been open to offers recently, and getting a lot of them. Developer from pre-.Net but have seen it all so am pretty "senior". If the description starts pointing towards the client-side (web), I generally will back off. I'm a full-stack developer. I understand ECMAScript/JavaScript very well. I created my first successful website in the late 90s with it and I still can barely tolerate it (JavaScript). I know jQuery, I know Angular ... now Grunt, and Gulp, and Jasmine, and ... oh, who can forget about Node.js, because we have to get JavaScript over on the other side too. I still am shocked that a language that 1 guy created in 10 days has become the *only* option on the client. To be honest, I can't stand the state of client-side web at the moment. These frameworks come and go. Which one do I need to know now? Ember? Backbone? Angular? You pick one then it goes away because the other one is the "new thing". At least with Angular you have the Google name behind it, but people have left because Angular 2 isn't what they signed up for. I really try to stay on the "other side" of these things, back-end C#, Web API, SQL, and now Azure. More than that, but those are the core focus. There are *way* too many companies that are requiring Angular, yet are using it completely wrong, and some of them don't even realize that Angular 2.0 is going to be a "let's try it this way" sort of deal. Hope they are up on their TypeScript. They use it because it's the new buzzword, and they bring in tens of thousands of lines of code to ... honestly, in most cases I've seen, do nothing that it's meant for that can't be done in 10 lines of standard JavaScript. I want to stay out of it.
Please don't get me wrong about what I said. I **want** ASP.Net MVC to survive, to me in the vast majority of business applications, this is what is correct. It feels beyond wrong to have to deal with npm, require.js, Angular.js (or whichever other one) to create what in almost all cases is a basic LOB application. Let's add 50,000 lines of non-compiled/interpreted code (that we don't realize we're adding), that is now running not on our server but on machine we don't know the specs of, so we can sort this grid. That sort of thing. It can be completed in *much* less time, without all this fragile, come-and-go stuff, if it's just done properly under what is still solid technology. See my response here: https://www.reddit.com/r/dotnet/comments/4jmijd/aspnet_core_rc_2_is_out/d38xyco
Yes, that's what the "big deal" is now (but they come and go, the problem with Angular is that Google is behind it). SPAs (Single Page Applications), what Angular is all about and you'll see in a very large number of job postings. Essentially everything is running in the browser and you make AJAX calls to the server to GET/POST/PUT etc, and then hand it back over to JavaScript (Angular) to do what it does. It's essentially "MVC on the client", and is used incorrectly more times than I can even explain. When it's used correctly, and needs to be, it's very well designed. But most cases I've seen are really abusing it, and adding extreme complexity, for no reason.
JavaScript is a complete mess as a language. The code is ugly, doesn't have many features, is loosely typed, and well I just hate it. However, Angular, React, Aurelia, etc seem to serve a very good purpose. But it still feels like it's not quite there yet. ES6 is a step in the right direction, but it won't run on IE, so it's pretty much unusable in the real world. ES7 should be a huge improvement over ES6 as well, but we probably won't see it become fully usable for 10 or so years. That's an eternity for developers. I absolutely love the abilities these frontend libraries give me, and Angular seems to be the prevailing library so far, so it's what I'm primarily focusing on. However, it does seem like Aurelia might be the "best" of the bunch. React brought a lot of neat concepts into the picture, and it seems like Angular 2 is kind of a marriage of Angular 1 and React, so Angular 2 should be pretty good. I haven't really given it a big look since it's still in beta, but I'll probably start playing with it pretty soon. But again, I absolutely despise JavaScript. God, it sucks. So there's something new on the horizon. All of the big browser companies recently got together and collaborated on something. Microsoft, Google, Apple, and Mozilla. They're working on a new standard, called WebAssembly. This will be a binary format supported by all browsers. It will do what JavaScript does, but be written in a much more powerful full featured language like C#, and allow you to write client side browser code with a mature and full featured programming language, and it will compile to a format that every browser supports. I have a feeling that this will be a nail in JavaScript's coffin. Unfortunately, since we're talking about web browsers, we'll have to wait for 2 full Windows release cycles before we can really use it, so just like ES7, it's probably about 10 years off from being truly usable. But think about what this means. A traditional client/server desktop application, OS agnostic, with instant on demand delivery/updating to the client side. It will be everything that Java set out to be, but hopefully it won't suck near as bad. Edit: Corrected name of WebAssembly
That will be the case if the CourseDays was removed individually, in practice there is a calendar widget that support multi select. When the user click save, the client sends the whole Course object with the edited CourseDay collection.
&gt; The core features of the language, that we all have to use, were written by 1 guy in 10 days. I guess it's "pretty good" for 10 days? Well yeah, but JavaScript has been around longer than 15 years, and it really hasn't advanced all that much. You would think in all of that time, they would have made some significant improvements to it. The problem is JavaScript was never intended to be a full programming language. The way people are using it today is kind of a "hack" on top of its original intent. We've rubber banded a bunch of popsicle sticks together and made a car. But in the future, we won't have to do that, we'll have better tools more suited to that purpose. JavaScript is more for manipulating the mouse cursor and stuff like that. &gt; Can you imagine how many shops are out there heavily invested in Angular 1.x only to realize it is not backwards compatible with Angular 2.0 and that the syntax is very different? You have to expect that in this ecosystem, though. Web standards are constantly changing, and nobody can ever agree on the best way to do anything, so it seems at least once a year the whole paradigm completely changes. If you build something and expect it to never change, that's just naïve. The good thing is you can build an Angular app, and it won't break. It will continue working. Just the next app you build, you might think about starting with Angular 2, and investing the time to learn it. It will have new features that you'll love, but of course there will be some growing pains. &gt; I believe you're talking about WebAssembly. Yeah, I was actually coming back to correct myself, but you had already responded. ;) Either way it looks exciting. Apparently it's already out and usable, but like I said, we'll mainly be waiting for everybody to get onto Windows 10 or newer, before we can truly start using it. Where I work, we have enterprise managed computers, so now all of our computers are on IE10 at the very least, so we recently got to start using CSS3 in our web development. That's the bad thing about web development, is you always live on the cutting edge, and you're always a few steps ahead of the IT department, waiting on a new rollout before you can start utilizing that new feature. But unfortunately, this time around Microsoft refuses to add any new features to IE, and is requiring people to get Edge to support any new web technology. And, Edge depends on Windows 10. Where I'm at, the users are all still on Windows 7, so it will be a while till we will have Edge as a baseline. It could be worse. Developing web apps for the internet, like Facebook, is a nightmare. If I still had to support IE6 I'd consider suicide. ;)
Resharper helps enforce coding conventions. Most of it is aligned to the microsoft design guidelines. My only complaint with the coding style in TailBlazer so much the convention as it is the organization and design methodology -- the frequent SRP deviations. Not that big of a deal; I don't think it's bad and we've all seen much worse, but some methods are bit monolithic for my taste. By no means what I'd call bad. The only other thing is the functional nature of Rx.NET. Maybe that's what you're referring to. There are a lot of method chainings which isn't wrong (it's kind of the way you're shown to use it), but it's hard to follow, for sure. Particularly if you aren't experienced with Rx.
I would still put it in an api and just have the endpoint take a list of ids.
Yeah, so maybe "coding style" wasn't the best way to describe it. I was mainly referring to the organization of the code, naming conventions, etc and how all that ties together. It's kind of weird. It's almost like it's organized, but not organized at the same time. But you're right, it's hard to follow. Also, and this is dumb, I can't get over why the other projects are named "Domain" and "Fixtures." I'm sure there's a reason behind it, but it doesn't make sense. The names and namespaces for other projects typically describe what those projects do. Like MS namespaces: `System.Math` `System.String` `System.Collections` `System.Data.SqlClient` Reading those names, you have a pretty good idea of what they're used for. But Domain and Fixtures seems super ambiguous. It's dumb, but I can't get over it.
It's better to mark your deleted items as deleted. This way you don't need to figure out which items are gone afterwards. Just add a boolean "IsDeleted" to your CourseDay class.
This is why I don't like the REST design pattern. From an engineering perspective, the client should just be sending you a list of deleted keys. Uploading the entire object graph just to say one data point was removed is wasteful. And then there is the tedious work server-side to figure out what the real delta is. 
I think the REST was of doing this will look like this: DELETE /api/courseday/id for every removed day. right ? I am sending the whole object graph back because the edit view also allow the user to edit properties on the root Course object, like the title property.
&gt; Well yeah, but JavaScript has been around longer than 15 years, and it really hasn't advanced all that much. You would think in all of that time, they would have made some significant improvements to it. The problem is JavaScript was never intended to be a full programming language. The way people are using it today is kind of a "hack" on top of its original intent. What else can they do? It has to be fully backwards compatible (well, very very close) otherwise changing anything fundamental from v1 could honestly "break the internet" unless *everybody* is onboard, from all the major browser vendors, to the tablets, phones, kiosks. It can't be changed at any fundamental level. &gt; Just the next app you build, you might think about starting with Angular 2, and investing the time to learn it. It will have new features that you'll love, but of course there will be some growing pains. It really depends on the app. I fully understand Angular on the client, and when it needs to be used. Angular 2, from what I've seen so, gives a major performance boost (due to the constant observing thing going on), and is slicker, but there are a lot of apps out there that should obviously *not* be using this stuff. It's like hitting a nail with a sledgehammer. Regarding WebAssembly, I am not sure I fully understand the concept of this. It seems like a much better version of Java, in essence. So you can create (at first) ASM/C code that runs safely sandboxed in a browser, and the roadmap shows them going all the way to implementing garbage collection, so then you literally have Java, and .Net. But then the entire internet no longer resembles anything as it is, and I'm not sure I understand the point of a browser? 
I'm disinclined to download something. Any way you could get it on [jsfiddle](https://jsfiddle.net/) or similar?
I'm not sure what your experience is but this is incredibly common, and his naming conventions are just about perfect imo. As for project structure, it is also common. Domain is the business logic, so to speak. Fixtures is a test project -- it's not part of the solution; it's a project built to exercise areas of the solution. You can still drill deeper like TB.Domain.FileHandling. You're kind of comparing infrastructure libraries, which are far more granular, to application libraries and they're simply not the same -- it's not just about namespace, these are completely different assemblies. And if you know what a domain is, then you'll know what that project is for and why it's in its own separate assembly, walled off from everything else. In fact, by separating like this, he could roll a completely new presentation layer (e.g. a web interface) and reuse the domain library. I don't see what the problem is. That being said, as the project grows, I'm sure he'll break it down even more as it makes sense to and as related areas begin to become more apparent. At least enough to warrant their own namespaces.
I've never seen any projects ever name the "business" project "Domain" and I guess I didn't look enough into "Fixtures" to see what it was. But if it's a test project, why not just call it a test or something similar? I'm aware of separating out the "business" logic from the presentation layer. I'm sure the code has changed since I last looked at it, but it just didn't look organized or easy to follow when I first checked it out. I'll probably have another look later.
There are some questionable decisions made in the Domain. Most of the stuff in the domain project are more like infrastructure than business logic. There isn't much business logic to speak of, to be honest. Domain as a business project/layer is a bit of a nip from Domain Driven Design. It's roughly the same thing but implies a lot more than is appropriate here. This isn't a DDD project. But I know a lot of people treat Domain and Business as synonymous. In my opinion, it's neither domain nor business nor infrastructure. It's kind of a mashup of an application layer and infrastructure. But he has infrastructure namespaces in a couple different locations and I think their semantic meaning is a bit thin. As for Fixtures vs. Test, you can't have tests without fixtures, so it just seems like a snotty way to say the same thing. I dunno. I believe he's British. lol. It's the same thing either way. There is definitely more organization and normalization that can be done within each namespace, but I wouldn't go as far as to call it unconventional at all. I opened the project for the first time and it felt very familiar. Everyone's exposure is different, I guess. As for the mild disorganization, I'm sure it's just a case of him working in a bit of a silo and not really prioritizing it. I know he would like some help in some areas, but I got the impression that he's a little defensive about it all. I wrote up some features and pointed to a tailer that I wrote as an example. He got a bit defensive and deleted my enhancement requests. I would have helped with it as a contributor, but he didn't give me a chance to offer and I didn't like the vibe I got, so I just backed off.
RC1 was go-live. Thankfully I waited, now I'm gonna wait longer :)
Since you called out aspx, I think you mean WebForms. And, since you called out a server framework when asking for client-side help, I think you are wanting some super easy controls to do the client work for you. Checkout AjaxControlToolkit. I think I remember it having support for modals. https://github.com/DevExpress/AjaxControlToolkit
I've seen all three. Fowler is there a lot, Damian pretty frquently.
i don't see why anybody would. there are plugins to make visual studio's text editor behave like vim.
Check out Epplus
I agree, I don't like the sound of the views being required by both domain and web server. It sounds leaky. I'm also not sure why shared code for permissions and identity is required. I'd assume in CQRS that permissions would all happen in the presentation layer (controllers) unless there was some logic required in which case the the logic itself would exist in the Domain/Business layer and the Presentation Layer would call into those layers to execute said logic.
Agreed. I would (and we are where I work) terminate authentication at the web API tier, do whatever minimal amount of authorization there is helpful for, say, synchronous validation of some minimum bar of permission, and then (in our case, our transition to asynchronous happens right here...) pass assumed-good identifiers through to the back where whatever further authorization work can be performed as and where needed for the operation in question. (And then, in our case, the client is waiting for resulting domain events via pubsub or polling or sync-over-async leveraging one of the aforementioned two, etc.)
you can use OleDb - some examples of connection string and options are here: https://www.connectionstrings.com/excel/ i can whip up a quick example if you'd like. i believe this way you don't need excel installed.
ClosedXML makes it easier for developers to create Excel 2007/2010 files. It provides a nice object oriented way to manipulate the files (similar to VBA) without dealing with the hassles of XML Documents. It can be used by any .NET language like C# and Visual Basic (VB). https://closedxml.codeplex.com/
That sounds like a Resharper problem. Visual Studio makes working with linked files a pain, but I haven't encountered that particular issue. (And I use a lot of linked files.)
That sounds like a Resharper problem. Visual Studio makes working with linked files a pain, but I haven't encountered that particular issue. (And I use a lot of linked files.)
I still don't think I agree with the way things are abstracted there but without seeing everything in action it's hard to really judge so I'll abstain from that discussion. I will instead put my weight behind the idea of a 'Security' solution that is exposed as a Nuget package for your Web/Domain server. We have lots of this going on where I work. In our case its because several different products rely on the same security measures and other cross cutting concerns like logging and auditing. We created a Standards solution with libs for logging/auditing/security each with their own nuget package. That way any product that needs to rely on these concepts will do so in a generic way across the company.
&gt; I will instead put my weight behind the idea of a 'Security' solution that is exposed as a Nuget package for your Web/Domain server. That's basically what we have. It just needs to be consumable by both .NET Core and .NET Framework.
Advanced vim functions don't work right in it, and vim plugins don't work at all. I still use vim.
That was an idea I had, just drop a project.json into the same folder as an existing .csproj and see if it will compile the same code. I don't know how Visual Studio will feel about that, I guess it's another thing to try.
Second on ClosedXML. Excel interop are slow and error prone. Right now I'm converting a tool from interop to use ClosedXML. It's a dream. I only wish it would work with 2d arrays but I just had to rewrite a small part of my code to use jagged arrays instead. I believe it does support direct insert of ado datatables. Even create the whole worksheet from a data table. Also don't worry about the name, it's open source with MIT license. 
Another +1 for closed XML. 
If this is a server-based process, then you're going to find using the Excel Object Library extremely limiting from a scalability standpoint as well as speed. It's been a few years since I've touched it, but Aspose is an excellent product for these kinds of needs: http://www.aspose.com/.net/excel-component.aspx We used it for an ASP.NET based product that was provided as a COTS system to business partners, so I know it's up to the task. It not only allowed us to generate Excel data files, but let us put formulas in, hide columns, and do other advanced formatting. It might be overkill for your needs, but it will give you plenty of room to wiggle in the future. 
DevExpress
 * [Open XML](https://msdn.microsoft.com/en-us/library/office/bb448854.aspx) * [SSIS](https://msdn.microsoft.com/en-us/library/ms139836.aspx) * [Jetcell.NET](http://www.devtriogroup.com/exceljetcell/) Or create a CSV file instead
Is it free?
If performance is your only concern this might be overkill but the excelDna library is very good. We are using it 'in anger' to do a lot of stuff and it's working really well.
NPOI.
I've been using libxl for its performance and ability to read/write large spreadsheets.
My best guess is that they're going to sit you down at a computer and give you a programming project to work out. Very likely it will in in the MS stack, and probably it is not intended for you to finish it as a polished product. They're looking to see A) how good your fundamentals are ( your ability as a programmer), B) how much you've learned about the Microsoft frameworks since the last interview (do you *really* want the job?) and C) how far you can get on what you know plus what you can find out (practical application of skills and problem solving ability plus you google-fu). At least, that's what I would be doing to a candidate at this step.
If the architecture is CQRS as you described then there absolutely positively is a conceptual boundary involved: read vs write. Such a view-related projection should not flow up from the subsystem concerned with mutating the aggregates. In other words: If you need to split your system, it is likely not into two but rather into three. Web tier, read-side service(s,) and write-side service(s.) If you choose to split into just two, that would be web tier and write side services, and it'd be achieved by embedding the read side up in the web tier. I'd go for the three-way breakdown, personally, but either way the view models for projections won't go anywhere near the write-side service(s.) (Updated to fix typos from while walking to the train and to address OP directly after realizing I was replying to them. ;)
No disagreement there. Authorization needs to happen where authorization needs to happen. ;) Any more interesting wrinkles are likely related to where you terminate external representations of identity.
With your experience it sounds like you should be fine.
I do not agree with your premise that every app should handle it's own logging. We standardized across all products so our support teams have an easier time supporting the products. Also, it's no different then bringing in nlog or entlib package. 
Linq2Excel is a handy library I use for spreadsheet processing in .Net. You may want to evaluate your code for excessive memory consumption. Maybe there's opportunity to dump some of your scope.
The only difference between .Net mvc framework and other frameworks is how you set up the configuration between m,v and c. Everything else is pretty much the same.
It's because EF is an implementation of UoW and Repository that it works so well with those patterns on top of it. Now to answer your question it's a little more tricky. The **most** common answer is that you want to abstract away your reliance on the underlying tools. This provides you an easy way to switch to a new framework or implementation should the need arise. Most people that advocate it have been burned before by the framework they use deprecating an obscure function they use and it having far reaching effects. If you have an abstraction of your own in place, then the problem is limited to a relatively small area that is known. Mind you, you don't **need** to abstract it away, but it does help by trying not leak the underlying tools outside of their intended scopes. It also can aid in unit tests as you only need to mock *your abstraction* and not implement the underlying tools. People are also worried about the next person to work on the code base. Having an abstraction that is well defined makes it easier for someone else to understand that is new to the team or if the original team/person isn't available to explain it or maintain it. Lastly, do keep in mind that patterns are there to help, not hinder, and they are guidelines, not hard and fast rules. Patterns have a tendency to get abused quite a lot and forced in to situations where they really don't belong. Many people dogmatically follow patterns for no reason and enforce them like religious beliefs. If a pattern isn't working anymore, rip it out. If you need something similar to a pattern, change it to your needs. 
jQuery Ui, Bootstrap, roll your own. It's just javascript &amp; css. 
Next time it happens, try deleting your .vs folder. That seems to clear up other linked file issues I've seen recently.
I use spreadsheetlight. Works great to create excel spreadsheets or edit them... although technically, it does not Interact with excel
It might be because I'm currently looking at an implementation of it handled poorly, with all the configuration etc handled by the god forsaken base library.
Uh, no. I'll do some experimenting, but I'm not going to do a formal port until the tooling settles down a bit. 
Just write a handler to intercept and store it. I found you a [link](https://leastprivilege.com/2013/03/11/alternative-to-thread-currentprincipal-in-asp-net-web-api/). Edit: realized you said you don't want to do this in your web project. What kind of project are you trying this?
scale up with the code as is? fix the concurrency issues with using List&lt;&gt; for everything and it should work fine until you exhaust the resources of the PC. I can't imagine it would be much worse than not using a singleton with your current implementation. But scale out? You handcuff yourself here pretty badly. You'll very quickly need to expand this to using something like redis or sql server to store the users and some connection info so it'll work across multiple servers, and reading/writing that's not going to be a ton of fun to deal with a singleton being hit from all over the place. You'll end up making a bottleneck as you try and keep single requests from stomping over each other or doing a bunch of manual work with thread contexts and the such to try and keep these connections to one instance per request...and at that point you might as well have your middleware an instance per request.
Some really good tips on speeding things up [here](https://www.add-in-express.com/creating-addins-blog/2011/09/29/excel-read-update-cells/) and [here](http://chandoo.org/wp/2012/03/27/75-excel-speeding-up-tips/)
Hmm. What else is out there? I haven't touched WebForms in years.
I bet there's a hand full of packages that are popular dependencies between many other packages. Those guys better be fast! ;)
Use the new cli tooling and in the project.json, set the target framework to netstandard1.x where x is the minimum you need to support the rest of your dependencies. dotnet pack will generate a nuget package you can use in both .NET Core and 4.6 that is identical. You can also use the .dll generated directly but that only works in csproj obviously. You'll need to add NETStandard.Library as a dependency as well. You can create a project with the new .net core class library to see how it's set up. I had a problem using netstandard1.5 (which is the default for the template) for some reason that I haven't figured out yet, but otherwise it has worked fine.
So let's throw the word scale right out of this conversation. Because doing this stuff at scale is Hard™. Like really, really hard. But to keep it simple and assume that you have application that people can connect to there's all kinds of ways to connect to redis and persist and retrieve data. One way is very similar to how you'd define things in EF. You have the model that you want to persist, and you'd use something like Stackexchange's redis client to read and write to the key store those values. There's also Jason Punyon's library which I've not used in production but looks interesting that might fit the need for this too which implements things totally differently. https://github.com/JasonPunyon/Rol 
 You use moq with a fake context with fake dbsets, it's easy bro https://github.com/scott-xu/EntityFramework.Testing
This comment may have been relevant with EF5 or earlier, but starting with EF6, mocking DbSets is quite simple. See for yourself, even supports asynchronous queries. https://msdn.microsoft.com/en-us/library/dn314429.aspx 
Authentication happens in a pre-hook on a web request, and also when a message is deserialized. Every message has a `SecurityId` by which we can look up the principal, which is cached because it's hit so often. Authorization happens in the API code after routing, and just before a command is sent to an aggregate.
You compile using command line? Is your Makefile a secret or could I find it on github? :D
works fine for me
RTM is supposed to be out next month right?
Makes sense. It sounds like you might fare well enough with the creation of a project for the read projection types, which can then be referenced by the other two. Then as the system grows you can break those three pieces apart over time, as and when needed, to align units of change with units of release. If it does get large enough over time that very distinct deliverables (likely delivered by multiple teams) need to share some representations then I'd suggest jumping straight to sharing schema rather than going down the rabbit holes of either submodules or nuget packages. As for submodules, yes they suck. They're just turning out to suck less (for this kind of representation sharing, at least) than NuGet packages, though, again, some amount of the suckage of both likely stems more from them having become too monolithic. Had that not happened then both would be nicer to work with. That said, when you're adding and/or changing representation code and the consuming code at the same time then submodules do (and would still, even if less monolithic) do really come out far ahead of packages. For your sake I hope you suffer neither. :)
I think this is the right approach. Better to experiment now and overcome the big hurdles then small tweaks for RTM. I know they have made some big changes in RC2 but surely there won't be big changes between RC2 and RTM.
PS just in case it hasn't been clear: when I mention sharing schema I mean schema for things like read projections, request and response types, commands, events, etc. Certainly not db schema. Absolutely not db schema. :)
Well it's fine if you only use the basic stuff.
[To the rescue] (http://www.codeproject.com/Tips/1036630/Using-Effort-Entity-Framework-Unit-Testing-Tool)
Honestly, it sounds like you're covering all of your bases pretty well. Not sure which version of VS you're working with, which could matter a bit. As far as I know, not many people are rushing to adopt .NET Core yet since it's a somewhat immature platform. Most development exists in the ASP.NET 4.6 platform (where it will likely stay for another 1-3 years I bet) because MVC5 is a very battle-tested production facilitator. I think it's worth delving slightly into ASP.NET Core 1.0 and learning some of the new improvements introduced; especially support for JavaScript client frameworks and MVVM architecture. If you have some experience with things like AngularJS, that might be a major reason why they're interviewing you: a move to .NET Core. If you have the time, I'd suggest reviewing how to use EF with a database first approach; I learned the hard way that it can be obnoxious sometimes with complex existing relational databases that have strong key structure. It can also be super easy, it really just depends on how the database was designed. Also knowing a bit of ADO.NET (which is the foundation of EF) to interface with databases will go a long way I've found. Basic things like being able to write code to call a stored procedure/view, or using inline parameterized SQL, or returning POCO results from a database. I suspect this is because fewer and fewer newer developers are actually proficient with those skills because ORMs and code-first can be so easy to use. Coffee is in short supply, so that's all I can think of at the moment, sorry!
&gt; Have a Kernel assembly that knows about all the necessary implementations. My IoC assembly doesn't have any references to any other projects in the solution. Instead, every project in the solution has an IModule that is exported. The IModule does registration for its own assembly. Then my IoC library scans all the DLLS for IModule and calls the Register method via reflection. This prevents any circular references or something similar and every assembly knows how to register itself. Do you have any reading on this 'pure DI'. I'm not sure what that term means.
Honestly, sounds to me like you've got this well covered!
&gt; surely Fool me once...
You might find the user reviews for mobile app platforms on IT Central Station to be helpful. As an example, this user reviewed Oracle Mobile Application Framework and wrote, "There exists different hybrid solutions in the market with open source products, but if you want to have support and a real business solution, then there is no better solution that I've seen." You can see the full user review here: https://www.itcentralstation.com/product_reviews/oracle-mobile-application-framework-review-37239-by-marcus-hammer You can see reviews for PhoneGap and other solutions here: https://www.itcentralstation.com/categories/mobile-app-platforms
Ahh, good to know. Thanks! 
I don't believe that Thread.Principal is available in ASP.Net core. I used that same method in previous versions Also, do you replace name identifier with their usernames from the thread principal?
Apologies you are correct that was 4.5 not Core. nameidentifier is the key value for the user id. 
These things tend to resolve themselves. Cream will always rise to the top.
Its not a joke at all. Loads of jobs at both MS or affiliated companies. 
Where do you work? Will PM you a link to my resume.
I'm a dev @ Skype for Business, and I cannot recommend getting a job there, but Microsoft is a good generic answer for .NET jobs in the Seattle area.
Glassdoor.com
Post resume on dice.com and indeed.com
As others have said, I would recommend posting your resume up on Dice.com / Robert Half Technology / Indeed.com / CyberCoders - You will get a good amount of calls per week on those alone, even from other technical recruiting firms with open positions. I just went through the process and landing my first gig in the development field, and started on Monday. Edit: Angular seems to be pretty popular atm. Any knowledge on that can only help you in the process. 
I wouldn't recommend it because the on-call experience and live site tooling are abysmal. I wrote a big rant that I then deleted, but a couple of the key points are: * 12+ weeks to make a complete deployment to every machine in prod (this is with a build in hand, and does not count the 2-4 week lag between commit and being approved for deployment). * Telemetry and performance counters are on a 24-hour lag, so we have no visibility into the live site * Almost all alerting is via synthetics that are prone to false positives. * Retrieving logs can take several hours (the way our logging is structured is such that I need to get logs from A, see that the error is coming from B, repeat all the way to Z). * Devs are the ones that get paged, but we need to ask ops to make any live site modifications because we don't have write access due to a bunch of compliance reasons. I understand the reasons, but it is super frustrating when trying to correct an incident. * The on-call dev gets paged all the damn time. My average is about 15 separate pager incidents a week, many false positives, and the trendline is getting worse. We also get more lower severity alerts than we have the manpower to investigate. Until the on-call experience gets better, I cannot recommend my division at all. I stay only because my bosses keep promoting me every review cycle; as soon as that gravy train stops, I'm bailing.
Each lib will define it's own registration routine (a module in autofac for instance) - lib2 will have internal dependencies, lib1 will have internal and lib2 dependencies, and myapp will have internal and lib1 dependencies. You will call lib1 initialisation from myapp, and that will in turn call lib2 init.
Sure, but you wouldn't be able to call it because your app doesn't reference Lib1. Here's what we do at my work: Interfaces +Lib1 +Lib2 +IoCHelper +App So everything references the Interfaces library (We name it contracts, and it only defines the edges of your layers for dependency injection). IOCHelper brings in the references to Lib1 and Lib2 and has static methods which handle the dependency registration. App references IoCHelper and calls the static registration method at startup. Now the app does not *directly* reference Lib1 or Lib2, but can still call their implementation though DI.
Man, that's brutal. They were selling this like the next big thing... I can't say I'm dissapointed though. Hefty and hard to use as it is at times, MSBuild/.csproj files are well documented, fairly flexible, and easily "google debuggable". The integrated JSON schema tools/syntax completion in VS (i've used them in the Data Factory tools) are decent, but lacking compared to the XML tools.
Not the mention project.json's big appeal can easily be applied to .csproj: &gt;For example, .csproj already support wildcards If only Visual Studio would start using that it would be great specially on those merge conflicts because VS decided to shuffle the same files around for no obvious reason.
Does this apply to you? https://blogs.msdn.microsoft.com/msgulfcommunity/2015/05/18/upgrade-to-visual-studio-premium-before-july-1-for-half-price-and-get-a-free-upgrade-to-vs-enterprise-2015/
Have you tried emailing Microsoft?
Damian and David are in, and a bunch of other MS peeps. No Hanselman though.
http://www.asp.net/mvc/overview/getting-started/introduction/getting-started
Great question. I'm in the same boat. I have been doing webforms for a small company as a solo developer for the past 2 years. I have talked to my boss about redoing our portal in MVC, more so I can have a good excuse to spend work time learning more. He is reluctant since we are doing just fine with webforms. Since I want to focus on something I cant decide if I should learn more advanced C# topics, MVC or learn more frameworks like AngularJS, EMber or Knockout.
Learn javascript and one of the top client-side frameworks such as ReactJS. WebAPI is still quite useful if you're committed to using the microsoft stack server-side, but I wouldn't use any MS stuff client side for a new web site.
The lol was in reference to how obvious it was - not that this was a joke. You want a job working in Microsoft technologies, start at Microsoft.
The SEV-1s are routed to the right person. One of the issues is that most are false positives or issues with the synthetics. For example, the PSTN fraud detection service (correctly) flagged one of the international calling synthetics because it detected it was making many short calls to exotic locations. Doesn't affect users, but on-call gets paged because someone screwed up the config, and the on-call then needs to drag an ops guy in to fix the config. Another one would be a hybrid customer screwed up their own configuration, but they opened an incident against us. I get paged, but the only thing I can do is diagnose the misconfiguration, and then sit on a conference call while customer support walks the customer through the resolution steps. There is also some distortion because there are only two severity levels. SEV-1 is a pager incident and SEV-2 is not. SEV-2s get put onto a backlog while SEV-1 gets looked at immediately. So if you want your issue to jump the backlog, you assign it a SEV-1 severity. Most SEV-1 issues would be classified as a SEV-2 or a SEV-2.5 at places like Amazon. (And you'd be terrified of using AWS if you could see the SEV-2 rate at Amazon). The rest of the incidents are usually automatically handled by built-in redundancy and automated fail-over. The most frequent SEV-1 incident my area deals with is "server 123 isn't responding. I'm going to automatically bypass it". The on-call needs to figure out why the server isn't responding and either pull the server completely or add it back in before going back to sleep. The tooling problems comes into play when figuring out why. All outages that impact real users also get posted to the service health dashboard, so you can look at the frequency. We actually almost always hit our SLA too, and we pay you back if we don't. It's just that all the "magic" that causes us to hit that SLA is actually a bunch of engineers having shitty on-call experiences that destroy work-life balance. If it helps assuage you, I've only seen 1 global outage in the time I've been there, and that was several years ago.
Well in case you were curious the interview wasn't at all what I expected. It wasn't even an interview really. I just sat down in a test room by myself at a computer and took multiple choice tests. I did pretty well on the .NET test. Above average at least, which I guess is good for a complete newbie. I'm pleased with that result. I did slightly below average on the test for the language I work in every day haha. I suppose part of it was just that I've been so intensely studying .NET and wasn't expecting a test on another language. But that test was crazy anyway. There were several questions about bitwise operators which I don't think I've ever seen anyone use in my life and I certainly haven't had a use for them. And there were a lot of questions on Array manipulation methods which I struggled with. Sounds surprising since I work with Arrays constantly, but honestly, I rarely do much manipulating to Arrays in code. Generally I generate Arrays from iterating database result sets that are already filtered, sorted, and paginated. So generally with Arrays I'm just passing them around or instantiating Objects with them more than anything. Very rarely do I have to do things like merge them, shift them, sort them, and remove specific items from them. They also gave me an MS SQL test which I completely bombed. But I also have never worked in MS SQL, only MySQL. I just know how to write queries well and I've created a few stored procedures and triggers in my day. That test seemed more like it was for a DBA. Lots of MS SQL specific things, and stuff involving deep knowledge of how to administer an MS SQL server. I'm optimistic though. I did really well on the .NET test! Thanks again for giving me some direction.
What he said. Forget about certifications. Start a couple of projects, understand the technologies, apply. 
I found this instructor on Youtube, he shares course snippets on there. He recently released a beginner's course for MVC 5 and I find his teaching style excellent. Check it out: https://www.udemy.com/the-complete-aspnet-mvc-5-course/ 
You mention ajax, we're you referring to asp.net ajax extensions? If so, I highly recommend you attempt a project using more Javascript libraries like JQuery/AngularJS. I feel like I was highly mislead for years with Web Development due to ASP.ney blurring the lines between server side and client side code. You write code behind and it's compiling to Javascript, or you're writing code behind and it actually is running server side. You add a control to the .aspx file that is actually being interpreted as a simple html element but in also includes a webresource file to manage that element. As a developer, debugging that crap can be awful and in real world situations you hear to many people say, "add js code to fix it after it renders." I like the direction MVC is attempting to make but out of the box MS needs to make better decisions, like loading JS Libraries before the view loads. 
Depends if you are making a website or a browser application. A website doesn't need the overhead of a client-side framework and ASP.NET/Razor is still a good fit. 
Gotcha, lots of info there...You sure you should be sharing all this???
If you are a silver or gold partner, those msdn licensed will become enterprise licenses. If you purchased your msdn licenses, then I don't know, it could be a mistake, however I was under the assumption that pro was going away because the pro edition became the community edition, so anyone that paid for pro would just get enterprise. 
The same thing happened to me. I haven't done anything about it yet though.
I'm a fan of .Net Reflector. It lets you decompile and debug assemblies without needing the source code. It's really useful for those situations where your code is encountering a bug below several layers of abstraction. Also Beyond Compare is a pretty good diff tool.
I would like to say I agree in the .NET reflector, would cast a vote for dotpeak, dotmemory, resharper, all things jet brains. 
Once you "get" MVC it is far less complicated than WebForms. The biggest problem with WebForms is that it takes standard internet things and abstracts then behind a crazy system to try and make everything act like a Windows application in the code. The problem is that the web is very different from windows and the abstraction ends up being a heavyweight burden. A simplistic comparison goes like this: .cs code behind -&gt; controller .cshtml -&gt; view .ashx -&gt; partial view Models serve to send data between the controller and the view, which doesn't really have an analog in WebForms. Working with "dynamic" pages is easier because you can use Razor and partial views to make temples for your data, where this normally needs to be done in the code-behind with WebForms. As a general statement, if you have HTML that is not in an MVC view/partial then you're doing it wrong.
It wasn't there today when I left work at 17:00 UTC. I'll check Monday...
Updated my RC1 project to RC2 right away. It was actually less painful than I had hoped, took about 5 hours of work from installs to successful compile. The CLI interface seems very well designed, and I'm glad that as painful as it might have been to get here, we are finally at a point where this thing is looking to be in great shape. I definitely feel that it's finally at a good point where I don't believe there is much risk looking at it to either build new projects or start thinking about migrating over. All in all, it seems to be an actual Release Candidate this time rather than an improperly labeled beta. Very curious to see how the .NET Core ecosystem grows now that the barrier to entry is lower than it's ever been. I find it doubtful that it will be the next "big thing" ala what MEAN is right now, but I think this is the start of .NET growing in a pretty large way. 
I haven't spent a lot of time on it yet but so far I've enjoyed the experience. One of the first things I've delved into was configuration sections (like in an app config) so that got me to understand how all the setup works better. Beyond the setup stuff, the rest is kind of similar to how it has always been. Has email been solved yet?
RestSharp has made API interaction a breeze. Inject IRestClient and all you have to do is specify the resource URI, method, and query string / request body. Fun stuff. I'll check out Patterns, that sounds very useful.
&gt; It was actually less painful than I had hoped Wait, what?
My problem is that I have issues figuring out which packages I need in order to get a specific feature that I used in the full .net
Why not use a scrum board?
&gt; scrum board Googling this now.
I personally use visual studio online and it's fantastic
According to this https://blogs.msdn.microsoft.com/webdev/2016/05/16/announcing-asp-net-core-rc2/ &gt; We are rolling the RC2 out to the Azure App Services and expect it to become available later this week. Keep an eye on the Azure Blog for an announcement when it is available. It's the last working day of the week for them now and I can't find the news on it, so might any time between now and a couple of weeks?
the MailMessage class isn't available anymore as far as I know
So send grid won't work?
I wrote a backend system from scratch with RC2 + EF7 on my iMac + Visual Studio Code. Everything runs so smooth although encountered some bugs in VS code and have to restart VS code occasionally. Now my backend system has been deployed to Ubuntu with PM2 + nginx running the ASP.NET console application. SQL Server is used and I am now trying to make another system with Postgres. (or if Microsoft would release Linux SQL Server sooner than this project starts)
Now that I figured out how to use NPM, webpack and react within MVC 5 / Web API 2, I have less desire to switch to .NET Core.
So I am currently working on a startup idea and I started it in RC1, this week it took about 4 hours to convert it to RC2, the longest part being EF7. Overall still really liking it. My production all once I'm done will be in .NET Core and Angular 2 w/ typescript. So pretty bleeding edge stuff and I love both :) Edit: Also user secrets are super cool. They aren't new to RC2 but man are they awesome. Check me out if you haven't yet. 
https://www.microsoft.com/net/download
&gt; send grid I have absolutely no idea
2 hours in and I still can't figure out how to build a core web project that references a portable class library project... I think I'm going to wait for the next visual studio release.
I wish Microsoft would push all their departments to release RC2 compatible versions really soon. They need to be on the forefront of this, but sadly, I soon discovered some Azure related libraries are lagging behind for example. The framework/platform stuff is far from obvious. It's very hard to figure out how it's all connected. netcoreapp1.0, netstandard1.5, Microsoft.NETCore.App etc, what the hell. What do I need in my web app project, in my library project, unit test project? There is almost zero guidance for that. Once it's setup and running, it's running really well though. Fast, stable. Good shit.
It's awesome. I love it. Join us!
That man was a wordsmith 
"Imports"
Just wondering how you've handled the architecture/routing of your project. Do you use static pages to serve up your angular app then handle all routing from there and only use MVC for an API? I'm starting a project with the same stack but I'm not sure on the best way to handle these aspects, I do rather like the tag helpers from Razor aswell as the server rendering but need the dynamics Angular
Personally, and I mean this is my opinion and you can take it or leave it, the purpose of onion architecture is that you define your domain models once in your core and use them throughout - including in your view models if you want. The only mapping that's necessary is when you're pulling in data from some external source (like a database). There are different situations that will call for different designs, but by and large I do not think it's a problem if your view model has some instances of a domain model. As for your view models, they should consume services, not the other way around. 
I would probably do that with D3. Might not be obvious at a first glance, but should be rather easy...
Seriously, writing a code generator isn't hard. The &lt;%..%&gt; template stuff is really easy: create a simple parser that takes &lt;% %&gt; blocks and uses its internals as code, and all other text is written out as text to the output stream. So if you have: Some text &lt;% for(int i=0;i&lt;10;i++) { %&gt; some more text &lt;% } %&gt; blabla you'll get: outputWriter.Write("Some text"); for(int i=0;i&lt;10;i++) { outputWriter.Write("some more text"); } outputWriter.Write("blabla"); you'll get the drill. Then you take that and compile it in-memory by adding it to a simple template you generate in-memory which forms a class. Call the instance in the compiled dll and you'll get output. Won't take much code, perhaps 200-300 lines tops. You can of course make this as feature rich as you want, but it's really this simple. 
I know that many DDD experts state that you should not expose your entities and keep a boundary within your services e.g. the infrastructure uses the domain services and entities, but it should expose the DTO instead of domain models. I was thinking about the similar approach like yours, as it would simplify a lot of things - just create the core entities and use them in services that will be injected into the ViewModels. I'll probably go with such solution as it removes the unnecessary code duplication required for DTO + ViewModel and seems to be a much more pragmatic approach.
I'm actually not using a visual studio project at all for my front end stuff. It's a completely disconnected angular app that calls the web api. I am not a big fan of mixing Razor with Angular as much as I love Razor. I actually contemplated a long time between a mix of mvc and angular but found it was just simpler and cleaner to have a separate front end project, of which I am coding in visual studio code.
I am definitely not an expert, but this approach has worked well for me and it "makes sense" in my head. I can't say for sure if it's the hammer for all cases, but to me it makes sense to have that approach.
For what it's worth, the last time I checked Swashbuckle was dependent on ApiExplorer, and from a quick glance at the GH project just now it appears to still be. I did the check on my phone, so I may very well have missed something, but that's what seems to be the case, at least. The main problem here is that ApiExplorer has a number of (well known) problems, and fails to properly explore even fairly standard APIs, causing people to resort to unstable hacks to get it working. In my most recent case, we aren't doing anything even remotely strange with the setup of our API project, but I eventually had to completely abandon ApiExplorer and build my own mechanism. Even when ApiExplorer works, it has these bugs, does not appear to be maintained, and involves an odd parallel object model which largely restricts reimplementation of its contracted behaviour and makes me wonder about its history, and in turn its future. I would against investing in a solution which depends on ApiExplorer.
I'll definitely give it a shot, thanks :).
I like JIRA
Uninstalling ASP.NET 5 RC1 was a pain. No headaches other than that. I've only written small test projects with RC1 and I won't bother to migrate them. I started new ones in RC2. No issues so far. :) Oh, and Azure web apps already support RC2 so that's helpful.
Does that work in Windows?
Thanks. I figured as much.
Less painful is still painful :) . What I meant is I thought it would take more than 8 hours. 
A good number of issues have been raised on SO and elsewhere and are easily found, so I'm not going to repeat them here. Many seem to appear with the transition to attributed routing, suggesting that the ApiExplorer implementation hasn't fully accounted for it. For our implementation, it couldn't (among other things) handle actions with constraining attributes, though my replacement for it had no trouble exploring the routes and documenting everything, and I did so using only the public interfaces to route information, so it is not as if I was doing anything sneaky. ;)
I've recently moved to native JS frameworks (Angluar) all the way. Strictly split client and server stuff. For the client I even use WebStorm (TypeScript) and for the server / web api part VS with Web Api 2. Mixing client and server stuff doesn't make sense to me anymore.
When you say it's not great what do you mean? Did you get it working and ran into some snags? 
Yep it sure does. Not needed in Linux / OSX (rimraf = rm -rf) because they don't have quite the same restrictions regarding path length.
wow I just noticed notice something when i go to my JSON [{"SectionId":1,"CodeNumber":129076,"Date":"\/Date(1420178400000)\/","UserId":6,"RankId":3,"Rank":null,"User":null}, {"SectionId":2,"CodeNumber":850314,"Date":"\/Date(1434344400000)\/","UserId":7,"RankId":3,"Rank":null,"User":null}, {"SectionId":3,"CodeNumber":176033,"Date":"\/Date(1438318800000)\/","UserId":7,"RankId":0,"Rank":null,"User":null}, {"SectionId":4,"CodeNumber":129629,"Date":"\/Date(1421820000000)\/","UserId":3,"RankId":0,"Rank":null,"User":null}, {"SectionId":5,"CodeNumber":773561,"Date":"\/Date(1424498400000)\/","UserId":9,"RankId":1,"Rank":null,"User":null}, {"SectionId":6,"CodeNumber":255253,"Date":"\/Date(1431493200000)\/","UserId":9,"RankId":0,"Rank":null,"User":null}, Noticed this: **"Rank":null,"User":null** SO it's not pulling any data from my other tables. and I changed the values in my homecontroller of : var samples = db.Samples.Include("User").Include("Rank").ToList(); i get the following ERROR: A circular reference was detected while serializing an object of type I saw this error right after i used EF and its generated code so I went to the DbContext and added **Configuration.ProxyCreationEnabled = false;** ( according to google) public partial class FireDBEntities : DbContext { public FireDBEntities() : base("name=FireDBEntities") { Configuration.ProxyCreationEnabled = false; } which allowed me to see the data of my associative table. Apparently the foreign keys are missing. is this VS2015 thing ? man I should use VS2013 ...stuck 3 days til today 
You might want to consider using different models for your database and your API. But until then, add the [JsonIgnore] attribute on the Samples properties in the User and Rank classes. That way, the User and Rank related properties on the Samples will be included in the JSON, but those related Users and Ranks won't serialize their list of Samples (i.e. a circular reference/infinite loop of serialization).
Does user or rank contain an fk column referencing the sample table included in your query? That's where your circular reference would come into play. If i recall correctly EF would build a sample object with a navigation property to the (e.g) rank object which would build a navigation property relationship back to the sample object and so on and so forth. What happens if you disable lazy loading?
I use [ui-router](http://angular-ui.github.io/ui-router/site/#/api/ui.router).
No jokes, come work at Microsoft. I got my job through a recruiter based out of Seattle. PM me, I will tell you about it
[You should read up on loading related entities](https://msdn.microsoft.com/en-us/data/jj574232.aspx). If that's the way you want to go. With a restful API, you generally don't want to load all related entities as it defeats the purpose. Also, I noticed you keep using table instead of object when talking about web API. While this may just be a slip, it's very important to make that distinction. Once you get to the web API level, you're dealing with objects, not tables. You can create a viewmodel that pulls from multiple tables and send that back and forth as an object. Somebody else mentioned that you may want to create a new object, this would be your view model. It's really up to you, this calls in the "chunky vs chatty" debate and what kind of application you're building, and if you need it to scale.
Basically `System.Net.Mail` is not yet implemented and it is advised to use `MailKit` for now.
How do you deal with SEO issues with that setup?
Hey sorry for the late reply! Regarding the ambiguous finish to my comments, I meant the experience of trying to get it working was not great. Also, in the end, I did not get it to work. There are couple of 'hacks' they actually espouse you have to use to get things working in the documentation along the way, and in the end, whenever I tried to reverse-engineer the database with EF it just crapped out. Perhaps I'm missing something, perhaps it's the fact that the newer release of Npgsql is still unstable, but the whole experience felt even buggier than RC1 to me. Having said all that, I was trying to use core in a very specific way; I'll just start learning it with a SQL database and just wait for the tooling to catch up so I can get my hands on using postgres with it. 
Not too many people are going to help you because you've posted very little explanation and a lot of code for us to read; you've given anybody that would help you a lot of homework to do first. Also, if you're going to use screenshots, host them on stack overflow's servers. Nothing worse than coming across some question/answer in the future and having dead links. Post a minimal, complete, verifiable, example. 
Try [ScriptIgnore] instead of [JsonIgnore] since you might be using the built-in serializer instead of Json.NET. And yes, you're using MVC, not Web API. Your controller should extend ApiController if you want Web API, which is probably a good idea if you are creating a REST/angular SPA.
There are a lot of tutorials on the web that show how to handle one-to-many relationships with MVC + EF just a quick Google away, is there a reason none of those are suitable? Are you just wanting somebody to write it for you?
&gt; Are you just wanting somebody to write it for you? Pretty much this. I've voted to close his SO post, and reported this reddit post as spam.
I'd like to. I've had OmniSharp working before and that does help, but it's not enough. It doesn't do any manipulation of .csproj/.sln files. 
If you're looking to learn, chances are you won't find very many resources at all about the brand new release of [VNext/.Net Core](http://www.asp.net/core) (ASP.NET 5) just due to how new it is, and it only recently in May they [released RC2](https://blogs.msdn.microsoft.com/dotnet/2016/05/06/net-core-rc2-improvements-schedule-and-roadmap/) which they promised that any projects made with these tools shouldn't break unless they change something critical. Currently I just started a two days ago and used this to help understand the basics; https://docs.asp.net/en/latest/tutorials/index.html However Microsoft seems to already have quite a bit about it already written with most of the basics; https://docs.asp.net/en/latest/ 
I'd like that too. Contrary to someone thoughts who have likely never used the tool, Vim is indeed a good tool for development, fast and reliable when you get used to it. It's such a pity that we have too many "civilized coders" here. =/
Having a mock data set to suit the needs of unit tests isn't unusual, and is indeed necessary to make sure that data manipulation is occurring correctly. Assuming you are using EF, the easiest way to do this is to use dependency injection to inject your mocked context. Your `new MyService(new UnitOfWork());`should probably use IoC too - you should sort this out, you don't want to be calling constructors anywhere. On a final note, if you *are* using EF, `IUnitOfWork` and repositories in front of your EF entities is not the best thing to be doing - you lose all the benefits of using EF and the amount of code you need to maintain is enormous. EF already implements these things - `DbContext.TableName` is your repository and your unit of work is commited with `DbContext.SaveChanges()`.
How about React and .NET?
Right on. I have been working on trying to get EF core working on postgres using npgsql. It took some searching, but I was able to get it going. The key for me has been typing into the package manager: Install-Package Npgsql.EntityFrameworkCore.PostgreSQL -Pre
So I should figure out if the user is authenticated in the master layout page then set the authenticated user context right? How do I check authentication in .net core?
Moq + Effort (mocks EF contexts and lets you fill stub data)
I prefer [NSubstitute](http://nsubstitute.github.io/), myself.
Please keep the project.json. XML and XAML configuration looks fucking disgusting. Moreover, why are people freaking out over comments? If there is a bug with a NuGet package, make a note in the README? I have been using .json in node for the last 2 years and have had no problems. It's just easier to look at. Moreover, people are complaining about having to make an additional document to note comments in the package.json. Why not just put it in the README? Why is that so hard or scary?
Looks neat, I will have to check it out. 
If you already know C# this is the book you need to master, it's the best in class at the moment; http://www.amazon.com/Adaptive-Code-via-principles-Developer/dp/0735683204/ref=sr_1_1?ie=UTF8&amp;qid=1463992776&amp;sr=8-1&amp;keywords=adaptive+code+via+c%23 Other than that Pluralsight will be a good option. I know you're looking for mostly free.. unfortunately for the most part (not the newer things like .net core) most of it won't come for free. Best of luck. 
Spreadsheet for mvc: http://mvc.syncfusion.com/demos/web/spreadsheet/default
&gt; code2.Text = code.text; `Text` must start with an uppercase T.
hold on i'll make edits
* [EPPlus](http://epplus.codeplex.com/) for XLSX generation (for reports and stuff) * [MVVMCross](https://github.com/MvvmCross/MvvmCross) for crossplatform apps (in combination with Xamarin) * [CommandLineParser](https://commandline.codeplex.com/) for, well, parsing command line arguments * Besides this, the standard Microsoft stuff, like MVC, WebAPI, Entity Framework, Unity etc. etc. Edit: Oh yeah, if you're using SQL Server as database, use [One-Click SQL Restore](http://sqlbackupandftp.com/restore/). Awesome and very simple tool to restore backups. If I'm using MySQL as database, I always use the community version of [SqlYOG](https://github.com/webyog/sqlyog-community/wiki/Downloads).
Top menu bar -&gt; View -&gt; Other Windows -&gt; Entity Data Model Browser It works if Entity Model is opened in current VS tab
Been a while since I've played with Web Forms, but I'd take a look at using different points in the page lifecycle. For example, if you put that line in Page_Load, does that help?
Have you run it in debug mode and checked whether the event is firing? If not are you putting the buttons in a dynamically instantiated panel/container?
Would you mind putting yours on github for everyone else to see? I'll be writing a booking system soon and would appreciate to see some ideas :)
No reason why? Seems like a step back to me.
Here you go bro: https://www.reddit.com/r/webdev/comments/4kquof/how_i_got_aspnet_core_rc2_with_entity_framework/
This isn't real content - it's marketing. Not very good at that.
As far as I can tell, the context menu is on the grid level. Once you invoke the context menu on a grid row you may have access to the DataGrid.Selecteditem which is your view model that has the Hash property you want to bind to. Maybe something along the lines of: DataContext={Binding ElementName="myGrid", Path=SelectedItem} or use RelativeSource bindings
ContextMenus are a pain with data binding. Context Menus are a separate window so you the binding is janky. Look up "Context Menu Proxy DataBinding" on StackOverflow. I'm on mobile, otherwise I'd pull it for you. If you still haven't gotten it, I'll whip up some code for you tomorrow. 
I'm working on updating it to RC2. Should be done in a week or two. /cc /u/lurkingforawhile
The article is a bit poor in explaining the reasons why they are backpedaling. But honestly if VS starts using wildcards and they add NuGet support to MSBuild csproj files would have everything project.json had.
Thanks dude, I'll have a run through of this over the weekend when I have some time :)
You better should test the abstract factory and the factory methods.
If you do not know the outcome, you cannot test it. at least that is my understanding of unit testing. 
They must really proud of what they have achieved with the current project file. Only Microsoft can rewrite their mobile platform few times but not touch a single project file. 
You could refactor the code so you have an extra method creates and returns your factory object. Then you could make a unittest to assert the factory is created successfully.
I think a faction in Microsoft has probably been wanting to redo the current msbuild project files for a while now. It makes sense, the file format is pretty ugly and it's pretty awful working with them in a shared version control environment. Not necessarily sure that replacing XML with JSON is a fantastic choice, but I'm not sure it's a bad one either. Don't really know a better option either.
Yeah, totally agree with you. I was just trying to figure out based on a constructor what he should possibly try. :)
What boggles my mind was their theory that command line tools could edit JSON files but somehow not be able to edit XML files. I'd like to dope slap the genius who thought that one up.
Isn't JSON used for configuration in a lot of different front end libraries? I mean, XML isn't a configuration language either, at least it wasn't written with that in mind. It was written as a data serialization format as well and has been coerced into being a 'catch-all' format for describing anything we need to parse. In either case, I don't know if I like JSON for configuration either but I'm not convinced XML is better.
I don't know why I completely forgot XML is a X-Markup-Language. I agreed that JSON was lacking but I'm still not a huge fan of XML either. It gets very complicated to read/write when you have to manually edit some .csproj files. But that could just be due to the extreme amount of information that is stored in .csproj files. Anyhow, I think I've been convinced that XML *is* a better choice then JSON.
I would just prefer JSON5 or TOML over XML. Especially TOML is a great format for configurations.
&gt; What I'm waiting for is people to figure out how horrible JSON files are to merge. You mean like.. trailing commas?
http://up-for-grabs.net/#/
&gt; Isn't JSON used for configuration in a lot of different front end libraries? Yes. And it often sucks. Lack of schema/comments/etc.
I've had to do this before. Check out this link https://msdn.microsoft.com/en-us/library/hh925568(v=vs.110).aspx
Seems like a lot of hassle for something that seems so simple. I might just scrap the ContextMenu and go with another command that doesn't need such spaghetti code to work.
I'm unsure how to do that exactly. [This is what I tried.](https://gist.github.com/diggerton/41890182438e72e7cd18ee681dfba4e9)
Split the constructor into a second ConstructorOfClass(String connectionString, String HubName) {} Then test (verify) that the privateClient is what you expect it to be. Edit: actually create a separate method public static HubClient CreateHubClient(String connectionString, String HubName) and test that, then leave the constructor privateClient = CreateHubClient( CloudConfigurationManager.GetSetting("SomeConnectionString"), CloudConfigurationManager.GetSetting("SomeHubName")); Alone
Thank you!
If we get to pick any format, I'd go with YAML.
I know this is an old post, but I'll try. How did you get EF working in Linux? My connections fail. If you get me connecting, I promise to play with DateTime and see if I can help you.
I have never used docker before. I'm curious how you use it. I don't have docker installed so I'm not sure the commands you listed would work. As far as regarding using insecure things. I find ftp to be nice and easy for development purposes. This article is for users getting started. Thanks for the advice about using scp (secure copy, I never heard of it until your post).
How experienced are you? 
GitExtensions is easy enough to work on if you are familiar with git
I've been doing dotnet development for 6 years or so, but generally with older techs. I've done some MVC work, but very limited single page app experience. 
When you write a comment, there's a note below the text field that reads "Please remain civil and on-topic". You got it half right.
I think I am familiar with that method. Sounds like Gherkin language. Thanks for reminding me. This could greatly help me in doing unit tests.
Google for an online C# to VB converter. They work fairly well. Then stop using VB and switch to C# when you have a choice because VB pretty much sucks and is no longer the norm.
Lucene.net could use contributors.
I have to disagree with a lot of the comments. If you're testing the constructor, then the unit you are testing is *initialization* and nothing else. Test that the object is not null and that any exposed properties have the values that you expect them to have after instantiating the object with that particular ctor. I may have also verified that those variables are actually getting returned by the cloud service and throwing if they're null if that means it would leave the object in a bad state. Throw if they're null, for example. By doing so, you'll find that you can't test them as being null vs. not being null (can't check for the exception if you can't control whether or not the cloud config is returning null in the test, which meaaaannns... those variables should be part of the ctor parameters. So again, you should test initialization and every variant of it.
Disagree. Those variables are dependencies of the class, so the class should be tested to ensure that it the object is not in a bad state, or throw to prevent the object from ever being used in a bad state. You're not testing configuration; you're testing the class's ability to handle possibly invalid values from the configuration.
I'd like to expand on this to say that if you *can* determine what the outcome should be, then you have a test case. Using xUnit, you can also test the same basic logic with multiple inputs to make sure that the first test wasn't a fluke. For instance maybe 1+1=2, but somehow 5+1=20; you'd probably wanna know about that. If there are multiple possible outcomes due to additional logic in the function (e.g. conditional logic, exceptions, null values, etc.), then those conditions should also be tested. Each variant is a single unit. For example: /u/Arcusremiel08 public void Add_SameType_IsCorrect() public void Add_IsNull_Throws() // check for argument null exception public void Add_DifferentType_Throws() // check for invalid operation exception or more to the point: public void Ctor_NullConnectionString_Throws() // check for argument null exception public void Ctor_NullHubName_Throws() // check for argument null exception public void Ctor_SunnyDay_Initializes() // check that the object is not null and is in an expected state For what it's worth, this is also added reason to keep your methods small and focused/cohesive (see: the 'S' in SOLID)
By testing the CloudConfigurationManager to make sure it's returning a value that you expect is actually a test on the CloudConfigurationManager, which is a different unit. That would be inappropriate here. 
true, if you wanted to address the 'long line smell' you could do var config = CloudConfigurationManager.GetSetting; privateClient = CreateHubClient(config("SomeConnectionString"),config("SomeHubName"));
Yeah, just your basic given/when/then style. It's generally pretty useful regardless of what specific test tooling you utilize, from right down at the level of units tests all the way up to end user acceptance criteria. Helps keep the focus on observable behaviour instead of getting sucked too far into implementation details or "testing the mocks" or "testing the test," etc.
Clarity consulting is a pretty good firm. I worked for them a few years ago. 
Avanade would be a pretty good place as well. Work closely with Microsoft and I know our solution developers that were just out of college were working with ASP.NET in training.
&gt;Why not just put it in the README? Why is that so hard or scary? consider you are reading a project.json, you wonder *why is it `1.19` instead of `1.20` on line 16*. Then you look into readme *which is 10 pages long* &gt; 1.19 is used instead of `1.20` on **line 8** because ... see the problem? and that's just one area of problem. the context switch between project.json and readme along with mapping of docs to code is very error prone
If you want to know the ballpark pricing on proprietary products linked here before reading all about them like myself, here ya go - 1 dev, 1 year, $1,000
You might have a look at this discussion: https://www.reddit.com/r/csharp/comments/4kpt49/whats_the_current_state_of_play_with_di_frameworks/
These need some help * https://github.com/JimBobSquarePants/ImageProcessor#imageprocessor-needs-your-help * https://github.com/refactorthis/GraphDiff#graphdiff And there is this: http://www.hanselman.com/blog/RFCServersideImageAndGraphicsProcessingWithNETCoreAndASPNET5.aspx 
In what way is this an improvement over var result = await Task.Run(work); uiUpdate(result); It screws up exception handling and is less convenient to use. It uses `ContinueWith` and unwraps the task manually, so I would assume the author simply isn't aware of the existence of `async`/`await`, but the "helper" method uses `await` *and* `ContinueWith` for no apparent reason.
your*
Good catch. We couldn't see the forest for the trees. Opened an issue, tackle it soon.
And, as usual, you're still responding like an asshat even when someone gives you an exact answer. Get over yourself.
I expect that thread is stale since no one mentioned registration dependency configuration and no code example there used it. But by all means keep on shoveling instead of digging. It's doing you great so far. /s
Sure, testing that construction of the object works fine, assuming that there are expected outputs for different inputs. Testing that the configuration manager returns the appropriate values is not appropriate and is what I was trying to advocate against. &gt; Assert that the cloud setting is returning what you expect Hope that clarifies where I was coming from. 
Dude, seriously. I'm using it in production. You really need to step back and look at the way you conduct yourself here. You regularly shit on anyone who doesn't reply exactly the way you want, and you've been doing it for years. Some of your recent topics and comments related to your ORM work have been better than usual, but not by much, to be honest. Probably worth a re-read and some consideration of what else might have been going on in your life then vs now, in case there's something you can recognize as influencing your behavior. It happens to me from time to time (disproportionately on Reddit, for some reason, probably by caring enough to reply but without enough time to reply _well_.) perhaps something similar may happen to you too. Story time. My clearest early memory of you goes back a few years, to when you were shitting on frameworks and people who used them. I suspected then that you either were exposed only to trivial problems where frameworks are unnecessary and inappropriate (and, to be clear, that's great work when you can get it!) or you had exposure to non-trivial problems but had issues researching them and seeking help and probably came to dislike frameworks and their proponents as a result. The time since has repeatedly reinforced this suspicion, particularly where related to seeking help as that is the most directly observable. Now here you are today, encountering a non-trivial problem, looking at how it might be solved in the context of a framework, clearly having trouble researching it on your own, and frankly unable to productively seek help. I guess my hunch was right back then. For your sake, I truly do hope that you can find a way to improve things. Good day, sir. I will be contributing no further.
I've been looking for something like this. Thanks.
I can see that if you move into the injection route. I guess I definitely would refactor the method to make it more testable first then. However, I stand by my stance that this is more of a gray area for testing, and question the utility of a test around this. How much safety does a test add around it? That's the gold standard for me, and I don't follow the "test all the things" mentality anymore. Verifying behavior of a method is an easy win. Verifying that you can construct the object correctly, when the implied contract is that you will pass me my dependencies in a usable manner? A lot less so in my opinion...
I turned off ReSharper this week solely because I hate how pokey it makes VS feel. Even on my brand-new Skylake desktop quadcore i7 with 32 gigs of RAM. I've supplemented it with [Productivity Power Tools](https://visualstudiogallery.msdn.microsoft.com/d0d33361-18e2-46c0-8ff2-4adea1e34fef) and [Refactoring Essentials for Visual Studio](https://visualstudiogallery.msdn.microsoft.com/68c1575b-e0bf-420d-a94b-1b0f4bcdcbcc) and I really love how snappy VS feels. The _one_ thing I really really miss and haven't found a replacement for is how you can have R# apply your code style on auto-complete? Which basically makes everything snap into nicely-formatted line as I'm typing it. I really love that feature, if I could find a replacement for that I'd never be tempted to install R# again.
The issue with merging on the XML config is the fact that the files and projects are all listed in the solution and project files as is every build config. If two developers add a file to the same project or two projects to the same solution you have a merge event. If you lock on checkout two developers simply can't do that.
I've got two words for you - Code Rush
Also a large investment.
Thanks
I'm not sure if this does all the same stuff but you can set your code formatting preferences in vs and then just hit ctrl+k, ctrl+d to format. It will reformat existing code to your style also.
I start Kestrel programmatically, but I still pass the --server.urls and it works fine for me: var config = new ConfigurationBuilder() .AddCommandLine(new[] { "--server.urls", "http://localhost:8111" }); var builder = new WebHostBuilder() .UseConfiguration(config.Build()) .UseStartup(typeof(Startup)) .UseServer("Microsoft.AspNetCore.Server.Kestrel"); using (var app = builder.Build()) { bla bla } Edit: https://github.com/stofte/linq-editor/query has some example code/configs
There's a format on save option, that's as close as I think you can get.
What kind of desktop are you running? 
I usually find that R# works fine, but if you're refactoring or something and you have a lot of errors, it starts to lose it. 
I got myself a [Skull Canyon NUC](http://www.intel.com/content/www/us/en/nuc/change-the-game-with-nuc.html), put in 32 gigs RAM and a [NVMe SSD drive](http://www.newegg.com/Product/Product.aspx?Item=N82E16820147467). I can't even begin to tell you how awesome this machine is, everything I do on it is IMMEDIATE. And being a NUC, at the end of the day I unplug it and throw it in backpack. :)
Thanks, I'm aware of the various key combos for VS, R#, etc., but for some reason it's just so much nicer when it just happens. Typing that final semicolon suddenly becomes super-satisfying when it triggers an auto-format. :)
That was one theory we were going to try. Any idea how to get this property checked in the middle of a drag drop?
I was going to mention the ctrl k, d, but glad /u/jstillwell already did. In regards to auto-format, in Options -&gt; Text Editor -&gt; C# -&gt; Formatting there's a checkbox for "Automatically format statement on ;" Does that not do what you want it do?
There also appears to be a memory leak in Visual Studio 2015 Update 2 which causes Roslyn to keep using more and more memory. It may be related because the more memory is used, the more the GC runs, the more unresponsive Visual Studio becomes.
That could very well be it.
Ah yes, but that only applies the (comparatively) few rules that VS has. Apologies, I've been inaccurately describing what I find to be missing. What I really want is for some of these other nice formatting tools which let you define thorough formatting rules (Code Rush, JustCode, CodeMaid) apply _their_ formatting rules on those occasions. With all the tools I've experienced, they only apply their rules either a) when they generate code for you or b) when you invoke some command.
Have you installed the cumulative [update to Update 2](https://msdn.microsoft.com/en-us/library/mt695655.aspx)?
We should all just start contributing to freesharper. 
&gt;Maybe change tracking could be turned off for search screens though. At the end of the day I doubt the CUD part of CRUD would work with it turned off. So that would be a special case. It's very dependent on your scenario. For CUD change tracking is not needed if you don't have strict requirements on data integrity with concurrent access. Some interesting database model approaches don't need change tracking at all because these approaches are self-correcting. These approaches doesn't require even a operation transaction. I use in one of my projects (it's a custom HR personnel management system, if it make sense here) INSERT-only approach. Well, strictly speaking it have some small amount of UPDATE-s. In SQL database there is common "history mark" table, in this table alongside auto increment bigint ID field also included information about logged on (or impersonated for test purposes) user, time stamp and some flags, one of these flags is "complete" flag. Any update to data in other sql tables is done via insert with history mark ID, history mark is shared with other updates which must be done together. After data is "updated" (technically inserted), then in "history mark" table "complete" flag is setted to "true", so any marked with this ID updates become "visible" to readers as all read queries have check on "complete" flag in corresponding updates and history mark is simply a "snapshot" mark. So, I have complete history for data audit, all "updates" are essentially inserts and "update" with last history ID wins, if some data in different tables must be updated at once for data integrity purposes, they just share same history mark ID and all that updated data become "visible" to read at once when single "complete" flag is set on history mark. In this scheme data integrity (alongside with full data history needed for audit) is guaranteed without change tracking on entities.
Dude, that smacks of effort. j/k! I'd totally contribute to that. Is there such a project? I just gurgled it and don't see anything in the way of an obvious concerted effort.
yes for the love of god make it stop sometimes my laptop freezes for 30 minutes. IT is baffled by it, but when we disable resharper i have absolutely no performance problems. Other people in the office who use resharper have the same problems
That's pretty much the definition of a unit test. A constructor is a method. You should test that every path within the method is producing expected results. The point is that if the constructor allows you to create an object that is in a bad state, then the constructor failed to do it's job correctly. That's exactly the same thing that we test for in other methods. Pretty straight forward stuff...
Yeah, see that's where I disagree. Code coverage is not a good metric for utility / usefulness of the effort. I think it's very easy to fall into that trap when you first start doing unit testing, and you end up with a lot of brittle tests that don't add enough value to justify the breadth of a lot of the tests. And I used to be a big proponent of it, but have since swung to a more nuanced / middle ground stance on unit testing.
Strange, I don't see those performance problems. However, I'm still on VS2013.
Just curious - why did you go this route instead of building your own mini? Portability? 
This
At the beginning of the ConfigureServices method: public void ConfigureServices(IServiceCollection services) { // Add framework services. services.AddMvc(); services.AddEntityFramework() .AddSqlServer() .AddDbContext&lt;Models.MyDatabaseModelNamespace.MyProjectContext&gt;(options =&gt; { options.UseSqlServer(Configuration["ConnectionStrings:MyProject"]); } ); And in one of the appsettings files: { "ConnectionStrings": { "MyProject": "Server=sqlServerName;Database=MyProjectsDatabase;MultipleActiveResultSets=False;App=EntityFramework;User Id=MyUser;Password=MyPassword;" } } Do you have it laid out about like that, with the .AddSqlServer().AddDbContext thing?
Sure I have. You yourself point at simple getters and setters not needing testing because that wouldn't have much value. I simply draw the line on how much value is added by a test higher up the stack than you. As I said I'd probably inject the hydrated dependency instead and throw on null. I would not consider a test around that necessary, or that having one would be worth the overhead of having to maintain it everytime the contract changed there, like adding a second dependency. This conversation is so funny. You can literally go back in my post history and see me having the same opinion on the matter you seem to with others that I disagreed vehemently with only a few years ago. It's a judgement call where you draw that line. The end result is you have to ask the question of how much do you buy with full test coverage at the cost of test maintenance, which becomes non negligible with non trivial projects. In time I found that it was more important to get tests around high value targets rather than everything. It was a better use of time, at least for line of business apps. Now if you're writing for the air force, etc., I would probably go the other way, as the risk / value equation balances a different way. You don't have to agree, its just a different opinion on where value can be added and at what cost.
I think that came off sharper than I meant it to. Sorry bout that. &gt; In this particular case, I'd probably opt for INJECTING the EventHubClient dependency if it's possible, and move the configuration up to the calling code or an IoC container if you're using one. So I think I must have missed this part, but definitely the better solution here imo. That way the validation can be delegated back to the client where it belongs, and you can remove that stuff from this object completely, thus negating the need for the test. 
Yup, definitely did the same. But now my company has gone with a required sonar cube plug in for quality control. It's has less features but more code quality control. Still figuring out if it's a bottle kneck.
Yep, but that's a Productivity Power Tools feature. 
Still waiting for the Roslyn version to be finished.
Yes. I disabled it months ago because it made VS dog slow. I now only enable it on special occasions when I need specific functionality.
There is no shortage of books if you really want to learn! 1) Beginning ASP.NET 4 in C# and VB by Imar Spaanjaars. 2) Professional ASP.NET 4 in C# and VB by Bill Evjen, Scott Hanselman and Devin Rader and ofcourse only books wont help. remember Reading is one thing; doing is another. so whatevere you read try to implement.
Is there an alternative for Ctrl+T combination, so I can remove Resgharper without pain?
That sounds really cool! I'm so getting one of these when I can afford it.
Same here. I'm so used to CTRL T, and ALT \ I don't know what I'd do without it.
As much as I love the functionality in the JetBrains tools, the performance hit is so bad across the board that I just can't use it.
Cool, thanks for the info. What did you spend, all told? My employer buys up laptops every couple of years, and I immediately dock them and never undock. I think this would be a neat change. 
&gt; These kinds of packages exist for other languages and frameworks and are extremely useful. Give examples of those packages in other languages and someone will say "ah, yes it's xxxx in .net"
Laravel Socialite.
It says “aspx”, so it's obviously relevant for .NET . /s
Can the NUC drive 3 monitors at once without an external GPU?
I did, but after moving to RC 2 I changed it to services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt; options.UseSqlServer(Configuration["Data:DefaultConnection:ConnectionString"])); per the migration notes.
IIRC, the first part of the book Professional ASP.NET MVC walks you through implementing http://www.nerddinner.com .
I'd take a look at [Scott Allen's ASP.NET Core 1.0](https://app.pluralsight.com/library/courses/aspdotnet-core-1-0-fundamentals/table-of-contents) course, the 'Core' approach to web development has changed significantly I believe (though I am NOT an experienced C# web developer), and I think it may provide the more bare bones experience you're looking for.
thanks for the tips guys! I'm using the correct .NET version, will double check the app mode. as I'm working with a webapplication hosted inside a Dynamics CRM 2011 WebSite, I'm thinking theres some rule that is still being used from the parent.
Well done
[SDD](http://www.newegg.com/Product/Product.aspx?Item=N82E16820147467) - $317 [Memory](http://www.newegg.com/Product/Product.aspx?Item=N82E16820232150) - $134 [NUC](http://www.newegg.com/Product/Product.aspx?Item=N82E16856102166) - $650 = $1,101
According to [this](http://www.pcworld.com/article/3045374/hardware/hands-on-with-intels-skull-canyon-nuc-the-most-powerful-game-ready-mini-pc.html): &gt; ... Intel says you can drive up to three 4K UltraHD monitors at up to 60Hz. 
Is the target system a Windows Server 2008 (not R2)? If yes, install the following: * [Extensionless Request Handler](https://www.microsoft.com/en-us/download/details.aspx?id=6015) * [Windows Management Framework Core (PowerShell 2.0)](https://www.microsoft.com/en-us/download/details.aspx?id=20430) * Restart * [ASP.net MVC 4 Standalone Installer](https://www.microsoft.com/en-us/download/details.aspx?id=30683)
I can see how your specific scenario would work without change tracking. In my case I definitely need concurrency control over individual records. I don't have histories for records unless they are needed by the business. Doing that as a general pattern would create a lot of unnecessary data and a more complicated design in my case.. 
I'm afraid I don't know, then. :(
To speed up EF you should follow these hints: 1. Use "timestamp" (aka "rowversion") SQL table columns and mark such columns with "fixed" concurrency mode in entity model. It greatly speed up concurrency control. Rowversion columns are primarily used by EF for efficient concurrency control. 2. Do not reuse entity context. Create new entity context for each transaction. Detach entities from context if you don't need them anymore. More entities are attached to entity context - more performance penalty you have for each update and insert. It's very critical for massive inserts. If you are doing essentially inserts in transaction without subsequent updates to same entities - turn concurrency control off. Better way is to not reuse entity context and absolutely avoid concurrency control in a massive inserts scenario. Performance penalty may be around 100x-300x for really big amount of inserts if concurrency control is turned on. It's safe to turn concurrency control off if you're doing **only** inserts and reads within transaction. 3. Use raw SQL if you need massive uniform updates or deletes based on well defined condition. For example, if you need to clean up "stale" entries from cache and delete all rows which have their "date and time" older than 1 hour - don't use entities. Just execute "DELETE FROM cache WHERE cache_time &lt; @dt" and it will be incomparably faster. 4. Always debug complex SQL queries generated by entity LINQ queries in SQL query plan window. You will see what indexes you should create or how you can optimize your LINQ query. 5. Feel free to use anonymous classes or custom named classes in read-only queries for query results. It's much better to query for 2 columns if you're really need exactly 2 columns than query for full entity. 6. Use indexes with "included" columns if you query for subset of table columns often. "Key lookup" operation in SQL have somewhat high cost. If you're querying on non-inclusive index, then index search will be followed with nasty "key lookup" query plan operation which will retrieve every full row by primary key, even if you need only small subset of columns. Use inclusive indexes if you're doing aggregate computations (such as MAX or MIN), include aggregated columns in index. 7. Complex queries with many joins tends to be more speedy if you split query into two parts. In one part (in which you have all joins and complex filtering) select anonymous class set with only primary keys. In second part join entity tables by selected primary keys. It looks like obscure magic but MS SQL Server tends to execute such queries significantly faster. Anyway it's not an universal rule, it runs faster more often, not always. 
Interesting, I use a Macbook and work full time on Mono. I've got a couple of tutorials on my [blog](http://coderscoffeehouse.com/) that show how to build and deploy a website to a Linux production server - I patched the pieces together because I found so little out there!
SignalR would be pretty painless for a chat. It basically makes it very easy to send requests to &amp; from the client to the server. You could utilize groups as chatrooms.
I wish RemObjects tools were more popular on Reddit. They are high quality and the company is very responsive. More developers should check them out.
I've been running with it turned off for a week or so. Doesn't really help. Today I finally broke down and uninstalled it and then realized just how much of a pig R# is. So I cancelled my subscription.
1. Because a lot of things still use it. 2. In many ways it is better than REST/JSON, especially when you take into consideration the WS-* stack. 3. Because technology doesn't just disappear as soon as you find a new toy.
I am running OSX on VMware Workstation on my SP4. If you use unlocker for VMware, it will allow you to install OSX, and it even supports VMware Tools (file sharing, etc). Doesn't run as nicely as a real Mac though. Anyway, got here actually looking for an answer on OSX on HyperV. 
As stated signalr is good. But not included in rc2. Bootstrap is for theming a site. Nothing to do with ajax
Man Microsoft is working at an epic pace these days. I am just trying to keep up with ASP.Net at the moment, as I am a full stack developer. My last project was straight ASP.Net MVC 5 EF6 type stuff. I am currently working on a project using .Net Core RC2 (targetting 4.6.1, not cross-platform), EF Core 1.0.0-rc2-final, and pretty much else that is not production-ready stuff. I'm even dealing with the whole new ecosystem in VS for the client tools. Meaning I'm bringing in Bootstrap 4 alpha and configuring bower, grunt, npm, and all the rest, when it was just a "bundle". It's wild how fast they are changing all this, on all fronts.
hi
Can someone clarify for me: Can one containerize windows itself now, or is it still just Docker on windows?
Oh hi Ben! Love your work on Age of Ascent! But especially on Kestrel! :-D
Since the machine isnt directly accessible from my pc, i cant try it right away. I think i've tried sr-Cyrl-RS and it yielded same result (not sure about sr-Latn because i have maybe misspelled it to sr-Latin-RS). I will try tho again. Thanks
Yes they built support for docker container like technology but for the windows kernel. You use mostly the same docker commands but you can containerize the windows OS and applications. 
&gt; Man Microsoft is working at an epic pace these days. I imagine they have some really excited developers pushing hard to get this stuff done.
Are you aware of what limitations it has? IE do the containers have access to the full graphics API?
They are headless. I am sure you can access the graphics card for computing or rendering but no you can't spawn a new desktop window. 
Yeah headless should be fine so long as I can still render. Thanks for the info!
/r/forhire 
This reads to me like something isn't being serialized when it's coming from / going to the sever. Without looking at the code, it appears a .Where() is being done on a select list, but either .ToList() isn't being called on it, or something along those lines. Check this article out: http://www.dotnetcurry.com/ShowArticle.aspx?ID=466
Hard to tell what's going on without seeing the responsible code, but the parameter doesn't actually contain a LINQ object, just the name of a type. The "\`2" denotes that the class `WhereSelectListIterator` has two generic type parameters, it has no important function. My guess is that someone just did the logical equivalent of this: List&lt;string&gt; list = new List&lt;string&gt;() { ... }; IEnumerable&lt;SelectListItem&gt; items = list.Where(str =&gt; SomeCondition(str)).Select(str =&gt; GetSelectListItem(str)); // probably forgot to call .First() or something at this point string urlParameter = items.ToString(); // this will just return the name of the type because there is no other useful conversion
It just doesn't work.
Greate article. I'll need to play around with it this weekend.
Ahhh so you think its a case of an error in generating that dynamic parameter, rather than them intentionally passing linq data types to existing code?
That would be my assumption. There isn't really anything useful you could do with that value.
I've only had it for 1.5 weeks, and it feels like I'm not pushing it very much. Note per an earlier comment, I *did* uninstall ReSharper. I should put a game on here... Occasionally I can hear the fan go up a notch or two. I saw in an interview with some Intel folks saying they put a good bit of effort into making sure the cooling was good enough. I *think* this might be the first full-powered i7 quad-core NUC, and if you look around on Intel's pages they really push its capabilities for "content creation" and "heavy workloads". Comparing to your crappy Dell, right off the bat I'd say one advantage of the NUC is that there isn't much "in there" to get hot anyway. The CPU airflow seems in-and-out pretty directly. For some more info: - [Link to Intel's spec sheet for the CPU in this thing](http://ark.intel.com/products/93341/Intel-Core-i7-6770HQ-Processor-6M-Cache-up-to-3_50-GHz) - [Link to my Geekbench benchmark on it](http://browser.primatelabs.com/geekbench3/6637266)
Not necessarily. .NET 4.5 is an in-place upgrade for .NET 4.0; no changes should be needed other the the targetFramework setting within your web/app.config.
A game would be a proper trial. By crappy Dell basically just has a single fan trying to get air across the power supply and CPU. If I run it at 100% for an extended period the CPU reaches 90C and it freezes. Before my medium-sized crappy Dell, my boss ordered a mini-sized crappy Dell. Also an i7. I tiny bit taller but less wider than the Skull Canyon. Passed it straight away to someone in another department since there was no way to put a second SSD in it. Boss has this crazy idea to only order i7s so it wouldn't be the only i7 going to waste doing Outlook all day. On the other hand if you push those Dells they can't keep the i7s cool anyway, not for long at least. I bet Intel did a better job. 
thanks! 
In my inbox. :)
Port to .NET Core RC1/RC2/RCX. Because every RC involves breaking changes.
Even better, there's an extension that will use R# to format your code on save. Between that and sharing a common dotfile it's a great way to not waste time worrying about formatting issues.
I've not noticed any difference from before. Might not help, but have you tried the standard troubleshooting steps, like clearing out its cache?
Insurgency? It's not the fanciest one but for some reason it was the only one to make my home PC's GPU crash and burn before I dusted it thoroughly. Plus as a source game it should run on the iGPU.
There's that, and there's also the confusion as for where Microsoft is going. Is .NET Core the way forward, even for Windows? Will there be a .NET Framework 5? I wonder if there's a small hint here, in that ASP .NET 5 got renamed to ASP .NET Core. That "5" there... As well as MS now talking about make it easier to port there. It's become an elephant in the room to not talk about .NET Framework. If Core is the way forward for Windows too, I think MS should communicate that much more clearly. Because that would mean a quite different future, skills to train, and job opportunities for Windows developers. I saw a MS developer say that there'll be new versions of .NET Framework in the future, but that doesn't say much about where their focus is.
Try passing the assembly through de4dot
Intel has specifically mentioned the ability to use a [Razor Core](http://www.razerzone.com/gaming-systems/razer-blade-stealth#ultrabook-desktop), among other things. Re: your comment on storage - note that it has two M.2 drive slots, I'm only using one. Since SSDs came around I've gotten into the habit of using Steam's backup feature to dump game installs on a portable USB spinning disk. Whenever my SSD gets a little crowded I just swap out some games I haven't played in a while.
Yessir! Here's a pic of the insides, you can see there's quite enough room for a whole 'nother 80mm M.2. [Skull Canyon NUC Innards](http://i.imgur.com/BjXrPVc.png)
Coming from a newbie, could someone explain this bit to me? &gt; .NET Core will be expanding its API support to include the Mono API surface area, to support Unity / UWP apps As this part seems pretty odd for me as .NET Core has already been said that its already been built for cross-platform use. What would be the main purpose to expand the API for using Mono at that point?
Yeah I wasn't dealing with migrations, but some of the optimization stuff and other areas (GroupBy in memory??). You can see it all under "Critical O/RM Features not in v1.0.0". The word "Critical" seems to imply they know these have to get in there soon. https://blogs.msdn.microsoft.com/dotnet/2016/05/16/announcing-entity-framework-core-rc2/ The last one happened to be the "Complex/value types are types that do not have a primary key and are used to represent a set of properties on an entity type." one. But I dealt with the others too before throwing in the towel.
Thanks, deleted.
[module: ConfusedBy("ConfuserEx v0.5.0-3-g8fdf1b7")] i think assembly is confused by ConfuserEx which cannot be deobfuscated using de4dot and de4dot writes "Detected Confuser (not supported)"
Play to your developer's strengths. All three have their pluses and minuses so without a better understanding of your project, its goals, and your team that's a pretty reasonable starting point.
Agreed. Most of the developers are used to WinForms, but I wouldn't say they're WinForms experts. They could probably reach their current level of competence with WinForms in any GUI framework within a month.
If you're looking into using javascript/html/css, look into Node WebKit. It allows you to write the application in those languages and it's cross platform. You can host it in Linux and no installer is required.
Don't UWP apps require deployment through the store? I know there is supposed to be a way to host enterprise apps through the store, not sure how easy it is to do though.
[Microsoft says](https://msdn.microsoft.com/en-us/library/windows/desktop/dn614993\(v=vs.85\).aspx): &gt; [WPF] is the preferred technology for Windows-based desktop applications that require UI complexity, styles customization, and graphics-intensive scenarios for the desktop. WPF also takes advantage of XAML views. You can leverage the new simplified asynchronous capabilities (async/await) in .NET 4.5. WPF development skills are similar to Windows Store development skills, so migration from WPF to Windows Store apps is easier than migration from Windows Forms. I'd recommend WPF given that it has a nicer separation of concerns because of data-binding, has a lesser dependence on C# code-behind, and is really easy to apply custom styles for controls... but it also has some rather annoying problems that Microsoft refuses to fix. It seems to be another one of their abandoned projects, albeit still the preferred one. Their focus is currently on universal apps, but you'll need to make sure that everyone is running platforms that support universal apps, i.e. Windows 10.
Winforms with strict MVVM will transition well to WPF. Event-driven coders may take a while to get full utility.
^^^ neckbeard
WinForms or WPF. UWP sandboxing is likely to cause you problems unless your app has few to no local storage requirements. Distributing the app as a sideload with full perms is possible but WPF is currently a lot easier to work with. 
Eto looks cool.
I agree with you. I am working on a HUGE app right now that will require winforms as well as a web and mobile solution. Winforms just sometimes is the right answer. In my case it is literally just a front-end to several large libraries. The libraries are doing all the heavy lifting.
While I'll agree that WPF/Winforms are more what Op requires, WPF is much much faster than Winforms in most cases. 
If UWP meets your needs, go with it. Otherwise WPF. I'm using myself WPF heavily, however for Rich Clients. The reason I'm using a Rich Client is that I need seriously good 3D capabilites (visualising CAD data for 1000s of parts). But I'm also running a Webserver. If that's the case for you as well, I'd pick a Web-GUI, probably React stack. Or if you need a powerful grid, Kendo UI with Angular. If there was a proper 3D viewing solution for the browser I would have already migrated from WPF to a HTML5 based solution.
Don't underestimate the learning curve of WPF. It's the most powerful and cleanly designed framework, but it really takes time to grasp compared to WinForms or say Qt, which are both quite easy to pick up.
Yes and it "doesn't matter": Yes for the Electron part. "It doesn't matter" for the Winforms faster than WPF part. Winforms might be measurably faster than WPF in some scenrios, but the user will hardly notice. I'm using WPF grids with 50000+ entries just fine. Everythings is smooth and instantanious.
There are a lot confusion because we are tracking the development of .NET core closely. But if you just ignore what's happening and return back next year .NET core will be something similar to .NET 4.6.2 but cross platform.
Last year our team spent quite a bit of time building a WPF application, and required a lot of GUI code to get it up and running (we used prism). This year, we have been developing a Web API/AngularJS application and the development process has been much nicer. A bit of time was spent learning web technologies, but the user interface now takes a fraction of the time to develop, plus you have a web application so you don't have to worry about distributing or out of date apps. We have been very happy so far, as have our users.
You forgot the link?
Same with their Winforms controls, nice for demos, but you will enter a world of pain when you start using them in production. Their controls just have way too many subtle bugs and each bugfix release introduces even more bugs.
[Link here?](https://www.docker.com/microsoft)
Going on just the potential future expansion, it may even be worthwhile sticking to web straight up. Being an enterprise environment they will likely have hosting capabilities. You can knock all platforms on the head with MVC/Bootstrap - or whatever mobile first framework takes your fancy - and be able to code the project with the future expansion in mind or just take it as a feature. If the desktop thing is something that is 100% required, there is always the wrapper controls you can use to pull web into a desktop app. I'm not gonna say desktop is obsolete as it has it benefits - but web sure makes multi-platform programming a hell of a lot easier if you have the ability to do it.
WinForms will give you the fastest time to market. You can still do MVVM with WinForms. WPF is top-heavy and overly complicated compared to WinForms. UWP is a non-starter for me. I won't touch it until there is an alternative to the Microsoft app store and until there is zero friction for sideloading.
I use var every time because the type is always declared there and a developer should know that. Hover over the right side of the = to see the type in visual studio. Makes refactoring when the method changes its type so much easier. 
UWP or Github Electron.
A lot of that is my own difficulty understanding things. Are there books that are decent that I could read? So many technical books are poorly written.
You can also hover over the 'var' itself and it will report the type in the same way.
That's actually how I do it too.
I need to do some research before I answer you. I have a design pattern in mind that should make you feel better about huge views, but I need to make sure the optimizer does what I think it does.
`var` is just a keyword which the compiler then replaces with a strongly typed, type. It's a tool to help you write less verbose code when it's clear what Type a variable is going to be. It's also useful when you have a really long variable type like `Dictionary&lt;int, List&lt;string&gt;&gt;` which you might not want to type out all the time.
He's pointing out that "GetMeMyList()" doesn't make clear what the list is of. In this case it's a list of integers. If the type wasn't specified there you would have to take an extra step to figure it out.
From this statement, can you tell me what type var is? var result = GetMeMyList(); From a quick glance at System.Collections*, with "List" in the name, it could be `List&lt;T&gt;`, `List`, `IList`, `IList&lt;T&gt;`, `ArrayList`, `SortedList`, `SortedList&lt;T&gt;`, `LinkedList&lt;T&gt;`, `IReadOnlyList&lt;T&gt;`, or `ListDictionary`. If it is generic, what is `T`? Is it even a BCL list, or did someone roll their own type of list? Maybe it's an array? Unless the type can be *fully* inferred from reading the right-hand side, always put in the type. Also, surprise, it's an `IEnumerable&lt;KeyValuePair&lt;string, object&gt;&gt;` that's actually an `ExpandoObject`, but in this case, the method is providing the current list of members. (I've seen more bizarre code out there. Don't pretend you haven't seen it too.)
GetMeMyListImplementingGenericObjectint 
Yeah, but the point is you really rarely need to know he exact type of some returned value. Especially with IDEs. 
You can just hover on `var`
Except in cases where you can't not use var, you can always give the type yourself with the exact same result. 
I prefer var whenever I can. If I end up changing response type of a lower method, I end up needing to refactor less. Moreso helpful when using linq or extension methods. I use VS and intellisense so if I don't know and need to, I can just mouse over and be fine
&gt; PaymentProcessor.dll
I use var whenever I can. Having worked with a few dynamic languages I find adding the datatype very noisy and unnecessary. All the code editors have hints that tell you what is being returned and if you've named things properly you can usually tell anyway. I realise this might not be the case for everyone though :-)
EF is mostly for quick and dirty apps. There are tricks you can learn to make it faster, but most of them involve fighting against the framework. You're better off picking up a micro ORM such as Dapper to handle mapping and otherwise use traditional techniques.
This is the correct answer but the old C++ programmer in me hates the var keyword. Every now and again I force myself to use it just to try and get over my bias. 
Second on dapper. If I don't have a plan already this is my default and has been for a while.
This is what we implemented on my dev team, and it worked just fine. It was simple to use var everywhere except for where the type wasn't explicit. I get people's fear of var, but we really had no issue with compliance.
I either render the table out in the main view, or use the controller to spit out Json and load the table via Ajax. Same thing with the modals, if you're rendering the table yourself you can stash data elements, or get a response from the server and set it that way. 
This may be my own bias by why do you need a TryGetNext with an out variable? Why couldn't it just return null when it is at the end.
Because collections can contain the value null?
Load up SQL profiler and check what's happening when you step through the code. Odds are there's a ToList() or something responsible.
As a Microsoft fanboy I must say "Boo" to you sir. Windows is not irrelevant; it's transcending.
Yussss! Thanks for sharing that. It's not just core code I need either. Tests, a proper API documentation strategy, a hug once in awhile and reassurance that everyone needs this.... Every little helps. 
Why not render the modal from a partialview which contains the content for everything including the tabs? You don't need to load each tab as a partial, wrap it all up into one. 
Eh, what? I'm pretty sure I do have programs that handle 260+ character paths e.g. TortoiseSVN.
What if I'm not using Visual Studio? My code review tool does not have the hover ability. Ditto with the pull requests in GitHub that I need to read. None of my diff tools know about it either. There are many reasons where `var` can lead to confusion. If the right hand side is not 100% clear in the type, it is safer to spell out the type than it is to use `var`.
I'd hug you over tcp/ip if you continue development of GraphDiff ;)
There are APIs that can handle up to the NTFS max (IIRC 32767 chars), but Explorer couldn't, and neither could any .NET app without resorting to P/Invoke, up until now. Unless I misunderstood, though, this requires a per-application manifest that tells the system "yes, the app can handle long paths" - I bet the world is still full of software written with the assumption that paths can not be that long (buffer allocations and whatnot), so to a large degree, we're still stuck with the issue. With any luck though, we can at least now delete node_modules from Explorer...
There's a MAX_PATH constant in Windows set to 260. Some things work, some don't, and some are buggy.
And where to go from there? If I wanted to use Visual Studio Code?
Deer Got. I can finally get from TFS stuff without attempting to be creative with my workspace name.
Hmm... that could work. I do have the modal loading via ajax (as a partial view of course) and would like to load all of the tables similarly. I'll have to see how the DataTables plugin works with updating. 
Well, not really they aren't.
Still, asserting a variable is of a certain type has a meaning, and if the type is already given by the right hand side of the assignment, it is one assertion too many. Sure, it can make the code more readable, but semantically wrong. In general, I consider semantic correctness to be more important than readability (although they often come hand to hand).
Auto is newer than var though, I believe. I've only used it once in my life and that was to prove it was there. I've used var much more often.
Seriously that's all I want. Now the problem will be convincing my IT department to upgrade to Windows 10...
&gt; ubuntu on his macbook. Not a vm, not a dual boot- actually wiped osx. I'm pretty sure that's illegal. Doesn't Apple's TOS/EULA require you to use exclusively Apple OSes?
In almost 20 years of using computers, I have run into this issue exactly 3 times, and they were the most infuriating situations imaginable. Good to see it finally addressed.
My bias comes from years developing in PHP: the number of silly errors that can slip past (particularly during refactoring) because of dynamic typing.... **shudders** I'd rather spend the time specifying the return type and chasing down the "Why are you assigning a string to a boolean?" error messages while developing, rather than hunt down the bug when the user can't work out why their checkbox isn't working.
And Google search provides a tiny, tiny part of Google's revenue. That doesn't make Google.com irrelevant, and doesn't mean Google isn't a search engine company anymore. The lion's share of the desktop OS market belongs to Windows, even if Windows is small compared to Microsoft's other enterprises. If you're developing a desktop application or game, Windows is not, in any way, irrelevant. But please, tell me more about how you're not going to bother supporting Windows for your next software release because it's irellevant./s
But the thing is, when I'm **writing** the code, I know exactly what the return type is - because I've either just written the important function, modified it, or looked it up to decide what I'm going to be calling. That means that for the person writing the code, it's zero effort to add the type When it comes to maintenance, though, the person looking at the code has enough to check without adding one more job: okay, so you only have to go to definition and that takes 5 seconds, but when you're doing that 100 times, that's 10 minutes I've wasted for no reason. IMO, unless you have a specific reason to use it, then 99% of the time var solves no problems while potentially introducing some
Exactly - by the time you've made the function name obvious enough, often it's clearer to just do it the "regular" way to start with
Here is my rule of thumb: ELMAH (ELMAH.MVC) for **UNHANDLED** exceptions. Your choice of logging library for anything else.
IMO it would've been more useful if this was just in the IDE I type var list = SomeFunction(); I hit enter, and the IDE replaces Var with the type. Ta-dah: I've saved the same amount of time, and my code is still complete
Impossible to answer without knowing the internals of the software in question. It's probably using a run of the mill database, but which one they use, what export options it has, what kind of access the client has, and what schema it contains, is anyone's guess.
That problem more or less went away with the flattened structure you can get out of npm v3+. Still nice to see though.
&gt; When they eventually announce that Code is the successor to VS, that'll be the end to any serious WPF enhancements. You're an idiot if you think a text editor will be replacing an entire compiler/debugger toolchain anytime soon. Also, WPF died ages ago. I know of maybe 3 programs that still use it. But yes. The failure of a novel UI library is the death knell for the most popular desktop OS in the world. /s
I agree with you about dynamic typing but the var keyword doesn't indicate dynamic typing. var just indicates that the compiler will infer the type from the initial assignment. The variable remains that type for the rest of it's usage and cannot be assigned another type. Assigning a value of a different type to that variable from then on will result in a compile time error. That's why, as long as it's clear what type it is, it's not frowned upon. The dynamic keyword in C# provided true dynamic typing, meaning the type is set at runtime instead of compile time. That's really only there so interoperate with other dynamic languages that might exist in .NET and some COM stuff. 
Appropriately scathing. 
You have that backwards. Apple doesn't allow you to use OS X on non-Apple hardware. 
&gt; I upgraded to a Z8500 tablet... I've never owned a better tablet. Make, model please?
Like I said, its a chinese import. A [Teclast X98 Pro](https://www.youtube.com/watch?v=gKf2ofP1iwY).
Typically you add a property like `public virtual ICollection&lt;Repack&gt; Repacks { get; set; }`
It sure is virtual and ICollection. // You can add profile data for the user by adding more properties to your ApplicationUser class, please visit http://go.microsoft.com/fwlink/?LinkID=317594 to learn more. public class ApplicationUser : IdentityUser { public async Task&lt;ClaimsIdentity&gt; GenerateUserIdentityAsync(UserManager&lt;ApplicationUser&gt; manager) { // Note the authenticationType must match the one defined in CookieAuthenticationOptions.AuthenticationType var userIdentity = await manager.CreateIdentityAsync(this, DefaultAuthenticationTypes.ApplicationCookie); // Add custom user claims here return userIdentity; } public virtual ICollection&lt;RepackViewModel&gt; Repacks { get; set; } } -- public class RepackViewModel { public int Id { get; set; } [Display(Name = "Rig Name")] [Required(ErrorMessage = "Rig Name required")] public string RigName { get; set; } [Display(Name = "Last Repack")] [Required(ErrorMessage = "Last Repack Date required")] public DateTime DateRepacked { get; set; } public virtual ApplicationUser Owner { get; set; } } The other thing that isn't quite clear to me, is the default table that gets created is labeled AspNetUsers yet any custom models I make go by their respective names *edit added in RepackViewModel class
Correct me if i'm wrong guys, but all your basically going to need to do is: - Get DB credentials, dump the database. - Prep new database. - Import data into new database. - Write new front end. - Port over server side logic. - See what else you missed. Depending on the complexity of the web app, this can be either a massive undertaking involving a team of developers, or can be handled by an intern that has a thing for .NET This all may be a bit of an understatement, so don't think this will be cheap. 
Aspnetusers and such is defined in the library code. I also think you're missing the point of a view model if you're using one as a database class. But I'm not clear on why you're not getting the expected tables. 
EF Core doesn't support lazy loading so that may be your problem. Also are you calling save changes?
Its still not illegal, just breaking the ToS and its kinda difficult because they're not creating drivers for hardware thats not used in Macs. So you can do it, you just have to build a system or buy a notebook that has the similar hardware.
Can't tell if joking. .NET 4.5 was released in 2012. The current version number for the full framework is 4.6.1 and the lightweight version called .NET CORE is on RC2. The full framework is generally always ready for production as they don't introduce breaking changes. I'd wait for a release of .NET CORE before jumping into that one.
&gt; Who cares, lmao. This is the company that - Patented the rectangle and the color white - Have sued and shut down companies for having an "i" in the front of a product name - Sued Google and Microsoft because selling software on an online shop is clearly an Apple invention - Sued a tiny cafe in Europe because there's a picture of an apple (as in the fruit) in their logo, despite the cafe being older than Apple Don't fuck with Apple. They are completely without ethics or empathy. &gt;ToS aren't law TIL contracts don't real.
I don't know. I used to use Apple products (I thought shiny and expensive meant a powerful computer) and the guys at the Genius Bar told me that running Linux on a Mac is illegal. I ended up ditching my Macbook for an HP ProBook 4730s I found in a dumpster (i7 CPU and 16GB RAM!), and replaced my iTouch with an Android burn phone.
Ha yes that was my thoughts! A very odd post.
Just a guess but considering when it's failing you might be hitting thread size limits. You could try cranking up the limits in IIS if you have access, or you could pass the chunks off to a file writer service and dump them from working memory once they're on disk to keep you process size down.
I don't even have a legit reason, but I hate using var and always prefer using the actual type. 
Most of the time, error isn't logged in our DB, which means it is not being caught in *catch* block. The most recent related to upload is: System.ArgumentOutOfRangeException: Index was out of range. Must be non-negative and less than the size of the collection. The primary suspect for that error is one line of code: **var file = Request.Files[0];** 
Hmmm....you might be onto something. It didn't occur to me that app might be hitting thread size limits. Is this relevant? http://stackoverflow.com/questions/4571118/how-to-increase-thread-pool-threads-on-iis-7-0
This may be something to look at (request limits in the web.config) https://msdn.microsoft.com/en-us/library/e1f13641(v=vs.71).aspx 
Yes maybe, but a dump of some database will not really get him anywhere if he has no way of importing it into a new system.
It's nice to have these features and they help, but doing the language level interop is still challenging, at least for porting existing languages like Haskell.
I'd have to see the rest of the code, but yes that certainly would cause that exception to be thrown
in the past if I run into a thread size limit it will throw a OutOfMemoryException. have you tried updating the max content size in your web.config file?
That sounds like the server rejected/discarded the upload in which case I'd say /u/SaggyBagz is correct and you should look into your request limit on your server config.
DOn't care. It has things that I didn't know about but will probably use.
With WCF you can also make use of other protocols such as SOAP, setup a duplex connection or specify a transport like WsHttp or NetTcp depending on your needs. Web API is strict for building the HTTP services including the ones that are being RESTful.
&gt; Accessing a single value requires 2 interface invocations: MoveNext and Current. Interface method invocation has extra overhead associated with it. Why pay that price twice instead of using a single method call? If you use List&lt;T&gt; instead of the interface, you don't have to pay for interface invocation. &gt; It forces the allocation of a IEnumerable&lt;T&gt; value on the heap even when the enumerator could be implemented as a struct. Again, use List&lt;T&gt; in your foreach loop. It gives you a struct as an enumerator. &gt; The legacy of pre-generics .Net forces type safe collections to eventually implement the non-generic IEnumerable, IEnumerator and even IDisposable. I can’t remember the last time I actually used one of these and yet I have to write this boiler plate code every time I author a new collection. No you don't. Just subclass the correct collection base and it is handled for you. &gt; Many collections, like List&lt;T&gt;, implement pattern based enumeration in part to avoid the above inefficiencies 1. This is more code to write, test and maintain yet really doesn’t add any new features. So you knew all of this already? Then what are you bitching about? &gt; After some tinkering I settled on the following design: &gt; public interface IEnumerable&lt;TElement, TEnumerator&gt; Weren't you just bitching about virtual dispatch? 
&gt; closures Not a CLR feature. Created by the compiler using normal classes. &gt; coroutines Not a CLR feature. Created by the compiler using normal classes. &gt; value types Not new, they've been there since version 1. &gt; pointer support Not new, they've been there since version 1.
.NET has countless languages already. But nobody cares because C# doesn't suck. Had Java not been hamstringed by Sun, Clojure, Groovy, Scala, etc would just have been someone's personal research project.
I will look into this, but maxRequestLength should not be a problem here, as a chunking basically works around this limit (I have set chunk size to 3MB). Is appRequestQueueLimit what I should be looking at?
VS has had this functionality since at least VS 2008 I believe. If I'm understanding you correctly, anyway. VIEW --&gt; Other Windows --&gt; Document Outline http://i.imgur.com/c3nn8uC.png
Why would this kill robocopy?
I only used it for deleting node directories. Didn't think anybody used it for anything else.
If you want to mirror directories or just copy a bunch of files it's by far the fastest way to do it, especially if you enable multithreaded copy
&gt; &gt; Accessing a single value requires 2 interface invocations: MoveNext and Current. Interface method invocation has extra overhead associated with it. Why pay that price twice instead of using a single method call? &gt; If you use List&lt;T&gt; instead of the interface, you don't have to pay for interface invocation. I believe this post was in reference to enumerables produced via `yield` returning functions but I don't remember the original blog and forum and stackoverflow posts that led Jared to writing this. Still even `List&lt;T&gt;` enumeration via the pattern methods requires 2 method calls when it could be done with one instead. &gt; &gt; It forces the allocation of a IEnumerable&lt;T&gt; value on the heap even when the enumerator could be implemented as a struct. &gt; Again, use List&lt;T&gt; in your foreach loop. It gives you a struct as an enumerator. `List&lt;T&gt;` should not be used in public apis, but that doesn't detract from your point; a well designed enumerable type intended for this purpose could be written. Again though this article was written while there was some discussion going on about `yield` returning methods. When you opt in to the language supported functionality you are forced into allocations. There is today talk about a 0 allocation (on the hot path) tasklike returning method for non-awaited async methods and a low allocation IAsyncEnumerable state machine implementation but these problems are far enough removed from the vast majority of code that it is arguable on the point of whether or not the compiler complexity to support them at the language level is worth the savings they may generate in the correct cases (and the pain they will cause in poor code). &gt; &gt; The legacy of pre-generics .Net forces type safe collections to eventually implement the non-generic IEnumerable, IEnumerator and even IDisposable. I can’t remember the last time I actually used one of these and yet I have to write this boiler plate code every time I author a new collection. &gt; No you don't. Just subclass the correct collection base and it is handled for you. ok, I'll inherit from a `MinHeap&lt;T&gt;` or `BTree&lt;TKey,TValue&gt;` ... Perhaps a `RedBlackTree` or an `AvlTree` or even simply an unbalanced `BinaryTree`. Maybe a `SkipList&lt;T&gt;`? Except I don't want to inherit from them. Maybe the full implementation for my data structure exists already in unmanaged code that has been DOD certified. Maybe I should follow SOLID and compose using them instead because my type isn't logically a specific case of one of them. &gt; &gt; Many collections, like List&lt;T&gt;, implement pattern based enumeration in part to avoid the above inefficiencies 1. This is more code to write, test and maintain yet really doesn’t add any new features. &gt; So you knew all of this already? Then what are you bitching about? Umm, Jared Parsons is one of those people who might as well have written `List&lt;T&gt;`. He almost certainly has written several of the collection types in Roslyn and previous compilers and Microsoft.* libraries and is currently sitting as the #3 committer on https://github.com/dotnet/roslyn/graphs/contributors This whole post was about "what if" the BCL and compiler team did things a little different and what affect that would have on usages today. And it has the perspective of someone who is forced to deal with supporting the decisions made in those teams over the years within Microsoft. &gt; &gt; After some tinkering I settled on the following design: &gt; &gt; public interface IEnumerable&lt;TElement, TEnumerator&gt; &gt; Weren't you just bitching about virtual dispatch? Using a pattern based enumeration (just like current foreach statements) you would avoid avoid virtual dispatch (just like you do today), only with a simpler implementation (though a slightly more complex code behind the compiled `foreach` statement).
The URL you're calling is wrong. Perhaps drop the window.location.href and just use; url: "clockIn" This should do the trick; $.ajax({type: "POST" , url: "clockIn", data: { lat: latitude, lon: longitude }, success: function (msg) { alert(msg); }
Yes you can run Code in Linux - https://code.visualstudio.com/Docs/editor/setup#_linux Or you can just run .NET Core apps on Linux.
 can I just ask what you mean by "chunk" size? When you upload files from a browser it streams the bytes to the server from the client browser are you streaming bytes with JavScript?
&gt; Still even List&lt;T&gt; enumeration via the pattern methods requires 2 method calls when it could be done with one instead. It looks that way, but inlining will remove even that overhead. Perhaps I missed it, but that should have been covered. It's hard to take the second half seriously when the first half feels like a bait &amp; switch. 
Honestly I don't have a ton of experience with this in web forms. So my answer is I don't know. How do I check that?
No. "WCF Web api" is just the old name for "ASP.NET WebAPI". It actually has nothing at all to do with WCF other than the fact that the WCF team at Microsoft created it. "WCF Rest" is using WCF to expose restful services. As far as I know, nobody actually uses it. http://www.dotnet-tricks.com/Tutorial/webapi/JI2X050413-Difference-between-WCF-and-Web-API-and-WCF-REST-and-Web-Service.html
&gt; WCF Web api Was renamed to asp.net web api in 2012. If you're using "wcf web api" you're either using a very old version, or just using the wrong name.
Can you add screenshots of the configuration page? I've set up continuous deployments recently and didn't have any issues, but Azure changes so rapidly.
I'm at work so I can't grab any screenshots, but when I click Deployment in settings and select visual studio team services it shows no projects
That is fair, he could have mentioned that there is an awful lot of machinery involved in optimizing various aspects of enumeration so any of his points may or may not apply to particular classes. Consider though an enumerable like this: public IEnumerable&lt;int&gt; ModuloFib(int first, int second, int mod) { first = first % mod; yield return first; second = second % mod; yield return second; while (true) { first = (first + second)% mod; yield return first; second = (first + second) % mod; yield return second; } } In using this: foreach (var num in ModuloFib(1,1,10000)) { ... } compiling as: IEnumerator&lt;int&gt; e = ModuloFib(1,1,10000).GetEnumerator();//callvirt try { while(e.MoveNext()) {//callvirt var num = e.Current; ... } } finally { ((IDisposable)e).Dispose(); } That method could have been something like: public sequence int ModuloFib(... compiling as: public EnumerableStateMachineRunner&lt;int, ModuloFibMachine&gt; ModuleFib2(int first, int second, int mod) { var machine = default(ModuloFibMachine); machine.State = -1; machine.First = first; machine.Second = second; machine.Mod = mod; return EnumerableStateMachineRunner&lt;int, ModuloFibMachine&gt;.Create(machine); } public struct ModuloFibMachine : IEnumerator2&lt;int&gt; { public int State; public int First; public int Second; public int Mod; private int _current; public bool MoveNext() { if (State == -1) { First = First % Mod; _current = First; State = 0; return true; } if (State == 0) { Second = Second % Mod; _current= Second; State = 1; return true; } while (true) { if (State == 1) { First = (First + Second) % Mod; _current = First; State = 2; return true; } if (State == 2) { Second = (First + Second) % Mod; _current = Second; State = 1; return true; } } } public int Current =&gt; _current; } with framework types: public struct EnumerableStateMachineRunner&lt;TElement, TEnumerator&gt; : IEnumerable&lt;TElement, TEnumerator&gt; where TEnumerator:IEnumerator2&lt;TElement&gt; { public static EnumerableStateMachineRunner&lt;TElement, TEnumerator&gt; Create(TEnumerator start) { var runner = default(EnumerableStateMachineRunner&lt;TElement, TEnumerator&gt;); runner._start = start; return runner; } private TEnumerator _start; public TEnumerator Start =&gt; _start; public bool TryGetNext(ref TEnumerator enumerator, out TElement value) { var result = enumerator.MoveNext(); value = enumerator.Current; return result; } } public interface IEnumerable&lt;TElement, TEnumerator&gt; { TEnumerator Start { get; } bool TryGetNext(ref TEnumerator enumerator, out TElement value); } //this could be the IEnumerator&lt;T&gt; type but that is IDisposable public interface IEnumerator2&lt;out TElement&gt; { bool MoveNext(); TElement Current { get; } } and now usage: foreach (var num in ModuloFib(1,1,10000)) { ... } compiling as: EnumerableStateMachineRunner&lt;int, ModuloFibMachine&gt; e = ModuloFib(1,1,10000); ModuloFibMachine m = e.Start; //call int current; while(e.TryMoveNext(ref m, out current)) { //call var num = current; ... } And now you have 0 allocations. 
Thanks, this will work! I appreciate the help!
I edited the post and included an image
You have access to Pluralsight, might as well start there. Scott Allens videos are good and Mosh Hamedani just published a very large set of videos for MVC. Shane Wildermuth has one using the RC of the next MVC. All of those combined would give you enough tools to build something. I find Mosh and Shanes to be the best as they actually build projects. Although Scott Allen is great and explaining features in a easy to understand fashion. 
Never mind, there they are, they're just late getting going 
[OSS Index](https://ossindex.net/start/nuget/) cross references package and vulnerability information into a single database. There are two associated auditing tools for NuGet packages: * [DevAudit](https://github.com/OSSIndex/DevAudit), which is a command line tool that scans a packages.config file and reports on known vulnerabilities for your package dependencies * [Audit.NET](https://visualstudiogallery.msdn.microsoft.com/73493090-b219-452a-989e-e3d228023927?SRC=Home), which is a Visual Studio plugin that does the same [DevAudit](https://github.com/OSSIndex/DevAudit) has pre-built binaries as a zip or an executable installer [here](https://github.com/OSSIndex/DevAudit/releases/tag/v1.0.37). It is also available as a [chocolatey package](https://chocolatey.org/packages/devaudit). Simple usage: ``` devaudit nuget --file &lt;packages.config&gt; ``` The --file option is not required if you run from in the same directory as the packages.config file. [Audit.NET](https://visualstudiogallery.msdn.microsoft.com/73493090-b219-452a-989e-e3d228023927?SRC=Home) is directly installable from the Visual Studio Gallery. We are constantly adding new vulnerabilities to the database, and greatly appreciate any suggestions, feedback, or bug reports.
Call Microsoft. They've got licensing specialists to answer those questions.
http://up-for-grabs.net/#/
Is this for a class or the like? Just curious. Virtually everyone creates diagrams, but they normally do so with diagramming tools like Visio, SmartDraw, Dia, or Umbrello. I don't know *anyone* using real CASE tools today beyond that; not including the looser definition of CASE which seems to include a whole host of development tools like static analysis tools that some people seem to think CASE should include. When I talk about "real CASE tools", I'm thinking of something like [Enterprise Architect](http://www.sparxsystems.com.au/products/ea/index.html) which goes WAY beyond mere diagramming and is very comprehensive. It not only helps you create diagrams, but can actually help you find issues in your design; it uses the CASE metadata to perform real systems analysis, which I think must be a key feature of real CASE tools. [Visual Paradigm](https://www.visual-paradigm.com/features/) is probably a good example as well, though I'm less familiar with it.
Both. First, that there was a change at all, then, to make it even stronger, that it changed back, as unreleased APIs sometimes do. A big change to an API is something I always approach with care and patience, since after a breaking change there are likely more changes: the aftershock model of API breaking changes, if you will.
True, i like its u say. or try to look here http://www.microsoftvirtualacademy.com/
https://github.com/PawelTroka/Computator.NET/blob/master/Computator.NET/Compilation/TSLCompiler.cs That is kinda scary looking. You transform TSL into C# via a series of regular expressions... which means this dsl is a strict superset of C# which happens to not get caught by the list of regular expressions. Figuring out where a bug in a nontrivial script is must be super frustrating. Do you have reasons why it is being done this way as opposed to being a defined language? Also I see complex is an alias for System.Numerics.Complex, but you are largely using MathNet elsewhere; is there a reason to use this complex over the mathnet one? One more thing; are you avoiding large numbers (BigInteger, BigRational) for any particular reason?
You are 100% right about TSL. It started like that because it was easier for me back then and I could with relative ease made something that works. Now I am thinking about maybe changing it. MathNet is using mostly System.Numerics.Complex internally too. I think because it's from .NET Framework it's kind of standard way now. About large numbers again you are 100% right and I really want to use them. I think I will put it in TODO list.
Microsoft Powerpoint Easier and faster than Visio although the elbow connectors aren't as great
Sounds like [nLog](http://nlog-project.org) is what you want. 
Sorry, I posted my question this morning, and then completely forgot about Reddit for the day. I just did a quick test on one of my web apps, and it shows up just fine for me. Maybe check that the VSTS account you have associated with Azure is the correct one? If you have more than one (work vs personal), create a dummy project in the one that's empty and see if it shows up.
Valid point, in that case, I'd probably stick to some custom implementation e.g. create a simple console application that can both encrypt and decrypt the sensitive data, store the encrypted values in config.json and eventually decrypt them in the web app using the same strategy as in this app. I guess that we'll need to wait either for such tool or for the Azure Portal team to let us store credentials in the same way as for the old Web.config within the Azure Website configuration. http://stackoverflow.com/questions/36062670/asp-net-5-mvc-6-encrypted-configuration https://github.com/aspnet/Configuration/issues/290
Thanks for the advice and links, the custom provider was one of my original considerations. Knowing that the decryption key is the weak point doesn't sound fully secure, but at least the appSettings won't be in plain text, so some security is better than none. It sounds like this is the only option at the moment (unless someone else can offer a more secure solution).
I use Common.Logging (via nuget) for manually raised logging throughout our libraries, and that enables me to use whatever we wish on the final web project, configurable via the web.config. E .g. Common.Logging.log4net or Common.Logging.nlog. We couple that with Elmah MVC for handling uncaught issues. Can even find Elmah for Common.Logging which will log any of our manual issues to Elmah too.
Give draw.io a try.
We usually place connection strings in environment config on production server. If production server is compromised to the point when attacker can read web.config or iis settings - it's usually a game over anyway.
Do you happen to know of a step-by-step guide for Elmah.MVC? The official ELMAH docs *appear* a touch daunting.
Please. There are some subreddits left where you did not spam this.
no, this has nothing to do with compliance or regulation. Our internal users want to see the change history (probably to attribute blame if an error is made) and event logs, like 'useracb logged in at 06-03-16' so they can know if and how frequently their customer users are using the applications.
Install Elmah.Mvc via nuget and it mostly configures itself. You may need to provide a database connection string but that's about it.
I've done this by catching SaveChanges in the DbContext pre-EF 7.0/Core. Haven't looked into it with newer stuff, though. Basically you could just iterate through any changed properties and store them, preferably using an interface (ex. IAuditable) to narrow down the types you care about logging. After all that it was as easy as adding that interface to the appropriate classes to get Type, Property, NewValue, OldValue. This can cover any changes to data objects (amounts on an order, for example), or user activity (login dates, requests, etc.).
Your webpage breaks gestures on Safari
No worrys, just thought I should give you a heads up :)
Hmm that's interesting, works fine on my machine but I'll take a look into it.
Nice work! Does this have any built in support for remote process monitoring? If not, is it something that could be easily implemented? Im creating a realtime process monitoring solution in .net using wmi queries, signalR, a windows service, and angular on the front end to display real time data. Im now at the stage where I plan to modularize and refactor the code for extensibility so other devs can built upon it and wondering if this might have some of the features I plan to implement already in place so that I could possibly plug in my code and work from there. Thanks
Nice, 'dotnet' is much nicer. 
Ill poke around thanks
Thanks!
Hey, it should just be the same command line switch as xunit since the arguments are passed through
Yeah but VB.net , obviously, doesn't. Why is it not VB# instead 
Well J# was an official language and it is not a musical scale. I mean I get how it started but now # is more or less equivalent to ".NET"
Branding decision. It doesn't have to follow some consistency. Why isn't it ASP #, why don't they cram sharp into everything? Besides the fact it would be lame. The consistency would be superficial. Visual Basic Sharp doesn't roll off the tongue well. And maybe VB just isn't cool enough... For as many reasons as I can invent why it's not VB# there's really no particular reason why it should be either.
While all # languages might be .NET, not all .NET languages are #. Claiming they are equivalent or interchangeable is not correct. Even if you feel differently, that's your feeling, it doesn't represent everyone else's reality. Sometimes we want order and patterns to exist in a particular way, but sometimes that just human craziness rattling around and not rational sensibility.
But then we'd have kids asking nonstop about the difference between B and B# and it would be just as annoying as the misconceptions about Java and Javascript. Or maybe not since B isn't as widely known...
This got pretty meta for a question about branding. 
You can also call C# Db (D flat). They're literally the same thing.
Webforms and Linq are definitely in my wheelhouse, send me a link!
Fort hose that dont know... B was the precursor to C. The next one was supposed to be P... I've always been sad that never happened. EDIT: It was supposed to be P because B and C were based on a language called BCPL.
There was a J++, probably became J# as C# did
Use Validation Groups. https://msdn.microsoft.com/en-us/library/ms227424.aspx
As someone else mentioned, in your menu put the tag causesvalidation="false". However if it's a control, put all those validators in a validation group. Otherwise you coops hit this issue anywhere you put it. Trust me, do it
Thank you all! This may turn into a lot of work.
It's only a lot of work the first time you don't do it, and then have to go to fix it later on
legacy app nearly a decade old. There's a lot of not having done it the first time that I now have to repair.
I'd check the EF Core issues [here](https://github.com/aspnet/EntityFramework/issues?utf8=%E2%9C%93&amp;q=is%3Aissue+is%3Aopen) and post there regarding this matter. The devs would be the most capable for helping to resolve the issue.
Yeah, so essentially I dropped the whole database, deleted all Migrations. Created a new ASP.NET Core Project, copied the CreateIdentitySchema and ApplicationDbContextModelSnapshot migrations from the new project to my current project and ran the *dotnet ef database update*. This resolved the issue. 
You would be surprised, then, at how meta branding discussions go when making them. Discussions about color choices for conveying a feeling etc.
Good to hear it worked. Sometimes we spend so long staring at something we need fresh input to see the problem
Not 800MB more.
Depends what you've got open in it, but no electron isn't the most memory efficient platform.
That's super cool!
What if there's a package on bower that's not in npm?
How do you do that? Does it work with the build tasks pane in VS like gulp does?
1) There isn't, or 2) It was written by a terrible developer, and you should avoid it.
I see, I'll go and experiment now, thanks. AWESOME! I see! Thank you. 
I've just started working on a new WPF recently and honestly things haven't changed that much. Take some MVVM framework like Caliburn.Micro or MVVM Light and look at some basic tutorials, on top of that you may include https://github.com/Fody/PropertyChanged to get rid ofINotifyPropertyChanged. Separate your views from view models, setup two-way data binding and commands, expose only required properties and commands in the view models and keep the actual business logic in some other classes/services that will be injected into VM. If you need to add some code behind in your view don't worry about it - just use the provided IMessenger as a Service Bus (when using MVVM Light) to communicate between the views and view models - don't treat the MVVM as the religion, because sometimes things are very difficult to achieve by using only that pattern (e.g. starting a storyboard).
Awesome! Nuget package please :-)
I once used embedded Javascript [EdgeJS](http://tjanczuk.github.io/edge/#/) to construct a JSON payload for a REST call to AWS. Since the keys in the payload were dynamic, it was *way* easier to do it in JS than it was in C#.
That would be a special case in which you can't really get rid of e.g. configuring WCF without using XML. I'd say that JSON approach is a good fit for the internal settings classes used within the application as you can easily create complex objects and map them into classes. If it wasn't legit I doubt that. NET Core team would make use of this.
Wouldn't be the first time Microsoft did something stupid because it was trendy. A lot of .NET core seems to be semi random attempts to try to get people who might otherwise use electron into the .NET ecosystem.
Nice one, I always wondered what you'd use EdgeJS for :-)
cool! :) 
You can also take a look at the Pluralsight courses - there's quite a few of them :).
This seems to require a username and password to access the link?
&gt; AUGUST 17, **2012** 4:49 PM
http://video.ch9.ms/ch9/8960/2bb77ec5-71dc-4669-9bb0-35d3b2538960/OEMTVComputex2016_high.mp4 direct mp4 link no password needed.
I just don't think it'll scale, especially since it's not actually using the latest JSON spec and so won't support quite a few things.
Will check it out. Hadnt heard of pluralsight. Its becoming obvious that the transition from Winforms to WPF is pretty steep. I did this free course on Udemy but a few more courses would definitely be helpful. Might be worth signing up to pluralsight for a month or two. https://www.udemy.com/enterprise-wpf-with-xaml-from-scratch/learn/v4/overview
This was of great help, thanks!
BTW, I tried installing the PropertyChanged.Fody package but got "Unable to find package" error. Does this also require the Fody package ? I got the same error for this too.
Absolutely, thanks again!
I've had to implement OAuth2 and to be perfectly frank, the best resource I found was https://tools.ietf.org/html/rfc6749 Its dry, its hard to read, but its honestly the only resource on the topic I was able to find that was generic enough to be helpful while still being factually acccurate.
Oauth in general is a bitch to get right. I had to deal with implementing oauth 1.0a in an asp/VB.net website that would interface with a php oauth 1.0a server. *Everything* is for Twitter or Google, and the worst part is that Twitter doesn't follow the protocol properly, so you run into libraries that work for Twitter, but nothing else. I'm sorry I don't have any sage advice, but good luck. It took me and another programmer about a month of on and off work to get it working.
Yeah, that's one way to do it when a JSON payload can be easily represented by a strongly typed object, but that wasn't the case.
Thanks for the link. It seems that I can access the website itself without a password, today. I don't know if the login request was a temporary thing or because I was previously signed into a Microsoft account and it wanted me to view the web page as that account.
The issue isn't with your code it's just with the idea of JSON as an overall config. Config is a problem where all the solutions suck, but XML seems to be the least horrible with all the above plus transforms.
I know, but like I said, each of us seems to have different preferences. Configuration is usually something that can't be avoided, so if I have to create one, I'd rather stick to JSON as it's easier for me ;).
True that, but it's still possible. It really depends on what are you trying to achieve, there's no silver bullet.
You're not using constructor injection in your controller, and appear to be using a convoluted Service Locator. The more 'normal' way would be to have an IPageService / PageService implementation, and the constructor of 'HomeController' being something like public HomeController(IPageService pageService) { this.pageService = pageService; } The interface is sort of optional though - it could just be PageService. Then you change the default controller factory so that it uses your DI framework to construct a controller. This is probably what StructureMap.MVC5 does, although I suppose you could probably implement your own likely-worse version of this. 
&gt; Please note that I'm not looking for Structuremap.MVC5. But that **is** the proper way to use Structuremap with MVC. It enables you to take dependencies through the constructor, instead of making calls to the container everywhere.
no, no no! This is just sad :(
Why does it have to be middleware? Are you building a framework, or just supporting a single app? I've found that "middleware" is usually the wrong abstraction for login *flows* like OAuth. Middleware is for cross cutting concerns. But I know why MS decided to implement their version as middleware: it's easier to package into a drop-in* library (*drop-in with a thousand undocumented lines of config). But for OAuth (and SAML, and the like), I think it's far easier to just do-it-yourself. And since I tend to use ASP.NET MVC, I just handle the auth callbacks as a simple MVC route. Here's an implementation I've build for one of my public domain side-projects: https://github.com/jdaigle/FriendsOfDT/blob/master/src/FODT/Controllers/LoginController.cs
Regardless if you care or not, it's the simplest, cleanest way to do it.
Are you using CI or at least a standalone build server? It seems like this should be done as part of that process. Maybe check out [GitVersion] (https://github.com/GitTools/GitVersion). Edit: nevermind the build server comment, I just noticed VSTS mentioned in the question.
I am using Precision Infinity's automatic versioning package: [Gallery link](https://visualstudiogallery.msdn.microsoft.com/dd8c5682-58a4-4c13-a0b4-9eadaba919fe)
The number of code files in a solution is a metric only novices are concerned with. Modern IDEs make it a non-issue. You're wasting a lot of time in an attempt to avoid imagined pain. Look how messy your code is already and you're worried about a few extra files. Can you give a solid concrete reason why having 6 more files in your project is bad? Not just "it's not clean" or "I don't like it", but something like "having more files makes doing X thing harder because of Y reason". 
I don't suppose that it *must* be middleware, that's just how Microsoft does it with the built in OAuth clients (MS/Google/Facebook) so I'd like to learn that method.
I've seen that, and have read many of his articles/posts, however, it doesn't appear to create middleware, rather using Angular to complete the auth/authorization. So while he's definitely got some great information, it's not exactly what I am looking for.
Thank God, a nice alternative to T4. Using Roslyn? How about Linux/Mac support?
What you want is the Micro framework. http://www.netmf.com/ The source is on github and can be compiled to many boards - the requirement being 32bit processor. http://netmf.github.io/
At least it wasn't Scripty McScriptface.
 EmployeeContext employeeContext = new EmployeeContext(); So uh...I don't know how else to put this, but there's this thing called IDisposable. It's pretty important. Don't write .NET articles (or if you didn't write it, don't link to random and poorly written .NET articles) if you don't understand basic .NET concepts. You're only serving to mislead people who are new to .NET and cause them to create applications that leak unmanaged resources and most likely cause deadlocks. :\
From what I read I think that shouldn't be a problem. It's using Roslyn and is independent of Visual Studio so it sounds pretty similar to writing a conventional script file.
Im still playing around with Windows 10 IoT core, but it looks pretty interesting so far. https://developer.microsoft.com/en-us/windows/iot
&gt; but **all** the code is already written I don't know where you got that idea from, Umbraco is a framework that using other frameworks (MVC, .NET, etc), to add functionality that is relevant to a CMS; Content editable content, media handling and User permissions to name just three items of a very long, long list. While out of the box, you can create simple websites with limited Razor, css and javascript skills, but at this level Umbraco offers nothing extra that other CMSes don't already do better. Where Umbraco excels is it's much loved level of extendibility, because really at its core its a framework not a CMS ([www.cmscritic.com](https://www.cmscritic.com) even refer to Umbraco as the only CMF not a CMS - Content Management Framework). This allows the creation of highly bespoke website solutions, that at their heart are easy to use by Content Managers, while maintaining the principles and paradigms of programming for those that write backend code (c#) so love. Though in fairness, its not that loved by straight up front end coders (Those that concentrate on Html, Css, Javascript), who generally find Windows yucky to start with, and then something that needs to be run in Visual Studio an anathema. I even go one step further and think of Umbraco as a Framework to allows you complete freedom to Data Model a situation; that situation so happens to be a website and the output is HTML, but nothing in the framework dictates that. 
&gt; download So, why isn't this included in the post itself?
I'm using T4MVC. Is there an easy way to convert this over to Scripty? (Yeah, yeah, I know. Nothing's ever easy. I just had to ask, though.)
Unfortunately there's no direct way to "convert" a T4 script to Scripty. The underlying concepts are just too different. T4 is a templating engine, so the stuff you write is what gets put in the output files *unless* you escape to write code. By contrast, C# scripts (and by extension, Scripty) *actually are* code and to output to a file you write code statements. Besides that conceptual difference, there's also a big difference in the API for accessing project information. T4 uses the DTE object model and Scripty uses the Roslyn Workspaces API. While the two present mostly the same information, the APIs aren't similar at all. So while Scripty can "replace" T4 in that it can theoretically do all the things T4 can do, it'll take some work to port existing T4 templates.
Your partial modal view javascript won't execute any $(document).ready(function() { So any javascript calls you need should be in the parent page. I.e. when the parent page fetches your modal via ajax, you should do the 'stuff' you need in the success part of the ajax call. 
Offscreen rendering (RenderTargetBitmap) is not hardware-accelerated afaik.
Things to note: * C# 7 has not been released yet * It is called RyuJIT, not RuyJIT * The features listed under the .NET Framework are a small subset of what you can actually do * Likewise, the .NET Core features offer both new and changed API's * .NET Core RTM is being released on June 27th so there will be much, much less API churn after that * UWP Apps run across all Windows 10 installations (desktop/laptops, tablets, phone) plus the Xbox one and HoloLens * .NET Core apparently *can* be installed on Windows 8.1 and Windows 7 SP1, but it does not come standard and UWP apps will not run there * MS is working on .NET Standard which will be a common set of API's across the full .NET Framework, .NET Core, and Xamarin to help unify the platforms
That's a good question. There isn't much documentation on this (yet) that I can find. This question was asked recently on StackOverflow but doesn't have any answers yet: http://stackoverflow.com/q/37694211/3191599
I guess I don't know the space well enough to give you an informed answer on that. I'm not opposed to using any board over another, I just (think?) there is more support/community around raspberry pi/arduino. As you can tell I haven't really even settled on board vs minicomputer route yet. I'm very intrigued by the platformio and others like it. I mean..I come from the Eclipse/Visual studio world. When working with say C#, how in the world can in manage referenced libraries, etc. Is it simply done with a package config file and regular imports/using? Does it provide intellisense for C#? Oh..I forgot to mention, part of the reason I am a bit worried about the particular hardware is I am doing stuff for my home, but I also have a commercial venture I will try to launch...very small...for home automation things and I want a platform that once I build I can rely on to deploy. By the way..thank you very much for your informative answers. I already appreciate it greatly! 
OP, the best advice in this thread is hidden away in the middle of screwdad's response: &gt; Use IdentityServer3. End of story. He's absolutely right. 
You are a god. Keep up the good work. Question: In creating the whole MaterialDesignInXAML project, including this new motion stuff, have you run into any issues with timely UI responsiveness due to WPF's fairly dated rendering tech?
It's an on going battle to keep things responsive, but I work on real time trading systems which have a lot of data and activity flowing through the GUI...there's plenty of places we shave valuable milliseconds to keep things alive.
Yeah, unless you're offering something truly unique and different, it probably doesn't have much, if any, sale value. If you don't have time for it but think it's too good to abandon, release the source and see if anyone picks it up
What makes you think people will buy it? What do they gain?
Not entirely true... you can open a personal account without having written a Series 7 at IB, and use their API. https://www.interactivebrokers.com/en/index.php?f=1325 
I'll be honest, .NET on micro controller hardware just **isn't** the way to go. You're much better using C++ - the community is huge! I am a asp.net C# developer and even I know that C# on embedded is a flop. Grab a WeMos D1 R2 and have a tinker with it. Its only 10$ and you can find a supplier close to you.
Other component vendors cross my mind. With bigger development, business and marketing capabilities than we have. We had used control in apps that we made for clients, and it's really nice control - objectivly. 
That looks awesome! Thanks for sharing
Thanks a lot for these clarifications, I'll make sure to include them in the doco!
I see, thanks!
Exactly! So for instance you have a WeMos device you've programmed with PlatformIO. It might have a little web server that you can post commands to restfully from a C# windows app. The C# windows app can have all the brains, or the Windows 10 IoT board can have all the brains. Make the embedded micro controllers dumb - the less they do the points of failure. For instance you can just poll your WeMos for the temperature reading from C# over Wifi using HttpClient. A better way would be to use MQTT. MQTT is a lightweight messaging protocol that your C# apps can connect to and then relay messages onto embedded devices. Its super easy! Your C# Windows app just subscribes to temperature events that are sent from the embedded device. https://www.nuget.org/packages/M2Mqtt/ https://github.com/ppatierno/m2mqtt Have a look at this example. Which sends the temperature to a MQTT server. https://home-assistant.io/blog/2015/10/11/measure-temperature-with-esp8266-and-report-to-mqtt/ Then use a C# application to join the same server on the same channel/topic and you will see the messages from the embedded device.
If you are like you like to know what kinda of pricing proprietary software entails before reading about it -&gt; here you go Pricing is Per Seat / Per Build server and you have have to pay unlisted additional costs for version number upgrades 1-2 $339 / $681 3-5 $317 / $635 6-10 $294 / $590 11-20 $283 / $567 20+ $271 / $544
Just manually remove all of the files your installer drops in place (and undo any other changes, such as registry entries), then remove your app's key in the registry under HKLM\Software\Microsoft\Windows\CurrentVersion\Uninstall. The key name is the GUID used in your install script (I'm unfamiliar with InstallShield, so My apologies for vagueness). Bear in mind that if your install is 32-bit on a 64-bit system, you'll find your app's key under the SysWow64 sub key instead. This should allow you to run an updated installer and have Windows act like it was never installed.
It does look interesting. But I'm failing to see how its different than guys from companies like DevExpress or Telerik.
WPF was released in 2006 and does its rendering via Direct3D 9 which was released in 2002. If you want detailed explanations on [WPF Graphics Rendering](https://msdn.microsoft.com/en-us/library/ms748373) and how [WPF interacts with Direct3D and the GPU](https://jeremiahmorrill.wordpress.com/2011/02/14/a-critical-deep-dive-into-the-wpf-rendering-system/) you can follow those links. Here is a summary: Every WPF application is composed of a complex visual tree which is rendered top-to-bottom, left-to-right. Any time you want to make a change to the UI you are telling WPF that you want to change some part of this tree, and it is up to the rendering system to actually push the update and draw on the screen. The whole system keeps you very far away and abstracted from doing any direct drawing of pixels. This is actually a fine system - as long as that system takes full advantage of hardware acceleration and scene optimization. You do have some fine control over scene optimization - which involves manipulating the visual tree to prevent certain background elements from rendering which are just going to be covered up by something else anyways. The problem comes from WPF's poor interaction with Direct3D 9 and the GPU. In essence: * WPF makes multiple linear draw calls per frame to render simple shapes, which when added up can cause the GPU buffers to fill up, causing frame rendering stalls (skipping motion as opposed to smooth) * These draw calls could be reduced by an order of magnitude or more if WPF used Direct2D which was released in 2007 (the calls could even be optimized in Direct3D 9, but they haven't been) * WPF does make use of BitmapCache's to reduce rendering time, however it doesn't batch these caches together to improve performance (16x for Direct3D 9, greater for newer versions) * WPF has very poor pixel shader optimization as it creates and destroys large numbers of textures unnecessarily Also, by default, WPF makes use of a single UI rendering thread to process all elements in the visual tree - however, you can actually [launch additional windows in their own background threads](https://eprystupa.wordpress.com/2008/07/28/running-wpf-application-with-multiple-ui-threads/) which will drastically improve performance (none of this is documented well on MSDN, but at least it is easy to do). 
Two things. First: Amazing, eloquent and succinct information provided. Easy to understand with many points backing up your statements. This is the sort of thing I come across years later on google and proclaim thanks upon that someone had the foresight to provide such information. Second: I suppose this is all ok when we're talking about applications. I mean, we're not dealing with games which would need to be far more optimised and while wpf (at least now) seems like it has room for improvement it is still sufficient for most cases isn't it? I mean, if you want enterprise level stuff you're probably sitting on ASP/Winforms (either due to the fact it doesn't have to be fancy or because you're sitting on piles of legacy code), but for most applications wouldn't those drawbacks be less evident? 
I happen to agree with you, problem is, we are required by our employer, the government to have a certification in the technologies we use. So we don't really have a choice.
Sure, simple applications will rarely tax the rendering system and won't cause any kind of visual stutters. The problem shows up when: * You start using animations (such as in the form of transitions, which is what the op's post is about) * You introduce large numbers of elements to the visual tree, each with their own children which want to display their on visuals - such as a big Datagrid with lots and rows and cells which each have their on custom data renderers * You start drawing complex shapes in your UI - such as those found in data visualizations (pie charts, line graphs, etc.) These and many other scenarios are common when you want to develop an application that looks cool, displays large amounts of live data, or otherwise has some unique features to pique your users' interests. Take a big Datagrid for instance. Imagine displaying just 20 rows of 8 cells (~180). Each cell has 1 or more visual children (~500), and each child may require multiple draw calls to display (because of lack of optimization). Now you have 1000-2000 draw calls (could easily be a lot more than that) every frame just for the rows and cells of a data grid (and WPF is trying to render 60 frames a second). This doesn't even count the draw calls and rendering of the headers, surrounding UI, and window, let alone other windows opened by the same application - which is common for a big data analysis piece of software. Another example would be when trying to quickly scroll through a list. WPF will do way more draw calls than it needs to, they get processed linearly too slowly or flood the GPU buffers and cause the list to start jumping and skipping around instead of scrolling smoothly.
That was a very useful primer for ASP.NET Core.
How does a virtual application differ from a regular application?
I will most like to continue development, since I really enjoy creating controls. But I think that I didn't do marketing and business part from the start.
Thanks for the suggestion. Tried this but it didn't quite work - the product was removed from the "Add/remove programs" screen in Windows but my installer still detected it there, and tried to uninstall before re-installing the new version. Found a great utility by Microsoft that took care of this, it's called [MsiZap](https://msdn.microsoft.com/en-us/library/windows/desktop/aa370523\(v=vs.85\).aspx) and comes with the Windows 7 SDK. Cleaned up everything else needed to make InstallShield think the program was uninstalled successfully, so I could fix my package and try again :) 
This article is pure shit. Anyone could do better in less than an hour.
Within IIS, you can have zero or more virtual applications beneath a website. Virtual apps and websites share the same AppPool and configuration. Not always a valid option but good to know its there.