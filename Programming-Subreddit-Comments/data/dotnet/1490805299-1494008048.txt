Ignite is more about IT admins than developers. Build is the Microsoft conference aimed at developers.
In my opinion, look into Dev Intersections or one of the Visual Studio Live events. I think you'd get more bang for your buck out of those.
I think every .Net dev should go to one of the big MS conferences at least once in their career, primarily to see what the fuss is about.
I go once every 2 or 3 years. I went to the last one in Atlanta in 2016. Awful venue, too much walking (I walked 6 miles one day, and that was just for the conference, between sessions and the floor - I run and also did a 4 mile run in the morning.) Too many people. Not enough time between sessions. Awful food - they ran out of salad within minutes of the start of every lunch. And they ran out of coffee at breakfast!! At breakfast!! If you aren't doing cloud stuff (Azure and O365), much of it won't apply. They are really pushing cloud there, and they showcase new stuff, which is cloud-first (or cloud-only). Ignite is a merger of the SharePoint Conference and TechEd. They were much better apart. I can only take it every few years, and I go just to take in some ideas about new things MS is doing. 
You can get away with doing a lot of work up front, particularly if it's done during a loading screen, if it improves performance during actual game play. That said, try it out, and make sure to measure as you go along. 
I'm trying something similar to this. public struct AnimationKey : IEquatable&lt;AnimationKey&gt; { StringBuilder Name { get; set; } public AnimationKey(StringBuilder name) { Name = name; } public void SetName(StringBuilder b) { Name = b; } // structs have a default implementation of Equals(object), but that will box it and generate garbage public bool Equals(AnimationKey other) { return other.Name.Equals(Name); } public override int GetHashCode() { return Name.GetHashCode(); } } I removed Frame because there's no way for me to tell what the frame is when I load the animation. Some folders that I load have multiple animations in them (A boss for example might have a "Run" and "Attack" animation). I set AnimationKey as the key for my dictionary and I pass in another AnimationKey (which I update using the SetName() method) to access the dictionary values. It doesn't work though; it doesn't ever check the Equals() method. Is my approach possible or were you intending a different use of Equals()? Edit: The Equals() method of AnimationKey works as intended, I just need to figure out how to get the dictionary to refer to that method when looking up a value.
Yeah, I started to understand your code a bit more after replying. I had never toyed around with hash codes before. I like this approach a lot, I just have to figure out how to get it to work with the way I load Textures. I wouldn't be able to use the filenames as the Name variable, it would have to be a more generic string like "Rain", "Attack", etc. that I pass in when I load the content. I would also have to split every animation out into its own folder instead of grouping related animations together, otherwise I wouldn't be able to set the frames correctly. Seems like no matter what I'm going to have a lot of refactoring to do, unfortunately.
You could use a [regex](https://msdn.microsoft.com/en-us/library/system.text.regularexpressions.regex.aspx) to split the file name into the generic name and number. This should work: var textureRegex = new Regex(@"^(?&lt;name&gt;[A-Za-z]+)(?&lt;number&gt;[0-9]+)\."); var match = textureRegex.Match("Rain01.png"); if (match.Success) { string name = match.Groups["name"].Value; // "Rain" int number = Int32.Parse(match.Groups["number"].Value); // 1 }
Oh man, this might just work for everything. I'm pretty burnt out at the moment, but I'll give this a shot later. Thanks so much for the help! 
Honest question: why use make instead of a .sh or .bat that does the same thing? Bear in mind that I don't know how to use make and I know how to create a .sh or a .bat that does the same thing, so I want to know if it's worth it to invest my time in learning more about it.
Honest question: Why _would_ they count? If you've marked it "won't fix" then you're acknowledging that it's working as intended, or at least that's the idea. I don't know of anyone who would knowingly wontfix a bug just because they don't feel like dealing with it, though I'm sure it happens.
Me too! And make is fine for cross platform. On Unix-like environments (including MinGW or Cygwin or such on Windows) you can probably use the same makefile even for cross compiling: make CC=mingw32-gcc for example. You can make a different makefile if the platform is sufficiently different that you'd want different rules or logic: make -f Makefile You can also simply add a vcxproj or such.
Great to see I'm not the only one who uses make with .net. In my case, I've been using it for small VB projects for which VS was overkill ever since .net 1.1.
I think they just integrated System.Ben.
I just love this kind of comments )
Gotcha. I've always treated wontfix and "not a bug" as the same thing; wontfix has meant to me that it isn't a bug in the first place. Two different ways of going about it, I suppose.
The one situation where I wouldn't recommend storing *just* the UTC datetime by itself is the case where you're dealing with recurring events across multiple time zones (think scheduling recurring calendar appointments where the user doesn't know or shouldn't have to know in which time zone the event occurs and they only know an address for the event) If you've ever dealt with this problem, you'll quickly realize that it's only sane to "project" out events if you have a datetime local to the event location plus an IANA time zone ID (i.e. "America/New_York") and a schedule of some kind (I.e. iCal, Cron, etc). That way, you can just take the first local appointment date and generate some addt'l local datetimes based on the schedule. NodaTime can be used to get the UTC datetime for each of these occurrences. Important corner case with projecting local datetimes specifically: some projected datetimes will not occur or will actually occur *multiple times*! You need to figure out what to do in these cases based on business requirements. Time zones and DST = too much fun, and by fun I mean pain :)
2015 can do it too.
Don't know if there is a general one. Maybe we should create one?
Reading is for peasants. If you wait someone will read it for you and tell you. 
Headline implies it's a new feature. 
Wow I haven't heard of this before. From a marketing point of view it sounds great. Have you used this framework before? One difference is that it seems to take a code first approach to creating applications. So you have to create a project in visual studio and start manually writing scaffolding code(including data layer code) before you can build an app. Am I correct in this? You don't need to write any code to create prototypical applications in AppFoundry. You just a need a database and the user application. This was intended to allow analysts to take on a more active design role and shorten the user feedback loop. Coding is only required when you want to write custom business and presentation logic and at that point it takes a model first approach so it generates all entities for you automatically. There is no coding or scaffolding required for the database side of things at all. XAF looks like a pretty big framework so I'd to take a closer look to see how else it compares. 
The link to "Falsehoods programmers believe about time" is infinitely more valuable that the article submitted by the OP. All his/her article says is "watch out, date and time are tricky". 
Have you done much research on what already exists? There are a lot of [code/application generators out there](https://en.wikipedia.org/wiki/Comparison_of_code_generation_tools). I have not looked at it, but Lightswitch seems to Microsoft's own offering in this space. I think codesmith is pretty popular as well. Even with the crowded space, you could likely make money consulting or maybe even selling to non-devs if your willing to do the sales. In any case, researching could help you find your market.
I don't have an answer for you, just surprised to find someone else with an irrational hate for azure haha. Every time I use it, it frustrates me so much. I'm currently looking at finding a Windows VPS and just manage my own server
I've been using [ZEIT now](https://zeit.co/now) recently. It's really really good. I've been meaning to write a blog post about their service, but haven't got around to it yet. [Scott Hanselman](https://www.hanselman.com/blog/ZEITNowDeploymentsOfOpenSourceASPNETCoreWebAppsWithDocker.aspx) and [Dominik Kundel](https://www.twilio.com/blog/2017/03/deploy-net-core-with-docker-and-now-sh.html) have written good posts on how to get it going with asp.net core. It's as easy as dropping in a simple dockerfile - something like this: FROM microsoft/aspnetcore LABEL name="YourAppName" ENTRYPOINT ["dotnet", "YourApp.dll"] ARG source=. WORKDIR /app EXPOSE 80 COPY $source . and then just running the 'now deploy' command. 
That's awesome, now if only it could run for more than 5 minutes without locking up.
Used it for a couple of prototypes and our first big production apps are in development right now. It works like a box full of magic tricks, so many modules to discover. We start by designing the business classes with the visual designer that generates the classes (https://documentation.devexpress.com/#eXpressAppFramework/CustomDocument113450). We add controllers for the app logic. And then it is mostly customizing instead of programming: all the views, reports, roles and rights can be set up in an editor, either at runtime or designtime. I think that from a developer point of view there is more fun and satisfaction in creating a framework like this than in creating an app. But we make money with solutions for our clients, not with selling frameworks. This has more then doubled our development speed, well worth the investment.
ReSharper can do that for several years.. 
appreciate the effort, cant really look into it during work when its a video though!
I wonder if it **might be down to missing vendor file**s. These get copied to wwwroot when you run webpack... webpack --config webpack.config.vendor.js webpack If you haven't got the WebPack command line tool you might need to run this too... npm install -g webpack One of the template authors (Steve Sanderson) [explains this in his post](http://blog.stevensanderson.com/2016/10/04/angular2-template-for-visual-studio/). Basically they elected to split third-party dependencies into a separate bundle controlled by webpack.config.vendor.js. The thinking was that rebuilds are faster if Webpack doesn't have to re-analyze large libraries like Angular 2 on every build (and you'll be adding third-party libraries quite infrequently). Hope that helps :-)
Cool, thanks for the info. I did wonder if the same idea has been used in other runtimes
do your devs do a combination of Development and IT? Do your devs end up researching/exploring things in Azure or other new tech that they then push IT to implement? Ignite is more about IT than it is about developers, but there is overlap for some, especially senior developers depending how you function as a company.
It might have been edited after a comment on the article brought it up. 
I saw this the other day on his blog, very cool!
Definitely agree docker has made things much easier. If not using some whizzy tool, use `dotnet publish ...` to prepare the directory structure, then that'll work with the `microsoft/dotnet:1.0.4-runtime` container. I got my latest thing running on AWS ECS with deploys triggered by travis. Took me a while to get it all set up though so you might need to be a bit handy with terraform.
So you're competing with Salesforce.com. GLWT.
In the sense that it might take less time to build a custom app from scratch then to customize something like salesforce yes. A lot of small companies get duped by these large CRM's. They're told it will only require a small amount of customization. Then when they get their foot in the door it turns into a multi-million dollar, decade long ordeal. Every time this happens the post analysis is always something along the lines of "it would have taken us less time and money to just build it in house". AppFoundry would let you build it in-house, to your requirements, very quickly. 
Thanks for the info. This process of building a business model looks somewhat similar to the model first approach using entity framework. However, then it seems to generated a starting user interface. That's definitely interesting. Just based on that link I would say building objects in AppFoundry is similar. The one big difference is that it doesn't require VS or writing any code. At that point though AppFoundry doesn't generate forms for you. You still have to build them using the designer. Even the simple example they showed can be visually represented in AppFoundry in three different ways with one of them being the same as what XAF generated. I assume you can customize this in XAF though.AppFoundry does provide starting templates and building forms is quite simple using mapped controls. Knowing XAF exists might give me some ideas for improvements. For example I suppose I could generate starting forms for objects. I would need to make some assumptions though. In general I doubt I could compete with what appears to be a well established and well featured framework. 
https://www.asp.net/web-api I would read through this. You can def use dapper as an ORM or you could use a entity generator for EF and create all your models and stuff that way. That's how i did it when migrating from classic asp.net to use web api with it. It's super easy to use though and returning stuff via web api is as simple as this.You will have to create a custom authorization filter, but that's straightforward as well and you should be able to find how to do that in the docs. public async Task&lt;IHttpActionResult&gt; Get() { var repo = new JobRepository(); var data = await repo.GetData(); // data is a list of objects, can be returned from dapper or EF // when returned from the controller, the web api will automatically serialize the data to json return Ok(data); } 
Isn't this almost the exact same process Scott Hanselman published a couple of weeks ago?
No, you can see the frameworks that are supported on the nuget page: https://www.nuget.org/packages/PactNet 
Why noy Azure?
This. Or AWS, which would by my preference. Both have a free try-out option. https://aws.amazon.com/free/ https://azure.microsoft.com/en-gb/free https://azure.microsoft.com/en-gb/blog/scalable-umbraco-cms-solution-for-azure-web-apps
heavy ajax in asp classic? are you sure you don't mean webforms or something else? feels really odd to have stuck with asp classic long enough to bother with much ajax.
No idea why you've been downvoted for making a recommendation in a thread asking for recomendations
Anyone know about any alternatives?
Need to figure out pricing and server specs. The client might not be willing to pay for much per month. On that note, I might consider both options.
I might consider it. Thank you.
Needs to service 12000 concurrent views, but the client might not want to pay that much?
As I said, I need to price Azure. I don't want the monthly pricing to go over $150/month. Views might not be concurrent, but that's the size of the site's clientele. I'm looking at some shared hosting options, which usually don't go over $20/month, but all options are open at the moment.
OH, okay. That makes more sense.
That would require replicating the hashcode implementation from System.String and applying it rather manually. Not impossible, not the most straightforward.
RavenDB pricing is also ridiculous.
Did you plagiarize this blog post from 2 years ago? https://www.captechconsulting.com/blogs/xbind-xphase-and-xaml-performance-in-windows-10 Entire paragraphs are taken word for word, yet you fail to attribute any source or mention that it isn't your work. 
This is a new implementation based on dotnet core. It takes all the good things from v1 like modularity and multitenancy and changed some things to make it faster. Orchard v1 ready has a workflow module that is better than anything you can dream of, just Google for it, especially one video from the conferences.
https://www.amazon.com/Microsoft-Visual-Step-Developer-Reference/dp/1509301046
Sounds like it could replace sharepoint for some workflow apps. 
I would recommend learning C# over Visual Basic. Have a look at [Stack Overflow's 2016 Poll results](http://stackoverflow.com/insights/survey/2016), C# is consistently one of the "most loved" languages while Visual Basic is the #1 "most dreaded". C# definitely gets preferential treatment with new features such as .Net Core for example. I also see many more job openings for it. If you have a decent understanding of programming and concepts like OOP then I'd recommend Adam Freeman's Pro ASP.NET MVC series to get started on web. That's the series I used. ASP.NET MVC 5 is the current framework for the full .NET framework and ASP.NET Core has been released since then but 5 is probably more applicable to most jobs since Core is so new.
Thank you for the advice, and taking the time to post on my month-old question! I'll give those books a read, are they specific to .NET/C# or just about dev in general? I've been in my position for a solid two months now, and I'm learning so much. It feels like I'm drinking out of a firehose... Maybe I should slow down and consider reading these books in my spare time instead of training videos in the evenings. Again, thank you!
Does it have post publish/render hook so people can implement plugins such as static rendering?
The Pragmatic Programmer is about the discipline of Software Engineering. It focuses on treating Software Engineering like a craft, just like any other craft. Turning the focus of "just producing code" into an apprenticeship. Clean Code is about how to write your code as cleanly as possible. Clean Code allows for better productivity, better maintenance, clearer understanding of your code, and makes communication with your peers easier. Neither book is focused on .NET or any type of specific tech, they are more about how to approach the discipline of Software Engineering. Having that kind of mentality will help you conquer any Software Engineering hurdles you may be having. If you feel you need to learn more about how .NET/C# works. These books might be best saved for later. But they will come in handy in the long run. It sounds to me like you have a grasp of being able to do your day-to-day job now. So maybe some general knowledge about Software Engineering would help. If you are looking for something to read that is C# specific I cannot recommend [C# in Depth](https://www.amazon.com/C-Depth-3rd-Jon-Skeet/dp/161729134X/ref=sr_1_1) enough. It is a rather dry read, but it completely covers all aspects of the language. It even does it in a chronological way so you can see the language grow and develop over time. If you have ever thought "Why does C# do it that way?" this book will answer that question. I found it fascinating to see the language grow over time and realize why the language is what it is today. Make sure to get the latest version of the book which I believe is currently the 3rd Edition.
On a related topic, I found that concatenating large amounts of strings is difficult to do efficiently in .Net. Let's say you have sequential parts of a document in an array. Theoretically, it should be possible to concatenate those strings together with exactly one memory allocation: * Loop over the array to sum together the string lengths to find out how much memory you'll need to allocate. * Allocate memory large enough to fit the concatenated string. * Loop over each string, copying into the new memory allocation. Unfortunately, every public method available in .Net doesn't allow for this. If you do the above by allocating a character array, then when you pass that character array to the string constructor, it will copy the contents of the array to a new array to use as string's internal array. It has to do this because strings are supposed to be immutable. If you retain a reference to the same array that string is using internally, then you could theoretically modify the array. This means that if you have a total of 100 MB of character data to concatenate, you have to perform 200 MB worth of content copies, 2x more than you theoretically need. If instead you pass a `string[]` to `String.Join()` then it's better, but still not perfect. Join will make a copy of the array's references before it copies the contents of the strings. If you have 100 MB of string data in an 1,000,000 element array, then Join is going to copy 104 MB or 108 MB of data, depending on how large your pointers are. That's a lot better, but still not as good as it theoretically could be. Using unsafe code it's possible to get this down to as efficient as possible - first allocate a large enough string via `new string( '\0', size)` then use unsafe to get a pointer to the underlying char array (which c#/.Net supports idiomatically) and then copy. I created a gist to show how to do this: https://gist.github.com/antiduh/426a4d22ab2449601842 public static string Concat( IReadOnlyList&lt;string&gt; list ) { string destinationString; int destLengthChars = 0; for( int i = 0; i &lt; list.Count; i++ ) { destLengthChars += list[i].Length; } destinationString = new string( '\0', destLengthChars ); unsafe { fixed( char* origDestPtr = destinationString ) { char* destPtr = origDestPtr; // a pointer we can modify. string source; for( int i = 0; i &lt; list.Count; i++ ) { source = list[i]; fixed( char* sourcePtr = source ) { // `Buffer.MemoryCopy()` was first introduced in .Net 4.6 There are a // few techniques for implementing your own version of this method if // you need support for .Net 4.5 or below, // for instance: http://stackoverflow.com/questions/2658380/ Buffer.MemoryCopy( sourcePtr, destPtr, long.MaxValue, source.Length * sizeof( char ) ); } destPtr += source.Length; } } } return destinationString; } 
How is that better than `string.Concat()`? It seems to work exactly the same way. https://referencesource.microsoft.com/#mscorlib/system/string.cs,3067
Isn't this kind of thing what `StringBuilder` is made for?
&gt; Buffer.MemoryCopy(..., long.MaxValue, ...); That seems like a really bad idea, especially in combination with the omission of the argument copy that `String.Concat`/`Join` create. 
StringBuilder is one of the absolute worst in this scenario, since it makes intermediate copies of the content. Great for other purposes, but terrible for this particular purpose.
There's still a full copy when you call the `StringBuilder.ToString()` method to create an actual `string`.
* When you call `builder.Append(string mystring)`, it copies that string to its internal buffers. * When you call `builder.ToString()` it copies its internal buffers into a new string object. That means if you have 100 MB of characters, you have to perform 200 MB of copies. You just cut your performance in half. Woops. 
He means Git. As we know, GitLab is a competitor of Github.
Performing double the minimum needed copies hardly makes it "one of the absolute worst".
Since dotnet is now open source, why not contribute this change, prove it works under all existing test cases and prove that the performance is better - you'll be a hero.
You are right. I added text from your link to my post. I edit my post in another tool and copy it to wordpress. When i write this post i want to copy last part of my post to WP. Intead of this I copied (by mistake) part of this article (from clipboard). This is my fault. I change end of article to my version and add link in post. I appologize for that. Thank you for notification. I am grateful to you.
No surprise this, CodePlex has been an open source graveyard for some time (some might argue it always was). I can't think of a major OSS project that isn't on Github now - even the Ruby and Python projects have moved over. SourceForge does host a few useful projects that people still download so there is a reason for it still to exist but I can't remember the last time I downloaded anything from CodePlex, it was probably years ago.
Can you name some repositories/projects they've removed?
Gotcha. Let's say we had extension methods / functions that did those StartsWith things you did - how would ArraySegment perform vs your implementation? Ran any benchmarks?
"Apress Pro C# and the .Net Framerwork" is a good book for programmers who want to learn C#.
I would start off by creating a simple website using .NET MVC. It will give you a good understanding of the workings of a .NET website. There are a lot of fantastic resources on [pluralsite](https://www.asp.net/mvc/videos/pluralsight-aspnet-mvc-5-fundamentals) 
I disabled git on 2013 as it significantly slowed some operations, but haven't yet on 2017. Either they fixed it, or they optimized something else. I really missed git integration, especially with code lenses. I use NCrunch on multiple large project. If it means anything, NCrunch used to warn about git integration on 2013, no more in 2017. 
Because we are provide asp .net development services.
A couple years ago when my team switched to git, the integration was so horrible. We are planning to move to VS2017 soon, but I don't​ expect the situation to improve. I am not so sure I want it anyway. I particularly​ do not use any IDE integration and use the command line direct. I would love to see something like the Package Manager Console for git. 
Good to know.. nice post, Thanks for sharing..
I agree that most of the integration in VS is pretty useless. The only features I use are blame/annotate, and the codelens integration. When the integration was disabled, it was a bit annoying to have to go out of VS to perform a blame.
I'd really live keep everything in one IDE/Editor but sometimes its a little frustrating working with VS17 when VSCode is so light weight.
Most of our devs use VS to edit the C# backend code, VSCode to edit front-end TS and chrome to debug the front-end. Some die hards use VS to edit front end as well but I don't really see the point
If I were hiring, I'd disqualify you just for posting this here.
Thanks. I read patch notes for every update they push out, but this refers to an issue that's been in VS since Git integrations inception (VS2013?) So it's a bit difficult to find anything concrete that pertains exclusively to VS2017.
ssh works for me now. http only was annoying.
I use VSCode. I tried to have just VS2017 but I quickly went back to normal setup by having both.
I can't help you much on that... Maybe list all libraries, frameworks an nuget packages, then check their compatibility on Google?
https://packagesearch.azurewebsites.net/ Or search the corefx and coreclr repos on the dotnet org in GitHub.
Well, I started with a string, so it would cost at least one extra copy. If I switched from reading a file as a string to reading it as bytes, it would save a copy + transcode (because I would assume and validate that the input is UTF-8, whereas System.String uses UTF-16). I would have to do a fair bit more work, but it might be worthwhile. I'll implement that this week and post my results.
I'll have to look into it again. It definitely was aimed at small apps though and while mobile is a feature, that's not it's only feature.
I definitely prefer VS Code for front-end development. Lately I've been using TypeScript and the TypeScript Hero plugin for VS Code is one of the best things that has happened to me lately. 
I use vs code for frontend because I use the angular-cli for build/run/debug of my Angular 2 project. Getting that to glue together with VS 2015 in a dotnet core project was a nightmare so after months of wonkey half assed solutions we finally just dumped vs2015 for vscode when it comes to front end.
Which does....?
VS Code or (better) Webstorm.
https://apisof.net/ is another good source.
Alt-click and drag to select a square (or vertical line) and edit multiple lines at once. View -&gt; Other Windows -&gt; C# Interactive, to open an interactive REPL in which you can evaluate C# expressions while writing your code. Right click a project in the solution explorer -&gt; Initialize Interactive with Project, to load all the code from your project into the C# Interactive window, allowing you to quickly test portions of your code without having to start the entire application. You can right click a breakpoint to set conditions for when the breakpoint should trigger. Debug -&gt; Windows -&gt; Exception Settings, allows you to choose which exception types should cause your debugger to break instantly, even in a background thread or when an exception handler is registered further up the call stack. If you have string variables in your autos/locals/watch window, you can use the little triangle button at the right of the Value column to visualise the contents of the string in a text, HTML, XML or JSON viewer.
I use Notepad for everything, it's just so lightweight.
Alt-drag works in all sorts of places like Notepad++. And using it proficiently makes you look like a deity. We deal with lots of processing where there are large numbers of fields. I use code alignment extension mixed with this trick to do some crazy stuff. Lastly, EditPad Light actually lets you alt-select without having to drag. You can click to set your cursor position then scroll down your document and alt-click then just start typing! This saved me from having to use a (very slow) macro in notepad++ or having to write some code.
I prefer VS Code for both, since I'm mostly using .NET Core. The key bindings and settings are just a lot easier to me. Wish there was some kind of key binding profile I could use for Visual Studio that matches Code. 
I love the integration. It is really nice when hunting bugs to click on a method and view the commit history for that specific method. It is also nice to see who wrote what. 
For JavaScript (not just front end, but NodeJS as well), I definitely prefer VS Code. VS is just too heavy and JS still feels like it was added as an afterthought. I even use VS Code for blogging now by writing in markdown and then using Pandoc to convert it to HTML. VS Code takes a little bit of work to get set up, but it's a cleaner environment. I just wish they hadn't remapped so many common shortcuts. 
I'll consider it. Sounds like you are a happy customer. What are your favorite features?
Are you placing your css in the head before you load any content? If you put the style tag after the html elements that display data, they won't load until after the grid finishes the query and render.
So when you build something in AppFoundry do you have to keep using AppFoundry to maintain it? Or does app foundry create a visual studio project that you can use as you wish? 
If it's anything like latest EAP - then bugs, bugs everywhere 
Also personal edition you're allowed to install on home + work computer at the same time.
A way around this is by using feature flagging. It allows you to deploy a feature to prod switched off so you can turn it on when it's required/when that other project is ready. The plus of this is you discover any integration issues very early on rather than when you merge in a long lived branch. Spotify follow a similar process with feature flagging check it out [here](https://labs.spotify.com/2014/03/27/spotify-engineering-culture-part-1/). 
I started using it in my latest project when time is important. It works pretty well! It is not collecting anything until my say so. To this article I would add that from my experience GC.Collect() runs always when you call it within mentioned region. 
The CSS link was originally in the head, but I've just tried placing right at the end before the closing BODY tag and it's still the same :(
Unpopular opinion, but don't we even mention the Desktop as a platform anymore now? It is covered by the "Mobile Applications" category by Xamarin I guess...
Not that I have anything against freely given knowledge, but handing out early draft papers is a bit lame. Most people who might be interested already have a job, so taking the time to read something unfinished is a big ask. Make them public when they're finished...
That's a great question. We use feature flags a lot for these types of conflicts but there are other options as well. Check out my Release Readiness post that goes into more details - http://www.dotnetcatch.com/2016/02/16/are-you-release-ready/ Our teams had this same question 3+ years ago but we tried it as an experiment and its been very successful across 10 teams and 120 people.
I am still butt hurt over their subscription model! I probably need to cave in :(
By some parts I meant code &amp; patterns. Those seem complete, and I agree with you about inefficiency. I'd wait for the books to be finished too.
Just a word of advice, I'd take a look at some example material and, in particular, some of the Microsoft libraries tested in the cert and look into those a bit. I tried taking (what I believe was) the MCSD some time ago, and there were many concepts (i.e. lock) and libraries not covered throughout my entire associates degree (CIT: Developer @ Northwestern Michigan College) which focused primarily on C# in terms of programming languages. 
Windows applications have an entire site https://developer.microsoft.com/en-us/windows
MSFT has some intro articles: https://docs.microsoft.com/en-us/aspnet/core/migration/ Migrating MVC: https://docs.microsoft.com/en-us/aspnet/core/migration/mvc Migrating Web API: https://docs.microsoft.com/en-us/aspnet/core/migration/webapi otherwise, if you have a specific problem or question, feel free to post it. we've been using .NET Core for over a year now. 
A lot of folks are and I don't blame you one bit. That was a big failure on their part. 
Where it was (at the top in the head) is preferred. Do you have a git repo or a code sample ? 
We do it all where I work, but we get help from others on our non-specialized stuff. I do end to end. There are some people who help with the visual-design, visual-UI, and customer interactions, but I still talk to the customer a bit, and do basic UI stuff. It depends a lot on project size, we are 8 in the office and we just got a PM. 
I am an alumnus of NMC and I love that school dearly and I would encourage anyone who asked to go there (despite the current dipshit president). But I've also been programming computers for a long time and have dealt with a lot of graduates from their software developer program and I promise you, you learned nothing about C# or about programming in general. I don't know you, maybe you are a great programmer but if you are then you are in spite of having gone through their program, not because of it. We've interviewed dozens (ok, maybe one dozen) applicants that graduated from there and even hired a few. We were shocked at how shallow their depth of understanding was and how narrow the focus of the program seemed to be. We learned our lesson, and because of them, we have truly improved our interview process. Regardless, I was floored that so many of them felt they were qualified to be professional programmers. We should have done a better job filtering them out, but they should never have said yes to a job offer that clearly explained what skills and knowledge they would need when they clearly didn't have them. NMC's software development program is an overly long, overly expensive programming boot camp. They are both great for finding people with the interest and basic skills to go on to be a computer programmer, but completing either of them does NOT make you a computer programmer. In all fairness though, even a four year degree in programming from a university only prepares you for a job that is willing to invest a lot of time in teaching you how to apply what you learned to a real project.
I am a full stack developer totally by choice. I wouldn't work somewhere with more specialized roles.
Been doing this professionally for 12 years in the Microsoft stack the entire time (Web Forms, Win Forms, ASP.NET MVC, Web Api, SQL Server, etc). Every shop I've ever worked at, developers were all "full-stack", "end-to-end", "generalists", whatever you want to call it. The only exception for technical roles were QAs and DBAs, and that was only for shops that actually had them. Lately in the industry I'm seeing calls for more specialization among developers, but to be totally frank, it's mostly JavaScript devs I ever see harping on this. I've worked at four different shops across two different companies, and my experience has always been that most shops just don't staff for that kind of specialization. That's not related to shop size, either, but instead to the sizes of individual teams/projects/assigned areas of responsibility.
Personally I have been a full stack developer for my whole career so far (6 years) and generally most .NET jobs require you to know the whole stack. I worked as a consultant the first 4 years and did 10+ projects for different clients in that time, all of which required full stack experience. On the other hand my wife is also a developer but she specialises in Angular and at her current role she is in a team of people who only do the front-end and there is a separate team for the back-end. I still think though, that most roles you will find out there would still prefer a full stack dev.
do you guys feel Resharper is still beneficial after the many improvements in Visual Studio 2017?
&gt; JS still feels like it was added as an afterthought That statement can stand on it's own. :)
Can you provide an example or an explanation of how you separate auth in a different DB then have the application communicate with it? Thanks.
double check your configs. Manually enable detailed errors so you can see what the crash is. It is most likely either a bad config or there is an access issue with the DB instance.
This has been my experience too. When I first started in the industry (almost 20 years ago, damn), I worked for a small software company. There were 2 developers, including me. So, we did it all from specs, development, testing, deployment, even hardware. I now work for a fortune 50 company. Things are done a bit differently here. Everyone has a specialization. I need a site set up, that's a request. Need a new database, request. Firewall rule, request. Toolset, request. Deviate from the norm, request. It's 2 different mindsets, each with pros and cons. On the one hand, I've never had such a fulfilling job as doing everything from a to z. On the other hand, with specializations, I can focus really closely in one task or feature and do a truly compete and thorough job at it. 
This is something Spotify does. In my team we use compilation tags for features so we can compile and deploy the same code base but with different compilation tags in the build. Once the feature is accepted we remove the compilation tags and deploy with the next release.
&gt; Visual Studio already runs on macOS This is not true. "Visual Studio for Mac" runs on Mac OS X, but not Visual Studio. Those are completely different products.
Disregard this entire post. I couldn't see the wood for the trees. Same dude has written a database first tutorial.
Umbraco will probably do what you need
Thanks, I wasn't aware no, will observe.
Before blindly porting your MVC 5 to MVC Core, I suggest learning the ASP.NET Core first (https://github.com/dodyg/practical-aspnetcore)
Is there any write up on this? Any place that I have worked so far does a terrible job with this.
Visual Studio for me I'd rather have one IDE/editor set up exactly how I like, rather than two that are slightly more suited for an individual task. I just find it means I can forget the IDE entirely and focus on the application I'm trying to actually produce. At the end of the day, the IDE is just a tool to get the job done: the more I can push it into the background the better. No trying to use a feature that doesn't exist in one or the other, no searching the menus for something that's moved between the two etc I only develop on machines that are powerful enough to run VS without noticing, so there's no concern about how lightweight the editor is other than the options, and...well, I can just not click buttons I don't need right now
dont forget to learn about migrations. its super easy to do and adds substantial version control to your database. you could even include it as part of your app-start process so that your database is always in the right form whenever your application starts. 
"I am blind" You had me thinking you were ACTUALLY blind! haha
I didn't catch that. I am blind, you see?
Actually I'll just give it to you: firstly, make sure your entity framework stuff is in its OWN SEPARATE project. (it can be a part of the same solution, but separate project for managing your entities will be a life saver) you need 3 commands to execute in package manager console Enable-Migrations will configure your project for migrations make some changes to your entity models (or define them in the first place) Add-Migrations &lt;migrationName&gt; Will create a migration file class time stamped and show you the process it will go through. the UP function is the transformation to bring to the current migration. The down will go down to the migration. Make sure your package manager console is pointing to the right project and you have a DbContext/inherited class defined with a connection string. It needs a database to compare. Update-Database will update the database you have defined to the newest version of your migration. Update-Database -t &lt;migrationName&gt; to target going back to a specific migration. Migrations will automatically go up/down as necessary to migrate the database back to that specific state. 
X-Post referenced from [/r/programmingtools](http://np.reddit.com/r/programmingtools) by /u/OzmosisGames [VSBlockJumper - A Visual Studio Extension that allows you to jump over blocks of code](http://np.reddit.com/r/programmingtools/comments/63mqst/vsblockjumper_a_visual_studio_extension_that/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Its been a while, but I had a really good experience with EpiServer CMS Version 5. They are up to 10 now. http://world.episerver.com/cms/ 
We've had success with http://www.kentico.com/. 
You **should** push the `Startup.Auth.cs` file, but that file should **not** contain secrets. Those you should add separately (e.g. a not checked in Web.config transformation, or environment variables).
Thank you.
What benefit does this have over Run to Cursor?
That's the one. 
Nope, it's the other way around. WPF is probably the best choice for rich Windows desktop applications these days. 
Boooring!
You are forgiven. 
That wouldn't exactly make sense. .NET Framework explicitly requires Windows OS as it utilizes APIs only available on that system. .NET Core any is generalized to be able to target a large variety of OSes. It is hopefully intended to support .NET Standard 2.0, though?
Yeah. I do winforms development thanks to the major api we use still holding onto activex controls. Kill me
Here's the Spotify video on their development model. They use feature toggles to turn on and turn off features not ready for use. https://youtu.be/R2o-Xm3UVjs I can try and write something up tomorrow about how we use compilation tags. 
I think I confused this as a .Net Standard release as well. Thanks for clearing that up.
Not Metro/WinRT/UAP/UWP/whatever it's called this week?
UWP isn't as mature and has some additional constraints WPF doesn't have because it has to function on mobile devices. 
I think this graphic does well to help explain the relationships (it doesn't help that the names all seem rather arbitrary): https://docs.microsoft.com/en-us/dotnet/articles/standard/components I'm also a big fan of this post by Scott Hanselman: http://www.hanselman.com/blog/WhatNETDevelopersOughtToKnowToStartIn2017.aspx
I have done migrations before, including full data migrations between differing database structures. Difficult, yes. But mind boggling? IMHO there comes a point where maintenance/extensions on an increasingly spaghettified codebase outweighs a nuke and pave. And Web Forms is infamous for its ability to mutate into spaghetti code extraordinaire with shockingly little warning or effort.
We do full data migrations all the time as part of our implementation for customers. Most of the time, from many different data sources to our database. I think that's actually easier than it would be to migrate markup and code because screens have been written differently over the course of 10 years. We also have 30 widely different customized versions of our software for clients. Data is easier because a lot of the time it's either already in some sort of uniform structure or it can be manipulated into that. 
Yeah, this just bit me last week, had to revert back from 1.6 to 1.5. And now all my `BitArray`s are useless because they can't `CopyTo(byte[])` in 1.5.
And now I can't wait until they publish the targetting packs for earlier versions of VS. Just a couple days ago we got a ticket in for our UI going wonko when DPI scaling, so imagine my surprise when after 15 years, they suddenly fix this just as soon as I need it.
Most of my company's internal apps are written in VB/Silverlight, I feel your pain. 
Does it have native support for C# 7? Can we stop using the nuget package to use the new tuples?
It says it does.
A guide to monitoring execution of Azure Functions, using the built-in tools provided by Microsoft. As a better alternative, you can use the power of elmah.io.
Old skool VB? Because VB.NET can use WPF (or UWP if you only care about Win10).
Yeah, .net core runs like shit on Windows /s
No, it's not, read the text under the table: &gt; The table above reflects the mappings that will happen when we release the .NET Standard 2.0 tooling. You can see that .NET Framework 4.6.1 mapping is being moved from 1.4 to 2.0.
Repost of https://www.reddit.com/r/dotnet/comments/63ookp/announcing_the_net_framework_47/ ... With only tracking parameters added.
The biggest one would be Visual Studio. If I remember correctly, zone parts of Windows are also written in it. I used to think of Qt as the main competitor to .NET in the desktop app space, but as of lately Electron.js has been gaining a lot of traction. All of these (and even Windows Forms, with Mono) are more portable solutions, which is pretty much the only argument against WPF I can think of. As for MVVM, the worst thing about it is the amount of boilerplate code it requires. Do learn how to write it from scratch, but after that look into frameworks like MvvmLight.
I searched for 'List' and the obvious List&lt;T&gt; was way down at the bottom. The first result was a Windows Form ListBox... I am happy they are finally centralising all documentation but they might need to work on their search algorithms a bit more as I can see myself simply going back to Google for actually searching through the thing; which, incidentally, is the way I search through Microsoft documentation today.
VS Code has code lens for TypeScript https://code.visualstudio.com/updates/v1_11#_implementation-codelens-for-typescript 
Bricked my visual studio...bricked a bunch of others too (avoid)
Yes i hope they don't. But it also means we need to maintain application with 2 framework in future. 
I have the .Net Core 2 beta but I couldn't get a project building with it. I couldn't get Visual Studio to recognize it even exists. Manually using the csproj file didn't help either. Building with the dotnet command line tool failed as well, I couldn't even run a restore. [I followed the dogfooding example from GitHub.](https://github.com/dotnet/corefx/blob/master/Documentation/project-docs/dogfooding.md)
Those are the correct instructions. Does `dotnet --info` tell you that it's 2.0? If it does, you might want to file an issue on the corefx repo asking for help.
Rider is great but just the TFS integration is buggy is reason enough for me to avoid it. No idea how the git integration works but I imagine its better than the TFS version.
Its so fast for me to use Google to find the MSDN page that I never looked for another way.
Which versions of .Net Standard will be supported/implemented/targeted by this?
I don't work on Sitecore, but there is a big project here. The issue I see with Sitecore is that it seems to take over the platform, becoming the platform. In my experience with EpiServer (which isn't cheap either), it always felt like they were enhancing the .Net platform rather than replacing it with their own. 
I'm sure the performance of .NET core on windows is just fine. What you lose out one is having to find replacements for many of the libraries that you are accustomed to using. In some cases this means resulting to writing your own Win32 calls.
P.S. Unless you are using Entity Framework Core. Performance for that really is shit.
&gt; Difficult, yes. But mind boggling? Depends on how you used it. If you kept things simple then I don't expect it to be too bad. But if you really got into building your own custom controls that took full advantage of the page lifecycle then it could be a nightmare.
When Web Assembly is implemented in all browsers, Silverlight will be back.
Still beats the old method. Extension methods require a special attribute. If it didn't exist because you were targeting an old version of .NET, you had to create the attribute yourself and compile it into your project. 
I've been having trouble loading my .net 4.6.2 webAPI project in it. Something about a few EntityFramework-specific logic doesn't compile. Hopefully this version fixes it.
Web Assembly has me quite excited. 
You mean Run to Cursor while you're debugging? This is more of a text navigation tool, in the same way you use ctrl+left or ctrl+right to move to jump a word you can use the commands in this tool to jump a block (up or down).
Good feedback, thank you! We are still working on tweaking our search index, so anything like this helps.
If the data you're getting back fromthe API call is in JSON format, you can use a deserializer to deserialize it into an object and translate the object to put it inot SQL (using EF or SQL Insert statements). Please check below for sample deserialization in C#: http://www.newtonsoft.com/json/help/html/DeserializeObject.htm
Thanks. So will I need to define a variable for each field in the JSON response or is it smart enough to figure out what each field is?
That was the plan at first, but they changed it.
SQL Server 2016 also has JSON support so you do not necessarily need to serialize the JSON object to a c# object and then store all the properties in seperate columns. You can use SQL Server's new functiaonlity to store the JSON object and also do queries on it. Check [this](https://blogs.msdn.microsoft.com/jocapc/2015/05/16/json-support-in-sql-server-2016/) out!
&gt;None of this core versioning makes any sense anymore. I'll just cover my head for the next six months and hope it makes sense then. -- me for the last 2 years 
Is anyone using Rider for developing ASP.NET MVC 5 apps? If so how is the experience so far?
Your string called 'json' would go there. The one you have in the code snippet above.
&gt; String json = connection.getADPData("/link/link"); If I get what you're saying, this line of code is returning a JSON object in a string? If that's right, the JOBject.Parse(json); will give you a JOBject with all the json properties that were in the JSON string. You don't have to set up the JOBject at all, it automatically determines whats a property etc inside the json string you give it. 
Yes, you're correct that is returning a JSON object. So I don't need to define any public classes for the data? It'll already know what they are by what's in the json string? How do I access and use the properties? More specifically, how can I use them in a SQL insert query. Thank you so much for your help on this.
It's a complete end-to-end platform so you would need AppFoundry to maintain it. It's meant to replace all the other tools, libraries and languages with one platform that is designed specifically for building enterprise apps in order to save time. Rules and validation logic are added in .NET assemblies created in VS. However, these are then picked up by the website and applied automatically. You could optionally use it only up to the business layer and then it would be similar to other ORMs, but you would lose the ability to create apps without writing any code. 
Have you tried to install the 64 bit remote debugger? https://msdn.microsoft.com/en-us/library/y7f5zaaa.aspx
:/ seems like a problem with their installer. It's not updating the dotnet executable on windows that runs and I don't think Visual Studio has any idea what to do either. It's a pity the docs focus so heavily on *NIX systems, adding things to your path doesn't work on Windows. I suppose I could directly reference the files from my shell but that seems like a huge time sink.
Thanks a lot! Guess I will have to give Auth0 a shot. I've never used anything like that but I will check it out this whole weekend and hopefully having it working soon.
[FYI](https://www.matthewproctor.com/json-to-c-sharp-class-using-paste-special/)
UserManager has a users property on it that has a collection of all users
Will give another +1 for Auth0. I used stormpath for a long time, and loved them, but they are changing things up so I am migrating to Auth0. They do things differently, but just as efficiently as Stormpath. They are definitely headache savers. Before moving to to a third party provider, it was safe to say almost half of our time on big consumer facing apps were dealing with Authentication or Authorization. These problems don't go away altogether, but reduces the work dramatically. Also note that third party auth providers are not all created equal, Auth0 is, from our research, head and shoulders better than other we looked at.
The "Build a CMS With ASP.NET MVC5 " Tuts+ course was the first tutorial I could find with halfway decent coverage of Identity. The source code is here: https://github.com/tutsplus/Building-a-CMS-With-ASP.NET-MVC5
I promise I don't work for Auth0, but I will just go out there and say that Auth0 is the best auth provider out there, period. Note that it's not cheap if you need some of their enterprise features (AD integration, HIPAA BAA, etc), but most people don't need those things. They also have some of the best "getting started" documentation I've seen, and it's extremely easy to use from a developer perspective. Add 1-2 lines to your app startup code and bam, auth just works as long as you pass a valid JWT to the server. The only bad thing I can say about them is that their Authorization (Roles) plugin is flaky or at least it was the last time I used it. It's supposed to add claims and things like that to the user's JWT token, except...it totally didn't. Maybe I was doing something wrong.
What else could this be?
thanks watching this 4 hour video lol I never knew idenitity was so monolithic : https://www.youtube.com/watch?v=oHMjJ_go8EA
I just recently set Roles and Groups up and everything works correctly. Gotta make sure your scope is set the right way for it to come through.
Keycloak is really great. A very polished open source tool. 
MVC has always been superior in every way. You just need to know how to configure it for your desired purposes. Web api has no purpose at all. Except self hosting which I don't think is an issue anymore with Core
.NET Standard will be less work if it actually meets your needs. Shared code will allow you to take full advantage of each platform. In my open source projects I tend of offer both because I don't know what my users are going to need. 
.Net Standard is a kind of lowest common denominator for what different ~~jvms~~ run-times/platforms support. 
Think of .NET Standard as a set of interfaces. No code, just interfaces and definitions of methods. (It's not actually a set of interfaces, but the concept is the same.) Now think of .NET Core as the implementation of the "interfaces" in .NET Standard. Now think of .NET Framework as the implementation of the "interfaces" in .NET Standard plus additional code not defined in .NET Standard. .NET Framework is .NET Core with additions not defined in .NET Standard.
Since I use Azure for everything, I found [Azure Active Directory ***B2C***](https://azure.microsoft.com/en-in/services/active-directory-b2c/) suitable. It becomes easy to setup once you get used to it, and it works great with "easy" authentication of App Service or without it. It is not fully released (GA) yet, IIRC, and sometimes it can be hard to find good code examples. Meanwhile I am going to try Auth0 for myself since it's quite popular. But I am surprised no one mentioned B2C in a Microsoft community :P 
Really useful, thanks! 
Thanks! This looks very good as well. Wonder if it might end up being better than auth0. Certantly auth0 is easier to set up. How easy/fast is Azure Active Directory B2C set up?
I've found nothing that's not better in 2017.
tl;dr: MVC (by default) uses JavaScriptSerializer, WebAPI (by default) uses Json.Net.
If that's true then angular does it wrong, since pretty much all templates are html fragments pulled from the server. 
&gt;I've found nothing that's not better in 2017. Try to use CodeContracts in VS 2017 😜
Did you update your Mono installation?
this link actually works http://www.codingdefined.com/2017/04/optimization-tips-in-entity-framework.html
Sorry my sites are so much faster than yours. And require next to zero javascript which is why they run so fast. Client-side templating is a colossal waste of electricity. 
There doesn't seem to be a Mono 5.0. Neither in the regular releases (https://download.mono-project.com/sources/mono/) nor in the nightly releases (https://download.mono-project.com/sources/mono/nightly/).
I just installed the latest VS for Mac... no idea how that is possible.
I had VS for Mac installed already then it prompted me for an update. When I checked the different components one of them was for Mono - version 5.0. I realised this was an Alpha release which seemed odd as I had the updates on the 'Stable' setting. Anyhow, this seemed like a bad idea so I didn't update. I'd see if you can install a previous version as mine has been running reasonable well. FWIW my installation is build 7.0 1659.
Yeah... my Visual Studio on macOS only seems to work with Mono 4.8.x at the moment (seemingly for IDE internals) but you can target other .NET frameworks in your solutions.
I think you missed the points, though it could have been written better. &gt; You can use a Where clause […] Sure, but the admonishment is not to call `ToList` in order to access a `Count` property. &gt; It's doing a LEFT JOIN […] The part to focus on is the `Include` method, so that the values for the `UserDetails` property are loaded efficiently. 
I would personally hope that a list like this might include some tips to create views on the DB when querying for particularly complicated relationships rather than letting LINQ try to generate a mess of a query. Also to always consider the cost of hydrating large/hierarchical result sets. Some good general best-practice tips though.
I'll take it a step further - if you're just fetching objects from the database, skip the AsNoTracking and go right to turning off proxies. This means no tracking, no lazy loading, nothing but POCOs. Unproxied POCOs behave nice and predictably, and error out pleasantly if you forgot to .include something that should be eager-loaded. You can even feed them into a Json or Xml Serializer (except the stupid fucking XmlSerializer that shits its pants at the sight of IList).
[jQuery Chosen](https://harvesthq.github.io/chosen/) is a neat choice for multi-select inputs.
Look at "AutoFixture". Is perfect for generating test data to be passed to your services while testing.
So I am boarding on Integration Test over Unit Testing according to normal convention. The APIs I'm attempting to test are thing like GetGamesByWeek(int Week) GetActiveGames() etc. My goal is really to test my EF queries and make sure I'm getting the correct data filtering as that's where I'm seeing most of my spaghetti code. I think after doing a bit more research last night im going to add a whole EF implementation into my Test Project and seed it with static data. This just seems like a anti pattern...
Sorry, I made a Edit. The "is this correct" is referring to the way I'm making Linq references within my Games Model in the test data. Also no, I can't even get the tests to work correctly in VS2017 right now - separate issues after trying to just make a simplified failing test.
It may just be semantics but at this point I'm not sure that you're talking about mocking data so much as seeding test data into a database that you'll be accessing in your integration test. As far as creating an entire EF implementation in your Test Project if you separated access to the database into its own project in your solution then you should just be able to reference that instead of reimplementing it. I think this goes back to the question of what exactly are you wanting to test? Just saying GetGamesByWeek() isn't sufficiently narrow. While its possible to write an integration test that goes through your controller, service layer, data layer and retrieves data I don't know that it's something that I would personally want as a coded test that runs regularly. That's a pretty brittle test since the scope is so large. Think about what you would learn if that test were to break. If your test fails, what caused it? FWIW, here's probably the order that I'd go about writing tests during development (**Note: This is not TDD**). * First I'd write an integration test that uses my data layer project just to test the methods on my GamesRepository. That lets me know that I've got all the mappings and whatnot in place and that I'm able to get data into my code from the DB. * I'd start up my project, set a breakpoint in my controller, and debug through a call to my controller method as it makes it all the way down to the data layer. * I'd probably write unit tests for my service layer showing that they bubble data they get from the data layer up (perhaps transforming it if necessary). * Then I'd write a combination of unit/integration tests against the controller to verify that they are accepting params for the week correctly and serving up what they get out of the service layer (potentially transforming it in the process). This process lets me see early on that I'm most likely do get wrong during the development (the ORM stuff) is working early on. I get that this isn't TDD but I wanted you to see an example of what I mean when I say "What are you wanting to test". 
Thanks for taking the time to reply, this conversation is extremely helpful. I am definitely talking more about Seeding data then Mocking data at this point I guess. The problem I'm having is in essence what your suggesting I do (set breakpoints and walk code) is exactly where I'm running into issue - and its clearly a result of bad code. My code is such trash in this project that I hesitate to post full code, but for example I have a API called ListActiveGamePick(int WeedId) (code at end). As you can see, 1) It's horribly coded as this was my first real project 2) It has a ton of EF calls to get data, then takes that data and transforms it as necessary - everything I have is just extremely bloated right now There are a lot of EF calls in order to get to the final result, 1 EF call depends on data from the previous EF call. What I'm working on doing is refactoring all the functions to further abstract all the EF Code into single responsibility functions. So taking the first large EF call at the very top my plan is to make a new function GetUserEnrollments(string UserID) {&lt;ef code here&gt;} and then I need Mock/Seed data to verify that 1) Given a list of 4 Seasons, the user is Enrolled in 2 - I only return a count of 2, and that it is a correct Enrollment model with XYZ Properties. Then I can be sure my next code will have the right properties, is the right type, and most importantly makes sure I'm getting just the Season's the user should be displayed. The only way it makes sense for me to do this is to have my Unit Test have a set of static Seed data that I can make my assertions against. public IEnumerable&lt;EnrolledGamesDTO&gt; ListActiveGamePick(string UserID) { // Get only the Active Seasons/Weeks the user is Enrolled in var enrollments = _dbContext.Enrollments .Where(e =&gt; e.ApplicationUserId == UserID) .Include(e =&gt; e.Season) .ThenInclude(s =&gt; s.Weeks) // This filter includes only a Season that contains an active week (but will return all Weeks for the season) .Where(a =&gt; a.Season.Weeks.Any(w =&gt; w.IsActive == true)) .GroupBy(e =&gt; e.Season.SeasonId) // Safety encase a user is "double enrolled" .Select(e =&gt; e.First()); List&lt;EnrolledGamesDTO&gt; ActiveGames = new List&lt;EnrolledGamesDTO&gt;(); foreach (var enrollment in enrollments) { // Get the active Week from the Enrollment // TODO this is a potential bug, we assume only 1 active week per Season/Sport type can be returned var Week = enrollment.Season.Weeks.First(w =&gt; w.IsActive); EnrolledGamesDTO GameDTO = new EnrolledGamesDTO { Year = enrollment.Season.Year, Sport = enrollment.Season.Sport, Week = Week.weekNum, Closed = (DateTime.Now &gt;= Week.SubmissionDeadline), Games = new List&lt;GamePickDTO&gt;(), //Populate this Weeks Games below Breaker = new List&lt;BreakerPickDTO&gt;() // Populate the Breakers seperatly }; // Get the Games for the active week with user picks TODO refactor naming var Games = (_dbContext.Game .Where(m =&gt; m.Week.WeekId == Week.WeekId)) .GroupJoin(_dbContext.UserPick .Where(u =&gt; u.ApplicationUserId == UserID), m =&gt; m.GameId, up =&gt; up.GameId, (m, p) =&gt; new { GameId = m.GameId, aTeam = m.aTeam, hTeam = m.hTeam, hFavor = m.hFavor, Favor = m.Favor, Spread = m.Spread, isBreaker = m.isBreaker, Pick = p }); foreach (var m in Games) { if (m.isBreaker == true) { /* This section is the refactor via //int[] GameIds = new int[] { m.GameId }; //var Breaker = GetBreakerGamePicks(GameIds) // .Where(mp =&gt; mp.ApplicationUserId == UserID); //GameDTO.Breaker.AddRange(Breaker); //var xyz = ""; */ var Breaker = new BreakerPickDTO { GameId = m.GameId, aTeam = m.aTeam, aScore = 0, // default to 0 if no pick is found hTeam = m.hTeam, hScore = 0 // default to 0 if no pick is found }; // For the Breaker Game, get the submitted breaker scores to display var BreakerPick = _dbContext.BreakerPick .Where(b =&gt; b.ApplicationUserId == UserID &amp;&amp; b.GameId == m.GameId) .ToList(); // if a user has a BreakPick already mod the DTO with the users // previously submitted score if (BreakerPick.Count != 0) { Breaker.aScore = BreakerPick.Select(bp =&gt; bp.aScore).FirstOrDefault(); Breaker.hScore = BreakerPick.Select(bp =&gt; bp.hScore).FirstOrDefault(); } GameDTO.Breaker.Add(Breaker); // break the iteration for now continue; }; // All this shit just to default to a null pick...gotta be a better way! string pick = ""; if (m.Pick.FirstOrDefault() != null) pick = m.Pick.ToArray()[0].Pick; // WTF is this? Why is it required to cast to Array just to access the string object GameDTO.Games.Add(new GamePickDTO { GameId = m.GameId, aTeam = m.aTeam, hTeam = m.hTeam, hFavor = m.hFavor, Favor = m.Favor, Spread = m.Spread, isBreaker = m.isBreaker, Pick = pick }); } // Add the populated GameDTO to the list of ActiveGames ActiveGames.Add(GameDTO); } return ActiveGames; }
$scope isn't really recommended for best practices in ng1 anymore. try reading up on components which were introduced in angular 1.5, they're a simpler directive. in fact, even controller is kind of frowned upon nowadays. it is much better to wrap everything in directives/components and share data via services. if you ever decide to move onto ng2 or a different js framework, you will have a hard time because hardly anyone uses MVC in the DOM anymore. it's all components and increasing amounts of functional programming. https://toddmotto.com/exploring-the-angular-1-5-component-method/ https://toddmotto.com/no-scope-soup-bind-to-controller-angularjs/ 
Angular 1.0 tutorial despite Angular 4.0 being released. Just stop. This is the same level as promoting .NET 1.1 in 2017.
Ahh yes. The fun issue of serializing the proxy classes. 
If you are using MVC5, your data types are all weird. There is no need to specify the int size, just use the normal names of byte, short, int, etc. And with nullable types, you put a question mark after the type. So a nullable int would be int?, and a nullable datetime would be DateTime?. **Edit:** if you want code hinting extraordinaire, and are still a student with a valid student eMail under your school’s domain, check out ReSharper. Students can get one year free trials that can be extended for as long as you control your student eMail account. It has taught me more by pointing out things that need correcting than anything else. CodeMaid (a NuGet download to VS itself, not your project) is a decent free alternative **Edit2:** you are not leveraging routing properly. Your partial method has three useless QueryString statements -- catch those in your `public ActionResult partial(int QuerySeting1, string QueryString2) { }` method instead. **Ninja Edit:** on phone, heading home soon. Will try to offer more concrete assistance on a proper system then. --- **Edit3:** Okay, now that I am on a proper computer, here is what I would do for the db model: public class Task { public int Id { get; set; } public int UserId{ get; set; } public string Title { get; set; } public string Description { get; set; } public DateTime? LastModified { get; set; } public DateTime? DueDate { get; set; } public bool IsAllDay { get; set; } public bool IsComplete { get; set; } public Task() { // for setting default values when a new Task is created. Non-nullable booleans *always* need default values. IsAllDay = false; IsComplete = false; } } Note the capitalization - this follows convention. Also, you name an actual data model exactly as it is, you add “ViewModel” for those models which are actually meant to be pushed through to the view. Removed dueTime because at first blush this doesn’t make sense - if you want to look only at the time of the day that something is due, pull/isolate it from the DueDate itself. A TimeSpan is useful for recording how long something took, or the amount of time between two events, not for when something is due. Now for the Fluent API: internal class TaskConfiguration : EntityTypeConfiguration&lt;Task&gt; { Internal TaskConfiguration() { //key HasKey(x =&gt; x.Id); //fields Property(x =&gt; x.Id) .HasColumnName("Id") .HasColumnType("int") .HasDatabaseGeneratedOption(DatabaseGeneratedOption.Identity) .HasColumnAnnotation("Id", new IndexAnnotation(new IndexAttribute("IX_Id") { IsUnique = true })) .IsRequired(); Property(x =&gt; x.UserId) .HasColumnName("UserId") .HasColumnType("int") .IsRequired(); Property(x =&gt; x.Title) .HasColumnName("Title") .HasColumnType("nvarchar") .HasMaxLength(128) .IsRequired(); Property(x =&gt; x.Description) .HasColumnName("Description") .HasColumnType("nvarchar") .HasMaxLength(512) .IsRequired(); Property(x =&gt; x.LastModified) .HasColumnName("LastModified") .HasColumnType("DateTime2") .IsOptional(); Property(x =&gt; x.DueDate) .HasColumnName("DueDate") .HasColumnType("Date") .IsOptional(); Property(x =&gt; x.IsAllDay) .HasColumnName("IsAllDay") .HasColumnType("bit") .IsRequired(); Property(x =&gt; x.IsComplete) .HasColumnName("IsComplete") .HasColumnType("bit") .IsRequired(); // table ToTable("Task") } } Notice how the two boolean/bit items are required. Be sure to ensure that they are always given explicit values when entries are made, as per the no-arg constructor I stuck in the data model. Make sure you link the Fluent API Config file in your ApplicationDbContext file: public class ApplicationDbContext : DbContext { // other stuff, db initializer, etc. internal IDbSet&lt;Task&gt; Task { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { base.OnModelCreating(modelBuilder); modelBuilder.Conventions.Remove&lt;PluralizingTableNameConvention&gt;(); modelBuilder.Configurations.Add(new TaskConfiguration()); } Now, in order to load a partial, you obviously have to link to the method that triggers it: @Html.ActionLink("Load Partial", "TasksPartialGrid", "CurrentController", new { id = searchString }, new { @class = "anyCssClassYouWant" title = "Link Title for ToolTips" }) Now, for the method itself: private ApplicationDbContext _taskDb; public ActionResult TasksPartialGrid(string id) { // we have to name the string ID because that is what is handled tranparently by routing. If you use anything else, you have to change the id in your link, above, as well. using(_taskDb = new ApplicationDbContext()){ // Always use using() to auto-dispose disposable items int userId = User.Identity.GetUserId(); // Why stuff the user ID into a session when Identity can serve it up? if (string.IsNullOrEmpty(id)) return RedirectToAction("HandleAnyErrors", "ErrorController"); return PartialView("_tasksPartialGrid", new TaskViewModel(_taskDb.Tasks.Where(x =&gt; x.Description.Contains(id) || x.Title.Conains(id))); } } If you noticed, I referenced a specific model -- `TaskViewModel`. This is the “bucket” with which we display content in the view. It *can be* functionally identical to the actual data model, but is designed *specifically* to cycle data through the view, in order to properly encapsulate *only* the fields you want to deal with. This becomes especially important when you have forms -- you don’t want to expose data you aren’t actually editing, so you pass a ViewModel that contains only those fields that you are editing. You can do some really nifty things if you separate out your models like this, like providing just a single string to the view that contains an entire name, even if the first last and middle names are separate in the DB. Now why would we want to do this? Simple: the model is where you put all the business rules. Like how to display the name. Or format an address. So while a name might be broken up into four sections in the DB itself - honorific, first, middle last - you use the ViewModel model to assemble the name into the proper format before squirting it over to the view. So here is the ViewModel itself: public class TaskViewModel { public string Title { get; set; } public string Description { get; set; } public DateTime? LastModified { get; set; } public DateTime? DueDate { get; set; } public bool IsAllDay { get; set; } public bool IsComplete { get; set; } public TaskViewModel() { } //You always need a no-arg constructor, even if it is completely empty. public TaskViewModel(Task data) { // Now we provide a way to fill the ViewModel when it is called. Title = data.Title; Description = data.Description; LastModified = data.LastModified; DueDate = data.DueDate; IsAllDay = false; IsComplete = false; } } Notice how the Id and the UserId are gone? We only need them if we need to explicitly link to individual items, like a link to an edit page. We dump that Id much like the searchString above, so that the method at the far end knows what Task to edit.
I'm not even sure where to start... Most of these are *code smells*. 1. Why are you using EF Entity as a ViewModel and/or vice versa? 2. Why do you have the `Serializable` attribute on your `TaskViewModel`? 3. `DbSet&lt;T&gt;` implements `IQueryable` so your `Tasks` property on your DbContext. Why have a get-only property that masks the already publicly accessible `DbSet&lt;TaskViewModel&gt;` property? 4. Why are you accessing the Request.QueryString property so often? Use ViewModels! I skimmed [this article][1] and seems like it'd help quite a bit. 5. I've never seen/used a `PartialViewResult`; just use an `ActionResult`. 6. Instead of `.Where()` then `.ToList().First()` in your controller, just do `.First()`. Other than those nitpicks, I don't see anything that sticks out. I'm not familiar with your Grid.Mvc library either though. [1]:https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions/mvc-music-store/mvc-music-store-part-3
The functionality is brilliant, but the documents and tooling are severely limited for this functionality. I feel the conversion from project.json doesn't handle it well, either - but maybe my library was just badly configured in the first instance.
Just had a reply from a Microsoft engineer (after leaving feedback through the app): "The third party NuGet extension needs rebuilding and republishing for Visual Studio for Mac. This is on my todo list."
Just had a reply from a Microsoft engineer (after leaving feedback through the app): "The third party NuGet extension needs rebuilding and republishing for Visual Studio for Mac. This is on my todo list."
Just had a reply from a Microsoft engineer (after leaving feedback through the app): "The third party NuGet extension needs rebuilding and republishing for Visual Studio for Mac. This is on my todo list."
When Electron (think NodeJS) needs to perform functionality it cannot do with JavaScript alone, a C/C++ addon is used. See [https://nodejs.org/api/addons.html](https://nodejs.org/api/addons.html). Thus, there is no limitation from using C/C++ (or even C#) code from within VS Code. In fact, chances are your current install of VS Code is already using several such addons.
I don't think debugging C# is available now but you can debug Node programs. In theory Microsoft could create debugging tools that have an API allowing Node/Electron to access them. I'm not sure Microsoft would want to go in that direction but there's nothing technical stopping them. Disclosure: I don't have a deep understanding of how .NET debugging works under the covers so take what I say with a grain of salt. 
The burn is real
Are you saying VS is fast an responsive and VS Code isn't?
I just opened a project in both. VS uses 216MB of memory. When I'm debugging large projects it often runs out of memory for me. It took 23 seconds to open (another project just took 35 seconds). Currently using 0 CPU. Compared to VS Code: Currently hogging a whole 11MB of memory, took &lt;5s to open. It's currently using 0 CPU. (Edit: wasn't taking into account associated processes which probably bring it closer if not to the VS memory footprint.) I find VS Code much quicker, but it has a lot less functionality. It's more an editor, where VS is a full IDE. I've never heard anyone call VS responsive though! 
&gt;Compared to VS Code: Currently hogging a whole 11MB of memory Bullshit. [This is VS Code on a completely blank document with 0 extensions installed, it's eating ~210MB of memory and took almost 3 seconds to launch from a cold boot reading from a fairly high end M.2 SSD.](http://i.imgur.com/ZjLL396.png) For comparison, [Notepad++ is using 5.5MB of RAM and launched instantly to the human eye](http://i.imgur.com/RA33feO.png) on the exact same setup, let that sink in for a moment... VS Code is using roughly **37 TIMES MORE MEMORY** than other equally capable applications like Notepad++ or Sublime. If the Electron fad isn't the most atrocious software engineering ever done, I don't know what it is. 
What? The company project I working on has 24 projects (about 20 smaller, 4 is over 250k lines of code each and a lot of classes) - VS using 450mb memory. What the hell you can open to reach 3.4GB memory? O.o
That's the thing, i don't think it does reach that high. But it still has issues and windows kills it with an out of memory. It's 32 bit isn't it? Doesn't that limit it to 2gb at least? Still don't think it uses that much though. It's usually around 400 - 500 mb but still runs out of memory. Could be an issue with my machine/setup.
&gt; Bullshit. [http://imgur.com/a/ntzYm](http://imgur.com/a/ntzYm) The screenshot above is as of this moment as I pause from coding to call you on your incorrect information. I've had this instance of VS Code open for a couple hours, I have one or two extensions installed and it is using a whole 23 MB of memory. Compared to Notepad++ ya I would agree with you, but we're comparing it with Visual Studio. **EDIT:** reply intended for /u/BigotedCaveman
What's Windows? No, you're right. There are multiple child processes and they do indeed add up to a couple hundred meg, but it's still not a fair comparison to put VS Code against Notepad++. 
VS 2015 is 32 bit, yes, and the 32 bit's limit is around 3.4GB. Either you hardly have enough ram (which is most likely not the case - you need to have SUPER big projects to reach more than 3GB memory usage) or one of your ram modules maybe has a hardware problem? And are you sure VS closing because out of memory? Maybe there is some other, system-level error on your PC?
Comes up with "this application needs to close because there's not enough memory" or something, usually running at most at 9Gb of 16Gb ram but maybe the extra sticks weren't installed right or something. Never have the issue with anything but VS though! 
Oh i use it about 9 hours a day lol, it used to crash all the time before i finally persuaded IT that, yes, an extra 8 GB of ram will pay for itself I'm my time saved in about a day. (Seriously though, my old laptop had 3Gb of ram. 3! I had to 'accidentally' drop it off a ladder. Well, okay, it was actually an accident.). I have a few extensions, code maid is the only one i can think of off the top of my head though. It seems to only happen with WPF apps, I suspect it's the WPF designer which has always been unstable and ropy in my opinion. Hogs memory and crashes/freezes all the time. Running a WCF backend seemed to prod it into happening more too. With console or class libraries I've never had much of an issue.
The plan is to bundle Microsoft's own TFS plugin with Rider. It has had issues recently that led us (JetBrains) to temporarily disable it, but now it's better, and it will be back in subsequent builds.
I'd look into Auth0. If it's a good fit for you, it's a wonderful solution. It allows your users to plug in whatever identity provider they want without you having to do a lot of work on your end. I evaluated Okta, Azure AD, and Auth0 for a software project (a bit of a retrofit, really). If it weren't for some really odd features I needed, we would have ended up with Auth0.
TLDR: Turn off lazy loading and use AsNoTracking()
Could do without the 3edgy5me writing and cursing all over the place just to say "be careful how you write your db queries"
Seems like there is no alternative. There is a vote over at jetbrains to add the feature to rider but its only got about 10 votes. I never knew how much I used that feature until it went away.
Auth0 I think is what I'll land on if I can wrap my head around how it does things. Just more reading, I guess. Azure AD B2C is what I was looking into a year ago but you're right, it doesn't seem to be growing fast enough.
This is why we moved to dapper for Queries and still use EF for writes/updates. 
Any ideas on timelines for when the next build with TFS will be working?
Okay man, chill down. Deep breath - exhale. And now explain your problem, I am pretty sure we can find a solution for you.
Would still love some feedback. Allready have some ideas for next upcoming features like: Lambda support. Table support eg: Creating of tables. ( New tables in AWS with indexes and all from code). What more would you like for getting started with some microservices 
I can't advise on dotnet, sorry, but would be very interested to know why your agency is moving away from Django/django CMS to asp.net. Is there something significant that Django lacks, or a need it has proved unable to meet?
&gt; if you know SQL, use dapper. Or any other (micro)ORM which supports SQL queries, which are... almost all! (except the MS variants) 
Nope sorry my brains foggy atm. I do lazy load the entity first then apply changes from web. I need some sleep
except it isnt that easy at all, microsoft completely fucked up the project.json and i didnt find this option in the new=old csproj
&gt; But my understanding (and what my tests show) is that when the task becomes async, it gets popped onto a thread other than the main UI thread. You should take time and actually learn async &amp; await. The actual operation is performed on a thread-pool. Where the continuation is happening (essentially anything after the `await`) depends on how you configured it. By default the synchronization context is restored - that means it would continue on the UI thread, as you'd want. But when you explicitly configure it to false calling `ConfigureAwait(false)`, then it will continue on any thread from the thread pool.
Your problem is the `ConfigureAwait(false)` bit. Remove it. That call causes everything after the `await` to run on the async task thread. That's usually a good thing, except when you want to do UI stuff after the `await`. Also, make sure your method is returning `async Task`, not `async void`. The latter should only be used on methods that will be called by constructors. [Here's a great read on `async/await` best practices.](https://msdn.microsoft.com/en-us/magazine/jj991977.aspx)
When you don't explicitly require a synchronization context, it's better to configure it as false. That way the work can continue at **any** thread, instead of a **specific** one. Worst case the work has to wait until that specific thread is free. Note that the synchronization context is stored and restored at **every await**. Means you can still freely use `ConfigureAwait(false)` in nested method calls, just not at the level where you access the UI.
I don't use var in production code. I use the ternary conditional all the time. I always use braces with a single expression if (required in C#). I use null-coalescing whenever appropriate. I guess using var is the only thing I avoid in production c# code.
"C# Player's Guide" is a good combination of .NET theory and practical coding exercises. Beginners to coding will enjoy it but it also won't turn off experienced devs who happen to want to look into C#/.NET.
With me it's completely different. I use var whenever i can, except in places where the intent isn't clearly visible, but that's rare. 
I'd agree that I can't really think of a reason to use `var` except for when necessary, but I'm definitely guilty of using it instead of writing it things like `IEnumerable&lt;Some.Long.Namespace.To.Avoid.Ambihuity.Compiler.Errors&gt; myObj` 
I let it infer from the right side: `var someStuff = new com.companyName.impl.other.foo.bar.baz.quix.SomeStuff(); ` etc...
My post isn't as simple as "VS vs VS Code;" it's more along the lines of: what, if any, technical limitations does VS Code have, that would prevent it from being as featureful as VS? I chose a misrepresentative title
The problem I have with both of those cases is I am a native English speaker and reader. English is read Left to right, top to bottom. When reading code and I stumble across a var it breaks my flow as I either need to use intellisense in visual studio to tell me what var is supposed to be or hope what's on the right side of the equals sign is intuitive enough. If that person just wrote out the type rather than var I wouldn't have this problem.
Honestly? I forgot that's even an option. So rarely do I use namespace aliases that I never remember it.
This is more of a style and preference thing but I can't stand seeing an if block that has an else but the condition is a !=. If you are handling both then it is just weird to read as it is inverted logic.
It's especially useful when you are working with two separate "domains" that have a lot of like-named types. 
I think that sugar is mostly desirable, though you need to know which constructs are likely to cause errors (and how) so you can use it effectively. Semicolon insertion has a high potential for causing errors. It makes JS into this weird hybrid semicolons-but-also-significant-whitespace language. I have never encountered a compelling argument for leaving out semicolons. Even discussing the topic is strangely enervating. Ugh. Ternary expressions are fine when properly parenthesized; I use them all the time. If you don't parenthesize them then unless you've memorized the precedence and associativity of all the operators you're using you will encounter a (hopefully compile-time) error. Not encapsulating single lines in braces in IF and FOR is just asking for trouble. [This practice has been the cause of some serious, widespread bugs](https://www.dwheeler.com/essays/apple-goto-fail.html). Please wrap your statements in braces. Using var has very little potential to cause errors. Some people apparently hate it, but I love it and use it almost* without a second thought. It makes working with code that uses a lot of intermediate variables nicer. \*"almost" because sometimes you want to be clear that this variable should be thought of as an instance of a particular interface. But this is a pretty rare case. EDIT: and to answer your actual question, I never use dynamic. Like anyone sane I minimize the use of null. I don't remember ever using an unsafe block. Maybe bizarrely, for someone who has worked with .NET for about ten years, I've **never written a finalizer**.
It would be a lot better if there was a reasonable variable name in there. Consider this: var matchingCustomer = CustomerRepository.GetById(1);
One rule for using var that I learned and now go by, is that you should only use "var" instead of the actual name of the variable when it is simple to determine the actual type. Ex: var test1 = "this is clearly a string"; //Good var test2 = CheckEngineLevels(engine); //Bad, what is being returned?
I used to be anti-var but have since done a complete 180. If you can't understand the intent or usage immediately it probably means that your naming is shit. I rarely need to know the exact type and I rarely see it helping. Most of the time it's crap like this: int x = getValue(encodedString); The type is the very least of my problems. It could have been much better as: var userIdentifier = decodeBase36(encodedUserIdentifier);
I'm working on a medical system that interfaces through Thunderbolt. I don't think the drivers would work through a VM but I'll give that a shot
I think a lot (if not all) of these examples are very situational. I can't think of one type of 'syntatctic sugar' that is universally a better idea or objectively always a bad idea. If Linq to SQL or Linq to Entities count, I would say they are my favorite to use however sometimes I purposely don't because I know that although it may be less lines of code and more efficient, that (sometimes) small boost to efficiency is not worth the confusion or time it'll take others (or me a couple days later) trying to figure out what's going on. Sidebar: I also hate how JS has a "feature" where it can auto-fill missing semi-colons. I've heard HTML or RAZOR or something is starting to do that with closing tags, which sounds horrible...
I develop on a MacBook Pro 2013 retina (with GeForce GT 750M) running Windows 7 in Boot Camp and I'm trying to get it to run on an Intel NUC with Intel Iris 650 (of which I don't have a physical system on hand, but that's what they're testing the application with at the office)
If you are fixing up js there are great analyzers available. When I was using Sublime text for javascript develoment you could download a JSLint extension that will list a warning for all 'against standards' stuff. In VS Code if you have a ts-lint.json file to define your standards and download the extension TSLINT you can get green squiggles around bad practice. It's great. I've found that dotnet devs tend to be terrible at javascript and most javascript devs are bad too. linting saves lives. As far as C# goes I stick pretty close to what ever ReSharper or SonarQube standards are. We use both at my work and we flunk code reviews that don't follow closely to those standards. Its important to have a consistent flow to code for a large company like ours because people shift projects/teams all the time so it's very useful when everything looks/acts the same. I too enjoy no brace if statements but our standards say "no fuck off" so I follow the standards just for consistency sake.
If you have Resharper installed, uninstall it, if you don't have Resharper installed, install it. /s
Example? If you mean something like this: if (x == y) Foo(); else if (x != z) Bar(); I'm not sure what you take issue with? Or do you mean if (x != y) Foo(); else Bar(); Even still, I don't think it's any better or worse than using `==` and "leaving" the `else` block to handle inequalities.
The way I look at it is it's never preferable to use var. In your example, why not just type "string", it's 2 extra keystrokes? I've heard people make the argument that typing long class names is a pain, but you'll have to type out the name anyway if calling the constructor. If the assignment is from a method call, you should be typing it out for clarity. With intellisense I really see no use for it.
 &gt;I've found that dotnet devs tend to be terrible at javascript and most javascript devs are bad too. linting saves lives. Heh. So I actually work for a medical company. When they were just getting started in the 80's / 90's they outsourced the development for their Windows desktop application to an Indian firm. Eventually paid them to rewrite it as a web application and purchased the source code so they could hire some devs in house to maintain / improve it over the years. It was recently re-written *again* using more recent technologies / frameworks. So yeaaa... Definitely lots of *quality* code here. I'll have to look at one of those JS analyzers. I just spent 2 months using a static analyser for the .NET stuff, didn't even occur to me that there might be a similar tool for JS.
Interfaces aren't syntactic sugar though.
I had to post this because it's epic :-)
The best one I've heard is that it can save you a lot of time when doing refactoring (e.g. renaming a class). And if you do use the IDE refactoring tool, then you're to impact a lot less of files if you used var everywhere. This makes your code less depending on things that doesn't change the application logic
Why is it epic? Microsoft has deprecated classes before. 
There are good reasons to do both. IEnumerable&lt;CustomerDetails&gt; customerDetails = customerRepository.GetCustomerDetails(customerIds); var customerDetails = customerRepository.GetCustomerDetails(customerIds); In this case, it's all about readability. Explicitly declaring the type creates a lot of redundant words that results in visual clutter that doesn't add information that cannot already be gathered by the rest of the statement. And if you're unsure of the type coming back, hovering your mouse over the method name will make it clear. IQueryable&lt;CustomerDetails&gt; customerDetails = customerRepository.GetCustomerDetails(customerIds).Where(customer =&gt; customer.age &gt; 18); If the CustomerRepository is an ORM, you should be returning IQueryable and by making it explicit, you remind the reader that the filtering is being done by the ORM and not on the client. If used "var" instead, someone could wrongly assume the filtering is done on the client. Or worse, by explicitly specifying "IEnumerable" you've just created a performance issue. 
What is the directionality of your native language? I seems to me that RTL languages should have an advantage wth var because the key information is to the right of the assignment operator. I imagine, however that this argument of flow disruption is ultimately the primary argument against var.
If you need to use an interface, then you don't have a unit test. What you have is a poorly written integration test begging to be put out of its misery.
Oh, BTW, was there another blog you were going to recommend? You pasted a quote from me twice - was the second time going to be a blog link?
This all the way! Methods should express their intent. Types are not often necessary to do that, good naming and structuring are more important. I don't mind using var or not using it, I basically choose on demand whatever I believe makes most sense. A colleague of mine will complain about every single var he encounters when reading code. Even after hours of debate I cannot relate. We literally have the most powerful tooling at our hands. VisualStudio can give you a tooltip or jump to the method signature immediately, if you really need to know the exact type.
I guess it depends on the nature of the change and the context. If the return type changes and I've got code a few lines down that references members that no longer exist, I'd prefer to get the compiler error at the assignment since it will be quicker and easier to identify what the problem is. I know the code would have to be changed anyway, I just think it is more straightforward flow this way. I primarily hold this opinion in consideration of ease of maintenance over the life of the software, primarily because people that maintain software tend to be more junior and typically don't have a full understanding of the big picture.
Frosting. Extra, empty calories smeared all over the top of my code. * private (it's implied) * Explicit types when var is acceptable * braces when it is clear from the indentation * try-finally instead of using * interfaces that are implemented by a single class * casting everything to IEnumerable to make it "decoupled" As a rule, if I don't need code to accomplish something then I remove it.
You are going to have to explain that one, as it is exactly the opposite case. You need to test a single unit of work, not an entire vertical with your integration/functional test.
 var implicitBool = o1.isFoo &amp;&amp; o2.isBar; var implicitInt = enumerable.Count(); var implicitEnemerable = listOfStrings.Where(s =&gt; s.Starts with("F")); var implicitFoo = fooSource.GetAll(); var otherExamples = brain.RecallScenarios(Scenarios.ProgrammingVarCases);
Whoops. https://blog.stephencleary.com/
That is exactly what I'm talking about. I used to work with a guy that would do this If (x == 1) { // purposely empty } else { doSomething(); } His argument was that you might need that other case one day and he hated using !. I'm fine with not if you don't need the else. I think the intent is actually clearer.
Why is your code so badly factored that you can't test your business logic without dragging in the data access layer? Instead of mocking your DAL, pull that code into models or rules engines that don't have a dependency on the DAL at all. Not only will it be easier to test, it will be easier to use. This is dependency inversion 101. 
This article is so wrong on so many levels. It has nothing to do with composition as an alternative to inheritance. Using empty interfaces as marker interfaces? C'mon man. Using extension methods that check for types/markers? A horrible way to go. The author got the use case totally wrong.
The second example. Sorry. I should have used code but I'm on my phone. It is just a personal thing because the logic is inverted and therefore you have to flip it in your head. Not a big deal but just my style and easier to read.
You should really try to not make assumptions about code, especially when you are so far off.
When is the intent not clearly visible? * Badly named functions * Grouping operators in LINQ * Mathematical operations over mixed types (decimals, singles, doubles, and integers in the same expression)
U can use var to shorten things var dict = new Dictionary&lt;string, int&gt;(); instead of Dictionary&lt;string, int&gt; dict = new Dictionary&lt;string, int&gt;(); in this case var is better
Says the person who wrote, &gt; I can only read that as "tests, I have no unit tests" or "I make testing as difficult as I can" 
It seems [System.Data.Odbc comes back in netstandard2.0](https://github.com/dotnet/corefx/pull/15646) and if this happen we'll able to use Oracle ODBC-driver without waiting for official Oracle ADO.NET connector for .NET Core.
Laziness is surely a component. But there is also the consideration that they library may be public and not accessable to make the change yourself. We're of the same opinion on the readability, at least. I use var copiously.
Not everyone can be as perfect as you, AB :) I've had a few junior engineers ask me to explain this behavior in the past. I wrote this post from the assumption that the reader has never read the documentation on String.Length either.
Here's another video on the topic (with a summary of said video) https://www.infoq.com/articles/Async-API-Design
&gt; new HttpClient(handler) That could be a problem later on. https://www.infoq.com/news/2016/09/HttpClient 
how is it being "shorter" better?
&gt; try-finally instead of using How will a using-statement allow you to catch an exception? 
Thats interesting, becaue I use var 98% of the time due to the same reason, readability and verbosity. My take on it is that if code read more than typed, I want to make it as easy as possible for the reader to skim my code as much as possible and quicker get to the part thats wrong / weird / suspicious. For that reason I try to reduce the amount of text and make the obvious parts as similar and small as possible and making the non-obvious parts stand out with (usually) long and descriptive variable names. Another angle of my view is that the the types any class uses can be inferred from the context of the class. There is no need to specify the type (sometimes twice) when its going to be what you expect almost always. (When its not the variable name is the part that stands out). 
For me the main reason is it makes the variable name, which is what I really care about, much more prominent.
Amen, whatever the tools enforce is what I stick to by default. I don't even care about tabs vs. spaces as long as it's consistent.
An interface is simply the public interface to a concrete type, i.e. the contract. An interface allows concrete implementations to be swapped without affecting the client code and is one of the steps to SOLID code. If you're not using interfaces, you likely uave highly coupled code.
It won't, they're talking about people that use the finally to dispose of objects without having a catch block instead of using *using*.
I don't use var unless absolutely necessary in C#. 
Wooo! :)
That's a try-catch or try-catch-finally block.
Yea, but what? The old WebRequest class was even worse for other reasons.
Try-finally doesn't catch exceptions, either. For that you need try-catch-finally. ;)
I agree 100%. I would also add: 1) Using `var`, the code is nicely aligned and easier to read. Example: var myVar1 = GetSomething1(); var myVar2 = GetSomething2(); // versus: ShortTypeName myVar1 = GetSomething1(); VeryLoooongTypeName myVar2 = GetSomething2(); 2) Refactoring is easier. Your code is agnostic to the type, except when it's unavoidable (a property type, or the return type of a function). You want to change the return type of a function from `Dictionary`to `IDictionary`? Suit yourself, all callers using `var` will have no problem with that. But if you used `Dictionary`instead of `var`, you created a dependency... now go re-write your code everywhere and put `IDictionary`.
&gt; interfaces that are implemented by a single class You could still potentially want these if those classes are being passed into other code and that other code needs to be tested. In my experience, most mocking frameworks handle mocking interfaces MUCH better than mocking concrete types.
&gt; casting everything to IEnumerable to make it "decoupled" Ewwwwwwwwwww. This is one place where I find R# a little over-aggressive. I don't usually mind if it wants me to use `IEnumerable` as a parameter to an interface method, but leave my private methods alone, willya?
In my experience the need to mock classes implies a design mistake that should be corrected. Either the test is wrong and should be using the real dependency or the code is wrong and it should be decoupled from the dependency. Really decoupled, not just "I'm going to pretend A doesn't need B by calling it IB".
Yes it wasn't a criticism of you personally. Feel free to post whatever you wish, I was making a wider observation about the .NET community, probably badly :-)
Could you provide an example? Because I find that very hard to believe, especially as something to use as a general rule rather than an exception. For example, any code that stores/retrieves data from an external source before acting on it (database, file system, network, etc) should be storing/retrieving against an interface so you can test that classes interactions with the data separate from the interactions with the source.
Sometimes != is the thing that makes more sense to read. I have a perfect example that I literally wrote yesterday: byte tile = data[dataOffset++]; if ((tile &amp; 0x80) != 0) { tile &amp;= 0x7F; int count = data[dataOffset++]; if (count == 0) { count = 256; } for (int j = 0; j &lt; count; j++) { _map[y, x++] = tile; if (x &gt;= RowLength) { x = 0; y++; } } } else { _map[y, x++] = tile; if (x &gt;= RowLength) { x = 0; y++; } } The thing I'm checking for is the presence of the MSB, which indicates a run of values (this is RLE decoding). You could argue that the base case is a single value, but it makes more sense to me to read it as "if this is a run, do this, else do that".
I don't really have much stuffs that I need or can swap. * I am not gonna change my DB. * I am not gonna change my filesystem. * I am not gonna break my monolith to micro services. * I already use Redis for caching. * I am already using ORM. The list goes on and on.
Exactly. If I need to add extra public properties or methods, I can just do it without breaking existing user of the class. With interface, I have to do Interface2, Interface3, Interface4. Modifying the member of an existing interfaces make the whole exercise moot in the first place. I rather just focus my effort in maintaining versioning on my API payload for mobile/ajax or what not.
Am I the only one who chuckles a bit when they see "[OLE DB](https://youtu.be/h2zgB93KANE)"? 10+ years on and I'm still entertained by that.
 bool is_run = (tile &amp; 0x80) != 0 if (is_run) { ...
Why is having a clean desk better than one covered in junk? Because it makes it easier to find the stuff that is actually important.
try this assuming worker is the deserialized json as a JObject: SqlCommand iWICommand = new SqlCommand(string.format("insert into ADPDumpWorkInfo (FirstName,LastName,DOB) values ({0},{1],{2})", worker.FirstName, worker.lastName, worker.DOB) ,DBConn); 
Meh, that's just wasteful. I might accept `if (IsRun(tile))`.
Fortunately, I don't. And that is the only place I should *ever* see it.
Thank you for taking the time to RTFM for those of us who will inevitably neglect to do so.
My kid has that exact same device. 5/10 - tentatively recommend. "Mmm! Mushroom! That's the food you like!"
Here are a few very basic .NET Core sample apps that may help you get started: https://github.com/dotnet/core/tree/master/samples
Yep, it works with .NET Core 1.1. If you run into problems, let me know. https://github.com/jstedfast/MailKit
Thank you very much guys, really appreciate the help. Currently going through Microsoft's documentation regarding Core and trying to get an idea whats it about.
Yeah, I only really like `IsRun()` for the potential to be re-used. The condition in the example is simple enough that I think it's better the way it is, and it ends up not being reused anywhere else anyway. I'm not always opposed to temporary bools, but I don't usually like them being the entire condition. If the condition is complex, with lots of ands and ors, I like simplifying it by naming the subexpressions. Like `if (isRun &amp;&amp; !endOfRow &amp;&amp; !countOverflow)` or something. There just wasn't any of that happening here.
Yeah, I subscribe to both of those but haven't taken the leap to actually check out the sidebar links. Thanks for the reminder. 
We... Already use IOC? Everything's injected in the constructor. That's why everything implements an interface, so it can be mocked.
You've read too many blog posts
I find the readability depends a lot on the context. In cases where one thing is much more likely to happen than the other, I prefer to see the main flow at the top. For example: if (result != Result.Error) doStuff(); else handleError();
Note that this also affects combining characters such as umlauts. EG you can write ü as either \u00FC (LATIN SMALL LETTER U WITH DIAERESIS) or \u0075\u0308 (LATIN SMALL LETTER U and COMBINING DIAERESIS).
Lol, a week?
/r/ProgrammerHumor
In any case, the short while I've spent browsing the source, documentation, etc. it seems well thought out and well engineered. I may not know what good code actually looks like though so there's that. 
Honestly, I think .NET Core is confusing and I don't feel it stabilizing. If I were you I'd call Visual Studio 2017 and .NET 4.7 "new" and have an easier time getting the same grade. It doesn't seem like your faculty can tell the difference. (Otherwise maybe you'd have an actual class)
But you just moved Customer from the type declaration to the variable name and lost functionality and readability?
Interfaces
Shoving everything into a constructor isn't inversion of control, that's just shoving everything into the constructor. Nothing has been inverted, your dependency chain is still exactly the same at runtime.
IIterable ICollection
Yes, but the bool is verified by the compiler whereas the comment could become inaccurate when the code changes. Expressive code beats comments if they accomplish the same task.
And you want to use those for local variables why? Do you enjoy to small performance tax that comes from not using the class's public interface? Is function inlining a scary concept to you? 
/r/ForHire
I've seen one bug in production caused by a badly formated if. But the guy was a junior, and an mediocre one, so it could have happen anyway.
Can you show us an example of how you do IOC? I'm curious. I don't want to be writing 90's code in 2017. 
Protobuf.net? I don't have any experience with it, but you might want to take a look.
I actually think SmtpClient is pretty good. I reviewed the reference source implementation a few years back and concluded it was actually pretty well designed/implemented for the most part. I would have liked to have seen support for rfc2231 encoding of parameter values, but that was probably the biggest complaint and there's an argument to be made about that being due to simplicity and compatibility. 
Thanks. Do they require specific metadata to work? Seems like protobuf doesn't work without it, and I need to serialize exceptions from third party components.
Looks promising. I'll try it out next time I'm looking for a micro-ORM.
Both of them can work without it, you just need to explicitly tell the serialiser to do it. It's not recommend for compatibility reasons across versions, but it will work.
I assume most people found it, but I figured I'd post for anyone who wants it in the future: https://www.hanselman.com/blog/ZEITNowDeploymentsOfOpenSourceASPNETCoreWebAppsWithDocker.aspx
The variable name itself isn't verified either, so the code could change without the variable being updated. Just as likely as the comment not being updated.
There's a multiple chapter tutorial on building an e-commerce front-end and simplified back-end using ASP.NET Core MVC and SQL Server. It would take you less than a few hours to read those chapters alone (SportsStore - chapters 8 to 12). [Pro ASP.NET Core MVC](http://www.apress.com/us/book/9781484203989) http://www.apress.com/us/book/9781484203989 The book comes with [source code](https://github.com/Apress/pro-asp.net-core-mvc). Who knows, you might learn something... in case you ever want one of those job thingees? Please excuse my misjudgment if your request is because your parents have been kidnapped and being held ransom until you turn in an ASP.NET e-commerce website to their captor. 
Competition is a healthy thing, and TinyORM does very well against competition.
Why not wrap the exceptions into proper data structures and serialize those?
Definitely this. (Binary) serialization of arbitrary object graphs is for masochists and not much else, especially once issues like versioning come into play. As for the specific topic of exceptions, OP needs to be aware that a surprising percentage of custom exception types people build are not properly serializable, and OP would only find that out all too late. Don't serialize graphs of objects. Serialize trees of data. (And yes, I did mean to switch from "graphs" in the former sentence to "trees" in the latter.)
Hm, sounds interesting. Will consider trying this next time I need to choose a micro-ORM.
I guess I could do that, but it's a fairly large change to the compiler. It would no longer just be related to the serializer, but there would be a code generation aspect as well. The variable would no longer be of the exception but of a wrapper type instead since the exception itself would generally be unusable by compiler. All member access would have to be delegated. The obvious advantage is that changing the exception would not break pending transactions which would be a huge win.
As someone that uses Dapper at a very basic level, what advantage would this give me by switching?
Generally the type must be preserved across hibernation borders, but I can special-case exceptions. I could create a serializable contract for everything else though and throw a compilation error if some type doesn't adhere to it. But it would create some frustration for the user.
https://github.com/sdrapkin/SecurityDriven.TinyORM/wiki/Why-TinyORM%3F If I had to pick just one reason to use TinyORM over Dapper: TinyORM has automatic transparent connection management. Forget about "connections" as a concept, all you need is a connection string.
 // static projection of a list of rows: var ids = await db.QueryAsync("select [Answer] = object_id from sys.objects;"); var pocoArray = ids.ToObjectArray(() =&gt; new POCO()); var pocoArray_AlternativeSyntax = ids.ToObjectArray&lt;POCO&gt;(); The first syntax newing up a POCO seems odd, almost like it exposes an implementation detail. The second syntax means you're still bringing in the DLR. Why not: var ids = await db.QueryAsync&lt;POCO&gt;("select [Answer] = object_id from sys.objects;"); 
Yeah, it's a minor thing but I'd never suggest a library with this kind of "edgy" attitude in their documentation. I work with professional software developers and they (and I) are going to instantly assume an immaturity on the part of the developers of the project that means even if it were the best micro-ORM on the planet, we may never get to the point of discovering that. There are too many open source micro-ORMs out there so if you want to be taken seriously, you'll need to demonstrate you take the work seriously. Of course, if it's a project you're doing just for fun, then ignore everything I just said. 
Both the ".ToObjectArray( &lt;entity_factory&gt; )" and ".ToObjectArray&lt;Entity&gt;()" work the same way -- no "bringing of DLR" is involved. The reason for not doing ".QueryAsync&lt;POCO&gt;()" is that TinyORM does not fuse data-fetching and projections, which virtually all other micro-ORMs do. The competition fuses data-fetching and projections for performance reasons. TinyORM has 2 distinct layers - data-fetching, and, optionally, projections (which consumers have full control over), and yet still manages to be the fastest micro-ORM.
I assume db.QueryAsync(...) is: IEnumerable&lt;dynamic&gt; QueryAsync(...) DLR is awesome, tons of uses, but if I'm not using dynamic types I'd prefer to not see a dynamic type returned. And handling a dynamic object does [bring in the DLR](http://stackoverflow.com/questions/7478387/how-does-having-a-dynamic-variable-affect-performance). It's not an end-of-the-world performance hit compared to DB queries, but it would be nice to avoid altogether. From the library's perspective I can see the argument for separating those layers. But from a caller's perspective do you think providing a generic QueryAsync&lt;&gt; call to combine the dynamic QueryAsync and ToObjectArray() calls is a bad thing?
First red flag in here: SQL Server only. I'm sure they can refactor out of that later, but right now that is a problem. I very much doubt they are on par with Dapper performance, except on isolated queries (meaning, a query that gets run once and thrown away). But it probably isn't enough to jump up and down about.
Yep. I'd love to have that referenced in my external dependencies in the documentation and put it in front of QA/my manager...
The problem is the language used
Personally I almost exclusively use LINQ in the form of method calls. I don't like the SQL-like syntax and never have.
&gt; But it would create some frustration for the user. Why? Your entire plan as described thus far already requires that they properly support serialization. Better to make it formal and explicit rather than informal and implicit, and better to afford them the opportunity to serialize their types as they best see fit, in the full contextual awareness of your application and its nature, rather than you bringing some arbitrary general serializer to the party and treading too close to their private state :) As for exceptions picking up where they left off: to be honest, you were already setting yourself up for pain with this plan. You're going to have problems with types that don't serialize properly. You're going to have versioning headaches. But this kind of execution state management, such that you can stop in the midst of an exception, is likely to bury you. Find reasonable points in execution where you can safely stop in a cooperative fashion. Then formalize state management around what needs to be persisted and restored in (the much simpler world of) those safe spots. And then run, screaming, away from object serialization. :)
That's probably fine for a lot (probably even most) .NET developers but it creates a dependency in your code if you want to move to something later. I find this a problem myself.
Types are a feature of the language... Using var is like going backwards 
Um.. are you confusing var with dynamic? Do you really think var disables the type checker?
Oh, you've already reached the later stages of defeat, where you lunge at tragically related topics... Congrats
Perhaps you should reach out to the professor on twitter. I found him at https://twitter.com/dstovell. 
&gt;Types are a feature of the language... Using var is like going backwards What? Using the keyword var declares a type, it just does it implicitly. It compiles to the same thing as declaring the type explicitly. There are 0 features of the language being lost by using var. Nothing was "defeated" here other than your uneducated comment.
Use Perspex instead
I have used just about every DB out there, and I am really fine with all of them in different circumstances, except for Oracle, which can go fuck itself with a rake.
Thank you, I will. 
I don't think just a database server choices requires a whole stack with it. Besides, I just can't stand PHP personally. For anything. Anything, but PHP. Please. Now that ASP.NET Core runs well on all platforms, you can make any stack combinations with it. Windows+IIS+React+MongoDB+C# or Ubuntu+Kestrel+Apache+Angular+CouchDB+C# or whatever the hell you want. But yeah, unless you are too damn lazy and avoid stacks which don't have their own website with ELI5 instructions, then stick to LAMP and get used to ===.
This one has the native looks though.
http://www.simplcommerce.com/ Done, how much do I get?* *Its a joke, I'm not the dev of mentioned project.
Just repeating the 2nd sentence from the docs on one of the most basic principles in C#. Maybe you should RTFM before you comment next time :) https://msdn.microsoft.com/en-us/library/bb383973.aspx
Oh? Show me: https://github.com/FransBouma/RawDataAccessBencher Dapper isn't fast compared to many other (micro)ORMs btw, but reading your wiki you return dynamic, which suggests you return Expando's? My work on Massive has learned me that these aren't particularly... fast ;)
How does it 'well' against the competition? The top microORMs on .NET as well as the full ORMs have a tremendous amount of features and for performance are close together (except EF and NH): https://github.com/FransBouma/RawDataAccessBencher/blob/master/Results/2016-11-22.txt You seem to provide words but not really any proof.
adverts? There are no adverts there :o
And you seem to confirm to everyone that you don't bother to read the submission. https://gist.github.com/anonymous/5e11edaeaec86753c475cbc13c30d6dd (linked to under Features-#4).
Checked it out on a few browsers and updated. 
This is actually surprisingly awesome! I set up a sample repository for myself that includes examples on how to consume a .NET Core dependency from various UI frameworks. I just started, so there is not much there, yet. Added an example for this library. https://github.com/rschili/netcore-UI-samples
Ok, so where are the others? The benchmark code comes with full nuget references, you can run all other orms without problems. Leaving these out does look a bit off. Also, there have been others before you who thought they were very fast but simply didn't play by the rules (as in, kept connections open, kept things cached for later fetches). A link to the TinyORM bencher code would be preferable.
I think I asked a question about expandos... You can retort that with some lame ass reply, but that doesn't really help you. 
I'd take a look at [Angular](https://angular.io) for this.
This is sad that support of .net core is not here yet. 
The bencher code is trivial, but here you go: https://gist.github.com/anonymous/52915eb34be950357ee10346e3b1ba6f The only modification to the controller is running each bencher in a TransactionScope: private static void RunRegisteredBenchers() { Console.WriteLine("\nStarting benchmarks."); Console.WriteLine("===================================================================="); foreach (var bencher in RegisteredBenchers) { using (var ts = SecurityDriven.TinyORM.DbContext.CreateTransactionScope()) { OriginalController.DisplayBencherInfo(bencher); try { OriginalController.RunBencher(bencher); } catch (Exception ex) { BencherUtils.DisplayException(ex); } ts.Complete(); } } } Enjoy! Btw. I just realized that Otis_Inf is likely Frans Bouma, LLBLGen author. You have my full respect. I would kindly ask you to add TinyORM to the RawBencher on Github. You should also run sync benches separately and async (awaited) benches separately. When I bench TinyORM against competition, I only bench the async flavors since TinyORM is async-only. LLBLGen has the closest timings with TinyORM: essentially identical on set-fetches, but TinyORM is the fastest on individual fetches. Also, your [ThreadStatic] 10000-items-per-thread-and-then-wipe caching is a pretty lame approach, especially in a multi-threaded ThreadPool-heavy use, but perhaps it helps win artificial benchmarks. I understand that it's not your first rodeo and you've done your homework, but so have I.
You are confusing Expandos with DynamicObjects. TinyORM does not use Expandos at all. Don't "assume" what I return - you can look at the code (but I can't look at yours).
What I have done before is use angular router and make it load all the pages as templates inside the ng-view while killing the template cache manually. It works and is almost as fast as a regular angular app. You have to be careful with your JavaScript inside your pages. Make sure you clean up your event handlers and stuff. Also make sure you remove elements that get added to the DOM outside of the view on view change.
Don't have time for screenshots, but I imagine you could install the api as an Application under the existing IIS site, and set the virtual path. This will effectively expose the separate API site at the path you specify under the main site. Hope this helps, good luck.
Idk how I didn't find this during my search. Seems interesting, but I don't like how it tries to force the Windows Metro style in other OS's.
Seems like an interesting approach. If you don't mind giving me an example please.
It doesn't really, its the theme-able like WPF. You can talk to the devs on their gitter.im channel of you want to talk to them about it.
Elmahs UI is totally garbage, it also doesn't support .NET Core. I don't know if this supports core offhand. The second part is that it's also a service offering that you can have a third party capture errors. This is commonly a very smart choice since if your database goes down and now you can't log errors either. I always use hard isolation of my error store, I want pretty much 100% availability for where I write errors.
Download the latest community edition (2017) Google latest language features since you've been gone C# 6 and 7 for instance You haven't said specifics about what you want to target. If web or console applications and want cross platform support look into .net core. Most of the research after the initial project setup will go into what front end framework(s) you want to use and learning them. Overall desktop apps and .net haven't really changed much in this time period that I can think of.
In an SPA environment you will most likely not have any views outside layout and loading views. The SPA will have its own files / format for pages. Most common setup : MVC layout/index view serves enough to load SPA SPA communicates with MVC via calling API 
I love that article! That's typically what I send them. In fact my blog contains a link to it :)
* Limited Access to Open Source Libraries I don't think so. At least in 2017, there's a lot of .NET open source libraries that work fine with Xamarin, and then there's dedicated Xamarin open source libraries on GitHub and Nuget. * Xamarin Ecosystem Problems I find Xamarin developers to be much more mature and experienced. Not every kid out of college learns Xamarin. Quality of apps isn't that bad, at least it's better than hiring 20 college students as interns. * Slightly Delayed Support for the Latest Platform Updates Xamarin does fine with support, but Xamarin Forms is kinda sluggish in updating to latest libraries. * Expensive Xamarin License As the author mentioned, this one is no longer valid. 
My biggest issues with Xamarin were:- 1. The distinct lack of *quality* documentation 2. How brittle and sensitive Xamarin is to environment configuration
Why should I use this over something like [**Prefix**](https://stackify.com/prefix/)? Which can do everything Exceptionless can and more - my favourite being showing executed queries.
Try looking up some podcasts related to the particular discipline you want to get up to speed on. They aren't necessarily tutorials, but they help keep you up to date on the latest trends and emerging technology. I find them very handy to listen to while I am running, working out, or just getting other things done around the house.
When someone says 'dynamic' I expect expandos, as that's what normally is used when 'dynamic' is used, at least AFAIK. I do maintain Massive (which uses expandos) and as such have read about them, but haven't looked at DynamicObjects as I have never ran into the need of those. They seem to look more rigid than expando's but then also likely are much faster to instantiate (expando's are terribly slow when instantiating them, which is the core reason Massive is slower than any other microORM)
It really is a judgement call. Lots of "production" libraries use dependents that are not production and you never know if I don't look. For things related to security I tend to not. Example: it's been v0.8.1 and has been the same for 6 months and I don't mind the open issues I see listed I go ahead and use it.
Watch "The State of .NET with Markus Egge" on [youtube](https://www.youtube.com/watch?v=u0_NLptFDkM), then decide what to do next. Topics include: • What’s new and useful in the world of .NET? What’s .NET’s role (and Microsoft’s role) in a modern world of development? The new approach, and more! • Cloud Development with Microsoft Azure (including a look at various individual Azure offerings) • Visual Studio &amp; Visual Studio Code • The dramatically new world of .NET Core • .NET Development on the Mac and Linux • Web development with Angular 2.0 (from a .NET perspective) • HTML5, JavaScript, TypeScript, Frameworks, and more! • Xamarin • Desktop development options (WPF, UWP, HTML on the desktop with Electron, …) • Visual Studio for the Mac 
I would only use core as the limitations of deploying standard .net only to windows servers are making companies cease to use it. My fav relevant mini project advice would be create a free AWS account, then create an api gateway backed by core lambda functions and maybe s3 json objects as a data store. For front end create a simple ui in react and host the site as a static website on s3. All the front ends i know prefer react over angular now.
&gt; Prefix Prefix is a profiler/apm/tracing where as exceptionless focuses on exception reporting. It all comes down to the best tool for the job. If you are looking for profiler or tracing data stick to something like prefix or zipkin. If you want to capture exceptions and all contextual metadata before, during and after an exception occurs (including session data) use exeptionless :). Also it's 100% open source and you're not locking yourself into a corner.
I just set it to null You can use razor views by just loading the URL to a controller action for the template URL
Is there any tutorial I can follow to achieve it? Sounds pretty good.
The biggest argument against any all-in-one mobile dev solution is support for third party libraries. You don't use them that much, it's pretty nice. A lot of our apps depend on them and we end up working on compatibilities issues more than features.
Thanks friend. I'm into website, console app and web services development. Now a days even dotnet has dotnet core which runs on open source. So it become a pretty complex for me to understand to go with open source or to stick with my core knowledge. 
That's a valid answer, but ODAC also provides integrated tools listed [here](http://www.oracle.com/technetwork/developer-tools/visual-studio/overview/index.html). Depending on OP's intentions, he might need them.
Please take a look at Reddits rules regarding self-promotion: https://www.reddit.com/wiki/selfpromotion
I think Mono 5.0 wasn't listed by mistake into the nightly releases. (it is now) 
I put an AspNet core web app to production in September and have a .Net core windows service app going to production next month. Development has been a pleasure. EF core hasn't given me any problems on than between tooling upgrades. I'm also not worried about VS 2015 rc3 -&gt; 2017 upgrade path with project.json -&gt; csproj, it looks like dotnet.exe has a pretty simple conversion command. So as someone with hands on experience for over a year with it, and a team of 6-8 developers using it, I disagree, respectfully, that dotnet core isnt ready. It would make a very good technology to re-launch a career on.
Yes, I was talking about the projection-caching, not result-caching. I didn't realize you had to support .NET 3.5 and couldn't easily use ConcurrentDictionary. Thanks for the explanation of your design decisions. On a loaded server you can easily get 500+ ThreadPool threads, and the same simple "SELECT 1+2" query can be forced to incur projection-cache-buildup in each one of those 500 threads if you assume worst-case scenario (ThreadPool gives you a unique thread every time). Ie. the potential concern is not about killed-and-recreated threads, but about running on ever-new thread because there could be plenty of them. 
X-Post referenced from [/r/programming](http://np.reddit.com/r/programming) by /u/thetyrone [How We Build Documentation for .NET-based SDKs](http://np.reddit.com/r/programming/comments/65ck0f/how_we_build_documentation_for_netbased_sdks/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
Interesting, sounds like I'm lucky I picked xunit for our tests.
Ok, I have marked as hidden this post. 
I've done this when I have multiple things to dispose and I don't want to have 4 nested using statements.
Here is a pastebin for the razor layout I use when loading the views as templates https://pastebin.com/ayBPqULS 
Here is an example of a main layout https://pastebin.com/uHXk0PSP
And here is the _viewStart.cshtml @{ if (Request.QueryString["angulartemplate"] != null) { Layout = "~/Views/Shared/_AjaxLayout.cshtml"; } else { Layout = "~/Views/Shared/_Layout.cshtml"; } }
Yeah, we've fought against some of that as well. In some cases, we put a hard limit on how many flags we will support. When we hit the limit, we will only add a new flag if we can delete another. It also helps to quantify the amount of additional work the flag causes long term then calculate a dollar amount or opportunity cost for the flag.
Yeah, but with that setup, you're losing all of the power of using razor views and all the helpers and things that go along with that. You can get the responsiveness of a SPA by just making it so it loads all of the common scripts and stylesheets in a master layout, but then loads each page's contents via ajax. You can even switch this behavior on and off for easier debugging by just using some conditional statements. 
I gave SignalR a try (some beta version) about 2 months ago, but eventually needed something much more stable and reliable, so I switched to socket.io. I see no point in using SignalR at least till the end of the year, as I'm pretty sure it will change again and again. Take a loot at this video. https://vimeo.com/204078084
&gt;Note: As mentioned, this is a servicing release for SignalR 2.2, not a new release for .NET Core. We’re hard at work on the SignalR port to .NET Core, with an expected release later this year. :) 
Despite wholeheartedly preferring the C# language to Java, I still choose native Java for Android development over Xamarin because of the vast number of libraries already written for native. For example, I don't know if Xamarin-Android has C# libraries that mirror Java libraries like Butterknife, RxAndroid, Retrofit etc. etc....... and I heavily rely on 3rd party libraries rather than rolling those functionalities myself. I personally won't switch to cross-platform if I already know how to develop in that native platform. On the other hand since I don't know native iOS development then, sure, I'd be happy to learn Xarmarin-iOS if the day comes that I need to create an iOS app.
Good point, was talking about .NET Core version :).
This won't be a popular view here but if I had taken a break for 4 years I would not go back to .NET, I think your friends are right in saying there are other things to look at. For me .NET is what I know - I progressed from writing VB6 to .NET like a lot of others - but if I could have my time again I wouldn't choose Microsoft tech. There's nothing wrong with Microsoft (or .NET) but you're really tied in - people will say it's changing (and to be fair it is) but it will be a long time before the community catches up with other OSS offerings. Starting from scratch .NET is a tough sell for me - there are better options for RAD (I've tried a few frameworks and ASP.NET doesn't compare well for productivity) and although .NET is great for solid stable business apps you can also get this with the JVM which has a much better variety of languages. And you don't have the licensing issues you get with Windows (I know .NET runs on Linux but my point above is it will take a long time for the community to have the answers / tutorials etc you can get elsewhere, for the foreseeable future I think it will be Windows dominated). Unless you have a compelling reason to go with .NET take the chance while you can and try something else. As I say, I don't expect many to agree but that's my 2 cents :-)
where are you exactly stuck We not going to spoonfeed you but of course we are willing to help if you are stuck or having errors you cannot solve. 
These are all the things I need to store from the application. It's a lot of data and I just don't know if it all belongs in one table or not. Name (First, Middle, Last), SSN, Email Address, Phone Number, Address, Street, City, State, Zipcode, DOB (Datepicker), Gender,High School (Name, City), Grad Date, GPA, SAT Score (Math, Verbal), Area of Interest (Drop Down List of Majors), Full Time or Part Time (Radio Button List), Prospective Enrollment Date , Fall/Spring, Year 
I couldn't find an answer for this anywhere, so I just started poking around. I just compiled a .NET Framework 4.7 console app that references a .NET Standard 1.6 class library.
Oke, You could do it in one table, I do not know if you can for example have a input field for High School or High school will be a fixed. that is the question with more things you need to fill in. So it difficult to answer your question about that. I would make Gender a radio box so it would be stored as a boolean. 
Yes it's something to improve in v5.3, when we move to .netstandard2.0 (and to .NET 4.5.2+ finally). It hadn't occurred to me that much, but thanks for the reminder: workitem added to get this changed :)
It doesn't, same exact error. I had tried that already.
Not fully, but it's damn close.
By reference tables, I mean tables that store reference values that don't change often, if ever. Examples may include countries, states, university names, etc. Some people also call them lookup tables. Using reference tables and foreign keys in your main table(s) will allow you to easily populate dropdown menus that can both be bound directly to your viewmodels and updated/maintained in one place. Example: MAIN_TABLE FIRST_NAME VARCHAR LAST_NAME VARCHAR STUDENT_TYPE INT FOREIGN KEY STUDENT_TYPE_REF STUDENT_TYPE INT STUDENT_TYPE_TEXT VARCHAR Now, if you want to change "Full-time" to "Full Time," you only need to do it in one place (one student type text record), and the change will be reflected not only in your main table, but also in any dropdowns you've created based on the student type reference table.
How is the .net support in socket.io?
I didn't mean about support for .NET - since I've a distributed application using service bus such as RabbitMQ I needed something to make use of websockets - SignalR wasn't stable enough, thus I went with socket.io :).
Why would you want a service bus to use websockets?? Queues, tcp, named pipes, those are transports made for service buses.
I think the problem is that if you have installed e.g. .Net Core SDK 1.0.3 and 2.0.0-preview1, then `dotnet` will prefer the 1.x version. I have [created a PR](https://github.com/dotnet/corefx/pull/18426) to update the instructions. Does that help?
We tried that once, then product whined to the CEO that we were being obstructionists for no reason. The CEO then bitched at the CTO who had our backs but ultimately had to back down. Resulting in nothing but bad blood between product and development.
Thanks for posting!
That makes sense that you'd have to do some extra steps to enable the early access builds, but maybe I don't understand. I should be getting a 2.* version number output right? Unless there is some other step for setting up this global.json I think I'm missing something. Assuming this *should* work then I think maybe my installation didn't work great. I've tried reinstalling it, but that didn't change the result. PM&gt; cat global.json { "sdk": { "version": "2.0.0-*" } } PM&gt; dotnet --version 1.0.3 PM&gt; dotnet --info .NET Command Line Tools (1.0.3) Product Information: Version: 1.0.3 Commit SHA-1 hash: 37224c9917 Runtime Environment: OS Name: Windows OS Version: 10.0.14986 OS Platform: Windows RID: win10-x64 Base Path: C:\Program Files\dotnet\sdk\1.0.3 Wanted to say, I appreciate the help! Thanks a bunch for the help so far.
Got it. You use the service bus to glue your workers into your client system. I've built systems just like that with SignalR + Azure service bus backplane (HA website), and Azure service bus. I pretty much never had any problems with SignalR. 
&gt; I have worked with signalr but the application I made could not save the messages. Why not? I think if you were to spend more time understanding this part you could make some good progress. SignalR is a natural fit for this type of feature - chatting is pub/sub by nature. 
I honestly wouldnt know where to go. I was using the jquery client and a signalr hub how would you go about saving the messages to the database and linking up things like User.Identity.Name to the message output? if you have built something similar please share some insight instead of telling me to spend more time on something im trying to understand.
 var msg = String.Format("{0} :{1}", Context.ConnectionId, message); Why not save the `msg` before or after it's sent?
With my implementation, JWTs are generated via the `/token` service, but that route is easily configurable. My token service accepts a username and password via the POST form body and if it succeeds, it returns a JSON object containing the token expiration date and the `access_token` field - the `access_token` field contains the encoded JWT token, which a client application can store and send via the `Authorization` header, in this form: Authorization: Bearer &lt;access_token&gt; An example of a real one might be: Authorization: Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJURVNUIiwianRpIjoiMmI4YzI2M2UtZDFkNy00MDg0LThmMTktYmI4YjdjMTJmMjdmIiwiaWF0IjoxNDkyMjc0MzAzLCJuYmYiOjE0OTIyNzQzMDMsImV4cCI6MTQ5MjI3NDYwMywiaXNzIjoiRXhhbXBsZUlzc3VlciIsImF1ZCI6IkV4YW1wbGVBdWRpZW5jZSJ9.vqc6e1z4E4TsOvAUY8BHhIejkjsh3kjweLwg2ad0ziA You could in theory have different authorization methods for generating the JWT access token such a whitelisted range of IPs, uploading an access data file, or in my case just does a check on the username and password. It's up to you, and as you probably understand now, it's powerful for that reason alone. The most convenient thing about them for me is they make writing an API easier, because users can just authenticate using an (in my opinion) very simple way to manage a logged in session on the client side without cookies and just using simple HTTP requests.
I never thought of that. so your saying create a ChatModel Class that takes an int Id and a string Msg property create a dbset&lt;ChatModel&gt; of that model in applicationDbContext make a controller that lists the ChatModel.msg value thatt will put it in a view? Is there a way to save while the person is typing?
The first part is correct. What would that controller be for? The user is already seeing the messages, correct? So all you're needing to do is save the message while it's at the hub. For saving while typing, what do you mean by this? Save their partial messages (unsent messages)? Or let your server be non-blocking on the message? If the former, you'll have to send the partial messages to the server, just an AJAX request.
This submission has been randomly featured in /r/serendipity, a bot-driven subreddit discovery engine. More here: https://www.reddit.com/r/Serendipity/comments/65nbpj/health_checking_aspnet_core_services_with_consul/
Ohh maybe I over thought it. Would I save the msg(string from ChatHub variable) in an instance object of ChatModel.msg(the Model i just made) and save that into the ApplicationDBContext? 
Okay thanks. Im going to have to some more research to customize this and fit my requirements but thank you for your help. 
Look into Twilio's programmable messaging. Cost money sure, but unless this is for throw away project, don't reinvent the wheel.
Second one, your intent is shown
I cannot help you with "the right way". But it sounds odd. I have never heard of the 200 mb size limit. Are you sure you are right? I believe that we are generating files larger than that at work
Check if there is a parent web.config overriding this setting as it is a sub-webapp. The webConfig inheritance thing is a pain in iis.
From what I understand, the second one (if it contains *new CookieAuthenticationOptions*) would override the first (ConfigureServices)? 
Who knows the second one might have private methods that's setting other values
In your second approach you overwrite the default registered `CookieAuthenticationOptions`. If ever inject that value to one of your classes, you will get different values than the ones you set in your `Configure` method.
Yea, I think I figured out the gist of it. The second will overwrite anything set in the ConfigureServices method. Also, if using Identity, I don't think you need app.UseCookieAuthentication() as app.UseIdentity() implements the cookie auth by default. I think that's correct, but please correct me if I'm wrong.
You should only be declaring those additional interfaces if you've got a new type. Single responsibility says a class should have a single responsibility and do it well. If you're adding properties, etc. then the most likely case is that you're breaking SRP. Also, adding properties should *never* break a code client unless you remove existing properties, in which case it doesn't matter whether you're using interfaces or not. *Extending* an interface rather than modifying the existing one will ensure all existing code works and only the clients of that code that requires the new properties will need to be updated (in which case - they required a chance whether you were using interfaces or not). public interface ICamera { byte[] CaptureImage(int exposureTime); } public interface IXRayMachine : ICamera { bool XRayOn { get; set; } byte[] CaptureXRay(int exposureTime); } An XRay machine is a type of camera. You don't adjust the camera interface so you suddenly have XRay capabilities, otherwise you now don't have a regular camera and anywhere you require a regular camera you would either need to duplicate code (hint: don't), or you would be forced to use an XRay camera every time you want a normal camera (unncessary). Now if you wanted, you sure can implement a Camera base class, in which case your new XRay machine simply extends Camera and implements IXRayCamera, you now have two different implementations, and client code should not have been changed unless the code requires the newer implementation. It's just clean design. Also, if you're doing API design, then using interfaces is even more important if you want API users to customise (not sure if this is the right word) their input. If I build an API that did flat field correction of XRay images (rough example), I wouldn't want my users being stuck to using my implementation. Instead, I would want them to be able to use their own implementations that satisfy their needs. void CreateFlatFieldCorrectionFrames(IXRayCamera camera); The user's implementation of IXRayCamera does not matter to me as an API designer. I don't care how they capture an image, how they turn on the XRay tube or even what camera they use to capture the frame. As long as the capture function returns a byte array (my contract) then I'm happy to use their implementation. As long as I stick to that contract, and I don't assume the clients needs, then the API is easy to use. An example of where this user need could be important? Most XRay machine implementations could go straight to exposing the moment the XRay tube is turned on. A certain clients hardware may require that the XRay is on for a set warmup time before the camera starts exposing. Yes, I could just use a base class, but that's still tying the user to that base implementation (in which case they can do overrides, but then you're pulling in redundant code). Instead, the user should have the choice of fully implementing the interface themselves, or they can partially implement that interface while simultaneously extending the base class (which implements the base interface) to gain the common functionality. I don't know whether you've ever worked in a larger development team before but sticking to SOLID principles correctly keeps a project moving a lot faster and keeps the developers on the same page. EDIT: Also just as a final addition. Interfaces make for more fluid code. It is easier to change modules. You say you're not going to change certain choices, and that may be so and well done you for having the full design done out and being 100% certain there is no need for change, but... requirements constantly change (more so when you're working with customers) and having a code base where you can delay implementation choices til way later in the project, and you can change implementation choices later in the project, will leader to far faster development and less time doing major refactoring.
log4net is quite powerful. Why don't you try a service like exceptionless.com. They even have a log4net appender on NuGet.
I use filebeat with the ELK stack.
How do you do it for web app as part of azure app service?
We use application insights for everything on azure and it works really well. Logs all requests/exceptions and custom traces/events plus dependencies making it possible to trace stuff through the entire stack. It's also possible to enable continous export which would write all logs to azure blob storage and write some kind of parser to pipe it to elasticsearch etc.
Pretty sure Azure has built in logging that you can just hook in to. I think they call it Application Insights.
I'd recommend looking at integrating with a third party service like https://www.loggly.com/. You can add an appender to log4net that calls their service. With the log4net appender you still have the ability to change log level at runtime with centralized, searchable logs in loggly. It looks like loggly doesn't do alerting at their free tier but any paid tier does. To your other question about dynamic log level changes: You can change log4net's level at runtime. A secured api call/web method as you mentioned would work just fine.
Oh, ugh, haven't had coffee - I honestly didn't see that bit in your post. I honestly don't know how webapps in azure function. You said they were VMs though - do you have the ability to put other services on those VMs? I do know that you can set up elastic in azure. If you at the bare minimum have the ability to ship the logs to a VM where you can install a service, you could ship the logs to that VM and then leverage filebeat to send them to elastic. Filebeat is a service that tails log files in real time and sends the data to elastic. It watches a directory for new files and attaches to them, so if you shipped your files on a periodic basis, it would work great.
There are Azure VMs that already have ELK installed: https://azuremarketplace.microsoft.com/en-us/marketplace/apps/elastic.elasticsearch?tab=Overview https://azuremarketplace.microsoft.com/en-us/marketplace/apps/bitnami.elk?tab=Overview 
I want to try application insights, but worried if it would affect the performance of the application. As we are already using new relic for monitoring the management wants to check if it can be dine cia new relic as well. Again, it should not affect performance of the app.i.e., it should not be making any service calls for each and every log operation. As I need yo implement for multiple applications I need to so it right and not bring down the apps because of logging.
I will try the elk stuff. Think of web app as shared service hosting where you have almost no control of file system.
Isn't that super expensive? Last time I looked it was something like $15/app/month? 
How is the performance? Does it make AI calls for every log operation? I thought if you store the data in table storage it would be costly to delete. Not sure about file blob service though. Can I set a data retwntion limit of say 30 days or something?
By the way, the value of ELK is that its APIs give you * Visibility into your logging (as opposed to "ah fuck something broke better go get the log file", it's just on a web site) * Collation IDs. If you have multiple services handling something related, the logging should be related, right? That's called collating. * It scales like crazy * It's very searchable It's not the most simple logging implementation, but it forces your application to have a more mature approach to what logging actually means.
Thank you. Do you think splunk would also be an overkill? We dont do many transactional level items like shopping cart etc How do you do the centralized logging for web app (azure app service)?
If you don't need a lot of custom event tracking but mostly just request/response and error tracking, AI is probably the easiest thing for central logging/monitoring. There's a walkthrough and some videos here: https://docs.microsoft.com/en-us/azure/application-insights/app-insights-azure-web-apps
Alerting on logfiles is possible but ugly. It's easier if [your logger can output JSON](https://www.nuget.org/packages/log4net.Ext.Json/) or some similar format. It's much easier to alert on metrics instead, using something like InfluxDB. At my current company, I set up an interceptor for API calls that records how long each API method call takes and what response code it returns. Sometime before launch, I'll add in a series of Sensu alerts and hook them up to PagerDuty.
Look into Azure EventFlow. You can use an EventSource as input, and publish to ELK from your app. https://github.com/Azure/diagnostics-eventflow
Exceptionless is what we use at work. Its very easyto work with and it works well. No complaints.
ITT as per usual: log4net, cos theres literally nothing else ever for .NET /s Seriously, Log4Net is abandonware, its under apache projects so that's enough said. The only commits it gets are months apart and for tiny things. Use NLog ffs.
StackDriver logging from google is another service and they have a free tier. Plugs in with the familiar log4net.
I love Splunk, it's well worth the money. We pay like $30K a year but wouldn't trade it for ELK, ever.
Serilog to Papertrail
Serilog structured logging to Seq for centralized logging. Works great for .NET logging. 
What I always miss in this heavily simplified examples is when the data structure needs changes in the version too. Do you really copy it and make a v2 namespace? Or how do you deal with these kinda changes? Also: return actionDescriptor?.Properties .Where((kvp) =&gt; (kvp.Key as Type).Equals(typeof(ApiVersionModel))) .Select(kvp =&gt; kvp.Value as ApiVersionModel).FirstOrDefault(); Please just don't write bad code like that, not even in example code. Don't use the `as`-operator without a `null`-check. If you **know** it's always of that type, use an **explicit** cast. Since you access the `.Equals` method right away, you clearly assume this can't be `null`. 
Money talks. Estimate the cost of maintaining the old code, number of flag permutations to test, etc. 
Fixed. Thanks 
Versioning is painful. I have done so much research into the best ways to handle it in a microservices approach... they all suck. The point you are making hits every level. 1. Common data store? Gotta deal with the changes here at least. 2. Changes to functionality? Gotta add tech debt to the code base to handle the old version and the new version, or copy and make new versions of each piece in the stack. 3. Try and inheritance structure with overrides for that (e.g. V2 inherits V1 and overrides specific stuff)? Deprecation becomes a chore. The BEST way I have seen so far is to run separate instances of your service for each version, i.e., run v1 codebase and v2 codebase side by side, don't do ANY versioning in the application code at all. Then you have to figure out how to get callers to go to the right one based on contract or URL, so two approaches: 1. Add a message queue to the system for all communication. Version contracts and have each version of your service subscribe to it's version of the contracts. 2. Add a service gateway which everything calls to talk to everything else either as (a) a real gateway (proxy the calls) or (b) provide URI mappings to callers to cache at startup for it's dependencies. This kind of moves the ROUTING to the service gateway. Your service would still have it's own routing for its endpoints and such, but something looking to talk to it would call the gateway saying it wants something like service/v1/someEndpoint, it either proxies the request to the real endpoint, or tells the caller what the real endpoint should be. The goal here is to avoid versioning concerns in the app code as much as possible, and push the concern of routing to the right version to a self-contained layer that will deal with the mappings. It's not perfect, you STILL have to modify v1 when you release v2 to make sure that common infrastructure (DB for instance) is handled correctly in v1 so v2 callers are both satisfied for their data. But honestly? There is NO EASY WAY to do versioning of APIs. None. It is painful, and shitty, and all the answers have drawbacks and issues. It is a total COST of choosing a microservice based architecture, or exposing public APIs. You pay that cost to gain the other benefits, and try and minimize the impact.
With regards to dealing with data structure changes, that's a bit more complex. Unfortunately it often does mean copying the v1 data structure tweaking it slightly and moving it into a v2 namespace. In the best case you only need to do that for the publicly exposed data structures and the internal ones can be reused, worst case you have to duplicate everything (but I'd avoid duplicating internal data structures as much as possible). In practice I've found this normally isn't too much of a burden on the code base.
Mind you everyone has a different learning curve. However, am now a little over half a year into .NET development and am feeling very confident. I think a part of it is realising there's a lot of pieces involved in .NET seeing as it's such a massive framework. Some things are, essentially, being solved for you automagically and not every bit of information is need-to-know as much as it is good-to-know. Keep working on projects and you will encounter new information which should progressively give you more good-to-know knowledge. That's essentially how I've been working. Good luck!
There is so much to .NET development that it's hard to narrow down "good enough for a job" territory. I learned a lot from picking my own project and started building the framework, datamodels, javacript, etc and googling one issue at a time. I have 6 or 7 years under my belt with .net development but if you tossed me into the deep with UWP, the first weeks are going to be overwhelming. So much more of being employable is being able to deal with the business workflow, understanding/writing documentation, other interfield skills. At the very least though you should have a handle on classes, inheritance, basic and complex data structures (Lists, collections, and the like) and a basic understanding of MVC (if you're looking at web jobs) Other than that, you'll learn what you need on the job.
Keep with it. When I was starting, I actually read the same book you are reading (for MVC 5 though). I struggled through it but finished. I then took the MVC fundamentals course on pluralsight. When that was done, I went back to the Pro MVC book and did it again. It wasn't til this second go around where everything started to click. It just takes time and practice. But just remember, that there are people that have been at this for a long time and still don't know half of what the architecture presents and plus, things are always changing.
Proficient to get a job? Probably 3-4 months (I already knew a little bit of C#). Proficient to where I feel like I can build almost anything that's a small to mid sized project? About 2 1/2 years. However, I still have questions about stuff - mainly identity related stuff, which is a massive clusterf*ck for me at the moment... :) 
You are right for one app. We have a microservices architecture with over 100 apps. The pricing model led us away from AI. 
I'll take a stab at this. Aside from the obvious fact that they are different languages, there are some very big differences, as well as small ones. First let's talk about [typing](https://en.m.wikipedia.org/wiki/Type_system#STATIC). Unless you're using the DLR in C#, then C# is a statically typed language as well as being strongly typed. You have to declare your type up front and the compiler will do some checking for you. JavaScript is both dynamically and weakly typed. There are no compilers that will do type checking for you with JavaScript. There are tools that can help though, linters come in handy here, as well as transpilers like Babel (if you need to take advantage of new features that may not be available in all targeted environments). There have been many studies that show that weak/dynamic languages are faster and easier to use. Additionally, unit testing becomes much easier since you don't need to worry about writing a very significant amount of boilerplate code. In many static or strong languages you will find mocking frameworks to help with the boilerplate, but it will never be as easy as a dynamic or weak language. Now I will say that if you are of the mindset that you don't need unit tests for a strong or static languages due to compiler checking, don't bother reading the rest of this post. If you just don't want to write unit tests, that's okay and please continue reading. As far as paradigms go, C# is object-oriented (OO) with some added Functional Programming (FP) flavor. C# continues to add more tools from FP with each release and does a really nice job with it. Java is sort of trying to play catchup, but C# is far ahead. There are some things that C# can't support in regard to FP, until they make some compiler and framework-level changes, such as currying and immutable lists with cons operations. JavaScript natively supports currying, with immutable-JS you get access to immutable collections with cons operations. It really depends on what your goal is for which paradigm you want to use in JS. I wouldn't say one is better, or more popular. Both are good, both are popular, and you can always use a blend of the two (many people do without realizing it). Now let's talk about dev environments and when to use what, cause those topics have a lot to do with each other. I said before not to bother reading if you that the compiler would protect you from errors, here's why. The compiler will NOT protect you from errors, sure it may give you a heads up on a simple issue, but you're still going to get runtime errors. One of the big advantages of strong or static languages is developer tooling. You can see this in the tools available, VisualStudio/IntelliJ vs Sublime/Atom/VSCode. Visual Studio and IntelliJ are large and robust IDEs that actually write code for you based on minimum input. Add ReSharper to Visual Studio and it writes even more for you. This is great if you're still learning or have legacy systems or teams that don't, or can't, communicate. The IDE itself describes the code to you and what it does. Now if you have a team that has high communication, good habits, unit tests, and documentation, then a heavy IDE is just that, heavy and bloated. This is why the editors used for JS are much lighter, they don't need to worry about more than half the stuff that an IDE does for you and why you can still write C# or any language with those same editors. One thing I will say is, every line of code written or generated is a line of code that you have to support. So why use which languages? Why use any language. There are some things that you can do better or faster with one, but not the other. Node gives us JS on the backend, but it does so on ONE process. Node is not and cannot be multithreaded. This is why Node relies so heavily on asynchronous programming. You cannot block the process without blocking everything. This is usually where you hear about callback hell. Callback hell is just a code formatting issue and nothing more. It hasn't been an issue for some time, there are many libs available to help format your code better and the good ones are just shims for the ES6 promises and get a performance boost when your underlying platform is ES6. Because of all this async stuff and single process nature, Node works great for IO bound operations. I can have a node project serving REST requests up and running faster than you can update all your nuget packages in C#. What I'll never be able to do in Node is crunch numbers or do massive data transformations efficiently and handle IO requests at the same time. That's where other languages shine and one of the first questions to ask before starting anything in Node, am I going to have long running in-memory transformations or calculations? If so, don't use Node. Use C#/Java/Scala/Clojure/FORTRAN anything but Node. You also mentioned phonegap, one thing I will say is that I and users know the difference between a natively developed app and one that was created with phonegap/appcelerator/etc. Tools like that are great for companies that want to get something out on multiple mobile platforms quickly and cheaply, but as they grow, they will likely transition to native apps for each platform. Edit: Source, I'm a software engineer and consultant with many years experience. I have the most in C#, but I also use Scala, Clojure, JavaScript, Java, Typescript, and CoffeeScript. No, not all on the same project or even at the same client. I use many of those languages for hobby projects too. Happy Coding!
What version of MVC/.Net are you using?
You can control ingestion limits with a max/day to make sure costs don't get out of control. You can also use adaptive sampling at the app side to prevent every single piece of telemetry from actually being reported. If you need to customize this you can implement any algorithm you want as an `ITelemetryProcessor`.
That is great news. I took someone's word for the need to be 1:1. Thanks! 
Alright. Well since your site uses SSL, encryption is already handled for you if you post to your controller using a form. As far as handling session goes, that depends on if you have more than one server. If you do, you'll need to set up asp state so that your users session persists if they were to connect to a different server on a request. You can set up a global authorization filter that handles authenticating users and have it redirect to login if it fails. I can give more info/links tomorrow, but that should give you a starting point.
It's just one server and one web app. Do I need to encrypt anything on the DB?Thank you for the help I really appreciate it.
I'll second this. I'm a C# developer by choice and trade, but got pulled in to help start a new Node project. The sheer lack of ceremony and boilerplate made it ridiculously fast to develop--I had a functioning web API in less time than it would take me to strip away all the unnecessary references in an ASP.NET project out-of-the-box. But the tooling, oh is it terrible. Atom is a lovely text editor, but it's no IDE. It doesn't prevent you from shooting yourself in the foot. The C# compiler catches trivial problems. Node will let you hang yourself at runtime (our biggest problems were calling async methods synchronously and changing return types without changing every caller). There's no smart find-and-replace, no helpful warnings that you're doing something stupid, and no chance to find most of the mistakes until you actually run into them. All that said, I'd still do it again. I never want to maintain that app, but even as comfortable as I am in C#, I can build something more quickly in Node, and sometimes, for something simple or past its deadline, that's all that matters.
You will need to encrypt the passwords in the DB. I have not personally implemented that portion but I am sure a simple Google search will yield helpful results in that area.
I've been using VS 2015 and 2017 for Core projects since last July and have had no problems whatsoever. Honestly not a single issue with Visual Studio. I can't say I've read about many other people having issues either.
Try going with ASP.NET MVC 5 or thereabouts. .NET Core isn't production-ready, in my opinion, and given how much it's been changing, I'd bet your book is out of date. I've been using C# and ASP.NET off and on since 2007. I tried converting a small library (plus its test library and a related command line tool) from C# 4.5 to .NET Core. I'd read articles about Core. It took me about five hours to convert it, largely due to poor documentation, tools working in unexpected ways, and error messages that were outright lies. Even then, I couldn't get my nunit tests to run. (There are at least two open source projects for it, and neither of them worked for me.) I ended up figuring things out largely through github issues. And I'm quick to assume that something's broken or missing now, having read through some of those issues.
Even I was surprised the same way I'll share the link which I read shortly. Thanks for your support. 
VS2017 has issues whether or not you're using core. You're lucky you haven't had problems
Yea it's definitely a mess. The documentation is really poor IMO. Instead of showing ID4 in a console app, they should just focus what 80+% of what it will be used for (asp.net core app with asp.net identity). From what I understand is that it should be standalone in it's own project. But even with that, should it have it's own DB? Does it share the main project DB? What if you don't want the user to go to a separate login screen? (I think you can have server to server auth to accomplish this while using your own apps login screen.) They also barely explain what clients, scopes, flows, etc are too. So many unanswered questions with hardly any documentation outside of shitty conference cams where they display their product but their code on screen is unorganized like crazy. Is it really that hard for the documentation to have a basic tutorial of creating a web app/api/ID4 in the repository pattern? I've basically given up on ID4 at the moment (until there's better documentation) and just using the built in .net core identity to handle auth. I doubt anything I build will have a million users, so I'm hoping identity alone will be fine. /rant Edit - I'd almost pay money for someone to throw up a tutorial of a working project of asp.net core web app, mobile ready api, ID4, and asp.net identity in the repository pattern project structure on github! 
I have to say I disagree with this. For starters "Certification of Microsoft". In Denmark they arent really used for anything. They are not going to get you a job over another developer. And then theres "ASP.NET applications for Windows Phone operating systems". Dont think anyone is making Windows Phone apps anymore. Dont really like your References. One written "November 29, 2010" and then "quora.com". But do agree with Client side is getting more important and NoSQL ;)
To have a deep understanding of ASP.NET, several years. To have a functional programmer knowledge, a couple of months. To have expert understanding about 6 years **minimum** give or take. 
The best usage for certifications is if you want to change tech stacks. Suppose you did java for the last 5 years and decide you want to write c#. You'll struggle in job searches against experienced .NET developers. Attaining certification shows you understand how to do stuff even if your experience stemmed from java. 
I have 0 problems on 3 different physical machines (one is a locked down work machine) and I even run it on VMware fusion for mac (win10). The issues I've run into are related to the core tooling and not visual studio itself. 
Serilog -&gt; direct Elasticsearch sink (no ETL). We set up alerts in elasticsearch for when we get too many exceptions, when processing takes too long, etc. Works pretty well.
&gt; ification shows you understand how to do But then again. Wouldn't you pick the one who did C# the last 5 years instead of the one who just took the certification. I did C# for 3-4 years. Then Java for 2 years and then back again to C# now. I have never been asked "Do you have any certification?". It might just be Denmark which in general dont care for certifications. 
I would hold both candidates rather equal on the technical side if one had 5 years of .NET and the other had 5 years of Java but passed some of the .NET certifications. Said a different way, I'd interview both of these candidates if their experience/resume seemed good. If that java developer applied without the certifications I would not interview them.
I haven't had any obvious problems. There's the whole "PDB file is in use"-thing but I got around that with the first search online. I've messed with IS4 without any problems. I never had pre-release version installed so I don't know if that makes any difference.
Also have a ton of issues with VS2017, mostly crashes and hangs that seemingly can't be avoided.
The only problem I had with 2017 was resource file compilation which they seemed to fix. Patches are frequent and super quick to install. I'm loving it. 
Extensions are usually the problem. Even though they're supposedly more sandboxed they still crash it.
what even is this question
Thank you.
Check [ReactJS.NET](https://reactjs.net/). However you can also create decent dynamic controls in ASP MVC 5/Core using 'old-fashioned' JQuery and Ajax calls. Introducing the whole react framework just to create a few dynamic controls is overkill IMO. 
If you want to connect your real-time graph using websockets than SignalR might be idd your best option. But you could also just write some code to poll every x seconds. &gt;The only problem I run with JQuery/Ajax is the fact that it easily gets disorganized. I never really ran into this problem TBH, I just always put my JS code in a different file (separate from the HTML), and tried to embrace the functional nature of JQuery as much as possible.
I have the same problems. I especially have a lot of problems with Resharper + VS 2017 with .net core. So much so is that it's almost unusable, I get "can't be resolved" errors in Resharper all the time even though it builds and runs fine. Or ambiguous reference errors even though it suggests the same namespace in either option. The project claiming it doesnt have a csproj and so on. I get one show stopper bug about every 1 - 2 hours. I'd say it's more of a Resharper problem than VS 2017 problem though.
Horses for courses. If you have a requirement for being about to go "Add Service Reference" in Visual Studio, then WCF is probably your best option (I would personally avoid ASMX services altogether). WCF also gives you the option of hosting on named pipes, TCP and so on. If code generation in VS is not a hard requirement, then I'd personally go with Web API, since it's really easy to host it just about anywhere these days. You could use something like Swashbuckle to generate Swagger documentation for the API, and let developers generate their own code from that. Or, you could create a client library that contains your model classes and uses HttpClient to call your API, and distribute that to developers who will be interfacing with your API. You could reference the same library in your API project so that you're reusing the same model classes.
Thanks for your response and I will use the timeout function. I have exactly the same background as yours, working in .NET for about 8 years now, started with WinForms and went to WPF and now web app so I am glad someone can relate to my situation. And thanks for recommending the book as well, will checkout later tonight and have a read. 
The only problem I have that you listed is intellisense randomly dying, otherwise, I am not having any problems.
Thanks for your advice. I'm mainly desktop developer that does about 20% web work. In today's development environment, are less and less people moving from the server side and to the client side even if they are VS developers? I'm trying to think if considerably less people will be expecting "Add Service Reference" then I'd rather stick with Web API. 
&gt; When do you need to use tracked anyway? When you turn off change tracking, EF will no longer track the changes you make to your Entity Models. This will make it so when you try to save your changes that you made to your entity model to the DB, you will need to manually call a function (I forget the specific one) so that the context knows your entity model has changed. If you do not manually call the function, the entity model's state will be 'Unchanged' and the context will think it does not need to save the model to the DB. There is also other stuff you need to be concerned about that I cannot remember off the top of my head. If all you are doing is reading data from the DB then turning off tracking should be okay. However, I am not sure how much, if any, of a performance gain you will experience. IMHO, I would try to optimize other code as much as possible *before* turning off change tracking. 
Same, i7 with 32GB.
I originally felt it was probably resharper, since resharper wasn't quite ready for 2017. But, I think they just released the R# update with no improvement. I reckon I'll uninstall it and give it another chance.
Instead of complaining about complaints on reddit, maybe read the question. I have a curse when it comes to this stuff, so I'm checking in to see if it's just me or if the things I'm seeing are being seen by others. Apparently, it's not just me. Of course I've reported everything I've seen that I could repro. It's just so intermittent that I never know when to hit record...
loool
It's really strange. For me, it only happens in core projects though. This was supposed to be the first version that was really built around core... :|
&gt; I am currently developing a web application for a small business .NET Core is the wrong choice. Try again using the full .NET framework. If you really want to keep the .NET Core app, you could create a 'reporting' site written in the full framework with url links between the two. It's very common for an internal business app to have a separate app just for reports. e.g. SSRS, or in your case it would be crystal.
There was also a problem with git (i think it's still there). Apparently, if you allow too many unstaged files to pile up, it starts freezing up every few seconds. `git add .` will clear it up. What a bizarre bug to have D:
You should be able to target framework 4.6, https://jonhilton.net/2016/09/07/using-asp-net-core-against-net-4-6/ Though some specific .Net core libraries might not work with that target framework, depends on what your dependencies are.
I think that Microsoft has made such a big push towards Web API over the past few years that most developers would probably be expecting to interface with that rather than ASMX or WCF these days, although the latter two definitely still exist in corporate environments. In a lot of ways it's a pain that Microsoft doesn't have some sort of Web API metadata generator that it can automatically create clients from, but I guess they don't want to be too restrictive about what sort of messages a Web API could return (e.g. returning different response messages depending on the status code). If you go down the client library approach then your developers will get an Add Service Reference style experience (or at least, you could have a shared library that defines the input and output classes used by the service), but as long as your API is well documented that will allow people to write their own clients.
Overall, I think its pretty good. I do have some opinions though, most of which are superficial. 1. I don't think the official coding guidelines require you to make private methods lowercase, of course this is entirely your choice. 2. I would try to put the enums in different files, even if they are small. Functionally its meaningless, but as someone else looking at the code, my first instinct is to look for the filename, even if I can press f12 and navigate. 3. I would probably never use a library that didn't implement async/await. In your function calls you have a wait boolean that you pass in, but really this should be an awaitable Task that the consumer can choose to await or not. The introduction of the boolean is uneccesary and cumbersome in the long run. This is probably my biggest sticking point. Dynamo is meant to be a high throughput document database, but your web server will never be high throughput as long as you are blocking on each and every database call. Pretty good job though. The most interesting thing in this project is the converter and you've chosen to implement your own. I would love to see the performance comparison between doing it this way or simply using a json serializer.
I would recommend [Vue.js](https://vuejs.org/) instead of React if you only need it for smaller pieces of your application. It pairs very well with SignalR.
&gt; Microsoft doesn't have some sort of Web API metadata generator that it can automatically create clients from Swagger for that. Edit: Also they have `System.Web.Http.Description` but I don't know how good it is or what for can it be used.
4 years ago my company &amp; I started a big new project and chose WCF services over anything else. Worst decision ever, even considering the fact that we have to run WebForms. There are just so, so many problems with WCF, some of which are beyond critical. 2 months ago I gave up and started implementing new services with Web API running on OWIN. It's a real breeze – so much better than WCF. I can write more detailed comparison if you want. You can generate clients for Web API using Swagger specifications – I use NSwag for that, I like it better than Swashbuckle. And again, it is better than WCF code gen in my opinion. So far I'm using TypeScript and C# clients, but I'll need python and php clients later. In addition to JSON I also respond with XML and have some very complex models and some binary data too (20 year old communication standard I have to support). It's been very good so far. WCF's serialization is cancer.
Thanks, will check it out! Does it require Node or anything else?
Nope! Totally standalone. 
I would take this a step further and suggest that you check out [ASP.NET Core](https://www.asp.net/core). Very similar to the Web Api/MVC framework. I'm not aware of any client code generators unfortunately but maybe somebody can point some out.
What were the worst problems you had?
Worst writing style ever. Is it translated from another language by machine? Gave up reading owing to language. Probably just spam. 
We're running AspNetCore targetting full framework so we can use the full framework version of WCF.
Excellent that will solve the deliverable part of it. Can I replicate the code so there are two different case bases as well? Thanks! 
I think your right about ASMX over WCF. I've done a lot of work in both, and WCF seems to make the simple complex and the complex mind boggling That said web api is probably the better choice if you have a free hand
Data Contract Serializer has terrible extensibility and customizability, terrible opt-in/opt-out approach, terrible idea of converting XML to JSON, no support for very basic things like XML attributes, and much more stuff I don't remember any more. I cannot call WCF config options "extensibility" since that should really have been default and automatic options for WCF. All the 350 lines do is registering WCF services with HTTP and HTTPS endpoints each. I tried many other ways of running both HTTP and HTTPS but none worked. As for the exception, I have custom IErrorHandler, and the implementation was throwing an exception when trying to process another exception because sometimes WCF would execute Autofac's lifetime scope management hooks out of regular order, and control was passed to IErrorHandler after the request lifetime has been disposed. Even then I think it didn't always crash, but again it was intermittent. I might be missing some details here since the bugfix happened as a side effect of something else and after we've started WCF outroduction.
DataContractSerializer is designed to address 90% of cases while being fast. If you need to customize the serialization more, you use the XmlSerializer and all the attributes that has available. It is a little slower but has a lot more features. I agree about the conversion to JSON. The DataContractJsonSerializer was created to support WebHttpBinding with REST services. WCF is a poor fit for REST, it was shoehorned in. WebAPI/MVC is the better solution and that uses JSON.NET. The Autofac issue sound more like a usage issue than a product issue. There is no contract implicit or otherwise that the IErrorHandler executes while a request is still being processed. I don't think Autofac even existed when WCF was designed so saying the WCF doesn't support the lifetime management required by Autofac is a little unfair to WCF. This sounds like a different extensibility point might have been a better choice, e.g. a custom FaultFormatter.
Why has no one mentioned application insights? It puts all of these to shame.
If you can get a "basic blog" going (driven from a database) relatively bug free and with a good notion of DB transactions and security, then you're good enough for a junior .net developer. If you've got a good grasp of unit tests (arrange, act, assert) (edit: and how to use source control like git) then you're definitely good enough. That's just from a purely technical perspective. To flip from "Developer" to "engineer" (not that there's really a definition other than in my mind), then you need to add a solid grasp of software design patterns, application architecture, and "enterprise" integration patterns. That's assuming your focus in is in typical "business" or consumer apps. If you're building games, Internet of Things, or any of the dozens of other things you can do, then it's a whole other world. .Net is a generalist framework, so there are many sub-disciplines you can find yourself in. You'll be an expert 1 day before you retire. I've been at it 20 years, I'm still finding tons of new things to learn.
You should be able to use the Microsoft.AspNetCore.Authentication.OpenIdConnect NuGet package with IdentityServer 4 (or another OpenID compatible provider hosting a login page) https://identityserver4.readthedocs.io/en/release/quickstarts/3_interactive_login.html I've set up IdentityServer 3 as a general purpose STS and I've enjoyed working with it. [edit] In regards to your JWT question, I'm a fan of JWTs and they are usable in any auth scenario. You get signed tokens that are transparent and easy to pull information out of; very much preferable to the default opaque tokens OWIN uses.
It so happens that serialization performance is almost never an issue. (Except that one time when a client uploaded a 1 GB XML file – that exhausted 32-bit address space.) So I'm not willing to trade extensibility, sanity, and other normal serializer benefits for a negligible performance improvement. Web API also implements switching between different serializers and formatters much better – it's just one class. In WCF you have behaviors... let's not talk about that. Whatever happens in a product, I don't think it's an acceptable solution to crash in unmanaged code with no debugging information whatsoever.
It is also insanely annoying to me that you can do things that don't make sense because the tools don't do any kind of validation. For example, the Microsoft.AspNetCore.WebUtils nuget package (from, obviously, MS themselves) targets netstandard1.3... but it also has a dependency on NETStandard.Library &gt;= 1.6.1. Can someone please tell me wtf is up with that? Of corse there's also no easy way to report that issue, which is a separate problem. 
Why don't you do it in API, and let the developers use "Add Rest API Client" from Visual Studio. https://docs.microsoft.com/en-us/azure/app-service-api/app-service-api-dotnet-get-started Personally I have WCF because of all the configuration mess. There must be like thousands of attributes in there. 
Thank god. It's about time. Shame that System.Ben will likely go away. 😜
For visual studio users consuming it from various web apps and desktop apps I'd recommend wcf. It's pretty simple to setup and get going. Plus getting updates is basically click a button. If you're looking strictly at web and javascript (or external users over the web), webapi would be more suited. I've see horrible asmx, wcf, and webapi services so a lot comes down to how you customize it. Swagger (and Swashbuckle) do help if you use webapi but be careful about what http codes you return.
Entity Framework is amazing and as far as I know completely database independent. I have no idea about dapper though. Typically, you'll have one controller for each type of object with an endpoint for each page/data return. You'll have a model for each thing you are working with and one view for each page. Think of the views as the aspx page and the controller to be the aspx.cs part. You can condense them, but really it comes down to how you want to organize your code. Using entity framework you can scaffold out most models and basic access mvc pages for those models. I'll need more data on the query to figure that out. Typically you'll pass the info to the controller which will do the heavy lifting on the query. Why not, it's a fun tool. You don't have to if you don't want.
What class is this for?
While VS2017 starts faster, during daily development ist a lot slower. Building of core projects takes considerably longer (which is a tooling issue) and Intellisense is having its "Seconds of thoughts". And don't let me get started with mobile x-plat development... Interestingly, I'm not the only one experiencing these issues. 
&gt; We will not retrofit this to all previously submitted packages (so we don’t destroy existing dependencies) I hope not. All my libraries have a dependency on System.Ben!
it is a dll
I like using vs code for net core development, the only thing I haven't quite figured out yet is dealing with nuget packages specifically viewing available and then upgrading them. What do people use to do this?
Trying to look into this now, most notably I was looking for something like the Visual Studio nuget manager that shows you what packages are already installed (And their versions), to allow you to install the same package elsewhere.
I created another project with ASP .NET Core targetting the full .NET framework. But I couldn't find a way to add a crystal report into that project (didn't try copying and pasting an existing old report yet). But for now, I think it is better to move on with a new ASP project targeting the 4.6 framework.
An example of a dynamic query/ grid view is like this. Users selects 2 weeks sales Product Qty Week1 Week2 User selector to view 4 week sales. Product Qty Week1 Week2 Week3 Week4 So in webforms I just write query and pass it to grid view with generate autocolumns =true and that's it. If I use a model how would I accomplish this ? 
What about ITextSharp?
This is a very vague article. There's little to no details and there's no examples, quirks, explanation of your experience or anything. It's just a quick blurb about setting up your environment and a short pro/con list.
How does this approach compare to using linq? Something like list.Where(x =&gt; x.length == 3); Then I can return it to other places not just the UI.
Linq uses yield return under the hood for Where: https://github.com/dotnet/corefx/blob/8de23df14d48798a5826e43dd9996754d0084ea1/src/System.Linq/src/System/Linq/Where.cs#L75
Thanks for this, it's extremely helpful and I didn't see it mentioned anywhere while learning about this feature. I'm processing text files from one system's format into one compatible with another system's input. Files are only about 1,000 rows at most and run times can be several minutes without anyone raising an eyebrow so, luckily, I don't think I'll have to rewrite this code, but will be sure to be mindful of this in the future. 
For this one "for Webforms, I have 2 files the page and code behind. So for each Report page in MVC. Do I need to have a model, controller and view? or is there a way to condensed this?" No - let's say you have a folder "reports" with four webform pages, one per report. All you have to have is a single controller ReportsController with four action methods (on per old report webform page) and four views (one per old report webform page).
Pretty much the only time we use `yield return` any more is when generating new data. For example, maybe you are creating the data from a file or network socket. After that, LINQ all the way.
You can run up to 10 apps on Azure for free in the App Service Free Tier. https://azure.microsoft.com/en-gb/pricing/details/app-service/
azure and AWS have a free tiers https://azure.microsoft.com/en-us/pricing/details/app-service/ https://aws.amazon.com/free/
What did the author use it for? What are things Elixir can do that C# can't? 
A good friend, I try to visit when I can. 
Lol I've fallen for that Azure free before, they actually do charge you even if you don't go over their max requests etc.
Yield return is just short hand for writing a state machine. It also doesn't have anything to do with UI. 
Not really. What do you mean?
I think your talking about NotifyIcon class? https://msdn.microsoft.com/en-us/library/system.windows.forms.notifyicon(v=vs.110).aspx I've used that in the past
I just wish there was some `yield* return` syntax for methods with a `Task&lt;IEnumerable&lt;T&gt;&gt;` type signature.
Which means you should use those methods whenever possible instead of relying on less readable and more bug prone implementations that use yield return. LINQ pipelines are declarative, so you only leave room for errors in logic 
It was months ago, I've since had the account deleted.
Something like this: try { var sweetVariableName = DoAwesomeStuff('BeanBurrito'); ... do stuff here ... yield return sweetVariableName; // This throws an exception } catch (Exception exception) { ... exception handley stuff here ... } Try ... Finally with no catch will work though
Not in my experience. I've used it for testing without issues for websites.
I recommend using an external Node rather than the in built one. For one it will go out of date rather quickly especially if working with front end developers using VS Code etc and may fix some npm issues.
What do you mean by LINQ pipelines?
Uhhhh... wrong subreddit?
That looks like a bug
It looks like you are right. Possibly asking on StackOverflow might get a better answer, if this is important enough.
Got it. That's simplifies things. 
Good
Or an alternative to IIS, e.g. nginx. Kestrel is pretty lacking in terms of functionality.
I would say the following are obvious candidates for using yield return: - If you're writing a library and don't know how much data the consumer may need - If you're streaming the results because the dataset is huge and would consume too much ram. When else can you think of?
It means you just haven't used any of the new features. Things I have noticed from 2012 to 2017: - Diagnostics Tools - Edit XAML files in real time - Live Visual Tree - Edit And Continue inside Lambdas - Git Integration - Code Lens - That light-bulb feature (like resharper) - Live Visual Tree and Property Editor - Exception Settings - Timeline Tool - C# REPL - Better Nuget Integration (Import package based on class name) - Much smaller and faster installation and modularization - Better code navigation (more like resharper) - Better exception helper (for NullReferenceException) - Live Unit Testing This is just the features I can think of right now. There are a lot of new features to web development as well, but I can't name them off my head.
Thanks for the hard work, don't hesitate to go deeper in the OAuth world, there is a lot of cool stuff to do with it.
I was really looking forward to the wildcard file inclusion and saying goodbye to .csproj merge conflicts. However, I don't think the support is quite there in Visual Studio yet for even the simplest projects. I couldn't even customise the output directories or create separate x86 and x64 configurations.
Might only be WPF? You should check the 2017 release notes for clarifications.
In your link they kind of confirm it's a bug, but one that is quite complex to fix, so they decided to just throw instead. It makes sense, considering the black magic that's going on under the hood with `yield`. If I remember correctly, you also can't use `yield` and `async` at the same time.
Terrible documentation for the command line tools. Inconsistency in the behavior of the command line tools. Solutions are a second-class citizen. Because of this, it took me multiple hours to convert one codebase to the point where, if I typed `dotnet build`, it would start to build my code. And that was only three projects... No NUnit support, and a lot of other libraries I need aren't there yet. (And when I tried migrating one of my libraries over to Core + Xunit to help out with this situation, the project I created with `dotnet new xunit` couldn't import the Xunit namespace...) Core libraries are in flux with a bunch of breaking changes, some of which are less motivated than others. Inconsistent package format, so all tooling for .NET projects and solutions needs to be redone. No Linux IDE as a result of that (I've been using MonoDevelop quite happily since 2010 and am not going to switch operating systems in order to use an immature framework when Mono still works).
Cool article. The slower and more bloated ReSharper gets, the more I consider getting rid of it.
I've also disliked resharpers new annual pricing. Thanks!
Is it only the Prog1440 files you want?
I love my ReSharper subscription. Always latest versions.
I'm Ron Burgundy?
Really the main thing I get from Resharper nowadays is import suggestions. I'll just type MyClassName and it will suggest "did you mean MyProject.SomeNamespace.MyClassName?" and stick the relevant import up top. Even better, I can type SomeObscureClass that's not even defined in my solution and it will suggest "Add reference to System.OfficialMicrosoftThing and use System.OfficialMicrosoftThing.SomeObscureClass?".
That's how a lot of software is licensed these days, term-based. It's a good model for a lot of companies. They might sell you a perpetual license if you're willing to pay for it ;)
&gt; sonarqube Thanks! Looking into it now. 
If anyone can tell me another extension that can do the below please let me know so I can get rid of resharper. * Format/Align code (move braces, new lines, spaces before/after paren, ...) CodeMaid/VS default doesn't move code once it is there (only as you type it) * Unit test runner that gives the little bubble next to the test I can live without the other features I use in R# but those two are tough for me to go without. The new go to everything in VS2017 works great so I won't miss the R# that much.
They can make various list functions, such as select, where, ect. a lot more readable. If you make it a private function it's scoped for the entire class. If it's local it's really clear where it belongs and you cannot have some bloke refactor said function to the buttom because that is what he believes is correct.
Although it works fine of course since they're not really instance members, some people think you shouldn't call extension methods on null references. 
I think just using the null coalescing operator would be cleaner and less opaque than those `SafeX` methods.
Haha yes. Those SafeX method are quite ancient, before null coalescing was introduced to the language. 
[CodeRush Roslyn](https://www.devexpress.com/products/coderush/) I've been using CodeRush since 2009. Roslyn is extremely fast, but doesn't have all the feature set of CodeRush Ultimate...yet. They are doing 45 day sprints to try and get it caught up. Currently $49 for a one year subscription/perpetual license. (One year of updates, if you don't renew, you keep what you have).
In our ecosystem, roles are used as a broad grouping mechanism for permissions, while claims are used to provide a granular method to enumerate permissions. Also, we treat roles as fluid: they can be added, deleted, or changed at any moment. Users can be added or removed from roles at any moment through the application. Permissions, on the other hand, are static: they never change unless the application changes. It's not ideal by any stretch, but it does work cleanly and quickly for our needs.
You're aware these are available as of last month, right?
For the second, give [NCrunch](http://www.ncrunch.net) a try. The video in the home pag should give you an idea of its features. 
Take a look through some Scala code. It really helps to organize things and make things clear what is going on. For example. if you were to have a recursive function, you generally have a private helper function that does the actual recursion, while the public function just jump starts things. With local functions, the recursion helper is defined inside the public function since that is the only place it is used. I've generally found that a lot of private functions are really helper functions to public ones and only used by one function. They end up getting spread around the class definition as people add and remove code and to figure out what is going on you are constantly moving around in the file. Another example might be, you have a complex predicate that you are using in a LINQ expression or maybe you use the same predicate a couple times in a function. Rather than writing it out twice, and then possibly introducing a bug when someone forgets to update the second instance, you put the predicate inside a well named local function and use that where you need the predicate. 
Right now I'm using Auth0 and adding the users to a Azure SQL in order to have a cross reference in login with the user both in Auth0 and Azure SQL, in order to have other entities tied to the user outside Auth0. Would you recommend me using Auth0 + Azure AD? (Which Azure AD Plugin are you talking about btw?)
In VS2015 as well? Because I haven't upgraded our toolchain yet, and will not be able to for some time.
Cant rely on free .net hosting, except azure. If you only host simple website without database, you can go with them. If you use SQL db, then the price is higher than shared hosting. You can use their calculator first. My opinion, you can just use simple shared hosting, you can consider asphostportal. I also use their services. Very good and not so expensive
Honestly, I wouldn't call this inexperienced developers. Perhaps in this particular case, but in general not. Extension methods appear to be instance methods (due to the syntax sugar), and then they really should mostly also behave like instance methods. Calling extension methods on `null` is just nasty. If I saw a method being called on an object, I definitely expect the object to not be `null`, otherwise that call should have caused a `NullReferenceException` already. I do not want to check every time whether that was an instance or an extension method.
Hmm interesting. Thank you very much for the write up.
This always seems true for the first few months of a VS release. Eventually before VS2018 comes out it will get bloated down and feel as sluggish as ever.
I've been watching Prog1440 and P1612. Both have very useful information. Unfortunately, he moves so fast and you don't really see the creation of a lot of the code, so it's difficult for me to follow. If you can get both of those, I'd be very grateful, but if not anything would be great. Thanks again. 
Another cause can be if your error pages have an error this occurs 
&gt; but I can't think of anytime I would want/need a local function over a normal private method. The key benefit, I believe, is that you can create a local function that is only ever called by the enclosing method. So from a design perspective this might be preferred in order to *prevent other methods in the same class from calling the function*. For some use cases it could also look cleaner (subjectively I'm sure): specifically iterator and Task related methods.
It doesn't really matter, it's up to you if you're going to be the only one looking at the code. However, having said that, in C#, the standard is PascalCase for public identifiers (class names, public properties, public method names, etc.) and camelCase for other things (variables, private members, etc.) If you choose to go the route of snake_case, you will be fighting the IDE the whole way.
Naming Guidelines: https://msdn.microsoft.com/en-us/library/ms229002(v=vs.110).aspx 
Claims are simple key value pairs, think of them as attributes of a user. Roles are like claims but do not have values. They are just keys. You can think about it like this: "all roles are claims, but not all claims are roles". I prefer to always use claims and map them to resources using policy and avoid roles all together. https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies
Use camelCase or prepare to be beheaded. 
When you're coding in C#, you must deal with the framework's convention. You must also deal with your own code. It will probably be less work and less thought if you have the same naming conventions throughout. Alternatively, if it's essential that you be able to distinguish between your own code and the framework's at a glance, a distinct naming convention might help.
I'll check that out, thanks.
Have you checked this package out https://www.nuget.org/packages/Orchard.Parser.Yaml/? 
I've always found CodeMaid to move all code around upon save (as long as it's turned on). The only times when it gets a bit weird is multi-line strings and when you want to neatly align Lambda expressions. I'd be interested to know where it's not working out for you.
&gt;with the large scope of my current .NET app would I really benefit from going through the tens of thousands lines of code to switch my naming convention? How do you guys deal with this? Set up Resharper and let it do the heavy lifting for you.
For YAML I used [this package](https://www.nuget.org/packages/YamlDotNet/4.1.1-pre325). I never used RAML, but [this package](https://github.com/raml-org/raml-dotnet-parser-2) could help.
I had pretty negative experiences trying to scale out with IIS. We tried Microsoft's web farm framework and frequently had nodes go "unhealthy" for no reason and remove themselves from the cluster. This would happen a few times until there were none left. If you were to go with a traditional load balancer, the next problem is deployment. The only thing I could find as a decent solution is Octopus deploy, a 3rd party tool chain. In short, there's nowhere near the same level of support for these things on Windows as there is on Linux. I really feel like a second class citizen when trying to deploy our web apps to Windows. I'm sure other people have different experiences and I would love to hear otherwise. (Azure service magic doesn't count, imo - only what you can roll yourself on your own servers)
Not yet, thank you.
This library deserve a more in depth look, good job ! (Typo on first line)
I prefer attribute-based validation. It makes it really easy to match-up validation rules with the properties that it applies to.
Nice article. I started using Fluent Validation for validating incoming requests on a Web API project.
When using Convert.ToWhatever() or whatever.Parse(), you often have two variables that represent the same concept as different types. I think is is also an appropriate time to use Hungarian notation. 
Present a topic that you are familiar with
Thanks!
Hopefully this helps: https://techjourney.net/speed-up-and-download-faster-from-microsoft-msdn-downloads/ It's helped me tons in the past.
Give talks on low level ASP.Net Core https://github.com/dodyg/practical-aspnetcore
No documentation, Readme only tells me how to install, and unit tests only cover a portion of the developed code. I don't even know what purpose there was to making this.
There's a little more work involved to get some support for client side validation. Maybe I'll write up a post on that 
how do you determine when nodes go unhealthy? are there tools for this? particularly docker i was thinking of rewriting my app to asp.net core / sql server hosted in docker with angular 2 front end
I don't know, it's just something the Web Farm Framework decided for me. I think typically a lot of sites use a ping check of some kind.
My company uses SonarQube which has similar metrics to the maintainability matrix. We have a &lt;5% technical debt requirement (which is an aggregate of all the configured standards.) It has greatly reduced our technical debt and has forced our developers to follow the company standards. I fully support it and I believe 10% is a great starting point.
You might try setting your DNS to use Google's. Helped my speed with Xbox downloads: 8.8.8.8 8.8.4.4
Use the naming conventions appropriate for the langage you're using. There is no snake casing in C#. 
I haven't played with rider but VS mac is better than VS Code with code completion. 
SIMD support alone is a big deal if you're doing perf-intensive work in a 32 bit process. Not sure who that would be, but there's gotta be someone. 
How does it compare to visual studio?
Most of the code templates can be customized. I only modified it slightly for MassTransit. The fact it opens a big complex solution like mine on windows and builds it is pretty sweet. And it runs unit tests with RabbitMQ on Mac. Crazy. It's like what I've wanted since oh 2008. 
What exactly does RabbitMQ do for unit tests?
I've tried both Rider and VS Studio for Mac (aka Xamarin Studio) as well. Between the two of them, right now there's no way to have your cake and eat it too. Rider is still in alpha and there's no telling just how long until we see a v1.0 release. From a functional standpoint I like Rider better, but this is probably just be because I'm a VS/ReSharper user and the JetBrains shortcuts and features are what I've gotten used to. VS for Mac is currently a stable product, but it has a leaner feature set and I would assume Microsoft would prefer to keep it that way considering it's a free product on a competing OS. It's also worth noting that Rider isn't going to be free when it's finally released. You might as well start using what you'll be sticking with. If you know you won't pay for the IDE, get used to VS for Mac. If you're willing to pony up for your tools, Rider (or whatever it's final name is) is the way I'd go.
Awesome thank you
Except you'd have to map _all_ of the shortcuts and then mess up ones you mapped that already exist. A plugin or even just a config file with the shortcuts could be super helpful.
I know people have been saying that VS'17 is a hog, but I figure I could disable extensions and turn off various static analysis features if need be.
Any modern machine for dev work needs a SSD.
I'm not sure what the question is. SonarQube runs on our build server during our gated checkin process. Our build server is a virtual machine and not a docker image. Does that help!
I used VS for Mac for a bit and then the update installed an alpha version of Mono which was very annoying and there was no way to roll back so I had to uninstall. So I'm switching between Xamarin Studio, VS Code and Vim!
Yea, I have no problems actually accessing the Gmail API using the access token once I have one, I'm more curious about best practices for initiating the authorization flow using a SPA, and then storing the access tokens with an ASP.NET backend.
I haven't used ildasm but the Mono.Cecil package should also be able to do this.
Download the tool ILSpy Drag and drop your exe into it Done 
Terrible assignment. You realise a .NET EXE is just an ordinary PE format file right?
Well it's not necessarily the case. You can have a terminal middleware.
Lmgtfy: https://www.simple-talk.com/blogs/anatomy-of-a-net-assembly-pe-headers/
Sure. But from the concept, for it all to work, a chain must be constructed - even if one element in the chain will *always* terminate. The last chain element is a framework internal component that notifies the framework that no chain element handled the request.
I'm going to guess that once you write the response, you actually return an output stream, which starts with the headers. You won't be able to pass the context down to a controller that would then attempt to, let's say, write headers and then a JSON response.
In HTTP you need to send response code first, then headers, and then body. This order can't be broken. So as soon as you start sending the response, no other middleware will be able to set headers. So to save everyone from headache, Microsoft suggest you to either handle the request completely and return the response, or delegate creating the response to the next middleware in the chain. 
&gt;Technical analysis software automates the charting, analysis and reporting functions that support technical analysts in their review and prediction of financial markets (e.g. the stock market). (Wikipedia) According to the help screen (CTRL+SHIFT+C -&gt; "help calc") and the API Doc, MathStudio Maths Module offers several functions for statistics, analysis and probability distribution. Hope you'll enjoy the IDE ^^
They probably referring to the "never easyer" part.
If you're referring to the typo, I just fixed the "y". If you're not, I think that, providing a built-in expression and functions parser and a huge collection of pre-defined functions, MathStudio has an edge over other technical IDEs. Have a nice day^^
The wording is fucked. The correct phrase would be `software development easier than ever before` or `software development has never been easier before` That's what the person was referring to.
If you're okay with a small number false positives, all .NET binaries have the string "BSJB" as one of the headers in the metadata tables. Obviously, non .NET binaries can also have this string bit it's kind of cute to use findstr to get a list of (rouhly) all .NET binaries on a machine. &gt;findstr /s BSJB * You can look for PE files in a similar fashion by scanning for MZ at the beginning of files. Here's an article that talks about both: http://www.moserware.com/2007/11/mz-bsjb-and-joys-of-magic-constants-in.html
I'd also add that if a programmer feels the need to mock only part of a dependency then they are probably not unit testing the class in isolation and should instead be asking what is this class's purpose in life, and how can I test that in isolation.
Thank your for your effort but I believe that some more comments in your code and an explanation about why should I use this library and what it does in the read me would be much appreciated
&gt; Making members virtual for testing only is even worse than sprinkling interfaces everywhere What I see is people usually create interfaces for testing purposes only, so basically making methods virtual or create interfaces is the same from this perspective. I know there is a difference between direct call and virtual call but I think this is not an issue when we speak about web apps (it's only relevant for some specific code).
Hi, sorry for the typos, I'll fix asap. To have an idea of the mathemathical capabilities of the application, I'll make a new blog post, with several examples (maybe I'll read again before publish, this time :P). You can use hundreds of pre-built functions (unary, binary, variadic, iterative, calculus) and you can declare your own, too. For example, if we wanted to redefine the factorial function (it's already defined and you can use with "n!" syntax): fdef fact(n) = if(n&gt;1, n*fact(n-1), 1) We can now use the fact(n) function in our mathemathical expression. Another example is to solve more complex equations. In fact, MathStudio provides the Brent's solver function and several calculus functions(integral, derivative, difference) fdef f(x) = cos(x-3*x^2) fdef g(x) = der(f(x), x) calc solve(g(x), x, -pi, pi) Another example is the sum of the coefficient of the Newton binomial: fdef Nwtn(n, k) = sum(k, 0, n, C(n,k)) (Which is the same of 2^n)
How does a web app exclude itself from n-tier architecture? If you are doing data access, that should be in its own layer. If there is business logic to translate data access to meaningful business objects, that's another layer. At the very least, each layer should be separated by interfaces. It's a general rule I've created after many years of being burned by not doing so. Coding to concretes only is great for small projects that can be easily rewritten. It is a nightmare for the multi-million LoC enterprise software I work with on a daily basis. Also, the use of virtual solely to make the mocking framework work is evil. Yes, creating an interface can be considered needless overhead, but it doesn't require altering the behavior of the consuming class to make it testable. Mocking a concrete object works by the mocking tool creating an inheritor and overriding that method. It is altering the consuming class, no longer only mocking it's dependencies. Ergo, you cannot test the logic *within* that virtual method. No big deal if it's a one-liner wrapper kind of call, but any sort of if/else logic in there became relegated to integration testing. Enjoy spinning up that database for a simple "if this" test. Lastly, if everything is virtual just to make mocking possible, the code is ultimately more complex than having the set of matching interfaces. In declaring every class abstract, it takes a lot more time to analyze the code for inheritance. I've spent far too much time chasing down bugs due to inheritance: things like the timing of the base method call in the override (better yet, did it even *get* called?), virtual member initialization in constructors (the horror...), etc. You might say "yeah, but I never do that" - if you are working on a project that involves different devs on different teams, you better believe someone is going to try to do something sneaky with inheritance to solve their issue at hand. I declare all my classes sealed/non-inheritable upon creation, unless I know for a fact that I want to use inheritance to solve a problem. It delivers a clear message to other developers that I want this logic to execute exactly this way, without any child classes mucking up the method call sequence. 
&gt; Mocking off of a concrete class behaves very differently than mocking off of an interface - with a class, you only mock the methods you actually define as mocked and all other members remain as they were. Sometimes that is desirable, sometime it is not. not to mention when the concrete class itself has a non default constructor you need to specify the parameters during the creation of the mock (adding more coupling with other stuff). I can understand that writing a lot of "mirror interfaces" is an issue, but I also feel the proposed solution makes more harm than good.
That kind of load I would expect to run fine on Azure's free tier. I use that one for development sites along with the $5 SQL Server database.
Does this mean there are separate cost for azure and sql server? How about if I use MySQL?
I recently opened a Digital Ocean account - a server starts at $5 a month. I've installed Mono but you can easily install .NET Core - Digital Ocean's USP seems to be targeting developers and I'd strongly advise using them if costs are your concern. Azure and AWS are really more focused on business customers and they make a lot of money from enterprises. Also - this should go without saying - stay away from Windows (DO don't support Windows anyhow).
That is a pretty large missing piece. Any code you "saved" during registration comes back twice as much and coupled in your unit tests.
There is not enough compelling arguments in the article for me to consider this change.
This argument starts with a false dilemma: interfaces create clutter. Calling it an anti-pattern because it offends his sense of order or creates tedium is just silly. Abusing a language feature in order to prevent "clutter" or typing is the anti-pattern. This guy claims to know SOLID, then breaks the goals of every single letter for convenience. I don't understand how this saved him any effort. If anything it could lead to confusion by readers because he didn't follow the usual programming pattern. 
Interfaces can be overused though often I find them underused in production code. I tend to start with the practice of creating interfaces for near everything and then strip out what is not needed or does not make sense in later refactors. To try and keep them out as a general rule will really cause all kinds of trouble (read, hours of extra work) particularly in large and complex code bases as evidenced by many of the other posts here.
If cost is a concern, you'd be well advised to stay away from the non-open-source stack. That means no Windows, no SQL Server. Stick with .NET Core on Linux, and go with open source databases. Even if Windows and SQL usage may fall under a free tier right now, think about the possible costs if you ever exit that free tier level. SQL Server, especially, can hit you like a ton of bricks if you ever need to scale.
It's free to develop. There's been an open source ASP.NET MVC available from the Mono project for ages. The newfangled dotnet-core and its MVC version is open source.
Wrong problem. The real issue here is that you are trying to unit test a Contoller. All of the pain and confusion and arguing comes from using the wrong kind of test. Lets look at the controller method: public async Task&lt;OrderStatus&gt; Order() { logger.Log("Doing some ordering logic here...\n"); await storage.Save(); return OrderStatus.Ok; } Ok, that's stupid. Let's try something that looks a bit more realistic. public async Task&lt;OrderStatus&gt; Order(Order order) { await orderEngine.Verify(order); await storage.Save(order); return OrderStatus.Ok; } All of the unit testable logic is now inside `orderEngine`. But wait, verifying an order may require some additional information. So let's extract that. public async Task&lt;OrderStatus&gt; Order(Order order) { var details = await storage.LoadPreorderDetails(order); orderEngine.Verify(order, details); await storage.Save(order); return OrderStatus.Ok; } So here's what we got: Unit tests: orderEngine.Verify(order, details); Integration tests var details = await storage.LoadPreorderDetails(order); await storage.Save(order); Bullshit test that doesn't need to be written public async Task&lt;OrderStatus&gt; Order(Order order) By decoupling your data access from your business logic, everything that needs to be unit tested is unit tested without mocks. 
&gt; Totally disagree about having a dependent class (controller in this example) accept a concrete class instead of interface. Why? If you think very carefully about why you are trying to unit test an integration component such as a Controller, As a general rule, I find that if I feel the need to mock a dependency one of three things is true: 1. The class shouldn't have that dependency in the first place. (e.g. mixing business and storage logic) 2. I'm trying to use the wrong kind of test. (e.g. unit testing an integration or storage component) 3. I'm working with hardware that needs a full simulator, not just a quick and dirty mock. 
Since the OP is a student, they might be better served by having "Azure" on their resume then by having a custom domain. That said, if OP wants a custom domain they can always look at shared hosting for $5/month. That option seems to be dismissed around /r/dotnet, but it's a viable option. 
Since the article seems to be aimed at new programmers, for anyone doing interview prep questions at Leetcode, Hackerrank, etc, the `HashSet&lt;T&gt;` has much better performance than the others listed in the article for certain types of algorithm problems that (reportedly) come up in interviews a lot.
If using less than 200mb in SQL, then you can use the free db - it's not obvious how to select it though (of course it wouldn't...).
No, it is a licensing issue, MS is restricting debugging support. Looks like it works now for Rider, but who knows for the future. https://www.infoq.com/news/2017/02/rider-eap18
Yes, and I'll go on the record to say this is why I don't like C#'s implicit vs explicit interface behavior. The article makes a few decent arguments against interfaces that solely exist to 100% *implicitly* cover a concrete type, which is a common thing I see in C# code. C# favors implicit interface implementation. I rarely see explicit interface implementation out in the wild, unless it's a source base where the devs truly understand the importance of interface contract vs concrete implementation. This is where C# took a fundamental swerve from VB.NET (yes, I dare to utter that name), because VB doesn't have such a language construct. You must explicitly declare the interface implementation on each property or method on the implementation class in VB.NET, and you use the access modifier to say whether or not it is publicly accessible on the concrete type as well. Either way, I still think making everything virtual to make it mockable is bad. It's just as bad as making every method public so that you can write method-level tests. That kind of technique is naive and misguided TDD. It's a training-wheels kind of approach; those need to be removed at some point ;-)
I'll second the asp.net core idea. Still C#. Still can be written in Visual Studio. Runs on linux. I've ported a very complex library to it before and had it running on linux no problem. DigitalOcean has VMs as cheap as $5/month that you could run dotnet core apps on. I've also developed a whole solution for a client using nothing but Azure's free tier. They purchased a full subscription after development was mostly done.
I don't see what good getting at the thread doing the work would do. Just have a central place that stores information about running validations, the threads themselves don't matter. There are also a few thread safe collections like [ConcurrentDictionary](https://msdn.microsoft.com/en-us/library/dd287191.aspx) that could be useful.
Create a single method that calls the stored proc. Apply your cache in that method. Rewrite each controller to call that method and do your linq filter on the results. You can use system.runtime.caching to implement the cache https://msdn.microsoft.com/en-us/library/dd997357(v=vs.110).aspx. 
Need to see some code, but I suspect you are returning control to the default model binder. Do you "return base." anywhere? Also, override the BindModel method as well, not just the CreateModel method.
Thanks. What should the cache expiry be? Time bound e.g. 10 seconds or so (enough to complete all three requests)? In that case isn't there a very slim chance that another user comes along and gets one or two panels loaded from cache and the third from a cache refresh i.e. possibly different data. Or should I use a guid on the page to reference a unique cache item?
You can get a free sql db if you have less than 200mb of data.
Scott Hanselman wrote an awesome post about the ease of deploying ASP.NET Core apps with ZEIT Now. https://www.hanselman.com/blog/ZEITNowDeploymentsOfOpenSourceASPNETCoreWebAppsWithDocker.aspx
Your business models are not your UI models. Stop sharing them and you won't have this issue at all.
What a pathetic community .NET is. Downvoting anyone steering away from Windows. This post is about COST - there's a reason startups almost NEVER use Windows. FFS.
It depends on how quickly your data is updated and how much tolerance you have for stale data. I usually set longer expirations, but that data is less frequently updated 
&gt; I inherited the Ajax architecture, would it be quicker to return all three grids' data from initial page action and forget the Ajax? I don't know about "quicker" but I would say "better". Remove the knockout, remove the Ajax, and make them child actions. Load the data into the model once, then pass the model into each child action.
&gt; you'll more than likely make more money professionally with .net over PHP It's not only about money, just ask r/ProgrammerHumor
My thought was getting a reference to it would allow me to check its state, basically whether or not it's ran to completion. And since with MVC the state of the application is built anew with every request, the only way for me to be able to have the background thread store that info for a different request to access it would be in the database. I guess that's not terrible, I suppose I could go that route. Unless there's something obvious that I'm just missing or don't understand/know about MVC.
Could you give an example of a use of the Span and how it is better than a normal array? I know the Microsoft web socket uses spans to transport information into and out of the socket. Moreover, the ArrayPool uses these objects for memory pooling... I just don`t get why exactly. My guess is these things are long-lived stack allocations. So, in other words, we can reference these things without a box cast or a heap allocation, is that about right?
Yes. 
Remember, you can choose asp.net core on .net core on on the full .net framework. 
You thinking of `ArraySegment&lt;T&gt;` which is public struct ArraySegment&lt;T&gt; { public T[] Array; public int Count; public int Offset; } While that tells the called function what it should use (start at Offset and only use Count elements); the function can still modify the whole array even outside those bounds. `Span&lt;T&gt;` on the other hand is more like: public struct Span&lt;T&gt; { private IntPtr Start; public int Count { get; } public ref T this[index]; // Read/Write } So is both smaller and only allows the called function to use the specified range of the Array and not outside it. Also it can be converted to a `ReadOnlySpan&lt;T&gt;` public struct ReadOnlySpan&lt;T&gt; { private IntPtr Start; public int Count { get; } public T this[index] { get; }; // ReadOnly } Which means the called function can only read values from the "array" window, it cannot modify them. Whereas array elements are always read/write. All this is a zero-copy window over the array; so there is no overhead. It also goes further as you can create the same Spans over unmanaged pointers of native memory to turn them into type checked and bounded windows that look exactly the same and behave the same as the array window. The same is true with [stackalloc](https://msdn.microsoft.com/en-us/library/cx9s2sy4.aspx)ed data; where you allocate an array in stack space rather than heap space.
His question still stands, even with the choice. ;-)
Create a ValidationCache class with constructor ValidationCache(TimeSpan defaultExpiration) and two methods SetValidation&lt;TValidation&gt;(string accountKey, TValidation validation) and TValidation GetValidation&lt;TValidation&gt;(string accountKey) then add that as a Singleton to your ServiceCollection and add that Singleton to both co troller a as dependencies.
Don't overthink it. 
Or that http://joeduffyblog.com/2013/12/27/csharp-for-systems-programming/ ?
There is no session storage but if you create a singleton, it should be accessible from every request.
A few years ago I had some success with their WCF 3-tier architecture. But they do also have a rest based version. Can anyone else speak to the quality of this? http://www.dofactory.com/ 
It depends on your project. If it is a small project then there is no need for complex architecture or to worry about architecture too much. There was similar post not too long ago: https://www.reddit.com/r/dotnet/comments/5yf5qa/how_do_you_break_up_your_mvc_solution/ Read top two posts, they sum up your question. Boilerplate from https://aspnetboilerplate.com offers great out of the box project architecture but it could be a bit hard to understand if you are beginner.
This is good information. I wish this had been written 2 days ago, I spent many hours over the past couple of days trying to figure out how to get the host header from my reverse proxy so I can do multi-tenancy by subdomain. Finally came across BasicMiddleware last night and came to the same solution (minus some of the options that I didn't need) as the article. 
I hope so UpdateEntity&lt;TParent, TEntity&gt;(DbContext context, TParent to, TParent from, Func&lt;TParent, TEntity&gt; propertyGetter) where TEntity : class
&gt; Every time a request is made, all of the .NET objects are rebuilt, and so even a singleton will be empty on any subsequent requests, Not entirely true. Singletons and static class members persist across requests. Their lifetime is the lifetime of the application itself so once created they live until the application is recycled or shutdown. Objects/data placed into Cache also persist across requests (if not, what purpose would a cache have?). If you're running on a webfarm without using sticky sessions on your load balancers then there are many more considerations that will apply. In this case, you would likely be forced to look into a distributed cache implementation (or a database as you've said).
Been using it in production since it RC2. Not by choice. The guy in charge liked to make rash choices and liked bleeding edge things. But its worked well for us. We had problems with azure deployments for a while in rc1 and rc2. But 1.0 fixed everything. There's some performance problems in EF that were addressed in 1.1 that helped a lot. That things have been pretty stable since 1.0.2 though. I've not ran into any bugs. Just minor pain points and upgrading packages only to find out some are behind.
&gt;This includes things like Entity framework - keep that away from your core and hide it in another layer. What are your thoughts on using DataAnnotations instead of using the Fluent API? I personally use both at times. That could be bad practice though. 
Architecture is something that serves the needs of the application. Therefore you have to understand what your application needs to do before you decide on an architecture. I'll add that not knowing the extent of what it will need to do and allowing for that flexibility can be part of those needs. You mentioned SOLID, and it's important to know that it is not an architecture, but a principle. Starting with it is a good way of understanding what architecture might be a good fit as you identify separate concerns within your application. I assume you have some sort of persistence with your application, so it might be good to start with getting any database related code into its own structures. I'd also imagine you have a lot of logic in your controllers. Try moving logic into distinct application services in a way that's organized by concern. Those two tasks are nearly universal in almost any web app I've written. From there you should be able to start seeing different areas of concerns and how they're related. Hopefully you should have more clarity about what your application's needs are once you pull out discrete pieces.
I think the truth is that there's a time and a place for both approaches. There's a *right* way to do things and a *correct* way to do things but they aren't always one and the same. For simple enough POCO objects? I'd use DataAnnotations. I'd use DataAnnotations where it makes sense and for everything else, use the Fluent API. The Fluent API gives you more control, but if you don't *need* that control then does it really matter? Are you gaining anything by using the Fluent API? My only personal rule when it comes to EF, which is slightly controversial, is to not use the EF data models elsewhere in the solution. I keep them all in the DAL and map them to domain objects in the core of my solution. That way, I can move back and forth between fluent API or DataAnnotations without much hassle for the rest of the project. Hell, I can rip out EF entirely and use Dapper if I want, again, without affecting the rest of the project. 
ASP.NET Core with the full framework is production ready. ASP.NET Core with .NET Core depends on your requirements. 
I think CreateModel is meant to create your model much like a factory method. Bind Model is where you assign the values to the model. I think this SO answer gives a good explanation http://stackoverflow.com/a/11171053/516419
Where do you put business logic?
Depends on the logic, but essentially it goes into the ~~infrastructure​~~ intermdiate layers of the "onion". For an MVC app, it's best to keep your controllers as thin as possible, so look at patterns like CQRS which work really well with this kind of architecture.
Maybe slightly controversial... but I say a great design. 
In the original onion architecture articles I read, your core was wrapped in a domain and application services which contained most of the business logic, which then had a UI layer (MVC), test libraries, and infrastructure (dependencies) on the outside. Infrastructure pieces were implementations of interfaces in the core for dependencies, but explicitly NOT business logic. There were some weird things around DI principals they did around this arch, but otherwise it was pretty solid the couple times I used it.
See http://jeffreypalermo.com/blog/the-onion-architecture-part-1/ This is how I've always seen the "onion" approach explained, and the two missisng layers, domain services and application services, is where business logic traditionally went.
Cool. I didn't know that it worked that way.
I know exactly what I'm refactoring at work tomorrow. I literally just got done implementing a new registry class that this would work quite well in, I think.
In your models or the few implementations in Core
Unless I'm mistaken (and it's been a long day so who knows) the benchmark isn't accessing array items by index, but rather searching an array of items for all items meeting a specific criteria that isn't represented by the item's array index: public List&lt;string&gt; GetFromArray() { return items.Where(i =&gt; i.Date.Month == 2) .Select(i =&gt; i.Name) .ToList(); } In the test data there are multiple values that share the same key (Date.Month), so using a Dictionary for fast access to find all items where Date.Month is 2 would require another layer of indirection. LookUp handles that for you. &gt; Lookup is collection designed for storing multiple values with shared key
Ironic how the SyncFusion Tech Portal didn’t even build its registration form correctly. When I was inputting my information via my phone, I was presented with the default keyboard for all form elements; you would think that they would make the eMail form field an actual eMail input (to suppress autocorrect capitalization and put the @ symbol on the bottom row), and the phone field an actual phone number input (to bring up the numeric tenkey). This kind of design does not inspire confidence, especially from a supposedly professional company and when a more holistic approach is so simple to implement.
You're absolutely right and I've worded my original post badly here. I've edited it to clarify things.
&gt; What’s really different is times needed to access stored data. Why there is so much difference is content for another post. Would love to read this follow-up. Right now I'm not convinced this is anything more than an apples to orange comparison of two things (e.g. that perhaps the dictionary implementation is somehow much more inefficient than it needs to be and ToLookup isn't really a silver bullet). Conceptually my assumption is ToLookup needs to build a map internally to avoid multiple scans of an IEnumerable, anyway?
Take it one step further. Can you restructure your business logic so that it doesn't have external dependencies. Instead of dependency injection, dependency rejection.
I just finished setting up dotnet core with VSTS. It used WinRM to deploy. It works good enough. If you have to do it manually, take the site down via a powershell script, then copy the files. Then take it back up via another powershell script. One caveat, however, is that I had issues with folder permissions being reset. I am going to switch to Team City soon due to VSTS limits on builds. Then, I am planning on using Octopus Deploy. They have a free version if you'd like to try it out. The way that works is you create a nuget package from the output of dotnet publish. Octopus can do everything you need and it's easy to use. 
If you run out of ideas, try using Mailkit, it's a much more robust library and very easy to swap in. 
Unfortunately, there's only 1 email (well, I can replicate it, but it's not global) that's affected by this. It's just emails I send through .NET's SmtpClient library and my SMTP provider (fastmail), who informed me that they looked at the emails sent and they see them passing incorrect headers that were provided to them by my SMTP client. That really just leaves SmtpClient as my concern... is there no known fix for that?
You'd have to go through the usual steps. Usually I'd start with checking what timezone the server is set to. Can you reproduce the issue locally? I can't imagine it's an issue with the SmtpClient code, I can't see anything related to it online.
This is the first time I've attempted this - I'd love to know if there's a better/tidier way :-)
I would [fetch](https://github.com/github/fetch) the data then dispatch it using redux. If you don't want to implement redux or anything like it, then you can at least fetch the data in your container components, then pass it as props to its children. Serializing an object as a JSON string for the sole purpose of passing it to a React component just seems kind of smelly. Maybe that's just me. 
This is the second set of documentation that I have seen for .NET core applications telling me to edit project files directly. Does nuget not modify the csproj file for you?
I like using [Squirrel](https://github.com/Squirrel/Squirrel.Windows); makes things a bit easier. 
It does , but only using full blown visual studio in windows . A lot of people are looking at .net core on macs or Linux In those cases you have to edit csproj and do a restore ( with visual studio dotnet restore runs after adding a package via nuget package manager )
Hey, thanks for the tip! I meant "better/tider way of building an MSI", but "don't" is probably the best option, if you can avoid it, and Squirrel looks perfect in that case. (I'm in MSI land because I'm ...slowly... working towards moving another software product to .NET Core, and need MSI for that one.)
What about gitlab ci? https://about.gitlab.com/gitlab-ci/
Perfect timing! I was just starting my first core web app that will need two-factor authentication. 
- You don't have to run it behind IIS. It will work behind NGinx. - Yeah I don't know why project.json was created in the first place. - ASP.NET Core simple takes much less memory compared to ASP.NET - ASP.NET Core is simply a nicer framework to program against.
I admit I am a react newbie, but this doesn't seem to be how you are serializing and passing it back from the server, but what you are passing. It seems as though you should have some sort of viewmodel or "light" version of the object you are passing back to make sure you are using too much bloat. Passing JSON back to your view is a pretty standard method, I think that's not the stuff you need to tweak.
I don't understand the point of this blog post, it is literally a copy of the ASP .NET Core documentation without any additional information or insight on the matter. Also the title is strange: "top 2 ways", ar there any other ways to pass parameters ? Why the examples shown are the "best" way ?
&gt; https://about.gitlab.com/gitlab-ci/ oh I didn't know about that. Neat!
Yup, this is data passed from the server to the front end. And I do agree that there should be a "lighter" model that just brings back only what I need, which is what I have. But I think I need to look more at what I "need". I don't think there's a best way to get the data from the server to the client on that initial load in a way that leaves me feeling clean. It'll always have to go through the html to get at it on initial load, so it's either deserialize and serialize, or put up the loading icon and do the ajax call to grab the data. I think I just need to find a happy medium. Thanks for your input though, I appreciate it!
That looks like a nicely designed API. Kudos!
Squirrel will generate an MSI for you.
Oh - nice! Sounds like I need to check it out again.
I've worked with a lot of generics and I disagree.
AT my shop we take the following approach. - Unit tests for testing business logic implementation server side. We dont test too much of the basic CRUD operations as there is no real need to test the ORM. - Specflow tests for API endpoints to ensure that DTOs are as expected and signatures dont change - Protractor for coded UI tests (we use AngularJS). We typically handle that for security roles and happy path for those given roles. We dont even attempt to try to get to 100% code coverage and the team has agreed to what we have coined Appropriate Level of Testing. We want to make sure that we're spending the appropriate amount of time on testing, where the greatest value recognized. 
We have unit tests around the core business logic (command handlers, mainly) and then a semi informal QA process once something gets out of dev. It's not necessarily ideal, but for a smaller team and no dedicated QA resources it's still actually handled a lot better than anywhere else I've ever worked. 
At my shop we compile it and then a human theoretically tests it at some point. Unit tests are frowned upon as a waste of time. I know it's wrong. We're heathens.
You've seen open generics in production code?
The automated testing we do: * unit tests - lots and lots of them, front and back end * API integration tests - much less than unit tests * e2e - as little as possible The manual testing: * QA does manual testing in various environments * UAT by PO's and putting functionality in front of real users, early and often * smoke testing on stage before promotion to prod Pen tests are only run quarterly. Load testing depends, it's adhoc really. We know the behaviour of our apps well, if something is introduced to make us doubt that, we'll load test. 
I was one - at the time they started the program a long, long time ago :-) 
&gt; Oh, and happily, you don’t need a particular version of the .NET Framework installed to run this app: it’s fully self-contained, carrying everything it needs in the 20 MB MSI package. &gt; Unless I’ve missed something vital - which is entirely possible - building your setup project will create an MSI that installs everything generated by dotnet publish. So which is it? Does it include the .NET Core installer or not? First statement suggests it does, second does not, and I don't see any evidence it does? Of course I've only used the preview tools for VS2015, do the final tools publish the full .NET Core with your project? Or is WiX smart enough to package the installer up with yours? Anyway I think the general approach to any installer for a .NET Core project will be the same as yours... package up the publish folder. That's what it's for, after all. I use NSIS myself and it's a single File statement to point it to that folder as a source for install files.
It probably helps if you want to work for a Microsoft Certified Partner. I got the certification many years ago and thought it was nice to have on the resume at the time. It probably helped a bit... hard to say. Although I haven't kept up with certifications because it hasn't been necessary for me.
It's not as complicated as you would think. You centralize the OAuth server and instead of using one identity per website, you centralize the Identity server in a way to use it as a identity provider, just like Google or Facebook. That way, you will be able to extend beyond just Google/you if the need comes. You can then at the authorization server handle the claims from the multiple providers to ensure consistency of privileges.
as someone who interviews a reasonable number of devs I ignore/laugh at Microsoft certifications. Some take a bit of effort but in all they really mean nothing. i'm either looking for someone experienced (in which case the cert means nothing) or a fresh grad (in which case they'd never have the cert and i'd be expecting to train them from scratch). 
We do TDD as much as possible so every bit of production code should be properly unit-tested. Then there are integration tests that test the full pipeline (it's an OWIN application, so that's pretty easy) and all database operations. Then we have acceptance tests that test the application from the outside on all the acceptance criteria. Unit tests are run at build time, as well as the integration tests that don't test external dependencies. Integration tests that require them are tested automatically after deployment to the testing environment. After deployment to the acceptance environment, the acceptance tests are run. Failure of any of those block progression to the next stage (build, testing, acceptance, production).
As promised I'm making a serie of blogs post to show how to use all the commands properly with some examples and tutorials. Here's the first one: http://blog.gomanga.it/simple-second-grade-equation-solver-mathstudio-tutorial/
Not a .NET guy any more, have moved on to SQL, but I do this same exact thing to keep sharp on all the new features of SQL every few years. You can read the blogs all day long, but they usually only write about the cool new features, the MCSE test will make sure you are at least AWARE of all the features, the boring ones too. 
Don't do it unless someone else is paying for it, and perhaps, you plan on becoming a contractor likely to encounter a variety of MS technologies in the field. Certifications peaked in value just after the turn of the century, and I haven't heard of any of my peers pursuing a cert in over a decade. In general, you're waaaay better off building up your personal Github profile these days so companies can see code you've written, what you're interested in, and what you've contributed to the community. edit: source: 21 years as a pro software dev, mostly on microsoft stacks.
I did a couple in the beginning of my career to compensate for my lack of experience. I genuinely think they helped me to go from trainee into a permanent position because my certificate helped the company save a fortune in MS license costs. I quit thinking about certificates entirely once I had some real projects on my resume and would not do them anymore unless they were specifically required for some project.
The mentality here is that customers don't mind bugs. They are happier when we fix things fast. It blows my mind.
Even if they were to look at it, are they seriously going to go through and test ALL of the features over again or just piddle at the new bits for a hot minute and say "I'm a busy manager and that's good enough!"? Regressions hide in the darkest corners.
Not just regressions, we convinced them to try to reproduce a bug and they got stuck on some required field validation that never really worked well if you didn't fill the fields 
What is smoke testing? 
Have you checked ASP.NET Core with Kestrel? It's quite similar to node.js and express? edit: there's also https://github.com/Azure/DotNetty for more generic TCP/UDP server.
Been there, done that. Imo the certs are useless for anything except keeping your org's partner status. For that reason, they have a nominal value. The low level certs (at least) are not a good technical judge because they're heavy on useless trivia (eg "what''s the ordering of parameters to this function") and the practice tests historically contained effectively identical questions to the real tests
Hi @nirataro, yes indeed (Kestrel). I wanted this to be lightweight, low learning curve, and not dictate a design pattern (not that the pattern is a bad thing). I have not used DotNetty yet and will check it out. Thank you!
Seconded. That said, it helped me more when I was just in the first 5 years or so of professional programming. It doesn't help as much when you're more experienced. Even so, it's still not a waste of time because it helps you dig into those new corners of the technology in a pretty efficient manner. 
Fix this: "Or Call +1 123 456 7890"
Some unit tests, a few integration tests. Then during the release process there is a suit of smoke/integration tests running in a staging phase before the new version is gradually rolled out in production. Focus is on metrics, monitoring and logging during the rollout. Quite different from earlier projects/employments where there was much more automated and manual testing.
If you're new to MVC, there's a course on Pluralsight called "Rebuilding Web Forms Applications in MVC" by Alex Wolf that would probably help you a lot, if you're able to use a trial or existing subscription for Pluralsight.
The problem with regex based routing is that it is quite easy to mess up. You can 'borrow' the implementation of the TemplateMatcher from ASP.NET Core to enable the fancy segment based routing.
I find the practice exams really good for this cause I will read the books (Like you said only the interesting parts) and watch the MVA, edX or Pluralsight videos. Then I will get to a question in the practice test and have no idea what they are talking about, click the link on the description information have a read and make some notes and then you become that guy in the office who knows all the peculiar things.
Hum, I don't know if and how Google is implementing that. I bet you can still track the token they provide. Indeed you will need to assign claims based upon your directory, but that's how you provide an identity. 
Check out also WiX. 
Yep, this. I got my cert to help get my company become Silver certified. The extra MSDN subscriptions were the target.
I have done so already, to no result. 
Event Viewer application section is probably a good place to look as IIS/IISExpress can put stuff in there What platform are you running on?
Aye odd thing is, my other projects work just fine 
Windows 10 and Linux in a VM for testing
The debug play button should say something about what is going to run; likely "IIS Express"; you can drop this down and change it to name of your app (if its ASP.NET Core) and then run that. This won't use IIS Express as a proxy; so you should be able to see if its your app (or the IIS Express setup: Web.config, lauchsettings.json, `.UseIISIntegration()` in program main etc)
not by default no, but I can configure it to build allow for that, good idea btw.
You can port forward your router and then use something like no-ip.com to setup a URL that binds to your public IP (I do this with my Jenkins build server). This is called dynamic DNS, most routers should have an option that will automatically update your IP address with the dynamic DNS provider. 
Ok, I'll look for that. Just stepped out for lunch after messing with it for a while. What if it was a closed network with no internet access, there has to be a way to do it without an external site like that, right? Does that just make it easier to configure?
Yes, exactly. 
It's not that easy, as said below you might be better off connecting to it via IP and open the port in the firewall. For the DNS server method, you need to be running a DNS Server (for example I use Windows Server 2016). I believe there are linux distros that can do the same as well. Once setup, look up tutorials on how to add records with your chosen DNS server.
It is indeed tricky. This specific application is the only one that would need access to those claims, though, so I was considering just using a [custom cookie middleware](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie) in lieu of identity and pulling claims through the [admin sdk](https://developers.google.com/admin-sdk/directory/v1/guides/manage-users#get_user). We'll see how things go...
Are you sure you need Windows hosting? Microsoft Azure will let you host up to 10 web apps for free.
Oh? Including databases?
That depends what you're trying to do. If you're trying to use cheap/free hosting then my guess is you're not trying to host a "real" app, in which case SQL Server CE will probably be fine. Are you sure you need a database?
It's for my billing area, so yeah, needs a database. :P
This doesn't add up. You need a dedicated database yet you're trying to scrape by on free or dirt cheap hosting? If app downtime will result in any meaningful loss of revenue, pay for proper infrastructure. If it won't, you probably also don't need a dedicated database yet. What are you trying to do?
I saw this earlier, does it work with IIS express?
It should.
&gt;The site that will be hosted is my client billing area, where clients pay any invoices I have for side jobs (sometimes they're frequent, sometimes they're not - never more than 10-20 people each month on there). You don't need a dedicated database. SQL Server CE will be fine. EDIT: I wrote this when I was tired. Do NOT use SQL Server CE on Azure if you're deploying it as a web app, as Azure can redeploy your app whenever it feels like it which will cause your database to be overwritten with the copy that you provided when you deployed the app.
You could look at SQLite.
I just want reviews on 1&amp;1, or alternatives if people think it sucks. I don't want any database that is a file in the same place as my website.
You're wasting your own time and the time of everyone else here. If your argument is "but I _want_" then by all means get whatever it is you want, you don't need feedback from other people pointing out that it's unnecessary. I agree, if it were my app I'd just purchase a SQL Server instance on Azure (SQL Database pricing on Azure starts at £3.72 a month) and just run the app itself on the free tier, as it avoids the complications of trying to use an in-memory database on a cloud server. However, the fact that you're suggesting paying for _3 years_ of basic hosting up front for the sake of saving a few pounds a month is insane, and supports the idea you have no idea how insignificant your app actually is nor do you have any idea what you're doing. "20-30 people each month" is **nothing**, our company's internal test sites get WAY more traffic than that. Even 20-30 people a _minute_ will barely register on most free hosting. That load is not something that warrants being put in the same sentence as "production environment". Get the Azure free tier and add a SQL Server instance to it. It'll cost you £3.72 a month and allow you to scale your app as necessary, and most likely you'll never need to upgrade from that.
I suspect you're restarting the app but then posting back an already loaded page. Webforms outputs hidden validation fields onto every page which it encrypts, this encryption key is changed on startup so if you post a page back after restarting the server it's very common to get this error. Another fix is to use a static machine key instead of generating one on startup but that's not great security wise. (it's better than disabling event validation though) edit: if it's not due to restarts i'd be tempted to remove the session check in the aspx as its possibly related. Also if you can you should stop using webforms.
TL;DR: Accumulate changes to save to decrease calls to the database.
I prefer zipwhip over twilio
The session check is only there as a "logged in" simulation since the error is causing my actual login unable to work haha I've removed it before with no change. What's wrong with web forms? 
I do have an idea how insignificant my application is. At work we have systems which process near enough a million requests a day. I get it. What I have is nothing. I just want to know if 1&amp;1 is worth it, or if there's a better alternative. As for the free tier &amp; SQL Server instance - I'll take a look at that.
I use Wyam. I like Wyam. I did a lightning talk with the author of Wyam (who is super nice by the way)
....grok your database and use stored procedures...
Meh. I'll take a thousand SPs over a situation where nothing can be changed because nobody has any idea about how the db is being queried because everybody just throws custom SQL at the DB 
Since this appears to be an entry level article it would be helpful to explain the difference between the two. In your example, SaveChanges will issue an update statement for each entity, which means more database executions.
The error message OP posted isn't even making it to OP's code. It's blowing out prior to that.
&gt; That's what the repository layer is for: defined queries. If you have one application or at least one code base per database, that works. That won't be the case after a few years, though. 
Webforms is basically a dead framework and isn't as good as alternatives. Actually restarted website would give Mac validation error, this feels like something is tampering with the post back values. Can you post the master page code? Is the form run at=server? 
False. I work on a large 14 year old Enterprise system. There are at least 10 applications, 5 of them are critical to each part of the business they serve. They are all using the same database from one repo layer. Takes discipline, but it's way better than stored procs for everything.
I do not really understand your point. While you have to muster a lot of discipline and code reviews to enforce these things, I can use database mechanisms like schemas, views, stored procedures and occasionally even triggers to do the same. On top of that, if I want to change something, the database can tell me, down to individual columns, which objects will be affected. I can implement sweeping changes without touching client code at all. 
Is there a reason why your repo layer cannot call SPs? 
Great post. I think the ASP.NET Core team could have done a better job with their DI Container abstraction. Or maybe they could have gone full Seeman and his DI-friendly framework stuff. The way I see it, this new "configure container" facility is an attempt at fixing things, but it's a bit late. As a framework author, I have two options: either conform to MS's container (which has no out of the box support for decorators, or configurable nested scopes), or create my own DI abstractions and provide implementations for existing DI containers such as Autofac - unless my framework explicitly depends on Autofac, which would be bad for adoption. 
Microsoft actually did a good job with their documentation: https://docs.microsoft.com/en-us/aspnet/core/tutorials/
Thanks for the fast reply! I was reading the docs and I cant seem to find what I need. As I stated with my post, I need to have a registration form that saves the data to my custom database with tables that I designed. And use that data to login the user. Am I missing something from the article? I saw the **Security &gt; Authentication &gt; Introduction to Identity** from the side panel. But it doesnt help me with what I need. Anymore ideas? Thank you for the help and suggestion, I really appreciate it.
Nope. It's a choice to not maintain them unless necessary.
Depending on your situation, multiple calls to SaveChanges might not be part of the same transaction.
I'm not sure, it doesn't seem to say where the nuget.config file goes.
Hey, so a friend pointed out that it was not a server configuration but rather my application configuration. I needed to add a `UseUrls()` method call to the main method declaring i want to use the specific network address that vagrant has configured. My `Main` looks like this now: public static void Main(string[] args) { var host = new WebHostBuilder() .UseKestrel() .UseUrls("http://192.168.111.111:5000") .UseContentRoot(Directory.GetCurrentDirectory()) .UseIISIntegration() .UseStartup&lt;Startup&gt;() .Build(); host.Run(); }
I have to agree... expecting support for an old version of TFS is pretty unfair. 
Neat, never heard of twillio, or given any thoughts towards how phone machines like that actually work. I'd like to see the code that handles user input, i.e." press 2 for sales" etc... Great tutorial production quality. 
I will be messaging you on [**2017-05-02 16:05:18 UTC**](http://www.wolframalpha.com/input/?i=2017-05-02 16:05:18 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/68nuxa/how_to_make_and_receive_phone_calls_using_c_and/dh0v4q7) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/68nuxa/how_to_make_and_receive_phone_calls_using_c_and/dh0v4q7]%0A%0ARemindMe! : 6 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! dh0v4t6) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
&gt; My point is the multi-app situation you describe is quickly going out of style in favor of service based approaches. I'm not convinced that this is always happening for the right reasons. If your services implement business logic, then fine. Throwing http in front of the DB just because you do not want to use database mechanisms is not a good reason IMHO. Especially if the harsh performance penalty leads to increasingly byzantine caching constructs. &gt; [T]hough I'll say that lack of good VCS and deployment strategies, and tooling options that don't approach those on the managed code layer for navigating SPROC structures or organizing them in a coherent manner, have made that route a nightmare for multiple teams I've been on in the past. Most, if not nearly all SPs belong to the application, and are maintained and VC'd together with the source code. The idea behind using SPs for everything is not access control, although they can be for that. It's about making dependencies between applications and database objects explicit. 
I'm not super familiar with it as far as the features you're looking for like backplane/load balancing, but this project: https://github.com/AdaptiveConsulting/ReactiveTraderCloud uses crossbar.io (http://crossbar.io) which is based on WAMP (http://wamp-proto.org/). You might find some alternatives in these.
It all depends on what you are doing. We've not encountered any performance issue as long as you don't do anything stupid like allowing a grid to extend infinitely beyond the end of the screen (give your datagrid a MaxHeight property!). We are using WPF with databinding in a professional environment, displaying in a datagrid ~40'000 rows with ~20-40 columns of databound data. It takes about 0.25s to load and render. Is that fast enough for you?
I just started reading up on BindingLists vs ObservableCollections and yeah that do seems to be one large pile of possible pitfalls. 
&gt; xSockets Their site is a little odd, where do you see that they charge?
&gt;unusable in a large application cause of performance issues This is incorrect. WPF is used for a lot of large applications that utilize Data Binding. Visual Studio being one of them (try snooping it, you'll see). If you are having performance issues, make sure you are using the right controls, virtualization, async, etc. There is also async databinding as well. Note that if you have a splash screen, you can "pre-load" all of your ViewModels and/or have all the Views render here. This will allow the initial rendering to happen only once (if your Views are alive for the lifetime of the application. 
You may want to avoid ascx files for managing the rich content. Ideally you should have your site pre-compiled anyway for performance and thus will have no access to those files at runtime. You can still offload the rich content markup/down to a local file if you don't want to host it in a DB. You will have to lock the file when it's being edited for concurrency. You s/b able load the markup from that file into an ascx at runtime easily enough though.
Yeah I moved from short polling to signalR because I wasn't happy with having to constantly ping my server at a rapid pace for updates when in many cases (especially when my client's businesses are closed but they left the page open) nothing new is happening. 
Thanks, I'll have to check it out. Is this just a change server-side or did the actual client signalR scripts change as well? What kind of things did it alleviate for you? I'm a little leery about throwing a preview build into production but if its solid enough it might be better than running the old version with bugs
RabbitMQ looks interesting... How does it differ from signalR? I've grown kind of accustomed to having websockets and their fallbacks like SSE spoon-fed to me by signalR at a high level
&gt; Is this just a change server-side or did the actual client signalR scripts change as well? From what I've seen so far, server-side only. &gt; What kind of things did it alleviate for you? There were 2 major things fixed that we were running into: 1. Notifications weren't always getting sent to clients randomly. 2. There was a bug around the reconnect which caused notifications to stop sending, which made us have to restart our app in Azure anytime that happened. You can view all the changes for this release here: https://github.com/SignalR/SignalR/milestone/44?closed=1 &gt; I'm a little leery about throwing a preview build into production but if its solid enough it might be better than running the old version with bugs Definitely understandable. We've been running the preview build in our dev/test environments so far for the last 3 weeks with no issues. It's been looking promising so far.
True, website has been changed from the last time I visited it and I could find pricing section as well.
We use PubNub but only from .net -&gt; js clients, but the .net API isn't too shabby, so I wouldn't have a problem running it to .net clients as well. Only thing is it's never been app critical for us, so I'm not sure about dropped messages or duplications..
Yeah, we're really ingrained in the .NET environment, both server-side and in WPF apps
Page the data! You can't possibly for that many rows on the screen, so why on Earth would you try to render them all? That should greatly reduce any lag.
IMO, one of the frustrating things about these JWT tutorials is that they completely skip one of the most essential parts of this type of auth - the refresh token. I think it's awfully naive to expect your user to sign back in every 15-20 minutes (short lifespan of JWT). The only group that seems to address this issue is identityserver.
I wouldn't do that. When you change .ASCX markup, the app pool will often recycle causing the site to effectively restart. This will be very annoying for users.
Unit tests, and manual QA, but recently we've just taken on an automated tester and he is doing a brilliant job of taking away the manual QA testing sheets. We use webdriver.io and really you should be writing automated tests from the start. Our biggest gap at the minute is proper integration tests with our databases/providers.
The .NET Framework 4.7 is supported on the following Windows versions: * Windows 10 Creators Update (included in-box) * Windows 10 Anniversary Update * Windows 8.1 * Windows 7 SP1 The .NET Framework 4.7 is supported on the following Windows Server versions: * Windows Server 2016 * Windows Server 2012 R2 * Windows Server 2012 * Windows Server 2008 R2 SP1
It's less hard to get it right with a service bus lol...
&gt; It's an abstraction that encourages dispatch instead of eventing I don't understand this either. &gt; I personally think NServiceBus's extensibility is its biggest flaw: it's way too big and complex. I find this kinda hilarious. There's nothing complex about it as if it has become so simple, it crossed back over into complex. But I supposed it depends on the selected transport.
I'm experimenting with wampsharp atm and it seems ok. Websockets only and the documentation isn't great compared to Signalr. On the plus side it works with ASP.net core and there's plenty of JS (client) libraries that don't have any extra dependencies (aka no need to pull in Angular).
It really does depend. I have done it both ways, and my rule of thumb is how closely the data is coupled to the actual user. If it is tightly coupled, such as the user’s birthdate or SIN (SSN for you Yanks), not only is it in the User table but it is also encrypted using the new SQL 2016 methods. If it is loosely coupled, such as preferences or user-generated content, it is in a UserProfile table that has a 1:0…1 relationship with the User table (don't do an actual 1:1 relationship).
Outdated. https://github.com/fluentribbon/Fluent.Ribbon
I doubt it. You can install it from visual studio installer for 2017. 
Um .NET beyond 4.0 has never been supported on XP. People born when Windows XP came out are starting to prepare their college applications. Get over it.
If you're going to explain Func vs Expression, maybe showing why you would use one over the other would be a must have? In a nutshell (and I'm sure there are other cases) Func if you just want to execute the lambda and Expression if you want to generate/analyse/store/transmit the lambda. Expression can also easily be transformed into a Func. Personally I've mostly used Expressions for things like property selectors in my own DSLs and Func for delegates. I imagine Expression would be useful in some on-the-fly code generation scenarios specially with libraries like LINQ Dynamic Query. 
Haha, hard to believe it's been 16 years.
I stopped updating after .net 2.0 since that was the last version to support Windows 98.
I know right! Dave's an awesome guy, and he worked pretty closely with us (Cake team) migrating our website to Wyam :)
I don't even get why people liked XP it was a terrible OS. The only reason I ever upgraded from 2K Pro was games completely stopped working on it. Graphics would be see through, random colors, or just total black screens.
I used the base doc for this: https://docs.microsoft.com/en-us/aspnet/web-api/overview/formats-and-model-binding/json-and-xml-serialization You can remove the JSON serializer if they're not going to be making requests for it. Other doc will help you is top answer here: http://stackoverflow.com/questions/16713549/how-to-make-asp-net-web-api-to-only-return-xml Good luck.
Unless your objects are absolutely massive, then the weight of the patch operations surely destroys the scaling argument. I do like the test application work however, but again, why not just send up a whole object?
I'm more of a ASP.NET guy so I got a question: &gt;Don't use a BindingList when ObservableCollection will do. Why? Are you referring to the way `BindingList` fires an event if the items implement the `INotifyPropertyChanged` interface or is there something else to understand?
In other words, you can now reference .NET Core libraries in your .NET Framework projects, I think.
Do you have any detail about why it need dx11 for wpf ?
Is it webforms? If so, when creating button with that kind of loop, do you have viewstate? If you were using a repeater and databinding, the commandArgument might still not work (I can't recall for sure) but if you add an arbitrary attribute, it should be available on the server (from viewstate). &lt;asp:Button ID="btnDelete" runat="server" Text="Delete" OnCommand="btnDelete_Click" ItemId="&lt;%# item.ItemId %&gt;" /&gt;&lt;/td&gt; string itemId = btnDelete.Attributes["ItemId"] 
Yeah, it's webforms. Gave it a try there, it didn't work. itemId = null;
I am a little confused as to what functionality the package is meant to provide. Is it the attribute decoration of properties you are trying to remove? Also I think you accidentally linked to the wrong SDK, you linked to the Java SDK but your code is in .NET. 
Thanks @galloo2 :)
Targeting and developer packs are not tied to Visual Studio.
&gt; .NET Core 2.0 is released (or is that NET Standard 2.0?) It’s both this time! (But that’s happenstance) No it isn't. .NET Core 1.2 was renumbered to .NET Core 2.0 specifically to match version numbers with .NET Standard 2.0.
Good point, but for the most part I'll be primarily developing on the laptop. I do have 2 monitors at home that I can plug into so perhaps a docking station could be in my future.
I appreciate the warning and I will take it into serious consideration. But...as a general statement regarding the XPS 13 as a decent developer machine, what say you?
If they are going to version multiple packages in unison, what is the advantage of having multiple packages? 
Actually, the problem here is that you need to re-run `dotnet restore` after adding the project reference. If you add that step, then everything should work as expected. I've been tripped up by that a couple of times; I think it should be explained better somewhere. &gt; the lib is netcoreapp1.1 and the console app is netstandard1.4 by default To clarify, it's actually the other way around (app == netcoreapp1.1, library == netstandard1.4) (Disclaimer: I work at Microsoft on .NET Core)
Yep, you're totally right, I saw the SO post about it earlier. https://github.com/NuGet/Home/issues/5127 is the more problematic one for people really, I guess. I mean, I just tried `dotnet pack` -&gt; `dotnet add package LibFoo -s ../../LibFoo/bin/Debug` and I got the whole "Package 'LibFoo' is incompatible with 'all' frameworks in project". I get that a lot of work is going on here, but it's pretty disheartening to stumble from one issue straight into another one every time I try to pick the .NET core stuff up and actually do anything with it. 
Yeah ups :) Linked to the wrong SDK. But it works in the same way. The idea behind the package is just simplify working with DynamoDB.
Thanks for the suggestion! We ultimately decided to halt work on using .NET core and continued to use 4.5, as it's becoming apparent to us that it's not quite there yet. We'll probably look at picking it up again in a year or so!
&gt; So the same release of .NET Core 1.0 works with .NET Standard 1.0-1.6; how is that possible you ask? I have no idea. In fact, if I continue to look at this chart I may start drinking early.
I still have no idea what the hell MS is thinking with this nonsense. My company had me evaluate .NET Core 1.0 RC to see if our 10-year codebase would work in it... but it's basically a 100% rewrite with zero library support. My boss's reaction when I gave him my findings: If we have to rewrite all of our code and none of the expensive MS tools we've bought work anymore, then why the \*hell\* should we stick to .NET anymore? I didn't have a good answer. So our company shitcanned all new .NET dev work and took up NodeJS. Congrats.
&gt; VS is pretty good at knowing which projects need re-compiling Unless you do one of the many easy changes that cause your project to **always** rebuild, e.g. set a file as "copy always".
I also have the regular XPS13 (Skylake) with 16GB RAM and it's a great machine. In fact, it's the best laptop I've ever owned.
&gt; but you ignore the fact that both of these issues exist by switching languages. That's precisely the point. It was going to be the same amount of effort no matter what choice we had. The benefit of .NET completely disappeared, and thus it put NodeJS on the same footing as .NET. In fact Node had the advantage, because Node devs are cheaper than C# devs overall. Shit like .NET Core is why MS is losing more and more marketshare. The primary benefit to the ecosystem was stability, support, tooling, and documentation. Now we get none of that. &gt; You should really be a consultant these types of ideas are worth a lot of $$ I fought like hell to keep .NET. I was overruled because I couldn't come up with any logical reasons to stay. But thanks for being a judgemental smartass. 
Thanks a lot! You're message was short and simple, yet it was very helpful. The code in the second URL didn't worked at all, but it guided to a solution (After googling, I had to add "Configuration.Formatters.XmlFormatter.UseXmlSerializer = true;" in order to make it work) Thanks a lot! Seriously, thank you very much!
Just a follow-up: The part I was missing was *how cookies work*; That's the sort of knowledge I should really invest more time in. Because the cookies are domain-specific, the _RequestVerificationToken *shouldn't* be passable from a malicious site to another. Let me know if that's not correct, I guess. But, when not using Fiddler cooperatively, it becomes impossible for me to cross-site anything.
Every time I would like to use one, I just look at the code and rather write a struct just for that method. Tuples seem so cheap.
What are the advantages over MSbuild? At first, it just appears to be another layer of indirection?
Spam. And poorly written at that.
If i was a dev at a dot net shop and our manager said we're going to rewrite our entire code base in javascript, i'd quit.
It's not really the cookies that are the secret in this scheme, it's the token. Even though websites cannot read the cookies for other domains, a malicious website visited by a victim can cause the victim's browser to make arbitrary GET and POST requests to other websites (CSRF), and the browser will send all the relevant cookies along to the target website. But the malicious website cannot read the user-specific token from the target website and include it in the forged request, allowing your website to distinguish requests originating on your site from those originating on a malicious website. The anti-forgery token is used because cookies being domain-specific is not sufficient protection on its own.
I really like how Twilio are constantly doing clear and concise how to guides.
On the server side, the most basic is to put your key in ApplicationInsights.config in the element &lt;InstrumentationKey&gt;Your Key&lt;/InstrumentationKey&gt;. Alternatively, you can set it at app startup by setting TelemetryConfiguration.Active.InstrumentationKey On the client side (javascript), I am not aware of any way to hide the instrumentation key. At best, you can obfuscate the value.
All good, kinda new at this lol. Ill try putting the whole path in when i can, ( was messing around with it and now i have another problem). I'm gonna make a new project and transfer my code cause i was messing with the .Net versions and i already tried to put it back but it still wont work. Pretty much my problem is when I'm trying to login, It activates my try/catch and says my password and username is incorrect even thought I'm inputting the correct information. 
Sweet. There's your issue then. What happens when you hard code the username and password into your application, rather than get them from a front end input? Can you set a breakpoint at the start of Login_Click, and tell me what the values are for the inputs?
I've been in the .NET world for a bit more than a decade. I've used a lot of build systems through the years. I recommend against using CAKE. While C# is a great language, it's a terrible build definition language. There CAKE team has done a lot to help, but it's terse and verbose in the wrong places for a build system. The plugins are half baked and lackluster. The PowerShell plugin for instance, blocks and waits with no output until the script successfully completes. Exceptions written to STDERR will get almost no output unless things are configured _just right_. By default, CAKE will float versions and break your build. It's very important that a build system is consistent first and has glitter on top. I challenge someone considering CAKE to find where to peg the version in the documentation. In short, I tried CAKE on a dozen projects from FAKE. Our team has decided to go back to FAKE since it was more consistent, had better plugins, and safe defaults. A few people are reading the basics of F# but we're happier doing that than staying with CAKE. Maybe CAKE will be awesome in a year. Until then, I recommend FAKE, MSBUILD, or PSAKE in that order. CAKE is a step up from NANT and whatnot if you're using an extremely esoteric system.
Seems a little "non standard" from what I'm used to but definitely will tuck this knowledge away. thanks for sharing.
You'll find information on how to lock to a specific version of Cake on the Cake website under "Documentation / Tutorials / Pinning Cake Version" http://cakebuild.net/docs/tutorials/pinning-cake-version 
PowerShell addin is a community contributed addin, not using that specific addin myself (there's over 150 to chose from and most projects might not even need addins as there's about 300-400 features built-in). If you're having an issue with a community addin please raise an issue in its GitHub repo or the Cake contrib gitter chat ( https://gitter.im/cake-contrib/Lobby ). So any issues can be addressed, addin authors often created it with a specific need, might not have thought about your specific scenario - giving them feedback or sending PRs will help bake &amp; improve the addins too. 
If you can afford the price jump the XPS 15 is awesome and can power three monitors via the d3200 dock. 
Why do you guys have to migrate to .Net Core? Is MS going to discontinue .Net Framework?
Good to know about it, thanks
It isn't clear to me whether you are referring to the version of Cake itself, or whether you are referring to the version of the addin's that you are pulling into your build system. If the latter, then there is a known issue that is being addressed. You also have the option of use Paket for dependency management: https://github.com/larzw/Cake.Paket If the former, then the solution to this (in addition to the article that @devlead pointed out) is actually documented in very first tutorial on the Cake website: http://cakebuild.net/docs/tutorials/getting-started
Selling it with ReSharper is actually a neat trick. What is the general opinion of Rider? Is it superior to VS in any relevant way, for those that can use VS?
Yeah, thats the point, just do a single rewrite to .net core and you're fine. Seems more reasonable than teaching a team to do it in a whole different language.
But .NET Core is incomplete, has horrible tooling, and requires more of our code to be rewritten than going with MVC5, which prompted management to say "well if we have to rewrite the front end, lose support of all our 3rd party libraries, and rewrite our support libraries too, might as well go with a more stable platform" Get it? MVC5 -&gt; rewrite front end now, rewrite front end again later, rewrite support libs later, find 3rd party lib replacements later. MVC6 -&gt; rewrite literally everything now, alter platform to support what MVC6 is missing. NodeJS -&gt; rewrite literally everything now. 
Thank you.
Is there a reason you didn't consider going with ASP.Net Core + .Net Framework? I think you'd have been able to keep all your libs if you went that route?
This is cool. What about a readme with some info on each design pattern? 
I use it exclusively; the EAP is a bit crashy, but its pretty great. For example, the keyboard shortcuts... actually work! crazy, I know.
Sorry for the delay, my pc decided it was done living so I am waiting for parts to arrive. If you still want the other files I will upload them when its back up and running
I apologize. Last time I looked, the only mention was on the [getting started](http://cakebuild.net/docs/tutorials/getting-started) page. I am quite happy that's more visible now. I still stand by the version should be pinned by default. The pin method is pretty counter-intuitive (add file and ignore folder). It's very unsafe to have a floating build by default. 
FAKE is only F# so pretty much every editor that supports F# does fine with it out of the box. There's really no special tools for editing it necessary. I don't see how Frosting could be easier than that. 
I understand that's a community plugin. I also believe executing PowerShell is core functionality. I wasn't too happy with my experiences in the Gitter chat room for cake previously. Please feel free to bubble up my ideas here to the necessary parties. 
First, documenting pinning CAKE in Getting Started isn't enough. I'm glad there's a separate article for those who know to look for "pinning". I've never had a need for Paket. I'll stay away from its complexities by using NuGet. Thank you though. 
Well first off, :( Second off, I was getting into design patterns and head first is a good starting point, or so I heard. Third, Thank you for recommending the book, I was too intimidated by the gang of four book and that is why I took to reading Head First first.