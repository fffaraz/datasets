Indeed haha
I agree with your statement completely. Its semantics. I referred to view models since he mentioned he is using controllers implying its being send to some sort of front end :) 
I have an aversion to Robert Martin too. I find his work too much like preaching rather than explaining why doing things in certain ways is better than others. I'm curious what tick you off about him. 
You can use the CliMutable attribute on your records to make them mutable which would work with AutoMapper.
I’m planning on picking up clean architecture. Can anyone recommend?
Want clean code? Go functional, go F#!
Hi /u/Vijayankit - could you remove the auto-popup to subscribe as soon as I visit your site? It is too aggressive. 
Who at Martin designed those book spines? Each one is different!
Thanks for the feedback, let me look into the issue. It is suppose to come only once for a user.
I have returned anonymous types and it works but it is bad in the long run. You are better off with the way you are doing it now because you are sending in and returning exactly what you need, meaning it is as efficient as possible. You could use automapper, but I feel it makes it more complicated. Try creating a MapperMediator that you can IoC inject so you can store all your mappings in other classes, isolated. It will clean up your code a lot and make unit tests of the mappings easier. Also, you can use EF or Dapper, but you should project into the DTOs you are returning directly. It will reduce the SQL you need and you don't need a mapper.
Yeah, you're right, but if you just want to use the records as a C# consumer, CliMutable is the way to go.
I found this [.NET Core with Angular](https://www.udemy.com/aspnet-core-angular/) by the same instructor, Mosh Hamedani. Is this good to get me started? 
Seems about right for an api. Is it really bad though? Each one serves its purpose and there shouldn't be too much overlap. I've done more where each layer has its own models. Api, domain, data, etc. It makes for clean layers but you spend to much time mapping. I shared DTOs between layers this last project. Automapper helps. 
Two git repos that I've used for a template : https://github.com/emonney/QuickApp https://github.com/aspnet/templating (in src/SPATemplates) 
Also knowledge of MVC 5 should help in learning asp.net core 2.0 so it is not a total waste of time. 
Thanks for this. I will try it out 
My team started development of our current web product in MVC before Core was out. If we could start the project over, it would 100% be with Core and webapi + some MV* framework. It would save us a lot on cloud services just by being able to use Linux machines rather than Windows. The only reason we haven't switched to Core is because we now have this big product with a huge backlog and a lot of dependencies. If you're going to learn .net for web, I definitely recommend core over MVC.
97 things and clean code I'm sort of having a books to read on the pooper ha...there might be some really interesting parts so I don't want to skip them. But I'm liking clean coder and clean arch so far. Clean coder is a bit like a biography and there's lots of things to learn from his experiences.
Sure, two Pluralsight courses, a book and a youtube video are completely useless.
Hi /u/amirrorbehind I have updated the settings to make pop ups less aggressive. Hopefully that helps 
I feel like the Robert Martin books were good early in my career, but they’re just one guy’s advice and they shouldn’t be taken as gospel, which can be hard for young programmers to realize.
Clean code is one of my personal favorites. Honestly I found it more useful then code complete. Clean coder on the other hand not so much. 
Visual Studio version of Mac and Visual Studio on Windows Virtual Machine as well.
Possibly this: https://www.emailarchitect.net/eagetmail/kb/csharp.aspx?cat=2
Just my opinion.. I worked with Automapper before.. and I’m on a new project now for almost 2 years without it. Couldn’t be better. Mappers can get quite complicated and in the end it becomes harder to maintain. Instead, we just write out own mappers or just map in the controllers. So much easier. 
I love and recommend Clean Code to everyone with the explanation that it’s good advice in general, but one doesn’t have to follow it to the letter in order to benefit from it. That said, I agree that much of Clean Code is just one guy’s opinion, much like Joel On Software. I’ve never read Clean Coder or Clean Architecture; I would love to hear opinions on those. I have a 1st or 2nd ed copy of Code Complete hanging around here somewhere...
Would it be cheaper for me to run through an AWS free tier?
Very much so, we are in an industry where there are no real hard and fast rules just ways of doing things some people have found easier than others. 
For your scenario I gotta go with what the others said: [your best bet as a newbie is to stick with .net core](https://purple.pizza/where-to-start-in-dotnet/). You may have trouble finding version 2 specific tutorials but fortunately, many of the skills are transferable. The runtime is set up differently in core, but a lot of the MVC paradigm is quite similar (names of things, controllers, filters..).
I work at an agency so primarily we use MVC 5. I still learn MVC and Web API 2 because my work is currently on projects that use these technologies. How would it look if I turned round and said "I only use Core because MVC is dead according to people online"? Point being, the technologies are not dead. At all. No way. If you were starting a greenfield project then sure, look into core. Although I do not see many agencies in my area using Core as of yet. It's a slow shift. Please don't be that new developer who only works on 'current' tech. Here's a secret - you'll probably start off on projects using older frameworks that aren't as new and shiny. This is normal. Best of luck 
Do you already know the book "Clean C# Code"? It's more specific for dotnet Developers ;-)
Are there any other books on your reading list that I might be interested in and might not know?
In my experience the underlying framework is less important than having a firm understanding of the MVC architecture. You should continue with learning MVC using MVC 5 and once you have a solid grasp you can switch to .NET Core. A lot of the knowledge will transfer as you will still be dealing with the same MVC components in Core (Models, Views, and Controllers) and a lot of the implementation is the same. The main differences in Core are the cross-platform capabilities and the built in IOC/Middleware capabilities. If you’re willing to pay a subscription, Pluralsight has excellent .NET Core courses. Specifically, the ones by Shawn Wildermuth have helped me a lot.
To be fair, it's just .Net. It's all just .Net. There's a bunch of magic going on in the background that you mostly don't need to worry about. In practice, the biggest differences between the different frameworks are just the details of *how* something is done. We just made the switch to netcore2.0 recently (the day it was officially released) and have stood up 7 services with it. Microsoft essentially simplified everything. The biggest differences I've encountered are really just about how your pipeline is setup and how you configure it. I think this is why finding "good" learning resources is kinda hard; what you're actually learning is a different way to accomplish the same thing. So if you're choosing one to learn, I'd recommend learning aspnetcore. At least that way, you'll be poised to move forward. What you learn will absolutely be transferrable. The older frameworks are just... messier. There's a lot more minutiae to deal with. That's all stuff you'll learn by doing on the job. Other than that, all of the same concepts are there (startup, controllers, routes, code, razor etc.). Syntax has changed here and there, but it's nothing to worry about. As far as a technology being dead: if you mean "is it no longer the new hotness?" then yes, it's dead. If you mean "is no one using asp 5 anymore?" then no, it's no dead. In that sense, no technology is dead because every shop you encounter for new jobs is gonna be in some varying state of evolution. In fact, most shops you encounter will be at least 3 years behind the curve. In reality, they will be far more behind than that. You will almost definitely come across plenty of shops that are still on asp.net 2.0 web forms and WCF for services... or worse. At your level, just learn to code; learn to be a developer, and stay aware of what's out there so that you can pick it up at a more appropriate time. Don't focus on finding the "right path" or you'll just drive yourself mad and maybe possibly broke. The differences between frameworks is something you'll need to learn, but they are concerns that are a bit out of scope of your experience level, and will likely be taken care of by a more senior developer at whatever job you end up at. You will gradually learn learn that stuff on the job, in a real context, and it will be relevant for that shop. In the end, you'll be learning many frameworks -- that will never, every stop. TL;DR - There is no correct answer. So like I said, just pick up what's in front of you. The more you understand about any one thing in this field, the easier it'll be to understand everything else. 
You can use SPA templates to scaffold a .NET Core Angular 4 project: https://docs.microsoft.com/en-us/aspnet/core/spa/ The template gives you an example of an Angular service that retrieves data from the web api, and also an example of a component that doesn’t interact with the web api at all.
Not sure how much you can demand from the third party application but they could use one of these libraries for authenticating: https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-authentication-libraries
Personally I would stay away from it. It looks like Asp.Net webforms 2.0. Classic ASP - full control over HTML/output of server Webforms - all got taken away from you with microsofts own custom tags MVC - we got full control back - yay! Razor actually worked well. Razor Pages - some sort of hybrid. Invest your time in React/Angular etc. The Microsoft WebApi stuff is fine as its just server side code talking to a database.
True, but once a shop finds the things that work for them, those things get carved in stone, and become "gospel" and "the right way" and you'll be hard pressed to get any changes there until they're all finally unemployed because their skills finally became unmarketable.
It's about layers. You don't want to expose your data model through the API. That's a huge break in SRP, and will absolutely lead to spaghetti as the application grows. It can (and will) also lead to exposing too much about your persistence implementation which you definitely don't want to do. Lemme just define some terms, in the way that I might use them: - **DTO**: generic term for a bag of properties. Some people call these POCOs, which I've never agreed with. - **ViewModel**: A model that is intended to serve the purposes of the view. The view can be a web page, a windows/wpf form, or an API endpoint. - **Service Contract**: Basically another term for a ViewModel that is probably more appropriate for web APIs. - **Data Model**: Overloaded term that refer to a number of different contexts. In context of this discussion, it is the object that shapes the data for storage. This could be the model created for EF, in your case. - **Business/Domain object**: a rich object model that represents a piece of the business. It has behavior that enforces business rules. - **Entities**: typically a business object. They are an object that represents a singular business entity or resource created on the server. For example "Person" is an entity, and I might want to create one on the server using an HTTP PUT or something to that effect. Generally speaking, your data access layer should be shielded from the rest of the application. In fact, I don't even allow the underlying data model to leave the access layer boundary (usually a repository), whether it be EF, objects made for dapper, or otherwise. Your API should have it's own set of contracts that serve the consuming client. That includes data in and data out. They're called contracts because you're obligating the API to speaking a particular language to its consumers. They expect a certain format, and your API provides it. If you think in terms of resource entities, then you should have a decent amount of re-usability on that layer. For example "Person" is the same thing whether or not it's an input or an output. Any variation in the shape of that object implies a different semantic and should require more thought. Input validation is a presentation layer concern just as are the contracts. This means your contracts can contain that validation. ASP frameworks (including aspnetcore) have this supported as validation attributes. So just create those objects and decorate the properties with your validation. There's no need to separate those since they share the same concern. If it's a true CRUD model, then you could allow the repository interface to accept and deliver these contracts, so there would only be 1 transformation in either direction. I don't think that would be a crime within a true CRUD model. If it there is any business logic in between, then it's not a true CRUD model. You could call it that, but it would be misleading. Wrong, really. CRUD is data in/data out, not data in/processed/data out. That said, if you have business rules to run, then you should have another layer in between the presentation layer and the data layer. That means you will need another layer of mapping: presentation &lt;-&gt; domain &lt;-&gt; data. Presentation layer would deal with contracts, domain/business layer would deal with business objects, and data layer would deal with data models. No crossing of boundaries. No conflation of responsibilities. So yes, it seems like a lot of work, but you're preserving proper layering. The reason being is SRP: there should only be one reason for an object to change. For example, if you used the same model between the presentation and domain layers, then any change in either model would cause a change in the other. What if you added a new column to the database? Now your domain AND presentation layer would be broken. I've seen many people try to rationalize this in many different ways, but it always lead to an absolute mess. With proper models, you can focus on what is important within the layer of that model -- like validation in the presentation layer, or behavior and encapsulation in the business layer, or structure and normalization in the data layer. 
For the dynamic search form here is how i'm doing it in one of my projects. You have 2 - 3 drop downs side by side with the ability to add or remove rows of drop downs as needed. One dropdown contains the possible fields, the next contains the operation, and the third field is the value. On lines after line 1, you can provide a "join" operator drop down for "or" or "and" searches. https://imgur.com/a/ag705 This means you'll need to provide your UI with metadata about the data objects you're searching on. I've done it by providing possible fields and possible operations for each field. Different data types have different possible operations. Here is an example of metadata for one field. { "name": "Company Name", "allowedOperations": [ "contains", "equals", "notequals", "startswith", "endswith", "notcontains" ] } While the user is filling out the form, you are building up a "filters" array in your Javascript which looks something like this: [ { "field":"Company Name", "operation":"contains", "value":"Acme" }, { "joinOperator":"or", "field":"Company Name", "operation":"contains", "value":"XYZ Company" } ] As soon as they complete at least one row, you can send this object to your server (either as a post body or a query string param), consume it by your dynamic query builder in the backend (i can help with this too if needed), and return the results.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/vQYelvR.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20du1syi4) 
Thanks that makes a lot of sense! Only difference is that fields can be dates, text, numbers and booleans. But that’s probably just another metadata. It’s also only And and never Or in my case and backend is already done. I guess it shouldn’t be too complicated.. thing is just that I’m a complete newbie to vue with only 4 days of using it. I’m not yet very confident.. So far I’m liking it
&gt; "gospel" and "the right way" Ive seen that, ive ignored it my entire career and as a result I know more languages and patterns than most of those zealots you run across. Mainly because I don't camp on any particular rule set and always keep learning. They are employed as long as a fad is around and then have to retrain. I'm always employed and never have a training gap that is very large.
I’ve never used Vue but from what I read about it this should be easily achievable. Mine is in Angular and was pretty straightforward. Best of luck!
A bit outdated because currently is net core 2 and I think he run the course in net core 1.1, but yes, that course is very good to start. Highly recommended. 
Has anyone found anything interesting from the article? Their suggestion can be summarized as make your program load fast and if you can't make it server based by hosting the serverless program on a dedicated server.
Some good replies here and don't want to repeat anything. So I'll just say that I've recently got a new job and during my interviews I would probe about what the current projects are and the architecture used. If the company used web forms then they were in the process of migrating over to MVC. The rest all used MVC for any web based projects. Only a few actually used .net core. So my advice around web technologies is to drop web forms and focus on MVC and learn react or angular for a JavaScript Framework work. By the time you are out of university those skills will get you a good job and a great start for your career. Doing this work now will hugely pay off. Also I couldn't stress enough how important it is to write your code in using a TDD process and focus on producing clean code (watch some YouTube videos on this at least)
Not right now but if I come across any then I'll update here. To be honest ive known about clean code and Robert c Martin (uncle Bob) for years but only bought his books because of his recent one clean architecture which came out last year. The problem is that these books are always pretty expensive. I think another good one is "pragmatic programmer" which as about £50
That book is on my Amazon list but it's a bit too expensive for me at the moment. I will purchase it one day though!
I've been trying it for four months and I'm still not loving it. (the practice not the book)
Go without DTOs then. Use your one set of EF models for everything. Then one day you’ll realize that you’re passing info back that wasn’t needed. Oh well, now you have to do the work. Uh, wait. Now you’re changing your data contracts, and changing them breaks a client’s application. Have fun with that. 
There are better ways to evangelise than this :)
I would highly recommend it. It's not all about the specific Clean Architecture "practice", it also offers a lot of good advice on how to design dependencies and keep obvious boundaries.
Good luck! I wish I was a .net developer. Sigh
What aspect of refactoring is Visual Studio missing? See the sub pages of [this doc](https://docs.microsoft.com/en-us/visualstudio/ide/refactoring-in-visual-studio) for what's available. I'm keen to know whats missing - I sometimes feel like I live in a VS/MS bubble.
When you move a class it should allow you to fix the namespaces for one. It drives me nuts when the namespace is not correct. Most of the refactoring abilities are “enabled” through Resharper but I find it annoying I have to run it to be able to use decent refactoring tools. To be fair, I did come from a Java background and used Netbeans and it has all the refactoring tools build in. I had expected more from VS in coming back to it after8 years. 
It's just spam. I'm not sure why mods haven't banned this dude yet. 
There are a *lot* of incompatible changes between v1.1 and v2, so aside from the common stuff (MVC itself), I would not recommend looking at any v1.1 docs if you plan to work with v2.
Maybe I am wrong but I thought VS2017 supported this now? Or maybe I have a plugin that does it for me...
Your webjob is hosted alongside its parent web app, so you should see identical performance for the two. It's highly unlikely that they will be hosted on the same physical machine as your db, even though regions and resource groups match. *I don't remember the details, but I imagine MS would spec a db machine differently from a web app machine.*
Is your web app (if you have one) experiencing similarly poor performance? If not, take a look at how you access the db. Is there a difference between the two? Also, how do you define "slow"? My experience with Azure SQL db is that it performs very well. 
Dear god help us you will bE worse off for reading those clean garbage books. 
Azure Web job I think you can scale your database so it give you better performance. Also you can use hang fire it has nice visibility and you don't have to separately deploy your Web job to azure. Only need your Web app should run 24 hours
Hi, did you tried FullCalendar js ?
I loved clean code, I did not like clean architecture at all. it felt like preaching to the choir, it makes a case for good practices to the same kind of people who would buy a book on best practices
Yes hang fire will give you more control over your background process. 
It’s possible to spin the process back up after restarting the web app?
If you can use Windiws as hosting check out OData for ASP.NET Web Api with Entity framework. For simple CRUD Api you will not need to write much code at all, since Microsoft made their OData implementation plug in very well with Entity Framework. OData is like "SQL query for HTTP". Not sexy tech anymore but still used.
Are you using the free tier 20 MB database? If so, it's shared and doesn't exactly have much in the way of DTU. Even the basic 1GB database instance can be slow. Consider upgrading if you want significant speed. If cost is an issue, consider signing up for Dream Spark/Imagine. If you're not a student, consider Bizspark (all you need is an idea, a domain, and an email linked to that domain)
I've never had to set any of these for any of the two dozen .NET Standard libraries I've written. Are you sure you need to set these paths? They look identical to the defaults.
I’ll have to look into that. Thank you
Wow, thank you everyone for your advice. I'll stick to learning MVC5 for now. Get a few MVC5 projects under my resume and then maybe move on to .NET Core. Again, thank you everyone!
Ok cool. 
It sounds like you've probably jumped a little bit too quickly into the deep end, and you'll probably be better off getting you bearings straight before things get too overwhelming. **.Net Core vs Full framework** So .Net Core is definitely all the rage, which is why you'll find a lot of posts about. It is really interesting, but at the same time, you don't absolutely need it. Frankly if you're not running cross platform and/or trying to deploy to Docker, then you probably don't need .Net Core. There are other things that might come into play, but in this case, you might just want to get things working in the full framework. **Dependencies** Things have changed a lot since the wide adoption of NuGet and the broader use of build pipelines and continuous integration tools. While you still shouldn't take dependencies willy nilly, its much more acceptable now that it's much more easily managed. A fair amount of .Net components are now meant to be delivered in this way, and there are packages that the community has developed that are widely accepted as the ones to use for this or that. Basically it isn't all in the framework and controlled by MS anymore, which is a good thing. As for specifics, OWIN is the specification that handles the request management in ASP MVC projects now. JSON.Net is the defacto library for serializing to and from JSON. jQuery is likely used in the signalR case as a fallback for clients that don't support websockets and have to use long polling or something else. I believe there's a way to exclude jQuery if you don't want to support older clients and want to remove the dependency, but I can't remember any specifics about that. I think it's up to you whether you want to continue trying to get this project up and running on .Net Core or revert back to the full framework. If you're willing to deal with any gotchas to get up to speed, then sure use .Net Core. If you just want to update the chat app, then maybe just update it to the full framework and SignalR, then migrate to Core after you have it updated. I think SignalR is one of the newer components to add .Net Core support, so there may be some growing pains there too. 
You seem to be mixing up some terminology here. .NET Core isn't an alternative to ASP.NET, they're mutually compatible concepts. The n newest generation of ASP.NET is ASP.NET Core, and it runs on both the full .NET framework, as well as .NET Core.
Yes! Why reinvent the wheel.
What I wanted to do was to move all build artifacts in the folders bin/ and obj/ out of the working directory to the /tmp folder in Visual Studio for Mac. By adding all those paths I was able to build the solution without any problem, but then after upgrading to .Net Standard 2.0 adding those paths doesn't work anymore. Here is the guide I followed to upgrade the solution https://developer.xamarin.com/guides/xamarin-forms/under-the-hood/net-standard/
I would suggest stick with what you know as long as you are getting quality output.
This is an inaccurate description of ASP.NET Core. As stated in the official docs: "ASP.NET Core is a redesign of ASP.NET 4.x, with architectural changes that result in a leaner, more modular framework." You only need to look at the Startup.cs file of a Core project to see how different it is.
Er, does something I said contradict this? I didn't offer an opinion on how similar ASP.NET Core is to ASP.NET 4, I was focusing on the fact that ASP.NET Core and .NET Core are distinct things that don't have to go together. 
I was in a similar situation not long ago with a personal project. It was just at the release of Core 2.0 which I was anticipating, and it was my preferred option, but SignalR for Core had some catching up to do and just wasn't ready in time, so I reverted to the full framework which was actually a good move at that time because it brought some needed functionality lacking in .Net Core that really helped my project. I'm not quite sure I understand your dilemma though. From Npm to .Net everything is wrapped up into packages, especially for web-app related development. In my opinion it's just better to go with the flow on that one, and managing your dependencies will become second-nature in no time. On that note, you can specify the versions you use. Also, .Net Core is all the rage and where new development is, but I wouldn't think about it as if it's replacing the full framework. It's actually part of a bigger family of frameworks in line with Microsoft's plans extending into cross-platform development.
ThereKanBOnly1 provided a good summary. I would just like to add that you don't need to use SignalR to use websockets. If you don't care about falling back to long polling, they're pretty easy to use. Here's an example of a websockets chat application in .NET Core. I'm not saying I did everything correctly, but it works and might be helpful for getting started. Startup.cs is where websockets are added to the configuration for the app. Right after that is the check for if it's a websockets request (vs standard HTTP request) and handles it accordingly. https://github.com/Jay-Rad/Lettuce_Chat 
I recommend you steer clear of the default aspnet codegenerator. I ended up making a custom build tool to reflect upon my model and template out a DAL and client side dependencies. The default aspnet code generator is slow, Framework bound, and not extendable without framework dependency in your build process. I used string.replace and a template token dictionary. It works like a charm. I'm running a rest api and I'm about to start building out a front-end using vue.js.
Fair enough, I've edited my comment to hopefully make that part clearer.
Thanks for the detailed reply. I will steer clear of the asp.net code gen. I understand 85% of what you explained. I have a couple of follow up questions. 1) what exactly does your custom build tool do? Is this your code generator? 2) what are you doing string.replace &amp; the dictionary for? 3) should I use WebAPI for my rest api?
The template: public class EntityController{ [HttpGet] public ICollection&lt;Entity&gt; ListEntities(){ // Impl... } [HttpGet] public Entity GetEntity(int id){ // Impl... } [HttpPut] public Entity PutEntity(int id, [FromBody] Entity entity){ // Impl... } [HttpPost] public Entity Post([FromBody] Entity entity){ // Impl... } [HttpPut] public Entity Delete([FromBody] Entity entity){ // Impl... } [HttpOptions] public ICollection&lt;INavigationInfo&gt; Options(){ // Impl } } The replacer: /// Implement this interface... public interface CodeGenerator{ string Template {get;set;} Dictionary&lt;string, string&gt; Values {get;set;} void Generate(); Func&lt;string&gt; ReadTemplate {get;set;} Action&lt;string&gt; WriteOutput {get;set;} } The dictionary is generated by using Entity Framework Pluralization Services. Generate a collection of them, one for each entity type. See https://msdn.microsoft.com/en-us/library/system.data.entity.design.pluralizationservices.pluralizationservice(v=vs.110).aspx. I.E. For entity Customer, you need a dictionary like this: [ {key:'Entity', value:'Customer'}, {key:'Entities', value:'Customers'} ]. As you build out the framework for codegen, you will end up with a datastructure like this: [ { type:'Customer', template:'Entity.cstemplate', values: [ {key:'Entity', value:'Customer'}, {key:'Entities', value:'Customers'} ] }, { type:'Product', template:'Entity.cstemplate', values: [ {key:'Entity', value:'Product'}, {key:'Entities', value:'Products'} ] } ] This approach will probably because irrelevant once Web API supports adding generic controller instances at runtime.
I’m guessing this is you - with more information to help someone understand what the issue is? https://stackoverflow.com/questions/48664217/empty-image-from-blob-storage Really needs more information to help!
Yeah, I'm new to this, and yeah I thought the same, changing everything probably wasn't a good idea, thank you for your advise, changing the looks without touching the code it can be done easy and fast
From a different perspective from some of the other comments, I would like to mention ASP.NET Core has been designed to take advantage of the latest best practices. The startup with configuring the middleware and specifying the services for dependency injection is great. The configuration is also great. If you’re starting from near-zero and are looking to get up to speed for future work, starting with ASP.NET Core (on either the full framework or .NET Core) is the way to go in my book. The other comments covered your other questions pretty well it seems.
I would have a close look at your code. Things like pulling back byte arrays or accidently pulling back entire tables will really hurt your performance while you wont notice it in local development and often wont notice it initially when going to release until you start getting some big data.
Sorry, SQL &gt;&gt; Cosmos/DocDb for anything remotely serious than hello world projects. 
I've solved the problem with streamdecksharp 0.1.9. You can check the source if you want to know how I did it. In fact it was a combination of two problems. If you update the images to often (more than 15 FPS per key) they start to toggle on the last few frames. I limited that to about 13,5 FPS. The second problem was a kind of memory corruption. I'm still not exactly sure whats going on with that second problem. Here more details: https://github.com/OpenStreamDeck/StreamDeckSharp/wiki/Maximum-update-rate Hope that helps with your project ;-)
I have been using DocDb and the performance has been really good. It is not orientated for relational data though so I think they both have different applications. 
Released v5 with improved API. More useful extensions (and less unnecessary redundancy).
When running inside the Docker ckntainer, "localhost" refers to the container itself. If you need to connect to an application running on your machine outside that container, you need to use Bridged mode when starting your client container. There's some example here: [https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach](https://stackoverflow.com/questions/24319662/from-inside-of-a-docker-container-how-do-i-connect-to-the-localhost-of-the-mach) 
Cognito is used to manage and authenticate users, that's it. You can use it in conjunction with API gateway and Lambda to achieve your goals, provided you replace step 3 in your workflow with a POST to API gateway. 
In that case you can use the AWS .NET SDK to upload to S3 directly from your app: https://docs.aws.amazon.com/sdkfornet/v3/apidocs/items/S3/TS3Client.html Alternatively you can use the S3 API: https://docs.aws.amazon.com/AmazonS3/latest/API/RESTObjectPUT.html You can generate tokens on the fly but they won't have any meaning to AWS; you'll have to have something in the middle that uses an authentication scheme that AWS understands.
Lambda would be perfect for this, I'd highly recommend giving it a try.
IMHO, most real world data is related to other data in some way or the other. Too often I’ve seen data that’s actually relational being dumped into nosql datastores ... only to be later carefully normalized into SQL stores when business needs are more solidified. I guess it’s a good temp capture db 
both VS and commandline git will use the same setting.
I looked at .NET Core a couple times in the last year or so. My conclusion is if you are not doing cross platform projects stick with full framework. It just works with no “hacks” per se. The ASP.NET side wasn’t the issue for me it was all my backend projects (dlls) for the solutions I have created over the years, they use full framework libraries and it wasn’t simple to migrate (especially EF 6 shit I’ve done) Like I said, am not doing cross platform and all my solutions use the full windows desktops or browsers hitting IIS web server, so it was an easy decision to stick with full framework. I don’t even write UW apps, I stick with WPF, the shit just works and it isn’t going away anytime soon. 
That depends a lot on which command line git you're thinking of: Cygwin, Git For Windows, WSL, ....
Source code, can be found here: https://github.com/Boriszn/DeviceManager.Api.
something like this maybe? You can of course create any attributes you need and just use them in your op filter as needed. Usually the default authorize or allow anonymous attributes are enough though. var controllerPolicies = context.ApiDescription.ControllerAttributes() .OfType&lt;AuthorizeAttribute&gt;() .Select(attr =&gt; attr.Policy); var actionPolicies = context.ApiDescription.ActionAttributes() .OfType&lt;AuthorizeAttribute&gt;() .Select(attr =&gt; attr.Policy); var policies = controllerPolicies.Union(actionPolicies).Distinct(); var requiredClaimTypes = policies .Where(z =&gt; !string.IsNullOrWhiteSpace(z)) .Select(x =&gt; this.authorizationOptions.Value.GetPolicy(x)) .SelectMany(x =&gt; x.Requirements) .OfType&lt;ClaimsAuthorizationRequirement&gt;() .Select(x =&gt; x.ClaimType); if (requiredClaimTypes.Any() || customPolicies.Any()) operation.Security = ....
Hangfire doesn't work well on cloud environments from my experience. Azure web apps tend to be suspended after a period inactivity, suspending hangfire with it. 
after thought with my architect, i think unit of work is not requried with entity framework, its already built in, adding it on top just adds unrequried complexity
**That's exactly what I was looking for!** However, like you said, it's "only if I need no user context". I would like to restrict the service that I'll expose to avoid mere users to do things on it. After looking at that example, the *UseOpenIdConnectAuthentication* I'm using right now is not the one used in the example. It's another authentication system, *UseWindowsAzureActiveDirectoryBearerAuthentication*, but I can use the already existing Azure App of my web application. By putting them side by side in my ConfigureAuth, it seems both worlds can live and exist: going on the website asks for the user to authenticate with AAD, and the third party app can reaches [Authorize] actions with the Bearer token. However, the Bearer auth doesn't work with my custom [ExtendedAuthorize] since I made the assumption I have a real full user account coming from AAD. Thus I have some code that doesn't work with the Bearer authentication (a redirection to the End-User License Agreement page). I believe I'll have to make some changes to make everything works but since, at least, I can reaches protected actions of the website from the third party app, the biggest part is done. Thanks a lot for your help!
Do you manage your own user accounts? If so I believe you should use https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html
Any new projects should be in .Net Core. Maintain existing projects in their current framework. Migrating old projects without a reason will be more work than its worth. Eventually you will need to use core. Just go all in when you can start using it. It'll be worth it.
As usual the answer is 'it depends'. MVC is designed to build large scalable web apps but it's often overkill, especially for really simple sites. I've seen people use MVC where they could have used a static generator like Jekyll instead and that's a scenario that's ideal for Razor Pages. It might not have as much support as MVC but you'll also find you'll have nowhere near the number of issues you will with MVC.
If I am understanding this right, the bridge is created by default. So I should just use the ip:port instead of localhost:port? I tried that and received the same cannot connect to server.
If you want scalable, I'd use MVC. You can still use Razor as your views if you want but it's also easy to use another front end framework (Angular, React, etc) if you wanted using MVC.
I'd say MVC is the best choice for many reasons. While I've never actually used Razor Pages, I have used Web Forms and it seems similar. I really question who razor pages was designed for. I can see it being useful to knock out a quick form, but MVC is going to do everything it can plus some. MVC is also going to help you when you want to build REST APIs. It's very common and very flexible. 
**WebForms** are old and with a lot of baggage. The only reason to do anything with this tech is maintaining an existing app. Both **ASP.NET Core MVC** and **Razor Pages** are great and they share a lot of components (pages are basically built using core of mvc) - I don't believe there is significant performance difference between the two - Pages are younger and not yet polished and have smaller community around them - I dare to say that pages scale better with proper architecture - REST api is better done with MVC (but I believe that nothing is stopping you from using api controller for rest and pages for ui) In my opinion you can't go wrong either way.
Thanks. I haven't studied the ASP.NET Web API framework to any serious degree yet so I am in the dark with this aspect. I have used some third-party technologies for dealing with REST (RestSharp) but never used it with MVC. It does make sense though because using specific actions for REST requests seems to logically flow better.
What's your opinion of using Razor Pages just for the View and MVC for everything else? I like that Razor Pages only have 2 main functions (OnGetAsync + OnPostAsync) rather than switching between many Actions just for 1 request through RedirectToAction. It seems that most of the code using Razor Pages is more coherent by not breaking relating code all over the place. Still, my experience is limited and I could be horribly wrong!
There are a couple ways to look at this. 1. If a user works for one org at a time, then you can just have them "switch accounts" which really just resets their login information to the new org and their new roles. This way you only ever care about one org at a time. A lot of apps ask you which org you are logging in for in a multi-tenant system, and sometimes there are legal reasons to require that. 2. If they want to browse around to all the orgs, like a call center rep, it gets complicated quick. Most examples have a custom policy check built into code to see if you have access to a row coming back from the DB, never do that. Once you hit more than a few thousand rows, you need to move some part of that check to the DB. Like a claim that they are a simple "user", who can see those resources but can't edit, etc. Also imagine trying to set up paging and you keep hitting the DB for that extra row just to show 10 rows, not good. In this case, it may be best to use routes carefully, so it is /Org/1/{your controller's route goes here}. Then you can custom policy the first part of the route as a quick and easy check to see if they have any specific roles. On your "OnValidatePrincipal" event, which is loaded after if you use cookies (assuming since its a web app), you can re-organize the claims using the route and just keep things pretty standard. Then your magic code to handle things is contained in one spot. Side note, there is a "Properties" that hangs off of each Claim that can survive serialization. You can add in which accounts that claim belongs too, but that could bloat your cookie so you may want to load that elsewhere after the OnvalidatePrincipal event. 
I roll my own custom user authentication, I'm not actually using AWS for anything like that.
My body is ready. Thanks for sharing. 
I normally due this using Unity and interceptors there, does this method generate IL during compile or does it use dynamic and reflection to intercept calls? I tend to prefer the latter even though it is slower.
Its not uncommon to have both kinds of apps or both types served from one app. The MVC side provides the user UI page strcuture and its where you define your templates, CSS and JScript for the UI. The WebAPI components are your backend and are called by the JScript you put in the MVC app. IIRC there are templates in VS that do most of the lifting for you for some basic examples.
It is something in between. It generates IL at runtime for proxy classes which is cached so it's not compiled every time. For dispatch it uses a custom lookup (think Dictionary&lt;Type, Delegate&gt; - a bit more complicated than that, of course, but that's the basic idea). Definitely a much lower runtime overhead than vanilla reflection, but somewhat comparable to using dynamic. Precompiled code is much faster, though it doesn't really matter for most IO bound applications. Source: we use DynamicProxy in production services serving hundreds of millions of users.
It generates a proxy class dynamically. The proxy class is the one that gets executed first, and then forwards the call to your actual implementation. 
So no dusting of IL code in your classes then? If that's the case you could reasonable put the emitted IL in its own assembly. I like!
I am ok with side loading magic powder, I just hate when it messes with my testability or code signing. Thanks! Really the entire AOP/Mixin concept is rather limited functionally, I would pity the soul that actually tried to do more than basic over-arching features.
YAY
You sound seasoned if you coded since 1.0. However you have specialized in a technology that couldn't be more different than what web development have evolved towards the last 10 years. The days of drag n drop are long overdue. Unfortunately you will have to beat a somewhat steep learning curve. On the bright sight you are an expert in C#! If I were you I'd check out some of the videos discussing webforms vs mvc on pluralsight. Also get your basics with html and css up to speed.
Not sure about Web Forms these days, but using MVC gives you the opportunity for model binding and makes writing web services much easier. Model binding alone is a good reason to use MVC.
Razor is very cool. I use it as a templating engine (using RazorEngine) in my systems.
JavaScript
PWA
Yes that's right: &gt;*you can register and authenticate users via your own existing authentication process*, while still using Amazon Cognito to synchronize user data and *access AWS resources* You use this in your back-end to create temporary AWS credentials that you then return to be used by your front-end, perhaps during your sign-in.
Your mom has always cleaned spiderwebs
Well, this much I do know. I am kind of stuck in asp.net from 3-4 years ago. With javascript, ajax, mvc and stuff like that.
You can also use temporary AWS credentials in the same way. https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html
I mean that as getting good with JavaScript. Whatever stack you choose, this is always valuable. It doesn't matter if you're in .Net, Java, Ruby, Go, PHP, or anything else. Your JavaScript skills will be useful anywhere. If you understand the language, the runtime, and the various caveats within the language then you should be able to easily pick up any of the major frameworks... and you should be at be at least very familiar with one of the top 3 React/Vue/Angular4+. Being extremely proficient in ES5/ES6/ES.Next is a skill that's going to be around for the foreseeable even with WebASM coming on the scene. Client heavy apps aren't going anywhere. 
Excellent.
What is SignalR used for? I mean what sort of apps is it ideal for?
We're using for near realtime data. Think IOT with oil wells.
We use it at work for background processing and notifications (e.g. toast messages that say "Analysis complete"). It's also useful for any kind of in browser chat and multiplayer browser games.
On the news tomorrow, "Exxon oil rig exploded due to IOT security flaw that allowed hackers to mine bitcoin for months!" 
About damn time!
Thanks for that! Anyone here have an idea on how to pass an oAuth token for a connection with the new SignalR? The documentation seems to be lacking and we haven't found a good way to do this with the new SignalR.
Visual Studio is a great development environment. But with a few extensions, it will be even better. There's a lot of free extensions out there, that will improve the experience of writing code and even help you reduce bugs too. We have collected a list of the best free extensions for Visual Studio.
Thanks for sharing list. It's helpful 
I would add AxoCover, gives you free code coverage with OpenCover. https://marketplace.visualstudio.com/items?itemName=axodox1.AxoCover
Nice list! However, the File Nesting extension shouldn’t be needed anymore. It is coming [as a core feature of VS 15.6](https://blogs.msdn.microsoft.com/webdev/2018/02/07/file-nesting-in-solution-explorer/)
Does the site really need to open two separate popups (including a fake chat) before it lets me read the content?
It's on the first visit only. But you may have a point. Maybe one on the first visit and the other if a visitor keeps visiting the site? Or are you against popups in general?
After experiencing problems with this myself throughout the years, I prefer the Swift way where you opt-in to lazy execution and it's otherwise eager.
Longer description is a good idea. Which typos is that?
dotnet core is a development platform designed to run on all platforms (Windows, Linux, iOS). It does not have a GUI other than a console output. You can imagine how difficult it would be for dotnet core to display GUIs on different platforms so instead it just leaves the hard work of displaying GUIs to other frameworks such as [Xamarin](https://www.xamarin.com/), which is compatible with dotnet core. If your heart it set on the **.NET ecosystem** then you have a few options as far as GUIs go: - **Desktop** - .NET Core, WPF or UWP. For cross-platform apps, you could use Xamarin. - **Web** - ASP.NET or ASP.NET Core. Ultimately it really depends on what application you are trying to create. If you just want to display a GUI on the desktop then dive into WPF or UWP, if you want to create a web application then ASP.NET Core all the way. My advice would be to follow the current trend and get familar with **web applications**. Desktop is no where near dead but just look at any jobsite and you'll see that web development is in high demand. Therefore I suggest you take a look at **ASP.NET Core**. Microsoft have some great tutorials on [Microsoft Virtual Academy](https://mva.microsoft.com/) and [Channel9](https://channel9.msdn.com/).
i wish there was a comparison of Rosylinator and Resharper. Resharper kill my performance on large sln/proj/files.
I think that Roslynator has a lot of the quick fixes from ReSharper. There's a lot of other stuff bundled with ReSharper, though. I haven't found a better extension than ReSharper, but it comes with a price ($$$).
I'm working on a multiplayer rpg that runs in the browser, uses signalr for tracking player position, etc. just switched to .net core and have started messing around with the new signalR, quite a bit has changed and I haven't yet been able to refactor it completely.
PWA's offer offline-mode by default, and there's the existence of a web worker service (think that's what it's called iirc). There's a tool from google called 'Lighthouse' that will check PWA's boxes off for you as you go. Javascript starter kits such as create-react-app are PWA's by default :)
Javascript isn't as bad as people make it out to be. If the past 2-3 years haven't proven that it's huge and being improved upon, then maybe they shouldn't be web developers lol
Which frameworks do you think will be dominant in creating PWA's?
Any untyped language and runtime is wrong since C++/C#/Java, sorry.
It doesn't help the JS runtime to do all these nasty type inferrence roundtrips just because the language itself is subpar. It would be acceptable in GW Basic era but not 30 years ago.
You're parsing xml as string? Please don't. You should probably have a look at the System.Xml.XmlDocument for storage and read how to make a XmlSchema which is in the System.Xml.Schema namespace. There's the xsd.exe from microsoft that will automatically create schemas out xml examples you give it. Your templates should be schemas and input from the user should call XmlDocument.Validate When it comes to WPF that's a whole 'nother bag of goodies. It provides input validation if your class implements INotifyDataErrorInfo. You could wrap the XmlDocument in your ViewModel datacontext which then does input validation against the schema.
No no no, I am not. Sorry, that's just copied out of a test line. I'm using XmlDocument to parse. The line you're seeing is from a test for creating the new XML file from the template (the attribute names in the XML are shorted to 1 letter vs the full attribute name in the template). I can't just use the XSD because the template XML has a bunch of a options that won't all be used in the actual file. Does that make sense? Here's a bigger sample of the template, and an example of said template in use. Template: https://pastebin.com/W64qbEeT An example of template in use: https://pastebin.com/FNSPAATB 
A normal page in MVC will have a [Get] Action for pulling a page and a [Post] action for sending data to the controller. Seems the same. I guess someone might do the redirect to make rebuilding the view after a post easier. Not sure where you are getting the idea that code in Razor Pages is more coherent. Nothing about MVC is going to be breaking code all over the page. If anything I would say Razor Pages is less coherent because it lets you put more code in the 'controller' level. MVC is better for clean &amp; organized code. Razor pages is just helper code on top of MVC and it kind of breaks the MVC pattern.
That's a good tip. Do you know if it still works for VS2015 and VS2017?
I see. May I suggest a reference to the System.Configuration API?
Thanks, I'll look into that
Or to make it even simpler: your use case sounds like it should be implemented as types that know how to serialize themselves, either using System.Xml.Serialization.IXmlSerializable or attributes in System.Xml.Serialization. Then the user could simply tweak whatever databound properties you have, be it number values, enums in a drop-down or checkboxes for booleans and once save is clicked the XmlSerializer takes care of the rest.
I totally missed the webhooks.. so that's really awesome! I will look into those docs and see if I can utilize it for a couple of other areas where I will need to run scheduled jobs.. thank you for the information!
It's all about organization. Areas certainly can help with that. Otherwise, you just need to split up your controllers which is really easy. Razor Pages cannot claim to solve this problem, its up to the developer to keep things clean. You can easily fall into the same problem with any platform. Controllers in MVC can easily be setup as a single responsibility. It gets even easier in core. I do not like Razor Pages for the same reason that I do not like WebForms, because its basically creating a "fat" controller. Where most of your business logic is in the controller level. This is bad practice. If you keep your controllers "thin" and your business logic in its own layer, then you'll never run into any of these code issues regardless of what framework you are using. 'Thin' controllers means nicely refactored, clean, shareable code. Razor Pages is trying to solve a problem for lazy developers that should not be a problem. Once you figure out MVC correctly then you'll realize how useless Razor Pages are.
Not a specific answer to your question, but I’d look into something more along the lines of Selenium and Protractor. Rather than sniffing and re-creating HTTP requests you automate the navigating and clicking of buttons. Potentially slower (assuming you might have been able to skip a couple of page loads by grabbing the final result directly after reverse engineering the requests), but much easier to automate and maintain. 
Is there any good tool for code extraction / refactoring?
You should check out PhantomJS. It is a headless web browser (a browser with no gui), and it was made specifically for tasks like this. Normally you would automate it using JavaScript, but there are libraries that let you use .net as well.
Thanks for putting this into context. I've been a programmer since 1989 and when I look at all of this I feel like I did on the first day of school. It's easy to learn a new language, but this is a completely different design model. And I have developed a lot of bad lazy habits with web forms. I came to web development from desktop development right when .NET 1.0 first shipped. And yes, I didn't have to really understand HTML. And to be totally honest, coming to the web from desktop development, I really looked down on HTML because I didn't consider a document markup language to be a real programming language. So... I'm really where I should have been if I had never started out with web forms. This is excellent advice, and again, very helpful context. Thank you. Now, one last question: I got used to working with 3rd party web form controls, mainly Telerik - they add a lot of bloat, but for knowledge worker type of LOB applications that run internally on a corporate network, who cares, right? Anyway, I created a MVC project in VS and downloaded the Telerik Kendo/MVC UI controls. They, like MVC, are a completely different animal. Do you have an opinion on these? Any recommendations?
On mobile, so haven’t checked it yet. I was under the impression it only did simple stuff like “string” to “var” etc, versus more complex ones like code duplication detection, code extraction (breaking long method into smaller methods).
Ahh yes this looks ideal. Thank you.
Thanks. I think Selenium and PhantomJS together will work well. Either way, I've got enough to keep moving forward with my project.
Oh. I may be in the wrong actually! My bad. &gt; The feature is currently **only supported by ASP.NET Core projects**, but tell us that you want it for other projects as well and we will try to make it happen. 
here's the ones I use a lot: Add New File: https://marketplace.visualstudio.com/items?itemName=MadsKristensen.AddNewFile VSColorOutput: http://mike-ward.net/vscoloroutput/ ErrorCatcher II: https://marketplace.visualstudio.com/items?itemName=MadsKristensen.ErrorCatcherII Glyphfriend: https://marketplace.visualstudio.com/items?itemName=RionWilliams.Glyphfriend 
I see. Better keep it installed then :) Thanks for the update.
I use Rider daily as my only IDE. It has everything I need for the majority of my work. .NET R# Test Runner Nuget Javascript Database tools And best of all? It's not a dog like some other IDEs I know. 
I've went as far to not renew the license for my team. They often disable the extension because it's slowing VS to a crawl with a large solution. When they fix the performance, I'll renew.
You can also use Mono amd GTK# for cross platform GUI desktop development.
VS for me. I don't use resharper, tried it a few years ago, its had repeated issues that cause it to slow down VS. If you are using VS with resharper and find it slow, try Rider.
Agree in most points. I always use extension method syntax when writing linq, why I never really used that feature in R#. R#'s simplyfying of LINQ (Where(...).Any() to Any(...)) is missed though. VS2017 have cleanup in usings, but no reference cleanup. I did experience problems with R# though. It told me multiple times that references aren't used, but in fact they are. So didn't use that feature much. I miss the unit test runner in R#. I'm using Test Explorer now, but it isn't as good. The new CTRL+T in VS2017 is a nice addition, but I still miss the possibility to search in javascript and other files too.
Oh nice!! This is cool... I'll have to read into this more to get a firmer grasp on it.
I want a command palette like vs code
Isn't that what the quick launch input box does?
Let's say I want this analyzer: https://github.com/JosefPihrt/Roslynator/blob/master/source/Analyzers/DiagnosticAnalyzers/BitwiseOperatorDiagnosticAnalyzer.cs. I can install Roslynator and disable all analyzers but this. But would rather include this analyzer (together with others from different projects) in my own package. Don't see how this would be possible without copying the code.
You should give credit to the aspnet team for the StockTicker example code you plagiarized [from the official samples repo](https://github.com/aspnet/signalr-samples) 
Actually, I think installing the NuGet package and then disabling all but the ones you want is *exactly* what I would do in that case. They have a NuGet package. It would just be a dev dependency to my personal ruleset NuGet package which would only enable the rules I want. The nice thing about these Analyzer packages is that they are DEV dependencies, not regular app references. They don't get copied to output. I'm not saying adding a 5 Gb list of dev dependency isn't going to affect build times (what are we, node.js developers?)... but I don't see how it hurts anything in this case.
I do much the same with StyleCop in my RuleSet package. In that case the .ruleset file just drives everything (well, and stylecop.json or whatever their config file is, but thats special to StyleCop) and I just disable the Warning action on the analyzers I don't want to raise warnings. If you ever find yourself looking to pursue this, take a look at that repo, its not super complicated. Building the NuGet package the correct way is harder than the ruleset stuff, especially with Core project structures. Its still a little clunky to edit and modify the .ruleset file in the repo though.
GitDiffMargin, VsColorConsole, Open Command Love, Add New File are also must have
GitDiffMargin looks awesome. Do you know if it requires the git source control plugin to be enabled in VS?
Pretty sure it doesn't require it. I turn off that plugin for perf reasons and never had any issues
Slow Cheetah is unnecessary now, VS natively supports config transformations.
I have sent you an email.
I'd like to set up a raspberry pi with windows 10 IoT but basic hardware isn't supported. Stuff like tft screens, camera, etc. Can't get to azure before that's available.
Currently on paternal leave and have been for a while so i dont have any fresh situation in my mind. I'll contact you once I have been back in biz for a while and have some fresh stuff for you :) 
I remember getting really frustrated with Azure tools. Things that install through the web platform installer would commonly install multiple versions, but then not all of them would uninstall correctly. Especially if you had multiple version of VS installed. And, of course, getting the specific installer to try and remove things by reinstalling then removing is its own can of worms. So half the time I just end up not installing or disabling azure tools unless I'm actually going to need them for a project. Some of the SQL data tools had similar problem of insufficient QC. So my request on that note would be clear documentation on the components and their versions, and installation as well as removal instructions. I'd like to know why I need components instead of just installing them all. And I'd like the option to install them all through one place instead of dealing with 3-4 MS installation platforms. Definitely not asking for a new installation platform that'll never die. Just links to components, **all past versions**, ops details and actual docs on what's needed as well as why. I'd give you more feedback on what to build, but I haven't looked too deeply at the Azure docs. I work with enterprise, and mostly on-premise. But I'd love some examples about how to build authenticated SaaS worker swarms. Like I could have an on-premise install that could farm out some work to the cloud, where I'd take an authed request, spin up a worker, handle it, and close it down if there aren't open requests. You know, how to reduce costs by auto-managing the running services so I'm only running stuff when I've got actual requests. OCR would be be a good example subject. Most clients I work with want a desktop app and not an SPA, but they'd be okay with a cloud processing piece. Sample projects that cover not only code, but **maintained** instructions on account setup and deployment strategies could help encourage people to make the jump. Part of the problem is that all the microsoft doc sites and tutorial pages are scattered, managed by different teams, and aren't linked or don't cooperate. So I'm not even certain what's up there, and if it has what I want, because I avoid looking until I absolutely have to. While you're at it, can you pass along to the request to the other docs teams to either make better wrappers for Win32 API calls, or provide better docs so it's easier to p/invoke? Pinvoke.net usually only has 80% of the story except for the most common, basic usage, and digging around through struct and type info is frustrating. A lot of Win32 docs are very misleading. Have them put actual header info and definitions on the site. I should have to look online for info, then grab a copy of a windows SDK and look for a header to see what the constants are. And if you've got any pull with the windows platform team, maybe look into a pluggable update service (not windows updates, pls) where we can install configurations for non-administrative updating (both all users and current user). Not looking for centralized hosting. It would just read update locations and provide a simple way to check updates on a provided HTTP site, close the open process or check when it's closed, then update. That way we can manage external software updates in one place, and disable/enable them or select update channels. And then I won't have 20 software update services (or, worse, tray apps and hidden processes) running simultaneously on my computer. Chrome, firefox, java, adobe, dropbox, skype, and a million other software applications all install updaters with differing levels of CPU usage and user hostility. It'd be nice to conquer that problem from the OS side. And if you pushed all the way back to Win 7, you could get a lot more buyin, because they wouldn't need to maintain multiple installers, then.
The only problem I've had right now is projects created in VS are not fully working in Rider. There are some quirks that need to be worked out. I have submitted tickets with my main issue and it's currently being looked at. I really like the feel of it though. Not nearly as bloated as VS.
&gt; Xamarin, which is compatible with dotnet core It is not.
We've been using Rider for a few months at work. For the most part, it's good as or better than VS with ReSharper. There are some features missing in Rider, such as extract class, but I'm happy with Rider overall.
Is it for Azure docs only?
Can you paste the full error message? In the last pic, is the Value column at the bottom.
I think one of your biggest challenges is keeping up with the changes. I've run into so many cases of out of date documentation especially concerning core libraries. It makes the entire experience extremely frustrating. 
Maybe he meant to say .NET Standard (gotta love the naming confusion) which *is* compatible with Xamarin.
Hey, thanks for getting the HttpClient docs updated so we can finally put that discussion to bed. Blows my mind how long that class’ correct usage was obscured by the lack of instance member thread safety (documentation). 
Sort of... It's not actually runnable on all platforms implementing .NET Standard, so that is stretching it a bit for me personally. That's probably the intended sentiment, though.
/u/landofozzyman helped me out on that part. Got rid of that error message. Not sure why it's going to a blank screen now though.
My favorite is : * [Supercharger](http://supercharger.tools/) : Code map, flow lines, rich code &amp; comments formatting, etc * [Viasfora](https://viasfora.com/) : Colorized syntax, especially for braces so you can easily see your position in deeply nested block. * [RevDebug Prompter](https://revdebug.com/Prompter) : Instantly see all variable values while debugging, no more clicking or checking the Watch Window * [Ref12](https://marketplace.visualstudio.com/items?itemName=SchabseSLaks.Ref12) : F12 jump to the .NET reference source code.
Thanks!
The web and HTML is weird. When it started, it was child's play and a lot of serious developers just passed on it and focused on other things going on at the time. This lead to a lot of development on web technologies that were not really led with the insight and expertise of seasoned developers (&lt;cough&gt;php&lt;/cough&gt;). Whether because of this, in spite of it, or just because the web was that powerful, things took off. Those technologies have come a really long way, and while there are still some fundamental issues with some technologies, there are also really good approaches to deal with those issues and use solid development practices. It sounds like you've checked your judgments at the door, and are willing to learn web technologies from the ground up. They're not hard, but it'll take time. In the end, I think you'll be better served by doing it though. &gt; Do you have an opinion on [3rd party controls]? Well, you can't really develop on the web without some 3rd party something. I mean, yea you could, but productivity wise, it'd be just horrible and you'll wind up solving a heck of a lot of problems that others have done a lot better several times over. That being said, something like Telerik just doesn't translate to the web IMHO. So assuming you don't go with a SPA framework (which you shouldn't at this stage), then you'll need solutions to do three things; DOM manipulation, AJAX requests, and some general UI/CSS components. jQuery will handle two and a half out of the three. Its very approachable and flexible, and while you're slightly removed from some of the ins and outs of Javascript, you'll be better for it. It has UI components that will be helpful, but aren't massively full featured. There are CSS frameworks that help give you a really great base for styling. I really wish I started using a CSS framework earlier than I did because I can quickly get a page laid out and generically styled without pulling my hair out. The great thing about the web is that there are a lot of "piecemeal" solutions that you can use as needed. Want to do something advanced with tooltips? Just grab a tooltip library. Need a robust table/grid? You can find one that can do some pretty advanced stuff. I'm not saying cobble together solutions, but you don't need to go with a huge UI library just to get one control. One thing that I don't think came up, but I'd be remiss to mention is .Net Core. You may have come across a lot of resources mention it, and while it's great in its own right, it's probably way to much for where you are right now. Just focus on working with the full .Net framework, and cross the .Net Core bridge farther down the line if it becomes a necessity. 
Good question. Does your browser’s debugger console give you any errors when you try to navigate to those pages?
wat
Thanks for doing this! 
What are you pushing these messages from? In other words, what's your background processor? Using netcoreapp or full framework?
&gt; farm out some work to the cloud, where I'd take an authed request, spin up a worker, handle it, and close it down Take a look at Azure Functions. They're basically easy-packaged automagic single-purpose web-hosted worker methods that you host on Azure but call via HTTP where you don't need to think much about how it's hosted. I whipped a quick proof-of-concept to move some repetitive work that needs to scale up to an Azure Function and it performed surprisingly well.
When I copy or move a file in visual studio it reloads the whole project and forces me to search for the file again in my project folder hierarchy. This is specific to unity projects. I filed a bug report and some developer got back to me saying that a fix is planned. This was a month ago and 2 hotfixes back. When will it be resolved?
They are pretty cool, but thing can get hairy in strange ways. 
Don't leave me hanging! What sort of issues have you run into?
It has been a minute. The time limit kind of sucks. I was generating map tiles and 99% of the time it would take less than the time. But occasionally one would time out. Also I recall having issues with deserializing subscription messages using the built in deserializer. It wouldn’t give an easy to understand error message. The old C# script stuff was a total pita. The new compiled version is a million times better. We started moving towards Service Fabric and a bunch of the azure functions are now just components of their respective micro services. 
|DataDirectory| won't be replaced automatically in .NET Core. You should do it yourself.
It's a thing: https://www.microsoft.com/en-us/download/details.aspx?id=55029 Don't just downvote this guy.
Hey, I have already mentioned the npm install path coming from the aspnet repo. Anyways I have updated the post to be more specific. Thanks for your suggestion and I will take care of this from next time.
Are you sure about that? The extension is maintained my Microsoft now, yes. But it has been ported to VS2017. I know about transforms in Core, but you'd still need slow cheetah to transform xml (besides web.config) and json files, right?
So much good stuff. Better start updating the post, when I have tried out all the suggested extensions :) Thx!
Would love to see some definitive tutorials on azure AD and auth in both webapi and mvc projects. Also some doc's comparing different azure storage solutions / caching and ideal projects or some sort of matrix to determine best choice? What situation would I use the new ready roll stuff in VS 2017 over something like a ssdt project?
There's a whole subreddit for azure?
How about https://github.com/migueldeicaza/gui.cs ?!
I'm spending my time ~50/50 between VS and Rider. I like a lot about both IDEs - I don't have a really strong preference for either, but what struck me immediately about Rider is that despite being a relatively-new, "third party" offering, it feels every bit as complete and polished as Visual Studio, when using it for .NET Core development on Linux. I guess a lot of credit has to go to .NET Core, Roslyn, the new MSBuild etc., which is mostly the same amazing toolchain regardless of the platform/IDE. Definitely going to spend a lot more time with Rider this year.
Full framework asp.net (some mvc, some webforms).
Hey Cam! Just a suggestion: could you please - *please* put the (return) type of things a classes' properties/methods etc. lists? For properties maybe even whether get/set is available? You would make me a very happy man.
So if I want it to go into the root folder of my project, how do I do that?
Yes, I use VS2017 and it still has VSRestart. 
No errors.
Right so I have this in my appsettings.json: "ConnectionStrings": { "DefaultConnection": "Server=(localdb)\\mssqllocaldb;AttachDbFilename=|DataDirectory|\\aspnet-ContentManagementSystemMVC6-3E0F11DB-33F2-4837-A443-AD7E740B4B8C.mdf;Database=aspnet-TestWebApp;Trusted_Connection=True;MultipleActiveResultSets=true" } And this in my "ConfigureServices" method: // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices(IServiceCollection services) { services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt; options.UseSqlServer(Configuration.GetConnectionString("DefaultConnection") .Replace("|DataDirectory|", Path.Combine(Directory.GetCurrentDirectory(), "wwwroot", "app_data"))));
What does it have to do, at all, with .net documentation?
You're asking that of the documentation guy?
OP said, &gt; it's my job to make sure that you as a .NET developer are aware of what you can do with Azure and the other guy said he needed better documentation on Win 10 IoT before he could get to Azure. Seems reasonable to me.
That blank page means you have an unhandled exception somewhere. To see its details, you should enable the `DeveloperExceptionPage`: public void Configure(IApplicationBuilder app, IHostingEnvironment env) { if (env.IsDevelopment()) { app.UseDeveloperExceptionPage(); app.UseDatabaseErrorPage(); }
This.
I've been thinking about making the jump to the all products pack for jetbrains since my resharper license just renewed, but i'm kind of holding out to see if a new job opportunity comes my way. webstorm is great, but vscode is amazing too (and free). Datagrip is the other tool that I absolutely love, so rider, resharper, and datagrip would be my reasoning for the all products pack.
You can't deploy a planet-scale system in your own DC. 
Great, thanks!
Sounds like your routing is off. You can F12 your browser to bring up the console and see what kind of code is being returned. If it's 404, your routing is wrong. If it's 403 or 401, your authentication rules aren't right. Post your routing template from your startup's `Configure()` method, as well as the method that receives your login post request (minus any sensitive details).
Bit of a confusing title. This article is not referring to authentication. Perhaps ".NET Core Console App Logging" would have been clearer.
I find "logging in" ambiguous.
Hmm. I hadn't thought about that but it makes complete sense. 
The title was clear to me.
Doesn't take 10 minutes to perform any given operation
User name checks out. How you doin,doc?
Well, my team covers .NET, but my background is cloud and I used to be on the Azure team. Did you have non-Azure stuff you wanted to mention? I'm still on the right team to talk to. :)
This is EXACTLY the type of feedback I need. Would you be willing to talk more about this?
That would be "Logging In in .NET Core Console Apps". The title as is seems comprehensible with a little critical thinking.
Something that should have been the damn default since Razor version 1.
``` // dotnet add package Serilog.Sinks.File Log.Logger = new LoggerConfiguration() .WriteTo.File("logs.txt") .CreateLogger(); Log.Information("Hello, world!"); ``` Does it really need to be any more complex than that?
Yeah that's how it is right now. Hmm... any other ideas?
I think you can create an analyzer which runs on compile (and on CI), which errors on such cases. Look at https://github.com/dotnet/roslyn/wiki/How-To-Write-a-C%23-Analyzer-and-Code-Fix. 
I have the same problem... My coworkers have 15.15.6 and I got 15.15.5 from the same installer (I configured an offline installer for 15.15.2 and have updated it; we got new machines and we copied the share to our local systems to install and this happened). It is very frustrating that it changes a line in the solution file every time I save something.
But we could totally get a resuable quantum logging component by using these 6 really simple libraries and mashing them together so devs get a paper copy teleported to their desk after the coffee finishes brewing on even days of the month! /s
How many implementations do you have for each interface? If it's one-to-one, then just drop the interface. The whole point of using interfaces is that one could serve other concrete implementations as long as they fulfill the contract. Preventing/restricting it doesn't sound like a good idea. 
It doesn't need to be, but it's not really completely insane either, it really depends on what you are trying to do. Is it just for simple command line app? Sure, there is no need. But what if your app takes dependency that can talk with `ILogger`, would benefit from basic dependency injection or asp.net cores configuration model and `IOptions`? We currently have an app that has library with app logic leveraging `IServiceCollection`, `ILogger` and `IOptions` linked to asp.net frontend and a companion cli app for some tasks and it's pretty great having the setup synced up ensuring things work the same in cli and web :) This might also become more common with the introduction of generic host in 2.1 https://github.com/aspnet/Hosting/issues/1163 .
I would include Serilog.Sinks.Async too as a wrapper for the File sink
&gt;Javascript isn't as bad as people make it out to be I agree. It's far worse :) 
The services assembly registers types in an IoC container. Those other concrete implementations might at one point exist in an entirely different assembly that I want to be able to enable or disable as I wish, maybe something configuration or role based, without completely crashing my coworkers' code. Scenario: Colleague creates a new separate project to be added to the application. This requires an implementation of IModule. Colleague needs to connect his classes to services. He adds a reference to our Services Module. It is at this point I want Visual Studio to scream "bad developer!" because it notices the IModule implementation and suggest he rather add a reference to the Infrastructure project which contains interfaces for whatever service he wants.
That's good info. The Azure SDK for Python docs person is a friend of mine. I'll make sure she sees this.
That's a very good request. I can't make any promises, but I'll make sure the team sees it.
Wew lad
There are many benefits for developers, tech specialists, and programmers of attending conferences in IT sphere. Speaking about the most attractive events in .Net technologies that are about to happen this year, I know about four of them: 1. Visual Studio conference 2. Microsoft Inspire 2018 3. TechEd Barcelona 2018 4. Progressive .Net All of them are worth visiting and will cover a lot of interesting topics like web client, .Net framework, database and analytics, cloud computing, etc. 
Not an answer to your question, but why are you even creating the context yourself and include `using` everywhere in a stateless service? Just provide it via dependency injection and a new context will be created every request and disposed afterwards automatically. https://msdn.microsoft.com/en-us/magazine/mt703433.aspx https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/intro#register-the-context-with-dependency-injection
 //add or update as necessary and dispose I think everybody uses tracking in some form. How do you do this 'add or update' without change tracking? DbContext.SaveChanges() is the core method used to persist data to the context. It requires change tracking to know which fields to update. It uses all of those tracked changes to build the expression graph and generate an update statement. Without it, you would have to loop through all child entities, explicitly set them to modified, call SaveChanges for each child, and end up sending hundreds of sql update statements to the database, instead of just the one you would get with the change tracker. 
My pleasure! I really hope I can make some valuable improvements!
Total, absolute agreement. In the .NET Core world, we've got a backlog several months deep. We're working hard to get caught up, though! On that note, we really, really value community contributions to our docs. If you're so inclined, please feel free to jump in! I'll be happy to get you started.
It gets messier, though, when you want punctual transaction management along with more than one consecutive persistence action during the same request. 
Could you elaborate what you mean exactly? Because EF Core supports transaction handling https://docs.microsoft.com/en-us/ef/core/saving/transactions
&gt; Just provide it via dependency injection Yes that's right I put that example to illustrate the context is disposed on each call.
&gt;How do you do this 'add or update' without change tracking? I see your point about how does the context know what to update. However, in a stateless app we dispose the context on each call so the new context knows nothing about the activities of any prior instance. No tracking needed. Example: Request 1 - User wants to edit an order. 1.) new up a context 2.) Get the order 3.) dispose the context 4.) Display on page for user to edit. Request 2 - User wants to save changes 1.) new up a context 2.) Get the order 3.) Update the order : order.Property = changedOrder.Property 4.) SaveChanges(); 5.) dispose the context
without change tracking EF won't know what changes where made and what to actually update in your second request when calling SaveChanges() (you can manually set the status to Modified though)
You are certainly welcome good sir, pay it forward :-) I have lived in azure authentication land for way too long, sometimes their magical libraries are just that, but makes it way too hard to see what's happening under the magic. 
Thanks for the reply. Agreed that it's not totally crazy - I'm just a bit disappointed that we're turning the lean, decoupled .NET Core platform into yet another monolithic framework, where to use one component means dragging in a whole lot more. I don't think we want to be in a place where kicking off a tiny CLI tool means building an IoC container, or where we carry a bunch of half-loved obsolete packages along with new apps just because the whole ecosystem got tangled up with a raft of "blessed" dependencies at some historical point-in-time. I guess we'll just have to hang in there and see where it all goes.. :-)
When do you wan to use this sink? For example, is 500 events/second high enough to use this sink?
I have no idea it's been a couple of months since the change. We moved away from Sinks.File to the Async wrapper (better performance) and then we moved everything to Sumologic and said 'fuck it we'll ingest data directly' and now we have no logs.txt anymore
EF supports transactions, but the story doesn't end there. Suppose you have two persistence methods: Task&lt;bool&gt; UserHasPermissionAsync(int userId, int permissionId) { … } Task UpdateWidgetValueAsync(int widgetId, string widgetValue) { … } and then you have a higher-level code construct that does something like this: if (await UserHasPermissionAsync(…)) { await UpdateWidgetValueAsync(…); } …how would you guarantee that they happen in the same transaction? (And yes, you want them to.) There are many ways to approach the problem, but it is tricky to make all of these happen: * All CRUD operations in the same user request all happens inside one transaction (regardless as to which class they are implemented in) * Transactions are only started when they are needed and are closed as soon as they aren't * Satisfy the above two constraints with a DRY implementation that is hidden from day-to-day development You end up with a deep rabbit-hole that makes sure you understand your DI really really well, along with object lifetimes (those both of your persistence implementation classes, and the database/EF connection types.)
Maybe layer/dependency diagrams could help? I think you need VS enterprise for the initial creation at least. https://msdn.microsoft.com/en-us/library/dd409395.aspx https://docs.microsoft.com/en-us/visualstudio/modeling/layer-diagrams-reference
This is a pattern? I thought that abstracting DA was just part of n-tier architecture. 
&gt; Why would you abstract the data? &gt; The most obvious reason is the very reason we try to reduce code duplication. &gt; Imagine you wrote that same piece of code over and over in your application. Once in the API so your API could save Todo items, a couple times behind the desktop app UI pages, and various other random places for dramatic effect. (Etcetc, "this is bad") Yes, this is bad, but what I could have done is have a function in a module and call it from all these places. Nobody **needs** repository pattern for that. 
&gt; you will still need change tracking for SaveChanges() to work properly so deactivating it only makes sense for readonly use cases. For CQRS, this is paramount.
The best way to do this in the new project format is to use a "Directory.Build.props" file. This is an MSBuild props file that is automatically imported before your project and which can be used to inject common properties. If you place it at the root of your repository, all of the projects underneath it (in all subdirectories) will automatically pick it up and use it. For example, here's how I set my output folders: https://github.com/mellinoe/veldrid/blob/master/Directory.Build.props#L4-L8
Just use Rider. You get the same awesomness with much better performance.
Entity Framework essentially is repository pattern. 
Isn't this the part where you actually talk to each other, or have some kind of documentation to describe the architecture and how to implement new features? If you only have one-to-one relationships, I'd suggest that you rather just use the concrete class in the IoC container, and extract a interface later on when needed. 
If only it was that simple. I cannot just switch the entire team to an entirely new IDE because I feel like it. Perhaps in a few years when it's more mature, but for the time being, we need VS for developing Service Fabric applications. That is not to say I don't want to decouple from VS, but the productivity gain is higher than the cost so it's not high priority. Ideally, everything would work from the command line and be versioned as well. We're getting there, slowly.
How about writing some unit tests that use reflection to check referenced assemblies?
+1, I came here to say the same thing.
So EF or Repository Pattern around EF?... is EF a Unit of Work?.... just want to know what is best practice... interested to know what people do with data access whilst using EF
I'm struggling to understand how you've never had to persist changes in a service layer. At some point someone is tracking those changes. EF core just has a little bit of magic to make it easier. Furthermore you can by default disable change tracking on the context. Or you can disable per query by tacking on .AsNoTracking(). So your complaint about it being a core design feature is incorrect. Its just on by default. The reason its on by default is because an overwhelming majority of APIs boil down to CRUD. 3/4 of the operations in CRUD can utilize change tracking.
Perfect.
It's a very opinionated one - a lot of functionality, but pushes you towards a certain way of working, with particular technologies. Not as easy to swap away.
&gt; Isn't this the part where you actually talk to each other lol I wish I'm the software newbie yet I'm the one suggesting unit testing and code reviews because currently there is none. 
I concur. Too often I see “repositories” used as the internal API of an application, when it would be more appropriate to create business services that *might* consume a repository or too (many use cases don’t need explicit repositories when you’re using libraries like EF or NHibernate). I’d take it a step further and say that for any sufficiently complex application, each “service operation” (i.e. logical “command” or “query”) ought to be encapsulated in its own individual service - which at this point just means one class per command/query.
Even in a stateless app, within the same context you need the change tracking to handle related entities. If your context had change tracking disabled, this example wouldn't work: 1.) new up a context 2.) Get the order.Include(o =&gt; o.OrderDetails) 3.) Update the order details: order.OrderDetails.Property = changedOrder.OrderDetails.Property 4.) SaveChanges() 5.) dispose the context The change to OrderDetails would not persist with change tracking disabled. The alternative, which I think you are getting at, is user wants to edit order 1.) on login create context per user 2.) Get the order 3. ) user edits on page. 4.) User wants to save, api simply calls SaveChanges() on the user's personal context. 5.) on logout, dispose context. That might make sense for a simple win forms app with a handful of users and a pessimistic db locking strategy, but nobody does that in a web api, which is what the majority of entity framework apps are. 
What we started doing as well. Eventually you'll shoot yourself in the feet using things like AutoMapper after refactoring. Manually mapping DTOs consume more time, but so does finding out there is an exception at runtime because properties don't match anymore. 
Well, at least you are suggesting the right things 🙂 Those are important pieces to the development process as well as for when the maintenance part begins. Having tests for the important parts will probably save someone a lot of trouble down the road when the tests fail, rather than deploying broken logic. Back to topic: if you decide to pursue the approach you initially wanted, than an analyser would probably be your best bet, as suggested by others. 
It all comes down to whether you need to encapsulate dependencies or not. Sometimes, even if you know that you won't have to change the ORM, it can be convenient to have that thin layer to put breakpoints on or to have some hook executed on CRUD events. 
Thank you. Following your example, I've been able to get my analyzer up and running.
I hear this often and I think it's accurate but only if your entities only come the database AND they're relatively easy to piece together from the database. If you're entities come from multiple data stores or piecing then together is involved, then the repository pattern is much more attractive. For example, your users are stored in an LDAP server and an attribute on them is the key in the database and to fully construct a user you need to hit both. 
I'm trying to think of a situation where you don't know enough about the data you will be storing before you start writing.
&gt;&gt; nobody does that in a web api, which is what the majority of entity framework apps are. Right! That is exactly my point in this question. &gt;&gt;The change to OrderDetails would not persist with change tracking disabled. This is the same as what you have already mentioned and is a matter of semantics. The same hypothetical api as I mentioned above would work. 
Yeah, I'm all for abstractions, but I've never hit a situation where my data is so arbitrary that this works. Maybe in a non-relational NoSQL kind of situation?
Yeah it's kinda weird. The first thing when I design a system is to design the database first. Code is last.
[removed]
I disagree. EF is an ORM. If you have 7 projects that use EF to connect to the same tables you have problems. If you roll EF into a shared dll across assemblies then you are following the spirit of RP architecturally.
It matches mongo in the storage and API, so you can build against that and use cosmos when you need the scale. 
&gt; But I think a lot of people also do stuff like one repository per entity This is what I think of when I hear "repository anti-pattern"
The state tracking functionality is used implicitly when you modify objects from your context and call SaveChanges. It's there for EF to implement the Unit Of Work pattern: https://martinfowler.com/eaaCatalog/unitOfWork.html 
I like how people immediately latch on to EF and say doing this is an anti pattern. For EF, sure, but that’s not the only ORM in existence. I don’t even use EF anymore for the majority of my projects because it makes no sense to use something with so much going on behind the scenes for a majority of small projects, unless you’re doing code first (bit let’s be honest, how often do you actually get to use that - at least in my world where I’d have our DBAs kill me). I use Dapper a lot in my projects because of that and I find the repository pattern works great with that and makes my unit testing so much more robust, too.
So you changed the class name, correct? I think the next thing you need to do is bring up the Package Manager Console, and update the database so it matches your model. PM &gt; Update-Database -Force
maybe i'm using the wrong word. can you tell me the difference between isolating and abstracting and why the latter is disastrous?
Any time your starting development before the business has fully defined what it is they want. Happens all the time.
I saw an old project using edmx with a slightly different file in each project, changed it to one code first model in a Foo.Data.dll
Can you give me a scenario where the business has such a ambiguous idea of what they that the potential data stores can change? Its not like you are building an accounting application OR a VR game or something.
Will I need to create a new migration after each build but before each update?
&gt; If you have 7 projects that use EF to connect to the same tables you have problems Doesn't the ChangeTracker solve this?
That is with this line included as well? https://github.com/mellinoe/veldrid/blob/master/Directory.Build.props#L3 I've not seen that behavior myself.
DbSet&lt;T&gt; is already a repository. You could make a IRepository&lt;T&gt; which would wrap around the DbSet or even the whole context to stitch together data from multiple tables and returns a combined type you use in your viewmodels.
Yes. Do look at the other commentors suggestions. I am new to MVC myself. 
I was going down the no entities direction - which on second thought does not apply here. You're totally right, you usually know if SQL/EF/whatever is the right choice at the onset. That said, you still might not know *where* you want to store data at the start of development. Things like PCI Compliance require CC's numbers to be stored in separate database. We use Azure table storage and an Azure SQL database and are going to be rolling out some form of noSQL in the near future. The UoW/Repo pattern allows us to abstract that "which data store is this in" question away and not be concerned with it until we are ready to implement.
Fully agreed. But that’s an entirely different company, product and support channel - which is my point about the gap for on-prem (or different cloud) deployments. These things matter when getting sign off for production deployments. 
&gt; just want to know what is best practice And you won't get a straight answer on this. This is one of the most contentious arguments in designing applications using EF as an ORM. If you Google this very question, you'll find that half the results are vehemently against wrapping EF in a Repository or UnitOfWork pattern and call it an anti-pattern whereas the other half have the opposite viewpoint. It all comes down to how reliant you want your design to be on the EF implementation. Personally, I am in favor of using Repository and UnitOfWork patterns on top of EF, for a couple different reasons: * It better follows the Dependency Inversion principle in ensuring that you are programming against an abstraction as opposed to a concrete implementation. * Due to having better adherence to the DI principle, it makes unit testing and TDD more feasible as your code isn't coupled to EF. * EF is a very powerful tool. With that power comes a lot of potential abuses. By abstracting away IQueryable and only exposing what you need, you avoid the potential for design abuses and some of these crazy EF queries you sometimes see. I'm of the opinion that you should only expose the functionality you need and if a situation arises where you need some functionality that hasn't been exposed, it should be a conscious chose to expose it. * By not being coupled to EF, makes it easier to swap out ORMs or data access libraries down the road. This probably isn't a huge concern, but it is worth noting. Those that argue against all this would make the following points: * EF already implements Repository and UnitOfWork patterns * Adding an abstraction adds a decent amount of overhead that isn't worth it Basically, you have to account for your own needs and the size and scope of the project on which you are working. If you are working on a very small project or console application, it's probably not worth the additional overhead to add an abstraction on top of EF. If you have a much larger project which will have multiple people working on it simultaneously which requires a more rich design, then my opinion is to implement Repository and UnitOfWork patterns for the reasons stated above. Sorry for the long-winded response. At the end of the day, there's no right or wrong answer. It all boils down to preference and what best suits your needs at the time.
You may have an idea about the data, but if you are using [Domain Driven Design](https://en.wikipedia.org/wiki/Domain-driven_design), your first and primary focus is on the domain logic housed within your domain models. Ideally, you have your domain layer implemented or designed before you start thinking about the data access layer and persistence models.
**Domain-driven design** Domain-driven design (DDD) is an approach to software development for complex needs by connecting the implementation to an evolving model. The premise of domain-driven design is the following: placing the project's primary focus on the core domain and domain logic; basing complex designs on a model of the domain; initiating a creative collaboration between technical and domain experts to iteratively refine a conceptual model that addresses particular domain problems. The term was coined by Eric Evans in his book of the same title. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^| [^Donate](https://www.reddit.com/r/WikiTextBot/wiki/donate) ^] ^Downvote ^to ^remove ^| ^v0.28
Ugh. Tell that to people at work. We use Postgres with tables that only have an ID and a jsonb field because “if we have to alter the schema 2 years from now it will take hours to run!”
Yeah if you are using an approach that intensively abstracts away data access then you'll intensively abstract away data access.
IT Grabs the content and sends content to the class in an email.
Absolutely! Please do! :)
That is not viable long-term. The longer the wait, the greater the delta for changes. If you don't start learning now then you're stuck and cannot adapt.
Abstraction basically means hiding how some action is done. So as long as the action is done and some intended results happens we don't care exactly how that action was carried. This is fine in many cases, for example when I wanna sort a list I don't care about the details of how it's sorted, I just want it done. And all is well, until the day it starts taking ages to sort that list, because instead of a couple thousand elements there's now a couple million in it. So now I have to dig into the details of how the list is sorted to fix it. Back to the data access. Abstracting that part means hiding a lot of costly operations. Signing a blank check to the ORM leads very quickly to SQL queries being done at unexpected times and much more data loaded than necessary. So most of the time if an operation is gonna lead to an SQL query (or a hundred), I wanna know about it, it's not just an unimportant detail. Aside from efficiency, there's always a long list of gotchas and best practices with ORMs. With Hibernate, for example, it's usually expected to do queries inside the scope of a transaction. But to do that I need to know the method I'm calling is making an SQL query. Once again it's not just a detail. As a real use case: there's over 800 results when searching for [LazyInitializationException](https://stackoverflow.com/search?tab=relevance&amp;q=LazyInitializationException) on SO. However data access is huge and there's a lot of details that, while important, I don't want them to be peppered all across the code base. So isolation, in this case, simply means to keep them in their own module, package, namespace or whatever separation mechanism. At the end isolation and abstraction boils down to the same thing, it's just that far too often abstracting data access means hiding away many things that shouldn't be.
I agree that is not viable long term. but sometime to learn something it impact current productivity that's my experience. and there are always coming new which we need to learn possible we can't learn everything and it impact productivity that's my thinking
True. And I definitely agree it can be overwhelming and frustrating as things constantly change, but that is the nature of the industry and how innovation comes. The newer technologies are evolving because they are new approaches at solving old problems. People have different priorities. If current productivity is a concern then definitely do what one knows best. But the risk of irrelevancy is there.
I'll give you an example. It's happening right now at my job. We have two projects that are out of sync. The first project is massive data centralization project that's being spec'd out. It's going very slowly because talking to everyone to determine what needs to be stored and what data overlaps take times. Then you have a new greenfield project that's going to take on a subset of that data from project 1. Well this project needs to be up and goin ASAP. It can't wait on Project 2 to complete. I have no idea what the schema of Project 2 will ultimately look like, but with a repository Most of my application doesn't have to care. And in fact I'm using sqlite right now as a stopgap until I can hook into the real database system of project 1. Another thing that happened is that we started using mongo for various reason on a project and after 3 or 4 years we ran into the limits of nosql, so we switched to sql. using a repository pattern made the changing of a database engine less traumatic to our application.
I agree of course innovation required. Based on timeline can work with new technologies. 
&gt; I remember getting really frustrated with Azure tools. When it comes to the Azure tools, the hands-down best experience is what's built into VS2017 now. Earlier than that, yeah, it can be hard to figure out what you need to install and then you get version conflicts and other such problems. Which version of VS were you using at the time? I agree I may need to make sure we cover those scenarios better. &gt; Sample projects that cover not only code, but maintained instructions on account setup and deployment strategies could help encourage people to make the jump. I hear you loud and clear. But... &gt; Part of the problem is that all the microsoft doc sites and tutorial pages are scattered, managed by different teams, and aren't linked or don't cooperate. Yep! This is what I'm trying to solve. As a single resource in a large organization, I'm not going to be able to fix the whole experience (though I will certainly try to draw organizational attention to it!) but I'm hoping I can focus on the scenarios that are priorities to address. &gt; While you're at it, can you pass along to the request to the other docs teams to either make better wrappers for Win32 API calls, or provide better docs so it's easier to p/invoke? I'm just a simple .NET developer. Your unmanaged code is strange and scary to me. ;) Seriously, this is good feedback. To be honest, I'm not 100% sure who to send it to, but I promise I'll try to find out. &gt; And if you've got any pull with the windows platform team, maybe look into a pluggable update service (not windows updates, pls) where we can install configurations for non-administrative updating (both all users and current user). I get what you're saying, and that'd be pretty slick. In the absence of such a service, though, I'm wondering if [Chocolately](https://chocolatey.org/) or maybe [Ninite Pro](https://ninite.com/pro) might help solve that problem? You also might want to look into [Desired State Configuration](https://docs.microsoft.com/en-us/powershell/dsc/overview). I'll be honest, I've not gotten too far into DSC, but I'm pretty sure it can do what you need. Let me know if you'd like me to try to find someone to talk with you about it.
There is several issues with your argument. 1. You're complaining about EF Cores entity tracking and how its useless and no one uses it. Yet using .Add() or .Update() and .SaveChanges() means you are going to use the change tracking. Its a way for the framework to figure out what all you want saved when you eventually call save changes. So this is more of a case that you don't actually understand EF Core's purpose. Its an ORM. Its there to map your entities and make it easier to change and save them. You really don't have to concern yourself with how it does it just to use it. 2. context.Update(entity) absolutely does not fail if you call update on a tracked or untracked entity. I've uploaded a snip of an example that I just wrote and tested: https://imgur.com/a/afl8U I believe it will fail if the entity does not exist in the database. Which would make sense because its called Update not Add. 3. "context.Add(entity) will throw an error if some related entity is not attached". I think you're doing a lot of things wrong in general. I've never had this happen. If I'm adding an object to an entity I'll usually just set the related id field instead of putting the entire object on it. That accomplishes 2 things; the id you're adding doesn't need to be in the context and its a lot simpler to understand. 4. You want me to demonstrate a CRUD based app that utilizes change tracking? Microsoft literally has dozens of examples that demonstrate its utility. There are even more blogs/tutorials. You honestly cannot build a service these days that wouldn't be able to take advantage of the change tracking. Spotify doesn't just serve up songs they record metadata about everything they can get each song you play. Netlfix does the exact same thing. Hell even loading a static asset can utilize change tracking if you want to keep track of who, when, and where you're serving the static asset to. The actual use case for a 100% read only API is pretty slim these days. Here is a microsoft tutorial that makes a simple CRUD app to track grades of students: https://docs.microsoft.com/en-us/aspnet/core/data/ef-mvc/intro Here is a blog that builds a service to track authors and their books: http://www.learnentityframeworkcore.com/walkthroughs/aspnetcore-application Here is one that builds a service to track employees in a company: http://www.dotnetcurry.com/entityframework/1347/entity-framework-ef-core-tutorial 5. Just because a service layer is stateless doesn't mean you don't need to be able to persist things to the database. In fact that's how most rest services work, they're stateless. You make a call to get some info. Then you make another call to update them. You pass an API key or a cookie back and forth to keep track of the user. But the API is still stateless. 
I think you can use up and down commands to add migration on that table. Might you need to drop and recreate table using migration commands
You can use a data annotation on your EngineOlHome class to target the EngineOlHome table: [Table(“EngineOlHome”)]
Why is that? Why stay with a technology designed decades ago that has many shortcomings, which is basically why people created NoSQL alternatives? It's not just about schema.
the solution should always be about picking the right technology for the job at hand. There is ALMOST never a true use case for NoSQL, and I assure you, just because a technology was designed "decades" ago has no bearing on it's efficacy. Yes, it is all about schema and structure.
You are making this way too complicated and it feels like you don't understand how ORMs work. It's simple, if you never intend to save changes to an object use .AsNoTracking(). If you do intend to save changes to an object, don't.
Your account almost entirely spams courses. Are you sure about this recommendation?
I recommend you look up guides by Julie Lerman.
It wasn't built to scale for one thing. It's also pretty bad for searching, handling blobs, etc. That's already plenty of use cases.
(This is throwing ssl errors)
Facebook and Google disagree with you...
Maybe going to the cloud and start some Azure setup?
Azure has now (in preview) Azure domain for US government and military. You could use Functions as scheduler, Service Bus for all messaging and Webjobs/Functions for the rest
I'm from Russia, so noway, but for personal projects. Thanks!
Indeed another option. Azure Stack (the on premise version)
Someone has posted a workaround in the MS forum, to install Service Fabric module feature :-) The installer is really "good" software.
Okay those links you're pointing to are things you PR'd into the dotnet repo. You also answered what you're complaining about in them. &gt; EF Core can only track one instance of any entity with a given primary key value (from here: https://github.com/aspnet/EntityFramework.Docs/pull/605/files) I honestly don't see a problem with this. If you're trying to track 2 entities with the same id and then trying to assign different values you've messed something up. The fact that it fails on savechanges is annoying. Runtime errors are quite annoying. But throwing an error makes perfect sense here given what its doing. Perhaps you just needed the documentation to be more clear so you could understand it. &gt; context.dbset.Add(entity) is a horrible API I have to disagree. Its pretty clear what it does. It adds a new object to the list of tracked objects. Yes there are more technical aspects behind how it does that. But you can use it without any knowledge of that. That is the beauty of programming. You can build on other's code so you don't waste time reinventing the wheel. &gt; You really don't have to concern yourself with how it does it just to use it. - me &gt; I agree!! So why are "Tracking" and "Attached" and "Detached" words that exist in my EF dictionary. - you At this point you're arguing just to argue. Of all the ORMs I've dealt, and believe me there have been some truly shitty ones, EF Core has just worked. There are a few gotchas just like in *any* framework. The conversion between DNX -&gt; Core 1.0 -&gt; Core 2.0 has been rough. But it has been consistently getting better. I wish the documentation would catch up to 2.0 already because there is a considerable amount of old tutorials even on Microsoft's sites. If you don't like the way they implemented things switch to something like dapper so you can manage every single detail yourself. Issue raw sql queries in every single call if you want. Given your other comments and github issues that you opened, I have a feeling you're doing so much wrong that you feel like you're fighting every single thing in EF Core. That tends to happen when you don't follow the suggested patterns that clearly work quite well. Especially coming from EF6 which is drastically different because EF Core was a *complete* rewrite. So lets reiterate cause this is the last time I'm commenting. .AsNoTracking() on queries where you don't want the objects to be tracked .Update() if you choose to add an object's changes to the list of tracked entities when its not already tracked .Add() when you want to add a new object that needs to be saved .SaveChanges() when you want to save anything that has changed Use dependency injection. Its amazing. 
I second Pluralsight. These authors are great. Each with a different style: Scott Allen Shawn Wildermuth Gill Cleeren If you prefer Udemy pay-per-course (never pay more than $10-$15, there are ALWAYS sales) Mosh Hamedani (although his personality rubs me wrong for some reason but a LOT of people like him so it's only fair I mention him. He knows his stuff) 
From my experience, very few developers stick to 1 thing. Unless you are working for a really large team and hired for a very specific role, expect to work on front end, back end, APIs, databases etc... 
I found Scott Allen's voice and personality incredibly grating the first time I listened to him. Over the course of the course(ha) and as I worked through other pluralsight material he grew on me to the point of being one of my favorite presenters. 
Now that I think of it, my first course by him, I was in the same boat. It was just so long ago. I've been a PS member since 2010. As for Scott, you make royalties for your courses through them. He made something like $2 million in 2013 off of PS royalties.
We use mostly Web API with a TypeScript frontend. 
Quartz and MediatR are great for what they do. But also check out NServiceBus or MassTransit for dealing with abstracting a service bus over a queue (they’re both pluggable in terms of the queue).
You're one of the lucky ones then.
I've decided to go with VS 2017's python tools and deploy to Raspian. It really sucks having to go to a brand new language just because I need to use the pi camera for my project. I really wanted to build in C# but haven't found many references for using hardware and deploying .net to Raspian.
We’re building a .NET Core 2.0 and React app on my team. Basically a Web API backend with a separate React front end. 
A medium size company here. A stand-alone webapi serving multiple websites and console application. Most of websites in VueJs (currently migrating some old asp mvc)
Well, I don't know if this id what the dude is getting at directly. But typically i focus on what he's saying because I don't know the best data store to use. What's the most effective for cost, performance, maintainability, scaling, stability, ACID, etc. 
WordPress site, for dot net forum 😁 
I can vouch for it - it's a good course.
&gt;Use dependency injection. Its amazing. meah. You don't impress me. But I like it when you try :) 
Dotnet core 2.0 backend, grapql API in dotnet, and typescript/angular5 frontend.
our frontend/backend split isn't based on C# vs Javascript as we expect all developers to be able to do both. it's more based on our designers being technical enough to write html/css (and a little bit of JS). Our dev's do know html/css but they tend to lack the visual co-ordination/detail to make it look right. Our lines are fairly blurry and we try use whatever skills people have and accept their limitations. 
Thanks. Have not heard of Sumologic before.
I use Rebus. It’s pretty much a drop in replacement for NServiceBus but unencumbered by commercial licensing. 
Yep I use Repository pattern without interface for this reason... I want all my persistence code to be one place, start in memory, then embedded no sql, then database if it starts to be complicated. Because I wrote tests along the way, changing the implementation is easy.
Also when data is huge and that you know exactly what will be requested beforehand. You basically trade recurrent DB cost against one time dev cost. (At the price of flexibility) A no sql DB is less flexible because you just can't change the requests as you want afterward.
Anything with Scott Hansleman is great, imo
I saw Razor (with Kendo UI) used just a couple times. For some small internal websites at Microsoft. What I usually see in production is REST API backend (ASP.NET WebAPI) and some JS framework on front-end. So far I mostly saw Angular, but there are many UI frameworks over there.
It depends on the application, allotted timeframe and resources, and the requirements. If we have the time or need the reactivity that it has, my team tries to develop web APIs with an Angular client-side application. However, if we have a smaller timeframe we will develop straight MVC with Razor.
Government project caused by "**I** dont like search etc?" Best job ever! 😁
I'm not 100% sure, but I think this was the article that did me in: https://docs.microsoft.com/en-us/azure/storage/blobs/storage-dotnet-how-to-use-blobs
If you don't need/want microservices (and it sounds like you don't), then perhaps looking at simple in-memory solutions to these problems might be of some use. I've recently implemented most of what you describe, so here goes. **Queueing** I've implemented queueing in my database by serialising a command object to Json, and storing it, along with its type name and a `LastUpdated` field in a `PendingOperations` table. I then have a background thread that fetches a bunch of operations at a time, deserialises them, executes them, and then marks them as done/deletes them. My use case for the LastUpdated field is that, if the same operation on the same entity happens more than once in the space of 5 seconds or so, I update the LastUpdated field to now - pretty much as a debounce so I don't unnecessarily re-index things too much (all operations are idempotent btw, so should yours be if possible for much greater simplicity). Your use case might be slightly different, but this method is transactional, safe, simple, and doesn't use an external bus. **Background worker** Just a `Task.Run` with a while loop (and a 20ms Task.Delay after each iteration so as to not thrash the CPU), started up as an Autofac IStartable. Some mutexing required if you run more than one instance against the same database, but otherwise simple code - it's single threaded after all. **System Bus** I use [`Microbus`](https://github.com/Lavinski/Enexure.MicroBus), partly because a friend of mine wrote it, but mostly because its awesome, just works, and has a simpler API/registration than MediatR. Supports commands, queries, and events. I delegate all of my controller actions to commands or queries, and use events to notify the wider system of things happening. **Scheduling** I used FluentScheduler for a while, but it doesn't really support calendar based schedules as it doesn't store its state in a database, so loses its place on app restart. Great if you're scheduling things like 'run this thing once per hour/minute/day' or 'at this time each week', but not great for things like 'schedule this thing for two weeks from now' (though I'd argue that should be modelled as a separate workflow and not in a generic scheduler). Quartz.Net supports database persistence, and can handle pretty much whatever scheduling you can throw at it, though it is a Java port and comes with baggage. &amp;nbsp;&amp;nbsp; On top of these fundamentals, you can write your own controllers/UI to trigger actions, do whatever work you want etc, just by firing off commands to do things. An admin UI could consist of just a few buttons that call the same API's (or specific admin API's) that fire off the same commands. You could query your operations queue to see how many are pending (or better yet, show it on a dashboard), or add an `ErrorTime` field if a command fails a few times, and create a page to show failing commands. If you need to scale, you can push your operations to a bus instead of the database, and move your handlers out of process to a webjob or other process. Great things are possible if you keep it simple at the start :). Good luck, hope this helps!
You are not Facebook or Google. Odds are that you do not have Facebook problems or Google problems.
Do you even read the post? I did my research, now I want people share some knowledge and experience may some success stories with aforementioned combos.
Well, thanks for sharing, but inmemory doesn't work here. The app is already shared, balanced etc, so schedule, jobs are spread through 5 nodes. Actually microservices is the path that I'm going
This. I do WebForms, MVC, WCF, WebAPI, console, Logic Apps, Functions, Web Jobs. Mostly Framework but almost everything new gets made in Core.
&gt; Which version of VS were you using at the time? I agree I may need to make sure we cover those scenarios better. You'll love this. I had four different VS installations at the time (around a year ago): 2012, 2013, 2015, 2017. Adding in 2017 made me want to clean up old installs and remove the massive, tangled spiderweb of dependencies left over. Some packages weren't removed on install, some got updated through the VSIX interface, some through external installers, etc. And I didn't have any Azure projects working, so the goal was to remove all but the latest version of the toolkit, but then I got stuck with malfunctioning installers that couldn't be removed. It's not really a scenario that can be fixed _now_, as it involves multiple versions of the same products spanning several years. But ideally, there'd be some sort of policy in place to tame the installers. Either no shared resources, no simultaneous installs, or massive automated testing of scenarios with all sorts of versions in there. Of course that's a lot easier to talk about than to actually implement. But the tendency to hide old installers and move things around really compounds the issue for end-users. In the end, we end up using install clean-up utilities or editing the registry. It's not the first time this sort of thing has happened, just the most recent. Which is why I'd just really like a lot more transparency and a lot less behind-the-scenes magic going on with dependencies and toolkits. &gt; As a single resource in a large organization, I'm not going to be able to fix the whole experience Oh, I know. I'm talking about it mostly so you have visible feedback. MS as an organization collects a lot of automated metrics, but they miss a lot of (non-enterprise) verbal feedback because of their obtuse feedback collection mechanisms and blind eye to negative feedback. Those old Win32 doc sites are evidence of that, as they're spattered with comments talking about how information is wrong, out of date, or incomplete. Despite the fact that they always ask if you want to take a survey when you land on the page. I can definitely say, though, that working with everyone involved in .NET has been so much better. Interaction is much more straightforward and open. &gt; I'm just a simple .NET developer. Your unmanaged code is strange and scary to me. ;) Yeah, we generally like to avoid it, but there's enough holes that sometimes you've got to stick a `DllImport` in. I've even been hunting around for SDK documentation looking for API details on the new action center, because I want to define my own quick action buttons. &gt; I get what you're saying, and that'd be pretty slick. In the absence of such a service, though, I'm wondering if Chocolately or maybe Ninite Pro might help solve that problem? You also might want to look into Desired State Configuration. Those work, but this isn't really about end-user installation or enterprise package management. Rather, it's about providing an integrated piece of infrastructure so that vendors don't make their own bad implementations that can interfere with policies, slow down computers, and end up ineffective because people disable them for being obnoxious. Having an integratable update service (that could be disabled via the service manager or group policy) that software vendors could interact with would prevent another decade of high bodycounts due to the Java Update process and Adobe Acrobat updates. Some companies are _really bad_ about how they handle updates. Enterprise updating can be handled with things like DSC or Puppet. But software vendors make updaters for the users without IT departments. And they won't go away until there's a solution that's prepared for that. Which is why having one that can be easily disabled in corporate scenarios with group policy would be really nice. Anyway, thanks for listening. I know that there's not really a lot you can do about the updated stuff. It's a big organization of organizations.
Hey, horses always worked as mode of transportation, stop trying to justify use of cards!
Well, we haven't stopped using cars just because we have rockets. 
Either MVC or MVVM in .NET, or MVC API with an AngularJS frontend. 
Thanks!
Given the sheer amount of cash they put in to make it scale I'm not so sure. Facebook has dozens if not hundreds of engineers just for MySQL.
We do work on all sorts of platforms. For the most part we generate html on the server but we do have a project with a Craft CMS as the front end which uses a .Net api. The .Net api is called by both the CMS server and from the browser.
Thanks for the link and the notes! I'm very familiar with that article. I'll add it to my list for some prioritized love. I really like your idea of disclosing what version the NuGet package was at the time of writing. Let me bounce that idea around and see what traction it gets.
The Azure UI looks like Razor + Knockout JS. 
&gt; Anyway, thanks for listening. I know that there's not really a lot you can do about the updated stuff. It's a big organization of organizations. Glad to listen! Thanks for giving me all the feedback, too! Feel free to reach out to me here on Reddit or at my MSFT email any time! I can't promise I can always help, but I do promise I will always try.
Hey, don't know if you're still interested, but recently I've discovered MiniCover.[Here's](https://automationrhapsody.com/net-core-code-coverage-linux-minicover/) a link to a recent article (not written by me).
Hell yeah! Looks awesome, i will be trying it out ASAP. Thanks!!
&gt; Hell yeah! Looks awesome, i will be trying it out ASAP. &gt; Thanks!! 
Hell yeah! Looks awesome, i will be trying it out ASAP. Thanks!!
Namespaces in code examples is something that is easy to trip up on, then trying to find the nuget package for that namespace sometimes is not 100% because that namespace may be included in a larger nuget package with a different name. 
Thanks, I will play around with it.
WPF has routed events. No need to have 100 handlers. One will be plenty.
Very cool, thanks!
nope, still using ReSharper sadly lol.
That never doesn't happen.
Ah, that's a good catch - I'll run this again for 12 hours and see if that's what's causing the crash, thanks.
Your loop is actually infinite recursion, instead of infinitely looping. If you want to repeat an action infinitely, go with something like ``` public static void DoNotifyLoop() { while (true) { if (ShouldRun()) { Console.WriteLine(...); RunJob(); } else { Console.WriteLine(...); Thread.Sleep(...); } } } ``` Then your RunJob() method should _not_ call DoNotifyLoop again
https://puu.sh/zpn9X/5e4038a47d.png
This is most likely the issue after a 12 hour run. DoNotifyLoop() is called every 5 seconds recursively. That's going to be at least 18k DoNotify deep per day. He's filling the stack.
It's an internal server error or an unhandled exception. Probably you are not running your app with `Development` environment settings and because of that `if (env.IsDevelopment())` is false. Remove this check for now and try again to see the real exception. Also to set this environment variable you will need a new file at this location:`Properties\launchSettings.json`, according to: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/environments
If you’re after patterns principles and best practices only then skip the clean coder. That’s more of a book on how to conduct yourself in a professional environment, how to say no to project managers etc. Not doubt it’s important however as a heads up it’s not a book on actual practices and principles.
You could very easily become one. Whoever says it’s difficult is a liar.
It's great that you can do it from inside of the application, but for concerns like this, I really trust properly set nginx or specialized public cloud API gateways much more. I think you might have case for throttling from application when charging different customers for different load levels for example -- when there is some business logic to apply.
Yes. Her is exploding the stack. Do the while(true).
The number of stacked calls depends on the size of locals and parameters per frame, stack "bookkeeping" (EBP, return address) and stack size. On Windows, stack size, by default, is 1MB. In a 64-bit process, stack bookkeeping will be at least 16 octets per frame (I think), so that's ~60000 calls. As for locals and parameters, every reference type on stack is 8 octets, every int 4 etc. 9 to 18000 look like a "simple" recursion with a couple locals/parameters per frame. Source: recently dealt with a crash caused by a stack overflow, they manages some 1300 recursive calls 😀
learn .netcore 2, and use vscode
okay thanks for answer, i was not sure about that i could still develope apps after support lifecycle completed! thanks a lot
thnx for advice, ill try it, just my main question was that if i could use versions which has support lifecycle completed! 
If you are making console application or class library the difference is insignificant. I recently ported a bunch of 10 year old .NET 4 apps and library to .NET Core 2 with no code modification. There is some differences in ASP.NET Core, but not significant from a learning concept standpoint. If you are comfortable with command line (cmd or powershell), you can use .NET Core 2 without Visual Studio. You would use .NET Core command line tool `dotnet` for project creation, package management, and building. `dotnet` is very similar to the build tools in other programming languages. In that workflow, you can use the open source editor VS Code for editing your code. This would actually help you learn build/deploy aspects.
&gt; my computer can not run visual studio 2017 What computer do you have lol?
question, why vscode and not vs2017 community?
really bad pc, 2gb ram, geforce gt8600, athlon 64 2x processor 5000+, win7 sp1... yeah i know it is really bad but my family can not afford new pc and i can not start working because i am still in high school 
Consider using VSCode and command line tools so you aren’t stuck in the past due to the limitations of your PC. It’ll actually make you a stronger developer.
Thought this was dead
I don't think you're missing anything. I can't see how they could really work together except maybe to seperate off the sites and run them on different urls with some common layout and CSS. But more hassle than it's worth.
Thanks again everyone for pointing me in the right direction. I got it working with some tweaks as needed. Here is what I ended with. (Went with mouse up instead of Mouse Enter) private void p1_ailpoisoned_MouseUp(object sender, MouseButtonEventArgs e) { Image i = e.OriginalSource as Image; { if (i.Opacity == 0.2) { i.Opacity = 1; } else { i.Opacity = 0.2; } }; } Then I put p1_ailpoisoned in the other images mouseup events.
Stop worrying about support lifecycles and start learning to think about problems and solutions, because that is what engineers do. Which programming language or framework you choose is irrelevant.
Stop worrying about support lifecycles and start learning to think about problems and solutions, because that is what engineers do. Which programming language or framework you choose is irrelevant.
According to op he can't run vs2017
I should learn to read I guess. my bad.
What about Python 3.x support? 
actually yes, i can get it free and any other jetbrains product, but thing is that visual studio 2017 is lighter than rider :D and my pc can not even run visual studio 2017(btw it has no problem running 2015 version)
VS 2017 is significantly lighter than Rider? I’d have guessed they were about the same. They’re both “heavy” IDEs vs something like VS Code
yes they are both heavy ides but i was comparing them using thier system requirements.. rider needs min 4gb ram and vs2017 needs 2gb ram.. 
ikr, my pc is 10 years old and im just poor af xD 
https://github.com/IronLanguages/ironpython3
Thanks for the recommendation. I'm taking his C# fundamentals course now, and it's great so far.
DISCLAIMER: I am the author of this project! Hello! This is my first ever project that ended up as a package on nuget, so I am quite excited. The reason I started working on this was because I couldn't really find anything simliar for .Net Core that already existed, so I ended up developing it myself. For names I was kinda split between Delve and BudgetOData, but ended up with Delve :d It is still in a bit of an early stage so I am looking for some feedback here. Go easy on me, I am not a professional developer.. yet. 
DISCLAIMER: I am the author of this project! Hello! This is my first ever project that ended up as a package on nuget, so I am quite excited. The reason I started working on this was because I couldn't really find anything simliar for .Net Core that already existed, so I ended up developing it myself. For names I was kinda split between Delve and BudgetOData, but ended up with Delve :d It is still in a bit of an early stage so I am looking for some feedback here. Go easy on me, I am not a professional developer.. yet.
No, that's impossible.
I managed to get it to click on the location the slider is at already, all I need to do is make it click like a specific amount of pixels next to it
No, it very recently gained new developers after being dead.
I would actually even go as far say that OData is vastly superior to Delve. (Probably not doing the best job "selling" my project with this comment, but that's just how it is :d) The reason why I looked at OData and still decided to develop my own version was that it was just too complex and requires too much configuration to get it working. Delve is fairly straightforward, you add it to Asp.Net Core Mvc, build your validationmodel and you are good to go. Not as much hassle as working through the huge documentation that OData provides (Which I personally found quite confusing). So what I am basically trying to say, if you are working on a huge enterprise application, Delve is probably not the way to go. OData is far more mature and provides a lot more capabilities. However if you just want to get started quickly with your API and don't need all the features of OData then Delve might just be the better option.
what's wrong with normal python?
This is 'normal' Python. You mean the standard runtime C Python that most people use. There's nothing wrong with it, though it is a bit slow. But it's also good to have other .NET languages to choose from.
ok thanks
yes i already have downloaded vs code and trying to play around it using dotnet cli tools.. i hope it will work like visual studio 😂 
I looked at Delve configuration. OData looks easier to me, plus it has support for PATCH/POST/PUT.
We use Quartz. I haven't personally used it in a project yet, but I will soon
What does serverless mean? I mean, its deployed on a server in AWS. 
Thanks!
Cool
you can conditionally route to different servers based on the request location. all standard requests would go to your PHP site, while others to a specific route would be passed to core nginx example: location /core { proxy_pass 127.0.0.1:xxxx; #your net core 2 server } location / { root /var/www/phpwebsite; fastcgi_pass unix:/var/run/php/php7.1-fpm.sock; } you could also run the .net core server on a subdomain which follows the same principles but would be a bit easier to configure
Akka.net is great for this. We use akka.net in buses to track them and talk to the different systems on the bus, passenger counters etc. 
What programming background are you coming from?
We use Hangfire extensively with ASP.net MVC projects. Hasn’t failed us yet!
Experience with Python, Lua, js, html (including css) and basic knowledge of C# (enough to write basic console apps that use different data types and some knowledge of winforms). 
I came here to ask almost exactly the same except for a Web API project. I would really like to be able to avoid setting up a Windows Service. We frequently deploy, and I'm wondering, if there are bugs such as a Web API site restarting in the middle of a job running.
It is really easy to setup a simple windows service with Quartz + TopShelf. It is nearly the same effort as doing in a web project, but you don't couple your jobs to your web app and IIS.
I think this would be a problem with any tech if you are trying to run multiple sites on the same server. However, you CAN make this work if each site has its own hostname. The proxy directs requests to your site.com to the php site and it directs requests to core.site.com to your aspnet core site. Not sure how to set it up on linux but that's how you handle multiple sites on a single server. 
Hangfire. I love it! Working just dandy in a 2.0 core site I'm working on. Make sure to have the keep alive option turned on in Azure if you are deploying there though. 
Are there any benchmarks comparing this to cpython?
I'm not sure, but I feel you on the WPF issues they ignore. I gave up and switched industries.
Didn't know such library exists. Do you think it can make querying MS Dynamic easier?
I've used https://www.visualcron.com on my Windows server for years without issue. I like it because it's separate from my applications and it also allows me to run basically any task: SQL, system, .net, node, file copying, etc
Quartz. Their 2.x APIs were strange (too verbose) but 3.x seem cleaner. Hangfire was too basic for us.
It's better, also uses less CPU (much better GPU rendering btw). You can also easily use Windows.UI.Composition, so any animation and even other things, runs directly on dwm.exe, not on your app thread. Startup time it's also way better. 
No bugs but a feature. Any long running thread in IIS can be terminated at will. If you deploy during a long running task there's a good chance it will be terminated. Definitely a good move to go to a Windows service or something similar for long running tasks. 
I'd like to mention that we will deploy to Unix platform, so Windows Service is not an option.
I am using hang fire and it has good dashboard and all the jobs details
https://github.com/dodyg/practical-aspnetcore
If you are just starting, I don’t think it’s that useful for you to learn the view part of MVC (the razor pages etc.), that’s rarely used nowadays. Instead most applications implement a REST API and use a frontamd framework like Angular, React, Vue or Elm on the frontend. Once you understand the basics of REST api, try picking up Angular. It’s underlying ideas are closest to winforms/wpf than the other frontend frameworks I mentioned.
Using hangfire. Their in memory back end and SQLite back end is broken. Never had problem with Postgres. I am not using it for very important stuff though.
Not sure it helps but long time ago I wrote https://www.codeproject.com/Articles/338036/BrowserAutomationCrawler?fid=1689685&amp;df=90&amp;mpp=25&amp;prof=True&amp;sort=Position&amp;view=Normal&amp;spc=Relaxed&amp;fr=26
From what I have discovered (working on a UWP project at home and previously at work to working on a WPF project), UWP seems to run alot smoother. Animations are fast and don't lag (as usual don't block the UI thead). {x:Bind} is absolutely amazing, death to {Binding}! :P .NET Native can be, errr interesting - it can break sometimes with no real helpful error - currently dealing with that. Reflection and some other APIs are broken, but oh man. The performance increase and memory usage decrease is awesome!
If you want something really simple that works, go for FluentScheduler https://github.com/fluentscheduler/FluentScheduler
I’ve used Hangfire for our Web API 2 project. We use MySQL db and EF for our Web API, but MongoDB for hangfire services (MySQL support for Hangfire is not really maintained). We use Windows Services to host a Hangfire Service. There are nice guides on the Hangfire website about how to do this.
I don't agree many people using new asp.net core so you can learn that. But all depend on your interest 
ASP.NET Core, of course, but mainly the "API part" from what I've seen. Not Pages, and Razor. I am not saying that the OP won't ever use it, but investing the time that would be spent learning in in a frontend frameworks instead, is a much better use of his time. Once he has that down, it's very easy to go back to Razor and learn it. 
I think MS dynamics already has OData built in https://msdn.microsoft.com/en-us/library/mt593051.aspx
I used Hangfire for a project recently with MySQL as our database engine on asp.net core. I used pomelo nuget package to connect to the database using EF code first approach, and so far, everything have been running very well. I mostly use Hangfire to store recurring jobs task on this project. Always On mode should be enable if you're using IIS as your Web server, or if your app have a good traffic, then you would not need to worry about it.
I've used both, though on framework, not core, and not exactly for the same purposes so take that fwiw. In my experience if you step outside the standard usage patterns at all, hangfire can get unpleasant to work with and documentation non existent. Unix + MySQL is a very non standard usage patterns ( does core even support Unix), especially if that combo means zOS.
Using quartz. Hangfire looked great but didn't seem to support intervals in seconds
Using WebBrowser, you can interact with the DOM to do most things. Search for HtmlDocument or HtmlElement to start. There's plenty of info out there.
Yes, I know. The problem is, I cannot make it simulate *dragging* the slider, so I need to find out how to click on a specific location on the slider. I mean I might not even need to use any code at all that even interacts with the website, I could just do it so it just simulates a mouse click on a coordinate in the webbrowser, problem is I cannot figure out how as the webbrowser will not be visible when the app is finished
Guessing now, but must the action be dynamic? If all that gets back from a control is a value, then you might be set that value directly. If the control is dynamic then maybe you can simulate action using javascript?
Since you have a programming background you must have the logic down? For me tutorials are helpful however sometimes I find they hold my hand too much so I don't pick up what is going on. I personally seem to learn best by starting a silly little project. Maybe a simple game, or simple utility. Sometimes I take an existing sample project and expand on it for fun. Or I just dive in and set out to obtain my original goal. I find it difficult to simply learn by reading without something I want to do. I seem to learn more by researching something I want to accomplish. Weirdly I always seem to learn more in general unrelated to what I am researching. Maybe if you would like to share an example of something you want to accomplish it can be used as a teaching tool to help you along?
This is exactly what I was thinking but it's just the matter of getting enough of a basic understanding to begin working on my own projects, you reckon the documentation would be any use?
I use the documentation as a reference or supplement. Unless you meant something else? Sometimes I don't worry about understanding a particular aspect immediately. I just roll with it and then something clicks further in with an aha! moment.
I have experience with this, both in terms of console apps/windows services as well as ASP.NET apps. I use FluentScheduler a lot, because Quartz.NET is just a total abomination with a Java style API and XML hell, as well as a dependency on Common.Logging which is just dreadful as well. One problem that a lot of people don't realise initially is that long-running processes *should not* be run within a web context. You've no idea if the application pool will be recycled, the ASP.NET pipeline times out or the HTTP request times out. Instead for *reliable* long-running processes you need to have said tasks running on something guaranteed to run for a long time. This could be a Windows Service, console app, whatever. This is the place I use FluentScheduler. &gt; mission-critical On a related note, perhaps consider something like a message queue that sits between your web front end and the services will work well for you. This is the approach I take when I need scalability, decoupled architectures, and guaranteed delivery. RabbitMQ is one option.
Yeah, I have a little experience with Windows Services, and feel safe using it. The thing is that we have a Web API project that needs some sort of polling functionality. It would save us a bit of time, if we could poll directly from the Web API project instead of having to write endpoints that a Windows Service uses, and it would also allow us to just add it to the project, and not have to spend time changing the Jenkins build and deploy process. Say if we use a Windows Service, we need to do some thing like this: 1. Windows Service polls one API, and receives data from it. 2. Windows Service sends data back to our Web API through an endpoint It also needs to happen the other way around too, where the Windows Service polls for data from our Web API and send it to another API. It would be much easier, if we could replace those to scenarios by adding two Quartz jobs to the Web API that handle the querying for and posting of data. It would also reduce complexity a bit in the sense that we won't expand the project to include yet another piece of software that needs to be built and maintained. My concerns are mostly about any unforeseen pitfalls that might exist, when using Quartz.NET. One of my fears, as mentioned above, is that we have some sort of weird problem, where the Web API shuts down in the middle of a job or something like it. But that could of course also happen with a Windows Service, so it probably doesn't make any sense at all.
I uncommented "if (env.IsDevelopment)" and the rest of that code and also added launchSettings.json in that location and it still returns blank... :(.
Cheers :-)
I've run into a similar situation in the past. I tried out OData, and got it to work, but found that the client side APIs were lacking. Basically, I had to roll my own client API. I ended up removing it and writing my own generic 'pagination' model. That said, take a look at this https://github.com/Biarity/Sieve/
Exactly what I meant, followed your previous advice and I think I'm understanding the file structure a bit more and getting the hang of it and it's not quite as bad as I thought. Thanks!
Hangfire here with a SQL back end running in parallel on several redundant web servers, never had a problem.
Take a look at service stacks autoquery. It is awesome but service stack is commercial so keep that in mind. I've integrated it into kendo grids, vuetify datatables and it rocks.
You could run either in a CLI app as a Linux service as well. Although frankly, I wouldn't try until Microsoft.Extensions.Hosting 2.1, which includes a generic host interface that [will make this much, much better](https://jmezach.github.io/2017/10/29/having-fun-with-the-.net-core-generic-host/) with Core. I have a handful of Windows services I am porting over, but am blocked by this release too.
Hello, Based on the comments until now i understand that you trying to learn asp.net core 2. When i started my journey on asp.net i started with [Professional ASP.NET MVC 5](https://www.amazon.com/Professional-ASP-NET-MVC-Jon-Galloway/dp/1118794753) great book. For Asp.net Core i started with: [Pro ASP.NET Core MVC](https://www.amazon.com/Pro-ASP-NET-Core-ADAM-FREEMAN/dp/1484203984/) its a nice book for asp.net core 1. for asp.net core 2 i would suggest the [Pro ASP.NET Core MVC 2](https://www.amazon.com/Pro-ASP-NET-Core-MVC-2/dp/148423149X/) but with a slight hesitation because asp.net core 2 at the time of the publishing was still new. Also this [MVA course](https://mva.microsoft.com/en-US/training-courses/aspnet-core-beginner-18153?l=VM5gy36dE_6611787171) could help you. If you need more info and tutorials - courses. Comment bellow and i will try to help you find the best courses for you. Thanks.
Thanks I’ll check it out! 
I need a .NET core library. 
We use hangfire in production alongside webAPI projects, which isn't quite MVC, but close enough. Hangfire has a lot of quirks but it can be configured to spit out and log all sorts of info for you, and it is extremely easy to get started with. Lots of good documentation. You just tie it to a database, publish it, and it will set up the tables and everything for you... then it checks the database for jobs and runs them as needed. Another nice thing is it scales really well, as it does a good job dividing jobs up between servers as long as they're pointed at the same hangfire database.
I have never in my life used this nor do I know any developers that do.
Most people at the company I work for are self taught. We are working with C# and VB (don’t ask) middleware with JS/React front end. Based out of NJ, have one guy that commutes from Staten every day.
I think the key is to find projects that interest you personally. So take your time, look around for something you like. A good place to start is on the [.NET Foundation website](http://dotnetfoundation.org/), as they list the projects which fall under them. That does not include all .NET open source projects, but many of those are fairly high profile and may give you nice exposure - along with doing a good deed ;)
Use Hangfire, very nice library
The simplest way is to use a transactional email service. Sendgrid seems to have some sort of partnership with azure so that would be the logical choice. As for scheduling, you could look at [Azure scheduler](https://azure.microsoft.com/en-us/services/scheduler/) or a web job or azure functions. 
You could use a [WebRole!](https://docs.microsoft.com/en-us/azure/app-service/web-sites-create-web-jobs) to schedule when the emails are sent. You still have to write the code that actually sends the emails, and for this you have a couple of options, including a basic SMTP client, or using a paid service like Mandrill or SendGrid (there are actually a ton of providers). Hope this helps.
It varies depending on area. In a major city if you are flexible on price and can flub some references then it shouldn't be too difficult. If you are in the middle of nowhere then freelance might be easier.
Hello, you can create simple console app which sends email to user using SMTP services and you can add that in as Web Job if you are using azure. Else you can use scheduler and add your console app to scheduler which runs every week at one time. 
I can't necessarily give you advice on getting jobs in NYC, but I can give you some advice on your situation in general: Just make sure you have somewhat of a portfolio that you can show during job interviews. Github or bitbucket is a good place. Do some pet projects, but try to do them well, or contribute to some community projects, however small the contribution. Showing them what you can do and have done is often worth far more than a piece of paper that says you can. I've interviewed several self taught developers, and for me, the self taught without having a degree was never a problem. In fact, it showed a tremendous amount of effort and dedication to get there all by yourself. In our line of work, hands on experience is a lot more valuable than knowing all the theories but not being able to apply them. However, the interviews were for positions to work on the software of the company itself. So try to aim for those in the beginning. Usually when you apply at consultancy or bespoke software companies, and don't have a lot of experience speaking for you, they are hell bent on you having a degree because they need to "sell" you to the customers. The more years of experience you have, the more likely they are to oversee your lack of a degree, but the bigger companies might still not show interest. Lastly, make sure you read some books about the theories behind software, if you haven't already. These enable you to actively contribute to discussions about the software, and also to see the bigger picture of things. Some areas you should definitely investigate if you want to work on corporate software: - automated testing - design patterns - solid, dry and yagni principles - overall architecture of applications - continually integrating and deploying (optional, bigger companies usually have dedicated people for this) Now, this is the opinion of a dutch developer, so I'm not sure if it applies to your area, but it's based on the fact that I myself never finished my computer science degree and still wound up as a senior developer at one of the biggest online retailers in the country. My wife is actually a self taught developer herself and also worked at that company. I hope this helps, and if you have any questions, shoot me a message! 
Do you have a bachelor's degree? I have gotten along just fine as a software engineer with a degree in mechanical engineering. All my CS knowledge (except a little Matlab) is self taught. In every interview I have been in my degree comes up, I definitely get the feeling they wouldn't be looking at me as closely if I didn't have one.
My background is similar to yours. I have a masters in structural engineering from a very good university. I also learned Matlab in college, haha. 
You should be GTG then for at least a junior position. Definitely pick up the infrastructure stuff like I said above. There's lots more that goes into being a good developer than just writing code. Having a project on github, hosted in a Kubernetes/Swarm/ServiceFabric cluster, with full CI/CD including unit testing and automated UI testing, will blow them away. So much better than just "Hey, look at this cool app that I wrote and threw up on github"
Pretty convenient way to share server side validation logic to the client side without making a round trip.
You mentioned you have a degree, that and your existing knowledge should be enough for anyone. 
Some of you may be stuck maintaining terrible legacy batch-based build scripts. In that case, this Emacs-module can help make the job easier :)
I highly recommend [ASP.NET (MVC) 5 CORE: Web Development, JQuery &amp; RESTful API](http://www.programmingcourses.info/2018/02/aspnet-mvc-5-core-web-development.html).
Kind of hard to help because it sounds like you are just missing something small. To answer your question about requiring the user to re-login when the token expires, this is what a refresh token is for. When a user makes an ajax call with an expired token, the server will respond with an unauthorized error. When this happens you make an ajax call to get a new auth token using the refresh token. You then repeat the last failed request so it seems like nothing happened to the user except a little extra delay. If you are using angular then its easy add this check using interceptors.
If your email is accessible from outside of your domain [Azure Functions!](https://azure.microsoft.com/en-us/services/functions/) would be perfect for this. Alternatively, if you don't have access to your email server from azure you could use something live [postmark!](https://postmarkapp.com/). You could even create an azure function for generating a new number each week. With Azure functions, there are multiple ways to trigger the function cron like time trigger being one of them. They are basically small c# scripts running in Azure.
Much better than what we have had before. This is closer to the NPM docs for a package which I think is helpful. Too many times you find a package, hit the "Project URL" and get taken to not that git repo, dotnet included. I would suggest the following changes. "Install the NuGet Package", should read "Install the System.Data.SqlClient" and it is hyperlinked to the nuget feed. My brain first thought that link was to talk more about NuGet packages. I like the "Include these 'using' directives...", I do wish there was a way to make them standout more from typical comment syntax to "You need this or this code sample won't work". Maybe a "Namespaces" header or "Package Namespaces" to make it more specific to the package? Can this link up with the nuget package? The nuget package's link goes to just "https://dot.net/" while this doc is the best landing page for that package. Maybe one of those nice redirect for like "https://dot.net/package/System.Data.SqlClient". This would keep the branding, and drop a user to the most important page for that package. I really appreciate and like how engaged with the community the dotnet team(s) has been since core started. I know some of these seem maybe too detailed or picky. I have worked with a lot of interns and junior devs and remember what trips them up when learning something new. Side note, if you did un-comment out the using statements I'm sure someone would find a github project where the namespaces are in the middle of a function :) 
I have been filling my `View()` with the model, empty or not, since day one. I would chalk it up to better training, except that I never received any -- completely self-taught.
I know
Hey thanks for the reply. You’re right I missed something simple, I found out how to retrieve the username from the token (I’ll post the code when I’m not on my mobile). Thanks for letting me know about a refresh token I’ll look into it :) I’m keeping the app as barebones HTML / CSS / JS (with jquery) for learning purposes so I’m avoiding any binding frameworks for now. 
The legacy system is not being replaced, only supplemented.
Re-read my last paragraph and link. You should wrap the legacy system so that it available as a third party authentication provider.
As promised, my code: After I sent the token along with my AJAX requests I can get the id of the logged in user by first including the namespace using Microsoft.AspNet.Identity; Then in any action of my API controller I could use var id = User.Identity.GetUserId(); Next I need to look at * Refresh Tokens * Safe storage Thanks /u/neilg for sort of pointing me in the right direction (I came across this whilst searching for refresh tokens :) )
Checked your repository and I like the looks of it. However I see that you are targetting .NET Framework 4.5. If for some reason I need to target the full .NET Framework I created an ASP.NET Core 2.0 application and change the &lt;TargetFramework&gt; attribute to something like net471. I really like the clean look of the .net core project structure.
Yup, that code looks right. The request passed in a JWT token which contains the users identity, so .net doesn't try to fill in anything else. You can also pass/read more via the 'Claims' array in User.Identity, if there is any.
Yeah, this was made years ago and I just quickly updated it to give you an example. I could recreated this in core, but it would look basically identical as far as file structure goes. The only difference is the project files/framework reference.
Thank you very much. This is far easier on the eyes to interpret. I'll check it out more thoroughly tomorrow. Much appreciated!
thanks for the response. I removed it from the layout and still no change 
Started reading the post title and forgot what subreddit this was. I personally prefer the Brazilian way of optimising references.
What's that, just removing everything and adding back one by one? Assuming that's what you mean, that can work for a handful of projects, but when you have 100+ that can get pretty tedious.
[removed]
I know it's popular with most, but I don't care for the whole "organize by category of thing" approach most people go for. I prefer my folders and namespaces to contain cohesive classes that tend to be used together; i.e. a controller and the models that go with it would be in the same place.
Looks like the lumen template defines elements used by datatables differently than base bootstrap does. To fix this and keep the template will probably take a lot of work finding relevant differences between vanilla bootstrap and lumen.
&gt;unnecessary dependencies Lmao
Yeah me too. I do something like this for web api, for MVC I do the same but under areas. Targets/Thing/ThingController.cs (just http scaffold) Targets/Thing/Models.cs (view models and action dtos) Targets/Thing/Service.cs (the logic) Targets/Thing/List/ThingListController.cs Targets/Thing/List/Models.cs ... etc Only thing I'd want to do is also get the views in the same folders, but haven't figured out a way to do it yet. 
Here is a way to organize by "Features": https://github.com/OdeToCode/AddFeatureFolders
The main disadvantages I see with this are namespaces and layering. By default the fully qualified names will be e.g. app.thing.Model, app.thing.Controller, rather than app.Models.ThingModel, app.Controllers.ThingController. You could manually change them, but then they don't match then directory structure, which I'm quite in favour of (and in our team's case ReSharper enforces). While I can see (V)MVC classes sitting in the same folder because I don't often separate them, I would prefer my services and their interfaces separate so that if we want to reuse the code somewhere else (e.g. we create a web service to accompany our MVC site), then we can quickly move the Service/Contracts folder into a new project, have the other two projects reference it, and boom, we have shared code. I can see the argument for it, but I think it'd give me more grief, and other devs picking it up would probably wonder why it didn't confirm to [unofficial?] standards.
I don't see how your method improves separation of concerns. The namespaces that things live in doesn't change their composition or responsibility.
That's a problem I've been facing. Working on a 200+ projects solution. After successive refactorings, some dependencies are useless and can cause troubles. Would it work on NET Framework 4.0? Does it use the solution file? The project folders are not stored in the usual visual studio structure. I should try that out today! 
Definitely a very good point. Although I have to say, I learned more about corporate etiquette and skills from my internship at a company than the years at college. But that was maybe because my college was shitty anyway and refused to teach or accept any other language than Java. Unfortunately, doing an internship is hard for someone who is already in his "working" years, since money still needs to come in. Fortunately, if I'm not mistaken, OP already has work experience in other fields. Lots of those skills are transferable, since most lines of work deal with working on projects, working in teams, communicating within those teams etc. In my opinion, real life experience with those skills is much more applicable than the theory and slight practical brushing with it from college. But in the end, this is easy for us to say. It all depends on finding a company that is willing to give you that chance and spend the effort and resources to get you acclimated in their corporate culture. 
I haven't explicitly tried it on a .NET Framework 4.0 project, but I don't see why it wouldn't work. It doesn't use the sln; it just scrapes for *.*proj files. It really just uses MsBuild to parse the projects, and potentially do a design-time build if needed, and compares what it finds with what it finds in the already-built assembly metadata. I'm sure there are edge cases and ease-of-use issues, so let me know how it goes and if you run into any snags.
Thanks a lot for the reply. It does indeed work and it spotted project references that could be removed. However, I had the following crash, so the report had only scanned a dozen projects: Unhandled Exception: System.Xml.XmlException: System does not support 'Windows-1252' encoding. Line 1, position 31. ---&gt; System.ArgumentException: 'Wi ndows-1252' is not a supported encoding name. For information on defining a custom encoding, see the documentation for the Encoding.RegisterProvider m ethod. Parameter name: name at System.Globalization.EncodingTable.internalGetCodePageFromName(String name) at System.Globalization.EncodingTable.GetCodePageFromName(String name) at System.Text.Encoding.GetEncoding(String name) at System.Xml.XmlTextReaderImpl.CheckEncoding(String newEncodingName) --- End of inner exception stack trace --- at System.Xml.XmlTextReaderImpl.Throw(Exception e) at System.Xml.XmlTextReaderImpl.Throw(String res, String[] args, Exception innerException) at System.Xml.XmlTextReaderImpl.Throw(String res, String arg, Exception innerException) at System.Xml.XmlTextReaderImpl.CheckEncoding(String newEncodingName) at System.Xml.XmlTextReaderImpl.ParseXmlDeclaration(Boolean isTextDecl) at System.Xml.XmlTextReaderImpl.Read() at System.Xml.Linq.XDocument.Load(XmlReader reader, LoadOptions options) at System.Xml.Linq.XDocument.Load(String uri, LoadOptions options) at Buildalyzer.AnalyzerManager.GetProject(String projectFilePath) at ReferenceReducer.Project.Create(AnalyzerManager manager, Options options, String projectFile) at ReferenceReducer.Project.GetProject(AnalyzerManager manager, Options options, String projectFile) in C:\Users\David\Code\ReferenceTrimmer\src\Pr oject.cs:line 39 at ReferenceReducer.Program.Run(Options options) in C:\Users\David\Code\ReferenceTrimmer\src\Program.cs:line 48 at CommandLine.ParserResultExtensions.WithParsed[T](ParserResult`1 result, Action`1 action) at ReferenceReducer.Program.Main(String[] args) in C:\Users\David\Code\ReferenceTrimmer\src\Program.cs:line 20 
Looks great! Thanks for doing this hard work
Thanks, hope it's useful
Anyone know how this compares to Xamarin.Forms?
Nice work. I hope we see more of this - C# is undoubtably a nicer language than Java but it's well behind when it comes to available OSS libraries. 
wait.. calling `View()` without a model invokes a NRE under the hood?! Wtf, MS?
I believe that it's scheduled to be fixed in Core 2.2 or something?
I'm not sure how the IIS Application Initialization actually makes it faster? It's prettier sure because you can show a loading splash page but it's not actually loading the real site any faster?
Has anyone attempted to use UWP for an internal tool at their place of work? What has user acceptance been like? Is sideloading the app onto everyones machines a nightmare? Genuinely curious if UWP could replace WPF in this scenario.
It is
Unsure what you ar etalking about here. IIS Application Initialization "warms up" application pools by firing "fake" requests that keeps the application primed and out of hibernation. https://docs.microsoft.com/en-us/iis/get-started/whats-new-in-iis-8/iis-80-application-initialization 
If the app pool recycles while under load it won't make it warm up any faster though?
Correct
Sorry, I was mistaking. You can find the answer to your question under the heading "Configuring overlapped process recycling" on the page I already linked to.
My previous answer was mistaking, so I deleted it. You can find the answer to your question under the heading "Configuring overlapped process recycling" on the page I already linked to.
One for each use of the null model I think. I got in the habit of passing emtpy models because I use a lot of subviews, if the child property is null and you try to pass it to the subview you can get weird errors.
I'm baffled as to why it was so slow, I don't think I've anything seen near that slow. (Maybe a webforms site with batch compile enabled for ascx files) Are there 1000s of view files?
Nice! Hope to have time to kick the tires later this week.
Never heard of Paket, but at a glance it seems to only deal with NuGet, not references or project references.
It looks like the library I'm using to help with the MsBuild parsing (Buildalyzer) does some initial xml parsing to detect if it's a traditional MsBuild project or a new Sdk-style one. That xml parsing seems to be failing because of the encoding you're using. I'll report this to the Buildalyzer folks and maybe shoot a PR their way if I can figure it out myself. A mitigation on your end could be ro re-encode your proj files as UTF-8 or something.
[removed]
My bet is doing from Debug to Release at least halved the time.
I see. Change the encoding of the csproj files. Thanks a lot for the feedback! 
Most websites use a parameter after the file-extension like so: `www.website.url/content/javascript.js?verion=12345` Version can be the date in a funny format, the actual version or a random string but keep in mind that this kills performance a bit more than you probably desire. Your way and the method mentioned above are both dirty. You should instead use cache control headers or opt for a service worker to handle your cache busting. If you do actually want to do it your way it's not too hard to set up your own MVC helper that just checks the folder and returns the latest file there. Doing it for every page load absolutely tanks performance of your app (like seriously don't even consider it, disk IO sucks ass). You should set up a mechanism that checks periodically or is triggered by the bundler and keeps the latest name in a static var. 
I'd like to throw a completely different suggestion in and say that it's possible your recurring background job would be better suited as an aws lambda or an Azure function (take your pick). Of course that does depend on your use case, but we use Azure functions extensively and they're incredibly cheap to run (pennies a month). Particularly if your jobs are the "polling" kind (i.e. wake up, check for condition x, go to sleep otherwise) then you should definitely consider an event bus instead. Far more efficient and decouples your business logic in a nice, scaleable way.
Ive experienced crazy long startup times with having a even 100s of views. Especially when running as an Azure WebApp where the disk IO is pretty slow. 
Instead of using a hash on the filename. suffixing the filename with the assembly version. Since the file only changes when we change the assembly version. That makes its easy to figure out what the filename is within razor
There's no need if you're using that. Just use my first example. `www.website.url/content/javascript.js?version=[assembly version]`
The only time I've not been able to find a usable C# equivalent was during the dnx -&gt; core 1.0. It seemed like a lot of companies were hesistant to write core versions until things settled down. 
It would make this a lot more useful as an insight.
I don't fully agree it breaks SRP, but I do 100% know exactly what you are getting at, because it's usually a massive red flag that a whole bunch of horror is going on including breaking SRP. I do not like nor use the partial keyword, because it's a complete code smell. If your classes are so fucking big, learn to write clean code and refactor it. I've only worked at one place that insisted on using it, while also using #regions, classes with thousands of lines (that was their actual goal as well), a ban on interfaces, doing the service locator anti-pattern, abusing DI, forcing people to use `System.Boolean` etc instead of just `bool`... Bit of a rant but basically if I see people seriously using partial or region I will immediately question their technical competence and "experience". Basically, don't be like that place I worked at and don't use partial... or region! It's not hard.
That hasn't been my experience. You're not seriously disputing the OSS ecosystem around the JVM is superior to .NET?
I understand it’s not relevant to the original question but I figured I’d ask: can you elaborate on what you mean by “abusing DI”? Just curious. 
Partial is useful when you deal with generated code. The code generator can create what it needs to for a class while your logic is in a separate file and will not interfere with any of the code generation and vice versa.
I've worked with a guy who genuinely thought using partial classes was a good way to abstract his design and separate his concerns. He had about 30 different MainForm.*.vb files, most of which were several thousand lines long. Not including the designer files. It's an utter nightmare and should be discouraged. HOWEVER. There is one great use-case for partial classes - modifying machine-generated code. For example, if you generate a client class from a swagger endpoint, you might want to extend that class with some utility method or whatever. You could just edit the generated code file, but if the endpoint updates and you regenerate it, you'll lose all your changes. You could create extension methods, but these are currently limited (a newer C# version will allow you to extend more things, like properties). You could also create a child class that inherits from this base class, but that has its own set of issues and might be brittle if the endpoint changes a lot. Creating a *partial* class means you can extend it and be safe in the knowledge that some autogenerator isn't going to overwrite it later. It's exactly what the Winforms designer does. That's probably the best use-case I've seen for partial classes.
Yes, generated code is the only valid case where I think it's useful (DB schema, Form designers etc.).
It's OK. They didn't understand why DI is a thing or how to use it properly. They used the service locator pattern for everything, they passed around a giant global collection of services and injected that into everything, and then a class would choose from that what it needed. Then they threw in Func&lt;1,2&gt; for the fun of it. It's like they'd only read about DI once after reading a very crappy article on it. It led to code that was impossible to unit test under isolation (they decided they didn't need to unit test lmao), it was just a total disaster. Hope that answers your question!
In the argument that the user is making in your link - it's not the partial keyword that's breaking the SRP it's the fact that the class had a lot of responsibilities to begin with. The partial keyword is just a convenience; syntactical sugar. As has been mentioned in these comments, it's very nice when using a code generation tool.
What exactly are you trying to do with your programs? I'm in charge of 2 in house suites for my company. One is a home brew customer/order/production management suite and the other is sales analytics, HR, payroll, etc. In that respect I've only needed mailkit, octokit, stypecop, Twilio, telerik pdf generator shit, newtonsoft, and some csv parsing tools in dotnet core. For a while there wasnt a great imaging solution but thats been taken care of too with ImageSharp. I can also see the use for the maps project OP posted as we have delivery trucks that will eventually get gps trackers. So I'm not suggesting any particular OSS community is better or worse. Simply that C# isn't so lacking its unusable.
At a "high level" they have a different approach. Xamarin.Forms translates everything down into native controls, which means that your application will look "native" (if you buy into that concept), but will also look different on each platform. Avalonia renders controls themselves, so the controls will not look "native", but they will look the same everywhere. Avalonia supports .NET Core, Xamarin.Forms doesn't. There are obviously other differences in terms of usage, syntax, etc. but I'm not familar enough with both of them to say. They both use XAML to describe the application.
We use some pretty bad service-locator nonsense too in places, so I feel you pain. Thanks for the reply!
I've had to use it to break things up when working with someone's COM interop nonsense that required me to keep track of 3-4 different objects (including one god object) when interacting with an SDK. There's one god object in particular I'm thinking of that has around 30 direct methods and a few hundred methods that can only be called through chained god objects (demigods?). It's also required to instantiate new objects for interacting with the API. I guess that's what happens when you combine two connections (using different protocols) into one and require that connection for everything. Even parsing, internationalization and schema stuff. Of course, that SDK library also spawns internal exceptions and directly calls `exit` because of things it did wrong, rather than notifying me and allowing me to exit cleanly. It's got inconsistent rules about when objects need to be disposed of (which can cause crashes on violation as it does partial tracking of the objects it instantiates). Also, making _good_ connections spawns several exceptions in the stack. It's good to remember that not every programming sin is voluntary. Sometimes it's just somebody doing their best to survive after being forcibly drafted into a starring role in _SAW XIII - Enterprise Development_. When it's frustrating to the level that you can't do anything without delving into either argument soup or enormous, bloated classes, partial classes can help you do a bit of organization that would otherwise be impossible.
"Simply that C# isn't so lacking its unusable." Straw man, I never said such a thing. I really don't know why you're pushing this, I haven't said anything contentious. And a lot of the stuff you cite is proprietary, and also limited to Windows.
I'm excited about Unity in 2018 too, but Unreal 4.19 will be wildly better.
My God. That sounds horrendous. Forcing a ban of interfaces... that alone would cause to rage quit on the spot.
Can you give some examples?
There are still other valid use cases. For example event-sourced Aggregate Roots (DDD). They often have dozens of event applicators that tend to cloud the code. Enough of them will even slow down VS. The knee jerk reply would be something like "Well that class should be split up because it's doing too much" which isn't necessarily correct, if an aggregate has a lot of events, you're simply going to have a lot of applicators in the root, so it's just a big class, as ugly as it is. By removing the "ownership" of those events, you'd actually be breaking the pattern. By using `partial`, you can preserve the pattern by splitting up the class without actually splitting up the class (place all applicators in a different partial). Just be sure to structure the project in a way that won't make maintenance any harder than it needs to be. I think the intended use case was probably to cover the exact scenario that you're describing, but it's a bit of a flop because the original developer would have to mark the original class as partial which no one does. Extending a DAO entities is a smell IMO. I know they put it there to allow you to do that, but that was really just to keep people from complaining about EF not allowing you to do bad data access design (extending DAO entities makes it no longer a DAO entity or impure, no?). So I try not to use that as an example, but that's just an opinion. I think this question can potentially fall into a category similar to those discussions around `#region` being a design smell. I think it's a silly statement. Using the tool isn't a design smell, but using it to cover up bad practices would be. So before using something like this, I'd always stop to contemplate what it is I'm trying to accomplish and why. Ask yourself if it breaks any other best practices or patterns. If not, I don't see a problem with it. Just try to make sure any other side effects (like maintenance) are mitigated as much as possible. Also, ask your lead if it's ok. They'll probably say no. ... lol
SQL is just an API which you use to connect and interact with the database, it doesn't change anything in the DB itself. To change DB behavior look at cosmos db consistency levels.
Ahh ok thats actually alot easier. Didnt realize asp.net would ignore the parameters
While SQL sounds familiar it doesn't change NO SQL database into a relational one. Feel free to check what SQL API means for cosmos DB at https://docs.microsoft.com/en-us/azure/cosmos-db/sql-api-introduction
A new hire would constantly mess up source control and overwrite fixes with his old code. After 3 months of complaining about it, the project manager gave me a bad performance review for not fixing bugs, told me I was the problem and to sort it out. I put everything but new guy's code into partial files, and like magic, my problem went away. New guy continued to have issues with other people. Best practice? Hell no. Not even a good solution, but it worked and allowed me to continue working there long enough to find another job.
&gt;If your classes are so fucking big, learn to write clean code and refactor it. Working in WPF I always end up with uncomfortably large view models once I cram all the commands, routing and validation in there, even with all the business logic in the models. Where I work is just starting to move to .Net and I was asked to do a prototype for a replacement program since I'm the only one there with functional .net experience, so I worked my ass off to manage the view model in to a well commented and passably readable block of code, complete with single line dependency properties, simplified command routing, etc. When I got the project back from code review the architecture consultant and senior PA expanded and unrolled everything because they're used to writing massive blocks of code in C for our back end system and "these single line functions are confusing." public int MyProperty { get { return _myProperty; } set { _myProperty = value; OnPropertyUpdate(); } } This (reiterated for a dozen or so bindable properties), apparently each needed to be expanded into a full block of newlined braces "for readability." I was tempted to partial the dependency property definition block out to another file "for readability" just out of spite.
&gt; The partial keyword is just a convenience; syntactical sugar. This. I don't know why no one is making this point. It's the EXACT SAME CLASS. Physically, and conceptually the same.
&gt; The knee jerk reply would be something like "Well that class should be split up because it's doing too much" which isn't necessarily correct, if an aggregate has a lot of events, you're simply going to have a lot of applicators in the root, so it's just a big class, as ugly as it is. By removing the "ownership" of those events, you'd actually be breaking the pattern. By using partial, you can preserve the pattern by splitting up the class without actually splitting up the class (place all applicators in a different partial). Just be sure to structure the project in a way that won't make maintenance any harder than it needs to be. It sounds like you're making excuses for what seems to be very smelly code. If you need to split up a class definition in multiple files because they're "too large", that screams code smell to me and definitely violates SRP. Splitting up a "god object" doesn't change the fact that it's still a "god object". &gt; I think the intended use case was probably to cover the exact scenario that you're describing, but it's a bit of a flop because the original developer would have to mark the original class as partial which no one does. Extending a DAO entities is a smell IMO. I know they put it there to allow you to do that, but that was really just to keep people from complaining about EF not allowing you to do bad data access design (extending DAO entities makes it no longer a DAO entity or impure, no?). By original developer, do you mean the developer of the code generation tool? If so, most code generation tools I've used take advantage of `partial` classes. Also, I would tend to agree that adding onto a DAO entity definition is a bit of a smell and would encourage keeping them as slim as possible. &gt; I think this question can potentially fall into a category similar to those discussions around #region being a design smell. I think it's a silly statement. `#region`'s are absolutely a code smell. Not silly.
&gt; if I see people seriously using partial or region I will immediately question their technical competence and "experience". This mentality is not objective. I'd suggest inserting at least one more step in there which is actually looking at what's in front of you. For example, some classes are just big. That should raise an eyebrow but not guide you to immediately questioning someone's technical competence. This is particularly true in DDD. So long as every method in the class belongs in that class and nowhere else, then there's no violation. I realize that more often than not, SRP has been shit on in a large class, but your described mentality on the subject is a sign of being jaded. In an *appropriately* large class, regions are not a smell. They are made for exactly this situation, because... some classes are just big. Finding a region *within* a method would be the only real thing that concerns me. I think it's important to reiterate that a "code smell" is a smell, not an error. It is something that should raise an eyebrow, and I think that's probably what you were really trying to say. It's the "immediately question their technical competence" mindset that I think is unnerving and unproductive. &gt; Basically, don't be like that place I worked at and don't use partial... or region! Maybe not use access modifiers, auto-properties, xml docs, interfaces... or iis express, build configurations, precompiler, or attributes. Probably shouldn't use Visual Studio either... just portable code or gtfo amirite? lol...
I would recommend you look into the [Repository Pattern](http://timschreiber.com/2015/01/14/persistence-ignorant-asp-net-identity-with-patterns-part-1/). I use a variation of that one in particular.
Totally agree. That's what my company uses them for. 
Yeah it is as bad as it sounds, I only lasted there a few months before quitting!
Glad I’m not alone! If you ever fancy reading some interesting articles, Mark Seeman has a great blog that covers DI as well as a great book on it. I learnt a lot from them!
VS has this very confusing thing that you can add to a solution that isn't an actual project, it's a "web site". It's weird. Try that out for your SPA. https://msdn.microsoft.com/en-us/library/dd547590(v=vs.110).aspx
- Crashes just as often. - Tests still don't work in CI. - Mac support still rubbish (doesn't compile without debug mode) - More abandoned experimental features (eg. box2d) - Hot reloading still broken. - Gorgeous graphics. - Blue prints are faster and debuggable. - C# support (maybe, eventually) ie. Its mostly bug fixes and stability improvements for windows users; but ironically its super unstable in preview... and really, not terribly exciting, unless you're already a user. ...in which case, not crashing (or less anyway...) is *super exciting*.
Why hasn't Microsoft bought Unity yet? Seems like the next logical step after Xamarin.
Doesn't this get horribly messy if you need to reference other silos?
It's good to see this take the approach from the create-react-app perspective. It's nice that visual studio includes a react/redux template in their project templates, but it's a huge drawback that they force typescript down your throat when you choose that project template. Typescript is starting to gain some traction in the world of react, but having 2 different type checking systems (typescript and flow) is probably going to leave some devs not choosing either path /shrug.
I used these steps for my new project: https://medium.com/@levifuller/building-an-angular-application-with-asp-net-core-in-visual-studio-2017-visualized-f4b163830eaa I’m happy with the way it works and I use Visual Studio for both server and client side work. It should be fairly straightforward to follow these steps and then port across your existing code. 
Most features can be abused, partial is often abused. 
This works for SPAs that are hosted by an ASP web application, not if I want to host them as a static site with nginx. Not sure what the best course of action here is, on the one hand hosting it as static files feels more seperated, on the other hand we couldn't use features like server-side-rendering. 
I've never been part of a team that cared about server-side rendering because if you're behind a login, you probably don't care about SEO*, and I suppose 100% of my work has been for applications not accessible to a search engine. The arrangement I described is something I have used when hosting the static content separately via IIS on Windows Server. You can have a project that pretty much just tells MSBuild to fire off Ng build. I have also used the described arrangement when hosting via Nginx. I am not certain I remember completely accurately, but I am pretty certain that Nginx was a different Docker container than the static content so that it would not be installed redundantly. I believe I used the proxy functionality in Angular to reverse proxy to the API for development and had an alternative way to build it to test it via Nginx in Docker containers before pushing to production (optimally we would have had more CI, but the team was very green on average and that was further down the priority list). In this case the angular application still could have been a separate project with msbuild calling out to npm / Ng despite not being a .Net project, though in our case, we either used Visual Studio code or manipulated the Angular project from the command line. *You could technically still have reasons for server-side rendering, but you can be pretty responsive with dynamic module loading taking the burden off the initial load, and I have run into much more trouble with server-side rendering than it seems to be worth when I don't have to care about search engines. I'm not sure on my rambling is as useful as I thought it would be when I started speech to text 10 minutes ago... I think my overall point is that I've tried a number of things, they all have pros and cons, but overall I have always had a parent folder with a folder for different projects inside that. I have never liked the parent SRC folder that Visual Studio 2015 started creating for projects with dotnet core code.
It is impossible for a framework to be a code smell, only a developer's command of said framework. Many people hate MVVM and would argue the entire thing is a bad design. But the partial keyword there makes sense IMO. It's all the same class, but classes that render UI elements are LARGE. They just are. That doesn't mean they violate SRP, it just takes a lot to render a UI. The best one can do is break it down the smallest cohesive element, but even that will be large. They separate the XAML from the code behind through partials, but they are still the same class and still focus on one concern. In Winforms, they separate the generated UI code from the codebehind. Same story with the added use case of code generation. There's simply no avoiding the size of the class, and partials help that, which is exactly my point.
Thanks for the reply. I see your points. I'll continue to stick by only using `partial` to interact with generated code and leave it at that. I'm not sure I could say anything to convince you otherwise, so I'll just let this discussion die. 
Well, to be clear, I'm not trying to convince you to use them. I'm simply defending against the notion that they're wrong to use, and asserting that like everything else, "it depends" on why you're using them. Laterally, I'm defending against the notion that large classes automatically violate SRP, which is a logical fallacy that I'm constantly battling.
&gt; Best practice? Hell no. Not even a good solution, lol I dunno. MSDN basically sites this as a use case
My current favorites: - [Performance is a Feature!](http://www.mattwarren.org/) (feed: http://mattwarren.org/atom.xml) - [The Visual Studio Blog](https://blogs.msdn.microsoft.com/visualstudio) (feed: https://blogs.msdn.microsoft.com/visualstudio/feed/) - [.NET Blog](https://blogs.msdn.microsoft.com/dotnet) (feed: https://blogs.msdn.microsoft.com/dotnet/feed/) Also those, though they are pretty slow or maybe a bit different topic: - [Jon Skeet's coding blog](https://codeblog.jonskeet.uk) (feed: http://codeblog.jonskeet.uk/feed/) - [Mono Project](http://www.mono-project.com/) (feed: http://www.mono-project.com/atom.xml)
One can easily break SRP without `partial`: `class c2: c1, i1, i2, i3...`, so... Let's not forget why it was added: to split machine-generated and "user" code for the class. So there ...
Follow dotnetkicks, get the daily email. It aggregates all .net news together. 
It’s too bad he suggests downgrading to plain http and hiding the “not https” warning.
Thanks for your thoughts, I love talking about things like that ;) Yeah, it's true, server-side-rendering would be pretty useless in our case, I did miss that and thought more along the lines of things I've always wanted to try. It seems that we both did almost the same thing. For local development we are running the ng dev server with a [`--proxy-config`](https://github.com/angular/angular-cli/blob/master/docs/documentation/stories/proxy.md) to our self-hosted API. In production, I have a docker container that only includes the output of `ng build -prod` and define my API container as an [upstream proxy](http://nginx.org/en/docs/http/ngx_http_upstream_module.html). Since I can point this also to my host machine, I can run a mixture of docker frontend + local API with debugger attached. This also helps a lot since the remote debugger within a docker container runs really slow for us. This all works very well from the command line, the issue is simply Visual Studio. Not only the project thing, but the docker tools aren't really usable. You get weird errors when trying to run the docker-compose project, as it's trying to start the dotnet executable in my angular service and generates weird docker-compose overwrites. P.S: Can you point me to any resource as to how to fire off ng build with msbuild?
Couple of nice link round ups that I like: * [C# Digest](https://csharpdigest.net/) - Nice weekly email newsletter that has five of the best articles. Though, they may appear in this subreddit before you see them in the newsletter. * [Morning Dew](https://www.alvinashcraft.com/) - Daily link round up of all things .NET.
I don't see that in article, can you quote? 
It's actually the opposite - it upgrades http calls to https so you don't have to rewrite all your URLs (if you are referring to the upgrade-insecure-requests feature).
Sorry, but what is an NRE?
ASP.NET Weekly: https://www.getrevue.co/profile/aspnetweekly
Heyyyyyy richard!
What's up, buddy?
Alvinashcraft.com
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Z2W4VIB.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dume039) 
I don't get that message (Chrome 64).
ASP.NET Identity. It is based on interfaces so you can customize your user store and what not. You can then add JWT on top of that
Can I use that on existing tables that I can't change? I was pretty sure I couldn't.
Yes basically you need a User object and a Store object. In your store object, you implement what you need. That is IUserStore&lt;UserObject&gt;. You can then inject in your controllers the ASP.NET Identity (Core) UserManager and SignInManager object and they will use your implementation Check out the signatures for IUserStore here: https://github.com/aspnet/Identity/blob/dev/src/Core/IUserStore.cs Some methods may throw an exception in your implementation because some stuff you may not be able to do
Thanks a lot for the help. I looked more into this with the information you provided. I found that I could add a claim to the JWT with whatever user information I wanted via: `var claims = new[] { new Claim(ClaimTypes.Email, user.Email) };` I passed that to the claims parameter of my `JwtSecurityToken` object and on the return request, I was able to grab the claim from `Users.Claims`.
You're right, I kind of hinted at it by saying Roslyn was smart enough to not include the unused refs in the metadata, but I didn't actually suggest using ILSpy or reflection to actually get at that metadata. I'll update the blog post in a bit. Thanks for the feedback!
+1 for C# Digest
This is my biggest suggestion. I find the podcast very motivating and informative.
Tl;dr use usings This is more of a bug fix than writing a scalable and performant api...
I'm sorry but I didn't get this message, checked on Chrome, Firefox, Edge desktop and mobile.
So much work must have gone into this one. :) But I do think it's something .NET Core needs.
I wont lie. For what seems like a pretty straightforward idea, this is a fairly lengthy article. 
Trust me, it is not as easy at its seem, especially when you have a huge website, thousands of resources along with some third party libraries. It can be only one such resource, some small almost useless image that can take your secure symbol away from address bar. Even if you are very careful, you still can't control third party libraries. I was reading somewhere recently how Disqus was loading some it's content on http. Don't have reference at the moment, but you can easily find the details on internet. You dont have much control in the situation like this. If it was easy, so many websites would jit have this issue. 
Could be pretty obvious for most of people but.. after attending NDC London this year I started to use Twitter. I notice everyone used it there.. so I just followed all the interesting guys from dotnet. Now, my twitter feed is only dotnet stuff and it's pretty easy to keep up to date with all it's going on. Specially with dotnet core. I'd say it's the best thing. 
.Net Rocks is pretty cool.. but there's 2 things on it that just annoys me. 1 - It's pretty long. I don't have long commuting to work.. and I can't work while I listen. So for me it's a bit hard to sit still and listen for an 1 hour podcast 2 - Most of it is just chit chat. I know it's got a cool vibe and relaxed but, for me would be more productive to just cut to the chase. 
I was just going to send you a snippet about claims. Put all your user details in the token as claims, then you won't need to keep retrieving the user from the database. This is a bearer token, so once you've got that token, you can store it in the client storage. If you're using Angular, you can stick token right in your Store for the session and keep using it until it expires.
Does core have the ability to set custom headers via config like .Net apps hosted in IIS?
Structuring projects is as much an art as it is a science, and you'll find a ton of ways to do it. I'm learning ASP.Net/MVC and struggled with project structure as well. I actually really like the "clean" architecture that is [mentioned here](https://docs.microsoft.com/en-us/dotnet/standard/modern-web-apps-azure-architecture/common-web-application-architectures) (in fact, that whole "book" is pretty good IMO. It's a little more advanced, but there's a [sample project](https://github.com/dotnet-architecture/eShopOnWeb) to review that helps put everything into context.
If it's running behind IIS or nginx, yes. If it's running exposed directly? No, but it's pretty easy to add such support in code.
Nice to see a post for Core + React for variety's sake, as many use Core + Angular by default.
1) Create some system for enumerating through all the windows in the app, instantiating an instance of each one, and showing it on the screen. You could use reflection to loop through all the classes defined in the app's assembly, find all the ones which inherit from Form, creating an instance of each, and showing it on the screen. Or, if you already know all the classes, you could have some helper method which just enumerates them (but if the app is large as you say, this might not be reasonable.) For taking the actual screenshot, once you have the form instantiated and shown on the screen, you just do: Bitmap b = new Bitmap(form.Width, form.Height); form.DrawToBitmap(b, new Rectangle(0, 0, Width, Height)); b.Save(form.Text + ".bmp"); This would save the caption of the form as the file name; you could also just use form.GetType().Name if you wanted the files named after the class.
Pretty good description of a few methods over at Stack Overflow: [Link thingie](https://stackoverflow.com/questions/2969321/how-can-i-do-a-screen-capture-in-windows-powershell)
I think you might be able to accomplish this with BrowserStack. 
&gt;Probably. You can use a relatively normal webconfig for the iis level, but don't quote me. What you gonna do about it
Well, I'm not validating the token manually. I'm leaving that up to the [Authorize] tag. But it's okay to use claims with the token, right?
Mods, please ban!
&gt;Mods, please ban! YOU CAN'T STOP ME
me neither Chrome Version 63.0.3239.132 (Official Build) (64-bit) 
It's in Part 1 of that person's blog.
Yes, once the JWT has been validated against the secret and passed the [Authorize] attribute (if implemented correctly) you can just use the claims populated by [Authorize]
I like using the built-in Angular2+ template since it sets the build system up for me. Minimal messing with Weback, HMR, JS test setup, ASP.NET prerendering, etc. I now have 4 projects spawned from the built-in templates. Taking a few hours to update the NPM packages manually is worth it for me since it would probably take me days to set up everything that it does. Actually learning about the javascript environment and their tools? Not for me, I just like jumping in and coding :P
Decent article but Imo I hate the generic repository pattern and even worse the predicate builder. Much easier to just use iqueuable in the domain level to query the repository and utilitie auto mappers to send back up to the ui. U will save a ton of time interacting with the Db and still allows mocking away the Db. Dbconext already implements unit of work which is often shown in examples of good architecture, it just ends up being a pointless wrapper. I realise this article did not mention uof but it annoys me when it's mentioned as good architecture
U don't need it
I'll check it out thanks, god I hate npm
Can u plz link me to the built in angular templates not sure exactly what u mean
Ah that's what I thought u were referring too. I found pain with that but since I learned to troubshoot front end. I'll give it another go. Thanks
You should update to the new (preview? at least a few days ago it was still rc2) version of the template, as it is much more in line with how the default angular setup would look like. https://docs.microsoft.com/en-us/aspnet/core/spa/angular?tabs=visual-studio
Here's a working package.json of the project I just started this morning. Make sure to update Node on your computer to the latest. When making additional changes to this file, I first run `npm run vendor-config` and then build webpack a single time to check if everything's ok. Remember that the builtin templates have two build steps, the vendor webpack and the webpack with your own code. { "name": "dkjhfksfdjhksjfdhks", "private": true, "version": "0.0.0", "scripts": { "test": "karma start ClientApp/test/karma.conf.js", "vendor-config": "webpack --config webpack.config.vendor.js" }, "devDependencies": { "@angular/animations": "5.2.6", "@angular/common": "5.2.6", "@angular/compiler": "5.2.6", "@angular/compiler-cli": "5.2.6", "@angular/core": "5.2.6", "@angular/forms": "5.2.6", "@angular/http": "5.2.6", "@angular/platform-browser": "5.2.6", "@angular/platform-browser-dynamic": "5.2.6", "@angular/platform-server": "5.2.6", "@angular/router": "5.2.6", "@aspnet/signalr-client": "1.0.0-alpha2-final", "@ngtools/webpack": "1.10.1", "@types/chai": "~4.1.2", "@types/jasmine": "~2.8.6", "@types/node": "~9.4.6", "@types/webpack-env": "~1.13.5", "angular2-router-loader": "~0.3.5", "angular2-template-loader": "~0.6.2", "aspnet-prerendering": "~3.0.1", "aspnet-webpack": "~2.0.3", "awesome-typescript-loader": "~3.4.1", "bootstrap": "3.3.7", "chai": "~4.1.2", "css": "~2.2.1", "css-loader": "~0.28.9", "event-source-polyfill": "~0.0.12", "expose-loader": "~0.7.4", "extract-text-webpack-plugin": "~3.0.2", "file-loader": "~1.1.9", "html-loader": "~0.5.5", "isomorphic-fetch": "~2.2.1", "jasmine-core": "~2.99.1", "jquery": "~3.3.1", "json-loader": "~0.5.7", "karma": "~2.0.0", "karma-chai": "~0.1.0", "karma-chrome-launcher": "~2.2.0", "karma-cli": "~1.0.1", "karma-jasmine": "~1.1.1", "karma-webpack": "~2.0.9", "less": "~2.3.1", "less-loader": "~4.0.5", "preboot": "5.1.7", "raw-loader": "~0.5.1", "reflect-metadata": "~0.1.12", "rxjs": "~5.5.6", "style-loader": "~0.20.2", "to-string-loader": "~1.1.5", "typescript": "~2.6.2", "url-loader": "~0.6.2", "webpack": "~3.11.0", "webpack-hot-middleware": "~2.21.0", "webpack-merge": "~4.1.1", "zone.js": "~0.8.20" } } 
Is the `ng serve` support only for .NET Core 2.1 and up?
Thankyou!
Awesome that's probably my problem
no it's also working with 2.0
Yes, I plan to add them, together with pros/cons/links to code smells and many other things. Right now I am implementing sharing of the best practices via email so the people will be able to spread the information within the team easily. Then it will be time to focus on the content itself. I am looking forward to that.
Great. That's exactly how I'm using it. Thanks!
Im looking forward to the new angular spa template getting released. https://www.hanselman.com/blog/ASPNETSinglePageApplicationsAngularReleaseCandidate.aspx
Agree... you'd have to constantly be second guessing if something still applies, plus why waste time learning something that has become obsolete? And to be clear, it's not that any 1.x functionality has gone away as much as **how** that functionality is tapped into. For example, some things that used to be in startup.cs have moved to program.cs ...
His course it's well worth it. Tons of great content and real world application.
Nope. Aspnet Core 2 yes. Fairly large changes and a huge amount of new features. Core 1.0 was a bit of a flop to be honest, 2.0 is awesome.
Check: https://support.microsoft.com/en-us/help/22878/windows-10-record-steps Will take a screenshot of each click/step you take.
It doesnt really matter which one you take as long as it has a lot of RAM for VS and an SSD. Everything else is negotiatable
I have a Surface Laptop, a Yoga 910 and an XPS. From a computing power perspective, they’re all more than adequate. Like you, most of my work is done at my desk where I have my monitors and mechanical keyboard. But if I am coding away from my desk, The the Surface Laptop is easily my favourite. The screen is a great size for Visual Studio, the keyboard is great and the trackpad doesn’t get in the way (I had MBPs for 10 years before this, so I totally get what you mean about the trackpad). If your budget can stretch to a 15” Surface Book 2, then that might also serve you quite well. If I was going to get a new laptop, that’s the first thing I would be looking at.
I have been looking at it, but the price tag is just way too high. In my country, I need to pay 4300 usd for the surface book 2 with a 15" screen. 
So, you do asp.net core on the mac environment right? I mean, the macbook pro is an awesome machine with lots of powers. I just can't get the thing for perform properly with a lot of issues all the time. Makes me very unproductive.
Yes, I've switched to .NET Core 2 years ago, at first started using Linux on my PC, then bought Macbook Pro. I use VS Code mostly with omnisharp and other plugins for C# and it works great, just takes some time to adjust.
And what is lots of ram? 8Gb or 16Gb?
The more the better. As much as you can afford
"Database round trips - using Dapper" There are lots of ways to solve the issue of making excessive DB calls, the article only mentions how to do it with Dapper.
Have you tried VurtualBox to see if it suffers the same?
VS Code and SQL Server Operator Studio or whatever it's called for everything on a Macbook Pro 15" if the projects you're working on are new enough and you can get away with it. Otherwise, I'd go with a Dell or Lenovo.
Go to Microsoft mva they do have a new dot net core 2 beginner course and I did take this course back when it was dotnet core 1. It was a good video training as a starter IMO
The new templates are supposed be included in asp.net core 2.1. The nuget package for the server side stuff (spa extensions ) hit 2.0 rtm yesterday or the day before....
I have 8gb on work laptop and i do vs, sql server, photoshop and lots more.... no weird problems.
Do yourself a favor and buy an XPS 13. You simply will not regret it.
Confirming this as well. To me, Core is the future regardless and if you’re starting out you’re just as well off with Core as all the features it’s missing are Windows kernel or enterprise related. Oh and it’s way faster and the debug server is much lighter to run. 
The MVA course is kind of rambling and conversational. It's broken into beginner, intermediate, and advanced sections, but it's unstructured and incomplete. If you've already done ASP.NET MVC 5 it's enough to get you going, but if you're coming into it without that background, I could easily see someone getting lost. P.S. The MVA search and course discovery is awful. I've had to use Google to find most courses.
I have 64gb if memory and an i7 on a work laptop and have a lot of battery issues... battery life anxiety is something that sucks to deal with. I cart my power brick everywhere and that’s extra weight, extra things to remember and you just aren’t as portable. I think going as low power on the cpu and moderate on the ram as possible and having a nice battery that’s still lightweight is the most ideal setup possible...
Agreed. 
VirtualBox has incredibly bad performance compared to Parallels and VMWare. 
Exactly. Same happens with the ASP.NET community standup. It’s too long and let’s be honest, I don’t really care if you got a new piece of furniture. 
&gt; 8gb What? How? I'm currently using 7.6GB of my 16GB and all I have open is: VS, SSMS, Firefox, Edge, and Outlook. God forbid I open 3 more instances of VS and try to get debuggers running. I can't survive without 16GB of RAM. 
I'm a Mac user and almost never use Parallels for .NET work (even before Core was released - I keep it primarily for a couple of really old legacy apps). Unless you have specific Windows requirements you can do everything you need with ASP.NET without using a VM. I flit between VS for Mac and VS Code (and occasionally Vim). You also have the option of Jetbrains Rider.
Your system uses as much RAM as it can, leaving enough RAM available for new process. So if you were doing exactly the same thing on a machine with 8gb, the usage would be lower.
I love my XPS 13. It packs a pretty solid punch for the size of it. It's a perfect on the go / coffee shop / boardroom laptop. My only complaint is going from dual 32" 4k monitors on my desktop to the 13" 1080 screen takes a bit of getting used to when switching between the two.
Too late! 😁
I'm sure that's true to some extent, but just 2 months ago I was using a laptop with 8GB of RAM for the same purpose and was running into memory issues multiple times per day. In my experience browsers and VS just consume huge piles of memory. I just can't recommend 8GB to anyone because it's so easy to run into issues with that small an amount.
http://avaloniaui.net
Never heard of this one. Anyone ever use it? Any good?
I use the "enterprise" equivalent of a Dell XPS 15 (Precision 5510): Intel Xeon E3 Mobile processor 32 GB RAM 1 TB NVMe SSD Works great with several VS2017 and SSMS instances running, a few in-development microservices talking to each other, etc..
I haven't used it, but it's very much in alpha.
It's in alpha but it has a lot of press coverage recently. Google it and see how poeple are using it.
You should use some sort of view model instead of putting stuff into ViewData
All the Microsoft docks are great, check them out at http://asp.net
Thanks for the feedback. The viewdata part is arbitrary part of the implementation, but you are right that this could be distracting. I will update the examples with more suitable practices.
Xamarin costs an arm and a leg, so i'll pass. GTK# seems interesting, tho it uses mono instead of the official .NET, but since Core is so underwhelming right now, Mono is acceptable 
I built some CMS functionality to plug into a client site "misc" pages. We have a base model, so I added a "CmsContent" property to the base model
I appreciate this, thanks! I think a lot of the confusion here is in the example project. I may have been to quick to put together rough examples that come across as prescribing implementation; I am just selling it badly! To address each of your points * This should not rely on ViewData Yeah this is me failing with a bad example, I will definitely update it. The core library does not depend on it at all. * Code is missing out on a lot of best practices/needs cleaned up Anything specific outside of the other points, would love to hear! * Not taking advantage of dependency injection This would also be an implementation thing. I will update the example to show how to use it, and make any updates if needed to make it more friendly. * Too much usage of static classes and properties. SpoonDataWorker for example (This would be fixed with DI) Are static classes considered a bad practice? I may have missed this trend. I know DI is popular but is there a performance or maintenance consideration outside of stylistic preference? * Data seems to be stored in static classes so its not great for large sites. I am not sure what you mean here. No data is stored in a static class other than some fixed string. All data returned from the data layer would be in client defined variables. Thanks again for the feedback, I appreciate the advice!
I may do the same when I update the example. Thanks!
I moved from Yii/PHP to ASP.NET. I found [this](https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/creating-an-entity-framework-data-model-for-an-asp-net-mvc-application) very helpful 
Xamarin is free now
They really need to update their Github documentation, then. It still refers to Avalonia as an alpha.
https://www.xamarin.com/compare-visual-studio it used to, much better now
Start with .NET Core 2.0. It's the latest and greatest.
Xamarin.Forms is an option, but it doesn't support .NET Core.
16 gigs of ram and a Core I5 is plenty. I need more hard drive space (512 gigs) for different test virtual machines.
My work laptop has 8gb ram and a i7 6500u and I hit 90%+ usage on both memory and cpu constantly with 2 vs instances, outlook and browser.. utter shite for working. Work thought they’d be great instead of the desktop with 16gb and i7 6700k I had before :/
VS is a 32 bit app so it isn't going to be chewing up close to that 8gb. Probably more related to other dev environment stuff like Slack, Outlook and maybe even your own app chewing that up. Don't get me wrong, I've been running 32gb for a few years. But that's because I love leaving stuff open and virtual machines
Also recommend the xps 15. Great build quality. If I could change anything, I would have opted for the 4k display. Still got a killer deal on it though, so I'm completely happy with my purchase. If you want to do cross platform mobile development, I would highly suggest a macbook just because that will give you the most flexibility out of the box. Apple has their sdk locked down to their OS only, so you can build for iOS on a mac. Lame as hell.
Mono/Xamarin is officially supported by Microsoft now and has even started pulling in code from .NET Core. No reason not to use it if the situation calls for it.
Oh they actually did a nice move. I'll consider it
the new asp.net core 2 stuff is really good. but coming from php, be prepared for a different development flow. asp.net core has a compile step, it's not interpreted. they are saying that the next release (2.1) is going to have vastly improved compile times (can look up dotnet core 2.1 roadmap), so if you find it slow, don't give it up. just imagine all of the type checking and optimization happening during that stage.
There are a ton of advantages to dependency injection. I really suggest reading up on it if you are not familiar. DI replaces the need for static classes because you configure the scope/life of each object lives in startup. This means you can make an object be around forever, for just the life of a request, or new each time its injected. Which is nice if you don't want a large object hanging around in memory when its not used. It's also nessesary for unit testing. Plus, it helps keep your code clean. Difference in code: public class HomeController : Controller { public IActionResult Index() { ViewData["Title"] = SpoonDataWorker.GetContainer("HomePage").GetItem("pageTitle").Value; ViewData["Carousel"] = SpoonDataWorker.GetContainer("HomePage").GetItem("myCarousel").Value; return View(); } { public class HomeController : Controller { private ISpoonDataWorker _spoonDataWorker; public HomeController(ISpoonDataWorker spoonDataWorker) { _spoonDataWorker = spoonDataWorker; } public IActionResult Index() { ViewData["Title"] = _spoonDataWorker.GetContainer("HomePage").GetItem("pageTitle").Value; ViewData["Carousel"] = _spoonDataWorker.GetContainer("HomePage").GetItem("myCarousel").Value; return View(); } { I was wrong about the data in a static object. I was confused looking at `ISpoonData` in `SpoonDataWorker`. Again, the `ISpoonData` object should be injected into the constructor instead of newed up. Define how to create the DI object in the startup.cs I guess the only best practice I was thinking of at the time was DI. Which I'm sure you can tell, ha. I don't have more suggestions without looking at it again. I think you have a good idea and should keep going with it. 
Static classes are non ideal in a web environment if they introduce state. You have a single instance of your db but it is only threadsafe if you use ReaderWriterLockSlim per the docs. If you use DI and inject instances of the db you can setup instances that are scoped per request. Also if you inject dependencies mocking them out in tests becomes a possibility... you will find that to be impossible if you stick to statics. 
I believe they just want you to use web as your forms ui. Razor or JS + webapi.
Well.. you are in the dot net section of Reddit. Welcome!
I think the perfect balance is a Dell XPS 15” laptop. I’ve been using a lot of different laptops recently (Dell XPS 13”/Surface Book 2 13”/Razer Blade/Surface Laptop/MacBook Pro 13” with Touchbar). I really like the Apple ecosystem and MBP’s but I like how full featured Visual Studio is on Windows so at the end of the day, I am most personally productive in that environment. I also use Parallels but find the performance on it (even with 16 GB) lackluster for sure. Plus the new keyboard is just okay. 
I've got a couple generations old MacBook Pro and I run [JetBrains Rider](https://www.jetbrains.com/rider/). There also exists Visual Studio for Mac (which is just rebranded Xamarin Studio), which works just fine as well.
My experience was that all the freelance websites had you competing with $10 an hour devs from Pakistan in a race to the bottom for pay.
I use it on my Mac and feel it's worth the money. It's basically VS + ReSharper. My only regret is there's still some extensions I use with VS that don't have equivalents in Rider, like SpecFlow.
Where are you based?
&gt; Are static classes considered a bad practice? I may have missed this trend. Static state has been pretty much always been bad practice, especially in multi-threaded environments.
I think they said on .NET rocks Mono will be used for client side more and .NET core more for server stuff, so that would be in line with that.
Which do you like the vest, jetbrains rider og visual studio for mac?
Am suggesting you try ZetPDF.com. Best website.
Allowing direct entry of HTML from a UI into a data repository is asking for all kinds of security issues revolving around injection of script code which could be executed on the server. A better option would be to implement input formatting the way Reddit does. Use this particular *CMS* with a healthy dose of caution. If you want to know why I say this I suggest you do some research.
Yes it's pretty great, and I do love using it (I prefer it actually), and the entire auth pipeline is exceptionally well refined, but it's also the most buggy (we're still having many problems with Entity Framework Core; the inability to control lazy loading to name one major fucking issue that's been plaguing it for as long as I can remember). If you want stability and/or to get a new job soon (you'll pick it all up fast as you're experienced already), at least *consider* vanilla ASP.NET because the majority of companies are still using it due to that stability, and almost all ASP.NET apps being maintained today are *not* running Core. In saying that, they are so similar that I switch my daily tasks between vanilla and Core without skipping a beat so ymmv
Go with 16 imo. 
Does it support composite primary keys?
Just updated the docs, right now there is no support for composite primary keys, although that's definitely something I want to add.
PL
What is exactly vanilla aspnet? Webforms? Because mvc and webapi are really almost the same compared to Core. At least from high level api point of view. And btw, lazy loading is evil, if you want total control of what your app is doing with db, you need to turn it off. Core 2 is stable and i have several core apps in prod for more than a year
I appreciate the input! Security is an important thing to take care of for the user when we can. CKEditor, by default, gives back the input encoded. Nothing is stored in the DB, or even in memory, as unencoded strings until the user decides to make it otherwise. If there is a use case where you think this could still be a concern, I would love to hear it so I can address it.
TL;DR: You can't :\(
What editor you use to enter data is irrelevant, I can send whatever data I like to your controller/api by bypassing the UI and crafting a raw request. If you want to prevent XSS attacks you must sanitize your data server side, either on input or when rendering it back out again to the client. Advising someone to switch to markdown to prevent XSS is bad advice, you must still sanitize output: https://michelf.ca/blog/2010/markdown-and-xss/ Anyway, OP would have to confirm, but my point is that output encoding and sanitization is probably outside the scope of the CMS and should be something all web devs are thinking about when rendering user input out to the client.
I updated the project, made dependency injection standard throughout and removed the static references. Luckily was very easy to do. I updated the github and this post if you wouldn't mind giving it another look and giving some feedback. Thanks again!
First thought that comes to mind is to have a DB trigger, or expand the scope of whatever function makes the insert into the DB table. The azure function would certainly work for what you want, but nowadays its better to try and push updates out when you can instead of pull them in as architecture allows.
just thought id let u know, i had to do some updates, latest VS, i had all hte web developer stuff already. but yeah just needed teh update. working from visual studio debug mode, but man it runs like a dog. got any tips in that regard? the site im migrating from had angular 1 and asp.net core web api in the same project. perhaps i should seperate them and use ng serve --open for the angular project.
If you want a good touchpad, you should get Surface Book. (But well, $$$)
I would keep the queue, in the azure function that processes the notification check for failure and re-queue the notification. Careful of infinite re-queue so look into some mechanism to detect a retry count. When it reaches retry threshold send it into a separate error queue which you can monitor and investigate
Have you considered something like [Hangfire](https://www.hangfire.io/)? There's a number of ways you can do it, probably putting in one job per notification so that you can track any failures and take advantage of the auto-retry? We're using it for all sorts of notifications, things like the "you've been a member for a week but haven't done &lt;action&gt;?" reminder emails. Just put them in there at registration to run in a week, and check then if they've done the action.
You should really completely decouple this from your application. There are a couple of different ways to do this but one of the more straightforward ways to do it would be to create a storage account and use the queue feature. [Storage queues and service bus queues https://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-azure-and-service-bus-queues-compared-contrasted ] Add items to your queue and have subscribers that process the requests. If you raising multiple events here how I would architect it. App/api/timed service - creates a queue item that a type of notification has been raised. E.g. friendCommentOnImageNotification Using functions or web jobs - subscribe to that queue and processes friendCommentOnImageNotification and checks the type of notifications that user has accepted. Raise a new queue event for email Raise a new queue event for mobile notification Raise a new queue event for releasing carrier pigeon Have processor subscribers for each of the above queue events and process as you would normally. By completely decoupling all these individual items you allow your application to scale only notifications that are required. You don’t need to worry about your queue processor running too long or stopping for some reason. You can update or change your email/sms/mob notification provider without redeploying your whole all. And you have much smaller components that are easier to test making your testers happier as much smaller regression, smoke and pen testing 😁
You can use an Avalonia http://avaloniaui.net/
This is the first thing I add to all of my mvc projects.
It's great to see effort in this direction as this is much needed. I am working on an app that downloads a lot of data - it's a little late for me to switch but I will keep an eye on this. 
Curious, if you had to pick between an Apple MacBook with Windows Bootcamp or a Dell XPS, would would be the deciding factor?
 I actually wanted to lean towards a iMac 27" Retina 5k. Something about being able to fit 4 full-screen displays on 1 large monitor. As a Systems guy and a developer, that is a beautiful amount of screen real estate. I just need a mobile solution, too.
[removed]
I really need to try boot camp on my MBP again. I haven't in maybe 3 years because there were driver issues with the trackpad and scrolling that I couldn't find any solution for. Parallels worked great though, just a bit too slow. I really really like the MBP's in mac OS but the performance in Windows on Parallels is just not good enough to warrant a 2k+ laptop for everyday work imo. I do need to try boot camp again though because I'm sure performance would be great. As long as I don't run into any weird driver issues. I'm not a huge fan of alot of changes with the latest MBP. I can't get behind the keyboard, every other laptop I mentioned above has a much better typing experience. There's no depth to the keys on the MBP and I just don't like it nearly as much as the SB2 or Razer Blade's. Those are awesome. Touchbar is kinda lame too,I don't like the virtual escape key at all. It feels too weird. Dell XPS 13" is awesome but it's just too small, which is weird because I like smaller footprint laptops but the 15" Dell has a smaller footprint than most other laptops at that size. Feels more like a 14" or something which is great. There's a lot of things that Visual Studio for Mac OS is missing. Don't get me wrong, it's AMAZING that we have that IDE (even if it is just a rebranded Xamarin or whatever) but there's plenty of things that are missing or just don't feel as good. Like source control with git. There's no option to manage web secrets for an ASP.Net Core project from the UI (like you can in Windows) and more. It's great, not not feature parody to VS on Windows by any means. I'm sure alot of people just use VS Code and git command line, etc but if you're using to having everything you need from VS you'll notice quite a lot missing. 
I have not considered hangfire. What perks does it have over something like azure storage queue? 
Yeah this is pretty much what we used to do. Iterate and instantiate all forms take a screenshot. Get labels and tooltips and throw them into a word doc. 
We use object comparison in our repository update action that will compare current and new values: https://github.com/GregFinzer/Compare-Net-Objects It works fairly well but would be nicer to be fully automated. Interested in seeing what people suggest/do.
Hangfire allows you to push code execution to other servers (or run them in the background on your web servers if you want). You pass it a function to execute via lambda and it'll store that in a job queue, and then pull it off the queue and execute it. It also supports dependency injection frameworks so you can do something like: BackgroundJobClient.Enqueue&lt;IDashboardAlertService&gt;(s =&gt; s.SendAlert("Alert!")); And it'll run it for you, injecting all dependencies etc. It's capable of running jobs straight away (returning as soon as the job is successfully created), in a set amount of time, recurring on a cron schedule and after completion of another job. You can tie it in with your error logging framework, it auto-retries failed jobs (or not if you don't want it to) As you can see here, it's pretty capable: https://discuss.hangfire.io/uploads/default/original/1X/c49b7d2a902eb2321425215e600c37a8e923b29c.png I think it's main advantage, is it's ease of use. Just pass it a lambda and as long as you can serialise the parameters, you're good.
We use http://entityframework-plus.net/tutorial-audit works really well and is easy to use. 
As a self taught, former Architecture major, if your goal is to get a job as a junior dev (I assume that's what you mean by "becoming" one), then you should be applying for jobs. First off, the technologies you've listed off are great for a Jr dev to have, if not more than I'd be expecting. I don't think you need more technologies or buzzwords to get hired. Second, I was really surprised when I transitioned from my personal projects to working professionally, and my first handful of months I felt a little lost and that I didn't know what I was doing. There's something about have a specific requirement or task that isn't 100% perfect or ideal, that has to work within some odd parameters. Being forced to work within constraints makes the problem solving aspect of development much more challenging. I learned a lot in those first months, way more than I would've on my own. Third, the job hunt starts now. Don't worry about finishing up your portfolio or learning N more technologies. Get your resume together. Apply for some jobs. Go on a couple interviews. Some employers will pass over you because you don't have a CS degree. Don't let that discourage you. You want to get into an interview and convince them with the work you've done, how you can learn and pick up on things, and your drive to become a dev that you are worth hiring. If you really know the stuff you say you do, then I'd be quite comfortable hiring you for a jr position. Fourth. Your first job doesn't have to be perfect, it has to get you in the door. Without a CS degree, you offer might be a bit lower than a typical Jr dev, which is fine. Unless they are moving you up the ranks and paying you accordingly, you will need to move on to another job in order to keep your salary up with your experience. Without a CS degree, the first job is the hardest to get, and after that you've proven that you're good and can do the work. I wouldn't recommend moving on too quickly, but if after 18 months you're not getting what you'd like at your position, then it may be time to get your resume back out there. Hope that helps.
They've recently gone beta.
It would depend how strCustomerID is declared. Assuming it's a string you'd get an error unless you added .ToString() to the query. But if the query can return a NULL result you'd be better off doing something like: var qCustomerID = from c in dcCustomers.Customers where c.CompanyName == strCustomer select c.CustomerID; int? customerID = qCustomerID.FirstOrDefault(); 
What is it doing that is unexpected? FirstOrDefault() returns the first item in an IEnumerable&lt;T&gt;, or null if the item specified does not exist. I would work backwards and remove the where clause in your LINQ query, then use .ToList() to force its evaluation. If that list is empty, you need to refine your query. If it is not, you need to refine your where clause. Disclosure: I dislike the query syntax and vastly prefer the fluent expression syntax, so my terminology may not be perfect, but the underlying code operates the same.
thanks! Very helpful. I'll top focusing on the technologies, buzzwords, and perfecting my portfolio. I'll the job hunt a priority and see where it takes me.
In the context of getting hired for a paid developer position, perfecting your portfolio and learning new technologies won't help you any more than what you've already done. However, those will continue to help you as you progress in your career. Focus on getting a job, but don't forget about those things. Continue to push forward with them and integrate things that you're learning on the job, so that when it comes time to put your resume out there again, you'll already have your portfolio in place and some additional technologies under your belt.
Like others have said, you should tell us why it isn't working, because anyone trying to run that code and help you out isn't going to have the same data set to work with. What is `strCustomerID`'s value after running? Is there a case mismatch, like you're expecting to get CompanyA but instead companyA is in there? Because if that's the case you'll get unexpected results. I always default to calling `ToLower()` on strings before comparing unless case sensitivity is important, it's a common source of unexpected behavior. Also keep in mind a LINQ query returns an IQueryable, not an actual query - it's an IEnumerable which is guaranteed to be LINQ queryable. You may want to convert your `qCustomerID` to a List before doing any more work with it. 
What type of audit logging are you needing? Is logging to a log file/server good enough? Are you needing an audit table that could be populated by a trigger or does the app need to know about the audit log. I’ve usually found asking what the log is for helps answer what type of log you need. Will you display it in the app? Then you’ll probably need a proper table that EF can connect too. If you just need a CYA type table then a trigger table might work. A lot of people dog on triggers but as long as they are managed by a DBA (not a developer database person) I think they work great. 
If you call something part 4 you should always link to parts 1, 2 and 3! Anything else is just bad design.
Albert Einstein said "Things should be as simple as possible and no simpler". Microservices are an example of what he was referring to.
I think you made your comment too simple, because it could be interpreted either as "microservices are as simple as possible" or "microservices are too simple".
True, but I had to scroll twice on my mobile, to get past header and image, and had to start reading an article, I didn't want to read before I read the previous three. A clear link (not same color and then underlined) at the very top, would have been more clear.
In general I think Microservices break things into pieces that are too small. The ratio of infrastructure to usable code is too high. There are other, better ways to achieve simplicity without the sacrifices that Microservices encourage. Redundancy and inefficiency tends to creep in along the seams - and with Microservices there are many seams. I don't mean to be a naysayer. I'm sure there are specialized applications for which Microservices are very suitable.
I think it unfair to create a false dilemma that gives the reader the options of "Microservice architecture OR Monolithic architecture". Not every application that is not a Microservice is a Monolith. A well designed n-tier application can be scalable, testable, and loosely coupled.
Maybe if I was training to be on Jeopardy 
I like that a lot! Thanks for sharing. 
* If you have an abstract base class with an abstract method. * If you are deriving from a base class and you want to change behavior of the base class virtual method. Ex: lion.Speak() vs houseCat.Speak(). This is useful in situations like a provider model where core behavior is mostly the same in derived classes but each provider may have specific implementation detail. The documentation actually has an example of using it to calculate salaries for employees: https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/override In general, favor composition over inheritance, but it is a tool that has uses.
The problem I see with these attempts to give people answers to job interview questions is the subjective nature of the questions. Never mind the ethical side of attempting to game the process and help people get into roles they're unqualified for. 
While the examples help and make sense to reduce duplication, the use of `dynamic` is almost always a code smell. When reading the result of the `ServiceReponse&lt;T&gt;` instead of using dynamic and creating brittle code you should instead use a non-generic interface with the `Status` property. In C# 7 this lets you simplify it to a simple `if (result.Value as IServiceResponse serviceResponse) { }` and now you have compile time safety again! public interface IServiceResponse { ServiceResponseState Result { get; } } public ServiceResponse&lt;T&gt; : IServiceResponse { public ServiceResponseState State { get; set; } }
Abstract classes are like interfaces with the ability to add implementation. An abstract class can not be instantiated. You usually use them if you want implementation added at the root of the inheritance hierarchy but don't ever want to instantiate the root class. Going back to the animal example. A lion is an animal and a house cat is an animal, but you can't 'make' a new animal, it is just the base classifier and therefore is abstract. You make one by creating a sub class instance like new Lion();
I mean I get how to use them, a class derived from an abstract class must implement all members if not marked abstract and if it only wants some members or none at all then the derived class must also be marked abstract. I just don’t really get the reason or why to use them. I mean what would be the difference from just making non derived classes that do the same thing? Or maybe I’m looking at this all wrong.
I was not trying to imply that Monolith or Microservices are the only options. Of course there are many more known architectures. Though n-tier architecture is used for many years, but it cannot provide some of the features that Microservices provide. ex: In n-tier application, I assume all the features would use all the tiers, so if one of the tiers is down the whole system is down. Or even in an SOA architecture for instance, if there is a Products service and every service that needs products information calls that service (Following DRY principle and keeping one functionality just in one service) then if Product service goes down, anything that needs product information will go down, where in Microservice architecture this would not be a problem. Please see the first part of this series for more drivers and drawbacks of Microservices Architecture if you haven't already: https://koukia.ca/a-microservices-implementation-journey-part-1-9f6471fe917 Cheers!
Thanks for your feedback. i just added the links p=more properly to te top of each part of the series. Cheers
It looks a lot clearer now, and thanks for taking well to feedback.
The app will need to have access to the log, so a proper table is required and why logging to a file isn't really useful. 
+1 because penguins are my favorite animal. But why would you need to inherit in the first place? I mean, why not just have separate classes for each animal and take care of your methods individually for each class? Because in the end, no matter which route you go, you’ll be having to write those methods.
Eat(), LayEggs(),Speak(), And Fly() can have implementation in the base class that applies to all birds that inherrit from them. So any class inheriting them does not need to rewrite anything...They can use basic bird behavior (the methods mentioned), then as subclasses can have specific class behavior (new methods eg Swims or whatever)... but in the case of Penguin Fly() does not make sense so you just override that, and you can still call the other methods that apply to all birds. If you had separate classes for each Bird, you would have to repeat writing certain methods each time. Theses methods might be identical across most birds, so having the implementation in a base class means you only write it there and all the children get it. Hope that makes sense. 
Instead of using a return response from your business layer with success / failure I like to use exceptions for the exceptions... and then use an error handling middleware approach which transforms the exception into the appropriate http status code. I seems a bit simpler to me and you wont accidently forget to check a response from your bsuiness layer and you wont have to daisy chain response checks etc..
Grouping common functionality in a base class allows you to have one single touch point to change later. When you create an application you can certainly just repeat all the methods in each class but if you ever had to change that functionality would you remember months maybe a couple of years down the line that you have to change it in multiple locations? How about if another developer takes on your code? This just gives you a good way to have a compile time check ensuring all common methods are changed for all classes that care about them simultaneously.
&gt; ex: In n-tier application, I assume all the features would use all the tiers, so if one of the tiers is down the whole system is down. No, that is not an example of a monolithic architecture. That is an example of a poor and badly designed architecture, that is violating the point of having tiers to begin with.
&gt;I mean what would be the difference from just making non derived classes that do the same thing? With the Animal example (so the base class is "Animal" and the derived calsses are say, "Dog" and "Cat"), you can have a method that takes any Animal: void TakeToVet(Animal myAnimal) { ... } Or a generic list of Animals: List&lt;Animal&gt; myPets;
Akin to this, I sometimes prefer to pass down a validation interface for a given context instead of generic wrappers around responses. public interface IDoTheThingValidator { void YouForgotTheThing(); void YouProvidedTheWrongThing(); void ThingNotFound(); void YouAreNotAllowedToDoThisThing(); /* etc.. */ } Then the implementation of that can collate the errors, or throw an immediate exception, translate them to whatever the client requires, etc. etc. 
You are referring to composition. Making components that perform a function instead of deriving/extending/restricting function from an inheritance chain. It's a valid design choice. You use composition when your class uses or contains another class vs owning the implementation internally. I think action filters are a good example of abstract classes and overriding. You can see here that the abstract methods are used to help the user to implement their own action filter.: https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions-1/controllers-and-routing/understanding-action-filters-cs How would you do this via composition? You could require instances of ActionFilterAttribute take an IOnActionExecuting, IOnActionExecuted, ... etc interface during construction or via setter method or property. I would argue it is more clear to the user that overriding the method allows them to extend the behavior. Also, this class is used as an attribute decorator so there are limits on construction.
just implemented this at the day job and it seems pretty great so fat
That sounds like a pretty good idea. Who do you follow - or mind sharing your handle so I can follow the .NET people you follow? 
Regardless of architectural style, if a required service is down, the system is effectively down.
Any chance you would be able to share some code for your middleware approach?
Thanks. I am a fan of Umbraco, but sometimes I just want something much more simple and easy to integrate into existing solutions. So I wrote this to give me something simple and flexible. I am with you though, excited to see more CMS solutions move to core. Feel free to give this a try, poke some holes in it, and I would love any feedback on how it could be better!
What a strange choice of image to alter and use for a profile image
You'll likely want to use some kind of token bearer authentication (JWT is the popular one these days) along with some form of identity management. This post goes into some detail on how it all works together https://fullstackmark.com/post/13/jwt-authentication-with-aspnet-core-2-web-api-angular-5-net-core-identity-and-facebook-login
Still learning ASP.NET Core and I had always assumed IdentityUser and the authorization system would handle all this for me but I have never created a full project yet. Thanks for sharing this. I'll be sure to study it!
Still learning ASP.NET Core and I had always assumed IdentityUser and the authorization system would handle all this for me but I have never created a full project yet. Thanks for sharing this. I'll be sure to study it!
Supporting OAuth 2 would be good for your Web API. I found Thinktecture lib to secure Web API much better than Microsoft's OAuth2 server implementation. See [Thinktecture here](https://github.com/IdentityModel/Thinktecture.IdentityModel.45/tree/master/Samples/Web%20API%20Security) and [here](https://stackoverflow.com/questions/39727808/thinktecture-identity-server-securing-web-apis-authorization-best-approach). Some work will be needed like having a db table as a user store etc. but the lib supports all scenarios.
Misleading title. This isn't the typical file share one would have in mind as a "network drive". This is just a basic app to download/upload files to a storage account. Here's a "network drive" using an Azure storage account: https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows
Daily emails become a PITA. I'd drop this to a weekly newsletter type of thing in terms of frequency.
&gt; The other benefit of .NET Standard 2.0 is developers are no longer spammed with NuGet dependencies. UWP app authors for example saw NuGet pull in over 100 packages when referencing Json.NET. UWP supporting .NET Standard 2.0 and consuming a netstandard2.0 package eliminates that problem. I've been noticing this in my projects but had no idea why they had so many dependencies.
https://rafpe.ninja/2017/07/30/net-core-jwt-authentication-using-aws-cognito-user-pool/
Using ASP.NET, you can override methods in the Controller-class to have stuff happen at certain points during the request lifetime. Examples can be OnBeginRequest(), OnEndRequest(), OnActionExecuting() etc. 
You've effectively added 6 lines of code to reduce the possibility of a developer making a typo on one property. Sure you've added compile time safety, but safety wasn't really a problem in this case. dynamic being a code smell is purely an opinion rather than fact. The team I'm on happened to agree on it in this particular case. I believe it to be a language feature which, like all features, have their use. Thanks for your comment though, I'll definitely think twice the next time I use dynamic.
You're using exceptions for the wrong reason. Errors you anticipate happening should not use exceptions. Exceptions are errors you didn't expect. In high performance applications, exceptions are expensive, and to throw an exception for a "Phone number format exception" is very bad practice. "Accidentally forgetting" to do something as a developer shouldn't be an excuse to follow bad practices. I'd rather change my behaviour than write code :-) Thanks for taking the time to comment though, much appreciated.
Exceptions are returned as a 500 Server error automatically so there's no need for middleware. If your expecting an error to happen, just return it as part of your model :-)
That's a great idea! Thanks for your comment.
I am not using ASP and have no experience in it. I have been programming in Visual Basic for years, and am very good at it. I just got started using Unity and it so I have to learn C#. Learning C# hard really opened my eyes to OOP, polymorphism, inheritance, and hopefully I’ll be able to lean abstraction and encapsulation. These are concepts I’ve never really knew about or have ever implemented into any of my projects. So out of the years of thinking I’m good at programming, now I feel like I’ve been a shitty programmer this whole time.
Sounds nice. Will definitely try it out.
Love the idea! Maybe add another TODO item and add some Unit Tests? Make sure your "units of work" are working correctly [or incorrectly]
I think you miss-understood me or possibly I miss-understood you. I use exceptions for errors you didn't expect not for expected cases. Using your example the 'phone number format' should be captured by the UI or the validation layer baked into .net. In your example you also used NotAuth as a return result. So I tend to stop this happening in the UI, and then capture the rest with the authorisation layers that are baked into .net. If the user manages to do something they are not allowed to do after those safeguards, it's an exception. It will be logged, I will see it.
I hate questions like this because people love giving theoretical answers without any real code. Here is a real example of a collection that keeps a total of its contents: class OrderLineCollection : Collection&lt;OrderLine&gt; public decimal Total {get; private set;} protected overrides void InsertItem(int index, OrderLine item) { this.Total += item.Quantity * item.Price; base.InsertItem(index, item) } protected overrides void RemoveItem(int index) { var item = this[index]; this.Total -= item.Quantity * item.Price; base.RemoveItem(index) } For a more complete example of subclassing Collection&lt;T&gt; see https://msdn.microsoft.com/en-us/library/ms132415(v=vs.110).aspx
Something like this: https://code.msdn.microsoft.com/How-to-handle-global-41fe6d52 It's a pretty simple and saves a lot of fluffing about.
I hear you. Got my own personal projects that seem to never get finished because of "some day" :)
@gibwar What about the .Data property which is used a bit further down?
I'm really sick of articles saying how to solve problems like. This..you know what you do? Make a damn view or stored procedure. And keep to that as the contract. And leave the data in the damn database to do what it should. That's how you solve round trip problem. 
The ones on the official Microsoft docs which give you walk through examples which all can be cloned from githib
&gt;Obviously I've tried to use dotnet publish, this is the command that tells me project.json is required but this isn't a 1.0 application, it never was. Could it be that you're running an old version of dotnet binary? Try to do **dotnet --version** just to be sure which version it is.
Sure no problem. Just check at @jotagrassi. It's also useful to go to the github repo's and just find the people there :). I also subscribe to their profiles on Github. Pretty cool.
Do we have any numbers on the performance improvements?
I'd just use the off the shelf Microsoft OAuth2 libraries and federate authentication through IdentityServer: https://github.com/IdentityServer That way, all you have to do it is import a few nuget packages, add about 6 lines of code into your Startup.cs and then decorate your controllers with [Authorize]. Then all you need to do is get a Bearer Token or login through IdentityServer and present your token to your API through the Authorize Header.
Store the HP value as an integer (I'm guessing you're doing something with D&amp;D which uses whole values for health) then have a method to increment it. You can then call that method with any button or event you need. https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/methods
Thanks for the reply and yes, D&amp;D :) So I understand the custom event handler, but how do I tell it to specify the HP textbox for a specific canvas. TextBox tb = sender as TextBox; won't work because I am not clicking on the textbox, just a button. I think I am doing a poor job of explaining this so hopefully this new image will add some clarity. https://pasteboard.co/H9Bvurs.jpg
Oh I think I got you. Each button has a corresponding textbox that you want it to update, and we're assuming there are more of these than is feasible to do by hand. This might be another piece of the puzzle: [Using TextBox Array to Correspond them with a Button ](https://stackoverflow.com/questions/46371204/using-textbox-array-to-correspond-them-with-a-button) Your "universal" event handler for the click event on the buttons would pass the button's corresponding textbox. If you follow a numeric/predictable naming convention I imagine you could do a loop to set up the constructor part mentioned in that link's answer portion.
Another option is using a service bus and scheduling the message so it’s delivered at the appropriate time. A listener on the queue then picks up the message and sends the email. This guarantees delivery just once, you don’t have to deal with creating your own scheduler and it decouples the system. Check out the “Scheduled Messages” section here: https://docs.microsoft.com/en-us/azure/service-bus-messaging/message-sequencing
It's the same problem as the EF Core LIKE. It's incredibly slow, even with just one Contains.
We have the same feature set up. Here's how we have it working at a high level: - Use a service bus. - Notification requests go on the bus and get picked up by a notification service that you write. - Notification service picks up the message and queues additional messages, one for push notifications (e.g. web dashboard, mobile app, etc.), one for SMS, and one for email. 3 separate messages because each will have it's own handler and will be handled differently - A handler processes the email command by constructing it and sending it through SendGrid - A handler processes the SMS command by constructing it and sending it through Twilio - A handler processes the Push notification(s) and either queues more commands (if there are more push receivers such as web or android or ios, etc.). - The push notifications are stored cache style (e.g. Redis), or in a stream type (append only) database if you want it to be persisted even after acknowledging it.The notification can be "pushed" to the dashboard either by long poll or open socket / SignalR I disagree with the approach of polling a database for things like this. If you use a proper service bus, then subscribers will be notified when there is an event which is more efficient. Then, rather than explicitly telling the system to send notifications, you just notify the system that something happened, and the business layer(s) can respond by sending a notifications. How this is implemented really just depends on your overall system architecture. Depending on how fine-grained your SOA is, you could want to make the notification service "dumber" by just having it blindly send email, sms, or push. Then it becomes the command-publisher's responsibility to build the command with the content. For example, our surveys service constructs the content of the SMS and Email messages and then queues the request for it be sent using a specific delivery method. The notification service gets the command and basically just uses the content and destination and switches on the delivery method to figure out how to handle it.
It's not telling you your project needs to be a json one. It needs a file called project.json with information about the project in I believe. 
I see. Does the table you're querying into have a lot of data in it?
Thanks for the tips, I will check it out!
I guess it depends on what you define as a lot. It has a little over a million records. But the size isn't the problem. A normal query works just fine. The problem is .NET converting the contains/LIKE to the SQL equivalent. The slow down happens before the query is executed.
I would not try to do this within IIS or whatever web server you are using. A windows service, windows task scheduler job, or scheduled job in the db are probably more appropriate options. If you do it within IIS, you have to contend with app pool recycling or the entire website being stopped and restarted for whatever reason. IIS and other web servers just aren't intended to host long-running or periodic background services.
Try adding some junk to the querystring: page.aspx?junk=1231
In my experience it's best to apply as many predicates as you can before you enumerate. We have an app here that very regularly queries into tables with several million records. If we were pretty general on what we asked, it would take FOREVER to enumerate with all the columns and the child entities it filled in. I guess what I'm saying is to try to be as specific as you can. If there is a solution you or someone else finds, I would be very curious to see it!
What? I'm new to this, sorry.
FromSql? https://docs.microsoft.com/en-us/ef/core/querying/raw-sql
This suggestion will help ensure you're not viewing a cached web page (which may be the root of your problem)
Yeah, that's exactly what I'm tying to do. This query: `var query = from part in _context.CurrentInventory` doesn't actually execute on the DB. It just builds the initial query. It isn't until I call `.ToList()` that it executes. Adding the `WHERE` doesn't execute either. It just adds to the query. I've profiled this to ensure this is what's happening. When doing research, I found that .NET takes a log time translating the .NET parts to the SQL parts, before the query is executed. It's what's happening here.
I'm not sure how to do what he said.
This has to be done outside of your web app. Either schedule a job in your SQL database, write a PowerShell script and schedule it via Task Scheduler, or write a Windows Service.
Hmm, I load it as http://myip/ so I entered https://myip/?junk=asdasda and it still isn't showing the new code. And I've tried it in different browsers.
I registered right away and have been wondering the same thing. I wonder if it has something to do with the overlap with Google I/O this year. 
I wonder how this compares to using semaphores and TPL. I have done that for some heavily multi-threaded applications (1000+ threads) and had decent performance though not being a part of the language you have to implement it yourself. 
I think the lack of a hardware giveaway as weeded out a lot of interest
If you stop the web server, does the page still load?
I'm curious why you're going 1.0-&gt;1.1 and not 1.0-&gt;2.0?
Are you sure it's .aspx page and not a code-behind file (c# or VB.net)? Your app may be precompiled, in that case editing server-side code will not take effect until you recompile the application.
This would be my best guess as to what is happening as well. OP needs to get a hold of the .vb or .cs file to know for sure what is happening.
I have been doing exactly this, for years, for the exact same reasons, and it works well.
Yeah, I'm pretty sure it is. I updated the OP with some file if you would take a look, thanks!
Hey, I updated the OP with the file I edited and what I believe is the .vb file you're referencing. Would you mind taking a look at it, please?
I'm pretty sure it's not caching it, I updated the OP with more info.
It doesn't load with the server stopped. I updated with OP with the files if you want to take a look :)
first off, it looks cool as a project. while playing around a bit what's really stumping me is that there's no error output in the web interface. i can see in the console what's wrong and do something about that, but's that not something i'd want to have to do ;)
I took a quick look. And Menu1 isn't referenced in the vb file. Have you tried loading it in a incognito window? Or maybe erasing all your browser cache and trying to load it again? Looking at your comments above it looks like you have. I am out of ideas, sorry.
Hmm I used powergrep to search through all of the website files and added the code I wanted into all of the files that had those other menu items listed. The one that finally worked was called index.aspx
C# and EF
Can't you set a service to auto-restart?
Put a break point in your view. I'd your class called Object? That's a pretty bad name considering all .net classes derived from Object. I'm surprised this would even compile.
Thats one of the reasons I made this more public, to help me get more motivation to actually continue working on it :)
Yeah I know, there are a lot of plans for the project and things like this are part of it. I just wanted to make the project a bit more public to see if there was any good respons to actually motivate me to continue to build it beyond my own needs :)
No no I’m giving you generic terms so you know, my namespace isn’t namespace either and my class properties aren’t Property1 and Property2 haha I don’t know why I didn’t think I could put a break point in my view. I’ll give that shot
Cool, I'm too scared to release my code to the public. Best of luck! I love the idea of it.
I’m driving but I’ll answer soon 
C#, Dapper or EF.
Sure. Just didn't like having a service basically constantly running when it had 1 nightly email to send. Seemed... Silly.
Are you sure you're not doing return View(); instead of return View(model);
Thanks! From experiance, you learn very much by having someone else doing code reviews on your code and coming with feedback, both negative and positive. So just got to take a chance :)
Do you normally code and drive at the same time?
This
OK, so when I put the break point right on the @model reference at the top of the page, In the debugger it is saying this.Model threw an exception of type NullReferenceException if I look at the type for this.Model, it shows a generic List\&lt;Object\&gt;, so it is trying to create the list, with the correct object type, but then it throws a null reference. 
yes, double checked, the view method has the model passed into it. 
In regards to the view model, I just read about them a little. So I created a view model that has a property of type List\&lt;Object\&gt; for my and a default constructor that takes a List\&lt;Object\&gt; parameter and assigns it to that property. Now instead of passing the list object through and making the @model declaration a List\&lt;\&gt;. Is that what you mean? It still doesn't work... so I hope you meant something else hahaha
Yes it is getting populated with the output of the GetObjects\(\) method. I can see all the data in there
I can see the benefits to your solution, I can, but you've added 5 lines of code to combat a single use of dynamic in the hope that a developer doesn't do a typo on the .Value property. Using dynamic is less code in this particular case :) An interface, in this case, is over-engineered. Thanks for your feedback though.
also, just realized I'm getting a null reference exception from this.Route too when in the index.chshtml
Well thank you to /[u/fartinator\_](/user/fartinator_), [u/TheManFromOregon](/user/TheManFromOregon), [u/kjhunkler](/user/kjhunkler), and [u/markfl12](/user/markfl12) but I figured it out. .Net Core generates an @page reference at the top; I don't know what it does, but I got rid of it and it worked. So, I guess I'll figure out the problem when I need that @page back lol
Just finished a project where we werent allowed to use a mapping tool so probably spent the equivalent of a few weeks just writing converters Would be nice to see this tool generate unit tests as well
If you forget the foreach loop can you make something simpler work? 
At the top of your view, you should reference your viewModel. Something like @Model Project.ViewModels.IndexPageViewModel Then it’s: @foreach(var obj in Model.myList) { } Is that what you did? 
Ahhh just saw this. Cool! @Page is used for cookies I believe. Sorry I took so long to get back to you 
Tried it. Seems alright, i guess it’d be nice to a) add a couple of settings e.g. instead of (string)objectProp cast i would want to always do JsonConvert.SerializeObject(), but i would rather have it as a setting for my VS. Another thing which i don’t know if you can do much about - had an object to Map from DTO wrapped as a property on another model e.g. like so class Entity{ public string Prop1 {get;set;} public DateTime Time {get;set;} } class EntityDTO{ public string Prop1 {get;set;} } class Message{ public EntityDTO Record {get;set;} public DateTime Time {get;set;} } Be cool if i could make it convert the message to entity. Further, there is a small nuisance - codefix appears before i give my argument name e.g. public Type1 Map(Type2 ) {} At that point codefix is already there and it tries to generate the code but the argument is empty so the code is not valid.
C#, Entity Framework, LINQ to SQL, ADO. NET, SQL Server DataTools, SSMS, SQLite, SqlCE, Azure Table Storage, Azure Blob Storage, and don't forget Excel. 
I'm waiting on 2.1 for the generic host in Microsoft.Extensions.Hosting to do this, but have you considered Dockerizing those daemons? That's my plan right now as soon as 2.1 releases.
After just learning that 2.0 does not have RequestCache, hoping it will be working in 2.1 
It's difficult to say. The jump in id's isn't really an issue, but it indicates you have added and deleted a bunch of records, which you may have done accidentally by manipulating lists of connected entities and then saving the db context. This can happen very easily if you use a single context for the entire app as everything you do to a connected entity will be recorded and applied the next time you save the context. 
Without any code at all people are just going to be throwing darts blindly. Need to see the EF mapping and how you are doing your read and update
Is it happening on only new entities, or on entities in the 1-13 id range too? From what I can tell, you are probably using a single context to load all you entities, then manipulate them individually, then save all the associated changes. Moving to a unit of work style model where you create and use the context only when it is needed might help. Alternately you may be using multiple overlapping contexts, and making changes in one and not refreshing the entity in the other context before attempting to save. I'd advise you to enable logging and check what query is being executed, for will generate update queries with a specific concurrency field in the where clause, so if you are using a timestamp field as the concurrency field, you will get this error every time you attempt to save an entity that has been updated in another context. If you are using concurrency fields, check the value in the db before your app starts, before the update is called, and the vale in the query ef generates. If there's a difference at any point it means you have code somewhere that is changing the record unexpectedly. Track down all your SaveChanges and see which one is causing the update and work out whether it is an unexpected side effect of tracked context changes, or if it rises expected, make sure your update context refreshes the entity before it saves your changes. 
using System; using System.Collections.Generic; using System.Linq; using System.Threading.Tasks; using Microsoft.AspNetCore.Mvc; using Microsoft.AspNetCore.Mvc.Rendering; using Microsoft.EntityFrameworkCore; using MyApp.Web.Models; namespace MyApp.Web.Controllers { public class ContactsController : Controller { private readonly MyAppContext _context; public ContactsController(MyAppContext context) { _context = context; } // GET: Contacts public async Task&lt;IActionResult&gt; Index() { //return View(await _context.Contacts.ToListAsync()); return View(_context.Contacts.OrderByDescending(t =&gt; t.ContactNumber)); } // GET: Contacts/Details/5 public async Task&lt;IActionResult&gt; Details(int? id) { if (id == null) { return NotFound(); } var contacts = await _context.Contacts .SingleOrDefaultAsync(m =&gt; m.ContactNumber == id); if (contacts == null) { return NotFound(); } return View(contacts); } // GET: Contacts/Create public IActionResult Create() { return View(); } // POST: Contacts/Create // To protect from overposting attacks, please enable the specific properties you want to bind to, for // more details see http://go.microsoft.com/fwlink/?LinkId=317598. [HttpPost] [ValidateAntiForgeryToken] public async Task&lt;IActionResult&gt; Create([Bind("ContactNumber,ContactType,Company,Prefix,FirstName,MiddleName,LastName,Suffix,Ss,Dob,Spouse,SpousePrefix,SpouseMiddle,SpouseLast,SouseSuffix,SpouseSs,SpouseDob,Address1,Address2,City,St,Zip,Phone,Phone2Type,Phone2,Phone2Ext,Phone3Type,Phone3,Phone3Ext,Phone4Type,Phone4,Phone4Ext,Phone5Type,Phone5,Phone5Ext,Phone6Type,Phone6,Phone6Ext,GridNumber,Circulation,TimesAvailable,GridsAvailable,Notes,Pool,CallBack,EntryDate,CallBackStatus,PeachUpdated,Demo,Password,LeadSource,Print,Level,Single,Extension,LastContact,SalesKit,Schedule,Lp,CreditApproval,CreditScore,CreditApprovalSpouse,CreditScoreSpouse,CellPhone,Kit,Email,Pktr,SpecialNotes,Internet,Tstamp,Pkt,DisCode,County")] Contacts contacts) { if (ModelState.IsValid) { _context.Add(contacts); await _context.SaveChangesAsync(); // return RedirectToAction(nameof(Index)); return RedirectToAction("Edit", new { ID = contacts.ContactNumber }); } return View(contacts); } // GET: Contacts/Edit/5 public async Task&lt;IActionResult&gt; Edit(int? id) { if (id == null) { return NotFound(); } var contacts = await _context.Contacts.SingleOrDefaultAsync(m =&gt; m.ContactNumber == id); if (contacts == null) { return NotFound(); } return View(contacts); } // POST: Contacts/Edit/5 // To protect from overposting attacks, please enable the specific properties you want to bind to, for // more details see http://go.microsoft.com/fwlink/?LinkId=317598. [HttpPost] [ValidateAntiForgeryToken] public async Task&lt;IActionResult&gt; Edit(int id, [Bind("ContactNumber,ContactType,Company,Prefix,FirstName,MiddleName,LastName,Suffix,Ss,Dob,Spouse,SpousePrefix,SpouseMiddle,SpouseLast,SouseSuffix,SpouseSs,SpouseDob,Address1,Address2,City,St,Zip,Phone,Phone2Type,Phone2,Phone2Ext,Phone3Type,Phone3,Phone3Ext,Phone4Type,Phone4,Phone4Ext,Phone5Type,Phone5,Phone5Ext,Phone6Type,Phone6,Phone6Ext,GridNumber,Circulation,TimesAvailable,GridsAvailable,Notes,Pool,CallBack,EntryDate,CallBackStatus,PeachUpdated,Demo,Password,LeadSource,Print,Level,Single,Extension,LastContact,SalesKit,Schedule,Lp,CreditApproval,CreditScore,CreditApprovalSpouse,CreditScoreSpouse,CellPhone,Kit,Email,Pktr,SpecialNotes,Internet,Tstamp,Pkt,DisCode,County")] Contacts contacts) { if (id != contacts.ContactNumber) { return NotFound(); } if (ModelState.IsValid) { try { _context.Update(contacts); await _context.SaveChangesAsync(); } catch (DbUpdateConcurrencyException) { if (!ContactsExists(contacts.ContactNumber)) { return NotFound(); } else { throw; } } return RedirectToAction(nameof(Index)); } return View(contacts); } // GET: Contacts/Delete/5 public async Task&lt;IActionResult&gt; Delete(int? id) { if (id == null) { return NotFound(); } var contacts = await _context.Contacts .SingleOrDefaultAsync(m =&gt; m.ContactNumber == id); if (contacts == null) { return NotFound(); } return View(contacts); } // POST: Contacts/Delete/5 [HttpPost, ActionName("Delete")] [ValidateAntiForgeryToken] public async Task&lt;IActionResult&gt; DeleteConfirmed(int id) { var contacts = await _context.Contacts.SingleOrDefaultAsync(m =&gt; m.ContactNumber == id); _context.Contacts.Remove(contacts); await _context.SaveChangesAsync(); return RedirectToAction(nameof(Index)); } private bool ContactsExists(int id) { return _context.Contacts.Any(e =&gt; e.ContactNumber == id); } } } 
That's a good point. :) I've revised my post to include my code.
thanks for the feedback, I've added it to my todo list. Could you show how should look the generated mapping code for your examples? 
https://stackoverflow.com/a/17591518 https://stackoverflow.com/a/14146237 Jumping by exactly 1000 screams this to me. We had this "issue" in the past but after reading the above decided not to worry about it. 
I can live with the jump, but I need to be able to update records.
Ok that looks OK, I assume the context is created per http request, but that's only likely to be a problem in multi user scenarios, if it is happening on the first request that's not the problem. My best guess is that it is time stamp related and the original value is not set on the entity and instead is trying g to set the old value as the new value instead. Either the sql query or a debug break to inspect the field taking properties is your best bet. 
Isn't that used for Razor Pages? It declares the file as a razor page and thus it's model would be passed into it from a code behind file rather than a controller? Is there a View.cshtml.cs file "underneath" the View.cshtml file?
This is a great tool. I don't like to use mappers, so this does make life a tiny bit simpler.
Lazy loading in EF Core! Finally!
This is great! However there is one thing that makes AutoMapper superior to most other mapping solutions and that is `AssertConfigurationIsValid`. Enforcing this is a bit of a pain when you're working on first version of your app, but it makes maintaining it so much better.. If you'd be able to somehow replicate this behavior on compile, now that would be something :)
I'm not familiar with this method, I should definitely verify it. Thanks for the hint. 
Thanks for the explanation!
Performance concerns by tech leads. Premature optimisation mostly
I'm wondering why the sdk doesn't work yet - AFAIK, the entire build system and compiler (Roslyn) are all written in C# and compiled to .Net executables. Given that the runtime works on ARM, it's actually strange that the SDK doesn't yet. Maybe there are native blobs that aren't ported yet?
Can I just say that the .NET sockets improvements for Kestrel and HttpClient are *so fucking cool*. Having a managed, high performance http client and server stack that's cross-plat with minimal to no native dependencies is seriously impressive.
I have no love for AutoMapper, but I don't think it is invalid to generate trivial mapping methods via expression trees. Where things go wrong is when that mapper layer tries to do non-trivial things (formatting, building sub-objects, etc). A notional mapper which does simple copies of direct properties could still provide a degree of static validation if it had methods like MapSubset and MapSuperset, which could throw if a property is missing or unmappable on the "smaller" class (which at least for me is a common scenario). If devs understand the mapper is mechanical and not magical and that they'll have to hand-code anything non-trivial, it should lead to fewer regressions.
Thank you for explaining it. I am trying to lean .Net Core and it was really helpful.
Honestly, Id rather this didn't exist. Causes more problems than it helps.
Seems like a good way to accidentally end up with a lot of n+1 query problems if you’re not careful
If you have any suggestions regarding the library design, maybe based on its actual use, it would be most valuable. I haven't written enough documentation yet, a rough overview of the design only exists on another website in Russian at the moment, but anyone with experience in WPF should feel natural. Simple usage should also be simple. You can find example usage in sample apps and tests. Let me know if you find anything unclear, I'll know what to document first.
&gt;Isn't that used for Razor Pages? It declares the file as a razor page and thus it's model would be passed into it from a code behind file rather than a controller? That's exactly this.
From [the announcement](https://github.com/dotnet/coreclr/issues/13369) several months ago: &gt; The SDK has not been made available for ARM32 to yet as it doesn't fit within a reasonable performance envelope (space and time) for ARM32. We will resolve this although it isn't a top priority currently (unless feedback suggests otherwise). Our belief is that building on a desktop or build machine will be both more efficient and a reasonable work around to doing development on the Pi for the time being. We also wanted to prioritize the runtime being available since it's necessary for anything to work. I don't think anything changed since then.
A massive windows server (running SQL standard) with 8 cores and 256gb ram installed with a hot failover on the client site with the structure that held the robots. The system consists of 1-500 individual robotic units used for moving materials. Along with this there are the various lifts, conveyors and gate systems both custom built and some integrated vendor stuff. I think that is about all the detail I can give without crossing NDA lines. I did not design the system, the person who did decided that each entity would get no only its only service but would get its own thread in the client software. This creates insane levels of contention. I was not tasked with building it, just making it so calls to data did not take forever while various methods just wanted to read a value in a locked collection.
This is awesome. Keep up the good work!
Thanks for your tip on the timespam field! I went back through my code and found it and I added it to my edit view. I'm all good to go!
Have you used them? Are they any good? I'm having .aspx flashbacks when I think about them, but I hear they're more like MVC, just that the code behind is a model and a tiny controller for one action?
each bot and control system sported its own CPU, the bots straddled an odd line between being proper standalone robots and glorified RC cars. Each had 2 custom boards and processors connected to the warehouse network via a WiFi network and an assload of repeaters (wifi hates steel)
But the machine had only 8 cores. Having 1000 threads from the application on such machine is just wrong, no matter how hard the OS is going to try to help you. 
*Excuses, excuses!* :P Anyways, one suggestion I have is to add screenshots of what the actual outputs look like for the code examples you have in the `ReadMe.md`. I think being able to see both the sample code and what output it produces right on the first page of the repo would be extremely helpful. One other thing you should also probably do is add a link to the library's NuGet page and brief instructions for installing the library via NuGet command line (i.e., the install command for the package that you can just copy and paste into the NuGet Package Manager CLI to install the package). These two little things can go a LONG ways towards getting people to use your library. 
sent a dm
Good point on the screenshots. I wonder whether `gh-pages` is an appropriate place for the images... 2 of the 19 badges at the top of the ReadMe file are actually linked to NuGet. Good luck finding them though. 😆 The instruction on installing using NuGet is [somewhere in the middle of the ReadMe file](https://github.com/Athari/CsConsoleFormat#getting-started). There's no link there though... need to fix it.
&gt;I wonder whether gh-pages is an appropriate place for the images... There's a number of different ways you can do it. Many people just add the screenshot images right to the repo (either in the root directory of the repo or in a `\Images\` subfolder), as it generally makes linking/embedding in the `ReadMe.md` easier. Another option is to upload the image to your library's GitHub wiki. &gt;2 of the 19 badges at the top of the ReadMe file are actually linked to NuGet. *Good god*, I didn't notice how many badges you actually have! It's actually kind of impressive! &gt; The instruction on installing using NuGet is somewhere in the middle of the ReadMe file. Ah, well I would recommend putting something minimal at the top of the page (similar to how [ConsoleTableExt does it](https://github.com/minhhungit/ConsoleTableExt#nuget))... like just adding: &gt;### Nuget &gt; PM&gt; Install-Package Alba.CsConsoleFormat at the top of the ReadMe (just below all of your badges). I know that ***I*** (and I am sure I'm not the only one out there like this) generally will generally skip the entire "Getting Started" section unless the starting point and usage isn't apparent from the example code that's there. So including that line at the top makes it more likely that people will be able to see and locate it quickly. 
I'm no expert in Identity and Claims, but I was always under the impression that each Claim is supposed only to represent a single piece of information. I found this statement while [skimming the documentation](https://docs.microsoft.com/en-us/dotnet/framework/wcf/feature-details/managing-claims-and-authorization-with-the-identity-model): A claim is the expression of a right with respect to a particular value. A right could be something like "Read", "Write", or "Execute." A value could be a database, a file, a mailbox, or a property. Claims also have a claim type. The combination of claim type and right provides the mechanism for specifying capabilities with respect to the value. Perhaps the values can be saved as CSV within that particular claim?
I've never got round to using Piranha but I've been aware of the framework for some time and it's one of those I keep an eye on. Now it's cross platform I might have to give it a try. Nice work!
I am working on a project now that I am evaluating options for so I am excited to check this out!
Thanks! If you have any questions, just post them here or at twitter to @piranhacms or @tidyui
&gt; Piranha CMS is a totally **headless** CMS, meaning that it doesn't force any HTML or restrictions on you. Build your website in any way you want. But that's not what headless means...
Do you have any screenshots?
I can't tell you why its not working without seeing the full code and debugging it. Here is code that I used to add multiple 'Roles' to a users claim which is basically the same thing you are doing foreach (var role in userCreds.Roles) { claims.Add(new Claim(ClaimTypes.Role, role)); } Its just adding multiple 'Role' claims. I have never had a problem with this.
i've not taken the time to figure out how to move docker images between machines. so i either need to copy over a published project then dockerize, or copy a docker image between machines. just kind of a pain.
I just updated my post, that I found the answer. I have to just query for the claims separately, instead of relying on the userIdentity to come back with all of them. I may try also updating the current principal to see if that fixes it as well, but for now, it's at least nice to know that this *definitely* works: var allClaims = await manager.GetClaimsAsync(userIdentity.GetUserId());
I checked out Piranha a while ago and it looked like it had a lot of potential, but it seems to have some build errors and compatibility problems. I ended up writing one of my own, but excited to try it out now for Core; I love minimalist CMS approach!
 Both yes and no. Headless means a CMS without a front end which only has a backend API and this is 100% true. The core of Piranha only contains an API with repositories for accessing the different models, how you wish to export that is your choice (WebAPI or whatever). On top of that there are also packages available for middleware routing and so on for AspNet Core, but these are optional, and if you’d like to build an Angular web only accessing the API’s that’s fine too! Regards
Ah, yeah, mine will just build in App Veyor from github and then Docker Hub's automatic git watcher will just pick up the release tag and publish it publically for me. I imagine you'd have to do some more setup if you wanted to keep it all private. The path for me seems pretty easy and automatic, which is why I'm pursuing it.
Still no descent spatial queries support.
Nope. I'm working on a follow-up piece on that topic, but it's pretty messy and extends well beyond the EF Core team.
https://i.imgur.com/oMXnMgM.png — output from sample project.
ITT: people who don't call Mapper.AssertConfigurationIsValid, don't write tests and like to code boilerplate? I find that AutoMapper actually helps me in remembering to handle added/changed properties.
The lack of perspective in this post is almost embarrassing, and its clear that the author hasn't been developing for very long or with that many languages. &gt;Your role as a developer is essentially the same as it has been for 50+ years. No it hasn't been. 50 years ago (circa 1968) there's a high likelihood you'd be using [punch cards](https://en.wikipedia.org/wiki/Computer_programming_in_the_punched_card_era) or maybe programming in something like COBOL. Neither of which resembles any sort of modern programming that is done today. It feels like the author's general thesis is "it's all tooling", and that tooling is purely for convenience. That's a gross understatement of what tooling allows from a development perspective, and how that tooling shapes our process as developers. Given that logic a compiler is simply a developer convenience. I'm sorry, it's not. Without a compiler, my response is not, "well okay I'm just going to write assembly then". Chances are I'm not writing any code at that point. I'm not sure why I took the time out of my day to write this response other than to make sure that someone else didn't read this article and think the author might be onto something.
Think what u/kiksen1987 is saying. Why would I use this over other CMS that offer headless options. Like Umbraco, and Kentico. And then there are ones that are not headless like Evoq, Sitecore, and sitefinity All the mentioned above have a good maturity. Also, you mention features. I'd call out why those features are better then the same features of the CMSs I mention. Other than cross platform I didn't see any feature you mentioned that one one of the others I mentioned don't do already. 
Man, can we please blacklist this site from being posted? Everytime I've seen a topic from it, it's blasted down to hell. And rarely does it add value to this sub. 
To date, as a DevOps person. I will *never* use Cake for its over complexity, steep learning curve, and horrific troubleshooting issues it has. Developers *hated* using and creating MAKE files. Why in gods name are we moving the clock back and adding something like this to the mix? 
Neato, I was just looking for a CMS for .NET Core. I always found orchard and umbraco to be inflexible and kind of a pain to get going with.
Cool. I'll check out the source once I'm back from holidays.
The only thing I can think of is, you remove some code, and then some queries end up never happening because you don't access that data. Versus, you remove the same code, but forget to remove the eager loading code, so you're still retrieving that data. I still think it's not great practice to "hide" your queries with lazy loading. At least if you do have a performance issue, you can see exactly where it is with eager loading.
I need to access the data property in a strongly types way. Your solution doesnt work for that use case
I would say a good reason for trying it would be that you want a CMS for .NET core. Also the content model is very flexible and easy to set up and there’s a very short learning curve if you’re already familiar with asp.net MVC. Another good reason is that it’s possible to add Piranha to an already existing application without having to rewrite anything to suite the structure of the CMS. Regards
A time you have a method which returns a bunch of data in which sometimes you use a lazy loaded navigation property and sometimes you don't. With lazy loading the navigation property is loaded only the first time something tries to access it. With eager loading you have loaded this data every time the method is called, at the point the method is called. Of course you could create two separate methods, one which loads the navigation properties and one which doesn't, but this works only if you know all of the times you will need the navigation properties and all of the times you won't. This becomes less feasible when you're doing something complex such as parsing a tree where the depth is unknown until runtime, where it is based upon the data itself. An additional use case would be where the performance overhead of getting all of the data in one query slows down execution by blocking expensive work being done on the parent objects, where the navigation property values aren't needed until the end. So if you were getting a Student, performing some expensive computation on the Student, then just returning the Grades, if the Grades query was such that it took significantly more time to execute, then you could lazy load it, and process it in a separate thread. Another example would be somewhere where the lazy loaded navigation property values are constantly updated, you have some work to do on the parent class, and you want to defer getting the property values until the last opportunity so that the latest data is worked upon, e.g. producing a daily sales report in the middle of the day. There's also the case where your application isn't some super high performance solution and is just your company's internal parking slot booking app and lazy loading prevents 4 nested .Include() statements - situations where cleaner code is nicer for the team than saving a few ms off a SQL query which still happens in half a second. Also in a high throughput system in which the data is very large, it can be a memory management strategy - loading data into memory for as short a space of time is as necessary may reduce the average memory footprint of the application. But, like anything else, if you don't like it, don't use it. A lot of people run SQL Server instances when they could be running much lighter options. The trade off is performance vs maintainability and familiarity. Programming is full of them :) 
I've been out of the game for a few years, but I admit I've always been intrigued by .Net Core. You can compile once in Visual Studio and run it on Win/Mac/Linux ? Is there an implementation for Android/iOS? I'm aware that they already bought a solution there in Xamarin.. but it would be awesome if this were an alternative. Or is this mainly a server side solution to cross compatibility? Is this just code in a command line or can you create user interfaces with .Net Core? I was burned once when I adopted an amazing cross-compatibility technology from Microsoft (Silverlight). You think .Net Core is here to stay?
I am not 100% aware if VS itself can build Mac and Linux binaries, but the `dotnet publish` command sure can do. It can build so called "self contained deliverable" (SCD) builds that bring their own executable and even include the .NET Core Library/Framework binaries that are needed. So you need separate builds for each platform, but you do not need any code changes to switch the platform; just switch the RID parameter on the build/publish command. (RID = Runtime Identifier) As far as I know .NET Core so far ist only powering ASP.NET Core (web), UWP (Windows Store Apps) and the command line right now. But as Core and Xamarin both are built on top of .NET Standard by now no one prevents you from building all your logic in .NET Standard and present it via ASP.NET Core, UWP and Xamarin in parallel. I don't think Core will cannibalize Xamarin anytime soon, but Standard will become stronger and stronger over time, so I believe the presentational layer can become thinner and thinner over time. Just keep a strong separation of concerns. I already have multiple Apps that use the same .NET Standard logic for ASP.NET Core and WPF (.NET Framework). For the reasons given I think Core will definitely stay longer than Silverlight, as it does not put any demand on the host. Open Source is definitely a good direction to head towards and Core is clear on the targeted platforms. As I wrote on the about page of my blog: I believe Core (or Standard for that matter) to be Java on steroids. So I hope it will be a strong contender in the same main field as Java in a couple of years and I think it is here to stay.
Avalonia supports .NET Core and recently entered beta but I haven't heard of apps using it and the experience on multiple platforms. https://github.com/AvaloniaUI/Avalonia
 Sounds very interesting. I will have a look at this later. Maybe I will write a review in some later post - *taking notes*
agree... that's why I prefer to use https://github.com/nuke-build/nuke
I've started down a path that you only return projections so you are only ever returning what you need. Thus you don't need lazy loading and you don't run into the too much scenario. I've gotten burned big time on this over the past few years.
For trees on sql server you actually want CTE expressions that EF doesn't do. 
If you know you will need it, why not load it all at once? trips to the DB can be expensive.
I'm writing my API in .NET Core and am really enjoying it. I like that I can write in linux (VS Code), compile and build it into a docker conatiner, and deploy on a linux server.
Are you a bot?
Last week I wrote a demo app with an API, another service for the UI (calling the API), and then a Linux SQL Server - all running in linux docker containers. 
Erm. No? Do I seem like one? I'm new to reddit, excuse me if I misbehaved on some way. 
Another bonus to this article would be to talk about enforcing ordering data by column in SQL. I know this is a .NET sub, but I feel like it would be the icing on the top. Otherwise clean article and to the point. Awesome work
Sorry, I was kinda being sarcastic. This post seemed like "If you liked 'blah' then maybe you'll also like 'blah" It felt like my Netflix subscription
True... I will try to better myself for the future. For now I try not to fall of my chair while laughing really hard.
I wish EF Core was a little more capable - then we could take the leap. Missing group by translation made the move impossible, but even the new support for it in 2.1 is super limited so I am not sure we'll be able to use it. It's what's holding us back from .NET Core entirely right now which is really unfortunate.
Also in my experience vs for mac os pretty shit compared to the windows version
why the heck is your sql server in a container? &gt; demo app got out the pitchfork too early. carry on.
No need to be hard on yourself. I thing I've noticed a lot about this community, myself included, is were pretty critical of people posting projects or websites. Especially if its for ad promotions and profit. If you do that you'll very likely get down voted or banned. Also, as a future note, if you are going to do training videos, remember that there is an unbelievable number of other ones out there, and if you want to really make ones that are worth while make sure you do something better or different than the others. 
&gt; Is there an implementation for Android/iOS? Do you want to run console apps or host webapps on your phone? Because that's the only thing .Net Core supports.
Just wondering, why is it bad to run the db in a container? Is it because the data is stored in that container with it?
I just installed Rider a few days ago for linux. I won a raffle with a free jetbrains license and am considering it.
IMO, migration is as simple as loading the project into the newer copy of Visual Studio, and then changing the framework target. 
It wasn't just EF 6. The entire .NET Framework is a bit messy. Consider .NET quirks mode(s). And then you add .NET Core and its cross platform capabilities with its inevitable future quirks and the API cleanup in the form of .NET Standard (along with the wish for .NET Core to NOT carry forward any of the non-.NET Standard APIs which are mostly crufty)... and you have to ask yourself: how the heck would you make all that work? You can't. So, you do the smart thing. You start something new. You implement to the lowest common denominator functionality first, because that's how WORA really gets done (I know this isn't Java, but there's definitely some deja vu here), you implement against a new non-crufty standard, and then you start implementing the functionality everyone is clamoring for: EF, WCF client, etc. and you leave the old stuff behind. Anyway, have you looked at the Windows Compatibility Pack in .NET Core 2.1? I don't know if it covers what you needed, but it seems like a fantastic start.
As my company pays for my Rider license I only use VisualStudio for some quality control plugins, some ReSharper features that didn't make it to Rider so far and the better resource file view. All the rest of ASP.NET Core development is 100% Rider for me - I love it! Especially the refactoring tools. Probably would buy it for myself, just for hobby development, if I had to.
I understand that. But the only thing I have a problem with is EF. The reasons for the transition to .Net Core make a lot of sense. However, EF Core is an unacceptable replacement to EF6 in every way (unlike other libraries such as ASP.NET Core which is improved in many ways). Not only that, the team seems to move at a snail pace only having achieved basic group by support several years into the project. Which tells me they lack resources and support. It's really unfortunate as it is such a critical piece of the stack. What other ORMs work on .NET Core (that are not micro-orms?).
yea that's generally the idea. containers work best to deploy stateless stuff. dbs are by their very nature not stateless, so not a good fit for containers.
Yea, and they're moving from 4.0 to 4.6, so it's not nearly as dramatic as moving from 2.0 or 3.0. IIRC, we moved from 4.0 to 4.5 or 4.6 a couple of years ago (a very large solution, with 40+ projects and close to 300k LOC), and had a trivial number of issues to deal with. 
tldr; I'm sick of keeping up with technology and still want to use Foxpro.
I had not realized EF in .NET Core was that bad. My current team is using it, but only in a limited fashion, and I was away from EF for quite a while before this project, so I'm a bit out of touch with it overall. I do get why it would be a real impediment though. As for alternatives, I did find a couple of things. First of all, this article: https://www.infoq.com/news/2017/12/NetCore-ORMs So, [LLBLGen Pro](http://llblgen.com/) stacks up quite favorably in that article. Looking at their benchmarks at https://github.com/FransBouma/RawDataAccessBencher they appear to be serious about their .NET Core performance. FWIW - They've been in this market a long time. I would expect a favorable experience with them. It's not free, but I would guess that neither is your time. Anyway, EF isn't the only gap in their .NET Core repertoire. We found real issues in the WCF client (doesn't support WsHttpBinding or encoding), and of course the desktop support, etc. YMMV when it comes to expecting it to be just like .NET Framework; because it's clearly not apples to apples. My 2 cents on that is that it's mostly a good thing, but only when the basics work correctly and support the kind of work you need to do.
Post your Dockerfile and docker-compose files
Added!