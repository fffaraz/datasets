Just checked and you’re right. Thank you!
Oh yeah my bad. I meant EF6
If you are concerned about implement of JWT based auth in a SPA then Angular is simply not a good platform for you. Identity Core can handle all this via WebAPI. If you want something in the middle of a server rendered app and SPA then Angular JS or I've heard Vue might be appropriate.
Thanks for the warning. 
You replied directly to a comment about .NET Core being awesome. You made it sound like it was a .NET Core issue, when it's really a C# \(and reflection\) issue? That makes sense.
Yeah, I think this article is hurting people's view of the feature.
Thank you! This is what I was looking for. 
I think what you really want is to get user info in the .net side and pass it to the front end using webapi. You should be able to do that pretty straight forward.
In Angular 2\+ there's a compilation step that will "compile" the markup. So having the views as razor files is probably too much of a hassle to setup. With AngularJs you could do this relatively easily, but if you get familiar with Angular2\+ you'll see there's no real benefit in using the MVC part for angular's views. If you need a guide on how to setup [ASP.NET](https://ASP.NET) Core and Angular \([ASP.NET](https://ASP.NET) Core acting as a web api\) here it is: [https://www.blinkingcaret.com/2018/01/24/angular\-and\-asp\-net\-core/](https://www.blinkingcaret.com/2018/01/24/angular-and-asp-net-core/) There's also articles on how to setup security there.
Wow, I think this one will trip many people up. Thanks for the heads up!
Thanks !
GDPR...
There's nothing wrong with the web site. The test code you linked to on pastebin isn't running the same test strings. Your code doesn't take into consideration empty strings, that's why you're getting an IndexOutOfRangeException on the last test example.
Thanks. I'll mainly worried about security and probably using user Identity in ASP.Net Core so hopefully that'll help. Much appreciated!
Ahh, good old edge cases
Well, if Apk07 adds a hub then it should work, but not interacting directly with WebAPI stuff. Just remember that the OLD SignalR \(with jQuery dependencies and based on the full .NET framework\) is not compatible with the the new SignalR so you can't mix and match client/server versions, both sides need to be the new SignalR version \(and same version number at this point in time\) to work.
Glad you got it knocked out, congrats!
You can add a dependency on ILogger in your constructors and use that. 
Depends on your need. I tend log little more than errors and exceptions in my controllers. My repositories yet a bit more logging. 
Like informational logging?
Every layer. Why not? inb4 someone moans about too much logging, that is what log levels and categories are for
Yeah. My controllers have very little business in them so there's minimal logging opportunities in them. In my repos I have more stuff logging what is requested and what my data layer returns and how that checks out. But don't take what I'm writing as law. I'm only an amateur. 
Inb4 `_log.LogDebug($"Adding 1 and 1 returns {sum}.");`
I think if someone has such an aversion to logging that they start malicious compliance like that I'd write them up.
My logging rule set is logging what a dependency returns when I call them. Esp if I have logic depending on what's being returned. Like I said in another comment, I'm no progressional but I guess that's a good practice. 
Performance impact? 
Valid response ^^^
Well if you're using a decent logging library such as NLog it can be setup to have async logging so the performance impact might not be a problem unless you're seriously over-logging. My point was more if someone is being a massive dick after asking them to log more, by logging tedious shit like "1 add 1 is 2", then I'd not want to work with that kind of person.
Yeah that's OK. I also tend to log what if anything gets passed to a dependent too.
Yeah. But for me in my personal project it's so I can follow what being done. 
Gotcha, so you log on what deps return, and also from within a dep, what arguments were passed.
It made sense to me. I don't necessarily know what 'magic' happens inside the dependants. I know what I put in and what I get back. So it made sense for me to record that. 
Sounds good. Ever had any performance issues?
Gotcha. Thanks for the input. I'm definitely going to pick this up.
`if(!String.IsNullOrWhiteSpace(str)){}` should do the trick
This is great. Thanks. Identity as a library sucks btw. Especially when you scaffold it, it only shows up as razor pages.....no controllers...
yeah I just noticed that myself, thanks though :)
I have a single web app up with no db, it uses sqlite, and no traffic. I am using it for learning. I use up my $40 pm credit in about 3 weeks,. 
&gt; why the downvotes? Probably because you're complaining about a very specific issue in response to a general statement that Core is kicking ass. It's off topic, and just generally annoying. There are better times and places to voice that kind of concern.
And then there are threads waiting on sync I/O
Every week a new update when I start to work. Sigh, guess I have to.
Duplicating data isn't necessarily a bad thing. I recommend letting users save a copy and if that copy diverges at any point, duplicate it then. If you want to get into minimizing storage use, look into tracking changes with Deltas. Tbh, that would probably slow your app down more than would make it worth saving the disk space.
NLog will use `Task.Run` etc
Yes. Those are the courses. He starts off very basic. Give the first full stack course a try. Good luck!
Largely yes, but if you're looking at the plaintext benchmarks, expect the top 10 to diverge soon. They're all saturating the gigabit ethernet, so they're going to upgrade to 10gig.
lol why? I check out the release notes and if nothing is of interest, I dismiss. There's always next week :)
Check out this book https://www.jerriepelser.com/books/airport-explorer/basic/intro/ Its free, and recent, but there are faster ways to create the app than typing all the code in.
Thanks for your comments, i have incorporated your suggestion. 
Here is my Azure bill [https://imgur.com/a/ZuUWvLU](https://imgur.com/a/ZuUWvLU)
This. Taking every update is lazy and potentially problematic. There's a reason IT teams choose to roll out updates manually. If it isn't broken don't fix it. Update logs will tell you if there's some potentially fatal security issue and you can then evaluate if it will apply to you or not. Beyond that, not worth updating unless you're looking for certain bug fixes or features.
I don't use the built in git client, but this has an important security fix for those that do https://blogs.msdn.microsoft.com/devops/2018/05/29/announcing-the-may-2018-git-security-vulnerability/
So if you plan to do this all in vs, step 1 is going to be to make sure you have the latest version of 2017 pro or better with all the mobile app and web app tools installed. You would have a small handful of shared projects likely written to be shared PCL style and then specific web projects usually one for the web UI and one for the services though some people roll them into one app. Then you will have a project or 2 for your mobile app. Poke around MSDN and Xamrian developer forums for more ideas, I'm sure someone has played with this some. 
Hmm... I can't, it was a Dev Essentials account and can't be moved 
Ive got a lot of inspiration from https://www.microsoft.com/net/learn/architecture and are heading into a Clean Architecture/Domain driven design approach with a Business/Core, Infrastructure and interface(s) (web, mobile, api ect) projects in one solution. The hard part is defining the business layer and infrastructure so it makes sense and I easy to maintain. 
Ah. I'm not sure if it's something that's worth your time but you can file a support ticket and get it moved. I did that in the past when my MSDN year was up and I needed to move an app to a different service plan. I don't have a support plan but MS was very helpful and responsive. Surprising actually.
Trace. Result only in Debug
Yeah that could cause a lot of tears.
Trace doesn't provide log levels like ILogger does. Nor filtering. And with logging in ASP.Net Core, you can setup logging to console, to debug, to file, to a database, etc. 
... Yea using NLog you get all of that
I'm on VS 2017 Enterprise edition. I have all components installed as well. tried looking around msdn/xamarin, just not sure what are keywords to look for :| 
thanks for the link. I'll give that a go. 
.NET Core 2.1 is significantly faster in areas related to memory management, and GC usage. The fact that ASP.NET Core is now at the top tier of all web performance benchmarks is pretty awesome. In fact, many of the others at the top are research projects, and not general HTTP frameworks, so it's a really a huge milestone for the .NET team to score that highly.
Thanks for the detail, it looks like C# is definitely the way to go. Since I'm pretty much starting afresh is it a good idea to become familiar with C# in general before touching ASP.NET or can they be studied side by side?
i have built some basic xamarin app, mostly following tutorials :) i'm totally lost when it comes to building cenrtral db with mobile app and web views. thanks for link. sure will check that out. 
What do you mean? Are there not examples of doing things like JSON over HTTPS with a web app from a xamarin app? Its does not need to be anything crazy, a basic WebAPI application with an ASP.NET MVC app for web UI is all you need. It sounds like you are needing more of a primer on the whole client server and communications side of things? 
All of the examples and code samples will assume a basic understanding of C#. There are endless ways to go about it, and most of it depends on what you already understand...but if I had to give suggestions I would say most of it depends on learning bits of the library more than the language, but here is an example path you could take: 1. Basic C# syntax \(methods, classes, basic types, generic lists and dictionaries\) 2. You need to know basic OOP \(though modern OOP doesn't favor inheritance very much anymore, keep that in mind\) 3. The dotnet command line tool \(use it to create a new project, build and debug...though you can skip this step if you prefer using Visual Studio for these operations\) 4. Read up on the MVC pattern in [ASP.NET](https://ASP.NET) MVC and Razor templates \(or alternatively learn about Razor Pages, or a JavaScript front\-end framework like React or Angular, though the JavaScript approach is much more to learn\) 5. Learn about Interfaces and Dependency Injection 6. Read up on Entity Framework or Dapper if you plan to use a database to store and retrieve your data 7. If you plan to secure your app, learn about identity framework At this point you'll have enough knowledge to create simple CRUD applications and could start branching your focus off to advanced C#, Web API, CI build setup, deployment and hosting, and more sophisticated approaches on the client/browser.
Fantastic framework. Replaced a terrible CI pipeline with it. Never looking back. 
Are actually saturating 10Gbps network; so needs to go higher (20GBps - 100GBps)
 &gt;&gt;&gt; ASP.NET WebHooks The core of ASP.NET WebHooks receivers has now been ported into ASP.NET Core, allowing you to easily implement WebHooks. WebHookes are a simple way of sharing event notifications across different sites and services on the internet. Unfortunately this didn't make it into 2.1. The preview is looking good though. :( &gt;&gt;&gt;HTTPClient and HTTPClientFactory Apparently we developers kind of suck as using HTTPClient correctly. We create to many of them, for one thing, and they can eat up all the sockets on our machines, greatly impacting performance. HTTPClientFactory comes to the rescue by handling HTTPClient lifetimes for us. To be fair, it's more that HttpClient is a terrible abstraction. Mixing thread safe shareable functionality with state in one class, i.e it's expected to be reused but offers non multi thread safe configuration properties like BaseUrl. 
We've switched (almost) all of our corporate projects over to Cake. Better than the MSBuild XML. Less boilerplate too in the build.cake file. (We deploy using AppVeyor to Azure Web Apps.)
2.1 also has significantly faster startup times. I've been writing a daemon for a raspberrypi (armhf) and comparing the startup times between 2.0 and 2.1. 2.1 boots significantly faster, maybe 2 or 3 times faster.
On the older .NET MVC/WebAPI side, we use the approach of wiring up Raygun at application start to log any un-handled exception. (Pretty sure we could still do this with .NET Core, but I have not done it.) This means that we have very little need to pass `ILogger` around or constantly catch &amp; log in the C# code. Less boilerplate code == less lines to maintain. If we have a case where we need to catch the exception, add useful context (like the UserID and a more human-readable error message); we will catch the exception and wrap it inside a new exception that gets thrown (so we have both a better top level message, but still have the inner exception). For everything else (performance metrics, dependency metrics), we lean on Application Insights in Azure.
We went with Vue over Angular for that reason. It was originally a web MVC application that called a few other APIs (but also had its own database too). The application is getting close to being more of a static site (using Vue) and composing/components against APIs. It no longer has its own database. We did not have to 100% throw out the old MVC+Razor views site while switching over to Vue. So it was a more gradual migration with not as much "big bang" (except for moving the database models out to APIs).
I’d like a nice console that I could do all my git in. One that is much faster than the power shell console to open, and maybe that could pop open with a hot key and would put you in the root of the solution. 
I recommend conemu. Look at the quake style console shortcuts, hit ctrl+tilde for a persistent slide out console of your choice.
Yes I use it, but without the slide out. Mainly because sometime I like to have multiple instances not just tabs. 
We despise browser link and disable it. The problem we have with it is that when you stop execution of the site locally, the browser window forgets what it was doing. So you can't: - Start the site - Spot a minor error while using the local site in the browser - Stop the site, fix it, restart the site - Continue where you were in the browser (possibly by hitting F5 or Ctrl-R)
Did you actually read the post? “Strings” is not the same thing as “String”. If you weren’t supposed to use the “String” class then your code would have far more thugs to worry about. String.IsNullOrWhiteSpace() is one of most essential methods in the tool box. 
i think people get itchy when there’s an update. new shiny! i would usually recommend updating the minor releases, but for the patch releases... whatever. take it if you’re blocked on something.
conemu stopped accepting the right click+paste in wsl shells for me. driving me nuts.
ilogger is just the interface. youll have to check how the implementation works to be sure of any perf impacts.
I'd store user created programs and the sets that they consist of as JSON, and the individual exercises a user can choose from to add to a program as relational data. If you store the programs as relational data, the table that stores the sets that make up each program is going to get huge.
you have to be careful with some of this stuff. task.run for every log is way too expensive. but background threads that never shutdown can hang your application. you can run the actual logger as a background service that gets gracefully shutdown at as the process exits. imo the default console logger implementation doesn’t do this well, but maybe there are requirements i’m unaware of.
its not slow if the actual output of the logs is done somewhere else than where the log was emitted. log() sends to a queue, and theres a thing that just chews that queue and sends it wherever. that way the hot path is just and add() to a queue. if you’re having perf issues with logging, solve the perf issues at their source, don’t log less. logs are all you have in prod.
exceptions arent the only interesting thing. applications can do the wrong thing without throwing. having positive logs are just as useful as negative ones. positive logs say “the problem wasn’t here”.
This is because the runtime builds are being crossgen'd for ARM32 with 2.1. I don't think other platforms will see the same improvements as their builds were being crossgen'd prior to 2.1. Also, ASP.NET Core builds are not yet being crossgen'd for ARM32 and startup performance is still poor.
I'd like to know how this will affect FaaS cold start times, because currently .NET Core is two orders of magnitude greater than Python and Node on AWS Lambda.
You can run all of ASP.NET Core on .NET Framework (as its .NET Standard so works on both .NET Core and .NET Framework) However you can't use it with the previous ASP.NET System.Web stack. Can always create a new project for the SignalR stuff? Don't have to migrate everything.
Span and other optimizations play a part but the real reason ASP.NET is so high is the ASP.NET team have added a "platform" level (i.e. low level) benchmark to TechEmpower. There is no middlewhere or HTTP objects in it, just the raw request and response streams. The platform benchmark is the one you see near the top of the results.
Logging always was a cross-cutting concern. It needs to be "everywhere".
That's BS, string is the same as String so you can try `string` instead. Also, String has nothing to do with VB.Net. i dont know about code wars but it sounds quite shitty. 
A skilled developer can only give new vertical to anyone's website if the website developed well it generates the revenue well.
I’ve never had any problems with using nlog 
Try setting `&lt;TieredCompilation&gt;true&lt;/TieredCompilation&gt;` in your .csproj - is a new feature in .NET Core 2.1 About halfway down the page under `Tiered Compilation` in https://blogs.msdn.microsoft.com/dotnet/2018/05/30/announcing-net-core-2-1/
NLog is popular and is widely used, i suspect they have taken that stuff into consideration. The original "default" logger for aspnet core *was* synchronous though. So if your log sink is open source, it's worth poking around and seeing just what it does.
Then update in the evening or when you get your coffee or lunch break.
Fuck off scumbag, that's a malicious site. There are no benefits only negatives to outsourcing from india anyway. reported
&gt; However, to say that you need slow hashing functions, is flat out a lie! - /u/mr-gaiasoul (/u/lizaard64's alt) This guy's software uses single-iteration SHA256 with a system-level salt to hash passwords, and thinks this is not only secure but that "5.7 clock cycles" of hashing effort is too expensive for the likes of twitter and facebook to use. He legitimately, genuinely has no idea why this is wrong. You can see the code [here](https://github.com/polterguy/phosphorusfive/blob/4361a1d388571318afc174385de74a84349f1dd9/plugins/extras/p5.crypto/Hash.cs#L126), called from [here](https://github.com/polterguy/phosphorusfive/blob/4361a1d388571318afc174385de74a84349f1dd9/plugins/extras/p5.auth/helpers/AuthenticationHelper.cs#L116) If you care about security at all, stay well clear of Phosphorus Five.
Watch out, this is the same poor individual with a number of (genuine) psychological problems (e.g. believes the NSA gave him a nuke, the CIA try to kill him via hitting him with cars, other tinfoil nonsense). He has *somehow* got his stupid articles published on MSDN online mag a few times. Last I heard MSDN were reviewing if they will allow any future articles from him due to the comments raised last time. Like /u/Cifize says, this is yet another bullshit product that should be avoided. Take a look at his insane commenting or his blog if you don't believe me.
just fuck off u ash hole. 
Always like 2 GB of updates too! I assume they are not using delta updates. Each update like installing a full Windows OS.
.NET Core has gone from 1.0 = Interesting but not directly in my field of work, 2.0 = I can see where this could get useful, 3.0 = \*drooling happens\*
If you're working for a business .NET Framework is still massively relevant, probably often moreso than .NET Core, but if it's personal projects I recommend .NET Core! It's not that MS is abandoning .NET Framework but reading between the lines, they are putting .NET Framework in maintenance mode. MS rarely makes things totally obsolete and dysfunctional though, it's just that they aren't pushing forward with it anymore, just mostly fixing bugs.
I think it hurts Microsoft a bit to run Xamarin under its own brand. I'm not sure why they are doing it, if their bought technologies aren't up to .NET "quality" or what, but whatever it is, I wish they would clean it up and fix whatever needs improving, and make it go directly under the .NET Core API and namespaces.
Thanks for the feedback. I tried to improve it. Hope you will like it now.
ImageSharp
&gt; positive logs say “the problem wasn’t here” There is a fuzzy line there, and think positive logs fall more into the bucket of "debug information". How you handle that can depend on how your application is structured and whether you can easily recreate the error in a local development environment. In MVC applications, errors should be reasonably easy to reproduce locally. You poke the web server with request X, which causes problem Y and stepping through with a debugger and well-chosen break points should make it easy to drill down to method Z. In batch operations where the error only occurs after a period of time (hundreds / thousands of iterations) and not on a specific record within the batch; debug output becomes more useful. But before doing debug output, I would attempt to add a try/catch block around the spot that is blowing up and put a break point on it. That is, assuming that you can recreate the problem locally (or have a QA environment where you can attach the debugger to the QA server). In those two examples (where you can recreate the error relatively easily), outputting debug statements everywhere would clutter up the code for little to no gain. It can also hide exceptions by drowning them in debug/info lines. On the flip-side, there are definitely cases where the call stack trace is not enough. It may be that the error can't be reproduced in local development or QA environments reliably enough to hook a debugger in. Or it's a language that doesn't have good debugger capability. Or it's a timing issue where the debugger perturbs the timing too much. Or it's an issue where the error only occurs when talking to a 3rd party service/server and you can't replay the steps leading up to the error (e.g recording SMTP conversations on a mail server). 
Thx, I took the opportunity to include your testimonial about me in my _"testimonial"_ section on my _"crazy"_ blog. Feel free to send me whatever they are saying about my craziness, and/or the inferiority of Phosphorus Five ... :) Read more here - https://gaiasoul.com/testimonials/ You can send me a link to a screenshot on PM. All testimonials are deeply appreciated ...
https://tealswan.com/resources/articles/deflection-the-coping-mechanism-from-hell-r234/ I hope you can get help one day, but I don't feel you are willing to accept. The blatant way that you display peoples interactions with you is silly and proves how unstable you are.
BTW, I have blocked Cifize, due to _"health issues"_. In case he should be able to come up with a couple of additional gold corns about how inferior Phosphorus Five is, or how sick I am, I would deeply appreciate getting a PM with a screenshot ... :)
Speaky English?
That's because Beanstalk doesn't support .NET Core 2.1 yet - https://aws.amazon.com/blogs/developer/net-core-2-1-and-aws/
I've integrated with matlab before through traditional conduits \(drop input files, kick off an external process, pick up output files\). Is this a wrapper for that or is it a laser guided shovel project?
How are you daemonizing it?
Thank you, but this wasn't nearly as good as your previous one, which was so hysterically funny, I showed it to my wife, and we both laughed ... :D
Yes, this library wraps octave-cli process and controls it through standard streams. It also uses internal object pool to recycle processes to avoid "cold starts", since spawning a process can become costly if you do it a lot. The goal was to have one code for all platforms, that's why I didn't bother with native C++ code and standard streams should be relatively the same across all platforms.
To return to EF 2.0.3, you must remove the meta packages for 2.1 and manually add all the packages that you actually use. This is what I added to get my api running again. &lt;PackageReference Include="Microsoft.AspNetCore" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Authentication.JwtBearer" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Cors" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Hosting.Abstractions" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Mvc" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.Mvc.Core" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.AspNetCore.StaticFiles" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.EntityFrameworkCore.SqlServer" Version="2.0.3" /&gt; &lt;PackageReference Include="Microsoft.Extensions.DependencyInjection" Version="2.1.0" /&gt; &lt;PackageReference Include="Microsoft.VisualStudio.Web.CodeGeneration.Design" Version="2.1.0" PrivateAssets="All" /&gt; &lt;PackageReference Include="Pomelo.EntityFrameworkCore.MySql" Version="2.0.1" /&gt; &lt;PackageReference Include="Swashbuckle.AspNetCore" Version="2.4.0" /&gt; &lt;PackageReference Include="System.IdentityModel.Tokens.Jwt" Version="5.2.2" /&gt; 
Put lots everywhere. Exceptions are the obvious spot, but you also want them to capture events and application state. Log each time a record is created or the countdown of files being processed. This will help you track down where and when an issue arises.
Heads up: Needs newest Xcode version and new VS for Mac version installed. So schedule a few hours for this update.
ConfigureServices in startup would be the place.
The classifications used by TechEmpower are really casually interpreted by the competition. At the platform level the field is fairly close to each other in terms of capabilities and everything is rather difficult to do and get right. TE micro level aspcore-mw is fairly close to what you would normally write for a really optimized hot path in your application but other frameworks (for example Proteus) might as well be platform level and others (ex Sinatra) are almost normal unoptimized application code. At the full level aspnetcore-mvc is basically what you would normally write. Cutelyst and Gemini are comparable approaches(among those I compared). I'd argue Beego is much closer to a micro level (kinda iffy because the plaintext case is so simplistic). Revenj is an oddball because it is a realistic approach but the domain of problems that it deals with is so restricted that it isn't really comparable to other full stack mvc approaches. Still aspnetcore is near the top of all 3 classifications. I'd say TE needs to have a better classification system (for example specialized domains vs general) and more knowledgeable people advocating for the solutions being properly placed so consumers can compare things better. 
I'm using a systemd unit, the process itself is just a console application (non-forking). Today I actually figured out how to wrap that all neatly into a self-installing .deb package too. The unit file just looks kinda like this: [Unit] Description=Some Daemon # Wait for the network before starting After=network.target [Service] ExecStart=/opt/SomeDaemon/ConsoleApp WorkingDirectory=/opt/SomeDaemon/ Environment=CONFIG="/etc/SomeDaemon/settings.json" Restart=always RestartSec=5s # Try to kill the process nicely KillSignal=SIGINT # (but kill it forcefully after a 10 second timeout) TimeoutStopSec=10 # Run as a locked down user for security purposes. # Without this, it runs as root. User=some-daemon-system-account NoNewPrivileges=true [Install] # Start this at boot, when we go multi-user WantedBy=multi-user.target There's a few places that file can go, I use `/etc/systemd/system/some-daemon.service` for manual installations, while the debian package (made with `dh_make` uses `/lib/systemd/system/some-daemon.service`. Then `systemctl reload`, and `systemd enable some-daemon.service` to set up the symlinks to auto-start at boot. `systemctl start some-daemon.service` and `systemctl stop some-daemon.service` start and stop it manually.
&gt; Nash proved more than 50 years ago that if everybody competes to become number 1, they would only inflict harm, not only to themselves, but also to the group as a whole. That's not what Nash said. That's not even close.
No you're right, what he said is that we shouldn't compete for the same goal, where there can only be one winner. Implying in a zero sum game, it's sometimes better to not go for the first prize ... (Oversimplified) Thank you for constructive feedback ... Psst, do you have an actual reference to a quote by him about this ...? (Preferably one which is understandable by non-PhD humanoids)
This was very helpful, thank you! Is it tough getting the app to interface with systemd?
If an error occurs on the server you should be returning a 5xx code with the error
There's a lot of context missing from your question. Assuming you're asking how a WebAPI should handle errors (inc. exceptions), I'd suggesting you're looking at this upside down. You should stop coding, go grab a piece of paper, and write out the public interface of your WebAPI. It needs to be self-consistent, support every feature required, and have a standard way of returning info (inc. errors). Simply wrapping the entire result in a result&lt;t&gt; object is a little redundant, since not wrapping them in that type still results in the same exact properties (and it is one less unwrap at the other side). But consistency is important, if you always expect to return a Error property then put that in the public interface's doc you're writing. For example look at how Twilio's APIs handle errors: https://www.twilio.com/docs/sms/api/message 
What we do in current project is throwing exceptions and having custom error handler/MVC filter, that converts every exception into an `ErrorModel` that contains text message, type of error \(enum with \~10 types like unauthorized, licenseError, accountDisabled, ...\) and some other error related data and sends it to client with HTTP 4xx So the controller returns always T,but the client gets `T` \(in case of HTTP 200\) or ErrorModel \(in case of any failure\) It works well for us. It easy very easy to work with. Everywhere in your code, you always return T and throw exception anywhere you want. There are however a cases when you will have to have result\&lt;T\&gt; even with this approach: If you want to return all records, you simply return `List&lt;T&gt;`, but if you want to return subset of records \(paged data\) you have to have some PagedResult\&lt;T\&gt; that holds total number of records, current page,... I would probably choose this approach again, but I don't have any strong opinion about it. Both ways make sense and have their pros&amp;cons.
Thanks! I understand this concept, but forget about the fact that I'm writing a web app here. If an error occurs in my app, let's say I'm unable to reach a necessary external API, would you say it's better to throw some exception internally and handle it internally, or return a result object that has a property for if there was an error in the operation or not?
You're right, missing context! I am aiming my question at the internal structure of my app. Let's forget that I'm writing a web app here, but let's say I'm writing an app that has many internal services and repositories. How should all of those classes be exchanging errors?
Depends how critical that info is to your API. If it's core functionality then better to return an error than a garbled object. If it's something handleable, handle it, but if it affects how the data returned in your API is laid out, better to report that.
Thanks for the help! I think I worded my question wrong though. I am aiming my question at the internal structure of my app. Let's forget that I'm writing a web app here, but let's say I'm writing an app that has many internal services and repositories. How should all of those classes be exchanging errors?
Thanks!
I make my own (a project specific) exception when I need to throw an error that is due to business related rules rather than something like a null reference. When to throw an exception or return a result code depends a lot on your situation. ATM I have an internal debate about one. If the user's widget container doesn't exist when they try to access it, I throw an exception, if the user tries to do something in the container that is not allowed, I return a result code. I throw the exception because the widget container should have been set up before the user ever tried to access it. 
Gotcha. My main problems come from the fact that my app relies on a lot of external web APIs. Its challenging to decide what to do here. Let's say I have a repository that uses some external json web api. If the json deserialization fails for some reason, how do I throw that error internally? Do I really need to expect all my controllers to be watching for a JsonReaderException?
No worries! The app itself doesn't actually need to interface with systemd at all, it just has to run like any other console application. Systemd will monitor the application, save its console output to the journal, and automatically restart the app if it crashes (as specified in the unit file). The only thing that you should do is make sure that the app is well behaved. It should handle the SIGINT signal correctly (same signal that's send with a CTRL-C). This isn't too much of a problem because by default the app will just close, but if you want to perform some cleanup before exiting you can hook SIGINT by attaching an event handler to `Console.CancelKeyPress`. I like to use a `CancellationToken` like this: CancellationTokenSource cts = new CancellationTokenSource(); Console.CancelKeyPress += (s, e) =&gt; { e.Cancel = true; cts.Cancel(); }; And then pass that `cts.Token` down through the rest of the tasks within my application. Then, the app will have 10 seconds after that event is triggered to neatly clean up and exit before it gets killed hard.
That depends: Do you want a single exception handler at the top or a handful of smaller exception handlers throughout the stack which convert exceptions into a Result&lt;T&gt; style abstraction. What I mean is, that if the intention to to capture every possible exception and convert them to your custom type throughout your whole codebase then what will ultimately happen is that every single method you write will wind up with a full scope exception handler (because even common and local operations can return an exception in some rare circumstances, so to be consistent you'll need to be ready to capture those). The often cited advantage if this style is that both exceptions and other error types are all handled the same way. And while I have sympathy for that argument in theory, what happens in reality is that people will have one Error structure for exceptions then a different structure for their "Soft Errors" (e.g. "length too long," "value must be unique," etc) since one they want to be end-user visible and the other they don't (and they don't want to hard-code in a list of soft errors). In traditional functional languages there were no exceptions, only results, and therefore there's never a concern about code execution order, .Net languages (inc. F#) need to support exceptions so even in F#'s case (particularly when using external libraries) you wind up with ugly spaghetti code that supports both. In my opinion the functional Result&lt;T&gt; thing is a bit of a fad, and doesn't really support true functional style in .Net because .Net itself isn't functional: Exceptions aren't going away. So you just wind up doing Result&lt;T&gt; AND exceptions, which adds no value. 
Thanks for the input! I've got another Q for you. Let's say my app relies on some external web json web API. So some controller invokes an internal service to call the external API, then for whatever reason, it can't deserialize the response and a JsonReaderException is thrown. How should the internal service respond to this? Should it just throw the exception or wrap it in another exception with more data? If the second option is the way to go, \(I think it is\), what exception would I even wrap around the JsonReaderException? Thanks!
It depend how critical the external api is. You could catch the exception in the method that calls the external api and throw a custom ExternalApiException. I've been using an ErrorHandlingMiddleware to detect specific exceptions and format a response, so there are no try/catch blocks in my controllers. 
Thanks!
Really depends on the error, and if it should be an exception. As the name implies, I try to make throwing an exception an exceptional event. For instance if there is bad input, I don't need to throw an exception. If there is a problem connecting to the database unpredictably, there should be an exception thrown. For me I almost always return a result&lt;T&gt; sort of object and it has been pretty successful for me. This is a version the simple class I have been tweaking and using for years: public class ServiceResponse&lt;T&gt; { public string Message { get; set; } public bool Success { get; set; } public T Data { get; set; } /// &lt;summary&gt; /// Use if generated from an exception /// &lt;/summary&gt; /// &lt;param name="ex"&gt;Generated exception&lt;/param&gt; public void SetToError(Exception ex) { this.Data = default(T); this.Message = ex.Message; this.Success = false; } /// &lt;summary&gt; /// Use if controlled problem /// &lt;/summary&gt; /// &lt;param name="message"&gt;Problem to respond with&lt;/param&gt; public void SetToError(string message) { this.Data = default(T); this.Message = message; this.Success = false; } /// &lt;summary&gt; /// Set parameters for success /// &lt;/summary&gt; /// &lt;param name="value"&gt;value of the response&lt;/param&gt; public void SetToSuccess(T value) { this.Data = value; this.Message = "Success"; this.Success = true; } } I find checking response.success just as simple, if not more so, than checking the response code and it would include more pertinent information. Note for more public services, probably don't want to include the exception message plainly and set that to something more palatable. :)
Thanks!
There's two rules about throwing exceptions: * Exceptions are for exceptional conditions * Exceptional conditions are defined locally Take registration for example: * Username exists already * Email exists already * Password doesn't meet criteria * Password and match field don't match * User didn't accept the TOS * User didn't fill out a required field These may or may not be exceptional for your validation. I recently decided they were but later regretted they because it locks me into an odd control flow and depercation will be finicky - I wish I'd gone with returning a list of failures because validators could look at the whole thing and make multiple decisions rather than raising a ruckus about one thing. On the other hand, if a user manages to get into a flow that has preconditions - e. g. a checkout flow with an empty basket - that sounds exceptional to me. 
&gt; If the second option is the way to go, (I think it is), what exception would I even wrap around the JsonReaderException? Create your own? You need an exception that takes an exception but also takes custom data, only you know what additional data it needs so you are best placed to create it. 
Probably just have to wait until Swashbuckle catches up. Have you checked their GitHub site yet?
I don't mind waiting for the package to get updated, but I wasn't sure if there was something built-in in 2.1. At least I can write a test now to ensure the ProducesResponseType matches the return type :)
Actually that is not a lot of data for each user if you use an efficient format to store programs for parsing. 1MB of just text, is ALOT of text \(about 500 printed pages\). And if you have 1000 users, that is 1000mb, which is just 1 GB. And i doubt all your users with have 500 printed pages of workouts. 
how do you do that ? curious :)
Reflection on the controller methods.
Did you check this article already? https://aws.amazon.com/blogs/developer/net-core-2-1-and-aws/ It should contain info to make 2.1 work. 
More the syntax.
Hi, never used Octave before but have a hobbyist's interest in statistics (I made a markov bot once). What are some projects I can use this with?
I basically work on the principle that exceptions shouldn't control program flow and should be for exceptional/unexpected behaviour. If you are validating something return a validation result if you try to open a file and it's locked throw an exception. If you expect there to be an error it's not exceptional behaviour.
I'm not on my dev computer at the moment. What does this do? public ActionResult&lt;int&gt; GetInt() { ActionResult&lt;int&gt; result = Ok("not an int"); Debug.WriteLine(result.GetType().FullName); //&lt;----- }
Awesome. Not entirely sure how all that works but I'll figure it out. Thank you!
No worries. I've bern a developer for 11 years and I've never known how anything works 
I believe you'll want AddSingleton. Here's a snippet from the Dependency Injection help page from microsoft for ASP.NET Core. https://docs.microsoft.com/en-us/aspnet/core/fundamentals/dependency-injection?view=aspnetcore-2.1 &gt;**Singleton** &gt;Singleton lifetime services are created the first time they're requested (or when ConfigureServices is run if you specify an instance there) and then every subsequent request will use the same instance. If your application requires singleton behavior, allowing the services container to manage the service's lifetime is recommended instead of implementing the singleton design pattern and managing your object's lifetime in the class yourself.
Personally I throw errors but have very few catches no matter what I'm building. It keeps code cleaner imo 
Ok\(\)'s inheritance chain is OkObjectResult \&gt; ObjectResult \&gt; ActionResult. It's a non\-generic version of the return type, which the compiler can't force against \(since how does it know with you passing an "object" type into the Ok constructor\)
&gt;No you're right, what he said is that we shouldn't compete for the same goal, where there can only be one winner. Implying in a zero sum game, it's sometimes better to not go for the first prize ... Change that last bit to "it's sometimes better to not go for the same prize". It's why Twitter and Facebook both exist. Along with Reddit. Since they are aiming to different places, they can all win. So you won't compete with Facebook, you'd make "Facebook for people that want to communicate with each other, but not based on their personal information and only on what they post" and you get something like Reddit.
Looks like a nice change, i'll be updating my build scripts in the not to distant future.
Microsoft.AspNetCore.Mvc.ActionResult`1[[System.Int32, System.Private.CoreLib, Version=4.0.0.0, Culture=neutral, PublicKeyToken=7cec85d7bea7798e]] 
&gt; Change that last bit to "it's sometimes better to not go for the same prize". Thx, much better wording, and more accurate. I know the theory, but don't know how to express it :)
That's not good. WTF is Ok being case into that?
Notice that `Action&lt;T&gt;` isn't in that inheritance chain? There must be an implicit cast defined.
Whoa. That sounds like what I'll need. Time to dive into the documentation once more. Thanks a lot! Hours saved for sure.
**Edited version** -https://gaiasoul.com/2018/06/01/game-theory-and-framework-architecture/ Would the first sentence/paragraph be a more accurate description ...? I am gonna assume you don't want credit? If you want, let me know ...
Yes, and ActionResult&lt;TValue&gt; is implicitly convertible from ActionResult. I was hoping that the whole chain would support the type parameter so that my initial example would not compile, but it seems like its just a way to avoid the ProducesResponseType attribute
I am curious, obviously you understand Game Theory and zero sum games. Obviously you're also a developer. Do you think my arguments that a framework behaves according to group dynamics, and hence applies by the same rules holds any water ...?
No credit needed. But thanks for asking.
I take this approach but you might find that all these properties are redundant. The Success property is not needed because your HTTP status should be a 5xx if its an error. A 2xx if its a success. The Message property is really only used for error messages. Why not just made Data an error object that contains your Message? If its a success it returns a 2xx with the result. If its an error it returns a 5xx with an object that gives you all the error details.
What you’re talking about, and what other people have been answering, is Dependency Injection. It comes baked into Core MVC, and you register your services in your startup.cs file, and then you add private fields in your controller classes that are set to the instance of the service object passed into the controller through its default constructor.
All good idea, but personal preference as to why not. I prefer success because usually in my services I don't care WHY the call was not successful, just that if it was or not. If I gave a bad input, I would want success to be false, but find it misleading to return a 500. If there is some sort of networking error, it would be legit to return a 500, but again as the client I don't really care why it failed, just that it did and I should handle that. I see what you mean by having only a data object, but I feel like this is more readable, for the client and the server, to always return a ServiceResponse&lt;User&gt; and not have to overload things to be a User or an Error type. Again, personal preference I suppose.
time slicing? any project where the main "for" loop is not implemented in the m\-file
The names could be better... `ActionResult&lt;T&gt;` is a type that wraps either a `T` (represented in an `ObjectResult`) or an `ActionResult`. It has implicit conversions from both types. `Ok(object)` returns an `OkObjectResult` which inherits from `ActionResult` eventually. It would have been a really massive breaking design change if this was done in a comprehensive manner and it is actually a bit of a question just how comprehensive it should be. For example would a redirect to action need to go to a covariant method? The way it was done is entirely nonbreaking (though I think `ActionResult&lt;T&gt;` should convert the `T` to an `OkObjectResult` instead).
Yes, most things where there is competition do. It's how the Prisoner Dilemma and the 3 Girls example in the movie can both work. One of the big things with Game Theory's use of zero sum is that you don't want to compete in a Zero Sum Game. The best way to compete it so compete until it starts to become zero sum. Overall the framework components don't compete with each other, but with other frameworks that do the same things. But if we look at just the framework, ignoring other frameworks, it's a question of focus. For a framework to be the _best_ at everything they need to focus on that. This focus is where the zero sum game comes into play. If you have 100 units of time to devote to your framework development, spending more time on one of them to make it better, means the others will be worse.
https://github.com/aspnet/Mvc/blob/1aeaf69f87cbf91f4f56183eb1254f69a50b406a/src/Microsoft.AspNetCore.Mvc.Core/ActionResultOfT.cs#L48 Yep. of (minor) note, `ActionResult&lt;T&gt;` does not set a status code when converting the value to an `ObjectResult` which means the [response status code doesn't get set](https://github.com/aspnet/Mvc/blob/38712609bb2291202bfa760ee62348691de0ddae/src/Microsoft.AspNetCore.Mvc.Core/ObjectResult.cs#L51). I think that ultimately means a number of people are going to need to continue to return `Ok(value)` (I am not sure if it gets set somewhere else to a default value; could someone with a project open verify?).
Hmmm. I'll have to read up more on you're saying so it isn't quite over my head. So the service I register will be set to a private property in my controller when an instance of my controller is defined... Come to think of it, I never quite considered how exactly an action method gets called, but it makes sense that a controller object was instantiated before it was required the first time... Ok. Ok. I will look into this. Thanks ton! This will help a great deal!
In my opinion that's a stupid way to set status codes anyways. I much prefer just throwing an exception and having a filter convert it. 
If the reference can be brought into the solution via your project files then yet you can automate it with MSBuild. If the reference is bound in the .sln file then you might be able to do it but you are going to have to edit the text, there is no simple API for modifying a solution file.
The way I manage this right now is I update my DLLs via an internal NuGet server if that helps. That "modifies" the solution file to update my DLLs
Nothing wrong with any of that, then you are going to want to use poweshell and load it into your nuget package. Note that if you need to change your solution file its going to reload the whole solution. If I was doing this for a team in a larger shared project I would not allow the team to update this client side anyway. I would have it done as part of my CI processes and force everyone to the version I want. Depends on your use-case though.
That's all fine and what I do. I am able to package my changes for nuget and push them up. The issue is with the visual studio toolbox in the designer. It doesn't update. For instance, if I created a new user control and update my dll, the toolbox remains the same. In order to get the changes, I have to go through the process of removing and then readding the refence
Any script is going to trigger a project or solution reload anyone since you are changing loaded files. This seems like a bunch of extra work to me, I would just tell the dev team to restart their IDE.
You should be splitting your string into chunks. Read more about the [String.Split](https://docs.microsoft.com/en-us/dotnet/api/system.string.split?view=netframework-4.7.1) methods.
Oh... Spam. I love spam. 
Don't forget to include the following `using System;` at the top of your file.
Sorry, yeah I didn't go into a ton of detail, but you'll be better off going with some of the tutorials and documentation on Dependency Injection. I wont be able to adequately describe it all here... But yeah, I never thought about my controller being instantiated either, so I didn't even think about the default constructor. But the easiest way to think about it, for me, is with your DbContext class. You register a DbContext service in the configure services method in startup.cs. Then in your, say, HomeController.cs file, you'll have your class declaration, typically, if you don't designate a default constructor, it will assume a default with no inputs, but if you give it a private field within the class.. `private DbContext _context;` and then designate a default constructor that takes a DbContext object and assign it to that private field `public HomeController(DbContext context){ _context = context; }` when your controller is called it will bring that service into controller class, assign it to that private field, and then can be accessed from that private field and used in your controller logic. so you can do things like: `List&lt;Entity&gt; resultsList = _context.Entity.Where(e =&gt;` [`e.Active`](https://e.Active) `== true).ToList();` using EF with DbContext becomes very simple, because you aren't creating a DbContext in each Controller, you're injecting it from your startup services. I'm sure a lot of that is gibberish, but it's easy to learn. I learned about DI, EF, and the IdentityDbContext class in about a week using Adam Freemans PRO [ASP.NET](https://ASP.NET) Core 2.0 MVC I did just take a screen shot of a very basic controller I'm working on, [maybe this will help. ](https://imgur.com/a/JPQ7y5u)
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/G2h0QIT.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20dzydeh0) 
Thank you! Brand new article so didn't see it. 
&gt; It would have been a really massive breaking design change if this was done in a comprehensive manner and it is actually a bit of a question just how comprehensive it should be. Its difficult for me to judge because I felt that it was a mistake* to stop returning explicit types from actions and effective just return object in an ActionResult. (* and because I don't have a fix suggest) 
Glad I could help! 
&gt; it's a question of focus. For a framework to be the best at everything they need to focus on that Yes, an no ... I took a trip, got myself a Gyros (which is Greek for _"spinning"_, and also a beautiful Greek dish) - And I did some thinking, and realized I was using the _wrong analogy_, and possibly also the wrong parts from Game Theory. Let me rephrase ... https://gaiasoul.com/2018/06/01/software-architecture-lessons-from-norwegian-soccer-history/ I think I was close to the truth in my original posting, but I think I've _"refined"_ my thoughts a little bit ... My reasons for starting thinking about these things, is because I can't understand myself why I am able to produce software as fast as I happen to can with Phosphorus Five. I am not the best JavaScript developer on the planet, I am in no means the best C# developer either, there are thousands of software developers that knows more about security and cryptography than me. Still, my result as a whole, always runs in circles around my competition. I think I have finally realized *why* ...
This perfect. It makes so much sense now haha. Yeah, this is exactly what I needed. Ok. So all I need to do then is add my service to the container in Startup. Im reading the documentation on that so I'll be fine from here i believe. I just followed the tutorial from the Microsoft docs when I started learning all this so I didn't quite get it. But now that I legit need to use it, things are falling into place. Seriously, thanks so much. Just one more thing about the ASP.NET "pattern" that I have some light on.
well there is something like rest api standard... and you should look at it. what to return when will be more clear
Not talking about public facing code. Thanks for the input though!
it doesn't matter, FrontEnd development will be easier if needed, debuging will be easyer, maintaining the code will be easier. There is a lot of benefits using some standard set of solutions in coding. 
Sorry sorry, I'm still misleading you. The error handling that I'm talking about it's taking place purely between the class within my application, and not any of the public facing controller code.
oh, ok. then i would suggest a custom exception with a explanation what happens there, maby a result included in this custom exception but exception nevertheless. why, well standards :)
Shouldn’t exceptions be very rare though? In all the projects I work on exceptions are usually bugs that need fixing, not something you expect might happen... If you are coding for something you expect might occur, it isn’t an exception, right?
Thanks!
hope that helps. I had a lot of bugs at first becouse of bad code and nobody could help me becouse i didn't want to use any of the coding principles and standards. Learned it the hard hard way.
This might work in my base controller , but I will need to set the status codes myself. public ActionResult&lt;T&gt; Ok&lt;T&gt;(T value) { return new ActionResult&lt;T&gt;(value); } 
Do you have \[ApiController\] applied to your class?
Oddly enough, on the Code Wars site (string.IsNullOrWhiteSpace(str)) works but (String.IsNullOrWhiteSpace(str)) does not.
They are all expected cases, not exceptions. No wonder you regretted it. It is a hard lesson to learn sometimes and I suspect it is the real question the op is askjng.
I’m not at my pc so I cannot test this but why can you just return int? Why use the ActionResult&lt;&gt;?
In your csproj file add &lt;LangVersion&gt;latest&lt;/LangVersion&gt; a minimal complete file: &lt;Project Sdk="Microsoft.NET.Sdk"&gt; &lt;PropertyGroup&gt; &lt;OutputType&gt;Exe&lt;/OutputType&gt; &lt;TargetFramework&gt;netcoreapp2.1&lt;/TargetFramework&gt; &lt;LangVersion&gt;latest&lt;/LangVersion&gt; &lt;/PropertyGroup&gt; &lt;/Project&gt; the language version is not tied to the runtime (you can use C# 7.3 and target 3.5 I think)
Thats too simple. Lets find a more complex solution than that.
Oh excellent, thanks! 
Yeah, there's more to it than me just deciding that. It's a python application and for some reason every validation framework I know of uses exceptions to communicate failure - I think this is because they either throw or return the value rather communicating through a result object, I won't go into Either types here but they're not pleasant in Python, proper Either types at least. So in an effort to make others feel at home, I followed that pattern since it's so common. But ours doesn't need to return a value, they just need to check if everything is hunky dory or not. So it's the worst of both worlds. However, this locks me into an odd flow control where my plugin system needs to gather the validation things and hand them over to the registration manager which runs them in a for loop with a try/catch inside. There's a DI approach that's slightly better but the project's not there yet (gonna need to drop support for Py2 first unless we want to a bunch of nasty wiring ourselves - spoilers, we don't). 
In case you hadn't checked yet App Service is now on 2.1 
Personally I’d argue that in the context of MVC/WebAPI it’s worth violating the typical advice. Filters let you have generic error handling (in the sense of translating exception to response) logic and allow your controller code to be much cleaner. 
I guess. Usually MVC and Web API projects should have very little code anyway... All of our projects have one error handler in the global file, I haven’t used filters at all. Occasionally we will have an error handler in the base controller class. But there are so many ways to accomplish the same thing...
Seconded, with the addendum that exceptions are slow. It's like taking a debrillator to your app. Functional style has always worked better for me, especially if you make discriminated unions for them.
Sounds interesting. Is that internal middleware or a Nuget pkg?
I’ve got one too
Why not make an Error object part of your standard return envelope interface, along with a Result member? And just use a generic interface with those so your envelope can be custom to each use. This way, every return type implements that interface and is expected to be able to return it's internal Error object. You could use this Error member only in the expected cases, like validation. Truly unexpected errors could simply be thrown as exceptions. This will allow you to avoid using exceptions for flow control; which is a very messy road to go down. For inspiration, look at the JSON-RPC spec, where this is given a bit more treatment. http://www.jsonrpc.org/specification In that model, every return type is required to have a result and an error member. If there was an error, then the result is null and the error member is populated with details. If not, then vice-versa. Granted, that spec is meant to be externally facing, but if you use that pattern throughout, you'll have a consistent model you can use externally and internally. This is a bit OT, but here goes: And by the way, try to avoid calling your services REST if they just use JSON, but aren't idempotent, don't use HTTP verbs properly, and/or don't use hypermedia. It's perfectly fine to have an API that just uses REST, but if you adhere to something a little more formal like JSON-RPC, you'll give yourself more room to grow with a consistent model and a standard to fall back on when the inevitable debate about REST comes up yet again. The discussion is simplified a lot by simply determining whether your API is action or resource / entity oriented. Google for more info. 
Some language features such as async require runtime support that doesn’t exist on 3.5.
It sounds like some kind of browser error, it might be too old or you might have an extension interfering. No errors are logged to your browser console? Try disabling all extensions and make sure you're using the latest chrome or Firefox. Certain antivirus and firewalls are also known to be a problem, since they can block dlls being downloaded over the web, so try disabling those if you have one.
Vue is awesome After dealing with the nightmare that is angular (eg: it never works out of the box for some reason, and why the eff would I want to install node?), I'm finding Vue to be very easy and just what I needed.
Set what up? It's just a js include, just like you'd do with jQuery or any script file
I had to follow the instructions on https://blogs.msdn.microsoft.com/webdev/2018/05/02/blazor-0-3-0-experimental-release-now-available/ including the command lines. I suspected the problem was the SDK installation.
Please use pinned down versions for all that is holy.
10000% results over exceptions, every time. I've *seen* the absolute miserable clusterfuck of using exceptions for flow control - including "username already exists" scenarios and it is a Bad Idea^TM The functional world handles this IMO a much more elegant approach, via the concept of an "option monad". Don't worry, you don't need to be a functional expert to use this. This is the article I used https://enterprisecraftsmanship.com/2015/03/20/functional-c-handling-failures-input-errors/ My code is cleaner as a result.
I've always wanted to try this approach, but haven't had the change yet to implement it in any of my project. I'm glad to hear it's working fine for somebody else. :)
Blazor.
If, ultimately, you are allowing people to create and execute DLLs within the context of your application, they can execute any code they want. That's very hard to secure. Using reflection, it would be possible for that DLL to gain access to most of the internals of your application too, so any obfuscation could be overcome eventually as an attacker gains more knowledge of your application. It's possible to load a DLL into a separate Application Domain, which offers a degree of isolation, so you should look into that (I'm afraid I can't tell you much about them or limitations with doing this).
I think the safest approach would be to take the script based plugin approach. Using Roslyn API you can run C# as a script and it's all nicely embedded and can access the types you provide it. You can also restrict what namespaces and assemblies it can load. So you can prevent people using System.IO for example.
I had wrong sdk installed, so I went through the steps one more time and the app launched fine
I think you could write an AsyncMethodBuilder and actually use the new tasklike async patterns in 3.5... But yes you are very limited with certain language features and no library support in old runtimes. I wonder if safe stackalloc works.
For code that might potentially be worked on in multiple versions of VS at the same time, sure. Anyway /u/Trifectard, documentation here: https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/configure-language-version 
Well if you're writing Python then that's a different story. Exceptions aren't nearly as expensive in Python as they are in other languages. Look up "EAFP" or "easier to ask forgiveness than permission" - in Python rather than first checking for the existence of a key before accessing, it's more idiomatic to just try to access and them catch the exception. The way `for .. in` terminates (other than early termination via `break` or `return` is by catching a `StopIteration` exception from the call to `next()`
Exactly this. You get the instance in your code through a constructor parameter. Simple example with a controller action: public class SomeController : Controller { IOperationSingleton _instance; public EmailsController(IOperationSingleton instance) { _instance = instance; } public IActionResult Welcome() { _instance.DoSomething(); return Ok(); } } Asp.net core's dependency injection automatically injects the registered IOperationSingleton instance when constructing the SomeController object, making it available in your code. Let us know how you fare out!
It wasn't really a choice between EAFP vs LBYL, it was me thinking "well, wtforms, marshmallow, djangoforms, etc all do this, so that's what people will expect. But then I was exposed to the return a list of failures at work and my viewpoint radically changed. Like holy crap, why did I never think of this before?! It's also not that exceptions are cheaper in Python, they are don't get me wrong. It's that always checking an if block vs sometimes the file isn't there and we need to handle that, the if block is more expensive in the happy path but catching the exception is more expensive in the sad path. They're around the same magnitude (the number of frames between raising and catching affects it, but I don't recall off hand how much). In either case, I cargo culted error handling and it but me in the butt. 
Create a CommentList that encapsulates this logic so it’s not distributed into an AutoMapper profile. Allows more discoverability and simplicity - leave automapper to doing things it does best — 1:1 mappings. I went down the path of pushing complex logic into automapper and looking at how messy it gets, big regrets.
I don't have answers for you but can you share any articles/tutorials/books that helped you build a plugin architecture? I'm working on a project that needs to have people change/update/suppliment processing workflows within the API logic and am struggling.
ahh i thought it could be done with automapper ok guess theres no other way then. i wanted to avoid that comment root object having a comment list inside it thanks
Yes of course you can. But people generally won’t do that. Not sure why I was downvoted for a simple fact. 
Me either.
Hey! Yes of course. I’m on my phone at the moment and won’t be able to get on my laptop until my sons in bed but I’ll send you some resources that I’ve used. I’m currently rearchitecting mine a bit so it’s in a mangled state to share - plus I’m transitioning from .NET TO .NET Core. 
don't map a list, just map two objects and then use map it like _mapper.Map&lt;IENUMARABLE&lt;DestinationObject&gt;&gt;(objectToMap);
You’ll want the dotnet cli, visual studio code, and the csharp extension. If you need more help than that, you should be more specific. 
Why am I not surprised with this...
How does it not gain anything for use? How is this not major?! It just added an additional tool for using switches instead of Dictionary&lt;Type, Action&lt;object&gt;&gt; but more powerful as it allows for evaluation of each case and would require orders of magnitude less code to be written. It's faaaaar more expressive! And you don't have to use it. Basic switch statements are portable and will always be available for you to use until you figure these out. 
I think you need to update your skill set. This is falls right in line with so much else in the field...
The "philosophy" you speak of is really dogma, and it's how things die. Like the Windows Phone SDK and development philosophy; they were dogmatic about it which basically limited its growth and eventually it died because developers didn't wanna put up with that bullshit; developers want to develop. What they did is identify that people love C# and they want people to be able to use it in the way they want to use it without turning it into some cluttered mess like Objective-C. They are succeeding. The community is filled with comments like "I really like C# but it's silly that it cant do &lt;insert common programming paradigm&gt; like everything else can." -- This is literally the problems being addressed with these updates. They're adding to the tool belt things that allow a programmer to translate a thought in their brain to code in the most *direct* way possible. Use it or don't. I think the fault in this way of thinking is believing that someone should be able to learn the entire language at some point. Software is never complete, we know this as developers, so why should the language and/or framework ever be complete if it has room to improve? This starts to toe the line of this religious C programming vs. C# or Java, etc. It's not always about what's more efficient or performant or what requires a developer to have a deep understanding of computer science. It just isn't and it hasn't been in some time unless you're working in an area of engineering that requires it that level of efficiency/knowledge. It's 2018 and most systems we build are information systems which means PRODUCT is king. Whatever enables us to push product out faster is king. If this stupid switch statement enhancement allows me to save an hour boilerplating a complex decision structure, then let's have it! That shit adds up. So if C# can offer a solid development experience and a cohesive tool belt that offers a little bit of everything a programmer would need, then what's the problem? Don't make this like a comic book vs. movie type purist nerd war. It doesn't help anything.
The keyword you are looking for is Virtualization. Wpf has some of that built in for displaying uielements, but you probably want to do something similar with your data.
Data Virtualization is the correct terminology to search on, this seems like a promising introduction... https://alphachitech.wordpress.com/2015/01/31/virtualizing-observable-collection/ 
Hi Darth_Vaporizer, Thanks for weighing in. The title gives the impression that the post is a question. Sorry about that. It's actually a link to an article.
WPF has built in UI virtualization, think of occlusion culling in games, UI elements that cannot be seen are not drawn/rendered.
So, the guy is pretty off the wall. He is, in fact, a meme at my office now. *At the same time,* We looked at some of our oldest code at said office; it's got some very 'odd' concepts to wrap your head around, but at the same time once you 'get' it the code is very easy to maintain throughout. I then blurted it out. "Hey, isn't this just one or two layers on top of Active Events?" They all looked at me, frightened. There was no denying it. The main engine of this legacy codebase was the same pattern, done in the early 2000s. That said, there is a definite separation on the team of devs who "get" it and devs that don't. The separation is further confounded by the developers skillsets / talent not being entirely uniform among those groups either. Another way to look at things: If you did Active-events with a somewhat-more-immutable implementation, is it really that different than Packrat Event sourcing?
Any info/guides on it aside from the source code?
Nothing to see here guys, unless you like overly wordy blog posts in broken English. This one feels like either Russian or Polish, given the missing and incorrect usage of "the," "a," and "an." Save yourself some time, just read the MS official docs: https://blogs.msdn.microsoft.com/webdev/2018/02/02/asp-net-core-2-1-roadmap/#webhooks If you still don’t understand it, re-evaluate your career choices.
&gt; _mapper.Map&lt;IEnumerable&lt;DestinationObject&gt;&gt;(objectToMap); Using my existing code then i'll use this line? sorry, im new to asp.net core and this is the last hurdle. I came across automapping so i'm not yet sure what to do.
I sped up ReSharper the good old fashioned way - by upgrading to a beastlier workstation. i7 8700k, 32 GB DDR4 RAM, Samsung 960 PRO SSD.
Hi /u/drehwurm, Trying to decompose your post into multiple subquestions. Before I do that, I will give you my assumptions about your problem: Assumptions: 1. You have a collection or table of cues 2. Each of these cues has a reference to a long list of values. 3. Your concern is how to display this data to the user (read and filter data)? 4. And how to present this data keeping the footprint low? 5. And how to render this data quickly? Are these assumptions (on my part) correct? If what I stated above is correct the answer is tricky. The obvious was to keep things small is to not load all the data into memory (although 2m of ints/doubles is not a massive amount of data), this means that you only load the information you want to use each time. As you mentioned before a ListCollectionView which you feed an ObservableCollection. You should be able to only load through this object the minimum data that is required yet provide the entire information a data grid might require to render its scrolls properly and understand how to get data from the collection. The second optimisation you would need to do is to just render what is necessary, as others pointed out a virtualised panel. This will enable the wpf app to just render enough content to fill up the screen (per rows), meaning you will be recycling views/object that has been already created, just swapping the content instead of creating a new object. Why this point above makes it quicker is: 1. You are not displaying a lot of UI elements. 2. You are not destroying/discarding all UI elements, reusing the dependency properties you have already created at some point. To understand this in greater detail I will recommend you to read this link: https://docs.microsoft.com/en-us/dotnet/framework/wpf/advanced/optimizing-performance-data-binding) The link provided by /u/Warshrimp is pretty good to follow the implementation of a virtualised collection which is a further improvement you can make, this keeps the data small from the beginning, meaning you are not loading everything from the get-go, just a summarised version that helps you populate everything else. Now, I would argue that presenting 10k numbers to the user is not an engaging way to attract users and that you might need to solve a usability issue first (which is the same problem with Excel when you are dealing with a document that has tons of data).
To give you some context of why you have to opt in they decided that the default should be the highest major version, but no minor version bumps, so the default won’t bump up until C#8 is released.
Channels are a bit sparse in documentation but they are very logical. The general idea is of course an async producer/consumer queue. Where you have a *writer* (in) and a *reader* (out). In general you can figure it out pretty easily, but here's a quick start: _channel = Channel.CreateBounded&lt;byte[]&gt;(new BoundedChannelOptions(boundedCapacity) { FullMode = BoundedChannelFullMode.Wait, SingleReader = true }); There are a variety of options for creating a channel. Having a 'bounded' capacity is an obvious one. Once you have a channel you can: while(!_channel.Writer.TryWrite(b)) { await _channel.Writer.WaitToWriteAsync().ConfigureAwait(false); } and while (await reader.WaitToReadAsync().ConfigureAwait(false)) { while (reader.TryRead(out byte[] bytes)) { } }
You're not kidding, that's a beastly setup. Did you end up trying it out?
What do you think improved the performance the most? Iam sitting on an ssd and 8gb ram + semi good processor and i have sometimes microlaggs when deleting lots of code with holding backspace/writing "false/true" in a function parameter
Hard to say since I improved everything in one go, but my guess is the RAM was the limiting factor - speed not quantity. I had a decent 20 GB beforehand, but it was 1000 Mb/s DDR3. I figure R# probably needs to shuttle things between memory and the CPU quite a lot so the 3000 MB/s DDR4 I upgraded to probably helped a fair amount.
End up trying what out, sorry?
Alright i wanted to upgrade to 16gb anyway, ill just get a nice ddr4 and we will see how it goes
In code, a word isn't just a word. Was it a string literal, a variable name, a model property, or any other significant identifier ? 
Hey, sorry I'm a lil late. It worked quite well! I understand the whole dependency injection process now. Very useful.
Thats great man! 
Thats great man!
Why are you giving advice without reading the article? The article is a how to and quite frankly the title makes that obvious as well.
This has a great potential as a .Net aggregator of sorts, but the current design is horrible.
I used a different approach to write "AsyncFileWriter" which does not use Tasks etc. I used my library "[Zds.Flow](https://github.com/ZdsAlpha/Zds.Flow)". Here is code for AsyncFileWriter: using System; using System.IO; using System.Text; using Zds.Flow.Collections; using Zds.Flow.Updatables; namespace DataFlow { public class AsyncFileWriter : SyncObject, IDisposable { public readonly string FilePath; public readonly Encoding Encoding; public readonly FileShare FileShareMode; public readonly int BufferSize; private readonly Round&lt;byte[]&gt; buffer; public AsyncFileWriter(string path,Encoding encoding=null,FileShare fileShare=FileShare.None,int bufferSize= 4096) { FilePath = path; Encoding = encoding ?? Encoding.UTF8; FileShareMode = fileShare; BufferSize = bufferSize; buffer = new Round&lt;byte[]&gt;(4096); } private byte[] _buffer = null; protected override void SyncUpdate() { if (buffer.Count == 0 &amp;&amp; _isdisposed) Destroy(); if (_buffer != null || buffer.Count != 0) { using (var fs = new FileStream(FilePath, FileMode.Append, FileAccess.Write, FileShareMode, bufferSize: BufferSize)) { if (_buffer != null) fs.Write(_buffer, 0, _buffer.Length); while (buffer.Count != 0) { buffer.Dequeue(ref _buffer); fs.Write(_buffer, 0, _buffer.Length); } _buffer = null; } } } public void Write(byte[] bytes) { if(!_isdisposed) buffer.Enqueue(ref bytes); } public void Write(char[] chars) =&gt; Write(Encoding.GetBytes(chars)); public void Write(string text) =&gt; Write(Encoding.GetBytes(text)); public void WriteLine(string line) =&gt; Write(Encoding.GetBytes(line + "\n")); private bool _isdisposed = false; public void Dispose() =&gt; _isdisposed = true; public void Complete() { while (buffer.Count != 0) Updater.DelayHandler.Delay(); } } } Here is test: using System; using Zds.Flow.Stopwatch; using Zds.Flow.Updaters; namespace DataFlow { public class Program { public static readonly UpdaterX Threads = new UpdaterX(); public static void Main() { Threads.Start(); Stopwatch sp = new Stopwatch(); using (var writer = new AsyncFileWriter("logs.txt")) { writer.Updater = Threads; writer.Start(); sp.Start(); Random r = new Random(); for (int i = 0; i &lt; 1024; i++) { byte[] bytes = new byte[1024]; r.NextBytes(bytes); writer.Write(bytes); } Console.WriteLine("Completed writing to memory in " + sp.Elapsed.ToString()); writer.Complete(); sp.Stop(); Console.WriteLine("Completed writing to file in " + sp.Elapsed.ToString()); } Console.ReadKey(); } } } Here are results: Completed writing to memory in 00:00:00.0420196 Completed writing to file in 00:00:00.0641055 Total file size: 1,024 KB Here is another test: using System; using Zds.Flow.Stopwatch; using Zds.Flow.Updatables; using Zds.Flow.Updaters; namespace DataFlow { public class Program { public static readonly UpdaterX Threads = new UpdaterX(); public static void Main() { Threads.Start(); var Writer = new AsyncFileWriter("logs.txt"); Writer.Updater = Threads; var Timer = new AsyncTimer(Threads, (AsyncTimer timer, ref TimeSpan time) =&gt; Writer.WriteLine("Tick on time " + timer.Stopwatch.Elapsed.ToString())); Timer.Delay = TimeSpan.FromSeconds(1); Writer.Start(); Timer.Start(); } } } Here is output: Tick on time 00:00:01.0090501 Tick on time 00:00:02.0110866 Tick on time 00:00:03.0110775 Tick on time 00:00:04.0032944 Tick on time 00:00:05.0130177 Tick on time 00:00:06.0003235 Tick on time 00:00:07.0000739 Tick on time 00:00:08.0000624 Tick on time 00:00:09.0003666 Tick on time 00:00:10.0001418 Tick on time 00:00:11.0008320 Tick on time 00:00:12.0015332 Tick on time 00:00:13.0002967 Tick on time 00:00:14.0004298 Tick on time 00:00:15.0006025 Tick on time 00:00:16.0007293 Tick on time 00:00:17.0000130 Tick on time 00:00:18.0003591 Tick on time 00:00:19.0015656 Here is GIF: [https://imgur.com/G6JbOkz](https://imgur.com/G6JbOkz)
This will sound foolish, but test if PropertyChanged is being fired/listened for at all in your current implementation. No on property changed definition in the snippet suggests to me it's defined in NPBase, along with PropertyChanged. However, you also have PropertyChanged defined in NPFolder (and not an override). I imagine your listener is subscribing to the NPFolder event, and the NPBase is firing. 
I used a different approach to write "AsyncFileWriter" which does not use Tasks etc. I used my library "[Zds.Flow](https://github.com/ZdsAlpha/Zds.Flow)". Here is code for AsyncFileWriter: using System; using System.IO; using System.Text; using Zds.Flow.Collections; using Zds.Flow.Updatables; namespace DataFlow { public class AsyncFileWriter : SyncObject, IDisposable { public readonly string FilePath; public readonly Encoding Encoding; public readonly FileShare FileShareMode; public readonly int BufferSize; private readonly Round&lt;byte[]&gt; buffer; public AsyncFileWriter(string path,Encoding encoding=null,FileShare fileShare=FileShare.None,int bufferSize= 4096) { FilePath = path; Encoding = encoding ?? Encoding.UTF8; FileShareMode = fileShare; BufferSize = bufferSize; buffer = new Round&lt;byte[]&gt;(4096); } private byte[] _buffer = null; protected override void SyncUpdate() { if (buffer.Count == 0 &amp;&amp; _isdisposed) Destroy(); if (_buffer != null || buffer.Count != 0) { using (var fs = new FileStream(FilePath, FileMode.Append, FileAccess.Write, FileShareMode, bufferSize: BufferSize)) { if (_buffer != null) fs.Write(_buffer, 0, _buffer.Length); while (buffer.Count != 0) { buffer.Dequeue(ref _buffer); fs.Write(_buffer, 0, _buffer.Length); } _buffer = null; } } } public void Write(byte[] bytes) { if(!_isdisposed) buffer.Enqueue(ref bytes); } public void Write(char[] chars) =&gt; Write(Encoding.GetBytes(chars)); public void Write(string text) =&gt; Write(Encoding.GetBytes(text)); public void WriteLine(string line) =&gt; Write(Encoding.GetBytes(line + "\n")); private bool _isdisposed = false; public void Dispose() =&gt; _isdisposed = true; public void Complete() { while (buffer.Count != 0) Updater.DelayHandler.Delay(); } } } Here is test: using System; using Zds.Flow.Stopwatch; using Zds.Flow.Updaters; namespace DataFlow { public class Program { public static readonly UpdaterX Threads = new UpdaterX(); public static void Main() { Threads.Start(); Stopwatch sp = new Stopwatch(); using (var writer = new AsyncFileWriter("logs.txt")) { writer.Updater = Threads; writer.Start(); sp.Start(); Random r = new Random(); for (int i = 0; i &lt; 1024; i++) { byte[] bytes = new byte[1024]; r.NextBytes(bytes); writer.Write(bytes); } Console.WriteLine("Completed writing to memory in " + sp.Elapsed.ToString()); writer.Complete(); sp.Stop(); Console.WriteLine("Completed writing to file in " + sp.Elapsed.ToString()); } Console.ReadKey(); } } } Here are results: Completed writing to memory in 00:00:00.0420196 Completed writing to file in 00:00:00.0641055 Total file size: 1,024 KB Here is another test: using System; using Zds.Flow.Stopwatch; using Zds.Flow.Updatables; using Zds.Flow.Updaters; namespace DataFlow { public class Program { public static readonly UpdaterX Threads = new UpdaterX(); public static void Main() { Threads.Start(); var Writer = new AsyncFileWriter("logs.txt"); Writer.Updater = Threads; var Timer = new AsyncTimer(Threads, (AsyncTimer timer, ref TimeSpan time) =&gt; Writer.WriteLine("Tick on time " + timer.Stopwatch.Elapsed.ToString())); Timer.Delay = TimeSpan.FromSeconds(1); Writer.Start(); Timer.Start(); } } } Here is output: Tick on time 00:00:01.0090501 Tick on time 00:00:02.0110866 Tick on time 00:00:03.0110775 Tick on time 00:00:04.0032944 Tick on time 00:00:05.0130177 Tick on time 00:00:06.0003235 Tick on time 00:00:07.0000739 Tick on time 00:00:08.0000624 Tick on time 00:00:09.0003666 Tick on time 00:00:10.0001418 Tick on time 00:00:11.0008320 Tick on time 00:00:12.0015332 Tick on time 00:00:13.0002967 Tick on time 00:00:14.0004298 Tick on time 00:00:15.0006025 Tick on time 00:00:16.0007293 Tick on time 00:00:17.0000130 Tick on time 00:00:18.0003591 Tick on time 00:00:19.0015656 Here is GIF: [https://imgur.com/G6JbOkz](https://imgur.com/G6JbOkz)
I’ve really not found that these perf increases happen on large projects, R# just does not work on large solutions. We have projects with 2-3 mobile apps and 1-10 web apps in them and it degrades perf too much to make it workable. Tools like ctrl + . Or plugins like https://marketplace.visualstudio.com/items?itemName=JustinClareburtMSFT.HotKeys are the way forward IMO. Just went from surface book pro i7 2.1ghz 16gb + SSD (3 weeks old) to dell XPS i7 2.8ghz 32gb + SSD perf degradation still too much for me to handle. Uninstalled R#, mapped ctrl + . to ctrl +shift + r, ctrl + t to Go To Member (Ctrl 1, M) etc...
Big R# fanboy here... I've ditched Resharper for a bunch of free, fast plugins, Roslynator and Intellicode chief amongst them. I have never been happier, or more productive.
I hope they don't buy it, diversity in source control can only be a good thing. Furthermore many still don't trust Microsoft (deserved or not)... who have their source on GitHub not vso... Honestly gitlab might do very well out of this.
Good move for Microsoft, bad move for Github probably. 
My concern with it is that there are dead packages on there that haven't been active in years. I don't want to "discover" dead projects.
This was posted last week, the banter on the domain name discoverdot.net was quite funny.
This would be an interesting move, but I'm struggling to see what Microsoft gets out of it. They've already started opening up VSTS to have public repositories, if they remove the limited user restriction they'd have a genuinely compelling competitor to github (I actually prefer VSTS myself for private projects). One thing I will say, as great as github as been to the open-source community, I don't like how github itself is closed source. It's kind of contradictory to me. If Microsoft bought them out, I'd expect them to open source the platform.
Sold with 'heart' by github
&gt; If Microsoft does acquire GitHub, it could be seen as a visible attempt to further integrate Linux and Windows and increase interoperability, as demonstrated by the Windows Subsystem for Linux (WSL) compatibility layer found in Windows 10. That doesn't make any kind of sense.
A $$ 2 billion dollar reason for GitHub to make such a move.
Microsoft has many open source projects, all hosted by GitHub. So part of it could be controlling what they use. Though I don't see how that alone could be worth 5 billion USD.
Could indeed be that. Microsoft were very open when they said the reason they're using Github is because they want their open source efforts to be where everyone else's are - and everyone is on Github. I suppose you could consider it a "business risk" to have all the code on a platform you can't control, but I'd be surprised if they don't have all that code synchronised to a private VSTS instance as well. Interesting times ahead.
Plus you will get a lot of folks remembering "embrace extend extinguish" many would have made a decision not to use Microsofts source control by using GitHub instead, this will feel like a betrayal for them.
Working on a similar machine, I saw a big difference when switchin from 16GB RAM to 32, so I susspect the RAM
Does anyone use .net core on Mac? I don't have a Mac, but wonder how the experience is. I use it on Windows mostly, but also test on Linux. It's pretty neat to be able to run .net on both Windows and Linux.
I use .net core on mac 90% of the time. It's a solid experience. Using either VS Code + CLI or VS for Mac, both allow you to build pretty much anything you want, mind some still missing namespaces from Framework. But that's not mac related. I've built 2 web apps and one serious console app. I can highly recommend it.
I would love to get the code! been testing rider for 2 weeks now on the mac and absolutely love it. Its more lightweight then vs for mac and with the integrated resharper its even better.
Sounds like a classic case of someone believing GitHub == git to me...
Yea thats a good one! Missed that in all the talks / release notes about 2.1
Because TFS went well.... No, just no
Honestly I can't hear this EEE shit anymore. Can't people just for one day not use abstract trigger words? This shit has no meaning. Just like gaslighting and all that shit. Words are no fashion show, you can't just fo certain some smart sounding words in your sentences and expect them to make sense. God almighty...
Shut up
Wow, pick up a history book ..or.. you know, just keep shout nonsense at people.
I think most people who use the EEE phrase whenever MS does anything don't know what it means. Can you explain how does it apply here?
That's my problem, it doesn't reference where the problem is. I've ran into this error many times while developing my app, but the compiler always told me a file name and line number. The activity logs in azure literally only gave me that phrase, with no indication of where in the program that error was being triggered 
VSTS is pretty awesome. The only thing that I don't like in the product is that TFVC version control is still available for new projects. 
https://en.m.wikipedia.org/wiki/Embrace,_extend,_and_extinguish https://en.m.wikipedia.org/wiki/Gaslighting 
**Embrace, extend, and extinguish** "Embrace, extend, and extinguish", also known as "Embrace, extend, and exterminate", is a phrase that the U.S. Department of Justice found was used internally by Microsoft to describe its strategy for entering product categories involving widely used standards, extending those standards with proprietary capabilities, and then using those differences to disadvantage its competitors. *** **Gaslighting** Gaslighting is a form of manipulation that seeks to sow seeds of doubt in a targeted individual or in members of a targeted group, hoping to make them question their own memory, perception, and sanity. Using persistent denial, misdirection, contradiction, and lying, it attempts to destabilize the target and delegitimize the target's belief. Instances may range from the denial by an abuser that previous abusive incidents ever occurred up to the staging of bizarre events by the abuser with the intention of disorienting the victim. The term owes its origin to the 1938 Patrick Hamilton play Gas Light and its 1940 and 1944 film adaptations. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Non-Mobile link: https://en.wikipedia.org/wiki/Embrace,_extend,_and_extinguish *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^188764
I know what it means. I don't have anything against the term. I have something against the hivemind pushing a term to it's absolute limits. I'm pretty sure if you'd put a search on this term on Reddit you'd see a sudden peak around the last 3 months.
That is not an explanation.
Yeah, that was a side effect of copy-paste (errr...I mean rapid-fire prototyping). I tried removing implementations from all but the base class and only inheriting from the base class in my sub-classes (no INotifyPropertyChanged implementation). Even though everything was being called at the appropriate time, that didn't work. Then I tried switching things up by using Prism's Bindable Base. Had the base class implement Bindable Base and set up the appropriate setters in my derived classes. And instead of calling on property changed when collection changes, I call BindableBase's RaisePropertyChanged("Items") with my collection name in their. Similarly, everything appears to being called at the appropriate time... but no luck. Proof in the pudding. https://imgur.com/a/KERPEjn Photos 1 and 2 are the first attempt. Photo 3 is Bindable Base.
16gb will help your life in general, but Resharper runs in process with VS right now. So both only have about 3gb of total memory no matter what you have on your system. You'll be glad you did it, but don't expect Resharper to show big improvements
"Acquiring" doesn't mean "Improving" or even "Maintaining". They could easily buy it and turn it off.
But even then.. git runs on Windows.. has done for years. VSTS supports it.
I guess if we were to come up with crazy scenarios, one possible route 'extinguishing' here could take, in the eyes of MS detractors, would be to assimilate Github into Team Services Git in some fashion and thus become the 'limited, free version'. 
Microsoft has deprecated TFS and Git is now the default recommendation within the Microsoft ecosystem. &gt; [Git is the default version](https://docs.microsoft.com/en-us/vsts/tfvc/comparison-git-tfvc?view=vsts) control provider for new projects. You should use Git for version control in your projects unless you have a specific need for centralized version control features in TFVC. 
"Extinguish" specifically refers to extinguishing competition. At that point, GitHub wouldn't be Microsoft's competition, so I don't think describing that as the third E makes sense.
Looking this over, I see that ```SyncObject``` is your base class and is what does the actual writing at some point. It's not that clear how it's called. It's also not clear how it ends up being thread safe. Is ```Round``` a class in your library that handles concurrency? You may want to pit your class against mine in this test harness: https://github.com/electricessence/AsyncFileWriter/tree/master/AsyncFileWriterTester 
I'm thinking they want it for brand visibility and to have Microsoft more associated with Open Source and promote .net core development over things like java.
Have you tried System.Threading.Tasks.Dataflow? Does Zds.Flow have some benefit over it?
&gt;"Extinguish" specifically refers to extinguishing competition. With Microsoft, that was not strictly the case. Breaking well used standards, killing or stunting services loved by the Open Source community, after acquistion, were all considered to be backed by predatory intent.
It's good news for gitlab
Microsoft has not deprecated TFS, they deprecated TFVC. Big difference
Considering how much of Microsofts coffee is open sourced on GitHub I very much doubt it
Thanks. It was a typo, fixed above. 
Sorry for being very unspecific. I did not have enough time to fully document my library. \(I will soon\) `Round` is a circular buffer that behaves as a ThreadSaf`e Que`ue`, Sta`ck as well a`s MemoryStre`am. I used it as `a Que`ue to safely transfer byte arrays from external thread\(s\) t`o AsyncFileWrit`er. `UpdaterX` and `SyncObject` must be confusing. `SyncObject` does not perform actual writing at any point. `UpdaterX` is simply a thread source. Once `SyncObject` is added to `UpdaterX`, its `SyncUpdate` method is called recursively. But only one thread is calling `SyncUpdate` method at time because writing operation must be synchronised. In `SyncUpdate` method I take byte arrays one by one if available and write them into the file . Once it has been disposed, it waits for byte arrays to be written to the file. After that it `Destroy()` itself \(disposing resources and removing itself from `UpdaterX`\). `AsyncTimer` in one of the test is a Timer class. Each Tick of this timer is independent of previous tick. If you put `Threading.Thread.Sleep(10000);` in Tick method body. Next tick will be called in time. Its because `UpdaterX` automatically creates thread when load is high. I will try to your perform test on my code. 
Ok where do I buy the Gitlab stock
They've just released 18.1.2 which fixes the most egregious performance problems of 18.1. I found that document while I was in the 18.1 hell of last week and found it rather patronising. To summarise: "Don't write code, use our product, or Visual Studio".
I'm not saying Microsoft didn't do it. I'm saying it's not EEE. Though I am curious, what services are you talking about?
MS has had the business structure of "purchase quality development and then label it Microsoft" for a long time. 
Git via TFS is actually very nice.
They’re also the most active contributor to Github as a company, IIRC, but in number of projects and contributors. 
They haven't deprecated TFVC. While they (rightly) position git as the better option for TFS/VSTS users, TFVC is in active development and not deprecated. Personally I believe they should deprecate it but I suspect some of their on-prem TFS customers will tie them into supporting it for 10+ years easily.
This is BS
Total BS. They want developers to use Azure. They have zero interest in reading random source code.
You can generate code using T4 templates, but I wouldn't say it's a good metaprogramming experience.
100% agreed!
Native Azure integration. Also github is profitable so it isn't an endless money sink for them. 
That's not a bad shout, Azure is the future of Microsoft after all.
well if your backend will always be windows then why would you switch?
Thanks for that! One of the things I did (and do) before I post is look for documentation and I definitely missed that.
Good bot
Thank you, mvonballmo, for voting on WikiTextBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
That makes sense! I keep up on the incremental changes and try to test them when I can, and this is my first foray into .net core on linux, so definitely learning a lot :)
If I'm not mistaken, I think core has a lot of performance improvements. Check into that.
I can’t speak for every team, but it’s definitely not all also on VSTS. GitHub is the primary store for many project using VSTS for build pipelines.
It works...almost. Since the default Split is one whitespace it fails where there is more than one whitespace, so I had to use a regex which was [a surprisingly easy solution](https://pastebin.com/7LDeBaeF): string pattern = @"\s+"; String[] substrings = Regex.Split(newString, pattern);
Is it at least backed up in any way? As a contingency in case github went down or something?
Well it’s git - so naturally everyone working on it has their own copy. As it’s all under the Microsoft GitHub organization there’s probably some backups happening by corporate but I don’t have details about that. Each team maintains their own repos and may have different practices.
New projects should always use .NET core, unless there are some requirements that can only be implemented using .NET framework. I think Microsoft is improving .NET core to include many features that were missing from .NET framework take for example SignalR. In future, .NET framework will be mainly a legacy framework used in projects that cannot be moved to .NET core for financial or technical reasons.
Apparently,. NET Core is faster, at least on some tasks compared to. Net 4.7, even on Windows. 
&gt; Visual SourceSafe 3000 Cloud Domination Edition Ok, I want you to do me a favor. Message me on reddit right now, and tell me to sell the bank stocks at 25 €. Use the codeword "thunderbird". I know it doesn't make sense to you, but you're apparently stuck in a time loop in the early 2000s, so, by sending me this message, you'll allow me to sell those stocks at 1000x their current price. I just hope the message reaches me on time.
I had the same problem come up trying to update my standard 2.0 libraries I maintain to take advantage of Span. 
Until EF Core catches up with EF6 or .Net Core can use EF6 directly, then .Net Framework isn't going anywhere even for new projects. .Net Core is definitely the future, for cheaper Linux hosting, architecture improvements, and performance gains. But until EF Core is "there" then it won't be fully baked. PS - EF Core is actually an upgrade on EF6 in several ways, but also lacks "basic" features and third party support. 
.net core, or asp.net core? Either way, it's getting harder and harder to justify "full" framework development these days. Performance enhancements aside, the deployment of .net core apps is far better than full framework apps. You can migrate the app to the latest code (and security updates) with no risk of breaking anything else on the machine.
.net core 3.0 should make EF6 cross-platform. That'll pretty much be the final nail in the full .net framework coffin.
He already knows Core is good, he's asking if there's any reason to keep using non-Core
Exactly. That and the UI stacks (UWP, WinForms and UWP) means .NET Core 3 will be the death stroke to .NET Framework.
There are many key features of ASP.NET not available on Core (IFAIK), like build providers. If you use the web frameworks from Microsoft then you won't miss them, but if you use alternative frameworks or view engines they might not be available on Core.
I ran some benchmarks comparing linq2db vs efcore and linq2db was so much faster then efcore and I was thinking about using it for my orm 
Are the UI stacks actually coming in 3.0? I heard from someone that they're not coming? Now I'm confused.
Ignore that someone and get the information from the source: https://blogs.msdn.microsoft.com/dotnet/2018/05/07/net-core-3-and-support-for-windows-desktop-applications/
So if I understand that right, it's only for Windows Desktop? A bit of a shame but I guess it's not like someone else can't develop them for Linux instead.
Correct. The UI stacks will be ported to Core but they won't be cross platform.
Correct. The UI stacks will be ported to Core but they won't be cross platform.
French\-style surrender.
&gt; Microsofts coffee is open sourced Like device drivers for coffee pots? [I thought they only did toasters](https://superuser.com/questions/792607/why-does-windows-think-that-my-wireless-keyboard-is-a-toaster) 
&gt; I hope they don’t buy it, diversity in source control can only be a good thing. Yes, but (largely) thanks to github there is no diversity now, regardless of who owns them.
Irrational outcry begins now.
If you are doing a new project, I think you can make it .Net Standard which can be compiled to run on either. The Full Framework will be good for like 10 more years, but I wouldn’t use it for a new project.
Guess googl, fb, Amazon and other big tech will move their code. Sucks. Blogs sound like most are going to GitLab.
Plus can't imagine Google, FB and other keeping code on GitHub. Their big projects will move to GitLab most likely. Sucks.
No it doesn't. If you are building something new you should be building it on ASP.Net Core unless there's something you just can't achieve on the platform. That's where Microsoft seems to be investing their engineering hours. 
which is fine. a lot of work is going in to making the api surface of .net core sufficient, then doing the other stuff in nuget packages. imo it's a better way to do it anyway. 
Hahaha apologies, meant code*
What's the word on hosting Windows Services on .NET Core?
i personally think that net standard should lead, just like the c++ standard leads. platforms can implement them at their pace.
Use .NET Core in cases where it serves your needs. .NET Framework isn't going to vanish overnight.
So long as you haven't done anything particularly insane you can migrate from TFVC to git on any given project relatively quickly and painlessly, history intact. I've done this on a bunch of projects, and it's really not too bad. That said, it's important to remember that git on TFS/VSTS is not a single experience, but one with a lot of variation. The latest version of VS 2017 with VSTS or TFS 2018 is pretty good, VS 2013 with TFS 2013 is pretty awful. You can counteract a lot of this with the command line, but not all devs are comfortable with that. It's also much easier to fuck up in git than a lot of people are willing to admit, and TFVC is nowhere near as bad as people think. I don't think TFVC will remain for 10 years, but it'll be around till at least when the majority of on premise sites are at what is currently the latest.
The thing about enterprise development is that 1. It moves slowly 2. Old systems need maintaining/improving 3. Older developers tend not to move to new technologies as fast (warning: unfair generalization) I still see developers making software using VB6, not even "Classic" ASP.NET. For many companies, only the "new" web development stuff is starting to use MVC, and many systems in maintenance are in older versions of .NET I absolutely recommend moving to newer technologies for new projects - although I'm not convinced Core is fully production ready for all use cases yet. And if your organization is happy with Windows hosting, all that cross platform stuff is wasted anyway, and MVC is hardly slow, even if Core can be faster. So yeah, where it's sensible to do so I'd aim to use Core for new projects, but don't write Framework off entirely - Entity Framework still isn't fully supported, for example.
I don’t see the majority users staying with the platform if Microsoft buys it.
[Supported as a nuget package.](https://www.nuget.org/packages/System.ServiceProcess.ServiceController/)
Is there somewhere that shows the feature parity between ef6 and ef core? That is the major blocker for us. We use ef for super complex queries against contexts with hundreds of tables.
I don’t think so. I will never make another framework app again. It is all .net core from here on out so I can use Lambda or any provider and not pay the Windows tax for serving apps.
Thanks
Bye github.
Close, but that just looks like it allows you to control an existing service.
Maybe not by Microsoft, but the whole thing is open source. Someone will (try to) do it eventually.
You can't port it. It's a rewrite (see Avalonia). WinForms is all GDI and WPF is DirectX. Both very much Windows technologies.
EF Core should be used so that you can do LINQ queries and abstract your data access layer. If you want performance you can write [ADO.NET](https://ADO.NET) or Dapper queries in your repository layer for each database you support, though that is just my opinion.
Typically you would want to make your libraries .NET standard, and then your applications can target framework or core and then reference the libraries. We had to do this on a recent project where we had a model library that was shared between a desktop service \(which required .NET Framework\), and an [ASP.NET](https://ASP.NET) Core application.
You need to use .NET Framework to create a proper windows service. I tried it out, and it was easier to just create a web application and host it via reverse proxy in IIS and turn off the app pool recycling than it was to create a .NET windows service.
I feel like he was saying you could port the UI framework into Gnome/KDE or whatever since it’s open source. It might be a little weird since I’m not at all confident in readability of Microsoft’s legacy code so even the Core rewrite could be unintelligible, but it should be possible. 
Is System.IO.Ports coming to Core 3? I noticed this was only full framework and I ended up soloing a native serial library for Linux, it’d be nice to have the real deal though. 
Unless EF is not an absolute requirement for you.
Reasons typically given are: * Its faster \(both when running, and when installing\) * Multiple versions of it can be installed on a server side\-by\-side without causing an issue * You can develop on Mac / Linux with VSCode or VS for Mac, if that is your preference \(or designer takes advantage of this\) * Its MIT licensed and open source. Its easy to go look at its code on github anytime you want, and its really easy to submit an issue or ask the devs a question. There isn't a way to do this for the full .NET Framework. * It gets updated more often.
EF seems to be like more of a crutch than a requirement, not really a necessary factor in using .net I've actively avoided the EF bloat throughout my entire .ner career with great success.
The Roslyn API is your best bet. See also: expression trees.
In my benchmarks with linq2db it looked like you got close to dapper and ado.net performance while still being strongly typed and using linq.
Honestly, I think a lot of this is making a mountain out of a molehill, and lots of the rest seems to miss the forest for the trees. A major point of .NET Core was to be able to deliver useful tools to developers at a much faster pace than what's appropriate to do with .NET Fat. Lots of these tools can be, have been, and will almost certainly continue to be be delivered via new packages that can work on a variety of different platforms, but when it comes to addressing longstanding shortcomings on types that have been central to .NET since the very beginning, **of course** we're going to see some Core-exclusive content for a while until the rest of the ecosystem catches up. Core is open-source, and this stuff is very desirable. I don't expect it to take long for this to hit the rest of the major .NET platforms other than .NET Fat, which moves slowly by its nature. Once it's available everywhere, then it will start to matter whether or not it's in .NET Standard, just like everything else up to this point. The only difference is that now, Core leads the pack instead of Fat. None of this surprises me at all, nor does it seriously worry me about the future of .NET. If anything, it gives me confidence that Microsoft made the right decision to focus on Core instead of Fat.
&gt; I've actively avoided the EF bloat throughout my entire .net career with great success. So in other words your opinion shouldn't be trusted on the topic. Personally I prefer catching query issues at compile time rather than waiting for them to become runtime exceptions (not to mention parameterized queries are the default, query concatenation is secure, and LINQ is pure sex). And while there are other good ORMs around, they all solve roughly similar issues. 
Add WCF and yes.
That doesn't sound easier than overriding one method like you do with Windows Services.
I just ported my EF6 code to EF Core 2.1 and had no issues. EF Core 1.0 was terrible, and 2.0 close to being okay, but 2.1 is actually worthy of full use. 
Thanks for the replies everyone! I agree and believe that Core is the future but like someone here said, change moves slowly in businesses. One other question. Unfortunately, we are still using VS2015. The .NET core tooling in VS2015 appears to be preview only. Is there any way to do modern .net core development in VS2015 or am I going to have to convince my boss to upgrade to VS2017? (not that I mind using the CLI tools)
&gt; it's not like someone else can't develop them for Linux instead The Mono team tried this for WPF and gave up.
I forget what it is called but there is an implementation of WPF being made for Linux atm. and afaik it's moving along nicely.
Very well said. I work in a team of 3 developers and we all do our best to keep our tech stacks modern (our flagship app is ASP.NET MVC on .NET 4.6.1). However sometimes doing so costs money that management isn't willing to spend. Even if we know that certain technological moves are for the best for the company, it can be pretty demoralizing when you can't convince non-tech people of that.
From what I've seen,. Aspnet core right now is faster because it's more lightweight, makes sense. But overall all projects can benefit from performance gains here and there. In my case im coding a game engine and related tools in. Net core and all is going well. I can use all the libs im used to like SDL2, OpenGL etc and pretty much every native lib with ease. Only heavy gui work like wpf and the old winforms is still unavailable.
You should give dapper a try. I can't imagine ever going back to ef.
You certainly wouldn't think so. I'm willing to admit it is likely most of that difficulty came from our team's unfamiliarity with deploying windows services.
I recently needed to make a .net core app I maintain run as a Windows service, but the service container package only supported .net framework. I ended up going with this library, and I'm so happy I did. https://github.com/PeterKottas/DotNetCore.WindowsService
This package will let you run as a Windows service in .net core. https://github.com/PeterKottas/DotNetCore.WindowsService
Search for `AOP` frameworks: https://stackoverflow.com/questions/633710/what-is-the-best-implementation-for-aop-in-net
That's fair. I have to admit that I pretty much lived in Windows Services for five years so I'm a bit biased.
There's C++/CLI as well.
Well said! I used core for a web api recently and I can't upgrade from 2.0 to 2.1 because it breaks stuff so yeah.. New stuff is fun and flashy and cool but unstable too. I don't want to waste my time digging into framework code to fix bugs I have my own code to take care of. 
Or, you step away from relational databases for OLTP implementations and eliminate EF altogether. Problem solved. That's what we did. Couldn't be happier.
You need to copy over the dlls and create the right registry keys for it. On mobile right now but you can google "excel vsto registry keys" it's really simple you need a key for the name and one for the path of the dll if I remember right. One gotcha is to make sure you have the right path in the registry depending on if you have a 32 bits or 64 bits installation of office. 
I've spent a little bit of time in dapper. The problem I saw was that it isn't strongly typed.
Justifying ROI I big framework changes is always complicated. Have you presented them with estimated gains on what they get for moving in business terms? "It will run faster hosted on Linux" by it self is going to get dismissed almost every time.
We just use nssm which has been fantastic. We use it for everything, even our database.
Starting with the test comparison would be a good idea. I think the important thing is to avoid contention. The reason why I ended up using Channels is so I could take advantage of a bounded queue. It may not be perfect because it's not 'byte' bound but it provides the simplicity of a block limit. Then integrating ITargetBlock helps if you wanted to integrate with TPL Dataflow.
Curious now of what would break from 2.0 to 2.1? From what I’ve been reading it’s supposed to be a really smooth update. 
Mono could and did already port WinForms. It looks like shot though. Windows NT era controls on macOS/Linux. Terrible. But the only choice for compatibility I suppose. WPF can probably also be ported on an API level and have the underpinnings use Vulcan rather than DirectX.
GitLab has reported a huge uptick in new accounts after this was announced. I doubt the majority will migrate away from GitHub, but there will certainly be quite a few who do.
Work for an enterprise company. We've already made the move to .NET Core, especially since we've been starting to push more and more to the cloud. Being able to write code and not care where it's getting hosted is a pretty liberating feeling. We left Entity Framework a while ago and am never looking back - so we don't care about EF Core like some people here apparently are.
We left EF a while ago because of the performance issues. I don't understand why people are so hung up on an ORM that they probably don't even use 20% of it's features.
He's probably running multiple versions of the runtime and sdk. I'm currently working with the 2.1 RC version of core and I'm having a blast, no issues upgrading from 2.0 to 2.1 
Mind to elaborate why you guys left EF?
Specifically a Windows service? Not yet. However the Microsoft.Extensions.Hosting generic host builder let's you build a long running console app with the same sort of capabilities. I'm moving a lot of my personal ones to console apps running in docker for lifecycle management. That's not native Windows services, but it opens the door for Linux hosted services.
I always find EF more trouble than it's worth on anything other than prototypes. Dapper + some hand written SQL is fine for me. And I hate SQL.
It mostly came down to performance reasons. We were able to increase response times on our APIs up to 20x by switching to Dapper. And in an industry where you have people waiting in line at a point of sales register that makes a huge difference. We’ve also been able to handle larger loads of traffic which comes especially beneficial around the holidays when we do most of our business.
&gt; til EF Core catches up with EF6 ... .Net Framework isn't going anywhere even for new projects. Not everyone uses EF at all - ASP and EF are not joined at the hip, it's just one ORM, and you don't really have to use an orm at all. FWIW: try Dapper. 
That's not exclusive to Microsoft. EA too. Except they buy the developer and ruin their franchises.
On the soft side its hard to get developers to work with "old" stuff. A lot of us are attracted to interesting \(new...\) technology. Were in it for the challenge. Its very important if you want motivated developers on the projects.
The Mono port uses GTK - hense why it looks like shit
[this has exactly that](https://ef.readthedocs.io/en/staging/efcore-vs-ef6/features.html)
What I mean is that it looks bad because the controls are clones of the Win32 user interface design, so that it harshly conflicts whatever UI toolkit that the target operating system is using.
Can you tell us a bit more about your use case? What are you building that requires meta programming?
Integration with Cloud matters a lot. Currently we are building .net core web app on AWS. Most of the things works great but we are having challenges integrating it with AWS Lambda. I believe by .net Core 3.0 these should be resolved. Also cross platform desktop apps in .net core 3 looks exciting too.
I haved used IHostedService in .net core 2. It works well, you can deploy in container too. https://blogs.msdn.microsoft.com/cesardelatorre/2017/11/18/implementing\-background\-tasks\-in\-microservices\-with\-ihostedservice\-and\-the\-backgroundservice\-class\-net\-core\-2\-x/
https://github.com/OData/WebApi/issues/1447 Granted its prerelease, still doesn't change the fact that it 2.1 breaks it and I need it. Our webapi works great and someone wanted to use it with PowerBI (which is garbage but what can I do about it). PowerBI works horribly with rest APIs but does work decently with OData so we decided to go that route, now it blocks us from upgrading to 2.1. Wouldn't have this issue had I stayed with the old .Net framework
&gt; increase response times on our APIs up to 20x by How do manage queries? i mean, your app calls StoredProcedures or direct queries? &gt; an industry where you have people waiting in line at a point of sales Same domain for me, that's why Im interested about your experience. &gt; We ask have some older legacy systems we have to integrate with and EF did not play nicely with them at all I do have the same challenges over here, code first was key to map horrible non normalized databases. 
It's because you write benchmarks no real world applications, strip everything out except the parts you 100% need and pre allocate every response. Fortunes is probably the only test you should care about
We do direct raw queries - any adds or updates we send to a service bus. When it came to DB2 especially we had our DBAs around to help make those queries more efficient (especially since we were getting high priority access on the mainframe).
I tried that but it doesn't seem to change anything in swagger. Is there something built-in that should be used instead of Swashbuckle?
Confirmed https://blogs.microsoft.com/blog/2018/06/04/microsoft-github-empowering-developers/
I think you are forgetting about Github Enterprise
Well you can’t actually blame power bi problems into .NET Core. From a “normal” api it should work pretty easily. Just upgraded and it was a afternoon work. 
We still have a VSS repository that the Powerbuilder guys use. They are considering upgrading to TFVC (their workflow depends on an exclusive checkout model). I'm sure we are not alone there. That workflow is why TFVC is not going anywhere anytime soon.
No it is absolutely a .Net core problem. The upgrade to 2.1 broke the Microsoft.AspNetCore.OData package that was working perfectly in 2.0 and it has nothing to do with PowerBI. We added OData support to our api because our use case was PowerBI, but OData is just a different kind of Rest API. It's like deciding between XML serializer and JSON serializer, we simply added OData and .Net core broke that integration with 2.1 
Unfortunately.
&gt; exclusive checkout model You mean a fork? &gt; That workflow is why TFVC is not going anywhere anytime soon. People not understanding Git is why TFVC isn't going anywhere anytime soon. Git can do everything TFVC can do and more, and you can even enforce that model via mandatory pull requests into mainline. Git definitely has a learning curve, particularly with terminology and ideas, but once you get it you *get it*. There's no going back after that. 
https://github.com/kevin-montrose/Sigil
Fyi, from Scott Hunter: “Our goal is to bring Windows desktop development onto .NET Core. Cross platform UI is not in scope for this release and those two technologies might not be right answer to that anyways.” https://twitter.com/coolcsh/status/994003496021114880?s=21
Is it really efficient to use http for comunication between microservices ?
If your company won’t upgrade to VS2017 right now, you could use VS Code if you’re doing stuff like ASP .NET, Azure Functions, Bot Framework, etc. (but not Windows dev or Xamarin) But if you plan on using the full VS for .NET Core development, you should be using VS2017 over VS2015. Even when 1.x was the current release, the documentation suggested using 2017 instead of 2015: “It's possible to use Visual Studio 2015 for .NET Core 1.x development, but it's not recommended for the following reasons: * The .NET Core tooling is a preview version, which is not supported. * The projects are project.json-based, which is deprecated.” * Source: https://docs.microsoft.com/en-us/dotnet/core/windows-prerequisites?tabs=netcore1x 
Definitely not between microservices, as you should try to have as little of that as possible. But this is about an external client communicating with an application that uses a microservice architecture. 
No they need an exclusive lock per file that is visible to all users of the source control system. All files not checked out to one person must remain read only to prevent various (imo broken) tools from making modifications to them. What they are doing is analogous to versioning multiple zip files that depend on each other via strings in each file. The tooling makes this all transparent but when one package changes all those that depend on it are also updated. This fails because the packages are read only if they are not checked out which causes the chain to stop. Since they aren't versioning anything that can be merged, they cannot have 2 people making changes to a file at the same time. Thus only 1 person can check out any package at any time.
I found an example on the web and modified it. The (dynamic) cast on exception will let you define overloads of GetJsonForException with specific exception types In Startup::Configure if (env.IsDevelopment()) { app.UseMiddleware&lt;ErrorHandlingWithDetailsMiddleware&gt;(); } else { app.UseMiddleware&lt;ErrorHandlingMiddleware&gt;(); } the classes public class ErrorHandlingWithDetailsMiddleware : ErrorHandlingMiddleware { public ErrorHandlingWithDetailsMiddleware(RequestDelegate next) : base(next) { base.IncludeExceptionDetails = true; } } public class ErrorHandlingMiddleware { private readonly RequestDelegate next; protected bool IncludeExceptionDetails { get; set; } public ErrorHandlingMiddleware(RequestDelegate next) { this.next = next; } public async Task Invoke(HttpContext context /* other dependencies */) { try { await next(context); } catch (Exception ex) { await HandleExceptionAsync(context, ex); } } private Task HandleExceptionAsync(HttpContext context, Exception exception) { HttpStatusCode code; string json = GetJsonForException((dynamic)exception, out code); context.Response.ContentType = "application/json"; context.Response.StatusCode = (int)code; return context.Response.WriteAsync(json); } private string GetJsonForException(Exception exception, out HttpStatusCode statusCode) { statusCode = HttpStatusCode.InternalServerError; string result = GetDetailsString(exception); return result; } private string GetDetailsString(Exception exception) { if (IncludeExceptionDetails) { var obj = new { Exception = new List&lt;String&gt; { exception.Message, exception.ToString() } }; return JsonConvert.SerializeObject(obj); } else { var obj = new { Exception = new List&lt;String&gt; { exception.Message } }; return JsonConvert.SerializeObject(obj); } } } 
This is for an API gateway which in itself is not a microservice, it simply routes the requests to the appropriate microservice. If you want communication between microservices, publishing and subscribing to events pushed onto a service bus of some sort \(RabbitMQ, Azure Service Bus etc\) is the recommended way to do that.
EF is very bloated. I've also actively avoided it, or if it was in a project then I'd just strip it out for a better alternative. I use dapper in conjunction with my own data layer classes. Always faster and easier to maintain. If you're not familiar with SQL, then sure, I can understand how EF seems good.
&gt; If you're not familiar with SQL, then sure, I can understand how EF seems good. Or if you are, EF still seems good. No need for the petty backhanded remarks because you disagree on a technology choice. 
Thanks for sharing 
Not sure if thats based on a wide range of experience - “definitely not” would be at odds with, for example, the Building Microservices book?
I mean it CAN be done with automapper, I was just arguing that I don’t think it SHOULD. 
I love reading things like this showing the flexibility of .net core and how you have a much more base level of control than in full framework. It was one of the things I loved being able to do in Node, so happy I can do in C#. Not sure it is really practical other than in very niche cases to build your own, but was so cool to see how easy it would be. I do something very similar to manage the admin page for my CMS.
Request/response communication between microservices is a bit of an anti-pattern. As /u/jaynoj mentioned, a service bus would be the correct way to do it. 
you’re gonna have to back that up with something.. never come across that viewpoint before and request tracing seems not only extremely commonplace but a major factor in architecting a microservices solution
It's not a backhanded remark. Architecturally it doesn't make much sense to use EF when you can just wrap dapper around basic data connection. Run your sql/stored proc, convert to your output type, benchmarked is magnitudes faster than EF. It's another layer of complexity that I don't find necessary. It's fine for rapidly building proof of concepts (depending on data needs), but it's no different than the generic data scaffolding in [whatever framework]. And most certainly is not a factor in deciding to use .net core or not.
There's a nice article about communication in a microservice architecture [here](https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/architect-microservice-container-applications/communication-in-microservice-architecture)
that literally says http is super common
&gt; Architecturally it doesn't make much sense to use EF when you can just wrap dapper around basic data connection. It makes your code much easier to maintain. Dapper makes it easy to take data out, but consistency of the query isn't checked until execution. In EF Code First's case, the model has to match the table[s], and the code has to match the model. This results in a lot of errors being caught by the compiler, and also allows you to alter queries via built in refactoring tools. With a raw SQL query you're left doing search/replace on strings, and just have to hope you find all the uses and don't inadvertently replace something else. With EF you get reference counts, a full tree of uses, and a anything else that actions regular code. There's other ORMs that are comparable to EF, but EF does what it does very well. &gt; Run your sql/stored proc, convert to your output type, benchmarked is magnitudes faster than EF. Which is optimizing for the wrong thing. Programmer time (inc. maintenance, bugs, etc) is expensive and a bottleneck, database execution time is expensive and a bottleneck, actual query construction is a relatively minor operation that gets cached anyway. You're proclaiming an advantage on query construction, which is such a tiny subset it may have well be a rounding error. &gt; And most certainly is not a factor in deciding to use .net core or not. It absolutely is in large professional projects. MVC/WebAPI + EF CF is practically a stereotype for .Net jobs these days. The only thing that varies is what JavaScript frameworks they're using on the front end (Rust, AngularJS, plain JQuery, Vue.Js, etc). 
&gt;If possible, never depend on synchronous communication (request/response) between multiple microservices, not even for queries.
Ah I see, that makes a good argument. I hadn’t come across something saying that multiple comms protocols were the way to go (and seems a bit overcomplicated to me) but I see the reasoning here. Microservices truly are to be reserved for only the biggest teams with the most complex of product innovation and integration needs. Way way complex
I see .. So what would be the best way to comunicate between microservices ? Shuld we just load them in memory as a reference assembly dynamically ? If so we will couple them together .. 
Ahh the OData got broken.. that’s sucks big time. Luckily we just have a “normal” usage so.. nothing affecting us atm. 
Personally, I find them more simple than most in the wild monoliths. My experience has been that microservices, when diligently maintained to remain "micro", are extremely simplistic in nature, sometimes only being a single project in a solution and easily covered by unit and integration tests. However, in a monolith you are terrified of touching anything due to systemic risk, and have a plethora of code that is "untestable." Sure, there are infrastructure issues, but those fairly easily get abstracted away by hardware, libraries (home grown or otherwise), and sometimes off the shelf software like nginx, or services like Mashery.
Typically this is where you'd see an enterprise service bus (ESB), like RabbitMQ, MSMQ, Tibco, Azure ServiceBus, etc... However, webhooks are gaining traction, I not personally a fan when used for domain events, but they have uses that are very isolated and beneficial so I would definitely expect to see them in conjuction with an ESB. Then you sometimes also see a shared database as a means of handing off work, think loading a staging table in some shared database for some other micro-service to eventually perform work on.
by means of example could I ask the kind of system that is simpler than a monolithic approach? even abstracting one service out added so much overhead and complexity in a testbed project for us for no real gain. that said, we’re nowhere near the ‘minimum size’ that amazon et al recommend for microservices (each microservice maintained by a ‘two-pizza team’)
&gt; Personally I prefer catching query issues at compile time rather than waiting for them to become runtime exceptions Riiight, because LINQ to SQL translation *never* offers up any runtime surprises 🙄 and of course we all just use magic strings to write our raw SQL
&gt; and of course we all just use magic strings to write our raw SQL There's literally a sibling comment arguing for just that. Plus you aren't proposing a different ORM or a solution offering compile time guarantees. 
&gt; Not everyone uses EF at all Try telling that to every job advert for .NET ever, super annoying! I've been using MongoDB recently. I like it.
&gt; a solution offering compile time guarantees A fluent SQL builder API along with a T4 template that generates helpers for table/column/sproc/parameter/etc. names catches the vast majority of compiler catch-able issues. In my experience, "full" (not micro) ORM's inevitably become more of a problem than they are worth.
What did you switch to?
See the way I read that is you're just re-producing EF CF but making it much more difficult on yourself. I've never found full ORMs to be problematic, but you definitely need to understand how they work under the hood or you'll run into strangeness. 
I understand the inner working of a handful of major .Net ORMs, including L2S and EF, quite well, thanks. There's an quite large swath of functionality in SQL Server, much more any other RDBMS, that is inaccessible through EF+LINQ without significant, complicated, custom extension of SQL translation, through hooks that are only available in some versions of EF, if you can even accomplish it at all. But sure, not stuff you're likely to need in toy apps with no real life span that you could've just as well written in Rails. ORM's can be ok for some limited scenarios, if you've architected it well, but in my 20+ years as a professional developer I can confidently say I've seen more time burned than saved from using ORMs, and with how often you end up needing to work around the ORM it rarely seems worth the trouble at all. Developers are especially happy to work with last decade's ORM that no one can realistically replace without "The Big Rewrite" because it's managed to leak it's way through the code base trying to use all those handy features. But I'm getting to be an old fart. Tried and true. Keep it light. Unless you enjoy your web app framework version being locked to your ORM version (looking at you ASP.Net Core &amp; EF Core).
Take a look at [LiquidWeb](https://liquidweb.evyy.net/c/1232138/278394/4464) Cloud Sites for .net.
Before anything you have to abstract your data from .NET framework and storage them into a file format such as txt, csv and so on. Then you have to edit your script files to get data from these file formats with arguments. And you have to run a command line to work with python scripts. For instance, Your script files must be work on command line interface $ python dfs.py data.txt Then you need a .NET code for run a such kind of command. For example, https://loune.net/2017/06/running-shell-bash-commands-in-net-core/ (it is for unix based OS) Then you will run that command line in your .NET code like this. var result = "python dfs.py data.txt".Bash(); result variable is output of that script file on command line interface. To summarize above, -Abstract your data from .NET -Edit your scripts to get that data -Use process.start method to run a python script. 
Interesting, I've only ever heard of the two-pizza rule in regards to meeting productivity, not from a architectural design perspective. TLDR: Monoliths incur too much complexity over time and the appearance of simplicity is really just the comfort of familiarity in disguise. Plus microservice architectures have ancillary benefits through communication with the business. When a micro-service represents a bounded context and a ubiquitous language has been established communication with the business and with the technical representatives contain little friction. Finally, monoliths are far less resilient than microservices to failure as microservices can continue to work in a degraded state where with a monolith if one part goes down it all goes down. So this might be a little long, apologies: I don't know if or how I'd package an example for you in regards to this but my experience has been that monoliths that are &gt; 3-5 years old and remain in active development are unfortunately almost always complex beasts where even the smallest change introduces systemic risk. Not to say a monolith is a bad architectural decision and that it can't be built well, because it is entirely possible that it is the right decision and I know they can be built well. Monoliths are perceived as simple because that is what we're used to but we regularly encounter issues with complexity. Depending on how you've scoped your service and what you define as a service your inability for you to extract one from your monolith means you're architecture is far too coupled. That therein lies the problem, a lot of people are in the same state as you, where replacing a CRUD call to a database with a call to an api or a message on a service bus is a monumental task. That should be a story that a developer can do in a couple of hours with sufficient testing. If you're trying hard to adhere to SOLID principles then replacing any call with another, regardless of implementation, should be fairly trivial. Most code bases have varying levels of adherence to SOLID and most of them are further away than closer to strict adherence. Well defined bounded contexts/micro-services should make that problem fall away on its own. While it is entirely possible to build a micro-service into a monolith, if every discussion about adding feature x starts with, does it make sense within this context, you should have a pretty clear delineation between roles and those roles should primarily, although not always, be relatively small in scope and stature. An overly simplistic example of what I am saying would be for you to consider a medical scheduling application. There are many different bounded contexts within this, scheduling, resources (doctors, nurses, rooms, etc), facilities (clinics, hospitals, labs, etc), insurance, etc. In a monolith all of these different concepts would be represented within a single code base through different namespaces/projects. There would likely be some shared service layer that speaks to n number of bounded contexts and there are service methods with crazy cyclomatic complexity and/or could be being called by 50+ different code locations within your application not counting all the unit tests that are wrapped around it (hopefully). You make a change there and it is extremely risk to areas not even immediately concerned with the new feature you're adding. So, many developers/teams will simply add a new method, and now you have cruft and many different ways to implement approximately the same thing and new developers don't know which implementation to use as their standard bearer. However, considering the example, if you were working in a microservice that change would be isolated to only whatever bounded context you're working in. Cyclomatic complexity should be reduced, although not always, but the risk of change is reduced. Reduced more by the ability to test easier through reduced dependencies and easier to achieve code coverage but also by implicit cognitive simplification of the single responsibility principle, which entails doing one thing and doing it really well. In closing there is nothing that doesn't include fault tolerance that I can think of that micro-services can do that a monolith can not, but the clear physical separation of these bounded contexts causes thought processes to change in such a way as to produce cleaner, leaner, and more productive software. Again, this is my experience and opinion only.
Ehh..... Some of that depends on your network. If you're in a local (i.e. intranet) environment the calls will be pretty cheap. Out in the wild, the cost could be greater. As others have mentioned, Durable Message queues like RabbitMQ or MSMQ are another option. Another option would be something like Akka.NET, Proto Actor, or SignalR. These systems typically use long-lived connections between services, which can help with overhead/latency.
Thank you so much for the response. Sorry if I'm misunderstanding, but you're saying I should make a bash script and run that from vb? Also, I made my python script so that it wouldn't need any arguments. All I need it to do is run. It will take in a txt file and output two txt files. 
Agree with you on most of this. You tend to think about service boundaries *far* more in a microservice approach. This changes certain things about how you write code, but over time it tends to be more maintainable as a result. The biggest frustration I see with developers getting started with microservices is development setup. All you have to do is script the problem away; at my last two shops we simply set up scripts to automatically 'set-up' the microservices in IIS for you. This is also really handy if you're in a shop that is too big to use VS Publish-to-IIS features, but too small (or cheap) to pay for something like Octopus.
No, you don't have to make a bash script to run python script. Let me clear the what I meant. There is no difference between bash script and python script. So, lets say you opened your Command Prompt (CMD) on your windows. Then you entered your script folder with command below. $ cd script_folder Or $ cd C:/Users/blabla/Desktop/py_scripts Then you just entered python command $ python dfs.py It is equivalent as double-click the dfs.py file on File Browser on Windows. So, thats mean, if you run that command on CMD with Process.Start style then it will run that script file. 
Oh I see, thank you very much. My only problem is I am on Windows and actually using vb code. In your example you said: var result = "a string for the cmd to enter in".Bash() I don't know how I'd go about using .Bash() in visual basic
Ah, forget about my example I tought you were writing C#. You can reference any example in internet with "run a command in cmd with VB" (e.g. https://www.codeproject.com/Questions/1108915/How-to-run-CMD-command-from-visual-basic) 
I have no idea what that is, I do however have a long memory of Microsoft pimping something as "The Greatest Thing That Ever Happened To Computers" spending a great deal of money, then killing it. Just because Github is the current Golden Child means nothing. In fact, even .Net is not guaranteed a free pass. 
Title got me excited. Excitement quickly gone.
Can the AWS CLI tool be used to deploy to a Linux Elastic Beanstalk environment?
Bump! I was having a HUGE issue with linking local content within an app. This helped me out so much! # [Using site root relative links in Razor](https://stackoverflow.com/questions/8574237/using-site-root-relative-links-in-razor)
Yeah, basically roll your own container or wait for an announcement.
ok. i think the solution would be to place the list inside that same class and use the parent id as the foreign key but since im just building a sample i called savechanges twice. first get generated id then assign to parent id of objects in the list
You can use the dotnet CLI to do that.
Use Core, it's the newer one and the direction the platform is headed. At this point I personally think Core is mature enough that there is never a reason to use the legacy version for a new project.
Depends on what your app requires. Some packages that can be used on .NET do not have a .NET Core equivalent. We have an Oracle DB and the EF core driver for it will not be released until 3rd quarter of this year. Until it comes out, I will be forced to work with ASP.NET. 
Simple enough lol, thanks. I’ll get started using Core then.
Core is the way to go for exploratory projects. It seems to be the future framework for .NET going forward once things like Entity Framework, SignalR, Windows Desktop, etc. are production ready \(That likely won't happen until Core 3.0, but might as well start learning now\). .NET Framework is for an all Windows environment. .NET Core is for cross platform \(Linux server or Windows\). .NET Core is faster in many cases and is direction Microsoft is looking to take going forward.
Thanks, I’ll keep this in mind for the future, but for now since I’m simply trying to get a feel for the environment and my knowledge on some languages I’ll use Core. But in the future lll make sure to check what I’ll end up needing and make the call based on that.
Okay cool. So Core definitely seems like the way to start learning. But what exactly do you mean that Framework is solely for windows. If I were to build a webpage would it not be displayable on a browser running within MacOs?
Does anyone know if it is possible to CI from travis-ci to AWS?
.NET Framework would need to have a Windows Server to serve it up publicly via IIS. You can run any page in Visual Studio locally cause it launches in a local copy of IIS Express.
The server application (for .NET Framework) runs on Windows; the webpage it produces will display on anything. With .NET Core you can run the server application happily on Windows, macOS or Linux.
.NET Core 2.0 has an awesome *.All meta package so it seems more lightweight and intelligent. 
Hey thanks for all your help. Sorry I haven't been responding. I've been trying to try it out. I think it should work, but I'm having problems because my Python is installed in one drive (C:) and the script I'm trying to access is in another (D:) I tried calling Process.Start("D:") before but that didn't work out
When you double click the .py script from Windows Explorer it actually runs python.exe with the script path as an argument (like a command line argument). So you need to use Process.Start in the same fashion: // python.exe must be in PATH env variable for this to work Process.Start("python.exe", @"D:\path\to\scripts\myscript.py") // if python.exe is not in PATH env variable Process.Start(@"C:\path\to\python.exe", "D:\path\to\scripts\myscript.py") The syntax may be a little different if you're using VB (vs. C#) - but the class/method names and parameter order is the same
Okokok this is really helpful. I think I'm getting closer. If I open up cmd and enter in: C:\path\to\python.exe "D:\path\to\pythonScript.py" It outputs the two .txt files in D:\ instead of where the script is programmed to output them. This is probably because I used relative path names for their output instead of absolute. I'm gonna go and change that and see if it works, but do you know of any way to make it so that they would output relative to where the .py script is?
Replaced in 2.1 with Microsoft.aspnetcore.app
You could change your working directory to where the .py script is. Then when you run the process.start mentioned above with your abs path it should result in the files writing where you want.
Maybe this could be helpful too http://ironpython.net/
Interesting, thanks for sharing. Our architecture feels pretty good / maintainable / testable - we separate command, query and event handlers and have minimal fluff and abstractions, but code to interfaces and it's for the most part good SOLID design. The first recommendation found and our foray into pulling out a service was auth (entication/orisation) - security principles and the like. The pain point with this is that user access, user properties, identity and access rights are a layer/offering that apply to almost every bit of functionality in the system. For instance, administering users (who are stored in the auth bounded context) and their claims (which point to IDs of objects in other contexts) and non-other auth-related relationships (contact information) is much more complex than having a single entity model with simple joins to them, for example. It's all solveable (indeed we did), but the different in complexity is great. I actually think auth for that reason may not be a great one to pull out. In the past we've done document repositories as separate services ("blob storage++") which is fine. Would you say a uS approach is a trade-off, design/development/deployment complexity and time up-front, so that maintainability is better?
Agree with previous commentators. Where I work we are stuck with .Net Framework since we depend on libraries that are not yet supported by Core (specifically the Dynamics CRM SDK). I guess/hope it's just a matter of time until we also can switch to Core.
Yeah I've seen that. Just didn't wanna resort to that yet haha
&gt; one async method, which can return Task, Task or ValueTask. This is still not correct, as I told you on your last post already. &gt; One way or another all Tasks returned from TAP method must be activated. This is not correct. They don't have to be activated, but they definitely **should** be. That is because awaiting a `Task` will not start it, but will wait for it to finish - which just never happens if no one starts it. &gt; In order for TAP method to be cancelable, it has to accept a CancellationToken as a parameter It doesn't has to be a cancellation token. It can be anything, the cancellation token is just the preferred approach. &gt; TAP method will return Task that is in the Canceled state, which is considered to be the final state for the task. This depends on **how** the task is cancelled. It's a valid approach to not cancel the task, and simply return a result when cancellation is requested. &gt; It is important to notice that no exception has been thrown. If you use `ThrowIfCancellationRequested`, then an exception has been thrown - it was just catched. That is an important distinction. Also, your code is wrong here. You are `awaiting` the `OperationAsync` returned task, and in the next line "at some point later" you call `Cancel`. The way the example is structured it's hardly understandable what you mean. And if you cancel a task, then `awaiting` it will result in an exception. &gt; The event is raised on SynchronizationContext on which instance of Progress was created. Only if there was a synchronization context present. &gt; UploadFilesAsync Very poor code example. there's no reason why you'd wrap the actual logic in a `Task.Run()`. You're just creating unnecessary overhead. &gt; we used the ContinueWith method, which is basically creating another task which will execute asynchronously once the Task is done. If it's done, cancelled, or faulted. Not only when it's done. Probably depends on how you define "done - I'd understand it as "Completed", aka the task finished successfully. Plenty of wrong and misleading information here. And a fucking big ass advertisement popup everytime you visit the website.
By not hiring from such crapshops like weblineindia.
But EF is not the only way write data access layer :) i have seen quite a few arguments stating that .Net core is not yet usable as EF core is missing many of ef6 features. Sure, when migrating a complex app it may be not that easy to switch, but I think people need to stop thinking of entity framework as the one and only way to access database. From my experience EF is very often causing issues by giving developers a false impression they are dealing with objects and not a database underneath. This is especially pronounced if underlying data model is not really designed to facilitate most frequent queries performed in the application. In my past 3 projects I’ve seen similar issues, abysmal performance caused by select n+1, weird hacks due to misunderstandings of how entity tracking works, people using code first but forgetting or not caring to create indices leading to performance issues months after the code has been deployed, etc. At the moment, for my personal projects dapper is the default choice for writing a dal and it has worked well so far. 
Cries in Framework. Actually though, I'm just moving off a .NET Framework project onto my first major production .NET Core project. Still have not read up on all the minuta but honestly, if the only thing was better package manager support (in Visual Studio) and cross platform support then I'd still be all in for it.
Honestly I wouldn't get hung up too much on Framework vs Core at this point. Once you go into the industry unless you get lucky you'll likely find yourself using whatever .NET &amp; version of it that your place of employment uses. If you're super new to C# (And OO), then I'd suggest just worrying about learning what good Object Oriented coding practices look like. Maybe learn about the values of package management (IE: Nuget or npm) And from there just have some fun. Ultimately JARU is right, .NET is heading for Core as a platform. But for what a beginner needs to worry about, there really isn't much that matters.
.NET Core is Microsoft opening up to platforms other than Windows. Core takes functionality of the "big" .NET Framework that are not tightly coupled with the OS. Some stuff come easily, others need to be re-written, and some will probably never make it over. It's still a growing platform. While Core is absolutely fine to start learning the language, you should be aware that it had a very bumpy start (1.0 - 1.1). This resulted in tutorials, blog posts and stackoverflow answers that might still have high ranking on Google searches but are very confusing at best and totally useless at worst.
If your an experienced developer then I would agree with people here go with .net core. But note you'll come across issues with documentation out in the wild inconsistencies with core 1.1 and 2.x. 
This is all about "the stack". I have read some great articles with diagram about Microsoft .NET stack, but can't find source of this information - only I was able to find some "second-hand" blog post: https://kaushalsubedi.com/blog/2018/04/24/how-microsoft-made-me-love-net-core-and-c-again/
Thanks for the tip. It didn't seem to work however... I'll keep experimenting
Core is newer, Framework is Windows-only but has a vaster ecosystem of package support at this time. Target NetStandard 2.0 (aimed at unifying the difference), and you can execute that code on EITHER Core 2.1 or full Framework. 
Yeap!
mjao
&gt;one async method, which can return Task, Task or ValueTask. Mentioned later in the text &gt;One way or another all Tasks returned from TAP method must be activated. Updated to should. &gt;In order for TAP method to be cancelable, it has to accept a CancellationToken as a parameter It is mentioned that this is the preferred approach. &gt;This depends on how the task is cancelled. It's a valid approach to not cancel the task, and simply return a result when cancellation is requested. This doesn't change the fact that Canceled state is the final state in the flow. &gt; If you use ThrowIfCancellationRequested, then an exception has been thrown - it was just catched. That is an important distinction. It is not wrong, or misleading, however. &gt; Only if there was a synchronization context present. Mentioned later in the text 
&gt; Ultimately JARU is right, .NET is heading for Core as a platform. But for what a beginner needs to worry about, there really isn't much that matters. Exactly this. 
OK, umm I guess this settles which source control system Microsoft is going forward with in Visual Studio.
Haven't read it, but \[this\]\([https://blogs.msdn.microsoft.com/devops/2018/06/04/vsts\-github/](https://blogs.msdn.microsoft.com/devops/2018/06/04/vsts-github/)\) might give you an answer
They actually mentioned in a blog post that development on tfs would continue. https://blogs.msdn.microsoft.com/devops/2018/06/04/vsts-github/?utm_source=t.co&amp;utm_medium=referral
Yeah this post mentions that development for tfs will continue 
Very nice, thank you. The Axios docs don't really cover progress reporting, so it's nice to see a good example of it. As a side note, I see you've created a JS class for the `saveCoffee` function. In general you don't actually need classes too much in JS, and free standing exported functions in modules is the preferred approach. While I can understand that you want to create a "singleton" in order to make sure there's only one place dealing with an API endpoint - in this example it doesn't make any sense to do so because there's only one method in the class and it doesn't depend on anything else. I think a better approach would be to simply export the `saveCoffee` function.
Silly question, why do you need kudu to create a folder in Azure? Cant you just create a folder in the solution and put a placeholder content file in there?
inb4 someone gets all butthurt and goes on a tirade about how TFS is different to TFVC blah blah... it's all the same thing collectively referred to as TFS
Pretty new to the .NET scene myself, and I cannot tell you how happy I am you brought this up! From what I am seeing, Dapper should work fine as long as an Oracle driver for .NET Core exists. Oracle currently has a beta release for ManagedDataAccess.Core, so my only hangup at this point is not having an official driver to use. 
Have you seen https://pythonnet.github.io project? Embedding Python API part: https://github.com/pythonnet/pythonnet/blob/master/README.md#embedding-python-in-net
But, its not. TFVC is an SCM like git, and TFS is an SDLC tool \(in scope its like if you combined Jira, git, and Jenkins into one product\). You can use TFS without ever using TFVC.
I have literally not once heard someone say "Yeah the code is in TFVC" or "get latest in TFVC". It's always been "TFS".
Microsoft uses github to interact with their developers on their open source offerings in an independent and open way. Its already a big part of their company. TFS is the on\-premise version of VSTS, and is their premium SDLC tool. They aren't going to get rid of either...the two products fulfill different needs for Microsoft.
Yes, you can. :)
But they could be referring to a git repository when they say code is in 'TFS'. Your statement only works at all because TFS is the only well known implementation of TFVC. If it was something implemented in multiple tools then your statement becomes less logical...like this statement that doesn't work: *...it's all the same thing collectively referred to as github.* *I have literally not once heard someone say "Yeah the code is in git" or "get latest in git". It's always been "github".*
[https://sourceforge.net/projects/canusb/](https://sourceforge.net/projects/canusb/) [http://www.can232.com/?page\_id=72](http://www.can232.com/?page_id=72) If you want to make something from scratch \- CAN is a serial bus protocol, so my best bet would be start from .NET SerialPort. If you want to use Sockets, you'd need an adaptor to ethernet.
#5 should probably read "your" (or you're if you really want to rile up the internet)
Fixed. Thank you.
I love C# but somewhat ashamed to say that didn't know anything about Anders Hejlsberg; so thanks for sharing!
Yeah. Outsource to offshore because it's "cheap." Lol. Watch your project take 10x longer than budgeted for and quality go completely down the drain. Ive seen it happen first-hand countless times.
Quickly found other grammar errors. You need an editor.
I agree about acquisitions simply being killed. I was just reminding that much most?) of GitHub’s revenue doesn’t come from github.com, but from licenses for on premise installs. Not a “turn it off” kinda service, but I’m sure they’ll find a way to ruin it.
11. Unity. 
Try linq2db, it is great.
you should give linq2db a try. 
Can't use the 2.1 meta package and MySql. The MySql provider doesn't work with EF 2.1
Gotcha. Hope they update that soon.
Eh, there are better DI frameworks ;\)
I don't know if I'm in the minority here but I do not like async/await. Maybe it is just because one of my coworkers doesn't know what he is doing and writes synchronous code using async/await. He'll await immediately after calling an async call all the time, which means that it turns into a blocking call. The problem is, to somebody not that familiar with it the call \*looks\* asynchronous but really isn't.
12. [Godot](https://godotengine.org/article/godot-3-0-released#csharp)
&gt; He'll await immediately after calling an async call all the time If I understand you right, that's exactly what is supposed to happen. When awaiting, the entire method is suspended until the async event completes and (roughly) execution continues elsewhere on the call stack.
&gt; He'll await immediately after calling an async call all the time, Like this? var t = SomethingAsync(); var r = await t; It should be var r = await SomethingAsync(); 
&gt; which means that it turns into a blocking call This is not true. The call won't block; the runtime can/will still go away and do other things while the `async` method call completes You dont need to have something between the `async` and `await` keywords to see performance benefits. 
That's the same thing. The variable `t` is a task object.
As someone who has a strong tendency to put things on separate lines, I think that this is not something that should be on a separate line.
I agree. But since it compiles to the same thing, I can't rightly complain about it.
I wouldn't let a dev on my projects use that extra line ;) I'm not sure it if jhartwell had an issue with style or just doesn't like the async/await
That still frees up the thread, which is very important if you are calling from the UI thread. 
This is sound cool 
Well played.
It should be Apps instead of App's as well...
That is how is supposed to work. The problem I have with async methods is that, by conventions, they are prefixed “Async”, which: 1) Breaks interface abstraction as it clearly states that is asynchronous, while it should be inferred from the return Task type 2) Makes the code more verbose: Do() vs await DoAsync() 3) Many interfaces are bloated with pairs of “DoSomething” and “DoSomethingAsync” methods, breaking YAGNI principle. I know why is like this, and I know that the C# team is well aware of it, but still it feels not right and it feels more and more outdated with every passing year
We're using MongoDB for OLTP. Azure offers CosmosDB as a M$ solution. By using a document database, you eliminate the [Object Relational Impedence Mismatch](https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch) problem, thus eliminating the need for an ORM like EF. It's painfully simple to use once you get your head out of relational-land and start thinking in terms of documents (at least when in this context). Just build a DAO the way you need it to look, fill it with data, and save it. It serializes as BSON (basically JSON). It took us a few weeks to get comfortable with but now data persistence isn't even much of a concern anymore. We barely think about it. It's also easier to evolve since you don't have to design schemas and/or necessarily migrate data into them. They're just objects. Add properties and collections to them as needed. Of course, normal programming stuff applies. That is, adding properties is non-breaking, but removing properties could be. The nice thing is that the build-in serializer can handle things like missing properties with a simple decoration in the DAO. For example, if old documents are missing a property or new documents have extra properties, they can be ignored. The query language is different and takes a bit to get used to. Basic queries are stupid simple. You basically template your search like... "Give me stuff that looks like this" kind of queries. Querying into collections get a bit more different. Logical operators take some getting used to. But all in all, it's a more expressive query language than SQL so once you actually learn it, life gets easier. That said, IDEs now support SQL on top of MongoDB and it works pretty well. So for example, Studio 3T is basically SSMS for Mongo. It supports both query languages and helps bridge the knowledge gap. It's worth mentioning though, if you don't have a data warehousing pipeline, then it's gonna be challenging later on when it comes time to do reporting, which is why I call out using this as an OLTP solution (i.e. application side, not reporting side). Don't get me wrong, MongoDB does fantastic with reporting, but not until you get really comfortable with the query syntax along with aggregation pipelines, and map reduce; and it's tough to cross link data. That said, we just had a demo from MongoDB that showed it in a data warehousing scenario and it was great, with some annoying gotchas. Not quite ready for prime time imo unless your entire system is built on top of MongoDB. TL;DR, it's a shift in mindset and skill set addition for sure, but not a super difficult one and totally worth it imo if you have the option to bring it in. It's far more natural to application programming where relational DBs are far more natural for reporting. Licensing is free, you just have to either host it or use a cloud provider like Atlas or Azure (if you go with CosmosDB).
No I haven't, thanks. That's really interesting, but I specifically want separate Python files because I want people to be able to make their own algorithms if they wish
beginners indeterminate .. ?
These are some serious weak arguments imho. Only 10 is about the actual language. 
Maybe he was referring to the game engine
just wanted to say I think for front ends you meant React, not Rust :\) Rust is a native language usually for system development. Otherwise, agreed completely. There's like 2 users that spread hate for EF on this sub and the newer developers just grab hold of it and don't know why.
Most definitely was, I was just makin' a joke.
Haha, sorry I didn’t get the joke. Good one mate
Haven't checked this out yet but this is exactly what I've been planning on setting out to do recently! Awesome to have a sort of guideline like this to help out!
&gt; I've seen more time burned than saved from using ORMs, and with how often you end up needing to work around the ORM it rarely seems worth the trouble at all. This. Trying to box with the damn ORM is a colossal waste of time over something that could have been written correctly the first time in a stored procedure. That said, most of these problems occur when you reach outside of the intended scope of EF --&gt; CRUD. If you're writing reports in EF, then you are doing it wrong. In all likeliness, you're probably a terrorist. If you stick to CRUD, then you're probably not having these problems and my opinion of EF is less negative... &gt; Developers are especially happy to work with last decade's ORM that no one can realistically replace without "The Big Rewrite" because it's managed to leak it's way through the code base trying to use all those handy features. Omg this. This is one of those few areas where I actually prefer to stick to the basics. I'll use a micro ORM like dapper, but only because it mostly just abstracts away the ADO boilerplate and helps with mapping. When I was still using RDMS in a CRUD model, I'd write those as query/nonquery sources using dapper and wrap those sources in generic repositories/data-access abstractions. Before that, back when I was using EF, we did the same thing. Before that, when we were just writing pure ADO, it was still the same thing. It's a little more work up front to lock in the pattern, but now the "Big Rewrite" becomes simply a matter of re-implementing the interfaces which I've actually had to do a few times while we were still trying different solutions and it really wasn't that much of a problem. To be fair, this is a global design paradigm imo, not just with data. Everything in the code should be replaceable just like parts in a car or computer. Using EF is like Apple using that effing Thunderbolt connector instead of embracing USB-C earlier like the rest of the world was in the middle of doing. It works... it works well... for things that were made just for it, but now you're stuck with a $3000 macbook that you can't even plug your new iPhone into and then you buy a newer macbook with the TB3 connectors and now none of your damn peripherals can be plugged into it without this giant hub tethered to your "wireless" laptop?!?! (table flip). I jest... mostly. That wasn't me, that was all of my Apple fanboi friends. &gt; Unless you enjoy your web app framework and ORM versions being locked in step (looking at you ASP.Net Core &amp; EF Core). The ... *core*... of the problem...? EF is bad. Even when you're good at EF, it's still not great. It has always been a huge topic of debate for a reason. It's always something like a compatibility/migration issue, or a someone doesn't know the inner workings of EF issue, or a custom mapping issue, or an enum issue, or a someone eff'd up the edmx issue, or source control issue, or a someone didn't quite get this schema right and now the mapping is dorked up issue, or a this was db-first and it's making code-first more challenging (or vice versa) issue. Even our DW architect (who design DriveTime's DW btw) is tearing apart the "good queries" that are being run and replacing them with procs to fix compounding efficiency issues and showing us the difference. It's huge. In addition to that, slowly removing EF from the equation is actually speeding up development (that's weird... EF is supposed to speed it up... right?). Accidental complexity is what it is. You tried to oversimplify a problem and ended up making it worse. For small projects, I have no quarrel; it's great at rapid prototyping, but for large evolving projects or REPORTING? No. Just no. I commented above that we actually just removed relational databases from the application space completely and we couldn't be happier. It took some adjustment but now the shackles are off. No use for an ORM let alone a *need*. Personally, I am done with RDMS as a preferred OLTP solution. But that's a different discussion. Side thought: You know... EF core is viable right now. It is. MS just stripped it down to a more appropriate, less "crutchy" implementation. Just sayin... Annnnyway, I think the point is that saying EF is a blocker for netcore being "viable" is absurd. I can see it being a blocker for those that chose that route trying to convert to netcore, but that kind of just proves the point made above. Basically... **EF is technical debt**. It's there because you wanted to save time instead of doing something right and now the price -- other than less than optimal performance (which equates to money lost) and a slew of development speed bumps (which is money lost) -- is that you can't upgrade right now, which means you're still locked onto expensive windows servers (money lost) and are going to have less options should you want to containerize (money lost). I'd suggest learning from that mistake and building your new projects a bit more loosely coupled and in .net core and start a research effort for identifying those kinds of coupling and create phased plans to remove them. But "no problem" you say? "We don't really have a development problem with EF" you say? How many more iterations are you going to spend with your head in the sand? It's not your money, I suppose... ugh... I apologize for that. It's just frustrating watching people paint themselves into corners when the bulk of the development world is throwing up warning signs at every turn. P.S. I'm not even an old fart. I love picking up new frameworks or practices that make development faster and more consistent. I'm a relatively young solution architect and it's what most of my job is -- finding better solutions to meet business objectives as quickly and reliably as possible and then driving those efforts to fruition. The one thing I do not budge on is EF. And I tend to choose frameworks and practices that are easy to get out of. 
It isn't the style I don't like, it is just the async/await keywords. I prefer using TPL directly instead of the abstraction. 
To be fair, you don't need to outsource to fuck up a software development project.
yes, beginner-intermediate
Very cool, thanks for doing this. Looking forward to part 3. I didn't know Id4 had `dotnet new` templates, that alone was worth the watch. Id4 gets a lot of praise, but it has never been very clear to me how Id4 and ASP.NET Identity work together or what exactly Id4 puts on top of ASP.NET. So I'm enjoying this series.
ok, watching already thanks
Sure. But this isnt about that. Its about outsourcing.
Will it all run in docker? Add that and this will be perfect 
That is something that you can definitely do with pythonnet \+ you can call python code directly, without needing to read/write some text files to pass information between .NET and python code. Various usage examples: [https://github.com/pythonnet/pythonnet/tree/master/src/embed\_tests](https://github.com/pythonnet/pythonnet/tree/master/src/embed_tests)
Regarding the Async suffix by the way \- why is that even a convention as any function returning a Task can be awaited \- the async enables the await within the calling function. So yeah seems a bit of an overkill to say "this can be awaited" when the return type allows it to be either way.
 The Async suffix convention is quite simple: overloading doesn't allow for the same function name with the same parameter set. Async functions often do the exact same as their synchronous counterpart so the parameters are the same. You could argue that it should instead be a convention to have a CancellationToken parameter to Task functions but that might be an even worse convention as now everything will have to pretend to be cancellable despite not using the token for anything.
I used TPL sparingly in the past. I'm currently building a new system with async everywhere, which I wouldn't be inclined to do with TPL. 
Glad to hear it, if you watch it and see anything you'd like to add, let me know, I'll keep the series going.
That's a great idea, I'll try to get it wired up during the deployment section of the series
Yes, the templates are a great feature
They are not all horror stories. Go Jek - a unicorn - (https://economictimes.indiatimes.com/small-biz/hr-leadership/people/go-jek-books-a-long-ride-with-indian-startups/articleshow/55306892.cms) scaled out their operations on an offshoring project which then they acquired. 
It's far too tied to Windows to go cross-platform. We're probably going to be stuck with shitty REST interfaces for IPC.
Anders had mostly moved on. Mads Torgersen is mostly in charge now.
Yeah, this post is huge! What a pain. 
I've been meddling with the Admin UI template. Awesome that I can quickly shell out users, roles, claims, etc. What's not clear to me is how to take my exiting API, which is a simple .NET Core Web API project and a few controllers/pages, and secure it under the "umbrella" of the Id4 Admin UI template. Is this something you think you'll cover in a screencast?
Thanks! I didn't expect them to realize the topic would come up and answer it this quickly. :)
.NET and Windows are built on COM, of course it has a future. After Longhorn's failure, all the Windows components that were being written in .NET got reborn as COM APIs. Since Vista all major native API additions to Windows are done in COM. WinRT, nowadays known as UWP, is built on COM. It follows the original ideas for .NET genesis ([Ext-VOS](https://blogs.msdn.microsoft.com/dsyme/2012/07/05/more-c-net-generics-research-project-history-the-msr-white-paper-from-mid-1999/)) aka COM Runtime, which is why .NET environment variables all start with [COMPlus](https://github.com/dotnet/coreclr/blob/master/Documentation/project-docs/clr-configuration-knobs.md). Just that instead of using COM type libraries, UWP makes use of .NET metadata and allows for implementation inheritance as well. Check [IInspectable ](https://msdn.microsoft.com/en-us/library/br205821\(v=vs.85\).aspx) for the root interface. Is it going to be cross-platform? Most likely not. Is it going to be the future of Windows development? Unless Microsoft decides to reboot Windows again, most certainly.
&gt; I know why is like this, As someone who has never thought about this, but totally agrees with you, why is it like that? 
If you are mocking a static function, something is seriously wrong. 1. You are mocking something that should just be unit tested and then used with confidence. (e.g. `Math.Max`). Learn how to write unit tests. 2. You are using a static function to represent global state or an external dependency. Don't do that.
Easy, don’t use static based singletons and static classes, instead use inversion of control framework with Singleton scoped components. Solves the problem, allows mocking, and is a cleaner more readable and more extendable architecture. 
Another *real* reason c# is hip is its use in Cake Build Scripts.
I’ll try to find a source, but in summary: when await/async was introduced, they didn’t find a better way to integrate it without making confusing the existing code. So you have to mark a method with “async” if you use “await” on it instead of depending on the compiller. One example of why it must be verbose is that on “async void” methods the caller has no clear way to know that it is actually an async call. So you should append a “Async” suffix on it. If C# would be rewritten with async functionality from scratch, it probably would be cleanner, but we are stuck with that for now. But hey, it’s much better than old callback styles
This is good stuff, I've been following this loosely over the last year or so and am glad you've stuck with the project. Really love it and am keen to use it for personal projects.
Uh oh. Soon the moron brigade will come and shit all over the place and tell you that what you proposed is not a singleton. (Been there recently, and it ain't pretty. It's sad when the junior devs can't differentiate between concepts and implementation)
It was also so api's could support async and sync versions of the same method - you can't overload a method just by changing its return type, so a different name is required 
Wes, my man, been following your series on YT for the ASP.NET tutorials and got your Forums course. Hope the job search is going well? Glad your back. Also, r/learnprogramming might be interested as well, though that subreddit has become biased towards python as of late. I have noticed that some developers or instructors call Services and Repositories to be synonyms and create multiple class libraries where others keep them organized in a Data folder. Might be something to keep in mind.
I am implementing is4 at the moment. Something I want to do is have separate applications on separate containers for IS4, the SPA (an mvc project) and the API. I am a little unsure how to validate against the api. The spa should send a token to the api. Then I assume I’ll use a httpclient to call the is4 server to validate authentication and then store authorization in memory. Any ideas about that?
So when do you decide to put stuff on your site vs on YouTube? Do you go back and forth? Great series btw!
The documentation on SPAs is pretty good. http://docs.identityserver.io/en/release/quickstarts/7_javascript_client.html
lol why will I need two database connections *two weeks later* How do I inherit from a static class? 
This post is describing strategies for adapting an existing codebase and getting it under test so that you can begin to safely refactor it.
I like to understand the business process first, then model classes around the entities involved. Think about the scenarios from the perspective of the various people that will be using the app. From this base, I can do ahead with modelling the data, the entities, the classes, and then translating the human processes into code. Here I think about efficiency, robustness, error checking. Best Practices are applied to all areas, and is a constant reading on Google to see what is recommended by experts, by Framework authors, etc. And then adopting them into your design and implementation.
This is an excellent start! I go scouring the web for tutorials like this. Every tutorial I find is just slightly off from what I want to do. Either they're not creating a SPA, or the authentication piece is missing, or they're using AngularJS, or the tutorial is very old. I'm VERY excited to see not only all the pieces I want to use, but also in a video that's about as new as you can get (.Net core 2.1!). Thanks!
I really need to learn Docker and microservice architecture so if this is a possibility then that would be perfect! But I've been waiting for a .Net Core 2.1 full stack with angular tutorial for a while. I'll subscribe right away! Thank you so much.
This is awesome, but I think i'll finish your [ASP.NET](https://ASP.NET) Core series first. 
Always wanted to check dev express! I'm interested if still available.
/u/ben_a_adams Is there any written recap of these community standups? They used to do them in the past, but I can't find any for months. I want the information, but I don't have the patience to watch the 45 minutes talking nonsense for the 5 minutes content.
If anyone has any questions, u/mtz94 and me can help :)
I didn't see anyone drop this link from Microsoft so I thought I would. [Choosing between .NET Core and .NET Framework for server apps](https://docs.microsoft.com/en-us/dotnet/standard/choosing-core-framework-server)
Jon Galloway does post the community links from it; but I'm not sure if there is a write up: https://www.one-tab.com/page/mMnTUMCLTp-HVnQ7OBK5yw
Great effort!
I'm not sure about the name. When I saw ML.NET I honestly thought it was an ML based language for .NET.
Hi BenReadit, Wow, this a broad question :-). There is a full book of material in those few sentences. So at this point, I can only give a 30.000ft view: 1. Gather the initial basic requirements. Don't overdo it. 70% is going to change anyway. You just need something to start of from. Preferably in the form of handful of user stories. Especially when working with clients. 2. Model the domain classes that are required, from the user stories 3. Build some service objects that handle some of the obvious logic from the user stories. Unit test this logic. DO NOT build a simple UI at this point just to wire up some code to see if it works! Thank me in a few days when requirements change and then again in 2 years when you have to revisit the project. 4. Build a minimum UI based on the user stories and wire up the views, controllers, services, etc 5. See what need edits/changes/augmentation for the user stories to complete and work on that, always writing tests for at least the service objects in the process. 6. Show it to the people who decide and, based on what they say, edit or expand on the user stories. 7. Start again at step 2. Rinse and repeat. Hope this helps :-)
IMO, the ecosystem around a language is quite important as well. It's no use getting into C# if there are no tools, frameworks, companies, etc build on it. Unless you're talking about language semantics, syntax etc but this post is about why, from a developer's and productivity perspective, C# is strong. Not so much about the inner workings of the language itself :-).
You can keep all Python code in separate files, near you main exe, for example, or in a special folder of your choice. And directly execute this files from .NET.
&gt; IMO, the ecosystem around a language is quite important as well. It's no use getting into C# if there are no tools, frameworks, companies, etc build on it. Unless you're talking about language semantics, syntax etc but this post is about why, from a developer's and productivity perspective, C# is strong. Not so much about the inner workings of the language itself :-). I'm glad you are excited about C# (again). But you make it sound as if Microsoft never supplied good tools and frameworks, and as if companies couldn't build software with it. The arguments in the article are supported with weak arguments about vendor lock-in, crazy licenses and other tin foil opinions. This is how I read your article; C# is great in 2018 because Microsoft used to be shit, but is no longer. Then you make an argument about Xamarin, because well Windows is shit right? Luckily you can now build software on Mac for Linux, because apparently that is what business truely seek in a development platform. Let's praise .NETCore because that's cool right now. Don't go into details, why would you? Visual Studio? Sucks! Let's use something else because it isn't one of the most loved IDE's out there for at least two decades. Forget Mads, forget C# 7.x and all the upcoming features. Anders is mostly occupied with TypeScript but he must be the reason C# is hip in 2018 but not in 2001. Async/Await .. We've been using that for 6 years at least. I'm probably biased because I used to work for Microsoft and have been a .NET developer for more than 10 years. You on the other hand are the opposite. I'm not trying to shit on your article but I hope this has given you som insight into how you might come across to at least some part of the .NET development community. Peace
You sound passionate, so let's take that as our common ground :-). You make good points but the scope of what you're saying is different from the intent of my post and would require a completely different article. &gt; Let's praise .NETCore because that's cool right now. Don't go into details, why would you? Again, scope matters. &gt; Visual Studio? Sucks! The article does say that it just highlights that it's optional today. This something many dev from outside the .net space are not aware of. VS doesn't suck. But it does have a reputation for sucking to those outside of .net &gt; You on the other hand are the opposite. I have worked with .net since the early 2000's as well. &gt; I'm not trying to shit on your article but I hope this has given you som insight into how you might come across to at least some part of the .NET development community. No worries :-). I appreciate you trying to move .net forward as well. We just both have our own way of doing it. Have a great day my brother. 
Hey. Was Click in Bricks involved with one of the Kiosky in-store terminals by any chance? I once helped a friend apply for a job Sales Management job there. What are the odds? 
Unfortunately no :-). Bricks &amp; Clicks was my small dev shop here in NL, building WebForms apps. Nothing kiosky I'm afraid.
So what does this mean - readme is empty
Allright. The name sounded very familiar. Was referring to this: https://www.emerce.nl/nieuws/bricks-clicks-toekomst-retailer Have a nice day. 
That's weird.. can you refresh the page? Github is the mirror (PRs open), we have always used gitab for the main repo though https://code.videolan.org/videolan/LibVLCSharp
C# is a better version of Java
Now I see it thanks!
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/rp0PGkK.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e07d2wc) 
I develop a set of Tag Helpers that help me prototypes UI very quickly (based on Bootstrap 4) at work. I use it mostly with ASP.NET Core Pages although it should work with ASP.NET MVC Core as well. So far it has helped me tremendously on having everything in one place instead of having the UI prototypes in different software, etc. If there's enough interest, I'll put some effort to open source it. Otherwise, I'll keep this for myself.
Looks interesting! I have some buggy directshow based code, i'll probably try rewrite it to use vlc.
Thank you for your response. I will go through the links after finishing my work at hand.
Seems like Microsoft's new-ish notification shaming them: &gt; Extension ‘JetBrains ReSharper Ultimate’ likely caused 9 seconds of unresponsiveness. Disabling it may improve your experience. Finally got Jetbrains to take it seriously after how many years? Or is it that customers (like me) are jumping ship to solutions that use Roslyn natively and perform much better (Roslynator, etc)? Every ReSharper patch for the last two years all I've looked for is performance improvements and yet nope. I'll keep an eye on this, and might even re-sub if they actually do something. But if all this is is a serious of excuses and blaming others, I'm just done with ReSharper. 
Or you can host in any number of non-IIS webserver on windows too. Apache/Nginx/no webserver and just exposing the app yourself/Nancy/Suave/etc. There's a wide world out there, and most of them run on other platforms too
 &gt;So in other words your opinion shouldn't be trusted on the topic. Sure, if you don't like learning different approaches to things I guess. I can promise you that EF will always be magnitudes slower than making a call to a stored procedure and simply mapping the results yourself. I work on systems that have tens of thousands concurrent connections where EF would absolutely crumble. I've also worked on projects where EF was the bottleneck. &gt;Personally I prefer catching query issues at compile time rather than waiting for them to become runtime exceptions I prefer designing my solutions with proper unit and integration testing to where this would never even be an issue.
&gt; Sure, if you don't like learning different approaches to things I guess. Sure, but someone who is proud of never trying alternatives over their entire career is likely someone with little perspective on how it sits. &gt; I can promise you that EF will always be magnitudes slower than making a call to a stored procedure and simply mapping the results yourself. I can promise you that it executes exactly the same query on the database, and that results generation and connection latency remains the primary bottlenecks for most queries. We're talking sub-10 ms difference even on a complex model at both ends. A "magnitude" slower on a fast thing is still very fast. You're prematurely optimizing the wrong thing. &gt; I work on systems that have tens of thousands concurrent connections As am I. &gt; EF would absolutely crumble. EF doesn't really scale in that way, one connection or a million, the overhead scales linearly or better (due to structure and query caching). I'd be interested to know what you were doing with EF that caused it to collapse at millions of connections? &gt; I prefer designing my solutions with proper unit and integration testing to where this would never even be an issue. I too would like to live in a fantasy land with rainbows and unicorns. Let me know which fortune 500 has full coverage and I'll apply today. I've worked at financial companies, medical, industrial, and never seen it yet even with the whole team behind it. 
Am I the only one who cringed multiple times reading that? Yes, you definitely wrote your dB access code poorly if you’re getting those errors. Why even mention that? Also, the pattern of speech just annoyed me...
I still love Teamcity, regardless of what is being built, simply because it seems the most logical and most flexible to me. It’s just the perfect combination of GUI + the option to run any arbitrary command. 
I've been using Travis CI because that's what I'm used to. It also seems faster than the (limited) others I've tried. Are there any benefits that others might have over Travis? (a Linux build environment is preferred)
i would definitely be interested. i'm leaning towards using razor pages for my next project, and i've started the process of looking for free to use and open source tag helpers, web ui components. etc. and so far havent turned up a lot. hoping to find alternatives to kendo ui, jqWidgets, syncfusion web ui components etc
I use Jenkins and Travis CI on a colo server. Jenkins is great if you need to build regular .net stuff too. If your project is public you can't beat Travis CI. I see lots of people using AppVeyor but I haven't used it yet. 
I'm curious to know if SignalR .core will deal with the issues you identified.
&gt;Sure, but someone who is proud of never trying alternatives over their entire career is likely someone with little perspective on how it sits. I agree. Are you implying I've never used it? I'm very familiar with it. It still underperforms &gt;I can promise you that it executes exactly the same query on the database, Sure, if you're writing basic selects. Once we move beyond "babbys first SQL query" then things get a little interesting. &gt;EF doesn't really scale in that way, one connection or a million, the overhead scales linearly or better (due to structure and query caching). I'd be interested to know what you were doing with EF that caused it to collapse at millions of connections? There's significantly more to the architecture that I left out because I didn't feel like typing paragraphs on end about it. EF would fail. It did fail which is why it was removed. &gt;I too would like to live in a fantasy land with rainbows and unicorns. Let me know which fortune 500 has full coverage and I'll apply today. I've worked at financial companies, medical, industrial, and never seen it yet even with the whole team behind it. Can't help you there. Be more assertive I suppose. I've been on dysfunctional projects with no standards, gave them the standards, ioc, expected code coverage, you name it. I am the giver of puppies and rainbows, and most of that is because I don't allow EF into my shit 😋 
Hmm, this is an 'it depends' answer. The 'best' is probably TeamCity but I prefer out the box solutions these days and roll with Bitbucket's Pipelines because it's so much simpler and you don't have to manage it.
Hey; Yes, I more or less go back and forth. I put more time into building premium courses for my site, and try to go a bit more in-depth there. 
Gotcha. Thanks for the info. One last question. Do you post your code on GitHub?
Suppose you have a complicated build involving multiple projects and you need to build a web app with it. It could get complicated quick. That would be a reason to ditch Travis CI. But for anything on GitHub it's stupid easy to get a build running. People underestimate how much overhead goes into setting up and maintaing your own build server.
&gt; Sure, if you're writing basic selects. Once we move beyond "babbys first SQL query" then things get a little interesting. Apart from it being a little verbose in its assigned names, I've never seen it and I've created plenty of joins that would turn your hair white. Never had it generate queries that I could make faster without changing the underlying logic of the query itself. If you have specific circumstances or examples I am all ears. &gt; EF would fail. It did fail which is why it was removed. Via what technical mechanism? You're effectively arguing that EF gets less efficient as the scale increases, you have to explain what in the technology stack causes that to occur. EF is a little heavy at light workloads (due to the overhead of the cache and object hierarchy) but gets more efficient, not less, as it grows. All of your post on this topic come across like someone who has never really dived too deeply into EF because they rejected the entire concept out of hand. Which is to say, you don't seem to know enough about it to criticize it, that's why you're so vague in your critizems. 
My gut feeling is that it will help with many but not all the challenges I've run into. If I ever do implement a site with SignalR Core I'd like to write a post comparing it to vanilla WebSockets.
I love VSTS. they have agents for windows, linux and Mac and it's easy to let it use your own build agents if you like. Has great flow from builds, all the way to deploying to servers. I use VSTS to build and publish everything from WPF to web apps to Xamarin, including pushing to the Google and Apple stores. For me, being able to use one build system to hit all the .NET technologies is very powerful.
MS has been doing a good job polishing VSTS the last few years. It continues to rapidly evolve. 
Using bamboo ... UI is better than Jenkins, but Jenkins probably has everyone beat with the number of plugins, plus it's free. 
Hadn't heard of Roslynator before. Were there any ReSharper features that you missed when you moved to it (other than the extra time for tea/coffee)?
Saved, thanks!!
Where are the dataset files? Some of these examples have the "upload data" step with text or CSV files. Figured it would just be a resource in the project file.
I think the feature I missed the most after switching to Roslynator was the "Adjust namespaces" for those situations when you're moving the classes around. You can install one of the available, small extensions that do that though.
A few, the Solution-wide Code analysis is pretty comprehensive, some ReSharper extensions (e.g. Exceptional), and ReSharper still has great suggestions and hints. Roslynator, Roslynator Refactorings, and Microsoft IntelliCode make moving away from ReSharper less painful, but they don't replace everything ReSharper did. 
Jenkins pipeline has been all I need. 
Yes, I have a few projects on GitHub. I'm at github.com/wesdoyle
This is awesome. I was looking for something VLC related for Xamarin a couple years ago. Can this library scale/resize/compress videos? If it's in the readme and I missed it, feel free to be sarcastic about it. I'm under the weather today and possibly didn't read very well.
I like taking a bottom up approach. I make sure that anything I do is directly valuable to the customer on the short term. I don't like up front design process which often result in a architect astronaut syndrome or analysis paralysis with endless armchair philosophical discussion with the peers. I never create an interface until I have an actual use of two implementation. (Test or Mock does not count.... except for external web services like exchange rates) I then refactor as I go when the customer is able to give me feedback. (and he can always give feedback, because what I do is directly useful on the short term) The younger the project, the more refactoring you will have. As the project mature, your understanding of the project, and the customer's become more stable. For dev environment: I bundle every dependencies like database or external tool in docker images, then I create a dev time docker\-compose. Devs then just have to run this docker compose and can debug F5 the project and run tests. It saves documentation time, mock development time (no mock), maximize test code coverage, and you have no possible "it works on my machine" issues. I publish the app as a docker image. And I create production docker\-compose for the customer. (It works because my users are generally small enough to not need any fancy kubernetes... those who are big enough, can just customize easily by learning from my docker\-compose files)
I've had solution wide analysis turned off for a while now. Having it on brings my machine to it's knees!
You have a typo here: http://archive.is/UX3Dw#selection-261.0-263.0
Yeah we ran into both the closing issues and the thread safety, so my coworker wrote a Hopac-channel-based wrapper around them to help ensure proper concurrency: https://github.com/TheAngryByrd/Hopac.Websockets
Use anything that supports a docker-based build pipeline. We use gitlab ci with a base build image that has mono and recent versions of the dotnet SDK, use FAKE to perform builds/tests packaging, then publish Debian packages. We could also use the same setup to build binaries and copy them into alpine dotnet runtime containers for very slim deploys
Fixed. Thanks!
There are tools, like Forge, that can help with this. Also look into Mechanic to help as well. Both of these can be used from VS Code because they've been integrated into the Ionide plugin suite.
Nice. Thanks! I’ll check those out. 
My vote goes for VSTS. It's the dream tool for doing devops in .Net
As did I, but it is useful tool to run manually from the menu during deployments. 
Literally just posted the same point in the comments section.
Another vote for VSTS. Not a lot of scripting involved. Just a bunch of built in actions. Not that there’s anything wrong with scripting. 
So VS 2019 will be basically be the continuation of the incremental VS 2017 improvements.
Oh well, there is always Rider, which is awesome, IMO. 
There's [CoreRT](https://github.com/dotnet/corert/blob/master/Documentation/intro-to-corert.md) but I don't know if it's any good or what its development roadmap looks like.
Nothing wrong with that.
That photo is not creepy at all... 
So what have we learned from this announcement? It’s called VS 2019. It’s faster. More refactorings..
* it’s happeniiiiing *
Yep. Using CircleCI with `microsoft/aspnetcore-build:2.0` and deploying to `microsoft/aspnetcore:2.0`.
As long as updating gets easier/quicker when it comes to installing the update. I mean most of us will have VS open all day, why not download in the background if I allow a setting. Actually installing the update in the background is a lot harder but I can think of a couple of possible ways to update as much as possible before requiring the main process to close.
I answered one of their surveys and this was one of the questions they'd asked about. So they are working on it / prioritizing it.
Another good reason for this kind of conversation is Blazor
I think there was a fair bit of feedback about it with 2017 so it may have improved recently, but it’s still stung me in the past (although a lot of the speed, or lack of, can probably be attributed to our crappy office network) and basically being out of action for half a day due to an update is not good/fun
Yes, it can, but this is hard to do with that lib.
 .NET Native, IL2CPP and CoreRT are certainly not dead. .NET Native powers UWP applications, and Microsoft is still strong going forward with UWP no matter what. IL2CPP powers Unity native builds, specially their console targets. CoreRT's latest commit was 23 hours ago.
Can Microsoft stop naming visual studio after the year and just call it VS 19 or what ever it actual version is
Wait, what year is it?
http://i0.kym-cdn.com/photos/images/facebook/000/209/945/D6PfW.jpg
That and the size. For me it is 3 Gb over a 3 Mb line, it takes a full day and then there is a very small improvement or none I can see. I am not complaining and I try to be on the latest but it is just painful.
It was hard to do with the libraries we found at the time too. We had to do platform-specific. iOS was easier, and Android was a complete mess. Do you have any examples of using this lib for that purpose I could look at? Otherwise. I'll plan to play with it to see if we can simplify our codebase by transitioning to this
VS2017 is Version 15. Based on Assumptions 2019 would be Version 16
What are you trying to do exactly?
I prefer TeamCity, but I haven't used VSTS in quite a while and I hear it's much improved. AppVeyor had too many intermittent issues. They had the simplest Azure integration a few years ago but we dropped them like a hot potato before long.
Is Travis good for .NET too?
I'm on VSTS too. Would be awesome to be able to combine local and hosted build agents, but besides that, it's working very well for me.
That would only allow building core, right?
That's confusing, they should just name it after the year, so VS2017 is version 2017.
Basically .Net Core is great because you can deploy and run your code on almost anything. You can run it on Linux, Mac OSX, and of course windows. This makes it a much better choice if you want that kind of flexibility. Just the other day I ran a .Net Core console application on my Linux based raspberry pi! Also Microsoft is putting a lot of resources into .Net Core so learning it is a great way to stay competitive with your coding skills. If you are used to the standard .Net Framework then don't be scared. Writing code in .NET Core isn't THAT different from what you're used to. However, it is worth noting that not all of the libraries and APIs you might be used to in the .Net Framework have been ported over to .Net Core. The support for libraries should improve as time goes by but they are not 100&amp;#37; there yet.
Also, ctrl + , = good. More good like that.
Is there a Blade Runner easter egg? There's gotta be one!
No, you can use the dotnet SDK to build mono/full-framework projects as well. We do this with multiple products.
Why? Waaay more confusing that using years that everyone knows about. IMHO all software should begin with YYYY.&lt;whatever&gt; 
Doesn't do regular .net but does .net core. 
So it's like traditional sockets programming? Makes sense.
For god sake, can you stop developing the bloated Visual Studio and focus on making a Resharper-like plugin for VSCode that would take us 5 mins to be up &amp; running on any machine without worrying about spending the whole day downloading &amp; installing ! I really don't see any use of VS if we have all Resharper features in VSCode !
VS2019: now we own your soul.... 
I thought that was Alphabet's new tagline. 
Can you get a debugger as good in code ?
There is a debugger.... decent one actually: https://channel9.msdn.com/Shows/On-NET/John-Kemnetz-C-debugging-in-VS-Code Though I don't use the debugger a lot recently, thanks to the code tips from Resharper Also, Resharper recently launched a debugging killer feature (showing variable values on the right side of each line): https://imgur.com/a/K2Neh3U
If you know your webpack, you are fine.
https://www.youtube.com/watch?v=c6LqRw0NHtQ https://github.com/viewtool/CAN_Adapter Are also good links
Why, ASP.NET isn't a language...
I just wan to run in on linux man, my whole stack is running on linux except vs
O.o that's nice. One of my teammates uses code exclusively I'll point him in that direction.
So many haters here....
Got to admit, this would be rather nice.
The software versioning if for their use, not yours. Which is why they call it VS2017 and VS2019 for us.
Essentially scale or compress video to be sent over a mobile network without eating a lot of data 
Really eager to try out their AI powered refactoring though. It might be as good/better than Resharper. Speed is always appreciated too. Pretty neat news overall.
I'd switch to Linux in a heartbeat beat if they did this.
Someday I want to learn F# just cause I know there has to be a better way to do concurrency.
Be that as it may, if I have to choose between one or the other, I say go by year (in direct defiance to OP's suggestion)
Come to the foundation slack and join #beginners and learn! Or sign up for the next mentoring round! foundation.fsharp.com/join will get you started :)
That wouldn't really matter too much. Remember NET 2.0/3.5 are disabled by default in Windows 10 anyway. You should look into migrating to .NET 4 which should fix your issue and make sure users can run your app out of the box at all.
https://github.com/OmniSharp/omnisharp-roslyn/pull/1076
Let's rewind a bit to my original reply to your post, where you stated: &gt;.Net Core is definitely the future, for cheaper Linux hosting, architecture improvements, and performance gains. But until EF Core is "there" then it won't be fully baked. Which is not at all true, because the same data layers I've hand rolled in .net framework still work just as good as the ones I've done in .net core. You make it sound like EF is absolutely necessary, hence my reply about it being more of a crutch than a necessity. That shouldn't be your limiting factor. You seem really passionate about EF, so if it works for you, then more power to you. However, and this is a definite given, it will vastly under-perform a hand rolled data layer any day. The thing is, you don't need to constantly re-write code if you properly develop a series of base classes that can be reused amongst any project you're on. Which is what I've done. And I can implement it just as fast, if not faster, than someone using EF. As a result, I don't have that crutch, and can use .net core just fine. Architecturally, I prefer to keep responsibilities isolated. Let the database do database things, which is what it's built for, where my code has no business constructing queries. All I care about is making the call, getting the data, done. Another reason is unless there's an explicit need to use whatever framework, I don't need it. Very similar to starting a new project and immediately the UI people go "Let's use Angular!" - like okay, why? We're not doing anything fancy, regular jquery will work here just fine without all the obnoxious overhead. Long story short, you may think I'm not familiar with it, and I very much am, which is why I can confidently say "EF is not necessary anywhere." 
Correct link (using .org, not .com): http://foundation.fsharp.org/join :)
Has anyone gotten the Https redirection working with load balancers and elastic beanstalk? I can get my apex working but not www
Gah, thanks. Harder than I expected to get that right on mobile :(
Great news! Are there code samples available in F# ? ML is very popular with Python and since F# feels like typed Python then it might be a match made in heaven. F# &lt;3 ML
You could combine Website and Api into a single project / web application. We've done it both ways. I just find that the API concerns get lost in the mass of code that is an MVC website. So I would keep your web API service (with its controllers) in a separate project from the website (with its controllers). Easier to have different pipelines for the API vs the website. It's also clearer about what is website logic and what is API logic. That means a minimum of three projects. - Website or Host to house the MVC Website code - Api to house the Web API code - Client to house the request/response models being transmitted over the wire (can be published as a Nuget package). Both Host/Api projects can reference this Client project so that they speak the same language. Having them separate could also let you deploy just the API or just deploy the Website when making changes. If you do split into separate solutions, the Api and Client projects should live together. Then the Website project pulls in Client as a Nuget package.
Interesting, we switched to AppVeyor maybe a year ago and the only issue we've had was the Nuget outage (which wasn't AppVeyor's fault). Overall, we're happy with it. We use `appveyor.yml` in our projects so there's very little configuration that is done via the web UI. Used to use TeamCity as self-hosted, but were spending way too much time managing the box (keeping it patched, keeping the build tools up to date, building new images).
Unless you're a really big dev shop (spare bodies) or a really tiny dev shop (no budget), go with a hosted solution over a self-hosted solution. Let someone else worry about creating new build servers, new images, keeping the tooling up to date, troubleshooting problems. Beyond that, we're pretty happy with our switch from TeamCity to AppVeyor. We do Jenkins sites, Azure ASP.NET MVC/WebApi applications and .NET Core applications using private GitHub repos.
For search, my usual answer is ElasticSearch. Push the content of your models into an ES document index and use ES search features. It's lightning fast and very powerful search. You can configure it to do all sorts of things with how you weight / parse. The downside is that you need a process that pushes updates to the ES index whenever the source data changes. Ideally via a background job (Hangfire for us) which will retry if the ES index has a hiccup or latency on intake. So there's a bit of complexity. Various databases also have full-text search, but there's limitations and trade-offs there as well.
When I use api and and regular mvc controllers in the same project, I create a specific area folder for the api controllers. It helps keep things organized.
By client you mean a service/repository layer? Totally agree on the separation. Have you tried an api first approach before? 
I would rather suggest clean architecture. Build your domain services and domain objects at the core. Have your Web API or Rest APIs as just one mechanism of delivery. I would go with Web API and a angular front end for web. Same APIs can be consumed by Mobile Apps. Desktop will consume Domain services directly as the whole system (UI + Domain) will be sitting together unless you are building a dumb client.
I'll have to look at that. Have read a few Fowler books but don't remember seeing that before! 
Well yeah except you’ll be reading about something and they’ll say “you need to upgrade to 15.7” and you have no idea what version you’ve got. 
Yeah can’t wait for an AI to rename my files.
hope it is 64bit
You "don't use the debugger a lot recently"? What kind of software development are you doing where that makes any sense?
Good explanation, but nothing new here, and doesn't even mention any DI frameworks.
I would assume he means data transport objects. So simple "no logic" data model description objects that describe the request and response data for the api endpoints.
We are using this approach and it's working great, so go for it :) If you are using Visual Studio you might want to disable built in typescript compile since webpack will take care of that. &lt;PropertyGroup&gt; &lt;TypeScriptCompileBlocked&gt;true&lt;/TypeScriptCompileBlocked&gt; &lt;/PropertyGroup&gt; Also if you are used to using `npm` from command line it's best to disable automatic package restore from VS by going into VS options and setting **false** to everything in "Projects and Solutions &gt; Web Package Management &gt; Package Restore" otherwise you might get some weird strange behavior if you have different node version from VS.
Hope we can get rid of the tons of Node.js external processes junk that contributes to the total terrible VM size even for simple projects. Windows and .NET has threads and appdomains, looks like JavaScript kiddies can't understand it.
Hey_baby_wanna_kill_all_humans.cs
agreed, it feels almost weird to see a new post on this in 2018. i thought it was fairly common place in .net land now.
Sure - https://www.youtube.com/watch?v=lZq8Jlq18Ec 
But then people would complain about VS taking 1+GB of RAM ( because of in memory caches and other nifty things) People think less RAM usage == better performance 
It looks funny when you talk out of nowhere... cuz spinning the debugger in Xamirian Android and Asp core takes 5 seconds to run, instead of waiting, you can use check the logs !
I think people discover things for the first time and have an epiphany, and so feel the need to write a blog post. You get a similar thing with git: a whole load of blog posts about how git really works under the covers.
Yes, the classes specifically for passing data over the wire. Sometimes extension methods and constants/enums which can be used in the case of a Nuget package by the consumer of the API. But that only works well when the API consumer can use Nuget packages. Otherwise you have to expose things via an options endpoint on the API or as supplemental metadata in the response.
Our newer projects are API-only with Vue front-end.
I think versions should tell you about the size of the changes, not what the current year is (I have a calendar for that).
I have used Solr in the past as well. Worked quite good. Although ElasticSearch might be a more "popular" option.
Not really but this is a bit much. No need for tons of external processes, we have threads and CLR appartments for such purpose. The user experience is also clear, VS 2015 feels much more faster than VS 2017 on similar hardware.
Very cool and looks super easy to use. Is it this easy in Python as well? 
Are you referring to Intellicode? If so it is available as a preview already.
Xamirian my love
i always thought that as well. but you would be surprised how many people don't know about DI and once taught will think it is useless and they prefer to not use interfaces, etc because "it is a waste of time and provides no benefit"
I've tried AspCore with VueJs... and it was kinda annoying. So, I just kept the two apps separate, and used docker to let them talk to each others.
From the first video you are still using 2.0 but the video says 2.1. The project is still on the 2.0 template. Are you going to upgrade to 2.1.300 later?
LOL ... not for knowing the current year but knowing if version X is current or not.
Depends on the needs of the user registration, if its the same for all projects a shared page is fine. If its template (IE the project owner can add custom fields you store) you need templates for each project and then a general handler.
Its a spam site IMO, I am seeing this more code "help" sites that churn out a ton of helpful articles using low effort content and concepts from a decade ago. Sometimes they paraphrase or re-work something already published in official documentation. I would not be surprised if you could even have an article writing bot do this all day.
There is no right way really; it depends on the project requirements. But it's important to try and keep things as simple as you can and only add complexity as necessary and required since that complexity comes with a cost. 
I'm grateful for these kind of posts. I'll see DI mentioned and not really grasp what it is, but then when I see it clearly explained here, I go "Oh, that's what that is."
Or are attempting to pad thier resume by blogging.
think about how often you'll deploy each sub app and what you'll need to do to update the user registration code across all of them. how could you share that code and UI, or make it into its own sub app so it doesn't need updating/testing every time you update one of the sub apps.
Thank You all ! ;)
[https://exceptionnotfound.net/setting\-a\-custom\-default\-page\-in\-asp\-net\-core\-razor\-pages/](https://exceptionnotfound.net/setting-a-custom-default-page-in-asp-net-core-razor-pages/)
That's perfectly fine. I call that modules and have done the same thing but with Vue, VueX, TypeScript and Jest. It's multipage but with mini spa's.
Wouldn’t this just be an O Auth implementation? 
Have you tried Rider? I much prefer it to VS and its native on Linux. 
PostgreSQL, sure. I like it almost as much as I like SQL Server. But why waste all of that power on EF? PostgreSQL has the most comprehensive, standards compliant implementation of the ANSI SQL standard. Take advantage of it.
OAuth for Authentication. It doesn’t cover registration flow
I normally do the following: 1. API project 2. API.Dto project 3. API.Sdk project 4. Consumer project The API, API.Sdk and consumer references the API.Dto project, which contains the DTOs and Enums your mention. The API.Sdk project is the middleman, and it contains a HTTP client wrapper that deals with auth headers, json serialization/deserialization and such. Example of API.Sdk: using Example.Api.Dto; namespace Example.Api.Sdk { public interface ISdk { ICar Car { get; } } public class Sdk : ISdk { private IAuthentication _auth; public Sdk(IAuthentication auth) { _auth = auth; } public void Auth(IAuthentication auth) =&gt; _auth = auth; public ICar Car =&gt; new Car(_auth); } } namespace Example.Api.Sdk.Targets { public interface ICar { Task&lt;Dto.Car.Post.ResponseDto&gt; AddAsync(string model); Task&lt;Dto.Car.Get.ResponseDto&gt; GetAsync(int carId); ... } public class Car: BaseTarget, ICar { public Car(IAuthentication auth) : base(auth) { } public async Task&lt;Dto.Car.Post.ResponseDto&gt; AddAsync(string model) { return await new RestClient&lt;Dto.Car.Post.ResponseDto&gt;(Auth).PostAsync( "api/car", new Dto.Car.Post.RequestDto { model = model, }); } public async Task&lt;Dto.Car.Get.ResponseDto&gt; GetAsync(int carId) { return await new RestClient&lt;Dto.Car.Get.ResponseDto&gt;(Auth).GetAsync( $"api/car/{carId}", new Dto.Car.Get.RequestDto()); } } Well that's the basic idea anyways, and this allows me to use the sdk thusly in my consumer app (after doing the auth bit in middleware): using Example.Api.Sdk; namespace Web.Areas.Example.Targets.Car { public class Service : BaseService { private readonly ISdk _sdk; public Service(ISdk sdk) { _sdk = sdk; } public async Task&lt;IndexViewModel&gt; IndexViewAsync(int carId) { var carResult = await _sdk.Car.GetAsync(carId); return new IndexViewModel { CarId = carResult.CarId, }; } } There are probably better ways of doing this, but this assist in my architectural dogma of keeping the front end projects as easy as possible so the frontend guys can't mess up too much or struggle because they are exposed too much irrelevant and complex code.
Look into SSO (Single Sign On)... it might be what you wanna really achieve.
Oh didn't follow the news that much. Found the extension, will be trying it out. Thanks for the heads up.
Yeah sure... I don't use SQL server too much (at least in deployment) since it requires more resources than Postgres, aside from the fact there is a lightweight postgres docker image postgres:alpine compared to the gaint mssql (1 gb) image. SQL server requirements: &gt;At least 2GB of RAM (3.25 GB prior to 2017-CU2). Make sure to assign enough memory to the Docker VM if you're running on Docker for Mac or Windows. Postgres requirements: &gt;● a 1 GHz processor &gt;● 1 GB of RAM &gt;● 512 MB of HDD 
This is a good idea. Be your own idp and isp. The architecture will look pretty different from OP's drawing but I think it could work. Just need to do a profile generation on first sign on.
My bad, I meant SSO (single sign on). Like Live accounts or Google accounts (one registration, multiple tenants)
aspose pdf
I wrote something similar to WkWrap for a project, and it seemed to work much better than iTextSharp (which can be a huge pain) In my experience I had to run it in full trust so it could call the external process, so that might be something to keep in mind for you. 
Reporting Services would be an easy pure-MS way if your data could be considered a "set" and you're not trying to fill in a form. Create a client report definition (RDLC) file, instantiate a LocalReport object, give it the report path on disk (or embedded resource reference), the dataset, and export it as PDF in a byte array. Return the byte array as necessary. public byte[] RenderReportToPDF() { var localReport = new LocalReport(); var dataSetForReport = new List&lt;MyDataObject&gt; { ... }; // whatever you design the report to take a list of localReport.ReportEmbeddedResource = "Myproject.Reporting.ReportDefinitions.MyReport.rdlc"; // Embedded resource exmaple localReport.DataSources.Add(new ReportDataSource("MyDataSourceName", dataSetForReport)); return localReport.Render("PDF"); } 
Try [Winnovative HTML to PDF Converter](http://www.winnovative-software.com/html-To-Pdf-Converter.aspx)...
Check out the itextsharp library. The older versions are GPL. But the licenses are pretty cheap for the new versions and their support is great.
Are you only looking for free solutions? Or only listing free solutions? There quite a bit of PDF libraries that are okay. But all of them have rendering issues. My experience is there are no free solutions that are good enough. They either aren't stable, hard to upgrade or have major rendering issues. Your definition of good might be different though. I used [HiQPDF](http://www.hiqpdf.com/). Their site is quite 90's but the only thing that didn't render correctly was opaque text in a SVG chart. It can sometimes render rounded borders quite ugly. Personally I'm hoping someone manages to make the headless Chrome browser available through .NET or a REST api or some other technique that is useful for me. I didn't check but I suspect rendering in Chrome would be close to perfect. More info on that: [https://developers.google.com/web/updates/2017/04/headless\-chrome](https://developers.google.com/web/updates/2017/04/headless-chrome)
You would prefer to skip this ORM layer?
Python as a language is fundamentally flawed. However, the frameworks are pretty nice.
Keep up the good work, Microsoft. 
Thank you very much! I was having problems regarding npm packages because of VS npm's version and "you" solved that :D.
Prince is the best I’ve used, but it isn’t free. https://www.princexml.com
Could just have an auth server? Ie if you wanted separate domains doesn't necessarily mean the Auth isn't the same server(s). Look into identity server IMO
Not necessarily. It isn't that I don't like ORMs, in fact I wrote my own (https://www.nuget.org/packages/Tortuga.Chain.PostgreSql). They are really handy for basic CRUD operations and for calling stored procedures. The problem with EF specifically is that its designed for the lowest common denominator. Lets say you notice that you are moving a lot of data between the database and app-server. So write a stored procedure that processes the data in the database and returns the data in a pair of result sets. Can EF handle that? Hell no. But Tortuga Chain can do it natively. Or what about table-valued functions? The are basically parameterized views, but they're incredibly useful. Both Dapper and Tortuga Chain have no problem with this. But again, EF struggles. (You can force it to work in EF using raw SQL, but the syntax is clumsy and the framework fights against you.)
I wrote my own ORM. I needed to make sure it could read every table in the sample database. xUnit allowed me to scan the database for a list of tables. Then it feeds that list back into the test cases as parameters so if I have N tables in the database, I'll have M x N test cases in my results.
We use Telerik Reporting. It may be a bit heavy handed, but it does work.
i had trouble deploying this to azure, just mentioning in case op is, too. 
We use https://www.nuget.org/packages/HtmlRenderer.PdfSharp/1.5.1-beta1 and it works great.
I have [https://github.com/alvarcarto/url\-to\-pdf\-api?utm\_campaign=explore\-email&amp;utm\_medium=email&amp;utm\_source=newsletter&amp;utm\_term=weekly](https://github.com/alvarcarto/url-to-pdf-api?utm_campaign=explore-email&amp;utm_medium=email&amp;utm_source=newsletter&amp;utm_term=weekly) bookmarked... just passing along in case it's useful
We use https://selectpdf.com/ very easy abd works very well
Crystal reports export to pdf
You could check out [GemBox.Document](https://www.gemboxsoftware.com/document), it has a free version and a pro version. You can use it for a straightforward conversion, like this: var document = DocumentModel.Load("Input.html"); document.Save("Output.pdf"); Or something complex, like this: var document = new DocumentModel(); var options = new HtmlLoadOptions(); document.Content.LoadText("&lt;p style='color:red'&gt;Sample body.&lt;/p&gt;", options); var section = document.Sections[0]; var header = new HeaderFooter(document, HeaderFooterType.HeaderDefault); section.HeadersFooters.Add(header); header.Blocks.Content.LoadText("&lt;p style='color:green'&gt;Sample header.&lt;/p&gt;", options); var footer = new HeaderFooter(document, HeaderFooterType.FooterDefault); section.HeadersFooters.Add(footer); footer.Blocks.Content.LoadText("&lt;p style='color:blue'&gt;Sample footer.&lt;/p&gt;", options); document.Save("Sample.pdf");
Ew. Please don't go down this road if you aren't already vendor locked-in to Telerik. What an awful experience working with their products.
Thank You all for answers ;) On each registration form, only email and password fields are same, other fields are different. ( https://imgur.com/a/UaY6zb4 ). So let's generalize what we gonna do. What approach we're going to use.
^(Hi, I'm a bot for linking direct images of albums with only 1 image) **https://i.imgur.com/Y9TCyTJ.png** ^^[Source](https://github.com/AUTplayed/imguralbumbot) ^^| ^^[Why?](https://github.com/AUTplayed/imguralbumbot/blob/master/README.md) ^^| ^^[Creator](https://np.reddit.com/user/AUTplayed/) ^^| ^^[ignoreme](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=ignoreme&amp;message=ignoreme) ^^| ^^[deletthis](https://np.reddit.com/message/compose/?to=imguralbumbot&amp;subject=delet%20this&amp;message=delet%20this%20e0a7xei) 
Postgres for development. SQL Server for prod. That's a good idea.
Yes, I'm using IdentityServer for Authentication. I just want to let users register from all projects with different fields in the same DB for further work. On DB layer I resolved this task, but I don't know how to realize it on the server layer.
I've used it with no problems on a small project. 
Piggybacking on SSO... IdentityServer is a good solution for this with a big support community online.
How about something like this, guys? 
Haha, yeah... like I said, it works, but is heavy handed. Knowing what I know now, I probably would go a different route.
For Python, you might want to look at TensorFlow instead.
just to be clear. what azure plan? 
Last time I was playing around with it I wasn't able to run it in conjunction with ReSharper. Just a heads up.
Yes, almost like that ;)
I've just started using xUnit and so far I'm not getting any additional value. InlineData is so much better than trying to parameter MSTest tests, so it will serve me well at some point. One thing that bugs me is the lack of message parameters on asserts. I considered fixing it myself, but there is a lot of work involved because the exceptions in it have a hardcoded message they pass to the base exception and no ctor to pass the message parameter. I got a good laugh from one of the analyzers:"xUnit1004 : Test methods should not be skipped". If they shouldn't be skipped, then the Fact attribute shouldn't have a Skip option.
I believe Standard S1 is what trouble shooting (old trouble shooting doc from 2014?) stated. Last time I tried this was a couple months ago, still GDI errors and only on deployment. works in dev.
Chrome headless outputs pdf. It's free and all formatting is done in modern HTML and CSS.
We use [https://www.nrecosite.com/pdf\_generator\_net.aspx](https://www.nrecosite.com/pdf_generator_net.aspx) and love it.
[Remote Access to Local ASP.NET Core Applications from Mobile Devices](https://blog.kloud.com.au/2017/02/27/remote-access-to-local-aspnet-core-apps-from-mobile-devices/)
Start by separating business logic from presentation layer in the old site. Wrap everything which doesn't alter the page output in functions that can easily be extracted and ported to C#, then leave the rest behind and built interface and routing from scratch. 
[https://www.devexpress.com/Products/NET/Office\-File\-API/](https://www.devexpress.com/Products/NET/Office-File-API/) if you don't mind using a commercial library.
Maybe try doing in their same domain? Of course not a full blown application, but just the core parts. Also count points for you because it shows interest from your side. Put it on Github and mention during the interview as well. Good luck, you got this! 
A todo app is a classic small project. Add, update, delete tasks from a list. Maybe have support for categories so you can practice joins. Do it as a console app or the GUI/web framework of your choice 
We used phantomjs to convert from HTML to pdf. Pros: * Free * Modern html+css to pdf conversion is quite accurate * With a bit of fiddling you can add page headers and footers and the result is good Cons: * Need to start a process so it depends on the trust level of your application * Adding headers and footers is not really intuitive * The conversion process is a bit slow
Details? We just started using Telerik grids for Angular. I want to know what we're getting ourselves into. Thanks!
Prefer it the most when it comes to RDBS, been using it with [marten](https://github.com/JasperFx/Marten) and like it a ton.
Prince is by far the most feature rich and standard compliant I have found. I love the print media support and find it critical for controlling things like headers and footers.
Marten has been on my radar for a while. What are you using for identity management with marten persistence?
Not 100% I understand your question, but a majority of my projects now are .Net Core MVC, using Marten/Postgres as my data layer, Auth0 for user Auth when applicable, hosted on Azure. Is that what you were wondering?
Yes. Was wondering how/if your authorization mechanism is tied into the persistence layer. Auth0 is the answer.
I really love this idea! I don't know what kind of projects they do, but I'm going to do a research, get "inspiration" from their past projects and do a mock-up or something similar and put it on Github. I have a few days to do it. Seriously, nice advice. Thanks a lot!
&gt; A todo app is a classic small project. &gt; Add, update, delete tasks from a list. Maybe have support for categories so you can practice joins. It's a cliche idea, but the point is to create something from scratch and make it work! About creating the Database and project, any recommended approaches/technologies?? Should I create it with MVC and Entity Framework??? What DB approach is commonly used?? &gt; In terms of advice: don’t fret not knowing something off the top of your head and having to do some google searches to figure it out. If they give you bad marks for doing research, you don’t wanna work with them anyway. Nice advice! I was a bit afraid of googling, that's why I'm practicing and learning things during this week. But now that I think about it, I shouldn't hesitate about googling, because it'll be a time-limited evaluation. Thanks a lot!
&gt; prefer to not use interfaces, etc because "it is a waste of time and provides no benefit" I tend to only bother with interfaces once I have a second implementation.
Unit tests
I decided to convert the old web forms project to asp.net core mvc and just add the API in its own area since it is so small. I'm using a simple service layer with dependency injection, automapping and servicestack ormlite - everything worked out great! Will look at VueJS and maybe Express for something else.
They deleted their old documentation and removed a lot of support threads to push their newer product. I had to requested in a support ticket for the older documentation.
Assert.True() takes in a message parameter. But it's sad that most of the others like Assert.Empty(), Assert.Contains(), Assert.NotNull() do not. Some of that can be worked around with creating helper methods, or structuring tests to be more obvious. If I'm heavily testing a particular method in the SUT, I'll often create a helper that takes the result, the expected result, and other contextual information and pass it to a static helper method that handles the Assert. In there, I'll use Assert.True() and then pass in a message string that provides useful contextual information.
`Theory` and `InlineData` are at the top of the list. Lets you test half a dozen to a few dozen variations on a theme. Without needing a few dozen test methods. Beyond that, most of what I do is periodically refactor tests to make the Arrange / Act / Assert steps more obvious. Creating helper methods to make the various steps easier. Also learning that if I have to Mock more than 2-3 things or deal with the IoC container... it's time to break the logic that I'm testing out into a more testable static method.
We use this at work - https://wkhtmltopdf.org/ 
I got the gdi errors when trying to use a custom font. After some back and forth with support they said the only option is to use preinstalled fonts or spin up a VM and host it in IIS where you can install it yourself. Ended up making the reports a microservice and hosted it in a small vm because I had to have the micr font.
You are in for a hell of a ride. How do I know? I did exactly what you are doing last week. It sucked. Oh the fun part of this? Lots of things **say** they can convert html to pdf - but the problem is they don't do it well. The html has to be rendered - and the HTMLtoPDF library its using an older html rendering engine, and the page has lots of complicated CSS and other stuff in it then the results will look like shit. Even if the html and css are simple, it may look like shit. Some HTMLtoPDF libraries are stuck in html4 and old versions of CSS and can't even get a lot of that right. Think about how the new, modern Reddit web site would look in Internet Explorer 5.0. Yeah. Some of the solutions are that bad. (I'm looking at you PDFSharp.) And maybe that doesn't matter to you. Maybe the html page you want to turn into a PDF is stupid simple, 600 pixels wide, text only, no graphics and limited formatting. Then most libraries will work... UNLESS you host on Azure. Is your site hosted on Azure? If it is, and you are NOT doing a VM, then you have some problems. Why? Azure doesn't give you access to full GDI+ (its a web server. Why would you need GDI?), many of the html to pdf solutions rely on that. (I'm glaring at you this time PDFSharp. You need a day glow red warning label alerting everyone that because of this GDI problem you don't run on Azure.) If you are **hosted on Azure, you can go with Rotativa**. Its free, open source, and uses wkhtmltopdf. You can read more about Rotativa here. https://letsfollowtheyellowbrickroad.blogspot.com/search/label/Rotativa Its one of the few options that work with Azure, and its working well for me. (Except the latest versions wouldn't work. I had to roll back to 1.6.4. That worked like a charm.) I still had to make some CSS tweaks to get PDF's that look ok... but Rotativa was the best option by far. You can read about Azure's Web App Sandbox here: https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox#unsupported-frameworks **Pay close attention to the 3rd last paragraph** If you are NOT hosted on Azure, you have a lot of other possibilities. The one that had the best results of html to pdf was Headless Chrome, which makes sense because it's chrome that we all know and love. You give it a command to go to a URL and turn that into a PDF and it does. Nicely. But it will only work on Azure if you are running a VM. Wanna skip the bullshit and just hire it out to a service with an API? Funny enough there are several, at a cost of less than pennies per pdf. https://rotativahq.com/ looked good, efficient and cheap. https://www.html2pdfrocket.com/#pricing seemed ok. And some other services are FAR more expensive. Wanna pay lots of money for DLL's that claim to do what you need? Have at it. There are plenty of those out there too - but I didn't look into them. Maybe I should have. 
Razor Pages are the closest you are going to get to the classic ASP development experience. https://docs.microsoft.com/en-us/aspnet/core/mvc/razor-pages/?view=aspnetcore-2.1&amp;tabs=visual-studio 
Yep. Ran into this problem last week. Had a solution working in dev with PDFSharp. Even though PDFSharp rendered the html like shit and kept screwing up paragraphs and text indents and other little annoying problems, we managed to get a working solution with a custom written html and css. AAAaaannndd... The damn thing wouldn't work on Azure. We spent time exploring path name problems, but it turns out its because of Azure and its lack of GDI. Read about all of that here: https://github.com/projectkudu/kudu/wiki/Azure-Web-App-sandbox#unsupported-frameworks
&gt; iTextSharp (which can be a huge pain) Oh yeah. I went down that road for a while. It was nothing but headaches and mangled, badly rendered html and css. Rotativa did a MUCH better job and there's no licensing issues.
&gt; But all of them have rendering issues. My experience is there are no free solutions that are good enough. I went through most of them. PDFSharp, iTextSharp... a few others. Headless Chrome had the best looking, most accurate results. BUT it doesn't work on Azure unless you are running a VM... It needs GDI and Azure doesn't give you that.
A large selling point of their software is being able to "Ajaxify" anything, allowing you to refresh content/controls on your page without a postback. This is cool in practice but their documentation is fucking terrible, the API is just terribly laid out and often makes no sense, and there are a million quirks for it that their own support can't solve because they "can't reproduce the issue on their end, please send a small program that reproduces the problem." It really takes a seasoned programming veteran who has seen a darker time (think the year 1998) to hammer them out. We had one such guy, one of the most productive programmers you will ever meet, a guy who powers through brick walls just on coffee, cigarettes, and liquor, who had to help me snuff out countless of these instances where something on a page. He is a brilliant programmer, but not like John Skeet. He's more like a surgeon operating on decomposing fetuses (terrible codebases and products). So this one time a simple Telerik RadGrid on the page just refused to refresh after page load. Another time the report viewer had a report that was way longer than the little frame it was in, but the report viewer didn't have a scrollbar and to this day as far as I know, no one has been able to find a way on this Earth to get a scrollbar to show up (the frame is part of the report viewer control, and there's no easy way to change that to my knowledge without completely breaking the report viewer itself). I have a million stories like these, and whenever we were able to solve them the answer was usually something completely stupid and seemingly unrelated that you will not be able to find by googling. Something like you have added an attribute to a completely unrelated control on the other side of the page, which doesn't work well with an attribute on the control that's having the issue. And if you Google, nobody says you can't do what you were doing before. Just a fucking pain in the ass to deal with. Their documentation seems okay, until you have to do something not described in their documentation or something slightly custom. It just describes the API with minimal examples, and there isn't much help online. I wish you all the best.
Sure that's a potential second implementation. Although I generally try separate code I want to test in a way that it has few dependencies.
it's going to be hard to sell the idea of buying a new IDE =/
Can someone ELI5 this to me?
thanks for confirming, sometimes i think im going crazy! there's little on gdi errors in azure (recent) and the ironic part is i HAD an app once on azure that worked. suddenly, i had a new deployment a yr or so later and it wouldn't work. i asked at a booth about it at MS Build and it is by design since that specific report dependency uses gdi and gdi, i believe, ties you to a server (hence /u/SoiledShip below mentioning having to use a VM). She suggested an api solution, which is why I was so interested in this thread: to see what you guys are using.
This is for creating AOT (Ahead of Time) compilation. Normally, .net and .net core is run in a JIT (Just in Time) compilation, which means code is compiled as it's being used. AOT would mean it wouldn't depend on the .net dependencies to be on the target machine to run, outside of linked resources.
If Visual Studio actually became faster every time Microsoft said it was faster than it would be able to load projects before I ever knew I needed them. 
&gt; AOT would mean it wouldn't depend on the .net dependencies to be on the target machine to run, outside of linked resources. You can already do that by [bundling the .Net runtime with your application](https://docs.microsoft.com/en-us/dotnet/core/deploying/#self-contained-deployments-scd).
Make a project related to their domain, never ever watch the following course while not working on a real project... here is a great course (of course you should follow his course while applying on the latest AspNetCore2.1): https://app.pluralsight.com/library/courses/aspdotnet-core-1-0-fundamentals/table-of-contents You should be able to: 1. Deal with responses and requests in .net core. 2. Persist data (in memory) and (on a db). 3. Handle authentication (signup login) and authorization (permissions). 4. Deploy your app in seconds. 5. (bonus) apply continuous integration. Don't forget to do all the steps: 1. Think (requirements &amp; design) 2. Develop (implement) 3. Test (verify) 4. Deploy (release) Best of luck ! 
I'd also recommend just using wkhtmltopdf directly. We are using this in Azure albeit on a VM because we need custom font support.
I'm willing to bet it's more common that people know what DI is, they just don't realize it's DI. Dependency Injection is a 25-dollar term for a 5-cent concept.
Will keep that in mind when using R# on my work machine. Thanks
You can compile apps to single native binary for win/mac/linux that is smaller(~4mb), starts faster, and uses less memory than the CLR.
Also won't run in an Azure App service since it uses GDI and development has stopped since the release of headless Chrome.
Indeed
Yes. PostgreSQL is my go to relational database for projects.
Custom theory attributes. [Theory, [RandomData](https://github.com/CreateAndFake/CreateAndFake/blob/master/tests/CreateAndFakeTests/RandomDataAttribute.cs)] populates my test method parameters with random values, stubs if I specify fakes as a parameter, and even automatically injects those stubs into following objects (with the rest of the values random). Switching from MSTest to xUnit was magical. My second favorite aspect is how you model classes like you'd normally do in code. No instance members? Static class. So much better.
I was all set to go with an API solution (or just build my own with headless chrome and a server that can run it) but I found that obscure article posted on Github.com (of all places) that detailed the gdi problem and specifically mentioned that Rotatable would work on Azure hosting. Tried it, it did. Winner winner, chicken dinner. 
Yay! Thanks everyone on the dev team and those contributing in other ways... I'm all over this...
how is it different than NGEN?
Fair point, I honestly forgot that you had to pay for it. I have an educational licence 
You should compliment xUnit with Shouldly.
I've used [NReco.PdfGenerator](https://www.nuget.org/packages/NReco.PdfGenerator/) for HTML to PDF. Worked best of those I tried. But HTML to PDF is no fun.
NGEN is dotnet framework only and closed source. 
I used https://www.nuget.org/packages/NReco.PdfGenerator/ yesterday at work and it seems to do the trick!
You would only need to update all 3 apps if the login stuff changed. Which IMHO tends to be pretty static.
This answer needs more consideration. Identity server is not simple.
Ive just finished writing something similar using angular. Each angular app connects to the same backend api and each account is given different jwt claim which defines what they have access to. My goal was to make it as simple as possible.
So you encrypted your passwords instead of hashing them?
I encrypted the _entire file_ that contains my passwords. With a PGP key that is only accessible to the server where P5 is installed ... But yes ... :)
If you can decrypt the passwords it is possible that other people can too. There's a reason for the time-constant hashing algorithms. Use them.
I'll chuck another one into the mix, especially if you're using Azure which can be a pain due to sandboxing issues. https://www.browserless.io/ Essentially this is "just" chrome instances running inside a Docker container, with a lightweight API for communicating with it. Now you could use websockets to connect directly to it, and using puppeteer get it to print you PDFs, but I actually contributed to the project recently and added a PDF endpoint, so now it's just a post. You can give it either HTML or a URL. Nice thing about this compared to other methods is the level of CSS support you get. Someone else in the thread has mentioned JsReport, which also has Chrome support. We evaluated us but we couldn't get the performance out of it because they were only using one instance of Chrome, which was being spun up and down for each request although they did mention they were looking to use a swarm of them instead but it wouldn't be ready for when we wanted to use it. But the interface and flexibility was really nice. You could also look into hosting AthenaPDF, which is very similar to Browserless but uses wkhmtltopdf instead of chrome. https://www.athenapdf.com/ Good luck.
Mega post.... thank you.
It's somehow still fine when you hashed the passwords. Encrypting them is worthless imo.
No no, don't you see? They know way better than all the security experts.
Clif notes: fuck Telerik...
If you have junior developers do all the crystal stuff. Its almkst a rite of passage. If they dont quit and take up making coffee when they are a senior programmer they will make the next poor sap do the same.
Regardless of what amounts of time-constant hashing algos I could possibly implement, due to the nature of C#, some C/ASM algo would always be able to **dwarf** my C# algos. 1 second of C# execution ==&gt; 0.00001 second of ASM execution. In addition, by invoking a hashing function written in C# hundreds of thousands of times, sure you'll probably make your password function take a second or two, and such implement _"slow hashing"_. However, do you want to know what other thing would occur (in C#) ...? **The garbage collector would kick in after 5 users have tried to log in** ...!! Making my server become literally useless ... Since the argument is as follows; _"If I had physical access to your password file, I could 'brute force' it with a Rainbow attack, using a dictionary"_. Well, here's my password file. Good luck :) gnupg-keypair:1EF027678DEB0FCF06266C448C4A8A3647CC8B3B Content-Type: multipart/encrypted; boundary="=-LLyo/DkZazvC4JmU6M3Qag=="; protocol="application/pgp-encrypted" --=-LLyo/DkZazvC4JmU6M3Qag== Content-Type: application/pgp-encrypted Content-Disposition: attachment Content-Transfer-Encoding: 7bit Version: 1 --=-LLyo/DkZazvC4JmU6M3Qag== Content-Type: application/octet-stream Content-Disposition: attachment -----BEGIN PGP MESSAGE----- Version: hQIMA4xKijZHzIs7AQ//U5kniwVL6jOR1onW5Itr3CZ5BvL3LykxcjMYyVC2Qdjj RZ4EWZ3W++i4CgTLynwxWz47kq20Mn9RmKbVkXqWsj51bHGAbIi/U6np/STlrWaJ /9dnDSR9UeaLmpP0h0uVmWrzCx2NmljGSY8ClrCWKN1iNhSlIDGgVNYf2mpps20l C4bqO+HYiYWWdr9uesPlk+MKJrIgKoGZq9OY3kYp2pS1+w4t2M2/rwP8sx8cWXOk TV2Bp+NQIIccGEwzRpDiTKtZANI+uQX0c7IzQEYAXm1EfKH636NlCQtGNWYR3Dex R4flBFUvzSp4eJa3/B+Y3eqs0R55wMubqSXsPzVCp5kMFTv1xSnJbsRsUoPBeNS6 1T92mlHB1Uviy5FOktNJrAuMtw5E8Z7ZD4/0sXIeplo9fKt6XoO3ou7w7CKckxoH lUIc/OBx8N0dAljYICpR4IObNPb3T3XRJ3m/+Onsf0NBJIrznLUUGtCFE/GFrMRL 4DVU9P0cAWyVAwHzZUgoejfw8g04jmIld0dLttPeq0oBZxiG5T04hBlarivl2Nb9 M31fdg8fmD2C9hDI/W68BT07SaoVkLDoUTA7c56mOtHnjLhYGqULIKpL3nCaWUg8 efj7n6P5b8eavGY+2DgGenrdblWgypk9H61iSuTcaNMk3fGkbAlKcwPXEyus2gvS tQGtb/sNVJS7PqhY3Lg28PQN2AlSw3DfUsdy4WfyRNDUgIPMHA0C22eRTsNVqyVN tzO7F/n8FkaYDH2hZstocgcMHnAxUcOzanO/DQF4m49UtfMHcb6Vhhlh2X2zDUok L8CDiVWC7XNjhjyapiJGA/I3NdMjQWwrhbdvFFH3qiuPGybMnUIjAX+kcPCGY024 ieVj67Lntuh+ILnIDqdrJPXRfAJ9xhUpBNR2/sv5AxDpFWiNm9A= =fHHa -----END PGP MESSAGE----- --=-LLyo/DkZazvC4JmU6M3Qag==-- 
Not really, see my other comment here where I pasted in my password file. The reasons is because I am not using a _"slow hashing implementation"_. However, implementing _"slow hashing"_ in C#, is arguably _impossible_ due to my comments in another place here ... So encrypting the file protects against an adversary having physical access to the file, performing a Rainbow/Dictionary brute force attack on it, to _"reverse engineer"_ passwords from it ... So even though you're technically right, you're unfortunately for all practical concerns not right ...
&gt; If you can decrypt the passwords it is possible that other people can too. My private PGP key is stored outside of the file system that is normally available from the _"application level"_ of my system. In addition, it is AES256 encrypted, with a password only accessible for those having access to my web.config - Which again, you guessed it, is not accessible from the _"application level"_ of my server. A _"root"_ account in the system arguably could decrypt the file. However, if you're already _"root"_ you wouldn't need to hack the system, since at that point you already _have_ hacked the system. Since the system also provides _"brute force"_ intrusion protection mechanisms, by preventing the same username from trying to login more than once every 20 seconds, this (at least in theory) makes the system **impenetrable** ... I realize there are no guarantees when it comes to security, but this logic is the equivalent of Fort Knox, and arguably _dwarfes_ any amounts of _"slow hashing"_ I could have possibly used in it - At least from a C# perspective.
This is all dangerous nonsense and you're clearly clueless about security, but from what I've seen in other posts you are immune to even the most well meaning advice and any attempt to educate you about better alternatives will just make you double down on even more convoluted schemes, so whatever floats your boat.
&gt; This is all dangerous nonsense I would love to hear your argument in regards to this ...? However, let's keep the _"superlatives"_ at a minimum please ... ... after all, we are adults, right ...?
&gt;he'd still need to know its AES password to be able to decrypt the private key. var confNode = new Node ("", "gpg-server-keypair-password"); var gnuPgPassword = context.RaiseEvent ("p5.config.get", confNode).FirstChild.Get&lt;string&gt; (context); [....] XUtil.Get (context, e.Args, ix =&gt; ConfigurationManager.AppSettings [ix]); This is a joke. You may as well put the passwords right in the web.config, the only "security" you have is hoping an attacker is so horrified by your framework that they don't want to spend 5 minutes of their lives trying to read your code.
I and others have had this conversation with him ad nauseum. He'll never get it. I'm also pretty sure that he [leaks password hashes in cookies too](https://github.com/polterguy/phosphorusfive/blob/master/plugins/extras/p5.auth/helpers/AuthenticationHelper.cs#L1070), which he then uses for auth - so you don't even *need* to break the (weak) hash, you can just construct a cookie containing the hash and submit it directly.
Amen to that
Ngen is technically AOT too, although pretty basic. It prejits managed dlls, so you can avoid having to jit anything in an ngened module at startup. Other than not having to jit there is no difference, everything still runs through the clr. CoreRT compiles everything in to a single native executable, like C++. The only runtime components involved is the GC, there is no clr at all. 
Why are you using sha\-256 as a password hash? Security best practice is to [assume eventual compromise of your password database](https://www.owasp.org/index.php/Password_Storage_Cheat_Sheet#Design_password_storage_assuming_eventual_compromise). You should therefore be using a strong, recent, one\-way hash function. This vulnerability is A3 in the [OWASP Top 10 vulnerabilities in 2017](https://www.owasp.org/images/7/72/OWASP_Top_10-2017_%28en%29.pdf.pdf). Any extra security making your password hashes hard to get to is irrelevant. Assume an attacker will have full access to your system and work out what impact that will have. With a weak hash, your system could be leveraged to compromise other systems.
NGEN is basically "JIT it all in advance", and that's it. You save some startup time costs, but ultimately, long-running applications like web servers don't give a crap. CoreRT is more serious about it. You can eliminate unused code for a smaller distribution, do profile-guided optimization, and just generally afford smarter code generation strategies that would be insane in a JIT because of how expensive they would be if you had to be prepared to do them on-the-fly every time.
I've tried so many HTML to PDF components before settling on Prince. It supports more CSS features and better than most browsers. CSS columns? Implements them perfectly. CSS page variables and other advanced CSS stuff to make table of contents and things like that? Like a champ. Fancy CSS footers, headers with content reflowing? Yep. The only place it lacked was in its JS engine. Too basic for some JS charting libraries but I solved that but running my JS with a headless browser before feeding the resulting HTML/SVG to Prince. I wonder why they didn't plug their amazing rendering engine with an existing modern JS engine like V8.
&gt; Why are you using sha-256 as a password hash? Hmm, I am interested in hearing more about this, but couldn't find no SHA256 issues at that link your provided me. And in fact, the _"A3"_ parts said; _"Cross - Site Scripting (XSS)"_, and didn't mention anything about SHA256 ...? However, the point is, that even with physical access to the file, the file is PGP encrypted, with a 4096 bits private PGP key - Which was kind of _"the point"_ with the article. So even if an adversary gains access to the _"password database"_, it's still impenetrable due to that it's encrypted, with a private key, only accessible to the server. Still, tell me more about that SHA256 issue please, and please propose alternatives if you happen to know them, _with_ references preferably ... :)
Hmm, good point, I should have shielded that event from being evaluated from _"application space"_, which I can easily do by simply prepending the config with a _"."_ ... &gt; You may as well put the user passwords right in the web.config Actually, that's where the password **is**. However, an adversary will still need access to the private PGP key, which should not be accessible from _"application level"_ ... The password is useless by itself, since it's only used to decrypt the PGP key, from its internal AES256 stored format ... ;)
Hint, you're looking at the _wrong thing_. No offense Sir, but that's a fact ...
To prove you're looking at the _wrong thing_, I will even **give you my password file and my password** Here is my password file gnupg-keypair:1EF027678DEB0FCF06266C448C4A8A3647CC8B3B Content-Type: multipart/encrypted; boundary="=-LLyo/DkZazvC4JmU6M3Qag=="; protocol="application/pgp-encrypted" --=-LLyo/DkZazvC4JmU6M3Qag== Content-Type: application/pgp-encrypted Content-Disposition: attachment Content-Transfer-Encoding: 7bit Version: 1 --=-LLyo/DkZazvC4JmU6M3Qag== Content-Type: application/octet-stream Content-Disposition: attachment -----BEGIN PGP MESSAGE----- Version: hQIMA4xKijZHzIs7AQ//U5kniwVL6jOR1onW5Itr3CZ5BvL3LykxcjMYyVC2Qdjj RZ4EWZ3W++i4CgTLynwxWz47kq20Mn9RmKbVkXqWsj51bHGAbIi/U6np/STlrWaJ /9dnDSR9UeaLmpP0h0uVmWrzCx2NmljGSY8ClrCWKN1iNhSlIDGgVNYf2mpps20l C4bqO+HYiYWWdr9uesPlk+MKJrIgKoGZq9OY3kYp2pS1+w4t2M2/rwP8sx8cWXOk TV2Bp+NQIIccGEwzRpDiTKtZANI+uQX0c7IzQEYAXm1EfKH636NlCQtGNWYR3Dex R4flBFUvzSp4eJa3/B+Y3eqs0R55wMubqSXsPzVCp5kMFTv1xSnJbsRsUoPBeNS6 1T92mlHB1Uviy5FOktNJrAuMtw5E8Z7ZD4/0sXIeplo9fKt6XoO3ou7w7CKckxoH lUIc/OBx8N0dAljYICpR4IObNPb3T3XRJ3m/+Onsf0NBJIrznLUUGtCFE/GFrMRL 4DVU9P0cAWyVAwHzZUgoejfw8g04jmIld0dLttPeq0oBZxiG5T04hBlarivl2Nb9 M31fdg8fmD2C9hDI/W68BT07SaoVkLDoUTA7c56mOtHnjLhYGqULIKpL3nCaWUg8 efj7n6P5b8eavGY+2DgGenrdblWgypk9H61iSuTcaNMk3fGkbAlKcwPXEyus2gvS tQGtb/sNVJS7PqhY3Lg28PQN2AlSw3DfUsdy4WfyRNDUgIPMHA0C22eRTsNVqyVN tzO7F/n8FkaYDH2hZstocgcMHnAxUcOzanO/DQF4m49UtfMHcb6Vhhlh2X2zDUok L8CDiVWC7XNjhjyapiJGA/I3NdMjQWwrhbdvFFH3qiuPGybMnUIjAX+kcPCGY024 ieVj67Lntuh+ILnIDqdrJPXRfAJ9xhUpBNR2/sv5AxDpFWiNm9A= =fHHa -----END PGP MESSAGE----- --=-LLyo/DkZazvC4JmU6M3Qag==-- **And here is its password**; https://github.com/polterguy/phosphorusfive/blob/master/core/p5.webapp/web.config#L122 Feel free to crack it ... :D
I understand the difference between the user's password and the GPG password. It really doesn't matter, you can recursively encrypt the user's password a million times, at the end still stands a plain text key that anyone with access to your server can use just as well as your application can. This is a bad idea, period. Get over your compulsive need to make up your own stupid and insecure solutions just to feel clever, and make use perfectly safe and convenient industry standards for password hashing. Worrying about GC pauses after 5 logins because of password hashing (?!) when your entire framework is based on reflection calls and allocating trees for method arguments is laughable.
&gt; at the end still stands a plain text key that anyone with access to your server can use just as well as your application can Well, I have given you that key, and I have given you my physical password file. Feel free to prove that I am a retard - If you can ... ;) &gt; plain text key that anyone with access to your server can use just as well as your application can Errh ... **NO!!** - https://github.com/polterguy/phosphorusfive/blob/master/plugins/extras/p5.auth/helpers/AuthenticationHelper.cs#L117 &gt; Get over your compulsive need to make up your own stupid and insecure solutions just to feel clever, and make use perfectly safe and convenient industry standards for password hashing Are you implying to _"slow hashing"_, that slow hashing function that takes 1 second in C#, and any teenager with $10,000 and some ASM/C knowledge can create an exe, hires a million servers for some few hours, and break through as if it was ice cream in Sahara ...? That _"slow hashing"_ function of yours, is worth bat-shit when you just apply enough resources to the problem. An intelligence organization such as the CIA, NSA, or the FSB (**pun!!**) wouldn't even notice a **BLIP** in their server farms to crack through your _"slow hashing function"_, and they'd know your password in a couple of milliseconds ... While a 4096 PGP encrypted file is officially stated by among other things both the Director of the NSA, Edward Snowden, in addition to WikiLeaks that it's **practically impenetrable**! But you can keep your _"slow hashing"_, while I keep my PGP crypto, and we can go back home. I'll put my money on my PGP encryption algorithm before your _"best practices"_ any day of the week!
Psst, the only reference I could find in regards to SHA256 was the following ...? https://stackoverflow.com/questions/2549988/whats-the-recommended-hashing-algorithm-to-use-for-stored-passwords
Your application has access to the keypair, so does anyone who compromises your server.
.Net Core has its own version of it, called [crossgen](https://github.com/dotnet/coreclr/blob/311322b/Documentation/building/crossgen.md).
Page 9, is " A3:2017 \- Sensitive Data Exposure ………………….. 9 ". It states: \&gt; • Store passwords using strong adaptive and salted hashing functions with a work factor (delay factor), such as Argon2, scrypt, bcrypt, or PBKDF2. It doesn't mention SHA\-256 because a single pass of SHA\-256 by itself is not intended for password hashing. In short, it's far too fast to compute. This makes it vulnerable to brute force attacks even if it's not a "broken" hash function. \&gt; So even if an adversary gains access to the *"password database"*, it's still impenetrable due to that it's encrypted, with a private key, only accessible to the server. Sure, but there are ways around that. If I had full admin on your server, could I modify your code to recover the unencrypted password hashes? The point is, it should be safe for your password hashes to be \*\*public\*\*, and you should design your system assuming they \*\*will\*\* one day be public. \&gt; please propose alternatives if you happen to know them, *with* references preferably ... :) Ok, starting with guidelines on password hashing: [**NIST section 5.1.1.2**](https://pages.nist.gov/800-63-3/sp800-63b.html#sec5)**:** \&gt; Verifiers SHALL store memorized secrets in a form that is resistant to offline attack. Memorized secrets SHALL be salted and hashed using a suitable one\-way key derivation function. Key derivation functions take a password, a salt, and a cost factor as inputs then generate a password hash. Their purpose is to make each password guessing trial by an attacker who has obtained a password hash file expensive and therefore the cost of a guessing attack high or prohibitive. \&gt; For PBKDF2, the cost factor is an iteration count: the more times the PBKDF2 function is iterated, the longer it takes to compute the password hash. Therefore, the iteration count SHOULD be as large as verification server performance will allow, **typically at least 10,000 iterations** i.e., 1 pass is nowhere near enough! The recommended "newest" hashing algorithm is [Argon2](https://en.wikipedia.org/wiki/Argon2), winner of the [2015 password hashing competition](https://password-hashing.net/), as recommended by: [OWASP](https://www.owasp.org/index.php/Password_Storage_Cheat_Sheet#Leverage_an_adaptive_one-way_function). Even then, iterations matter: \&gt; While there is a minimum number of iterations recommended to ensure data safety, this value changes every year as technology improves. An example of the iteration count chosen by a well known company is the 10,000 iterations Apple uses for its iTunes passwords (using PBKDF2)[\[2\]](http://images.apple.com/ipad/business/docs/iOS_Security_May12.pdf)(PDF file). **However, it is critical to understand that a single work factor does not fit all designs**. Experimentation is important 
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](http://images.apple.com/ipad/business/docs/iOS_Security_May12.pdf) - Previous text "[2]" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
&gt; In short, it's far too fast to compute I have already answered that question here -https://gaiasoul.com/2018/06/08/here-is-my-weakly-hashed-password-file-feel-free-to-try-to-crack-it/ &gt; If I had full admin on your server ... If you had _"full admin"_ on my server, why would you need to crack it? If you had _"full admin"_ on my server, I would assume you had already cracked it ...? &gt; could I modify your code to recover the unencrypted password hashes Well, inside of my encrypted password file, the passwords are still stored as hashed salted values. However, if you somehow manage to decrypt the file, you would have an easier job creating a Rainbow/Dictionary database to reverse engineer its password arguably ... However, this first of all implies that you're able to decrypt it in the first place. Besides, I don't believe in _"slow hashing"_ anymore than I believe in NIST's official _"best practices"_ for Elliptic Curve RNGs. Feel free to Google it if you wish ... I took the opportunity to write something about _"slow hashing"_ in the above article. However, the problem condensed, is that what is _"slow hashing"_ for your server, wouldn't even be a **BLIP** on Russian or Chinese intelligence organization's servers ... ... or for that matter a teenager with a bot net, and some Assembly/C knowledge ... &gt; Ok, starting with guidelines on password hashing (NIST reference) I don't intend to smear anyone here, least of all NIST, but they don't actually have very good _"track records"_ when it comes to suggesting _"best practices"_ do they ...? ... hint; Google _"NIST Elliptic Curve"_ ... &gt; i.e., 1 pass is nowhere near enough! And my argument is that a million passes is neither nowhere near _"enough"_ ...
&gt; Your application has access to the key pair OK, so how should I fix that? Implement encryption which even my server isn't able to decrypt ...? Yet again, I want to emphasize that from the _application layer_, accessing the private PGP key (which is needed to decrypt the password file) is in general terms not possible, unless you seriously mess things up, or installs some _"random DLL"_ you've found on the internet into your app ... ... at which point somebody should probably chase you out of the building with a stick anyways ... ;)
&gt;OK, so how should I fix that? By using proper password hashing, as has been explained to you countless times. This is why you don't use encryption to protect passwords, it's logically impossible to do so safely. If you assume that your server is impossible to compromise and there will never be a bug in your software or other software running on the same server, why protect the passwords at all?
ITT everyone poos on OP. 
&gt;If you had "full admin" on my server, why would you need to crack it? If you had "full admin" on my server, I would assume you had already cracked it ...? Because I want the passwords so I can compromise other systems. Password storage security is not about preventing access to your system, but preventing people recovering passwords from your hashes. &gt;Secondly, what is “slow” for your server, is easily within the reach of a teenager with $10,000 to rent a server farm for some few hours, and some small amounts of C/Assembly knowledge Argon2 and some of the other recommended algorithms are written in low level languages like C, with wrappers in other languages, exactly because of that potential slowdown due to higher level languages. It is not possible to create a perfectly secure system. Security is all about proportionate defense. You will never be secure against government entities, because they can just seize your servers. What you need to be secure against is the majority of attackers. $10,000 of server time will not be enough to brute force one of the four algorithms listed by OWASP with their recommended. $10,000, or even $1000, of server time **will** be enough to brute a password hashed using a single pass of SHA-256. This site: https://automationrhapsody.com/md5-sha-1-sha-256-sha-512-speed-performance/ shows that a an old i7 can do 1,000,000 SHA-256 hashes per second. There are around about a million english words, so it would take 1 second to recover a dictionary password from your current hashes. **The real question is what is the bar you want to set? How much should it cost an attacker to break one password?** Currently, it costs around $1.
I cannot wait to give this a spin.
&gt; but preventing people recovering passwords from your hashes And this I do, within reasonable amounts of what I have capacity to do, by making sure I salt my passwords, and then hash them, before I store them into the password file. When the file is saved though, I provide an _additional layer of protection_, by encrypting the file, with RSA/PGP encryption. However, _"slow hashing"_ is a red herring, and alone is the equivalent of securing the entrance to your house with a cheese doodles. Let me ask you a question. Below is a photo of my server ... https://phosphorusfive.files.wordpress.com/2018/06/my-server.jpg You can in fact access it here - https://home.gaiasoul.com/ It's running on a 5Mb connection, literally _out of my home_, so please be nice to it ... How do you expect the above _"server"_ to implement _"slow hashing"_, in C# may I add, that is slow when the same algorithm is being executed in Assembly/C on the following _"server"_ ...? https://phosphorusfive.files.wordpress.com/2018/06/maxresdefault.jpg Facts are, any amount of _"slow hashing"_ for your passwords are lubricated into oblivion when faced with a determined adversary, and hence provides you with _"false security"_, resulting in that you believe you are secure, when in fact your passwords are effectively **widely open** for anyone with a _real_ server at their fingertips, or for that matter a bot net ... Understanding this requires nothing more than basic 5th grade math knowledge, multiplication, addition, subtraction, etc ... &gt; Argon2 and some of the other recommended algorithms are written in low level languages like C OK, still my _"server"_ has 4MB of RAM and 3Ghz CPU. Faced with an adversary with a million botnet machines, any _"slow hashing"_ is literally turned into **DUST** before you can start counting seconds ... One of the design criteria for Phosphorus Five, was that I wanted it to be able to run on _literally_ _"ancient machines"_. To still being able to provide security on stuff such as (the above photo) a $1,000 old discarded Windows laptop (which is actually what my _"server"_ is), I needed to _"rethink"_ some of our _"best practices"_. Slow hashing simply went out the window in that process ... &gt; You will never be secure against government entities, because they can just seize your servers Yup, but even with physical access to my box (from the image above), they'd still have to crack the Linux file system encryption. In addition, you could also physically destroy your _"server"_ with a rock, if you see them coming. It's difficult to reproduce the contents of a hard drive, when it's literally scattered around your apartment in a thousands pieces ... ;) And anyway if you were right, and it was impossible to, quote; _"defend against governments"_, we still have a **holy duty to try!!** Simply since if you can't defend against a government, you can't defend against Russian Intelligence, Chinese Intelligence, or the mafia with access to 250,000 botnet machines for that matter ... &gt; What you need to be secure against is the majority of attackers I agree with you here. Though I'd like to add the following; _"Do your best"_ ... ...sometimes this allows us to **also** guard against government entities ... ;) &gt; shows that a an old i7 can do 1,000,000 SHA-256 hashes per second Sometimes the best protection is actually literally thousands of years old. Do you want to know my protection against that problem ...? I encourage my customers to create passwords made out of sentences. The entropy for an 8 characters long password is (roughly) 70-90 something to the power of 8, right ...? The entropy of a password consisting of 8 words, simply assuming the English language, becomes 150,000 words (content of Oxford Dictionary) to the power of 8. This is before you start exchanging random characters such as _"S"_ with _"$"_, or for that matter starts adding _"dialectical"_ words to your password, such as _"dude"_ of _"fuzzball"_ etc. When you start adding some random words from alternative languages, the entropy literally **EXPLODES**! For instance, I am a native Norwegian. A password I might have chosen for my apps could be something such as the following; _"DetteErEttJævlaBraPa$$wordForMittBruk!!"_ How do you even start out trying to brute force something such as the above ...? Then since I can easily choose to allow my client to simply remember my password, by checking off the _"Remember me"_ checkbox, I'll never have to type it more than once for each of my clients. In addition, I wouldn't have to write down the password, simply because for me the above password would be ridiculously simple to remember ... Now try to _"brute force"_ it, after it has been salted, and hashed ... Here it is again for your reference; _"DetteErEttJævlaBraPa$$wordForMittBruk!!"_
&gt; why encrypt the passwords at all For instance since I (would assume) an admin would possibly enjoy creating backups of his _"password"_ file ... &gt; it's logically impossible to do so safely Yet again, here is my password file. Try to crack it. Good luck :) https://gaiasoul.com/2018/06/08/here-is-my-weakly-hashed-password-file-feel-free-to-try-to-crack-it/ Now feel free to share your _"strongly hashed"_ password, **in public**, on the world wide web, and let's see who's password file is cracked first ... ;)
&gt;OK, still my "server" has 4MB of RAM and 3Ghz CPU. Faced with an adversary with a million botnet machines, any "slow hashing" is literally turned into DUST before you can start counting seconds ... Ok, but still, would you rather **everybody** can brute force it, or just people with millions of machines? I'd rather make it so that only *huge* supercomputers can. If you swap to using argon2 (via https://github.com/adamcaudill/libsodium-net for example), you can choose a threshold that is low enough that it's still fast on your server. Even better, you could let server configuration decide how long it takes so people with **even slower** machines can still make login take a fixed time. Once you do that, your server will be just as fast (users aren't going to notice 1ms on login), but everyone who doesn't have millions of dollars to spend won't be able to brute force you anymore. Isn't that better?
It's nice to see a group of devs at MS having fun. From the Blazor website: &gt; Where did the name "Blazor" come from? &gt;Blazor makes heavy use of Razor, a markup syntax for HTML and C#. Browser + Razor = Blazor! When pronounced, it's also the name of a swanky jacket worn by hipsters that have excellent taste in fashion, style, and programming languages. 
Cliff notes 
Did you somehow forget what we talked about 2 comments ago? Jeez. Anyway, you don't get it or don't want to accept it, and that's fine. Do whatever you want. But maybe just stop posting about your stuff on reddit. No one wants to use your code and ideas because they are terrible, and you refuse to learn anything from the discussions, so there's really no point.
Yup, I mostly use Assert.True
strings aren't actually evil. naive file parsing can allocate a lot of memory.
StringBuilder, ftw.
Which he eventually got rid off because it was slowing down his program. 
Awesome read. Had to chuckle how he eventually got rid of the *integer.Parse* or *decimal.Parse*. 
Good series Jon really enjoying it \&gt;Fetch is available “out of the box” with modern browsers Could somebody let Gretchen Weiners know that Fetch finally caught on! Up yours Regina George!!
They're not all that useful for parsing. 
I recently started working with Angular for my SPA. Wish I had explored React a bit more, or even understood the main differences between Angular and React. But with my prior web development experience/knowledge (which is close to none), and using Angular, it feels like you're out on a stormy sea on an aircraft carrier instead of on a fishing dinghy. 
Javascipt is like that if you're coming from .net. I have found that react/redux with typescript is nice to work with. I'm not a fan of angular. Functional seems to make more sense in a browser.
Is react more functional than Angular? I guess I've never really understood what functional programming exactly. I think I just clinged onto Angular with TypeScript and saw that it was far easier to work with than JS. (I'm using Angular5, not AngularJS)
but at times it's not static, you will need simultaneous releases of all products that use the same code. that has bitten me in the butt before. even if the code itself is shared, the implementation has to be synched to what the backend expects.
I really like Blazor. I hope it is able to leave the experimental phase and become officially supported. I'm having a lot of fun playing around with it and prefer it over other client side frameworks. Please keep going Blazor team.
You are welcome. It was like 8 days of Dev hell. I was gonna advertise my skills now that I leveled up - but is there a huge market demand for this particular thing? Then I was gonna blog about it... Decided to give back to the community and subreddit that's helped me so much over the years. :) 
Probably. I'd like to know what you find out. But why would you want to do that vs just a rest API?
The fact that React is less opinionated than Angular makes it better IMO. Angular forces Typescript down your throat, which a lot of devs may not want. Does react have some opinions? Of course, but a lot of them are up to the developer to decide. Typescript and React has picked up as of late, but there's still a lot of developers/teams writing great apps with just js/jsx and no typings at all or just using proptypes. Angular is also a much larger framework than React. So it feels like an Aircraft carrier that might be in a pond lol.
is there reason why we can't have everything? regular jit, ngen, and corert? regular jit for truely xplat binaries, ngen to avoid jit costs (especially for startup) but for easy reflection as well, and corert when you just want bare metal perf and you're willing to deal with some hassle. i see value in all three, and just having the startup perf of ngen would make me very happy.
Of course all of that would have been pretty much trivial if the Framework had span support.
do you guys recommend to learn react ? i want to learn some front\-end framework and im between Vuejs and react 
&gt; just people with millions of machines I would want **nobody** to be able to brute force it! https://gaiasoul.com/2018/06/08/nist-bcrypt-slow-hashing-and-elliptic-curve/ &gt; I'd rather make it so that only huge supercomputers can You mean the kind of supercomputer that are owned by North Korea or Iran ...? Or maybe you were intending for botnets with millions of computers in them ...? &gt; If you swap to using argon2 If it's recommended by NIST, it would be the last thing on the planet I'd choose. If you don't understand why, I suggest you read my above article ... &gt; Isn't that better? The only thing I would consider adequate, would be that even those with millions of dollar, or billions too for that matter, are 100% completely left in the dark. If you don't agree with me here, you are publicly admitting that you have no troubles sending your users' passwords to North Korea or Iran for that matter ...
Oh goodness, thanks for the write up! After the little we've done with the grids so far I think we'll be staying far away from Telerik in the future.
Using StringBuilder to append single characters seemed a bit odd. It seems like the length was known and could have been used to specify the size of the builder on creation. Given a known size, an array is probably the better choice. 
\&gt; I would want **nobody** to be able to brute force it! Well, your current setup allows brute forcing. A single vulnerability, in your website, in your server, in your database, in your OS drivers, in the CLR, means an attacker can decrypt your passwords. You can't prevent that, because you're using other services (e.g. C# itself could have an accidental backdoor). \&gt; If it's recommended by NIST, it would be **the last thing on the planet I'd choose** They also recommend PGP... [This is their certification of it](https://csrc.nist.gov/csrc/media/projects/cryptographic-module-validation-program/documents/security-policies/140sp1101.pdf). They clearly recommend gnupg as well, looking at their submission process... [https://www.nist.gov/itl/iad/image\-group/products\-and\-services/encrypting\-softwaredata\-transmission\-nist](https://www.nist.gov/itl/iad/image-group/products-and-services/encrypting-softwaredata-transmission-nist) \&gt; If you don't agree with me here, you are publicly admitting that you have no troubles sending your users' passwords to North Korea or Iran ... If North Korea kidnap you, they could make you give up the encryption key. If they kidnap me, I cannot compromise my user's passwords.
Your mileage may vary. If you're developing something where speed of development matters a ton and you don't care about doing anything the controls weren't specifically designed to do (maybe a corporate intranet site), it may be fine. Personally I'd rather just use standard ASP.NET controls and customize them as needed or create my own controls. What you gain in productivity using Telerik controls because they automate some things for you, you lose in productivity when troubleshooting anything that might go wrong. Their support and documentation are not good.
Really? I had no trouble loading a 40MB file into memory using StringBuilder and then parse it... Maybe I'm misunderstanding you, though. Care to elaborate?
&gt; Using StringBuilder to append single characters seemed a bit odd. Yeah, that was weird. But "strings aren't evil". That's just click-bait.
Probably doesn't matter what you choose, it'll all be outdated and useless within a year.
Web development moves frustratingly fast. We still use MVC5 and jQuery. I'm basically unemployable :)
Parsing usually involves handling a lot of strings that are only used briefly. There's no way of saying "pass characters 5 thru 10 to interger.Tryparse", you have to allocate a new string. That's why the new `Span&lt;T&gt;` type is so interesting. 
And only because strings are not really that precise, being UTF-16 and all that. One codepoint isn’t necessarily a Unicode scalar, much less a grapheme. 
The limit for tabs in a tab control is pretty large, so what's the exception that you're getting?
Exception thrown: 'System.ComponentModel.Win32Exception' in System.Windows.Forms.dll is there some sort of limit for desktop application that it can't have that many forms open at once? I can't really dispose the memory because those screens are still open
React is the epitome of the JavaScript ecosystem. It's not at all opinionated, and lets you do virtually anything you want. React its self is just a view library, but the React community has created a somewhat standardized stack of multiple libraries that together can act as a somewhat cohesive SPA framework and even developed some nice tooling around creating this stack from a command\-line interface. React *can* be like a mature framework, but it isn't necessarily a framework at all. You need the ability to see through the noise and understand concepts at the abstract level for when pieces of the stack inevitably get swapped out to thrive in the React world. Angular is the polar opposite. Angular is extremely opinionated and extremely all\-encompassing. Angular is configuration heavy and light on convention. An Angular app will be very verbosely written and deliberately crafted in accordance with Angular's guidelines... otherwise you're gonna have a bad time. VueJS sits pretty much right in the middle and focuses on simplicity. Rather than everything being an explicit configuration like Angular, VueJS offers a lot of conventions. VueJS is a more full\-featured SPA framework out of the box than React, but still doesn't include absolutely everything and allows you to include optional pieces as needed. Unlike React though, those optional pieces are first\-class citizens of the VueJS team and not entirely separate libraries from other development teams. You can't go wrong with learning any one of those 3. Learning React is probably the most challenging if you're not already familiar with the concepts of a SPA framework, because it will be harder to separate signal from noise when creating your React stack. If you are familiar with SPA framework concepts, Angular can be both a bit of a bear and also liberating with it's strong opinions. It has great documentation, but the workflow will seem heavy for learning purposes. That's intentional. VueJS is a very friendly framework to learn. So if you're a true beginner, that would be my choice. What you learn in VueJS, as long as you have the capacity to think about it in an abstract fashion, will translate very well to learning the others. Good luck!
What's the NativeErrorCode of the Win32Exception? 
At my workplace, we had a series of discussion on Angular vs React for our next FE stack. Started with defining our criteria: - Must have established, large community (more help/libraries/tools online, higher chance of longevity/kept up-to-date, larger talent pool) - Must use component-based architecture (better than MVC in promoting single responsibility, loose coupling and composability) - Ease of migration from the old stack (ours was AngularJS) - Minimize vendor lock-in (easier migration to next, future stack) - Performance - Great developer experience Our findings: - Both have large community, but React is much larger (based on github stars, # of github projects, npm # of download trend) - Ease of migration: Angular is terrible, even when starting from AngularJS! In theory, it offers ngUpgrade for migration path, but in reality there's just too much risk to system performance and development speed. React is much more viable for hybrid approach. - Vendor lock-in: Angular is a very opinionated framework, has higher due to large API surface (&gt; 100 the last time we counted), custom HTML directives, complex abstractions. In contrast, React has much smaller API surface, HTML large free of DSL, and even though JSX is vendor-specific, logic is mostly written within idiomatic javascript. - Performance: both are pretty comparable, we're confident any pick will greatly improve what we have with the old stack. There's concern with Angular because it could only do Webpack code-splitting on the Route level, where React is much more flexible. - Developer experience: Angular CLI is a great build tool, strangely less opinionated than React's CRA. Obviously better Typescript support, which we agreed it's very nice to have. But... * for a component-based framework, it puts up a lot of ceremony/overhead just to create a component, to the point it could become a deterrent for developers to do good UI decomposition. * while some like JS and HTML separated, in practice we found JSX gives better debugging experience. * learning curve is higher due to large number of abstractions to learn. * while its walled garden approach has great benefits, doesn't seem conducive to innovation. We felt at some point, we'd be banging our heads against its wall fighting the framework when want to do something out-of-the-box. Outside of work, as a React-based open source project maintainer, I know there are cool things with React that is hard to do in Angular or even it simply cannot do. Dynamic loading of components is one. Angular requires many steps just to achieve this, compared to simplicity of React that only employs ES6 import. And consider this React code: @import {ComponentA, ComponentB} from './myComponents' const SomeComponent = props =&gt; { let ChildComponent = props.childComponent || ComponentB; return ( &lt;ComponentA&gt; &lt;ChildComponent /&gt; &lt;/ComponentA&gt; ); } The above component allows you to replace the child component of 'ComponentA' with any component on the fly, and defaults to 'ComponentB' if not provided. This sort of dynamicity, expressiveness and conciseness - and there are many more like this - is not something that a template-based framework like Angular can easily achieve, if at all. Yet, this is the very stuff that can propel developers to think out of the box and be more innovative. 
You seem to have not read the article.
&gt; loading a 40MB file into memory using StringBuilder /and then/ parse it [without using the StringBuilder] A StringBuilder is for making strings. In parsing you want to make numbers, not strings. 
And if you ever find yourself on the side of needing to concatenate lots of strings very efficiently, I wrote some (unsafe) code to do this a while ago: [https://gist.github.com/antiduh/426a4d22ab2449601842](https://gist.github.com/antiduh/426a4d22ab2449601842) See also: [http://stackoverflow.com/questions/32217255/](http://stackoverflow.com/questions/32217255/)
&gt; You can't prevent that, because you're using other services (e.g. C# itself could have an accidental backdoor) You are right, and of course I cannot guarantee that for instance Mono, Linux or Apache contains security holes that somehow allows an adversary to gain access to my server. I have (of course) taken steps to eliminate that risk as much as I possibly can, by for instance in my _"install.sh"_ script install _"ufw"_ ==&gt; _"Uncomplicated Fire Wall"_ to ensure no ports are open, by default creating an SSL keypair and making sure any unencrypted access is re-routed to port 443 (SSL), etc, etc, etc ... However, all in all, my qualified guess, is that Phosphorus Five's security measurements will dwarf the average _"enterprise web app's"_ security measures by at least one order of magnitude. Which can arguably be seen in this debate, and how some people are fiercely protecting _"slow hash"_ and _"bcrypt"_. It doesn't take a rocket scientist in SW architecture to understand why, and the reasons are of course because most devs have implemented their passwords as simply a column in a MySQL/MSSQL table somewhere. Now compare the needs to first of all access a privately generated and stored private PGP key, stored outside of the filesystem available from _"application level"_, then being able to access **two** files, both of which also are protected from any non-root accounts in the system, before one is able to retrieve its passwords. At which point you'd still have to create a Rainbow/Dictionary file, with the server-salt to reverse-engineer the passwords. At which point your task would still be monumental since I encourage my users to use _"DetteErEttJævlaBraPa$$wordForMineBehov!!" (Yes, I remembered it without looking it up). The compare the above security considerations with being able to inject a single malicious SQL statement, **anywhere** in some application, for then to retrieve **ALL** passwords, spend a million dollars (_your_ words) to rent a server-farm, and do a Rainbow/Dictionary brute force attack ... I think the odds here are **clearly** in my favor ...
Sorry how do i get the Native Error Code. This is what I see on the output after it crashes: Exception thrown: 'System.ComponentModel.Win32Exception' in System.Windows.Forms.dll The thread 0x1e50 has exited with code 0 (0x0). The thread 0x20e0 has exited with code 0 (0x0). Exception thrown: 'System.ComponentModel.Win32Exception' in System.Windows.Forms.dll 'SV2.exe' (Win32): Loaded 'C:\Windows\SysWOW64\shell32.dll'. Loading disabled by Include/Exclude setting. 'SV2.exe' (Win32): Unloaded 'C:\Windows\SysWOW64\shell32.dll'
Maybe if it was rendering it in almost-real time? Like a computer generated animation that couldn't keep up so it sends notifications as frames are ready... But as I type this, even that sounds stupid. 
The NativeErrorCode is a property of the Win32Exception. It should point back to which Win32 exception code got thrown, which you can use to determine why it is happening. 
Yeah a notification per frame would be super slow. I would say file stream result is the way to go with a standard mvc controller
Then add the fact of that a web app also have _additional_ sensitive data, which I also happen to store in this encrypted file, as you can see in the following code ... https://github.com/polterguy/phosphorusfive/blob/master/plugins/extras/p5.auth/Settings.cs#L42 And ... https://github.com/polterguy/phosphorusfive/blob/master/plugins/extras/p5.auth/Settings.cs#L53 (Same file) Examples includes for instance if your server needs to access a POP3 server or an SMTP server, with a username/password combination - Which is an actual use case for one of the _"satellite"_ apps I created as I developed Hyperlambda an Phosphorus Five, as you can see below ... https://github.com/polterguy/sephia-five/blob/master/startup/internals/events/sephia._internals.create-pop3-user-thread.hl#L30 And ... https://github.com/polterguy/sephia-five/blob/master/startup/internals/events/sephia._internals.create-pop3-user-thread.hl#L320 Where I need a safe storage for the user's POP3 account's password. (For the record, Sephia Five is not a supported project at the moment, but a pretty interesting use case, since it's a _"GMail clone"_ with using 1/100th the amount of bandwidth as GMail does, in addition to providing PGP cryptography services on emails, etc ...) Now the above use case is just one. I could probably twist my brain a little bit, and come up with hundreds of additional scenarios where I would want to securely store _"user information"_ - Which my encrypted _"auth.hl"_ file just so happens to provide for me ...
Yeah, you're right. Click bait titles like that aren't worth reading. 
&gt; NativeErrorCode Its saying 'Error creating window handle.' I'm looking at the process in task manager and it seems to crash when the 'USER Objects' is at 10, 000. Maybe the application has reached its limit
CoreRT isn't done yet, but it's not intended to replace coreclr. They are separate products, so you can have everything. You can already do ngen on desktop .net and crossgen on coreclr, or you can ship your binaries as IL only if you're not worried about perf. As an aside, ngen and crossgen aren't necessary in a lot of cases. When it is necessary it's super helpful, but any small projects the amount of framework code will dwarf the user code - so it doesn't really matter if your app with a couple thousand lines of code is prejitted or not, since most of the jilting will be in either asp.net or the .net framework. Also, startup time doesn't typically matter as much for web servers, where most people start it once and let it run for days. There definitely are cases where ngen/crossgen is very valuable, they just tend to be very large products with lots of code where startup time matters, for example visual studio.
On my Windows 10 machine, the USERProcessHandleQuota is set at 10K, so that would explain what is going on. That's a whole lot of handles though. It is worth noting that each control and window utilizes a handle. So for example generating edit boxes in a loop (instead of using a grid) can consume many more handles than are needed. Or there's an infinite loop generating windows/controls. 
Ah thats what it is. Thanks for the help! I'll have to try to eliminate some of the labels or whatever I can from that form that is has many objects. Maybe that will help a bit 
You can learn React as well but vue.js is marvelous. Vuex is such a pleasure to use.
I'm on the opposite side, I love Typescript and I think it might make more sense to someone coming from C#/.NET (I didn't come from that side, but it is where I have ended up).
Strings in C# are immutable so every string operation is an allocation. The higher level .Net string APIs do a lot of allocations. `ArrayPool&lt;T&gt;.Shared` is a thing and can save you a ton of allocations.
Just as a pro-tip, when a window is hidden or minimized the handles and control's handles (since they're child-windows) remain consumed. Only way to free them (aside from using less of course) is to actually dispose of the parent (e.g. window). 
I started out as a C# dev, and I don't really see the need for types with javascript (at least right now). I understand TS, and I've added it to my skillset for future potential employment, but if it's up to me I bypass it when setting up a project.
The following are my personal opinions. Vue is easier and has less fuckery. It also works better with .net mvc because you can simply drop the cdn into your view and get going (similar to angular 1). Is it better than React? No. Is React better than Vue? Maybe? Does it matter? No. You'll probably gain more front end knowledge learning React. However, you'll also most likely get led down a rabbit hole you wished you wouldn't have gotten into at about the 45 hour mark of messing around with React. You'll learn things that are great and then the next release, those things will be wiped out with a breaking change. You'll be stucking using dependencies of node and webpack, which are things you probably don't care to dive into because you have a background in dotnet - which does kind of defeat the purpose. My advice - save yourself a headache and learn vue.
I'm actually converting a similar site. not classic asp but webforms. Because is a portal for reports, I chose Razor Pages in core 2. 
Razor Pages is the preferred method to create new applications according to Microsoft. I guess Razor pages is the modern standards? 
Um, no. That doesn't sound right.
the big scenario i am imagining for startup times is for serverless webservers (as backwards as that sounds). potentially short lived processes where response time is the priority. the tiered jit in 2.1 will help to address this to some degree.
I just want to write "video calls" like skype or something like that.
Use the output window to confirm 100% that it's publishing where you think it should. I know that seems ridiculous because I'm sure you double checked, but i can't count how many times the output window has enlightened me to some path i was not expecting.
It was a very informative article. You should read it. He shows how a fairly direct approach to ingesting text data performed 7 GBs of allocations mostly because of string allocations; and then a smarter approach performed a total of 32 kb of allocations. I have to agree with the author a little bit, strings can be evil sometimes because it's so easy to create enormous performance bugs.
Yes, I'm quite sure it's the right path that it's publishing to.
Since I wasn't familiar with it, [here's an overview of the ArrayPool&lt;T&gt;.Shared technique described here.](http://adamsitnik.com/Array-Pool/)
That's VOIP and a whole different topic.
Sorry, I'm about video calls in the browser.
No you are not. After 4 years of WinForms as Junior developer I moved to Javascript and Jquery 5 years ago, most likely with Asp.Net 3.5 stack, just a few ASP.Net WebAPI methods (10%?!). I found a job within 7 days when I decided I will quit my old job just for a few private reasons. Jquery is nearly the holy grail of old existing projects of the last 5 to 10 years and they all need maintaining, because... you know... jquery is shit... $('#ILoveMyString').prop('JustATypoHereAnWeDie', true) Haha. Just be ready to move forward. I learned React 1 month ago, still struggeling with splitting frontend and backend thoughts after 5 years of MVC, Razor and Jquery, but I LOVE it!
I love it! Keep up the great work! 