Http module to detect request type and then serve appropriate pages from there?
Thanks everyone for your advice - I've modified my code to use one constant connection that's opened on the initialisation of the app rather than reopening the connection each time, and performance is much improved. I still have occasional slowdowns but I suspect this is more to do with my code itself. Note I always invoke the hub on a different thread!
 //stringify var jsonData = JSON.stringify(data); //Parse JSON var objData = $.parseJSON(jsonData); var objData = $.parseJSON(jsonData); You do realize that the variable 'data' should already be an object that was deserialized by jQuery (both contentType for the server request and dataType for the response are set to 'json' for a reason, see: http://api.jquery.com/jquery.ajax/)? And that you're parsing the same string twice, after reserializing what has already been deserialized in the first place. This is pervasive throughout all your examples and looks like you just blindly copied and pasted code together, extremely unfit for a tutorial.
I used pluralsight to learn ASP.NET MVC. Most of the courses are for members only ($29/month), but [this one here](http://pluralsight.com/training/Courses/TableOfContents/aspdotnet-mvc3-intro) is free and targeted at beginners. It's using MVC 3, but that doesn't make that much of a difference for a beginner. 
Too funny. This course was where I realized I'm not really understanding controllers! I love love pluralsight. Just wish these tutorials would go a bit deeper and stay focused. It seems that most courses either they don't go deep ala Jesse Liberty 'from scratch' series, or they go all Scott Allen and whizzbang your ass and your head is spinning with all this stuff you could do, if you only knew how. 
Hah, fair enough. So what exactly is it about controllers that you're struggling with? Calling them from your views with the right paramaters? 
Depending on how you're handling user logins, the loginId could be taken from some security token, if not, then make it a part of the parameter you pass to your controller action. In your scenario, I'd create an HttpPost ActionResult that takes in the 3 strings and returns a model of the grid data. [HttpPost] Public ActionResult GetGridData(loginId, beginDate, endDate) { MyModel gridData = new MyModel(); /* Fill the model with data from the stored procedure */ return View(gridData); } Your view should be bound to the MyModel model which will update when the model changes. To call the action, use the HTML helpers to create the form on your view which POSTs your data to the action above.
I think the issue you might be having is your question isn't really about MVC controllers as much as it is about model binding. Model binding is basically the "magic" that happens that converts data sent in a request to objects (your model). [This MSDN article](http://msdn.microsoft.com/en-us/magazine/hh781022.aspx) gives a pretty good overview and the dives much deeper into the model binding functionality of MVC. A few others have given code examples here that while are not technically incorrect, in my opinion are not really 'embracing' MVC. Instead of passing parameters to a controller model, you'll be able to better leverage the features of MVC if you instead get used to the concept of passing models. For the example you've given, I'd recommend a model that looks something along the lines of: public class ReportModel { public DateTime BeginDate {get; set;} public DateTime EndDate {get; set;} public IDictionary&lt;string, int&gt; ReportGrid {get; set;} public int LoginId { get { // logic to retrieve the user's login id. // (varies depending on how you've implemented authentication) } } } Now you can create a strongly-typed view (or in the case, a partial view might be even better) for your model: @model MyWebsite.ReportModel @using(Html.BeginForm("Action", "Controller")) { &lt;div class="row"&gt; &lt;div class="form-label"&gt; @Html.LabelFor(m =&gt; m.BeginDate) &lt;/div&gt; &lt;div class="form-input"&gt; @Html.EditorFor(m =&gt; m.BeginDate) &lt;/div&gt; &lt;/div&gt; &lt;div class="row"&gt; &lt;div class="form-label"&gt; @Html.LabelFor(m =&gt; m.BeginDate) &lt;/div&gt; &lt;div class="form-input"&gt; @Html.EditorFor(m =&gt; m.BeginDate) &lt;/div&gt; &lt;/div&gt; &lt;button type="submit"&gt;Generate Report&lt;/button&gt; } @if(Model.ReportGrid != null) { &lt;!-- Example markup to generate grid --&gt; &lt;table&gt; foreach(var kvp in Model.ReportGrid) { &lt;tr&gt;&lt;td&gt;@kvp.Key&lt;/td&gt;&lt;td&gt;@kvp.Value&lt;/td&gt;&lt;/tr&gt; } &lt;/table&gt; } And finally, your controller: public class ReportController { public ActionResult Generate(ReportModel model) { if(!ModelState.IsValid) { return View(); } // Now use your data access logic // (varies depending on how you're accessing your database) // assume MyDataService.GetGrid is a data access method that // calls the stored procedure and formats it into an IDicitonary&lt;string, int&gt; model.ReportGrid = MyDataService.GetGrid( model.LoginId, model.BeginDate, model.EndDate); return View(model); } } Hopefully that clears some things up for you. If you have questions feel free to reply or PM me. \* edited to fix indentation
The main thing to keep in mind is that ASP.NET MVC is all about convention over configuration which means it searches for controllers and views by name instead of being told what it has. What I believe you are referring to is url routing which is basically how you tell mvc how to locate your controller class and which method (aka action) you would like to call on it. When you submit a request mvc looks at the all of the parameters in the query string and tries to match them as close as it can to one of these routes. First one found is the first one it goes with. The next step is then examining the rest of the query string parameters which happens in a process called model binding where mvc then tries to recreate the parameters to one of the previously discovered controller and action(s). When it finds a match it calls the action.
There are some technologies were you need to buy a book to.understand it.
your thoughts on at what threshold this switch makes sense at? I've worked with very mature and stable codebases with 50+ VS projects and never felt like project references were hindering anything or that dll references offered any particular advantages
The IIS is connected to the .NET. The .NET is connected to the default.aspx. The default.aspx is connected to the route handler. The route handler is connected to the controller factory. The controller factory is connected to the model binder. The model binder is connected to the controller action method. Your particular issue should probably look at how the route handler pushes specific values to the model binder.
Erm... .NET already has a thread safe, in-memory cache built in, the MemoryCache class
It can occur on a project by project basis. Usually infrequently changed, core ones first. I.e. Utility projects, data access provider, that type of thing. You can also set up nugget so that updating these dlls across multiple dependencies is as easy as updating the nugget repo. Still not as easy as a direct project reference, but easier than updating multiple projects which reference it. If you don't feel having all projects in your solution hinders you, don't change. Different methods work better for some people/teams than others.
yes, so is this just another wheel?
what are you guys using dotnetfiddle for? Just testing small pieces of code?
Looks like a scaled down version of [linqpad](http://www.linqpad.net/)
Pluralsight is legit. I've been watching courses for the past couple months and it's helped me a ton. Scott Allen is my boy if your trying to learn c# or asp.net MVC I'd check out his videos. Not to mention they have a huge library full of great content. 
When you're developing stuff on your own and you run into a mean bug, this is a good way to share your code on IRC channels and what not.
it does, but MemoryCache doesn't have a read-through API
I did it 2-3 days a week for about two years, then permanently for over year at my last job. And now for 5 years since I started my own business. If I was to work for someone again, it would definitely be a requirement. As an owner; I don't plan on having a physical location again. I work from an office in my house, and I have one employee that works remotely from his house. As I expand I'll only be adding remote team members. 
Current job is the first I've had as *just* a software engineer and I couldn't imagine not being able to work from home (WFH). We can WFH up to 2 days a week, but I don't think any employee takes advantage of it to the fullest. I wouldn't probably take another job if it didn't offer WFH because of the few instances I've seen where it can really be nice. -Last day or so of being sick where you're not too sick to work, but don't want to sneeze and hack around fellow employees. -Landlord or someone needs to come and fix something at the house. -General unwillingness to be a part of society for a day. So, to answer your question: Even though I rarely telecommute (few times month) it's a job requirement for me.
Like everyone else, I too work from home and wouldn't trade it for anything.
I work from home most of the time. I go into the office for half a day occasionally. I don't think it's a requirement, but if I can swing it, my next job will be remote too. With all of the tools we have available to us now, I don't see much advantage to being in the office. 
It's a requirement for some people for a position at your company, obviously. If it's about "team chemistry", well... I don't see how a move that caused multiple people to resign has *improved* team chemistry, unless you were trying to get rid of those people. I work in a city with terrible commute times. My job does not require me to actually be on site. So, if you want me to spend an hour each way to come in, there needs to be a reason or an incentive. In my current position, I only have to be physically present one day a week, for a weekly meeting. While there have been weeks where I've worked from home 4 or 5 days, most weeks I come in at least 3 days. Those days, I come in early and leave early. Overall, being inflexible with regard to work hours and location is certainly going to prevent some qualified candidates from accepting a position. Given that there are fewer candidates than positions in this industry, it sometimes boggles my mind the requirements that management will try to push. In the end, I'd say that the policy change obviously decreased satisfaction, and caused attrition, while filtering out the most qualified potential candidates. If the objective was to trim your team down and bring in inexperienced new workers, then it's going very well.
I currently freelance from home, but in my last office job working from home wasn't an issue; in my group it was mostly on an as-needed basis (e.g. snow day) with no notice needed, but some folks in other groups had anywhere from 1 day a week to full-time work from home. It was a national company with our groups scattered across about 5 offices and 3 time zones, so it was rare to get everyone physically together (aka "team building" excuses don't really apply to forcing people to get to the office). Flexibility concerning work-from-home as well as general hours (I prefer starting later) are rather important to me. The exception for working from home might be if I lived in walking distance, but even then it'd be nice to have as an option. When I see a place that outright bans telecommuting, I assume it's either something a pointy-haired boss read somewhere or it's a power play by a micro-manager. Neither is someone I'd want to report to.
I've been in IT for a while and for the most part the companies I have worked for don't seem to like it too much. There is always a few exceptions but for the most part they want you to show up each day.
I think a big factor is the nature of the job. My current job stresses responsiveness to a constantly changing requirements. It's a lot easier to hash out complex plans quickly when we're all sitting in a room together. When I'm slogging away on something that I already understand quite well and the requirements are sufficiently stable, there's not much of a penalty for working from home as the bandwidth of collaboration is much lower. It's easy enough to manage that level of collaboration via IM or whatever. I might work from 1 day in 10, and even then, it's usually only if I have a specific reason to be at home. Unfortunately there's been pressure from above to be at the office more and more. The upper management doesn't quite get their relatively new department of modern nerds doing modern work. The attitude is that if we're not at our desks, we're not working. It's a bit frustrating but not a huge problem since most of the time, it's beneficial to be in the office anyway. A company with a strict policy of no working from ever would definitely put me off. Not so much because it's something I demand, but because it gives the impression of a company that doesn't understand what their employees do. It wouldn't eliminate the job from consideration immediately, but I would definitely be on my toes for further warning signs. It sounds like your current employer is throwing up all kinds of red flags in this regard. Despite the down economy, I have only seen a consistent rise in the demand for quality developers in the decade and change that I've been doing this. Any one worth hiring probably has options, so a shitty work environment with shitty pay will not be competitive to those candidates.
We have used it in the following ways: * Remote job interviews - using collaborate feature to see developers write code in real time. It really helps to see their thought process, but makes them a bit nervous. * Testing out NuGet packages. Submitting links to fiddles when finding bugs in NuGet packages * Collaborating with other developers on some peace of problematic code for the client project. Only if easy to isolate it. * Submitting code examples for answers to questions on Stack Overflow and MSDN Forums * Testing out various code snippets found throughout the internets... Most don't work without some changes. * Quickly testing out ideas and proofs of concepts * Continue coding through mobile device when work day is over and wife doesn't allow using Desktop next room * Integrating Widgets in Blog entries to allow people to view code, its output and even customize the code inline and see how output changes 
My opinion might not be a popular one, but I don't like telecommuting. I like having the face-to-face interaction with my co-workers, and having home and work in separate locations helps me keep them separate mentally as well, which is difficult to do given that I don't have a specific area at home that I can set aside JUST for work. The only extra room in my place is where all my video games are, and I know I'll have problems fighting the temptation. Granted, I do appreciate having the option if I need to be at home for something during normal work hours.
No, you can't host it. The point of it is to read your locally installed fonts so you can choose one when doing graphics. A website on the internet couldn't read fonts from a client machine.
My regular stops: [Coding Horror](http://www.codinghorror.com/blog/) [Rands in Repose](http://randsinrepose.com/) [Ayende @ Rahien](http://ayende.com/blog) [Scott Hanselman](http://www.hanselman.com/)
I did it for a while before I was able to physically move, and hated every moment. First the issue was the home is where I do things. It is filled, literally, from basement to attic with distractions. I could work, or I could game. I could work, or I could do dishes. I could work or I could go work in the shop. Every time I needed a short break from staring at code, it turned into a long break. It took close to a month to get over this. That's when the next problem started. I had no idea how to separate work life from home life. Once I figured out how to focus on work, it was suddenly all I did. Every day became a 12+ hour day. It was horrible. In the end, I'm with a lot of these replies, though - I want the ability to stay home and work when I'm not well, when there's inclimate weather, or when the repair guy told me he'd be there sometime between 6AM and 2PM. If a company makes me take PTO for those things, it certainly would be a strike against them.
Your CIO is an idiot. Hopefully he won't be able to meet any of his deadlines now and his boss will see that he is an ass clown.
I've been telecommuting full time for over 3 years or so. In fact I've turned down job offers that didn't guarantee I could work from home. It's certainly not a requirement but from what I can tell it's more and more commonly accepted.
I have a contractor that telecommutes. I think it varies depending on your team make up. With screen sharing, webcams, skype, IM, etc. It's very easy to have a tight knit group even if they're not in the same room. I think it does have some costs as you're essentially an invisible employee to people not on the project, so you're probably less likely to get promoted, and easier to be let go. If you need to interact with a lot of people, it's a bad idea unless they're as easily tech savvy. Also if you're a social animal, this can make your job incredibly lonely.
That's the same thing I did, from May 2009 until now. Even the people that live local to our offices hardly go into them unless there's a reason. I probably make the trip back down (live ~160mi away now) not even once a year at this point. It gets pretty boring and you can go stir crazy. However, full control over temperature, lighting, private bathroom, and agreeable pet and dress policies make it worth it.
I am not 100% sure as I have yet to dig trough the new templates in lots of detail. I do know however that a similar kind of process happens when authenticating with an external provider using OAUTH or similar, you may want to poke at the templates for external providers to see if there is some logic worth stealing from there. Sorry I can't give you a clearer picture.
I've seen telecommunting come and go, it seems to be on the outs lately. IMO I tend to disagree that you cannot have team synergy with a remote group, I contend that management moving away from remote work for reasons of "team unity/whatever" are actually admitting failure in maintaining a community culture without the water-cooler and the ability to do fly-by interruptions. Rather than admit fault we will say the employees cant handle it and wash our hands of the situation. Sure there are exceptions but most software dev jobs can be remote, the trend I have seen seems to be heading away from that. I can live with it, i just ask for more money if you want me to commute into a major hub city on a daily basis.
I've also used it for off site interviews with some success.
What forms auth are you using? The old membership providers, the new OAuth stuff, etc.? 
I've been telecommuting for five or six years now. I've had few problems with it, especially from a communication aspect. Any time that I've needed to be on-site, it's never been an issue to travel over for a week or less stay (and that only happens once or twice a year). Granted, I've worked for some flexible employers. Honestly, though, I think it comes down to the individual, as well as what the team is willing to work with. When you have people that are progressive and willing to work with someone that's remote, even when the rest of the team is not, it makes it a lot easier. Here's a recent blog post about how we bridged the virtual gap at my last employer: http://www.brandonmartinez.com/2014/01/11/having-a-virtual-presence/
Where have you seen trends moving away from it? Just curious. 
I'm using the WebSecurity forms auth. The one that is baked into the forms auth template.
what does 'read-through' mean?
I am in New England and almost every office I have been in in the last 5 years has been in the process of cutting back or removing telecommute completely for software dev teams. 
An observer pattern/message bus is probably fine. You probably want to consider what happens when the remote web service is unavailable. Do they only need the most recent event, or should there be a durable queue of events which can be processed in sequence once the service becomes available again. In that case a micro-service might be most appropriate - libraries like MassTransit/NServiceBus and middleware such as RabbitMQ can take care of most of this for you.
You should use NServiceBus
Your CIO is an idiot and the team is on its way out if people are being lost that quickly. Find somewhere else to work. Quickly. You need to do this before your workload and stress becomes unbearable due to covering lost staff and before everyone else takes up local positions. Teams work perfectly when telecommuting unless there are serious problems with trust and motivation in the company. If there are trust and motivation issues, this is down to management not the employees. It's their job to trust and motivate people! You're being punished for their inability to manage, nothing more. Bar insurmountable technical debt this is the other major warning that a company is failing. I've been working from home for 2 years now as have nearly all of my colleagues. Our productivity has jumped massively as has our code quality. We also work flexible times. 
I telework 2 days a week (tuesdays and thursdays) in my current project. I feel like that's the ideal balance for me: * I'm never more than a workday away from the office, so meetings aren't an issue. * Days I'm at home, I can focus more and get a lot of work done. * Days I'm at the office, I tend to have more meetings and more interactions with the rest of the team. * Other team members can always get a hold of me on my mobile, on Skype or through email. * We do things like our 'Daily Scrum' over Skype anyway, as part of the team is in another country. I never felt like this is getting in the way of our team relationships or our productivity, on the contrary: * Less commuting time = happier people * Happier people = more productive people * Less distraction (at home) = more productivity But I think I'd go crazy working from home 5 days a week. The lack of social interactions would eat at me after a while.
Right, however last time I used it I ended up with huge memory leaks. It was on .NET 4.0 though, perhaps it has gotten better by now.
I created a similar cache for a project at my job, it is amazing how easy is to end up with the same names for classes/interfaces/properties/methods when addressing the same domain problem. My implementation also allows for the cache to be constrained to a certain size and allows to inject a custom eviction strategy (by default it uses LRU). Good job!
A read-through cache is a cache that fuses your backing store with your cache. It takes away the need for you to figure out if a value is in cache and if not getting it from db/web/disk and adding it to cache. you access everything through cache and cache is responsible to fetching values it doesn't already contains #In a typical cache your flow would look something like this 1. check if key is in cache 2. if in cache return cached value 3. if not in cache put underlaying dictionary in a write lock 4. check again to see if the value has been added to cache since lock has been acquired (this is to prevent more than one thread get to #3 at the same time ) 5. if in cache now return cached value 6. if not call a method that calculates/fetches the value 7. add the new value to cache 8. return the new value #A read-through cache 1. attempt to get the value from cache while passing a lambda to get value if it doesn't exist the rest of the logic is handled for you. 
Session is so last decade
Mother Fucker. I literally just spent this entire last week writing the god damned IUserStore for Azure Tables. 
Open source it?
I plan to when I'm completely finished, though this one is OS now: https://github.com/stuartleeks/leeksnet.AspNet.Identity.TableStorage Some of the implementation is more clever, though I strongly disagree with using usernames as any kind of primary key. Unique, yes, makes sense. Forever unchangeable? Less fun. This one also doesn't implement IUserClaimStore, IUserRoleStore and IUserSecurityStampStore so isn't a one to one replacement for the EF implementation. So at least my week wasn't a complete waste.
One important thing you have to understand when you're doing ASP.Net MVC is that you're doing web development and not Windows applications. A web application's UI cannot be exactly the same as a Windows application and something that is "standard" in a Windows application is not standard in a web UI. Developing in ASP.Net MVC requires also a very good knowledge of HTML (5) / CSS and JS. In order to do what you're asking, you'll have to use JS and JQuery and setup a web service server side to receive the update commands. It's quite a lot of work really. You could (and probably should) also use a JS library to help you doing that. [This one, Kendo UI](http://demos.kendoui.com/web/grid/editing-custom.html) even has examples in MVC to help you. good luck!
try stackoverflow
I use servicestack, which using IOC and DI. It took me about 10 hours to implement and get it running. More time converting more stuff over it like WebAPI and EF5 to FluentNHibernate. Then I discovered the performance benefits of ORMLite, Redis and PostgresSQL Changing from MySql to Postgres was a blink, changing from FluentNHibernate to ORMLite took 2 hours, models had to be changed. The development times were significantly decreased and I didn't need to rewrite my code. It makes code more manageable and makes it easy to read. Yes updating old projects would be a task, especially for beginners, but if you use a framework like ServiceStack is makes it easier and new projects are a lot easier to start out with. It's plug and play for code. It's the future of library development.
Cross-posted on: http://www.reddit.com/r/programming/comments/1vdj3z/how_to_measure_time_in_net_in_the_wrong_way/
Replaced a wrong way with a less wrong way. How about using a class that was specifically designed for this? http://msdn.microsoft.com/en-us/library/system.diagnostics.stopwatch(v=vs.110).aspx
The stopwatch is a great way of measuring, but in my specific case I wanted to save the date, and check it later (the program may finish and restart in between). Thanks for the comment.
And why didn't you simply save it as a DateTime then?
Correct me if I'm wrong, but your comment reads like the OP doesn't know web development. WebForms is also web development, not Windows development. It too requires knowledge of HTML, CSS, and JavaScript. 
We saved on a plain text file, and for not messing around with time formats and cultures when parsing the item back, we though it would be a good idea just save the ticks.
http://puu.sh/6o1sl.png http://puu.sh/6o1v1.png 
Kudos for that.
Oh, my bad... I read "Winforms" instead of "Webforms". Still, I think the question shows a kind of lack of knowledge about HTML/CSS/JS. Webforms development requires far less knowledge of "real" HTML/CSS/JS than ASP.Net MVC since you have access to all those pre-made controls who create the HTML/CSS/JS for you. In MVC you don't have all those controls, so you have more control over your HTML result but you have to do more yourself, thus needing more knowledge about HTML/CSS/JS. MVC and webforms are two very different approach to web development.
I get that they're very different. But you still have to know HTML&amp;CSS, webforms doens't do any of the CSS for you and you can't build a website with just .NET controls. Maybe not JS, you can get around that, but you can probably get around using is with MVC as well. I use all 3 extensively in webform development. I don't agree with you, however, that his question shows a lack of knowledge of either of those 3. Just because webforms has a textbox control doesn't mean you don't know what an input tag is - it just means you don't generally use them if you want to access your control from the code behind. I'm sure he knows HTML/CSS and perhaps JS, but didn't know if there was a way MVC would hold his hand the way webforms does.
I find it way easier to store a date in plain text in a "standard" format than saving ticks. If, for example, in the future someone would try to import your data by parsing the file and stumbles upon a float value he would have lots of trouble understanding that a) this is a timestamp b) the way this timestamp was represented. If you want to be sure about interoperability and cultures issues you can always use the RFC1123 pattern ([myDate.ToString("r") =&gt; Mon, 15 Jun 2009 20:45:30 GMT](http://msdn.microsoft.com/en-us/library/az4se3k1%28v=vs.110%29.aspx#RFC1123)) or the so-called [universal sortable format (myDate.toUniversal().ToString("u") =&gt; 2008-04-10 13:30:00Z](http://msdn.microsoft.com/en-us/library/az4se3k1%28v=vs.110%29.aspx#UniversalSortable) which has the added advantage to be sorteable.
I never understand why everyone always suggests things like KendoUI for stuff like this. I've always found that libraries like that are overkill most of the time. Either that or they can become a crutch for some people. Basically they learn the controls and never learn the underlying technology. For instance the number of people that I've interviewed who couldn't build an html table, each of which had multiple years of web development on their resume, is quite shocking... Anyway, what you're asking about is actually not that difficult. All you would need to do is set up an action on a controller to accept the data you want to save (assuming you have a model already, you could just use that as your data type), then use something like jquery or vanilla JS if you want to be all fancy to attach to the onchange event for the various inputs, and just have an ajax call that sends the row of data that was changed as a JSON object to the action that you set up. That's it really. Of course if you wanted to get fancy with things like client side caching, offline mode, etc. you can always use something like BreezeJS but that might be overkill for your needs.
Based on OPs desire to treat the web like it's a VB6 application writing to a local access database I think it's safe to say the OP doesn't know web development. They know some HTML and javascript, but they don't know web development.
Alternatively to the universal format you can also just pass the `InvariantCulture` as the `CultureInfo`. That way it does not matter what format the system has.
Then you have very specific needs that you really should mention in this article. Right now it seems like you just want to measure time, and `DateTime` is the way to go for this - but it generally is not. Please don't pollute the net with more half-hearted half-informed blog articles.
Good option as well, yes. The only thing I like better with the universal format (especially the sortable one) is that its format is obvious and you're sure where the month and the day are. I can't count the number of time I had to import old data and had to check a dozen of files to finally figure out if 12.01.1990 was the 12th January or the 1st December. If the format starts with the year you can be 99% sure that it is a "sortable" format and that it will be year followed by month and then day.
&gt;Basically they learn the controls and never learn the underlying technology. For instance the number of people that I've interviewed who couldn't build an html table, each of which had multiple years of web development on their resume, is quite shocking... That was one of the things I was asked in my most recent interview. My thought was "you're kidding me, right?" but the interviewer said a lot of people couldn't do it. Insane.
That is a very good point. :-)
thanks for pointing it up, I'll update the article
I'm not exactly sure what would have possessed you to use Environment.Ticks over DateTime in the first place.
I just updated the article, once again, thanks for the comment
Yeah it's dependent on where you are but locally it's pretty difficult to find decent candidates. We ask only a couple of tech questions and they filter out about 95% of candidates: 1) To create an HTML table (nothing fancy, just a plain table). 2) Write an SQL query that selects a specific row in a table (no joins, just a simple select). 3) Write a function that removes duplicates from a List&lt;int&gt; object (which is just a call to the Distinct extension method or about 10 lines of code if you don't know about the method). Note that if someone is up front about missing either HTML, SQL, or C# then we simply don't ask the question or modify it for the languages that they know, for instance Java instead of C#. But usually we have 50% of people who can't do the first item, 70% can't do the second, and 90% can't do the last. Last time we hired someone we went through 30 people and only 2 could do the last problem. Basically we consider someone a strong candidate if they can do two out of the three... Or just one but they know that subject REALLY well and we feel we can teach them the other two.
It was not my code on the first place :P I just found the bug ;)
Can you get access to the compiled libraries. I would assume the data is being pulled from a data layer possibly in a separate library
Do you have access to where the website lives? Maybe take a look in the app_bin folder. They may have placed it there as separate libraries. If so you would be better off. Any reason not to write your own access module to the data?
I don't have access to the specs, I'm working from another department and there's a great deal of distrust internally regarding security of information between departments. As far as they've been concerned, we "don't need access" due to the frontend we have access to. It's frustrating.
Figuring out the API calls (AJAX) would be far and beyond the best way
That's funny because I always wonder why people try to do things themselves when for the cost of a couple days of their time they get something that has much more functionality they they would have come up with in those couple of days.
Check out http://www.kimonolabs.com/. Not sure if it would help, but sounds like it might. HtmlAgilityPack could also do this.
Blech. I'm interested in checking out your test but extremely uninterested in creating an account to do so. I can understand why for the main functionality, you need an account but I think you should be able to take single tests without that requirement (if you're sufficiently interested in results without the attached account info, I don't know your business needs or strategy, so maybe you're not).
All valid concerns, but wouldn't any one who wanted to do that just create an account?
Some remarks in no particular order: * A lot of this is about ASP.NET. It's a fine technology and all, but if that's what you want rename it to "ASP.NET Test". ASP.NET is just one part of .NET. * Some of the questions are about C#; while it's always used with .NET, there are many other .NET languages (e.g. F#, VB.NET). Again, if you want a C# test, make a separate test for C#. * Some of the answers are wrong. `orderby`, `ascending` and `descending` aren't really keywords, they're contextual keywords (OK, this is a bit pedantic). `System.Type` isn't a namespace. * The scope of the question is too variable. Some of them are extremely specific, some are super-basic language questions, some are in the middle... You're falling in the trap of considering ".NET" as "whatever I'm currently doing with .NET", which in your case happens to be ASP.NET via C#, when it's so much more. IMHO, you should split the test in three: a basic C# test, a basic .NET/CLR test and an ASP.NET test.
10minutemail.com is your friend
Apache ftw. But no really... You can do it in IIS
I know how to do it in IIS 6/7. This particular client of mine does not have access to IIS though. His web hosting company is known for being quite lazy/unresponsive
Is there a way to check if those modules are installed with only FTP access? Sorry, I come from an Apache background and this is all very new to me.
Oracle hates you. 
Also, your ORM should allow you to map column names to sensible property names.
It does, but it takes time and therefore $$. It's also a pain in my ass to spend more time on something than I should have to. It screws up my groove when I'm trying to program something. I should be able to make "sensible" column names instead, don't you agree?
Just Say No. If you've been tasked with building an application that needs to pull data from another internal application, but you're not allowed to access any useful API because "you don't need access", then simply tell your boss/project manager that this simply cannot be done in any safe way because the pre-conditions are shit. Further, if you can access the data via a web interface, there's no reason that you shouldn't be able to access it via an API or web service, other than due to the other developers being lazy bums or (probably) protecting their turf; in which case you should either try to replace them or leave ship as soon as you can.
Yea, that one sucks.
Amen brother
More specifically, it's about ASP.Net WebForms. I haven't touched this technology in about 8 years. I use ASP.Net MVC frequently, however.
Ok, I ran through it. It's very ASP.Net WebForms heavy, which would have worked out great in 2004. If you replaced all the WebForms questions with ASP.Net MVC questions and called the test "ASP.Net MVC" instead of .Net it would have been much better. Actually just calling it ASP.Net WebForms would have been much better. There is also a sort of scattergun approach: there were questions which had VB, some with C#, some with ASP.Net WebForms markup. I don't really know anyone who values such a broad scope of knowledge about the .Net ecosystem. It would like having a test on "English" and having questions about grammar, the history and development of the language, Shakespeare, and the state of the art of ESL learning on one test. No one really wants one person to have a command in all these areas.
Here is a web.config that... 1. Matches the URL "product.aspx" 2. Checks for the specific query string (you can remove this if you want to redirect all requests, or update it with wildcards, etc) 3. Redirects to the new domain and location, appending the query string If it doesn't work or throws a YSOD (server error) then you don't have URL Rewrite support. If you want to know before deploying, simply open a support ticket with the hosting provider and ask them. &lt;?xml version="1.0" encoding="UTF-8"?&gt; &lt;configuration&gt; &lt;system.webServer&gt; &lt;rewrite&gt; &lt;rules&gt; &lt;rule name="RoadRat Redirect" patternSyntax="Wildcard" stopProcessing="true"&gt; &lt;match url="product.aspx" /&gt; &lt;action type="Redirect" url="http://www.citecarelectricvehicles.com/product.aspx" appendQueryString="true" /&gt; &lt;conditions&gt; &lt;add input="{QUERY_STRING}" pattern="ProductId=129&amp;amp;title=citEcar+Utility+Buddy+Enclosed" /&gt; &lt;/conditions&gt; &lt;/rule&gt; &lt;/rules&gt; &lt;/rewrite&gt; &lt;/system.webServer&gt; &lt;/configuration&gt; \* Edit: Updated with better version.
You don't "do it" in IIS. You configure the website via the `web.config` and associated files. Much of IIS is just a GUI front for those (or similar) files.
This seems silly to me... why write the conversion code for every property when you can return an entire object once? [Here is my rough, untested, slightly-more-than-pseudo-code take on the problem...](https://gist.github.com/anonymous/8485475) You can test the `WebSessionContext` class outside of the Session variable, or provide your own way of hydrating the data (perhaps you store a cookie and can load from a Db). You get a strongly typed "context" class that gives access to session data without having to write large amounts of conversion code.
Lucky. &gt;_&gt;
This is probably one of the better answers when it comes to developing an application. With an app, the view of the data changes based on the size of the screen: higher resolution screens get more options and content (in general). In this case you would want to swap out the View entirely for a mobile specific control set. In the case of a website I would go with zurb or bootstrap or some mixture. Just my two cents....
Even Asp.net MVC has ceased to be cutting edge these days. WebAPI and single page apps using knockout.j's or angular.j's is where the cool kids play these days. Since this is a .NET sub, I won't mention node.js. ^^^Whoops
Preferably not an ultra mobile model. Prefer quad core and more ram. Prefer an SSD
You should use mongodb because it is webscale. 
I like mongodb, but I don't have a choice, and I need a relational database.
Angular is the best.
WPF uses XAML which is also useable for Windows 8 and Windows Phone Apps, i guess it is alive as much as it can be. Building simple Forms like apps is just as easy but you can customize every little pixel if you want too. You also get the possibility to use data binding. Later you could look into MVVM, which helps disconnecting the UI from the logic part and makes a lot of code useable on the other Windows platforms, but that has a longer learning curve, at least for me ;-)
WPF isn't really a Win8 thing. It's an alternative to Winforms and is definitely used a lot. I started using WPF over Winforms about 2 years ago and when you get to know MVVM and how databinding works it's a brilliant way for designing and making applications. (Of course, depending on the projects, you can pick one or the other, but what you can make in Winforms you can make in WPF)
WPF is a brilliant technology, but quite a deep rabbit hole, and has a bigger learning curve than winforms. Presumably it is still relevant in Windows 8 desktop right?
Thank you for replying. I will definitely look into MVVM as soon as I get some experience on how WPF applications work.
Based on the answers I got so far, it seems so. I guess I'm going ahead with it :)
This is a good advice. You will also need the [MVVM Light Toolkit](http://mvvmlight.codeplex.com/) when doing MVVM in XAML projects. Check out the tutorial vids from Mix'10 there. They're really great for getting you started with MVVM (the pattern, not the library in specific)
As bad as Metro has been, it is so ridiculously improved over WPF in important areas. The entire graphics pipelines has been improved. WPF has major issues utilizing the GPU whereas a modern app is extremely efficient. Try implementing smooth touch scrolling in WPF, it's rough.
WPF is still the preferred method of creating windows desktop applications using .net. It is a mature system, and is still worth learning (and will be unless Windows 8 or 9 get enough marketshare to warrant a switch).
Agreed. While technically you *can* do some sort of parsing/scraping; it is a ton of work and your app will be super fragile (depending on the asp.net app to be predictable, and not to change). 
orderby, ascending and descending are keywords, specifically contextual keywords, but saying that those are keywords isn't wrong
KendoUI makes it so much easier, especially with its MVC helpers and the DataSourceRequest class that automagically applies the grid filters,sorts,paging, and aggregates to your enumerable before you serialize it. 
The part that really sucks is when the people that don't know that stuff get hired and work on your team getting paid the same or similar salary as you. Should extremely knowledgable programmers not get paid a lot more than the ones that can just barely get things done?
I've been a web forms developer for awhile now, and have begun "unofficially" (i.e., for my own edification) porting some apps over from web forms to MVC. I've had some of the same frustrations(?) as you. The key is *model binding*. For the grid-like things, you simply use for loops (not foreach) to render the "controls" (e.g., @Html.TextBoxFor(m =&gt; m.MyTextValue), @Html.CheckBoxFor(m =&gt; m.MyBoolValue), etc.) for each row, for each value in your model's List&lt;&gt;. I've gotten my MVC pages to look/act at least 95% equal to my web forms pages.
Telecommunication is one of the perks of the trade, when it is allowed by an employer. My last job allowed me to work from home any time wanted to, so I pretty much never went to my office. My current job does not allow me to telecommute and I really don't like that about it, but it pays almost twice what my old one did. That being said, if I found a job that paid close to this one that would allow me to telecommute, I would change jobs in a heartbeat. Your new CIO is making a big mistake. Good developers are hard to find, but it's not really hard for them to find a job where they can telecommute. Most of them would likely prefer a job that allowed it. I feel bad for you man. 
Just don't forget to "save" that whole session object again when you change any of its properties or they will not persist. Edit: you could call Session["MySessionObject"] = this; in all of your property setters just after setting the property. Better yet, implement INotifyPropertyChanged and do it in the handler.
You don't have to convert your session objects just cast them. They are already the type they were when you set them, you just have to tell the compiler that is what they are.
I'm working on an application that I wanted to speed up. I wanted it to only load and parse the common JavaScript files once and not have to reload the whole page every time the user navigated to a different page on the site. I ended up using angular and its router and just loading up my pages with the layout set to special one I made for injecting a pages into the ng-view element that moves the script blocks around so the work with jQuery's Ajax script execution and the many form element widgets we use in our forms. It actually works very well and the increase of the speed in which the pages load when navigating are very noticeable. I apologize if this description is hard to follow, I'm on my phone and tried to keep it short. 
+1 for this. One other thing that I've done is create a local (on my team's dev server) nuget repository server where we pack and push DLLs that are finished or mostly finished and are shared across multiple different solutions. This had made it much easier for me to maintain the framework DLL that we use.
I don't understand why this thread is at -4. Timekeeping is complicated and an often overlooked problem. The method I generally use: assume there is a DateTime StoredDateTime; (DateTime.Now - StoredDateTime).TotalMinutes (or whatever TimeSpan property you need; returned as a double) The - operator between datetime and datetime returns a timespan. http://msdn.microsoft.com/en-us/library/ff986512(v=vs.110).aspx I use this: http://msdn.microsoft.com/en-us/library/system.datetime.tobinary(v=vs.110).aspx to store the datetime in a database or wherever else you need to store it (as a long). There is a FromBinary static method to recreate the datetime. I don't believe there is any problem in using this method across daylight savings time transfers, but I could be wrong. I hoped I helped simplify your solution a bit. Stuff like this should be discussed; I upvoted this thread.
Thanks. Have you figured out how to pass multiple models from a controller to a razor view? Let's say I want to to show the top 10 cities (City model) and right below the top 10 movies. They are two different models. It would be so simple with a webform. You just get the two lists List&lt;City&gt;, and List&lt;Movie&gt; in code behind and you are done. I am still trying very hard to have that aha moment.
LINQ - n - Logs
Not a terrible idea. I just use a local nuget repository and packages that distribute the views.
You would make a class that exposes both class Name { public List&lt;City&gt; Cities { get; set; } public List&lt;Movie&gt; Movies { get; set; } }
Kentico is awesome for both developers and uses. SharePoint is a thorn in my side. 
Kentico has document types and custom tables which would be like umbraco value types. In my opinion, Kentico is leaps and bounds ahead of umbraco.
Holy shit.. that product is expensive. Is it open source as well? How do the schemas compare?
As a model? Just doesn't seem very elegant.
And if not, you can always go the ViewData route. I'm not a big fan due to its magic stringiness, but it does make sense for situations where you have different sections containing unrelated data.
Yeah. This is a view model. 
You could use partial views to make the display logic cleaner, but the controller will still need to send all the data in some form to the view (either a new class, or using ViewData). It sounds like these are two unrelated lists. If the movie lists are based on Cities, then add the movies collection to the city model and you're good to go. 
It's a view model.
That's exactly what a viewmodel is.... it's modeling the VIEW, which is where you want your two list.
This is the proper way to do it... you're modeling the view in this case
In my hypothetical example, I am listing on the page the top 10 Cities and top 10 Films. There is no relation between the two lists (like master-detail).
Create a folder called "ViewModels" and stick it in there. It's a very common MVC pattern. I usually create a separate folder for each controller. 
You could, but please don't.
How the data relates to each other is irrelevant. You want to model a view containing that information and you will need a viewmodel that contains that information.
You should use a viewmodel, but you could also use the viewbag.
It's as simple as it gets.. you have to define what your data looks like.
Have you compared your generated IL with IL generated by the C# compiler targeting x86 specifically when you write the code in C#? It might shed some light what might be needed. I always use that as the first step when generated IL crashes with weird errors or unexpected exceptions. 
MvvmLight is just short hand for memory leak. I don't understand why anyone uses it.
Why don't you use the Visual Studio built-in desinger tools to add new rows and columns? http://i.imgur.com/NQx2iGy.png It automatically updates column and row indexes. It works in VS2010 too, there you have to right click on the designer area and select Grid Row/Grid Column -&gt; Insert After/Insert Before.
How would you do this from a database first approach if your main model was all about movies and you simply wanted to add city to a page for a drop-down or something?
Very cool and unexpected. Does it also work when session is stored in the database? Edit: I haven't looked into how it is done under the hood, but I know that when using SQL session state it serializes the objects. Which is why I assumed it worked similarly with in-proc session state. Would you care to test this for us, since I am on my phone and won't be near my computer for a long while? 
When I was starting out, answers like this always bugged me. The *reason* why you shouldn't is that it needlessly restricts the model to 2 objects which is confusing and harder to maintain. That said, there may be certain circumstances where it is appropriate. They just aren't common.
Thanks for your reply. I have checked this, and there is a difference. C# emits a local variable which it stores the return value in. Then it jumps to the next instruction for some reason, which just pops the value and returns. Adding this seemed to fix the issue. I just don't understand _why_. I'll have to look deeper into this tomorrow though.
What do you mean by database first? Either way, I'd load cities from an HTML helper.
I don't work for Kentico, but I would if they paid me at least what I make working where I do now. I'll admit that I have not used umbraco much, but I was paid to convert a large website from umbraco to kentico last year. What I can tell you for sure is that the built in form field editors for Kentico document types are much nicer and it is easier to create your own. The management interface for the document types is way more user friendly. If you want to see it for yourself, just download the free version of Kentico. I can tell you a lot more about one of my favorite things about Kentico, which are the document transformations. Along with each document type, you can create transformations or templates. These are similar to partial razor views in MVC, but use a different syntax. You can choose to use an .ascx style syntax, jQuery templating syntax, or kentico's own K# syntax (which I like the best). These transformations can be used with any of their many data view widgets. For example: Say you want to make a list of employees on a page that shows a photo of them, their name, and some other bits of information. This will also be something that the non-technical CMS users can manage once you've set it up. First, you create your document type with the appropriate fields and form controls. This will probably only take you about 5 to 10 minutes. Then you create your transformation that will output the markup you want and apply your Document Type Model's property values where appropriate. You can make one for the list item and one for the detail if you want. Then you go to the CMS document tree and add your employee documents where you want them. Lets say you just put them under /Employees Then you go to the page you want to display them on and add a CMS DataList if you want a grid, or CMS Repeater if you just want a vertical listing. You configure this widget's datasource to display documents of type Employee under the /Employees/% path and use your List transformation. You can optionally do paging and order them by any order you wish and several other options. This is all very easy to do for even a novice developer to do. The more you use it, the more thankful you will be for how simple it is to use and customize. 
I'm not sure that your class will work for multiple sessions because of your static field for _current. Did you test it with two browsers open? Edit: I just tested it and I was right. Since you are using a static field to store the session context, it is shared across the whole application. When TestString is set in one session, it is also set for everyone. However, it does not actually store it in the session for the other users, it just stores it in the static field of the WebSessionContext class. You can modify the get and set accessors for Current to the following and it will work fine: public static WebSessionContext Current { get { return HydrateSession(); } set { HttpContext.Current.Session[MASTER_SESSION_KEY] = value; } } 
If you don't want to clutter your object model with classes that are only related because they're in a view together (like everyone else here seems to be suggesting) then I would look in to partial views. Your controller should be returning this data individually already, just bind each partial view to the appropriate property on the controller, then drop each partial view in a full view. Your full view might not even have to be bound to anything.
The alternative to view models if you don't want to clutter your object domain is partial views. They live entirely in the front end namespaces and they can bind to properties on the controllers individually.
Partial views. Don't know what the obsession is with view models here.
Why not just use a ViewBag?
Because view-bag is an anti-mvc crutch. If you're writing a one off application and/or you're the only one who will ever maintain it then go ahead, but it's not a design pattern and it's not something you should ever realistically do in a professional setting.
Are you sure your generated code is valid? Note that the runtime never actually checks this, amazingly enough. You could write the IL to a disk assembly and use "peverify" on it to make sure. Not all sequences of IL instructions are valid even though they make sense and are type safe.
I think there are definitely situations where a ViewBag is a far easier and less complicated solution than a partial view. It's not difficult to understand what it's doing. Yes, you don't want to build an entire application using them, but I really don't see an issue using them in a limited matter. 
As I've read it, you have a couple of choices. You can write more of a "view model," which can contain multiple member classes or member Lists: in your case you could have City and Movie *contained* in a parent class (which you can then call the view model). The other option is to pass things like your lists in the ViewBag. I think all in all, as long as you're not stuffing it full of dozens of objects, using the ViewBag is probably more time-efficient.
essentially what you want to do is scrape the pages. use htmlagilitypack to do this. its quite easy to log in, perform form submits and do anything the browser does. scrape the pages into xml, use xpath to scrape the values you want and put them into a database then have an application process the results. depending on your needs you could run it as needed or schedule it in a windows service
Oh that's a silly mistake!!
Database models and MVC viewmodels are two different things. Ideally your MVC views would know nothing about your database models (although this is often the quickest way to build an MVC app - just reuse database models). 
Using a database first approach to build the Model is something that is built into VS, why would you want to re-create the entire database and all the classes from scratch if you already have them? 
I don't understand how in a Database First development the relationship between the Views and the Model are any different. The db first route just creates classes for the db, you just pass what you need from your controller to your view. I've used the db first approach for pretty complex database structures, and it works really well (for MSQL though), never had an issue.
is this the only way? I don't really like using mouse while coding, it messes up sizes and positioning, generates ugly unmaintainable code. 
I'm not sure what the other guys are trying to say, but it might be along the lines of what I'm going to tell you. Your View model class should not be your entity class (even though it *can* be). It is not a good practice and it seems to cause confusion for people new to MVC. Think of your ViewModel as a data source for all the things in your view. You can put anything you want in it that you want to display in the view, including, but not limited to, your entity model. If you want have a form in your view and to include data beyond what is in your entity model in your view, then you can make your entity model a property of your viewmodel , let's say we call that property something generic so that you could maybe even create a base class for your viewmodel that can do some common things if necessary. As an example, let's call that property "FormModel". Bind your form fields to the properties of your FormModel like @Html.TextBoxFor(model =&gt; model.FormModel.FirstName) In your POST action, if you want to directly use an argument of your entity model's type for easy access, you can decorate it with a Bind attribute like this: [Post] public ActionResult Index([Bind(Prefix="FormModel")]YourEntityType formModel) That will tell the default model binder how to find your entity in the request. Make sense? Another reason to not use your Entity class as your viewmodel is that you may want to have different property types that represent those of your Entity. For example, you may want to have a nullable DateTime in your viewmodel for a non-nullable DateTime property in your Entity so that you can bind it in your view without it displaying as the DateTime.Min value and you can put a [Required] attribute to make it validate. You can still include your entity as a property on your viewmodel, but you would also have the additional property somewhere else on your viewmodel for this nullable DateTime. You would need to map the value between the two in your controller. Personally, I create an entirely separate class from my Entity to use as my viewmodel and then map the properties back and forth in common methods override from my viewmodel base class. (I use a scaffolding template to do this, so I don't have to use reflection or write all that code by hand). This is also good for unit testing because you can use an interface for the Entity argument and swap out the actual type you are using for testing if you want. Now I'm just getting into too much detail that may not relate to anything you need, so I'll leave you with that.
When you do this with a partial view do you retrieve the data using a code block in the partial view? If so, that's a bad practice because it doesn't have good separation of concerns.
What did the guy say? It would be useful to everyone to know what not to do.
ViewBag then. Magic expandoobjectness. ViewBag and ViewData access the same dictionary.
Why are we down-voting things that OP says just because they are wrong or we disagree with them. I know this isn't a forum, but a lot of people use this subreddit to learn. If we hide things like this by down voting them, how will others find them and the informative answers?
I realize that is the general consensus around here, "ViewBag bad", but using the ViewBag for things that aren't in your data entity *does* make for simpler code when binding posted data back to your controller.
What do you mean "using a code block"? You bind to a property on the controller just as if it was a full view. The alternative everyone else is suggesting is to create a view model, which means either the controller returns that view model (which means having view specific properties on the controller), or the view has to assemble the view-model itself. With partial views, each view can be bound to a different property on the controller. The assumption with a partial view is that is has no relational correspondence with the data in the main view. For example, say your main view is a list of theatres and the movies they're showing, and you have a sidebar of the top 10 movies this week. Your main view is bound to a controller property that returns the local theatres, each with a list of movies playing at it (simple relational database call, if you're using an ORM this is dead simple to linq through from theatres to movies). Then you have a partial view that shows a list of movies. This is bound to a property on the controller that returns the top 10 movies. The top 10 list has no relation to the local movies other than they will *likely* have some overlap. The partial view is rendered separately and inlined in the final view.
He said you could use a tuple.
Ok, so I see where you are coming from and it makes sense, but here is a practical example of where I have run into issues with this approach (and maybe I just don't understand it well enough to solve this). What if you have a Movie in a Movies db and that movie has 25 different attributes (genre, actors, year, etc..). The Movie is stored in a Movies table and the attributes are all stored in their own tables linked to the Movie table via one-many, many-many joins. You want a user to be able to edit the Movie and change/add attributes. So in the Edit controller you pass the view a Movie from your model and you need to populate the drop-downs/check-boxes with the existing attribute data for that specific Movie. So to do this, you need the existing Movie data (from your model) and the full lists of attributes from the other tables in the model. My first reaction is to pass the view the Movie and populate the lists/drop-down with Viewbag data from the controller. What you are saying is that you would create a new ViewModel for the Edit view that contains both the existing Movie (and all its data), as well as the data from all the tables that join to the Movie table so you have full access to the full list of say 'Genres' and not just the ones that are assigned to the Movie? So when you say: &gt;If you want have a form in your view and to include data beyond what is in your entity model in your view, then you can make your entity model a property of your viewmodel. My issue isn't trying to include data that is beyond the entity model, it's trying to include data that is in the entity model, but not available in the portion of the model you send to the view. Thanks for the reply by the way, appreciate it. I am still trying to learn this stuff. 
Javascript is so much fun though! It's like programming in a wild, untamed wilderness!
As long as you mean you bind to Actions on the controller instead of "properties", then this makes sense. I do like this approach as it is similar to the way I do it using kendo ui and their javascript datasource to make ajax calls to my controller actions to retrieve json. The main difference is that I don't use a partial view at all.
ok, then you can just create a view model class for your view that includes your entities as properties. That is the simplest and most recommended way to do it.
Fair play! I was just doing a bit of research on SignalR recently so I'm happy to see it in the wild. How's your experience been working with it so far?
Very cool. Thank you for sharing this. I've experimented with SignalR at work for communication between our control panel and a client application for remote control. I have been interested in remote support for the users if/when they hit snags. This will be great inspiration.
This is most likely your cause. Your IIS does most likely not have the handler configured for ASP.NET.
How are users at B authenticated, i.e Forms / Windows etc.
Forms. Edit: I assume it's forms. I not familiar enough with classic ASP to know if there are other ways of authenticating. But I assume not.
Have a look at: http://www.asp.net/web-forms/tutorials/security/introduction/an-overview-of-forms-authentication-cs There's an interesting paragraph: *The Scope of Forms Authentication* *The FormsAuthenticationModule is managed code that is part of the ASP.NET runtime. Prior to version 7 of Microsofts Internet Information Services (IIS) web server, there was a distinct barrier between IISs HTTP pipeline and the ASP.NET runtimes pipeline. In short, in IIS 6 and earlier, the FormsAuthenticationModule only executes when a request is delegated from IIS to the ASP.NET runtime. By default, IIS processes static content itself  like HTML pages and CSS and image files  and only hands off requests to the ASP.NET runtime when a page with an extension of .aspx, .asmx, or .ashx is requested.* *IIS 7, however, allows for integrated IIS and ASP.NET pipelines. With a few configuration settings you can setup IIS 7 to invoke the FormsAuthenticationModule for all requests. Furthermore, with IIS 7 you can define URL authorization rules for files of any type. For more information, see Changes Between IIS6 and IIS7 Security, Your Web Platform Security, and Understanding IIS7 URL Authorization.* *Long story short, in versions prior to IIS 7, you can only use forms authentication to protect resources handled by the ASP.NET runtime. Likewise, URL authorization rules are only applied to resources handled by the ASP.NET runtime. But with IIS 7 it is possible to integrate the FormsAuthenticationModule and UrlAuthorizationModule into IISs HTTP pipeline, thereby extending this functionality to all requests.* What I think right now is that you should be setting the cookie used by Forms authentication to something that gets sent both to website A and website B, otherwise Forms auth would not work. http://en.wikipedia.org/wiki/HTTP_cookie *Domain and Path[edit] The cookie domain and path define the scope of the cookiethey tell the browser that cookies should only be sent back to the server for the given domain and path. If not specified, they default to the domain and path of the object that was requested. An example of Set-Cookie directives from a website after a user logged in: Set-Cookie: LSID=DQAAAKEaem_vYg; Domain=docs.foo.com; Path=/accounts; Expires=Wed, 13 Jan 2021 22:23:01 GMT; Secure; HttpOnly Set-Cookie: HSID=AYQEVn.DKrdst; Domain=.foo.com; Path=/; Expires=Wed, 13 Jan 2021 22:23:01 GMT; HttpOnly Set-Cookie: SSID=Ap4P.GTEq; Domain=.foo.com; Path=/; Expires=Wed, 13 Jan 2021 22:23:01 GMT; Secure; HttpOnly ...... The first cookie LSID has default domain docs.foo.com and Path /accounts, which tells the browser to use the cookie only when requesting pages contained in docs.foo.com/accounts. The other 2 cookies HSID and SSID would be sent back by the browser while requesting any subdomain in .foo.com on any path, for example www.foo.com/. Cookies can only be set on the top domain and its sub domains. Setting cookies on www.foo.com from www.bar.com will not work for security reasons.[29]* hth! 
Nice answer I will be sure to read through that MS resource. My first idea was of course to just share the cookie, however I quickly came across an issue with this (without implementing anything). The cookie has the domain set to a sub domain and not to the top domain. I can't modify the existing app B, and I can't change the domain of A due to the network infrastructure already present. The contents of your post suggest that what I want to achieve might actually not be possible with the current set up. 
I think the problem is your using VB.NET. You should be using C#
It has been amazing. SignalR is simply fantastic. When you go more deep than the typical chat application that everyone is doing you start to see the potential. I started to port LCSK when SignalR was beta (not part of Microsoft). It was more challenging back then, but now the documentation is complete and there's lots of samples.
What makes you say that?
Because in every sphere, there's something that's "cooler" than another option. Some people love to bag on VB.Net as though it's somehow inferior to C#, although I'd be interested in having anyone to show me something you can do with it you can't do with VB.Net. I've worked professionally with both for a decade and I'd prefer the ease of VB over the rigidity of C# any day (case sensitivity, end line semicolons, scope bracing. Add to this that the select case functionality of VB.Net is much more flexible than C#'s switch and it's an easy choice for me, and this is coming from someone who learned C as their first language. You just have to deal with people who want to tell you, "VB sux use C# hurr hurr!!!"
That's the handles clause. Both page load and button click events have them. Changing auto wire to true doesn't change anything. I suspect a server config setting since it works fine on localhhost from the IDE. I'll have to sift through the control panel. 
tongue in cheek joke. I been working on vb.net for the past 3 years. back to c# for 2 months and its much nicer. VB.NET feels too verbose.
Case sensitivity I find useful. I like to end lines in semicolons. I hate having to put the _ to make a new line in VB. I like C# mostly when using linq and lambda statements.
[http://sessionservice.codeplex.com/](http://sessionservice.codeplex.com/)
A lot to think about, and thanks for taking the time to type that all out. I should have mentioned originally that I tried setting the autowireup to true and it didn't have any effect. Upon further review it's set to true in the master page. It seems like it must be something to do with the server configuration since everything works perfectly in my IDE on localhost, but when I deploy to the remotely hosted server I don't get any events. No page load events, no button click events, nothing... As for Vb/C#, I do have to use both for work. Despite that, I prefer VB.Net. I'm completely comfortable with the syntax and usage of C#, in fact, I started out with C over 10 years ago. I just find VB.Net to be a lot faster and easier for me to churn out code.
That's weird that it works locally and not when deployed. Remember I said that the MasterPage is a child control of the Page? That means that, even if AutoEventWireUp were inherited (I don't think it is, but I've never checked), setting it to True on the MasterPage would have no effect on the page itself. Even if it would, having the setting in the page declaration would override it anyway.
Did you check the file on your remote server to see if the AutoEventWireUp is "True"? It could perhaps be a deployment issue. Edit: Actually, since you said that none of the events are firing, that may not be the issue. Do they fire on other pages? Are you using CodeBehind? If you are, does your page file correctly inherit from that codebehind on not the remote server and your local copy?
I'm a fan of the method information displayed above the method (tests covered, usages)
Peek at definition. It's a little, but awesome change. http://blogs.msdn.com/b/zainnab/archive/2013/07/10/visual-studio-2013-preview-peek-definition-aka-go-to-definition-peek.aspx Other than that, it's just 2012 with an online account built in. 
Not mine, but worth bringing up: [VB Advantages](http://www.reddit.com/r/dotnet/comments/qww2n/what_would_you_think_about_a_language_that_had/) I don't get the impression that Microsoft plans to keep going the dual language route, though - The XML in VB is a personal favorite. 
VS2013 is required to write apps for Windows 8.1. Not much of a feature, but it's still a reason to upgrade.
TFS is a lot better. So is all of the testing stuff. Large solutions build noticeably faster. There are also random bugs. Database projects are still pretty abysmal. Restart visual studio usually fixes it, but it's telling of the stability. But really, the #1 reason you should upgrade, far far more important than all the others, edit and continue works in x64. It took them 10 years, but it finally works again. 
Someone actually uses windows apps on desktop or laptop?
The HTML editor is better and autoformatting Razor code indented with tabs finally works correctly!
personally i find its just quicker and less resource intensive - though i went from 2010 to 2013, skipped 2012 so .. Also plays very nicely with TFS and Git. Sidenote the 2013 TFS powertools are fantastic, mostly because unlike their 2010 counterparts - they work.
No. ASP.NET Web Pages is a separate thing, used in WebMatrix.
I don't think there's any reason if you already know normal MVC. It was meant as a low friction toolset to introduce people to .Net web development.
On top of the other reasons in this thread. If you want to work with the latest versions of MVC, WebAPI and the like only VS2013 has the full set of templates. You can make a WebAPI2 project work in VS2012 but it takes a lot of massaging to get it to build properly. The biggest gain for me though has been the dramatic speed improvement of this version. 
I think of it as a polished VS2012. It feels more responsive and polished.
I do, I use Mail and Music all the time. My only problem with Windows Store apps on a desktop is that you can't run them in a window, and they don't show up on the task bar. That being said, the OP may still want to write apps for tablet users, and would need VS2013 to do so.
This article is just plain wrong: &gt; Compile-time error checking to protect from typos and assigning improper types That's not correct, the casting operations are late bound since it's impossible to know at compile time what kind of object is coming from the object-typed session gets. I'm all for strongly typing session access, but at least understand what you're doing when you preach it...
Some of the addons I used were baked in. The global search (Ctrl + , ) is nice for finding anything. Intellisense now picks up CSS classes in the HTML editor. The enhanced scrollbar is a nice addition. Edit: AUTO-BRACE COMPLETE!!!
It's faster &amp; more responsive with more internal lazy loading &amp; async operations within the IDE itself. It also brings a new ASP.NET project type which is not backwards compatible, they have restructured the way you create WebForms/MVC projects which is a lot nicer and more modular. The user interface look is a lot nicer IMO.
What do you mean better? Does merge without file conflicts work yet? Visual Studio still stores references to each and every file in the project so when more than 1 dev works on a project at the same time we get conflicts when we add files to the project. Is that solved? 
Cool, thanks for clearing that up for me. Edit: don't down-vote things that are wrong, especially when you offer an answer or solution. That will make it so other people can't learn from other's mistakes.
There's one thing that really sucks with 2013: They moved the productivity power tools into the IDE but forgot one important detail: there's no more 'complete statement' support for auto brace completion. In 2012 with the productivity power tools installed, you could type: if( and the ) was automatically added and the cursor was placed in between them, _and_ when you were done typing the if expression you'd press cntrl-enter and the cursor would complete the line, moving the caret to the next line. In 2013 this doesn't work, so you always have to manually move to the end and complete the line, e.g. add the ; manually. This sucks if you type a method call: foo( in 2012, the ) was added automatically, you press cntrl-enter and the ; is added as well, caret to the next line, continue typing. In 2013, not so much. The ) is added, but you have to manually skip it to add the ;. I have reported this to MS, hopefully they'll fix it in update 2.
I'm gonna go out on a limb here and say that the changes to running and working with unit tests(mstest anyway) in VS2012/13 have been [horrible](http://visualstudio.uservoice.com/forums/121579-visual-studio/suggestions/5126061-test-explorer-is-severely-limited). 
That and the ability to break out TFS windows and have work items viewable next to changeset view next to pending changes. Those 2 changes made me never look back.
It's part of the productivity power tools, so it wasn't in 2012, they moved those features into 2013 (except this detail). I don't use coderush / resharper as they suck with respect to large solutions/code bases and I don't like slow IDEs. They're fine for smaller solutions, but after a point they start analyzing things at moments you can't use it, and I realized I don't need resharper/coderush to write code. besides, features which are in 2012+a free addon from the VSNET team should be in 2013 if the free addon is added to 2013, right?
Is anybody else less than impressed with the web grid implementations in MVC? I'm hoping this gets addressed sooner rather than later. They can be a pain to customize when trying to add web controls to things like the header. Also, I guess this would be a good time to ask if anybody has stumbled upon one they really like. We currently use WebGrid. 
Lol thanks for the proof.
I would love to, but most of the apps are not really even near feature parity with their desktop counterparts, so I don't, even though I really like "metro" (or whatever I'm supposed to cal it), even in the desktop. I know that part of the UI concept is to get rid of the bloat but most WinStore apps simply lack basic features. 
If merge worked at all in Visual Studio - I would be impressed. 
You're wrong, likely because you don't realize that the 30 character limit on identifiers causes large organizations to use ridiculous abbreviations, even if the resulting identifier would be much less than 30 characters, so that the abbreviations are consistent throughout the database. So we have to have a data dictionary for abbreviation lookups, which we do have in our ORM, but we had to roll our ORM ourselves. One of the problems with the data dictionary for the abbreviations is that one abbreviation could stand for many variations of a word, like VER could stand for Verify, Verified and Verification. It takes a human to decide which one to use. Here is an example of an abbreviated identifier that we have: "CRTKR_RLTN_TYP_CDE" which could be "CaretakerRelationshipTypeCode" if oracle stored CamelCase identifiers or "CARETAKER_RELATIONSHIP_TYPE_CODE" if it supported SQL-99 standard long identifiers of 128 characters. I'm not stupid, I'm not arguing for the sake of arguing. A real problem exists.
No you're wrong for not stating that was your issue in the first place you ass. 
I don't remember saying I had a problem in my initial post. I just asked a question.
Try to get away with the singer/songwritter way and get a nickle for everytime function x gets called.
That would be awesome
Could you elaborate on the MVC project changes you mentioned?
I've been using VS 2012. I'm used to all the shouting already. :)
Get modern mix from stardock
Did you check if it's in the the Productivity Power Tools for 2013? The only moved some (the most popular?) features of the Productivity Power Tools to 2012 to the IDE it self. http://visualstudiogallery.msdn.microsoft.com/dbcb8670-889e-4a54-a226-a48a15e4cace 
"Controls" aren't really a thing in MVC; that's a WebForms artifact. While WebGrid does exist, it's not something Microsoft really cares about (last commit around April 2012), which makes sense as it isn't "the MVC way". If you need a grid, you build it using plain old Javascript and HTML. If you want a ton of features out of the box, you just use a library. The best one I've found is [datatables](http://datatables.net/). It has an absolutely huge amount of functionality out of the box, and writing plugins is pretty straightforward too. There are [a bunch of others](http://flarnie.com/2013/javascript/best-javascript-grids/) out there too.
I mostly work on enterprisey Winforms apps that are top heavy with filters on database tables with hundreds of columns (google 'your company's app'). Lots of lovely dynamic SQL in other words. Part of the output of every query is the actual SQL that was executed as an OUT parameter, and I usually log the SQL to the log table for good measure. Has saved much wailing and gnashing of teeth.
Sure, keep apologising. 
Yeah you have a problem and its name is retardation.
Thanks for your mature response.
Thanks for your mature post.
There is a way to get rid of those you know.
Automatic logging of any and all unhandled exceptions and anything else that seems vaguely error-like to my ticket-tracking system. It didn't take me long at all to get very bored with finding out that a crash bug had been there for months and no-one had bothered reporting it. Or had reported it as "yeah, that crashes sometimes when I do $RANDOM_TOTALLY_UNCONNECTED_THING".
The apps are on the same domain. I have no possibility of modifying B in any way. My end users are not domain users though 
That's pretty gosh-darn neat :)
Any reason? How about if you already have another MVC framework?
I use the Microsoft enterprise exception and logging for that. It's really good. Takes care of everything for you. 
&gt;edit and continue works in x64 Since not being able to use edit and continue in VS2012, IMO, not having it is better than having it. 
I think it depends massively on where you are. And what kind of project you are doing. I would charge less for doing some small scale back office application in Middlesborough than I would for working on an enterprise banking application in The City in London. OP, what kind of project, and where, were you thinking? 
yes I did, the one I mentioned, auto brace completion, is now part of vs.net, which did the autocomplete statement as well (otherwise the completion is just annoying). In 2012 the powertools add a command to vs.net which is bound to cntr-enter. In 2013 this command isn't added, so the functionality isn't there. 
How do you do the background check? This could be useful to me and I never knew it was an option. 
Which country?
Ok, then mine might be a bit different as I'm in the UK. I work for government on enterprise solutions. I get between 425 a day and 500 a day. That's for a senior software developer. I know some more junior developers who get between 300 and 400 and a few that do very specialised developers who can get up to 700 a day. Is that roughly the same as the US?
Wahhh. Keep crying. 
I should mention, I also manage the team. 
Thanks. How sad.
&gt;History SQL triggers. *Use sparingly*. Many a database has been wrecked by abusing triggered auditing like that. It is not practical for many situations. 
Yea, you're not alone in feeling that was a downgrade
I don't really see the big deal here. If you have a back-button bound, I've always "peeked" by hitting F12 followed by Back (Ctrl--) 
It's incredibly stupid that this functionality isn't supplied out of the box
Yes, I usually check the suggestion anyway though. I've gotten quite good at line because of seeing at thinking of different ways yo write code.
That's an awful lot of work to accomplish less than what click once already does for you right out of the box.
Thanks, looks pretty easy. Somehow it never occurred to me that this would be an option. Unfortunately we can't force our users to restart the app because they might be using it for something that shouldn't be interrupted and I know from experience that many of them just won't do it, even if told they have to.
I will quickly become a fan of bad TDD if this becomes popular...one line functions all over the place, WOOHOO!
From a US standpoint, I've always liked expected salary / 1000 per hour as a starting point (i.e. 100,000 a year =&gt; 100 per hour), and then vary up or down by whatever factors you feel come into play.
That's a good rule, but I'm wondering how common it is to pay a Senior .Net Developer $100+ per hour on long term contracts
Are you using named parameters? I'd be interested in how you have this setup. 
Yes, you're right. But it's so simple and convenient. I just like it. The other downside to f12 is that it opens the tab and an hour later you have 30 tabs open.
If you have another *server-side* MVC framework like Nancy then by all means keep using that. But don't confuse a client-side MVC framework like Angular or Ember with a back end MVC. They're compliments to each other, not alternatives. The model, view and controller layers are completely separate from your JavaScript MVC layers.
That would be a steal for a legitimate senior developer. More realistic would be between $150 and $300 an hour. Contracting out a huge project that's going to take lots of development time is usually a bad idea. If you're going to need a lot of development plus continued support, you'll probably get both better results and save money in the end hiring some one on to do it in house. You'll have some one committed to the project that ideally understands the business needs intimately. You won't get that with contractors. They will be far less likely to make the mental investment in why you want to do things the way you do vs your own employee who has a better chance of understanding what you want to do and the best way to do it. Contractors just do what you tell them, even if it's dumb. Your own employee would ideally tell you it's dumb and give you suggestions on how to meet your needs better.
Since you're an independent contractor and are not getting any kind of benefits, they should be paying through the nose if you're a crucial part of the team. Contractors have expensive hourly rates specifically because they're not paying for any of that other stuff. If it's a government contract though, there's almost certainly a fixed budget for the position and depending on the government body, it's possibly unrealistically low. If you're really so indispensable, they probably can't afford to pay you what you're worth so you'll have to choose to either take what they can offer or spend your time elsewhere working for people who will pay what you're worth.
From what I remember of my sparse experience using it, ClickOnce wraps the application so that it does a version check and automagically updates if you've pushed an updated version when someone goes to run it. Once they get the .Net runtime installed on their client machine, updates become painless.
The government, man. For some reason they have decided that it is better to use contracted developers than employees for projects that will take years to complete and then keep them around during the maintenance period and then use them to update or convert it to another platform years later. There are contractors on my project that have been here for over 15 years. The part that sucks about my current situation is that they have trouble with contract renewal. They changed the way they do it last year. My project manager's contract ended in May last year and they couldn't get it him back in on his new contract until the end of September. He was out of work that whole time and my team was without a project manager, who is the one who knows everything about the application we are converting from PowerBuilder to MVC. The government makes some really stupid decisions. What would you define as a *real* senior developer? Edit: I don't think I work like a regular contractor. Possibly because my current job is the first one I've had as a contractor. I always care about why they are doing things the way they are and think about the big picture. If I think something is stupid, I tell them. I don't think I've ever just blindly done what they tell me to do without finding out the reason first. I have definitely seen that type of behavior in the other contractors though. They just do whatever the client says without disputing stupid requests or offering a better alternative. This has caused an otherwise clean and organized application with very good code reuse to become riddled with little pieces of code and functionality that are just hacked in to other widgets and modules by developers that don't fully understand the code they are modifying. 
Contrary to what you will hear, you **don't** need a Resharper license. The only thing you really need is nuget, and that's included in the 2012 express versions. On the other hand, there is a ton of great plugins (web essentials!) that make it reasonable to hand over the 500 bucks for a pro license. And no, you don't have to take every update. 
That's correct but it only does this on application startup. Sometimes we have to push breaking changes in the middle of the day that requires the app be updated with it. It would be nice to have this as be as undisruptive as possible. We also have users that keep the app running for weeks at a time without restarting, even though we tell them to shut it down at the end of the day. These people then bitch that their 3 versions out of date application is having problems.
I feel you. I work indirectly for the government (via a private employer) so I'm intimately familiar with how mind boggling stupid the decisions made by committee can be. If you're being underpaid and you have the option, your best be might be to go elsewhere. There's usually not a lot of room for negotiations with the government. It's just "here's what we'll pay you" and the only option is to take it or leave it. As for what makes a senior developer... In terms of contractors, some one calling themselves that should be able to handle any situation they're dropped into or at least tell you why it's impossible and what alternatives exist. We've had bad luck with contractors claiming to be senior developers when in reality they're not very capable at all. You don't want to see some hacky bullshit when you've been paying for some one who's supposed to be better at whatever it is than you are.
Something along these lines: http://msdn.microsoft.com/en-us/library/ms180975(v=vs.90).aspx
Pretty much nothing that great from the web side of things. For instance, MVC 5, WebAPI 2, Razor 3, etc. all work on 2012 with the [update here](http://blogs.msdn.com/b/webdev/archive/2013/11/18/announcing-release-of-asp-net-and-web-tools-2013-1-for-visual-studio-2012.aspx). The only cool stuff that you miss out on are the browser link feature, page inspector, and... Actually I think that's it. Could be forgetting something though. Both are cool features but it depends on what you're building, your environment, etc. as to how useful they would be to you. Oh, if you use TFS, as snarfy mentions, it's improved. That said my team moved off of it 5 years ago and it would take a lot to get us back. So I can't speak as to how much of an improvement it is, I've just heard positive feedback thus far. So just that and the edit and continue support.
You can't use plugins like [NCrunch](http://www.ncrunch.net/) and [Resharper](http://www.jetbrains.com/resharper/) with Express editions. We have a few NCrunch licenses at work and, if I had a Professional version of VS, I would drop money on it. Also, one installation and config for anything you're going to develop, as opposed to 4 separate ones (Web, Desktop, Modern, Phone) with 4 separate configurations.
Some people like having a legit license :)
I know, that's why I said to delete it if he doesn't want to keep it. If there is a trial version, he should go that route.
There's usually a 30 or 90 day trial of the Pro version.
Try https://github.com/Squirrel/Squirrel.Windows if you want a simpler alternative to ClickOnce. 
I've had an MSDN sub for years (I'm sure it never used to be so expensive) and it's not so bad to renew. But yeah, no way I'd get one now. Sucks if you don't qualify for one of the discount programs. 
It's actually incredibly useful when you need to rebuild/maintain that small, one-off NET 1.1 site on your intranet that no one has the source to anymore. And they don't need any extra functionality, so there is no point in using something like ASP.NET MVC. 
We just converted all of our projects over to this. As some of the comments point out, the workflow is great if building from the IDE as NuGet is integrated there. For build scripts and continuous integration environments the workflow changes though. We will typically have the build server actually run a NuGet task which handles package restore before firing off our build scripts (Powershell using PSake, PSake gets restored by previous step first). Some of the comments point out that if you want to run those PSake scripts locally as well as on the build server, that you have to split the build script: which is exactly what we do anyway. We have a bootstrap.ps1 file that sets up and disposes shares, etc., that are needed for the actual PSake build script (build.ps1), which it will invoke. This allows us, if we want, to run NuGet in that script first before invoking other dependent build operations... Though our projects are simple enough that we don't require local build events like that, so we don't *really* use it for that.
I have a couple of points to make. If you are syncing data between machine A and machine B then DateTime.Now is dangerous. Use DateTime.UtcNow is safer. Time zones... Better yet, if you want to schedule things then you should use Quartz.net. If you really want to handle dates and times properly in .net then use NodaTime. The project is run by Jon Skeet and that speaks volumes. http://noda-time.blogspot.de/2011/08/what-wrong-with-datetime-anyway.html
Tips hat to ServiceStack for the REST API.
Newtonsoft is the [JSON.NET](http://james.newtonking.com/json) libraries 
&gt;Why would I choose Web Pages over regular .Net MVC? If you're just doing something very simple, you don't need MVC. We used it for our own company site as it's pretty much static, just with the odd bit of functionality tossed in.
I'm saying there is no point. Why would you split up your application in just another server side framework, when you're already splitting up in your client side? For requests to the server side you would not use a MVC framework. You'd rather use a web service like ASP.NET (which is not related to MVC). I am well aware that you need a server side framework to get your pages to the browser. That's the whole point, you can use WebPages for this, what the thread opener asked for. There is no need for the additional MVC overhead on the server side.
Could you explain further? I have a newtonsoft.json file in another folder that was in the gdata api .tar file. Would I have to set up this file first before executing the make install instruction?
I mostly work on non-ASP.NET stuff, and ELMAH'd also be a bit heavyweight for a lot of the individual stuff I do. I'm mostly using my own lightweight library that plugs into the unhandled exception handlers (or can be called manually) that does basic information gathering with optional query-the-user-for-more-data and then auto-writes it to my [FogBugz](http://www.fogcreek.com/fogbugz/) instance. Over time, I've enhanced that with attribute-based configuration on the entry assembly (to minimize both hard-coding needed elsewhere and user-visible settings - limited to just one switch in the .config file for enable/disable) and a couple of PostSharp aspects for adding extra instrumentation when necessary.
* Raygun - exception logging * NewRelic - application monitoring * Serilog - general logging Not sure how well some of those would fly if your "enterprise" level apps are running on an internal network.
Its a library, you need to download the newtonsoft json dll that corresponds to the version of .net you're targeting and put it in your bin directory (for asp.net projects) or add it as a resource for non-web based projects.
Is that the /usr/bin/ folder? I tried that one and I unfortunately got the same error. Where can I add it as a resource?
Good to know that Enable Nuget Package Restore is so 2013.
The way I've done it before is automatically check ever 15 min or so and notify the user somehow with like a banner or put it in the window title that an update is available.
No the website's bin directory. If it didn't exist, add it.
If you're using Visual Studio you can use the NuGet package manager to add the necessary DLLs to your project. If not, you need to download the DLLs and include them in your project's BIN folder in order to compile.
This is amazing!! 
I found that genuinely interesting. Some good ideas in there. It's not something I would use on a daily basis, but I'll store it in my bag of tricks. Thanks for posting.
I ran across another [similar project](https://github.com/HeadspringLabs/Enumeration) the other day. I haven't had a chance to try it yet, though.
Yes, love MS Enterprise Library!
If you weren't such a disgusting person on here, I would leave you alone. I surely hope that you aren't like this in real life.
When it comes to display in MVC our enum values end up being keys in resx files for localized strings.
Why? For spelling out failings of Visual Studio? You can't take it crybaby? WAHHHHH.
Here is a generic helper class for something similar I've done. This also has an attribute for decoration of the name and also tries to assume the name if no attribute is defined. http://pastebin.com/EXq9Jt4Z
Reason not to get it: It's not as easy to pirate... since it's subscription based.
I was wondering about this a little bit. As far as I understand, you get to keep the licenses for things you've already purchased after your MSDN subscription expires. At least that's what they told our manager at my last job. I wonder if this is only because Microsoft really couldn't enforce you to uninstall the software. But now they could know that your Microsoft account no longer has a valid MSDN subscription and deactivate VS. I wonder if this policy will change.
Have you tried turning on tracing? http://msdn.microsoft.com/en-us/library/aa738639(v=vs.110).aspx
Sure, bake reflection into your everyday enum usage. Great call.
This is a cool idea. I actually have a project that will benefit from this, I'll try it out.
While the smart thing would be to cache the reflected values somewhere, it's not gonna be a performance hit unless it's being called millions of times.
This is great. It was one of the *few* reasons I stayed away from Azure. On any large S/W project, it always seems like I need a few Windows service-type applications running to handle some business. If these can be done without dedicating an entire instance, it's a huge win. 
Presumably you're storing this value in a db somewhere-- why wouldn't you have a "lookup table" attached to the enum value? So you can have additional fields (like description) and also maintain db referential integrity? Otherwise it's a good idea, keeping in mind the reflection performance considerations. 
I personally don't find it horrible... but certainly a step back.
http://blogs.msdn.com/b/endpoint/archive/2010/11/01/wcf-webhttp-service-returns-http-415-unsupported-media-type.aspx Specify WebServiceHostFactory in your svc file: &lt;%@ ServiceHost Language="C#" Debug="true" Service="Webhooks.Webhooks" CodeBehind="Webhooks.svc.cs" Factory="System.ServiceModel.Activation.WebServiceHostFactory" %&gt; I tried this out on a sample and I was able to capture the json via a stream. [ServiceContract] public interface IWebhooks { [OperationContract] [WebInvoke( UriTemplate = "AcceptNotification", BodyStyle=WebMessageBodyStyle.Bare, RequestFormat=WebMessageFormat.Json, ResponseFormat=WebMessageFormat.Json)] string AcceptNotification(Stream stream); } [ServiceBehavior(IncludeExceptionDetailInFaults = true)] public class Webhooks : IWebhooks { public string AcceptNotification(System.IO.Stream stream) { using (var reader = new StreamReader(stream)) { string data = reader.ReadToEnd(); return data; } } } &lt;system.serviceModel&gt; &lt;services&gt; &lt;service name="Webhooks"&gt; &lt;endpoint address="" behaviorConfiguration="Web" binding="webHttpBinding" contract="WcfRestRaw.IWebhooks"&gt; &lt;/endpoint&gt; &lt;/service&gt; &lt;/services&gt; &lt;behaviors&gt; &lt;endpointBehaviors&gt; &lt;behavior name="Web"&gt; &lt;webHttp/&gt; &lt;/behavior&gt; &lt;/endpointBehaviors&gt; &lt;serviceBehaviors&gt; &lt;behavior&gt; &lt;!-- To avoid disclosing metadata information, set the value below to false and remove the metadata endpoint above before deployment --&gt; &lt;serviceMetadata httpGetEnabled="true"/&gt; &lt;!-- To receive exception details in faults for debugging purposes, set the value below to true. Set to false before deployment to avoid disclosing exception information --&gt; &lt;serviceDebug includeExceptionDetailInFaults="true"/&gt; &lt;/behavior&gt; &lt;/serviceBehaviors&gt; &lt;/behaviors&gt; &lt;serviceHostingEnvironment multipleSiteBindingsEnabled="true" /&gt; &lt;/system.serviceModel&gt; 
Not overly happy with them to be honest. Didnt do what I wanted them to do which is run code in a loop (non azure queue related). 
To be fair, Erlang is a great language for resilient service orientated task architecture. RabbitMQ is a good example of something that takes advantage of this. If you have a good actor paradigm, you can make each class in effect it's own light weight process. You can have options to re-play the task for something that is likely to have transient issues. It does make sense to write a lot of things in such a framework.
I get paid to do work, not play. Back in the day I would try so many new languages, now I stick to the same thing (c# and sql server) because it's what brings in the dog food. "waahhh you're letting yourself slip by not learning the new django in rubys webscale mongoloid!" No.
I'd suggest trying out F#. It has many similarities to Haskell, and is quite different from C#. At the same time, you can continue to use much of the stack you're comfortable with. If you want to step out completely, I'd suggest giving python a real go, especially given that you know you like it. You might enjoy Clojure as well. I don't know if you'll like any of these enough to leave the .net stack, but there's some great concepts in these languages that will help your mental flexibility when solving problems in c#.
You're absolutely right that Java lacks some of the nice syntactic-sugar of C#, but I've learned a tonne from working on Java web applications. Spring MVC really pushes users towards the pit of success in ways ASP.Net MVC doesn't: * forcing the use of dependency injection * thin controllers using service objects * default repositories (you'll wonder why EF doesn't have this when you see these) * Java config Decent editors like Eclipse or IntelliJ have a lot of shortcuts to auto-generate the Java boilerplate you'll have to write. And because C# and Java are so similar you'll be able to easily bring your learnings back to .Net And at the end of the day you'll realize how spoiled you are with .Net (C# is amazing, Razor views are still my favourite templating language, NuGet does one thing and does it well, etc.).
Smalltalk. Best language ever. Ever ever. Seriously, ever. Though because most implementations of it are image based (image as in machine image, not image as in jpeg) and not file based the infrastructure of deploying and so forth can be difficult to get your head around. Pharo is my goto Smalltalk implementation and Seaside is the defacto web-stack in Smalltalk. Thoroughly recommend it.
Since you are server side and would like to toy with the power of Java I would say give Scala a try. If you want to try some of the latest and greatest pull in Akka or Netflix's Zuul frameworks and try a different methodology for looking at highly scaleable application stacks.
I love F# so much more than C#, it's not even funny.
One issue I find with this (using an iframe to their site) is that they can inject ads or malicious scripts into your site. Another issue with this is that it relies on the availability of the service. Should the service go down or scripts lost on the server, or free-turn-premium issue arises (ie: LogMeIn), your blog's code examples will disappear. In my opinion, this kind of dependency is not worth the risk of losing what people look for in your site, including possible future employers.
Which .net language do you code in and what do you like about smalltalk over it?
[Owin and Katana](http://msdn.microsoft.com/en-us/magazine/dn451439.aspx)
C# is my breadwinner. I like the simplicity of smalltalk, and the paradigm of message passing instead of method calling. The syntax is just beautifully simple. Only 7 reserved words, and *everything* is an object. Blocks/Closures are also integral to smalltalk and feel much more at home there than they do in any other language. The Smalltalk community is great, too.
My largest gripe with Java isn't the lack of sugar necessarily, it's the lack of expressive programming features like LINQ's expression's. It's so easy for me to do clever tricks with C# like taking a property expression to avoid typos in strings when I need to do reflection magic. Ultimately, Java is designed to be clear, not concise, and while it's a benefit it also means more experienced developers can't write more concise code. Still, I like not being bound to hosting my applications on a Windows Server box with a SQL Server database behind it (yes, yes, I *can* use another database with .net, but it's painful compared to SQL Server).
This provides the reassurance I need right now
I feel like the people who avoid .net and use free frameworks are cheap, poor or ideologically against MS.
Scala is amazing and will blow your mind. Going back to C# afterwards will be painful. But the Scala ecosystem is absolutely terrible. So I agree with other posters that F# is a good next step for someone accustomed to the .NET framework.
Java
I like Javascript and Angular. Equally, not better.
There is a huge world outside of .net. Quite often the .net world is lagging behind by several years, though not so much now. Turning a blond eye to the tools and techniques used beyond the ms stack can make your .net apps much much better. 
As others have said, if you want to dabble in the java world then scala is a great language. Even java has a couple of features I miss in c#.
I do Android development (essentially java) while being a .Net developer as my day job. Mobile development is definitely tougher, simply because things move so fast. Entire libraries get deprecated at the drop of a hat, and you're constantly running into the weirdest most obscure bugs. Android is definitely more "wild west" than the other mobile platforms out there, while .Net is well established and polished and doesn't follow the fast pace that consumer mobile development does. I enjoy it because it's a change of pace, it keeps me on edge having to keep up with all the new UI and design patterns. Every new OS update includes new features as well as breaking changes. It also makes me appreciate .Net more, not a coding session goes by where I realize something I take for granted in C# doesn't exist in Java.
I know I am going to have to do a lot of looping but I was wondering if maybe there was a class in the .Net framework that would help make it a little easier. 
This is trivially simple, there's no need for a helper class. Split your start and end ip into 4 octets then do 4 nested loops like so: for (byte octet1 = startOctet1; octet1 &lt;= endOctet1; octet1++) for (byte octet2 = startOctet2; octet2 &lt;= endOctet2; octet2++) for (byte octet3 = startOctet3; octet3 &lt;= endOctet3; octet3++) for (byte octet4 = startOctet4; octet4 &lt;= endOctet4; octet4++) Ping(string.Format("{0}.{1}.{2}.{3}", octet1, octet2, octet3, octet4); Actually, that might not be what you consider the whole range. Say you enter 10.1.1.1 and 10.1.2.2 as the start and end ranges. The approach above would ping 10.1.1.1, 10.1.1.2, 10.1.2.1 and 10.1.2.2. Another way to interpret it would be to ping 10.1.1.1, 10.1.1.2, 10.1.1.3, [...], 10.1.1.254, 10.1.1.255, 10.1.2.1, 10.1.2.2. The latter interprets all 4 octets as defining a single number vs my example that treats them as individual numbers. But this should be enough to get you started I would think.
I think the ecosystem is fantastic since you have access to the massive java OSS projects. The big complaint for Scala is that the tooling when compared to java or c# is terrible. However, I just deal with it the same way many OSS developers do: don't use an IDE.
Boooooooring. Why'd you even bother to post anything?
I was thinking I would have to do a couple of nested for loops. I should be able to figure the rest out. Thank you. 
Keep hitting the snooze button
A typical response from c# and java enterprise developers. It's really just an excuse to not make the effort to learn anything new. There are plenty of well paying jobs using more interesting languages. Often they'll pay better than enterprise gigs. It's good for your career to try new languages even if you never use them on the job. The different perspective you can gain is great for critical thinking required by the profession. Never stop learning. But if all you're looking for in your job is to pay the bills then ignore everything I just said.
&gt; But if all you're looking for in your job is to pay the bills then ignore everything I just said. Have career. Will do.
Does anyone have experience with Dart? I've been curious.
I haven't touched smalltalk since 05 and it wasn't a pleasant experience back then. In the class we used Squeak and it crashed so often it hurt. I'm surprised to see it still around and assume it's gotten a lot better.
Why would anybody choose to not use an IDE?
I don't have an android phone, but I do like them. The Android emulators are too slow for my taste. One day I'll get an android phone and mess around with it some more. I've done my share of SharePoint development and I wouldn't wish that on my worst enemy. Why have you made it such a pain in the ass, Microsoft? I love Kentico CMS though.
Yes, the Eclipse emulator is horribly slow. But download the one from [Genymotion](http://www.genymotion.com). It's free for personal use, is *incredibly* fast, and plugs into Eclipse seamlessly. Before giving up on Android, I recommend you try out the Genymotion emulator...along with the complete 'ADT Bundle' (which also is much faster than the standalone Eclipse installation). Working with both these components have made a *huge* difference for me and my Android Dev hobby (like you I'm a C# guy by trade).
Is it totally inconceivable to think that some people prefer open source tech like Python and ruby over .net on its merits rather than your reasons?
Totally agree. I have a successful career/job but learning new languages and technology allows me to tackle problems with a better perspective. It's like the old saying, for a hammer, everything looks like a nail. ATM I'm looking at Go, Node.js and learning Linux.
To each their own but PHP turns my stomach, reminds me of asp classic. Hate the syntax, I'd take any language over PHP except VB. 
I don't think this sentiment has anything to do with C# or Java per se. It has everything to do with your attitude as a developer. It's one thing to stick with one language because you really love working in that language, it's another to stick to one language because you don't feel comfortable trying anything new, or because you just get sick of learning/starting fresh again. I'm a C# dev by day, but I write code in a number of other languages in my spare time. I haven't written a project in C# in ages, but I have in F#, Python, Ruby, Scala, Erlang, Elixir, Clojure, and more recently, Rust. I find that the tools and lessons you learn in other languages can be easily brought back and applied to your day to day coding, which not only makes you a better programmer in the long run, but also expands your skillset for when the time comes that your bread and butter language starts to fall by the wayside. Each language I've tried has taught me something new that I highly doubt I would've learned had I stuck with C# this whole time. That said, I love C# as a language, and I hope it continues to improve and grow. It's still one of the most productive languages I've worked with.
ASP.NET is not related to MVC?? It's called ASP.NET MVC! Personally, I use MVC and razor to do initial page setup (most cases it's faster) and then JS takes over with ajax calls. I'd consider a Angular/Ember/Knockout framework on the frontend. There's no reason not to mix the 2, ASP.NET MVC is pretty front-end agnostic. I'd avoid WebPages tbh, just a dead-end crutch like Lightswitch.
I don't think you are "letting yourself slip" by not learning whatever is being hyped this year, but I do think your attitude about learning new languages is toxic, and has a tendency to bleed in to your interactions with people who *are* learning new things and want to talk about it, which poisons their enthusiasm. Being a curmudgeon about it doesn't make you a worse programmer, but it does make you a person I personally wouldn't want to work with. I value devs who push themselves a little in their spare time, even if it's only an hour a week. It's too bad that you have such blatant disrespect for the concept.
So if you write a project in C++, you are cheap, poor, or ideologically against Microsoft? I have to assume you're trolling.
So what about Erlang? A language and platform that powers a large amount of critical telecommunications software in the world, powers Facebook chat, parts of Heroku, RabbitMQ, Riak, and more, yet it is free. It's been around longer than .NET. Would you say it's only for startups or haters? I mean this in the nicest way possible, but you need to broaden your understanding of what powers a lot of the software you use and rely on day to day. On Windows a lot is written in C# these days I have to assume, but a large amount is probably also written in C++ or Java. It's a bit foolish to generalize anything *not* .NET as essentially useless bullshit with no real value. You don't use a hammer to satisfy all your home improvement needs, occasionally other tools are better suited to the job.
#####&amp;#009; ######&amp;#009; ####&amp;#009; *Here's a bit from linked Wikipedia article about* [***Endianness***](http://en.wikipedia.org/wiki/Endianness) : --- &gt; &gt;In computing, memory commonly stores binary data by organizing it in 8-bit units (bytes). When a data word uses multiple such units, the order of storing these bytes into memory becomes important. The terms **endian** and **endianness**, refer to how bytes of a data word are ordered within memory. &gt;Each byte of memory is associated with an index, called its address, which indicates its position. Bytes of a single data word (such as a 32 bit integer datatype) are generally stored in consecutive memory addresses (a 32 bit int needs 4 such locations). **Big-endian** systems are systems in which the most significant byte of the word is stored in the smallest address given and the least significant byte is stored in the largest. In contrast, **little endian** systems are those in which the least significant byte is stored in the smallest address. Say the data word was "0A 0B 0C 0D" (a set of 4 bytes) and memory addresses starting at a with offsets 0, 1, 2 and 3 are given. Then, in big e ... `(Truncated at 1000 characters)` --- [^(**Picture**)](http://i.imgur.com/0iZ6TLb.png) [^(image source)](http://commons.wikimedia.org/wiki/File:Big-Endian.svg) ^| [^(about)](http://www.reddit.com/r/autowikibot/wiki/index) ^| *^(/u/azurite_dragon can reply with 'delete'. Will also delete if comment's score is -1 or less.)* ^| ^(**Summon**: wikibot, what is something?) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=bot%20glitch&amp;message=%0Acontext:http://www.reddit.com/r/dotnet/comments/1w2dcq/simple_ping_utility_help/ceyckro)
You can view an IPv4 address as 4 octets, or you can view it as one int. The .NET IPAddress class actually has a constructor that creates an address from a long integer in big-endian format. This means you can do a simple for loop between two numbers, swap the endianness (the IPAddress class provides a handy method for this), create your IP address and ping it.
A good programmer can produce good code in any language. It sounds like you only tried php3 the new versions are quite good. 
Sharepoint developer checking in! Please do try it and cry haha. It feels like the part of the .net and visual studio they forgot about, not really smooth sailing alot of the time but once you learn it's alright and you'll be well payed and I do love it and you get to use Asp.net, c#, JavaScript, jQuery, Ajax, powershell, html and what not. 
Has using it made you a better c# coder?
Thanks for bringing up these issues. 1. We take all the security precautions to prevent people from injecting scripts, etc... If you find a loophole, please report it to us using contact information on the site to collect a reward. 2. We are using Azure cloud with auto-scale. There is always a possibility of site going down, like Gmail being down recently, but we are taking all the steps to limit the downtime. You are right that there is always a risk with using 3rd party web service or software. 
This. 
You must be kidding me. I've been a .NET developer for 10 years and I constantly feel inadequate and strapped for time to learn the advancements in .NET. The stack is so vast and fast developing that it's a full time job just trying to keep up. I think we're forgetting that there are "developers" and then there are "developers". Someone who knows his way around c# syntax will claim that they are a .NET developer. Then there is the guy who understands 3-tier architecture inside and out, which requires intimate knowledge of presentation layers (WPF/ASP.NET etc.), WCF, and the entity framework (esp in disconnected scenarios). Some knowledge of design patters, injection frameworks, etc. Hell even when we mention WCF some guys will say they know it because they created a WCF project in VS and it "just worked". Others will get into request/response patters with DTOs, mapping from entities to DTOs, cross cutting concerns, domain driven design, tdd, etc. bottom line you can get your "lack of effort" right out of here. I've been doing this for 10 years on a single stack and still feel inadequate because there are so many people that can run circles around me at my own profession. Oh and I've been a "senior" developer for years.
Because like the original poster of this chain stated, it puts food on my table. There are substantially more 6 figure salaries out there looking for an enterprise level developer on a particular stack than a "handyman" developer that has some level of experience on often unrelated stacks. Bottom line an enterprise will often pick a technology stack and stick to it, therefor being proficient at it will make you more desirable to them. You might thinks it's a terrible decision to stick to a particular stack because MS or IBM wrote it, but it's exactly how the majority of enterprises run. I know because I've worked for those companies for years.
Are we talking ASP.NET the framework or ASP.NET webforms? They're 2 different things. ASP.NET is a great barebones HTTP framework that has barely changed since it was developed. MVC 1-5 were built on it and are extremely capable and wonderful systems. WebForms, is an abomination that should be abandoned with all available haste...
Being in /r/dotnet, I'm not overly surprised that the prevailing opinion follows /u/wizardsfirstrule. At the same time I honestly expected to see more people like myself, making money off of .NET, but passionate about another language for personal projects. Either the majority here *have* no personal projects, or it's just rare for people to leave their comfort zone. Either way it's unfortunate.
&gt; but I do think your attitude about learning new languages is toxic, and has a tendency to bleed in to your interactions with people who are learning new things and want to talk about it, which poisons their enthusiasm. There are multiple reasons I don't talk with teenagers, or students about computers. There is an exception with my one friend who is doing his masters in math and computer science who loves functional languages and tries a new one out every once and awhile--until he found the one that worked and stuck with it. In the company I work for, you wouldn't be employed. "lol let's mysql up the python", no, you don't get to mysql up the python or ruby up the jvm with the node.js. You get to use .NET and SQL Server. We don't pay for licenses on our 156 instances to use mysql on a project.
&gt; There are multiple reasons I don't talk with teenagers, or students about computers. That's nice. Who said anything about teenagers or students? So you work for a company that has invested heavily in Microsoft. That's great. Am I supposed to be impressed that you are locked to that platform for all software development then? Despite what you may believe, expensive licensing doesn't imply that your software runs any better. I find it hilarious you think I would be unemployable at your company, when my career has been entirely driven by .NET - just because I believe that there are equally well designed platforms available doesn't mean I feel the need to pay tribute to the one I make my money with. Since apparently you feel that 156 servers is a significant thing, I'll give you just a single counter-example: Facebook. The vast majority of Facebook's data is stored in MySQL. Tens of thousands of servers. I don't even use MySQL, and never have, but your ignorance regarding even that one piece of software is rather potent. I feel pretty bad for your dev team.
Immutability by default and the actor model make for unreasonably easy concurrency; builder blocks (monads in academic parlance) mean you can define your own domain-specific languages and they will play nice with the rest of the code base. For example, the domain-specific language for actors (the "Async builder") makes for fast and easy design of concurrent programs, something that sounds like an oxymoron to any veteran imperative programmer. More generally, functional programming (which I wish was called "verb-oriented programming") significantly reduces the impedance between the conceptual representation of the problem and its implementation in code; as a result, code closely mirrors the problem space, most design decisions are obvious *a posteriori*, fewer comments are needed, et cetera. The single biggest bonus switching from C# to F# is the union type. If you have previously programmed with union types, you know exactly how much of a game-changer they are. They are immediately applicable to basically any problem space, they are easy to pick up, and their potential is immediately obvious. For the uninitiated, the generic binary tree data structure is an example of an application of union types: type 'a BinaryTree = Leaf of 'a | Branch of 'a BinaryTree * 'a BinaryTree This means "every object of type `'a BinaryTree` is either a `Leaf` containing a value of type `'a` or a `Branch` containing two more objects of type `'a BinaryTree.`" So, for example, an `int BinaryTree` is a binary tree of integer values. (There are other ways to define binary trees, for example where branches carry values instead of leaves, and they are all as easy to define and as readable.)
I would hazard to guess that php runs a helluva lot more of high throughput sites than any kind of .net stack. Php5 is much closer to typical OO concepts you're probably more amiable to. Php MVC frameworks are available too. I haven't personally written much php, but I give it the respect it deserves.
the V in MVC can be a json view, or any other representation of your resource (like HTML). you can therefore map a REST API directly onto the MVC paradigm.
I'm not entirely sure what you're after. But we use [MiniProfiler](http://miniprofiler.com) with great success. It's free, stays out of the way, can show you the SQL running and how long it took (if you can hook your SQL connection into it), and easily profile specific areas of your application. 
Generally speaking, it's a good idea to benchmark your app, so you have a baseline to compare to when issues arise but most people don't do this. But you should at least be logging and measuring as many things as you can within your application. I would try using one (or all) of the following and see if they can help pinpoint what your issue is: [MiniProfiler](http://miniprofiler.com/) [Glimpse](http://getglimpse.com/) [ELMAH](http://code.google.com/p/elmah/) [Sentry](https://github.com/getsentry)
I know this a subreddit for programming advice but from a business/career advice thread lightly on this one. If you start taxing this app to hard it's going to be your head when they track back the client doing it.
Man, that guy works with some really bad developers :-/
Since you are familiar with Java, here is how things match up on the .NET side: CLR = JVM .NET Framework = Java Runtime Environment ASP.NET, ASP.NET MVC, etc. = Play, Grails, Spring MVC, etc. C#, F# and VB.NET = Java, Scala, Clojure, etc. If you are getting into a job that requires ASP.NET, you will basically need to know the language (C# or VB.NET most likely) and the web framework (ASP.NET or ASP.NET MVC) at minimum. You will need to know about the .NET Framework to a certain extent too. For someone familiar with Java (or another OO language), learning C# won't be too difficult. A week or two at most should be enough. It takes longer to learn the frameworks IMO. Since you are new to Java, it might take a little bit longer for you but there are a ton of resources online. Here are a few of my go to links: http://channel9.msdn.com/Series/C-Sharp-Fundamentals-Development-for-Absolute-Beginners http://tekpub.com/blogs/tekpub-free-bin/tagged/asp-net-mvc-concepts http://www.asp.net/get-started Search through this subreddit and /r/csharp. There have been other threads posted that have more information/recommendations. Stack Overflow is also a good resource.
You would be surprised at how many of these actually happen on a daily basis at a lot of companies.
The third point around unused libraries is why I always suggest starting ASP.NET MVC websites with the "empty" template, and only adding extra packages if you need them. That way you don't have random stuff you'll never use And the last one around commit messages reminds me of Crockford's commit messages, things like [warn](https://github.com/douglascrockford/JSLint/commit/85ea659195bab5a277ca096911b2eae6dccf062c), [For Kyle](https://github.com/douglascrockford/JSON-js/commit/40f3377a631eaedeec877379f9cb338046cac0e0), [[\d+]](https://github.com/douglascrockford/JSON-js/commit/7a18252d0ec1b46b26afb933f65434dc1f7844f5) and [filter.first.first.first.first](https://github.com/douglascrockford/JSLint/commit/f574c2d07994fca591b9260c78278ee5814c5ffb)
Hire on a really tight budget. You become aware just how many developers are exactly like this. I'm really struggling at the moment because I need to hire about 2 devs, but have barely enough money to fund one. *sigh*.
He may not have a choice, but yeah, if at all possible go for MVC.
Hire one decent one, he will be better than two beginners or two that just aren't that good.
Not free but Red gate ANTS Performance profiler is really awesome. 
File &gt; New Project...
&gt; I would be unemployable at your company, You are. Also, I wrote instances, not servers. That could mean a minimum of physical 6 servers (following microsofts support statistics of 25 fail over instance per cluster).
Sheeple. We're sheeple.
I was talking about web frameworks mainly. As far as I know, C++ isn't a server side scripting language commonly used the way C# and VB are for ASP.net. If you say you're writing websites in C++, then I have to assume you're trolling.
*You're not your job. You're not how much money you have in the bank. You're not the car you drive. You're not the contents of your wallet. You're not your fucking khakis. You're the all-singing, all-dancing crap of the world.* 
I like movie quotes too. See if you know this one: "Fuck you. That's my name. You know why, mister? You drove a Hyundai to get here. I drove an eighty-thousand dollar BMW. THAT'S my name. And your name is you're wanting. You can't play in the man's game... And you know what you'll be saying - a bunch of losers sittin' around in a bar. 'Oh yeah. I used to be a [developer]. It's a tough racket.' These are the new [languages]. These are the [open source dev tools]. And to you they're gold, and you don't get them. Why? Because to give them to you is just throwing them away. They're for [hobbyists]. I'd wish you good luck but you wouldn't know what to do with it if you got it [visual studio]." 
They're fads until one sticks. I am sure the same people have said the same things about C# when it was in it's infancy.
Ah, I thought you were just trying to be funny with the sheeple comment. I was just playing along with the FC quote. Meant no offense.
I spend a large portion of my day helping other developers and I still get my work done. I have them watch everything I do and explain to them or ask them to explain to me why I'm doing what I'm doing. I hope it helps them learn. I don't really mind doing it because I like to help people, but these guys are supposed to be on the same level as me as Senior Developers. They probably get paid the same too. Everywhere I've worked it's been like this. It's like I'm the only one capable of figuring things out and remembering what I did to figure it out. I don't understand.
Exactly. We can see *what* was done by comparing the code. We need to know *why* it was done, with some short reference as to what was done to tie it to the reason.
Get on Pluralsight.com
I don't know why my experience would be considered anecdotal. Sure it might not represent the entire spectrum of experiences but it's not in any way "false". At the end of the day I am not here to prove anything to anyone, so take it for what it is, my $0.02.
I use a custom nuget repository hosted on a dev server to distribute the custom MVC framework I wrote to all the projects that use it. With Nuget 2.7+, Visual Studio will (almost reliably) automatically download the missing packages on each developer's machine after their team lead has applied the update and checked in the solution and the developers get the latest version. This works better than having them all reference a project that I may be causing breaking changes in while I add new functionality or debug. When it's ready, I just pack it and push it to the sever and let the team leads know there is an update. This also is a good solution for the fact that we have 4 tiers of environments, Development, Testing, Acceptance, and Production. Development is actually on our own machines. But we have separate servers for the other environments. We also have a requirement to modularize our projects by section, so we create a nuget package for each section. When we move changes to our Test environment, we package up our modules and push them to our nuget server and then update then on the Test solution. Once a project moves from Test to the other tiers, we don't update any packages on it. This process does a good job at keeping developers from fixing bugs in anything but the development environment. It also facilitates an easy way to keep an archive of all versions of a project's modules. Note: we always make both a debug and release version of our packages before pushing them to Test. That way we can switch between the two if we need more debug information. The packages will otherwise be identical and we will have the proper release version for production.
In addition to what these other folks have suggested, get Red Gate's Reflector so you can disassemble the framework assemblies and study how the Microsoft developers do things. This has been very valuable to for me to help me expand on what the framework itself provides.
Start diving in and playing around. That is by far the best way to learn. MSDN and Google are your best friend too. C# is very similar to Java, but with more features (seriously, read up on async/await and lambdas). If you need to know a specific class name just google "C# &lt;javaname&gt;". ASP.NET is a web server framework with a lot of modules. Chances you should be looking at MVC and WebApi.
I always try and do these things, i'm going to pass this list around the team since it's great documentation to add to our "done".
Much appreciated! The scrapes are limited in nature (30-40 records per instance of use, and instances of use are low in frequency), and will be artificially slowed to reduce burst stress. However, your concern is absolutely a legitimate one and I will definitely keep this in mind if I'm asked to build a larger version.
Point 9 is very often caused by ReSharper auto-importing libraries, and not by people doing something stupid- Albeit they may be lazy for not checking that ReSharper imported the *right way*, that can be easily overlooked
Because the language arguments are tired. The problems with java vs c# are so obvious. Properties for one. I hate passing a getSomething everywhere. All the (casting) (is (absurd)). The namespace standards are com.goofy. .Net doesn't ask to install ask Jeeves or some bullshit every time it updates. You like python? Well python can't even be updated successfully. You using 3.x yet 3 or 4 years after it came out? Is django up to date? Do you still need to run commands to generate files for ROR? I apologize for offending anyone. I'm opinionated and I exaggerate sometimes.
There is no "standard" way of handling content management in ASP.NET, unless you build a CMS in it. If you want people to be able to change pages via an admin page, you'd have to build that functionality, or use an OTS package that already provides a CMS. As for whether you should build your own or use someone else's solution, that really depends on a lot of factors- how complex the feature set is, how close to your needs an OTS package gets, etc.
Might also be dicy but maybe to talk to the other developers working on the system you're trying to work with. Sometimes a bottom up approach can overcome the egos of management.
Thanks for your comment. I was afraid of that, although it does make sense. I guess I was hoping there would be more out of the box solutions available for a newbie like myself. Thanks none the less though!
Getting one of the open source CMSs setup will give you a world of experience. I've built custom CMSs and used Umbarco and Orchard. Realistically use something already built for you and learn how it does things if you're just learning though will teach you a lot (though mostly about the CMSs). 
It seems that you are comparing apples and oranges. WordPress is built using PHP, and umbraco is built using ASP.NET. PHP doesn't have any built in CMS functionality either. Depending how your site is built it may be rather easy implementing it using umbraco.
Worked for me. They have great tutorials for using Visual Studio, MVC, Web Forms, C#, Visual Basic, whatever you need. Their beginner videos start assuming zero knowledge of the topic so they're a great place to start. Only drawback is that it isn't free.
hmm.... that is actually up to debate isn't it? for simple one purpose application it's better to use Web Forms IMO. Easier thus faster.
Using Umbraco.
There is a difference between language wars, and debating the merits of each. If you can't have a conversation about them without reverting to the former, then yeah, that is tired. I agree about Java for the most part, but I don't code in Java, so I can't speak too well about the specifics. The one thing Java does right though is the JVM, and there are some great languages without the verbosity of Java available (Scala, Clojure, etc.). I like Python the language, but I agree, the failure of the community there to move to 3 is truly unfortunate - that said, the community there *is* moving to 3 slowly but surely, there are just a lot of legacy libraries that are incredibly useful that keep people on 2.7+. &gt; Do you still need to run commands to generate files for [...] That's the standard method of working with many languages and platforms. The benefit of having a command line interface is that an IDE can easily integrate with that to do scaffolding/builds for you, which is what VS does under the covers as well. Personally I prefer living in the command line, but I know a lot of Java/C# devs have an unusual distaste for that due to the tendency to rely entirely on the IDE/GUI tools (just to be clear, I'm a C# dev going back 8 years, so I'm not hating on anyone here, it's just an observation). &gt; I'm opinionated and I exaggerate sometimes. It's all good man, I'm pretty opinionated myself, so no worries.
Thanks, I might just do that even if I have to start a new "play" project to test it out and learn. Do you have any experience implementing one of those open source CMSs into an already existing website? What's your opinion on trying that?
yes but a lot of the time you can copy paste large chunks of it. Especially once you figure out how the MVC pattern fits into said CMS. 
Been there and done that. Had a customer who couldn't understand why our app was performing poorly on their system. They really wanted up-to-the minute reporting so they put insert/update triggers on a table that was being updated well over 1mm times per hour.
For anything that exposes an API, logging all request parameters, headers, etc. along with the generated response XML/JSON.
in a content management system, the content is the "model" from MVC, and the templates are the "views". if you want CMS-like capabilities in your MVC app, you will need to find a way to incorporate your CMS content into your view model. if you've already got a model, ie you have an ecommerce site displaying products or whatever, this could get tricky. is that your situation?
Really depends on how the old site was set up, if content from the old site was static in HTML files or stored in a DB. I'm not sure on Orchard but Umbarco can be pretty straight forward, or if it was in a db you could move everything via SQL.
It's hard to answer without knowing more about your site but how would you use WordPress on an existing site? Typically you will install it to a sub domain or a sub directory. It will be similar with Orchard, Umbraco et al. It's obviously not going to be a clear cut solution but it should work. Doing a cursory search shows that it has been done before: https://stackoverflow.com/questions/5269170/integrate-existing-asp-net-mvc-application-with-orchard-cms http://n2cms.com/Documentation/Content-enabling%20an%20existing%20site.aspx
I think part of the issue (especially with the .NET world but Java too) is that you have a lot of people that don't have a passion for what they do. For them it's just a job. A way to make comfortable money. So they have no incentive/motivation to make themselves better or to learn new things. They are satisfied with what they know and as long as it pays the bills, they don't need to do anything extra. Not to say there aren't exceptions but it seems like the majority of people fall into this category. 
Most of the content is static in the views, but a small portion of it is pulled from the db. I wonder, with something like Umbraco, are you able to limit its control to only certain parts of the views? For example, if I only wanted to manage a few paragraph blocks from within Umbraco instead of the whole page.
We do have models for some of the stuff, but most of what we want to control with an open source CMS isn't from the models, it's static stuff inside the views. 
Yes, but given the quality of the material and the ease with which you can find the right video about the right topic makes it well worth the price. Also, when you use this in a professional context, employers are usually happy to pay for a subscription, as it's way cheaper than sending employees to classroom training.
I've noticed that too. I don't understand it. I love programming and I always want to learn new things. 
Second this. Pluralsight has a ton of high-quality content. I use it regularly and it's been a huge help. If you can afford it --or better yet can get your place of employment to cover it--then go for the Annual Plus plan. I found the exercise files and offline viewing to be useful. If you can't swing that, no worries, most of the value can be had by watching closely and working along on your own development machine. Good luck and don't give up; there's a ton of good paying work to be had with these skills :)
MVC is a development Framework for making any type of web application while CMSs are specific type of application used for managing content. You could build a CMS with MVC. That said, there are likely some packages you could add to add CMS functionality to an MVC app. Try this article: http://stackoverflow.com/questions/656455/building-a-cms-in-asp-net-mvc or google "building CMS in MVC". 
*Here is the text of the [accepted answer](http://stackoverflow.com/questions/656455/656460#656460) to the [question](http://www.stackoverflow.com/questions/656455/building-a-cms-in-asp-net-mvc) linked above, by user [Rex M](http://stackoverflow.com/users/67):* --- &gt;[N2](http://n2cms.com/) does what you describe - "bolts on" to existing &gt; ASP.NET solutions (including MVC). &gt; &gt; --- ^[about.StackBot](http://www.github.com/gabrieldain/StackBot) ^| ^(downvote to remove)
Is DotNetNuke no longer loved? http://www.dnnsoftware.com/Community/Download Just a thought. It's open source at least, though I can't speak to it compared to the other suggestions you included.
Maybe is better to leave it in the past.
The hardest thing for me in learning ASP.NET was figuring out the quirks of the IDE and .NET. C# is pretty straight forward (and there are tons of resources for it) and you may not need to know that much C# to do things like CRUD apps. I'd suggest taking a week long intro class or even a semester-long community college class if you can find one taught be an experienced instructor (I was fortunate to have that). Then when you're building basic apps and run into problems, you have someone to walk you through, to do demonstrations, etc... of particular issues. Good luck!
Did you try doing a http get first? Sometimes sites pass an anti forgery or view state token with the request. The client has to pass this back in order for the server to continue. 
I don't mean to sound disparaging, but if you couldn't think of a way to generate a range of IP addresses then you probably need to back to some basic programming. With that being said, /u/sweeneypng's answer would probably be the ideal/preferred method.
For a "trivial simple" solution this is a lot more complex than necessary.
Yep, I do a GET to the login page initially in order to populate my VIEWSTATE etc from the hidden input attributes that ASP.NET serves with the page, then after I've read those and taken their values, I add them into the POST as data. In the pastebin you'll see that both POSTs have the exact same request data, including those tokens... I wonder if it's a missing part of the header I can't see with Fiddler, or maybe a secret AJAX postback that Fiddler isn't picking up?
Please link the original article next time. http://taskmatics.com/blog/simplifying-producer-consumer-processing-with-tpl-dataflow-structures/ 
Not disparaging at all, any criticism is welcome that will help me get better and learn how to do things better. I haven't looked into what /u/sweeneypng said but that's what I plan on doing. 
If all you're doing is running Visual Studio Express and building Forms/WPF/WCF/MVC applications... I can't imagine a cheap enough modern laptop that wouldn't suffice. If you plan on doing any DirectX... then a dedicated graphics card would be helpful.
Eh, Visual Studio isn't exactly light weight. I don't think I could handle using it on a $400 laptop. I have a low end Thinkpad T-430s, and I would consider it near the minimum. It gets things done, but VS can take a few seconds to catch up sometimes.
What's the minimum RAM? Is 8GB sufficient?
I believe the minimum requirements are 1.6GHZ and 1GB of Ram.
Yes. My laptop has 4, and I don't think memory is the primary issue. I could be wrong, but I think VS is mostly CPU bound.
Minimum required to function and minimum required to be productive are very different things.
I would insist on an SSD. Minimum 250gb. 
Core i5, something with an ssd, faster compilation times with ssd. 8gb is enough don't go below that. Windows 8.1 is fine and faster than Win7 especially in booting + ssd for faster speeds. I would go ultrabook. Optical drives in computers are obsolete. Thin and light. Don't go to cheap, you want to have balance. You want it to last you 5 years. $600 to $800. You want it to be thin and light that does matter. 
Clavis: 1. doesn't rely on MVC, and can be incrementally integrated into existing web forms projects. 2. is simpler to use. 3. is more secure since it solves clickjacking and CSRF because it addresses the core insecurity of query strings carrying authorizations. (model binding doesn't address this at all that I can see) 4. is less extensible than model binding because it deals only with query string data, ie. not POST data. 
SSDs are so worth the extra cost. Couldn't agree more.
Just grasping at straws, but that [__EVENTVALIDATION](http://msdn.microsoft.com/en-us/magazine/cc163512.aspx#S3) field is apparently to prevent tampering with page elements, particularly adding unexpected fields to postbacks/callbacks. It looks like you're sending two copies of the "DynamicMenuUserControl%24Menu=" in your second request. I wonder if this is causing the validation to fail? (I would expect the second to replace the first, but who knows...)
Oh, I forgot about another important point: 1. Clavis provides taint-checking, ie. client-mutable data is distinguished by the type checker, so you know when you are using potentially unsafe data.
Heh taint 
An unfortunate term choice for sure, but it's the standard!
SSD Master Race
In my experience, VS is mostly I/O bound. I'll always take a faster hard drive over a faster CPU.
[This _might_ work](http://en.m.wikipedia.org/wiki/File:IBM_Blue_Gene_P_supercomputer.jpg)
RAM RAM RAM. Get as much as you can.
Reasons for SSD specifically? I mean they're cool, but why would you insist on it?
VS is a 32 bit process. I never understand people screaming about RAM, assuming you're not doing anything too intensive 4 gigs is fine. SSD is huge though.
Vs may take less than 4gb, but I usually run sql studio, chrome, notepad++, multiple instances of chrome, etc. so I'd recommend 8gb. Anything more is a waste IMO.
I bought a Lenovo carbon on Scott hanselman's recommendation. It's pricey but a dream, ultra fast, light, touchscreen, good battery.
yeh. but for a single purpose app it's not really worth the extra effort to use MVC IMO. 
I've been lusting after the x1 carbon for some time now. I only wish it supported 16GB ram.
ASP.net is a platform that runs on top of IIS. It comes in 2 different flavors, WebForms (old) and MVC (new). You'll need to learn the .net library and a .net language such as C#. You'll be able to read c# immediately coming from Java but the .net library is different from java's standard library. To learn: Go to Www.asp.net Download nerd dinner and open the project in VS. Run it and fool around with it. Tbh though, you can write 60% of your app in JavaScript just having the server handle data access, validation and serving views.
Good comments but don't forget sql studio profiler (comes with sql client tools) and Chrome f12 tools or equivalent. My guess would be sql server though, it's typically the bottleneck.
An ssd, real GPU (nvidia, etc), 12GB+ ram, and decent screen resolution (1080p+). I recommend getting something off of [powernotebooks](http://www.powernotebooks.com/) or [exoticpc](http://www.xoticpc.com/). It's also interesting to learn [who really makes laptops](http://www.xoticpc.com/laptop-manufacturers-really-makes-laptops-ip-11.html). That dell is probably a re-branded clevo, and you've probably never heard of [clevo](http://www.clevo.com.tw/en/index.asp). 
Yes, you can choose deep or wide knowledge, depends on how you want to proceed in your career. I don't think there's a wrong answer. Personally I feel I'd be better off learning at least 4-5 languages (C#, Go, Js, maybe f# and Obj-C) such as one that could be run on linux as opposed to learning the last 10-20% of the .net world. I don't like being tied to 1 single-vendor stack regardless of the quality/performance/reliability of it. 
Just get about 16GB+ of RAM. You might be able to get by with a little less but it's not worth it.
Yeah, that's a very good point. Nothing like waiting on disk i/o.
Possibly, it's been a while. I'd still be a tough sell vs. Visual studio/.net/C#/mvc IMO.
It seems you're right that php trails asp.net in terms of being used in a lot of high traffic sites. Although many other languages are used in even higher traffic website on average. This report was based on w3techs survey data. http://blog.websitesframeworks.com/2013/03/programming-language-statistics-in-server-side-161/#hversus
In Umbarco you can create your master pages then put in either Marcos are populate from other content types from your data layer.
The first laptop I've been happy with for full-time dev had 256GB SSD, 24GB RAM, quad-core i7 cpu and dedicated graphics. As for minimum requirements, that just depends on how patient you are. I was also running several instances of VS, MSSQL Server, multiple IIS sites, some VMs, etc. all at the same time so you may not need that much if you're doing lighter work.
I've run every VS2005-2013 on XP, 7, and 8.1 over the years on this aging workstation with 4 GB of slow FBRAM and a slow-ass hard drive (but a decent GPU) with no problems. You can get a better laptop for $2-300 these days. And by the way, that's in addition to SQL Server 2008, IIS, Fiddler, a couple instances of Adobe Reader, Chrome with 20 tabs, ... If swapping is an issue, it's really not enough to notice. I don't know what these "no less than 8GB!" people are smoking. Maybe they like to keep 4.5GB empty so the other bytes have room to stretch their legs. 
* 1gb for w3wp * 1gb for mssqls * 300mb for VS * 300mb x2 for two browsers testing and searching * 100mb for music or video With OS and random apps you quickly reach 4gb. Let's not even talk about Adobe apps... I personally won't accept anything less than 16gb for serious dev work. FWIW, I run at 12-14gb fairly consistently..
'minimum specs', well look on the box for Windows 7 (32bit) and you'll see: * CPU: 1Ghz * RAM: 1GB * HDD: 16GB But 'minimum' is pretty crap. * CPU: i5/i7 2nd Gen+ 2Ghz+ * RAM: 16GB * SSD: 250GB 3rd Gen+ ..that'll do you quite nicely as a core spec, and not run you too high a price tag either. I got a Lenovo W520 15" laptop ~3 years ago now that had these specs, and cost me under $1500. Newer generation CPUs and SSDs are faster again, and for around the same price (or less). 
I would have said this before I got my current laptop. I have an Alienware m18xR2 that came with 6GB that I planned to immediately upgrade to at least 16 GB since it was much cheaper to do so after purchase. A year later, I've never had a problem with RAM or speed. Perhaps the SSD makes swapping less painful, I'm not sure. It does have dedicated graphics as well.
I think 120gb ssd, 8gb ram, core i5 would be okay on the low end.
Funny, I was thinking the same thing and I'm the one who wrote the summary.
Compiling especially involves a lot of disk thrashing. An SSD can bring a 5 minute build down to 20 seconds. On top of that, everything loads almost instantly and the entire OS is more responsive. When I started at my job, I figured out they were paying me 6k a year to watch spinning circles. Investing 400 on a high quality SSD is a no brainier.
I like umbraco as its so easy to use. Just wish there wasn't frequent bugs and they deleted old documentation that is no longer current.
You guys have convinced me. I've been on the edge about adding a SSD to my computer and putting my OS and tools on it. Sounds like a good investment after all.
It's the best investment you can make - do your research, get a quality one.
Seems like lots of good ones with good reviews on newegg...
DNN and SiteFinity have no business being on that list. I'd probably add Sitecore for enterprise (shitty UI, but fantastic API) and Orchard instead of DNN.
I've had the best overall experience with N2CMS. 
There is no right or wrong answer to this. Whatever pays your bills and makes you happy. As much as I enjoy programming, I would rather be golfing. The money is why I go to work.
Whoosh
 &gt;In the company I work for, you wouldn't be employed. Your company sounds like a terrible place to work.
~~Top~~ 5 ASP.NET Based CMS (Content Management System) for Web Developers
Interesting, talk about right in the middle! Maybe we can agree that it's not the language but the architect/programmer? I imagine a lot of ASP.NET sites are behind corporate firewalls as well (intranets). I'll also hazard a guess as why PHP is such an outlier: WordPress behind some many blogs running on PHP. Would also explain why it's on so many low traffic sites.
Both are very popular and widely used... They might not be the most popular among developers, but they are certainly primary dotnet CMSs. 
Even for .Net development, I still believe the MacBook Pro is the best development laptop you can buy (although some may call me crazy). The screen real estate is sublime, and I actually find that running Windows on VM increases my productivity since I can have two workspaces running at once. I'll usually have my e-mail, chat, stackoverflow, etc running on OSX, and then with a three finger swipe I'm over to my Windows dev environment - free from distractions. Additionally, it's great being able to do testing on an Ipad emulator and native safari pointed to my localhost. Also, one more note because I was in the market recently: A similarly spec'ed Ultrabook will run you the same price as a Macbook Pro.
I know there are few issues with DNN but no CMS is perfect in all the ways. DNN is powering more websites that any other .NET CMS out there and as it there for a long time, it has matured as a development platform with good community support and lot of options for skins and plugins.
Bottom line, Open Source = not allowed
I'm currently using a Lenovo T430s 64 bit 8 gig ram and the SSD drive for .net and java development and it works great.
I have a T430s. It's great. It's tough, fairly compact and works with VMware (I think it should even allow pass-through for my AV programming projects). My preferred development environment is to build a Windows image on VMware Workstation (with a stripped down and secured minimal Linux host), install Visual Studio &amp; SQL Server and whatever else I need, update everything, update everything, update everything and then back up the image to a (USB 3.0 external drive). If I get a virus or anything else, I can just save my data (I always have my project directory backed up already), delete the image and copy it over in less than five minutes. At minimum, I'd get a machine with i5, USB 3.0, 500 GB drive.
My distaste for it probably stems from the fact that my very first project using SharePoint was for 3 public facing websites that the client wanted to look nice and not at all like a default SharePoint site. Microsoft has not made it easy customize the markup of anything and querying data is a pain in the ass. Simply making it so they could have a list of News articles on the site was a pain in the ass. Shortly after finishing those sites I discovered the joy that is Kentico CMS. Everything that was more difficult to do in SharePoint than in plain Asp.NET was actually easier than plain Asp.Net in Kentico. I would have to be paid $200k+ per year to be required to develop for SharePoint all the time.
I've tried a few times to get to PHP, but it's too much like classic ASP and the -&gt; syntax is retarded. It has always seemed like a disorganized mess to me compared to .Net and C#.
how orchard isn't top 5 and some of these are is beyond me.
Yet to use it, so I can't comment :)
No mention of Ektron? (hahahahha, kidding)
Php with symfony 2 is very much like mvc 4 and not like classic asp at all. The syntax is much like java. I dont really care about the small details, php gets jobs done and it is 100% up to the programmer to write good code not the language. The usage of php is huge and it is a good thing to know at many companies. 
If you are multitasking 8gb is just fine, consider 16gb only if you're going to have VMs (plus a decent CPU for virtualization).
Did you know [1060 NuGet packages depend on JsonNet](http://nugetmusthaves.com/Dependencies/Newtonsoft.Json)? 
I would also recommend MVC, web forms tend to root bad practices to new developers (i've seen applications using postback for every action and saving every variable on a Session instead of ViewState). And the resources over http://www.asp.net/mvc are very good to get you started.
What do you use for vm?
Exactly, I used DNN, mojoPortal and Orchard, and it's by far the best of the 3 and the one I stuck with.
I'm going to sound stupid, but I don't care. This site just led me to [Glimpse](http://getglimpse.com/) and I pretty sure it just made my life significantly better. Sometimes it's both inspiring and depressing at just how good other folks are at programming.
Nice find, going to dig into Glimpse a bit more later. JsonNet is utilized so often because it's something that the core framework should handle out of the box; it's not to say it's the greatest or that it should be used that widely. While I've certainly made use of it over the years, it's a pretty sizable DLL and is overkill if you just need a fast lightweight serializer. That said, memory at that scale is typically not a large concern in most shops.
&gt; the core framework should handle out of the box if I remember correctly there are at least 2 ways to read json with the core .net framework. I don't know of the capabilities though.
I may need to take a look at this.
Are you using an DI for MVC? I had a similar problem with castle where the action invoker was not invoking async.
Hmm... we use MiniProfiler in a few apps. This actually looks like it would be easier and more useful. May have to look at ripping it out and replacing it now.
A lot of sites used DNN as it was the best option a decade ago but it is no longer the best option (or even in the top 5 for that matter). I would much rather write my own or extend Orchard, Umbraco, N2 before I would even look at DNN.
Wow...it's been a long time since I've seen/heard about mojoPortal. Anyone use it recently? What are your opinions? 
I have *heard* of Ektron.
I believe with Json.NET 5.0, there was a huge emphasis placed on performance, so things should be better. [ServiceStack.Text](https://github.com/ServiceStack/ServiceStack.Text/) is also known for performance. 
There's nothing in the BCL. The Ajax extensions they hastily threw together were somewhat embarrassing and are the primary reason that you see all of the alternatives around today.
I think you're looking at the categories: http://www.nugetmusthaves.com/Category/Entity%20Framework The reverse dependencies are here: http://www.nugetmusthaves.com/Dependencies/EntityFramework Does that clear things up?
yep :)
I disagree. When you commit the packages to your repository, you always have them together with your code. With this automatic restore you add a dependency to the NuGet server. And when these servers are offline or have problems (happens now and then), you're stuck and your code is useless.
Ektron would be solid I'd they just stopped new development and clean up for about six months.
idk why people use this syntax $('#&lt;%=gv_dataGrid.ClientID %&gt;') instead of $("[id$=gv_dataGrid]"). If you're using jQuery and ASP.Net take advantage of the selectors. Edit: If you're not using jQuery you can use something like: document.querySelector("[id$=gv_dataGrid]").
One thing to remember about Umbraco - you don't need to upgrade. Security patches are released for existing sites and upgrades are really for new features only. They did a post last year that showed only a faction of existing sites are regularly updated and it is an acceptable practice.
I'd say get the best laptop you can afford with 16GB RAM and preferably with a 256GB SSD.
We've had a lot of success using [pdfLib](http://www.pdflib.com/). The API is not the easiest to learn but it's by far the most powerful library. If you don't need as many options you could check out [UltimatePDF](http://www.componentpro.com/pdf.net/) as well, we've used that and the API is much higher level. 
I've used abcpdf in the past. I liked it.
If you're looking to generate a massive number of PDF files [ in the thousands per minute ], I would recommend the [Apache FOP Project] ( http://xmlgraphics.apache.org/fop/ )
Have used iTextSharp in one or two commercial products. The licensing is a winner. I don't know if these are still the present and correct terms but from memory it's something like: If you're going to use it in a project without paying a license, you may do so as long as you post your implementation source code (+any modifications to the itext packages) publicly (AGPL). If you're not in a position to make your source public, or you just don't want to, you can buy a private license fee - i think it's a few thousand per site? There's a wiki on it http://en.wikipedia.org/wiki/IText (there's a brief overview on the licensing) and the open source project page where you can download it is http://sourceforge.net/projects/itextsharp/ edit: I believe this is the company page http://itextpdf.com/purchase - would get in contact with them to discuss licensing costs. 
Wrong tool for the job. Using a message queue will give you transactionality, redundancy, scalability and fault tolerance that TPL won't. 
It depends on what exactly you are looking to do. I am mostly generating different reports and I have used http://www.evopdf.com/ with good success. You can basically use a view to generate the html then simply pass it into the library and it will pass out the pdf.
we use essential objects, its easy to work with, no problems here
I have used a combo of Altova's Stylevision and Apache FOP.
I like [PdfSharp](http://pdfsharp.net/). The API is much nicer than iTextSharp.
A few years ago we evaluated a number of libraries and chose ABCpdf by WebSupergoo. It did all we needed, wanted, and more and the price was quite reasonable.
I would of liked to seen an example of how to maybe format the mobile version data a bit better once it's uncollapsed for view like that...
If this is just for display and not for archive or saving server-side, I highly recommend using [jsPDF](http://parall.ax/products/jspdf)
As someone who currently has to use VSS at work, I couldn't agree more.
How do you do complex layouts with xsl-fo? I've looked into it a little bit but it seems really weak
I can vouch for itextsharp. I've used this at every company I've worked for. Fun tip: use the view engine to make simple pdfs with razor.
Anyone use Aspose?
+1 on ABCPDF I've been using ABCPDF on a number of projects. I've generated PDFs with hundreds of pages with it. Its ability to convert a web page to a PDF is very useful and literally saved me hours. I'm a big fan. On numerous occasions I've contacted their support and they have provided me with a solution every time. They even sent me a patched version of their software to fix a bug I reported. You can find more info on http://www.websupergoo.com/ PS. I don't work for them :)
Oz-Code is a quite useful debugging tool. I think it'll get popular soon. Check it out folks :)
You poor bastard.
I used it until, if I remember correctly, reread their licensing which was per application per server. Which for a desktop application was not good.
Pdfsharp/migradoc
Dataflow and Message Queueing are not mutually exclusive technologies. The example provided shows how the Dataflow library can save time and effort to manage asynchronous processing of a complex workflow. Dataflows can be used both to send to a message queue as well as process messages from one or more message queues. Scalability and fault tolerance of a complete order processing system are orthogonal to the point I'm making, which is that writing efficient multi-threaded asynchronous code is made a lot simpler with the Dataflow library.
Also, if you're writing a program for use internally by your company then you only have to make the source available to the internal company users who use the program and not the entire world - http://www.affero.org/oagf.html#How_does_this_license_treat_commercial That's how my company uses iTextSharp without paying for a license. Also - if we WERE going to let customers get PDFs created by iTextSharp - we would create a standalone service that simply does the DATA (xml, json, db) -&gt; PDF conversion (the last step), at which point we could just publish the code for our standalone service, not the entire business logic of the applications producing the data.
Is it a pure .NET assembly now or still a wrapper for a COM object? (I see they have a NuGet package now, so that's a nice improvement but it might only be a trial.)
Great article. From somebody who writes a lot of tests in C# and WebClient I see a lot of the same pain points.
## Debugging the whole system instead of using unit tests &gt; As you write more tests, youll notice that tests are actually valuable in many other ways  they help you express what your code does (or at least, what it ought to do), they serve as documentation for others, showing them how to use your code, and they help verify the SOLIDness of your design. WTF does that have to do with SOLID? Nothing. Absolutely nothing. And for that matter, debugging and unit tests are orthogonal concepts. Tests just tell you something is wrong, debugging is what tells you how to fix it.
First time I've seen it, your comment made me go check it out. It looks like it could save a ton of time... installing it now.
Testing also gives you a very nice, repeatable way to debug.
I don't agree with the first point. If it's possible to recreate the scenario in a test environment (or locally), you should be able to step through the code and figure out where the code is failing with that particular input. THEN you can write a unit test to make sure whatever is failing is fixed correctly.
Exactly, and a way to prove what you think is a causing a bug is actually causing it. Reproducing failure conditions is important, even if they are hard to replicate. That's what mocks and dependency injection is for
Absolutely, most people I see criticizing mocks don't seem to realize that thay can simulate failures as well.
No idea what else taint means, now I'm slightly scared to google it.
It was never loved, it was widely despised. 
1 and 2 are the worst. I've seen so mamy places woth elaborate "standard environment" setups to try and fix this instead of doing it properly in the first place. One wretched place I was at for a short time even insisted that creating and developing on virtual machines was the best way to handle it.
* ReSharper * StyleCop * Wix * Fiddler (HTTP debugging) * Snoop (WPF tool) * BeyondCompare * dotPeek * CloudBerry Explorer for Azure Blob Storage * SQL Server Management Studio * Tortoise SVN and VisualSVN Server * Notepad++ for quick edits * Agent Ransack is a handy file search tool 
I hope I'm not already going off topic, but I use [BitFactory.Logging a.k.a. Termite Logging library](http://dotnetlog.theobjectguy.com/) by http://www.theobjectguy.com . It has a VS plugin for viewing your logs while debugging, so it's a tool in that regard. It is an amazingly simple logging library that just works with plenty of logging options (Email, Socket, File, Custom). It's super-cheap too. $5 for non-commercial license, $49 for commercial. 
Google image search 
 - **LinqPad** - is the most useful programming tool I've ever used (except arguably VS) - **XamlSpy** - like firebug for xaml. invaluable for debugging visual issues in wpf/silverlight - **Notepad++** - my quick text editor of choice, although Sublime Text is nice too - **Launchy** - quick app/shortcut launcher. I install this on every computer I use, alt+space, 3-4 letters of the app i want to run, enter. - **Wireshark** and **Fiddler** - for debugging network related shit - **Nuget Package Explorer** - we use NuGet for our references, even to our other internal libraries. NPE allows us to create/update nupkgs. - I don't know if it counts as a programming tool, but someone mentioned it, so I'll concur, **Spotify**. Music all day every day, because the alternative is listening to my coworkers yap about random bullshit. I do have to say, I didn't personally like **ReSharper** when I tried it. It never really offered me any actual code improvements, just offered to change my types to vars and my vars back to types. And it tried to make my intellisense better, but only really seemed to slow it down.
Dear god!
Pencil Project for prototyping UI and flow charts. Trello for a task list amongst peers. Notepad++ for writing down random stuff. HandyNote Pro on Android for drawing doodles of what I'm trying to accomplish when I'm stuck. 
Linqpad, Resharper, NP++ VC is Subversion + Tortoise Oracle SQL Developer, SSMS Stackoverflow, AskTom Nuttin' special
* linqpad * notepad++ * ms sql studio * github * tfs * tfs powertools * azure storage explorer/cloudberry * sqlazuremigrationwizard * fiddler2 as a side not i'm starting to wean myself off resharper, it seems to get more and more clunky :\
I find [grepWin](http://code.google.com/p/grepwin/) useful for doing regex folder based search and replace operations.
We do a lot more than .Net at my shop so the tools we use aren't always explicitly tailored for what we're doing. That being said, we use Git for version control which plugs into this awesome tool developed by the fellows at Facebook called Phabricator. Phabricator does bug tracking, code browsing, wiki, project notes and communication, a note wall that works pretty good for scrums, and a bunch of other things I haven't used yet. It's also under constant development so it's always getting better. 
I legitimately couldn't work without ncrunch at this point (http://www.ncrunch.net/). It's become as important as resharper in my workflow.
Besides some of the ones mentioned already: * FogBugz * Paint.NET for the rare times when I have to pretend to be a graphics guy * RazorGenerator VS Extension. Even if you don't care about compiled views, it's awesome for compile-time checking them and finding all references, which otherwise doesn't work for views.
ReSharper, dotPeek, git GUI, Wireshark. Losing any of those would seriously affect my productivity.
A hammer or bat -- To hit myself [over the head with](http://www.youtube.com/watch?v=oq7qqldlNXA) before I open VS. *That was almost as fun as a vigorous manhandling.*
Git for source control with [SourceTree](http://www.sourcetreeapp.com/) as the GUI interface. I spent a fair amount of time learning the command line interface, but rarely go back now that I have a sane GUI.
yeah, my muscle memory likes launchy, plus you can have it index stuff other than the start menu, and you can create custom commands with the Runner plugin, which i use for stuff like running the occassional bat file.
Can you point to another Microsoft stack based CMS with a stronger community? 
PSPad http://www.pspad.com/
* ReSharper * Bugzilla * Mercurial * Node * http://jsfiddle.net * http://regexhero.net * Firefox+Firebug * dotPeek * Powershell * Sql Server Management Studio * Aqua Data Studio (basically a crappy version of ssms for our Sybase databases) * Notepad++ * Console2 * Pandora * several dozen tools I wrote myself (to do things like aggregate a query across all of our production databases or to automate a task on each production site)
I use beyondCompare every day, well worth the money
* SQL Server Management Studio * Fiddler * Chrome Developer Tools * Notepad ++ * Visio * TFS * JustTrace * NuGet 
Winmerge, Notepad++, and Linqpad, are big helps. I also use Strawberry Perl for data collection, data analysis, benchmarking, scripting tests, etc.
In addition to Chrome, Firefox (with FireBug) and IE 10/11 both also have outstanding developer tools baked in. 
CodeRush, ILSpy, NP++
Vim emulation, either viemu or vsvim. And regular vim on the side.
WOW! How come I never use LINQPad? It's awesome. Fuck it I'm going doge on this one: Such speed. Very simple. Much productivity. Wow.
* Debian in a virtual machine * Sublime Text * Git * Console2 * SQL Server Management Studio * Launchy * Wireshark * A lot of custom scripts for deploying and testing
http://www.urbandictionary.com/define.php?term=taint Definitely one of the most unfortunate technical terms, right up there with the 'fsck' and 'finger' commands.
*Here's the Urban Dictionary definition of* [***taint***](http://www.urbandictionary.com/define.php?term=taint) : --- &gt;The area between the nutsack and asshole that prevent a man from shitting on his nuts. See [durf](http://www.urbandictionary.com/define.php?term=durf). --- _If it wasn't for the taint, my nuts would reek of poo!_ --- [^(about)](http://www.reddit.com/r/autourbanbot/wiki/index) ^| [^(flag for glitch)](http://www.reddit.com/message/compose?to=/r/autourbanbot&amp;subject=bot%20glitch&amp;message=%0Acontext:http://www.reddit.com/r/dotnet/comments/1wayr2/clavis_rebooted_secure_typesafe_urls_for_aspnet/cf3rxz4) ^| ^(**Summon**: urbanbot, what is something?)
Resharper x1000. BeyondCompare is also an awesome tool to replace the default compare/merge options. TFS or Git, depending on what you're doing (my company forces us to use their TFS servers, but git for personal projects).
+ Fiddler, Expresso (regex IDE), GIMP, TeamCity + Octopus Deploy.
Firefox. For Google. Apart from that, ReSharper mostly. VSNomad for Phonegap dev.
Most of these are probably mentioned: SSMS, linq pad, Git/Stash, notepad++, leankit for scrum stories, sublime text for multi line edit, WCF test client, outlook, lync, beyond compare, SQL Server data tools, SSIS, analysis services, and pluralsight for training
RedGate Sql compare is fantastic for checking differences in environments and pushing them. http://www.red-gate.com/products/sql-development/sql-compare/ Reflector is awesome too especially when doing things like integration projects where you really want to see what's going on inside another product.
Resharper has improved performance steadily over the past 3 or so versions. It is also a tool that rewards the investment. The immediate payback is low, but once you invest into training yourself to rely on code gen, navigation, unit testing and static analysis, you'll realize that it really does help you code better, faster.
While I don't have lots of advice, definitely check out Json.NET. It is easily one of the best JSON .NET libraries out there. Particularly, the Newtonsoft.Json.Linq namespace. Using the JObject, JArray, and JProperty classes was about the easiest way to work with JSON that I found. Other solutions that I found while I was searching required too much overhead, such as designing a class specifically for each JSON object you would be using, which would have meant classes inside classes inside of classes for me.
I'm a little confused. This is my best guess, please confirm or correct where needed: You have a file on the server (a txt file? or what is it?) that contains JSON. You are passing this to a webpage and jquery is making some change to the json. You want to post this json back to the server and save it to the file? Is the json the only thing in the file?
I've actually decided to try and implement couchdb to resolve my problem. The idea of a json database is too appealing to pass up.
First... The attribute has a spelling error. "EnableThrotting". Second, some ISPs and CDNs act like proxies so that the requests from many people come from the same IP. This could cause those people to get randomly blocked (horrible). Third, why would you outright block the request? Throttling by definition is rate limiting not blocking. Start putting a delay in for each response rather than killing it. Forth, do you really want the possibility of accidentally rate limiting Google?
HTML tags are not "no more" in razor. In fact, besides helping make validation easier, you could just not use the helpers and write the markup yourself. The reason the helpers are used so often (especially in forms) is that MVC does a lot of its work by following a convention that could be accidentally messed up (field named matching model properties). It does take a little longer to learn since razor has traded explicitness for brevity but the front-end guy I work with has been picking it up well. Though to his credit he is a very smart guy.
Excluding ones already mentioned... Note I'm still on vs2010. Beyond Compare for comparing files (often when deploying) Productivity power tools vs add-on Vs10 codemap add-on (makes getting an overview of coffee and navigating it easier)
You've got it all wrong you and your designer buddy should watch the videos on http://asp.net
Agreed. My company has a few "brand sites"... Micro sites which show of a particular brand of products we sell. The content just about never changes and each site is less than 10 pages. Full MVC is overkill for such things so we used webpages. It actually make managing small sites much easier since there is less overhead.
Use a post controller action if you're in mvc or an httphandler. You can also use a soap service or a Wcf service. You literally have like 11 ways to do this. 
You still use html (lots of it) in proper razor markup just look at MS's own Scott Guthrie's post: http://weblogs.asp.net/scottgu/archive/2010/07/02/introducing-razor.aspx
I have done this a lot, convert the JSON object into string and post it to the server. This works!
Interesting. I believe there is a similar way to achieve this simply by configuring iis7.
Have you tried [this](http://fishcodelib.com/database.htm) for sybase? 
Interesting tool. I'm trying to figure if I want to try that or SublimeText first..
Most graphic designers will be working with the css which doesn't change. 
I've been using Pspad for more than a decade, so it's all about familiarity and fast loading times. I use Visual Studio as an IDE, but if I am ever "searching entire projects" for something, I use pspad's built in file search which is super quick.
http://www.iis.net/learn/manage/configuring-security/using-dynamic-ip-restrictions
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Unobtrusive JavaScript**](http://en.wikipedia.org/wiki/Unobtrusive%20JavaScript): --- &gt; &gt;**Unobtrusive JavaScript** is a general approach to the use of [JavaScript](http://en.wikipedia.org/wiki/JavaScript) in [web pages](http://en.wikipedia.org/wiki/Web_page). Though the term is not formally defined, its basic principles are generally understood to include: &gt; &gt; --- ^Interesting: [^Progressive ^enhancement](http://en.wikipedia.org/wiki/Progressive_enhancement) ^| [^Responsive ^web ^design](http://en.wikipedia.org/wiki/Responsive_web_design) ^| [^Ruby ^on ^Rails](http://en.wikipedia.org/wiki/Ruby_on_Rails) ^| [^Lightbox ^\(JavaScript)](http://en.wikipedia.org/wiki/Lightbox_\(JavaScript\)) *^\/u/b-nasty55 ^can ^reply ^with ^'delete'. ^Will ^delete ^on ^comment ^score ^of ^-1 ^or ^less.* ^| [^(FAQs)](http://www.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.reddit.com/r/autowikibot/wiki/modfaqs) ^| [^Magic ^Words](http://www.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/) ^| [^flag ^a ^glitch](http://www.reddit.com/message/compose?to=/r/autowikibot&amp;subject=Glitched comment report&amp;message=What seems wrong: (optional description goes here\)%0A%0A---%0A%0AReply no. 39699:%0Ahttp://www.reddit.com/r/dotnet/comments/1wpxff/newbie_dumb_questions_about_aspnet_mvc/cf4lsgu)
You absolutely can set whatever additional HTML attributes you like by passing an anonymous object with those properties. @Html.TextBoxFor(m =&gt; m.FirstName, new { onchange = "alert('you changed me')" }) You seem to be missing knowledge of the fundamental features of the MVC framework. I don't have a particular link on hand, but I would do some more reading.
The open standard has limitations, but there are proprietary xsl-fo implementations from commercial vendors with more features. If you have the budget, I would look into those offerings. I used XSL-FO to generate simple reports and invoices in PDF format. The primary problem was that the reports were 1k+ pages. I was able to generate large PDF reports with 1-2k pages in a few seconds, however, Java can be a memory hog and requires tweaking the Java JVM memory settings to get optimal performance. The server I used had 24 GB of RAM and the process would use up to 19-21 GB of memory. 
Love all this stuff, but ASP.NET Identity is still incredibly weak compared to Rails tools like Devise
An argument can be made that it isn't built to be a one stop shop for auth anymore, thus the focus on extensibility. Essentially it's meant to be a base for building an auth system. I kind of agree with that, though I still think there are a few things blatantly missing. We're sticking with custom membership providers for a bit yet... Plus using it as a vehicle for OWIN adoption means there's more to it than just dropping in a new auth system.
Maybe you shouldn't be doing what you are doing and instead do the data stuff in a database, and put the processed data in the excel sheet?
The processed data is from a database. There are over a million rows. Why anyone would want a million rows in an excel sheet is a different question. However, my particular end users are not the only ones that want a million rows. Once you have an end user that wants a worksheet with a million rows, you simply have to deal with the problem that an xlsx file is simply a zip file full of xml files.
* ReSharper * Tortoise SVN * Fiddler (HTTP debugging) * Snoop * BeyondCompare * SQL Server Management Studio * Notepad++ * miniDBA (for helping out sql performance)
Down-modders: You're not hurting my feelings here, but if you want a seriously capable platform with which to experiment with many other languages and tools, then this is a great one to use. Eclipse presents many, many options as well. It just so happens that the tool I use the most next to Visual Studio is IntelliJ. One would think this audience would get that because of ReSharper at least, but maybe that's getting missed here.
Singleton for beginners: don't. It introduces a host of race conditions, especially in web applications and makes testing a lot more difficult since all dependencies of a class is not documented by the constructor. You cannot know the dependencies of an implementation without reviewing the code.
It looks interesting but I cannot install it to see if it works (we have some strange columns in out databases, for example: NUMERIC(19,6)) which do not work in either the Mono Sybase driver or the DataDirect one. As far as we have tested, the only driver that works is the official one. Also we are using Sybase ASE 15.5; which is not listed under the supported databases. Further the pricing information doesn't appear to be available (this would be commercial use).
Xpath navigator is pretty fast and reads forward or backward. Also linq can simplify it.
If you only need to be able to load the raw data in Excel, a simpler option would be to just output a CSV file. It won't give you much flexibility, but if it's good enough, it's good enough. If in the future you need to add more complex functionality, you can throw away the CSV formatter without having lost many hours of work.
How did LinqPad change how you work? I have it but I didn't find it that useful, but maybe I didn't grok its power. I tried using it with Marketplace but it rarely worked. This was a year or so ago, perhaps those bugs have been fixed.
This is not a flaw in the singleton pattern. Its a flaw that you didn't inject the Singleton into the class which depends on it. 
Came here to add RegexHero.net, but I see you've already got it covered!
Well, as a programmer I spend a lot of my time doing things I've never done before. Either it's new algorithms, new API's, fixing bugs that I've never seen before, etc... Building my codebase takes up to 3 minutes, depending on which project I'm editing, and then firing up the application and starting testing takes another minute or so, and that's for each change. So instead, you'd have unit tests and mockups, which just runs a single method or something. Unfortunately, I just inherited the code base without unit tests, and implementing unit testing would just be too complicated and time consuming, so it's not an option for me. So to test code, you create a new console application in VS and just run the code there. But that leaves you with 100 temporary projects (and if you're like me, you never bother to clear them out). And still, you still need to build, and properly format the data yourself. Enter LinqPad. It just allows you to write code, and run it. And the best part is the Dump() method. Do you have an array? It prints a nicely formatted list of all the elements. Do you have an IP Address? It outputs an interactive table of **all** properties and **all** sub properties. And do you have a 2D table? Well then it prints out a 2D table as well. And a bitmap? You don't have to save it to disk, or create a Form+PictureBox to contain it. It's just there. Do you need to test out a network application? Just open two tabs and write your client code in one tab and server code in the other. In short, you can use it for testing everything C#. The only thing it does not do is creating an exe file.
Thanks, but I need to write the XML, not read it.
Before you dump ReSharper though, consider taking a look at [ways to improve its performance](http://resharper-support.jetbrains.com/entries/24083148-Visual-Studio-with-ReSharper-is-slow)
Just going off the top of my head, the quick and dirty way is going to be to use a system.web.httprequest to get the relevant site HTML. with any luck, you should be able to pass your MAC address as part of the request URL. Look at the address when you go to the form in the browser for the format. Once you have that, I would try and treat the HTML as XML and use either Linq to XML or an XmlDataDocument to store the page, find the relevant data and then push that out however you need it.
We are using Syncfusion. We liked the ability to let the clients use tools they were familiar with (Office) to supply the views (docx mail merge templates) that are used to combine with data to generate the pdfs. Here was our research into it (probably severely outdated, this was slightly cleaned up from a 2009 document)... ###Must: * pdf output * client customizable ###customize abilities: 1. change header/logo (whole doc + pages) 2. change footer (whole doc + pages) 3. add/change fixed text blocks 4. add/remove fields used from dataset 5. add/remove fields to data (available to custom forms, not available to api based forms) 6. create brand new reports (custom forms only) (custom forms is our internal paradigm for client generated form content; they can be anything the client wants) ##considered solutions ###aspose pdf.kit * pros: ??? * cons: must write C# to generate pdf; very inflexible from end user perspective; we have old version (new one costs) ###WebSupergoo * pros: html input * cons: * report page must be accessible to anonymous users (gotten around via usage of guid known only to the application at runtime) * cannot do page footers (limitation of html) * report page must be accessible on the server ###telerik reporting: * pros: * html sections * could be filled in with razor (making it the same as creating html views) * cons: * limited html * images very difficult (impossible from html?) * need multiple sections: header-all, footer-all, header-p1, body-p1, footer-p1, ... ###print driver: * pros: any document we want * cons: * must create file on server * need way to output to virtual printer and specify options to where file gets created * must still figure out document format in order to print * have to install printer on server * slow ###itext: * pros: * simple xml specification * works well with spark * cons: * implementation has many bugs which are easy to cross (ex: whitespace importance in certain circumstances; "Cannot convert Table to Paragraph" error) ###sharpPDF: * pros: LGPL * cons: write C# to generate pdf; many bugs ###rewrite of sharpPdf: * pros: could do anything we want * cons: we have to write it (the rest of it anyway, currently it can print simple pdfs with only single lines of text, no paragraphs/tables/images/graphics yet), mostly untested ###aspose.pdf (not the kit version): * Pros: * Simple XML specification with validation/intellisense in visual studio (or any editor that supports xsd files and has intellisense like functionality) * supports headers/footers (with explicit even vs odd page options) * can create forms (which could be integrated with custom forms easily) * Cons: * Cost * limited internal testing done wrt finding bugs we might expect a client to run into ###Syncfusion: * Pros: * alternative workflow: we provide Excel data dictionary (can generate via Syncfusion) and intial Word mail merge template; client makes adjustments and uploads; pdf creation done via Word mail merge -&gt; convert to pdf * supports any customizations client wants and is possible in word mail merge * several other avenues possible due to the size of this library * Cons: * Cost * many bugs (though dev responsiveness a plus; already fixed a few I reported) * keep running into things I want to be able to do but cannot because there is no public method on the objects to let me do it (mostly though it is possible via reflection because the methods I need are there, just not accessible) 
I wrote a blog post around a year ago on getting started with the HTML Agility Pack: http://adrianlucathomas.blogspot.co.uk/2013/02/starters-guide-to-web-scraping-with.html 
ah, so you are using it as an REPL environment. Interesting, thanks.
If it's supposed to be a report, why does the customer need to have millions of rows of data?
...Um. You realize XML != XLS, right?
Most reports won't produce a million rows. Most customers will just want the pivot table that is produced by that report when there are that many rows. Some customers will manipulate that report and do their own aggregation in excel with that report. Finally these people are really good at excel and not good at real programming. So making a report that big means less one offs.
You haven't said much about the exact requirements, but could it be enough for the customers who just want the pivot tables, to just get premade graphs without bundling the raw data? And for the customers who want to do their own aggregations, to just get the raw data? This sounds more like a problem of bad specifications than a technical problem.
As /u/LandOfTheLostPass said, the first step is to get the HTML page by using HttpWebRequest. The next part depends on whether the HTML is well formed or not - ie. if it can be treated as an XML document without the XML objects having trouble parsing it. This might be really out of date advice - the last time I did page scraping was years ago, but I had some success with Majestic, which was written to handle badly formed HTML as best as possible. http://www.majestic12.co.uk/projects/html_parser.php There may be better/more appropriate tools out there now though.
Fair enough, just had to ask in case you were a newbie confused with your acronyms :) That said, I'm not familiar with the object model EPPlus uses, but can't you create a class to serialize each line on the fly with XmlSerializer, writing each line as you go?
I do webscraping all the time. The easiest way to do is with the HtmlAgility pack. I create a wrapper class that maintains any session cookies to deal with logins. Once you have the response you can parse the html into an xml object via htmlagilitypack. Once you have this you can use xpath to query which parts of the html you are after. Alternatively, you now you can use link to objects and css selectors to obtain the desired elements. I just prefer xpath.
http://stackoverflow.com/questions/20061082/how-do-you-do-dependency-injection-with-autofac-and-owin
Singletons are near impossible to unit test, though. 
By SPA template you mean Single Page Application template? Is it webforms or mvc? I can try it out on my local machine. Although I have VS2012 edition. Got the keys from MS and tried it out - [failed with this error](http://i.imgur.com/qapyQAS.png). Something about Microsoft account experiencing technical problems. Very vague. Will try again later to see if it works.
Everything that goes further than having to add a NuGet package and compile your project is just a big mess nowadays.
It is the Singple Page Application, yes. In VS 2013 MVC and Web API is selected per defualt and can't be unchecked. I am not sure if the template in VS2012 is the same becaue ASP.NET Identity got introduced together with VS2013 if I remember correctly. But it shouldn't be the problem since my problem is with the return url which must be entered online. The error you get looks exactly like mine. I think it is to don't let the user know that the developer messed it up with a wrong configuration. If you look at the url of that error page you can find the "real" error message which I posted in my post. What baffles me is why even on the ASP.NET website there are only guides for sign in with facebook or google, but I couldn't finde a single one for sign in with a microsoft account.
Unfortunately the Microsoft Account is not very developer friendly, as you can't even use localhost to test. I hope this article will help you with your problem: http://blogs.msdn.com/b/webdev/archive/2012/09/19/configuring-your-asp-net-application-for-microsoft-oauth-account.aspx
Thanks, will try it as soon as I come home. At the university where I am at the moment I have to use a proxy which seems to block some traffic necessary for localtest.me (open port 80 for incomming traffic)
I'm not sure whether I understand something from, or you do. **localtest.me** never goes outside your computer. That domain maps to the IP address **127.0.0.1** - your own computer.
Not any harder than testing a static class. Easier when combined with proper dependency injection. 
Having had to work so much of this out through guesswork, trial &amp; error etc. I have to say that this is a great find. Thanks.
Web Forms + File System access = bad practice. Mostly due to the security concerns around allowing ASP account access to File I/O. Why not store the contents in a relational database like MongoDB/SQL Server or something similar? Some thoughts to consider: * What happens when your hard drive crashes? * How do you scale to a multi-server farm? * How do you recover files that were accidently deleted? That all being said, if the size of your files are very large you really have no choice but store the files on the filesystem. I would suggest that you store them to a location isolated from your core OS drive.
We actually have an AutoMapper wrapper class that checks to see if a mapping exists for that source/dest type pair, and if it doesn't we use reflection to create the mappings (property/field -&gt; property/field) before passing it on to AutoMapper.
No kidding. The only actual docs are a shitty hour long video that you can't find any information in. Maybe someday someone will transcribe the video an examples. That somebody will prolly not be me. 
NP. I was apprehensive about posting a blog post on Reddit but I struggled with finding info that wasn't either really specific or fragmented. 
I have to second this - and even if your files are large, you can use something like the filestream option in sql server to persist and stream larger blobs of data without unnecessarily inflating database size.
Well firstly, having an object with 200 methods on seems to be a bit of a code smell in itself. To answer your question, I'd probably use WCF and pass an XML payload to the service. The payload would contain 1) the name of the method you wish to execute, 2) any parameters the method needs to successfully execute. I'd then use reflection to find the method named in the message and execute using the parameters provided. It's probably not the most secure approach, but you could also add custom attributes to each object method you wish to allow access to via reflection. Your reflection method could the check to make sure the object method had the require attribute before allowing execution (and the by prevent arbitrary execution on methods by the client). Why am I using reflection? Be because I can write a single method in my WCF endpoint that can execute any of the 200 object methods without having to mess about with T4 templates. Also if you were to go down the road of using custom attributes you can also create a second method which lists out all object methods which can be invoked by the client. This approach may or may not be appropriate for you, as it depends on your intended usage, security, and performance requirements. It's also not as explicate as a WCF contract should be... 
Just to add, you could probably use the same approach using MVC webAPI, but just allowing access to method invocation to HTTP verbs doesn't make it a restful service - you'll want to make your service discoverable and navigable by using the hateos approach, the custom attributes and reflection approach may help you out though... 
Somewhat unrelated, but ... I've been trying to figure out a more generic solution to mapping a datatable/dataset to xml output (also supporting nested data). Does anyone know of a decent solution using an xslt or xsd template, or something similar to this post? All other exports (although they are flat) can be built and managed through the UI I had built, and I'd like to continue this approach if possible, even if it means uploading a template. 
Why would you not use the built in serialization methods for this?
I'm not sure I know which methods you mean? For the data feed side of my solution, I've opted to use ado.net and stored procedures returning flattened data to be able to pass in field names a sproc names to a few methods that get a data table from a stored procedure. The rest is mvc and EF...because I built the database for that end, but their database design is a mess with no actual referential integrity (or foreign keys). I'm eventually replacing all of it, but for now I need to use what they have...so I opted to not go EF for this...so no real objects or nested models exist. I was just curious about my options. I could turn everything into POCOs and use this mapper, but I was looking for generic low level solutions.
I mean just XML serializing POCOs, as the .NET framework provides built in methods for doing exactly that. Typically, I'd go JSON serialization instead of XML serialization, but that's a religious debate. Automapper is a different beast. It takes two rather identical POCOs and transfers data between them (e.g. ClassA and ClassB both have a property called Thing, so it maps ClassA's Thing value to ClassB's Thing value). I'm not *exactly* clear on what your use case is here though, because your description of your setup didn't make a lot of sense. Are you accessing two databases in different ways (one ADO.NET and one EF) in the same application?
I knew that was going to be confusing. The short of it is...I'm replacing their old guy who writes code like classic asp (almost no OOP at all). I'm doing everything from scratch, but they need some immediate fixes to their exports that come out of their current system. I'm writing it with the intent to use it in the new solution I'm building (MVC and EF based CMS and CRM app)... The whole export solution only uses one hook to their current database (one method that gets a datatable over and over)...and I plan on using stored procedures for exports on the new solution as well...only because they frequently have new additions and omissions in feed files...and I don't want to have to write new classes for them every time to support this. Does that make sense? I'm not sure I'm familiar with the built in methods you had mentioned. Could you point me at what classes/methods I should be looking into? Thanks again.
Interesting that you're here too. Haha. I'm also doing DI and am trying to leverage OWIN for a self hosted websocket server within an API project... That is also self hosted...all of which will run as a windows service. I too am having some conflict with these two...but its been a few weeks since I've been back to that project.
Meaning I don't recall what the conflict was...but there were issues.
Unfortunately OWIN is still on my list of things to look into... I was just passing along a piece of info that I had found in my initial searches on the topic. Assuming the gentleman who wrote Owin.Dependencies knew what he was doing, and I have no reason to assume otherwise, it looks like OWN wasn't built with DI in mind like MVC and WebAPI were. Which would be why he fell back to a service locator implementation in the end. Essentially, all DI frameworks are service locators, and SOMETHING at the top level has to be in charge of instantiating objects from the top down. In MVC and WebAPI this is easy, because we have a ControllerFactory that handles *all* entry points into app calls. I'm not sure there is such a thing in OWIN, so this may be the best we have for now.
If you need to export a DataTable to XML, like I said, use DataTable.WriteXml. If you need to take a schema flattened to a data table, and then *unflatten* it to generate XML from it, you may need to write your own serializer. by definition, a dataTable is a flat structure. Basically, if that's what you need, I don't think there's a tool to do it for you, you'll just have to loop and build an Xml doc.
When you have 200 methods on an object and you want to expose these methods on a web service, then this has nothing to do with a RESTful service anymore.
OP: Rename the title to "Creating a metric HTTP-based RPC service" to end this discussion
This can be implemented in a RESTful approach though you will want to review some of the docs/standards used by groups like Apigee for guidance on how you would want to name and structure your URI's. You have generally the right idea for your endpoints though with 200 of them you may need to do some thinking on how these features would map to canonical URI's. Alternatively you could do a SOAP style service where the method being called is embedded in the request and you just have one endpoint for all the methods this however does limit some of the advantages of the RESTful approach should you wish to leverage things like contract guarantees and an API that is documented in part by the HTTP verb system itself. That said, in general here is what I tell folks who are trying to decide between various service stacks: * Encapsulate your implementation, the code that does the real work should know absolutely nothing about services or how they are exposed, your business logic should always be as portable as possible. * For .NET IN GENERAL - If you are fairly sure or know you will need to do more than just text over HTTP in the future (say a true RPC interface or some odd transport or message format) then you want the flexibility of WCF as it can be tuned to provide access to your logic using any number of transports and message formats. HOWEVER if you know that all you will ever care about is HTTP and like using REST with JSON or XML then WebApi is going to be much easier to work with for the short and long term. That said for an API there are other options as well though WCF and WebAPI are the most used.
I like that AutoMapper reduces boilerplate code, but it also breaks type-safety somewhat. If you rename a property on a server class, but forget to rename the property on the DTO or the client classes, the compiler won't tell you and your code will fail at runtime. It won't even throw an exception, it will simply silently not set a value.
&gt; it will silently not set a value. .... So if it the missing value matters your test will fail or the missing value will throw a NullRefExc?
I absolutely agree on not using WCF. It's a pain in the butt to set it up (mostly XML configuration), debug it and adjust it. Web API is nicely extensible for all your needs. I use Web API myself for RPC style web services and it works wonderful.
Thanks for the input, I'm looking at Nancy and already have something 80% close to what I want done. Much appreciated. 
If no code behind is defined for a page it is inferred as just using the System.Web.UI.Page base class for the page's code behind. 
I have triple now but I think I'd prefer 2 30 inch ultra high resolution displays. You said dell so http://www.dell.com/ed/business/p/dell-u3014/pd those would be ideal. Although if price is no concern, I'd love 3 of those...
I'm on my second triple setup. The last was a 24", and two older 4*3 20"s, and the current is all 3 @ 27". After having duals for many years, I like the symmetry of a center monitor and two on the side. 
I would highly recommend that you do not get one large monitor. Either dual or triple is the way to go.
Or since reflector isn't free use Just Decompile from Telerik.
what's special about 8.1 and multimonitors? I would like 3x 1920x1200, but I am intrigued by the 2560 monitors, what are peoples thoughts on them?
Two 24" Monitors set vertical, I've already got the U3014 so these would be good companions.
Or [ILSpy](http://ilspy.net/), which is Open Source
So... is there any code in the ASPX file which referes to an event *OnClick="XX_method"* for instance? That could explain the error. This will compile, but fail at runtime if I'm not mistaken. I.e. you *should* have code behind, but do not. Either you've lost the .aspx.cs-files, or someone have done some unfinished refactoring.
Why not? I'm currently on a single 24" and I'm happy with that.
I like two big monitors, one setup vertically for the IDE and the other horizontal for browsing/documentation/email etc. For a while I've had an additional smaller monitor just for email, but I've since gotten rid of it as it was very distracting.
I've seen a lot of people with kids show video on one screen to keep them entertained while they themselves get on with work. But yeah, its a pretty niche case.
I'd like to announce a recent release of Dependency Explorer  a tool that could handle complicated reference trees and provide a clear picture for the application structure, which is indispensable during enterprise-grade application development and maintenance.
I do have a [wiki](https://github.com/AutoMapper/AutoMapper/wiki) on the [GitHub site](https://github.com/AutoMapper/AutoMapper) that myself and others have contributed to. The 3.0 and later versions also have complete code comment documentation as well. If you are looking for information on a specific topic, please open a GitHub issue and I'll add the relevant docs in.
Many thanks for AutoMapper and the offer to open a support ticket. I think the questions that most people have are far too basic to need a support ticket for... To the point that it would be embarrassing to open a support ticket. Chris Pratt's blog was the first place I found all answers to the basic auto mapper questions. If you could validate the information on Chris's blog and add a link to the GitHub wiki I think it would go a long way to expanding the reach of AutoMapper. Thank you again for AutoMapper!
I thought 8 / 8.1 supported things like multiple task bars.... 
Thanks. 
That would be a sweet set up. 
I think that's what I didn't want about the dual. Thanks for the recommendation 
I'm assuming you've never worked on more than one monitor? For most people it becomes pretty much a requirement once they spend a few days with more than one. 
&gt; Everything for finding files. It's a bazillion times faster than normal search. I wish Microsoft would buy this and build it into the OS. 
Thanks a bunch! That does indeed seem like it would solve my problem.
My centre monitor is my laptop screen and I have two more on my docking station at my desk.
You should add an ability for it to perform the CLR search from a specific location to find any dependencies that are missing, including inspecting COM registrations for interop dependencies.
ILSpy hasn't been updated since June 2012.
That's a personal choice. I have tried Telerik, JetBrains DotPeek and ILSpy and Telerik is the best free one.
Dual 30" for work. For play, I'd go one 30" and two 20" setup in a PLP setup (two side monitors turned sideways). That gives me two monitors to dump reference data on for work while I focus on the main 30", and gives me one big monitor for playing games when I want to.
For play, I'm on triple 27s and love it. Most games can be stretched over all three monitors. 
3x24" git/ftp on left, code in center, code/docs/netflix on right.
I'm not entirely sure if it is pure .NET assembly. If it is still a wrapper for a COM object, I never had any problems with the fact that it is. I looked around at the site, but this was the only thing I found on the topic: &gt; The core engine incorporates our proprietary Direct to PDF technology designed for high performance PDF manipulation in a multithreaded environment. The .NET tier comprises the visible interface and less speed critical code. When the assembly is loaded it locates and loads the core engine establishing a direct link between the two components. &gt; Note that this architecture is fundamentally different from the COM callable wrappers you often see. The interface between the two components is a direct connection designed for maximum speed under all conditions and specifically avoids the .NET to COM wrappers for this very reason. 
But different monitor brands and models can highlight problems with design too.
I have VS 2013 on 8.1 and use triple displays, it's awesome. My co-worker has two large ones (versus my 1 large, two smaller). I prefer three to two larger.
DisplayAttribute already does this.. And it's localizable... public enum MyCars { [Display(Name = "Four door sedan")] FourDoorSedan } as for the HTML helper; typeof(MyCars) .GetMember(enumvalue.ToString()) .GetCustomAttribute&lt;DisplayAttribute&gt;() .GetName(); 
Why should we use this tool over this one? http://www.reddit.com/r/dotnet/comments/1u09go/visualizing_dll_dependencies_for_both_native_and/
With a Blank Check triple 4K monitors 
I often have video or a presentation up on one while working or taking notes on another. 
MVP stands for Model-View-Presenter. "Dapper is a single file you can drop in to your project that will extend your IDbConnection interface.", it basically helps you to write access to the database with less code.
Please just let webforms die!
WebForms in 2014?!
Yes, as crazy as it sound: it's 2014 and people still writing asp .net webforms apps in the wrong way! Then ms created the mvc framework to teach developers how to organize their code and they still doing it in the wrong way, and also razr syntax that makes me feel like i'm developing with php, markup and code in the same file ugly as shit. I think webforms are a powerful technology to develop web apps, and a high demand stills for it (at least in my experience).
Yeah, SPA are the new trend.
Fantastic, now if I could get the time to watch these.
Subscribing. I like svn but would rather have something built into vs.
Thanks for this. I just watched several of these!
[Download the file (228 MB)](https://mega.co.nz/#!VtpDxbQQ!zH4urR1DH6P-hyAB1-FguCEpxSyS9m1DRhpzXGzAKBA) [View it on YouTube](http://www.youtube.com/watch?v=JbDDGSpjG9U&amp;hd=1)
I've had from 1 to six monitors and my absolute favorite setup is one 30" Dell monitor. I have two 24" at work, one 30" at home. I would much rather be at home. The reason is that one 30" is *much* bigger than two 24". With window snapping it makes it useful.
Visual SVN is a plug in for VS.
git seems to be what most folks are using these days so you'd do well to learn it. The [book](http://git-scm.com/book) should get you started - you probably only need the first chapter or two as the rest gets into crazy situations you probably won't be faced with (at least now.) I haven't used TFVC but if it's anything like earlier versions of MS source control, git will be much more pleasant once you get the hang of it. If you do decide to go with git, learn to use it from the command line. GUI tools are ok for merge conflicts, but really you should just do everything else from the CLI.
Why should you do everything from the CLI?
TFS online (actually called Team Foundation Service, now called Visual Studio online) does not offer GIT with a TFVC background, it does offer either a TFVC repository or a GIT repository. However, there are tools to map from one kind of repository to another - but these is mostly not a nice way to go at all.
Your comment bring nothing of value. 
For a newbie, learning the CLI is probably not the best idea. @OP here is a pretty good guide from Microsoft on using Git from Visual Studio. Seems like it goes through most of the questions you have. http://msdn.microsoft.com/en-us/library/hh850437.aspx
Xmlreader and xsltransform both expose streaming methods xsltransform takes a reader and writer as parameters. You could capture your report metrics via the transform and output them as a XML dataset. If your library is using XmlDocument exclusively, you may have to look to other packages because that class loads the entire doc into memory.
I too don't understand the CLI love considering the GUI tools have gotten much better. Personally, I use Git Extensions (Windows/.NET) https://code.google.com/p/gitextensions/ and find it beautiful for working with a VCS. Visually, on a commit or pull-rebase, I can quickly see differences in files immediately, and the full commit history is visible. Since we use a central repo structure, I also turned on the automatic Fetch-All function, so I can easily see when I alt-tab over if the remote (master) has been updated. Even with the Posh Git extension for PowerShell, I still prefer the visual view of a dedicated GUI client. About the only thing I desire in Git Extensions would be better searching of commits/comments. 
I think the CLI teaches you better. Once you've done it for a while, switching to a GUI is easier as you understand what commands it is issuing under the hood. Less of a black box. Of course, once you're comfortable with the CLI, you won't want to switch. Powershell + PoshGit for me. Git was born and developed on the CLI, so that's the language it speaks. Plus, it's ultimately faster for me to use the CLI. Github for Windows is great though. Strictly anecdotally, all the people I've worked with who jumped straight to a GUI are the ones who always need help with git. Always, always. Then I pop open powershell and fix it for them until the next time. 
Just an FYI - there is also a "white paper" of sorts that has a list of things not to do: http://www.asp.net/aspnet/overview/web-development-best-practices/what-not-to-do-in-aspnet,-and-what-to-do-instead
Yes, last week I got a curt email from a manager about how I had a lock on a dll so he couldn't do a merge. A dll I've never gone anywhere near, and why the frick does a dll (from a nuget package no less!) ever get locked by source control. Ridiculous. But mostly TFS and I are friendly. 
AnkhSVN is an SVN plugin for Visual Studio.
I concur. On big projects using SVN etc. the Visual Studio plugins like AnkhSVN make you so much more productive. Want to revert a file? Right click / revert. Want to perform a complex merge? Visual merge tools every time. Revision graphs? Change highlighting? You're gonna want an integrated tool.
This. If you *do* want a separate form, I don't see a big issue with adding custom events as you say. Well, the events are not custom but the listeners are. just attach to childForm.FormClosing in the parent form. Instead of hiding I would disable the critical buttons. Or, if remy_porter meant "closing" instead of "hiding", I agree. 
Thanks for the feedback. 
&gt; ShowDialog is an application-wide bit of functionality It's actually per event loop thread.
That's not a really good idea, as the thread opening forms owns it and no other thread can access it (so event handlers responding to events from code which originate from another thread can't either) unless you marshal the call back to the owning thread. 
That's why we have synchronization contexts. Especially when using tasks and *especially* when using async/await, it's handled more or less automatically. We use separate threads for major windows that don't need to be accessed by code running on the main thread. This is a good solution.
http://lmgtfy.com/?q=nuget&amp;l=1 Nuget is a package manager for projects. More specifically, it allows third parties (including Microsoft) to put certain packaged libraries into a central repository, which will then be pulled down into your project when it is built. You can search for available packages on the nuget.org website [here](http://www.nuget.org/packages). Certain libraries, such as Microsoft.CSharp, are automatically pulled from the local install of .Net. You won't find those on Nuget, as they are assumed to be installed as part of .Net. If you can't find Microsoft.CSharp v4.0.x, then it's a sign that you need to [install .Net 4.0](http://www.microsoft.com/en-us/download/details.aspx?id=17851).
So its really for packages that are not as common and that are often found scattered around the web. Thus nuget centralizes these things. Doesn't that make things even harder to maintain? Now I have to know what has nuget packages and what doesn't along with all the other reference information such as version/location? It seems to me that unless it has all my required dlls then its just another headache. What's your thoughts? Why do you see it as usefull.
It makes it a lot easier since updating libraries is trivial, and getting the correct version of the library is work that take seconds. Without nuget it could take hours. If you could even find that version of the library. You can also have a private nuget server for nuget deployment of your own projects which enables you to easily get rid of monster solutions. Despite some of the frustrations nuget causes it's an invaluable tool.
Maintaining 3rd-party packages in any project in any language or framework can be a bit of a pain. Those which aren't either default packages which are always installed (such as Microsoft.CSharp,) or which aren't available in your package manager of choice are always going to be a pain to maintain. However, any 3rd-party library that you can have a package manager install is one less that I have to maintain myself. At my old job, we didn't use nuget. For each new developer, we'd have to download the installer packages from their website and install everything locally, even though we only wanted the dlls. Eventually we just said screw-it and put the dlls in the code repository, but that's not exactly best-practice. With nuget we don't have to put our dlls in the repo, so we have a smaller repo, and we also don't have to manually install all the third-party libraries. Everything from the Telerik tools to Log4Net are available there. We just tell Visual Studio and Nuget to install a certain package to a certain project, and it takes care of the rest  including dependencies (I almost forgot about dependencies; package managers are the _bomb_ at making sure you aren't missing any dependencies.) When we add a new developer machine or a new server, we just pull the git repo. The first time we run a build, nuget goes out and gets all the required packages. Another benefit: updating packages is as easy as typing [`Update-Package &lt;package_name&gt;`](http://docs.nuget.org/docs/start-here/using-the-package-manager-console#Updating_a_Package) in the package manager console. You can even specify a specific version if you like.
If that's the case then /u/general_nuisance could spin up a second message pump for the form and modal popup. There are some caveats, but they are well laid out here: * http://stackoverflow.com/questions/19382977/how-do-i-run-code-in-a-thread-that-called-a-parameterless-application-run/19384077?noredirect=1#comment28733838_19384077 * http://stackoverflow.com/questions/4815699/how-to-programmatically-exit-from-a-second-message-loop
This was the feeling I was starting to have. Its more about a package and its dependencies the on specific .dlls. I just don't find it usefull if you don't use a ton of 3rd party stuff. Ill have to go through our solution and see what's on nuget and what's not. Other places I work had a build server with a all the req .dlls for each solution on a shared read drive. So devs just copied that folder and everyone had the same stuff. 
If you're using Express versions of Visual studio, [TFS is built-in and free for up to 5 users](http://www.visualstudio.com/products/visual-studio-online-overview-vs). You can use either the default TFVC repository or a Git repository, and both are built into Visual Studio 2013. Alternatively, you can get a GitHub account, create a repo online and clone it locally, and then use Visual Studio to manage the code from there since it's a Git repository. There are also SVN and Mercurial plugins, but you need at least the Professional version of VS to install them.
I know that synchronization contexts are made for this situation, I use them too for this, though I still think that opening the UI windows should always be done on the main thread and work scheduled off should be done on background threads (if async isn't an option), which then marshal back their work to the main thread for displaying, this avoids the problem to remember which thread opened which window ;)
NuGet automates a large part of what you just described. There's options for automatically restoring NuGet packages on build. We have a CI server, that doesn't have any dll's on it. When we make a check-in, it will contact NuGet and get the libraries it needs prior to building and running our tests. In addition to that, any developer that comes along doesn't have to get access to a share or anything, he/she just downloads the code from Git, which contains our Packages.config, and when they build - presto, NuGet goes and gets all of the packages they need. That answers your other question: To see the version of the packages from NuGet, you look in the projects "packages.config". NuGet is the way to go.
http://www.smarterasp.net/ is a good start for 'cheap', but the performance isn't great. Really, if you are just learning the technology you are probably best sticking to working with your local machine.
You can host sites for free on Azure (as long as you don't need SSL / domain name).
I've been using [AppHarbor](http://appharbor.com/) for a couple years as a free hosting ground for small apps that I just want to crank out and use quickly. It's sorta like Heroku, but for .net projects.
If you're looking into inexpensive hosting, make sure you're looking at Windows Azure Web Sites, especially the Free and Shared levels. As DaRKoN said, you get 10 free sites as long as you don't need SSL or a domain name. If you need those, you can move up to shared for $10 / month. http://www.windowsazure.com/en-us/pricing/details/web-sites/ There's also a hosting gallery with current offers here: http://www.microsoft.com/web/hosting/home
Go Azure. Don't even bother with the other hosts. Obviously you need to do your due diligence, but it's not even close anymore. The value is far and away much better. The Azure dashboard is incredible. It's modern, always improving, and gives great usage and billing detail. Azure is 100% scriptable as well; anything you can do in the UI, you can script via PowerShell. /noshill
Also, just to let you know from my own experience, where I work we found going back through projects and switching out dlls with nuget packages wasn't as useful to us (mainly because those projects tended to be in maintenance mode anyway and thus dlls don't get updated that much). But when we started a green field project, it made our lives much easier in the long run.
Hey, jcannon was curious to see how your experience is/was? I am bout to start CoderCamps next week.
I don't start until March. So I guess this means I will see you there. 
Can you elaborate? I just priced out my site's current needs, and the sql database alone cost as much as I'm paying now somewhere else. Azure is expensive...
Azure Free Websites support SSL. Just not a custom certificate.
&gt; The downside is that I'm constantly dependent on an outside system for parts of my dependencies, which could be a vulnerability for nefarious DLL injection, etc. The NuGet servers quite often have connection problems. Personally I think it's better to store the files in the repository, instead of always being dependent on working NuGet servers (and therefor on a good internet connection!).
Haven't used CouchDB, but if you're looking for a NoSQL database in a .Net environment, I can recommend [RavenDB](http://ravendb.net/). It has been really easy to use for me, is pretty well documented, it's implemented in .Net and integrates really well and I haven't run into any major problems with it (though don't use enum Flags on your DB objects if you ever want to filter against them, use boolean flag properties instead. Otherwise it's a PITA).
Don't look at SQL Server instances, look at SQL Database. It costs $5 a month for a database up to 100MB. Edit : If you don't need a relational database, you can go for Azure Tables which can be even cheaper (as you only pay for the storage space you use) Edit2 : Remember to cache...
Azure for sure. Scott Hanselman has a ton of articles on it.. [Penny Pinching in the Cloud](http://www.hanselman.com/blog/PennyPinchingInTheCloudWhenDoAzureWebsitesMakeSense.aspx)
Ok I'm going to look into it, but short ELI5 version: What's the difference between Azure Tables and SQL? What do I gain? More importantly, what do I lose?
&gt; Don't look at SQL Server instances, look at SQL Database. It costs $5 a month for a database up to 100MB. Crap, how did I not notice this option before? And 1GB is only $5 more.
N is for .NET. if its written in C# it will work with any CLR language. Java runs in the JRE hence the J.
If it's just for learning purposes right now, Azure really is the way to go. Spin up your VM(s) only when you're working on the project, spin them down when you're away.
If you need europe server, you can visit [hostforlife.eu](http://hostforlife.eu) hosting plan, they offer asp.net mvc and you can find their reviews in microsoft spotlight site. I believe they are good.
Azure Table is basically what the name implies; a table where each row is like a value that is associated with a key (similar to primary key in relational databases). What you lose is the relational parts of a standard database; you cannot join two tables directly. But it (can) cost less than SQL databases, and insertions and updates may be faster. Azure tables can store virtually any type of object that can be serialized, and they don't need to be uniform. You can also work with it directly via JavaScript if that's something you'd want. Typical applications is storing a large sets of documents and log entries.
And it supports federated storage, so you can scale it out ad infinitum without having to worry about the infrastructure.
Only really an issue on new clones, since it caches locally, so after the first build I have everything and can go offline if I want. And you're in the same situation in regards new dependencies if you're offline anyway since you can't download those either. And I've never had a connection issue to the NuGet servers, though it's certainly possible. But that's why I pointed out that a lot of us will set up an in house NuGet server and source packages from there instead, or in addition to the official servers. Still beats having to check in large binaries to a repository. As an example, we have a repository for a custom NuGet package for a third party app. There are 100 megs worth of binaries in it that get updated every time they update. After 10 updates, that repo is now 1000 megs. It will continue to get bigger, but instead of that being my main repo for the actual app, my main app repo is about 40 megs total, and just has this larger NuGet package as a dependency. This may not be an obvious problem to you if you're still using a CVCS like SVN, but anyone who has worked with a DVCS like Git or mercurial with quickly changing binary dependencies will immediately see the benefit of this.
Just to add to the other comments, yes, it's because it's often a .NET version of a library originally written in another language. It has a special meaning to distinguish itself from the original code because often the API has been modified and/or the internal implementation details are different to take advantage of (or adapt to) the .NET platform and language features. In addition, sometimes these projects are developed and maintained by completely separate entities from the library written in the original language/platform. So, while "nHibernate" _could_ have been called just "Hibernate", it may be misleading as it is a different source code, has different feature sets (for example, it supports LINQ expressions), and its implementation details (database connectivity, nullable/struct types, lazy collections, etc.) likely differ than equivalent Java Hibernate implementations. This is useful when discussing the libraries as developers can easily distinguish the two for proper context, or to simplify google searches.
This is all well and good if you are serving hundreds of clients at once, and very good research to that end. Keep in mind "It's slow" is relative to the load though, and acceptable limits. 200 ms per request is well within the acceptable range for pretty much everything that *I* do at the moment, so NHibernate is a no brainer for our needs at the moment. Way easier to do what we need, and performance within acceptable limits for our project. Also, I'm curious what happens here if they turn on second level caching in NHibernate, and do warm pulls instead of cold pulls... Would be another interesting data point.
2nd level caching will make nhibernate faster of course, but it also won't be equivalent to a normal fetch, as it doesn't use live data. In the raw results there are caching results of LLBLGen Pro which shed a light on how fast caching can be, NH's cache is a bit faster as it uses materialized objects already. Warm pulls, you mean with an active session object ? (if not please elaborate so we can look into that ;)) The slow downs in nhibernate are sadly due to their own code. See the previous post, where I have added a screenshot of the profile I did of the NHibernate fetch (as I couldn't believe the performance they had). So it has little to do with warm/cold fetches, more with overhead related to related objects, creating proxies etc., same where EF suffers (perf of EF is especially slow...)
Yeah, I was just curious. I know you specifically didn't turn that on in the first article, and was just curious if the difference between the caching implementations on different ORMs was negligible or not. Clearly the proxy object hydration is not a negligible difference *in comparison*. As I said, this is good data to have, in the right context. While you dismiss fairly strongly that the context doesn't matter in the comments of the original article, I respectfully disagree. "Fast enough" is a thing for most of our situations, and frankly I like NHibernate and EF's approach better than LLBLGen's, at least enough to choose them over it. No offense of course. :-) Thanks for doing the work though! Stats on these things are always useful.
When you say .NET 4.5 with Razor v2, are you using WebForms or MVC? Just to clarify. Approaches may be similar but not identical.
Sure. Which is why I said it was good information. Also why I said I wasn't at all surprised to find NHibernate and EF where they were, given how much I know about the internal workings (Which is a bit more than most people, but not nearly deep enough to make guesses about why they decided to handle these situations the way they did).
TIL, thanks. 
It surprised me a lot, to be honest. The work to do for change tracking isn't that hard, I mean, Linq to Sql and my own framework do just fine with change tracking info, others do too, so why they drop the ball so much is actually beyond me. I think what happened is that they simply never ran a test how fast their code really was and if they did they might have been satisfied with the results, which IMHO is kind of sloppy: a lot of developers out there think what MS provides in EF is the best there is, while the performance is really lacking. For NH, it is likely also a matter of lack of direction: who is going to fix large performance issues through big refactorings if the main team isn't merging even basic bugfix PRs? I also think a lot of devs think it's 'ok' to have a 10x (!) slower performance as Linq to Sql, which is IMHO naive: if your application is db intensive, the performance you leak through less optimal code will hurt your scalability of your application. So devs really have to decide whether it can't hurt them (like you did, so you know it's there, but it doesn't matter much, which is fine), but sadly I think a lot of the devs out there don't even know how slow the code is they produce / work with. 
I'd suggest creating a folder with the 'member only' content, then create a new user under 'local users and groups', grant only that user permission to the folder and enable 'basic authentication' in IIS. Here's a page with a bit more detail: http://stackoverflow.com/questions/5373497/how-do-i-create-a-user-account-for-basic-authentication 
I read this in the voice of the kindergarten cop kid that said "Boys have a penis, girls have a vagina."
Thanks for clearing it up. No one I know refers to it as Net, they all say DotNet.
So I happen to undo pending, cshift delete my entire source folder and reget everything and I do this a lot. Sometimes four times in a day. If I use nuget for everything will I jave to redownload the entire set of .dlls everytime I first compile? That would be disastrously slow.
Sounds like you have some other issues if you're blowing out your repo four times a day... Learn to revert man! Yes NuGet would redownload everything. Clearly you haven't done this before if you think it would be disastrously slow... Even my 100 Meg package example, which is about 99 megs larger than almost all the packages available, take less than a minute to restore. Most of our projects have about a dozen lib packages or more, and your talking about a total of 30 seconds for those projects for the NuGet fetch. Hell, our build server does this dozens of times a day.
&gt; 200 ms per request shudder 
Context is everything.
You could say it's...key
The only difference between using + and string.Concat is the syntax. a + b + c + d compiles into string.Concat(a, b, c, d) For more arguments than 4 the overload string.Concat(object[]) is used.
I generally avoid using StringBuilder unless it's a performance critical part that I know will manipulate thousands of strings. string.Format and concatenation with '+' are a lot better for readability in my opinion.
It will eventually get garbage collected since there are no references to it, but it's not immediate. 
Derp. I forgot datetime is a struct, which is a stack allocated value type.
It helps it you load this into a program like Reflector and look at the MSIL. Your declaration of dateFirstoftheYear will result in a .locals init, which allocates space on the stack for the date. Each subsequent assignment will overwrite the the prior value in the same location on the stack. When control is returned to the calling function, the stack will be unwound, and that value will go out of scope. So in answer to your question, the memory the prior value was using is reused. No memory is released.
No worries, I have slightly modified the setup from time to time, right now I have PLP-Laptop with the middle P directly in front of me, that seems to be working nicely for me as well
It depends what you work on. I work on an app that has a baseline of 100 hits per second (and higher peaks) on one of the public sites. If you carelessly allocate objects you'll run into situations where threads start blocking on frequent garbage collections, which themselves spike the CPU unpleasantly. Obviously if you work on a small intranet app you'd be guilty of premature optimisation if you worried about the GC much.
Are you using vs2012 or 2013? 2012 Doesn't have templates other then MVC5 empty project.
I also used VS 2012 and the templates weren't there. I ended up installing VS 2013 Express for web and creating a project in it, which I then opened in VS 2012 to work on.
BundleConfig is basically just a static class with a static method that is called on application start (in Global.asax.cs). In BundleConfig.cs you define the bundles (please check the Web Optimisation documentation) which consists of giving the bundle a virtual path (what goes after @Script.Render("*bundleVirtualPath*") when you refer to it). In the bundle definition you add folders and/or files you want to be included in the bundle. Now putting something like bootstrap in a bundle or not is more of the question whether you'll use a CDN version instead (hopefully users already have it in cache and/or save on the bandwidth). Using the bundle means your users get all the JS libraries in one compressed burst (1 per bundle) instead of individual calls (one per reference). Do one or the other but don't just refer it from your project. It may not be the biggest performance issue but as you add more libraries, the burden will grow. Another great advantage of the bundle: it generates a parameter that changes when the content changes. So if "myown.js" changed in production, browsers will get the new version instead of getting stuck in their cache. PS: Have you tried adding the Web Optimisation library from Nuget? If not reinstall it. May solve your missing file issue.
Unless `a`, `b`, `c`, and `d` are `const` in which case the compiler just combines them during compilation when using the `+` operator. But I'm guessing this almost never, ever happens in practice (or if it does, we don't care much).
I think you're incorrect: `str1` is "foo", `str2` is "bar".
This might be over-thinking it (or rather, pointing out an irrelevancy), but wouldn't this code: var str1 = "foo"; str1 = "bar"; // the string "foo" is now garbage. Have "foo" compiled as an interned string? I'm not positive on how string interning is garbage collected/managed, but wouldn't the "foo" string exist in memory for the lifetime of the application? EDIT: Just to clarify, it is only for this example if used _verbatim_. If "foo" was a string generated at runtime and not a compile-time constant (and not [manually interned](http://msdn.microsoft.com/en-us/library/system.string.intern%28v=vs.110%29.aspx)), then yes, the "foo" would be garbage and collected.
Technically, the comment is correct (but rather useless) after the line it's on executes.
I feel that the author has missed a critical consideration beyond simply garbage collection. If you have a 10 megabyte string and add _1 character_ to it 100 times, you have to copy the original 10 megabyte underlying char array 100 times. The operation isn't an O(1) action like incrementing a number, it becomes O(N+M) combining the two character arrays together; this becomes even worse if it's iteratively building the string. Perhaps, in practice, you might run into out of memory issues earlier, and if not, garbage collection might be the critical big drain, but I've had this bite me before. (and maybe the big, noticeable performance hit is the garbage collector)
Ah yes that's true. Thanks for clarification.
Thanks everyone for the feedback.
No, IDisposable is not a problem by itself, as it's an opt-in mechanism. Some types define a finalizer as well, though, which delays their garbage collection by a generation. Several classes implement IDisposable without a finalizer.
..and the latter uses a StringBuilder internally, or at least it used to.
Optimization will erase this, but otherwise interning is not normally performed for strings in metadata (like this one) since it can prolong startup time.
Yes, I agree in general. We mainly use thread-opened windows to be able to use modality like described, and only in situations where the code managing the window is entirely self-contained. 
I understand, I was speaking in general as unless you have source the only classes that actively signal that they have close out processes are those implementing IDisposable. Generally implementing anything in a finalizer without also implementing IDisposable is an anti-pattern and should be avoided with only a handful of exceptions IMO. This opinion stems mainly from troubleshooting apps with broken GC due to orphaned instances because they don't close out right or count on the finalizer being called at a particular time or in a particular order. 
No garbage collection is happening since nothing is allocated. DateTime is a value type.
Quite a lot of you seem to be pushing Azure. Isn't Amazon a better alternative? http://aws.amazon.com/ec2/pricing/ Not that I think that Azure is bad, but man, how do you set up continuous integration effectively for big projects? It takes ages to upload the packages to the cloud, if you use application instances and quick-fixing files isn't possible due to their storage and necessary re-packaging... I would gladly use Azure, but currently it looks quite beta for me... 
also, correct me if I am wrong, connection between Azure tables and the application is SLOWER than the connection between Azure SQL and your application. I think it has something to do with the forced SSL between Azure tables and your web app...
No love for amazon? http://aws.amazon.com/ec2/pricing/ :) not that I don't like Azure, but CI and performance between Azure tables and your App can cause issues during development and high load... prove me wrong guys, I would love to push Azure but it looks very beta to me right now
cheers!
In those cases where StringBuilder will provide a performance gain (instead of decrease) it will also be much more readable than string.Format and concatenation with '+'.
Will this be recorded? I'm interested but it's at a weird hour for me to watch it live
Yep, that was my [derp.](http://www.reddit.com/r/dotnet/comments/1xpo1q/garbage_collection_question/cfdhnwl) Still, I left my answer because it's applicable to reference types. 
Sure. It's going to be available on [JetBrains YouTube channel](https://www.youtube.com/jetbrainstv) within a week from air time.
should probably xpost to /r/iowa and /r/desmoines 
Azure git deploy, or a build server with TeamCity...
Will you pay an international move to the united states and provide the work allowance?
Because Resharper can't make visual studio slow enough all by itself.
Then you clearly have the wrong target audience on an international page.
Set the [xpath](http://msdn.microsoft.com/en-us/library/system.web.ui.webcontrols.xmldatasource.xpath%28v=vs.110%29.aspx) property on the data source to filter the data. 
Quite true. I've been rolling with Entity Framework since it's been available with the philosophy of not doing up-front optimization. That means I write using EF, make it work, benchmark stuff that's not going fast enough, optimize and if needed convert the slow piece to hand-written SQL. In the end using an ORM for 95% of the work and hand-writing high performance SQL for the rest when needed is a pretty good deal in terms of productivity. That said I've done some things in EF that I expected to be placeholder considering the volume of data that was going to crunched and frankly final performance was acceptable enough not to require more optimisation. 
There's also StringBuilder.AppendFormat, handy when your manipulating large strings.
Currently we are all webforms but are evaluating migrating to MVC for a couple of our applications.
yep, "//Product[./ProductCategories/ProductCategory/Category/.='Sightseeing']"
Forgot about RavenDB. Those guys need to do a better job to increase the awareness about RavenDB. Still though, kind of want to try something different. 
You know, you're not wrong. I installed the trial about 2 weeks ago and it's cool for some things, but it slows down my typing and stuff in an annoying way.
I agree with that person. Look up xpath commands.. There are some easy tutorials out there and most of it will apply to this situation. At worst you'll have a better idea of what to google.
Nah, I have absolutely no interest to work in the states. :-)
I know I have 8 gigs of ram and an I5 on my 1 1/2 year old lap top and Resharper frequently causes lagging on larger projects. 
Ya Resharper isn't a memory hog that lags majorly on large projects.
It seems really silly that you ask if the company will hire a worker from overseas only to turn around and say that you have no interest in being hired as a worker from overseas.
I've looked at ETags and it seems to be running correctly. I get a bunch of 304 responses for a majority of my assets.
You're absolutely right. We do not want to completely disable cache. We've been adding version numbers to our JS files, but we've still had issues with them not grabbing a new version on refresh. 
you should try convert this xml to a dynamic, gonna be easier to manipualte the data heres the link how to do this! https://www.evernote.com/shard/s278/sh/93b4e137-4804-4d9a-a439-e11ca618b712/93d826a6100033ffc72716ad2a14c37b converting it to dynamic you can use lambda or linq to filter the data. good lucky! 
If memory serves you can [bundle/minify](http://blogs.msdn.com/b/rickandy/archive/2012/08/14/adding-bundling-and-minification-to-web-forms.aspx) the files and everytime the file changes a new hash is generated causing it to be a new url. 
Another thing you can do is leave cache control on but set Expires on the page to the date you released. The client keeps the time it downloaded the file along with the file. With cache control on, if the download time is newer than the Expires time (it downloaded the new version after expiration), it will used it's newer cached version.
Holy fuck, no. *Do not do this.* There is a thing called "LINQ to XML" that exists already.
Thanks! I will look into this.
It's cheap, but this works: "blabla.js?build=BUILDNUMBER".
The optimization framework works perfectly fine with .NET 4.0. It's an extension of razor, so as long as you use that, you're fine to go.
Excellent list of resources for a beginner! Even useful for an intermediate.
I'm glad it was helpful. I tried posting it to [here](http://www.reddit.com/r/dotnet/comments/1xiztg/but_how_do_i_learn_aspnet/) a few days ago, but it seems that once CodeProject picked it up, people noticed.
I vote for projects changing their name to C*. CLog, CHibernate. makes more sense right? :-)
No, not really. 99% of libraries written in C# and compiled into .NET assemblies work perfectly in VB.NET. The models of both languages are so similar that there are very few language constructs in one that don't map 1:1 to the other. If you wrote a DLL in VB.NET and I decompiled it to C#, I'd probably not be able to tell by decompiling it to C#. The resulting code would look "normal." This is because they generally map very cleanly to IL, the assembly language of the CLR. Now Languages like IronPython, IronRuby and F# don't map so cleanly. If you wrote a DLL in F# and decompiled it to C#, it would look weird. There are F# wrapper DLLs for C#/VB.NET dlls that make the APIs more F# friendly. I challenge you to find me one C# wrapper to a VB.NET DLL or VB.NET wrapper to a C# dll.
I was referring to the C of CLR not C#. And it was a joke really :-)
For me, MVC was useful but I no longer want any HTML on the server side. Instead I'm using C# for REST services (often using the Controller), then a separate project for the HTML5 interface that will consume them. I've been going back and forth between Visual Studio and IntelliJ for building HTML5 interfaces.
Set a machine key in your web.config first. http://msdn.microsoft.com/en-us/library/ff649308.aspx If that doesn't work, then you're fucked. Go hug the internet.
Well you should be able to disable viewstate validation, but that seems like a really bad idea and means you are treating the symptom and not the cause.
It sort of depends on where you are starting from. People familiar with WinForms can transition to WebForms more easily than MVC or a REST framework like Knockout.
This is the most practical approach. It's better to just let IIS do it's thing and authenticate your requests, rather than build something to do it. This is a pure configuration-only approach. Of course, if you are using some exotic hosting service, it will complicate things.
There's no silver bullet, but rewriting the app will be better. If it's on production try to start re-factoring parts of the app, following certain pattern/architecture, and doing incremental updates until you finish the full rewrite.
Something more straightforward like Subversion might be a better entry point if you are apprehensive about version control. Solutions like Git and Mercurial seem to have a higher learning curve. There are a number of VS and Windows Explorer plugins for most platforms.
Store it in SQL. Most modern SQL implementation have a way of storing large binary files off-page. Meaning that unless you SELECT * you suffer no query performance loss. Just make sure to read file contents in one at a time through a stream, and not a byte array.
For reading/parsing, use XmlReader. For writing, XmlWriter. If you need to do both, do it one element at a time and pragmatically remember any required intermediate state. XmlDocument loads the whole document into memory, which will mean massive IO and paging delays for larger files. I recently had do some work with the OpenStreetMap XML data, which is 330 GB.
It's on production as a beta right now but it's slotted for full release in a week and a half, and only NOW am I being told there is significant performance issues. I personally haven't experienced them, but I can see how there would be especially with more people using the app. I'm going to try to possibly mix in some MVC I guess and see how that goes.. I really wish there was a silver bullet, but I didn't assume there would be haha
They are just so wildly different I don't feel that one really helps you learn the other. If we are honest about MVC today, it looks nothing like webforms. When you start using client side MV* frameworks you get even farther away and more towards a MVCVM. So my suggestion to learn MVC is to learn how to do HTML5/JS/CSS3. Once you get the concepts in those frameworks, you will see where MVC falls in to it.
That's right, anything you do from this point forward is going to have to be incremental. It's hard to optimize viewstate, especially when you have nested controls. Edit: not to say that nested controls are a bad idea, but if they have viewstate issues it compounds the problem. Anything runat=server is going to have some viewstate presence. 
I don't know if I would recommend a complete rewrite. More often than not, those seem to fail but refactoring is probably the best bet here. It's a lot easier to start using MVC and WebForms in the same project now, so I would start by implementing new features and functionality in MVC rather than WebForms. This thread at SO should give some useful info: https://stackoverflow.com/questions/541703/can-i-use-asp-net-mvc-together-with-regular-asp-net-web-forms On the front-end side, look into using a MVC framework. Or even something like [SignalR](http://signalr.net/).
web forms is a valiant but bad attempt to pretend you're not writing a client-server app. if you color within the lines, things will go kind of smoothly; but if you try anything novel, especially if it depends on capitalizing on the illusion of maintaining application state in between requests, you will either have a hell of a time or have to deeply study the framework.
I have set a machine key in the root directory web.config file, do i need it in the Account folder or in the folder that I have password protected or is the root file fine, I forgot to mention before that I had Event Validation and State Encryption mode disabled. I enabled it when i set the machine key. It is still not working I am getting the same error message. Just for my own knowledge, If this was working, when It goes to validate the user ID and Pass, is it contacting some ASPNET web service outside of my webpage or is it only looking to the ASPNETDB.MDF file that is in my App_Data folder. I can see that when i click submit it is waiting to load from my website. Just in case it might help here is the stack trace. Thanks for the suggestions [SqlException (0x80131904): A network-related or instance-specific error occurred while establishing a connection to SQL Server. The server was not found or was not accessible. Verify that the instance name is correct and that SQL Server is configured to allow remote connections. (provider: SQL Network Interfaces, error: 26 - Error Locating Server/Instance Specified)] System.Data.SqlClient.SqlInternalConnection.OnError(SqlException exception, Boolean breakConnection) +5050122 System.Data.SqlClient.TdsParser.ThrowExceptionAndWarning() +234 System.Data.SqlClient.TdsParser.Connect(ServerInfo serverInfo, SqlInternalConnectionTds connHandler, Boolean ignoreSniOpenTimeout, Int64 timerExpire, Boolean encrypt, Boolean trustServerCert, Boolean integratedSecurity) +341 System.Data.SqlClient.SqlInternalConnectionTds.AttemptOneLogin(ServerInfo serverInfo, String newPassword, Boolean ignoreSniOpenTimeout, TimeoutTimer timeout, SqlConnection owningObject) +129 System.Data.SqlClient.SqlInternalConnectionTds.LoginNoFailover(ServerInfo serverInfo, String newPassword, Boolean redirectedUserInstance, SqlConnection owningObject, SqlConnectionString connectionOptions, TimeoutTimer timeout) +270 System.Data.SqlClient.SqlInternalConnectionTds.OpenLoginEnlist(SqlConnection owningObject, TimeoutTimer timeout, SqlConnectionString connectionOptions, String newPassword, Boolean redirectedUserInstance) +195 System.Data.SqlClient.SqlInternalConnectionTds..ctor(DbConnectionPoolIdentity identity, SqlConnectionString connectionOptions, Object providerInfo, String newPassword, SqlConnection owningObject, Boolean redirectedUserInstance) +232 System.Data.SqlClient.SqlConnectionFactory.CreateConnection(DbConnectionOptions options, Object poolGroupProviderInfo, DbConnectionPool pool, DbConnection owningConnection) +5063783 System.Data.ProviderBase.DbConnectionFactory.CreatePooledConnection(DbConnection owningConnection, DbConnectionPool pool, DbConnectionOptions options) +33 System.Data.ProviderBase.DbConnectionPool.CreateObject(DbConnection owningObject) +524 System.Data.ProviderBase.DbConnectionPool.UserCreateRequest(DbConnection owningObject) +66 System.Data.ProviderBase.DbConnectionPool.GetConnection(DbConnection owningObject) +479 System.Data.ProviderBase.DbConnectionFactory.GetConnection(DbConnection owningConnection) +108 System.Data.ProviderBase.DbConnectionClosed.OpenConnection(DbConnection outerConnection, DbConnectionFactory connectionFactory) +126 System.Data.SqlClient.SqlConnection.Open() +125 System.Web.DataAccess.SqlConnectionHolder.Open(HttpContext context, Boolean revertImpersonate) +95 System.Web.DataAccess.SqlConnectionHelper.GetConnection(String connectionString, Boolean revertImpersonation) +206 System.Web.Security.SqlMembershipProvider.GetPasswordWithFormat(String username, Boolean updateLastLoginActivityDate, Int32&amp; status, String&amp; password, Int32&amp; passwordFormat, String&amp; passwordSalt, Int32&amp; failedPasswordAttemptCount, Int32&amp; failedPasswordAnswerAttemptCount, Boolean&amp; isApproved, DateTime&amp; lastLoginDate, DateTime&amp; lastActivityDate) +827 System.Web.Security.SqlMembershipProvider.CheckPassword(String username, String password, Boolean updateLastLoginActivityDate, Boolean failIfNotApproved, String&amp; salt, Int32&amp; passwordFormat) +105 System.Web.Security.SqlMembershipProvider.ValidateUser(String username, String password) +106 System.Web.UI.WebControls.Login.AuthenticateUsingMembershipProvider(AuthenticateEventArgs e) +60 System.Web.UI.WebControls.Login.OnAuthenticate(AuthenticateEventArgs e) +129 System.Web.UI.WebControls.Login.AttemptLogin() +127 System.Web.UI.WebControls.Login.OnBubbleEvent(Object source, EventArgs e) +101 System.Web.UI.Control.RaiseBubbleEvent(Object source, EventArgs args) +37 System.Web.UI.WebControls.Button.OnCommand(CommandEventArgs e) +125 System.Web.UI.WebControls.Button.RaisePostBackEvent(String eventArgument) +167 System.Web.UI.WebControls.Button.System.Web.UI.IPostBackEventHandler.RaisePostBackEvent(String eventArgument) +10 System.Web.UI.Page.RaisePostBackEvent(IPostBackEventHandler sourceControl, String eventArgument) +13 System.Web.UI.Page.RaisePostBackEvent(NameValueCollection postData) +36 System.Web.UI.Page.ProcessRequestMain(Boolean includeStagesBeforeAsyncPoint, Boolean includeStagesAfterAsyncPoint) +5563
Did you disable debugging before uploading? 
debug is set to false
Can we permalink this in the side bar? 
How would you explain the internals of ASP.MVC without mentioning Webforms?
How do you handle authentication in that scenario?
I agree with you completely about webforms not being suitable for anything fancy. However my point was that learning webforms will inevitably give you a better understanding of the ASP.NET westsack in a way that ASP.MVC won't.
Hey, thanks for the gold!
1) This is the largest weakness of WinForms (and the reason why WPF exists). There is no good answer, but my personal preference would be to build a set of Panels and implement MVC with those as the Views. 2 &amp; 3) It's extremely convenient, but without knowing more about your data-model, it's hard to say if it's a good fit. If this is a client app with a local DB, I might prefer something like RavenDB (NoSQL), just because if I'm building something like this, I'm likely to want to just persist an entire object graph. 
Thank you for your response. re 1: So how are you using the panels? Are the panels a user control where you can do something like this? var panel = new ProductListPanel(controller); MainForm.SetActivePanel(panel); re 2/3. Interesting thought about RavenDB, however client requires a sql database. I will certainly put that on my later reading list.
Thank you, I will take a look at that. 
This isn't the right sub for classic asp, but to answer your question, you should be able to check it like so: if(model.text != "") { // do something with model.text here }
1: That'd be a good way to do it, yes. There is a Panel control object that lets you group controls together. You would likely use that inside of a user-control to build up the various views. 2: *Why*? I ask that, because if they expect to be connecting to the DB and querying it directly, LocalDB probably isn't going to meet their needs (nor would CE, Express, or something like SqlLite).
&gt;Second, how much are you loading on page load? How much are you hiding? How much does the user have to see? Yeah I knew that was going to be a problem so I created methods for all the user controls I used called loadData() and that is where it actually pulls in all the data like in a pageload() !ispostback; But viewstate still lingers around and because it's one big thing it's still what's causing 90% of my issues. &gt;In that javascript event you call jQuery.load and call a url that generates that chart. It could be an html page or an aspx page. Hmm did not know that jquery.load could load an aspx page, that could be quite useful.. Does that page then run essentially independently of the main page? Have its own life cycle etc? &gt;I'm a fan of pagemethods. here's a quick article on them. Basically it's like having the ability to call server side functions like you would a webservice or an api, without actually having to create a webservice. I currently do use a web method for something and I thought it was really cool, but I haven't done much with them. I will look through that article and I might see if they can benefit me a little more. I think I'm relying on storing data for the user controls a little too much when I should just be treating it as a form and submitting it at the end. &gt; I also like to use an mvvm like knockout.js to update my display with server data I'm hearing this from a lot of people, I'm really going to need to take a further look at knockout.js haha Thanks so much for the tips! I really appreciate it.
&gt; 2: Why? I ask that, because if they expect to be connecting to the DB and querying it directly, LocalDB probably isn't going to meet their needs (nor would CE, Express, or something like SqlLite). The client are electrical engineers who occasionally hack code. Their initial requirement was for an Access database. They intend to maintain the application after it has been delivered and I had to work hard to get them to relent on localDB in place of Access. No way will they be happy with an object store. 
&gt;Hmm did not know that jquery.load could load an aspx page, that could be quite useful.. Does that page then run essentially independently of the main page? Have its own life cycle etc? Yes it does, but keep in mind that this can pretty much only be used for static content. What I mean is you can't load stuff like a gridview if you expect to be able to page or sort the columns. The first call is independent of the main page, but once it's called and the page is rendered. The rendered html becomes part of the main page. You can call the same page over and over again with different query string parameters and update it that way if need be. But any interaction with the rendered html may not act as you would expect it to act if it were loaded natively within the page. &gt;I think I'm relying on storing data for the user controls a little too much when I should just be treating it as a form and submitting it at the end. Remember, if you're loading stuff like this with jquery.load, you'll have trouble getting the input values when you submit to the server and try to save data since it won't be part page anymore. You can always use html elements instead of server controls to ensure that you have static id's and names. That way when you submit the form, you can still get the value of your input elements by using request.form[""].
How about `string.IsNullOrWhiteSpace()` instead of the comparison to an empty string? It checks for null too and is much more expressive.
&gt; Sad day. Indeed. I just watched two videos from the MVVM Light Toolkit as suggested by /u/wbsimms. I would love to start using WPF. However as I explained above, my clients are engineers who want to be able to maintain the application. They are comfortable using the winforms designer and thus it shall be. There are some fights worth fighting but pushing for xaml is not one of them.
I don't think he means classic ASP, considering he's talking about models.
Why not check if model.text = null or "= nothing" in vb
As a former wannabe engineer, now sw dev, I confirm this. 
Thanks for replying. The issue I get is that the page can't even load because it gives an error page and says Variable not declared. Perhaps it's how the server is set up? The technical team where I work seem pretty useless. I asked them this question and they told me it wasn't possible... Which I'm pretty sure is wrong, surely any good language can do. this. (just to be clear model is just an example variable name, it could be version, or color etc) Thanks again everyone!
I don't trust you.
I can only suggest you make all your models inherit from a shared interface that contains all the parameters you use in that shared UI code. That means some models will contain parameters that are never set, but at least everything is nice and type safe and you can do a simple `&lt;% if(!string.IsNullOrEmpty(model.text)){ %&gt;` etc.
Now we need a post like this for WPF.
&gt;create user controls which replace each other all contained within a single main form, No. &gt;close the current form and show the next form (perhaps all inheriting from a master form to share a common menu), You can pop open new forms (in modal mode) when appropriate though, but don't do too much of this as it's a UX nightmare. Also remember, windows requires a main parent window to exist for the life of the process. That's the window that owns the message pump and can delegate messages to child windows. &gt;have one enormous form with panels which are hidden/shown depending upon the active form, I've done this a lot, either with or without tabs depending on the level of navigability you want to give the user. &gt;I am going with using Entity Framework. I hate EF. It's by far the worst performing ORM out there. And if you have a record that has a few thousand sub-records in another table, expect your software to grind to a halt while saving. Check out Peta Poco if you must have an ORM. I've seen people go nuts with their data model for local DBs and there's just not a lot of point to it. It's not like you're gonna change one or the other independently like in a client-server scenario. &gt;I am going with a localdb database. That's fine. CE works, SQLite works a little better but both are fine. 
Is your web server a proxy (for load balancing) for two (or more) machines, or is it a single server? In case you're using load balancing, you need to make sure each server has the same machine key value set.
In IIS only allow authenticated users (NTLM), so no anonymous users. On the server side you can read the current user in the application context, then check their roles in AD to see if you will allow the current request.
I believe this is what you want: http://msdn.microsoft.com/en-us/library/system.windows.forms.bindingsource(v=vs.110).aspx
System.ComponentModel.BindingList&lt;T&gt; is going to be better than List&lt;T&gt; for this, but doesn't provide any of those features natively. You'll have to inherit it and implement sort, filter, and search on your own. Depending upon your scenario, you might be able to use LINQ to handle some filtering without custom code. I originally started down this path before just moving to XtraGrid. I still use a custom implementation of BindingList&lt;T&gt;, but threw out the sort, filter, and search logic. This is fairly old, but should looks like it's still relevant (I only briefly scanned it): http://msdn.microsoft.com/en-us/library/ms951295.aspx 
Exactly this! Webforms might help a little if you want to learn Winforms... 
I'd bet that most MVC projects just throw everything in the controller and call it a day. 
I might be missing something but couldn't you just make an IEnumerable object. Most grids I've seen can perform sorting and filtering on anything that implements this interface.
The commercial ones do, but the DataGridView that's included with VS is very basic. 
Been using telerik at work so long I forgot. 
I follow the blogs of a few of the big .NET guys to keep up to date: [Scott Gu](http://weblogs.asp.net/scottgu/default.aspx?PageIndex=2) [Phil Haack](http://haacked.com/) [Eric Lippert](http://ericlippert.com/) Although the latter two have recently left Microsoft and are a bit less .NET centric than they used to be. 
There are other libraries that have done the same thing, long before MS made their own. I still prefer ajax minifier library. 
Check this article out: http://brockallen.com/2013/10/24/a-primer-on-owin-cookie-authentication-middleware-for-the-asp-net-developer/
Just curious, how should the degree be quantified?
I actually very recently wrote a (kinda) related utility. My utility (as part of a larger task) grabbed svn logs and formed a graph where nodes were commits and arcs were similarities between commits. To compute similarity, the utility formed a vector of words for each commit out of the paths of the files changed and the contents of the lines changed in the diffs, then rescaled each vector component by the overall popularity of its corresponding word; the similarity between two commits was just the dot product of the commits' word vectors. But if you're looking to quantify **differences** between commits instead of similarity, consider just using the zipped size of the unified diff between those commits.
Free: http://channel9.msdn.com/ http://www.microsoftvirtualacademy.com/ Paid: http://www.pluralsight.com/training
I find Scott Hanselman to have the best .NET blog nowadays: http://www.hanselman.com/ I used to read ScottGu's religiously but since his promotions out of the ASP.NET division he doesn't post many interesting things anymore.
I would try to make the push for them to use the WPF designer. Wpf is simply superior. Speaking of maintenance, they are easier to maintain because of the use of databound business objects. 
Not personally, but HMAC seems to be the modern way to deal with this: http://stackoverflow.com/questions/11775594/how-to-secure-an-asp-net-web-api/11782361#11782361 
You could always self-host WebAPI over Owin \ Katana - &lt;http://www.asp.net/web-api/overview/hosting-aspnet-web-api/use-owin-to-self-host-web-api&gt;
Sounds like a good case for Nancyfx or ServiceStack. Much easier vs WCF
If you go this route I'd be interested to see how it goes
There are a lot of things out there that run as a web service with a web interface. The ones I know of aren't .NET, but are easy to use. Sickbeard and sabnzbd are two that I use. Edit: my reply was meant to answer the part of your question where you asked if it would be easy for a user to use such an application.
Sure, if you do barely more than the wizard provides you with standard functionality it is easy. When you try to expand from that, need to read the (sometimes bad and incomplete) API or, god forbid, need to expand the VSCT file and find out the ID of the menu you're trying to add something, then it gets really hard and a pain in the ass.
What's with the code formatting? There's mysterious whitespace everywhere.
Oh this looks perfect. I even knew about it too, can't believe I didn't think about it for this purpose though. Thanks!
Think of it as an advancement of IntelliSense :)
&gt;&gt; How is this any different from hosting a Web API with Owin / Katana? &gt; It's completely different. You're not hosting a web server! Depending on what/how you want it to behave, having a web server can be severe overkill. That's what I meant. How is it any different? You can also **easily** host Web API in a console application. There is no need for a web server, just like with WCF. **This is why Owin and Katana was mentioned explicitly, instead of Web API alone.** &gt;&gt; And in my experience WebAPI is WAY easier to maintain and to extend. &gt; For doing RESTful APIs, yes WebAPI is awesome. Depending on the complexity of the application, it may not be necessary. I've implemented small HTTP contracts using WCF. If you're implementing some large enterprise API, yes WebAPI is what you want. For a small mobile app, WCF may be feasible. Also for non RESTful APIs WEbAPI is awesome. You can just as easily implement RPC like web services here, still levering all of the possibilities of the HTTP protocol. Also, personal opinion again, Web API is easier to set up, as you don't have to deal with XML configuration files. Web API is set up completely with code. (This could be very well outdated with the current WCF version.)
I personally dislike using the MSBuild Extension Pack, as it has to be installed on each build machine. (I could be very wrong about this, please correct me if so!) Another possibility is to use the CodeTaskFactory that comes along with MSBuild 4.0. This way you can add little C# snippets in your project file that will be compiled on the fly to MS Build Tasks. Here is an example that will create an "AsyncExec"-task, which will run a program in a non-blocking manner: http://stackoverflow.com/a/21181071/3137652
Testing as a job/career has evolved quite a bit. It used to be that being a Test Engineer or QA meant you did a lot of tests and kept a lot of spreadsheets. Today, we have automated tests: unit tests, integrated tests, functional tests, UI tests, etc. So there is much less manual testing and more time spent writing automated tests. Just be aware that testing is not writing code to create features or solve problems. At best you're writing code to better automate testing. It has its own set of challenges, since testing as a field is less developed than writing software. The tools are sparse and buggy. If you're testing browsers, there are automated tools but browsers are moving targets with buggy or no support for some features. It can be really challenging but also very rewarding. It may depend on your personality. I find that people best suited to test engineering are people who have black &amp; white personalities - that is, something is wrong if it isn't right. They aren't grey area kind of people. If you tend to be more of a creative, problem-solver type, you may find testing frustrating or unfulfilling. 
I mostly agree with this. I disagree with the implication that if you are a creative problem solver, test isn't for you. In my experience, the test role actually has substantially more opportunity for out of the box thinking and solution building. Developers are regularly constrained by the spec they are implementing, the chosen frameworks, legal obligations, etc. Test rarely has any restrictions. (The biggest is typically budget for tools.) 
&gt; I just found this article that mentions self-hosting WebAPI. This may be the best way to go for OP: Using the Owin self-host is essentially the same as using the WebAPI self-host. Owin happens to be the recommended method for new applications. See the callout on http://www.asp.net/web-api/overview/hosting-aspnet-web-api/self-host-a-web-api
Thanks for this. Good food for thought. I'll be having more meetings with our testers this week to get a better understanding of my learning path in that setting. All the tests that I would be wanting to do would be automated ones in code. I'm not really keen to start clicking around websites and noting stuff in Excel. If it starts to look like that's going to be in the job spec then I'll be jumping ship. I'd much rather spend twice as long doing something in code to then repeat it, than do it really quickly clicking on things. Our devs just now write unit tests for everything they implement and our testers are either manual testers, or are focused on writing acceptance tests. It's interesting and reassuring to hear you say that it can be quite interesting. Everything I've seen of manual testing looks pretty dry.
I too disagree with GPs assertion that black and white personalities are best for SDET and more grey-area personalities are best for non-SDET work. In fact, I will go as far as to say that many black and white personalities are good for _neither_. Black and white personalities in development tend to have difficulty understanding that the "best" choices are situational, and often struggle understanding users whose needs differ from theirs. Black and white personalities in test (and in QA in general) are fine at automating or manually-executing existing test plans but tend to write new test plans using a very narrow view and can be poor at exploratory testing. It's often good to have some percentage of these people on your team, as they can provide a counterpoint and can help reign things in when either test or development get too crazy (eg. When test wants to cover every permutation to the point of ridiculousness, or when dev wants to over-generalize, over-abstract, start reading too far between the lines of the spec, etc.) The percentage worth having is lower than a lot of people think, though. For a certain definition of "black and white personality" of course. ;)
is this like nodejs?
I agree. This direction is great. System.Web time to step down.
&gt; re 1: So how are you using the panels? Try doing it like this: http://stackoverflow.com/a/6953766/3264143
I've heard very little if frameworks like MVC will be able to use this. Anyone know? Edit: Not out of the box, but if MS is making the changes so that it will work.
Looks interesting. Let's hope they work on the Windows 8 / Server 2012 only requirement. 
Not at the moment, but I've done it before w/Mono. It's not as painful as you might think - though, to be fair, I wasn't doing desktop apps or anything like that. Don't get me wrong: VS is an outstanding editor. But emacs feels like an entirely different planet.
I wonder how difficult this would be to get running on Mono... The current ASP.NET stack runs well enough to run ASP.NET MVC. 
Check to see if the db exists on the web host. When I did something similar (since I'm not that familiar with EF) I had to create the tables for log in on the site. Once the tables are created, change DefaultConnection (IIRC) string to connect to db at host.
The signalr chatroom jabbr has this functionality. You may be able to find something in their github repo ( github.com/jabbr/jabbr) or pop on jabbr (jabbr.net) itself and join the #meta room to talk to one of the devs about how its done
Just for future reference with regards to how JabbR does it, the [Chat.ui.fileUpload.js file](https://github.com/JabbR/JabbR/blob/master/JabbR/Chat.ui.fileUpload.js) is likely what you are looking for.
Nice idea -I would love to see an example of that. I, too would love to see how someone else did this. Very likely you know more about this than I do . . .
Can somebody explain to somebody who's been out of the game a bit, whats wrong with System.Web?
haha, no way. I'm still experimenting, and am probably breaking so many rules XD Trying to keep MVVM with ASP.net role auth with enumerated types is taking too much of my time these days lol
Pffffft - "Rules" lol ;-)
Windows 8 required??
Hm, I find it more flexible to jus copy/paste into LinqPad or similar.
I dont know what model.text means in your case... But since you are using it in the header Im assuming that its something like titles or stuff like that... so maybe ViewBag is suitable for that
Dont do that... It will turn your models into a mess
I mean, you could just paste into VS, but this keeps it in the browser, no?
A couple of things. First, you are already defeating the point of async because you aren't doing anything asynchronously. There is no usage of await in the CopyFile method, and you block the thread until your reads and writes complete. That is the opposite of the async pattern. This is functionally equivalent to above. I can't guarantee your logic, but I can guarantee this matches your logic, except that no threads are blocked in the making of this program. using (var outputFile = File.Create(dest)) using (var inputFile = File.OpenRead(src)) { // we need two buffers so we can ping-pong var buffer1 = new byte[CopyBufferSize]; var buffer2 = new byte[CopyBufferSize]; var inputBuffer = buffer1; int bytesRead; while ((bytesRead = await inputFile.ReadAsync(inputBuffer, 0, CopyBufferSize)) != 0) { // Assign the output buffer var outputBuffer = inputBuffer; // and swap input buffers inputBuffer = (inputBuffer == buffer1) ? buffer2 : buffer1; // begin asynchronous write await outputFile.WriteAsync(outputBuffer, 0, bytesRead); } } Second, the convention is to end all of your methods with Async. So CopyFileAsync. CopyDirectoryAsync. And so forth. I suggest you follow the same convention. And know that *any* .NET code that does not end in Async is not an async method. (Begin/End is a different asynchronous paradigm. You can convert them into Task using Task.Factory.FromAsync(Begin,End)). Third, why is File.Copy not acceptable? (If the answer is not async then, I'm OK with that). Fourth, there is a much simpler way to copy using streams. They handle the buffers for you. using(Stream source = File.Open(sourcePath, FileMode.Open)) using(Stream dest = File.Create(destination, bufferSize)) { await source.CopyToAsync(destination); } Finally, to answer your question, my first guess would be the line where you toggle read-only. Directories are writable by default, so you shouldn't need to play with the attributes.