I'm also new to C# and ASP .NET Core. I've come from a Ruby/JS/PHP background and switching from that to a statically typed language like C# is a whole new experience for me. According to Stack Overflow survey, C# seems to be one of the programming languages that devs enjoy using and having experienced it myself, I can see why. &amp;#x200B; The route that Microsoft took with .Net Core is really positive, building it so it is cross platform alone makes it really accessible for me, especially being about to deploy it to a Linux server. I was always under the assumption that working on personal projects in c# (.NET) as well as SQL Server is going to be expensive due to it only compatible with Windows where by the servers you have to deploy it on is Windows. And also the assumption that it is typically used to create enterprise level projects which I guess using for a personal project could've been seen as overkill perhaps? &amp;#x200B; And finally, it appears that .NET Core is **substancially** faster than 'most' frameworks in the industry and it is still relatively new. Consider this [case-study](https://customers.microsoft.com/de-de/story/raygun) from Microsoft, by moving from NodeJS to .NET Core the performance gains there were achieved were huge. 
CSV Helper. I've rolled my own in the past but there are a variety of edge cases you'll hit especially if you take imports from varying sources. This library is as easy as it gets.
hah...I hear ya....Blogging? Oh noes :) 
I'm actually the opposite, outside of work it takes me a lot of effort to do a "pet project". I really enjoy programming as a job, I even do work at home when I don't need to because I'm interested but that's as far as it goes. What is it about your job that makes you hate it so much?
 &gt;Could one implement identityserver4 inside you web app (the client) itself? Yes, you could. You can just add the required packages inside an existing app though you'll need to be careful on the namespace, url pattern etc so separating them later won't be too painful. However, &gt;hoping to save by not having to purchase a separate ssl certificate for each app You don't need to purchase the cert. Depending on where you host, they may give the cert for free, you can set up LetsEncrypt in few clicks, or if you put them behind Cloudflare, you'll get SSL anyway. &gt;hosting (azure) would be cheaper Since the identityserver isn't directly visible to the user anyway, you can just host it on the free plan, no need to put it in the same domain with the main app. As the app grow you can then choose to upgrade the identityserver plan or move it away. So IMHO you can just develop it separately in the first place anyway.
Without sharing specifics, its frequently challenging but not very monetarily rewarding (in respect to similar jobs elsewhere) and the subject is overplayed and uninteresting. There is a dozen new projects or tasks a week, nothing is ever finished, and there is never downtime. Apart from the job, though, I was just wondering what else I could apply my skills towards. Something creative or fun to make me less bored with it all... Plus, I have always felt like having fun with a subject helps you improve at it significantly faster. I'd compare it to something like working in a quick-lube every day of your life versus going home and working on your project-car. One is monotonous and not very rewarding, the other is (or can be) fun, casual, and leaves you with a sense of pride and accomplishment.
i think you mean x64 :) they don't supply apt-get through x86
All sorts. For fun emulate an old CPU. Shove stuff into a micro controller and see what happens. Build a database of games I play. Whatever comes to mind. 
This helps a lot. Many thanks!
Sounds like you might be approaching burnout, you might want to look into that before it gets worse and takes longer to get past. This happened to me and boy did it suck, so I wish you the best and hope it's not that for your sake. To answer your actual question, try learning something new and different. Write web apps all day? Write your own programming language. Maybe a game? Something to expand your horizons and something that will let you be creative on your own terms.
I fully support the approach taken. Using messages for everything is just delightful. The funny things I picked up on that made me cringe were all stylistic. Things like having a variable called ```event``` and having to prefix it with ```@``` because it's a keyword. ```command``` as a parameter name everywhere. It's really clear code, but I'd have ```CreateDiscount createDiscountCommand```, so you see it's just silly stylistic. The architecture works though and you integrate with a lot of good tech. I didn't see any unit tests, doesn't particularly bother me. There are some stub projects in there for them. I found the project structure confusing. I get why you have separate repo's for each bit, but having to clone all of them into the top level directory is a pita. At some point I'm going to accidentally add changes from a child repo to the parent repo. Rider looks interesting, ctorp is available in Resharper for anyone interested. Same thing, just add the property and off you go. The default class structure usually goes statics, private fields, ctor, properties, public methods, private methods, so it's not a surprise that your ctor ended up where it was. In resharper you can configure this, so I assume Rider will allow that too. So I haven't really discussed any of the patterns or approaches, I think your explanations were clear so if anyone is interested just watch the video :)
Another one for CsvHelper!
Thanks for the feedback! We prefer short names, as you said: @event or command everywhere, but I do get your point :). Speaking of unit tests - at first, we implemented almost everything (and a lot of original ideas changed along the way), so we decided to skip the tests back then, as there were more urgent things to do. About project structure - we're following the repository per microservice approach, which quite often also means that there are independent teams taking care of the particular applications, so basically we try to simulate the real-life scenario :). However, because it's only 2 of us and we use .NET Core, in order to make it easier to work with (and demonstrate on the video) we put there a common .sln file - and that's the only reason. I found the ctorp recently, and it's quite cool, will look for an option to move the property above the constructor.
I understand, and echo the other comment about burnout. It's hard to get out of that hole once you're in. If you're a gamer, you can try modding, that might be fun. There are a few games where you can mod in C# such as XCOM 2 and Rimworld.
Yeah, it's stylistic stuff mostly, it's not important, and you win big on being consistent which is far more important :) [I'm the same with tests](https://www.somanydotnets.com/2018/08/tdd-ioc-and-30-other-related-terms.html). I actually do often start with a test and use it to get my ducks in a row, but quite often I'll test after while I'm exploring what shape the app should be. I think of it like painting a portrait. Sometimes you sketch out the face shape first, sometimes you start with the eye because that's the thing that interested you about the project. The git thing was a bit of a hassle, but I totally get how it works for you, especially in this situation where you're teaching. It's fine. I go monolithic solution until I've figured out what can be nugetted out, then break it down as you have it pretty much. I do do lots of this greenfield get em started stuff though and the rules are a bit different :) It looks like a good series, I'll try and make time to watch the other bits. Do message or post once your course is ready :) 
I write ORMs for a living (LLBLGen Pro), using C# In my spare time I write Reshade Shaders (like my Cinematic Depth of Field shader) using a HLSL dialect https://github.com/FransBouma/OtisFX and custom photomode mods for games, using C++ https://github.com/FransBouma/InjectableGenericCameraSystem. I hear you tho, I had a period in my life where I had the same thing: work was just like that: work, and I didn't want to write code outside work, as it mostly was related to work, so in effect more work. I think the key to a fun side project is that it's completely NOT related to whatever you do at work, nor focused on earning money, but all about in effect wasting time doing a fun thing. This means it's totally OK to waste a full saturday to figure out how to make a triangle rotate on a screen. It likely won't get you a job at Naughty Dog, but it will make you have fun and that's all that matters. Weird thing is: a side project which has totally 0 to do with work often triggers different ways of thinking and gives you different insights in how to do things at work. It's not really possible to work towards those however, you'll run into them by accident. 
The `CsvProvider` (http://fsharp.github.io/FSharp.Data/library/CsvProvider.html).
I couldn't agree more :). And the fact, that it's not a commercial solution, just a sandbox for trying out a new things gives us a possibility to break things and omit tests if we feel like there are other things to do. Speaking of NuGet packages - the same approach, start with a monolith and then break it down into smaller packages if needed (that's one of our plans for the DShop.Common package). And you're right, consistency usually wins. Following the best patterns &amp; principles possible being applied to each one project in a random way doesn't really make things easier (and better). It's hard to tell when are we going to finish the whole series, as there's a lot of things to cover, but for sure, we'll post a message here or our forums when the new episode is out :).
You know, good luck, I like your attitude and approach and as mentioned the architecture you're describing is great, so I look forward to seeing more :) I'd volunteer to help but you know one morning you'd come in and the entire codebase would follow my coding style ;p
Thanks! Hopefully, we will keep on releasing a new episode every week :).
Looking forward to future parts! I‚Äôm trying to learn ASP.Net as I feel like my development path has stagnated a little bit, so I‚Äôm eager to read more.
Glad you liked it! I'll probably have a 2nd part up tonight. I learned most of what I know from trying out the Microsoft tutorials so they're pretty good resources as well. 
I made a live video of me fixing several security bugs for 4 hours, in my .Net CORE app. I upgrade from 2.1 to 2.2, upgrade NPM, plus my bootstrap, I find out that I have a CSRF vulnerability I was unaware of, and more. I did this all with the help of my Azure DevSecOps pipeline. &amp;#x200B; If you like the video people consider subscribing to my YouTube channel. :) Questions and comments welcome! [https://www.youtube.com/channel/UCyxbNw11fMUgoR3XpVYVPIQ](https://www.youtube.com/channel/UCyxbNw11fMUgoR3XpVYVPIQ)
[removed]
I'm interested to know: Why did you decide to go database first instead of code-first with migrations?
Seems you are heading in the right direction. One thing I would note is try to learn how to use docker and dot net without relying too much on your IDE. Especially if you‚Äôre using dotnet core. 
Being able to debug using immediate window, watch, breakpoints, and variable inspection by mouse hovering are indispensable features I'm using for more than 13 years. Yes, I'm logging crucial data but logging every variables is bad design IMHO. Also, I don't have issues developing dotnet core applications using VS2017. My issues are related to debugging and deployment using microservices architecture. 
@event and command seems to be the most widely used though. It's convention.
eShopOnContainers will help you. Just read through it. 
I've been a .Net dev but now work as a Rails dev professionally, so your comment on going from dynamically typed to statically typed is something I can agree is weird, but from the opposite end of things. :P I've come to like Rails and feel it's really good, but I do feel I'm missing out on a pretty exciting time for .Net on a professional level.
Yeah, It's a minor thing that I dislike. It's like naming a column in SQL Server ```[key]``` because you're constantly putting square brackets around it. I also use ex in exception handlers instead of the default e because e is commonly used in the event handler pattern. I don't mind ```command``` particularly, it triggers something when it looks like command's type is irrelevant, so perhaps I can get away with an extension method for example, it's just a smell, not necessarily a problem. But they're all just stylistic choices, and style is a layer of security in that our brains have to parse less information and therefore are less likely to misunderstand some of it. 
If anyone has some suggestions to share, I'm all ears. :) With the amount of research I've done, it seems interesting there isn't any obvious alternatives to SSRS that doesn't cost $1k or more. Maybe that just comes with the .NET territory for now.
I started getting into .Net Core at work and we already have another codebase querying against the same schema. So I wanted to add some code in. Net but start from the same database. I wrote the tutorial since most of the ones I saw were code first so no real reason apart from that. 
I already converted an existing DB into EF Core like a year ago with v1.0 or 1.1 of EF Core. Worked fine.
I'm super excited about that. I've worked on a lot of JavaScript... more than enough to hate it. Being able to use c# with full compiler checks is going to be great.
I have that problem, I just use the SQLite backend for EF Core and then I can muck about with test databases all I want, and just switch over later.
I work on a [mobile app](https://itunes.apple.com/us/app/piniq/id1257344940?mt=8) for fun. It lets me dig into new patterns and ideas.
"Virtual Directory" implies it is just a collection of static files that are served, I think. There's nothing to compile. But it's been a while since I've worked with them. With "Applications" in IIS, it assumes there's code execution involved and will look for a web.config (editable in the Application's Configuration Management section in IIS, or just in notepad) that defines how IIS should run a compiled site. This can also include ".as?x" files which are traditionally uncompiled. I have not worked with those in a while either but I think the point is they can be changed on disk without recompiling the whole application. You're not really supposed to compile them. I'm working with .NET Core now which really has no analog; it has .cshtml files which work similarly but those get compiled into a AppName.Views.dll file and the original files don't get published to the server.
&gt; Write your own programming language. Maybe a game? &amp;#x200B; Ok so that will take like a couple of days, right? &amp;#x200B;
[removed]
Seriously. If you are using spatial queries, disable client-side query evaluation or you'll be in for a world of pain. https://docs.microsoft.com/en-us/ef/core/querying/client-eval
If you want your user model, you can use the `UserManager`. After you inject `UserManager&lt;YourUserModelClass&gt; userManager`, you can call it like this: var user = await _userManager.GetUserAsync(User); You should really only use this within authenticated actions, so you can also decorate the methos with `[Authenticate]`, unless you have controllers authenticated by default
Yeah, this.field instead of \_field is also terrible for me :).
If you're looking for the identity of the currently logged in user on a website/api that uses Windows Authentication, you can do this: User.Identity
This is great information. Looking forward to future parts as well. I see that you use postman to test the api. I created a fast and lightweight alternative called nightingale. Would cool to hear what I could do to make a rest api client more effective for your usage. https://www.microsoft.com/en-us/p/nightingale-rest-api-client/9n2t6f9f5zdn 
This is why I stay away from it too. Can someone debunk this for me? I want to use code first
Jesus does nobody even click the article any longer? It‚Äôs in the first paragraph: &gt; ‚ÄúMicrosoft has some great tutorials as well but they weren‚Äôt always fully applicable to my use-case of developing an API against an existing database schema.‚Äù
Glad you liked it! I'll probably have a 2nd part up tonight. I learned most of what I know from trying out the Microsoft tutorials so they're pretty good resources as well.
Why are you so aggressive? That quote does not tell me WHY thats his use-case.
Ah okay cool that makes sense.
You probably want to stick with db first in that scenario, but you can get POCO generators and then you can treat an existing db as code first if you really wanted to.
I hate this too. Even though a lot of public Microsoft code uses the underscore the defaults for stylecop are to not use it. Makes no sense to me. 
&lt;meme&gt; evERYthing! &lt;/meme&gt;
I mean, if you're a slacker, sure.
As I work with C# I do fun projects with other languages maybe you could try that.
Definitely gonna take time to watch this. Thanks. I bet I‚Äôm gonna learn a few cool thing thanks to you ! 
Automatic tests.
That's a brilliant flag! I want to turn it in in everything I do now lol
There's always more to learn about .NET. I've been working with it since 2003 and there's always something new it seems. When I interview someone for a junior or middle level position, I'm mainly concerned about them knowing the basics, stuff like LINQ, some database, some OOP and design pattern basics, unit testing basics and the like. If the position involves Windows apps, I would want to see how well they knew Winforms and WPF. If the position is web, then MVC, Web API and Frameworks like Angular or React would be a consideration. I'm not looking for expert level knowledge, just a level where the candidate can fit in and grow within our organization. I'd also recommend that you be prepared to connect this knowledge to actual projects you worked on. This makes you more relatable than someone who can spout off language trivia like they crammed for an exam. &amp;#x200B; &amp;#x200B;
It really depends on what the work experience provides. I don‚Äôt think there is a set amount to know by that point. Where I‚Äôm at we evaluate everybody differently based on a fair understanding of their work experience and what is needed for the job. 
Yea, I wish I had known about it on my last EF Core project. 
Ok, I got shot down a bit for this opinion last time, so I'll break it down a bit rather than be specific. I will ask you about: * General tech (not limited to .NET) * General dev stuff (Patterns, procedural vs oo vs functional, CC, CD, TDD, blah) * .NET (I ask questions in about 10 areas in a sort of pseudo "Who Wants To Be A Millionaire" style). * Stuff we use every day (You may not know, this is not a disaster, but Docker, Linux, SQL, JS, React, Messaging, the list is endless, you will not know all of them after three years unless you already work here) * Anything you put on your CV (You put down plastering I'm going to ask you about trowels) * Psychological profiling to see if you have team fit (I made it sound scary to see if you were scared) So for three years of experience we don't expect lots, but we do look for attitude and aptitude. What problems have you encountered? How did you overcome them? Sell me a desktop app then sell me the same app as a web app. Then different teams do it differently. Whiteboard code is the worst imo. I'd almost go as far as to say flow chart it don't code it. It's entirely unfair to make me remember what the whiteboard equivalent of my muscle memory semicolon is and I wouldn't ask you. Some people put you in front of a PC and ask you to code something. That never works for me, it's quicker and easier just to talk to you. Honestly, tell me you can't tell who around you is above or below your level of experience through conversation? It's a similar process. It's kind of complicated but interviewers ask questions based on their background, quite often on the questions they were asked. This means there's a load of people asking questions based on an experience they had a decade ago. Finally, and I'm sure I've said this before (not to you, but I repeat myself way too much :) ) Some interviewers will just not like you. That's humanity. See if you can find a lesson to learn, but really just accept that sometimes the dice don't roll right. 
We still use .net framework but if I were interviewing you, I would ask. In a LINQ query what is the difference between Single, First, FirstOrDefault and SingleOrDefault? What key word do you use when working with an object that implements iDisposable to make sure it is disposed without directly calling dispose? Describe the differ types of SQL Joins... Compare and contrast the benefits of using Entity Framework v Vanilla ADO.Net with stored procedures... How do you check if a string is a valid Integer? Describe Dependency Injection, what‚Äôs good about it and what is not so good about? How can you implement asynchronous code? What is the key word Action used for? 
I've barely touched automated testing due to lack of time. Worked once so time to deploy right?
Can you explain why someone would join that vs. r/csharp? Not intended as a snark, I'm honestly asking.
Right now my fun projects all have to do with my hobbies. I wanted to expand my understanding of class libraries and programming practices with regards to C#, so I'm making a D&amp;D character-builder library that I plan on including combat simulations for people that like to min-max their builds. I participate in a play-by-post roleplay community on Reddit, and made a bot to create the "current weather" of the realm we all occupy. I'm still working on that one but it was fun to learn REST and Reddit's API. My super-secret-squirrel project is also going to be a game based in the D&amp;D universe.
I rarely focus on time as a measurement of capability. I've worked with and managed people who'd been developing for 6+ years, who wouldn't know what patterns to use, where or when an interface should be used over an abstract class, why you always initialise arrays etc. I want to know/expect: * what you've been working on in work * how and when you've been up-skilling * what projects you've built in your spare time (your GitHub repos for ex) * basic concepts of clean separation of concerns in an app and more importantly, why that's important * Probably most importantly, I want hunger. Someone with a great attitude and that can-do spirit. What you lack in experience, you make up for with sheer determination and problem solving ability I never ask questions that can be answered by looking on StackOverflow; they're a waste of time. I'll ask scenario based questions like; my app is consistently crashing a point X, but only in production, what do I do? I'm about to run out of time on a story I estimated and I'm no where near finished, what should I do? Etc. Source: 11 years in web and app development. Currently practice lead for 25 devs and architects. Former saas CTO, contract architect and engineering manager.
I think it does. But yes, he can chill out.
:-)
Most people already gave you the generic it's not about the tech but your ability to learn and know principals. I'll list a few things pertaining to ASP.NET Core and some general web stuff. - setup dependency injection and understand the different lifestyles - setup multiple environments - separating view models from entities - authentication and authorization - if you use an orm such as entity framework - If you ever heard or have used CQRS - asynchronous/await how to use and best practices - swagger I wouldn't say have to know but a great tool - API versioning - GET, Post, put, etc 
&gt; Describe the differ types of SQL Joins... Better; the types of SQL joins that LINQ to EF can actually do.
I would say if you clear are covered by [C# Exam 70-483](http://download.microsoft.com/download/A/D/F/ADF027D4-0541-4CFB-9202-3C48571ABB54/483_OD_Changes.pdf) is good indicator. I have been debating about clearing this exam as often in interview they ask me rate my self out 10 in C# knowledge. If I clear this exam I can tell them I have passed Exam 70-483 with this score! &amp;#x200B; &amp;#x200B;
Had to google action... Programming C# since beta.
As others have said, there is always more to learn :)
As others have said, there is always more to learn :)
dammit i missed a win 10 joke? Something happened.
I've been programming C# professionally for about 8 years now and most of those I've never had to use...
My workplace now doesn't use DI. At this point I have no experience using DI and haven't read enough about it, but what are some of the not so great things about it? I'm at that point in learning about DI where all I can see are positives.
Action is a type (System.Action living in mscorlib [on framework]), not a keyword.
It adds complexity and it can make debugging / troubleshooting more difficult. Generally, DI is a good thing. You have to make sure that nobody breaks the pattern. If somebody uses a dependency directly, then you have a mess. Overall it is a positive thing. Microsoft loves it.
You don‚Äôt have to use anything. You don‚Äôt even need LINQ or generics if you don‚Äôt want to. I‚Äôve been coding .Net since 1.0, 17 years.
And to forget
Yup. I have a bad memory that‚Äôs why I am great developer. Been reteaching myself stuff for 20 years.
Correct. Thanks.
After checking out about temporal tables, my question is how do I rollback to a previous version using temporal tables? I know how to go back to a certain point in time, but how do I go back to **the previous version**?
Hi there, I run [https://www.github.com/api2pdf/api2pdf.dotnet](https://www.github.com/api2pdf/api2pdf.dotnet) which supports both wkhtmltopdf and headless chrome. Feel free to message me if you have any questions.
In my personal experience design patterns are WAAAAAY overplayed in academic forums like this one. The absolute vast majority of developers don't have the slightest clue what they are. More importantly they rarely would even need to use one. Everything today is a framework, one that has been designed with proper patterns in mind. I am not saying they are not important, but reading this forum you would think all developers know design patterns.
I came across that before but it is using docker, right? Is docker always required on kubernetes? I'm having lots of issues with docker like what and where to put the secrets, and development using swarm mode. Well, docker is really good compared to the old style of deployment. If only it has a built in solution for secrets and easier way to convert the docker-compose to deploy my project on kubernetes, I think I'll stick with it. 
&gt;Single, First I caught one of the junior developers using "FirstOrDefault" for username lookups on a system we are rewriting (it's currently written in Classic ASP). I pointed out that, as username must be unique as per business rules, "SingleOrDefault" would catch any deviations. Lo and behold, it turned out that the previous developer hadn't bothered with uniqueness constraints on the usernames. Having it throw an error allowed us to spot this extremely quickly.
Yup. I actually bring in the candidate and have them code a small webpage. They have to populate a list of authors from the pubs database and display titles in an html table after the author is selected. They can use wenforms or MVC and use Google. They sit in a corner office with the other devs and work on it. If they can get it to meet the requirements, they get job offer. If they can‚Äôt make it work the interview ends. It‚Äôs worked great.
DI is just a fancier/more explicit version of the old philosophy "program to interfaces, not implementations". What's bad about it? You don't know the concrete class of the object you're interacting with. That's... not really bad. Unless you're writing code where you are using a lot of `is` operators to test for class, which is a bad sign. DI gets bad when people * go overboard and overly abstract everything^1. * abuse the Service Locator pattern and pass around an IOC container everywhere^2. ^1 You only need to inject *dependencies*. A class that doesn't manipulate anything external may have no useful reason to wrap in an interface and inject as a dependency. Especially if it's internal/protected/private. ^2 Don't use dynamic type instantiation where it's not needed. This is an anti-pattern.
Actually, that's due to over-reliance on automated testing.
:-D
&gt;In a LINQ query what is the difference between Single, First, FirstOrDefault and SingleOrDefault? Ok no problem &gt; What key word do you use when working with an object that implements iDisposable to make sure it is disposed without directly calling dispose? Oh Jesus Christ. This is best out of 10 right? &gt;Describe the differ types of SQL Joins... Whew, back on track 
Forgive me don't mean to sound rude, but it mostly sounds like it's bad when used badly? Though I guess if it can easily be abused then it is kind of a con for it. Thanks for the answer.
Makes sense. Thanks for the answer.
Horrible way to interview. How your developers write code should matter... A lot.. That's how you end up with stable architecture that can handle requirements expansion instead of big ball of mud where after a while your velocity is so bad you have to throw in the towel and rewrite
Silly question, cant it do them all?
Action is also a method if considered in the context of an HTML helper, which I think is more where the question was eluding. 
Service locator is not di. Completely different (anti)pattern
Agreed. Didn't meant to imply that it was. It is used and abused alongside DI. The anti-pattern is injecting the Service Locator rather than the dependencies themselves.
I think also asking what is a design pattern, and explain one you know, would be good. No need to know them all, but knowledge of them and knowing that you probably use one like UnitOfWork/Repository every day, it's general gist of how it works and why you use it can be important.
I could do it, but I look like an idiot vrs the guy that just learnt programming and has been practicing greenfields MVC. If it was pair programming it would be better as I could talk through my knowledge.
&gt; but it mostly sounds like it's bad when used badly? Which is true of all design patterns. DI being built-in in dotnet core means it's prone to cargo cult programming by people who don't understand why it's used. Pretty much all design patterns have a specific problem they are trying to address and are prone to becoming anti-patterns when used in a cargo cult fashion. A steering wheel is a good design pattern for controlling a car or boat, but a terrible one for controlling a motorcycle or airplane.
3. Dont use interfaces when you dont need to. It adds complexity and gains nothing.
Not silly. No. Been a while but it does inner joins by default. Left/Right/Full outers are not supported and neither is a cross join. I think it's because EF is an ORM model. You can do it with a lot of tricks I believe but it wasn't ez/obvious. Also this was pre EF core. Not sure if EF core solves this issue.
You should know how to use using statements, its syntactic sugar thats been around for a while and is so much prettier. I wouldnt crucify someone if they didnt know it though I would be asking what to do if something implements IDisposable. And for bonus points we would talk about HttpClient.
Good response. N.b. Ive been programming .net since beta...
I kinda agree they are overplayed but if you don't have the slightest clue what they are then that's exactly why you need to spend time learning some of the more useful ones e.g. mediator, decorator, facade, factory etc. I do however completely disagree that you'd rarely need to use one and that everything today is a framework/library. Maybe this is more contextual to your particular working environment though, no idea, but I've used at least a couple of patterns in every application I've ever built e.g. Early on, the repository pattern, later CQRS, decorator etc. I'm all for frameworks but I wouldn't really let a junior-intermediate dev loose with one until they have a firm handle on the basics and how things tie together. Using a framework is not a get out of jail free card for mastering the fundamentals. Not saying that's your stance or anything, just adding to my own opinion in.
Well now I feel silly. I thought you were asking which keyword I would use to *implement* IDisposable without... directly using Dispose... there's some weird [shit you gotta do](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/implementing-dispose), and then there's a thing with the sealed keyword... who even knows. I just refer to the docs. Now if you were just asking how to use a "using" statement around some resource that needs to be closed, that's different. That shit's so basic you don't even think about it. Although I'd probably still fail the question because my mind didn't go there.
I think I found my answer [here](https://bertwagner.com/2017/05/30/how-to-roll-back-data-in-a-temporal-table/).
I do left outer joins quite a bit, i cant say ive ever needed the rest.
And thats why interviews and hard on both sides of the fence. Hopefully my follow up questions would have sorted it out pretty quickly. Mentioning code reviews would have been bonus points and would have lead to much more tailored questions.
What does your code look like for it using LINQ functions? Or are you using LINQ to SQL? I had a bear of a time deciphering outer joins using LINQ functions. 
You don't need to wrap something in an interface to inject it as a dependency and use DI. DI, as far as I've learned, is literally just not creating dependencies in the class/object that's dependent on it. Those are created outside that object and passed in through some means (constructor, method, property). Can you point me to where something specifies you need interfaces?
Left outer joins are supported, they are pretty trivial.
There is no keyword to implement IDisposable. You clearly know that implementing IDisposable is pretty complex, so I'm not sure why you would of been hung up on that point.
Yeah which is why I'm sort of hesitant to agree that that's a point against DI.
Di is basically a requirement to do proper unit testing. It allows much more control over testing components in isolation and mocking scenarios. Try writing a unit test without it. You'll soon realize you're loading huge chunks of your app to test a small portion of it struggling to force code to go through specific code branch
I tend to use linq query syntax for complex sql stuff and use select new Widgit() { Id = .... I write the sql statement and then just convert it. Which works great until a dev doesnt know sql. I use lambda for simple queries.
Didn't mean to imply that it was, actually. It's a non-point.
Yea, Windows 10 happened. 
Well if I include all children in a many to one I get essentially a left join but what does that look like in LINQ? How about a right? Full and Cross?
It doesn't need interfaces. It's a pattern, and the pattern usually involves interfaces, but that's not strictly necessary. However, in language like C# that don't have multiple inheritance but do have multiple interface implementation, it's a good idea to use interfaces for anything you might want to swap out. If you are injecting a dependency with the signature of a class rather than an interface, then you are potentially tying yourself to the behavior of the dependency rather than the interface, which is counter to the principle.
ahh ok. Makes sense. The linq query syntax seems to be a better fit or these. Mostly if I want a left join I select it out or include the children of a parent, essentially giving me a left join using standard EF notation. But when I needed a cross join or right outer or full join I'm like wat? when using the LINQ function syntax. 
I agree 100%. I find it gives better to ask about things that are on the resume instead. It's more casual, less stressful for candidates and you can tell quick enough if they do know what they're talking about or not 
Yes studying for this exam I learnt about features of C# that I never would have otherwise known.
Nice. I have been coding in Visual Studio almost daily since 2001 myself. I'm actually doing a pluralsight video on VUE.JS tonight. 
&gt; In a LINQ query what is the difference between Single, First, FirstOrDefault and SingleOrDefault? Hah, trick question. Those are extension methods for IEnumerable&lt;T&gt; and have nothing to do with LINQ.
The thing is, you wouldn't look like an idiot. Most people fail the test and the only requirement is that you make it work. If you make it do all the required stuff you would pass.
5 years of .NET experience
I don‚Äôt use using statements. I always use 1 try/finally block and set all of my objects that extend IDisposable to null above the try. Then I write an extension that is DisposeIfNotNull(). I do this because a using statement just breaks down into multiple try/finally blocks in IL. After awhile this can hurt performance.
Sorry for misinterpreting your point, just a little hard for me to see "go overboard with DI" as other than "using it badly". Mind guiding me through what I'm missing?
`DefaultIfEmpty` will cause EF to emit a left outer join. Not sure offhand if the others are possible. 
I disagree completely. Being able to make the application work under the pressure of job interview proves that you know what you are doing and are capable. How they write code matters, but that can always be fixed / corrected. You don't an architect for a beginner / mid level coder.
I usually always find a flaw in my code when I write the proper amount of unit test for a block of code.
Funny. Use the Using (){} syntax for any object that implements IDisposable. That might not be used as much as it used to be. I'm getting old.
They have specific meanings in a LINQ Query. Single throws an error if there is more than one. First is used if there are more than one, OrDefault means it will return a default and not error if there are no results. It wasn't meant to be a trick question.
I mean none of those things you mentioned are LINQ queries.
What do you call items.Single();? I guess the correct word is expression then? What is the word for querying a list or whatever implements IEnumberable&lt;T&gt;?
I guess my original reply was mostly tongue-in-cheek and wasn't really meaning to be that pedantic but since you ask.. I guess you can call them extension methods for IEnumerable&lt;T&gt;? LINQ certainly does roll off the tongue easier, but they aren't LINQ. Not even [the source](https://referencesource.microsoft.com/#System.Core/System/Linq/Enumerable.cs,35e2ff5965cb4b7e) for them contains any LINQ, perhaps its the fact they reside in the System.Linq namespace. One is extension methods for a type (.Single()), the other is a language feature of C# (from c in col select foo). I wonder what would happen if I brought this up in an interview when they asked me your original question. 
do you really interview developers on such specifics?
Just a phone screen, they don‚Äôt have to get all of them. Nobody gets every one.
The pattern has no requirement or relationship to interfaces at all. All dependency injection means is that every external resources a class needs is provided to it (ideally through it's constructor) rather than being instantiated by the class itself, as a way of preventing the consuming class from becoming responsible for the implementation details of the dependency.
In an interview I would say that I‚Äôm not really concerned if they are part of LINQ or extension methods of IEnumerable because they function very similarly. The point is to know how to use them. But that was fun to look at the source for LINQ. Thanks for the reply. They are in fact extension methods of IEnumerable and not part of LINQ, nobody ever brought that up in 20 or so times I asked the question.
I think you are talking about polymorphism, not dependency injection. Polymorphism is the ability of one type to be used in place of another type when they share a common base or interface type. Dependency injection only means that dependencies are provided to the class, rather than the class accessing the resource some other way such as global state or instantiation. dependency injection libraries can be configured with polymorphic types, which is described as inversion of control, and may also be more what you are describing. IOC can make things confusing, but also more configurable, and so people can go overboard with it, but dependency injection is almost never a bad thing. With dependency injection, you can eyeball a class and tell if it has the capability of modifying the database, while having a policy of allowing anything to new something up means you never know what's behind some resource.
&gt; as a way of preventing the consuming class from becoming responsible for the implementation details of the dependency. But if you take your dependency in as a class rather than an interface, you are tying yourself to the specific implementation of the dependency, or at least as much of the implementation is implemented at that point in its class hierarchy. You've removed the construction details, but not the implementation details.
I have never seen Dependency Injection that provided a class. I think I have only seen it provide an Interface. I actually thought the point of DI was to use the Inversion of Control pattern, but maybe that‚Äôs not the case. But you lost me at eyeballing a class to see if it can modify a database, no idea what that has to do with Dependency Injection.
The .net specific stuff I'd ask about are: Tasks, thread pool. Async/await. When each should be used. How the CLR does garbage collection. Structs vs classes. Maybe some interfaces vs abstract classes.
I think you're being a little too pedantic. After all, [those methods](https://msdn.microsoft.com/en-us/library/system.linq.immutablearrayextensions_methods.aspx) are implemented in the System.Linq namespace.
Yes, as I outlined below earlier.
Why structs? Do you actually use them
&gt; Action I've been using C# for over 10 years and I don't think I've ever had to actually use the keyword "Action" more than once or twice. Probably not a great question for someone with 3 years experience
I agree. It also forces you to think about your application design in such a way that it is modular which is another side effect(?) of it.
I commonly see people use interfaces for DTO objects where you could simply use the DTO. This is really the only place I see interfaces abused though.. can you think of anywhere else?
This isn't meant to be a pissing contest, but I've been programming for a decade and I'd say I use all of these. Frequently. This isn't memorization, just familiarity with common tools in my mind.
If developer doesn't understand / practices good coding habits / patterns what you gonna end up with as architect or team lead are instances where they spent a week coding, you review, it works but code is just crap. And now you are in a pickle - you have to accept this technical debt when business is pushing you to move forward. I've been there starting at garbage code in previous jobs but had to accept it, and have to work with clients who have these shitty systems built because of stuff like this. I work for Pivotal as platform architect - my job is literally enabling companies how to adjust their software development practices to maintain velocity over time
Yeah, I see no reason why anyone would be against interfaces in 99.9999% of these cases. There is really no (serious) disadvantage to using them unless you're trying to optimize code that simply has no other way to be optimized.
Just like there are more than one flavour candies at the store, there are more than one c# discord servers. In a nutshell we are the same. But we do things differently. I cannot speak on behalf of other server, so I will talk about mine. Innitially c# inn was made for lessons. Only months later we grew into a community. So the core of our structure is helping other people. As of today, we work on a community aspect of server, throwing social events (like games or rp), doing raffles, encouraging collaboration in group projects and cooperation while learning. We're a young discord still (7 months), but we're constantly changing based on what our community wants. Our ultimate goal is to build a community, where you can learn and collaborate, while having fun and making friends. Hope this answers your question üôÇ
I try to teach people this all the time. It's an assertion step. Assert assert assert. "Can this ever have more than one value? No? Then use Single..."
&gt;I think it's incredibly common to refer to all of this as LINQ even though it's technically not
You are very, very likely throwing away good candidates because they don't pass a test that's not very representative of a real world working environment. Or if it is representative of your environment, I suppose you're just doing them a favor. Maybe you're market is different but where I'm at we can't afford to pass on good potential just because they don't excel in some weird high pressure scenario. I'd just feel bad putting some one in that position honestly. If I was asked to do something like this in an interview and was at all on the fence about the place, I'd end the interview and not bother. I'd have to be really interested to put up with something like that.
The best way to maintain velocity is to keep things as simple as possible. Clean, concise easy to follow code. It‚Äôs really not hard and the developers I have hired do not write crap code. It‚Äôs a small company and developers take pride in their work. Crap code is most often caused by developers who feel like a cog in the wheel and don‚Äôt give a shit. It usually comes down to people over process. Developers who take ownership of the area they are responsible for usually do much better than developers who don‚Äôt take ownership of the application and are just contracted to do x amount of work.
Eh, hadn't refreshed the page when I made that comment... fair enough.
How is it weird to do a basic web screen that displays data from a database? That‚Äôs totally normal.
Thanks for this suggestion. I may look at this seriously, the price seems reasonable and most importantly the licensing is more flexible than others I've seen. I'll download the trial.
I can add that lots of people might understand what dependency injection is in principle, but they don't take the time to understand how a proper DI framework actually works, or miss the connection between having code that "works" and separating their interface so that a dependency is not intrinsically bound to either global state or I/O. I've seen many classes that are so muddied up by shared state and I/O that performing a simple unit or integration test would be more trouble than just performing a manual test. In the context of the topic, asp.net had a very rich framework for things like dependency injection, but failure to understand the point of the abstraction leads to improper use of the abstraction in many cases. Also, recognition of a particular pattern leads to better use. Any time you call a LINQ extension method, you're essentially creating an immutable builder. Anyone can figure that out with enough time looking at source code and documentation, but it's much easier when you're familiar with the basic pattern. In my opinion, learning the gang of four stuff is essential. Domain driven design is also a good one to get into. The language becomes largely irrelevant, because many of them implementing the same abstractions ubiquitously. When there's not standard library support, there's usually supplementary support via external frameworks, but you will miss out on a lot of those features if you don't understand what they are, and what problems they're intended to solve. 
The hardware platform is technically x86-64.
Just when I thought I was starting to understand things, this thread comes along and makes me feel dumb. To be fair, I'm just a SysAdmin who got thrown to the wolves and figured out how to make things. I'm sure my code looks awful lol. I'm having fun learning it though.
x86-64 is the exact same thing as x64. It means 64 bit version of x86 instructions. What I said is still true, they provide x64, not x86.
Why do my side projects have to have code? Can't I have a life, too? Fuck this idea that programmers have to have tons of fucking projects that are code focused. This mentality is seriously impaired. Do I need to make a Github repo for the time I spend focusing on my kid or family or fixing broken shit at the house? Project: Child. Last Commit: 3 hours ago. Comment: Bath given. Fixes issue #3 (dirty baby). 
Learning design patterns helps to be a better developper and it also helps you to understand how libraries and frameworks work. If you don't know them, how can you judge if they're useful ? 
You strike me as one of the people who would probably fair poorly on my last point above. I have a family, a house, 2 cars and 2 kids. I find time and I manage others that do too. I didn't say it was a deal breaker. Or that the interview would be over if I couldn't see a git repo with projects. OP asked what I *look for*. I also manage a guy who's a single dad, leaves work every day at 3 but is as dependable as they come and works irregular hours. He recently got bumped to tech lead. And still manages to study and go to conferences. Instead of flying off on a rant, cursing and throwing out sarcastic remarks, why don't you ask me to clarify what I mean. 
Single iterates the whole collection though. 
I'm sure you've had success with these questions but I gotta say, I completing disagree with using them as interview questions. They are standard, run of the mill, been around for years, .NET questions that could be easily memorised before the interview by jumping on Google and Stack. And even if they can't remember them, they can find the answers in 5 minutes, so what are you really testing?
ASP.NET Core is feature and API rich. Don't feel so bad about not knowing it all. My samples so far (https://github.com/dodyg/practical-aspnetcore) is about 214 and it barely scratch the surface of the tech. Keep going. ASP.NET Core 3.0 is coming :)
You wouldn't use swarm mode. Swarm fills kind of the same rule as Kubernetes (it just is much newer and doesn't have anywhere near the amount of features). Yes, usually you will use docker with kubernetes. The ability to set up and tear down containers is massively useful. Also, it shouldn't have a built in way of doing secrets, that is on your OS or build environment. For Windows, there are User Secrets, for instance. Docker-Compose should just be used for debugging your project, its easy to just docker-compose up and down to start your services.
If you're having problems debugging docker containers, I wrote an example here: [https://github.com/Dispersia/Dotnet-Watch-Docker-Example](https://github.com/Dispersia/Dotnet-Watch-Docker-Example) it uses dotnet watch for fast restart, docker for containers, but allows for debugging all at the same time. Basically, you download vsdbg into the container along with your image, and you point it to your application, then debug through vsdbg
We're rewriting a business-critical undocumented system from Classic ASP with at most a few hundred users. Performance is not a priority.
I guess it is just the implementation of dependency inversion. It's been awhile. I guess I go by the YAGNI principle. I usually end up using Dependency Injection for most dependencies but I don't usually make them interfaces until I need it. Especially seeing some code bases with almost 1:1 mappings of classes to interfaces. Testing with an actual class object is sufficient in many cases and mocking out everything can lead to other problems. 
Why studying and go to conferences is not on his work time instead of his spare time ? It's part of the developper work to keep learning. It is useful, not only for him, but also for the company that employ him. In fact, enterprises are not honest because they ask us to be better everyday to help them make more money without paying for our training. This has to stop. Developping is like other jobs. Would you ask a mason to build wall on his spare time ? Would you ask an aeronautical engineer to build spaceship engine on his spare time ? Sorry, but I prefer spending my spare time doing sports, music, have fun with my girlfriend and with my friends instead of working...
Look, I assume you mean well, but you replied with "you're being unreasonable, ask why I *am* reasonable". You didn't engage with their central point at all, probably because you don't (yet?) see the problem with expecting programmers to live and breathe code. 
&gt; I guess I go by the YAGNI principle. I usually end up using Dependency Injection for most dependencies but I don't usually make them interfaces until I need it. Completely fine for effectively internal/protected/private stuff you have full refactor control over. I'm a believer in YAGNI, but I always wrap any can't-run-easily-in-unit-test dependencies from the start.
Not really, I meant instead of being smart ass why not have a discussion
I use [wkhtmltopdf](https://wkhtmltopdf.org/) for rendering HTML strings to PDF, and for the simple scenarios I use it for (mainly generating contracts) it works just fine. It is essentially a headless browser that you can call from the command line. Besides this, it is open source.
It is on work time? 
These are some pretty good screeners. I would add in some systems design questions on the tail end to try to get a feel for any architectural talent as well.
Better to assert with a database constraint than require every consumer to throw an error if there is a dupe.
I agree, if you know it's actually meant to be unique and are not checking *if* it is unique then there's really no reason to use Single
In an ideal world, yes. However, the system (and the company procedures for updating it) are so screwed up that it was a magnitude of effort easier just to preemptively guard against screw-ups at code level; even finding out whether duplicates existed would have required a formal process to have been followed. We never actually expected the exception to be thrown. In addition, the administration system (which is where the constraint would have kicked in) was still written in Classic ASP, and hell would have broken out if people started getting errors there. I handed my notice in a couple of weeks ago, mostly due to the stupidity of the development procedures here and the oblivious attitude to data security.
I have added an example web output (though the styling is outdated and could do with a refresh) [https://livedocume](https://livedocume)nter.barryjones.me.uk. There is also another example web output which is used to show the API documentation for the software available at [http://livedocumenter.barryjones.me.uk/api/index.htm](http://livedocumenter.barryjones.me.uk/api/index.htm).
Don't want to spam the subreddit so I'll add links to the future parts here: [Part 2](https://medium.com/@kieran.gillibrand/creating-a-net-core-rest-api-part-2-controller-setup-8f71408d8f4e)
Anywhere that if you delete the interface the application still works the same.
Write for clarity and ease of reading first, write for performance when you need to (and have identified the slow sections of code)....
I think you'll quickly disable it once you start querying the database a lot, as still queries with some constructs aren't running db side. 
I truly wished the .NET team understood the necessity of spatial types on the framework level. They get out of their way to make duplicates of many OSS libraries out there to get their own implementation in the BCL, but oh boy, spatial types... no, why would we do that? 
I wish Reddit would deal better with cross posts. [Response](https://www.reddit.com/r/csharp/comments/ac4squ/find_fix_and_avoid_memory_leaks_in_c_8_best/ed55w3x/) 
They're both LINQ. What you're referring to is [Query Syntax].(https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/query-syntax-and-method-syntax-in-linq)
He pretty much had it. IMHO dependency injection stopped the passing around of a massive context object and helped with seperation of concerns... It also is a pretty obvious requirement for test driven / unit testing. Bonus side effect you get a widget service telling you in its constructor that it needs a database context/service, and email service and a messaging service. If I need to stub those services for a test it will be an interface, if I dont its going to be a class.
Developers like to make things complex. For example, all those dumb arse design pattern books. They all seem to think its a challenge to implement them all, as opposed to a way to find common ground when talking about a solution. 
Would you ask a doctor to go to conferences in his spare time?
More importantly, is he reading reddit on work time.... I would consider this forum a form of learning.
I agree. What I am describing is super easy to read and understand. Plus you can easily introduce exception handling.
r/dotnet specifically doesn't allow cross-posts unfortunately
Are they an actual specifics? I work in .NET Core for 6 months now and can answer everything but the 'Action' one(and maybe I lack some detailed knowledge about pure ADO since I've never used it directly). This is incredibly basic stuff he listed out...
I don't like the phrasing of the IDisposable question, I didn't get what that person was trying to get at until I read other comments. Fishing for a keyword answer is silly. If you are really concerned whether someone knows the "using" construct, ask about it directly in the question. Asking "What does a using block do, and why would you use one?" will let you know if the person even knows what it is, and whether or not they know its link to disposing of objects.
No ! Spare time should mean NO WORK. The company should not expect employees to work outside their working hours. If that's what they want, let them pay for it. I think it's called overtime, isn't it ? Even, if it's not really productive work, training is still work. If you want to give your life to your work, go for it. It's not how I want to live.
&gt; They are in fact extension methods of IEnumerable and not part of LINQ Since you might not see my reply to /u/bahwhateverr, they _are_ LINQ. They're [Method Syntax](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/query-syntax-and-method-syntax-in-linq). 
I also find typing and running examples from Essential C# I learned many .NET feature that I use day to day now. Also found that C# In Depth by Jon Skeet gives much clear understanding of C# language. 
Whata the answer to the second one
Also remember now that Scott Hanselman has a blog post entry about [What .NET Developers ought to know to start in 2017](https://www.hanselman.com/blog/WhatNETDevelopersOughtToKnowToStartIn2017.aspx) &amp;#x200B;
using(var x = new disposableObject){}
Not sure why you got the idea that I don't know them. My point is that in 15 years of development, I've worked in a handful of shops. By and large, most of the developers out there are not the enthusiasts you see posting in places like this one. Most of them are role players. They know MVC inside and out. They might be working on services. And so on and so forth. Most of them have a pattern/framework that has been in place for a while, pushed down by the rockstars above. They simply fill in the template with business logic and move on. People forget that the type of people that post here in this sub, are very very different than the vast majority of developers in the world. Not everyone reads this sort of thing on their free time.
Yes. They are allocated on 5ge stack, so get cleaned up as things get popped off. It's super useful for controlling memory usage.
Why not have both. There is a reason Single was created, when makes sense use it.
This sort of thing blows my mind. Maybe this guy is launching spacecraft to the moon, where performance somehow matters to split millisecond. 99.9% of us in the real world don't need to worry about such theoretical performance hits. I much rather use standardized, easier to read, understand, and maintain code.
Thank you so much for this comment, the light bulbs just went off in my head. If every dependency is listed in the constructor it really is easy to see if class sends an email or updates the database. That really does make for better code. I guess the next time I design any new application I will code it that way. Thanks again dude.
We probably worked in different places, hence why you come across their usage very often. You probably have more rockstar-ish environment. Where I've worked, some small shops some VERY large shops, it was not common at all. I remember some factories here and there (Java days), but that's about it. Like I said in a reply below to another user, 90% of the developers I've come across are role players. They fill in their template/framework with business rules, that's all they do. They also don't post on software developer message boards, you know what I mean. Again I'm simply making a point that there is a huge range of software developer out there, and reading forums like this one might skew one's view of what the landscape is actually like.
Oh. Thought he meant some other kind of "keyword". I always called the using a "using directive", not sure if that's correct or not
Same thing.
I can't view the course, but do you have a certificate? If not, either create one or remove the https requirement.
Sorry, I was logged in [https://www.pluralsight.com/courses/asp-dotnet-core-api-building-first](https://www.pluralsight.com/courses/asp-dotnet-core-api-building-first). And thank you for this, I made a quick search on how to [remove https](https://stackoverflow.com/questions/46507029/how-to-disable-https-in-visual-studio-2017-web-proj-asp-net-core-2-0) and was able to solve my problem. Thanks!!
Cool. Thanks.
IMHO the biggest problem with developers is we make stuff more complex than it needs to be then use mental gymnastics to justify why we did it if challenged. 
Two reasons: 1. It adds extra complexity. 2. YAGNI
Because reality is most companies will only let you learn / invest into skill development related to what you are doing. This usually means a fairly narrow slice of technology tree. If you want breath of knowledge you generally need more then one project
Check out digital ocean, comes for 5$ a month
Thank you!
Can I still use the Azure Key Vault if I went with Digital Ocean? And would I have to deploy with Docker?
No you cant, you'd have to intall something similar on the droplet. And yes docker is I think the most simple way of getting a .net core project on there.
Respectfully, you dont get to decide what answers my question and what doesnt. Op knew exactly what i was asking and answered accordingly. Perfect. I obviously must have read the article because how else do i know Op took db first approach instead of code first.
From that, I assume a cross post is posting a link from one sub to another as I did. What's a post replicated on several subs? I have never learned the lingo :) 
Haha I know a few of these and I‚Äôm a front end dev!
If you go to any subreddit, there's a "share" button under each post, which opens a context menu with a "crosspost" option. That's supposed to be the "intended" crossposting way I think
Single is not iterating the whole collection. it just retrieves 2 elements instead of one and checks
Don't want to sounds rude, but do you guys have actual experience using microservice architecture in a real world production?
&gt;For our API to query our database we need to store our connection string somewhere **and make our BlogContext available in the API controller.** Are you sure you want to do that? I assume this is only for demonstration purposes ...
[removed]
Yes, feel free to criticize :).
Damn playa, hardcore af.
When you pass a predicate, yes it will stop when it finds the second element. But if there is no second element, it will iterate the entire collection to verify that.
What do you use if your team doesn't use VS nor windows? My team is 100% using net core and vs code.
Afraid not, but another option is something like HashiCorp Vault.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [What features would you expect from a documenter](https://www.reddit.com/r/csharp/comments/ac7cf5/what_features_would_you_expect_from_a_documenter/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I don't know where you are using interfaces but I typically use mine if for nothing more than mocking unit tests
Can you be more specific? I can't imagine this is actually the case.
I like it and might use it as is. It must have taken a lot of work, so thanks very much. The web export option is excellent, and I have an idea for another export option: export in some form that can be easily embedded in another website without reloading whole page when browsing documentation. You could also have an export-data-only option, which just spits out the latest documentation data - maybe as JSON, SQLite or something, that can replace the existing data on the website and be rendered by the same embeddable component.
Never tried it, but I guess you can Save Dumps and then copy them to Windows and use your favorite memory profiler.
You can create encryption certificates that are stored at the machine or user level. The OS protects them from casual inspection. 
Here's an example https://www.infoq.com/articles/Secure-web.config
Would that be a bad practice? I was referencing registering it for dependency injection into the controller just to be clear.
Take a look at GhostDoc's configuration page
it's really weird that after almost 3 years, net core doesn't provide this basic functionality and we have to rely on software by a 3rd party or an IDE that runs on Windows.
Wouldn't it be relatively trivial for someone who can see the source code of my app (it's an open source desktop application) to write code that can access that just as easily as my own code can?
I'm guessing you could. I've never run into a need for Azure Key Vault anywhere I've worked, but you can expose most things to the web if for some reason you must use that for managing secrets. You don't need to deploy with Docker. You could use a much more expensive image and run Windows, but I've run production .Net Core projects on both Docker or, before my team embraced Docker, SystemD. In both cases we used Ubuntu. I recommend using snaps to install .Net Core on Linux to keep up top date easily if you aren't just using Docker.
I'm guessing most developers use Windows to develop and Linux for deployment so there isn't much demand. Although Rider IDE might be a good fit for you.
In order to encrypt, you must have some unique information - password, fingerprint, something. The only way to hide data safely is to encrypt it. Which means you need to encrypt it using something only the user knows, like a password. On windows, you can use the user's current security context they established when they logged in (usually typing a password, there's that small amount of unique information) to store secrets securely. The method File.Encrypt uses the process's cryotographic service provider to perform the encryption.
It just doesn't solve my problem, we have a memory leak in production right now that is so small that takes 10 days of real production traffic to reach a critical state and we haven't been able to replicate it in our dev environment. This is running inside a linux docker image, there's no way I can install an IDE there. Our sysop team thought we could use valgrind but nope
IMO, It's better to keep the dbcontext out of API layer and all your db calls can be in a different layer eg. Service Layer. This way you keep your controller skinny; instead of having a fat controller that do everything. 
Always interesting to read about fuzzing. Especially in c# libraries as that's what I use at work. Since you didn't mentioned it, have you heard about [Fuzzlyn](https://github.com/jakobbotsch/Fuzzlyn)? It fuzzes the RyuJIT for discrepancies between programs compiled in debug and release mode. 
I had projects in Azure that I had to take down because paying monthly for the service was getting expensive. Should I still put it on my resume? I feel worried putting down defunct sites on my resume. I also have other projects that I like to keep private as it has my Google API key and other such things. 
Can you recommend an alternative to using Key Vault? Where I work everything connects using Windows Authentication so I've not had to worry about passwords being in configs before. I'm using a shared hosting plan with A2hosting currently but I'm worried about putting secrets on there unprotected. Thanks I'll check out the Ubuntu set ups. I've never used Docker before and I'm fairly new to .Net core too so I'm not sure what is best practice.
Thanks not heard of that before, I'll look into it.
I use hangfire for it. Works great. If your web app is configured to be always on you can use it quite easily. If not you can create a Windows service with topshelf and hangfire. 
My only 2 ideas are: 1. Save dump files and analyze them [http://blogs.microsoft.co.il/sasha/2017/02/26/analyzing-a-net-core-core-dump-on-linux/](http://blogs.microsoft.co.il/sasha/2017/02/26/analyzing-a-net-core-core-dump-on-linux/) [https://stackoverflow.com/questions/27598986/how-to-analyze-memory-leak-from-coredump](https://stackoverflow.com/questions/27598986/how-to-analyze-memory-leak-from-coredump) 2. Use [ClrMD](https://github.com/Microsoft/clrmd) to log memory instances on your production server. Maybe log all instances greater than X memory and then slowly add logs to find their roots to GC path &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B; &amp;#x200B;
that may help us, thanks mate
If another process runs with high enough privilege, it would be able to inspect your process to get those secrets once your process uses them. File.Encrypt on windows or PKI (cross-platform) should be sufficient to store credentials. You aren't going to get much stronger protection unless you require the user to remember a password or start using HSMs or protected environments like intel's SGX.
Yep. If it's on the user's computer, the user can access it. 
I was just wondering if generating a self-signed cert was a bit overkill for the protection it provided. Something more akin to File.Encrypt is what I am looking for, but I see that it's not only Windows-specific, but the Docs article says it isn't even supported on Windows Home.
You could look at data protection API (DPAPI). It allows you to protect data at the machine level or user level. If the files were moved to a different computer they can't be decrypted. There is still a problem of another app decrypting it. You would want to add some extra secret as the entropy. The best option would be something inputted by the user. E.g. a password on launch of the application. However you could generate a secret and then encrypt that so that the protected data or file can't be used by it's self. A quick article about threat mitigation: https://docs.microsoft.com/en-us/windows/desktop/secbp/threat-mitigation-techniques You could also try using Isloated Storage in addition to protection: https://docs.microsoft.com/en-us/dotnet/standard/io/isolated-storage#secure_access Disclaimer, I've only had to deal with passwords (properly) twice and they were stored on a secure server (i.e. no user trying to download random apps, and the software was not published). Also if you aren't already look at using protected strings to capture and pass the password around your program. Lots of APIs accept them in place of plain strings.
[ProtectedData](https://docs.microsoft.com/en-us/dotnet/api/system.security.cryptography.protecteddata?view=netframework-4.7.2) looks like it's good for win2k+. Use the protect/unprotect methods.
Agreed, although I usually find I want to have that level of implemention control without needing to instantiate. It depends on if I'm building something for 1 purpose, or to be reused, but for purpose built components adding an interface just increases indirection
Regarding DB modifications, I wasn't very clear. If my constructor must provide my database dependency I can instantly grok the constructor and be confident in whether my class has access to a database to make changes, which is a given with dependency injection, as opposed to needing to look over the entire object for a database instantiation, or reaching out to some global obect in order to ensure it doesn't modify any data.
Also, in the grand scheme of things I'm incredibly envious of your experience and wish I had only ever seen interfaces being injected. While it leads to more indirection, if it was 100% consistent it would improve the flexibility of our codebase so mucb 
IMO absolutely. You can link to the git repo and mention what the project does and how it's built. Anything that gives a realistic indication of what you can do is really useful. I've been hiring devs for the past 6 years and I'll always take a look through a potential candidates git repos if they have them mentioned.
I get what you mean completely. I call them bread and butter developers. They get the job done, but only in a basic way. The upside here is now that we have a pretty decent PR code review policy, these people get less of an opportunity to fly under the radar anymore, coasting along.
Yes, C#.
Does the web app have to do the process? Seems like that would be an easy executable attached to Task Scheduler. 
Thank you kindly :)
The deployment agent should only be installed on the machines that will be **executing** the deployment. The TFS deployment pools are not the same as IIS pools. I think that is where you are getting confused.
As long as your application needs access to the secrets and holds them in its memory at any point, that means the user must by necessity have the ability to view those secrets (assuming the application runs under the user's account).
No, task scheduler could do that but my dev suggested to go with something more robust that will help implement other features better down the road 
Great article, thanks. I really wish MS would make memory usage much easier to manage. All I really need to see is a list of objects that take up the most memory at any time, but apparently that's too difficult. Frustrating. 
You might want to make sure when you post bugs to projects to check their contribution guidelines first, it looks like the SixLabors guys are touchy: [https://github.com/SixLabors/ImageSharp/issues/798](https://github.com/SixLabors/ImageSharp/issues/798)
That's right. And inside the service layer you hold an instance of the corresponding repository which provides all necessary methods for accessing the database. 
I‚Äôd suggest keeping the https requirement turned off in dotnet and let the reverse proxy (Nginx) handle all the SSL and serving the endpoint. 
I have hunger, many people do, doesn't mean I need to be doing code elsewhere to have a hunger. 
I never said they were mutually exclusive?
Could you not just put your mail client creds in the appsettings.json file and just make sure that file is listed in .gitignore? I thought that was standard procedure? If using docker, you can also use docker secrets.
That's not the junior dev's fault. FirstOrDefault is the right way to go about this due to performance, as it stops running when an item is found. You should have active logic that restricts duplicates. So I'd argue whoever was the lead on this project from an architectural standpoint needs to find a new line of work, because that's, ironically, a junior level mistake.
FirstOrDefault is right here. SingleOrDefault is wrong as the latter runs through the whole list of items. This should be on a database constraint, anyways. I hope you gave that junior an apology, because it looks like dev leadership fucked up here.
I'm definitely not including mail client creds in GitHub, but leaving plaintext passwords in a file on the PC just kinda feels wrong.
Of course. But ideally, if someone pulls the application config file off that PC, or accesses it from another user account, the password ideally should not be readable.
For API, I've been using Swagger to make it easy to see what calls exist and what parameters are needed as part of the call.
Assuming you're using ASP.Net MVC Core for a web framework, you could do something like this. I get the sense from your question you probably still need to learn what ASP.Net is in the first place. using System; using Microsoft.AspNetCore.Mvc; namespace YourApp { public AirplanesController : Controller { [HttpGet("/airplanes")] public IActionResult Airplanes() { ListAirplanesResponse airplanesResponse = GetAirplanesList(new AirplaneManagement.ListAirplanesRequest); using (var writer = new StreamWriter(Response.Body)) { foreach (airplane in airplanesResponse.Results) { writer.WriteLine("Airplane: " + airplane.Name + " | " + airplane.Engine); } } } } }
Why does it feel wrong? It's only used in production. Your dev settings should be in appsettings.development.json.
Yeah sounds like you want to go with encrypting the password using the Windows CryptoAPI methods that do that (this is how Chrome encrypts saved passwords) or the DPAPI that other guy mentioned that sounds like it does the same thing.
With random read distribution, First/FirstOrDefault will run in half the time on average of Single/SingleOrDefault.
Seconded the DPAPI. It's easy to use and you can easily create a base class and extend for different applications. A simple example would be a base class that handles encrypted registry entries for company "ABC", like perhaps the license key. Your app, "DEF", could have it's own class derived from the base to handle "DEF" stuff. "GHI" app has it's own, etc.
I've been using Quartz.NET for all our scheduled processes. It can be scheduled pretty much any way you like. Hourly, daily, certain times, only weekends, whatever.
If you can run in .NET core you should look into IHostedService, does exactly what you need, with shared threading with the web app. Probably there are already some quartz like implementations that use this nifty interface. 
If this is used in development time, the recommended and cleanest way is described here: https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2
That is very interesting
If you're looking for a cross platform version of DPAPI I'm pretty sure that they recreated something similar in dotnet core. https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/introduction?view=aspnetcore-2.2
I got it. It makes perfect senses actually. Great üëç explanation by you. I think I‚Äôll try to use DI as much as I can, certainly for new applications.
Well keep in mind there is no difference between a console app and a web app. Just that the web app uses the ASP.NET packages to host a web server. So you can even still do Console.WriteLine and see the output in the console if you run your web app from the console. Of course that's generally not how you want to do things. What you probably want to do is create a REST API. This is basically a url that, when you load it up in a web browser (or, more usefully, an application loads it in the background) returns the content you want.. In this case it can be a text blob describing all these airplanes. I recommend looking up documentation on MVC and Controllers. You don't need to do the full Model or View bits but Controllers are essential for what you'll want to do. I think all you need on top of a basic ASP.NET Core app is a call to .UseMvc to set up MVC support, and then you can add a Controller class with a method that runs the code in your original post. You can put a RouteAttribute on the method to control the url used to access it. To actually output data, a method for a controller should return an ActionResult. ActionResults can be of many different types. To return a simple string you can use the this.Content function on any controller which can take a string of content and another string describing the type ("text/plain" for simple text content).
Yeah this is pretty much what I'm talking about in my post about a Controller. Once you get to the level where you're actually making an API and returning JSON data or whatever, it's probably better to generate and return an ActionResult rather than using Response.Body directly. But this is probably good for OP's first go.
SPDY is Chrome's internal name for modern HTTP transport protocols, so your browser might be trying to use HTTP 2.0 or something. Check the bits of your project that deal with HTTPS support. If they specify a certificate make sure it exists and is a valid certificate for use in web requests (it needs a private key and you must specify the correct password to decrypt it in your app). If the project explicitly enables HTTP2 support the problem may lie in those bits of code. I have not used HTTP2 myself so I am not sure about what specifically would cause this error.
This is the exact link that I would have suggested, after stumbling across it just this morning.
App service hosting for low volume usage with a custom domain is about $10 per month on it's own. Table storage is the only cheap azure DB solution, and it is stupid cheap, but may not work well with entity framework. The guy who runs haveibeenpwned uses azure table storage and wrote a good blog post on it.
Well, its barely critics, I just want to do some reality checks, been googling a lot to find close to real-world examples and not just Netflix, Facebook, blabla. My experience is that I was hired to support a bunch of microservices wrote by a 3rd party outsource company. It was very confusing, because not only it was microservices - it was poorly written microservices with the lot of anti-patterns. So I try to do things the right way, and even after some research still struggle even with the basic things like some shared data between ui pages. Suddenly Its like building a rocket, when in fact its just a simple CRUD. And the only benefits are to have separate teams and horizontal scaling, well, monolith can scale too, is that really that much expensive compared to the microservice architecture? Correct me if i wrong - i don't think so. What do you think as someone who implemented fairly by the book example?
[www.grapecity.com](https://www.grapecity.com) has a fast pdf library for .net core called GrapeCity Documents. 
Sweet! This looks super useful man, thanks!
You probably want to backround the task out of the user's Http request context. You could look at using something like Hangfire to enqueue and process the job. You'd probably need a way to track the status of the job so you can display whether it's in progress or done to the user - im not sure if Hangfire provides such an API in their SDK libraries, or if you'd maybe want to track the status in your own table. Secondly, the background job should save the generated file off to a separate object store. That's where the user would end up downloading the file from. If you can use something like AWS S3, I've used their presigned link generation to send a redirect to the user for to eliminate the need to stream the whole file through the app server, without having to make the S3 bucket itself public.
I released [api2pdf.com](https://api2pdf.com) for this specific purpose. You can use the .net client library to connect to it conveniently here: [https://github.com/api2pdf/api2pdf.dotnet](https://github.com/api2pdf/api2pdf.dotnet) It's been tested with easily printing PDFs of hundreds of pages, even PDFs with file sizes in the gigabytes. Give it a shot and see if it works for you. Feel free to let me know if you have any questions.
I would check out the calculator again - you should be able to find a db that is sufficient for your traffic at $10 per month
I only interface for stubs/mocks in TDD or to fake multiple inheritance. DI gets an interface or a class if it doesnt have one.
I do get to decide. You must not of gotten the memo.
Leave a honeypot file and if someone attempts to use it log the IP and send message to user asking why you should not ban him for attempted hack of your system. 
This is not how you secure systems*
Sorry I was not clear. I meant to do this in addition to properly securing system.
Because I want to find a solution that bring the consistency when user print/download a small file and print a large batch of files. &amp;#x200B; I tried the method of printing by using queue and background worker and report back the progress using realtime (SignalR), but the result is user has to wait to see the background progress is done and download the file. This experience is like I said, so different from user just click print expected the result (in a "synchronous" expectation). &amp;#x200B; &amp;#x200B;
Display a list of all users on your page. Click a user that will take you to an edit page. Change the the role and hit submit. Submit action updates user.
I see, the UX of the seemingly automatic download is important. I still think the concept of saving off the file to a separate object store / CDN is important. And so is the potential for offloading the processing to a backround worker that could potentially be on separate infrastructure from your main web app server. Both of these concepts I would consider the "modern" approach in terms of scalability. Is it possible to initiate the download upon SignalR returning the success status?
We use [hashicorps vault](https://www.vaultproject.io/) that octopus accesses when deploying our programs and replaces values in our app settings/web config files. 
With what controller? There's no controller generated by Identity, and if I try to scaffold controller + views for the ApplicationUser class, I get a "Multiple object sets per type are not supported" error at runtime.
Where / how are you hosting? AWS has param store and Azure has Azure Key Vault and they both have .net libs to retrieve at app startup. 
Are these app secrets, as in API keys or database connection usernames and passwords? If so, you could look at the [Azure Vault](https://docs.microsoft.com/en-us/aspnet/core/security/key-vault-configuration?view=aspnetcore-2.2) and [this article on secrets](https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-2.2&amp;tabs=windows).
You'll have to create a user controller. You'll also likely have to create a user repository to run on your dbcontext to get all the users (I'm not sure if usermanager has a get all users method).
So weird this is where assumptions of .NET have moved. ...It's a desktop app.
No, this would be user-configured passwords on their machine. I just don't think they should be in plaintext. I don't want them reversible if someone were to acquire the configuration files, so I feel like a machine-specific key should be used to encrypt them. But I'm definitely curious what people's expectations are for this sort of thing.
I dont like you and i want bad things to happen to you
I've heard often it's "subset of C" or "augmented C" for performance critical code. I never thought how/if these projects solved the visual tooling problem. I'm glad to see the emphasis on ease of use / visual tooling. 
azure keyvault
Yeah, you can't really protect this stuff, but I do use DPAPI on Windows. The best protection is a secure machine.
I know how it feels, and unfortunately, the sad truth is that some (or maybe a lot) of companies try to incorporate particular tech, mostly because there's a lot of hype going on (microservices, blockchain, ML etc.) without giving it a second thought whether it really makes sense at all. What I've learned so far (I started playing with microservices over 2 years ago) is that a lot of things can go wrong along the way, simply because you might be not aware of the potential challenges that you're going to face and there are patterns to deal with that, but it's not a silver-bullet - some of them are useful, yet the other ones might make your code more complex if not used properly. Usually, the best approach is to start with a monolithic app and then slice it down (if needed) either to microservices or so-called modular monolith (which might be also a good idea). There are techniques that can help with defining the boundaries between services and the messages (command, events), views, hotspots etc. - take a look at Event Storming. Having this first context map available and the general overview between distinct microservices (or bounded contexts if you're into DDD world) gives you the possibility to start implementing the code with the clear boundaries in front of you. If you're stuck with a poor architecture, that was created by some external company, it might be difficult to work with. You can look for the techniques such as isolating the "legacy/poorly written code" with anti-corruption layers and then creating the proper services that will talk to the "legacy code" via such layers and so on. For example, shared data between UI pages - it seems more like a front end's concern, unless you mean something like session, tempdata etc. - maybe a simple Redis caching + common key convention will be enough? If you have some other questions, related to particular issues or patterns, please ask them either here or on our forums [https://forum.devmentors.io/](https://forum.devmentors.io/) so it doesn't get lost :). &amp;#x200B; 
As long as you store it on the users machine, there‚Äôs virtually no way to do it completely impenetrable. (Remember the top rule to security is don‚Äôt trust the client) well in this case, you can‚Äôt even trust the ‚Äúserver‚Äù since it‚Äôs the client. Usually, people will authenticate with the credentials provided by the user and never store them, then get some sort of leased token back that they use from then on forwards. This is why you are sometimes asked to re-authenticate for these kinds of things.
Are ArgumentException and subclasses really unexpected when using a fuzzer?
Take a look at Vault [https://www.vaultproject.io/](https://www.vaultproject.io/) \- it's a very robust and easy to use centralized credentials storage (BTW I created a project back then [https://github.com/lockbox-stack/lockbox](https://github.com/lockbox-stack/lockbox) before there was even Azure Key Vault). Vault is easy to integrate with, there's a library for .NET / .NET Core [https://github.com/rajanadar/VaultSharp](https://github.com/rajanadar/VaultSharp) and I've also created a wrapper on top of that, which loads the settings (JSON format) during the application startup for [ASP.NET](https://ASP.NET) Core (for classic .NET it will work in a similar way) [https://github.com/devmentors/DNC-DShop.Common/tree/master/src/DShop.Common/Vault](https://github.com/devmentors/DNC-DShop.Common/tree/master/src/DShop.Common/Vault)
Useful article for everyone with code reading from a StringBuilder.
I based my decision in my last project on this article: https://weblogs.asp.net/jongalloway/428303
For solution 1: - you'll need some kind of script that checks the files and deploys accordingly if you only want to upload the files that changed. - Use `dotnet application.dll` instead of `dotnet run`. - You only need to restart the application, not the VM. - If you're rewriting existing files, you'd need to stop the application, rewrite the existing files and then start the application again. - If you choose to go the blue-green way, you stop the old application and start the new application (on the same port as the old one). Solution 2 will only upload the layers that changed. While the layer that changed will most certainly have files that did not change too, it will definitely not be 100MBs. IMO this is a great option considering its low maintenance over 1. No experience with EBS to share for solution 3.
Yeah I can see the advantage of that if your API grows big enough. Especially if you're duplicating queries. Appreciate the feedback.
&gt; Finish the scenario that was supposed to free your instance from references. That line needs to be highlighted better in Point 4. Especially in complicated application, the reference might never be freed during debugging.
Hi, if you're talking about connection strings could you describe a little bit more about what your problem is? Your DB connection string should be stored in the Application Settings area in your Azure App Service resource. If you're developing locally you will use a local database and the connection string can be stored in a user secrets json. I'm not sure why you would want to use Azure Key Vault at this time for such a trivial matter unless I'm missing something?
DatoCMS. It‚Äòs a headless cms. We are using angular in the frontend with ssr. 
https://headlesscms.org/ gets shared over at /r/webdev every now and then. I don't have any personal experience with any tho. 
On the pricing: $150/month is too much. &amp;#x200B; With Azure you need to realise that the "App Service Plan" resource is what you're paying for. It's essentially the machine your apps reside on and it's what you will be paying for. With that in mind it's worth having one App Service Plan for many apps. That's what I do and you can save money by doing so. With the database I'm using Azure SQL which is basically MS SQL hosted in Azure. It's costing me around ¬£4/month which is nothing. Here's what I'm using App Service Plan: Name: " **ProductionServerServicePlan**". This is located in its own Resource Group called "ProductionServer". I am currently using plan S1 which is about ¬£69/month. This plan allows me to use staging slots, ssl, backups etc. I've then got two dot net core apps (both using their own Azure SQL) (both in their own Resource Groups) using the above **ProductionServerServicePlan.** I think I'm paying around ¬£80/month which isn't bad. My apps are both MVC using entity framework and work fine on the service plan. It remains to be seen how many more apps I can fit on that plan - the price per app goes down the more you add. I need to do more research into memory usage but as you can see this is one way to host on Azure. I'll have to write a blog about this... Hope this helps.
Umbraco
Umbraco (my personal go to cms), Sitefinity ($), Sitecore ($$$)
I'm trying to follow along with the Microsoft documentation and using Entity Framework Code First to set up the database. It generates the connection string in the code with this note: #warning To protect potentially sensitive information in your connection string, you should move it out of source code. See http://go.microsoft.com/fwlink/?LinkId=723263 for guidance on storing connection strings. When I've followed through the links I get to a bit on security which is recommending to use Azure Key Vault for production. Initially I was going to use shared hosting with A2Hosting, my understanding was I couldn't use environment variables, and I don't really understand how that's any more secure anyway. I'm worried about how to deploy and ensure my database connection is secure. I don't want to store my password in appsettings.json because it's plain text and it can get accidentally committed to source control, but I can't figure out what to do otherwise when I'm ready to deploy to production. Lots of articles I've read have said to put it outside the project in another file and encrypt it, but if someone gains access to the server my project is on then they can still see that and access the database can't they? I realise realistically the odds of someone getting access are slim but I'm trying to follow best practice and make sure my user's data is secure. 
Another vote for Umbraco. Orchard seems to be pretty popular but I found it bloated and overly complicated.
Thanks I'll check out the table storage and the blog
I used Cockpit for a few sites. I've also experimented with Squidex about a year ago but haven't had a chance to use it in production. Looks good from what I can tell. Honestly, though, we find WordPress satisfies a large chunk of our clients. I don't really like working with it but it works just well enough.
I have tried umbraco in a few projects and I like it too. Seems popular 
Umbraco and episerver($$$)
This sub is for .Net
Sitecore and sometimes Umbraco
Thanks for the reply, this is all very helpful! When I initially came across this issue these were (sort of) the steps I went about to fix it but it didn't seem to be the issue. I also have other WebAPI's that I run on VS 2017 with no issues, or at least not this issue.. I'm actually watching the course but not building the same app, I'm using something a little more complex and challenging myself to hit pitfalls as the course progress, see if I can pick up on patterns that entity framework/.net core provides. 
Sitefinity
Can someone cliff note me on why people use CMS sites instead of writing from scratch? I'm guessing it's for common prebuild functionality required by the clients? I haven't used a CMS since about 10 years ago with dontnetnuke (barf).
Massive flexibility and ease of use for the content editor.
For those using Umbraco, do you use the Umbraco Form plugin too? We don't have a CMS but we started using K2 (SQL backed) for forms.
Your appsettings.Development.json file should contain the following for your database: "ConnectionStrings": { "DefaultConnection": "Server=(localdb)\\\\mssqllocaldb;Database=5655566767767678;Trusted\_Connection=True;MultipleActiveResultSets=true" } Then in your Azure web app you need to go to Application Settings &gt; Enter the following in the Connection Strings area: DefaultConnection: \[YOUR LIVE CONNECTION STRING HERE\] What's going on is your web app will replace whatever is in your appsettings.json file with whatever is in the application settings of your web app. No need to worry.
I know one of our customers has the Umbraco Form plugin, haven't tried it myself tho. 
Yes, I guess it's mostly because it's very good for the content editors. But also saves a lot of development time. 
For those using umbraco, do you use some kind of view-model mapper like Ditto? 
For customer websites we use Sitecore...although we did build a few cheap Wordpress websites a while ago
passwords should never be stored, unless it is necessery. (Which it often isn't) Use a proper Hash-Algorithm like BCrypt/SCrypt/Any other accepted password hashing alg. and store the resulting Hash in a file/database/etc. Later on, in your application, you just verify that the user entered password (eg. InputBox), matches the stored Hash. BCrypt implementations for example often provide a \`public bool ComparePassword(string plainText, byte\[\] hash)\`-Method for this kind of stuff.
We use ditto. It's literally amazing and helps rapid development. Highly recommend it. Works well with all other packages from umco
Yep we use umbraco forms. We've extended it loads of times too.
Umbraco and sitecore
Sitecore, EpiServer, Piranha CMS
It allows us to build our own framework and small pocos to work with. It's easy to use and fits alongside other frameworks.
It allows us to build our own framework and small pocos to work with. It's easy to use and fits alongside other frameworks.
We start using Orchard Core in Decoupled mode. 
Because it makes no sense to build and maintain a custom site unless you need something massively different. It also depends on the maturity of the customer. If they are mature and ready to take on the responsibility of a home grown product, or just looking to get something in market. You need to take into consideration the constant maintenance and upkeep, most peole forget about that....
Sitevision and Sitecore 
Ok nice. I have used it in one project as well. I liked it, but does it support the latest versions of Umbraco? Would be kinda bad to build a site around Ditto and suddenly it's not supported.. 
Hard To read on a mobile:(
This would be extremely unhelpful since my application needs to be able to *log into something else* with this password.
Mainly use Umbraco, have used Episerver and Sitecore before as well. Would rate them in that order. 
Depends on how you are going to deploy the apps. IMO one solution would be good. 
What? In tons of cases if you delete the interfaces the application will still work the same in its *current* state. It just makes it that much harder to swap out or reuse components. That's the whole point of writing modular code.
Do you mind explaining why you rate umbraco over epi? 
Depends on the use case, preoptimizing can be an antipattern.
The fact that EF Core doesn't support many to many relationships without explicitly creating a separate join table has been my biggest pain point. Other than that its worked pretty well for me.
I feel like Sitecore should be ($$$$$).
If you are going to consume the API from other projects e.g. mobile app, backend code or expose it to other users then you might want to publish the API without the web app. If you work with another developer (one doing front end one doing back end) there is a clear separation of projects. You can add test projects that target each project. I'd keep it separate as it's easier to do it now than later.
&gt; writer.WriteLine Thanks! Instead of using writer.WriteLine, would it be possible to just return a JSON object?
Previously my employer used Composite C1, now called Orckestra. I like it pretty well for smaller sites. My new place uses DNN and I really don't like it. We are planning to migrate off it.
I used it on the very latest. Spoken to Lee one of the main developers and he's very supportive. Go ahead and use it and feel comfortable with it. Any problems let me know. My GitHub has some examples of sandboxes etc. GitHub.com/garpunkal.
Mainly for the community aspect, there a good number of packages available and it's pretty flexible in that way. I prefer the editor of Umbraco, although the blocks and on page editing of Episerver is nicely done. There's also the open source, free to use side to umbraco whereas Episerver requires a license and is rather expensive. Episerver gives you a bit more fully featured solution out of the box, umbraco you have to tailor it a bit more and there are a few quirks. v8 is due to be released soon with an updated ui, some new features and code clean up. On the whole, both are good options and depends on budget, project. 
We use a CMS tool for building our unsecured content. Aka, blogs, articles, info, marketing pages. That kind of stuff. It lets the business side of the house make updates and changes without getting developers to roll out a new release. Post secure (aka post login) where the real functionality lies we built custom with Angular + dotnet core. This split has worked well for us minus a few hiccups of getting the two to play nice when needed.
Thank you! 
Thank you! Will have a look at your examples! 
Copy editors are cheaper than Web Developers. A copy editor just needs to know how to teh English, they don't need to understand HTML/CSS/JS, the CMS takes care of that. 
The past few projects are not applicable but the last one that I am still supporting is DotNetNuke *sigh*
For the moment, it's just me. The API will be exposed to other folks at some point. So publishing wise, API will be published separately from the web frontend. 
Sitecore seems to dominate the paid market for CMS. It's super expensive though and it's often overkill if you're just using as a CMS. The product, while incredibly flexible, has too small of a community behind it and little documentation outside of what a few paid spokes people contribute. I live in the largest city in the US and there is little or no community usergroup. 
Umbraco is great if you don't want the end user to have a ton of control over the layout. I.e. a picture goes here, you can choose the picture, but it goes in this spot. DotNetNuke is more powerful, but depending on your scenario, you could be giving your client enough rope to hang themselves. As a developer any cookie cutter site I do, I always do in DNN, even though I have the capability to build it from scracth myself. The UI is pretty good and it's pretty powerful. If you need additional functionality, there are tons of good modules out there for &lt;$300 that will pretty much do whatever you need. 
The exact same reason you'd use a third party library in your code, as opposed to writing it yourself. Why do all that work if someone has already done it. I can spin up a CMS site in minutes. The longest part of it would be creating the design, which if you want custom, you can't get around. Once it's up, I can add the pages and content, deliver to a non technical client, and then they can add pages, edit/update content, etc. Building something like that from the ground up would take months. 
As an aside, here's a directory structure to consider: https://gist.github.com/davidfowl/ed7564297c61fe9ab814 $/ artifacts/ build/ docs/ lib/ packages/ samples/ src/ tests/ .editorconfig .gitignore .gitattributes build.cmd build.sh LICENSE NuGet.Config README.md {solution}.sln 
Sitecore.
they're semi old examples, but if you have any questions, feel free to ask me, I've built some pretty large products using Ditto and Umbraco. 
In Europe if you happen to have Sitecore experience listed on the CV, recruiters keep sending emails all the time.
The community, open-sourced, extensiblity and flexibility are the stand out features of Umbraco IMHO. 
Can your sales team write code?
SharePoint and now migrate off it to Sitefinity ($$).
[AspNetCoreRateLimit](https://github.com/stefanprodan/AspNetCoreRateLimit) | AspNetCoreRateLimit is an ASP.NET Core rate limiting solution designed to control the rate of requests that clients can make to a Web API or MVC app based on IP address or client ID. 
This is actually a pretty good broad questionnaire for someone around the 2-3 year mark. I like that it focuses on implementation details, such as ef vs ado, single vs singleordefault, some advanced features like delegates, and also covers knowledge of patterns such that of DI and async. Depending on job responsibilities and the tech stack at said company, you could possibly veer into topics such as source control, microservices, containers, and maybe some .net core specific features.
this. I circumvent using anything Azure related by using dotnetcore on a linux box. pulling the connection string as an option on startup from the appsettings.json is sufficient if I am not allowing public access to the repository. I save tons of money this way and there has never been a security concern... unless someone would like to provide some constructive criticism on the practice
A solution is basically just a container that lets Visual Studio keep track of multiple projects. I'd never introduce more solutions than strictly necessary.
Same here. Mostly Episerver though. 
I didn't read properly - what Ravek said, you want one solution with multiple projects.
Does a bear shit on the moon?
Sitecore \- Mostly for bigger projects, and especially for larger clients Umbraco \- Generally for smaller clients and projects &amp;#x200B; I just wish both would hurry up with changing to .net core! &amp;#x200B; We're exploring Sitecore headless with JSS to give us more freedom. &amp;#x200B; We do not use any other CMS products. If there's no CMS we're using .net core APIs and React.
cool. thank you.
thanks Ravek.
&gt;Sitecore seems to dominate the paid market for CMS. It's super expensive though and it's often overkill if you're just using as a CMS. I don't necessarily agree about the small community. It's not huge, but it's definitely there, and there's a lot of good info out there. However, Sitecore themselves produce some terrible documentation, and that can cause a lot of problems. Sitecore is hugely flexible, and once you grasp the basics it's actually not that complicated. It is ridiculous overkill for most of the sites it gets used for though, and should be componentised and not a big monolithic app. That's changing with the more recent releases, but there's a long way to go. I'd also say they do often try to make the app more 'enterprisesy'. which often just means insanely overcomplicated, badly documented, and nightmarish to use. See the new install process for Sitecore 9. There's a lot of money in Sitecore contracting right now if anyone wanted to do that. The day rates are very high here in the UK.
It might, it's just going to be a bit costly. So, same answer really.
Just create your own controller. You can't rely on scaffolding for everything. Plus you need custom forms anyway so it wouldn't work.
They have their own StackExchange [https://sitecore.stackexchange.com/](https://sitecore.stackexchange.com/)
&gt;However, Sitecore themselves produce some terrible documentation could not agree more. &gt;There's a lot of money in Sitecore contracting right now like 100GBP/hr?
&gt; Umbraco But why for such cases not to use something like WordPress? .NET devs are way more expensive than PHP devs.
Looks to not be actively maintained, still useful for some direction.
Yes. 
Orchard
&gt; .NET devs are way more expensive than PHP devs. Our company has loads of dotnet code. Hundreds of back end processes and years of experience going back to dotnet 2. We have a foundation of dotnet and a whole pool of dotnet devs available.... so why would we go higher php devs if we already got dotnet foundation?
In that case then there's no point. But for example if it would be a freelance project where customer wants CMS, would you consider such option as WordPress, or you would go with Umbraco, since you know it already and are experienced with .NET? I'm not PHP developer, but was just curious since I found out about Umbraco today. I knew only Orchard as CMS for .NET and our company wasn't pleased with it.
It's really hard for me to answer this for several reasons. First, I have never used Umbraco or WordPress. The only little experience I have with WordPress is that it's feels very difficult for a real developer to use. I can hardly find my way around WordPress. The CMS tool we use is TeamSite which sucks monkey nuts and should be forgotten into the abyss of time. Also, I have never been a freelancer. I guess I would probably stick with what ever tool was comfortable for me and fit the clients needs. So I may stick with Unbraco because I know dotnet. But I have to imagine that WordPress is more popular so I very well may just learn WordPress because it will make life easier in the future. 
YANGI?
Thx, looks like it needs some work based on commits
I don't know, I swap out databases and libraries all the time to chase performance during the early dev phase. Without an interface how am I going to switch from in memory to a RDBMS without rewriting a bunch of code?
What about giving a try to containers e.g. Docker? You could simply create a Dockerfile for [ASP.NET](https://ASP.NET) Core app, and by using Docker on your VM just pull and update the latest image version (which could be even automated using scripts).
Cool, I'll have a read of a tutorial. Thanks
Ef core in memory provider? If im testing a complex bit of logic I will interface out the datasource and stub it for unit testing. But IMHO adding interfaces for everything because of DI seems like added complexity for zero gain. On a side note writing modular code also has a cost to it and should be another tool in a developers chest, its not a golden hammer.
You could try nexuscore in combination with cpanel (a hosting panel for php), it will manage restarting on your behalf. Also it's free up to 5 accounts (www.nexuscore.io).
Not too good at kestrel but can't you blue green it? Or Upload to new folder (version number) then update the kestrel sys config &amp; reload
cpanel runs kestrel now??
I would never voluntarily work on another Sitecore project. Waaaay too cumbersome and far more powerful than what 99% of most use cases will ever need. I have rolled my own CMS a number of times. Simple, effective, maintainable.
Why not go new-school and use Ghost? [https://ghost.org/](https://ghost.org/) " Ghost is a fully open source, adaptable platform for building and running a modern online publication. We power blogs, magazines and journalists from Zappos to Sky News. "
In the pricing calculator add a Azure SQL database. Change the type dropdown to Single Database, change the Purchase Model to DTU. Cost per month = $4.90. I have a setup just like you describe including key vault and DNS. I am on track to spend $76 this this month which is about $10 more than what Azure pricing calculator said I would spend. I just switched to Azure a few days ago.
I have pretty much the same setup as yours, but use rsync from a wsl bash window in place of your #3. Works very well and fast. (And command line msbuild instead of #1 &amp; #2.)
Umbraco. Used a lot in the government of Alberta
Have a look at [Hangfire](https://www.hangfire.io/). Sounds like exactly what you're looking for.
talk about throwing out the baby with the bath water... your deployment has an extra step or two? just throw away and reinstance the entire server instead. nevermind that you already said you have flat files and logs on that server - rearchitecht it not to need those and then Docker. because Docker. lol
There probably is a better way of doing it but you could try spinning up a task in your http request and return your eventual result over a websocket. 
With that plugin it does that yes. You can simply deploy through FTP like you are used to (it comes with a guide for hosting customers). Basically it's a plugin to allow shared hosting companies to offer dotnet core hosting to their customers. 
Why would you use Single then if First with unique constraint is always better?
Hangfire is great. If you‚Äôre using an Azure App Service, [Azure Web Jobs](https://azure.microsoft.com/en-us/resources/videos/playlists/webjobs/) are the majority of what I use.
Have you tried the grid editor in umbraco? This allows a user to build a page up using the standard grid.
Forms is pretty good and very easy to extend. I built out some really cool workflows with it.
Yep, just `return Json(airplanesResponse);`
Nobody? I would expect most people know all answers. They are pretty basic questions.
Apparently, you have never conducted a phone screen for an open position before. Also, if you knew all of the answers, send me a message with your email. I‚Äôll reach out next time I need a developer.
I have this sticker on my work laptop: https://damianbrady.com.au/2018/02/01/friends-dont-let-friends-right-click-publish/ What you're looking for is a deployment system to handle the deployment process, well, for you. Azure DevOps is free for small teams and open-source teams, so if you're just working on your own website you can get plenty of build server time for freeeeee. Here's an example of how to get it set up: https://docs.microsoft.com/en-us/aspnet/core/azure/devops/cicd?view=aspnetcore-2.2 Note that this assumes you're using GitHub. If you're using something else (like a personal git server) the setup can be a little more involved.
Flat files and logs on the server would arguably be even cleaner with Docker and volume mounting. Especially, considering the OPs problem statement, and the fact he's literally manually deleting files after deployment. Add to that, he's positioned better for a 0 down time deployment model (especially considering he's using nginx where he can simply redirect to the newly running container). Docker is likely a very viable solution, and might be the best long term solution. Not sure, why you feel this would be re-architecture or is even a big deal...Do you just have something against Docker?
How long is it going to take to make the zip file? You could always fire off a task that continues to run and emails a link to the user once it is finished. You can just do Task.run I think.
It might be a bit heavy, but I highly recommend Octopus Deploy.
Docker. Works wonderfully with dotnet and kestrel.
Look at the IHostedService that was added in aspnet core 2.1: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-2.1 Sounds like to do what you want.
I kind of see where u/ifatree is saying though. OP has a working pipeline, and with a script or two can have zero down time and cleaner deployments. Adding Docker as the first thing to try is a little overkill. There is a whole tool chain to Docker and then learning how to handle file access, database access, cleaning up caches, and more. I spent nearly a month learning Docker for research into creating a DevOps pipeline and even now I wouldn't use Docker unless I was building something new from scratch or something I needed to scale dynamically (or maybe for some micro services). That said, if he did get Docker in her pipeline then it would only take them two commands.
Lots of good advice so for for you. So here is my two cents (repeating some of what has been said). * Look into build automation. For C# I like using Cake build scripts: [https://cakebuild.net/](https://cakebuild.net/). Self installing if needed, covers most build needs and automation. Uses C# for the scripts, so no need to switch context. You can automate stuff like building differnt version of your app, running tests, cleaning up files, running commands, probably even FTP. * If you are deploying from unix try the rsync command. It can check to see if files have changed (timestamp or file size) and will only copy files that have. It can also (if I'm remembering right) do partial transfers if there is enough to rebuild the file making the transfer quicker in some cases and lower bandwidth (again, I think): [https://rsync.samba.org/features.html](https://rsync.samba.org/features.html). * Use blue green deployment: [https://martinfowler.com/bliki/BlueGreenDeployment.html](https://martinfowler.com/bliki/BlueGreenDeployment.html) The idea here is to have two instances of your application running at the same time. Deploy to one, and the switch traffic to the new build. Nginx might be able to do this for you. This allows for pretty much zero downtime for your website. You could achieve this simply by copying your app to a new location and running from there. * I would suggest that you move your flat files to a different location that is not effected by a deployment, i.e. have an image path that is configurable and outside of where your code changes are deployed. Do the same thing with log files. This way you can wipe the root directory for your code deployment and have a fresh start without worry of losing assets. This of course depends on how hard coded your asset locations are. * Now if deployments are still taking you a long time after these changes you can look into more devops like tools and services. Like a push to a deployment branch in git could deploy to a green or blue deployment automatically without you doing anything (did you read that link yet?). * Do you have a rollback plan? If you implement the green blue deployment then you have an easy rollback, or with git deploying checkout a stable branch. * Are you finding your self rolling back too often, have you automated your testing yet? I think that covers the basics. I would start with the first three points, looking at the fourth if you can. This should help you dramatically reduce your deployment pipeline. My goal when deploying software is to make it as risk free as possible. Automating parts of it helps, because you start with a plan then implement that plan each and every time, where as a person could make a mistake. If I'm relaxed about a deployment, then everything is under control.
There's even an abstract implementation of IHostedService included called BackgroundService. Implement BackgroundService and register it as a singleton IHostedService in the service collection. 
Interesting. I‚Äôll give it a look. 
I don‚Äôt know yet, this is all theory as of right now. But the files could be as big as 80MB and there could be 10-20 of them so I imagine it‚Äôll take a while. At least long enough that the user shouldn‚Äôt have to sit there and wait on it to complete. 
Okay, I was wondering about that. So I could have the task continue to run at all times? It has to be completely independent. So if the user leaves the page it needs to keep running. I figured a task would be tied to that same instance, so that wouldn‚Äôt work if the user had to leave the page. 
Well can you email a link when the process is finished? That is what I would do. Code it to log the request for the file from the website. Make sure the code that creates the zip file can re-run if it fails. Then you can run the code to create the zip file via background process. There many ways to do that. Send a link to download which had a guid or uniqueidentifier which you can verify to secure it. That‚Äôs what I would do. As for how to run it in the background, there are many many options for that. Webjobs, scheduled tasks etc...
Yeah, it's possible because I'm familiar with Docker I'm trivializing it's complexity a bit, but, still it's a very good solution to solve exactly the issues OP described. What I take issue with is somehow using the existence of logs and flat files as a reason that it would take re-architecture or lose your files or something. Also, OP doesn't say whether they know about Docker, maybe they're an expert and don't know that a dotnet core app can be converted to a docker image with basically a VS extension and about 2 hours to learn docker basics.
Any reason you can‚Äôt use docker?
You get so much with a CMS like Umbraco. Authentication and authorization, routing, versioning, and more. I focused on site structure and page templating so the subject matter experts, who are geographically dispersed, can focus only on content. With my structure, the content is organized automatically and templates ensure standardization. The backend interface lets me create custom pages in minutes if I need something new. No compiling, testing, or deploying. With that said, a static site generator, like Jekyll, Hexo, or Gatsby, may be more appropriate than a CMS. With the ability to use Git triggers to auto-rebuild and deploy a site, you get many of the same benefits as a CMS and likely faster page loading. 
I just implemented something similar using queuebackgroundworkitem, mostly because if the job was interrupted by an app pool reset I needed to fire a handler that would restart the job. In your case I would do the same and provide some sort of view where they can view the status of all jobs, both completed and currently processing. When the job begins register it in your data layer and have it track status. Once it completes notify the user via whatever notification mechanism you are using (email always works) and give a link to action the result (download zip probably). The key is to give them a way to check progress of jobs via the view I mentioned. If you want to know more lmk and I'll outline the exact architecture I used. 
If you need to scale this, use a separate service to monitor and process. A Windows service, or console IHostedService, etc. Background workers in a hosted app are ok for basic stuff but your scaling options are limited, and heavy io I'd probably keep out of the main app unless it's like a single user thing.
Teamcity + octopus deploy. Unfortunately octopus' license model is a bit out of whack now and doesn't have a free version anymore. 
You could script that process and run it in an Azure Devops pipeline.
Exactly this. Or Jenkins. Or teamcity. 
For one, with a solution file you **can** use visual studio whereas without one you can't. Sln files are also literally designed to build dot net projects and are better at it than your separately maintained build script.
For the dotnet core cli though you only need the csproj file to build, OP - you can still use an sln using cli msbuild. 
Containerize? You can automate that workflow very easily using Azure DevOps.
Honestly, if you can simply start with converting your button clicks to a cmd script instead, you're off to a really good start. You can then take that script (or its commands) and place it into your choice of automation servers.
I just ran into this issue and found that Task.Run worked until the process took over a minute. Then I used QueueBackgroundWorkItem.
Or bitbucket and their pipelines. You get like 50 build minutes for free / month I am actually loving it so far.
This sounds right up my alley. I‚Äôll look into it and PM if needed. 
Yup. Azure DevOps is pretty great
But if you specify the csproj file, then only that project and dependent projects will be built. If you specify the solution file, then all projects in that solution will be built.
Not in core , it will resolve locally dependencies and build those as well. You don‚Äôt need the solution file for core , but you do for msbuild as I mentioned in my original comment :)
Teamcity either and the license system is not worth it
Yes, also in core. &gt; it will resolve locally dependencies and build those as well. I mentioned that it does that. But you're assuming that all projects reference each other. If you have project A and B, and both are independent (don't reference each other), then running dotnet build for project A will not build project B. But running it on the solution file, which references both projects, will build both projects.
Regarding your zip-objective but not your "run a background process" general question: Consider moving your zip-workload to a different host. I use requests to transient external hosts for a very similar problem (serverless computing with AWS Lambda), which scales without limits. They can also be used to zip-and-transfer on the fly to the user without affecting web cluster performance. Alternatively, if you change to gz, then you can pre-compress each file and then send them to the client using simple concatenation. If each file is large enough, you don't lose much compression and you might be able to simple send back the compressed file on the fly. Reference: [https://stackoverflow.com/questions/8005114/fast-concatenation-of-multiple-gzip-files](https://stackoverflow.com/questions/8005114/fast-concatenation-of-multiple-gzip-files) Maybe there's a similar trick with the zip format, you you might accept a zip-in-zip transfer with the wrapper zip doing zero compression.
What I did was having a separate application for running that specific long running task, hosted into IHostedService. Since that hosted service didn't run in the same process as your WebApi, for starting it, I used an event, passed using Rabbitmq (with MassTransit) Maybe overkill, but I wanted to be able to scale the WebApi and the long-running task separately
That is not a good solution design, to have totally disjoint projects in there.
It's perfectly fine if the projects are related to each other.
Remember the server is just a normal application. You can do anything a desktop application can do. So you can run exes, communicate with services etc. You're front end and back end aren't tied together.
Also gitlab We use Bamboo at work and it gets the job done.
Yap, we actually planning to use on upcoming project because we need to run stuff on premises. But if you're okay with running stuff in the cloud, I would argue bitbucket pipelines are actually a little less work to get started.
Not sure about .NET Core solution. Normally people use API Gateway like [Kong](https://docs.konghq.com/hub/kong-inc/rate-limiting/) (or other alternatives) for rate limiting
In my free time, I helped a friend rebuild a 1978 Honda CB750. It was a summer project that drained my energy, wallet, and required extensive research and out of the box thinking. For example we tried to do an acid wash on the tank to remove rust but worse because we didn‚Äôt neutralize the acid properly. We couldn‚Äôt find a tank and instead of waiting we made a ‚Äúghetto gas tank‚Äù using Gatorade and tubing from an art store. [I still have the bike build in my online portfolio. ](www.3002100.com/portfolio) Is it relevant? No but it shows long term planning and willingness to learn literally EVERYTHING from scratch. It‚Äôs not necessarily about the project, more so how you market yourself. 
Here again as a top-level comment what I mentioned as a comment: - When you use the `dotnet` CLI to build a `.csproj` project file, then it will build this project and all referenced projects (transitive). In the majority of use cases this is enough. - When you use the `dotnet` CLI to build a `.sln` solution file, then all projects and their referenced projects (transitive) in this solution will be built. This is relevant when you have multiple projects in the solution that don't reference each other.
So just for building. Got it. Thanks!
Can you please explain why you think that this is bad design? I mean, it's basically the same thing, only that the reference isn't in the .csproj file but in the .sln file. So far, I have yet to encounter any downsides to this approach, hence why I am so interested in why you think that way about it.
Ah yes i wanted to link that particulary but i had no time. The link i gave provide information on the BackgroundService.
I can recommend Azure DevOps pipelines for both CI and deployment.
https://docs.microsoft.com/en-US/azure/devops/pipelines/languages/dotnet-core?view=vsts&amp;tabs=yaml
You can build a project with a csproj but a project is not a solution. You can script a bunch of csproj builds, buy a solution already does that for you and does it better. 
You can use yaml based configuration or visual designer.
Bullshit. Projects are often such as: * MyCoolProject (the actual domain) * MyCoolProject.Api * MyCoolProject.Web * MyCoolProject.Data * MyCoolProject.MobileCore * MyCoolProject.iOS * MyCoolProject.Android * MyCoolProject.Etc Etc. There's no way to build all projects in that solution from any single project file without making some kind of pseudo-solution.xproj file just to reference them all. At which point you're just being daft and should use a sln file. 
Yes , but I‚Äôm taking core not msbuild . Even without a solution file , core will build local dependencies.
Which once again **is not** the same as a solution. 
You only get 50? Azure DevOps has 1800.. https://azure.microsoft.com/en-us/pricing/details/devops/azure-devops-services/
I liked the WPF way where certain interfaces weren't explicit; there's nothing that requires INotifyPropertyChanged, but the framework code checks if your class implements if and if that's the case, it subscribes.
I am not familiar with Azure anything, does Azure DevOps work with cloud deployment only or is this something you can install on prem? For completely on prem I'm using TFS/git (CI) + Octopus (Deploy).
I understand , but the point you made was that solution is needed to group projects so they all build together, I‚Äôm saying in core that is no longer needed. So the solution file is there to support visual studio builds, but not needed if I‚Äôm using the command line dotnet cli tool. 
1800 sounds unreasonable, is that for the free plan? 50 minutes on bitbucket is on the free plan.
That's on the free plan, my link details it. You're limited to 5 users (more users are $5/mo) unless you're an open source project which gets unlimited minutes.
consider me an expert in baked vs. fried deployments in linux. when you have some resources that need to persist across deployments of any baked system, you'd need to move those resources into a volume that doesn't reside in the container that's getting redeployed, or to a shared logging service/CDN outside the baked server. then you have to build out and test the server image to work that way. that's rearchitecting the server. whether it's docker, or otherwise. and you have to learn to publish those volumes separately on initial deploys. let's look it up to make sure i'm not misunderstanding... http://www.monitis.com/blog/containers-5-docker-logging-best-practices/ what do you know, it works exactly like i thought. probably because i've been doing linux dev ops since before it was called dev ops. if you sense any hesitancy in my answer, it's because i'm not familiar with .net core on Kestrel. in an IIS environment, he was also 2 MSBuild commands away from the publish step he already took being able to do his deployment for him. i'm just not sure about the SSH Kestrel restart. i was going to make another comment about that but couldn't find a good resource. it seems most people's Kestrel restarts itself when it detects changes to the deployed files, maybe? there's some configuration to explore there. maybe the box needs a small helper script to restart when the files change. it's not an overwhelmingly difficult problem to solve that requires rearchitecting the server... &gt; especially considering he's using nginx where he can simply redirect to the newly running container now when you say that, it makes me wonder if you know about Docker, or just the way your stuff is set up at your work. it sounds like you might have a setup where your code files are actually on that outside volume instead of within the docker image. so you're not actually using docker at all for deployments, just for initial server architecture? if that's the case, he's in exactly the same position your way as he's in now with a hand-built server and your suggestion that Docker would magically fix his problem is even less correct.
You configure your builds and deployments in the cloud, then you install agents in your on prem machines that execute your configured actions. IT supports a fuckton of stuff, which makes it great.
Well, all those listed are in the same dependency tree, so they should belong to the same solution, I totally agree. I was just arguing that having projects that belong to two different, totally disjoint dependency trees in one solution is a bad choice. What does the solution then represent? 
Just want to make sure I understand. You have to use their cloud source control? Then everything after that happens in the cloud. Then I have a local agent that deploys to my local machines?
And again, you don't. A solution is not a project and dependencies, it is more than that. The core cli will build a project without a solution file, it will not build a solution. You can build a solution without a solution file, but you need more than a single command line.
Well... Damn :|
Yeah, I was thinking about going this route. So would you have to launch the application after the site/api starts running, or could you have it to where when you start the site it automatically starts that application? 
Now that‚Äôs very interesting. Thanks for the heads up. I didn‚Äôt consider that. 
+1. I recently implemented Hangfire for a client to handle some delayed messaging. Super easy to get set up and use.
I'm pretty sure you can use other sources. It's really just your build and deployment configurations. And then those run on the machines you installed the agents onto. Afaik no code has to be stored with Microsoft as long as you configure it as such. Of course they probably could steal your stuff if they wanted to, but then we're going into some conspiracy stuff.
A solution file is a grouping of projects as well as solution level folders and solution options I get that . Please don‚Äôt be presumptuous . What I‚Äôm saying is you can build multiple projects using the dot net core cli without a solution file or custom scripts. In any case we are saying the same thing doe the most part , no need to be pedantic. 
I see. So for someone who is already running a local TFS/git, I don't really see the advantage of doing this. Since it seems it's about the same thing as the Azure version.
When he issues his `run` he includes a volume mount to point the docker container path to the root machines file system. https://docs.docker.com/storage/volumes/#start-a-container-with-a-volume As far as nginx is concerned - I've done the exact thing on a hobby project * Container 1 running on 0.0.0.0:8080 where nginx reverse proxies the traffic on port 80 to 8080. * Pull new version of image * Run it on port 8081 * `sed` nginx config to repoint :80 to :8081 (I actually ran nginx from within a container too, but, I'm simplifying) * Validate nginx config and save. * `rm` old docker container * ??? * Profit And that was on a dotnet core image and an elasticsearch image that persisted all the configuration and indexes on to the new ES image by volume mounting. What we use at work is much more complicated and involves docker networks and SQL images and runs in Fargate containers which uses more of the OS mounting you're referring to. Add to that, you can further orchestrate all of this with Docker compose and then you're sitting very pretty. I'm really not trying to be antgonistic or anything, maybe, I'm doing something wrong and can learn a thing or two. I will say, Docker checks a lot of the boxes from OPs problem statement, and wouldn't require much work at all.
And again, you're not understanding. The dot net cli will build a project and all dependent projects, but that's not the same thing as a solution. A solution can contain multiple projects which are related but not dependencies of each other. 
It's literally the same thing. The version of TFS we ship is a version of Azure DevOps we've already deployed a few sprints back. The advantage, depending on how you look at it, is whether or not you want new features and bug fixes fast, or the stability and unchanging nature of self-hosting. 
This is pretty cool. I've been wanting to absorb some ML.net info even though I dont think I'm quite ready to write something with it yet. Thanks for sharing!
Their site should be disappointed to be on mobile.
That‚Äôs interesting. Thanks. Still can‚Äôt grasp the whole concept though. 
If you're interested in learning more, the book [Applied Predictive Analytics](https://www.amazon.com/gp/product/1118727967/) is a good introduction to the concepts. You can also find the pdf on LibGen for free, if you're into that.
Reading the entire request and response stream, and saving each request log record individually, is a great way to murder your performance. I wouldn't use this code on anything with a decent amount of traffic (I log tens of millions of requests every day).
Of course you would only log these if really required and if your API has low traffic.
Exactly: side projects don't need to be code, but you should be able to (honestly) draw parallels from them to programming job skills. An engine rebuild sounds perfect for it. All that research and planning and lesson-learning matters. If I'm interviewing you and you show you've still got an intellectual passion somewhere in your life, I'll definitely vote in your favor. Otherwise, not so much.
Ow! That's what I was looking for these days! 
I would try to run both applications independently. Currently I am starting both in a single docker-compose. You can start the message producer before of after the message consumer, without problems. The only thing is that you would need to have the destination queue and the routings from the source exchange to the queue before sending the first message. Otherwise, the message will be silently lost.
[removed]
There doesn't seem to be a good way. A not so good way would be to digest the WSDL to an object model then use Roslyn to generate the class files for your models and services that use HttpClientFactory. This is a lot of work, but probably the best way. A significantly worse way might be to use XSLT (Stylesheet Transforms) to create models from the WSDL. I believe XSLT transforms are supported in some form in .NET Core with the `XslCompiledTransform` class, but the `WsdlImporter` class isn't. I've not used it myself so I can't vouch for it but a long time ago I worked with `protobuf-net`'s XSLT that turned the XML output of a `.proto` parser into C# classes so the approach itself is valid. Not advocating this path, just mentioning that it exists. 
You could generate the classes with add service reference and use the XmlSerializer by hand to generate the soap bodies. https://docs.microsoft.com/en-us/dotnet/core/additional-tools/xml-serializer-generator
You could generate the classes with add service reference and use the XmlSerializer by hand to generate the soap bodies. https://docs.microsoft.com/en-us/dotnet/core/additional-tools/xml-serializer-generator
A cursory reading makes me think async/await pattern and the various underpinnings are good enough.
It looks like Tasks. I'm not a Java person but I was interested so read a few things. It all felt a bit last gen tbh. Key points seem to be: * lightweight * different schedulers * that's it. So Tasks give us that. A Task doesn't even need to run concurrently, it can just return a result on the currently running thread for example. We have different schedulers, including the UI ones and the Test one. Tasks then underpin async await. But, as said, not a Java person, so there may be more to it. 
Very cool. How is the performance with this library? Can I pump gbs of data into it? Can this be used with azure functions? 
[removed]
Depending on how you install the encryption certificates, they can be impossible tip move to another machine. (Short of a full OS backup that is. ) Not perfect, but it will keep people from casually copying the encrypted config file. 
I'd take this route too...
I was considering it, but I also want something that I could share with other people at my company. They would be less than thrilled about the hand generation part.
Could consume the WSDL as a service reference and expose it as an API microservice... Then basically make httpclient calls to the API from whatever apps need access.
Yeah, that's probably what we're going to do. We're going to set up an API management portal that can turn SOAP into REST or vice versa, doing transforms and stuff. I was just too impatient.
'by hand' was probably the wrong term, you are just calling the serializer and wrapping the output with a soap envelope. It's what the channels in wcf do, but you will be doing it with http client like the example you posted
Every major ad provider is Javascript based. Google Adsense works fine on Core MVC e.g. https://support.google.com/adsense/answer/7477845?visit_id=636823473912576723-4263098941&amp;rd=1 The ONLY thing that might be hard to integrate ads into is Web Assembly. 
Okay thank you I had read something I found confusing about Google AdSense not working on .net core 
Yes, it's ready for any scale and used by Microsoft/Azure internally for years.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [Announcing Bassoon, a cross platform .NET audio playback library \[X-Post \/r\/dotnet\]](https://www.reddit.com/r/csharp/comments/ad31z4/announcing_bassoon_a_cross_platform_net_audio/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
I think that your current code example is not optimal due to your BassonEngine being an implicit Singleton: &gt; using (BassonEngine be = new BassoonEngine()) &gt; { &gt; Sound snd = new Sound("Kenny Loggins - I'm Free (Heaven Helps the Man).ogg"); &gt; snd.Play() &gt; } Sound should either require BassonEngine as a parameter using (BassonEngine be = new BassoonEngine()) { Sound snd = new Sound(be, "Kenny Loggins - I'm Free (Heaven Helps the Man).ogg"); snd.Play() } or even better imo, use the BassonEngine Singleton in the background. Sound snd = new Sound("Kenny Loggins - I'm Free (Heaven Helps the Man).ogg"); snd.Play() To accomplish this, the constructor should be private and your instance property should look like this: private static BassonEngine instance = null; public static BassonEngine Instance =&gt; instance ?? (instance = new BassonEngine());
Can you use it with a stream, not filename?
How do Sound and BassoonEngine integrate together? I see no references to BassoonEngine anywhere either in Sound constructor or in Play() method. I assume it‚Äôs using some kind of ThreadStatic/Local instance if BassoonEngine, and this is a suboptimal design.
IMO Tasks and async/await in C# are like a generalization of fibers and coroutines (i.e. a superset): both patterns can be implemented depending on how the execution context and scheduler(s) collaborate, as well as how the continuation (await) are configured. So I don't think there is a need to add yet one more implementation of collaborative scheduling.
And, according to [this issue](https://github.com/dotnet/machinelearning/issues/569), it should now be able to work in Azure Functions to read a model from ML.NET.
If a singleton is what you really want, Lazy&lt;BassonEngine&gt; is the way to go. It covers all the locking required if there are multiple threads trying to grab BassonEngine.Instance.
I would advise switching to c#. It sounds like you‚Äôre new to this and so it should be easy to change. 
For me it makes sense. In the constructor of the bassonengine the native libraries are initialised and since it's an idisposable, native resources are cleared when the object is disposed. That's why he is using the using statement like he should. I like this way better than a hidden Singleton. When would the static Singleton be disposed? In the current way I would have some control over when initialization and disposal takes place.
Looks awesome! I'd be keen to try this out sometime üòé
Why would I use this over NAudio?
Not sure why you were downvoted for this. You are right about how they are coupled. The BassoonEngine should be a parameter of the Sound constructor. Looking at the project there are no tests which makes sense since you would have a tough/impossible time mocking with this current design. This could easily be refactored to use DI so that the lifecycle can be controlled by the caller with their IOC pattern of choice.
It's stated (and designed) as such in the code
The there should be a parameter in the sound constructor. From looking at the example, the using is completely unnecessary
I write a lot of C# code and I do think async/await is nice but they do not solve the "color" problem: http://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/ Green threads, however, do solve this problem. The programmer can just write blocking code and not worry about asynchronous programming, like in Go. Java did try to implement Green threads in the past but it wasn't performant for reasons I don't remember but the hardware landscape was way different back then. 
Looking for this too!!!
What for a Database and it's okay when you don't have a IdentityUser
So the application is a default asp.net core website template and authentication was set to individual user account, you can register and sign in but it doesn‚Äôt do anything, I just want two roles one admin one customer 
I'm not an expert on this stuff and I hope someone will correct me if I'm wrong but the biggest difference will be where the threads/Tasks are scheduled. The idea is that they'll be scheduled by the CLR instead of the Operating System.
The UserManager object you are using has a method named AddToRoleAsync which accepts a user and a role name. There's a RoleManager object that has a method named CreateAsync which accepts a string. You can specify the name of the role there.
Here ya go: public static class SeedAuth { public static void CreateRolesAndUsers(IServiceProvider serviceProvider) { var roleManager = serviceProvider.GetRequiredService&lt;RoleManager&lt;IdentityRole&gt;&gt;(); var userManager = serviceProvider.GetRequiredService&lt;UserManager&lt;AppUser&gt;&gt;(); var context = serviceProvider.GetService&lt;authContext&gt;(); var configuration = serviceProvider.GetService&lt;IConfiguration&gt;(); //context.Database.EnsureCreated(); context.Database.Migrate(); Task&lt;IdentityResult&gt; roleResult; string email = configuration["Admin:Email"]; //Check that there is an Administrator role and create if not Task&lt;bool&gt; hasAdminRole = roleManager.RoleExistsAsync(Roles.ADMIN); hasAdminRole.Wait(); if (!hasAdminRole.Result) { roleResult = roleManager.CreateAsync(new IdentityRole(Roles.ADMIN)); roleResult.Wait(); } //Check if the admin user exists and create it if not //Add to the Administrator role Task&lt;AppUser&gt; testUser = userManager.FindByEmailAsync(email); testUser.Wait(); if (testUser.Result == null) { CreateAdminUser(userManager, email, configuration["Admin:Password"]); } else if(configuration.GetValue&lt;bool&gt;("Admin:Reset") == true) { userManager.DeleteAsync(testUser.Result); CreateAdminUser(userManager, email, configuration["Admin:Password"]); } } private static void CreateAdminUser(UserManager&lt;AppUser&gt; userManager, string email, string password) { AppUser user = new AppUser { Email = email, UserName = email }; Task&lt;IdentityResult&gt; newUser = userManager.CreateAsync(user, password); newUser.Wait(); if (newUser.Result.Succeeded) { Task&lt;IdentityResult&gt; newUserRole = userManager.AddToRoleAsync(user, Roles.ADMIN); newUserRole.Wait(); } } } public void Configure(IApplicationBuilder app, IHostingEnvironment env, IServiceProvider serviceProvider) { if (env.IsDevelopment()) { app.UseBrowserLink(); app.UseDeveloperExceptionPage(); SeedData.Development(serviceProvider.GetService&lt;IDbConnection&gt;()); } else { var options = new RewriteOptions().AddRedirectToWwwPermanent(); app.UseRewriter(options); app.UseExceptionHandler("/error"); SeedData.Production(serviceProvider.GetService&lt;IDbConnection&gt;()); } app.UseStatusCodePagesWithReExecute("/error", "?statusCode={0}"); app.UseStaticFiles(); app.UseAuthentication(); app.UseMvcWithDefaultRoute(); SeedAuth.CreateRolesAndUsers(serviceProvider); }
Oo, interesting. This is new ground for me, so thank you :)
You can query the role if it exists or not and create it based on if condition result. You can use this following code to assign a user to specific role if (!this.AppUserManager.IsInRole(user, role.Name)) { IdentityResult result = await this.AppUserManager.AddToRoleAsync(user, role.Name); if (!result.Succeeded) { ModelState.AddModelError("", String.Format("User: {0} could not be added to role", user)); } } AppUserManager is an instance of user manager in identity and role.name is a result set from RolesManager in identity queried for one result. If condition checks to see if user is already assigned to a role if not it will assign. This will work for new and existing users.
[removed]
It should be possible, but I haven't added that in yet. libsndfile does have some virtual IO functions that can be implemented to use a buffer/stream as a backend. This wasn't a priority for me to add in at the moment. Possibly in the future, or if someone wants to submit a MR I could get it in sooner.
I did look at that first, but after checking some issue tracker tickets and Stack Overflow, it looks like NAudio is moreso for the Windows platform. I needed something that would work on Linux &amp; OS X as well.
Thanks for the input. After reading the rest of the comment thread, I've added an issue on the tracker to look into this some more and implement it.
I think you're focusing on the wrong details. The prime numbers are not new to *double* hashing they're used to maintain optimal allocation in *any* hashing strategy, and it just happens that .NET uses double hashing. The reason why primes are used is because they have no factors which reduces the chance of two unrelated hashes being sent to the same bucket. Reference: https://cs.stackexchange.com/questions/11029/why-is-it-best-to-use-a-prime-number-as-a-mod-in-a-hashing-function As for doubling the value during resize, it's unrelated to double hashing and prime numbers. 2x is chosen simply because it offers a good trade off between allocating too much storage space at once (a hash table of size 1,000,000 when you're only storing 8 items) and needing to constantly resize (if you add 1,000,000 items and increase the storage space by 10 each time you run out, that's 100,000 resize commands - if you doubled instead, it would be about 20 resizes). Reference: https://stackoverflow.com/questions/2369467/why-are-hash-table-expansions-usually-done-by-doubling-the-size
That's exactly the kind of answer I was looking for! Crystal Clear! Thank you so much!
Something wrong with your EF Database Context class. Cannot tell without looking at the code. Likely you have defined your own AppUser inherited from IdentityUser but not specified it in your EF DB context implementation.
The other ‚Äì and to your comment thread better suited ‚Äì alternative is to give the Sound constructor another parameter containing the `BassonEngine` (instead of relying on `BassonEngine.Instance`). This way you get the disposal from the using but leave the magic out. üòä
Possible you have a missing property DbSet&lt;AppUser&gt; in your db context class.
Nice. Though I'd suggest you change the API so it is used like this: engine = new BassonEngine(); // Kept as member of some object... //audio effect Wav effect = engine.LoadWav... engine.play(effect); Or engine.Play(effect, pan: - 1, speed: 0.5f); Etc... // song Song bgmusic = engine.LoadSong... var instance = engine.Play(bgmusic); instance.Volume = 0.5f; Etc.. At the end of the game: engine.Terminate(); Just some ideas. I know making a nice and flexible API for an audio library can be pretty hard. Good api example: soloud audio lib. 
https://docs.microsoft.com/en-us/dotnet/visual-basic/ But really...if you don't have a specific reason to target VB.net...make things easier on yourself and switch to c#. It's pretty much the same stuff but with gazillions more examples.
We're in same dilemma. We haven't found anything ourselves, so what we are doing is hosting a .net framework asp.net mvc 5 app, which has endpoints to generate the crystal report pdfs for us.
Is PortAudio still being developed? I remember reading in the mailing list that it isn't under active development anymore.
TIL Crystal Reports is still around.
There are some HTML to PDF solutions that might work depending on how complicated your reports are.
I don't think `Dictionary` uses double hashing. If you look at the implementation, you'll see that it uses separate chaining (where all the chains are stored in the `entries` array).
I like: using (var engine = new BassoonEngine()) { Sound sound = engine.CreateSound("/path/to/file.mp3"); sound.Play(); } or simply (assuming the engine can play only one sound concurrently): using (var engine = new BassoonEngine()) { engine.Play("/path/to/file.mp3"); } 
Check out Developer Express's reporting package. https://community.devexpress.com/blogs/reporting/archive/2018/04/26/reporting-net-core-support-ctp-v18-1.aspx
What are you using for your database? If it's SQL Server, there's always SSRS which offers a few ways to produce pdf reports through.
No kidding... I started as a Crystal Report writer in 2000. I‚Äôm shocked it still exists.
Checking the mailing list, it still looks like there is activity. I have yet to run into any critical bugs.
Migrating from Crystal Reports to SSRS myself.
To be fair, I don't know that OP is going to get what s/he wants. Everyone loves to shit on CR, but it does a LOT and asking to have that same level of functionality for free is a big ask.
Mid-Senior. Being a senior developer or architect is just as much about being able to learn the domain you are working in and solve problems. I am an architect. I was in a meeting last week with the business. They wanted to make a new service that our company was going to offer. Knowing the code base and applications I convinced the business that we needed a new attribute or property on an existing service we already offer. Without getting into the details of my work, I simplified things and made it much easier for us to offer a new product. How do you measure or gauge something like that in a job interview? Generally a mid level developer is 5+ years professional experience, senior is 10. But the employment market is supply and demand like anything else. Good luck.
We have been using .liquid templates (also known as Shopify templates). They're basically HTML files with the ability to databind to json objects, with many similar features and syntax to razor pages in .net (if statements, for loops, etc.). They can be fully styles with css. Once the HTML is generated from the .liquid template you can use one of the many HTML to PDF libraries create the PDF (we use a .net wrapped to the the wkhtmltopdf tool - there are many different ones out there, you'll have to choose the one that's right for you). From there you need a library to print PDFs from code. Since our users need the ability to choose trays and set some other advanced printer settings, we use PdfiumViewer (a .net wrapper of Google's Pdfium library). Hope this helps.
What this thread shows is there is market for a good reports library for .Net Core. Who wants to start a business?
I think that's the last time I touched it. Right around 2002.
Yeah 2004 for me.
Maintenance is decreased by an order of magnitude as you don't have to worry about different running versions. Hooking multiple days datasets is kind of a pain, but can be done.
Sadly for all of us, it is.
The other two commenters before me are right. 
Microsoft power bi is a similar tool. Can be hosted on Azure for dirt cheap. 
I know this likely isn't a solution for most but in my last gig we decided to just dump our data into formatted excel sheets dynamically. It allowed the users to then format/sort/search/correlate/calculate those sheets however they liked. This was for a financial application so it fit nicely because the users were financial consultants and actually preferred excel files.
I have used Aspose PDF generator which is a third party plugin and also ABCpdf. 
Really appreciate this detailed answer. What's the benefit of using liquid over Razor here?
I hear its name (cursed) a lot, but it's before my time. What is the point of Crystal Reports? Is it still relevant now that computers are ubiquitous in organizations?
Uh... becoming a carpenter?
Agreed, the problem is that intent isn't clearly displayed. It should be clear the using is necessary.
PdfReport.Core is a code first reporting engine, which is built on top of the iTextSharp.LGPLv2.Core and EPPlus.Core libraries. https://github.com/VahidN/PdfReport.Core
Well.. OP asked for "free" so the market is not very good if you constraint it to that
It's very much still used.
The answer is to add your own footer and header and whatever else you want to the \_Layout file in for the idnetity stuff. Even those there is no header or footer in the file to begin with
You can use jsreport in .net core and it‚Äôs pretty easy to figure out. 
SSRS perhaps? Not a fan, would go with Power BI or Tableau these days. 
This is the solution I opted for last time. Very easy to set up and create your report templates, then pass the report server a json object and get a PDF back. Plus it's free for 2 or 3 report templates.
we just started looking at this : [https://github.com/FastReports/FastReport](https://github.com/FastReports/FastReport) &amp;#x200B; so not much experience with it yet, and can't tell how good it is compared to CR. but so far so good. this particular product is for use with web applications only, and FastReports just open sourced (MIT License) the code in October 2018, so not everything is working "smoothly" yet, but we've been able to get past problems we've encountered so far...
Allot of Basson/Bassoon spelling errors there.
Even better I'd say
You would just have to model in in a certain way, make the code free but you have a SaaS model combined with it for generating the reports or something like that, if you wanted to monetise it.
We were going to use Razor but ended up with Liquid, but the only reason we did that was that it was simpler syntax for people who aren't used to Razor, and we couldn't get Razor running in an Azure Function. That might well have been fixed now though.
Try to use Json.NET its better 
You probably made the right decision. I spent a while getting razor working in dotnet core and azure functions, and am now thinking of moving to dotliquid. The primary reasons are durability, simplicity and speed (invoking rosyln to generate an email seemed dumb, even with some aggressive caching)
These series is really good!
Thanks! It's the longest episode so far, and soon we will start talking about more interesting and complex topics :).
&gt;Reporting &gt; &gt;Good Choose one.
&gt; passwords should never be stored, **unless it is necessery.** (Which it often isn't) Just to be sure, you didn't miss that part. I was assuming application secrets like user accounts and or different managemant password for ***that*** application, havn't read that it was secrets for communication with other systems.
Syncfusion has a license you can qualify for if you are under their revenue and developer count. If you are over their revenue and developer count, then ideally you can afford a license.
Is it production ready?
&gt; Can **only** be hosted on Azure for dirt cheap. FTFY
can you explain in a most understandable way what is CQRS? thanks op.
Sounds like your Entity Framework model (as defined by your migrations) does not match your code. Did you remember to create a migration?
Invalidated occurs whenever the control is signalled that it needs to repaint or readjust its layout or whatever (I forget the exact set of things that can cause it) or when manually triggered. It's likely the control is being invalidated before it is added to the form. so ParentForm is null. OnInvalidated is likely the wrong event to use. It's going to depend on what exactly would trigger the Text property to change, but one generic approach is to use OnTextChanged. You should also verify ParentForm is not null before attempting to set its Text property. This should be sufficient though again it depends why you are doing this, you may need another override such as OnParentChanged in order to set Text for the first time.
... which is why you can build individual projects or the whole solution. You can get away without using solution files on both the command line AND Visual Studio. But when working with multiple related projects it will quickly become cumbersome in BOTH environments. A solution file will greatly aid in coordinating multiple projects in both cases. The distinction between command line or Visual Studio is not important, imo.
Yeah the use cases are the same as in VS. Solution files are just a way of coordinating related project files, mainly so you can build them all at once (if they depend on each other, which is usually the case, this is helpful).
CQRS = Command Query Responsibility Segregation. It's a separation between reading (via query) and updating (via command) the data. In practice, it means that you can use different models for writing (e.g. your domain aggregates) and different ones for reading (so-called read models e.g. DTO that is quite often view-specific). By doing so, you can achieve pretty good flexibility and isolation, as your models no longer have a mixed responsibility (read models are usually much richer and more complex than write models). A good starting point is CQS (Command Query Separation) and CQRS is more like an evolution of CQS :). Please take a look also here: [https://github.com/mspnp/architecture-center/blob/master/docs/patterns/cqrs.md](https://github.com/mspnp/architecture-center/blob/master/docs/patterns/cqrs.md)
The modern implementation of threads is the Task class. You would want to use Task.Run to spin off a new thread which does the compression. You would need to associate the user's session with this operation somehow, the Session property of the HttpContext would probably be the way to go to do this. For example assuming you generate a temporary file name with Path.GetTempFile you could store this value in the session and use it in the Task. Then use another session value to specify the operation state (completed or not). I have not used Sessions for this purpose so I'm not 100% sure that would work. But the way I would do this would be to have my View contain some JavaScript that calls window.open and points it at the API which generates the ZIP file. Then Kestrel will create a new thread for you and everything "just works". A cleaner approach which won't pop open a new window would be to use AJAX instead to download the ZIP file, but then due to browser security it becomes difficult to get the resulting data stream into a download file, especially if you need a method which works in IE11.
So I was thinking of using tasks but then I had a thought: will the task keep running if the user leaves the site/closes the browser? It should run completely independent from every thing. So if the user leaves the site, it doesn't cancel the process. I thought the tasks would be tied to that same instance, so terminating the instance would terminate that task as well. 
[removed]
This isn't a proper answer to the question at all... OP clearly knows some C# already as their question stated they are having trouble understanding how VB's functions relate to C#'s. Their desire to learn VB.NET shouldn't be discredited simply by the fact that an alternative language exists.
thank you! i did scan the article and the CQRS is paired with even sourcing? on what i‚Äôve read event sourcing allow operations to be executed sequentially to avoid data inconsistency? is that right? or is there more to it?
There‚Äôs no mention of c# in OPs question. 
If you are coming from C# trying to learn VB, this cheat-sheet will help: https://www.harding.edu/fmccown/vbnet_csharp_comparison.html
There's no need to use ES. It works well with ES, but it's not mandatory. You could simply store your read models in a different table, database or query a specialized view :).
[removed]
[removed]
I just read some articles about using repository pattern with EF (especially with EF Core) and I thing I have to apologise since my information were a little bit outdated. It seems like the classic repository pattern in combination with EF is not the way to go. /u/Favorablestream: Sorry about that! As I updated my knowledge about design patterns I would use the EF DB Context directly in the service layer as well! &amp;#x200B; &amp;#x200B;
I really appreciate your videos. I always enjoy watching other programmers and explaining what/how they are coding. 
Ah i see, so how does this different from creating read only and write only viewmodel classess?
Thanks!
I'm not certain what do you mean by write-only view models. You use the commands in order to write data (side effects) - call the repository, load a domain model and update it. Similarly, you can use the queries in order to read data (idempotent, no side effects), however, you don't need to use the repositories (which is more about providing a contract your domain models) and domain models at all. You could simply execute an SQL query on a distinct table/view/database in order to provide a read model that is required for the particular view or API endpoint, and the read model could be even stored in a different database or be "projected" when using ES.
Your code won't stop running however you can force it to stop by using a property/method from HttpContext (I forget the exact property name) which will trigger a CancellationToken you specify when the connection is closed. Useful for stopping long operations you know you don't care about if the user disconnects before it's done. And something from Task.Run goes off on its own and Kestrel can't track it any more anyway. So it can't be stopped unless you do. But I would recommend doing it my way and splitting it into two API calls by having the View call the second API. Then Kestrel is handling the threads for you AND you can potentially do fancy stuff like cancel the ZIP creation (using the method I noted above) if the user stops their browser.
If your site is anything beyond basic stuff, use something like Vue, Angular, or React. In vue, it would be like: &lt;div v-for='item in items'&gt; &lt;span&gt;{{item.Name}}&lt;/span&gt; &lt;/div&gt; 
Yeah I‚Äôve done a ton with just creating web pages with whatever rendering api you want (razor, asp.net, whatever) and then rendering the page to a html string and passing it to ABCPDF, which does a great job of rendering web into PDF‚Äôs. 
That's iterating a collection. I'm pretty sure he's talking about the user adding items to a property on a model that is a collection and posting it back.
Yeah, this. Specifically it's a page of addresses, the user can enter as many as they want, and they all need to bind to the List&lt;Address&gt; property in the view model, so when I get the form collection in the controller, I can interate over the list of Addresses there and save them to the db.
Their Vue example would do exactly that. You simply add a new item to the items collection and Vue will update the view automatically. Works the same in Angular and I assume React.
I been able to do this in the past, by providing the name of the parameter to be posted, indicating the array index on the client side. A quick example: On the Razor page have something like this: &gt;for (int i = 0; i &lt; YourAddressListModel.Count; i++) { &gt; &gt;var editControlName = "myAddressList\[" + i.ToString() + "\].Street"; &lt;input type="text" name="@editControlName" value="@YourAddressListModel\[i\].Street" /&gt; &gt; &gt;} The rendered results will end up being something like: &gt;&lt;input type="text" name="myAddressList\[0\].Street" value="123 Main Street /&gt; &gt; &gt;&lt;input type="text" name="myAddressList\[1\].Street" value="345 Any Street /&gt; When this is posted to your controller, it could have a parameter in this manner: &gt;public ActionResult(IList&lt;YourAddressModel&gt; myAddressList) Where myAddressList will be populated during model binding with a list of address objects (YourAddressModel in this case).
It's ugly but this is how I've had to do it in the past. I just don't like those names. I have also used js to build json and make the call.
Check out DevExpress XtraReport
You could try Coravel: https://github.com/jamesmh/coravel The queuing piece is what you'll need. Someone will inevitably ask how Coravel is better than Hangfire.... Coravel supports true async background tasks. Hangfire currently **does not support async** and will block a thread doing I/O. See [this issue](https://github.com/HangfireIO/Hangfire/issues/150). Even when that issue is fixed Hangfire will still not truly support async I/O (until some later time?) Coravel is also super easy to get up-and-running. (Yes I'm the author)
You could ask user to separate the addresses with some special character e.g. semi colon and then tokenize the submitted string. &amp;#x200B; If the multiple items are already in database you could use MultiSelectList with Chosen js plugin
Pdfsharp
Use full framework mvc and use react for the front end. Are you having a particular problem?
I would keep it very simple. Have one index view with a link to an add view. After they post the save or edit, go back to the list. If you need them on the same screen, you can do Ajax posts gets and partial views. That would be very easy and simple. Posting an array to save multiple addresses at once is a bad idea for many reasons. You could always implement a front end framework like VUE but that‚Äôs something you should do only if this is a new application.
well when the post comes back, won't the model be updated with new items?
nop ... the boilerplate and ‚Äúgenerated code‚Äù is exactly what you don‚Äôt want when you do ‚Äúbuild as code‚Äù ‚ÄúHey use a tool that generate code that will build‚Äù -_-, the idea is amazing and great and the result is not that amazing at all, i‚Äôll keep on using Fake5 / dotnet tool
I‚Äôm not sure what you mean. Are you talking about the CLI wrappers?
i‚Äôll have to watch again to be specific, one of our team used it ... lots of XML / attribute / generated code the fact that you are asked to ‚Äúdeclare‚Äù what‚Äôs you are going to do then code is generated for you ... this is like MsBuild again if i need to ‚Äúbuild as code‚Äù ... just execute the damn code it felt like you had to mix your own code + declarative + generated code it‚Äôs as hard as msbuild to debug One of the hardest thing in build is not to build but repeat locally a build * when you want * where you want * same on local / ci (no hack) * no magic bootstrap if you have generated code your are beginning to increase the potential point of failure
I really think you've got something wrong. There is barely any XML used, except for the csproj file, and this one is a simple console application. Yes there are attributes being used for auto-injection, but it's not a requirement, and they all have method-based counterparts. As a user, you don't need to deal with any generated code. [Here](https://github.com/Liminiens/json-provider/blob/master/build/Build.cs) is an example of an FSharp project, which uses NUKE. I'm not sure what you'd see as boilerplate or generated code. I also don't think there is _magic bootstrapping_. The generated build.ps1/sh only make sure that the correct dotnet version is installed, even if you don't have it. Not more, not less. If you like you can even delete them, and just call `dotnet run` on the build project file.
it‚Äôs a bit better from what i seen, i totally agree still i don‚Äôt get why the 3 attributes, for sln / gitversion / git repo also this is a simple build that could be done in a ps1, restore/build/test/publish + gitversion (that is an exe and can be called) i meant a bit more complex build, like library creating previous based branch name / preview so far this sample does not require to pull out something else than dotnet cli
Again, you don't need to use attributes for that. Some people like a little magic, others don't - and there are both ways supported. Basically, using attributes eliminates the need for something like a _mother target_, where all variables are getting initialized, no matter what actual target you'd call. I personally don't think that _parsing solution_ or _calculating version_ is worth a target declaration. Sure, probably this example could've been written with just dotnet CLI, but I just mentioned it here to demonstrate that no generated code is involved. There are other examples of more complex builds, for instance [Avalonia](https://github.com/AvaloniaUI/Avalonia/blob/master/nukebuild/Build.cs) or [nuke-build/web](https://github.com/nuke-build/web/tree/master/build).
then my question is why not Fake build then ? You said FSharp and i also see paket ;) CSharp is ‚Äúfine‚Äù if you like ; { return } etc ... just trying to compare to existing stuff why would i choose one against the other when both are already .Net, the language / syntax ?
Remember the paths you specify to styles or scripts or images or any other files are used by the browser, and will be relative to the URL of the view. I resolve this issue by using absolute urls. You can do something like (I forget the exact path to the property): @this.ViewContext.HttpContext.Request.Url.PathBase/path/to/file/from/app/root This will prepend any IIS (or ngix or whatever) folder name and you end up with an absolute path without specifying a server name or protocol. A bit messy but it should work.
&gt; Fake5 does literally the same thing just less support from microsoft oO
what do you mean by ‚Äúsupport from microsoft‚Äù ? also what would it change ? does it need Msft ? It got support from a huge part of the Fsharp community ;) that maybe why MSFT ‚Äúabsence‚Äù ;)
aren't c# threads managed threads already, aka threads on the threadpool?
Every tool that suits your needs is perfect. I was starting with NUKE with a handful of ideas, which included code-generation for CLI wrappers, having proper IDE support (OOTB), and most of all being as simple and approachable as possible. I don't think that the characteristics of the host language are reasonable indicators to choose one or the other.
That was another suggestion I read. I was under the impression that MVC was Razor Pages-specific, but am seeing that's not the case. Seems like this is the route I'll be taking. Thx for the suggestion.
"On top of that, the NUKE code generationmakes it easy to add support for additional CLI tools. There are literally hundreds of thousands of lines of code auto-generated, to provide a clean, consistent and powerful API for tools like MSBuild, Docker or Azure CLI. " - from the documentation page. Hundreds of thousands of lines of generated code??? The ide integration seems nice. But now that I'm used to a similar looking system in psake, going to the command line to build seems like something that's worth getting used to. Big reason I like psake - it's very easy to have a Jenkinsfile for a project which does nothing other than wrap that. powershell "Invoke-Psake -Task Clean" etc. Gives me confidence that I'm building the same way locally as for actually publishing. 
I don't have a strong background in concurrency but managed in this context just means that they are created in the CLR. The scheduling is still done by the operating system. https://docs.microsoft.com/en-us/dotnet/standard/threading/scheduling-threads &gt; Even though threads are executing within the runtime, all threads are assigned processor time slices by the operating system. 
&gt; Hundreds of thousands of lines of generated code??? That is part of the library, so I'm not sure how it matters. In contrast, other build systems do manual implementation for their supported CLI tools. The code-generation approach has the advantage that it's consistent, less error-prone, doesn't require tests, and - well - can be regenerated when new features are added. FOr instance, in this [commit](https://github.com/nuke-build/common/commit/10d7ef89cca4ac92553ed1622983f75784250842) we've added support for [invoking process multiple times more conveniently](https://github.com/nuke-build/common/commit/04cfa497ca23488e109fe9bd7e55f750b94a189d). &gt; Big reason I like psake - it's very easy to have a Jenkinsfile for a project which does nothing other than wrap that. powershell "Invoke-Psake -Task Clean" etc. Gives me confidence that I'm building the same way locally as for actually publishing. I think NUKE is not very different here.
Anyway, I agree that this should be phrased a little more accurately. Thanks for that.
That's what I thought too, I am kinda struggling: &gt;In the method known as ***separate chaining***\*\*, each bucket is independent, and has some sort of\*\* [**list**](https://en.wikipedia.org/wiki/List_(abstract_data_type)) **of entries with the same index**. The time for hash table operations is the time to find the bucket (which is constant) plus the time for the list operation. The problem is that sometimes it is said that when an hashtable is using separate chaining the number of entries can be bigger than the size of the buckets except that the `Resize()` method leads to: var newBuckets = new int[newSize]; // ... var newEntries = new Entry[newSize]; &gt;In another strategy, called **open addressing, all entry records are stored in the bucket array itself**. When a new entry has to be inserted, the buckets are examined, starting with the hashed-to slot and proceeding in some *probe sequence*, until an unoccupied slot is found. &amp;#x200B; So I'd say ok yes this is separate chaining since we clearly have a buckets `int[]` array and an entries `Entry[]` array... separated. Still don't get how the author of the Code Project article ended up saying that the .NET Dictionary leverage double hashing (i.e. open-addressing)
Wat?
#TRY TO USE JSON.NET ITS BETTER 
Do your src and href in script and link tags start with \~/ ?
DevExpress is pretty great, but not free. SQL Server Reporting Services is free if you already have a MS SQL license. If by free to meant free as in speech, you're going to be rolling your own using a FOSS graphing or PDF writing library or stuck with something like Java / jasperreports on the server side. 
You guys should really consider using something like MediatR. Writing your own classes using reflection and dynamics and whatnot isn't going to scale well. With MediatR pipelines, you can also use the mediator pattern to also have it automatically run things like validation. Not to mention just learning the mediator pattern in general is a good idea.
Starting a business to sell something to OP that he demands for free seems like a bad business model
Hey, thanks for the reply, we are aware of MediatR, but that's not the point. One of the goals is to talk about the basics and how it works under the hood (e.g. handlers), which is why we do some things this way as well :).
&gt; The problem is that sometimes it is said that when an hashtable is using separate chaining the number of entries can be bigger than the size of the buckets except that the Resize() method leads to Yes, in general, separate chaining can mean having more entries than buckets. But that just means what `Dictionary` does is a variant of separate chaining, it's certainly nothing close to double hashing or open addressing.
Excellent, bye bye BitBucket!
Now get back on time with all those people blaming Microsoft, saying they‚Äôd make it worse
It's easy. You just have to write the rest of the code. 10 bucks, seriously? Most professional developers make 5-10 times that an hour, or more.
Apologies if it‚Äôs insulting I mean no offence, I‚Äôm just stuck and it‚Äôs urgent that‚Äôs all, apologies.
Absolutely spot on thank you
I just deleted migration and started again 
This is just so cool! Way to go Microsoft!!
"They just want your data. Good bye to your privacy. They'll steal your code. Can't wait for the censorship." etc. etc.
Check Out the github repo [https://github.com/microservices-aspnetcore](https://github.com/microservices-aspnetcore), It's for [this book](https://www.amazon.ca/Building-Microservices-ASP-NET-Core-Cross-Platform/dp/1491961732) I did first few chapters and all apps are written with TDD in the mind. It might be dated as it's for .Net Core 2.1 or 2.0 but principles are the same. For example: [https://github.com/microservices-aspnetcore/locationservice](https://github.com/microservices-aspnetcore/locationservice) You will find the source code and test code for this location service. It's very neat as it give you integration tests as well. &amp;#x200B; &amp;#x200B;
Just tried now. That's awesome.
Exactly
There are 3rd party platforms that do this for you. Are you trying to do all of this by hand? 
I‚Äôm just trying to implement a quick simple system where any user can leave a comment, I‚Äôm trying to avoid using any third party features, what platforms are there out of interest? 
Why can't you just use a third party platform for this? If it's that urgent you're not going to write it yourself. This is like offering a $10 bounty on that old meme "now draw the rest of the fucking owl".
I just want some insight if anyone‚Äôs done it before or if anyone has any resources, I mean I‚Äôll look at the third party resources 
I‚Äôm just trying to see if anyone has done this before or knows any resources? I‚Äôm open to looking at some third party resources 
Anything platforms you suggest? I‚Äôm just seeing if anyone knows the best plan to move forward, any resources you know would be greatly appreciated 
I looked at your post history. You're making a blog. Use Orchard and stop this "uphill in the snow" stuff until you know what you're doing.
Your solution is, you don‚Äôt know how to do it so don‚Äôt it? I don‚Äôt want a blog I want to build a blog website 
&gt;Your solution is, you don‚Äôt know how to do it so don‚Äôt it? I don‚Äôt want a blog I want to build a blog website No. My solution is "you have no clue how to build this so learn it from something that already exists". &gt; I don't want a blog I want to build a blog website You have no idea what you're doing and this is way over your head. Start WAY SMALLER then build on it.
Very constructive of you, I‚Äôve managed to create this application up to the point where I have database seeding, claim biased authorisation system, all security risks mitigated, if you don‚Äôt ask for help you won‚Äôt get better. 
Good for you. Unfortunately you haven't learned how to ask for help. "How do I build a huge thing winner gets $10" isn't what you need to ask. That's a shitpost, honestly. Start working on your code and figure out your next step on what you want to build, then go until you get down to nuts and bolts of a problem. Then ask for help on that specific problem. Posting some vague class outline and asking "what next" is asking someone to do it for you, not asking for help.
Lol i just renewed my yearly account too just for the private repositories...
Does this work for organization accounts as well? 
Soooo... I can cancel my subscription now? Right? 7‚Ç¨/month for private repos wasn‚Äôt too bad, but I definitely appreciate this. TY MS!
They‚Äôll probably even wash your under garments if your not careful. 
Depending how recent. Try shooting them an email!
I'm not following your comment about the library. The library itself is not generated code. Presumably it generates code for you. I've used some code generation systems with no problem. And others where I've had to debug generated code, and tore my hair out. The ci files look fine. But with psake all I'm running on Jenkins is simple code that I wrote. A lot of times the psake files end up being copy paste replace jobs. But I can also use the same system for more complex jobs where I need to build packages from more than one scm system, and I have full control. 
Your question may not be the best, but I don't think you deserve to get roasted over it. So... couple of quick things: 1. Ditch the bounty. People on here want to help and be involved for entertainment/education, etc. There are much better avenues to get paid, and it apparently triggers people. 2. It sounds like you want to build it, not use a third party app. With that assumption in mind, the question is very broad, so it may be hard to just get a simple answer for this. It is like being asked how to build a house. The answer is to research it, because it isn't just a simple answer. 3. To try to get you pointed in the right direction, I would try jumping on github and find an open source sample app that does something similar. In fact, you are just asking how to do basic CRUD in asp,net core. There are a lot of simple examples of this... find a TODO sample app or something on there and browse around. 4. You are going to need to decide what data store to interact with, which framework pieces or libraries to use to interact with it, research Razor Views for posting back and displaying your form data. Research MVC model binding so you can see how data gets bound to an object to interact with it in the controller. You are wanting to do a very basic [asp.net](https://asp.net) core thing, so honestly the best advice... go read the [asp.net](https://asp.net) core 2.1 docs. They are actually really well written, not terrible long and drawn out, and will help you a TON. Good luck!
Appreciate it, went about it the wrong way. 
I‚Äôm not following either. The library (Nuke.Common) contains generated code that provides a small API to invoke third party CLI tools more conveniently. It‚Äôs wrong to say ‚ÄúHey use a tool that generates code that will build‚Äù because that is not what nuke is about. At least that seems what this thread is about. I don‚Äôt understand the ‚Äúbut with psake‚Äù part. Nuke provides a platform to define targets/build steps and their dependencies. Everything else is plain C#, just your code, and you have full control. 
I just cancelled my monthly subscription. I no longer get the warning saying that I have until a specific date to access my private repos. 
"[Parameter("Configuration to build - Default is 'Debug' (local) or 'Release' (server)")]" What we've been doing is building both, including source for debug. The packages go into our artifactory instance in sandbox. We also multi target, so we build for 2 frameworks, and run both test dlls (and yes there have been tests that failed only on net45). Can you do setups like that in nuke?
Yes
Wondering same thing. I got a paid account just so we could keep stuff private, made an Organization just to not have it under my own user/etc. But if we can do it for free now would be silly not to.
GitVersion.GetNormalizedAssemblyVersion()) .SetFileVersion(GitVersion.GetNormalizedFileVersion()) .SetInformationalVersion(GitVersion.InformationalVersion)); .... .SetPackageVersion(GitVersion.NuGetVersionV2)); Looks really weird. 2 methods and 2 properties for version? What's that for? It's simpler directly in the csproj. I'm not sure what git would ever tell me about my published versions. 
[This](https://github.com/nuke-build/common/blob/develop/build/Build.cs#L90-L111) is an example where `dotnet publish` is called with multiple project/framework combinations having the same settings for restore, assembly version, configuration. I guess you can see how this would look like in your case. In my opinion, this one demonstrates the benefits of the fluent syntax quite well. However, you can also fall back to using string-based invocations like `DotNetPublish($"{project} --no-restore")` and so on.
You're discussing the result, rather than the tool itself. It's a simple fact that MSBuild allows to set assembly-version, assembly-file-version and also nuget-package-version. In this case, I decided to pass them properly for various reasons. In your own build, you don't have to.
Props to the geek era of Microsoft. It‚Äôs the creator of ASP.NET (Scott Guthrie) that is in charge of the Cloud division. #geekrules
Never forget that BitBucket was there for you when no one else was
Never forget that BitBucket was there for you when no one else was
If someone claims that, then they're essentially brain dead. Especially considering that most private repos usually have very small projects. 
I've been hosting my own Git repo server (Gitea) on a home server. Now with this announcement, I'm going to dump Gitea and move over to GH. 
This. 
do all the filetypes you support have a magic value to determine the encoding type? if not, you may need an enum or something to say "attempt to treat this stream as on ogg", or whatever. 
i agree, but i think that c#'s async await is a superset of the functionality i want. i appreciate that having control over things like where the continuation runs or over the sync context is important to some people, but I'm not convinced that it is actually necessary. it'd be nice to "opt-out" somehow, and have it all just work transparently. it's "too late" for c#, but the good news is that async await is pretty damn good, and really the only downside is that you have to say async, await, and task to play.
Same for gitlab. I use GitHub for my public repos and gitlab for my private repos and collaborations. I don‚Äôt think that‚Äôll be changing but kudos to GitHub on the change. I paid a subscription for GitHub for a longtime for 5 private repos, which just encouraged bad practices for me.
"managed thread" is just .net hiding the implementation details. it's real threads underneath. 
in the net core/docker world, you need a publish step. this can only be done at the csproj level. build is great, but you usually can't do anything with that unless you already have custom build/package scripts anyway. it's obnoxious that you can't "publish" an sln and wind up with a bunch of published csproj folders. 
what? you can absolutely build a solution. it just builds the projects (not even necessarily csproj) in it. 
I'm not one of those people who said Microsoft will make it worse, but just because something is free does not mean it's in your best interests. Making it free is just an investment in future profits. The question is how they are going to collect? Probably by encouraging people to use Azure for hosting.
if you're developing for windows now, I'd recommend op investigate the windows credential manager. it's built for this sort of thing. as long as op introduces a reasonable abstraction, xplat can still be an option. i realize op doesn't want os specific stuff, but that's the nature of the beast when doing this sort of thing.
You make a fidget wider.
I use Azure DevOps, formerly Visual Studio Online, formerly Team Foundation Service. They've always allowed unlimited public repos for up to five users.
You can't build a solution without a Sln file, you can build projects, but a project is not a solution. 
Good job MS! Frist don't core and now this &lt;3
you're right about that, i must be confused about what you're trying to get across. building a project is building a single [DAG](https://en.m.wikipedia.org/wiki/Directed_acyclic_graph), a solution is capable of a DAG with more than one root. 
Desktop link: https://en.wikipedia.org/wiki/Directed_acyclic_graph *** ^^/r/HelperBot_ ^^Downvote ^^to ^^remove. ^^Counter: ^^230422
**Directed acyclic graph** In mathematics and computer science, a directed acyclic graph (DAG (listen)), is a finite directed graph with no directed cycles. That is, it consists of finitely many vertices and edges, with each edge directed from one vertex to another, such that there is no way to start at any vertex v and follow a consistently-directed sequence of edges that eventually loops back to v again. Equivalently, a DAG is a directed graph that has a topological ordering, a sequence of the vertices such that every edge is directed from earlier to later in the sequence. DAGs can model many different kinds of information. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Not quite sure. The file decoding is handled by `libsndfile`, and I don't recall seeing that the API needed the programmer to specify the file type. I'm guessing it's a magic number?
Doesn't every service do unlimited public repos? Private are usually what's limited. 
You could create a small inhabitant class with the pet property. On the DB side you would have a new Small Inhabitant table with a nullable PetID column.
My bad, I meant private repos.
Yup. 
Awesome news! 
Wow... I'm pretty sure nobody has ever written this before, ever. 
And if you don‚Äôt use the template store and only use jsreport-core then I don‚Äôt think the limit applies. Don‚Äôt quote me on that though. 
I believe you might be correct, as the limit applies to templates stored on the server. So in theory you can use the .net core middle-ware to render an MVC view (or just pass the HTML template to the server with every request), but I haven't tried this.
Gitlab‚Äôs success was achieved by mass-advertising. It‚Äôs a shit platform. They hyped it to be a real feature packed rich platform that would kill github. Turns out they have stricter constraints than github, more expensive, no apps, ... All they did was build a fucking website with a chat box and used gif.
.....while I'm wearing them?
I like bitbucket. :(
Thanks that makes sense!
This how I got my Transcation Templates to work. The emailModel is just a public class containing firstname, lastname. var apiKey = _configuration.GetSection("SENDGRID_API_KEY").Value; var client = new SendGridClient(apiKey); var msg = new SendGridMessage(); msg.SetFrom(new EmailAddress("no-reply@test.com", "Test Email Client")); msg.AddTo(new EmailAddress(email, "Customer")); msg.SetTemplateId(templateId); msg.SetTemplateData(emailModel); var response = await client.SendEmailAsync(msg); &amp;#x200B;
My only reason to use BitBucket, tbh.
You should keep it if you have a wiki on your repo.
Not everyone lives in the US though. I only make 1.65 times that and I'm in Western-Europe.
[removed]
What did you have in the Transaction template (- firstname -)? Still getting errors. 
I agree, it was just a joke about all those people that always blame Microsoft without any reason.
I‚Äôve saw many people saying that when Microsoft bought github 
Hey dude, just got it working! I see that they're using the handlebars now instead of the --. If anyone comes looking for the answer inside of the template use {{varToChange}}. Thanks again mate, 
Why not?
I'll keep paying just because I believe in supporting services I rely on and it's not like I can't afford it.
I'd love to, pauldotknopf at gmail dot com
Maybe you don't get attention through GitHub, but it's the best place to share code and allow people to review the program very easily.
Hey mate! Yeah I am using handle bars, sorry totally forgot about that! It was a real pain for me to getting working the first time! Glad I could be a little bit of help. 
Broad outline of how to approach the problem. Study the documentation for details. Entity. Ok, so you know how your Comment entity should look. That's a start. You are missing a reference to the blog post being commented on, and a date. You may also want to add a boolean Enabled field, just in case you want to add a moderation feature later. Data store. Set up a database (or add to an existing one) with a Comment table (based on the class you posted). Then import the model into your application using Entity Framework. Now you can interact with the DB from your code using Linq. Back end. Create a Comment controller class with methods to i) list comments and ii) create a comment. You may want to add filter and order parameters to your list method. User interface. You will need forms to i) display a list of comments, and ii) save a single comment (with some basic validation, posting back to the list). Your UI should be talking to your controller to accomplish this.
Then I will put it on Github. Thanks for your answer
Explaining how this compares to [cake](https://cakebuild.net/) would be helpful
Great read. Though I have never heard / saw someone use these "Chicago" and "London" style TDD terms and they do sound quite funny and unnecessary honestly.
Thanks! To be honest the difference between London style testing and Chicago style testing it huge - thus the naming distinction.
A whole $10 bucks? I'll get started right away!
One difference is in the title :) More information is on the website https://nuke.build And all questions are also welcome.
[This article](https://thenewstack.io/project-orleans-the-net-framework-from-microsoft-research-used-in-halo-4/) talks about how New Orleans and Trill are used for Halo 4.
why the hell there is no option to initialize new repo with .ignore for c#? that makes me so angry!
StackExchange has a site specifically for that
You may have not read my post then. That website works for a bunch of snippets not for something slightly bigger.
You could post a link to StackExchange...
That's not against the rule of SE CR?
So basically Visual Studio Online
Yeah I'll be sticking to BitBucket for that exact reason - although I guess I might back up some repos to GitHub too, since it's easy enough to add another remote.
Nah, a lot of us don't subscribe to the "We hate Microsoft" circle jerk. I don't think they're perfect, but they've done a lot of cool stuff. I mean, you're on a .NET sub, so clearly some of us like something MS is doing.
yay this rocks, was hoping for that to happen for years..
Oh don't worry, plenty of people screaming "extend!"
I don't know what you're talking about. I've been a administrating a gitlab service for some time at a company and it's been pretty good. Most of the features that you buy from Gitlab you can obtain from a different OpenSource product (like redmine for example for issue tracking). Another cool feature is monitoring with Prometheus which Gitlab supports by default... Again... I don't know what you're talking about but Gitlab CE is great.
[.gitignore for Visual Studio/VSCode](https://raw.githubusercontent.com/github/gitignore/master/VisualStudio.gitignore) Other types here at [GitHub/gitignore](https://github.com/github/gitignore)
Make basic reports free, but have a limit to the number of generated reports and lock bulk report generation and advanced features like logo placement etc behind a business tier. Basically built something like this at my last company. Shit is hard 
I searched in the search bare on nuke.build for "cake" and "comparison", and it sat there spinning. I searched the documentation for "cake" and "comparison", and didn't find anything. That's why I asked.
I'm just confused. What does git tell you about the version of your artifact? And why are two methods while two are properties?
&gt; GitHub also today announced that it is changing the name of the GitHub Developer suite to ‚ÄòGitHub Pro.‚Äô The company says it‚Äôs doing so in order to ‚Äúhelp developers better identify the tools they need.‚Äù Within a few years there gonna be a Github Pro Essentials and a Github Pro Premium, Pro Student, and different EU versions of each? :P
Yes we use it on multiple live apps
It's the different schools of thought on how to implement/employ TDD. Teams in both Chicago and London simultaneously developed the idea that is TDD. They are actually very common nomenclature used specifically for differentiating between the two approaches. 
So this lib turns linq into an sql string to execute on the database?
Oh yeah agreed. Just getting solid and easy PDF generation is difficult (especially at scale) and that's without any decent templating.
Nuke is just a .NET console application. Fake or Cake are custom DSLs with compilers built on top of the roslyn and/or F# compiler. You break out of the existing supported application models built by microsoft, you'll lose at least some level of support.
Informational version contains the git hash and branch from which it was built. That can be useful if it‚Äôs a prerelease and there is no tag or if it was built in some other environment. The difference in assembly-version vs. assembly-file-version is because it allows drop-in replacement if it‚Äôs only major-minor. Maybe not necessary in this case, but can be good habit. Package version is obvious I guess.
Yes building SQL strings and/or executing it directly check the docs at https://sqlkata.com
AWS LightSail - [https://aws.amazon.com/lightsail/pricing](https://aws.amazon.com/lightsail/pricing)
I was exaggerating for effect, but I've never witnessed anyone so genuinely praise a business move by Microsoft, ever. 
"Target Test =&gt; _ =&gt; _ .DependsOn(Compile) .Executes(() =&gt; { Solution.GetProjects("*.Tests") .ForEach(x =&gt; DotNetTest(s =&gt; s" not clear what framework (s) are tested.
In proper engineering its called validation and verification; ie: validate for completeness, verify for correctness. 
In proper engineering its called validation and verification; ie: validate for completeness, verify for correctness. Software industry pundits love to re-invent things so they can write books and speak at conferences, while ignorant of what engineers have been already been doing for the last 90 years.
In proper engineering its called validation and verification; ie: validate for completeness, verify for correctness. Software industry pundits love to re-invent things so they can write books and speak at conferences, while ignorant of what engineers have been already been doing for the last 90 years.
Its just validation vs verification. 
I think he is talking about Tasks which use threads from a thread pool, but they are managed for you. You don't get specific threads and run code on them. Your function may even end up running on the thread you launch it from. And you don't have to marshal back to the UI thread in order to manipulate UI, it's done for you.
It depends on what target frameworks are defined in the project. Nuke is an orchestration tool. You still have to have knowledge about all the other tools involved - like dotnet CLI here. 
What about performance? Tons of useless external processes or a Nodejs resource hungry crap even when create a new console application in VS 2017.
Is [EF6 migrations](https://github.com/aspnet/EntityFramework6/issues/382) fixed?
I'm with you. 2017 feels bloated.
Entity Framework is on its own release cadence.
What do you mean by bloated? You can choose what you want to install. If you don't want to install node.js, then just untick the box that says "node.js" in the VS installer. Same goes with any other framework/addon/designer/etc.
I tried to find out why is there the JS crap. It loads a JS that spawns these other host processes. Why it has to be written in JavaScript? Isn't a compiled C# code better in every aspect?
Ya, it doesn't matter if you can pick packages at install, anything I pick still feels slow with a sizable solution. 
Even worse, JS based.
No, it is nodejs used internally by the VS.
What do you mean by that? Can you provide an article or something about this?
How long did you try it for?
No, it spaws following child processes: devenv.exe PerfWatson2.exe ServiceHub.Host.Node.x86.exe &lt;---- HERE conhost.exe ServiceHub.IdentityHost.exe ServiceHub.VSDetouredHost.exe ServiceHub.SettingsHost.exe ServiceHub.Host.CLR.x86.exe It executes this script ./ServiceHub/controller/hubController.all.js. Why? What's wrong with the same C# code?
Live unit testing in all editions would be nice instead of it only being available in the enterprise edition (which my company is never going to pay for).
I think he's referring specifically to [https://codereview.stackexchange.com/](https://codereview.stackexchange.com/)
i already pay for my msdn subscription, sql server licensing and everything I host on azure. I'll not lose any sleep over saving on this particular fee.
No need to. JS interpreted code will be always terrible and unsafe compared to native one. And I need a real C# debugger :-)
Wonder what else is most useful in enterprise edition? 
You aren‚Äôt a real programmer 
Why? Because I have spent 20+ years of Windows API development so I can see how shitty the Electron is comaped to all existing native Windows frameworks from MFC to WPF?
did they speed up load times? I gave up on VS2017 simply because the load times were horrible. Even despite disabling extensions and other add-ons. 
You do not like them, so you say. Try them, try them, and you may. 
Xamarin Profiler. Time travel debugging. Personally I think Xamarin Profiler really needs to be in Community. It's not like I can just buy an enterprise license, so profiling [n](https://Xamarin.Android)on-Windows Xamarin apps is not very good for me.
It doesn't have any features I need like proper debugger. And any Google based malware is banned on my machine :-)
Please continue enjoying visual studio. I like it too. But maybe set up a vm and play around with VS Code sometime. A lot of us enjoy it and MS works very hard on it!
You didn't just *start* using a specific tool one day, you had to try it and experiment to find what you like. Saying you don't like it without even trying it is unreasonable.
[Simple fix](https://stackoverflow.com/questions/42769106/visual-studio-2017-node-js-server-process-turn-off) for this problem. Besides, I'd hardly call intellisense and code formatting bloatware, which VS uses node.js for.
Would you try a cheap $3 wine if you are were a wine consessuier used to drink more serious one? I have already tried the Skype 8 and it was enough :-) Moreover, I don't do any web applications (as I don't like HTML/CSS/JS) anyway.
TL;DR: nothing.
No, I even don't have any JavaScript item in the Options / Text Editor.
Did you read my reply? It's used for VS internal features, not web development. It's hardly "useless". You can criticize VS because it doesn't use C# for this, but to call it bloatware is false.
Then it is supposed to be rewritten (back to VS 2015?) because it is completely improper technology for the task.
Ooh are we comparing career length? 25 years here. Why are you bitching? Use it or don't. It's a tool. There's no need to whine about it.
I see this posted every time anyone even mentions C# and yeah, it takes a bit to start up, but after that I regularly run 5-6 instances of it and have 3 or 4 running when testing something complex and it works fine. I also have a decent laptop from work, not a piece of shit so ... i don't know. I also have resharper ultimate going. &amp;#x200B; At home I use Rider for mac and it's fucking awesome, much better than VS imo. It does cost money but not a fortune or anything. &amp;#x200B; I mainly do api/nservicebus services/web apps/etc, not unity/xamarin. .net core 4.6 at work and .net core 2.2 at home.
Snapshot debugger is enterprise only I believe
By chance were you also using Resharper? VS2017 was pretty snappy for me otherwise, even on my old 3770k. Resharper makes it bog pretty hard, though.
No, just technical aspect why desktop application (here Visual Studio) written in native code using Windows API will be always better than the one written in JavaScript running in embedded web browser
&gt; CodeLens for Everyone Nice. It's the one thing I missed in VS 2017 Community.
VS 2017 is more performant than VS 2015 though
I can't confirm that at all.
You're gonna have to back that up if you want to be taken seriously. You mention C# a couple times elsewhere. You know that isn't native code, right? &gt; developers coming from the "web world" can't see the difference as they don't have experiecne with native code and compiled languages. Gonna have to back that up, too. At any rate, nodejs isn't a browser.
Here is Microsoft's post on performance in 2017: https://blogs.msdn.microsoft.com/visualstudio/2016/10/14/improved-overall-visual-studio-15-responsiveness/
C# is compiled to MSIL and it is JITed directly to native code. It isn't interpreted as JS with complex and dangerous type inference behind the scene. Many Visual Studio components are still Win32 C++ or COM objects. VS Code is HTML/CSS/JS application running in embedded web browser / html engine, that's the core idea of the Electron. I haven't mention 'nodejs' in the last reply anywhere.
the 2017 installer runs like molasses. i'll want to see if the 2019 installer is faster, or has an offline option.
I don't. Must be your setup.
Because it is the Electron crap again. It is no longer native Windows application.
Yes. I tried disabling Resharper, but only noticed a small improvement in loading. VS2015 Ultimate on the same machine loads much faster. Even with Resharper. 
Searching through watched items. Fuck yeah!
I can confirm that :-)
See https://old.reddit.com/r/dotnet/comments/adzgha/visual_studio_whats_new_in_visual_studio_2019/edlkm4x/
See https://old.reddit.com/r/dotnet/comments/adzgha/visual_studio_whats_new_in_visual_studio_2019/edlkm4x/
It's a much faster on load and not as much of a resource hog but it's by no means perfect.
Indeed, me too.
I can't argue with anecdotal evidence but here is MSFT post about startup times in 2017: https://blogs.msdn.microsoft.com/visualstudio/2016/10/10/faster-visual-studio-15-startup/ 
DevOps still offers 5 users while the GitHub offering is limited to 3. Also GitHub's free offering doesn't include Wiki/Pages while DevOps does. There are lots of other differences but obviously they are in the same area of service so I wouldn't be surprised if they continue to converge. 
It's been some time that I didn't try the Xamarin Profiler, so take this comment with a grain of salt. IMO, the Xamarin Profiler isn't worth it. It's clunky and not very usable. I found most of my leaks with Android Studio. However, it was possible to start it as a standalone app while targeting a device (the supposedly enterprise only feature) I'll try again tomorrow with the latest version of the profiler.
Yeah, uh, I actually have used it sometimes, without an Enterprise license. I didn't find it to be very useful, but from screenshots I'd seen of it, compared with the way I was using it, it seemed to be missing options, so I didn't want to blame them. The way I did it was with the anonymous post here: [https://xamarin.uservoice.com/forums/144858-xamarin-platform-suggestions/suggestions/18625141-make-xamarin-profiler-available-for-professional-s](https://xamarin.uservoice.com/forums/144858-xamarin-platform-suggestions/suggestions/18625141-make-xamarin-profiler-available-for-professional-s) I have used the Android Studio profiler, which actually has a very nice UI in my opinion, but I found it difficult to trace things.
GitHub's newly free plan for private repos is also 5 users. The free service tops out at 5 collaborators, if you want to go above that you have to pay.
Electron apps can install faster than native apps... VSCode installs faster than the older native versions of VS.
Nothing remarkable for my use case. I am using VS 2017 Enterprise for C# development. Don‚Äôt see much that would make my life significantly better. 
But always will be sluggish regarding user experience.
Sluggish would be a more accurate term.
This confused me too when setting up a repo on Azure Dev Ops. It's listed under `VisualStudio` in the lookup.
We're using Biztalk to do EDI. A collegue of mine did a presentation and setup using LogicApps for EDI. I'll see if I acan find it. &amp;#x200B;
I'm tired of constantly new features that don't really improve the experience. How about fixing the abysmal git integration? Or improving javascript library debugging? How about when an exception is thrown and I land in the catch clause, you tell me which line of code above threw the exception? Better C++ integration would have been awesome on my last project.
JS is not interpreted it's JIT compiled to assembly. So you're wrong there. Beyond that electron is a platform, it has good stuff written on top of it (vs code) and shit stuff written on top of it (Spotify). It's fairly memory hungry because Chrome is fairly memory hungry, but at the same time, who the fuck cares? 
Except it isn't!
There are plenty of garbage native apps and excellent non-native apps using things like Electron. Hopefully web assembly will bridge a lot of that performance gap and it will be a moot point.
Dammit, I thought there's a new preview coming out.
Yeah. It's pretty bloated and slow. There is no other good alternative though so we're stuck with it.
VSCode is not a newer nonnative version of Visual Studio...
Visual Studio is such a mess now, I bet nobody at Microsoft wants to work on it.
I‚Äôve mostly switched to JetBrains Rider for more than six months now and can heartily recommend it for a lot of workflows. (mostly web apps)
Except you're completely and totally wrong. JavaScript is compiled to assembly just like C#. The node runtime makes exactly the same syscalls as any other windows app. Visual Studio Code is as Windows native as full Visual Studio is. It runs in exactly the same way. It's compiled to ASM same as anything else. It uses a different UI framework than full VS does, but it's just as native an app. That's true for both the Web and Electron BTW. You might want to crawl out of your cave and actually investigate these things you're scared of or you're going to end up as obsolete as your attitude. 
Uninstall resharper and VS 2017 practically flies. 
Hey thanks. The article was quite informative. 
I wish they would just build most of Resharpers features into it
Amen to this. 
Huge number of child processes is also because VS continues to be a 32bit app. So is limited to 4GB per process and fuckzillion processes is a workaround for that limitation. 
&gt; Visual Studio Code is as Windows native Wrong. If so, it would follow custom ClearType settings as any other Windows native (Win32 API, MFC, WinForms, WPF) application. 
No good alternative for what? If you‚Äôre doing .NET then Rider is an excellent replacement, I haven‚Äôt used vs professionally in months now. For C++ on Windows, CLion is _almost_ there but I‚Äôm still running into a few issues that mean I have to use vs instead
Single 64-bit host process would resolve it. But all these spawned processes are 32-bit again on a 64-bit system. My guess is that those new JavaScript kiddies haven't noticed there are threads in the OS :-)
&gt; The ‚ÄòConvertTo‚Äô function is used in the ‚ÄòDynamicUpdateMap‚Äô class at: &gt; System.Activities\System\Activities\DynamicUpdate\DynamicUpdateMap.cs &gt; As it was not possible to find out how this can be exploited and triggered, more investigation is required to check whether this can be abused. High quality research there. 
Again, you're wrong. JavaScript is JIT compiled, but so is C#, the compiler doesn't have to "guess" the types at all, assembly doesn't even have types. And yes, it makes win32 calls, how exactly do you think it's able to open windows or access files? Do you think it's fucking magic? Chrome by default chooses not to use clear type because it's crap, but it can, because it's win32 code. 
&gt; assembly doesn't even have types. Yes, but to run the code on physical CPU types are required, that's why JS is too slow, resource hungry and unsafe. C# has exact type information in both source code and MSIL, no complex type inference is required later. &gt; Chrome by default chooses not to use clear type because it's crap Yes, Chrome is crap, fully agree there.
In my view the difference is more about the approach to testing: \-London style TDD: testing a behaviour of an object in **isolation** with most of the other cooperating objects around it **stubbed/mocked** \-Chicago style TDD: testing a behaviour of an object and it's **interactions with other cooperating objects** in the same way those objects are used in the application; the behaviour is **not** tested in isolation; here you need to realize what is the boundary of the "cooperating objects" - you don't want to end up testing it end-to-end including http web service calls - you would stub/mock these web service calls even in Chicago style TDD, but the need to stub or mock in Chicago style TDD would be exceptional, only for these external facing services.
I'd keep them separate as the website may need to be updated more frequently than the API service (or vice-versa). The API host project and API client project (not shown in your example) would probably want a semantic versioning scheme, while the website doesn't need it. But it's easy enough to copy the git repo later to split the website project and the API projects.
&gt; I can compare VS 2015 and 2017 running on the same hardware and the difference in performace is huge. I noticed this as well, it bugged me so much I went back to using VS 2015. I can do all my work, no disadvantages as far as I can see. Am I missing out on anything not using 2017?
&gt; Simple fix for this problem. Not according to that SO post you linked to. I couldn't see where someone had the definitative answer that worked for everyone. And, as someoone in that SO post points out: &gt; https://developercommunity.visualstudio.com/content/problem/27033/nodejs-server-side-javascript-process-consuming-to.html?childToView=27629#comment-27629 &gt; Effectively I was told: &gt; In VS 2017, several features are implemented in JavaScript. Node.js is used by Visual Studio to run that JavaScript. Among other things, **Node is used to run the code that provides formatting and intellisense services when a user is editing TypeScript or JavaScript. This is a change from VS 2015.** My bold. So in VS 2017 if you want JS intellisense, you need to put up with it.
I installed the 2019 preview with C#, .NET Core and ASP workloads in about 10 minutes
Are you trolling or just really, really ignorant. The CPU has no concept of types at all, types are a compile time check, that's their purpose. At runtime it's just data. MSIL does have types, but when it's JIT compiled (just like JavaScript) the types are stripped away. Types have nothing to do with speed, resource usage or safety. JavaScript is compiled to the same assembly as everything else and it's got a garbage collector just like C# so it's equally safe. Win32 is the API to the Windows kernel (well actually win64 now), and every single Windows application uses it. Not every single application uses Windows UI controls, but that doesn't mean it's not native, because it is. Chrome is a native desktop application which renders a UI marked up with HTML, just like WPF renders a UI marked up with XAML, the output to your screen isn't either HTML or XAML. No, not Chrome, clear type. Clear type is a hack to fix apps with outdated font rendering. Windows doesn't use it on its own UI, and Chrome doesn't need it. You are dangerously incompetent. 
It should also include IntelliTest (does not work in .net core anyways)
If `PhantomReference` does not provide notifications about the object being GC'ed and requires polling, what does it do that a `WeakReference` can't?
New c# features.
The load times for 2017 were one of the first things I noticed when I started using it. It would load up twice as fast as 2015 in pretty much every case. I don't have resharper, but I have a bad habit of installing everything I have storage for. Still way better in comparison.
I never said it was a solution that fitted everyone's problem, but I tested this on a machine from work and it solved the problem, at least for me. But even without applying this possible fix, I didn't notice any issues whatsoever with VS 2017 so far, at least not to the point where I needed to think about going back to 2015, which brings it's own array of issues.
What would be an usecase for streaming via websockets?
If the most important features were free, what would be the reason for paid versions?
Start the container with a volumemount and safe the data there: this should solve your issue. Another solution would be to use a network-share and mount this in the container. `docker run -v localpath:containerpath`
I‚Äôve gotten so used to Rider now, and they keep improving as well. I doubt I‚Äôll switch back to VS+R#.
Maybe voice in the webrowser or sharing screen?
 [This](https://blogs.msdn.microsoft.com/visualstudio/2018/12/13/get-to-code-how-we-designed-the-new-visual-studio-start-window/) remarkable demonstration left me nauseous and angry about what is being done with VS. &gt; We knew we needed deeper insights into how we could help new users Gag me. https://stackoverflow.com/questions/22658749/is-there-a-way-to-automatically-collapse-the-script-documents-section-in-solutio https://visualstudio.uservoice.com/forums/121579-visual-studio-ide/suggestions/19305238-provide-a-way-to-prevent-the-script-documents-fold The above issue is as flagrant and fragrant as they come and hasn't been fixed in five years. Yet the VS team finds time to bask in the triumph of their ability to to create a list of recently opened projects. Something is wrong. 
In Java, it's not `PhantomReference&lt;T&gt;` itself that provides notification, it's the `ReferenceQueue&lt;T&gt;` where it's registered. And it's not really a notification, you have to check the queue. Currently, the closest you can get to a notification in .NET is to use a `ConditionalWeakTable&lt;TKey, TValue&gt;` where you use the tracked object as the key and an object with a finalizer as the value; you then assume that when this finalizer runs, the tracked object is no longer strongly reachable. In both cases, you can't assume the opposite: just because you haven't noted that an object has been queued/finalized, you can't assume it's still strongly reachable. In fact, if the `ReferenceQueue&lt;T&gt;`/`ConditionalWeakTable&lt;TKey, TValue&gt;` becomes unreachable, all odds are off.
I gave up mostly on supporting old netstandard versions. Almost nobody uses the software I write but I have mostly migrated to netstandard2.0 with an additional target to net46. Honestly. The whole netstandard1.x era was interesting but more or less I think it was a disaster. I feel bad if people are stuck in that. I did not like the tiered standard. I don't think anyone would create a netstandard1.3 project by choice now. Netstandard1.0 is still a useful target though if you can manage to meet the limitations. Since it reaches quite abit more than netstandard1.3 ever will.
I have used BitBucket a lot in the past for free private repos. Then moved to Gitlab (I guess due to hype? or better user interface). I will be sticking to Gitlab for a now however. Maybe for future projects I will use Github. Though I have always used Github for public repos.
Okay thanks its work 
You are wrong in every sentence but the discussion in apparently pointless. 
Yup, that was the method I was using. Still works with the latest 1.6.9(preview) version of Xamarin.Profiler. Android Studio is great for the "java" side of things. 
That is very useful, esp. for people on Linux/OSX where you can't get any decent profilers easily.
or even high speed stock or cryptocurrency trading websites
I can't open this &gt;Secure Connection Failed &gt; &gt;The connection to the server was reset while the page was loading. &gt; &gt;The page you are trying to view cannot be shown because the authenticity of the received data could not be verified. &gt; &gt;Please contact the website owners to inform them of this problem &amp;#x200B;
I wish they wouldn't. I mean some of them are very nice but integrating all of them would cause a severe performance bottleneck.
Given what I understand are the proper usages of async/await (abridged version: free up threads to process other request while you wait for some outside request to finish)....why in the world would you need the ability to use them during startup? The app is not handling any request right now that would require passing the processing off to another thread in order to allow the web server to accept another request, async methods still run serially in your code so you are not getting any parallel-ness going on... Now, if this article focused on creating a list of tasks and running them in parallel when starting up the app, I could see that being extremely more beneficial.
Have a look at IHostedService [https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/multi-container-microservice-net-applications/background-tasks-with-ihostedservice](https://docs.microsoft.com/en-us/dotnet/standard/microservices-architecture/multi-container-microservice-net-applications/background-tasks-with-ihostedservice)
You're trying to visit it over https
Thanks for the link, I've actually not looked much into that Interface and I think can probably leverage some of its abilities in our projects. With that said, my comment was more referencing what I thought would be more helpful to a wide-audience, not necessarily what I am looking to do at this moment.
nothing to justify that ridiculous price
the bottleneck is due to it being a 3rd party tool. they can only use the APIs given to them. the Visual Studio team can do whatever they want
the problem is if you would need node for a future project. having the option already available would be nice
how is NServiceBus for you? Always wanted to dabble but never got into it
lol their offline option is so ridiculous. uhh create your custom complicated XML file and have it generate an ISO for you. like, why not provide us with a simple GUI that generates the XML file for us?
If you turn on most features of ReSharper it slows Visual Studio down significantly. Integrating those features it would make no difference, there's no performance bottleneck for extensions just because. What those extensions do is what slows VS down. The biggest perpetrators in ReSharper being the constant file and assembly scanning. It doesn't do its magic without taking a toll.
nobody said "just because". do your research on it
I have Ubiquiti cameras, and their NVR software streams the actual camera feed over some proprietary web socket implementation. I don't know why, I just know its doable.
I cannot find anything online that would suggest Visual Studio extensions work slower than they would if they were integrated in, aka slower just because they're extensions and not native functionalities. The only difference is at startup time when they're loaded. So I'm researching your own argument and since I can't find anything on it I'm sure you can now explain why you think ReSharper's features integrated into Visual Studio would make them more performant. Besides the ones from ReSharper that overlap with Visual Studio's equivalent functions to no added benefit.
Amazing. I would never touch ampq without it. They are a commercial company of course but if you just ask they'll give you a license for personal use. I got in to it because of an engineer at work where we have windows everything and service pulse itself has saved me thousands of hours of BS work.
Na dawg go to your local library. Show some effort
tried using emojis as an encryption key (and end of file) and it did not work either. would be cool though
Here's the [specification for identifiers](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/language-specification/lexical-structure#identifiers). [More info on unicode character categories](https://en.wikipedia.org/wiki/Unicode_character_property#General_Category)
I just checked with HR and they assured me that it's well within policy to throw a dev off top of the parking structure for using an emoji as a variable name.
Additionally said programmer should have their "Programmer Card" taken, ripped up before their eyes and the pieces burned! : )
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/csharp] [I am mildly upset by the fact that C# doesn't support emojis](https://www.reddit.com/r/csharp/comments/ae9tvm/i_am_mildly_upset_by_the_fact_that_c_doesnt/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
No, there is no direct comparison on the website, as I think that would be one-sided. However, if you're okay with that, here are a few points which I think are unique to NUKE: - 100% C# with usual way of dependency management, debugging, navigation, refactoring - Parameter injection via attributes, which supports command-line arguments and environment variables - Shell-completion for targets, parameters and enumerations (think of `nuke --ap` can get completed to `nuke --api-key`) - HTML execution plan visualization - Auto-generated API for third party command-line tools like MSBuild, dotnet CLI, xUnit, etc. which gives a very rich experience for tool execution
Happy to help.
I've used it to watch the stdout for a running console application and spit it out in realtime to a browser window.
Yes that's the point. You didn't say anything about adding items. It depends on your use case. If you have a save / cancel button and your adding lots of items they may be cancelled there's no point sending each one back individually. But anyway the point is you didn't actually answer the question.
My interpretation of question was how to iterate a collection. Specifically, 'Build a dynamic collection in UI'
The value for `whatTile` never gets set anywhere in the code, hence it is always `null`.
Am I not setting it under Dirt() and Dirt2()??
Yeah that's incorrect. Read the stuff in brackets after that quote.
Maybe? Are those methods getting triggered from somewhere outside of the code you posted?
Yeah I have buttons that when clicked run them. Ive used breakpoints to see if the 'whatTile' gets set to anything and it does but when returing 'whatTile' the value is null.
Any way you can post the full class containing the `UseTool` method? I'm guessing you are actually ending up with 2 distinct instances of the `TileGrid` class - and the `whatTile` is getting set in one somewhere, but not the one in the `UseTool` method. Note how in the `UseTool` method, you are creating a new instance - that instance has nothing set on it and is only scoped to be available within that method call.
I could copy and paste it here, but its quite long. I think it is something to do with creating a new instance like you said 'TileGrid tileGrid = new TileGrid();'. How do I get around this?? Here are my two button clicks 'private void Dirt\_Click(object sender, RoutedEventArgs e) { TileGrid tileGrid = new TileGrid(); tileGrid.Dirt(); } &amp;#x200B; private void Dirt2\_Click(object sender, RoutedEventArgs e) { TileGrid tileGrid = new TileGrid(); tileGrid.Dirt2(); }' Im also creating new instances here. I dont need them to be new instances so how do i reference my 'TildGrid'? 
Huh?
In the constructor of the class containing your methods, instantiate the `TileGrid` class and store it as a private member.... ``` public class MyClass { private TileGrid _tileGrid; public MyClass() { _tileGrid = new TileGrid(); } // ........ private void Dirt2_Click(object sender, RoutedEventArgs e) { _tileGrid.Dirt2(); } // ........ } ```
Read over [the documentation](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-2.2#when-to-use-kestrel-with-a-reverse-proxy) to see if you need a reverse proxy or not, as it answers the same question as "Can I use Kestrel in web facing situations". In ASP.NET Core greater than 2.1 it is OK to run without a reverse proxy, but there are some limitations and I don't know if you are OK with those or not.
hmm
He posts on some shooter video game subs, he must have posted in the wrong sub.
[removed]
You are my saviour. I know it seemed quite simple but it worked fine. Thanks for your help :)
Sure thing!
You can embed a browser control into a Windows form, then use it to access Google maps as per usual, and inject JavaScript to do stuff. Not sure if Google's terms and conditions permit this. To answer a question you didn't ask, I've found at least in areas I've use it that openstreetmap is at least as up to date as Google ,and has more details about features etc. Especially things like paths in local parks (London UK). I'm not sure it's routing and directions are up to Google standards tho
Interesting about the Open Street Map, maybe I'm not using the correct layers? I searched up the company name of where I currently work at and it seems to be updated on Google, whereas Open Street Map can't even find it. It's been about 3 years since they moved buildings. I can give Open Street Map a few more tries, but I guess my eyes got used to Google. Thank you for the reply!
Maybe, just maybe, because over 25% of the population of NA speaks Spanish as their first language.
I‚Äôve been told they are Peruvian players tho.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/latexandloaf] [Monitoring GC and memory allocations with .NET Core 2.2 and Application Insights](https://www.reddit.com/r/LatexAndLoaf/comments/aeasdk/monitoring_gc_and_memory_allocations_with_net/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
If you need to use tuen, I recommend you this [nuget package](https://www.nuget.org/packages/Unicode.net/)
You should definitely use udp and not tcp for streaming.
The "North America" region of Nintendo actually works for The Americas as a whole.
I've been working in C#/.NET for 15 years. Every single project has required object pooling and basically always turns into a fight against the garbage collector. (Roslyn itself has, at last check, had to write three different pool classes.) I really really wish they had some form of first class GC tuning or allowed custom allocators / new operators for heap managed objects - pooling, permanently pinned swaths of memory for buffers. Stackalloc gives me hope, but not nearly going to get us there. 
This looks pretty cool! Thanks for sharing!
Switch to C# now unless you want a lifetime of working for Senior Developers writing WinForms apps with thousands of lines of code behind in a single class.
For me, -help would never work right the whole mess of defaults etc
32bit
&gt; why in the world would you need the ability to use them during startup? Simply because you're making external calls that use async APIs. e.g. you are running inside a Docker container and you need to check that your can successfully connect to the database backend before accepting requests. You do that by making a call to the backend using the configured credentials. That's an async operation.
&gt;Docker container and you need to check that your can successfully connect to the database backend before accepting requests. Valid, I understand when/what async operations are. But what I am asking is *why* do you need to run those async in startup when a normal synchronous request the database would work as well? From everything I've seen, Async and Await are not meant for parallel computing, rather they are used to not block your main thread. Given this, here is another way to ask that question: Why do we care if we block the main thread when starting up? I'm just looking where this could benefit your startup times/processes. 
That's essentially what he lays out in the article. "blocking the main thread in Program.cs" *is* the suggested solution for most cases. Pre-C#7.1, you'd have to use sync-over-async (e.g. Task.Run().Result) to do it. The disadvantage is that the DI pipeline and middlewear have not been instantiated yet. 
Maybe for streaming media. For streaming discrete values, TCP makes more sense.
Amazing job. Thanks for sharing.
His main argument for "why async" seems to come down to this: myDbContext.Database.MigrateAsync() .GetAwaiter() // Yuk! .GetResult(); // Yuk! And he's not completely wrong... unless you have very specific issues that make you want to avoid the async state machine overhead though, its usually better to do async top to bottom if you can, so it seems to mostly come down to "because Im calling something async that doesn't have a sync equivalent and every sync-over-async pattern is an anti-pattern". He also links to a David Fowler guideline that basically lays that opinion out, and even calls the fairly ubiquitous .GetAwaiter().GetResult() out as bad... though that tends to be the BEST way to do sync-over-async nowadays.
Yes, the compiler does type inference (C# does too when you use var). It does it once and does it very quickly. Your article is from 2011, it's now 2019. JavaScript used to be interpreted (so was C#) now neither of them are. JavaScript used to compile to horrible code, now it doesn't. Shit changes you buffoon. 
Good feedback, thanks!
Randomly looking through. https://github.com/nicoriff/ORMi/blob/master/ORMi/ORMi.nuspec shows you're packing your debug build artifacts. Intentional?