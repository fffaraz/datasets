You can run MVC5 on Mono using nginx. I wrote a tutorial on it [here](https://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html).
QASource is organizing a webinar which says ‘The Future is API Testing - Trends and How to Propel Your Testing’. The webinar is on April 18, at 10:00 am PST. Here are some reasons to register for this webinar. • This is an encore presentation. So, it is your final chance to know about latest trends, best practices, and ways to leverage API testing • The webinar will feature a FREE takeaway to track and monitor your progress. • Speaker at the webinar is Rajeev Rai, CEO at QASource Register at https://info.qasource.com/signup/the-future-is-api-testing today. 
10:00 am PDT happens when this comment is 4 hours and 4 minutes old. You can find the live countdown here: https://countle.com/pq8p176188 --- I'm a bot, if you want to send feedback, please comment below or send a PM.
sorry, as i said i'm a long time r# user. it seems than know VS added many features... 
Funny thing, as I tried to bookmark your blog, it was already in my bookmarks :) I know I can, I've created an install script, that does everything for me automatically. You can check it out here - https://github.com/polterguy/phosphorusfive/releases (Read the parts about installing binaries on Linux, and feel free to check out the _"install.sh"_ script) However, it still pulls down _"half the internet"_, due to all the dependencies in Mono, and its sheer number of packages. I would love a smaller distro, and have it more more _"modularised"_, which DeIcaza confirmed on Twitter BTW actually arguably exists (but only for Debian).
The cursor is on an assignment, which is in a for loop, which is in an if statement. Press ctrl+w, assignment is selected, press again, for loop is selected, press again and the if statement is selected. Something like this. Fun to use.
I think VS2017 is just slow in general as well. I've tried fresh installs of VS2017 and I still notice how laggy it is when using vscode whenever possible.
buzzwords that win interviews
This guy LINQs
This. This. This. I installed. I installed it once, many years ago. The delay for intellisense was completely unacceptable. 
SignalR generates some JS on the go and godaddy is having some issue with that. 
How can it be better than VS? Please elaborate. 
This is a great example of a query that is a great candidate for LINQ. Do your error checking up front and simplify your query. int numberOfAdults = 1; int numberOfChildren = 1; DateTime checkInDate = DateTime.Now; DateTime checkOutDate = DateTime.Now.AddDays(3); string errorMessage = null; if(numberOfAdults == 0) errorMessage = "At least one adult must rent the room."; else if (checkInDate &gt; checkOutDate) errorMessage = "Check in date must be earlier than check out date" if(! String.IsNullOrEmpty(errorMessage)) return; IEnumerable&lt;Room&gt; rooms = db.Rooms.Where(room =&gt; room.AdultsCapacity &gt;= numberOfAdults &amp;&amp; room.ChildrenCapacity &gt;= numberOfChildren &amp;&amp; ! room.Bookings.Any(booking =&gt; ! booking.IsCancelled &amp;&amp; (booking.CheckInDate &gt;= checkInDate &amp;&amp; booking.CheckInDate &lt;= checkOutDate))) .Orderby(x =&gt; x.Price); 
Thanks, maybe a idea to do the checking on the model or can I not check the second one on the model with attributes or a custom validation like this page : https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation?view=aspnetcore-2.1
I use the nuget package on my web server.
Put new line after each ;
Hi, I've uploaded progress so far to GitHub. I've got as far as having a shell app, with modularity, regions, and event aggregation running. It's very basic, and not exactly my finest ever coding... Excuses out of the way.. see here: https://github.com/AndyWatt83/prism-samples I'll try and flesh it out a bit over the weekend. Working on a medium post to accompany the code, but that's basically documentation, so expect that much later...
Which LINQ? EF isn't the only ORM with LINQ support and from what I hear, some of the others actually do things much better.
&gt; EF isn't the only ORM with LINQ support Would love to hear of any others. Do you have LINQs (pun intended)? 
NHibernate of course. LINQ to DB sounds really interesting. https://linq2db.github.io/ LLBLGen Pro used to have one, but I think it was deprecated.
Oh. NHibernate. LINQ to DB does look interesting. I wish I had time to try it on something that would test it's capabilities. Do you have any experience with it?
This was a point which surprised my actually. I thought the main interest of SignalR was automatic reconnection if connection is lost, and fallbacks. If I understand, they dropped both feature... so I end up using plain websocket which are not so bad. I fail to understand what signalR brings on the table.
I prefer ToArray() instead of ToList() and return IEnumerable from your function
Why?
Nope, but it is on my list to research. I actually want to integrate it into my own ORM.
Is SQLConn recreated or re-used for each request? If it is re-used, then there is the potential two clients are using the same connection object which may be causing the error. Not familiar with the language (VB ?) but I'd assume you need to create a new connection object every time a request is made. An alternative could be code based locks for any data access but this could be fiddly depending on how various read/write calls are intertwined in the application.
Where is this 'SQLConn' variable coming from? Because by the looks of your code, it seems like the connection is some kind of global static object, that's used across users. An SQL connection object should be scoped per request, and once you're done with it, be disposed (back into the pool) - Re-using connection objects across users will cause errors like that
you need to enable connection pooling or something along those lines. 
Try making a separate SQLConn for each request? I'm not a VB guru, but it looks like each call to getDataSet will re use SQLConn The DB (sql server 2008) should be ok for multiple connection 
SQLConn is coming from a ModFunctions.vb file in the solution and its created as: Public SQLConn As New SqlConnection
&gt; SQLConn is coming from a ModFunctions.vb file in the solution and its created as: &gt; &gt; Public SQLConn As New SqlConnection It's reused, SQLConn is coming from a ModFunctions.vb file in the solution and its created as: Public SQLConn As New SqlConnection I'm guessing what I am going to have to do is go back throw each call to SQL and rewrite it the right way, using *using* blocks and parameter based queries. Which is fine. 
it sounds like you need a custom route. are you using "classic" MVC or MVC Core?
Classic 
I updated some more information in the body of the post.
I generally keep all my DB stuff in a module called "DataEngine.vb" and only pass the SQL statement and connection string along when I call functions in that module. You could probably create your connection object at a class level in a module instead of trying to do it elsewhere, too. Here's an example of code I use in VB to grab some stuff from my MySQL DB: &amp;nbsp; Public Function GetDataSet(conStr As String, SQL As String, Optional ByRef errMsg As String = Nothing) As DataSet Dim objConn = New MySqlConnection(conStr) Try Dim objDataset As New DataSet Dim objDataAdapter As New MySqlDataAdapter() objConn.Open() objDataAdapter.SelectCommand = New MySqlCommand(SQL, objConn) objDataAdapter.Fill(objDataset) If objDataset Is Nothing OrElse objDataset.Tables(0).Rows.Count = 0 Then Return Nothing Else Return objDataset End If Catch ex As Exception sLastErrMsg = ex.InnerException.Message Return Nothing Finally objConn.Close() objConn.Dispose() End Try End Function 
I honestly can't imagine myself ever using this... hrm
Most of those are in free extensions or built into vs2017 these days
Something people don't mention about ReSharper is just how vastly superior its unit test runner is to Visual Studio. Does anyone know of an extension or plugin that can handle tests just as well?
Static strings for api uris? Role names? Static methods for functional operations? Extension methods on claims principals? I can think of a ton of examples for legit use of static in web applications... In both the front end (TS) and backend (c#). You just have to know what static means and when and where it's best used. "Never use statics, full stop" I think is bad advice. Just know your language and what you're doing, you can use statics. 
&gt; Static strings for api uris? Role names? Those things shouldn't be static strings, those should be constants. As for the other examples, static methods are generally still a bad idea. You cannot substitute static classes for other implementations, nor assign interfaces to them, and use them in a collection Even the examples that /u/Matosawitko is giving aren't good. &gt; except for items that explicitly need to be shared among multiple users. I'd say in that case your classes shouldn't be static. There should be a DI framework in place where you'd register that classes as singletons. It's not the responsibility of a class to be aware of it's lifecycle state (such as singleton, transient, per-request etc) - nor should they be in charge of it. usually a DI framework is used as a lifecycle management framework instead
Could you explain why? (Not a dig, literally want to learn)
&gt; you want the connection object to be per-query Generally you would want this to be per transaction- so one reusable connection object per web request. The exception to this would be if you are doing queries in parallel. 
Static strings become const due to constants folding. Doesn't really matter after compiling. Static methods make a lot of sense for extension methods. I have extension methods on claims principals that let me pass in other services like a user service or order service, and the services are injected. Being scared of statics is just a sign of a developer that doesn't understand their language.
This is true with SQL server, true, but with self contained databases, like SQLite or sql compact, closing and opening the connection gets VERY expensive, and it's better to just leave it open for he most part. This makes async a PITA though
Efficiency, putting the complexity where the work is being done.
That's a blanket statement that isn't always the case. Particularly when significant optimization is required by the SQL execution planner I don't really see stored procedures as being more of a deployment and maintenance barrier than the fact that there's a whole database schema there in the first place, probably with triggers, views, functions, and the whole panoply of relational objects. Even without stored procedures, you have the full problem.
I'm warning against static instance variables. Obviously the static keyword has other uses, but my warning stands: anyone who uses static instance variables in a web app, particularly if they don't understand the implications, is probably in for a bad time. This can even extend to DI dependencies, since the DI container controls their lifetime rather than being right in the code. One of my colleagues brought down our entire app a couple months ago by wiring up a DI dependency as Single Instance.
Putting an event in your service layer like I am showing below is arguably a bad idea. Ideally you will want this event to live somewhere up in your presentation layer. Hopefully this gives you a starting point and you can refactor it correctly. public Class JobInfoRepo { public event EventHandler TellEveryone; public IEnumerable&lt;JobInfo&gt; GetData() { var someData = null; using(MyDBContext db = new MyDBContext()) { someData = db.Table.Where(x =&gt; ...); } // raise the event if(someData != null) TellEveryone?.Invoke(this, new EventArgs()) } } In the class that consumes JobInfoRepo: public class SomeClass { public SomeClass() { JobInfoRepo repo = new jobInfoRepo(); repo.TellEveryone += TellSignalR; // be sure you unwire this or you get a memory leak } private void TellSignalR(object sender, EventArgs e) { // call signal R here } }
if you want to get technical, static string literals. as in `public static string AdminRole = "Administrator";` static. string. which, when compiled, isn't much different than `public const string AdminRole = "Administrator";` Point being, you can use static for things in web projects. Maybe you query the ebay api a lot... ``` public const string ebayApiVersion = "12.34"; public static string eBayApiUri = $"https://api.ebay.com/v{ebayApiVersion}"; ``` for example 
Wow. I have no words. That is absolutely *not* a constant. If anything changes that instance, it will change for everything in the app domain. You could make it read-only as well, which would lock it down after the constructor.
This is the first time you specifically mentioned `static instance variables`, but now that you have clarified, I agree. As for singletons... depends on context. I use singletons for functional things... even microsoft's own docs register `IAuthorizationHandlers` as singleton... (see here: https://docs.microsoft.com/en-us/aspnet/core/security/authorization/resourcebased?view=aspnetcore-2.1&amp;tabs=aspnetcore2x ) It's just that you gotta know when to do things right. 
Yes, I forgot the readonly keyword on the static string example because it's pseudo code. Anyways I would normally use const, but if you use a `static readonly string` after compile time it's a constant anyways due to constant folding. Look up how the compiler optimizes string literals. Also, good PR on the readonly bit, but look at the other pseudo code examples I offered. ebay api version won't change, it's const. Then I wrap it around a `static string` property, with no setter. That's because that URI won't ever need to change in the lifetime of the application. Therefore legit use of static. 
That's because it's a get-only property. (Private setter)
Pick a back end language, c#, java, php, whatever. JavaScript and a framework like jQuery. CSS and html. SQL/no-sql. Git/tfs/svn. Then maybe node.js . That's a great start. Then pick an application like search, ml/ai, app dev and expect to crash and burn at least once and you'll be just fine.
In my opinion long running tasks soaking up the threadpool, or wherever the task scheduler will resume async operations, seems like the issue here. What is the point of preserving such a supposed precious resource, your database connections, if your application is too busy to handle multiple requests anyway. Or are you worried about many applications and the database servicing all those open connections? I think the problem is the long running tasks moreso than the slight added latency and duration of connection. For example in ASP afaik you would nver want to saturate the threadpool with a bunch of long running tasks. I think that is the desgn issue, not async database IO. I am no expert on this stuff though.
I have been coding .net / SQL Server since 2000. I have had to fix many programs that had performance issues. The most frequent cause was queries that were not indexed. The second most common bottleneck was too many unintended database calls. I would not use async until you have a problem that async code might solve! 
We (Drawboard) are using it for Bullclip. &gt; How was your experience with the framework? * Experience was good - community is super helpful and full of smart people who are keen to help. &gt; Will you use it again? why / why not * Will use again 9\10. Why - it helps you focus on your product features instead of battling with distributed system complexities. It comes with some complexities and require some exploration and building the right framework mindset. We were relatively early adopters so as with any bleeding edge - someone eyes have to bleed :/ Also the speed of framework development is great and you need to put some efforts to keep up with their pace. 1.x upgrades were sometimes with breaking changes (compile time detection). They changed a lot in 2.0 and hope to see nicer smooth process. &gt; How was the bug finding in production with Orleans? * Same as with any .net code. Sometimes you need to dig a bit deeper to understand some obvious bit that you've missed in your code and Orleans chucks the wobble, but most of the time it's a problem in your code, not Orleans. Right mindset helps on this journey. &gt; What was the best parts of using it ? Don't need to think a lot about concurrency of every bit of your code and what's happens when your cluster scales out or in. &gt; What was the worst parts? This is my individual opinion on overall distributed OOP approach - it'd be much better if Orleans can explicitly surface what's going under the hood to developers - this helps me to learn and understand what I'm doing wrong. I'm not a fan of "magically works". Making messaging explicit, even if it is in a form of method calls. &gt; If you used it again what would you do differently? We are refactoring towards what we think our system should look like, so we will be there eventually. My experience is mostly for versions 0.9-&gt;1.4, not very relevant for 2.0. What I will do differently: 1. Be careful around storage providers model - understand it's strong and weak sides. Fast start but have a plan to move away from it, try to implement that plan on one or two grain types. 2. A little bit more messaging in queues and in external services. 3.Make your app less chatty - grain chattiness is a known anti-pattern and you need to keep it in mind to avoid it. This chattiness may be not based on your own code (this part is easy) but on user's data - so put limits and test on your 4xMax limits .... 4. I will try to build state machine in each grain so it would be easier to validate and filter out any incoming attempts for unexpected transitioning. (e.g. someone tries to delete non-existent grain or create already existing grain and so on). Hope this helps.
I have to disagree. From an initial effort standpoint, writing async from the beginning is no harder than writing sync code from the beginning. Later on, however, it is much harder to sync code and make async than it is the other way around. Furthermore, if using async is a problem, it is probably localized to one function. (At least that's been my experience.) If the problem is using sync (e.g. too much thread blocking), chances are it can only be fixed by changing most of your code to use async. Like allocating too much memory, you can't spot fix it.
Why does a regular database call need to be async though? You are adding complexity for no noticeable benefit. For most cases there is no noticeable difference. How can you say it is no harder? In C# async is more difficult and can lead to unintended bugs. Yeah, it is not much more work to add async in front of every method all the way from your controller. Yet one day I found out asp.net Session doesn’t always work in async method and random bugs appeared... Async is good for http requests and other operations that could take longer than expected. Index your queries and make sure they are fast. No need to use async unless you need to.
First of all, if you are doing anything involving a UI thread then you must using some sort of asynchronous call to avoid freezing the UI. That doesn't have to be async/await, but if not that then you need to use the more complex Task, background thread, or background worker patterns. If we're talking web programming, then the question becomes "how many backend connections are you allowed?". A simple website that only talks to a database won't benefit much because the default connection pool is 100, which isn't really a lot of threads. A complex website that talks to multiple databases and backend servers will probably see a benefit because you can starve yourself for threads before you run out of available connections. *** As for "random bugs", I have seen a grand total of 3. 1. In one case, pulling multi-megabyte records from the database was much faster using sync than async. Never found out why. 2. Using a ReaderWriterLock instead of an AsyncReaderWriterLock. 3. UI bugs from allowing a button to be pressed a second time when the first operation was still in progress. Not really an async/await thing though, as it would also happen using a background worker or other method for not blocking the UI. 4. Someone calling Task.Wait/Task.Result after I explicitly told them to never do that. That's not too bad considering that I've been using async/await for several years on a daily basis.
ToArray is faster than ToList, at least for enumerating the results afterwards. (Might be faster for creating it as well, but I'm not certain.) Returning IEnumerable is a great way to piss off the people using your code. They have no idea what the real object type is and whether or not it is safe to enumerate the result twice or if they need to first copy it into a list. Furthermore, calling `foreach` on an `IEnumerable&lt;T&gt;` is going to be much slower than calling `foreach` on an array or `List&lt;T&gt;`. (I wrote an article explaining why here: https://www.infoq.com/articles/For-Each-Performance)
Naw, I would use a Table Valued Function (TVF) instead. It gives you better code reuse possibilities.
Anything involving I/O should be async if possible. Your regular database call should be async because it's doing I/O, and if you make a synchronous call then you're blocking a thread pool thread from doing something useful while the OS waits for the database call to return. You're adding a small amount of complexity for greater throughput. It's horses for courses (as always). If your app is only going to be used by 10 people concurrently then you'll probably get away with using synchronous calls; if you're talking about 10,000 concurrent users then your performance is going to go downhill very fast.
Headless chrome
I tried that and this is what I got: https://imgur.com/a/fN1bb Do you have a CallbackPath in your example. I tried to remove it and I got an error message.
good bot!
Thank you, brettdavis4, for voting on imguralbumbot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
&gt; I tried to remove it [CallbackPath] and I got an error message. There must always be a callback path, it's an important part the of sign-in flow (see OAuth 2.0 Implicit and Hybrid flow) because it tells the Identity Provider (IDP; AzureAD in this case) where to redirect the user back to the application after authentication has succeeded. An application can use different callback paths depending on it's current state, so a callback path is not necessarily part of the configuration. You must also make sure that the callback path you're using is listed in the App's registration within Azure. This is for security reasons. However, your application isn't even able to start the sign-in flow yet. It tries to download the OpenID Connect Discovery document, which contains information about the IDP, for example where the user has to be to be redirected to in order to start the sign-in flow. (`authorization_endpoint`) If you try to open the url from your error message (https://login.microsoftonline.com/09fb04d0-7965-496b-a96e-d6df308dbec6/.well-known/openid-configuration) in your browser, it says: `AADSTS90002: Tenant 09fb04d0-7965-496b-a96e-d6df308dbec6 not found.` I see three possible reasons for this error: 1. I made a typo while writing off the url from the screenshot. 2. You switched the tenant id with your app id in the appsettings.json file. 3. IT actually gave you a wrong tenant id.
If you enter `https://login.microsoftonline.com/{tenent-id}/.well-known/openid-configuration` in your browser, you're supposed to get an json object like the one at https://login.microsoftonline.com/common/.well-known/openid-configuration Probably the easiest way to check the tenant id.
&gt; Naw, I would use an inline Table Valued Function (TVF) instead. It gives you better code reuse possibilities. &gt; I only used stored procs when I need to update values or mess around with temp tables. Also a valid approach
&gt; I hope you take this message constructively, I may seem intense but I don't mean any offense. Yes, of course. I also didn't meant there were *never* any reasons to use statics. The things you mentioned; functional things, mechanical operations, math. - can totally be static. A place where I use a lot of static (extention) methods are unittests. Those are mostly functional, and hardly ever require logic substitution. However, your examples are not really any of those cases, that's just business logic [HttpGet] public IActionResult PostOrder([FromBody] Order order) { var user = User.GetUserFromClaims(_userService); order.Purchaser = user; _orderService.PostService(order); } You're injecting the `_userService` (I assume), and then using a `GetUserFromClaims` from a static helper. This `GetUserFromClaims`sounds like it could be part of the `userservice` but if that didn't make sense, you also just could use injected a "UserClaimsProvider" which would look like this: public class UserClaimsProvider { public UserClaimsProvider(IPrincipal user, ITypedBusinessService&lt;User&gt; userService) { // set vars } public User GetUserFromClaims() { // logic } } Because in your example, it's the responsibility of the controller to call your extention method with the `Controller.User` and a service that you injected. Why not skip the middleman, and put that into a seperate class that you inject, instead of injecting the `_userService` And since I'm nitpicking already anyway, your other method should just be something like this: [HttpDelete] [Authorize(Roles = "SuperAdmin")] public IActionResult DeleteOrder(int orderId) { _orderService.Delete(id); return Ok(); } If the default authorization middleware doesn't cover your needs, role out your own middleware to check it. Methods in controllers shouldn't be in charge of authorization validation.
asp.net core web api, bootstrap, Sql and react or Vue. Maybe identityserver
I’m in an environment where there will never be 10,000 concurrent users.
Help me out then. Since I work with a code base where most of it was written before task based a sync was an option in C#, how do I make a dB method async if the method calling it isn’t ?
Fair enough. Like I say, it's always about picking the right tools for the job. Async all the way is clearly the path that is being pushed by Microsoft for new projects, but I appreciate that it can be more challenging when you're mixing and matching it with older technologies.
Please check if this is helpful: https://www.reddit.com/r/dotnet/comments/8bqx6v/comment/dx8x6mw?st=JFZHPUKZ&amp;sh=6bf5dcdb
The real point here is not the difficulty in writing async vs sync code. It's important to understand that some operations may just be better left synchronous. It may even be counter productive to use async especially when iterating. I've tested this with file access recently and if you have a processing task that waits for every iteration before writing, the latency introduced by awaiting for every block can draw out the total operation by magnitudes! So that got me thinking about data access. Doesn't the same thing apply? Why even introduce .ReadAsync() to ADO classes if it has such a high chance of being problematic.? Or maybe a better questions is, who uses .ReadAsync() and does so for more than just "async all the way down"?
Not really sure why I got downvoted so heavily. This is a pretty well accepted practice. While negligible, a Generic List has extra intent and scope (memory). Array's carry much less weight and the "immutable" intent is conveyed. Furthermore the IEnumerable&lt;&gt; collection provides behavior suitable for most purposes when using collections in a fraction of the size (memory). I try to only use List&lt;&gt; when I need to manage the array, not if I'm querying a database. 
Appreciate the testimonies. Definitely No.1 is interesting because it points to the same problem where let's say async was pulling your file in 'chunks' asynchronously and between each call another task is inserted from somewhere else and does even the smallest amount of work. It will then extend the total time that data access takes. So I'm really thinking async is bad for these types of 'connection' based operations that can have a limit on total active queries. Non-db operations (tasks) can be affecting these queries. Best to synchronously pull your data and get out, letting the other tasks get in line. And I'm wondering if there's any real downside to synchronous. Is the thread really getting blocked, or yielding to other threads while waiting for data? 
Not an option, you've got to rewrite the whole chain. That's why I default to async first.
General async rule No. 1: Never call 'async' code from a synchronous method. That doesn't mean you can't run tasks, just avoid calling async methods from synchronous ones. Stephen Cleary covers this in detail. General async rule No.2: "Async all the way." Which really means, try to avoid any calls that might 'block'. Using async methods all the way through an operation. Calling synchronous methods from async methods is obviously totally fine, but if those methods have long wait times, it's probably better to use the async versions. That said, there's no magic here. It's just really hard to understand what's really going on inside an asynchronous/multi-threaded application. And although, going async provides much better results in overall in many situations, it can produce some curious unintended behavior.
[This](https://msdn.microsoft.com/en-us/library/hh191443(v=vs.120).aspx) is a good document for understanding what happens when you call an async method. 
That's an interesting read, thanks! I guess it comes down to what trade-offs you're willing to accept. If you are running an ASP.NET app (Core or regular) and you are making blocking calls somewhere in your request pipeline, then you obviously run a greater risk of running out of thread pool threads under heavier load. I accept what you're saying about performance of asynchronous writes compared to synchronous (will check it out myself when I get a chance); if your application was constantly writing to files then bad on what you're saying maybe you would want to use a synchronous approach, but at face value this seems like it might be a special case?
Thank you!! This is what I wanted to see.
Thank you!! I’ll try this out!
I notice that it isn't on by default, are there any risks to enabling HTTP/2? 
I worry that it's not a special case and that best practices for certain operations should remain synchronous. With so much literature saying "async all the way" but then.. *Oops? What's happening? Why are things so slow?* There are ways of visually modeling what can happen (and does happen) and personally I'm fascinated on how to solve the problem. Pipelines are tricky because: Let's say that queuing items in a pipeline only takes 2 milliseconds. But it takes the processing task 3 milliseconds to dequeue and process an item. That means queues can look something like this: Incoming tasks: ```[++][++][++][++][++][++][++][++][++][++][++]``` Processing queue: ```[----][----][----][----][----][----][----]``` What may actually end up happening inside the task scheduler: ```[----][++][++][++][++][++][++][++][++][++][++][++][----][----][----][----][----][----]``` This is obviously problematic because for things to run smoothly you want the processing queue to have priority (like this): ```[++][----][++][++][----][++][++][----][++][++][----][++][++][----][++][++][----][----]``` When the incoming tasks dominate the scheduler you can have code that runs in 'spurts' and 'pauses' instead of a smooth flow.
i always feel dirty when calling an async method from sync code. Sometimes it can't be helped.
It actually can be a very bad (high risk) thing to do. Beware deadlocks. Best answer: https://stackoverflow.com/questions/9343594/how-to-call-asynchronous-method-from-synchronous-method-in-c/9343733
Yes, your code might run in fits and spurts in your example, but it runs, whereas if you were blocking threads then you might find that e.g. incoming HTTP requests to your web app were being rejected because there were no threads available to service them. In ASP.NET Core, every request, including requests for static content, are processed by the same thread pool (this is different to e.g. static content in regular ASP.NET on IIS). As discussed (and demonstrated) by Damian Edwards and David Fowler in this video, if you block in your request pipeline, your requests will fail under high load because of thread starvation [@42:30]: https://youtu.be/RYI0DHoIVaA Another point that the video raises [@38:35] is, while the thread pool is able to spin up new threads as required (up to its limit), doing so is slow. As soon as you block, you increase the possibility that a new thread must be spun up, which is also going to affect your performance, just not in the place that you're focused on. In the scenario that you specify, you maybe would be better off just making a synchronous call here, but I think that the single-digit millisecond times that you talk about make it a special case. There will always be exceptions to the rule, but if you're making e.g. a database query over a network, or hitting an HTTP API from your back-end, using asynchronous calls is almost certainly going to be the correct approach the majority of the time, especially when you're writing an ASP.NET or ASP.NET Core application. At the very least, I personal think that asynchronous should be the preferred approach until you run into the sort of scenario that you describe, at which point you as a developer can decide whether to block or not, based on how it might affect the rest of your application.
yea it's all about the synchronization context. i'm not doing UI stuff, so there's no synchronization context to speak of (except the thread pool). afaik the traditional synchronization context of the old asp.net doesn't exist in asp.net core either, so this sort of pitfall is harder to encounter.
You will need to set up oauth with either azure ad or with your local ad. https://docs.microsoft.com/en-us/azure/active-directory/develop/active-directory-protocols-oauth-code
You have significant blocking in the code; i.e its not fully async
Looks pretty easy. Just figure out what the M3U file format looks like: https://en.wikipedia.org/wiki/M3U
**M3U** M3U (MP3 URL or Moving Picture Experts Group Audio Layer 3 Uniform Resource Locator in full) is a computer file format for a multimedia playlist. One common use of the M3U file format is creating a single-entry playlist file pointing to a stream on the Internet. The created file provides easy access to that stream and is often used in downloads from a website, for emailing, and for listening to Internet radio. Although originally designed for audio files, such as MP3, it is commonly used to point media players to audio and video sources, including online sources. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Thanks, but I figured it out. Forgot to edit though, used the newMedia(string) on the array and appended those.
* HTML 5 (semantic markup) * CSS (**Responsive Design** (media queries), **Grid/Flex**, transitions/animations and some other fancy things if you're into "view" stuff) * JavaScript (**ES6 version**, knowing older JS is good, but don't bother too much (Babel will do its job), **DOM** and **BOM** manipuliation, try the [AirBnB style guide](https://github.com/airbnb/javascript) * Setting up your development environment for client-side projects (creating a **NPM** project config and, setting up a bundler/task runner like **Webpack**, **Webpack dev server**, if you want, you can try CSS preprocessor like SASS (use SCSS style), but it's optional) By now, you should know some good front-end dev skills. * HTTP basics (how does the WEB work, GET/POST/... methods, how do requests/responses look like (try browser dev tools or some RESTful clients like **Insomnia**) * What is a **MVC**, **WEB API (RESTful services)**? * Databases (**SQL**, **Relational** (RDB), **Non relational** (NoSql), what is an **ORM**?) * Try creating an **ASP.NET Core MVC** project first, a simple todolist application, just to Create, Update and Delete (**CRUD**) simple info from a database (you could use the MS SQL server or any other (like MongoDB) with or without an ORM. * Try creating a simple **Cookie based Authentication/Authorization** (would recommend without **Identity**) * Learn more about ASP.NET Cores services, middleware, DI (iow, basics of ASP.NET Core) * Try creating an **ASP.NET Core Web API** (here's a catch, you will use some other "view tech" unlike **Razor syntax** in MVC, which is going to be decoupled from back-end. Usually it's either Angular or React that is used as a client for your Web API. But instead of choosing the ASP.NET Core **Angular/React** templates, try creating a diff project for client side, your Web API will be your server side). A simple Web API project, that has almost the same Controllers and Models from the MVC project for CRUD actions. * Create a client side project, that will use the Web API. By now, you should have run into a problem, which is **CORS**, you need to learn more about it, how to enable on client side (usually JS **Fetch API** options) and on server side. * **Token auth** (**Bearer** (**JWT**)), what is XSS, CSRF, how to store tokens safely... * **Web servers**, **proxies/load-balancers**, deployment (cloud options (**AWS**/**Azure**, **Docker**) This also seems a bit messy, but I think by learning these things you will stumble on other things, that I haven't mentioned and you will have a decent "web development environment map" in your head, for what you should learn next. 
Dang, this is great info, especially the `"AccessToken"` and `"RefreshToken"` logic. I think i'm going to try implementing that in my own app.
Can you give some plugins you use with vs code? For example, what do you use to import namespaces when you use a class? I don't want to have to type using statements all the time.
Yeah, the answer provided on stack overflow really gives good guidance. 50ms is a good guideline. Longer that 50ms go async. And if you're doing quick iterative code, then stay synchronous with the outer portion as async.
There's good guidance there tho. Adding ```.ConfigureAwait(false)``` when possible can save you a lot of grief in these situations.
Thank you so much for this; i'm currently in the middle of implementing everything you said! 
It still blocking... The `using` statements will call `Dispose` which for the `Stream` and `StreamWriter` will call the blocking `Flush`, so you'd want to `FlushAsync` prior to the end of the using. The using on `AsyncFileWriter` will call `Dispose` and call the blocking `Complete().Wait()` `fs.Write` is blocking. `s =&gt; writer.AddAsync(s).Wait()` is blocking. 
This should be stickied to everywhere talking about aspnet core
&gt; Mainly because the `Parallel.For` is calling it in a synchronous manner. Without waiting, tasks/threads will queue up and not obey the bounded capacity and the test would be invalid. In a real world scenario you would await this. For testing, we 'block' the parallelized thread to simulate reaching the bounded capacity. Which results in blocking on the ThreadPool and you become bounded by the ThreadPool injection time due to the blocked threads. I added a `ParallelAsync.ForAsync` to get around this to the Parallel call is async rather than blocking. &gt; Going fully async (including the .WriteAsync()) provides +20x worse completion times. The PR should show now slow down from full async? 
Asp.net is a full framework stack. It won't even come close to running on .NET Core. There may be successful usages of it out there on Mono, but even that would surprise me a little - I wouldn't imagine something like that would be suitably stable for production.
That feature you are talking about should be available with just the [C# extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode.csharp) by using Ctrl + . (or Cmd + . on Mac). There are a few 3rd party extensions I've found that help a lot: * [C# Extensions](https://marketplace.visualstudio.com/items?itemName=jchannon.csharpextensions) * [C# XML Documentation](https://marketplace.visualstudio.com/items?itemName=k--kato.docomment) * [MSBuild Project Tools](https://marketplace.visualstudio.com/items?itemName=tintoy.msbuild-project-tools) * [NuGet PackageManager](https://marketplace.visualstudio.com/items?itemName=jmrog.vscode-nuget-package-manager) * [.NET Core Test Explorer](https://marketplace.visualstudio.com/items?itemName=formulahendry.dotnet-test-explorer)
Yeah, I merged your pull request, but lost the reference to ParallelAsync. I've left AsyncTester.cs open for editing. Go bananas. I'd love to see a version that actually demonstrates async working better. I've been prepping the master branch for your edit and implemented the options in AsyncFileWriter to allow for either configuration. Thanks a lot for the other edits too. You'll see I implemented your DisposeAsync patten but also allowed for IDisposable. ;)
You need to port to aspnet core. It's not too hard, don't worry. It's a good opportunity to refactor your architecture and introduce DI and service-oriented patterns.
Touch base with me before making more edits since I'm trying to implement your ParallelAsync code now.
I wrote a simple article about this (including source code) http://piotrgankiewicz.com/2017/12/07/jwt-refresh-tokens-and-net-core/
Check asphostportal.com. They support SignalR on their shared hosting plan. 
It's not too hard, because...? If you have a fairly big application with lots of libraries and a couple of years of development in it you're not going to port overnight.
Yes. I was running ASP.NET on Linux pre Core. I wrote a tutorial on my blog [here](https://coderscoffeehouse.com/tech/2016/01/19/aspnet-linux-setup.html).
If you're going down the OAuth2 route then Identity Server or Auth0 do seem to be the popular options. I spent some of last week setting up Identity Server 4 for a use case very similar to yours, primarily using this quickstart... http://docs.identityserver.io/en/release/quickstarts/1_client_credentials.html That solution uses a simple shared "secret" between the clients (your three web apps) and your web api. You can have multiple clients, multiple apis etc. I found it helped to get the simplest possible solution working first as there are a few moving parts and it can be slightly confusing the first time you set it all up :-)
Did some more digging on what to do in the route config. I found the option to use a catch all route, but that doesn't really get the results that I am looking for. It will pass the url to the controller that I've specified, but if there isn't a redirect. I need it to check the rest of the routes after it has checked the redirect.
&gt; perl ( ͡° ͜ʖ ͡°)
I had it saved in my bookmarks! Very nice article as I’m looking forward to implementing the same thing as OP
The first step is to get everything .NET Standard compliant. Then, literally copy the code over.
What's wrong with Python? I find Django a lot easier (and more productive) to use than ASP.NET.
It's passed as a header value or in the URL right? I think the standard is a header value.
Ha! Nothing! It was more a reference to my favorite language, perl... Love it but will cop to the fact that the 1 liners can be confusing as hell
It works for my medium-small MVC project. Its a have a multi-tenant data driven website. Each tenant has their own Db, topping out around 5Gb. We have to utilise DTOs, an in memory cache and a loose Repository Service pattern to keep things from crawling. Its not perfect, but it does do a lot for us!
We've used it pretty much exclusively in all of our projects. I'm not launching rockets to space, so I don't care about a couple of milliseconds here and there. I am not here to argue that EF is the best thing since sliced bread, but I very much like it. The only time we step out of EF is the query is simply too complicated, or requires something that EF 6.x doesn't support. For example the ability to pass table parameters to stored procs.
I'm reasonably happy with EF for database *versioning*, and it's pretty solid for first-pass rapid development. We have selectively replaced it for data access with a micro ORM when performance is an issue -- generally, any time an update requires joining more than let's say 4-5 tables, EF tends to generate gargantuan 2000+ line queries that don't run super fast.
I find EF extremely handy for CRUD-style applications. There are so many advantages to using it (or a similar ORM) that they easily outweigh the drawbacks in most scenarios: * Easy to make sure your data access code doesn't break, both on the code and the DB side. * No need to maintain hand-written queries. * Very quick to write and maintain most applications * Acceptable performance in most scenarios * Lots of knowledge about it available online * Very easy to get started with (this is also a curse) But you cannot mindlessly apply it and hope for the best. Or hope that it will be the only tool you will ever need. Luckily, a lot of EF's bad reputation comes from misuse. People who use it in a way that's not appropriate, without understanding what EF does, and then blame the tool and not themselves. All the developers that work on it need to have insight in how EF works: * how it handles eager and lazy loading of relations * how it handles change tracking * how you can use non-tracking queries to speed up performance * how you can use inheritance mapping for more complex data models * what typical performance issues are and how to solve them * etc. If you are aware of the workings you will be a lot less likely to use it in an inappropriate way. However, in a lot of the projects I used it on, there was a small portion of the data access code that needed another strategy (typically 5% or less). In all of these cases, we continued to use EF for the majority of our code, but added another strategy for those specific scenarios. Some I can remember: * Code that is called very often (thousands of times per second) suffered a latency issue because of EF overhead. We used raw ADO.NET for that one. * Code that yielded a very complex query that could be simplified a lot by writing the SQL by hand: we used Dapper there. * Code that for some reason couldn't convert the Linq query to SQL. The developer used a fetch (on a growing data set) and an in-memory filter that got slow over time. We converted that one to a stored procedure. Take the time to get acquainted with those tools and learn not only the 'hello world' scenarios. Learn the advantages and the drawbacks, so you know what to do when you run into something. And for Pete's sake, use a SQL profiler when developing. So you get a red flag when you've done something weird. Source: I teach '.Net data access' on a regular basis. I cover all of the above (and more) in a 2-day hands-on course.
This is our setup as well. Each customer has their own database, with an independent application database as it's home. EF was used where it made sense, but to get performance in some situations I wrote my own SQL and I some cases, server side stored procedures or functions were used. Use the tools you need when you need them. That's all there is to it, just document what you're doing and why!
Same here. Its very useful to just get stuff done. I feel like the problem a lot of people have is the expectation that it should be able to solve allllll of their problems. There are certain times when EF doesn't make sense, but that doesn't mean you should rip it out entirely. 
Can you explain what you mean by a loose repository service pattern? Specifically the loose part. 
I think this sums up my oppinion as well. Use it on the 95% that doesn't need tuning and focus on the 5%.
&gt; Code that is called very often (thousands of times per second) suffered a latency issue because of EF overhead. We used raw ADO.NET for that one. Wouldn't the change tracker cache solve this and simply return the cached entities instead of going back to the database?
\&gt; What drives me crazy is the amount of boilerplate code EF requires for even basic CRUD operations. \&gt; [https://www.infoq.com/articles/repository\-implementation\-strategies](https://www.infoq.com/articles/repository-implementation-strategies) Why do you compare Chain with implicit mapping vs EF without mapping? If you use Automapper \(or probably even copy paste the mapping code out of Chain\) then EF looks much more concise... \&gt; [https://www.infoq.com/articles/repository\-advanced](https://www.infoq.com/articles/repository-advanced) \&gt; It is especially bad when I start getting into things a repository should be able to do such as automatically set the CreatedBy/ModifiedBy columns for all tables. Well, again, you're comparing Chain with hardcoded features vs EF without such features. Why is it boilerplate when you have to extend someone else's library but not when you're doing it in your own? 
Those are excellent articles I've bookmarked them both. 
EF doesn't support HeirarchyID datatype in SQL Server. When inserting records EF tends to want to do multiple individual INSERTs rather than a single INSERT for whatever reason. This slows things down when you're inserting a lot of records. If for performance reasons I want to specify the actual sql query. Those are the only reasons I step away from EF. 
Yes, but 1. no usage of `.Include(...)` or returning a full entity unless there's a good reason for it and 2. the team and I review the queries it generates and make sure it works effectively with our DB. Most cases, all we need to do is add an index or two. The worst situation we've encountered was where we couldn't change the schema for a legacy DB and fed EF a custom query instead via `Dbcontext.Database.SqlQuery`
IMHO ORMs like EF makes your project unnecessarily more complex and drive your focus from business to technical difficulties. SQL is mandatory skill in the Enterprise and we can squeeze every performance bit from the Db engine. Like the others said, you only need a mapper to take repetitive boilerplate and Dapper is excellent at this.
We process 3milliom visits per day no problem with EF.
&gt; Why do you compare Chain with implicit mapping vs EF without mapping? Because that's how the respective libraries work out of the box. Chain has a fundamentally different mapping strategy than EF. It works using reflection against the database schema, rather than surrogate objects in the C# code like EF. The upside is that there is no difference between an "entity" and any other object that you want to populate from the database. The downside is that Chain expects you to do all of your joins in views. You can't pull back deep object graphs in one query. (Though many database admins will tell you those deep object graphs are a bad idea anyways, sometimes they do help.)
I saw some posts about improving speed. Anyone got a link to those? Because I know that this update won't improve performance.
Generally speaking people don't put a repository layer around EF. Instead they just use the DBContext as the repository. Other times they do have a repository, but it 'leaks' EF specific details. At the end of the day, the only way to know if you have "abstracted correctly" is by implementing two database providers at the same time. And most people aren't going to do that.
Please give me solution for ios 
EF made things way more complicated then they needed to be for the application. Switched to Dapper and haven't looked back. We've used dapper on several project since the initial conversion.
Most of my colleges work in outsourcing companies where the longest project is at most 2 years and I don't think you worry about maintenance. But for me I have to maintain a bunch of Legacy ERP softwares that existed for 20+ years where reporting sometimes becomes challenging, it almost impossible to have a descent report without stored procedures and understanding of the execution plan.
Are you looking for this article https://www.jetbrains.com/help/resharper/Speeding_Up_ReSharper.html? 
It's loose in that legacy code doesn't implement the pattern! Essentially as a team we decided that passing the dB context around was not best practice. Repositories jobs are to fetch / store data only. We have one repository per resource, the repos essentially exposes the iQueryable to our services. The services then apply business logic in reusable testable chunks. So something like get customers active between these dates. Our services also do any DTO work, as bringing back less data is specific to the data use case.
We used EF and now EF Core for all kinds of projects and we are very happy with it: - migrations are awesome.. there were some quirks in both core 1.1 and core 2.0, but 2.1 seem to fix everything we came upon - maintenance is easier.. we mostly don't have to worry about SQL exploding runtime after refactoring - you can quickly compose queries with linq and make reusable extensions (for instance `IQueryable&lt;Article&gt; Where_Published(this IQueryable&lt;Article&gt; query, DateTime date)`) - if you know what you're doing then inheritance can help you keep thing clean There are of course some bad things too: - no hierarchyid, spatial and other extra features Some tips to using EF: - pretend lazy load doesn't exist - pretend `Include` doesn't exist - `AsNoTracking` is your friend - projections (`.Select(x =&gt; new MyObj() { Id = x.Id })`) are also very helpful - use simple queries - better run three simple queries than one complex (at least on MS SQL) - if you are not satisfied with a query generated by EF nothing stops you from hand coding it (you can even use dapper for these, they are not exclusive)
Ah yes. I would say that EF is definitely not for reporting on anything and sql is king there. 
I would agree with all of those points. 
A C#7 sample is available on [Deck.NET](https://deck.net/csharp7).
Ha yea no kidding. The place I started working at recently uses a repository layer around EF (don't get me wrong, I've done it in the past too!) but it heavily relies on you knowing EF specific details and we end up having silly methods like GetXWithNoTracking. In this situation you lose the ability to swap out the layer unless the ORM you're replacing it with implements similar strategies for handling change tracking etc. We also have some complex query building which sits in the service layer and we (I) ended up using [PredicateBuilder](http://www.albahari.com/nutshell/predicatebuilder.aspx) to save leaking an IQueryable up from the repository, but it would have made much more sense to just use the DbContext directly in service.
 If you are aware of the workings you will be a lot less likely to use it in an inappropriate way The above is essentially the definition of a leaky abstraction, and sums up my issue with ORMs in general. The whole point of an abstraction is so that the client needn't know anything about the implementation. If you find yourself mixing in raw SQL or "tuning" linq to generate better queries, then there really isn't any abstraction happening anyway (will that raw SQL or linq perform the same, or at all, against a different data store?). I understand the benefits an ORM like EF can provide for basic CRUD (nobody wants to hand write the CUD portion for every Entity in a large system). But beyond a productivity tool, I never really here about the ways in which they shine, only stories like yours above where all sorts of other technology needs to be added to make up for the deficiencies.
I've been using it for the last few years on a large project. A lot of work went into DTOs and generic transformation code, and FTS support. All works well, performance is good. "Will recommend to a friend or family. :-)"
That's a weird question, it's not that difficult to be better than VS. But to answer it - performance, features, usability - it's all better in Rider.
Thank you, i never thought about this..but yeah compressing the stuff will save some time too.
Neat, installing now!
I never bothered to use EF. I've been using LLBLGen since forever. It's fast and it gets the job done. In general I love ORM. I just don't want to deal with the boring plumbing. I can optimize the hotpaths with some super duper SQL when it is necessary. Most things are just plain boring operations.
One thing that stands out from looking at everything you've tried so far is that there's quite a mix of different technologies/versions in there. MVC4, MVC5, EF, ASP.NET Core, Angular 2 etc. I think that's an issue with trying to pick up the Microsoft stack at this point, there are many many choices and it's easy to "fall between the cracks" and end up not being able to learn any of them. Maybe it would help to decide on one angle to pursue first. For example perhaps you could stick to ASP.NET Core MVC, forget older versions and front-end spas like Angular for now. That way you can cut out some of the noise that comes with all the competing frameworks/approaches. When it comes to then actually learning, I think it varies for different people but I've found some variation of this approach works best for me... * Think of an idea for a pet project (you've got that down already :-)) * Brainstorm all the features I can think of that I want to build (I think you've done this too!) * Pick the simplest feature to build first (least moving parts, as small as possible, really really really small) * Work out how to build that feature Now the job is to work out how to build one small feature, nothing more, nothing less. For this bit I find something along these lines works best... * Create a mockup of the feature (hand-drawn or using an online tool like Balsamiq mockups) * Create a ViewModel that maps directly to that mocked up UI * Create a controller action which populates and returns an instance of the ViewModel (using hardcoded data) * Create the view to match the mockup and bind it to the View Model I put together [a blog post](https://jonhilton.net/2018/01/11/8-practical-tips-for-learning-asp.net-core-mvc/) that covers these tips in a little more detail but this is really the best way I know to learn, by finding a way to actually start building real features. If you can get going this way I think you'll be surprised how many of the apparent complexities of adopting ASP.NET stay out of your way until you're ready to learn them... And, a real bonus is that as you build your features this way there will be times when you think "how do I do x?". Turns out those are the perfect questions to fire into Google because you'll get targeted specific advice on the narrow problem that you're trying to solve. It beats searching for "learn ASP.NET MVC Core" or "Dependency Injection" anyway :-) 
It is a known issue https://youtrack.jetbrains.com/issue/RSRP-468051. Will do our best to fix it in the future releases. Apologies for the inconvenience. 
No worries, and I think ReSharper is a great product. It's also not open source software, and I pay for it myself, so I don't feel bad complaining about the things it doesn't do so well. Please work on React support! Thanks
Unless we need the LINQ abstraction for something (i.e. OData) we ditched it. Largely, the promise of large ORMs remains undelivered on (abstraction of DB, etc.). I don't really feel like they are worth their weight anymore, and frankly I prefer to roll my own models, repos, uow (when needed) and queries. Dapper seems to hit the sweet spot for what we need, and EF / NHibernate just seem to get in the way instead of helping us. We spend as much time fighting them as we would just writing those pieces ourselves. Every team I've ever been on insists on writing their own Repository stuff anyway instead of using EF's directly, so we just never really saved anything with it. The only projects that it seemed to work ok on were small ones with very simple data models, at which point Dapper was easier to reason about and work with anyway, so we just struggled to find a spot where it really made sense at all.
That means Visual Studio will release a minor update in a few days that will break a ReSharper feature. Then ReSharper will release an update a week later to fix it, and repeat...
Can you elaborate more? If you don't pass a context around how can you have unit of work across multiple repositories in one request? This is in the context of a web api. What kinds of errors did you have?
Same here. Just always felt like I was jumping through extra hoops to do relatively simple things. Entity Framework seems to follow the latest trend of coming up with a really complicated way to make something simple. Dapper is just simple from the start. 
Yes, very however, EF implements the repository pattern... and every architect out there, the first thing they do is put a repository on top of EF. For actual fun, work with Query/Command classes. If used correctly (together with MS SQL) I can hardly fathom a scenario where you'd need to rip it out. The actual tuning you'd do on the SQL server and use EF for getting data in and out.
Try identity server 4.
Yes we started out with pure ADO .NET since we didn't want a hulking framework around a small Windows service accessing a single file database. It's like building castle walls and a moat around a cottage... Then soon enough found out that ADO .NET is bloody terrible in not interacting well with POCO's, strangely enough, since I think this should honestly be a standard part of a .NET database framework. It only makes sense to me. .NET is object oriented, a database framework should be better object oriented. Oh well, we moved to Dapper too because it was such a thin wrapper and easy to understand, and for being such a small abstraction on top of ADO .NET kind of just made it even more puzzling why something like it isn't part of ADO .NET anyway. We later moved to NHibernate, which in its modern incarnation gave us full CRUD support with LINQ syntax and all. I still think NHibernate can be made easier to get along with than EF. The only extra necessary step is really to specify if there are some one\-to\-many / many\-to\-many relationships going on, etc for the automated POCO relationships, but even that is not essential if you don't care for it.
&gt; What kinds of errors did you have? Performance is of course a concern. As the DB context's cache tracks more and more objects, everything becomes slower. The same cache doesn't understand stored procedures. So if you update something via a proc, then fetch the new value, it could return the old one instead. Objects also become polluted. Lets say I look at some order records, then fetch and return a Customer object. EF can decide to also fill out `Customer.Orders` based on the previous query when that's not what I want. This last point means that you cannot safely return EF entities from a WebAPI. You MUST map them to clean DTOs in order to prevent it from dragging along more data than you intended, which can lead to security breaches.
&gt; Yes, very however, EF implements the repository pattern... No it doesn't. Ask anyone to create an interface for a repository. They will differ, but none of them will look like DBContext.
Auth0 if you don't want to manage everything.
This is great advice, and kind of the way I’ve been trying to go moving forward. Just figuring out what feature I want to enable and dig into it. It’s been slow going at the moment, but I’m keeping at it. I’ll read your blogpost when I get a chance. I appreciate the feedback!
If EF is doing that then the developer is not understanding how their LINQ is being translated to SQL. It's easy to make a query with 10+ joins and not have a gargantuan query. In fact, it's incredibly rare to find a query I cannot express in Linq that maps to efficient SQL. The problem is that there is a burden on the developer to learn how the Linq-&gt;SQL mapping works for different constructs. This can be daunting at first but it can be well worth the type safety and composable nature of Linq queries.
Yes. As I said (for now).
A major problem that people have with an ORM like EF is that they write business logic all over their app (like in the controller) and wind up with a big blob of redundant code that is not reusable. After looking at their mess they conclude the problem must be the ORM so they choose a new one and repeat the process. The key to using an ORM or NO ORM AT ALL(!) is to relentlessly factor your business logic into your business layer where it is exposed as services. This has nothing to do with EF specifically and everything to do with writing a solid n-tier app. EF just makes it easier. n-tier has lost it's cool factor over the years to other preppy buzzwords like microservices. But the fact remains that if you want to write a busines app that is both DRY and SOLID you will use n-tier. 
Yup. I'm losing my patience. Normally it wouldn't be so bad but.... * Visual Studio updates take for frigging ever * ReSharper has to update the ReSharper updater and then restart VS * Restarting VS takes like 2 minutes before it's usable. Practically an eternity
I've never been a fan of plugins like this because it always comes with a drawback (sluggish Visual Studio, strange errors, etc). What's one thing that ReSharper does that would change my mind?
Does it still turn Visual Studio into congealed molasses?
I mean, the ‘can use for 95% of situations without much trouble’ *is* it shining. You can toss it at a project and handle some edge cases - that’s about what you expect out of an orm, and there are big productivity gains with this approach. These days EF has code-first and migrations more or less built in, which is a nice bonus too.
&gt;how can you have unit of work across multiple repositories in one request? This is in the context of a web api. You should inject the DbContext into each, and have one DbContext created per request. If you don't, you'll end up with data from one request inside another, or missing data when you go to update (multiple dbContexts for one request).
Try organizing your business logic into services that you inject into your controllers. Keep the controllers light, focused on respond to web requests.
Been using this for a while now, its very simple and nice, it also works for asp.net web api as well https://github.com/Microsoft/aspnet-api-versioning Works well with swagger as well. Lots of info from the main site vs this blog
I'm conflicted about this a little. From what I've seen having the business layer in between the controller and the repository creates the no man's land where you end up with some business logic in the controller and some in the business layer and even some in the repository and you have a ball of shit. In trying to not have the business layer and just have the controllers and repositories. I've moved validation to filters so it's not in the controller and validation in a rest API is most of the business logic. Yes you could not take this app and plug the business layer into a service bus for example but I'm ok with that to remove an extra layer to maintain. 
I also want to know this.
I like EF, have run into its limitations a couple times, and will be trying Dapper out for the forst time in the next few weeks. I am excited for the possibilities.
I think the template for the new project using individual accounts is a fantastic form of auth and works great. I wouldn't use anything that will take time to set up when so much is available in the template for both Framework or Core. Personally I used Core and switched from SQL Server to Postgres, but you could also use SQLite with only a couple lines of code different.
Step 1: Uninstall ReSharper
Any of the rosyln plug-ins are fast. I've been using code rush for about a week and it's like having a brand new ide
For someone working on their first application, $13 is pretty expensive. Auth0 is dirt cheap for business, but when most personal and sole proprietor sites can be run on a $2.50 / month Vultr instance with 0 to 3 active user accounts, using a Wordpress site or a .Net site from the Individual Auth template is all you need.
Thanks guy. Will check it out. &amp;nbsp; Side note: You and your team should try to work with Microsoft to allow for more flexibility in extension APIs. I know Rider is important to you guys, but I'm sure most of your customers use VS. Or maybe maybe a Rider Code similar to VS Code. Free with all the typical ReSharper features. Some type of freemium model.
I always recommend going with the most mainstream option if you aren't sure, which makes either Microsoft's Identity Framework or Identity Server 4 great choices.
I am building out a Microservices architecture, but I think the patter applies anywhere. I DI the context into a Service class, then DI an instance of the Service class into the Controllers. Keeps nice separation and if you want to add a layer between the Sercice class and the DB it's easy to do and doesn't impact the controller at all.
Auth0 here too. Yes, its a non-trivial expense but the time saved is huge too as well as having someone else keeping up with the latest and greatest.
I was a reporting analyst before I became a developer, so I favored Dapper or a DataReader for years, but I'm just now starting to use EF to try out migrations, better integration with Identity Framework, and super easy cross database compatibility. I still work on a team that is 100% against EF, but I'm thinking I might enjoy trying it out.
&gt; From what I've seen having the business layer in between the controller and the repository creates the no man's land Actually it works out really well. When your business logic is exposed as services it is highly reusable. I don't understand what you man by no-mans land. &gt; where you end up with some business logic in the controller and some in the business layer You should never put business logic in a controller (google skinny controller). Ask yourself: "If I re-write my website as a WPF application or a Winforms application will I be able to re-use this business logic?" If the logic exists in a controller the answer is "No". But if the logic is in the service layer (a .dll) you just make an API call or add a reference to your .dll and you are ready to roll. Quick rule of thumb: if you are doing some kind of operation on a domain object (order, customer, etc) than that logic is a candidate for being moved into the business logic layer. Try to put as much of that logic in your business layer as possible. 
With regard to boilerplate, you can set up some base classes and with the help of something like [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient) you can create a highly scalable and testable service layer in very short order. 
Not sure what you mean... Example of reusable business logic layer: public class CustomersService { private DbContext db; // never expose this outside your service. All data added to the db must come through methods exposed by the business layer public CustomersService(DbContext db) { this.db = db; } protected virtual string ValidateCustomer(Customer customer) { // ALL validation logic goes here. string errorMsg = null; if(something) errorMsg = "Not valid because this..."; else if(somethingElse) errorMsg = "Not valid because that..."; return errorMsg; } public string SaveCustomer(Customer customer) { // All customers added or updated to your db must come through this method // This method can be called from an app controller, api controller, winforms app, ajax call... you name it. // Everyone who calls this method get the exact same validation. string errorMsg = ValidateCustomer(customer); if(! string.IsNullOrEmpty(errorMsg) return; if(customer.ID == 0) // insert else // update } } 
its a docker app you should run it locally, its based on the chrome/webkit browser, it has a very simple json api you can call from any app https://github.com/arachnys/athenapdf We use it for several projects, we just create the reports in html/css and then convert them
It's written in Java, guessing it will always be kinda slow...
That's the first thing I looked for in their release note... no perf improve this time.
DbSet is the repository. DbContext is the UnitOfWork
A DbSet is a proxy for a table, not a repository. It's an implementation detail. As for the DBContext, it's technically a unit of work but it's a really piss-poor one. The real unit of work should be using most of the time is the DbContextTransaction. 
At the minimum, a repository should represent a logical data model, not just a single table. It would make no sense to have a Orders repository and a separate OrderLineItems repository. 
Take your pick. It's great at refactoring. Select a snippet and convert it into a variable with a single chord, change a local variable to a field or a property, bam! it's done, select a set of lines and refactor it into a method, just give the method a name, and everything is done for you. You've created a new method or property, but it isn't in the interface another refactor combo and now it's there. Want to swap the parameters for a method used in 20 places, refactor-&gt;change signature, a UI pops up and you can easily rearrange your method and all 20 methods are changed in one shot. Think some of your methods belong in another class, little tougher, but way easier than doing it by hand. There's a lot of autocompletion I use everyday. foreach/for/try/tryf followed by tab will create a template for you where it's easy to fill out the parameters without needing to type everything. Select some text and you can easily wrap it in one of the above as well. Or just select text and pick a bracket (){}[], now it's surrounded by brackets. Say I decide I want a new class which I want to call Person and it takes two paramaters, name and age, I just type var person = new Person(name, age); press altspace and choose the option to create a class with the constructor I wanted, it then gives you the option you generate fields for those parameters, convert them to readonly, etc... If you want to implement the IDisposable pattern, it's offers code generation for that. If you want to implement equals override for a class, it gives you code generation for that. The options it offers for both of those actually helped me understand *why* we would want one implementation over the other. I'm not a huge fan of Linq, but since it offers suggestions of how to convert it to linq, I get to taste what it would look like, and if it looks more readable, I use it. It helps to enforce coding guidelines, it points out logical errors such as a null check which occurs after an object has been used, and code paths which are logically impossible to reach. There's a whole lot of other suggestions it makes as well which help you write better code which I can't even remember right now. Code search! almost forgot that one! Sure it's in the latest visual studio now, but that's still not as good as what resharper has. You'd have to try it to see it. All that is just scratching the surface. I honestly consider myself a resharper Noob, given the number of things which that one plugin covers. On the flip side, when I'm working with others who don't use resharper, I personally find they code way slower or will be less likely to clean up as they code because they need to cut corners to keep up. 
Aaaaaand makes your visual studio unusable for the eternity. 
The best feature in reshaper is the uninstall one! You guys have to try it out.
Several of these vs2017 does out the box now. 
In my case it was includes. Include 4 columns? Okay. Include 5 or 6? 1000x slower.
Visual Studio without SSD is madness.
The videos are now available, https://channel9.msdn.com/Events/FSharp-Events/fsharpConf-2018
Gonna play with dapper today. Looks really interesting. Thanks for the heads up. 
The only thing you should be doing is ensuring there's a layer of abstraction between your code and the code that calls the API. You've probably already done this, but just in case you haven't: create an interface which looks like it does what you need from the WCF API and call that interface, then just code against that on your side and do whatever ugly mapping you need to do on the other side. This way, when the API changes to REST (in 5 years minimum) you can just swap out the implementation of the interface. 
Nice, thank you.
It's not written in Java 🤦‍♂️
Man, you're actually using and enjoying structural navigation? Good to know there are people like you: apart from it being a very rarely used feature, it usually confuses first-time users instead of helping them right away.
It's immensely helpful when you need to replace, move or copy-paste a logical block of code or markup that is not a straight one-liner. Imagine selecting and duplicating a piece of XML or JSON without using the mouse or drawing complex patterns with Shift+arrows.
Misread your comment. It's my current plan to do just that, I was hoping to remove the extra layer of abstraction between the wcf layer and my own data objects but it seems it may be more trouble than its worth.
404 :-(
Really? I thought pretty much all JetBrains products were written in Java.
Yeah, not really sure I follow TBH. Sounds like using the mouse would be just as fast
Dude... there are some things you just don’t joke about.
Yeah, I’ve never liked the official guide; it’s basically just “turn off the best parts of ReSharper.” I want it optimized better. If that means hooking into Roslyn or whatever, j want that. 
4 years back I changed jobs. Went from twin SSDs in RAID 0 to crappy spinning disk of rust... I kept thinking my computer was broken it was so unresponsive in VS. Nope, just cheapass boss, finally talked him into getting me a half\-decent PC with SSD. Now It's NVME SSD bliss. How does it make economical sense to pay me to spent half the day looking at my screen while my PC halts to a screeching halt is beyond me.
If you don't envisage using Facebook etc. then you can stick to a simpler approach (and thereby avoid the complexities of OAuth2 and OpenIdConnect) and issue tokens yourself. The gist is that you create an endpoint in your API which generates a JWT (Json Web Token) and returns it to any clients who call your API. The client then includes this JWT with every subsequent request. Your API can then check incoming requests for a valid token and rejects them if one isn't found (401 unauthorised). ASP.NET Core has built-in suppport for JWT authentication (which you configure in startup.cs). Once that's in place you can use things like the [Authorize] attribute to lock down parts of your API. I covered this in a [bit more detail here](https://jonhilton.net/2017/10/11/secure-your-asp.net-core-2.0-api-part-1---issuing-a-jwt/) if that helps :-) As for roles and permissions. If you plan to have different users with different levels of access to the API then you'll need a way to store your user details. For this you could use [ASP.NET Core Identity](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/identity?view=aspnetcore-2.1&amp;tabs=visual-studio%2Caspnetcore2x) to do the heavy lifting for you.
Nice article, thanks for sharing!
Step 1 Disable Step 2 Remove :)
Wouldn't you need some sort of server side rendering when working on something that needs to be listed on search engines? Web API + client side sounds appealing but maybe spiders aren't ready yet.
All JetBrains products are done in Java. 
Easy to follow and understand, thank you!
Can you try &lt;action type="Redirect" url="https://www.mydomain.co.uk/{R:1}" redirectType="Permanent" /&gt; I think the {R:1} part will include the part after your domain. Reference: https://stackoverflow.com/questions/17714732/web-config-redirect-non-www-to-www
just wondering out loud. would using razor pages make it easier to transition to blazor later on (should microsoft decide to greenlight that project)? i know it might be too early to speculate (because blazor is still considered experimental at this stage), but i was just curious.
Just starting to update myself a little on core... This seems like a return to webforms to me in a sense. Not a big fan really.
They didn't know about TransactionScope? Or why else re-use a dbcontext if it's not for multiple workloads to be persisted in a single transaction but started from different code points? (As EF merges unit-of-work with their persistence code, one of its many mistakes)
The client site can also do the server-side rendering for crawlers.
The lock-in remark is a bit nonsense, as every project uses external libraries they're tied to and changing them often takes a lot of work and time. If your software uses ASP.NET you're tied to your MVC framework for instance, going to change that? No of course not. Dapper and other micros are nice for fetching, but not really useful for OLTP if you have to persist data, as you're going to write the change tracking code yourself. Often this is forgotten/overlooked when people rip out the ORM and think they can do everything with a wrapper around SqlConnection. Erm... well, you can but you have to write it yourself, which comes down to effectively rewrite what the ORM already brought to the table. 
Oh that's cool, I didn't know that, do you have any link I can read about this? (eg: for Vue or Angular)
All ORMs leak specifics, even micros. If one is afraid of 'oh dear what if we have to replace the ORM!' they should stop writing software as it's highly likely they have other dependencies which are just as intrusive as an ORM but they don't bet an eye on that. Hell, using e.g. Dapper and SQL Server and then switching to PostgreSQL is a nightmare too, as you have to rewrite SQL statements to match the new DB. Not to mention little things like: myOrder.Customer = myCustomer; // is this true now? myCustomer.Orders.Contains(myOrder); Some ORMs make sure this is true (as it should), others don't and you have to manually write plumbing code to make this happen otherwise your UIs are e.g. not kept up to date in real time. 
Migrations... ever used these in production or don't you have a DBA governing your production data/databases? not using include and using AsNoTracking suggests you use it basically as a microORM, so why not use a MicroORM ? Only for migrations?
Thanks for the kind words :) 
I’ll have to try it then. Cause I thought it was all better in VS 
Most work issued with ORMs is boring code you don't want to write out in sql or the low-level change tracking code for. Good ORMs also offer a fast plain SQL API for the situations where you need to specify a SQL query which isn't generated by the ORM or which is complicated. you mention dapper, like many here, but every insert/update statement you issue through dapper requires you to do boring work yourself an ORM can do for you and it's not faster at all. Even fetches aren't faster through dapper. (see: https://weblogs.asp.net/fbouma/net-micro-orm-fetch-benchmark-results-and-the-fine-details) Even if you want to use a low-level microORM there are better alternatives than dapper, some even allow you to write compile time checked queries. Just because the hivemind here thinks there are only 2 options: EF or Dapper, doesn't mean there aren't better alternatives. Not to say Dapper is bad, it does what it must do very well, as it was written for that particular purpose in mind: low-memory using helper functionality around SqlConnection with good performance. But it's not 2008 anymore, many ORMs/microORMs have done their homework (well, except the EF team perhaps) and optimized their pipelines very well. 
As webassembly?
For simple CRUD with Dapper I use FastCrud extension, so I end up writing analysis and report queries in SQL.
iTextSharp works
PdfSharp or iTextSharp are easy enough to use. Both are available as NuGet packages. iTextSharp's current licensing requires your project to be open source if you use it, unless you pay.
Basically what happens then is you have a Node server that handles the pre-rendering, pulling data from your WebAPI. I would prefer that ASP.NET handle the pre-rendering itself and we don't have to touch Node at all, but I don't see that happening any time soon.
Here's some MSBuild that might help. This is from a .net core app. But shouldn't be specific to that. `&lt;ItemGroup&gt; &lt;None Remove="wwwroot\node_modules\**" /&gt; &lt;Compile Remove="wwwroot\node_modules\**" /&gt; &lt;Content Remove="wwwroot\node_modules\**" /&gt; &lt;EmbeddedResource Remove="wwwroot\node_modules\**" /&gt; &lt;/ItemGroup&gt; &lt;ItemGroup Condition="'$(Configuration)' != 'Debug'"&gt; &lt;Content Remove="wwwroot\public\**" /&gt; &lt;Content Remove="wwwroot\src\**" /&gt; &lt;Content Remove="wwwroot\.env.*" /&gt; &lt;Content Remove="wwwroot\*.json" /&gt; &lt;Content Remove="wwwroot\*.md" /&gt; &lt;Content Remove="wwwroot\*.log" /&gt; &lt;Content Remove="wwwroot\*.lock" /&gt; &lt;/ItemGroup&gt;` `&lt;Target Name="BuildSinglePageApp" BeforeTargets="Build" Condition="'$(DeployOnBuild)' == 'true'"&gt; &lt;Exec Command="npm run build" WorkingDirectory="wwwroot" /&gt; &lt;/Target&gt;`
I'm not saying that you should actually plan on swapping out your data access layer. It's just a thought experiment for testing if you are encapsulating your DAL. *** The fundamental problem with that example is bad data modeling. Circular references are problematic in general, without even considering the issue of ORMs. They cause problems for XML/JSON serializers, it's hard to ensure both directions around the circle are updated correctly, they often lead to memory leaks, etc. Generally speaking data models should be tree-like structures. Which unfortunately means we need two sets of classes depending if we want a Order-centric or Customer-centric view of the data.
I have never seen a single client use transactions properly with EF. Now granted they wouldn't be my client if they thought they could do the work correctly on their own, but still...
Nrecopdf is good too 
FYI: I interviewed at SpaceX recently. They are using EF too.
If you don’t feel like paying you can go download the last version with the free license. 
Yeah because they've been features of Resharper for years.
I see, so you end up with the WebAPI and client website each on a different domain/port?
Normally yes, but there is this thing: https://github.com/aspnet/JavaScriptServices
Thanks for this share! I've read articles by you previously and you explain things very clearly. Your site could be more ergonomic on a desktop imo. The fixed navigation bar is really dominating and the font size of the main text is too large. It would feel like a more solid experience if you don't have to scroll through a single smallish paragraph.
You might want to check out [Pandoc](http://pandoc.org/). Depending on what you find easy to render, you may be able to use this to output to multiple formats.
You can have a look to zetpdf.com .using this you will have more control over the PDF and you can control whatever you want.I've used and overall satisfied.
[removed]
No
You can have a look at https://zetpdf.com. using this you will have more control of the PDF and you can control whatever you want.I've used and overall satisfied.
Same here. 
&gt;Athena PDF This doesn’t look like anything you could just include into an MVC5 solution as a NuGet download.
Is anyone else getting the "Microsoft Visual Studio is Busy" screen (while Visual Studio is hung) a lot, after installing this new version of Resharper? Another developer on my team and I are both getting this a lot. We're both in large C# solutions. 
1. ReSharper is not written in Java. 2. Being written in Java doesn't make something inherently slow.
I wouldn't care if it ate 20GB of memory as long as it was quick, but it's still glacially slow :(
Aspose pdf is good, I like their documentation. https://products.aspose.com/pdf/net
Looks like you’re missing some commas in between your set clauses. Also, you’ve exposed your application to a serious security risk called SQL injection. http://csharp-station.com/Tutorial/AdoDotNet/Lesson06 Also also, for next time, it helps describing what’s wrong; what kind of error message do you get, what do you expect to happen, etc. 
Because it builds its own parse tree alongside Visual Studio's (which contains more information than what is exposed by Roslyn) and it blocks the UI thread. So not only is it doing a lot more work, but it makes the UI feel sluggish as well. Also Java is just a language that gets compiled down to Java bytecode. Saying it's inherently slow is idiotic, it'll run as fast as the JVM runs it, and that depends on the JVM implementation. 
Thanks for that feedback. Do you mean on mobile (rather than desktop)? My front-end skills aren't great ;-) but I've tweaked it now if you're viewing on mobile (to drop the font-size down and remove the fixed header). Hope that's a bit better!
I've seen a few mentions of it being the "recommended" approach for new apps. "Razor Pages is the recommended approach to create a new Web UI app with ASP.NET Core 2.0." From [this page](https://docs.microsoft.com/en-us/aspnet/core/tutorials/?view=aspnetcore-2.1) And Jimmy Bogard [references it here](https://jimmybogard.com/migrating-contoso-university-example-to-razor-pages/).
We will have to agree to strenuously disagree then.
No I actually did mean desktop! :)
[removed]
[removed]
Ohh, Thanks. This application is only for me to learn the language. I will not use it anywhere else. Yea i probably should give you the error message. I thought this is a really simple code, so people can see what is wrong immediately. My bad.
If you want full control ... use LaTeX. Build a custom library. Template away, pdflatex and enjoy...
I have used both. Webforms are a very thick abstraction. They have a complexe lifecycle/execution model. They make it easy to have lots of state and information that must be transported in each request &amp; response. Razor pages are much lighter. For low logic pages they are really nice to use.
I suggest you take a look at [wkhtml2pdf](https://wkhtmltopdf.org/). It's actually a command line utility that converts HTML to PDF. But there are C# wrappers available, like [this](https://github.com/codaxy/wkhtmltopdf) one. Using HTML makes it a lot easier to build the layout. 
What makes you say that? I've used both aspose.cells and aspose.pdf and had very few issues with them. Just curious what kind of problems you ran into.
Of course a continuum exists regarding "how leaky" a leaky abstraction may be, but that's really beside the point I was making. Extractions can be leaky in different ways, with some leaks being better than others. Forcing one to add implementation logic at a level conceptually below an abstraction undermines the entire point of having the abstraction in the first place. For example, SQL is an abstraction over a database engine. Instead of a developer needing to generate a query plan themselves, SQL allows you to just define the specification for what you want and the engine generates the actual procedural steps (and often any optimizations) necessary to fetch the data from a physical disk. This abstraction is also leaky. For example, there are times when simply re-ordering your joins or calling `OPTION( FORCE ORDER )` can result in an orders of magnitude difference in query time. This, though, isn't a "bad" leak. The point of SQL is not to optimize your queries (although many engines do this automatically). The point is to abstract the process of fetching the data. I have never heard of a developer needing to call `OPTION( USE PLAN ... )` in order to *fetch* the data they wanted. The abstraction holds. In contrast, the whole point of an ORM is to abstract away the storage mechanism. Hand writing SQL means you are now "locked in" to a SQL implementation (likely also the specific dialect you are using as well). This is a "bad" leak. It undermines the abstraction in way that doesn't just require a developer to understand how to "tune" the system, but forces them to know/understand the actual implementation. The abstraction truly fails. you can express all your business logic in managed code where it is far more expressive and reusable than a stored procedure This is patently false. The (anemic) data model generated by EF should be completely distinct from the (rich) domain model generated to solve your business problems. A proper DDD approach would be to include (yet another data abstraction) `Repository` layer on top of EF that includes the mapping between the two (this will save you when/if you decide to use a different data access/abstraction approach later on). In this way your domain is completely (truly) isolated from how it's persisted. And this is where the rubber meets the road for ORMs. If you already need a repository layer with methods like `findById` and `findAll` that map the results to your domain model, is extra the overhead of the ORM -with it's own querying and mapping- worth it? This is especially true for complex domains. ORMs are great for trivial applications. The good news is that an exceedingly high proportion of applications are, in fact, trivial.
I have used both. Razor Pages feels very much like a more compact and opinionated MVC with one controller per page, which is a pattern I already liked. It's very far from WebForms, no magic view state, life cycles or unnecessary abstractions added.
How does it handle actions that don't have a view? Like `FileResult`, or other kinds? It seems like an awkward arrangement to have everything go through views, when only some requests use them.
So the project i worked was as follows. Multi-tenant document portal, that supports upload of different document types (text, xhtml, word, pdf) by API into groupings. Each document had attached metadata (links to other docs, validity periods etc) The frontend for the system allowed searching of these documents and annotating them in the browser (for all doc types) and merging these into a multi user document. It's used by lawyers to assemble cases so they later on could print these out (with annotations) or view them on tablets. So we needed something that could convert Word to Pdf, Html to PDF and handling multi inline documents with individual pagination. Very quickly even on small docs we had memory errors. We had weird errors around from the API's. The API for making ToC's was horrible. The word api was buggy as hell when trying to extract headings. Word to PDF was horribly slow. Doing Word to HTML and then HTML to PDF was actually faster! So we did that. There is three different ways of interacting with a document, and i think two of them are deprecated. On there forums their "Happiness Developer" would tell people not to use one API only to post a sample that used all three in one! The documentation ONLY handles the most basic usecases. I am very glad i'm not working on that anymore. 
The same way you would for any field. Compare it in a LINQ query. In this case using a DateTime object.
You only use them for HTML pages. For API calls or files etc you have regular old controllers that can sit beside them or in another folder.
Thanks! That's what I needed.
[Vue.js Server-Side Rendering Guide](https://ssr.vuejs.org/en/). 
Nice! I got my first app up and running. I'm still a little confused how I'm going to get my client and server projects to work together. Usually my client app is in folder within my server project. Hrmm.
I am sure you aren't doing this..but I just have got to say it. Be sure not to use this for anything other than experimental purposes.. As this in itself is experimental right now. 
* Razor pages for yer standard load and postback pages * MVC [ApiController] for your API * MVC with View when warranted and for legacy * Razor pages in class libraries for modularity (also OrchardCore modules) 
No, it actually transpiles C# to Javascript. So Console.WriteLine will become console.log basically.
Blazer, Lazer, Blade, Fran Stalinovskovichdavidovitchsky
Honestly, I may use this for a few small projects around the office that I want to attach a quick ui to. Thanks for the warning. I did notice it was experimental.
Why use that over TypeScript?
Man, it's gonna be so cool.
No clue. But as far as i know, Bridge existed before Typescript. It is pretty nice though, since you can write everything in a single language (being C#). With the same bells and whistles (for the most part). Quick look might suggest that it's redundant due to Typescript, but to be honest, i haven't done a big enough project in Bridge to give any conclusive evidence on why to use either over the other. So i'm going to leave that to people who have more experience with both.
To show how old I am - you can always do it like we did in the old days - learn Postscript and convert that to PDF using Ghostscript.
You can design everything in a completely functional way with C# and lambdas. Just nobody will understand it... the whole point of design pattern is to communicate design easily. A design pattern is a communication tool, not a trick which "fix" flaws in a language. 
Geez, you're gonna start a reddit fire with questions like that, mah gawd!
If I understood correctly your problem, you got several services to consume, they share the same data structure but svcutil will generate different classes. If so... Svcutil supports the possibility to ingest your own implementation of the data contracts. Not sure the feasibility for the real case, but, ideally, you could import the services the first time, do a bit of merging and cleaning up to the generated data contracts into a new folder that you'll keep. Make sure to use the correct contact type/namespace. Then you remove the auto-generated code and reimport the services pointing at your version of the contracts. If the services share some contracts but have different namespace, create a base class with no data contract attribute. Then create a subclass for each offending contract and decorate the subclass with the proper data contract attribute. About the business logic, I'd suggest against injecting business logic into your data contracts. As someone else already suggested, consider the WCF service as a low level layer of abstraction, very much as you would do with a webapi controller. A way could be to automap your contract into your business logic types and apply your logic there. A general suggestion, avoid fighting WCF and definitely avoid approaching it with a restful mindset. WCF is a great tool to work with, but you got to play at its rules. (But I do agree that cross-business interaction with WCF is a peculiar choice)
Can someone explain why this is exciting? What does it offer over what we have already?
Write client side of web app in C# - without js. 
&gt; without js The exciting part
Gotta love CI/CD :)
You can use all the goodness of C#'s standard library without having to find a js library for it. You can use Linq and all it's goodies, you can use calculated properties with cleaner getters/setters, you have strong typing etc... Mostly the exciting part is that it's C# that we know and love. Heck even F# works as well for those who want to use it.
Holy cow, there's also Blazor-redux. [Blazor-redux](https://github.com/torhovland/blazor-redux) I'm so hyped for this, can't help myself :)
you can add a global filter to your MVC service container public void ConfigureServices(IServiceCollection services) { services .AddMvc() .AddJsonOptions(options =&gt; { options.SerializerSettings.ContractResolver = new CamelCasePropertyNamesContractResolver(); }) } If you do override your models with [JsonProperty] attribute then it will use that instead of the default resolver. Does this help?
&gt; A design pattern is a communication tool, a human language of software design, not a trick which "fix" flaws in a language. Sorry to disagree with you here. The need for design patterns is a symptom of an underlying fundamental flaw in the language, or to be even more specific; **OOP**! Let me ask you a question. Do you understand this code? https://github.com/polterguy/micro/blob/master/startup/widgets/micro.widgets.file.hl Pure, simple and 100% functional. It also just so happens to eliminate the need for interfaces, does not contain classes, and replaces roughly (at least) a handful of design patterns (abstract factory, factory method, etc, etc, etc). Its OOP equivalent, if that is even possible (since it dynamically loads files and uses these as _"objects"_) would imply at least 2 orders of magnitude more code, and boiler plate code to extent that it would end up repeating at least 95% of its code, in such a way that your code would not be reusable ... **Disclaimer** - I created that language (Hyperlambda).
If you are the only one using this language you don't need design pattern to communicate with others. I can't understand this code. Anyway I can't judge with one file, design pattern are useful to communicate the design of a big project.
pdfsharp + migradoc works for me
My main is C# 1) Using as well Typescript, C/C++, python, PHP when the need arise for chirurgical interventions on existing projects. 2) I am a backend guy, but frankly I think utility on the web has declined. A Razor website is way more usable than a full blown SPA. If anything, SPA compete with desktop apps. (https://ponyfoo.com/articles/stop-breaking-the-web) 3) Karma from Ballmer's time 4) C# let me do anything quick on all plateform, no compromise, very few bug, always work as advertized, get out of the way, and scale with projects. JS always break for big project... Typescript make it manageable and I like it. I don't like how much time is spent on configuration of your project versus actual coding. 5) The problems of JS are not so much performance but breaking complexity (you can't easily refactor without non obvious runtime breaking). With JS you are always trying to make building by sticking piece of woods together. C# libraries are way more solid, stable and predictable. 6) Python has same issues as JS. Does not scale with complexity, make refactoring impossible leading to horrible code which can't be refactored.
I would say you have no idea how long I've been waiting for this, but that'd be easy... just take `DateTime.Today` and subtract the `DateTime` for the day JavaScript was introduced.
It's definitely an improvement on the previous version. Still needs more changes to make it nice but it's certainly progressing in the right direction.
Puzzled headscratch
1) It's more the reverse, I'll always use C# in my own projects, I'll switch to Java, C++ or whatever else for a job 2) JS on the backend - no chance, JS on the front-end for existing projects, transpiled/webassembly for new projects. 3) There are lots of MS stack developers but the core + open source is more than enough. Personally I avoid MS solutions on smaller projects. 4) Love/Hate: * C#/Love: Excellent language designers who are pushing the langauge forward * C#/Love: Opening up to new platforms * C#/Love: Performance improvements making it very competitive * C#/Hate: Nuget is badly designed * C#/Hate: MSBuild is a PITA... but avoidable * C#/Hate: No good cross platform UI kit * JS/Hate: Language missing lots of features * JS/Love: Runs in browser 5) Probably, but it'll be harder to create the correct solution 6) No, sorry
**They've moved the view and controller to be in the same folder** - good change grouping by application function rather than class type is a good thing. **They've merged the controller and the viewmodel** - not such a fan of. Makes it rather hard to have separate models for GET / POST, which without leads to lots of accidental binding bugs where users can update fields they shouldn't be able to **They're not reintroducing view state or hiding html/http behind webcontrols** - great WebForms tried to simply html and state too much and made things harder to do. This doesn't seem to be doing that. **Overall** I still prefer MVC over this, although I normally introduce feature folders to my projects. So really it just comes down to their weird choice to merge controller and viewmodel.
Isn't camelCase the default? Your example wouldn't change it. Though this is how i would have changed it with approach 4 using PascalCase. Besides how, do you have any suggestions for what approach to do? Is it better to convert old data or should i keep it simple and only apply this to new data structures?
I recently started designing a version of reverse polish notation language, which can be used as a universal alias script to ANY programming language. Sounds too good to be true. But it is. Check my post history. The most recent post points to a LinkedIn article on the solution to the Google Oracle Java dispute.
Is this your Microsoft work accout or our home account at headexplodes@myorg.com?
Sometime back when Netscape Navigator ruled the world.
Neither ReSharper nor Visual Studio can eat more than ~3 Gb since devenv.exe is the 32-bit process. And it is one of the explanations why ReSharper is slow in Visual Studio but is extremely fast in JetBrains Rider - Rider is the 64-bit one.
**shudders** a bit of JavaScript
lol no jobs
&gt; -100 comment karma ( ͡° ͜ʖ ͡°)
Do you know if there is anything in the works to enable filtering by license type when searching nuget for packages?
&gt; You can easily gather all types in a namespace in C# using reflection and load them up as objects (which is just a different convention than loading them from a folder) This requires having your classes actually compiled. By loading a file, you can generate that file if you wish, dynamically - Or upload a package, in a zip file containing your file(s), without even requiring a JIT compilation or a recycling of your application pool to even occur - Among other things. However, ignoring that, how many lines of code do you think the reflection approach would require? The code I was linking to had 82 lines of code, where roughly 35% were comments. Removing the whitespaces and comments, we're down to less than 50 lines of code. &gt; I don’t know what you were trying to prove here That most software developers are using sub-optimal approaches to solving deterministic problems. If SW developers were using the best approach possible, with the fewest steps in their _"winning strategy"_, less projects would fail, and we would increase the quality of our projects in general. Check out the YouTube video from CBS/InfinitySeries on Chess and Winning Strategies if this is Greek to you. &gt; while your argument itself is fully ignorant This is a feeling you get because what I say creates cognitive dissonance for you. The cognitive dissonance comes from the fact of that what I show, doesn't match your existing world view - And hence, the better I argue, and the closer I come to actually convincing you; The more resistance your brain generates towards my arguments. This is a psychological effect, created to sustain and protect your current (sub-optimal) winning strategy - Our your _"local optimum"_, to speak in terms of evolution. Create a C# project that does what I suggested, or find one that does it, and count its lines of code - And if it is less than 50 lines of actual code, and have all the features of my _"file loader widget"_ - I will give you the copyright to my own work, and allow you to do whatever you want with it - Including throwing it in the garbage!
&gt; If you are the only one using this language you don't need design pattern to communicate with others. I can't understand this code. You might not be able to understand every minute detail of it, but the proclamation of that you don't understand its general structure, and roughly what it does, is flat out a lie - Unless you don't even understand English ... I could show that to my child (I have in fact), and they were able to tell me (roughly) what the code does ...
Reactor Xamarin is fine. Personal choice really. Although Xamarin will allow you to utilise your c# more and as dotnet developer you might find it a better fit. I certainly do. 
Checkout the .net core JavaScript services. https://github.com/aspnet/JavaScriptServices We are migrating an mvc project over and it has been nice and easy to use. 
Looking at https://blogs.msdn.microsoft.com/dotnet/2017/03/09/new-features-in-c-7-0/ * Out variables - very useful syntactic sugar. * Pattern matching - not sure how much I'll use this yet. * Is-expressions with patterns - again, not sure * Switch statements with patterns - again, not sure... * Tuples - Finally a nice usable Tuple. * Deconstruction - Goes hand in hand with tuples * Local functions - Good for code corectness, will use ocassionally * Literal improvements - Good stuff * Ref returns and locals - Great for gamedevs (along with new APIs) * Generalized async return types - Not using async too much but good step in generalising the solution * Expression bodied members - Improves code clarity * Throw expressions - nice clean-up around throw So all in all, I'd say pretty good. Now what I'm really excited about is what they're calling shapes (traits). I'll be able to throw away a lot of runtime codegen with this. https://github.com/dotnet/csharplang/issues/164
1. Powershell, VB (some old work code; not by choice), JS, TS, SQL, Python, Perl 5, Perl 6 and have dabbled in many other languages. 2. I consider the latest MS stack to be: Angular/AspNetCore/Docker/MSSQL which is pretty modern 3. ??? 4. JS is one of my favorite languages (the DOM historically sucks though). I like the syntax and the dynamic nature of the language and think the object model is interesting. C# I'd say I know and understand pretty well. I'd be comfortable writing just about anything in either language. I think C# is better for larger things worked on by more than one person because the language forces more organization on the code by requiring static types. TypeScript allows JS to be written in a more organized way though which helps there. 5. I honestly think most languages can be used for most projects interchangeably. 6. Yes (a while ago). It is comparable to both. I think Python is overrated for scientific stuff; Perl 6 could do exactly the same thing and would look better. So could C# or JS.
I suggest looking into using robocopy.exe if you can just copy the files to publish them. robocopy will automatically skip unchanged files which is exactly what you want.
 I'm not going to argue I'll argue, because I have arguments. I'll assume you invoked the "gravity" defense because you don't actually understand what the Entity Framework is or what problem it intends to solve. It's also clear that this discussion is going to be futile because you lack fundamental knowledge of application architecture, the entity relational model (ERM), and domain modeling. I have neither the time nor the inclination to explain these concepts to you. Unfortunately for me, these pieces of knowledge are fundamental and necessary to compose and understand a proper argument, which means we are going to be talking right passed one another. Because I want you to be a better developer, here is short list of books on these topics that I HIGHLY recommend: Domain Design/Architecture: * Domain Driven Design - Eric Evans * Implementing Domain Driven Design - Vaughn Vernon * Clean Architecture - Robert (Uncle Bob) Martin ERM: * Data Modeling Essentials - Graeme Simsion * Designing Quality Databases with IDEF1X Information Models - Bruce Thomas (oldie but goodie) Messaging: * Enterprise Integration Patterns - Gregor Hohpe, Bobby Woolf Okay, now that the disclaimer is out of the way, let's get to the meat of it. I'm going to focus on of the EF here because it's clear to me that expanding the scope is impossible for the reasons outlined above. Taken directly from the EF Core landing page: EF Core can serve as an object-relational mapper (O/RM), enabling .NET developers to work with a database using .NET objects, and eliminating the need for most of the data-access code they usually need to write. That's the goal: abstract data access. The EF is a replacement for the procedural code used to manage a database connection, compose and execute queries, and map result sets into strongly-typed objects. That's all it does. Every little bit of the EF code that is written in an application could be replaced by a database connection, SQL (usually), and a data mapper. This is objectively convenient when it works. It can make data access quite seamless and reduce a lot of boilerplate/hard-coded queries. Because it abstracts data access, it *may* also be possible to change the underlying data source for a project without needing to modify any EF code (this seems rather unlikely though for a complex domain). Even without a perfect abstraction, the EF can be useful. Especially for CUD. Now let's move on to the model EF generates*. Because the EF seeks to abstract data access, the model used by the EF is part of an application's Data Access layer. Clearly, this model is anemic: * properties are all public (everything is public like in a database) * there is literally no behavior (relationships are not behavior) That's not a bad thing. A data model can be anemic because it's essentially all DTOs (this stands for Data Transfer Objects - think about that for a second). That's the goal: to provide data. The operative question here is, "Provide data to what?". The answer: your domain. The Domain Model is home to ALL of an application's business logic - the kind of invariants that simply cannot be enforced by a database engine (e.g. a `Customer` cannot have collection of `Orders` whose total is more than $2000). This is where the actual "value" is located. While mixing your Domain Model and Data Model is certainly possible for trivial applications that have very little business logic, it's still bad architecture. A Domain Model should have no concept of how its persisted, and *very often* has a different shape than the Data Model (a Domain Model should be developed *first*. *Then* you figure out how to persist it in a normalized way). The "standard" approach used to isolate a Domain Model from persistence is to use storage-aware `Repositories` (this is where EF code would live). It *is* kind of redundant, but there must be a separation. Taking my example above, how else could that invariant be enforced other than creating a `Customer` aggregate root to encapsulate the `Orders` collection? Some ridiculous `Rule` extracted "upward" into a Service layer that every developer needs to remember to invoke when adding an `Order` to a `Customer`? No, business rules (behavior) should be grouped with the data they act upon. This is the fundamental purpose of OOP. Your "correct" implementation is insufficient. An Application Service layer's role is to coordinate your domain model (it will have a reference to `DbContext` - that it passes to `Repositories`), not execute business logic. Your description is a 2-layered architecture... that's not a thing. * I'm not even going to argue whether or not EF generates a model. You can google "model first" or simply apply yourself to understand that the underlying concept of the "code first" approach is still subscribing to the EF model, and that the mode of generation is irrelevant. 
I would like to mandatory see scoped packages. Though the damage is already done.
looks compelling... as soon as I have a moment I'll read it.
This is true to a degree. It's still very experimental and also somewhat "janky" in the rendered html. Now, if you are building an app that doesn't care about SEO, then I'm all for full client side and API. However, if you do care about SEO, I'd be hesitant.
When we build a web api, its just an api site, it doesn't serve any pages. If the front end/client site needs SEO, its up to the front to make it happen. If they have a framework that does it, its a bonus, but they may need to implement a bunch of server side stuff and call the api server-to-server. Its not for SEO, but we have a Craft CMS site that calls a .Net Api server-to-server for FeedMe and then most calls are made from browser on www.site.com to api.site.com.
&gt; If the front end/client site needs SEO, its up to the front to make it happen. If they have a framework that does it, its a bonus, but they may need to implement a bunch of server side stuff and call the api server-to-server. That's the problem with using a client side js framework. The SSR puts an unnecessary load on the server and the outputted SSR html isn't always good markup for crawlers.
If you figure it out, let me know. As someone with very little programming experience, I’m having a hard time figuring out where to start with ASP.Net Core 2.0....
What types of mvc apps would you be developing?
Our company builds custom apps \(web and backend for mobile\), which means we build all kinds of projects, typically 5 projects per year. The actual business logic usually isn't very complex and its typically just CRUD operations with some logic in\-between, however some projects can span hundreds of Models, which I could see getting pretty messy in .NET Core since each model will need an Entity, ModelViews, Controller, Repository, etc. Currently I use PHP/Laravel and build the server in a such a way where the Controller will handle all of the business logic and respond with HTML \(web\) or JSON \(api\) depending on the request. However in .NET Core, I would like to keep the API and web controllers separate and as thin as possible. Which means I need extract the actual business logic out into Repositories and Services \(I think that is the recommendation\), but I can't figure out how files/folders would be structured in this case to keep the project manageable/maintainable at larger size.
I understand what you are saying; I have my doubts about js based SSR, but its not my domain of expertise. I would hope that Angular would work well with Google's crawlers but there is more to SEO than just Google In a way, I guess I'm just saying that as the api dev, its not ~~my problem~~ up to me, its up to the implementer of the client facing website to figure out. :)
Async Main is the kind of syntactic sugar I want to have in every cup of coffee! Besides that, 7.1 and 7.2 are mostly pushing towards a highly performant stack. Long time due. Not for my own sake, I care about quickness of development. But for those who create the libraries I use. C# 8 will be a revolutionary change since 2.0. I suspect we might even get a new major release of the runtime for desktop.
Probably the data-driven kind. As he said, CRUD.
Do you think he means like hundreds of models as in hundreds of rows in a table or as in hundreds of different classes to represent those models?
Ok. So what would be an example project? The sample he gave was just a blog.
Lol, I get what you are saying.
The communality of the models between get and post are probably the only thing I am not enthusiast of. On the other hand, I look at those boring crud post/redirect/get pages and I see a natural fit for this technology. Especially with built-in support for dependency injection.
Consider an work order system that has 50 drop down menus the user can select. To keep the database normalized and allow the administrator to edit those drop down menus from the admin panel, they need to become tables, which means 50 models. Most of these models are just straight CRUD with no additional logic, but thats a good example of how the models in a project can quickly add up. I should also mention that most of our project are less than 30 models, but I want to keep a code base that is salable to hundreds of models if necessary, which I am able to do quite easily with my current structure in PHP/Laravel.
We used another smaller framework for some specifics where Aspose just didn't work. We paid 41.530,00$ for the license.
You shouldn't make an area for each type of entity you work with. It's massive overkill and your routes really won't make sense unless you customize the routing. Areas are more for something like separating out admin functionality from the main application. Following the normal MVC project structure already has you separating views, etc. by controller: MyProject/ Areas/ Admin/ Controllers/ UsersController.cs ViewModels/ BanUserViewModel.cs ... Views/ Ban.cshtml ... Controllers/ PostsController.cs UsersController.cs ViewModels/ Posts/ PostCreateViewModel.cs PostIndexViewModel.cs ... Users/ UserCreateViewModel.cs UserIndexViewModel.cs ... Views/ Posts/ Create.cshtml Index.cshtml ... Users/ Create.cshtml Index.cshtml ...
The book I've used thus far is "Pro ASP.NET Core MVC2" by Adam Freeman. Its been pretty good, what I've liked about it was it starts out going over the basics and building a boilerplate storefront app, but the second half of the book is a deep-dive into the framework - So its a solid reference for after you've learned the material.
Take a look at OrchardCore - this is a great framework for dotnet core web apps; lots of good examples and open source! 
&gt; On the other hand, I look at those boring crud post/redirect/get pages and I see a natural fit for this technology. Especially with built-in support for dependency injection. but it's so close to the existing MVC that I don't understand why they'd add yet another paradigm. Feels like a couple of tweaks to MVC could have made it just as suitable.
Lol, sorry it was just a random shot at microsoft accounts. Not directed at you at all.
Sorry for my ignorance, but are you doing your front-end with C# in Xamarin? 
Ok, cool. React Native seems to be more widely used and I have actually seen it mentioned in a few job postings. I also prefer to use JS than C#. 
These are those couple of tweaks. MVC is a good pattern for server side web applications, but it's extremely verbose. The amount of work you need to put for a crud site (say a simple intranet page) is insane. This is taking some of the good sides of webforms (cohesiveness) and inject it into MVC while taking it out its bad sides (verbosity, amount of classes/files spread in the solution). I've personally been in the business since classic asp and sometimes I miss the rapid development offered by webforms. I would never pick it (again) to run a public site but the ease you can create a master/detail view has been lost ever since. I'll tell you what. With razor pages and the ability to put razor components in class libraries, we'll see a new breed of "control libraries" in less than 12 months. Nothing based on the page lifecycle, view state, control state or markup obfuscating controls a la web forms. But highly reusable and styled by CSS controls. Now I'm sure this post will be downvoted into oblivion...
github.com/dodyg/practical-aspnetcore
Yeah, i actually created a powershell scrip using the msbuild and publish ignoring the files that are not needed to the server. I’ll try with robocopy if it actually speeds up it would be great. Than you!
The numpy library is amazing. It’s not about the language. 
Thanks for the suggestion. I think I misunderstood what the article was saying. &gt; By default, ASP.NET Core applications organize their folder structure to include Controllers and Views, and frequently ViewModels. Client\-side code to support these server\-side structures is typically stored separately in the wwwroot folder. However, large applications may encounter problems with this organization, since working on any given feature often requires jumping between these folders. This gets more and more difficult as the number of files and subfolders in each folder grows, resulting in a great deal of scrolling through Solution Explorer. One solution to this problem is to organize application code by *feature* instead of by file type. It seems like the best use would be to have a mix of the two like you suggested.
In my spare time, I like Go, F#, Elixir. I don’t have spare time these days. The complaints about C# is really how Microsoft makes things complex. Look at auth in Asp.Net core. It boils down to setting the current user. It took many iterations of MVC to get there. It’s so much easier in node. Try out Django as a python framework. It’s easy to use and has good documentation. The difference between Node/python vs C# is I typed vs typed. Dynamic vs static. Look up the differences. It’s about robustness. But you have a lot more boilerplate to write with static languages. Make up your own mind about it. 
You can always open a PR. :)
I think the idea is that when you build the client, it just produces a distribution, and you can host that with any server you like. If you want, you can just copy the output of the client build to the wwwroot of your server build. But you don't even necessarily have to host the client with the application server, it's just static content that can be hosted anywhere.
Just amazing!!! Love net core.
Ah, I see. Feature folders are a separate concept from areas. Take a look at [this](https://msdn.microsoft.com/en-us/magazine/mt763233.aspx) article, it covers the differences between them. 
microsoft's contoso university.
Performance profiled: https://stackoverflow.com/a/49910526/4171082 Async is not always the answer.
Yeah robocopy is a bit more complex to use than a normal copy command but it will ignore files that are the same on the other end which is great! I've used it in a few scripts for this purpose and even built my own file copy code to use that same mechanism (comparing file modified times and sizes to see if a file has been modified).
That's a lot of drop downs but I get it. I wish you luck! In this particular case you might think of making a drop down model that has a guid that is tied to a form and a json property that is all the values of that drop down.
I'm not following why you have 50 models for 50 dropdown menus, but that could just be down to not understanding the application. Generally when you find yourself repeating code, you can reduce the repetitiveness by representing the next level up. In your case, represent the notion of a drop down (and whatever logic follows). You increase the complexity, but gain manageability.
The .1 release really does not do justice to the number of features and improvements they put on the platform and related frameworks.
Since they are not API changes or any other breaking change, it is normal to be on that incremental update. https://semver.org/
The HasFlag improvement is pretty great for me personally. Can't wait for release! :)
If you can spare the $30, plurasight has a great course on asp.net called "ASP.NET Core Fundamentals" which goes into MVC, razor, working with entity framework, authorization, etc. Basically all the things needed to startup a web app. Can't recommend it enough
Oh wow, that's awesome. I don't need npm's dependencies, but I do need to be able to control my 20\-30 odd javascript libraries. They all have CDNs, so this looks awesome!
Definitely worth discussing. I'm interested in seeing how others feel about this.
I found [this GitHub issue](https://github.com/aspnet/LibraryManager/issues/65) containing some additional discussion regarding why they chose to build this.
GraphQL is cool, as long as you use e.g. JS and Apollo stack. For us, backend developers (not only .NET) it's a nightmare once you have to translate all of the complex queries into native DB language, e.g. SQL (using joins and so on), unless you want to keep the read-side purely in memory (e.g. Redis cache) then it's a no-brainer.
There are API changes in .Net Core 2.1, though. It's just not what this article was focused on.
TFVC doesn't care about the type of files or how the VS IDE handles them. Maybe you just need to read up on how the changesets work, the concept of workspaces, mappings, etc.
Well, I don't need the dependency side of Yarn/NPM, which seems to be the main reason to use it at all, as well as the horrible messy bit. I just need a nice way to manage the files I import from a CDN, or from our own custom javascript.
I've been weary of buying .NET core books, because it seems like it changes really fast. Has it slowed down to the point where I can buy a .NET core book and it won't be obsolete in a few months?
Hello folks. Just in case anyone isn't already aware of these guys... absolutely fantastic podcast. Pretty much every episode has something in there that makes it worth listening to. Highly recommended! 
Flatware is now called Blazor-Redux https://github.com/torhovland/blazor-redux
How's the compilation experience? Does it have hot reload feature? 
Probably not, LOL. However, from previous books by Freeman he is very good about updating and appending chapters on Github, along with making updates with new releases. In the case of MVC, to be honest the changes will not be as dramatic (i.e. 2.0 vs 2.1 vs 2.2). Getting in-depth understanding of the pattern along with the differences in how middleware is applied (.NET full vs .NET core) is key IMHO - And that you can get from the book without it being really obsolete.
There is no breaking change in 2.1. There are additions, but nothing was removed. If it can compile in 2.0, it will compile in 2.1. That's the point of semantic versioning.
There also weren't any breaking changes in 2.0 and there likely won't be breaking changes in 3.0. Not everyone uses semantic versioning.
They weren't doing semantic versioning before 2.0
Carl &amp; Richard are awesome, go listen to their podcast now.
In my understanding, response caching is not available until core 2.1, or is that only when using it within the Razor code? Kinda confused. Nice list though.
I still really like knockout for it's simplicity. Same goes for Vue. Angluar is easy enough but it's such a cumbersome and dictating library.
I never liked it myself. It felt like a solution looking for a problem.
The two MVCs have nothing to do with each other though , do they?
What advantage has graphql over odata?
Prism is used to write modular desktop applications. That is, applications that have a core and a number of either mandatory or optional modules that may have dependencies on other modules.
Dictating is the key word, right from the app="xxx". Great for a SPA, but complex applications with hundreds of views and view models, not so great, starts to break down fast. In the real world, where you may have many developers, some full stack, some angular only, I have found the angular people do everything client side, quickly turns to spaghetti code, impossible to test.
No, its really just semantics, both are Model View Control frameworks.
&gt; I am currently working with a very large complicated LOB angular/ ASP.NET MVC app. I think that's the problem. I think a lot of Angular apps really should be a several small apps that happen to be hosted from the same server. But Angular isn't built that way so things get messy fast. Razor, or Web Forms, tend to stay pretty simple because each page is basically its own mini-app. 
Aren't those API changes only there because .NET Core 2.1 is now built on C# 7.2?
Go with Razor Pages. It's awesome. 
Correct, but if you are utilizing containerization already it would be pretty darn easy to use.
Or maybe do whatever needs to be done to start using a real VCS. 
It's neat if your application is trying to solve multiple problems - it provides a tidy way to structure your application into separate modules that can either act independently or depend on each other, with conditional loading, but still be presented as one single application. Also the IoC containers with dependency injection is kinda cool.
Uh, no. That's way off. In classic MVC, you have tightly coupled View-Controller pairs. Models are long-lived and can shared across multiple V-C pairs so that altering the model causes all of the views bound to that model to update accordingly. In web MVC, each "controller" is really a request handler. It accepts a short-lived model (more likely a simple DTO) and returns a view. The view has no knowledge of the controller and the controller can return any number of views. And in both cases each represents a single tier of a N-tier architecture. They say nothing about how you organize your business logic or data access layers. 
Yea you totally right. He should be using CVS. Get outta here
Yes, yes, we know that they both have the same name. But that's a far cry from being the same thing. 
Huh? I would recommend git. But almost any other VCS would be better than the pretender TFVC. 
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/latexandloaf] [Build a GraphQL Service with ASP.NET Core 2 and Entity Framework Core](https://www.reddit.com/r/LatexAndLoaf/comments/8dj0q0/build_a_graphql_service_with_aspnet_core_2_and/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Started looking into this today, as mentioned, its been a long, long time since I used ASP.NET MVC as configured out of the box. Ever since WebAPI its been JS frameworks like angular, react, very little Razor (if any at all). I have come full circle with this. What seemed promising falls apart when complexity is involved. A property name or type changes somewhere in a CS view model returned from WebApi Controller, and all goes to hell, no warnings. Coupling is too tight.
Im considering using azure table storage as a simple, super scalable and distributed cache. (really cheap too) Has anyone attempted doing this or something similar? 
Yea, tell that to the assets developer downloading a few hundred gig of change log data from a git server. TFVC definitely has its trade offs and isn't for everyone. But none of the other options are perfect either.
None except it is available sooner.
Prism has been around for quite some time, I'm not sure how relevant it is today. Quite honestly, having written native apps both with and without Prism, I hadn't notice it solving any meaningful issues. It introduces some sensible patterns, sure. But they're no way tied explicitly to Prism, you could happily keep a modularised native app clean and maintainable without involving more dependencies.
MS Dev Show is very good as well
I'll upvote knockout.js. It's my go-to for small / mid sized projects that I need any real client side scripting in.
I have been listening to them for over 10 years now. It is still as great as it was when I started listening to them.
And then if your api is open, you have to worry about performance on all the possible edges of the relationship graph
Which version of Angular? I do not find what you are saying to be the case with Angular 5. 
there are breaking changes to the semantics of aspnet core, thats why they have the opt in/out enum for behavior. though i suppose it isn't breaking if it is opt in, but I'm not sure what the default is.
They both do routing. Its redundant to have two routing engines on top of one another. 
stoked to see it permeate so quickly. i probably won't have much use for it outside of a few niche scenarios, but I'm pumped to get all the perf goodness from it.
So prism doesn't really provide much novelty. The things prism provides are - view model implementation, ICommand implementation, ioc, modularization, event aggregator, and couple of other nifty features. I personally like Ninject as an IoC container rather than MEF/Unity. It's more powerful and more versatile container bar none. Also you can make your own Region Manager by tying up a content presenter/content control to a custom home grown navigation/presentation service. I prefer this approach because I don't like the concept of loading views in named regions, and the magic strings (the region names) that litter the code to implement such functionality. I much rather have a navigation service to load a view into a given content control, by simply telling it the type of view model the service is navigating to. I suppose this is called View-Model-First approach to presenting different UIs to the user. To facilitate the navigation/loading of custom views in different parts of the app I use two different approaches. The first relies on WPF's ability to resolve views based on content templates. For this to work all you need is a template to be associated to the type of view model. A content presenter automatically looks up the template for the view model bound to its content property. The navigation service simply exposes a property which is of type of your most basic view model. When this property is bound to the content control or presenter, WPF looks up the template associated to the view model and renders it on the screen. Simple. The second and more elaborate way is to decorate each view model with a "View" attribute, which accepts the type of view. When an instance of the view model is resolved by the IoC reference in the navigation/presentation service, the view attribute is used to resolve an instance of the view. Thereafter the two are married and set as the content of the content control that renders them. Modularization which is a big selling point of prism is nothing new to .Net. Modules can be implemented in a manner similar to how plugins are made with any other .Net application - you create a shared interface that declares the entry point for your plug-in assembly. Plug-in assemblies (modules in prism) are discovered by watching the file system with a file system watcher. Prism's implementation of this mechanism is wonky at best - it works better when you declare your modules in the module catalog. Though there are some really convoluted samples that show how to do it by watching the file system as well. Dynamically loading such assemblies happens via reflection by searching for the implementation of the entry point. Once discovered, the entry point is called by passing in a reference to the IoC container so that internal views and types can be registered with the app. It really is no brainier. Another feature of prism is the event aggregator. I do not like prism's implementation. It's convoluted. I prefer to roll my open event aggregator. The implementation I created uses interfaces, thereby if a class wants to listen for a particular type of message it has to implement the interface that declares the method signature for responding to said message type. Internally the event aggregator stores the listeners as weak references this allowing for view models that listen to messages to be garbage collected. The only thing really useful that prism provides is a basic implementation of a ICommand. However even this has it's problems. I don't remember if there was an async command implementation or other helpful bits and pieces such as async property and such. Also the implementation of INotifyPropertyChanged is wonky. The C# 6 syntax allows to use nameof() and further more - you can use the caller member name attribute to shorten the method signature of your base view model. I don't remember if prism 5 used it. Finally, although not many people talk about it, I like for my base view and view model to implement a proper dispose pattern. It's quite common scenario where a view may hang on to a view mode or vice versa thus creating memory leaks. This happens most often with the use of attached properties, behaviors, custom triggers, etc... so having the ability for my own home grown navigation service to dispose the view and view model (representing the previous view that is being navigated away from) is just a requirement I won't overlook. Either way, at the end of the day whether or not you use prism, in my opinion, is just a preference. For quick and dirty apps I use it as it saves me from doing a bunch of meaningless plumbing. But for bigger, more scalable applications I use my own home grown MVVM framework.
There’s still the DataLoader implementation, which wasn’t mentioned in the post. You could also add in Dynamic.Linq to make queries even slimmer. Of course it’s never going to be as performant as straight SQL, but it does create an easy to use API that’s will make most frontend devs cry tears of joy. At the end of the day it’s just another technology you can choose to add if you don’t mind its caveats. 
The way I've typically handled this is to have a composite primary key on every table, where the first column is the OrgId and the second is the unique value that identifies the row. That way you never have to do the second lookup and protect yourself against records accidentally getting matched to other user's data.
Bah, no. What grayenwolf said, really. "MVC" of ASP.NET MVC has nothing to do with MVC of the desktop (originally, but moves nicely to HTTP/JS based UI). Same words (M, V, C) are used, but their real meaning and what corresponding artifacts really do differ wildly.
We make heavy use of redis. We shard it based on an algorithm that lets us direct to the particular shard.
That is also the way SAP deals with this. They have in every table with org visibility a column that refers to the organisation (called client in SAP language). 
Vue?
I feel like their episodes are too long. Otherwise got to agree with you
It's a fun show and I have been listening to it for almost a decade. Recommended for casual dev chat on a lot of topics, not just dotnet. I do wish they would get a little more technical sometimes. I don't remember the last show that I 'learned' something from listening to it, it's pretty much entertainment value only these days. I do still enjoy the insight on MS direction from some pretty great guests, particularly the shows with the folks high up the .NET/ASP.NET food chain.
Not only are they too long, but there's too much "banter" at the beginning before you actually get the meat of the podcast. Coding blocks is on my subscription list, but when I'm &gt;5mins in and they're still chatting away, I get bored and switch to something else.
Glad I'm not the only one. While they're topics are interesting, i can't find a 2 hour block of time to listen to a podcast straight through. Its also too much "fluff" which they invariably put inbetween a lot of the actual content. It seemslike it could be edited down to 45 min -1hr and not lose anything imho
You better be sure you need event sourcing. I personally strongly believe that any RDBMS system will be better for 99&amp;#37; of customers. Very very few people truly need the level of latency and throughput that ES enables \(you still need to work for it, naive code is extremely hurtful in ES contexts, usually because we've all dealt with naive SQL code, but naive ES code is just different\). Personally, I never want to use it again \(unless actually purposeful, which as outlined above, I believe to be the case for almost no\-one\). When you hear ES, it's 99&amp;#37; because of developers wanting to "play around" which is an horrible reason for driving tech choices.
I really like the content of coding blocks... I find it a little more drilled down than the very high level DNR. DNR is a good podcast (for me) to learn the 30k view of new tech i've never heard of. But I find coding blocks helps with learning more about concepts that I'm already familiar with. That being said I like both, but I find CB is first priority for my listening on my commute.
Sounds like a class that needs to handle multiple events isn't SOLID anymore. Single responsiblity :)
That’s a very good point. I got the idea from some block post and am thinking that one class per handler is best. Listening to dotnet rocks podcast today they talked about keeping each class as small as possible to make the code more manageable for the next developer. The project I am working on is full of huge worker services that do everything and are difficult to test. My plan to implement cqrs is to simplify that and make it testable. Thanks for the pointer.
We're actively trying to trim down the news/intro thing, but we just love doing it so much so it's tough! We are some chatty folks :)
Nice. I'm really enjoying rider. I've dumped both vscode and visual studio. Aside from a few minor quirks its really lived up to it potential.
I agree in principal that ES is often not necessary. But it does force you to consider a highly decoupled architecture which in large projects is helpful in the long term. A scenario we use it for is creating read models on our audit logs. To aggregate and display the data more quickly to the UI, since parsing logs at read time is much more costly than parsing each log that comes in when it comes in. With ES the CQRS patten becomes much more engrained in your thinking about problems. For instance a physician list for search that used to be in a SQL database can now be offloaded to the right persistence tool (in my opinion): Elastic Search. By thinking through business logic as a series of events I feel like you can also build a better domain model understanding of your business. In addition the process of Event Storming can also highlight where things should be structured in your code. I’m aware that it’s got a shiny new toy feel to it but I could see ES flourishing in the realm of micro services and domain driven design. 
We really appreciate the feedback, and we'll keep trying to make the show better! And I agree with you on DotNetRocks...they are pros! I think I've been enjoying their show for 10+ years now, and they're a big part of the reason we even have a show.
&lt;3
Fills a niche I guess. 
In your startup class, you can inject the IConfiguration object. This object is the ConfigurationManager of .NET Core Here's an example: assuming your appsettings.json looks like the following: `` { "Data": { "DefaultConnection": { "ConnectionString": "CONNECTION_STRING_HERE" } } } `` `` public class Startup { private IHostingEnvironment CurrentEnvironment { get; } public Startup(IConfiguration configuration, IHostingEnvironment env) { Configuration = configuration; CurrentEnvironment = env; } public IConfiguration Configuration { get; } public IServiceProvider ConfigureServices(IServiceCollection services) { var databaseConnection = Configuration["Data:DefaultConnection:ConnectionString"]; // use a custom object that can be injected in your application using IOptions&lt;TObject&gt; services.Configure&lt;TOBject&gt;(obj =&gt; { obj.DatabaseConnection = Configuration["Data:DefaultConnection:ConnectionString"]; // add more properties here }); } // other methods go here } ``
Unless I'm missing something I still don't see how I can access the configuration in the class library.
I want to access the configuration from the static class
I guess as a back-end developer I'm always thinking about scaling performance. EF is nice for small/medium projects or if used as a micro ORM.
Are you saying it's more trouble than it's worth? Just with a lot more words?
I have a static HttpClient in my library and want to set the BaseAddress using the config so that I can change it for different environments. Is this bad practice? I appreciate your help on this, but I'm still missing how the library can access the MVC's ConfigManager when the library is referenced by the MVC app.
EF might get faster over time, but my SQL will always be fastee because I can add or drop indices on the fly.
You might want to look at ASP.NET Core 2.1 HttpClientFactory https://blogs.msdn.microsoft.com/webdev/2018/02/28/asp-net-core-2-1-preview1-introducing-httpclient-factory/
Would love if you could elaborate? Do you prefer something else (psake?)? Nothing at all? Or are you just not a fan of the Terminal?
I think you should probably make the class non static and use DI to add a singleton instead ( services.AddSingleton ). Then the class can have IConfiguration as a dependency that gets injected. 
Every time I have seen it used in a CI/CD pipeline it has been more trouble than it is worth vs just using straight up MSBuild / dotnet build. Usually I see it implemented because they felt that MSBuild was too complicated and then built a monstrous set of scripts out of Cake. You then have to carry that dependency forward into the build server and Jenkins/TeamCity/etc MSBuild is a perfectly capable tool that can be supplemented, if needed, by PowerShell or Bash scripts. Cake has always been a trade off of ease of initial implementation and pain in the ass over time, but that is just my experience, if it works for your team I say go for it!
Glad it is working out for you, it never has for me over time. I've worked at places where we have multiple projects/products being all piped through the same CI/CD pipeline and some teams chose MSBuild/dotnet and some chose Cake and the Cake ones always caused trouble because of the rework that had to be done to supplement those. Cake is definitely easier to implement, but it gets easily abused, and when it doesn't work it is cryptic as to why. I've personally just found that sticking with MSBuild/dotnet and supplementing with Powershell/Bash where needed has been consistently successful over time and in large complex CI/CD pipelines. I think if you can make Cake work for you that is excellent and if it makes your CI/CD pipeline easier to manage in the long term go for it.
We go back and forth on how much banter to do... when we cut it back, people complain... and when we go too long, people complain. 
Thanks Andy! We have fun doing them.
Drinking tea from mine right now...
Thanks! Yes, please do... otherwise we might have to get real jobs.
My comment was actually referring to Coding blocks, I think .net Rocks is one of the better podcasts in terms of banter - just enough to keep me amused, but the focus is very much on the topic being discussed. I think you guys do a great job on the podcast, but as with most things you're never going to please everyone. 
Big fan of Vue but OP would still run into the same problem with multiple devs although I have found organizing Vue with components makes them easier to manage and maintain especially for new developers. 
That I agree with, more or less. 
I shifted away from static classes when I started using DI. Core has DI built in so it a natural fit 
I don't get this love affair with CQRS. We used it with our last project, MediatR specifically, and it was a disaster. First of all, one-method-per-class does not make the code easier to read. It just makes it harder to find what you're looking for. Other problems: * You can't directly follow the logic of a request because the mediator is a black box * You can't tell at a glance which endpoints have been implemented and which are stubs * You can't check methods for pattern violations without opening countless files. * etc. etc. And what do you gain? Well since ASP.NET already has a pipeline you can tap into not a god damn thing besides more useless busy work. The only positive thing I can say about it is that it dramatically increased my billable hours.
.net core really doesn't want you to do this. The configuration is supposed to come from the "outside" (be injected, really), and that pretty much excludes using the configuration from static methods (or classes, same thing). Note that the .net configuration knows only the config file. Core knows pluggable configuration providers and they are plugged in by the 'end user', the program code.
Hey, RirinDesuyo, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Cracks me up that I read this in your voice :) The show is great, I doubt it needs more nitty gritty, I am not sure that's the purpose of it. My comment was not so much a request, more of an observation for people new to the show. I love what you guys do and I have plenty other podcasts in my feed that get into the weeds. 
I appreciate that - always thinking about the best way to communicate these ideas. And yeah, I sound like that *all* the time.
&gt;We’re currently planning our next major .NET releases and would love to hear your feedback on how you interact with .NET Framework and .NET Core today. Please fill out the survey below and help shape our next release by telling us about your biggest challenges and needs. It should only take 5 minutes to complete! &gt;#[Take the survey now!](https://microsoft.qualtrics.com/jfe/form/SV_8dJQUWHTAsoRWst)
Any change in your model will have that same affect with Razor. The main difference is getting rid of non full stack devs. 
Here is what I ended up doing In my class library I added this: public static class Startup { public static void Configure(IConfiguration configuration) { ExampleClient.Configure(configuration); //others here... } } public class ExampleClient : HttpClient { private static ExampleClient instance; public static void Configure(IConfiguration Configuration) { instance = new ExampleClient(Configuration); } private ExampleClient(IConfiguration Configuration) { this.BaseAddress = new Uri(Configuration.GetSection("AppCopnfiguration")["P2pUrl"]); } public static ExampleClient Instance { get { if (instance == null) throw new Exception("Not Configured!"); return instance; } } } In my MVC project I added this to the Startup class: public void ConfigureServices(IServiceCollection services) { services.AddMvc(); services.AddSingleton&lt;IConfiguration&gt;(Configuration); ClassLibrary.Startup.Configure(Configuration); } As I add other classes that need the running applications configuration I will add them to the business layers Startup.Configure method. Any input or suggestions would be appreciated. Thanks 
Check out Blazor, you can write client-side C# code. It's not production ready, but it's pretty fun https://blogs.msdn.microsoft.com/webdev/2018/04/17/blazor-0-2-0-release-now-available/
&gt; Then theres also vue.js. love me some vue.js. I normally just throw vue.js into a razor vue and use it for small client side stuff like form validation and api calls. It's my go-to at the moment.
I love c#. Sadly it's useless on Linux. 
Why, hello sir. It is an honour to be discussing drinking vessels with you.
The honour is mine! Actually, we're going to be retiring the mug for awhile... we're giving away Music to Code By for comments these days. We had fulfillment problems with the mugs, seemed like a good time to stop, straighten things out, and decide how we're going to go forward.
I'm currently working on a project which is all about DDD and microservices. It spans a fairly large domain on which at least 40 different bounded contexts have been identified. Each represented by one (or more) microservices all talking to eachother through events. Because we anticipate an imbalance in read and write operations CQRS seems logical to us. Because we already have some infrastructure in place for eventing and because we know users of the system will want to know what events lead up to the decisions the software will make we also store all events. From that was a small step to eventsourcing. Why eventflow was chosen I do not know (I came on later), but I do know it is serving us pretty good. It does have some minor quirks (mostly to do with how it (de)serializes events), but nothing we can't handle. 
Would love to have a modern UI framework for desktop Windows ......... that isn't required to be sandboxed in the Store.
Hey so I’m sort of in the same boat and wanted to put tests around the startup class as well. Been running into issues as most of the methods called are extension methods. Does anyone unit test the startup? If so, how? 
Uh....
C# works fine on Linux. Both Mono and .NET Core will run over there.
Since when does UWP/XAML work outside of the Store? Do you mean sideloading?
You literally just double check an APPX file and it installs. And since the Anniversary Update.
I tend to keep my controllers very streamlined, usually all it does is call a service method and all the logic sits in the service layer (business layer). Based off that I would most likely just test the services and moq the repository (if being used) to return now data for each unit test scenario.
Not just the current System.Management APIs. I feel the system.management is lacking a lot of things that have been added to powershell and it would be nice to have an simple equivalent. For Active Directory I have to extend user and group principals to get basic properties like department or convert the sid to get the manager, drop down from DirectoryServices.ActiveDirectory to DirectoryObject to manage DACLs. I have 300 lines of code to do basic one liner powershell in DFS I have 400 lines of code for basic one liners with AD Users/Groups We had to pinvoke to get basic file share info. I spend more time writing code to get basic management working than what my main task is. Items like powershells Microsoft.ActiveDirectory.Management should be available add as a reference to .net
.NET Core is awesome. Long time coming. The API changes and newer conventions along with the elegance of C# make working with the technologies quite enjoyable. I've been interested in Linux for a long time and .NET Core's cross platforms features have made that possible, though there's nothing out there quite as good as Visual Studio. I tried VSCode which is fantastic, but it lacked the ability to perform the productivity gains I had with VS and ReSharper. I Ended up going with Jet Brains Rider which is quite close but cost me a few bucks and still feels clunky in comparison. I'd much rather be using Visual Studio and would love to see it come to Linux some day. I'd pay for a license.
Microsoft themselves are moving away from TFVC in favour of git and even [built an entire feature into git](https://www.visualstudio.com/learn/gvfs-design-history/) to make it work in exactly the scenario you described.
A little late to the party, but: None of the issues you're describing are specific to TFVC. There's nothing about either TFVC or asp.net core that makes either incompatible with each other. What you're seeing are just the limitations of TFVC. Even git, to a degree, marks moved files as deleting 1 and creating another, it's just a bit cleverer about it.
Pretty unlikely... Mark and Karen live in Costa Rica these days, making coordination of the four of us even harder. Maybe when they come to visit New London, but don't hold your breath...
We do a lot of cross platform development, so having the same script work across environments using bash/PowerShell/Docker is really where Cake shines build.sh/build.ps1/dockerfile only contains what differs between environments, so essentially nothing is duplicated. Especially useful with multiplatform dockerfiles where we instead of bash / cmd dialects have essentially the same file just invoking Cake. Code is built, versioned, tested, etc. the same way regardless of environment or platform. As Cake can load scripts from nuget we've for many projects introduced recipes so we easily can share how we build, version, tests etc. between projects, turning the local "build.cake" into essentially a one liner for most and to just a couple of lines extra stating potential exceptions from conventions. Which means if we introduce a new step i.e. code signing then all projects can benefit from that just by pinning to a new version of the recipe. 
I can't imagine anybody complaining about .NET Rocks banter. It's the gold standard!
&gt; micro services and domain driven design. And very few projects really need micro services.
Visual Studio for Mac isn't the same thing as Visual Studio. VSfM is a [rebranded Xamarin Studio](https://arstechnica.com/information-technology/2016/11/microsoft-is-going-to-pretend-to-release-visual-studio-for-mac/). VSfM could conceivably come to Linux, but it's not the same as saying VS will come to Linux.
Can't argue with that 😜
Yes more nitty gritty please 😀 I love .NET rocks but my head associates it with TWiT \(This Week in Tech, a tech show with mostly chats\).
Right. VS proper is still very COM-dependent in many areas and would take quite a bit of effort to implement outside of Windows. I think the more likely scenario is seeing other apps like VS Code and VS for Mac offer more VS-like features and user experiences.
Services in general were underrepresented in this survey. 
This. There's basically no path forward from WinForms. I'd love to see some incremental path to something more modern that doesn't involve a complete rewrite of my UI code
Ide. Iis. Documentation. Anyway. I love c#. 
When you said IIS what do you mean?, you can run asp .net core on linux. https://docs.microsoft.com/en-us/dotnet/core/linux-prerequisites?tabs=netcore2x
It looks like you'll probably get the result you want, but if you could also add the utilities of that class library in the same way you added the configuration, then you'll avoid the global access all over your application to the class library. Accessing things through IoC has disadvantages (it IS more verbose and that feels weird and complicated at first), but after using it a while, you might be able to learn how to to testing and might come to appreciate how the dependencies of a class are more explicit. That's a lot of strong statements to not follow with examples, but I need to get to bed. Sorry.
I wish there was ldap support for Linux 
The short answer is yes. Ive been using .Net Core for a while now and it's held up very well. Your APIs will return models and you need a simple way to keep all those models in sync across applications. Otherwise you will have to depend on dynamic objects and that gets messy if what your looking for is not there.
It would be best to keep those models in a separate class library so thy can be reused in the API and in the apps itself. Web API is just for delivering data to and from the apps
Your controllers should be small, but you should still test them. I have not used the UserManager class, but see if there's an interface you can use instead of the concrete class. Then you can mock it and make testing extremely easy.
Microsoft never used TFVC.
VS Code. Nginx in front of Kestrel running .NET Core. Documentation is really good for Core and nothing changes on Linux as long as you use the CLI. 
I disagree with that. They both route requests and responses to views. Theyre both mechanisms for generating views except that with angular you are now doing MVCeption.
You are right that they should be very thin and limited, but that doesn't mean they can't be tested. You are thinking about the pattern where you test the model but essentially ignore the views and controllers part. An alternative to that is to actually test all of the controllers and then you end up with a test suite that verifies your application can answer all requests in the expected way - that's a perfectly sensible, wise way to test.
Have you checked this? https://docs.microsoft.com/en-us/aspnet/core/client-side/spa-services?view=aspnetcore-2.1#server-side-prerendering
Okay sure, but they're using git for nearly everything (if not everything).
IIRC this is functionality is planned with proper edit/resume coming to Roslyn. No need to interpret anything, just hot-patch and re-JIT the modified code on the fly. Then, disabling view-precompilation in debug mode fixes the ability to edit views in asp-net core at runtime.
That's not correct though. With aspnet you *can* route to views, but you can just as early route to REST endpoints. The controller's are the same in either case, the routing engine let's you set it up however you want and you return whatever you want. Asp.net core is exceptionally suited to building API's, that's why the two technologies are a good fit. 
LDAP is a huge one for us. We have an organisation-wide LDAP database, and being able to use it as a user store for out OpenId Connect SSO server would be awesome. It's basically the last missing link. Hopefully a totally managed implementation on top of netstandard is possible, getting access to LDAP on Windows would still be really useful.
It is entirely correct. Your assertions are equally valid for angular. Asp.net core is exceptionally suitted to building cross platform applications. Thats why you have asp.net core mvc and asp.net core razor pages that sit on top. Angular is a good fit for technologies that do not have their own routing. Just look at this post alone to see how many problems people are encountering from using angular on top of server level routing.
[Yes there is](https://developer.microsoft.com/en-us/windows/bridges/desktop). You can sandbox them within a UWP container for incredibly easy installation and uninstallation. It's still a winforms app, but it's a place to start porting code onto more modern apis.
Angular isn't for building API's though. Unless your site is completely static or relies entirely on 3rd party data, you'll need some kind of backing API - which asp.net is perfectly suited for. People having issues combining both have likely cross concerns and that's why it doesn't work. You aren't supposed to return multiple pages from asp.net to your angular app, you're ONLY supposed to expose REST endpoints for Angular to query when it needs to. The point that you and others don't seem to realise is that the Razor/View engine in Asp.net is *entirely optional* - you don't use it if you don't need it. 
You keep saying asp.net. im referring specifically to asp.net mvc. Asp.net is just the underlying framework/architecture. You are exactly right in that you are really only supposed to consume api / RESTful endpoints from angular. There is no need to be involving mvc in all of this. Though .net core has gone ahead and made mvc and web api use the same routing layer because people are used to doing it that way.
Well, you don't have to implement MVVM or similar pattern in every application. If you just manage UI events in code behind, the leap from WinForms won't be so daunting.
&gt;You keep saying asp.net. im referring specifically to asp.net mvc. Okay, so your entire argument boils down to semantics. Let me refer you to here: https://docs.microsoft.com/en-us/aspnet/core/mvc/overview?view=aspnetcore-2.1 &gt;ASP.NET Core MVC is a rich framework for building web apps **and APIs** using the Model-View-Controller design pattern. As I said earlier, there's nothing in MVC that requires the use of the razor engine, in this context the "view" is your API response, not some HTML - but all the principles of MVC still apply, the routing engine is still in place and still used - and unless you're doing something silly, it in no way influences or conflicts with the routing in angular. 
XUnit isn't the appropriate way to test the view. That should be done via a UI testing framework. Likewise the little amount of controller logic that needs testing should be tested against actual clients (e.g. browsers) as they behave differently in subtle and often surprising ways. 
1. Start the website 2. Invoke a REST endpoint from a test 3. Optional, but recommended. Have (2) run connectivity tests with databases, etc., to verify the configuration. The only way to actually test your startup class is to actually run it. If you start mocking stuff out you aren't testing the code that you are actually using.
My argument boils down to semantics? Lol. Okay my dude im out. Best of luck to you. 
&gt; I have not used the UserManager Um, you might want to get at least a little experience with the platform before you start making pronouncements about what should or shouldn't be tested. 
I didn't mention the views and you've entirely missed my point and the point of mvc. By saying you "test controllers in the browser" you haven't understood what a controller is. It's nothing to do with the browser - it's an independently testable component, divorced from any rendering technology like a browser, and it's output represents how your product answers requests using its dependencies. Not http requests, which is where you have become confused - requests.
What would that prove? That the compiler works and it actually invokes the method that you typed?
&gt; Secure design (such as application sandboxing) should be compulsory. Why? &gt; And UWP doesn't require the Store. But UWP is half baked, e.g. doesn't support F#. 
&gt; VS Code Grim. Provide a decent IDE like VS2015 was. We've had to downgrade our entire company from VS2017 back to VS2015 because it was so much better. Microsoft don't have a viable alternative on Windows, much less Linux. 
&gt; If you are looking for an ide then use VS Code VSCode is just an editor, it isn't even trying to be an IDE. 
&gt; What are the most important aspects of an application framework for you? Please order the items below by drag-and-drop (1 = most important) Followed by a list of irrelevant/unimportant features to choose from. Most important aspect of an application framework for me is reliability followed by GUI capabilities. I had to Google some of these "aspects" because they're so obscure... 
You generally want to have separate models for each project. So even if they can be shared, they shouldn't be. It's more work, but there's going to inevitably be a point where one model needs to be changed specifically for one project and it can cause problems for other projects.
Seriously. Look harder at what I wrote. 
Awesome. This will be impacting the Xamarin experience as well. The hot reload in Flutter is simply amazing.
Ew. That’s distressing. I thought they were really making an effort at porting Visual Studio proper to other platforms. Consider my comment withdrawn.
Depends on if there is any logic in there. While you may forward your call in 1 line of code, you may be doing something with the data before returning it to the client that is UI specific. Plus, some people want as much code coverage as possible.
Genuinely curious, what does VS 2015 do that VS 2017 doesn't in your experience? 
Completed the survey. Keep making .NET core awesome Microsoft :)
This is more integration testing. Unit tests consist of mocks for a reason, and you are ensuring that the code is doing what you want it to do without actually running it. I was really looking for ways to test the startup and ensure that the site is going to start the way I expect it to. 
You can have a look at https://zetpdf.com. using this you will have more control of the PDF and you can control whatever you want.I've used and overall satisfied.
True. I miss attributed vs code as an ide. I guess the better way of putting it is depending on the installed extensions you can have ide like features but not a full fledged ide. 
&gt; This is more integration testing. Stop right there. Every time someone says that they are thinking about testing totally wrong. The question is "what's the correct type of test to ensure this code is correct?". If you jump right to "how can I unit test this" without asking that first question you are setting yourself up for low quality tests.
Would you recommend to switch?
&gt; Why add complexity and dependencies to your tests when you can unit test and ensure the code is doing what you want? 1. Because you can't ensure it's doing what you want. 2. Therefore you need those integration tests whether or not you also have mock tests. The question is rarely integration or mock test, but rather integration only or integration and mock.
So what? The quality of the test is what's important, not what arbitrary classification you label it with.
'delete'
I'd say this depends on the situation. If your WebApi is going to be used in lots of projects then maintaining separate copies of the models can compound the work required to maintain them.
No need to get defensive. It all depends on the implementation as to what types of tests you write. Both are equally important.
I guess the delete function is broken. :-p Stupid bot. Anyone know where the author(s) keep the repo so that I may make a PR? I didn't see it on Github or Google.
I am familiar enough with dependency injection to know that mocking an interface will be orders of magnitude easier than working around a concrete implementation of a built-in class. I've never used it, doesn't mean I don't know what it is.
My interpretation of what he said was more about keeping a single library for the data transfer objects, which makes sense. You would just need separate models in each project for after you've converted them to whatever model your project needed. That said I tend towards dynamic objects at my app borders so I tend to skip the dto lib all together. But that was my take on what he said.
Your in-house productivity apps could allow someone to compromise your employer!
Two seconds of looking at the class definition would clearly show that you should not mock UserManager itself. 
They're employees. They already have access.
I like to have shared models and then have each client use the models as needed or convert the models to their own as needed. Things in the request/response - in my mind - can be shared. If the client needs the data organized in a different way, I usually just use extension methods to convert the shared model into a client-specific model. Many times I can use 80% or more the same models to help code re-use.
This is a better version of what I meant to say. Should have read all of the replies before doing a worse job explaining it!
Work
1. Not all your employees can access all things. 2. Attackers can generally successfully gain access to employee accounts.
I have used Cache manager previously https://github.com/MichaCo/CacheManager
Both of your statements are true but neither has any bearing on these apps. The apps don't have access to anything that the user doesn't already have access to. They cannot be used by anyone to gain any extra privileges. If a user is compromised the app is irrelevant. For example, the last app I wrote helps people in the marketing team to identify relevant keywords from a document. They give it the document (that they already have access to and the app does not) and it suggests keywords using the RAKE algorithm with a simple WPF GUI. Worst case scenario, an attacker can steal the app but we don't care if they do. I'd distribute the app as open source if I could be bothered. 
I've not used it. But it sounds like their version of [Elasticsearch](https://info.elastic.co/branded-ggl-elastic-exact-v3.html?camp=branded-us-ggl-exact&amp;src=adwords&amp;mdm=cpc&amp;trm=elastisearch&amp;gclid=CjwKCAjwwuvWBRBZEiwALXqjwzX9B8ohukQcgMMioU_Hvkt9oMK6nQ8AuY8qU429pi4ZqbLLY47vrxoCXOkQAvD_BwE), yeah?
Very true. It brings a level of complexity you really should avoid unless you have very good reasons not to. But this goes for CQRS and eventsourcing as well.
To be honest I did not know this existed but I could use this in a couple side projects. Generally I have used elastic search or some other lucene based sytem for this in the past.
Lucene.net
From your past experience, what would you recommend; Elasticsearch or [Lucense.Net](https://Lucense.net) ... basically we have a SQL Server database that has thousands of thousands of records that we would like to expose it for instant search?
If I'm deploying an app and need it local, lucene. If it's a web based app I tend towards elastic. I think on our main elasticsearch server we have a couple million entries on a pretty crappy server. Runs really well though. Biggest pain is configuring your index properly but if you use NEST, it's pretty easy.
Lucene is great because it does things like: - Query types inc: phrase queries, wildcard queries, proximity queries (hamming distance), range queries, boolean operators, and more - fielded searching (e.g. title, author, contents) - sorting by any field - multiple-index searching with merged results Meaning if you want to create a web-site search engine, you can essentially re-produce Google/Bing, spellchecker and all. You just run the search using several of the query types then merge the results giving you an incredibly flexible and elastic user experience (i.e. "typo every word and still get a good result"). A site I helped deliver has, in my opinion, a better search experience than many major consumer sites a hundred times its size, and I put full credit to that at Lucene's feat. Arriba seems fairly immature by comparison, supporting basic boolean operators but little else. It will "work" but when Lucene.Net is easily available the question is why not just use that? 
Precisely how I use it, but because it’s powering the search for a site that contains items titled in several languages and a lot of esoteric jargon, I automatically return the spell-checked results rather than including a suggested spelling link. Likewise, I’m running a site on a single server that offers better search performance than most major online retailers. Lucene is extremely mature (though the .NET port isn’t without its quirks), and I can’t fathom using anything else.
Have you looked at SQL Full Text Search? If your data is otherwise relational and you have just a few columns that need searchability in a known language it does a great job at speeding up queries. 
Yes. I'm using it in another project. The thing is each record has \~10 columns \(types; text, numeric, datetime\) that are searchable. FTS is more towards character\-based data and hence I think it is a recommended option for such kind of search.
Sounds great! Will give it a try for sure. Thanks a lot.
We use elasticsearch to store 1.5m documents which can be searched on multiple ways from your standard. A user can have a predefined list of filters which get built into an elastic query. A user can have predefined locations (latitude, longitude) which can be used to filter the documents in a certain distance from them. I'd recommend elasticsearch to anyone that needs fast complex searching.
What’re scoped packages?
Yeah just not the same though
Very descriptive. Been using VS 2017 across many servers with our team and zero issues. It's quicker and more feature packed. You complain about it but don't explain anything to people asking to see if they may help you.
In addition to other suggestions, you might want to check out Azure Search if you don't want to manage your own backend.
Didn't even know this existed, to be honest. But for anyone using Lucene, here is another tool worth looking into: https://flexsearch.net/
Latency may be too high. On one of my production systems, we have had about 50k accesses to table storage. duration is: 50th percentile: 35ms 90th percentile: 97ms 95th percentile: 152ms 98th percentile: 317ms 99th percentile: 395ms Redis in Azure is closer to 1-2ms.
I wish I could give this more upvotes. Sadly I think most people are too addicted to onion architecture to appreciate this mentality.
Then sandboxing the app properly should hardly impact you. :)
Packages where the organisation is part of the name of the package. Prevents naming conflicts and allows multiple forks of the same package to exist.
I think using a TestHost is stupid. Better to just host on HTTP and create a client Lib accessing it. That way you test both: the service and the client.
I used the TestServer for an OWIN-based Web API service in .NET 4.5 framework a couple of years ago, through the https://www.nuget.org/packages/Microsoft.Owin.Testing/ package, so this technique doesn't require .NET Core. 
The real question is what do those tests really offer over classic UTs for controller actions. Realistically, just a thin layer of making sure that all the middleware works; this can be critical but due to the nature of middleware, if it doesn’t work, then for the whole app, so this becomes very apparent very quickly. Neat package, though.
This seems applicable when developing an application in form of a microservices architecture. With these tests, teams can ensure they are not breaking the exposed API that is consumed by other services or API gateways. Esspecially if you're using consumer-driven contracts.
Yeah that’s exactly the system we use. The way we do it is writing reflection UTs for all interop models, and all of those are in a dedicated nuget :)
But then you are dependent on outside factors. What you propose is better suited for system tests, IMO.
Yet it does (e.g. file access) so I don't want it imposed everywhere whether it makes sense or not.
F#. They know full well it doesn't work. They break it every couple of weeks. Big picture: whomever is calling the shots doesn't care.
Can you give some examples of why it is more trouble than it's worth? We, (the Cake Team) are definitely open to suggestions on how things can be improved.
Can you confirm which extension you are referring to? The Visual Studio or Visual Studio Code?
I consider web api as another UI project. If it isn't UI specific, I put it in a class library in case I may want to utilize something in a different UI.
Requesting file access is not difficult. And an app should only have access to the files the user intends to work with.
Fun fact: Article today says they're adding in the next version of Windows 10 the ability for you to set up automatic updates for UWP apps distriubted outside of the Store.
Yes, it’s definitely a bummer that it’s always (well I haven’t checked for the last year or so), a full version behind the Apache build.
UWP apps can request and utilize full non-admin file system access.
.NET Framework apps don't need to.
UWP = .NET Besides, adding the capability is a one-liner: &lt;Capabilities&gt;&lt;rescap:Capability Name="broadFileSystemAccess"/&gt;&lt;/Capabilities&gt;
.NET Framework supports F#. UWP doesn't because a variety of fundamentals are missing or broken.
&gt; UWP doesn't because a variety of fundamentals ~~are missing or broken.~~ changed and I hate change 
I thought that Lucene.NET is dead? The last release was 4.8.0 beta from 2017-10 and thats many major versions behind the Java version (7.3.0). There also doesn't seem to be repository activity since then. The .NET ecosystem really needs a library like Lucene.NET which is really up-to-date, but I guess that it's an extremely huge undertaking for only one person.
My objective is to maximize `test coverage / test line of code written`. Testing both the client and server at one has huge advantage. I never once had problem with just using kestrel in my tests.
&gt; php Abort. Abort. Sounds like exactly the same mess that got created in my workplace. The more experienced developer only knew ADO.NET which meant instead of a proper database using Entity Framework we ended up with Excel sheets he reads into datatables and a whole lot of spaghetti.
You're going to get biased answers on this sub due to the fact that .net core is a dotnet technology. That said, PHP is old. That's not necessarily a bad thing, old also means well understood, well tested, well used. .net core is relatively new. It's still evolving, still has a few niggles being worked out. However, it's hard to make an objective reasoning between the two without some kind of requirements from yourselves. What is it you *need* from your framework choices? Ultimately, as you say, it sounds like the only reason the more senior developer wants to use PHP is because it's what he knows. In my experience, some senior developers simply don't like learning a new technology and it sounds to me like he doesn't want to move away from what's comfortable for him. That's too bad for him, though. - what's comfortable for him isn't the issue. This is a **business** decision. What works best for the *business*? If I were you, I would go to your boss and suggest that .net core is objectively *best* for the business. Don't mention about your worry about learning a dated technology, instead pitch it as "If &lt;senior&gt; ever leaves, it's going to be hard to replace him as Symfony developers are hard to find, whereas there's a booming market for .net developers". Remember, even experienced .net (not core) developers will pick up core no problem. Furthermore, .net core will beat PHP's performance any day of the week. That doesn't just mean a faster (=better) app, that means you need less hardware to run it - i.e. it's cheaper for the business to host it. Finally - and this is a big sell - .net core is supported by Microsoft. It's got the backing of a huge company that isn't going anywhere, it's also used by a lot of fortune 500 companies out there. Symphony is owned by SensioLabs, a much smaller company and its biggest customers don't seem to be in the same league. Look at your requirements and see which works best for the business. Break it down to costs at the end of the day - businesses love putting a $ figure on something. If you can work out what it'll cost the business in terms of time (and time == money) over 3 month, 12 month and a 36 month period, you'll be able to make a much more informed decision. 
My sense in these cases are that developers often tend to take sides with the languages they know well and make up reasons that are usually kind of excuses for wanting to work with something you are comfortable with. And working with a language you know well is a good reason to choose that one. If you have a more experienced tech lead it makes sense for him to choose a language he knows well since he will probably take the main responsibility of setting up architecture. Something that can go badly if hes working in areas hes uncertain about. That being said unless your working with specific integrations there are few things that cant be done by both dotnet and symphony. But dont be afraid of trying out a new language. And dont listen to people wanting to dismiss all PHP development just because theyve got bad experiences with it. Just because its easier to get into than other languages doesnt mean it cant be done in a professional way.
With .net core you can do a lot of things. With PHP only web
Event though i prefer dotnet over php i think perhaps someone should play devils advocate here. PHP is old, yes. But only 5 years older than C#. Of course c# and dotnet has changed a lot over the years. But so has PHP and Symphony. Perhaps not as much but then again dotnet started out much worse. As for dotnet being cheaper or faster to run I would say is a questionable fact. In some cases probably but if your comparing with the prices for shared PHP hosting I have yet to see someone offering the same kind of prices for dotnet. As for speed perhaps the case can be made stronger yet ive yet to see a site that was slow because of the language and not the (badly written) database queries. As for being supported by microsoft, i dunno. I like what they are doing with dotnet but thats one of they few places Microsoft isnt screwing up everything they touch (used skype lately?). As for being used by a lot of companies, yes dotnet is more standard in big corporations, but its market share pales in comparison to PHP and among the top tech companies neither PHP nor dotnet is very popular. And as for leaving for the choise of language to the business and economy side of your company. Well im sure that will work out nice for you. Im sure youll end up working with COBOL since someones mom said that was a good language. :)
I actually agree with a lot of the points you are raising. You make an excellent point about the shared hosting costs for PHP, but at the same time I wouldn't run any kind of business critical software on shared hosting of that kind. It's an area I do hope Microsoft convinces others to adopt though - you can actually get a .net core app to run on a run-of-the-mill cpanel shared instance, but it's a bit of a chore. Wouldn't take much to make that easier.
Personally I'm using Amazon Hondalé. I find it muy calinté.
Of course depends on the size of business your dealing with. And with the larger businesses i've worked with the hosting costs are usually pretty insignificant compared to the development and licencing costs anyway. So much so that it is often more cost efficient to solve performance bugs by inreasing the server power than to let a developer solve the bug.
Major reason against PHP+anything is that it involves PHP. https://eev.ee/blog/2012/04/09/php-a-fractal-of-bad-design/ The blog post is an old one, but entertaining and should still have quite a bit of relevance.
I've used PHP for years (as well as .NET) - but it's no comparison to .Net Core which I believe is far superior in every way. Advantages of .NET Core over PHP/Symfony 1) C# Language - Definitely my favorite language. It's always well-received with teams that I've worked with in the past, and you can use it across many platforms. Plus it uses static typing which is great for teams to help reduce errors whereas dynamic typing for things other than smaller projects/scripts tends to get messier over time. Unfortunately, PHP as a language hasn't got much better over time. I've been using it for over a decade but I still have to lookup basic array functions because some of them have the needle argument first where others have it second. Like any tool, you can write good PHP code with discipline but it's much harder than C# especially with multiple developers in the repo. 2) Backed by Microsoft - .NET isn't going anywhere, it will only continue to grow. On the other hand, the future of Symfony likely can't be considered nearly as reliable. 3) Tooling - Microsoft has been amazing lately with open source and quality tools (many of which are free now). Visual Studio is still the best IDE for me and the integrations with C#/.NET are excellent. Nuget is far better than any PHP package manager I've ever used. You're never short on tools - there's even 3rd party projects like Cake that lets you use C# for build scripts too. 4) More than just a web framework - .NET code has frameworks for every platform you'd ever need which all nicely tie together to be managed as a solution with Visual Studio. You can create DLLs to easily share code code between projects which does wonders for organization. Symfony is purely a web framework and can't do anything else. 5) Cross-Platform - You can run .NET Core web apps on Linux now. And even in Docker containers on Linux servers. One of the main reasons I used PHP during my early years was lack of cross-platform and expensive hosting (also not the case anymore). I could go on and on, but yeah.. definitely .NET Core if you can. I enjoy developing with that environment so much more than PHP now.
If I had to make up an excuse for using .net core over symphony it would be that the new symphony (4) is really a barebones framework, it breaks backward compatibility with most of the old "bundles" so you are going to make up a lot of things from scratch. Not that symphony bundles are something so great to begin with, take the fosuserbundle for instance, the-way-to-go-when-dealing-with-users. The documentation is lacking and until recently it wasnt fully compatible with version 4. Symphony uses a lot of "configuration" over code, so you are gonna have to stick your nose inside the frameworks code everytime you touch a configuration file because many things are not properly documented. I never liked it, or doctrine and now it seems that it doesn't bring anything to the table that other frameworks do. 
I could never understand why everything was going towards SPA, especially with advances in network speeds, faster server processing (especially with net core), limitless cloud capability, etc. and I just never really learned any of the SPA tech. I've been living/breathing Razor for a bit and it's so flexible. I could understand its use for certain things like banking, administration, etc. but for something like Youtube drives me insane.
.NET Core is a strongly typed, compiled language, with great debugging built in. PHP is a dynamic language, has lots of bad legacy decisions that make the APIs difficult to work with, it’s more difficult to debug, and is just so easy to write poor with. I’m just starting with .net core, but I would strongly encourage using a language like c# over PHP because it’ll be easier to maintain. That’s a business-positive mentality that they may buy into. Plus, you’ve already got work done with it.
It doesn't even support tail call optimisation.
I use both PHP and .NET Core at work. I find that they work hand in hand -- php is quite elegant in ways that .NET isn't and never will be, while .NET Core covers the need for strongly typed backend. This is *not* a binary switch. You can use both and have it work!
&gt; That’s too bad for him, though. - what’s comfortable for him isn’t the issue. This is a business decision. To be honest, that is a prt of the business decision. A lot of tools (Rails prominently brought this forward) are focused on saving developer time over otger costs: servers, tools, etc A senior developer being comfortable with a technology, particularly one so well established — Symphony is the Rails of PHP world — translates to considerable savings as they lead the project through the minefield that is development. You make a lot of valid points and i just wanted to add a bit of a clarifying perspective on that particular point. 
Yes, that is indeed a factor and one worth considering, though I'd counter it with two things: 1) As mentioned, having a much larger talent pool in the area for a given tech minimises overall risk for the company. It's never good to have tools or systems that only one guy knows or understands. 2) His *experience* should still prove valuable and worthwhile, regardless of the technology he's experienced in. It's not like developers don't have the same problems across different tech stacks. All that said, you are right that it all needs to be considered as s whole.
Can you go into more about how php is elegant?
Formal education is overrated. Online courses and pushing yourself into a hobby project are both good options. Don't forget the official MSDN documentation while you are at it.
I taught myself on the job. Started with Excel, dabbled with VBA, recognized its shortcomings and moved over to C# and created some WPF apps.
.NET is extremely accessible. Here's a checklist: -- download Visual Studio Community 2017 -- make a new Console App -- Console.WriteLine("Hello World"); -- Console.ReadKey(); -- Click the green arrow. -- Watch your first program go!
I really disagree with formal education being overrated. There are many classes a formal education requires (data structures, algorithms, programming language design) that are essential to a solid foundation of computer science. Without them, you will lack a true understanding of what is actually happening inside the compiler, memory, threads, processor...etc. Anyone can write code. Anyone can get better by following examples and grabbing code from StackOverflow. But at some point, you're going to need a much greater understanding of what you're doing. I am a lead developer at a Fortune 50 that interviews for all levels of developers - and most of the people we turn away lack the depth of knowledge it takes to be an effective developer. Just my two-cents.
Get some vocational training or do some pro bono work to demonstrate aptitude. Get an entry level job.
I always say that there are two types of developers "how" and "why" developers. Some developer just want to know how to make things work but never dive into the "why" aspect of it. They are surface level developers and only care about making their code compile and run. The "why" developers will see an answer on SO and it'll make their code work, then they'll spend the next hour or so understanding why it made it work and what else they can use it for. Formal education forces you to do both the "how" and "why". And that tends to stick with people as they advance in their career. 
&gt; I really disagree with formal education being overrated. There are many classes a formal education requires (data structures, algorithms, programming language design) that are essential to a solid foundation of computer science. Without them, you will lack a true understanding of what is actually happening inside the compiler, memory, threads, processor...etc. You could read up on those things though.
I was able to implement the server side templating part of a component ui system from scratch in under a day. Try that in ASP core. For the record, I'm a huge fan of asp.net core. I use it for pre-rendering of client-side component screens. There is a certain elegance to using php for its intended purpose- hypertext preprocessing -- though.
Yes definitely. I would love to do projects in more obscure frontend frameworks but to then go an try to hire people to work on the projects would be a nightmare. 
I agree that you need education, and would not hire anyone that did not understand the items in the first paragraph. That said, formal education is it expensive and in effective way of achieving this.
Completely doable. I start my first tech job tomorrow. I'm completely self taught. My new job is going to be &lt;50% coding, but I'm going to take that as an opportunity to grow and move into a full development roll down the road. There are so many online resources available. You just have to be disciplined.
While that's true, there's nothing you can learn in a formal class that you can't learn outside of a college. The materials available are abundant. The other thing is, OP didn't ask about being a top notch developer at a fortune 500 company, they asked about having a career. And I would say the vast majority of development jobs available do not require deep knowledge of much of anything. Throughout my nearly 20 year career, the vast majority of it has been doing basic business web apps. Insurance, medical, dealerships, etc. None of that has required any advanced math knowledge, algorithms, or any deep programming knowledge. I've had no shortage of offers and opportunities, so while a formal education might look better, i certainly wouldn't say that it's a requirement
I say learn from home and on the job. We care waaaay more about experience than degrees by a long shot.
This question comes up a lot. There is basically no one answer to this. It depends on the person and their aptitude for programming. Some people are natural problem-solvers and are able to learn on their own, some people need classes and training. Just as a reality check, hiring managers don't have time to search through the stack of resumes for a diamond in the rough. You need a bachelor's degree in computer science or other technical field to be considered for an entry-level programming job with no work experience. If you don't have a degree, you probably need one of two things - either experience with the company's product in a non-programming role (help desk, QA, training, etc), or relevant practical experience from working on projects at home.
And you can be a professional golfer without ever taking a lesson. The point isn't that you can learn special things in college that you couldn't possibly learn without it. Completing a higher education from a reputable school all but guarantees that you have been given a good foundation and you are well equipped to learn any relevant job skills. It also means you've been exposed to a lot of intangible things like specific processes to problem solve, working in a team on complex projects, creating and understanding technical documentation, OS agnosticism, best practices in arbitrary tasks like integrating projects from unrelated/incompatible runtimes, and what's going on with the specific implementation of whatever software stack you're using (.NET in this case). Plenty of things are revealed and tied together in a formal education which would probably only surface when they become problems in the real world. Modern compilers and environments like .NET are very awesome, but they allow people to write powerful code while skipping a lot of important material that you'd get from going to school. Just for example I've used Karnaugh maps several times to reduce the complexity of state machines at work, and no one around me had any idea what either of those things are until used them. After that everyone was able to look it up and learn when and why to use them, but not exactly topics that come up outside of a formal education.
And you can be a professional golfer without ever taking a lesson. The point isn't that you can learn special things in college that you couldn't possibly learn without it. Completing a higher education from a reputable school all but guarantees that you have been given a good foundation and you are well equipped to learn any relevant job skills. It also means you've been exposed to a lot of intangible things like specific processes to problem solve, working in a team on complex projects, creating and understanding technical documentation, OS agnosticism, best practices in arbitrary tasks like integrating projects from unrelated/incompatible runtimes, and what's going on with the specific implementation of whatever software stack you're using (.NET in this case). Plenty of things are revealed and tied together in a formal education which would probably only surface when they become problems in the real world. Modern compilers and environments like .NET are very awesome, but they allow people to write powerful code while skipping a lot of important material that you'd get from going to school. Just for example I've used Karnaugh maps several times to reduce the complexity and prove the completeness of state machines in code at work, and no one around me had any idea what either of those things are until I wrote the code and included the K map/truth table in comments to show where the logic came from. After that everyone was able to look it up and learn when and why to use them, but not exactly topics that come up outside of a formal education. 
watch all the videos at http://www.asp.net 
Adding to this, check out the [Bureau of Labor Statistics definition for different levels of programming occupation ](https://www.bls.gov/ncs/ocs/ocs95apb.htm#programmer), listing them CP1 (least experience) through CP5 (most experience and proven skill). In my experience - personal and what I have heard from old classmates - the first programming job someone with a BS in computer science or engineering will get after school is going to essentially be a combination of CP2 and 3 depending on the size of the company, and will be able to move up that scale relatively quickly. Someone with no formal education or prior work experience in programming is probably going to start in some variation of CP1, and it could take much longer to move up the scale because other potential employers could think your skills are specialized to your current job tasks. It's anecdotal, but to put it in perspective: I finished college four years ago and started at a small company with only two other programmers (my boss and his second in command basically), after 6 months to a year, I was sort of treated like consultant to newer tools like .NET and CP3 defined my responsibilities there pretty accurately. Then it only took two years until it felt like I was pushing into CP4 territory and it was reflected in what they paid me. I took a job with another company recently as a software engineering lead and CP4 is almost literally my job description. Compare that to my first boss, who was self taught and went from a shop employee to computer systems manager in 25 years. He told me that once he started programming for the company he was doing code maintenance for 4 or 5 years before ever being given any bigger responsibilities. Several other programmers were hired there while I was a part of that company: some older self taught guys that had at least 5 years as a programmer, and some fresh college grads. They were all paid the same, but it was much easier for me to introduce new stuff to the ones with the education because they had a solid foundation for learning new concepts (company specific and programming in general). It was also WAY easier for them to teach me stuff I didn't already know because they'd watched college professors teach things for four years. The guys who were self taught had plenty to teach me as well, but communicating the thoughts wasn't as easy for them most of the time. There are exceptions to everything, and plenty of self taught programmers out there who could code in circles around me and convey their thoughts much better than I can, but generally speaking it's much more likely someone can do that if they've got the advantage of a formal education on the subject.
Yes sir. Yes you can!
Coming in late to this but as far as I can tell, the free version doesn't stop after a time limit. I admit I used free for a week or so way back when, and then have been a paying customer long before a trial would have expired. From what I see and read, as long as you are under 7K users and okay with the limitations of the free plan, I don't think it goes away.
Thanks!
Hmm, the existence of this disturbs me. But I understand the appeal.
I don't necessarily disagree with anything you say, but your post sounds like a pretext for an underlying sunken cost fallacy. Not many people with degrees will admit to the knowledge being readily available without formal education, or seriously consider the possibility one could surf by without actually acquiring much deeper understanding even from reputable institutions.
I am hyped by System.Threading.Channels I waited for such abstraction for so long...
I started developing .Net a year ago. No prior coding experience. I have no college education and I don't even have a high school diploma, I have a GED(long story, drug addict parents). I make 95k a year plus international travel and bonus developing software for a small company. My roommate is 23 years old and went to a coding bootcamp in California called Sabio. He finished and landed his first job ever(in his entire life) making 75k with unlimited PTO. If you're serious, just spend 15k and go to a .Net bootcamp. It will pay itself off in no time. Software development is where it's at. Anyone can do it, it's not complicated, there is just a lot you have to learn. It's all about putting in the time.
I'm pretty much in the same boat as this guy, but I've been doing it for 11 years and I'm the lead developer
not only native mobile.
No. It one shot the whole party as soon as he appears in the screen WHOLE PARTY. 0 sec whole party death skill
Bad bot
I made a career-switch from IT-support to systems development a couple of years ago, but I made the decision to get a BS from university to get there. I think getting the first job is the major problem you will have as opposed to somebody with a formal degree. Atleast where I live employers premiere education when setting salary, so with a good education you will have a better salary than somebody without a degree. While employers might prefer a candidate with a degree, I would say that 60% of the programming I know I have learnt from udemy and from learning by doing at work. I dont think Uni is overrated, because they teach you something very valuable, and that is how to learn. :-) It was a hard 3 years but very worth it in the longer perspective!
I became a coder, and eventually a .NET developer, with a BA (philosophy and english lit). It's absolutely possible. Learning to code is the easy part, if you have the interest and aptitude. The tricky bit is finding those crucial few first jobs to build out your resume. I was able to do it by finding other jobs that were only peripherally related, but gave me a chance to show what I could do. First, a part time job doing tech support/sysadmin for an accounting firm (made them an intranet and various utility programs); then a similar job for a tech firm (took on some devops and side projects); and so on.
I have a lot of trouble taking the apologism seriously.
I did it. I have an arts degree and didn't get into programming until I was well in my 20s. I was working for a pensions company and started playing around with Excel macros - recording them to make tasks easier than playing with some of the code and progressed from there. It's a bit different these days but my guess is a large majority of professional programmers don't have a computer science or engineering background. You don't necessarily need to take a course - it sounds like you know the programming basics already - so just get a good book and work from there. I prefer to learn by building something so set yourself a project to build and stick it on Github.
I guess you should first of all answer yourself, do you want to jump in a job and start practicing your skills, or, do you want to dedicate some time for general knowledge, that any true specialist related in CS should have? If you're OK with taking some time without making much money, I would suggest learning basics of CS. This of course is a broad topic, from general Boolean logic, hardware to abstractions, like programming paradigms, algorithms, data structures, compilers and etc... You can always learn these things while working, but at least I prefer dedicating to one thing, rather then everything. In general, doing projects is always the best, by tinkering with Raspberries I've learnt a great deal of networking to OS, CPU architectures and so on. There's really a great ton of source material on the WEB to learn things. Or if you just want to go straight to making a career, well, it's really situation dependent, some people manage to get a job with little to no experience, others do some projects and place them on GitHub as a proof that they're able to develop software for the employer. I think some practice wouldn't hurt, like getting to know the .NET environment before going for a job apply (VS or VS Code?, MS SQL, Entity Framework, C# language features and so on...)
build something, show people.
I often see this argument about shared hosting, but shared hosting is good for something like wordpress with 100 views a day.. Using it for business app is really a big nono. You'll have slow unresponsive apps, because with your users, hundreds of other users are hitting completelly different apps, polluting caches. Maybe there are some good hostings (or SSD will fix this), but this is my experience, over and over again. Or another issue - on one hosting they told me 1GB db is too big and we have to move away.. And using VPS is usually cheaper, because people often try to solve this kind of problems by ordering the most expensive plan of shared hosting. I can order VPS with 2GB of RAM and SSD for cca 5€ a month and you can run even large PHP apps with that (with good config). That's less than price of 1 lunch, that's basically free. I don't know about requirements of .net, but I'd be able to run some serious java apps on that too. (for .net you'll probably need windows vps with iis, but that's different topic..) Yes I like the idea of shared hosting too, but not because of price. It's very effective for rarely used apps/services (and green in a way). But that's the only argument for it.
If you start building some small personal projects by yourself or through tutorials, this portfolio can be used demonstrate what level you are or how much knowledge you have. Also, it will be a good indicator of how experienced you are with the technology. The more you do, the more you learn as well. Once you feel confident in the basics of the tech (2-3 projects perhaps), then start applying for jobs. Be honest about not having any **commercial** experience but you do have a few personal projects you've been working on. Once you get the job, you're basically now a **professional** .NET developer. From there, make the most of it, learn on the job and more importantly, have fun with it. Once you get your foot through the door it gets easier to apply for new jobs as you'll have more knowledge, commercial experience as well as references etc. A lot of this is easier said than done. It will definitely depend on how much time and effort you're willing to put into this. If you are serious, stay motivated and continue to grow, you can get plenty of advice and help on the internet, just ask :) 
Considering that the $13 plan has a lower user limit than 7,000, they would have to be something else I'm missing. It's certainly possible.
Loved to read your guy's success stories. Congratulations on your perseverance. Hope to be able to write about myself the same way in a few years!
I got my undergrad in Computer and Information Science but what really taught me how to program was learnvisualstudio.net (Now [DevU](https://www.devu.com)). I've been a .NET developer for almost six years.
Formal education is overrated until the bubble pops
What specifically would you have OP learn in what order? Would you have OP start with core? Learn LINQ? I'm already doing .net in my day job and have been for a while. I'm all self taught and got stuck in a job and behind. I kind of wish I could start over. There are so many pieces to it while I've always been able to get the job done I'd like to know all the relevant pieces much more solidly than I do. 
Well, the grammar and punctuation in your writing is impeccable; that bodes well for your success as a programmer. I can't take half the posts in this sub seriously because the writing is so bad, and a lot of those authors claim to be professional programmers.
Formal education is important, I cant really argue that. But the best interview question, and the one that is least often asked is "What do you do in your spare time?". If the answer is something like "I write code!" you are talking to someone who is passionate about what they do and who loves to learn. That person is ten times more valuable than a straight-A robot clock-puncher who just goes through the motions. 
I doubt we are working for the same company, but I'm glad I'm not the only one running into this issue. I like what you did above more than what I suggested earlier. I am looking at what it would take to move legacy code too, and this is the first snag I hit. I agree changing the legacy stuff to the way core expects it is the right way to do it, but that's a much easier pill to swallow on a new application. I also think it takes DI to the extreme, or at least farther than I have ever taken it but that may be a good thing. 
Chromium has a scale-to-fit feature that's impossible to disable in wkhtmltopdf. Look for things that overflow the page. 
I did the same but started in 2000. I have been working with Visual Studio every day for 18 years now... It is still fun too.
This is correct, you have to start in help desk role and work your way up.
I also use these kind of tests to make sure all the DI and Startup code etc works fine, as in all the layers can communicate to each other and in classic UTs the test will be pretty much isolated to one method in one layer
This looks promising.
web api has nothing to do with UI's. You might include an MVC interface over your webapi in the same project, but web api is simply the controllers.
I'm 33 years old with a decade of software localization under my belt. I've helped a number of companies find large revenue growth by entering international markets. My base is 85k. I travel frequently to manage a remote dev team and also have conference travel like Microsoft Ignite. I get a daily per diem allowance that's paid up front as a check before I leave. A lot of things add up to make my final figure which floats around 95-110. I'm on the West coast, from what I understand the wages here are substantially higher but so is the cost of living. The job is stressful as fuck. I'm the only in house dev and there are 6 other developers. There is no process, no source control, it's small family company environment. Our business runs on dated technology built by one developer who founded the company. The owners friends work here and they all have an idea of how the software should work, and they feel that everything about the software sucks and they know how to fix it. It's nowhere near industry standards and I'm not learning aside from what I teach myself after work hours. This year I have been tasked to develop a mobile application on Xamarin working with one other developer overseas on an opposite schedule, one day ahead. If I don't execute a full blown mobile app in 8 months, I might as well consider my job gone. No one on our team has ever used Xamarin. For comparison, my roommate works a JR dev position at 75k, showing up to his office is totally optional and he has unlimited PTO. He works about 4 days a week on average. They have a large team, with clear defined process and roles. He only has to complete one card per week which is curated for him by a lead engineer. His tasks are evaluated and he's trained and instructed by developers with decades of experience. He's very relaxed, generally goes into the office to have a good time. I dunno about you guys but I'm more interested in peace of mind, not so much money. Child support takes about $2000 a month from me so I can't get ahead no matter how much money I make. They just take more and more if I advance my salary. I could see how a number figure would be more appealing if you're actually able to take that money home. 
Yes you can. But note that since this is a Microsoft Framework, a Windows server is required, which is less common, and therefore costs more. Unless you are using a .net core. Or unless you are a student with free azure account.
I'm consultant, the first project my manager told me about was migration from web form to MVC, it was all scheduled to start in beginning of may, but what happen is last Monday they ask me if I start tomorrow because they were in the need.. And yes sure.. And here I am..
I'm 32, when I'm going to get experience? Its a full time job and another part time job... So I don't have much time to work at home
Your age is irrelevant. Experience is all that matters. If you don't have it, this is how you get it.
OK nice, but without writing a line for months? I don't know
Note to all new comers: It looks like the trick to landing a great job as senior developer is to learn how to solve logic and high-school algebra problems. The last two interviews I had focused over half their time at having me solve puzzles like "If Bob is not married and he is looking at Jane and Jane is looking at John are two married people looking at each other?" That is not the verbatim statement of the question but it is very very close. Not once was I asked: "How would you design a loosely coupled, scalable service layer?" "How does the choice of UI frameworks affect your ability to create reusable components in the UI layer?" "What is the DRY principal? Is it important?" "It what ways is the producer/consumer pattern exposed in a business application?" "What's the problem with REST?" Or any other of a very long list of relevant questions. 
They might have separate teams doing separate parts of the migration.
Its will only in may
That's what I mean though. You're probably not on the team that will migrate the web parts, due to your inexperience.
Thank you for your time anyway.
It sounds like you've been hired as an analyst, not a developer. 
It think i dont know much about beeing an analist maybe i should search for tutorials
I do not think it's mandatory. It is designed and can run on Linux as well.
You have only been there a week. Give it a few months. 
What's the difference between this app and "Popcorn time" app ? Is this the newer version ? Thanks for sharing.
I ask that question but I am honestly looking for an answer outside of software development. It is very easy to get burned out...it helps to have other interests/hobbies to escape to on weekends to clear your mind. Passion is extremely important though, I just determine that through a different line of questioning.
Polly is sick. Just sayin’.
pretty sure this is just a client to Popcorn time application. C# client in wpf if you will.
I was planning to use asp.net core 2
Read this [https://www.stevejgordon.co.uk/introduction\-to\-httpclientfactory\-aspnetcore](https://www.stevejgordon.co.uk/introduction-to-httpclientfactory-aspnetcore) [https://www.stevejgordon.co.uk/httpclientfactory\-named\-typed\-clients\-aspnetcore](https://www.stevejgordon.co.uk/httpclientfactory-named-typed-clients-aspnetcore) tldr: It's great but I tried to use our custom DelegatingHandlers and it fails. I'll try to fix it soon, I'm waiting for his next article
Best Software Training Institute for Java,.NET, Python,Ios,Android,Php,Oracle ,Database, Matlab,VLSI&amp;ERP.We provides online&amp;classroom training classes.
Just wondering why not to use restsharp http://restsharp.org/ 
Discipline is the keyword here. I personally think learning .Net was hard and took a lot of gumption.
&gt; Anyone can do it, it's not complicated, there is just a lot you have to learn. It's all about putting in the time. I'm gonna have to politely disagree here. It's definitely doable with lots of grit and perseverance, but its also not as easy as you present it. As someone suggests elsewhere in this thread it's the instinct for keeping on going, even when it's tough and boring, that eventually will help you kickstart a career. It takes tough and focused work for an extended period.
It is a completly independant application which features exactly the same movies and show catalogs. The client is made in .NET/WPF and backend is an ASP.NET Core server. The SQL Server 2016 database is synced from the official YTS.am API source to retrieve the movies and shows every 24 hours, whose every torrents and images are stored in Azure Blob Storage. The cache is powered by Redis. Everything is hosted in Azure and the whole stack is aimed for performance and stability. The aim of this app is not to replace the original Popcorn Time application but to offer an alternative to it, using only Microsoft techs instead of web-based stack (Electron, Backbone...). To finish with, the development of this app is very active which is not the case for the original one.
Hey, rocksharp, just a quick heads-up: **completly** is actually spelled **completely**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
ASP.NET Core 2 will work fine on both Linux and Windows hosts.
Warning: Using this tool might violate copyright laws in your country.
The biggest advantage *by far*, and in fact the main reason for this to exist, is handling `HttpClient` lifetimes. Unless you like to always wrap `HttpClient` in a `using` statement every time you use it (which is inefficient), managing `HttpClient` lifetime is a minefield. A long lived `HttpClient` will eventually get stale DNS entries and stop connecting to things, so you occasionally have to create a new one. It will also hold sockets open to servers for connection reuse, and these often aren't disposed unless the `HttpClient` is explicitly disposed. If you assume that it'll all get garbage collected eventually, you'd be wrong. The socket will keep the object alive as long as it can, and you'll end up with wicked memory leaks, not only within managed memory in your application, but also at the kernel level with loads of TCP connections racking up for no reason. Until now, the only correct way to handle this was to literally write your own `HttpClientProvider` to hand out scopes track `HttpClient` lifetimes, and dispose of them after a timeout occured and the reference count got back to zero.
Based on GraphQL for .NET implemented a GraphQL API for existing SQL database: engine that translates GraphQL queries to SQL \(also can generate GraphQL schema by database schema\). I'd say it's more lightweight than EF \(especially if we're talking about just fetching data from db, without updating / adding\) Feel free to check it here \- [https://www.nrecosite.com/graphql\_to\_sql\_database.aspx](https://www.nrecosite.com/graphql_to_sql_database.aspx) 
Thanks for the heads up on the license, I will be sure to add that as soon as I get the chance today. I’m not sure what you mean about there being no description of what it does or how to use it, though. I believe I covered that in the README.
Garbage. They're useless, and you will learn only wrong things with them.
Very clever!! Is it fast?
In addition to the other, good replies, there's the reason that led me to discovering `DelegatingHandler`s in the first place: An `HttpRequestMessage` can't be reused outside of the `HttpClient`. If you want to implement retry on failure or to implement an authentication flow (try, receive 401, parse headers, get credentials, retry), there's not a good way to do it with a wrapper around `HttpClient`; you'd have to recreate the request each time. Another thing to consider is that `HttpClient` is already a wrapper itself. In order to recreate its functionality with a wrapper, there are a lot of methods and extension methods to support. But all those functions lead to the same method, `SendAsync`. By modifying the behavior of that one method, the behavior of everything that wraps it is modified in the same way. (These are the same topics that come up when one asks how to mock an `HttpClient` for unit testing. Don’t. Mock the innards of the `HttpClient` that all of those calls lead to.) 
Doesnt work, the compressed pdf file throws an error - Failed to load the document on opening the file
Try to open in google chrome 
So, this was so obvious I didn't think about it. We had Org Table in all the parent tables but none of the children/supporting tables. Makes perfect sense though. Thanks!
It is! Thank you so much :)
Interesting.. are there any benchmarks available?
There exists mathematical terms, check up https://www.youtube.com/watch?v=PN-I6u-AxMg for instance ... The point is to figure out _"the fewest possible steps"_ to _"victory"_, whatever that may be. However, quantifying this becomes imperative (obviously), since otherwise we can't compare two different strategies. LOC is an obvious candidate for quantification. At which point, the answer arguably gives itself, and has been known for decades, described by Paul Graham as; _"Given two different approaches, where both can equally well solve the problem, the highest abstraction will always 'win'"_ ...
He is very responsive on twitter if you have further questions for him. https://twitter.com/shanselman
It depends on your needs. But for majority of cases, if you already have Castle Windsor in place and you're familiar with it, it's fast enough. I also like the benefit of simply adding the attribute and caching is there.
Please make async constructors and async events. I am aware of the theoretical arguments against async constructors. However from a practical standpoint those arguments do nothing to mitigate the need. The axiom for async is "async all the way down". That means constructors and events.
Check the app config profile in azure, the reply urls are stored there. Just add the new one in
It does support core https://github.com/restsharp/RestSharp/blob/master/releasenotes.md. And last release was 1 hour ago. 
Wow, thanks for detailed response... I really loved this version, i think it looks beautiful and it works great! 
In 2019 we really might be able to stop writing Javascript 
So here is my big question, can I control printing via the browser without a plug in now? Can I select the printer and send a document to a printer without any prompts something in between from the user?
Goodbye JavaScript. Don't let the door hit you on the way out.
WASM can't do anything JS can't do today in terms of interacting with things outside of the browser sandbox.
Hmm. I couldn't find anything past the 105 last night.
&gt; And even with that background, it's still mostly dependent on luck (finding a company that has the time/resources/desperation to actually take a chance with a developer without formal credentials); location (it's more common to be hired without a formal education here than in, say, the US); etc. Generally true up until you get a few years of experience. A greenhorn developer with no work experience and no formal degree has a more difficult time making it past the HR gatekeepers. (I have no CS degree, only finished 3 years of college. The first job was definitely the hardest. But even there, I had referrals from working for the company in a different division as an intern the previous two summers.) After you've had a few years of work, you should have a network of people that you can reach out to. Which can help you get past the HR gatekeeper and get an interview. I'm now a few years into a job with someone I've known for almost two decades.
I think Blazor looks really cool, but haven't had any time to personally dive in. Does anyone know what the F# story looks like? 
I saw this the other day: https://github.com/panesofglass/trail
Ah I see. That'd be wonderful for package legitimacy: "Microsoft/System.X" is clearly a legitimate package, whereas "Bob/System.X" is not. This may not be easy to introduce in a backwards compatible way though... I'll try to create a prototype when I have time.
Cool, as a primarily F# dev, that looks like a great place for me to start! I think I was mostly confused because most of the Blazor example code I've seen so far has been using Razor, which I guess is C# only. I wasn't aware you could also use .NET Standard class libs (and therefore F#), so that clears up a bunch of my confusion. 
How old was it?
Sorry didn’t see this. 
Are you using the same client id? Ensure you've added the Azure URL into the application in Azure Active Directory.
Great article! 
I have used Rider everyday for 6 months now on both Windows and Linux. If is officially the first IDE that I like more than Visual Studio. I feel that it is more responsive and just an all around better application. I've used every version of Visual Studio since .Net was released so it was a huge step out of my comfort zone, but I'm not sure at this point that I'll look to go back.
What are the reasons for using Mono instead of Dotnet Core? 
I was at a bar recently talking to another developer and I was getting giddy explaining to him all the changes Microsoft has been making these past few years. He asked so I obliged. Great time to be a .Net dev. :\)
Did you use Resharper in VS? I do agree that Rider is great and I have paid for a subscription to it myself, but once I removed Resharper from VS all my complaints about VS 2017 being aggravatingly slow were gone. Still use Rider though for all the built in code refactors and the fact it runs on Linux.
Was really impressed with Visual Studio Code when I first started working with it. 
It sounds like I need to do this tutorial? https://blogs.msdn.microsoft.com/jpsanders/2018/01/30/azure-app-service-error-aadsts50011-the-reply-address-http-azurewebsites-netsignin-oidc-does-not-match-the-reply-addresses-configured-for-the-application/
What's the version control like in rider? Last I used it was buggy and really slow to track changes. 
Thanks for sharing the links to my posts and I'm glad you found them useful. I've just published part 3 on DelegatingHandlers as it turns out... https://www.stevejgordon.co.uk/httpclientfactory-aspnetcore-outgoing-request-middleware-pipeline-delegatinghandlers
thanks for the great work steve!
Hey, bobbaan, just a quick heads-up: **alot** is actually spelled **a lot**. You can remember it by **it is one lot, 'a lot'**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
Delete
I ask this every few months again, but is there by now a written recap of it?
Thanks, I really wish I had a mentor or something that could help guide me through all of this and at least make sure I'm on the right path. For now, I'm fading and need to get some sleep, but I'll consider what you've said for next time.
&gt; There's ton's of them out there but Entity does it really well, thats why Microsoft have made the descision to include it in thier standard solutions. It honestly doesn't do so well, but it at least works. And Microsoft includes it in their standard solutions because Entity Framework is by Microsoft - not because it does well.
These bots annoy alot.
Don't even think about it.
Please stop your spam.
You're right, it's from Microsoft, but i'd say it's pretty good. It's a library, which also means you need to use it according to thier documentation and thier standards in order to archive something long time working.
You're welcome! Where would you like to inject it? If you're using the HttpClietFactory then registering the delegates with AddHttpMessageHandler is the pattern. This ensures that as new client instances are created they include the correct handlers. You can register different clients with different handlers if you need to.
Thanks, I'll check my code tomorrow. I must have fucked somewhere
VSCode.
It works very well. In my experience it tracks the changes immediately. They also just added support for partial commits which was the only thing I really had a problem with before.
Rider is very nice, but for F# I prefer VSCode and Ionide (I find the Type Code Lens feature pretty handy) 
I'm still waiting for the real deal tbh. Resharper, support for .csproj and .sln, etc. 
Mono already has a long history of being ultra-portable - it's the runtime they use for Xamarin, Unity, etc. Plus, the Mono team are crazy geniuses (not that the .NET Core team isn't - the Mono team just seems to always be on the bleeding edge).
I doubt they're planning to implement project and solution support. That's what an IDE is for and that's not what Code is, it's a text editor and a phenomenal one at that.
As a senior developer, you'll still need to learn 20% of the tech on a new software team even if you stick with C# and change teams every 6 months. There is just WAY more to learn (even just in the Microsoft ecosystem) than any one human could learn. I've used a lot of ORMs (Dapper being my favorite), a lot of SQL (used raw with a DataReader object) and besides desktop have used too many web frameworks and libraries to count. However, my current role still has me learning--and more than usual! Most teams have a couple home-built ORMs, something weird they do in a base controller, and have overridden normal identity functionality. So don't worry about feeling overwhelmed. Think of it instead like a buffet, and focus on the fish tonight. Eat up on that one topic so that you can make something fun or useful for yourself. Your first employer might only use 20% of what you learned because they might be, for instance, a Knockout / ASP.net 3 / Dapper shop. As time goes on, you'll learn things faster, see a framework, and already know where to look in the documentation. Most of us fear that we are mere imposter programmers sometimes, but we aren't. We're just in a very wide field. :)
sure but MS is pushing for multiplatform development right? If I want to develop in linux using net core 2.x Microsoft provides everything but an IDE. So my team ends up using windows 10 because that's the only thing that runs vs2017, even though even my db (sql fucking server) runs on linux
&gt; “Why would you ever want to use ASP.NET, are you still stuck in the 90’s?”, these are the exact words uttered by an old coworker of mine when I brought up the idea of considering using ASP.NET for a project we were about to start a couple years ago. I agreed with him for the most part at the time. If I was stuck in the 90s, I'd be very excited to be able to use a technology that didn't exist until 2002.
Someone made this comment on the blog so I'll just copy paste my response: It's just a hyperbole, it's like when someone says "I told you not to do that 1000 times" they don't actually mean 1000 times. That statement meant asp.net was seen as a dated technology, not necessarily released in the 90s. If we started everything in English for face value, we would be living in a different world.
We use git and I have the root added but really don't use the features in Rider. The file names change colors and all. Whether it is slow or not, I don't really pay much attention to those features. But when I'm committing or checking out a branch, I use command line. The times that I have used it to pull a new branch, the drop down seems a little clunky trying to find the branch I need. The version history stuff works very well.
They're more likely to make vs cross platform than turn vscode into an IDE. Once we get wpf (or something comparable) in core it'll happen.
I know in webstorm and intelij they do that cool thing where they stash your changes before pulling and then reapply the stash so that you don't need to commit before pulling.
Of course it's sheer pedantry, I wouldn't pretend to be any more sophisticated than that, But if you want to be hyperbole, you would go with "What is this, the eighteenth century? Are you planning on writing it by telegram?"
Do you have the click event for textBox1 set to the event you created? I sometimes miss that when setting up events.
Ahh, ok I think i'm missing a core concept here. TextChanged event is working for me and I assumed I could just copy that bit of code and change it to _Click and voila have a click event. private void txtInput_TextChanged(object sender, EventArgs e) { MessageBox.Show("test"); } to private void txtInput_Click(object sender, MouseEventArgs e) { MessageBox.Show("test"); }
The names themselves don't matter. It could be named `Buttlicker_DoAThing` and if it was assigned as the event method, it'd still work. The underscored suffix just makes it easier to read through your own code. Go into the designer and click on the textbox. Under the Properties view there should be an event box where you can pick which method you want associated to each kind of event.
Thank you much! That has steered me in the right direction.
'rates.MinRateColumn' has information about that column, not any values in that column in different rows. What you need to look at is something like rates.Rows[0]["MinRate"].Value.
eh its too late to go back in time and tell that to my coworker.
Perfect! Thanks that was exactly what I was looking for. The below worked flawlessly. Label.Text = rates.Rows[0]["MinRate"].ToString();
If I understand well, Dink is basically another wkhtmltopdf wrapper like Codaxy or Pechkin I mentionned? Unfortunatly, Dink is in .NET Core, and I'm still on .NET Framework. But if you're saying a wkhtmltopdf wrapper does work in your case, that should mean that the other should work?
Hey, Pinguino21v, just a quick heads-up: **unfortunatly** is actually spelled **unfortunately**. You can remember it by **ends with -ely**. Have a nice day! ^^^^The ^^^^parent ^^^^commenter ^^^^can ^^^^reply ^^^^with ^^^^'delete' ^^^^to ^^^^delete ^^^^this ^^^^comment.
I recommend that you look into WPF. WinForms is nice for some quick small application. But WPF using XAML with the MVVM pattern is pretty much the standard for any enterprise applications. Just my 2 cents.
Yep. It is probably already configured to your local host which is why it works locally. Take the ClientId and paste it into your AAD to find your app, and check your reply urls. 
delete
I would disagree. For someone making the leap from VB6 to C# the familiarity of WinForms will help make the transition easier. After learning C#, I would then go back and start WPF or UWP from there. It might be too much to throw Xaml and patterns and everything into the pot all at once. 
Thanks for the advice! I currently have a pretty stable gig working in IT for a school district. It's a union position and I've been here about 5 years now, I'm well liked by my co-workers and supervisors, so I could ride this out and collect a pension in like 30 years... -_- But my current tasks aren't always the most rewarding. It's a mix of basic help desk stuff, because that's what my position is really supposed to focus on, but since my supervisors trust me with some other things, I do get put on some higher level projects where I actually get to use my brain some. But being a union gig, I'm also not paid based on merit, I get paid just as much as the tech who only changes out toner cartridges and half-ass fixes user end issues, because we have the same job title. I don't really like how our department is structured, as most of the "supervisors" in our departments are really just network techs... Which I'm not as interested in... So there's not much room for growth here. So yeah, I feel like I'm capable of pulling in more $$$, and I'm aware making a career change may mean risk. Also, good tech jobs in this area are hard to come by, and I am very thankful for the job that I have. I've taken lots of programming courses over the years, but never make the jump into making something of my own. But I'm determined, dammit! My initial impressions of the ecosystem of Visual Studio, C#, .NET, ASP.NET CORE has been really good. I like it, I feel like there's lots I can do in this area, so I've figured it's where I'd like to try to settle in to start. It helps just making a post like this and having thoughtful people like yourself break down some of the terminology. As I have people to talk things out with things start coming together a little better. I just wish I knew some developers in my area, specifically C# .Net developers, to bounce ideas off of. There is a local developer Meetup group that I may try to attend. Most of their meetups never sounded too enticing.... They held some events like, "How to make a website with wordpress", but some of their newer meetups have sounded a little more general and interesting. I figure the networking couldn't hurt... They're only holding them once a month and I can't make it in May, so I have until mid June to develop my skills before I can use that event to try to network some more. Do you think it would be helpful to be working on a project with another developer, even if they're a beginner too? Or do you think it's better to go solo?
&gt; half ass-fixes user end issues *** ^(Bleep-bloop, I'm a bot. This comment was inspired by )^[xkcd#37](https://xkcd.com/37)
Another comment from the peanut gallery!
Thank you. This is purely a quick and dirty replacement app for converting rates from monthly to weekly or annually, etc... it has one target audience (me) :-) Not doing anything robust atm but I sincerely thank you for your reply.
You've got to link it up. In the object properties there's a little lighting bolt. Left click and look for the word "Click" then link that bad boy up!
I last looked at this around 2 years ago, so the landscape may have changed, but at the time I concluded there are no good HTML to PDF tools available for free. If going paid is an option, I'd recommend looking at PDF Reactor or Prince XML. They're both exorbitant, and I have no idea how easy it would be to spin them up in Azure, but the CSS support is excellent in both (including the CSS Pages Media Module, which basically nothing else supports)
But our personal lives are so interesting!
Bad bot
Thank you, MellerTime, for voting on CommonMisspellingBot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
Might want to make this an issue on git hub for razor. 
Thank you, yup that was what I was missing.
No problem OP. Devs unite! Form of less confused person trying to make something work.......
It's actually a VS for Mac thing I think. I just remembered I was on the Beta update channel because I wanted to try the new Razor intellisense \(vs4mac doesn't have razor intellisense yet\) and I just switched back to the stable channel and the issue went away. So problem solved everybody, Beta means Beta for a reason.
LOL no big deal.
What do you mean support for sln and csproj? There's nothing special to them. You just reference them and dotnet knows how to build based on what folder you're located or what solution you say. What more support are you hoping to have?
Have you tried `FakeItEasy`? I prefer that over moq. It's available for .net core and you can make automatic fakes/mocks with it. Autofac has some feature outlined using FakeItEasy's autofakes here: http://autofaccn.readthedocs.io/en/latest/integration/fakeiteasy.html#options-for-fakes
As a side note you can go to the properties on the right for me and change it from properties to events and see all events on a control and double click the event to create
Ive used this in the past http://www.expertpdf.net/expertpdf-html-to-pdf-converter-introduction/ We used it in an old web forms app and it worked great. I can’t remember if you have to pay though. They have a blog post explaining some caveats about running it in azure http://www.expertpdf.net/expertpdf-html-to-pdf-converter-and-microsoft-azure/
Thanks, I saw that it had some support for auto mocking. I like Moq and have been using it for years; its not a big enough "lack" for me to change to a different mocking framework :)
What do you mean by the mono team being on the bleeding edge? As far as the C# standard is concerned, they are behind.
I don't think you need the nested foreach to have an @ in this case
That was my experience too - full PDF rendering is a complicated business, as is applying all the CSS rules. The best we found was https://www.antennahouse.com/antenna1/formatter/
They appear to experiment more and have always seemed more like a Skunkworks or prototyping lab to me (even before the Microsoft acquisition). Things like MonoDevelop/VS for Mac, AoT compilation and static linking, and the entire Xamarin offering all seemed a bit out there at first.
Just a guess... take a look at quotes that have no items. This might be cause by a null or empty string.
office was literally the only product and even that was so that it wouldnt hurt office's adoption. specially on the development side of things, microsoft pretended like other operating systems didnt even exist.
Open sln witb vscode and get the same behaviour that i get on vs. I have my fair share of legacy systems that i have to support and their folder structure is horrible, with vs at least im able to forget what shit goes on the fs
Literally the only product except for the dozens of other software products they released for mac, including some music software, age of empires and ofc internet explorer. They did stop really caring around 1996, that's true... 
That looks fine from what I can tell so I'm guessing in your controller the model might not include the actual quote items. You'd wanna do something like .Include(x =&gt; x.Quotes).ThenInclude(x =&gt; x.Items) to your model.
I get it, I would have a hard time leaving fakeiteasy behind too Good luck!
Yeah OP, VB6 wires up the event automatically based on the name. VB.NET uses the HANDLES clause. C# requires wiring up the events explicitly. 
Thank you for explaining!
The weird thing is that in typical enterprise scenarios there is relatively little advantage to WPF over Winforms. It's when you step out of typical enterprise "custom spreadsheets on steroids" LOB apps where WPF is a dramatic improvement. Making a kiosk-style front end with WinForms is... basically impossible, for example.
Yea.... I wasn't trying to be "silly". There are also many sites using WebForms with some absolutely amazing ViewState and inspiring AJAX Control Toolkit code. I just interviewed with a huge Fortune 500 bank and they showed me a beautifully architected classic ASP site. /s Many companies don't update their applications simply due to cost or fear of introducing bugs. If it ain't broke, don't fix it. Doesn't mean that I would want to use it for a new application. I was just trying to tell someone who is new to UI apps what might be beneficial to learn. 
These guys weren't worried about introducing bugs. But when 99% of your app is made up of beefed up data grids and you already have a clean, well separated architecture (it's not every day that you see a winforms app with no code-behind) - there is just very little to be gained from the switch. Honestly, their project was beautiful. At the lowest levels, it was old-tech, but conceptually, so well architected and cleanly executed. Looking back, you can see why they never switched to EF/NHibernate, etc, or WPF, because as these products came out they could look at their own codebase and say "hey, we already do that - and better, too". I came prepared to scoff a bit (after all, I myself abandoned WinForms ten years ago, and was very happy to do it)... but walked away very impressed. This is a well-staffed boutique team that has pretty much total freedom in execution, they're not being throttled by an overly conservative IT manager. 
No, and you *shouldn't* be able to get the printer to print something without a user prompt. That's a major security vulnerability.
Docker image https://github.com/arachnys/athenapdf
Not for embedded. WPF can barely run if at all on embedded systems. Although, WPF is a must if you need touch screen, you just need to make sure you're hardware is able to handle WPF and DirectX.
[Here you go.](http://www.drfreeman.com/wp-content/uploads/2013/01/hand.jpg)
There's probably a nuget package, or you can just copy the javascript files in yourself.
Take a look at nsubstitute. I worked with all 3 of these frameworks and find this one has cleanest / most intuitive syntax
Fuck WPF. It too is obsolete new. Xamarin Forms is the new hotness. WPF + mobile apps.
[YouTube kudvenkat](https://www.youtube.com/user/kudvenkat) is a great resource. I’ve done many of his tutorials and they are all bottoms up. Just start with the C# playlist and move up from there.
OK, I have looked at a new React project already, but I am hesitant about messing with my existing project by just adding in webpack, etc and hoping that it automagically wires itself up like the template does. I thought I would branch it first. 
#1 for the .Net community is Pluralsight, no questions asked. It is a paid subscription but they always have deals at Christmas time\(I know, that's really far away haha but just an FYI for down the road\). Beyond that, Microsoft Virtual Academy is another high quality resource .Net focused. After that I'd say it's just the usual learning sites \(udemy, coursera, edx\). To try and get your first gig, just have some same projects/experiences you can talk about during the interview. Getting the interview is the hardest part for sure for the initial job but keep at it. It *may* be worthwhile looking at taking some of the Developer MCSD exams. Their kinda a joke in the industry but if it's for your first job, it may be valuable leverage to prove your dedication to the craft beyond side projects. 
Wow very beneficial thank you.
Wow, I remember using this guy’s vids years ago to brush up on my MSSQL. 
If you can post a small project on GitHub tgat mimics your project and explain how you need it to work I'll add react for you. Definitely branch before you start though just to be safe.
I am a .NET guy, but also couldn’t find any good solution, so I made something like this in a node container using phantomjs2. I could probably share code if needed.
Pssst! For a longer free trail on pluralsight, check [this](https://www.windowslog.com/udemy-course-deal/free-pluralsight-subscription/) link.
I also found kudvenkat's videos helpful. His youtube thumbnails don't look as professional as those of other channels, in my opinion, but the content is high quality. I also like [Tim Corey's](https://www.youtube.com/user/IAmTimCorey) videos on c# on youtube.
With most things, start with a small project and build out more features as you see fit and learn along the way. I started with a C# web service that triggered a relay on and off.
Headless chrome is probably the easiest and less buggy way.
I found some people at my work using PhantomJS or [Puppeteer](https://github.com/GoogleChrome/puppeteer). It seems to be not as easy as would like (a simple library would have been nice), but that may work. I'll try that.
If you have the patience, the original can't be beaten: https://docs.microsoft.com/en-us/dotnet/index https://docs.microsoft.com/en-us/dotnet/csharp/
We started with phantom js but chrome produced better rendering
Microsoft Virtual Academy, Pluralsight, YouTube and the vast amount of samples they provide on GitHub.
In terms of keeping up to date with the latest C# changes, I've always enjoyed Mads Torgensen's videos on Channel 9: [https://channel9.msdn.com/Events/Speakers/Mads\-Torgersen](https://channel9.msdn.com/Events/Speakers/Mads-Torgersen)
Given that you like working with people, try to start doing workshops, at first totally free and open ones (e.g.) for some local development groups) get experience, see whether you enjoy it or not. Once you like it, you may get some pretty good money out of it, yet even more importantly, you'll switch from just writing a code, to actually teach others how to write a code, design systems and so on. Personally, I find it to be a great activity + you won't get bored after writing code over and over again for years 8 hours a day, being your only job. Other than that, maybe study concepts and practices such as DDD, learn (and implement) about distributed system (microservices hype), set up a fully automated CI &amp; CD pipeline using Docker and Kubernetes (DevOps thingy), or even read about something really different such as Event Storming and try to incorporate such techniques next time when designing a software :).
I really enjoyed the MIcrosoft Virtual Academy C# for beginners course instructor, but I really don’t like their intermediate course instructors. There are two, and they just crack jokes at each other the entire time. Beyond that they just aren’t great instructors. There’s one point where one was explaining something and started saying “and what I mean by that”, then looked at the other guy and said “well, you know what I meant by that.” I’ve started looking for some alternate options. 
https://blogs.msdn.microsoft.com/msgulfcommunity/2013/03/13/understanding-the-basics-of-mvvm-design-pattern/
yea you got me. i'm Freddy Krueger. Good think you helped out the OP instead of just talking shit
&gt; I am trying to understand how to implement a messenger in my MVVM application. MVVM is based on the idea that different views can share the same VM or model. So sending messages is always suspect in my eyes.
I can be patient if I must be thank you 
Oh! I was curious of this exact same thing a bit ago and wanted to make my own Messenger system. Basically it takes advantage of a Static instance of the Messenger class, which lets you publish and subscribe to messages across the entire program. These messages are typically objects, you can pass strings and integers but typically you want to pass objects. It relies on a event that is triggered within the Messenger class when you publish of a message with a corresponding subscription i.e. a object. Using this pattern you can loosely couple the viewmodels together.
Disregarding the massive comment in that class, that code is boilerplate standard BindableBase. Can't imagine what issue you have with it.
&gt;MVVM is based on the idea that different views can share the same VM or model. Sharing objects is a characteristic of good software design. It's ubiquitous in .net. &gt; So sending messages is always suspect in my eyes. non sequitur.
No it's not. While sharing objects is great for MVVM, you don't want any sharing in a multi-threaded system. It all comes down to context.
You might try /r/cscareerquestions as well, or sub and start reading as these sorts of questions come up all the time. I guess the direction you want to take your career has a lot to do with personal preference. I am preferring to stay in an engineering role, but I think PO and the skills/duties are valuable to software development in general. There is still room for growth as a senior software engineer. Architect or principal software engineer, consulting, or just jumping to another place that pays more for senior. Senior software engineer could itself be a title that lasts the rest of your career and still offers salary growth. I see it almost as open ended even if, say, your current employer has some salary cap on it, the market says that is BS. I tend to think staying in an engineering role is slightly safer, but I'm open to arguments against that view. 
&gt; you don't want any sharing in a multi-threaded system. Hence the design of messaging. Sending messages is a fundamental task - threading constraints may dictate HOW you do it but not WHY. The problem actually has less to do with threading (that is what dispatcher is for) and more to do with the hierarchical nature of the logical/visual tree. Communicating up and down the tree is pretty straightforward (tunneling / bubbling) but communication across the tree is tricky. [See this.](https://msdn.microsoft.com/en-us/magazine/jj694937.aspx)
I migrated to MVC from ASP.NET webforms and I can tell you that the transition was well worth it. If anything I feel that Razor gives much better separation of concerns and lends a much more WYSIWYG approach to developing the front end. I always despised developing user controls all of the associated boilerplate to go along with it when, in MVC, partial views and such are much easier to work with and there's little "magic" behind the scenes. That said, if you're looking to build single page sites or sites that rely heavily on AJAX and web sockets, then you probably don't want to look at MVC and want to go with something like WebAPI + Angular or React. MVC is great if you want to quickly spin up a heavily data driven website, though.
It's not WebForms 2.0 and anyone who says otherwise has no experience with either razor pages or WebForms. What it most closely resembles is classic ASP or PHP. I'll leave the reader this form his own opinion as to whether that is a good or bad thing,
At add to what you said, Angular and other SPAs still have some serious issues when it comes to search engines. So if you are building some thing that feels like a website, rather than an application, they are probably not going to be a good fit. For my most recent site I'm seriously considering MVC for the "public face" and Angular for the "admin tools".
In a way, it's kind of nice if you're a bit of a control freak... You can see exactly where everything is going and easily fine tune any aspect of the rendered content. But on the other hand, it can end up being a lot of code that can become a maintenance headache. Guess that's just the trade-off you have to be willing to make!
&gt; Communicating up and down the tree is pretty straightforward (tunneling / bubbling) but communication across the tree is tricky. While I certainly agree with that point, I don't like the author's conclusions. The messaging system he proposes either leaks memory or randomly breaks (depending on the version of MVVMLite you are using). Strangely, it would work great with WinForms because they have explicit object disposal that you can use to unsubscribe to messages.
I haven't tried using it yet. It baffles me that they made it look so different from the existing MVC. When I first heard about it, I thought it would be (or at least look like) a controller nested under a view with automatic routing. 
Oh yeah... I mean, standards have become much better since the 90s but it's still pretty wild out there. Back end stuff is pretty straightforward but the front end is full of "flavor the month" JS frameworks and subtle differences in browser rendering of CSS and spotty support for certain HTML5 features and the list goes on. It's... sensible at best and, at worst, fun if "fun" were synonymous with giving yourself a suppository with a cactus, LOL.
Run [create-react-app](https://github.com/facebook/create-react-app) in your Views folder? this will setup Webpack and Babel automatically 
A developet where I work learned to C# from nothing else but watching Pluralsight videos. It is amazing the amount of knowledge and quality courses they have.
I don't see a reason why razor pages would get discontinued as the maintenance cost is not very high - most of it runs on top of the same internals as mvc
Blazor, an experimental project with .NET and WebAssembly uses Razor.
I feel so unimaginative with project ideas! Good advice though, thank you
It can be small things. I worked on some database stuff in college because I built a little desktop app that would manage a database of entries for games I'd bought, because I kept purchasing duplicates. It was a basic inventory thing but being able to mess around with SQL queries was a good learning experience.
That sounds like a cool idea man, I’ll try to think of a few projects
No not really.
Nice. I saw them doing this in a Blazor demo, the browser would refresh automatically after you saved your code and it recompiled. This is a pretty slick way of doing it.
Razor pages is not Razor.. 
&gt; From what I hear, even with server-side pre-rendering its a nightmare because Google doesn't like to follow the virtual links. I'm not necessarily saying you're wrong, but I don't see how that could be true. I can't speak for every framework in existence, but at least for every one I've had exposure to, links are just `&lt;a&gt;` tags with extra behavior attached to them via JavaScript. If you have SSR, even without any JS support on the client at all, they're literally just regular links. 
I don't claim to know the details myself, but it may have to do with how the framework alters those links. Google does execute some of your JavaScript.
Docker image https://github.com/arachnys/athenapdf
Does your assumed role have s3:PutEncryptionConfiguration permission?
I like it for serving the HTML for a SPA. I can do things like conditionally drop in different js or css. And yest it reminds me so much of working on php or asp back when I had a head full of hair. Lets just remember that microsoft doesn't drop support for anything in .net. They just finish development of it. Webforms is still there and going strong.
To some up 50 percent of the comments in this thread: Microsoft did not really well in coming up with a name they were not already using for one of their existing products.
Lol Razor syntax vs Razor Page. Why would they do that
It wouldn't get overwritten because it's basically like chaining together a bunch of Where method calls on the table to build a bigger query. (e.g. context.MyTable.Where().Where().Where()...) IQueryable basically lets you build an expression tree that ultimately gets translated to a SQL statement. And the query doesn't even execute against the database until you try to access the results.
I've been doing .NET development more or less for as long as .NET has existed, and I've never seen any serious criticism of Razor. Care to cite some examples?
FYI Razor Pages is different from ASP.NET MVC/Razor view engine. Sure they both use razor .cshtml files but razor pages has a different programming model (no controllers actions etc ). It actually has a code behind file. See https://docs.microsoft.com/en-us/aspnet/core/mvc/razor-pages/?view=aspnetcore-2.1&amp;tabs=visual-studio The name is confusing but it’s important to note razor pages and asp.net MVC with razor view engine are two separate things (albeit related and uses the same plumbing )
Frankly, I don't think you need to decouple ```APIObject``` from ```OrderObject``` quite so much. It sounds like ```APIObject``` is something of a service class, so it's OK for ```OrderObject``` to hold an instance of it. You could keep things decoupled by using Dependency Injection (which practically all MVVM frameworks include).
Welcome to webpack world 
&gt; no way to map it to the List property as there was in EF6. I'm not sure there was ever a way to do that. Do you have a link to the docs? Many-to-many will require a join table in your database which should have a corresponding entity in your dbcontext. 
Razor pages is here to stay - it's gaining tons of features in asp.net core 2.1
I start a new project on github for everything I am learning 
The way that I do it because I want to declare a specific table name is as follows modelBuilder.Entity&lt;Employee&gt;() .HasMany(e =&gt; e.JobTitles) .WithMany(jt =&gt; jt.Employees) .Map(ejt =&gt; ejt.ToTable("EmployeeJobTitles")); But EF6 would just sort it out for you as in the example at this link: http://www.entityframeworktutorial.net/code-first/configure-many-to-many-relationship-in-code-first.aspx 
.NET, .NET Framework, .NET Core, .NET Standard ASP, ASP.NET, ASP.NET MVC, ASP.NET Core Xbox, Xbox 360, Xbox One, Xbox One S, Xbox One X &lt;- (really?... X.B.O.X.) I feel like Microsoft is just fucking with us at this point.
This post deserve more upvotes!
I wanted a single web page for domains I own but don’t use. I decided to give Razor Pages a try. It was really easy to use. I liked that it allows you to use (most?) of the same features you’d have in an MVC website, if you need them. Exposing properties on the ‘controller’/page as the model felt yucky. But it worked for my use. For really simple websites it would be fine to use. I don’t think it is as bad as ASP or PHP as some said. I mean it could be, but that is up to the programmer. An inexperienced programmer can also make a terrible website using ‘good’ tools.
Well that is very cool that EF could figure that out. I have always just coded up my own entities... I never want to trust EF any more than I have to for some reason. In any case you can easily do this yourself like this: public class CourseStudent { public int StudentID {get;set;} public int CourseID {get;set;} public virtual Student Student {get;set; public virtual Course Course {get; set;} } modelBuilder.Entity&lt;CourseStudent&gt;().HasKey(x =&gt; new {x.StudentID, x.CourseID});
If what you want is to make your entire database searchable look into Elasticsearch
A service object like ```APIObject``` would typically be held as a Singleton, which is a single instance reused wherever it is needed. Most Dependency Injection (DI) frameworks will handle this for you.
There's a [good, long\-running discussion on Github](https://github.com/aspnet/EntityFrameworkCore/issues/1368) regarding many\-to\-many relationships in EF Core. A lot of folks are missing the way that EF6 would wire up those relationships automatically, and using join table entities \(or another ORM\) instead.
I have run a few ARM Linux binaries in Android, if we're talking she'll here. You could see if running dotnet core for Linux yields any results as long as there arnt too many dependancies.
MVC is the standard for now and I don't think Razor Pages is meant to replace it. So go ahead and learn MVC if that feels easier for you. 
Razor Pages, not Razor for MVC. 
Not that I know of, but you can write .net applications in xamarian studio (which I'm probably spelling wrong) and run them on android
Well it makes sense to me to make it a singleton, But is OK to be accessed from any thread? What happens if I am placing two orders from a singleton APIObject at the same time? Using a DI container to make a class a singleton, I have had working in the past. I don't quiet understand constructor injection and all that other jazz, takes a lot to get ones head around. I'll try make the time.. Thank you kindly for your know how, I much appreaciate your time.
If you make the ```APIObject.Order()``` method ```async```, .Net will basically take care of the threading issues for you.
Try targeting the [.NET Standard](https://docs.microsoft.com/en-us/dotnet/standard/net-standard). Same code can be used for Xamarin.Android and .NET Core. If you use external libraries you might hava a problem targeting the standard. Though you still need to create an app to test it your code on android.
If you go fully azure there are some pretty cool benefits with full core stack. Mostly just tons of documentation and example. 
In order to even try running anything, here's what I had to do. This was trying to run a hello world app with -r linux-arm64 (which wound up not working) Running from /sdcard didn't work because the permissions on the executable didn't change even after chmod 777. But, you can copy files to /data/local/temp, and from there you can chmod the file successfully. I'll give that rid (android.21-arm64) a try. Thanks.
I've also put them to /data/local/temp, ls -l show shell as the owner and the permission granted, but no dice ¯\\\_(ツ)\_/¯
Nope, it didn't work. I was able to build and publish using the rid you provided (dotnet build|publish -c Debug -r android.21-arm64), I was able to copy the output to /data/local/tmp, I was able to chmod 777 my executable, but when I tried to run it (./Hello), it returned an error: "/system/bin/sh: ./Hello: No such file or directory"
This is what extensions are for. Microsoft probably won't add support themselves, but someone eventually will (or already has done). For instance, pne of the F# extensions already has a solution explorer for VS Code. 
When you say **Razor Pages**? What do you mean exactly? Razor has been around for a while now, so I'm little confused. Is there something new that has come out called 'Razor Pages' that I've not heard about!
Thanks unnDunn... So the solution you suggest would be not to use a messenger for such a circumstance; \- Make APIObject a singleton \- Make APIObject.Order\(\) an async message \- Return success or failure via the APIObject.Order Then keep messenger passing know how for other things? :\) \(In my mind this seems a whole lot easier with a bit of mild concern about the threading part working\)... I shall work towards this anyway, and hopefully it will work out :\) Thank you once again
wkhtmltopdf is based on QtWebKit 4.8 which originally was released in 2011 and it don't support new things like CSS3, flex/grid layouts etc, however it is still good for PDF exports of formal documents like orders, reports etc. I can recommend to check phantomjs 2.1, as it uses newer QtWebKit 5, and can render web pages to PDF or image like wkhtmltopdf. BTW, [NReco has wrapper for this tool](https://www.nrecosite.com/phantomjs_wrapper_net.aspx) too.
The PDF I need to make is a basic reports of projects lifecycle, but it includes a "work plan", somewhat like a Gantt chart. That part is custom made with flexbox. I took the whole day yesterday to try phantomjs, after the advice of coworkers. It wasn't as easy as I would have wanted, but in the end, it seems to do the work. However I have encountered several issues: * I need to create an HTML file to make phantomjs able to convert it to PDF, instead of giving it raw HTML. I tried to tweak *rasterize.js* script, but failed. * I tried to use NReco wrapper but quickly proceed to put it aside, since I wasn't able to make it work at my first try and saw *"RenderPdf (commercial package only)"* in the features discribed on their website. I didn't want to lose more time, but maybe it can work. I used System.Diagnostics.Process instead. * From *the same* HTML file, the "flexbox" style works well on a coworker phantomjs, and is not recognized on mine. I had [to add Safari 4 prefixes](https://github.com/ariya/phantomjs/issues/14365#issuecomment-254446265) to make it work. Why it works without them on my coworker PC and not mine remains a mystery.
 /// &lt;summary&gt; /// Create an HTML and a PDF file with the HTML content /// &lt;/summary&gt; /// &lt;param name="htmlContent"&gt;HTML content&lt;/param&gt; /// &lt;returns&gt;PDF file path&lt;/returns&gt; public static string CreatePDFFileFromRawHTML(string htmlContent) { Guid guid = Guid.NewGuid(); string exportTempRepository = "export_temp"; // Initialize path of phantomjs and files string baseDirectory = AppDomain.CurrentDomain.BaseDirectory; string exportDirectory = System.IO.Path.Combine(baseDirectory, exportTempRepository); System.IO.Directory.CreateDirectory(exportDirectory); string htmlPath = System.IO.Path.Combine(exportDirectory, guid.ToString() + ".html"); string pdfFilePath = System.IO.Path.Combine(exportDirectory, guid.ToString() + ".pdf"); string phantomjsPath = System.IO.Path.Combine(baseDirectory, "phantomjs.exe"); string scriptPath = System.IO.Path.Combine(baseDirectory, "rasterize.js"); // Create the HTML file using (StreamWriter writetext = new StreamWriter(htmlPath)) { writetext.WriteLine(htmlContent); } string url = Constants.URL_WEBSITE_BASE + exportTempRepository + "/" + guid + ".html"; // Process phantomjs var process = new System.Diagnostics.Process(); var startInfo = new System.Diagnostics.ProcessStartInfo { WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden, UseShellExecute = false, RedirectStandardOutput = true, FileName = phantomjsPath, Arguments = string.Format("\"{0}\" \"{1}\" \"{2}\" \"A4\"", scriptPath, url, pdfFilePath) }; process.StartInfo = startInfo; process.StartInfo.StandardOutputEncoding = Encoding.UTF8; process.Start(); string output = process.StandardOutput.ReadToEnd(); process.WaitForExit(); // Return the PDF file path return pdfFilePath; }
Thank you for clarifying, I've been using .NET Core for about 6 months, and I don't think I've come across Razor Pages before. I'll check it out :\)
As someone not overly knowledgeable about all this, do you have to root your phone to run ARM binaries like this?
SQL is a perfectly fine database to use for searching. It always is about proper indexing. You have to learn how to create a good schema, and proper indexes or it will be slow. Elastic allows you to cheat because everything is an index but you can mess up even bigger with Elastic and changing mappings after loading your data is still a mess. Now, if you are doing text based searching(anything that has a wildcard at the beginning) on over 100,000 rows, yeah elastic is designed for that. Simple Amount&gt;500 and Date&lt;1-1-2018, any sql/nosql database would be good. 
that error indicates that the executable is incompatible
I have no experience with any of that, however, I was at a convention a month or so ago and one of the panels they had was a "Azure vs AWS showdown" where they did a demo of creating the same simple .net app in each. Their overall conclusion was that AWS was faster in processing time so if you had a large app with a lot of through put, then AWS is something to look at, but, at this point, there were so many advantages to using Azure with visual studio that it made it a clearly better choice for most applications.
Yup it does :)
Im new to c# and coding in general, am i on the right track?
Don't you need to publish the dotnet runtime using --self-contained in your publish command ?
I had no idea this parameter existed, but when I looked it up the documentation says that if you provide a -r then this is true by default.
AFAIK, all the major frontend frameworks have server rendering methods, which can be used for SEO/static generation. I'm not familiar with usage with .Net specificly, I've only used gastby with react for a personal page, so I could be wrong. However, I did find these docs: https://docs.microsoft.com/en-us/aspnet/core/spa/angular?view=aspnetcore-2.1&amp;tabs=visual-studio#server-side-rendering I believe there are services for prerending as well for all major frameworks: https://prerender.io/ I don't know if this will work for you, or if you already know about it, but hopefully this was helpful. :)
This is desktop application, not an in-browser app. How does a browser affect things?
Sorry, I just assume that all applications are web apps these days. 
Is it .net core or full? (I'm guessing the latter) 
Oh, wordpress in the mix. Couldn't guess that! 
It's for .NET Core &gt;= 2.0 and .NET Framework &gt;= 4.6. It should also work for Mono.
If this is for enterprise or professional (work/career) usage and cost is not an issue, go with Azure. Microsoft has made everything seamless and clean. Example: * Create the Azure Account * Create/Open dotnet sln/proj * Right Click the .csproj, Click Publish From there you can create a new app service and deploy to it with ease. If you were using this professionally the process would most likely include some CI/CD and DevOps practices which you could then use VSTS to hold your repos, using a self-hosted or hosted build agent, and creating some build &amp; releases which can deploy to the azure app service. If this is for a personal project and budget is tight, go with AWS. There is absolutely nothing wrong with AWS; I personally use them and began a migration to AWS at my last job. Current job is hosting on Azure and we are a full-on dotnet/ms shop.
Can you share with us the .csproj files for both your library and the console app?
Since you are using old style project nuget packages are not resolved transitively. For that you'd need to use PackageReference or sdk style project. https://docs.microsoft.com/en-us/nuget/consume-packages/package-references-in-project-files 
I don't think that this should be a problem. My application uses the same kind of nesting... Window-&gt;Grid-&gt;TabCtrl-&gt;TabItem-&gt;UserCtrl-&gt;Grid-&gt;DataGrid You can try this out yourself in a simple example app: ```xaml // MainWindow.cs &lt;Window x:Class="WpfApplication1.MainWindow" xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation" xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml" xmlns:local="clr-namespace:WpfApplication1" Title="MainWindow" Height="350" Width="525"&gt; &lt;Grid&gt; &lt;TabControl&gt; &lt;TabItem Header="Tab 1"&gt; &lt;local:UserControl1 /&gt; &lt;/TabItem&gt; &lt;/TabControl&gt; &lt;/Grid&gt; &lt;/Window&gt; ``` And the user control: ```xaml // UserControl1 &lt;UserControl x:Class="WpfApplication1.UserControl1" xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation" xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml"&gt; &lt;Grid&gt; &lt;DataGrid&gt; &lt;DataGrid.Columns&gt; &lt;DataGridTextColumn Header="Expand" Width="*"/&gt; &lt;DataGridTextColumn Header="Small" Width="Auto"/&gt; &lt;/DataGrid.Columns&gt; &lt;/DataGrid&gt; &lt;/Grid&gt; &lt;/UserControl&gt; ``` Works fine - no tinkering required. 
I'd recommend that this commercial app vendor hire some qualified people to get to the bottom of this. Or, if you want to spare yourself and some tired, undercaffeinated developer the trouble of finding the root cause of your erratic system behavior - just reinstall windows. Reinstalling isn't that big of a deal these days. Usually you are back up and running in about an hour if you are on win 10 and don't need 100s of tools and settings ;).
Thank you :)
No problem! Glad you got it working. 
He changed that, it's supposed to be the static Equal method being called and then let the object itself figure out equality if it implements IEquatable\&lt;T\&gt;, otherwise basic reference comparison.
Iequalitycomparer&lt;T&gt; does this too, it checks for IEquatable and uses object.Equals if its not implemented
thanks for the tip. What kind of blog, what should I put on there?
Thank you. :\-\)
Yes, there are too many to chose from, haha!
I have a couple of ideas, but they involve databases, mostly. They're basically solutions to solve a problem we have at work, or at least make a process more efficient. However, I know that "this" cannot be used widely or commercialized.
Hah. I have so many tools on my Win7 box that it would be a severe hit to my productivity for several days... They literally have not seen this with any other customers. They even posted in their own customer forum to ask around and nobody else has the behavior. They tried to track it down. Yes, a vanilla install would fix the issue... But then after I install stuff, it may well come back again. That's why I am asking the question
That's what I hear a lot, do something rather than following a book. I have something in mind and I will start working on it.
Nice, thank you.
Azure is probably the most documented cloud services provider for .NET Core, so you won't have much of a problem, it really integrates well with development tools too. On the other hand, AWS is also doing a fine work with .NET SDK and tools. On the pricing scale, Azure and AWS are quite similar, so it depends on the actual services you're using. Azure has some basic free resources for testing your app, while AWS free tier only offers some EC2 hours for 12 months. GCloud is focused more on bigger enterprise and tends to be a bit more costly. I've also encountered problems (EU citizen) with using their services as an individual https://stackoverflow.com/questions/32413080/how-can-i-use-google-cloud-platform-as-a-private-customer Try creating an account on both and just find out which feels more appealing to you.
Ask them to give you a build of the application with debug symbols. Have their team remote in and investigate for themselves. It's a last ditch solution that I use occasionally for our client side software. The kind of bugs you see in the field are often absolutely impossible to replicate in the lab.
Nice work! I've been working on a libVIPS JNI wrapper for my job, we might even open source it soon. There's a lot of work to be done it it though. 
I always make my own tables for it because I like adding extra fields to the intermediate table like a timestamp and a soft delete flag. Really depends what you're trying to do with your data though.
That's clearly the right way to go; sadly, they politely declined. The fact that I'm the only user in their many\-thousands\-wide userbase that has this obscure buy makes it not worth spending time to investigate...
That is a very poor assumption to keep making in the future.
I ended up going with #1 except I went further. I added [JsonProperty] on all properties that are either exposed in public API:s or stored in serialized form. This way I can be sure that any change to ContractResolver won't affect old data or external software. It also protects against accidental change in property names from affecting the data read/written. I noticed that Dictionary&lt;string,...&gt; properties were getting their keys in camelCase too. So I replaced the use of the dictionary with a class and made a test that made sure no future properties of type Dictionary&lt;string,...&gt; is added.
Just post what you are working currently 
I think like other people have suggested, you need to find something like a project that you can engage with to keep you interested. Somebody wrote a C# wrapper for a game I used to play a lot. That was my introduction to C#, I began writing code to help keep track of items in the game and so on. If you can work on a project or idea that you find rewarding, you can try bringing in different concepts you are learning into that project. 
My plan is to release at least one version of FluentMigrator with the deprecated API to ease the transition for the current users to the DI-based API, but I'm uncertain if I should call it 2.1 or 3.0...
2.1. See React 15.6 as an example of a release to ease the transition to 16. 
Looks really cool and the performance could be really really handy for a couple projects I work on. Currently using ImageMagic. I tried it out but ran into the immediate lack of libs on OSX and similar on Linux Unhandled Exception: System.TypeInitializationException: The type initializer for '\&lt;Module\&gt;' threw an exception. \-\-\-\&gt; System.DllNotFoundException: Unable to load DLL 'libvips.42.dylib': The specified module or one of its dependencies could not be found. Google around some, it looks like one major undertaking to compile the libs on OSX. 
libvips is on Homebrew so `brew install vips` should be enough to make it work on macOS. For Linux it's distro specific. To get the most recent version, I can recommend to build it from the source or get a rolling release Linux distribution (for e.g. Alpine Linux).
I always forget about these great tools for osx. Thanks so much for your prompt response! 
I just want to say that FluentMigrator is an amazing project. 
Looks nice. One question: Is it possible somehow to read file from &lt;input type="file" /&gt; element in Blazor?
Eat Shit Javascript! The writing is on the wall. Finally we're coming to put you out of business mother fucker! 
It will be interesting to see people release entire UI frameworks like this or wrap bootstrap or something. Maybe even have an common interface that you can swap in and out to get a totally different look, maybe even on the fly per user. The possibilities seem endless! Going to be awesome! I just hope they get the download size down or maybe get the runtime as a plugin so all the pages save time.
I believe there are already some people trying to wrap libraries like bootstrap. There are a couple of useful additions to do with JS interop and a new event to know when a component has been rendered which will help greatly with UI frameworks. In terms of download, the team is highly confident of the size issue. The main problem is the mono runtime itself, not Blazor. But the current mono runtime has no optimization applied at all. Plus nothing is using AOT which will add a huge optimization. I think one thing is for sure though and that is it will never be a plugin that's Silverlight territory and no one wants to go back there. 
As well as brew, libvips is also on macports and fink, so you have a wide choice. It's also on Debian, Ubuntu, CentOS, RHEL, Fedora, Alpine, SuSE, FreeBSD etc etc, so check your package manager. As kleisauke says, you can also build your own libvips pretty easily if you need the latest version. It's just configure &amp;&amp; make &amp;&amp; make install.
(though the brew one is probably the best maintained)
That's a good question, I've not tried myself yet. I'll have a play and get back to you.
I doubt it. As far as I know WebAssembly can only do what JavaScript is already capable of
https://developer.mozilla.org/en-US/docs/Web/API/File
Wow, I realize that it's quirky but I honestly don't get the hate. I think JavaScript will stay for a long time to come though.
Ah my bad, my JavaScript knowledge is more outdated than I thought. Thanks!
Waaaat?
Yikes. JavaScript isn't going anywhere.
DataTemplates is on the roadmap as is discussed on the repo. Similar to how XAML does templating wherein you could optionally override parts of the component on how it's rendered which makes 3rd party components very versatile.
It doesn't store the bearer value in the cookie, it creates a separate cookie authentication altogether. I am trying to create a stateless web api which will be communicating with front-end Angular apps. Even when using JWT scheme as default and only one, identity creates cookies and forces the authentication to go through cookie instead of JWT. Even without Authorization/Bearer header, it does authentication fine. This is what my problem is. I want to make it so that cookies won't be created and .NET Core will use JWT as the actual default authentication scheme. It should automatically check for Authorization/Bearer header and parse the user info if it's a correct one. As of now, that doesn't happen and I have to explicitly mention the scheme in Authorization attribute (atop of the controller) and parse the current user from the JWT claim.
Web.config transforms.....
It's just good practice to store any configuration information like this in a config file instead of in code. A few examples of why: -You could have multiple instances of the application running on the server. This is super common for web apps. If the connection string is in the compiled code, you need to compile it differently for each instance. With web.config, each instance has its own copy of that file but otherwise the code is identical. -If some aspect of the connection string changes at any time, it's a simple matter of editing the config file using a text editor. You don't need access to the source code or a compiler. Anybody can do it, not just a programmer. Obviously there's a small risk there as well, but it's worth it. If somebody can access your server to modify critical config files, you already have a huge security hole anyway. Basically, any time some aspect of your application is configurable, that information should be in a config file or a database or some other non-compiled mechanism.
Those are some good points. I see one slight advantage of the connection string being accessed from the DB layer is that I don't have to write extra code to pass the connection string through the layers until it finally reaches the DB layer. Although, this may cause some issues with tight coupling.
Connection strings (and any other “secrets”) should never be in source code just for the simple fact that it’d end up checked in to source control. This could be somewhat forgivable if you’re using integrated security (thus no password in the string), but even then, it’s still yucky. :)
If this is true; I’d like to know as well. Definitely does not make sense to use cookies for an API tier - auth should come from JWT in authorization header for every request
Call it 2.1 if it still have the deprecated API and remove them at 3.0
If anyone ever gets your dll, they can just use a de-compiler to get your connection string. It takes moments to open it up. It is never a good idea to store any kind of credentials in source. 
The db layer can read the web.Config or app.Config. No need to pass it around to anything. 
If you are using services.AddIdentity(), this extension method adds cookies by default. If so, you need to use services.AddIdentityCore() instead. 
It should only connect once and keep the connection alive, not make a new one for every call to the dB. Making connections is expensive yo
I guess it depends on many factors. I did some benchmarks with this a few years ago against an Oracle database and the System.Data.OracleClient api. The performance gained from reusing a connection vs creating a new one was negligible. 
Indeed. This post is clear about it: https://wildermuth.com/2018/04/10/Using-JwtBearer-Authentication-in-an-API-only-ASP-NET-Core-Project
Nothing. Matter of preference but veteran code writers wouldn’t use #if debug and store connection string in web. Config. With deployment transformations. Ideally the production webconfig infected during CI from a secret place. If you do that sort of thing. 
Not in pure blazor, you'll have to call into javascript to use filereader and then return the base64 string.
Anything you put inside of #if debug won’t be included in source when building for release. It’s not good practice to have a connection string hardcoded into your application since you will have to re release the code if the string changes. For older web forms you can use config transforms. 
All of the checked-in configurations should be setup for local development. Nobody should be able to accidentally build the app in release mode and access production, but it sounds like that can happen. Use the built-in support for this. Right click your web project and select 'Publish...' and create a web deploy package. On the server, install the [web deploy extension](https://www.iis.net/downloads/microsoft/web-deploy). Make different versions of the parameters.xml file for each environment, e.g. local, test, prod with different connection strings, passwords, etc. This way you can use the built in script and publish to each environment which applies the transforms to web.config. 
CoreRT? Still progressing https://github.com/dotnet/corert Talk about it by Mark Rendle: Build Native Executables from .NET with CoreRT - https://vimeo.com/262938007
As far as I understand, you have to build different executables for each OS and Arch? There won't be a one universal package like Java's .jar?
Like others have mentioned it is easier to change from a config file without recompiling and redeploying the code. Another advantage to using the web.config is you can also encrypt that section of the web.config to secure your connection string information.
I may be missing a trick, but isn't that the point? A single executable HAS to be built for just one platform - the alternative is a JAR, a .NET assembly, etc - which are cross-platform but require a runtime?
Yes, indeed. Although you may have the wrong definition of "platform". In this sense, it means Windows / Linux / macOS / WebAssembly, not webserver / azure / aws.
If you're willing to install some pre-release bits, or wait for .NET Core 2.1, then I reckon "global tools" will do exactly what you want. The following resources are pretty good: * https://www.natemcmaster.com/blog/2018/02/02/dotnet-global-tool/ * https://dasmulli.blog/2018/01/23/exploring-global-net-core-tools/ Other than that, maybe Powershell and Set-Alias (in your profile file)?
Preview bits are out now, but I'm not sure on the official release. The roadmap just says "Q2 2018". So some time in the next month or two, I guess? I'm really excited for it, too - looks like it will be a great feature for the dotnet ecosystem.
It doesn't help you right now (unless you want to work in the preview) but the null checking can basically go away in the next version of c# by enabling the new nullable reference types feature. With it turned on, all reference types will become non nullable by default with compiler time checking. https://msdn.microsoft.com/en-us/magazine/mt829270.aspx
If that's what you want, then your .net core .dll *is* the 'jar' package. 
Yeah, it's the normal .dll output which needs .NET installed, just like .jars need Java installed.
Everyone in this thread needs to read this: https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/connection-strings-and-configuration-files
That is not an advantage, that is a disadvantage for the reasons previously stated (most notably, need to recompile to change it).
 &gt; Running from /sdcard didn't work because the permissions on the executable didn't change even after chmod 777. If your sd card is formatted through Windows or is otherwise browsable on a Windows PC it will not support the Linux permissions model. It would need to be formatted as ext2/3/4.
All of your references to Stopwatch are via the static `StartNew()`function, which returns a new stopwatch. You need to store a single stopwatch in its own variable and then use `.Stop()` after the request and `.Elapsed` to view the duration. Yes, stopwatch is the proper way to do these sorts of timing tests.
I am so looking forward to that. Especially with my current codebase where we're using nulls all over the place. (Normally I heavily limit their use, but this time we have a lot of optional data.)
Thanks, the below is starting to look better :\) $HTTPTimeBegin=\[System.Diagnostics.Stopwatch\]::StartNew\(\) Invoke\-WebRequest \-Uri [http://$URI](http://$URI) | Out\-Null $HTTPTimeBegin.Stop\(\).ElapsedMilliseconds $HTTPTimeBegin
Are you using web API?
Yep this is exactly what I want. Glad they are taking the step to make it possible to apply it project wide. 