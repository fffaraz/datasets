So your content is indexed easily. There are workarounds, but with limited resources you aren't always able to do backend JS view rendering just to position on google correctly.
Hi, Good initiative! But who uses inline queries these days?
&gt; Sure I know, but you do know that regardless of what Microsoft might do, some companies still run XP which only supports up to .NET 4.0? XP also gets no security updates anymore, and those companies are a security risk to themselves **and everyone else**. They should update ASAP. I know this is easier said than done. &gt; Also that they have commercial libraries bought that cannot be easily upgraded to newer versions, and the IT department doesn't have budget to buy new ones. .NET 4.0 libraries can be used in .NET 4.6 just fine. &gt; Upgrading to a new .NET version won't bring more money into their main business. You don't use SSL? Newer SSL versions are not supported in .NET 4.0. Also, it may not bring money into the business, but it can very well prevent the loss of money.
Hi (nice post), It's no one's fault to think this way, Angular and other frameworks have directly/indirectly enforced every developer to give this a thought. Though Google is still not capable of crawling pages configured with client-side routing (as far as I know), it leaves no choice for all google dependent websites to choose server-side page rendering technologies. But wait, if google comes over this limitation, would we still need server-side technologies? Easier said than done! 1. Though SPA can be attractive, but it needs more man-hours to implement something simple. 2. I can hire a fresher and make him work on MVC or core within a few days. But understanding JS and JS cumbersome frameworks are difficult for the beginner. 3. You stick to a single server-side technology for years to master it, but choosing and sticking to one client-side framework is difficult as there are too many - here I complemented your post :) To conclude I would say that using a mixture of server-side, client-side platforms is the need of the hour and this mix will remain for long. So don't think that you are left behind, if you are using MVC or core. 
They serve different purposes. The major difference is MVC is server side rendering. SPA/WebAPI is client side rendering. MVC is for websites. SPA/WebAPI is for web applications. MVC works with the http protocol, web caching, etc. SPA/WebAPI works against it. If what you are building is mostly content, I'd use MVC. If it's mostly functional, I'd use WebAPI. You here alot about WebAPI because most people using it are building applications, not websites. 
&gt; .NET 4.0 libraries can be used in .NET 4.6 just fine. Not when they also have native or OS specific dependencies, that fail to install in newer Windows versions.
Have you ever worked with large businesses in the real world? Just because you say they need to do something doesn't mean they do it
this is the absolute truth. Its so frustrating. With our server software we are able to get away with higher requirements (.NET 4.6), but the client UI still runs off of .NET 4 because we have big clients, almost all in Asia, that still run XP on their desktops.
I had to build a site LAST YEAR that had to be IE 8 compatible because a multi billion dollar gas station chain had some internal software that they used that would only run in IE 8. I used to be naive and think, oh gawd, that's so dumb to use something so old, just upgrade. Then I got a job.
Agreed. 
Dude, *VB6* is still relevant (as is COBOL and Pascal). WebForms probably still has a lot of life left. 
Wrong question. He isn't asking about ASP.NET vs ASP.NET Core.
+1 for limited resources. Creating a full API is hard (new security challenges, ahoy!), then making a full SPA around it is harder. Next, keep them both in sync ;)
I don't think centurijon is talking about someone stealing code. The way that SPAs are often architected, you have to reveal your workflow i.e. all the features of your application in the HTML and JS (even if they are hidden by CSS) that every user gets regardless of their actual permissions. A MVC application by comparison can often hide URLs and UI components on a user by user basis.
I'm a little confused. Are you trying to initialize PKs in a .NET list? Is there some Entity Framework here you didn't mention? Maybe you'd rather use a dictionary to specify the Key and Value?
No problem. I've already absorbed a lot of downvotes for stating this opinion on Reddit multiple times. Components with proper encapsulation forever! You take your html and C# mix that only works if it is coupled with an action by stringly typed URLs.
I was trying to inject SQL manually by hand and wasn't able to succeed (but I just assumed i wasn't injecting "properly"). Do you know if there is a way to automate this?
&gt; Components with proper encapsulation forever! And this is why you are getting downvoted IMHO (of which I am not one). WebForms did some neat stuff and brought a lot of WinForms developers over to the web....however...they function completely opposite of how the web is supposed to work. The web, by nature, is state*less* and WebForms controls made the web state*ful*. Combine that with losing control over your markup to a certain extent and you now see why the love for WebForms is just not there. Additionally, you have to work harder to accomplish some of the major tenants of software development such as Separation of Concerns, DI, unit testing, etc. The code behind files seem to make that a bit more challenging IMHO (not saying it cannot be done, but the path is not as clean as a true MVC application) &gt;You take your html and C# mix that only works if it is coupled with an action by stringly typed URLs. First, you really haven't gotten to know the MVC architecture that well if that is your opinion. Second, we can both sling that kinda mud 'round here! One word: **VIEWSTATE**. Puke! :P I'll keep my clean, non-autogenerated HTML and markup without random hidden viewstates and control names so ~~bad~~ ugly that they make my momma say bad words. 
To be fair, if you really look at what React tries to do, they are remarkably close to WinForms controls in nature... just completely client side. In fact, a lot of the way that the client side frameworks work right now are like that... It actually makes sense, as they are contained stateful UIs on that side, whereas it didn't make sense for WebForms and ViewState (where I share your aversion)... It did take me a bit to get over that fact though once I recognized the similarity :-)
Yeah... ViewState... are we going to have a technical argument or you are going to pull the ViewState scarecrow? First of all ViewState was never that big of a problem and second it has been turned off by default for almost a decade (8 years is the exact time I think). If you are going to pull ViewState as an argument against Web Forms I can just as easily claim that MVC does not exist because they turned off ViewState before they shipped non-beta MVC. Also ViewState is actually useful. I miss it every time I have to put a hidden input on those MVC pages which happens a lot. The fact that WebForms provides a good amount of statefulness is actually pretty cool. We want our UI to act statefully. This is our business requirement. Web Forms moved us away from the protocol and closer to our business logic and MVC is dragging us back. Luckily as discussed in this thread MVC is dead and we are back to the stateful component model with frameworks like React and Angular 2. The only valid criticism to Web Forms is the lack of control over the HTML and the bad state of the auto-generated one (although they improved things a lot lately). In the time of jquery animations and such it was a serious problem that drove people away from it more than anything else.
I think I get what you're saying. I think you need to create the PartType objects and then just assign the references to them in your Part objects. Let EF sort out the keys and don't worry about them :) It will figure it out at save time.
You can put the value in a hidden input element, and the action method will pick it up as a parameter.
Your biggest issue with Web Forms is the greatest thing about it. Statelessness is protocol level implementation detail. My UI is logically stateful and my users want it stateful. The good news is that React and Angular 2 that we are gonna be using in the coming years are pretty stateful and feel like *Forms. I enjoy how my younger coworkers who started with MVC struggle with the concept and I have to teach them those old *Forms tricks that work perfectly in the brand new frameworks. The bad news is that we have to write JavaScript but I guess we can't have everything.
So here is a test I just did with specifying the key // add records to Parts table var Parts = new List&lt;Part&gt; { new Part{PartTypeID=2,ManufacturerID=1,PartName="300gb 15K SAS HD ",PartDescription="300 Gigabyte 15K Serial Attached SCSI Hard Drive" }, }; Parts.ForEach(s =&gt; context.Parts.Add(s)); context.SaveChanges(); The Manufacturer is a table similar to the PartType table just listing Manufacturers. This seemed to work. I'm not sure how entity framework would be able to figure out the part type or the manufacturer on it's own. 
The company I work for has a number of web form applications for clients that were built as recently as 2014. And we are talking about major technical debt here‚Ä¶ everything hand rolled, raw SQL queries, very little DNRY, no frameworks, no LINQ or anything of the kind, spaghetti code extraordinaire, the works. Thankfully the boss trusts my skills and judgements, and I have been switching all greenfield and yet-uncompleted projects over to full MVC. Still not ‚Äúinto the weeds‚Äù with things like repository patterns and AutoMapper, but baby steps. Better to create something that *is* easily refactorable from step one than continue bolting mistake onto mistake in the hopes that they‚Äôll cancel each other out.
I down-voted this because, while you make valid points, your comment in no way contributes to the discussion at hand. /u/Eirenarch just said that at some point it becomes easier to build a SPA... Never once mentioned a specific back-end for said SPA.
lost me here. This seems to work when hard coding the PKs new Part{PartTypeID=2,ManufacturerID=1,PartName="DELL 300GB 10k SAS HD FOR DELL R620/R610/M610",PartDescription="Dell 300 Gigabyte 15K Serial Attached SCSI Hard Drive" }, This is my attempt of trying to use the part type. new Part{ PartType="Drive",ManufacturerID=1,PartName="DELL 300GB 10k SAS HD FOR DELL R620/R610/M610",PartDescription="Dell 300 Gigabyte 15K Serial Attached SCSI Hard Drive" }, I'm actually trying to use the PartTypeName so it figures that the above would fail but when I put PartTypeName it complains right away. 
Statefulness is the enemy of reproducibility and makes figuring out what code is doing much more difficult. Not that it's wrong, eventually something has to track state, I just try to reduce the areas where I depend on state as much as possible.
I'll play around with it and see what I can come up with. Thanks for your help :0)
Technically speaking WebAPI is dead. It was merged into ASP.Net MVC with Core. In fact here's their [migration example for getting off of WebAPI](https://docs.asp.net/en/latest/migration/webapi.html). Your actual question centers around thick client vs thick server architectures in general. I can think of a number of instances where you would want the processing to be done server side including items related to security, accessibility, etc. I can also think of instances where client side rendering is preferable such as any time you need constant feedback to be sent to the user. That said I tend towards a hybrid approach based on the needs of the application. Going all in one direction or the other just leads to issues in the long term in my experience.
That's why I am using mini-Flux patterns with Angular 2! Eventually i'd like to refactor to Redux-esque single store using Rxjs.
Curious what you are looking for. Are you wanting to port clojure-clr to coreclr? Clojure (jvm/Script) was my go to for side projects from about mid-2009 through 2013, and while I still enjoy the language, it's been harder to stay interested in the last few years. F# really offers most of what drew me to Clojure (plus static typing), and with C# and TypeScript being so nice I rarely want for more (except where I need C++, but Clojure is no help there). Additionally, every time I've played with clojure-clr over the years, it has been quite slow and a bit of a pain to use. I would like to say that if the issues were addressed and it was ported to coreclr that I would use it more, but unfortunately I realistically probably wouldn't.
Is reddit buggy or something? I've clearly written that I am using hidden inputs as a poor man's view state and yet two people tell me to use them. That part is even quoted in one of the replies but obviously it is invisible to other users.
Is there a link to the source code or is this just a black box DLL that I'm supposed to drop into my codebase and trust unequivocally?
I'm traditionally a .net developer. I run my own company, which works with a number of startups, and very occasionally contract. After increasingly working on single page applications, I never thought I'd say this, but I think it's more likely that an application should be an SPA than ASP.Net MVC. Trying to get the level of user experience required for a modern site using MVC ends up with a mess of JavaScript to try to copy functionality a single page application might do. It ends up being increasingly complex, and often with a worse result. Breaking the front end and back end up with an API should generally be done as a standard now, to avoid additional work when you need to support a mobile application. Something I had anticipated less, I'm finding that technologies like AngularJS bridge the gap between all the different developers (php developers working with .net developers) more so than anything I've used in the past. There can still be an argument about what the API should be wrote in, but pretty much everyone can do JavaScript and just learn some different syntax or application.
There was a time when people said MVC took too long so they'd stick to web forms. I don't think it would take any longer to build in JavaScript than MVC, it's just MVC is more familiar than something like AngularJS. An AngularJS developer is currently more expensive than MVC though.
A lot of graduate positions don't require any experience of actual programming as long as the candidate shows potential and willingness to learn. If they get the core concepts then that's enough for a lot of positions.
About a year ago I believed that MVC was good enough when you didn't need "fancy" JS/AJAX interactions. However, after working on an Angular site or two I realized that SPAs are just easier and allow for more options going forward. That is, if a project is already SPA you are in a more advantageous position. If you want to change am MVC project to SPA, good luck with your rewrite.
Your redirection of my joke is just 200.
Sitecore is moving fast and steady to MVC land too.
&gt; That being said there is still room for server generated HTML specifically where SEO matters. I wonder how long it'll be before the big engines start rendering javascript? Seems like Bing or Google would be all over that, since it'd give either of them a big advantage, and both have a major web rendering engine at their disposal.
&gt; Trying to get the level of user experience required for a modern site using MVC ends up with a mess of JavaScript to try to copy functionality a single page application might do. It ends up being increasingly complex, and often with a worse result. This has been my experience as well, so far. I was really excited to redo some old webforms projects in mvc, only to realize that React did it all so much better. 
One way would be to bind to the folder and just iterate down all the files. Don't worry about the names. Perhaps try an asp.net webforms project if you'd rather avoid javascript. I'm kind of shooting in the dark here since I don't know what you are capable of. That's a start anyway.
I think there are more than 2 valid criticisms of webforms, scalability and performance being the biggest two. My personal pet peeve with webforms is it abstracts too many things away from the developer and allows junior developers to write god awful code that while it works, it sets up a maintenance nightmare.
There's always reflector!
Still pretty floored that this was skipped and the time frame is no where near. 
I think there's still support for WebSockets in Core... but I could be wrong.
yes -&gt; https://github.com/dotnet/corefx/tree/dca1b0fcc8ec1d1dbe42d56222b18fcc5307d32b/src/Common/tests/System/Net/Prerequisites/Servers/CoreFxNetCloudService/WebServer/WebSocket https://github.com/dotnet/corefx/blob/d0dc5fc099946adc1035b34a8b1f6042eddb0c75/src/System.Net.WebSockets/src/System/Net/WebSockets/WebSocket.cs
True!
In my opinion AND experience junior developer write worst code with MVC. Things like SQL queries from the views are much worse than anything I have seen in Web Forms. Also even experience developers tend to put business logic in the controller. Business logic is part of the model (as is validation) but I doubt even half of the devs realize that. 
I've read a couple places that people have SignalR 3 working with .NET Core, but you have to build it yourself from the source at https://github.com/aspnet/SignalR-Server. Albeit, likely buggy right now.
I'm using SignalR from the CI feed with .Net Core: https://www.myget.org/F/aspnetcirelease/api/v3/index.json
I think there's a lack of variety on the CLR in general and wrote about it on my blog a while back [here](http://coderscoffeehouse.com/tech/2015/10/13/dotnet-needs-more-languages.html). I'm hoping .NET Core will change a lot of this as the JVM now has a real competitor in the open source world.
Doesn't Rx (and the .NET Core version System.Reactive) just make implementing the observer pattern more palatable by giving you LINQ?
Not the simplest (or cheapest) solution but NServiceBus (or other frameworks like it) will allow for 'real time' communication. 
I would agree with this. The only thing I would add as advice would be to try to separate out your work flows (DB, logic, API, UI) that way if something breaks you only have the one part to investigate. Again hard to say really as you may have already done this. 
Check out this project. https://github.com/AdaptiveConsulting/ReactiveTraderCloud It is using crossbar. http://crossbar.io
In webforms you can execute SQL from xaml and put business logic in the code behind and in my experience I've seen both. With MVC you at least have to have some awareness of HTTP, javascript, HTML forms, etc. I'll grant you that webforms makes it easy for a less experienced developer to churn out an app that will work and do it quickly, however if you're building an app from scratch that would need to scale and be flexible webforms doesn't come close. 
The benefits of finally having ready public access to source code! Thanks.
Well if that's where your priorities lie, then webforms is perfect. I actually like neither framework. I prefer SPA style apps with web APIs using JSON and MVVM in the front end. Total separation of concerns, unit testable, modular, scalable, flexible AND you have far more control over all the different parts of your application.
I do prefer the SPA dev model but I'd rather take a desktop app both as an end-user and as a dev who has to build it. Too bad that I get to work with desktop apps less and less on both fronts.
Not a big priority for them. They decided to use Json.NET for their internal serialization needs and unfortunately mixed those concerns with the serialization needs of the consuming application; i.e. using same JsonSerializer instances. Then the direct dependency on it spread throughout the code base to the point where they are hesitant to refactor that dependency behind an interface. As a result, you don't even have serialization options for a protocol which should support both text and binary content. It's Json.Net or GTFO. 
You might try the [SuperSocket](http://www.supersocket.net/) server. It doesn't have many dependencies so it'll probably build for Core. It has its own implementation of websockets that doesn't require Win8+, and you can specify handlers for both text and binary message types and parse them however you like. About the only things it doesn't do out of the box that SignalR does is protocol fallback (though again it's less necessary since you can run websockets on any OS), that chat-room grouping thing, and session recovery. It would be easy enough to add the latter two if you really wanted to, though. edit: Oh yeah, there is one pretty notable downside if you're running in Win7/Server2008: IIS flat-out can't figure out what to do with websocket traffic, so you can't even reverse-proxy to a standalone server that does. You can http-redirect to another port and have the other server listen on that, though.
I'm a beginner at AngularJS , is it time to switch to Angular2 or should I focus on the former?
Thank you for the detailed response
At present the SDKs are distributed as a binaries through NuGet, not open source. Your actual data never leaves your application. The only external calls are for the encryption keys and license requests which is by design; if your application or data store gets breached the keys are not stored under the mat. We have sample apps you can play with at: https://github.com/crypteron/crypteron-sample-apps We're currently securing sensitive law enforcement data, medical records and financial records in production for other organizations so we hope you can trust us! Here is one customer success story: https://www.crypteron.com/case-study And you can also always use reflector. 
I'll do that! I'm actually getting ready to start work on a new PoC project implementing http://www.primefaces.org/primeng/#/ (Since Angular Material 2 is nowhere near production ready). Your base app will serve as a great boilerplate to get started from. I'm also going to be teaching myself Angular2 (pretty fluent in 1.x) as part of the project, so whatever examples / etc you can provide will be certainly helpful!
I am very glad you liked it! I am using primeng in one of my projects and can recommend it. I will definitely be posting more of that. Probably next week I should blog about testing in angular2 - will add it to this project :-)
Yeah, I asked (https://redd.it/4y63sm) about UI toolkits for Angular2 just over 3 weeks ago and no one could really recommend anything better
The bootstrap one is also quite good and stable. But it's bootstrap ;-)
Well that was an interesting requirement to say the least. I'm going to resist the temptation to ask "why" 
There is a meetup group for dotnet that just did a talk about this subject. 
I'm definitely not a port/socket programmer. So i could be saying shit, but try to use some kind of software to find if that port isn't used by any other software. If you said it was working before. It's the only thing which comes to my mind 
Dan, out of curiosity, if we published the source code on GitHub, what would you really want to do with it? I mean how many hours a week could/would you spend on it? Genuinely asking since if we ever go down that path, we'd like to make sure we've got processes in place to handle it (e.g. pull requests, review/audit 3rd party commits etc). Personally, I'm not a lawyer and don't have qualms about the source. But for intellectual property purposes, our legal council prefers closed source or release to a large firm (e.g. tier 1 bank) under a tight NDA.
Just wondering, how do you know that the event isn't being fired? Essentially what I am asking is, are you relying on a breakpoint firing off to see if it is working? If so, what I have noticed is every now and again visual studio, for some odd reason, will stop hitting breakpoints, almost as if it no longer generates pdb files etc. Did you try to clean your project? (Build --&gt; Clean &lt;application name&gt;) That is what seems to work for me when stupid things like this happen...
Yep done all that and yes I was relying on breakpoints and the simple fact that the code had that event fired, would have been evident in the GUI. However I've now fixed my issue and I'll edit my post to showcase the idiocy, but it was the capabilities checkboxes. I had checked Client and Server when I created the project, and everything was working, however I switched my network to Private so I could turn on file sharing for my SO the other day. I didn't make the connection until just now but that was when it stopped working. There's a separate capabilities checkbox for client &amp; server on private networks.
Don't use WebSockets, It't unnecessary painful, and if you want diferent clients it's nightmare... Use SignalR, I heard it will ship in core vNext later this year or in the beginning of the next,w e just have to wait, or build from source...
If you published the source code on GitHub, at the very least, I'd clone the repo for myself and audit it before allowing it anywhere near our codebase. In terms of hours spent in a week on it, that depends on what I find. If it all looks good, then I probably won't touch it unless I think of something else that can be done, but would then submit an issue or pull request. Either way, in my opinion, security by obscurity (by being closed-source) provides a false sense of security because some people will just blindly believe that the software is good and can't be broken because nobody can see the source code. I'm not saying that you're that naive, but the advantages of being open source are far far greater. Take, for example, the possibility that something in your code is a bit janky. You've got something going which maybe takes longer than it should do. If I run perf tasks on a black-box DLL, I'm not going to find much out. All I'll see is a time-suck between a call to the DLL and when it's returned. That's not going to help me much. On the other hand, if I can see the code, I can step through it and debug it, understand what it's doing and perhaps let you know that you've got a bug or potential perf improvement in your code. So you've got the benefit of a large number of developers, **for free** helping you make your product better.
Thanks for the feedback. In general, I don't trust anything that's provided 'as-is' unless I can see what's under the hood so that I know it's not doing something I'm not expecting. I work with a tier-1 PCI DSS company so ***everything*** is audited, and if we don't know or can't explain a piece of functionality that we drop into a codebase, then we can't let it into our code. It's that simple. And the best way to do that is to audit the code, usually line-by-line. Whilst I do use Reflector, source code can be obfuscated enough to make disassembly an extremely difficult and painful process. And whilst I appreciate you suggesting it (which makes me believe that you aren't doing any obfuscation), it's not usually my first port-of-call.
If you are using Signal-R, maybe. Otherwise what would you be pushing data to?
http://stackoverflow.com/a/21147074
You can use it perfectly fine with web forms.
If you want a quick way to interact with a database, the most recent implementation is Entity Framework. You'll see it used in many tutorials. If I had to start over, I'd read about some of the concepts to get a better overview of what's going on, rather than just dive right in. Some suggested research: * MVC pattern * Models, model binding, and using Data annotations for validating data * Views, partial views, ViewBag vs Model * Controllers, action methods, ViewData vs ViewBag * Routing
This is exactly what I was hoping for. I will be researching this! Thank you a lot.
sorry a bit of a side question. How does the hosting work with this sort of project setup. In Azure, for example, will i need to host the SPA and the API on separate azure websites? 
The official asp.net core site covers all these topics in detail - https://docs.asp.net/en/latest/. It includes end to end examples too. 
no - it's all in one project, so only one host is needed. It's just modified routing which makes it serve index.html for all requests.
Yes the music store is a start. But I am trying to find something more substantial that uses common design patterns and I can have an explanation for all the code along the way
Active mode FTP connections will use a random port above 1024 for the data connection. Even Passive mode FTP connections can use and port they wish above 1024 for the data connection. That probably isn't your problem, but I just thought I'd mention it.
Great write-up! I recently discovered the CommandLineUtils package myself and started playing around with it, pretty cool so far...
One thing to be mindful of is that we are the cusp of ASP.NET Core being the standard, which offers some significant differences from traditional ASP.NET development. If you're surfing around tutorials just be sure to check which one they are geared towards.
Ive went the through books and there is only one that does what I am looking for. It builds a 'SportsStore' from scratch. The core docs are ok in that it helps you build and deploy a simple crud app and go from there
Can it run on full .NET and mono?
VSTS will do this, among other tools.
MVC is for Web, SPA for Browser.
&gt; Can it run on full .NET and mono? The dependencies are pretty minimal and looking at the source full .net is supported but I dont really use mono so I can't speak for that.
&gt; I understand why SPA is a good choice for live data, with two-way data binding, etc. Besides that, I don't. This covers a *huge* number of use cases, right there. &gt; Bonus question: Are SPAs Web applications? Yes. What's even the point of this question? Look, don't get me wrong- I think SPA is an abomination that misunderstands the purpose of browsers and the very nature of HTML and HTTP (it's documents, stupid!). But it arises from an understandable place. It's a "we're doing our best" adaptation to a shitty set of legacy technologies. The DOM should be burned to the ground. JavaScript? Don't even get me started. But since these are here *now*, since they can provide a rich client interface *now*, since they're cross-platform *now*- we use them. It sucks, but thems the breaks.
Huge enough to become a default architecture? That's my concern.
Here's the thing - technology changes, and it changes rapidly. The browser started serving documents - and the the browser became a runtime in itself. A standardized, cross-platform, distributed runtime. HTTP became more a vessel for distributing data and for abstracting business and domain logic. The browser became the glue that tied everything together into something that somebody directly uses. Of course they're web applications - there is nothing in the definition of a web application that states URLs need to be generated from the server - that's nonsense. Do you use it via a web browser, over the web, over HTTP? Then it's a WEB application - maybe just a tweak on your current understanding. SPA is a good choice for anytime when you want the end user experience to be more complex than page refreshes with some scripted component behaviors. I find both paradigms to be useful, and I've written both in the last year. They are becoming the default largely due to their being a natural fit for integrating a large number of web services - SOA style. Pass off everything that is safe to the runtime in the client, and focus on the efficiency of the backend.
Distributing things like rendering and managing the instantaneous state of the UI to the browser seems to make sense from the standpoint of scalability. Especially when you consider how well such an approach works with eventual consistency models. Also, stateless services or APIs are easier to test and manage. Modern server side development has been moving to API type development for a long, long time. I'm not sure where you work or what you work on, but the SPA or similar (mostly client side) architectures have largely been the "default" option, at least for most modern web applications, for a number of years already. 
I really can't agree with you on that, but I respect where you're coming from. I just think the notion that HTTP is "documents, stupid" is antiquated - and fails to acknowledge the advances since WebForms was the standard. Sure, better could be done - but it's contantly evolving technology, and the advances deserve their credit. What we can build now with these "oh-so-horrible" technologies is frankly amazing. With all due respect, I passionately disagree :-)
so if it runs on the browser it's a web app?
The SPA hype is about **user experience** and **performance**. You can have complex web applications without constantly making web requests. You have one static HTML, JS and CSS which is aggressively cached (so only loaded once), the rest remains in small web requests, only transferring the data needed. Compare that to WebForms development where **huge states** are constantly shipped around, and HTML is always rendered on the server according to the applications needs (so it can't be cached). &gt; Developing with browser technologies (HTML/CSS/JavaScript) instead of server technologies. I don't see why this is a problem. You never really developed full applications with "server technologies" anyway. You always still had HTML and CSS with JavaScript sprinkled here and there. I'd actually say this feels more alienated on the server side than on the client side. &gt; Developing in a stateful manner, rather than stateless HTTP. SPAs don't change much here. It makes no difference. With traditional web applications you can have state, with SPA you can have state. I'd actually say SPA are more fit for stateless web applications, as the state remains solely on the clients side and doesn't has to be stored on the server side. This benefits scaling out your server applications. &gt; SPAs are backwards. Instead of having the server return something that the user can understand, it returns raw data, and the client must transform it to HTML for human consumption. This makes... no sense. The user doesn't understand HTML, which is returned in traditional web apps. He relies on the browser to render it for him to consume. Why is that such a difference for you? The rendering of the HTML is just moved from the server to the client, which makes no difference in regards of the user perspective and what he understands. &gt; I understand why SPA is a good choice for live data, with two-way data binding, etc. Besides that, I don't. They are a good choice when you want flawless user experience. You heavily reduce the data that is transferred (see above) and the application can behave more fluent. You can give the user instant feedback, instead of the traditional approach where you load a whole new page and stare at the browser loading animation for a while. &gt; And you can always do AJAX with HTML instead of JSON. After all, the X in AJAX is XML. The X in ajax is not XML, it's the "XMLHttpRequest". It's a very old API that works and does the job. It's not tied to XML in any way. It was merely used by the inventors to retrieve XML (because it came from Microsoft). &gt; Are SPAs Web applications? I ask because I don't know. If URLs are generated on the client then it cannot be considered Web. Why are they not web applications, just because they can adjust the URLs on the client side? That makes no sense. Frankly, your whole post reeks of ignorance. It sounds like you're afraid of new technologies and advancements (to the better or the worse) and just want everything to stay exactly as you know it.
My favorite sites are not SPAs
Github is a mixture. Parts of it are SPA, e.g. the surfing through the source. SPA simply means the data is retrieved via AJAX calls, and instead of completely navigating the user to a new page it will just load the data and re-render parts of the application. He stays on a single page. AJAX is just a means to make HTTP requests possible from JavaScript - they make SPA possible, but AJAX itself is not relevant to the concept.
Yes. Of course. Is this a serious question? Would you really dispute that Gmail is a web app? It's amazing to be able to do SPAs because you write the API once and are forced implicitly to have good separation of concerns. It's been this way for a while now, but as usual the .NET community is a few years late to see the writing on the wall. MS even tried to encourage it with Web API 2 and then gave up and rolled it back into MVC.
No, we have a definition discrepancy. On GitHub: - The logic is on the server - URLs are generated on the server - Links are progressively-enhanced with AJAX which retrieves HTML content to avoid a full refresh SPA: - The logic is on the client - URLs are generated on the client - HTTP is used as a mere database (API)
Please inform yourself about SPA before asking further questions. You're completely missing and ignoring important points, and really don't want to repeat myself.
All that is true. I get what you're saying entirely. The post certainly deals more with having replaceable parts of a project not being super reliant on one another. I think the idea is that one project does not care at all how the thing is implemented, and DI takes care of making sure it has a thing that does the interface methods. If the implemented class is broken, stuff does break though. There is the case where you could have several things implement one interface (like maybe one repo does caching) but I have yet to figure out how to make the DI differentiate between the two so I can use the same interface but not always have the same implementation. Something I'll work through once it's necessary for something I'm working on. I'm certainly still learning so this is all the kind of feedback I want! I want to learn more. That's the whole point of all this. I think you are right that I need a better title that more accurately reflects what the post is about. I will have to think about this.
thanks. maybe a stupid question but what is vsts and can it work on a mac
You gotta love it when people wrap their repository in a repository.
Thanks! That's the first compliment I got on an article I wrote, nice :D #lifegoals
If you are working towards aspnetcore, check out my project https://github.com/dodyg/practical-aspnetcore
I am working on to solving this problem https://github.com/dodyg/practical-aspnetcore
I meant the "documents stupid" to apply to the HTML. But sure, we development is constantly advancing, and it's almost where client side GUI development was twenty years ago. 
From a business point of view, we have seen SPAs make sense because few clients want only a website. They want a website, an iPhone app, an Android app, and the assurance that someday they can have whatever other cakes they want and eat them too. The plan then becomes to make a stable API and then access it via SPA. The same APIs can be reused across other client apps without some funky middle layer translating it to server rendered HTML. Believe me, it's a hell of a lot more work than just bootstrapping an MVC site. If it doesn't make good business sense it's really hard to justify the effort. Otherwise, it can create good wins all around :)
For .Net core apps: https://github.com/aspnet/dotnet-watch Just run 'dotnet watch build' from the terminal
It's still an HTML document, just dynamically built by Javascript. At the end of the day, it's just HTML, CSS, and Javascript.
SPAs use URIs only because it's too important for users. In the beginning lots of SPAs didn't support history &amp; back/forth buttons. You cannot write a program to download a page, using its URI, from an SPA. You **have to** go through the browser.
I think you hit the nail. It's mobile that screwed up the Web.
Either Unit of Work pattern, or just give your DbContext additional interfaces if you want that.
Inject IDbContext. If you start wrapping it in a repository or unit of work you're just waiting effort. The DbContext already implements both of those. And as for the argument, you can swap out your ORM. I've done that once in over 10 year career, it look 4 hours and still required changes to use the new implementation. Additionally using a repository to wrap your ORM means you can only use features that they all implement. Anything that the one you actually use does better than anything else is abstracted away and isn't usable.
You are conflating things. SPA is independent of server architecture. Well designed servers support *any* client. SPA is a client architecture. It has no bearing on how you build your server. How you build the client does have dependence on the server. You mentioned bringing back data and having the client transforming it. This is flawed in several fashions. First, if a client needs to actually transform data: mutate, aggregate, slice, join, etc this a clear symptom of a badly designed system. The system does not return meaningful responses. Most likely this is the anti-pattern of CRUD as a system, or said another way the system as a database expressed over a (super slow) wire. Second, assuming you mean binding and not transforming. Merely the act of getting json to be visible to a human on the UI. This is *normal* but can very costly to performance. Now reframing your premise. What you return from your server has many options. You can return json, xml, html, Excel, csv, tsv, binary, and so on. Every media type is available. Don't assume json everything. In many systems, if not flatly every (single, not shared for 3rd party api access) system, the best solution is have the server do over 90% of the work. This means the server returns html in ajax response. Note this doesn't have to be html, it could be react jsx, it could be a custom descriptor language relevant to your system only. Now unless you're Netflix I wouldn't recommend building your own UI template definition. Returning (partial) html is common. When gzipped the payload is only marginally larger than json. Nominal server responses should be under 11KB so they transfer in a single packet. This is trivial to achieve in a well designed system that returns fully transformed meaningful ready to consume data. Inserting almost fully hydrated UI components is about as close to free as you can get. Now less theory crafting, real world. This is super easy to achieve. Server is ASP.NET MVC for razor. (You do not use Web api as it doesn't properly work with razor.) If you want to support content negotiation is trivial to handle in a base controller or action filter. On the client you can use anything, but I strongly recommend investigating IntercoolerJS. IC is just amazing. It takes all the pain out of standard development. It is my goto starting point and I only move away for the utmost levels of control. 
You're conflating things also. The web has **not replaced** client server, the web has **codified** client server is the **only method**. Everything is client server now. The only question is whether people are building DCOM over REST (comically bad) or building properly bounded and well defined SOA systems 
&gt;Distributing things like rendering and managing the instantaneous state of the UI to the browser seems to make sense from the standpoint of scalability. What does this statement even say. I currently believe it is wrong but it's borderline incoherent. 
Screwed up the web? Mobile is the best thing that ever happened to software development 
&gt;The SPA hype is about **user experience** and **performance**. &gt;You can have complex web applications without constantly making web requests. You have one static HTML, JS and CSS which is aggressively cached (so only loaded once), the rest remains in small web requests, only transferring the data needed. Compare that to WebForms development where **huge states** are constantly shipped around, and HTML is always rendered on the server according to the applications needs (so it can't be cached). This is a very false dichotomy. View state while overhead can be negligible, it's developer choice to let it to be dozens of kilobytes. I concede it's difficult to build a properly cacheable website on Web forms but that's because caching is insanely difficult to do properly to start with. Web forms only makes it a tiny bit more so. However webforms has some amazing caching options with VaryBy on the server. In terms of raw performance web forms has always exceeded MVC. You likely were unaware of that fact. Of course even with this fact I would rarely recommend webforms as the solution but I wanted to highlight you likely have multiple misconceptions about this technology. Lastly I'd add that very very very few systems want a browser cached website. Browser caching is important for resources, images, CDN hosted script dependencies. In almost every situtation where a user invokes your application, you want that to hit the server. The server can return not modified, serve from an in memory cache, or do a full request. You want the server to decide that, not the browser. 
But that's just the thing. You're starting from this perspective that something has "gone wrong" because of SPAs. Nothing has gone wrong. Mobile nor SPA frameworks have done anything wrong. They've just evolved, and as a developer who wishes to stay relevant you must evolve too. Nothing is wrong here - is actually good. As devs we change over time, and thank goodness we have. Those of us who haven't are still working in web forms. I was a reluctant part of a disaster project for a national retailer POS system written with UpsatePanels - two years ago. The devs had not modernized, and wrote an abomination project that failed miserably. Millions lost. Change should be embraced on this arena. Otherwise you might just get left behind.
Github is a great example of pinnacle level modern development. They leverage all the good parts of SPA while still maintaining http navigation for discrete activities. There's really no reason the Notifications page would have any view information of the Repo page, but in textbook SPA you are supposed to have the single app and both views. I recommend many mini SPA apps just like github does. You can leave comments, edit them, all in line with no visible post backs but you navigate to a separate app for the Repo.
I didn't think they gave up so much as realized there was so much in common that two frameworks was silly üòÄ. To be fair, you can write a server web API in webforms and have been able to for years. They didn't give up, just optimized
Arbitrary distinctions. If you use this app over the web it is a web app.
I think GitHub and Basecamp have done an excellent job of finding the middle ground between 'full SPA' and WebForms. Sure, there will always be some need for 'full SPA' (dashboards, realtime, etc.) but by and large I believe that there is way too much hype around it. Edit: I forgot to mention specifically that the thing about caching server-rendered HTML is not very black and white. The server can easily cache HTML fragments and handle things specific to the logged in user in various ways so as to not thwart the ability to cache fragments.
That's an interesting way to look at it. I didn't mean "client-server" the network architecture [even a thin client is a "client", but that's a system architecture rather than an application design]. I meant "client-server" the application design mindset. In a "web application" mindset _the web browser is the client_ that displays a highly constrained category of objects. In the SPA mindset, the web browser is a _host system for the client software_.
Just be careful with Entity Framework though. ORM's are powerful tools, but their lazy loading features can run a database into the ground. Lookup the n + 1 problem of ORM's and make sure you never encounter this problem on production. I advise to simply ignore lazy loading and just eagerly project all your queries to viewmodels.
If, instead of streaming the mp3, can you download it and play the mp3 locally in WMP without that codec? Maybe WMP just can't play mp3s by default.
I'm moving a db app I'm developing from Nancy to Core, and I'm quite impressed with the framework so far. MVC5 is too baroque for my taste. 
I'd add Dependency Injection to the list. It's central to ASP.Net Core.
oke, I think Im going to first learn more about c#. I have not studied the Dictionary and how to make a class definition to hold this
For your example you should make an array of items instead. Then you can have as many as you want with just the one model. [{ 'color' : 'blue', 'form' : 'triangle' }, { 'color' : 'blue', 'form' : 'circle' }] which would map to a List&lt;item&gt;
oke, and how does the model then look like ? 
oke, That was a example what I made up. What I try to do is make a app which displays info on a painting with this output : http://lpaste.net/ and I need to display only the title of every painting 
In my opinion it isn't. It couples your application to the data storage model. You may initially want this to be the same as your domain model, but you're forcing yourself to have them always be the same or else be jumping through hoops to make changes. 
Sounds like you should study some C# fundamentals. If you have access to it or can afford to subscribe, [pluralsight](https://www.pluralsight.com/) is great. To answer your question, public class Item { public string Color {get;set;} public string Form {get;set;} } Then, in your controller, public ActionResult ControllerMethod(List&lt;Item&gt; items) { // You can return this list of Items return Json(items, JsonRequestBehavior.AllowGet); }
Looks like you are trying to implement localization for your site. The path you are on *can* work but it will not scale well and it will require significant work if you decide to add a new language in the future for example. Have you considered using Resource Files? These would allow you to create "language dictionaries". For example, you may need to display the word "Hello". In a resource file you would create a key "Hello" and the word to use for this key in each language you wish to use. Like this: Key | English | Russian Hello | Hello | –ü—Ä–∏–≤–µ—Ç Then, in your code you would simply have &lt;div class="greeting"&gt;ResourceFileName.Hello&lt;/div&gt; and with little setup, using the browser's localization features, it would automatically display the word in the language of the user's browser. A big advantage of this method is that you would not have to create a copy of each html page for each language you wish to use.
We prioritise view model testing over UI testing (WPF), mainly because it isn't as easy to test views than it is to test view model behaviour and business logic. In fact, most of the behaviour is in the view model so we barely have any code behind for views because data binding negates the need for it. Any view code we have is tested manually simply because it's *hard* to automate it. When you're done with your framework I'd like to take a look and see if we can improve testability.
Sounds good. If I can get it to start working on Windows 10 I'll make another post. 
personally, i tend to create new tables and relate login information to the AspNetUsers table, that way i'm not altering the skeleton MS has generated. you could use the same *ApplicationDbContext* if you'd like, or create a new one. in your case, a table *UserShops* would have two columns for **Login** and **ShopId**. if a user only has one possible **ShopId** you can make them a unique key. 
good suggestion, just wondering though any idea why the table isn't updating ?
I thought people had moved away from layers and over to [slices?](https://msdn.microsoft.com/en-us/magazine/mt763233.aspx) If not I would recommend it 
Well... carry on then :)
ah alright, let me look in to this somemore. so far it points to having 2 db contexts. if nothing works out ill probably look at using a new table. thanks for the help!
It looks like bug in their generated interop wrapper code around COM objects. They should use .NET finalizer to clean up native resources after wrapper object is collected by GC, but they does not do it. After all, finalizers was created in first place to clean up resources unreachable by GC on garbage collection. Classic .NET close Windows handles at finalizer, for example. Windows handle wrapper class do it in finalizer after GC collect wrapper object. But UWP platform interop code isn't follow this pattern probably.
I think your original post was how I found Nimble Text that I still use a lot. Not much new for me this time around but still a very comprehensive list. I wanted to check out ELM but it seems the the domain expired and some one parked an ad host there.
Could be. I'm just starting to look into it myself.
So sorry - the link I had was incorrect! Here's the correct link for Elm: http://elm-lang.org/
Have you had any issues with Windows 10? I could potentially switch the framework to wrap CodedUI instead of Winium if it is going to be a better solution. 
Holy shit. The real news here is that there are developers writting UWP apps.
But someone should report this mess to Microsoft and kick they so they could fix these high-priority interop issues in UWP. If nobody care it wouldn't be fixed forever. In classic .NET there was similar interop issues (as I have found), but in classic .NET interop scenarios was relatively rare. And in UWP COM interop became blood and flesh of application code.
So everyone who planning to develop UWP apps should kick Microsoft to address critical platform issues. UWP platform isn't mature and vital feedback from developers is needed!
Or use Oz-Code.
They have a plethora of different options from Educational licenses (Free) to Discounted start up licenses, check the link here: https://www.jetbrains.com/resharper/buy/#edition=discounts 
Yup, there's a free licence for non commercial open source stuff. Seems to fit the bill.
Poor bastards. I looked at it for a while, then noped the fuck out of there. Thought the old 'wait until MS release v3' adage was old hat these days, but apparently not.
It's already v2 as v1 was Windows Runtime apps on Windows 8/8.1 Microsoft have chosen WinRT architecture as a foundation for UWP apps. **Most controversial part of Windows Runtime/UWP architecture - it's COM-based at its core**, probably for straightforward C++ compatibility. This approach isn't ideal for obvious reasons as 1. COM is an aged technology which was known for its over-complexity and was indeed painful even for C++ developers 2. COM doesn't provide any support for automatic memory management via garbage collector. Reference counting is a stone-age approach. Even modern C++ standards provide some foundation for garbage collected memory management. COM doesn't provide any support for external memory management either. 3. .NET was well-known for its high-overhead COM interop and some COM interop memory issues. Windows Runtime/UWP don't fix these issues. 4. In the not so distant past, COM was viewed as somewhat legacy technology with .NET to eventually replace COM in Windows app development. Windows Runtime/UWP is a huge comeback of COM into modern Windows development ecosystem. Modern platform based on very aged technology from the past? Very controversial decision! 5. Mostly .NET-based WPF had incomparably less issues and was much feature-rich if compared to UWP XAML platform. And WPF had **no** interop overhead for standard framework controls and classes. If we compare XAML UI speed of WPF and UWP on same x86-based desktop, well... UWP literally sucks. Even lame Silverlight have performed much better on ARM-based Windows Phones than Windows Runtime/UWP. It's all because of COM foundation of UWP platform. 6. .NET Native compilation was invented probably because of awful performance of COM/.NET interop on Windows Runtime platform. Well, this situation can be changed by extending basic COM platform on UWP to support modern needs without breaking C++ compatibility. But in current state of things ancient COM foundation of the modern UWP platform is a most problematic piece which generally dragging platform down.
It's elegance is in how ridiculously simple it makes most things. It's even built purely with developers in mind and doesn't try to be too opinionated or dogmatic. It blurs alot of lines that large frameworks separate, but it makes all common ajax scenarios be handled in minutes. It even makes using CSS3 transitions a breeze so it can create powerful experiences with trivial implementations and in many cases zero, or less than 10, lines of handwritten javascript 
Do you have any experience with Mass Transit, and if so; could you compare it to EasyNetQ? 
Yah...licensing's a b330tch, huh? Bummer.
Now Postman offers desktop apps to since Chrome dropped its web app support
If you want to use c# and import the spreadsheet, look into the spreadsheetlight library. I use it for generating excel docs, but it can easily read and parse.
R# is great, but I'm disappointed that after 3 years, I'm still waiting for this comma-first line break formatting issue to be resolved. Please support the issue if you agree. https://youtrack.jetbrains.com/issue/RSRP-380962
On custom types you can also just override ToString if you hadn't otherwise intended to use it :)
Although you want to mimic as much as possible production environment in your local, there are aspects that should not be made the same between these two. In development environment, you would need more responsiveness /faster development cycle (that means better debug information, auto-refresh after code changes, etc.); while in production you need it to be more lightweight and fast and secure. That explains the *development* and *production* images. *Build image* is for the build server itself, not your application. When setting up the build server, you would need to install dependency like msbuild.exe to make it works, this *build image* handles that job.
It provides plenty more features and is absolutely worth the price. And 79 $ is really not that much for what you get. :-)
Microsoft internal politics FTW. It was clear that WinDiv (which has always hated Dot Net and anything non-C++/COM) won out over DevDiv. Around the time of the Longhorn Reset, I remember reading articles from insiders that said that a lot of the problems arose from the the fact that the WinDiv teams insisted on writing Dot Net code as if it were traditional windows, and refused to make any changes to account for the differences in platform. There's stuff to like in UWP, but with Windows Phone circling the drain (or maybe caught in the accretion disk around the black hole of dying technologies), I just don't see the point at the moment. In my corporate life, we're still on 7 (and will probably stay there for a few more years) and our LoB apps are moving to the web. Which is the subject of another rant... 
Never seen NimbleText or OzCode. Both look great, might have to get some licenses.
I just find out that You are 100% right. I removed in example third property from code.
And, most serious problem with COM and UWP is - COM itself haven't evolved for so long time. COM absolutely must support garbage collected memory management, at least as an option by introducing new interfaces which if implemented could allow COM objects to be garbage collected by external memory manager instead of built-in internal reference counted memory management. All modern languages are garbage collected, even C++ standards have foundation for garbage collected memory management. COM indeed have potential to remain low-level foundation for Windows platform, but it should evolve to support modern development needs.
&gt; 7Zip‚Äôs context menu requires two clicks ‚Äì WinRAR‚Äôs only requires one. Not if you uncheck "Cascaded Context Menu"
Have you been having any problems upgrading Angular2 with each new RC release? Are you using it in production yet?
"To me, that‚Äôs like trying to get me to eat my veggies by telling me how great veganism is. If I don‚Äôt want to do it at all, trying to sell me on an entire lifestyle is not going to work." Love it.
Well, there are a couple of things at play here. I recommend starting with the MVC boilerplate and fiddling with it to understand how things work and what you can tweak. There are a ton of guides out there too. However, if you have to go several layers deep to understand whats calling a function or how it works, you might have inherited a poor architecture and code layout. Sadly there isn't TOO much you can do about this without some help from another dev with experience, especially if you are new to the language. Honestly though, after working in 4 different languages professionally and sampling almost all of them I could find, C#/asp.net/mvc is very direct, simple and lends itself to clean code and architecture. Of course a tool is only as good as those that use it, so your codebase may be very complicated, but don't give up on it. I wish I could help more, but not much else we can do without look right at the code.
Hey! Well i guess you could try and figure out the architecture style you are using first and foremost is it MVC is it MVVM .... I believe these styles are language agnostic but provide you some clues as to where you might be in the architecture. Typically these styles are defined by an Onion architecture. where you have some code that process view logic and eventually calls code that contains business logic until it eventually calls code that contains some logic to address data storage. Has the language in itself i don't know much about Ruby on Rails but C# is standard Object Oriented programming where typically you have a class that contains enough logic for a single purpose and you instantiante it somewhere in your code to fullfill the purpose. This link below contains the basics of OO programming on C# and some concepts quite common in OOP. https://msdn.microsoft.com/en-us/library/mt656686.aspx 
Looking through the source code, it appears that RESTSharp using HttpWebRequest instead of HttpClient. 
I don't see the reason to defend VB.Net. Anyone worth their salt in C# should be able to read VB code (new or legacy). If not, don't hire them.
I'd go for multiple interfaces and one implementation to begin with. That class would implement all interfaces and I would inject the correct interface in the right place. Super easy to refactor as requirements change or evolve in the future.
Thanks for your feedback. I don't think it's the architecture, it's more the sudden and extreme change going from my previous project at my other job (which was a green field I built alone) to a much larger set of projects, in a different language, in a different framework, on a different platform. C# is fairly understandable. I'm getting it. It's that some of the .NET stuff was obscure.
C# is pretty straightforward to me as is MVC. Ruby is a pure OOP language and Rails is MVC. They are just different implementations. That being said, thank you for the link. I think it's just the change of everything being different and feeling pulled in a lot of directions.
He never mentioned that the project was MVC, only that it was .NET and C#. For all we know it could be a mess of Web Forms. Up until I climbed aboard, the company I am currently working for was doing everything in web forms. Using techniques and design patterns that probably weren‚Äôt updated since the early 2000s. They weren‚Äôt even using LINQ, for Pete‚Äôs sake - raw SQL queries and custom-built data access classes.
How about you ask a coworker when you get stuck or overwhelmed?
If you want raw performance, Dapper beats EF by nearly an order of magnitude. As in EF spends 90% of its time overheating your CPU and only 10% waiting for the database. If you want ease of use, while still getting very good performance, then I'm going to indulge in a bit of self-promotion and recommend Tortuga Chain. https://github.com/docevaad/chain Here's a couple of articles comparing EF, Chain, and Dapper that I'm working on. (They'll be formally published sometime next month.) https://www.dropbox.com/s/wedeceekpdgljtc/Implementation%20Strategies%20for%20the%20Repository%20Pattern%20with%20Entity%20Framework%2C%20Dapper%2C%20and%20Chain.docx?dl=0 https://www.dropbox.com/s/7bn8dru41ku4xwy/Advanced%20Use%20Cases%20for%20the%20Repository%20Pattern%20in%20.NET.docx?dl=0 
Get yourself a copy of the .NET Framework Design Guidelines. This will teach you how to think like a .NET programmer. https://www.amazon.com/dp/0321545613/ref=pd_lpo_sbs_dp_ss_1?pf_rd_p=1944687502&amp;pf_rd_s=lpo-top-stripe-1&amp;pf_rd_t=201&amp;pf_rd_i=0321246756&amp;pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_r=XV4DVCGBMA1RFNDKKXAS
Which .NET stuff in particular is giving you trouble and why didn't you just include that info in the OP? Saying, " I get C# and MVC but .NET is confusing" doesn't give anybody any relevant information about your problem. Think about what somebody would need to know to answer the questions you have and then post that. Don't make us pry it out of you when you're the one asking for help, it's lazy and inconsiderate. Edit: http://stackoverflow.com/help/how-to-ask
Despite the name, Code First is not really about creating the database from your code. That's what migrations do, and it isn't compulsory to use Migrations with Code First. I personally think Code First really IS that much better and I have fairly extensive experience using both models in a project similar in size to yours. We started our project with Database First (and used it for months in production). It was a nightmare for merging changes. If you have database changes in two branches you will need to roll one back, merge, then re-create your database changes. It's truly awful, even in my tiny office with just three developers. Much more trouble than we anticipated. We now use Simon Hughes' "Entity Framework Reverse Poco Generator" https://github.com/sjh37/EntityFramework-Reverse-POCO-Code-First-Generator to generate Code First from our database- it merges beautifully and gives us a great amount of flexibility in a very simple way. In addition to issues with merging, community support is much better for Code First than it is for Database First. It feels like database first is dying. Between the tables, views and stored procedures we have ~250 objects and we use a single DbContext. It takes ~2.5 seconds to build the model for the first query each time we start our app. Subsequent queries are effectively instant. It's just time to first query that is slow. Database-first did not have that long initialization time. The EF team is supposedly fixing it in 6.2 per this pull request: https://github.com/aspnet/EntityFramework6/pull/40 but it has been in the works since 2013: http://entityframework.codeplex.com/workitem/1876 so I am not holding my breath. Our process could certainly be improved, but it works very well for us. Feel free to PM me if I can help answer any questions.
Google : .net cms Stay away from dot-net-nuke A basic blog type system is probably a couple of hours work in .net though, so a better idea would probably be to work through some tutorials and build one.
I am actually currently watching Simon's pluralsight video. I agree this looks like what I want, but was having an issue with trying to get it setup. My initial generations didn't seem to be working. Edit - had to change "EntityClient" to "SqlClient" in connection string. 
1. Do your migration using FluentMigrator. 2. Build your ORM using [LLBLGenPro 5](https://www.llblgen.com/). It's awesome.
Why don't you use a [dynamic object ?](https://msdn.microsoft.com/en-us/library/system.dynamic.dynamicobject(v=vs.110) See the example on MSDN, you could provide your override of TrySetMember and TryGetMember and add all the logic you need to validate and handle errors via IDataErrorInfo. Note that DynamicObject does not provide INotifyPropertyChanged out of the box but it is not hard to implementi it yourself. On a completely unrelated note, I personally prefer INotifyDataErrorInfo over IDataErrorInfo as it is more similar to INotifyPropertyChanged and more explicit than IDataErrorInfo, but this is just my opinion and preference.
The biggest issue I have is that it violates my expectations of IDisposable. They can replace it with another leaky abstraction as long as they don't make it disposable. 
I don't have a lot of experience with db first. I've working on an older project and if I update the diagram from the db, there are lots of errors and it deletes all the models. Its hard to tell how much is really wrong. I created a new code first data context and it works. There is a "-script" argument for Update-Database that will just generate the sql instead of applying it directly. 
I'll recommend "plain" .NET Core: https://www.microsoft.com/net/core It's extremely easy to setup and get going with (there's no GUI, it's just a few command line tools, where you'll need to learn 4-5 basic commands to get going). You can do most kinds of console programs, but no GUI stuff. Once you get your feet wet, it's simple to create a webserver, that you can run "locally" (as another console program), and visit by pointing your browser at "http://localhost". Try it out using VS Code: https://www.visualstudio.com/en-us/products/code-vs.aspx It's a great match :)
In the end, this ended up being a firewall issue. I ended up replacing the dated sonicwall 2400 and the problems went away. It was very difficult to track down because the firewall was "silently" dropping packets without any logging. As suggested I ran a trace on the firewall, database and application server to determine packets were being dropped. I suspect the problem caused by stateful firewall settings because sonicwall is known to drop problem packets without any log. Thanks for the help. Hopefully this post will help someone else someday.
&gt;it's lazy and inconsiderate What you're doing is rude and inconsiderate. You have every right to ignore my post.
It isn't "better" than Code First as far as performance and memory usage are concerned. Database First is actually significantly faster in what /u/graunwolf refers to as "warmup time" because it caches the model in an EDMX. However, the situation for Entity Framework is not as grim as the picture that micro-ORM proponents make it out to be. Aside from "warmup time" (first instantiation of a DbContext) I have found no performance degradation when using a large DbContext vs. a small DbContext. It's very true that you can get much better raw performance out of a micro-ORM. EF has a lot of features like navigation properties with optional lazy loading, change tracking, built-in optimistic concurrency, etc. and those features are not free (resource-wise). Additionally, using these features without understanding them can turn your app into a sluggish nightmare of bloat. It really depends on your needs. If you count data access time in single digit milliseconds and you don't need Entity Framework's features, then Entity Framework is probably a bad choice. If 13ms is just as good as 3ms for you and you'd like to write simpler code, Entity Framework might still be a good option. Especially if it's already familiar to you.
I really like this comparison, thanks. It's handy to see some basic Dapper and Chain code side-by-side.
How long have you been at this place?
I'm helping you, how is that rude and inconsiderate? Learn to accept criticism and drop the ego, you'll be a much better developer for it. 
Sounds painful but you could run windows on a virtual machine, I suppose... 
Run a VM. I fully develop with .NET Core right now using my Macbook Pro - I just run a Windows VM to do my development on. Works great.
Just wanted to say thanks. This worked for me, and appreciate the tip on INotifyDataErrorInfo. 
I use Parallels to run Windows on my Macbook. Having used all the other VM solutions I'd say it's the best option. Alternatively you can use VS Code and not bother with a VM.
We use EntityFramework (code first with migration disabled) with a few hundred tables. It's all in one db context because even things you would think of as being separate eventually end up together on a screen. The only performance issues we've noticed are with the size of the serialized data coming out of the webapi not with EF itself.
This is a great book that explains most of why the .NET API looks as it does (meaning the organizational principals in the namespaces, class names, method names, etc, why stuff is versioned as it is, principals for feature deprecation, etc). It's great if you're about to write an API yourself. However it does not explain how to use day-to-day concepts such as inversion of control, which is pervasive in .NET.
IoC isn't anywhere near as popular in production code as the bloggers make it out to be.
1.0.1, that is.
In memory caching is usually faster than performing a network call. It depends on a few factors. In general a distributed cache will help you with session management when you have multiple load balanced servers and you want to avoid sticky sessions. Another use case is data durability where you want to keep the data hot in the cache when the server reboots. A distributed cache can store replicates across failure boundaries. Along with that, another use case is the concept of an in memory data grid, where the cache serves as your system of record. Redis may or may not be good for that depending on your consistency requirements. We actually used in memory cache and distributed cache at work. If not in local cache, check distributed cache if not there goto source. Write behind if any data updates occur to both local and distributed cache to keep data fresh. (There are a bunch of other cache eviction policies).
The in memory cache will reset when the app pool is reset our a deploy happens. Having an off server cache solution can protect against it. The IMemoryCache also has no way to get the stats of how the cache is behaving. Just something to note.
I've got the keys to the kingdom!!! Next week we will collectively lie to the auditors!
"Wrong" is a relative term. Going to let a production problem linger because of principle? As I'm sure you know all too well, the client/business/sponsor wants results and doesn't care how it's accomplished. Sometimes we can't always play by the book. Isn't that exactly what your post is highlighting? 
(I'm biased, as I'm the lead dev) Have a look at LLBLGen Pro: http://www.llblgen.com. The designer has the most features of all ORM designers out there, supports EF, NHibernate, Linq to SQL and our own ORM framework (which is faster than EF, see: https://github.com/FransBouma/RawDataAccessBencher), allows you to do model first or database first development (or a mix), allows you to create multiple models in 1 project mapped to a single DB (or multiple), and for EF it generates code using Code first or DbContext, whatever you prefer. EF6 is a bit of a pain if you have a large project as it's not really fast. The tooling MS provides for large projects is poor, and doing code first is time consuming. I'm doing full time ORM framework / entity modeling system development now for over 14 years so if you have more questions, DM me or post a reply, cheers :)
You could see how fast it really is by adding a bencher class to https://github.com/FransBouma/RawDataAccessBencher and see how it performs (as most microORMs and main ORMs are in it). Dapper sure beats EF, but that's not that hard. Dapper itself isn't the fastest anymore btw. 
I really like Vue.js. It is lightweight and the syntax is easy to grasp. 
The deployment solution provided by Visual Studio. You are much better designing your own install system that you can actually control. Click Once is trouble for many reasons.
I'm scared
what of? ;-)
Type Danger. Which is the technical term for the opposite of Type Safety
I've not heard the term "Type Danger" before, I'll have to remember that one!
I can't wait for this to be released!
I definitely didn't just make it up, but it should totally be a thing
Some new really interesting benchmark results. "Bubble sort" of 4096 size array of integers **performs 1.2x-1.37x better in .NET Native than in C++!** And .NET Native performs about **6x** better than .NET JIT!
Good to know, thanks!
Thanks, I fixed the link!
I have none, sorry about that!
And this is why books on etiquette were invented. It's one thing to insult someone intentionally, its quite another to be so clueless as to not realize you are insulting someone.
What about being so clueless that you see insults where they don't exist?
Xamarin have been on it for a while: https://github.com/mono/SkiaSharp
That's not .net core though.
Aurelia. 
That's like using a gashammer to build a stone age hammer. 
http://i.imgur.com/Br00TCn.gif
Atwoods law at work :p
Don't need it post every minor bug fix.
This is a big decision if it's a "rather large" project. I am glad to see you are doing your research but for me it usually comes down to one question usually answered by preference: ***Do you want your app to be taken over by the the javascript framework?*** This is not a loaded question, there is no wrong answer here, it all comes down to preference really; once you have an angular or react app setup and going, it becomes a lot easier to do some big things than it would otherwise. For Angular or React, you are going to be locked into that platform it will take A LOT of work to transition to another. Something like [Aurelia](http://aurelia.io/), or even moreso [ractivejs](http://www.ractivejs.org/) or [vue.js](http://vuejs.org) (like mentioned below), will be much more lighterweight, you may potentially have to do some more work than the others, but will not be as difficult to move from and can sometimes grant you more direct control. I personally prefer the lightweight frameworks because I am mediocre at JS and don't want complex JS to be the heart of my app, but not afraid to do some of the heavy lifting. I know people who love react, especially if you are going to need reusable components, and you will find an army of users who can give docs and advice on angular. But for me the deepest I like to go is Aurelia, but now use Ractive.js mostly if it's something big; haven't used much else for a few months. And honestly nothing more than jquery with handlebars if I only need something simple, but no 2 way binding. Also worth looking into Ember because some people rave about it, but I haven't ever looked at it.
 "Your scientists were so preoccupied with whether or not they could, they didn‚Äôt stop to think if they should."
It has worked on several commercial projects before with Phalanger, the predecessor of Peachpie. Of course, not all projects are suitable.
Life finds a way
Here's a really cool open source project and already I see there are a few dissenters. The close mindedness of the .NET community makes me despair. Step outside .NET land and you'll find PHP is actually very popular - it even powers one of the biggest sites in the world (Facebook). In fact outside the corporate world you'll see none of the big tech sites use .NET - Pinterest, Instagram, YouTube, Airbnb et al all use Python, Ruby and other popular OS frameworks. I was hoping this will change with .NET becoming open source but I think the other battle will be to get the community to change its mindset. Anyhow, congrats - I can see a use for this. Where I work after struggling with DNN and other poor .NET CMS systems a colleague took Wordpress for a spin and we haven't looked back. I'd certainly be interested in running WP under .NET and maybe writing plugins in C#.
It's not that "the .net community is close-minded" (though whether or not they actually are is a separate debate), it's that PHP is objectively a worse choice for most web things you'd do in .net than one of the other dozens of tools for web dev. And it's a little disingenuous to say php "powers facebook". Hah. Not really at all. It's just their front end. There are undoubtedly dozens of systems that work together to form the "facebook experience", and at the very tip top of it is a layer of php to put shit on a web page. Also, something being popular doesn't make it "good". It just means a lot of people use it. And in the case of PHP, a lot of people use it because it's brain-dead simple to do basic stuff quickly. The issue is that in doing so, you typically end up with a lot of bad devs getting stuff "up and running quickly", resulting in shitty architecture, bad design choices, pisspoor security, and changes end up becoming a much bigger deal than they should. 
&gt; it even powers one of the biggest sites in the world (Facebook) ...which uses a Facebook made PHP to C++ compiler, clearly showing PHP alone is not good.
Don't use .NET Core for anything serious.
I'm aware of this. And that doesn't 'clearly' show PHP isn't any good rather it demonstrates that (unsurprisingly) a site that gets billions of visits a day requires a faster runtime. The standard compiler in most cases is fast enough (and really how many sites on the planet can you actually compare with Facebook?) and in fact version 7 uses a JIT compiler similar (I think) to the one Facebook built. In any case this is now talking about the VM rather than the language itself which is a different topic.
Its hard to say without knowing the details of the project but I wouldn't jump into .Net Core with this. Your learning curve would be on a base that's still developing. You'd likely hit roadblocks where finding the answers wouldn't be readily available due to the immaturity of the product. 
I hadn't really considered just using EF6, I don't have any particular reason to limit myself to .NET core. I would have expected the new version to be more performant, but I guess it's still rather early on isn't it. I will keep that in mind, thanks!
For a POST, you would use a shorter URL with no query params (e.g., http://localhost:60445/Demo/Export.aspx). The query params would go in the body of the request instead of on the URL. It's probably best to look up a tutorial on how to construct and process a POST request with a payload in its body; they are a little more involved than simple GET requests.
I do some work on project that use a lot of angular (using TypeScript) in pages that edit data. I don't know how easy it is to get setup, but with what is already there, it has been fairly simple for me (a C# dev) to make changes and add new stuff. 
Thats a good idea, it prevents someone else from doing it, but if I am the one who encrypts it, I could use the original non-encrypted version in dev. It been a while since I setup a project/site, but I always use to use integrated security so that the connection string alone was insufficient. 
I'm familiar with how it works (how it worked in earlier versions). If I'm the one who knows the production values and I'm the one who puts them on the server and then encrypts them, then I could add them to my local config. If the a username and password are in the connection string, the connection string may work. In a locked down environment, I might not even know the names of the production machines, let alone have access to them. 
Seconded. I have a great deal of experience with asp.net and my first Core project was an endless experience of reading issues on GitHub and just generally spinning my wheels on complicated issues with doing simple things. Stick to 4.6 for now. 
I would agree with the other user that warned you to stay away from .Net core if you are trying to learn as you build a production app. You are going to need as many resources as you can find, and core will just not have as much out there yet. As someone who started development in a similar fashion of being asked to create production ready software with precious little experience, I can relate. I learned a lot, very fast. Not having had the experience I do now, I realize I made a lot of rookie mistakes back then. Since you're going from VBA and you are saying you want an MVC style app I am having a lot of trouble understanding how you will deliver the product. And since it's hard to get into any concrete suggestions given the very limited knowledge provided about the product you're looking to develop, I will have to be pretty generic in my suggestions. Take the time to understand the business's goals for the product. Do you need to consider scalability? Will it be hosted in the cloud vs on site? Are they expecting to sell the package as the product, or is this a SAAS? What are your target platforms(Excel 2007-current)? Is it something you could develop as an add-in to sell on the Azure Marketplace? I wish you the best!
You should also consider not using .NET at all. For example there is node.js that uses JavaScript on the server and the client. So you will be using only one language. It includes the package manager npm which provides you about 300000+ packages made by the community. Pretty much anything you can imagine is up there. 
Peachpie is not a transpiler, it doesn't take PHP code to convert it to C#. Here, PHP is compiled to MSIL, which can then communicate with .NET. 
At some point it may need to scale, but I at first I don't think it will have more than 1k registered users for at least a few years after launch. Deployment would be in the cloud, thinking Azure. It would be sold as SAAS with annual license agreement. No, this isn't something that could be built as an add-in, I think it's simply too big.
This is a very common misconception pertaining to Peachpie. It will not port poorly written code into .NET for two reasons: - Many inadequacies and errors are in fact fixed in the compilation process - It is a compiler &amp; runtime, not a transpiler. The output is MSIL, not C#. Peachpie will not magically "fix" badly designed PHP projects, but people seem to think bad PHP devs will become part of the .NET ecosystem and will ruin your code. It is worth pointing out that the interoperability works both ways, so you can write plugins for Wordpress in C#, for example. I think we can agree that Wordpress is a pretty damn good PHP site. 
Thanks for the suggestion, I'll look into this 
As I don't know the target Excel version, this may not work, but the newest versions of Office add-ins operate significantly different from the days of old where you had it packaged as an installer. Now you are talking JavaScript for accessing everything in the workbook and since it's JavaScript it opens up sending the heavy lifting to backing web services you can host in Azure. On a completely separate note, and this is not meant to offend, are you using source control? When I first started and it was just me, it took me a big project and many sleepless nights before I saw the necessity of having it. If you don't use source control, start :)
How do you revert builds? Is this something that TFS/release manager handles? &amp;nbsp; &amp;nbsp; &gt;As to the deployment Diff, that goes away and you deploy new every time. From here, how do I know that certain binaries are not accidentally being included? Can there be an approval process of binaries being migrated from the build(s)? &amp;nbsp; &amp;nbsp; Can other development platforms leverage automated builds/release manager?
Everything is built in Excel 2010 (14.0). It may sound crazy, but the client had to jump through hopes just to get their IT to upgrade a handful a machines from '07 to '10 just so I can use PivotTables (and they had licenses laying around). So, 2013+ is out of the question for now. I'm aware of the new add-ins though, I just haven't dug into them yet. As far as version control goes, I used GitHub a lot in the John Hopkins Data Science program I completed with R, and have Git on my local machine as I was using it with Django, but never really came across a simple path I guess for VBA. To be honest, I have literally 100+ versions of this file in a Dropbox and would seriously prefer using a real solution. I'll look around, but in the mean what do/did you use, or do you suggest?
Thirded(?). I have been playing around ASP.NET core and the tooling is just not there. I had a hard enough time just getting up an automated build environment and I feel like it's a hacked together solution. Full Disclosure: I've been a junior Dev for 6 months, so your experiences may differ 
I didn't claim it was. Atwood's law is: "any application that can be written in JavaScript, will eventually be written in JavaScript." (this project is not "Atwoods law at work")
I started with kiln/mercurial, but now I use a TFS server with Git as the source control and really like it for our team. The TFS server license is free for teams under ten, you just need a server to host it on. Another option to look into that may be easier is, paying Github to host a private repo, which I think you can still do. Git will work for any files, VBA, image files, you name it. If there is anything you could do to become a more serious developer, source control is it! :) As far as being stuck with excel 2010, and considering an MVC app, do you plan to have a site where users will upload the excel file for you to process? I guess I don't understand how you envision doing the work on the file, but I understand not wanting to divulge too much.
Does the thing you are trying to download have unique id? If so, why not just GET/POST that?
You can not really trust urls to work properly that are well over 1000 characters long 
&gt; I think we can agree that Wordpress is a pretty damn good PHP site. Except all the vulnerabilities. I just had to fix an rpcxml.php attack last week, ugg.
Good points. I second looking into Ember, mostly because that's what I use. I have recently started development on a JSONAPI [1] library [2] for .Net Core that takes a lot of the boiler plate "data binding" out of the picture. Things to know before diving in: - Ember is highly opinionated, do the examples, follow the best practices, don't fight the framework - Use Ember CLI - If you take your time, read the docs, it will be a great tool in the long run - Great for large projects, but has a lot of dependencies and maybe overkill for smaller projects After you check it out, feel free to hit me up with any questions you have. [1] http://jsonapi.org/ [2] https://github.com/Research-Institute/json-api-dotnet-core EDIT: I also have a live code demo you can checkout https://m.youtube.com/watch?v=n3CqOtjKZGc
Oh this is awesome. This addresses one of the biggest pain points of developing in .net core.
Go with aspnetcore targeting framework 462. The 'dotnet watch' functionality that allow you to edit code and reload page (do it on an SSD) is super nice when you are developing the system. This is especially important when you are developing a JS heavy app like your second project. The tooling for aspnetcore will get better. It is viable as is now. If you have any particular problem, you can go directly to the github project and ask questions there. 
Er, I'm the implicit third...
That should all go in the post body.
Thanks, I'm glad others are finding this useful 
I'm so sorry, could you please explain what you mean by "put it into the body"? So far, I've been getting it to work by putting all the parameters into the "data" property of the $.ajax() call. Is this not the same thing? 
It really depends on your problem. Many base libraries from the full dotnet framework have not yet been ported however it is in progress. I'd attempt to use core for your problems as if they're introductory or learner level then the support should be there. The biggest missing component for most people is System.Drawing so if you're aware you're going to have to use that then the full framework is probably your best option
Sounds like you're a student. Does your school offer discounted Windows licenses? If so I would throw Windows into VirtualBox or something and go from there. You'll really be fighting the system trying to use .NET Core :(
Well, all I know is that we're going to use it for network programming, so can you recommend anything for that? I don't think I'll need System.Drawing for that.
You can always use MonoDevelop IDE on Linux. Mono implements most of .NET Framework except some of the Windows related stuff.
Thanks for your suggestion. We were told not to use Mono because "it's bad". I've used it before and it worked quite well. It seems like they're just trying to sell us Microsoft products at this point.
Maybe you could try using Mono and MonoDevelop? It mimics the .NET framework (not core) and is actually quite good! Not sure why I didn't think about that earlier :)
I'm not willing to pay for a Windows license. I get a lot out of my MacBook, but I can't get anything out of Windows except being forced to use it. I know this might be kind of an unpopular opinion here, but that's the way it is. I like .NET a lot because it offers great possibilities to developers and I appreciate it coming to other platforms.
I'll try, but unfortunately we were also told not to use that. I've used it before and you are right, it's actually quite good.
We were told it's mostly socket programming, but I can't say for sure.
[VituralBox](https://www.virtualbox.org/wiki/Downloads) + [Windows Server 2012 Eval (180 days) ](https://www.microsoft.com/en-US/evalcenter/evaluate-windows-server-2012-r2) + [Visual Studio Express](https://www.visualstudio.com/en-us/products/visual-studio-express-vs.aspx) Don't worry about disk space, it isn't that large (32GB should work).. use an external SSD if you can. [$21](http://www.newegg.com/Product/Product.aspx?Item=9SIAAZ64DW2056) + [$7](http://www.newegg.com/Product/Product.aspx?Item=9SIA1TX3WC1696&amp;cm_re=usb_3.0_enclosure-_-9SIA1TX3WC1696-_-Product) [Visual Studio Code](https://code.visualstudio.com/download) + [.NET Core 1.0](https://www.microsoft.com/net/core#macos) runs in OSX. 
That's not at all it. I used to teach. When dealing with students, removing possible sources of errors is vital. It's hard enough to support vs on Windows. Every other variable adds exponentially. Welcome to the real world. Work tells me I have to use a Mac. So I use the Mac, or I work elsewhere. For you the shoe is on the other foot. If you were learning iOS then "they'd be trying to sell Apple products". I am sorry but you are being absurd. Mono is not dependable. Dot net core means your TA has to take time out of their eighty hour per week schedule to learn YOUR specific environment. Windows will cost less than 1/13 of what your Mac did, and almost all schools offer discounted software if you would take the time to really explore options. Learn something new
College classes are a time to learn how to work with and under others, not a time for hard line stances. Unless you're in the humanities. You'll get experience with the most widely used platform in the enterprise.
If you're instituion is in the MSDNAA programme Windows and Visual Studio Professional is free even.
I know that my opinion is not a popular one and I can live with that, but I don't see the point of teaching network programming with C# on Windows. There's far more to it than what this combination offers. I think we're missing out on a lot of information here.
It's not anymore
Good chance that they won't want you using Core either in that case :(
You know what I think the problem here is? Us living in different countries obviously. Our teachers don't have an eighty hour per week schedule and don't help us with our problems because they have no idea anyway. That's how it works where I come from. Another problem is that only very few technical schools here have discounts on expensive software products. I did not say that I think Mono is better than .NET or something like that, but a teacher telling a student not to use something because "it's bad" doesn't really seem like an unbiased statement, does it? /u/semyon2105 suggested using MonoDevelop and I appreciate his/her input because it *is* a viable option for some people. 
You are confusing Visual Basic (the old one) and VB.Net. Latter is a pretty modern language and even contributed some features to C#.
I should have mentioned my location. I don't know where you're from, but my experience of being a computer science student in Austria has really made me almost aggressive towards this topic. I received a suggestion of installing Windows and VS on an external SSD and boot that with VirtualBox. I'll have to see about getting that to work, but it seems like something I'm willing to try out. That way I at least won't lose too much space on my built-in drive. Don't get me wrong, I don't hate Windows. Many people are happy with it and I get that, but I'd like for Austrian schools to teach multiple ways of doing things. There isn't just *the one right way* and it feels like understanding how communication works on a very low level and maybe writing some C socket servers and clients (no matter if they'd be for Windows, Linux, or whatever) might be a useful thing and might prove to be a way of making students *understand* what's going on. My colleagues are basically sitting there with their Laptops running pirated Windows all day having no idea how to write code or solve problems because we only touched every topic instead of gaining some in-depth knowledge and because they only learned how to press the "Run" button in Visual Studio. That makes me sad and sometimes even angry. I want to understand what's going on when a computer does something. That's just natural curiosity, which most of my colleagues and even our teachers sadly seem to lack. I hope you get what I mean and I apologize if I came across disrespectful. I did not mean to offend. Everyone's entitled to their own opinion :)
They actually tried to release it with only C# and there was a major backlash. Microsoft got tunnel vision because all the cool, cutting-edge Partner Program people overwhelmingly choose C#. But the reality is that a ton of businesses who don't live on the bleeding edge operate in a VB world.
America. Did my Master's at the University of Iowa, currently living in Denver, Colorado. Yeah that's a possibility, the VirtualBox thing, but since you're doing networking you really truly might have to have a native install - BootCamp would work, but the hard drive space is a bit unfortunate. I dunno if you can do it with external? You're right, there isn't one right way. The reason American Universities choose Windows is because they need something to standardize on to reduce support issues - that's why Linux isn't used - and Macs are WAY out of the affordability range for the vast majority of students, despite what the perception of Americans' money is outside of these borders. Most students are struggling to eat. I think you're approaching this the EXACT right way - I really don't disagree with anything you said, you seem smart as all hell, I can tell you're thinking about this right. *I* was wrong earlier, and it is I who should apologize for any disrespect, though I appreciate your response :-) And you're right, and I think that most of what you're experiencing is that you have a bit (probably a lot) better understanding of the context. Also, the only reason I still do .NET is that I can do it on Linux/OS X, so we agree there too :-) Did you find anything out with that un-registered version of Windows Server 2012?
&gt; Yet to met a F# developer, using it commercial setting. Yeah, I keep hearing about how its "magical" and one of the best languages ever and I should learn it anyway just to sharpen my skills. But I never see it in use anywhere and I never see job postings for it. :/ I'm not knocking it. I probably will do something with it sometime.
VB.NET is still a good language, just basically ugly and eclipsed in popularity by C#. It's still .NET, they still compile down to the same thing. There's a lot of legacy code, and even some not-so-legacy code that's in VB.NET. I personally majorly prefer C#, but VB.NET isn't bad by any means, if you prefer it's IMHO really verbose syntax.
I snuck it into two work companies I worked for :) Just made something in it and said: "Well, now it's already written and it'd be a waste of time to rewrite...". Works every time 80% of the time.
Unfortunately I don't get to choose classes. It doesn't work that way where I'm from. We're just unlucky to have a teacher who loves to use Microsoft technologies (which doesn't necessarily have to be a bad thing) and hates everything else (there's what's bad).
Well, in their defense, for the next year or so Core will primarily be used only by cutting edge shops.
And this is how development job specs end up requiring expertise in 10 plus languages/techs.
They were small things that don't require a lot of maintenance and where F# was an extremely good fit. One was for parsing texts and the other for a testing DSL. Then again, I'm of the opinion that: 1. Use the language that's the best fit, not the one your company happens to use. Ofcourse the benefits have to be significant. 2. You can learn most languages on the job just fine. Most jobs that ask for 5+ years of C# could easily be done by someone with 5+ years experience and no C# experience. 3. You're never too old to learn Sadly, these opinions don't fit well into most companies :(
It isnt.
The difference between C# and VB.net seems significant until you use another programming language (Ruby, for example), then you realise they are both basically the same thing.
Supporting F# was a high priority for them, but I've never heard of them actually doing anything with it.
I use VSTS as well at work. Just FYI, dotnet is on the PATH by default so you can just do Command: dotnet Arguments: restore Command: dotnet Arguments: build Command: dotnet Arguments: test -xml testresults.xml Command: dotnet Arguments: pack/publish Also, I recently discovered they now offer private nuget package hosting. So you can create a CI build that publishes a nuget package to a VSTS feed. Makes it very convenient for sharing packages across projects/solutions when they are proprietary. 
One of our oldest code bases at my job (and still performs like a champ 24/7 serving thousands of clients daily) is still VB6. Hell, I learned a shit-ton about functional programming using VBA. 
Realistically it's not important at all. And Microsoft at some point needs to cut the cord and abandon it to FOSS and let it go wherever it does or let it run itself into the ground. VB.NET really has no place in modern software development. 
I see value in VB6, I see none in VB.NET
That's why I code in Python, it's made with real bits of Python so you know it's good! 
If you're a web developer and looking for a relatively easy way of dipping your toes into the water, check out Fable. It's a new (and already *very* nice) F#-to-Javascript tool, with support for virtual-dom (the library that Elm uses) and React.js. https://fable-compiler.github.io/
I believe they use it for their own implementations.
F# is used for our build scripts for both Xamarin and intermediary library projects with the FAKE library. Interestingly, Xamarin.Forms does support F# for android, iOS, and shared libraries, but not for the UWP/Windows Phone 8.1 due to F# not yet having support with .NET Native. I always use it whenever I need a one-off.
Microsoft is still putting work and money into F#. Don Syme's efforts at MS Research to get the CLR to support F# is the reason why we have support for generics, and ultimately LINQ, in the first place.
The amount of code held together by shoe string, bubble gum, and one old guy who knew the guy that wrote the original application is frightening.
Any examples? 
You are basing your opinion on what? The fact that isn't *cool* enough? VB.NET may not be #1 any more, but is still a damn fine language and a heck of a lot more user friendly than C# for new programmers.
Functional programming has a lot of good benefits. I'm not in a setting where we use F# yet I've become intrigued with it and have started learning the concepts of functional programming in general. It really make you think about the effect your code will have and the reach it's effects have into other code. I've really started to adopt some of the best practices on my C# code and it's improved things considerably. 
Yes, they like it. But what are they actually using it for?
I don't know. His comment said they are using it.
Historically speaking, VB.NET was *really* useful in the early days of the .NET language for COM interop. VB.NET lets you do late binding in the language, whereas before the dynamic keyword was introduced in C# 4, C# only did early binding. I worked with several projects where the COM libraries would change between point releases of software we interfaced with. There were probably various ways around this issue (most obviously in release management) for the developers of the original software, but we were working in someone else's walled garden. Writing the code with the COM libraries referenced in C# would result in various runtime errors if the COM library was different from the one originally referenced. So a common practice was to write the code using the reference, change the types of any COM objects to `Object` to use late binding, and build and release. There was a *lot* of code written like this. F# has more respect from a language perspective, but VB.NET has a *much* larger usage in the wild. The other thing is VB.NET and C# aren't drastically different in structure - only syntax. Despite the extra work involved in maintaining it, having a second language has helped a lot for compiler stability and testing. You never really know how well your multi-language system works until it actually runs more than one language. VB.NET was probably a lower-hanging fruit in the implementation.
I'll focus on the PowerShell side since it seems the rest is answered well: As I understand it -- PowerShell is a scripting language. It's a shell, sort of like SH, BASH, KSH, CSH. It's not a compiled language or really a "programming" language in the way you'd normally think of a programming language. PowerShell is a script and usually implies it does a uncomplicated things. If your script is super complicated then you need to step back and see if you're doing it correctly or if you need to evaluate using a real program. A shell script rarely needs user input. Often times the input is from the command line and that's it. If your script gets to the point it's complicated -- it's time to look at making it an actual application before you make a cluster fuck that you will hate yourself for a year from now. I grep'ed this page and so something missing: .NET transforms code into IL. This means `Console.WriteLine "Hello"` and `Console.WriteLine("Hello");` will compile into, literally, the same code. This means it's *super easy* to combine C# and VB.NET. VB.NET is still fairly common actually though it still carries the stigma of VB6 and older. I'm used to C-like syntax so I'm sure you can infer my preference but really it's just that -- a preference. *fair warning: I know very little about F# beyond a few super simple examples so take everything with a very tiny grain of salt* So no one answered your question: Why not C# and F#, for instance? Well that's probably a numbers game. They want people to pick up .NET Core. More people use VB than F#. Now is not the time to push two *huge* fronts at once. That's dividing too many resources. You'd need to force the VB people to C# or F# and that's just asking for trouble while you're trying to make something cross platform. So now is *really* not the time to push for that. In fact you'll find a lot of modern examples will include VB and C# -- which should be a strong indicator of how many people use what. There's so little F# examples but so many VB+C# examples that it will help smooth transitions and help set expectations. We *know* what people need and use in those realms. F# is a bit more vague.
Use var everywhere, and so does C#.
I'm not sure VB.NET is that much friendlier. It has some friendlier syntax to someone unfamiliar with programming but I personally found the jump from VB6 to VB.NET much bigger than switching from VB.NET to C#. I agree though that there's nothing fundamentally 'wrong' with the language and the hate towards it by some people is just silly.
I still support a pretty large VBA project that's almost 20 years old. I imagine there are millions of developers out there in the same position.
Yes fair point. That does explain why Python is the most popular language for teaching now.
Sure, they weren't directly related. I should have been more clear. My second statement was regarding VB having the potential to drive new features to C#, I was trying to show that F# has had and may continue to have an role in driving features to C#. The improved Tuple syntax and local methods being proposed as strong interest for C# 7 are in line with functional programming interests. As for my first statement, I'll defer to the following [from Phillip Carter](https://github.com/Microsoft/visualfsharp/issues/1096), relevant parts quoted below. &gt; Does Microsoft Care about F#? &gt; &gt;Yes! Yes yes yes yes yes! And more love for F# is coming, believe me - and not just in the OSS world. But things are taking time because so many things - the Visual Studio platform, the entirety of .NET - have been making major shifts in direction and huge engineering investments. I apologize if I came across as combative in my response. I do think you are correct in saying that the F# community's efforts will have a large impact on it's success.
I like optional parameters but that next statement is false. Dynamic was initially planned for easing COM interop unless your argument is almost all COM interop is done by VB.NET. Even without COM dynamic would come to C#. Rigid type safety is an outdated concept. You want type safety that can be disregarded for general purpose computing. 
I used it a lot in the financial sector, but it was a small sample size.
The CLR infrastructure needed for dynamic to work was built for VB. 
It was a subjective point and you're overthinking it.
you should ship the complete folder, the exe file is just a 'entry'
The thing though people looking at your framework does not care whether it is converted to MVC actions or not. Those are just implementation details. If they want MVC actions, it is already available to them. People using your framework will have to learn new way of doing things anyway regardless of the implementation details.
That is true, good point. I do feel though that people working on existing MVC projects or with previous MVC experience may benefit from this tool as they already know how it works one level deeper. As long as they can wrap their head around the mapping between fluent actions and MVC actions they should be all set.
Thank you!
I use it for my commercial products @ www.wcfstorm.com. The resulting simplicity of the code when you only think in terms of functions (no class heirarchies, interfaces, polymorphism, etc.) is amazing.
A nice break from the constant scrolling up and down in the solution view :) 
And this is it in a nutshell. No computing platform can be successful without developer support. A platform is only as good as its developers. A very shitty API or platform will prevail if there is developer support, while a genius designed one will fail without it. 
I use it only on the desktop products and you're right it's F# and WPF. The main benefit I get from F# is its default support for immutability. When "variables" do not change its values, there is less state to manage and the resulting code is less error prone. When it does fail, it is easier to debug. F# code also tends to be shorter. In my experience, I tend to write small, single-purpose functions that I just combine to achieve more complex functionality. The short functions help a lot in achieving better readability. 
The CLR has no coupling to any language. Well high level language anyway 
Awesome work though. I am gonna go through your source code and steal some ideas.
The `GetFeatureName` method has a bug (it returns null if someone named their controller class "Features" for some dumb reason). I'd write it to not be using linq as well because the iterative form seems much simpler: private string GetFeatureName(TypeInfo controllerType) { string[] tokens = controllerType.FullName.Split('.'); bool found = false; foreach (var part in tokens) { if (found) return part; found = part.Equals("features", StringComparison.CurrentCultureIgnoreCase); } return ""; }
Thanks! Go ahead!
I've read about this and was led to it because of Durandal and knockout but I haven't seen many people using it IRL. How do you think it compares if you're using it? 
what problems have you seen with Ractive possibly? After looking more today into that and Aurelia and Ember, Ractive seems like the replacement to knockout I'm looking for. More components, an even more simple syntax on the JS models side of things, simple easy to read HTML + tags kind of implementation for binding. I think a library rather than a framework is going to be more my style long term. Just wondering if you might know if any limitations with this possible choice. I've loved knockout I just know I could do it all in less JS and there has to be something that's still a living project with future support. Thanks! 
sourceforge.... now there's a site i havent seen in a long time
The next level of organization is one project for every single feature.
This was actually posted on here yesterday I think. Pretty good article on getting app metrics https://www.erikheemskerk.nl/inspecting-aspects-interception/. While I do t necessarily agree with everything in the series, it should give you some good ideas.
Look at New Relic or Application Insights. Without code changes and with minimal CPU overhead, you can instrument your application. I like New Relic, personally, but Application Insights is nice too.
The macOS tutorial is no more than a 'hello world' that is available on dozens of other blog posts. That's fine, I don't want any more than that to get started - if I'm a new user I want to know how I install what I need then I can take it from there. But as soon as you've finished that tutorial where do you go? The more advanced stuff uses Visual Studio. Aside from a brief platform introduction all the tutorials should be platform (and IDE) agnostic. Literally every other framework takes this approach - I just went and had a look at the Spring framework (as the most direct ASP.NET comparison) and the guide's aren't built around Eclipse or IntelliJ (or Linux for that matter). It's the same with Rails, Django and all the others. It's quite frustrating.
Most of the "Advanced" stuff caries over from .Net 4.x (major programming paradigms and patterns remain the same). But, there are many things that are different (middleware for example -- see https://docs.asp.net/en/latest/fundamentals/middleware.html). I would suggest peeking at the contents of the official documentation (https://docs.asp.net/en/latest/) after checking out a tutorial like the one linked. These should answer most of your questions. Documentation can ALWAYS be improved. Everything about .Net Core is open sourced, including the documentation. Feel free to open a Pull Request (https://github.com/aspnet/Docs) or submit an Issue. Just take a look at the contribution guidelines first (https://github.com/aspnet/Docs/blob/master/CONTRIBUTING.md). But, I'm not really sure what you're looking for. If you can be more specific, I'd be happy to consider making a Live Coding tutorial if it seems like there is a need. 
For the most part, whatever laptop you choose should be fine. Just make sure you've got plenty of memory, at least 8 gigs would be ideal. For the time being, I'd stay away from all-in-ones the Surface Pro - I've personally heard mixed reviews on using them for development purposes. I tend to gravitate toward Asus laptops, specifically ZenBook Pro. I don't really use the trackpad for them, so I cannot vouch for that. They may be a bit out of budget if this is just for playing around, however I've been using them for 6+ years and find them to be great.
You could just use bootcamp on your Mac. Then you don't need to buy new hardware. Another option, as I assume you are connected to the internet almost all the time being a rails dev, you could host a Windows instance on Azure or AWS, or just set something up at home (this is what I do). A third option would be to start your journey with VS Code and ASP.NET MVC using .NET Core. That option would be totally free. If you really want to find a Windows laptop, any laptop in your price range will do. Track pads in the Windows world aren't great in my opinion. I would just buy a wireless mouse.
Could you elaborate on some of these shortcomings? I'm curious. 
"Not being able to" is an exaggeration. Its not smooth but its as simple as copying certain folders. (just did this)
Possibly [Orchard 2](https://github.com/OrchardCMS/Orchard2)?
They have had courier since about 2009. You have to pay though. 99 euros
Sadly, I don't think there is anything worth looking into at the moment. I would like to see 1! The following list (https://github.com/thangchung/awesome-dotnet-core/blob/master/README.md) contains 3 CMS projects, don't know their state. But I think Orcherd2 is going to be the most worth noting. 
Check out my projects. It is all command line. https://github.com/dodyg/practical-aspnetcore
Do you have an alternative suggestion? It seems like there's not that much out there in the .NET CMS world.
If I were to build yet another CMS (I have built 2 so far), I would build it on RethinkDB and implement Django style pluggable modules with Wyam.IO static generation capability. 
Thank you. One of the goals of collecting the statistics is to establish a baseline of performance, so we know were we are now. The hope is that this will allow us to have a more usefull discussion with our customer about what we need to improve first. They sometimes feel that the application is sluggish (and they are right). But they might also want a written promise of improvement, and we can't make a contract out of "make the site less sluggish".
We have found one cause of occasional slowdowns, caused by a known bug in old versions of SignalR where it will react to a failed connection attempt by retrying over and over again. We are not expecting a lot of request, somewhere around 10/minute or so. But some of the functionality is rather complex.
Few requests and complex logic is a recipe for random response times. You can have 1 request per 5 seconds, or suddenly 5 at the same time. What if you had 20 requests at the same time? What would be the bottleneck?
Will fix it in a week (I hope I'll find time).
I've seen bunches of .net tutorial sites over the years, but I have found that if I want to know something .net related, the best tactic is to google it and look for the first entry from StackOverFlow
Its for the user experience. The reason to put a max length is to prevent the user from entering a value that is going to be invalid when it gets validated. You must still validate the input. 
You're right, although, if you're using an ORM framework, they sometimes truncate the data for you before putting it into the column avoiding the exception entirely. 
I'm surprised it's even being questioned? If you can enter larger lengths of text than your DB can handle, then only bad things will happen. You'll either: * Throw an exception * Lose Data * Cause some kind of corruption None of which are good for the user.
Nuget won't but your build will fail
Could you elaborate on what you mean?
My reason is the same as any agile, you want to fail as soon as possible. By setting max lengths on the UI, you are telling the user that their data will fail the moment they type that 1 extra character. This prevents them from wasting time. The reason you do the db is that UIs change, and you don't want things to suddenly stop working, or to have odd errors. UIs are fickle at best, and programmers often forget to put all that stuff in. So the db is for future programmers, the UI is for the user. 
If it's important enough, you should automate it as much as possible. Writing a basic test suite that beats the hell out of your platform can be useful to see how far you can push your product. You mention MVC and WCF, so I'd imagine it's a multiple server setup, some frontend servers and some backend. It's important to figure out where the primary time is spent. IME, it's usually in backend, doing serial blocking network calls to 30 endpoints. I had a similar case, a service that couldn't handle more then a few hundred users before maxing out the CPU. We never fixed it (we just added servers), but I'd say the root cause was the tech employed (unity/wcf/ef) in combination with a massively complex network call pattern for everything, performed in synchronous calls, one at a time. This led to massive object allocations for the big/complex objects created on the way down to the network calls, and while all those calls went on, all those objects just sit in memory doing nothing, causing the GC to spaz out. Combine the long duration calls with the DI used, resulting in long living, huge object graphs, adding another nail in the coffin (the GC needs to walk the entire graph to perform GC, so small graphs are better in that regard).
dotnetkicks.com is the closest I know of. 
Does the enum declare a backing type other than int?
Change Data Capture is *not* the correct way to do it, for sure. Honestly, I'd suggest breaking out the *Submission* entity from the *Task* entity. |Task|(1)--(n)|Submission| To me, that seems simpler, and it has an advantage of being a pretty natural mapping to the Entity Domain. A user creates a *submission* for a *task*.
 public enum MyEnum : byte {...} Can make them use other types, like that.
Why is this a problem? Compiled code doesn't keep every piece of metadata.
Are you sure? I seem to remember ADO silently truncating the data, ignoring the warning that SQL Server returns.
It seems to be an isolated issue on one developers machine who deployed this. Thanks for all the help everyone.
Use SSRS URL access - https://msdn.microsoft.com/en-us/library/ms154040.aspx to download the exported PDF, then move it to the correct folder. It's pretty simple and can be done with PowerShell if you don't want to write a full app
Oh sorry. "Software Engineer in Test" is actually a job title. I've seen a bunch of job postings lately for that position and got curious how people are doing in their job.
I use SimpleInjector along side the built-in constructor DI. I populate the SI container with the services I registered so I can use other DI methods like property injection if needed. It's an ugly hack, but at least it's easier to migrate existing code.
Hi tech4ever4u, Thanks for the prompt reply. Will postman (chrome addon) do the trick? If not, is there a way to make them think I am requesting the 30 days data from July so that it will generate the data from June and July? Thanks
No. Basically the server is doing validation and will not allow a value that is not part of the dropdown. To use a different value you would have to change the server code
I second Postman, been using it for two years now. There's a chrome extension and a standalone chrome app, so try em both.
That's what I read into it. And they seem to be using Chromium as a html view engine to 'see' the html frontend for the 'app'.
[Electron](http://electron.atom.io/docs/tutorial/about/): &gt;Electron is an open source library developed by GitHub for building cross-platform desktop applications with HTML, CSS, and JavaScript. Electron accomplishes this by combining Chromium and Node.js into a single runtime [.NET](https://docs.microsoft.com/en-us/dotnet/articles/about/index): &gt;.NET is a general purpose development platform. It can be used for any kind of app type or workload where general purpose solutions are used. It supports different programming languages enabling a high-level programming environment with many convenience features, while providing low-level access to native memory and APIs. On top of that are frameworks to build various types of solutions like: * ASP.NET (mvc) for webserver solutions * UWP for modern windows apps * Xamarin for mobile apps * MonoGame for games 
Electron is really just a Web Browser (Chromium which Chrome is built on) wrapped up to behave like a native application. So you build using web technologies: HTML/CSS/JS to build your app, but also have access to Operating System tools like a native application would. .Net is a whole family of technologies, but the UI Framework aspect is what I suspect you're really getting at. .Net has a few different options for UIs and they're all native, that is the UI is built on Windows' subsystems and the .Net Common Language Runtime (CLR) which is what powers the C#/F#/VB.Net languages. .Net UIs generally fall into two categories: WinForms which is a UI based Forms builder, and WPF or UWP which use XAML, an XML markup similar in function to HTML but is more dedicated to application development rather than document markup. With both these options your coding language will be a CLR language, and how you build the UI will be a part of the UI Framework you choose (WinForms/XAML).
I've been using Fiddler, but I think it may be a bit overkill. It's pretty cool though, you should check it out.
If you're talking about Microsoft Web Api 2, they provide a test server component that essentially sets up a HttpClient and port bindings for you. 
I've used Runscope in the past. Postman for local development. Ping-API is also nice. Also, if you keep your controllers thin, and just delegate the request you have no real need to test the API. Just unit test the service that you delegate to.
Stupid TFS was saying they had the latest code on everything but they didn't. This function that accepts an enum took a boolean 2 weeks ago. Some how this one file out of thousands TFS decided to not update. And they just happened to have redeployed an update minutes after I had trying to double check things.
I guess I should specify a bit more detail, because looking at it, my model actually is more like what you've stated. We'll go with what you've stated here: |Task|(1)--(n)|Submission| Task describes the task that needs to be done, and Submission is each submission against it tied to a due date. When a Submission is marked Incomplete, a new Submission is created, with some way to to show how many times a Submission has failed. Many of these tasks can be a regular occurrence (Such as a monthly task). BUT if the submission is not marked complete, the original task is still not technically done, it just goes into a "Re-submission" phase, creating a new submission requirement and referencing the previous ones. And this is where my hangup is. Thanks for your input! 
It also allows you to test the UI more when you will be presenting those strings in some way. You can test with a string at the max size and make sure your layouts still work as expected on the different screen sizes etc you are targeting.
&gt; BUT if the submission is not marked complete, the original task is still not technically done, it just goes into a "Re-submission" phase, creating a new submission requirement and referencing the previous ones. And this is where my hangup is. Let me see if I understand (because I don't think I do). A task is created, and then submitted. The submission itself has the due date (calculated by the task's recurrence interval?). If the submission is not completed by that due date, it should be *auto* submitted again. If I am following this, I'm not sure what the difficulty is. Isn't this just a matter of finding the most recent Submission during the ETL process, checking whether the due date has passed, checking the status, and then if it's overdue and incomplete, resubmitting it? I guess I don't understand why a Submission needs a reference to previous Submissions- aren't they all part of the same task? Shouldn't the submission just have a link back to the task?
Well you said you're using entity framework? So don't you have that set up to connect to your database? If so, It'd be something like using(MyEntityClasses db = new MyEntityClasses()) { var tasks = db.Tasks.Where(t =&gt; t.TaskId == id); foreach(Task task in tasks) { //write data here } }
May you post a link for further reading please.
No, you were. My example does exactly that. Assuming you're following the structure I had laid out, you might have something like this in a databse. ID | TaskID | Name | CompletedDate ---|---|----|---- 1 | 1 | Task1 | 9-20-2016 2 | 2 | Task2 | NULL 3 | 3 | Task3 | NULL 4 | 1 | Task1-Updated | 9-21-2016 5 | 1 | Task1-Modified | NULL So, take this int id = 1; using(MyEntityClasses db = new MyEntityClasses()) { var tasks = db.Tasks.Where(t =&gt; t.TaskId == id); foreach(Task task in tasks) { console.WriteLine(task.ID.ToString() + " - " + task.Name); } } That would output something like 1 Task1 4 Task1-Updated 5 Task1-Modified The point there is the TaskID identifies a single task, and you save the values every time to track changes. You can see that the task name was changed to Task1-Updated on 9-20 because the version before that has a completed date of then. It was then modified again on 9-21 to Task1-Modified, which is it's current name (because the completed date is NULL). That's the entity framework version, that's the same as writing DECLARE @id int = 1; SELECT * FROM Task WHERE TaskID=@id; in SQL
I'll try to step through an example of what I mean. Let's say we have a Task named "Submit Time Sheet". This is a bi-weekly occurrence. (Perhaps it is more correct to call this table a Periodic Task?) A person submits this TimeSheet, but it is wrong, so it is marked incomplete, but the fact that person had originally submitted it on time is recorded (So it wasn't late, but it was wrong, so it needs to be rescheduled). The original Submission doesn't go away (so they can acknowledge that person did actually attempted to complete the task), but a new Submission request is created, because the original Task, as scheduled, is still technically not done. This way, we can still track the progress on the Task (and the fact that it was marked as incomplete), but also create a new schedule to make sure the Task does actually get done. I actually think I have a better model now (after assitance from /u/andrewsmd87), so perhaps it'll make more sense if I write out what I think will make more sense. |Periodic_Tasks| (1)---(n) |Assigned_Tasks| |Assigned_Tasks| (1)---(n) |Task_Submissions| Periodic_Tasks| --- ID: int| Name: varchar| Assigned_Tasks| --- ID: int| PeriodicTaskID: int| OriginalRequestDate: DateTime| Task_Submissions| --- ID: int| AssignedTaskID: int| DueDate: DateTime| SubmitDate: DateTime| Complete: binary| A periodic task is never technically "completed", but each assigned task is considered finished as soon as one of the Submissions is marked "Completed". Then, you can see how many attempts and reschedules it took to complete the assigned task. (*I think I have my problem solved now, but I didn't want to leave you hanging.*) Does this make sense now?
I guess I'm mixing up talking about how the data gets presented rather than saved. :P You're correct that it's in the right order, but I wouldn't want a user seeing the TaskID as the version Number, so taking your example output: ID | Name | ---|----| 1 |Task1| 4 |Task1-Updated 5 |Task1-Modified What I really meant was I need to display the output more like: Version| Name | ---|----| 1 |Task1| 2 |Task1-Updated| 3 |Task1-Modified| Even if the underlying data is still 1, 4, and 5. I think my silly for-loop would do the trick, but I just feel like there's a better way. I'm just being nitpicky, really. I really appreciate the help I've been given, thank you!
I use the built in tool in Webstorm 
Ahh, I gotcha. Yea, if you're in a for loop, then you'd just keep a counter. So using the EF example just start a counter. Also, don't forget we'd need to add a sort on your tasks, to make sure they come in the right order int id = 1; using(MyEntityClasses db = new MyEntityClasses()) { var tasks = db.Tasks.Where(t =&gt; t.TaskId == id).OrderBy(t =&gt; t.CompletedDate); //you may have to do some extra work here to handle the null date being first. int version = 1; foreach(Task task in tasks) { console.WriteLine(version.ToString() + " - " + task.Name); version++; } } The SQL equivalent would be SELECT ROW_NUMBER() OVER(ORDER BY ID) AS Version, Name from Task WHERE TaskID = 1 ORDER BY COALESCE(CompletedDate, '3000-1-1') The output for something like that would look like what you were asking for. 
Okay, that's basically the model I was working towards.
hail Git
Personally I think you should be focusing on aspnet core first. Might not be feature complete but it will probably replace the full framework eventually. (Except for legacy stuff) Some background info about dotnet core: https://docs.microsoft.com/en-us/dotnet/articles/core/index Tutorial for basic aspnet stuff: https://docs.asp.net/en/latest/tutorials/first-mvc-app/index.html And about Azure, after 3 years experience it's still overwhelming! I would suggest the following learn path: App Service Plan -&gt; WebJobs -&gt; Storage Account -&gt; Resource Manager (templates) -&gt; Redis Cache -&gt; Traffic Manager -&gt; PowerShell (AzureRM) This will keep you busy for some time
We ended up writing acceptance tests using specflow (doesnt really matter, unit tests would have been just as good). The key thing is we call the apis using HttpClient. We compare the result against queries we run against the db. They are tough to maintain, can be lengthy to create and to be honest a but fragile. But works for us and we have a high confidence the apis are behaving as expected. These run automated using TeamCity against a nightly build or on demand if required. 
Thanks! I assume if I learn Web API on core the skills are easily transferable back to .Net 4.6? Also if there a reason the Web App services isn't on the list of things to learn on Azure? I would have thought deploying websites and a database are the main uses of it? 
Typescript got non nullable types before C#. :(
You already know MVC, WebAPI will take less than a day to pick up. So I suggest to tackle that first. Then try aspnetcore
That looks cool, I wish there was an open source alt
Have you read .NET Framework Design Guidelines? 
Back when I was doing web development, we used SoapUI. https://www.soapui.org/ It is quite powerful.
Maybe you missed something last years; this technology was [abandoned by Netflix and EBay about 3 years ago](http://www.ben-morris.com/netflix-has-abandoned-odata-does-the-standard-have-a-future-without-an-ecosystem/). According to [official site](http://www.odata.org/ecosystem/) OData is used primarily by some MS products and supported by some vendors that rely on MS technologies. And the last, there are no any support for OData yet in newest .NET Core 1.0 platform. It seems this technology is not considered as "mainstream" by MS itself.
We use PCF at work and it does indeed make life easier for a developer. Things like log aggregation, simple monitors, routing services, easy scaling and service tiles really make app development and resource utilization a breeze. Our Ops guys had a bit of a struggle setting it up (they have it down now), so if you go with the hosted version it will make your life easier. Dotnet and windows works as advertised, though you may have some challenges taking a large monolithic app and pushing it to PCF because the architecture is really designed for smaller comoposable apps and services. It is possible, but we had to turn off some of the health check monitors to get over the long monolith startup time. For anyone who is interested https://pivotal.io/beyond-the-twelve-factor-app is a really good architecture guidance book on how to best design cloud native apps.
I'm not sure why its not there, but my understanding is that its pretty much deprecated; you can use the base Exception type. https://msdn.microsoft.com/en-us/library/system.applicationexception(v=vs.110).aspx 
From what I read, for uwp its different. Perhaps from the ongoing changes to .NetCore?
I'm not sure its the same sort of "non nullable" types. The description on that page talks about not being able to put a null into a number type which seems to be the equivalent of not being able to assign null to an int in C#. Edit: It looks like Anders checked that in, so anything is possible. https://github.com/Microsoft/TypeScript/pull/7140 
yes, and good!
I primarily build all our api's using data v4. It is so much more flexible than anything else out there and really gives web developers power to just pull the data they need. I think it never caught on because its from Microsoft
I just tested stakeholder and basic account levels and they can assign work items to other users. Are you getting an error?
The camel case resolver is not the default resolver. Remove this configuration entirely Edit: rereading your post, the correct thing is to not assign that at all. Which you didn't because you forgot 
Lots of things aren't supported by .NET Core yet, so that's not really a good argument.
Netflix abandoned having a public API entirely. So that's not really a good data point either.
No, when opening a task, I can't see any other users in the drop-down apart from myself. I would have thought the owner would be able to by default.
As an aside, why is camelcasing of JSON important? At what point did that become there defacto standard?
Can you try typing in their username?
Yup, you have to start typing their name. Once you've done that, they'll be in the dropdown.
Because camel casing is the standard in Javascript and JSON. Using standards and conventions when developing has a number of benefits. Edit: &gt; At what point did that become there defacto standard? Javascript has been camel case since it was invented, as one can see on the built-in functionality and standard library. JSON (JavaScript Object Notation) is deeply tied to Javascript and it's natural to apply the same conventions there as on Javascript.
Great, cheers!
Not stuck on Linux, just don't want to use Windows. Thanks for response. 
Welcome to the .NET. I share your frustrations. The big difference traditionally between those that use the Microsoft stack and everyone else is the former's use of Windows. And of course Windows isn't a command line based OS so it has lots of tools that are heavily relied on. I left a comment [here](https://www.reddit.com/r/dotnet/comments/53kvpu/get_started_with_aspnet_core_and_entity_framework/d7udwwr) a couple of days ago complaining that the brave new world of .NET Core looks like the old world as far as documentation goes (lots of tutorials based on Visual Studio). Unfortunately I can't think of any terminal based tutorials I can point you to although I can tell you Nuget can be used from the command line. A list of Nuget's commands can be found [here](http://docs.nuget.org/ndocs/tools/nuget.exe-cli-reference).
That has absolutely nothing at all to do with the .NET Framework Design Guidelines. As in there is zero overlap between the two.
You shouldn't need ASP for a console app, but we haven't migrated to Core as of yet: we're still on 4.6. I'm a senior dev of a .NET shop and we're keeping an eye on the core offering but we're just too engrained with our infrastructure to consider switching right now; there's no benefit to us. With that said if you're subscribing to all of these various feeds, web API is not going to help you. It's a RESTful format and if you're just transmuting data it doesn't make sense in this instance. You should be able to get by with the WebClient class for the most part for your subscriptions/publications. If it were me I'd create a console app (.exe in Windows, not sure what it is on the *nix side), and schedule it as appropriate. Espe if you're new I'd keep it simple. Also if you are doing anything in bulk (for loops) take a peek at how .NET lists hand things. There's a few different avenues but I prefer the ForEach method. The syntax is super nice, you can say .ForEach(thing =&gt; { thing.AccessOneItsProperties;}) so there's some handy shortcuts you should have. Not entirely sure if those exist in .NET core but worth checking out
Hang Fire isn't really a queue it's a scheduled activity broker 
While you CAN run Hangfire in process with your web app, I wouldn't. Hangfire needs to always be running, and you need to be careful of multiple instances running against the same data store, etc. Instead, create a topshelf windows service with Hangfire (there are examples out there). You can still expose the dashboard on a different port, which you could proxy through IIS if you need to expose it. This has been the most solid decision regarding Hangfire that I have made. I know it seems like more moving parts, but there are a few special needs Hangfire has that makes it better to separate it IMO. Hangfire works really good for things like "run task X every X minutes", where task is a static coded set of steps. It isn't a queue though, its more like CRON, but with built-in facilities for failure reporting, retries, etc.
I would highly advise to not look at it like a message queue 
Thank you. Yes I wanted to start learning the language features of C# hence the desire to port rather than continue working in Java. Cheers for the tips!
Cheers, thanks. I will check it out.
Excellent. Many thanks. I'll look for something similar to [Twitter4J](http://twitter4j.org/en/) which I used for my old project. I don't fancy wrestling with the full Twitter API just yet.
Cheers for that! Very helpful. I think doing Web API/MVC 5 on .Net core might be worth it. A lot of those features do look good although I got to understand them better by actually using them! 
IMO your endpoints should not be singular or per-entity. Your endpoints should be per-operation. i.e. /api/user/create /api/user/{id}/edit /api/user/{id}/delete /api/widget/{id}/disable This makes a few things simpler: * parameter validation * data validation * role (permission) checking * reasoning about process flow * automatic doc generation - think the webapi help pages, or a swagger doc Hey downvoters: whether or not the actual command is derived from the verb or the url my point still stands - your endpoints are per-operation. Also, REST is a great spec, but it isn't gospel
The problem is that my colleagues aren't agreeing with per-entity endpoints. We have more or less 20 entities and they don't want to keep repeating the CRUD operations. They want to make everything "generic". I can't convince them to use this per-entity approach.
Yep, that's what we're doing right now. I'm just worried if there will be some implications with a "generic" endpoint.
It doesn't have to be CRUD specific though. For an api I'm working on right now there's an entity that the system creates, later on a different system needs to provide their own ID for it, so we have an `.../applyId` endpoint. Similarly there are several endpoints that are basically triggers - they take minimal input and process a moderately complex workflow. Neither of those fit well in a strict CRUD definition, but are completely valid webapi endpoints.
Yep - his routes are made redundant by the correct usage of the HTTP verbs.
By the sound of it, your colleagues aren't the client(s) or the client stakeholder(s) of the API, and (to put it bluntly) they may need to be reminded of that. You need to look towards the clients and find out what best suits them. I strongly suspect that what will best suit them is for you and your colleagues to better align your API(s) with industry patterns, rather than obscuring everything behind some single opaque vague endpoint. The latter is just going to make things worse for the clients immediately (your snowflake API won't match the patterns of other APIs they use and no tooling will support it well, for example) and for everyone thereafter (since you'll (luckily, in some ways) suffer the same mental model and tooling problems as the client, for example when building and maintaining your test automation, and because premature generalization is almost always a mistake with a price paid in bugs and hampered maintenance and expansion.
Maybe that was misleading, I always hated those HtmlHelper classes that rendered input boxes in &lt;%%&gt; blocks for you, to use succinct, composed, real HTML is a huge boon. It was almost like you made your front end, then relied on server rendering too heavily. Is that clearer?
So your approach doesn't count as generic endpoint? Right now we have: `/api/Data?entity=entity_name` Just that.
I couldn't budge them from using the single endpoint API. If this all goes down to industry standards and principles, then my colleagues don't want to follow it. 
Ah you assume I know more than I do! Last I heard Razor was the tempting engine for MVC? Are you talking about Angular now? 
Thank you for your responses. I will be taking the time to read through the source code and the issues. I would like to hear more about not using it for a queue system. Implementing a producer/consumer queue can be complex and I was intrigued with the simplicity of the setup for HangFire. Does anyone have any examples about how it has not behaved as desired? One reason I ask is that I will need to take these examples back to my team for further discussions and right now they are seeing this as a "great solution."
This. Every time. Http verbs make everything readable, composed and less prone to nasty edge case behaviours onboarding new developers.
Razor is, there's just a lot you don't need. Front end I prefer angular or react though :) that's just personal preference though. MVC3/4/5 try too hard to be all things sometimes, but if you use a pattern you are productive with and abstract over it, using the strengths of each framework, I find my apps are far better composed. For example: I use forms authentication for login and access control - tight as a nut and strong over https Contact forms and data entry I use a client side model binding framework, like react/angular/vuejs. If I'm writing a typeahead input, I use angular and model binding as that's it's strength. If I want to SEO enable a page, I render the content server side, using razor so google picks it up. Then I use the WebApi for any endpoint I need to make available to angular/react/vuejs as this can take care of my access control. I was never one to jump on the Knockout bus though, it always seemed a bit poop. But I've worked with Developers who love it.... each to their own :) It's all about being productive though, use what ever technique allow you to get things done, fast, and well. 
I am not sure if who Sean Kelly is, and I mean no disrespect to him, but this has several fundamental misunderstandings with how microservies are used in practice, at least at the organizations I have been at or studied. It seems he is approaching this from a purely academic perspective. I won't do a huge write up reply, but a couple of examples that made me cringe... * His 5 truths are completely arbitrary, I have never heard of this as criteria for a successful microservice, and certainly never as a "truth." * ‚ÄúDistributed transactions are never easy.‚Äù is terribly absolutist and the basic tenet of modern application design, not even with microservices. Distributed transactions can be hard, but his reasons for why they could be hard are not even that complex. And the EASIER part this is listed under is more about maintenance than architecture or implementation; maintaining a microservice is far easier than maintaining a function in a monolith. * It‚Äôs Faster - ‚ÄúYou could gain a lot of performance in a monolith by simply applying a little extra discipline.‚Äù I mean sure you CAN, but it is magnitudes harder to do since the framework of the monolith will be working against you. You can make a minivan go faster than a corvette with a shit ton of engineering, but that's certainly not recommended. * Simple for Engineers - ‚ÄúA bunch of engineers working in isolated codebases leads to ‚ÄòNot my problem‚Äô syndrome.‚Äù And this one is just bullshit. If you are in charge of making sure, for example, a file gets converted from one format to another, the expectation of that service is very clear. When you have a monolith, that's when things get muddy. Which part of this singular use case, using dozens of functions, which dozens of developers have touched, should it be fixed? That's the hard question. * Better for Scalability -‚ÄúYou can scale a microservice outward just as easily as you can scale a monolith.‚Äù And this one is also bullshit. Any monolith that you can spin up a new VM, throw it on a server in 5 minutes and get it working is not a monolith. That's what I do weekly with our microservices. Resource implications alone are totally different. This ended up being longer than I planned, and there are times for a monolith, but with how good distributed computing, and the languages around it, have gotten... microservices tend to be easier to implement, easier to maintain, and perform better. They will have their problems like any piece of software, but they tend to be much easier to solve and fix.
I haven't mentioned yet that I went against the generic approach, because it seemed anti-pattern, and the decision to use generic endpoint was decided by the majority. The implementation is like this: `Get entity -&gt; Get repository method -&gt; return data`. Will this type of approach give us some problems down the road? With logging, I still don't know how they will implement this.
&gt; It‚Äôs Faster - ‚ÄúYou could gain a lot of performance in a monolith by simply applying a little extra discipline.‚Äù I mean sure you CAN, but it is magnitudes harder to do since the framework of the monolith will be working against you. No it's not. At least not in any that I've ever seen. Whether you have a micro-service or not is primarily dependent on how you package the various functions within your application. A naive micro-services implementation is literally just taking each controller or service class and deploying it in a separate package, with cross-class calls becoming cross-process calls. (The last bit is of course where the performance problems arise.) 
I always wonder at people who think any tech is a silver bullet. They are all just tools for doing our job and they can be misused. The architect's job is to make smart decisions about what makes sense
Man, I wish the notion of micro service were just dictched all together. Everyone sees how Neftlix does it and thinks, "I can do that, I want to be like one of the big boys" When in truth its probably not the best architectural approach and unfortunatly you and your organization dont have the technical acumen to vet out the solution properly and determine a plan for implementation. If you're gonna internets it for a solution, odds are, it will be wrong iteration 1, iteration 2, and probably iteration 3. This shit gets complicated really really quick. Personally I'd like to see the notion of domain driven services. Build your services to support a given domain. It's up to the teams to figure out how finite a domain really is. Figure out your logical boundaries, the apis for those boundaries and implement as appropriate. I like to htink of it as mini-monoliths that can be even further minified as/if necessary. _If_ your doing agile in the true sense, not from a process persepective but rather a arch/design perspective, refactoring to smaller domains shouldn't be an issue. 
If you're doing ReST this is not right. 
one tweak that we've started doing to keep versioning under control. api/[version]/user 
if you read Sam Newmans book on microservices,you see that he argued for microservices mainly for their increased agility,theirability to react to change more easily, and he says himself that you must model the domain so that your services are decoupled,otherwise you will not be able to attain any benefit from using microservices. This is like on page 5 of the introduction. 
Damn, that's unfortunate. About the best I can offer is a suggestion to at least keep everything behind that API surface as cleanly-decomposed as possible, so that your work is easier now and so that when their painful realization arrives the fixes won't be quite as difficult. :)
I used micro services in my systems for the following functionalities: 1. Send email (pass it via a message queue) whether it is transaction or batch. 2. Lookup values from a bunch of Excel sheet. Write a micro service for reading Excel Sheet data sources. 3. Image server (upload, image serving and manipulation) 4. Identity server based from different systems. 5. Doing projections/calculations of database values in memory That's it so far. 
&gt; It got to the point where processing an order could take 2 or 3 seconds a piece because the rules got so complex. That sounds like a perfectly valid reason to isolate a feature. Not to make it faster, but to protect the rest of the system from it.
Just do a quick binge on pluralsight videos
I find they vary in quality quite a bit. There are ones where some guy will drone on for ages about opening visual studio, and he like to configure VS in a certain way but I don't have to do that. Basically this: https://www.youtube.com/watch?v=9bUtZzi6hag
Yeah I agree they vary. They usually have two courses on the same topic. One buy a guy rushing through everything briefly and another with a guy taking his time. I usually put them on my second monitor when I'm playing counterstrike and just use them as a general introduction to a topic so I know enough to be able to understand the reference material. One I have the basic I just google everything anyway.
Don't use msmq backend. Just plain sql. Also- ninject works great with it. I prefer to run the job engine in its own service, and alow jobs to be queued from my front ends 
Not sure why you're being downvoted. You're adding to the discussion, whether people agree with you or not.
Is there a reason you have the models in the "middle" tier of your project? Wouldn't it make sense for them to be in the data layer since they should not have any logic in them anyway?
Here's a really good ASP.NET core tutorial, but he goes over Angular 1. https://app.pluralsight.com/library/courses/aspdotnetcore-efcore-bootstrap-angular-web-app/table-of-contents You could always just take a separate course for Angular 2
It does if you actually need your events to fire on a schedule
RemindMe!
If you don't specify a Controller/Action or request type, Html.Beginform defaults to the current Controller/Action as a POST request. Html.BeginForm() Is therefore basically equivalent to the following var action = ViewContext.RouteData.Values["action"]; var controller = ViewContext.RouteData.Values["controller"]; Html.BeginForm(action, controller, FormMethod.Post) Or, as a single call Html.BeginForm(ViewContext.RouteData.Values["action"], ViewContext.RouteData.Values["controller"], FormMethod.Post) `ViewContext.RouteData.Values["..."]` being a handy way to access the current action and controller. You can, of course, still set the controller, action, and method manually. Since you'd typically use Get requests to view the form, you can handle this with something like this. I'm using Create as the action name since that's how you described the page [HttpGet] public ActionResult Create() { // Code that **creates** the form // Typically mine just returns the view... } [HttpPost] public ActionResult Create() { // Code that **processes** the form // Typically mine processes the input, then returns the view with errors if necessary, or redirects elsewhere if everything's fine } Note the [HttpGet] and [HttpPost] attributes, which specify what kinds of requests to respond to. If you don't specify Post/Get (/Put/Delete) methods like this, MVC will just use the first matching action that it finds for the Action specified regardless of the HTTP method being used - you only need to specify the attribute if you have more than one action with the same name, in the same controller. I find the above to be quite neat, because it ties the form and the processing together without having to check for form data before validating. It also means that both can use the same `Views/&lt;Controller&gt;/Create.cshtml` View, which is nice!
Interesting I just had an interview where the term MICROSERVICES was used. I never heard of it.. I heard about IoT and using a service bus.. Is Micro services the new SOA ? 
It's a bot that lets you set a reminder to come back to a comment after a certain amount of time. I wanted to remind myself to come back to this comment to see if anyone responded to you with a good tutorial!
Ill need to find some 5 minute video to explain to me before my final interview with this non profit where the first thing they keep asking is 'How do you feel about microservices' Know anything clever I could say or maybe some simple source code to get my mind around what its about ? 
Oh, great! Thank you
I can vouch for this Pluralsight course 100% as Shawn Wildermuth literally explains every bit of the code he writes in this application from project conception to deployment. Many tutorials shows you how to do something, few explain the why. This course does both. Shawn pushes for true understanding as opposed to rote memorization/code regurgitation.
I'm sorry... This is clear as mud. This doesn't help me see what is in / out of core. WCF? WorkflowFoundation? XAML parsing? I get that what you are trying to do is hard, and you are doing a good job working towards that goal. The message is sometimes difficult to follow though. So keep going forward with core and availability of APIs, just remember to sit someone down to write a big long list of In / Out. Doesn't matter if the list is super long, I have CTRL-F and grep!
If you cant install 99% of this stuff without a dedicated hand-holding app to do it for you, you shouldn't be in charge of a Windows Server box.
Seems like they have branded the lowest common denominator as the new standard. 
Probably some hosting issues (it's a rather cheap hosting solution) ;).
For someone who's been out of the loop for a couple years, it's hard to keep track of these .NET changes and naming. .Net framework? ASP.NET 5.0? .NET Core 1.0? MVC 5 or 6? Does MVC version match .Net version? And now .Net Standard? I'm losing my mind. So much articles/tutorials out there are already out of date. I've been searching for something as simple as setting up ADFS authentication in MVC apps and found so much conflicting information (and none of it is helpful).
It's clear that MS confused lot of developers with introduction of .NET Core. NET Standard should fix it. That is version 2 should fix it. 
&gt;Do other platforms have this absurd rate of churn and change? Not all. C is basically the same forever. Python web frameworks are also very very stable (APIs don't change). Java frameworks are rock solid stable. D library stability is okay. On the other hand, .NET, Rust and Ruby annoy the hell out of me with their constant changes.
This is a replacement for pcl. It should make things easier actually, as there will be a defined set of core cross platform functionality.
Python 2 =&gt; Python 3 is a massive compat break
You forgot netcoreapp
Javascript or Swift. Take your pick. 
The article wasn't really about new features in Core. Think of it this way: the full .Net framework supports many features that aren't cross platform. How do you know the code you write in full .Net will work in .Net core on Linux? netstandard defines the *common API* between all the different "types" of .Net - full, core, Xamarin - so that the code you write can be shared between them.
Could be worse Win Forms and WPF were announced within about a year of each other.
Waiting for the .NET Standard Core
Hardly. Windows Forms applications were derived from VB6 forms. But looking at straight dates, windows forms were introduced in 2003. WPF wasn't introduced until September 2006.
What are child actions? I didn't get you. Edit: I did a quick research and it looks like you are referring to partial views specifically used to act as widgets. According to [this](http://haacked.com/archive/2009/11/18/aspnetmvc2-render-action.aspx/) blog post, it works by using - @Html.action
In general, yes.
Assuming you target only the Standard APIs? Is .Net Standard a single reference? 
Who is this aimed at? and late reply to what? Most useless thread of the year award.
So, if I'm reading correctly: .NET is now more cleanly organized so that there is a common set of libraries *all* .NET applications will use, regardless of platform or architecture. This underlying set of libraries are the ".NET Standard Libraries". The implementations of .NET (.NET for Windows, .NET Core, and Xamarin) all build upon that base for platform-specific functionality. This implies that I could write a shareable library that could be easily portable between all versions of .NET without worrying which version I'm targetting, no?
JavaScript hardly ever changes.
This is clearly spam. /u/Arowin /u/ZeroBugBounce /u/Cylons please take this thread down. 
done
It's not that the app "needs" a pristine OS. It's that it allows you options to do things if you're able to create a pristine isolated environment for your app to run in. Like when I want make a bunch of instances hosted on different environments and not care about the host environment and what is or is not installed on the server already and be able to create new pristine instances to match demand. Or as a host environment I don't want to worry about what the app needs or if it conflicts with other apps. It's the same needs that give us virtual machines, but with much much smaller virtual machines.
Whilst you may not be wrong, that's a fairly blinkered view of the benefits of containerisation. For example, image sizes are ***greatly*** reduced when compared to, say VMs. You can choose between `windowsservercore` or `nanoserver`for the host Windows image, but the latter is probably going to be the most commonly used since IIS works on this along with a few stripped down APIs.
I suspect that this was intended to be a response to the following topic: https://www.reddit.com/r/dotnet/comments/54ecsa/web_api_with_single_endpoint/
Serious question. What is docker for? I have yet to hear an explanation that doesn't sound like it's simply a bunch of standardized scripts for installing and installing various apps.
The point is isolation so that if one of the apps on your server is buggy or full of security holes it doesn't take the rest of the system down with it.
[removed]
Yes if you are writing a library, you should target .NET Standard if possible. When an application uses your library, it will specify which framework is actually used.
* Libraries should target .NET Standard (ASP.NET Core does for example). * If a library does target .NET Standard, an application built to .NET Core or to .NET Framework can use it (or Mono or UWP). * Applications are targeted to either Core or Framework (or Mono or UWP). * The bootstrapping code for an application does NOT appear to be in .NET Standard (you cannot create an application targeting Standard afaict). 
That makes sense. Clearly worded. Thanks.
Hey Fuzzy, the idea is really cool :) We tried it right now but sadly it doesnt work... A coworker and I installed it, we set our username and a sinple shared folder - he tried to edit a file, but nothing happened on my side. Did we do something wrong? 
Its a neat idea, but I'm not sure I would want to work on a project where it would be needed.
How are they isolated? If I run two docker containers on the same box they still share cpu, memory and disk don't they? If an app cause a system failure or hogs CPU, both containers are adversely affected.
Thanks. I've not properly turned off the sql connector when a connection string isn't entered. I'll take a look but that shouldn't stop the file sync method from working.
Is there a ELI5 for the TL;DR? 
Ahh, that explains it. SVN isn't supported yet. That has brought up one issue though, really it shouldn't create any files if a non-supported (or non-controlled) solution is opened. I'll also look at logging the fact that a non-supported solution has been opened. --- Thanks for the testing!
Not considered supporting SVN, but I'll definitely take a look. The idea of a window is great, thanks!
Got some examples?
Most definitely sounds like the app pool user (which in this case, if iis express, is you). There is a way to run Visual Studios so it launches IIS Express as a specific user through run, but that's on a another pc, so here is a stack article on how to maybe just run as administrator: http://stackoverflow.com/questions/12257110/can-you-force-visual-studio-to-always-run-as-an-administrator-in-windows-8 
[Portable Class Library](http://stackoverflow.com/a/5238981).
Just in time to port it to .NET Standard! Sorry, couldn't help myself. :-)
Great article, thanks!
The toolbar was changed in lollipop but everything else was mostly design standards. It was all MUCH needed in my opinion, so I would hesitate to call it "specially bad", but perhaps you've had a more painful experience than me. Fuck fragments tho ;)
It was... blame the absolutely horrible mobile site for that one... Apparently, trying to reply on a comment, when it works creates a new post....
String interpolation is nice.
Ah, that's what it's called? Thanks, I just found the MSDN page for it. It seems like you can do more than just what I was thinking, too. This is great. I haven't kept up on new language features in a while, especially since I've been doing a lot of Python for work, but this kinda makes me excited.
Yeah, lots of nice things. I really enjoy developing using C#. Still like F# better though :-) 
Try these too: https://github.com/gregoryyoung/m-r http://josephdaigle.me/2016/05/17/command-pattern-therapy.html 
I'm not clicking this out of protest for the clickbait headline. NO.
Good call, I won't be able to right away but at least flagged this post. Thanks!
Ahhh, now this makes more sense to me. I didn't realize they were using a VM. Do you know if in the container you can constrain Memory or CPU Cores?
According to those docs, it requires some nonsensical proprietary encoding that's not part of standard URL encoding. It doubt you'll find anything compatible in the core libraries.
I was on copywriting workshop two weeks ago, so I just want to try it in practice 
Sorry :( I will try better 
I was thinking about this title. The copywriting workshop just wash my brain.
Does it always require editing the class files to add attributes or is there a way to apply it to the whole assembly with a single attribute?
Glad to see you're on Reddit. I've come across your two series on Event Sourcing and am going to be working through them.
Right now, yes it only applies to single methods with the attribute so you have to edit class files. Having it apply at least at the class level did cross my mind so I'll probably take a stab at it soon. Searching through the assembly (reflection-ish) was easier than manipulating the IL so definitely doable. When I do make an update, I'll be sure to post something on my blog about it. Pull requests are welcome too :)
You could try the Sharepoint version SPEncode.UrlEncode https://msdn.microsoft.com/en-us/library/microsoft.sharepoint.utilities.spencode.urlencode.aspx 
Hi, thanks for the reply. I realized now that I need to change my question a bit, so I will just post it again with a clearer question. 
I am not working until next monday, but sure I will try. Thank you nice idea :)
You actually can target other versions of the framework. However in practice it is not stable. 
What are you trying to do? If you just want the html of a site that's as easy as doing a GET request. If you want the effective html after the JavaScript has rendered you need something to run the JavaScript like PhantomJS.
You could do that, or define the method in a base class that itself inherits from system.web.ui.page and then have your pages inherit that. Master pages should be used for layout/markup, not shared code methods.
Phantom isn't going to work for me. Ultimately im trying to grab a json file from a link in the page. When I use HttpClient() I get the "no script" version of the page which is useless to me. I need the HTML to feed into HtmlAgilityPack and parse for a specific element with a specific id.
Eh well if we start bringing personal preference into it, I'm dropping web forms like a bad habit and going to mvc or webapi. Still, within the constraints of web forms, if you're going to use it on every page of the same type, that's a class by definition. If you start needing an event handler in multiple different types of pages, I have to wonder what the heck that event handler is actually doing. Maybe look at a user control. I'm not a big fan of random static methods being invoked from who knows where... Inheritance is there for a reason. Use it.
I find having a repository useful for when I inevitably have to support more than just an EF context as a data source.
I don't like calling the wrapper for EF a repository, because it's really just the public interface of my data assembly. 
You only need request models and response models. Everything else is a waste of time and effort. My recommendation is to start by ditching EF. Use Dapper, Chain, or another micro ORM to handle the mapping for you. Yes, you will have to learn SQL. That's a good thing. *** Make sure you mark.your classes with DataContract/DataMember attributes. This is how you indicate what data stays on the server and what can be sent over the wire. *** Your project separations are somewhat flawed. At the bottom.should be pure models. They can have as much business logic as you care to shove in them, but no database access. Keep them 100% unit testable. Database access foes in your services project. This is all of your integration tested code.
Use AngleSharp instead of the WebView.
http://www.hanselman.com/blog/SelfcontainedNETCoreApplications.aspx
I believe this is what I read! Thanks a lot :)
Can't wait for this! The best current solution was linked to by /u/SikhGamer which is what I read about before
You are welcome :)
Yeah I tried that and there were no dice (yes I tried it with JINT). I ended up finding the link i needed buried deep in &lt;head&gt; but the problem of getting DOM after JS does work still exists. Was still given the noscript render of the page
Seems a clever solution to me
Fair enough. I just haven't played with page inheritance myself - granted webforms kinda lends itself to bad practice, but our solution tends towards static 'helper' classes in cases where methods are shared between projects or pages. At the end of the day we all hate the helper classes so you're certainly presenting the more desirable pattern. Just spitballing.
In my projects I just have a plain old object for EF and a DTO of that model. Only two models. The API accepts the DTO, maps to the POCO with EF for changes, and maps back to the DTO to return to the client. edit: I think it's all about walking the line between pragmatism and "patternism". Apply the techniques in RESPONSE to your applications needs. Do you predict a lot of your components will be swapped out/changed? The reason to have all those mapped models would possibly be because you expect each side of the connection might change. Do you think it will? If not, trim it down. I'm writing about it [here](http://makingoutwith.net/2016/budgeting-app-with-aspnet-core-part-2/) in case you're interested.
Regarding what to use if you are not using repositories on top of EF, many are starting to see success in looking at things vertically instead of the traditional "layered architecture" that pushes us towards using the Repository pattern to suppress Entity Framework below our Application Layer. [CQRS](http://martinfowler.com/bliki/CQRS.html) isn't new, but it has a lot of steam these days (there's another question about [CQRS and WebAPI](https://www.reddit.com/r/dotnet/comments/54tulc/cqrs_in_web_api/) posted recently) A good source for diving in is Dino Esposito and Andrea Saltarello's series [Introducing CQRS](https://www.microsoftpressstore.com/articles/article.aspx?p=2248809). With regards to Entity Framework and the Read Model (your controller actions that query data only), he proposes we expose `IQueryable` all the way to the Application Layer using a "Read Model Facade" (similar to /u/celluj34 's `Reader&lt;T&gt;` example). He describes using Layered Expression Trees in LINQ to drill down to the data you need for the View Model (I posted a bit ago about this [here](https://buildplease.com/pages/lets-iqueryable/), but have learned a bit more and might actually revise it). With the Write Model (your controller actions that alter state), you would use commands that are sent from your controllers to a "command bus", which then are passed to Entity Framework for updating the db. He discusses the command "bus", but something that might be considered a "bus" would be using the Mediator Pattern (a GoF pattern) as described [here](https://jonhilton.net/2016/06/06/simplify-your-controllers-with-the-command-pattern-and-mediatr/) by Jon Hilton and shown in Jimmy Bogard's version of Contoso University [here](https://github.com/jbogard/ContosoUniversity/blob/master/src/ContosoUniversity/Features/Course/Create.cs) (write model with MediatR command handlers that write to EF). You can see more clearly what's going on with MediatR in Jimmy's [Course/Create.cs](https://github.com/jbogard/ContosoUniversity/blob/master/src/ContosoUniversity/Features/Course/Create.cs) (Write Model) and the [Course/Index.cs](https://github.com/jbogard/ContosoUniversity/blob/master/src/ContosoUniversity/Features/Course/Index.cs) (Read Model) Anyway, I know this isn't necessarily *solving* your issues with mapping to different models between your layers, but it might change the way you're thinking about leaking EF into upper layers and eliminate some of the extraneous mapping you might be doing. For code to look at, I'd definitely suggest cloning Jimmy Bogard's example, especially since he wrote [Automapper](http://automapper.org/). Hope this helps.
Oh haha! Well guess you didn't need the intro then :) Hopefully the links help you with that one too.
I still need it, still wrapping my head around these stuff , taking a look at mediatr , haven't used it before
It's not clear how this could possibly work. I can't think of a way to make an attribute change the behavior of a method without introducing some kind of new build step, but there is no documentation I can find about what that build step might be. I tried applying the `[Cacheable]` attribute to a simple method in a simple class. It didn't appear to do any caching. Multiple calls caused multiple invokations. I'm not sure what to make of this.
The blog post is almost 2 years old, and contains no links to any code or framework references. I might seem as a nice approach, but I have a hard time seeing how it would be implemented in a clean way, without being part of the framework.
Saw this posted on r/programming but it wasn't submitted here. I found it very informative.
Is productivity power tools for VS2015 useful anymore? VS2015 has many of its features inbuilt.
it's up to 2.0.3 without anyone opening issues or having PRs (not counting the spelling bot). I assume it was done for the dev and no one else seemed to show interest.
[MiddleClickDefinition](https://visualstudiogallery.msdn.microsoft.com/953f6209-fd99-4208-96b2-315e2fa24744) FTW
How do you use the custom document well?
good question! maybe try injecting some JS of your own (load a wrapping frame in the webview of something) that does the dom inspection and saves the result somewhere?
Does this work with perforce?
Omg this is amazing 
Many of the Power Tools features make it to Visual Studio but many are still there. I use it all the time. In fact, I'm not sure what I'd do without it anymore. 
I am humbled.
It depends on what kind of extension we're talking about, if it's an analyzer, it slows down VS, heavy extensions too, but light and specific ones I don't think so, although they can throw exception or crash the VS if they're low quality.
thanks for sharing
See if [IL-Repack](https://github.com/gluck/il-repack) works for you. Eventually as [task for cake build](http://cakebuild.net/dsl/ilrepack) Just stumbled on something similar: https://github.com/Fody/Costura
Adding the package added the following for my project: &lt;Import Project="..\packages\Shaspect.1.2.0\build\net45\Shaspect.targets" Condition="Exists('..\packages\Shaspect.1.2.0\build\net45\Shaspect.targets')" /&gt; &lt;Target Name="EnsureNuGetPackageBuildImports" BeforeTargets="PrepareForBuild"&gt; &lt;PropertyGroup&gt; &lt;ErrorText&gt;This project references NuGet package(s) that are missing on this computer. Use NuGet Package Restore to download them. For more information, see http://go.microsoft.com/fwlink/?LinkID=322105. The missing file is {0}.&lt;/ErrorText&gt; &lt;/PropertyGroup&gt; &lt;Error Condition="!Exists('..\packages\Shaspect.1.2.0\build\net45\Shaspect.targets')" Text="$([System.String]::Format('$(ErrorText)', '..\packages\Shaspect.1.2.0\build\net45\Shaspect.targets'))" /&gt; &lt;/Target&gt;
#if NET45 // This method doesn't exist in the CoreCLR return Assembly.Load(assemblyName); #else return Assembly.Load(new AssemblyName(assemblyName)); #endif This difference seems like an simple overload to write, why wouldn't they add it?
Thank you for the response!
Thank for the response! I think I am going to start the project over using MVC.
Forgot to mention one other site, Scott Wlaschin [F# for Fun and Profit](http://fsharpforfunandprofit.com/). While some of the content targets F#, a lot of it explains functional programming in a language independent way, like his talk [Functional Programming Design Patterns](http://fsharpforfunandprofit.com/fppatterns/).
Have you looked at SignalR? https://www.asp.net/signalr
Yes, but how does it answer my questions?
As much as I prefer the clr over the jvm, I've found that most of the industry is using scala for FP these days. Also as others have said, C# can be used in a functional style.
Not sure if there is anything special in IIS that you need to configure with regards to the cert. When we started we didn't have access to IIS8 so we self hosted. IIS7.5 doesn't have support. But in general when a client contacts a WSS endpoint the handshake is TLS negotiated before it even gets to the HTTP Upgrade request so even that request with accompanying HTTP headers is encrypted. DDoS is a common concern with any public endpoint. This is not a security precaution to rely on, but in my experience most implementations start dropping new connect requests somewhere around the hundreds to low thousands per sec mark. But again, nothing special here unique to WS.
F# is a cool language but as mentioned elsewhere it's tooling is way behind C#. Personally I'm pretty comfortable with a basic text editor (when I use VS I don't use any extensions and hide the menus and sidebars) but others working with .NET aren't so comfortable. Haskell is really for the purists - I installed it, played with it for about an hour and thought this is too much hard work :-) A couple of others that aren't mentioned that really intrigue me are: 1. [Elixir](http://elixir-lang.org/). It runs on Erlang's VM (Beam) and is starting to get a lot of attention. It has Ruby inspired syntax which makes it really easy on the eye but it's a lot more powerful than Ruby. It has its own Rails web framework called [Phoenix](http://www.phoenixframework.org/) which has excellent documentation and is definitely something you can use in the real world. 2. [Elm](http://elm-lang.org/). Elm is a strictly functional language (unlike F# which is a hybrid) but it's only really built for the front end at the moment (though there are moves to move it to the server). Again, it has great documentation and looks really promising. I've played around with it a bit and really like it myself but it does have some shortcomings when working with other JS libraries but if you want to strictly focus on FP then it's one to explore.
Why do you prefer the CLR? I prefer C# over Java but I think the JVM is more impressive overall - better ecosystem, truly cross platform, much more variety in choice of languages.
I went from C# to Java to Scala. Look up Scala play. Check out this short 20min video which introduces play. https://youtu.be/eNCerkVyQdc also use IntelliJ as your IDE.
I did take a brief look at Elixir/Erlang and really liked it (I previously had experimented with Rails some years ago). It was more or less the idea that since I'm a .NET dev I'd have a better chance of getting anything cool I might learn used if it doesn't involve a whole different ecosystem, also .NET jobs are very prevalent where I am and everything else is basically unknown. However, Elixir is definitely something I thought was cool so I might go with that just on that fact alone.
Check out implementing a repository pattern with EF. There is a lot of stuff online about using it with asp.net and MVC. Good luck!
Would you give an example of where F# tooling lacks? I've been using it (coming form C#) and I'm happy so far.
I had to make this choice recently and played around with Haskell and F# for comparison. Haskell has really nice concise syntax (no tons of lets all over the place) but there is a severe lack of tools (good luck making an android/ios app). However, it's the opposite with F# (IMHO). Although with F#, it feels a little weird using the .NET library in a functional manner (most of it was designed for use in an object oriented language) but I found a library that wraps most .NET functions in a functional way (Fsharpx.Extras) and so far I've been enjoying F#! If you know most .NET then that is an absolute advantage. I could develop web apps (Sauve.io/Freya/Nancy/ASP.NET+Fable/Funscript) which is what I wanted but also F# works with .NET Core and Xamarin so it could virtually do everything that C# does. If you want more proof of F#'s wide array of tooling, visit fsharp.org and fsprojects.github.io. You mention SQL so I have to tell you to check out F#'s SQL Type Provider (pretty awesome!). Another thing is that most F# books are either out of date or have low reviews. I have found fsharpforfunandprofit.com (checkout the 'Why use F#' section!) to be an excellent resource along with a pluralsight course (F# Fundamentals). Hope that helps!
I recommend you do learn F# even if you are still going to continue to work with C#. This will teach you functional programming and you can apply some of the things you learn when writing C#. Plus there are upcoming features for C# which are straight from F# so you get a sneak peak on that while using the top-notch tooling C# does.
Yes I looked at that too, and it seemed a straight forward option, but [it seems](http://caniuse.com/#feat=eventsource) the Microsoft browsers have no support for it, and IE probably never will. I don't know if picking something widely supported or something that I'd need to hack up a fallback for is more favorable.
People notice the lack of folder management to structure their systems right away. The single-pass compiler complaining about unfinished constructs and being forced to order their code and the solutions code-files by logical order instead of by-type and by namespace also feels downright archaic. These are both "wrong", really, and simply solutions to needs that we have learned brought about by side effects of OOP code bases. Idiomatic OOP assumptions. The logical dependencies make new code bases much easier to read, and organizing code around function instead of form is communicates intent much more readily while also minimizing dependencies, simplifying testing and the creation of "local DSLs". But wow, oh wow, oh wow, does it *feel* like a big step back for someone used to Resharper and a folder structure that looks like their namespace hierarchy. Flip side: it's crazy how much extra work people have to do for want of an interactive interpreter baked into the core product...
You may went to look into having each context using it's own schema to prevent any conflicts, facilitate security and just plain make it easier to tell what table is used for by the console or the ASP.NET app.
Agree! Restrictiveness maybe good sometimes. However, I would still recommend a beginner to start with C#. Most F# books/resources assume you know some C# or some .NET in general, there is few to none that assumes no previous C# experience. But if you see the C# education landscape, you find there's tons of books that don't assume any previous experience with .NET so it would be easier for a new people to start from there. That's how I did it anyways!
I have to agree on certain points. At, this stage building iOS apps in VS 2015 is a pipe dream. Upgraded my Mac Build machine today to Sierra and the latest Xamarin today hoping it would improve the very slow, very unstable, and crash prone VS 2015. My demo at Dev Days was a complete disaster yesterday. Very embarrassing. The remote simulator (granted its a Preview Product) doesn't work with the latest upgrades to my Mac. 
Share your contexts in a common library. Or have the console app hit the ASP.Net app via a Web API.
ahh yeah of course. If I add a reference to the console app in the asp.net app then I can access the console apps namespaces and get to the context that way. Not sure how that will work, going to have to have a play with it.
I think that's great for iOS, but not Android - or both.
In hind sight I should have just removed the iOS project from the solution and carried on. At home and at work, the demo was flawless. The slides that were given to me to present also claimed that VS to work well with IOS. So I had to prove it. The day before I tested the demo at the venue it was fine. It seems that when attendants pitched with all their personal hotspots, it seems the wifi connections went up and down which caused the Mac Agent to totally lose it. Visual Studio gets an instant wobbly and then just crashes. I was told afterwards that the Mac Agent uses sockets so its very sensitive to slightest jitter in the connection. Regarding the coding bit, I didn't stay for the entire day. But those that did say were struggling to clone the repo +/- 150MB because of the slow wifi at the venue. 
I've been using Xamarin for around a year now. Everything breaks. All the time. It's a constant battle and my productivity has decreased significantly. I regret going with it. 
I've been using Xamarin for about 2 years now with a MacBook pro and Windows VM setup in an enterprise environment. I think it's the worst it's ever been right now. There old build host system was a pain and a licensing nightmare but I don't recall the constant crashing and failure to build issues happening constantly. With the latest update to Xamarin and Xcode I'm constantly getting failure to build due to it not being able to find an Images.xcassets file. The build agent seems to be looking in the entirely wrong directory. This requires me to clean my build, close Visual Studio, and delete the mtbs cache on my Mac side. Then if that works and I can get it to build half the time the simulator won't launch my app. My visual studio will tell me it's launching the application and then just stop and nothing happens. Then if it does launch sometimes the app will just stop leaving me wondering if my app just actually crashed or if Xamarin just shat out. Spoiler alert,90% of the time it's Xamarin. My productivity is at an all time low. A significant portion of my day is spent wrestling with my tools instead of solving actual issues. I'm switching jobs this week and won't be working in Xamarin anymore and honestly I'm relieved. I've been saying it since I started working in it but it's never been more true: Xamarin is not ready for the prime time.
Hey! I wanted to point out, that an update to the iOS Simulator for Windows has been released, which includes Sierra support. More info [here](https://developer.xamarin.com/guides/cross-platform/windows/ios-simulator/) Disclosure: I work on the Xamarin team at Microsoft
Xamarin dev here whos contemplating about switching jobs, what kind of job did you switch to. I still love working on mobile but find it difficult to learn swift and java at the same time and just thinking about learning javascript and work using react native or switch to web development. Any thoughts?
Perhaps you could get them to address the issue that has left the visual designer for xaml broken, or the excruciating android emulator speed, or even just the build times, when simply trying to run the project without having made any changes since the last build..... You are brave sticking your head over the parapet in the thread! :-)
&gt; The iOS tooling for Visual Studio is unusable. I've not met any Xamarin who use it day to day. So you don`t enjoy VS locking up for 5 minutes as it tries to connect to an non-existent build host when you start the environment ? Seriously.. Ive raised the issue with MS a few times, each time was 'remove the iOS project'. Quality.
I've not used react native but it certainly looks interesting and there have been some great apps made with it. Its probably worth the time to learn. Web dev is never a bad area to be in either, very javascript heavy like react native would be. Javascript is becoming a must know language. Personally I'm in a position writing go and javascript now.
https://www.reddit.com/r/dotnet/comments/55cp6a/slug/d89nogv Theres my comment explaining a few of the hard hitting productivity issues I and other developers in my shop have been encountering. It is written somewhat harshly but is a result of many months of frustration. The biggest complaint is I have to constantly wipe my /Users/name/Library/Caches/Xamarin/mtbs folder due to a xcassets file not being found that we don't even use. That process also involves a clean and rebuild and VM restart. Other annoyances include the sim not always starting, requiring closing the entire sim, build times increasing significantly when building to device to the point where it seems our build has hung. For a while we were ready to abandon async/await in our app due to crashes involving Async method builder core. These crashes only happen when built in release mode. When debugging it never happened. We tracked this down to only happening with async lambdas so we avoid those like the plague. We're terrified to update when Xamarin has new releases. For a long while we couldn't debug libraries other than our app. Breakpoints just wouldn't work. That finally seems to be fixed. As harsh as I've been its important to know that what Xamarin accomplishes is impressive. But productivity with it is bad due to struggling with the tooling crashing or failing to work and then hard to find and reproduce bugs popping up. Edit: Oh! The storyboard editor in Visual Studio is worthless. Have never been able to get that thing to work. Xamarin studio's isn't much better, I typically wind up editing them in Xcode. 
Obviously there are people experiencing issues. I didn't say otherwise.
Hi Miguel. Firstly let me say, I'm a huge fan of your work back to early GNOME days and the GIANT contribution you've made to so many areas of software development. Now, some truths... I've been a Xamarin developer since the days of monotouch. The thrill of being able to use C# instead of objective C was what first drew me in. Since then it's been a bumpy ride. I've been to 2 Evolve conventions in Atlanta and Florida and I'd consider myself quite an active member of the Xamarin community. This is why it hurts me to say this: Xamarin in Visual Studio is not a good experience and never has been. It's gone through periods of being more stable and less stable over the years, but it's never really been anywhere near reliable. Really, I think you guys have lost touch a bit with the image that Xamarin has even among its devotees. Developers used to consider it expensive and buggy. Now, thanks to Microsoft it's just buggy. You say that you have extended beta periods. There are showstopping bugs on your bugzilla site that have been there for a year or more and still haven't been fixed. How does having an extended beta help that scenario? It seems like you guys don't have a great process for resolution. Build times in Visual Studio are at times simply mindblowing. It will sit there for 30 minutes and eventually complete a build after changing one line of code, meanwhile the IDE gives you absolutely no clue what is going on. The next day it'll build in 60 seconds. Totally random. The XAML Previewing tool which was shown at Evolve back in April doesn't seem to have received a single update of substance. It's now more than 6 months since it was announced. It's still no further forward and is broken and unusable. Just IDE clutter. Wouldn't it have been better to just wait a bit instead of announcing it with a demo which made it look like a near future release? USB remoting is in the same situation, Datapages are the same. In fact what did you announce at Evolve that has shown itself to be much more than vaporware? The only thing that really stands out is Xamarin Studio 6.0, which has improved so much in the last 2 years that I can only suggest finding out what your Xamarin Studio developers are doing and try to copy it. The Test Cloud Recorder tool works pretty well on Mac (great!) but for Visual Studio developers who I would guess form the larger part of your paying customers, most reports I've seen are from people who can't even get it to run, including me. The storyboard editor in Visual Studio, especially for iOS is slow, clumsy and unusable for much more than dragging a button and a label onto a viewcontroller. I've never seen a demo in any conference video where your developers do much more than this. I'm 99% sure they're scared to use it for serious work as well. In the last couple of years, you guys have shipped multiple updates in Visual Studio which have broken debugging in one form or another. There have been releases where the chances of Visual Studio actually hitting a breakpoint seem to depend on what mood it's in that day. My company had an app with a WatchOS1 app extension. We had to drop it in order to release an update to our main app because you never released a stable version of WatchOS2. Now I know this was partly due to some very significant changes required by Apple as part of their app thinning process, but couldn't you guys have put some more developers onto it? It's a commercial product. You make a big deal of being up to date, but stuff like that really lets down the image of Xamarin and causes very severe inconvenience for customers who bought into Xamarin being able to do watchOS and then basically got screwed. What's worse, is that we've never been able to get good information from you about when stuff like this will be fixed. Surely you must have some kind of internal targets, even if they're just your best estimate. I could just keep listing stuff, but you get the picture. It's not all doom and gloom. Xamarin Studio as I mentioned earlier is a vastly improved developer experience. I've always put the difference in Visual Studio &amp; Xamarin Studio quality down to the fact that Xamarin Studio is yours so you have more control, whereas with Visual Studio you were basically providing plugins for someone else's IDE which I'd expect is a more difficult process. Now you guys are part of Microsoft I think we all hoped that this would no longer exist as a reason. I also think probably a lot of the issues that have existed in the past are down to the limited size of your development team when you were a startup company. I don't know how big you guys got in the end before you got bought. There were a lot of smiley people in Xamarin t-shirts at the convention this year, but most of them seemed to sales and marketing. I think most people are hoping and praying that whatever resources you need to address these issues will be provided now that you're part of Microsoft. Scott Guthrie are you listening!!!? :) Despite all this, I still admire what you've achieved. I can't even begin to imagine how hard it must be to keep pace with not just one but multiple OS's and it's an extraordinary thing to be able to use C# to build apps. Even though it can be like banging your head against a wall sometimes, with days of developer time lost due to bugs in your tools I still recommend Xamarin to people almost every week. To summarize: We've managed without USB remoting, iOS simulator on windows, XAML previewing, that Inspector thing which looks impressive, but I can't imagine ever using, datapages and a load of other stuff for years now. While those things are nice to have, what we really want is a stable development environment, especially in Visual Studio. I'm nobody in software compared to celebrated developer like yourself. But honestly, most of your customers and developer community would happily give up all those things if you just stopped releasing any new features for the next 6 months and put every developer in your company on bug fixing duty. And then astonished us with a release in the STABLE channel that actually lives up to its name. Peace out.
Thanks for the feedback. I have never seen a build that takes half an hour, and the next one takes sixty seconds, it looks like there is something broken that needs to be diagnosed. Email me, so I can get you in touch with someone in the team that can diagnose what is going on, perhaps some task that deadlocks and only gets resolved after a timeout? The challenge with watch was not doing app thinning, it is that Apple changed the substrate of the operating system so much and added so many requirements that we had to redo many large chunks of the Mono runtime to support it: a new mode of operation for garbage collection that touched every bit of code, rewrote every bit of hand-optimized assembly language into bitcode and had to rewrite every smart that exists in Unix for what is a very limited platform. To some, this was a must-have, and to some, it was purely another feature. Multiply this by hundreds of different competing needs from our users, and you can see that making a call for what is a bug and what is a must-have feature varies greatly by the particular task. Please reach out for the build issues, I am sure we can fix those.
I'm a huge C# fan, but cut your losses and move to Ionic 2. You'll be glad you did. All HTML and Typescript. Yes, Typescript is just gloss on top of the evils of Javascript, but the tooling is all there and Angular 2 is now ready for prime time. Now I know what you're thinking... "no, I've invested too much in Xamarin at this point." OK, you may be right, but at least follow the Hello World tutorial through to see what you're missing. I can guarantee an "oh wow" moment. 
Thanks @xamcb, the 0.10.0.6 version of the iOS Simulator For Windows (released 28th September) is better for the Serra Mac OSX. However the first time is launches, it still hangs at the progress animator. I shut it down, try again, and it doesn't show the progress animator. I choose another iPhone simulator (like the 6s Plus) and then it works. Small steps, seems like once the Simulator is "warmed up" it works flawelessly.
To say that Xamarin is *unusable* is a bit harsh. Tools marked as "Preview" I expect to be a bit unstable. With the very frequent updates (we're talking almost every month) for Xamarin and for the iOS Simulator its even more frequent,its also unfair to say Microsoft/Xamarin is not fixing issues. The list of issues that get fixed with each release is very encouraging and it is getting more and more stable. Sure there are some bugs with VS talking to the Mac Agent and the Simulator Preview, but honestly the fact that we can even do iOS development on Windows in Visual Studio is in itself making Steve Jobs turn in his grave. Its an amazing achievement. Despite the slow builds in Visual Studio, I still think Xamarin is a winning product and years ahead of anything out there. 
Why would you use VS in a windows VM when you could use Xamarin Studio?
Thank you for your response! With a tenant you refer in my case to an Organization, right? I guess I also need to learn how ASP.NET Identity works more indeepth, sine I sense this will be important in solving this problem.
What is and isn't overkill is more dependent on the requirements than whether the app is public or internal. My requirements are to **mitigate** copying privileged documents (alongside other forms of barriers, such as authentication and authorization) and if I manage to only admit a single viewer at a time per login, I made the application more secure. Right now I'll be working on this as my job, and by my contract, the code will be the property of the company. However, the first pass is never pretty, so after it's done, and confirmed that it performs as expected, I might find some time to make it somewhat plugable. 
Yes, in your case the tenant = organisation. But... there is nothing implemented in asp.net identity to manage/model this behaviour. Basically, for each user/collegue profile you should maintain a list of organisations for which he/she has access. 
I think most of complaints surround the Visual Studio -&gt; iOS integration, I certainly found it to go wrong more often than not - I just got tired of fighting the VS tools rather than doing development. I think the lack of feedback from the vs iOS build system about what is occurring makes reporting bugs difficult, when it goes wrong it tends to just sit there spinning its little build animation. Also there have been some issues with using XCode 8 as a storyboard designer, which is somewhat out of Xamarins hands but the poor quality of Xamarins storyboard designer has not helped them (i still don't know to do constraints properly on Xamarins designer). I actually think Xamarin/XF apps works the best on iOS - it certainly is the most performant platform in my experience. There has been a few complaints about the performance of XF on Android (I wish it was better also) and the reliability of XF on UWP. Generally if your only interested in xamarin forms on iOS and Android and use a Mac + Xamarin Studio for development you will have few issues. 
The System I work on does not use the built in ASP.NET Identity (everything is hand rolled). But we have two-level multi tenant, Account-&gt;Organisation-&gt;User. An organisation should certainly not be a "User" object. Model organisations in a separate table and add it as an extra property on the User (possibly a role if you need it to fit into Identity's model, but that's not very clean). Then add an "OrganisationAdmin" role and grant the first user in the organisation that role. I would advise against multiple organisations for a single user due to the complexity. If it's only needed for some edge cases make the person multiple users in the different orgs.
Thanks for your input! Now I see that I worded the description in my posting wrong. I am not using IdentityUser for the Organization, the Organization just holds the navigation key to the ApplicationUser and some extra properties (displayname, address, etc.).
I am must excuse my bad wording, Organization is not an IdentityUser, in fact I have it exactly modeled as you suggested. And no, I don't plan to use multiple organisations on a single user, one user - one organization - many members. And thanks your reply, makes me confident that I got at least the modeling right. I am now learning Owin and Identity stuff on MVA and Pluralsight, I think this is where I can find a solution to make the Members be able to sign-in.
E.g. when companies merge. Fusion, take over, ...
&gt; navigation key * a: navigation property * b: association / foreign key So which is it? A: Well, this one doesn tell us much. B: And this one doesn't change much as you still want organizations to be represented by ApplicationUsers in the database for some reason. Make organization a separate entity with all the properties you need, Add an "organization" field/column/property/whatever in your ApplicationUser class to associate users with organizations many:1.
&gt; data: {'**serialNumber**' &gt; deviceData(string **serial**) May be this, not sure.
&gt; data: {'serialNumber' deviceData(string serial) I agree, should match.
&gt; Make organization a separate entity with all the properties you need, Add an "organization" field/column/property/whatever in your ApplicationUser class to associate users with organizations many:1. This is actually what I did, but with the difference that the ApplicationUser entity and Organization entity are associated 1:1, but now I see how many:1 makes more sense. Afterall, there could more than 1 User who could manage an organization.
Create models, fill them with async await linq calls, pass them into a view. Learn that flow. Don't be afraid to create models and viewmodels for your needs. Partial views with view models to display.
You can simplify null checking, and only call a property/function when parent object exists: order?.Submit(); only calls Submit when order object exists.The questionmark makes this possible.
Models for the db, viewmodels for whatever you want. :)
I like using the nameof operator for ToStrings. Great if you need to refactor later on. For example: return $"{nameof(foo)}: {foo}"; 
This is one of the best new features, now if I could just figure out why this causes a syntax error on my build server I would be pleased.
Had the same issue recently. VS2015 allowed me to use this operator even though I had set the project to framework ver. 3.5. The build worked fine on my machine. Failed on the build server.
The difference isn't really that great unless you are also using List&lt;T&gt; or an array.
Are they really *much* faster? I hadn't thought it was significant enough for web applications to warrant avoiding LINQ...
expression body, null propagation
I have used Xamarin on and off since the beginning. It has always been this thing that has so much promise but has just never been able to realize that promise. Every Xamarin update would lead to days of lost time trying to figure out why everything stopped working. I've given up on it and had the hope reignited probably 10 times over the years. I really thought the Microsoft purchase was the answer. Your experience seems to say that it might not be. I am struggling to find the right way to say it, because I have so much respect and admiration for Miguel, but it is honestly like Xamarin is a bunch of undisciplined kids. Constantly distracted by shiny new stuff they could be working on instead of doing the hard work that is required to take something from the 80% solution to 100%. Starting new stuff is fun and getting to the 80% case is straightforward. Finishing is a lot of hard grunt work, and Xamarin has never seemed to have the discipline to focus on the work required to turn the "good start" into a full-fledged, hardened, solid, finished product, covering all the edge cases, crossing all the t's and dotting all the i's. This seems to apply to Xamarin as a whole, but is evidenced by all of Miguel's dalliances into MonoTouch.Dialog, CocosSharp, UrhoSharp, and on and on. There is a lot of energy and excitement at the start, and then once it hits that 80% solution point and people start using the stuff for real it gets too boring and becomes too much grunt work to maintain so they move onto something else. I see similar behaviors in the Xamarin Studio Workbooks stuff lately, and I have no doubt that will end up in the same boat. Really hoped Microsoft would fix these problems, but it sounds like maybe not. React Native is nice and the community is growing large. I also like NativeScript a lot. It's not quite as mature as Xamarin yet, as it is a much younger project, but it seems to be on the right track, and if C# is your main draw to Xamarin then try it with TypeScript, it's almost as nice in a lot of ways.
* LINQ / yield * Async/Await (and in general .. Tasks) * Null Propagation * Null Coalescing * String interpolation * nameof() * Properties got much better syntax Not language features but worth looking at: * TPL / PLINQ * CallerMember * Entity Framework 6.2 / Core 1.0 * Dapper * Logger Library * DI-Container * WeakReference * Visual Studio 2015 * Scaffolding
This isn't strictly true. Defining your expression in set notation allows the compiler to make optimizations it couldn't otherwise make. 
For most purposes the difference is trivial: https://www.infoq.com/articles/For-Each-Performance
Yes, they are *much* faster but most people aren't likely to care. I once was able to shortened a migration from several days down to several hours due to optimization in code. I get very intimate and romantic with the optimization side of C# .NET. I used condoms, don't worry. Most people aren't likely to care. In fact I'll be surprised if the majority of the webdev community *ever* cares -- **but** it's important to know that it certainly is *much* faster when you're itterating through a fuck ton of times (think in terms of "illions" -- millions, billions, trillions). Simply being aware of it is enough -- you'll know when you **need** to optimize enough to care because it'll be the difference in being reasonable and being unreasonable -- not "fast and slightly faster". When a ~~chance~~ change like this can make your migration time go from ~90 hours down to ~ 84 hours -- that's pretty significant. Start adding in all kinds of things *like this* and you start dropping that number fast. I think I got ours from ~4 days down to about 6 hours?
I was? I was migrating from SQL Server 2000 to SQL Server 2008 and the migration was fairly complex -- absolutely not something you would do in TSQL. edit: Unless You know how to do graphics manipulation and analysis inside of TSQL? (among other things)
Might be a stupid question, but are you sure it's setup for C# 6? ^^Have^you^tried^turning^it^on^and^off^again?
Spend some time learning about Tuples, they can make your life easier!
See that's the thing though, who actually is dealing with millions of records? Probably a minority for a lot of web applications. 
All the Tasks stuff is amazing. Parallel.ForEach for example.
What's your strategy for dealing with keeping the contracts expected by the client and server in sync?
This sounds like how Plex works. The difference there is that Plex is a media server, so their UI is really just communicating with extensions on a server whose primary purpose is something else. It's very feasible if that's the direction you want to go, even if a bit unusual. A standard app might perform a little better or have a few less hiccups with accessing the system, but nothing that can't be overcome.
While I disagree with ordering unit tests on principle... You can use `CallerLineNumberAttribute` to allow tests to be ordered in the order that they're defined in the file, rather than forcing to user to specify a custom order.
I'm not actually talking about deployment; I'm talking about *development*. OP said they didn't want to have the client and server in the same solution. As they work on the client, how do they want to ensure that the clients' types line up with the server's? It's going to be a source of friction to not even have them in the same *solution*. Unless there's an assembly that defines the contracts that's shared between the client and server solutions, I guess.
I agree. I'm not the OP, but everything I've read on writing unit tests, they should all be self sustained and not rely on other tests. I have had to break that, slightly. We have a set of Selenium tests, but everything in the site is behind a login. So I have two numbered tests, using the [Order] attribute though ( https://github.com/nunit/docs/wiki/Order-Attribute ). The first opens and confirms the login page works. The second does the login. Every other test starts by taking the user to the post login page, then navigating where they need to go. The desire was so that the page load for the login and the login itself were two separate tests, and I didn't need to log out and back in after each test, just once for the suite.
Anytime! Good luck in your project!
&gt; Unless You know how to do graphics manipulation and analysis inside of TSQL? I do, but I won't tell you how because it's not something that I recommend.
&gt; Some values could be null. Null could literally mean "null" or it could mean 0 or it could mean blank (edit: oh, I forgot -- it could also be the word 'null' ; yeah, fuck me... what the hell that person was thinking was beyond me). This means you can't just typecast it easily (this means catching an exception if you try/catch is EXPENSIVE; This also means you choose which is more statistically common in the database and plan for that). Typecasting is not an inexpensive proposition -- even in a database. The data is not stored consistently either -- so what might be 1/0's might be stored as a char(1). While I agree that graphics processing shouldn't be done in the database, all this other stuff is pretty basic T-SQL data cleanup work. You should count yourself lucky to be able to clean it up. I've been in situations where I've dealt with crap like that on every query.
Also pointless ranting; our build times for Android with Xamarin Studio increased by 3x upon upgrading to cycle 8. iOS times by 2x. We have a support ticket, and sent in detailed build logs, but haven't heard anything for ~2 weeks. Our build times were always horrifying (we have a massive app), but this has made developing ridiculously unproductive. These in addition to multiple Xamarin Studio crashes per day, frequent "beach ball" lockups, etc.
It's basic, for sure. But holy fuck you'd have to be masochistic to do it in TSQL and I can't imagine you'd make *significant* gains after the amount of work+troubleshooting you'd be working with. &gt; You should count yourself lucky to be able to clean it up. I've been in situations where I've dealt with crap like that on every query. It was part of the requirement. The database was a *clusterfuck* in so many other ways. If I didn't know any better I'd say they mowed through several developers *and* had a moving target the whole time. There were also parts where there was history -- but it didn't *always* keep history. That part was less fun. Trying to figure out the logic for what was the most recent one. The only *sane* way to do it was in a real program where you can break it apart into chunks. We ended up having to buy a decompiler to figure out some of the logic because there were times how the data was stored did not make sense. So imagine writing it all in TSQL. Thinking you have it even roughly figured out. Then having to go back and redo large chunks. Having an IDE and doing it in .NET savings you a *ton* of developer time and also allows for people to hop on quickly. Again, this is not something a normal person should have wanted to do in TSQL. Now if we *knew* the logic and *knew* everything except image manip, then I could see a point to do that. But if you're arguing you'd do this in TSQL... I'm sorry but I'm going to say you're a masochist. That would be utterly disgusting by the time you're through and had it all figured out -- then if something happens to you, it's a *nightmare* for the next person to figure out -- thus, I don't see why you'd even consider "using a database" (I'm assuming you really meant to say using TSQL).
I am not that skilled with TSQL to even *begin* to entertain the thought of image manipulation like that in TSQL. There is not enough cocaine or LSD in the world to have me do that. I do not miss that server at all. Oh, you need to remote in? you're going to need to reboot after you're done... it locks up when you log out (I shit you not).
While obviously I can't see what you were seeing, everything you've said so far sounds still like pretty standard cleanup work. (Other than the graphics processing.) &gt; The only sane way to do it was in a real program where you can break it apart into chunks. That's why they invented SQL Server Integration Services. Also, using a series of staging tables can make it much easier to figure out what went wrong than trying to catch everything in a C# debugger. &gt; Having an IDE and doing it in .NET savings you a ton of developer time and also allows for people to hop on quickly. SQL Server has an IDE called Visual Studio. Perhaps you've heard of it? And something to keep in mind is that you don't have to leave the database in order to use .NET. I've got no qualms against writing C# based helper functions that get uploaded and called by T-SQL or SSIS. 
&gt; While obviously I can't see what you were seeing, everything you've said so far sounds still like pretty standard cleanup work. (Other than the graphics processing.) You'd like to think that! And we tried, initially, to do it in TSQL but god damn did it turn into a complicated cluster fuck and always a PITA hurdle *somewhere*. There were two other big non-tsql hurdles that just made us throw our hands up. I'm trying to remember them though. Fuck now I'm going to have to pull up that code and try to remember now. &gt; That's why they invented SQL Server Integration Services. Also, using a series of staging tables can make it much easier to figure out what went wrong than trying to catch everything in a C# debugger. I'm trying to remember if we didn't have access to this for some reason or if we chose not to do it for another reason. I remember looking at DTS and having a meeting specifically about this but dammit I don't remember why we chose not to use SSIS. I seem to recall thinking "There *has* to be a smarter way to do this, there just has to be" and getting road blocked every time for one reason or another. &gt; SQL Server has an IDE called Visual Studio. Perhaps you've heard of it? I do not recall that being in VS 2005/2008. &gt; I've got no qualms against writing C# based helper functions that get uploaded and called by T-SQL or SSIS. This is not something I have heard of. I have not had to do TSQL in any large amount since those days though. The only stuff I've done is trivial CRUD and basic processing. So I'm fortunate in never having the *need* to use a DB backed for much.
Is this a custom asp.net program you are running that has a memory leak? Or some loop somewhere unchecked? Or is your PC running like crap and it's blaming IIS? Are you hosting? What Windows are you on? If you're not actively developing and IIS is taking CPU then your hosting something or iisexpress is running via a service. You can try uninstall Visual Studios and see if the processes are still running, then reinstall. 
Automapper is insanely useful. At the basic level it copies data from one POCO object to another, but its advanced features are incredibly powerful. Once you learn the basics look at features like custom type convertors, custom value resolvers, expression translation, etc.
My code is littered with var model = new object(); model.id = viewModel.ID model.name = viewmodel.Namel ... ... everywhere. :(
&gt; This is not something I have heard of. Started in SQL Server 2005: https://en.wikipedia.org/wiki/SQL_CLR 
Hi there, I'm the team lead on the Visual Studio integration for Xamarin. The connection to the Mac is fully asynchronous and never locks the UI thread. Are you using our latest stable drop? You might be on an older release. Many improvements were made in our "cycle 8" release related to solution loading time and releasing the UI thread quickly and often. If this is still happening, I'd love to get a repro solution and a proper bug report. We haven't seen this happen for few months already (since the current build was in alpha).
make all the strings in your code like this: Strings.Localize($"key {nameof(param1)}: {param1}, {nameof(param2)}: {param2}"); then for localization, you can do this: public static class Strings { // localize gets called a lot... // to reduce allocation pressure only allocate a single // list per thread which will ultimately wind up with // about 16 elements in it [ThreadStatic] private static List&lt;string&gt; _keys; private static readonly char[] Splits = {'{', ' ', ';'}; /// &lt;summary&gt; /// Localizes a string /// &lt;/summary&gt; /// &lt;param name="formattable"&gt;an interpolated string of the form /// $"key {nameof(param1)}: {param1}, {nameof(param2)}: {param2}, ..."&lt;/param&gt; /// &lt;returns&gt;a localized string&lt;/returns&gt; [NotNull] public static string Localize([NotNull] FormattableString formattable) { #if DEVSTRINGS return formattable.ToString(); #endif var keys = _keys ?? (_keys = new List&lt;string&gt;()); keys.Add(formattable.Format.Split(Splits)[0]); if (string.IsNullOrEmpty(keys[0])) { keys.Clear(); throw new CodeReviewShouldHaveFailedException( $"format string is not in the desired format for this codebase: {formattable.Format}"); } for (int i = 0; i &lt; formattable.ArgumentCount; i += 2) { var argument = formattable.GetArgument(i) as string; if (argument == null || string.IsInterned(argument) == null) { // By default all compile time constant strings are interned // these values make for good keys into the localization code. keys.Clear(); throw new CodeReviewShouldHaveFailedException( $"format string is not in the desired format for this codebase: {formattable.Format}"); } keys.Add(argument); } var key = string.Join(";", keys); string result; if (TryLookup(key, out result)) { // full match found localized result // almost always this should succeed keys.Clear(); return string.Format(result, formattable.GetArguments()); } #if THROW_LOCALE_OOD keys.Clear(); throw MissingLocalization(key); #endif var partial = LookupPartial(keys[0]).Select(Extractinformation).OrderByDescending(k =&gt; k.length).FirstOrDefault(); // once in a while it is acceptable that the current localization may not have // defined a key for the current version but does have one for a previous version if (partial.length == 0) { keys.Clear(); return formattable.ToString(); } List&lt;object&gt; args = new List&lt;object&gt;(partial.length * 2); foreach (var partialParam in partial.args) { args.Add(partialParam); args.Add(formattable.GetArgument((keys.IndexOf(partialParam) - 1) * 2)); } keys.Clear(); return string.Format(partial.value, args.ToArray()); } private static PartialResult Extractinformation(KeyValuePair&lt;string, string&gt; kvp) { var args = kvp.Key.Split(Splits); for (int i = 1; i &lt; args.Length; i++) { if (_keys.IndexOf(args[i]) &lt; 1) return default(PartialResult); } return new PartialResult(kvp.Key, kvp.Value, args, args.Length); } private struct PartialResult { public string key; public int length; public string[] args; public string value; public PartialResult(string key, string value, string[] args, int length) { this.key = key; this.value = value; this.args = args; this.length = length; } } //write these: private static bool TryLookup(string key, out string result) { throw new NotImplementedException(); } private static IEnumerable&lt;KeyValuePair&lt;string, string&gt;&gt; LookupPartial(string p0) { throw new NotImplementedException(); } private static void MissingLocalization(string key) { throw new NotImplementedException(); } } Or you can go the whole Stack Overflow route and do a bit of assembly rewriting to call a generated method: http://nickcraver.com/blog/2016/05/03/stack-overflow-how-we-do-deployment-2016-edition/#localizationtranslations-moonspeak tag /u/SikhGamer
I would be surprised if they didn't have the same backend. Why would they make something new when they already have it?
Yes, but as written, this is not meant for ordering unit tests.
hey dotnetdiva, I work on the test recorder so hopefully I can help you out with your test recorder for vs woes. The first thing we should try is to go to Tools -&gt; Options -&gt; Xamarin Test Recorder and make sure that your Android Sdk path is properly in there. If it is, then let's make sure that you have all of the required Android Sdk components. At minimum, you'll need the following tools: - platform-tools/adb.exe - build-tools/&lt;version&gt;/aapt.exe - build-tools/&lt;version&gt;/zipalign.exe - tools/emulator.exe - tools/android.exe If both of those conditions are met and you're still unable to see the recording icon, drop me a message and I'll set up some time to work out the issue with you. We will be adding documentation to make that issue much more clear very soon. Sorry for the bad experience, hopefully this fixes it for you! EDIT: formatting
Nope, not running them since the end of august. I think I fixed it with a registry key.
Because Microsoft. Kill a product and create a similar new one that does less, be surprised that noone uses it.
Well now... I suppose I need to invest some time in learning that. Part of the reason I came to this thread was to see what I've been out of the loop for and 'lo and behold I learn something I wasn't even in the loop to begin with. Thanks for the info.
Summary: These are Windows 10 virtual machines that are preloaded with software needed for a full development environment. The Enterprise images are a 30 day trial period, the pro version is the full thing but you'll have to enter your own key. Question: I wonder if the VM will recognize my motherboard and hardware enough to activate automatically considering I already have a pro license. 
Nuget?
I did find out the following on StackOverflow: &gt;The natural language processing capabilities of Cortana are derived from Tellme Networks (bought by Microsoft in 2007) and are coupled with a Semantic search database called Satori. &gt;From what I can find online, they use different systems, with Speech Runtime 11 being developed by Microsoft, and Cortana's voice recognition being bought in. 
Did they change them? How do they make life easier? One of the most convoluted things I ever worked on was something where some ass hole (me) didn't want to define actual classes for something and ended up with a nested Tuple structure. 
Yeah, I didn't expect this to get so much attention since I am fairly new on this subreddit. I work with a ton of data (aprox 25-200k it depends) every day, since I am doing structural analysis. And just today 2 functionalities (more like a set of them) and comparing both LINQ and for loop, I actually got somewhere around 35-40% faster execution time. 
Think I'll just stick with [the morning brew](http://blog.cwa.me.uk)
I'm in a shop using almost entirely the Microsoft stack, desktop exclusive. I'm looking to promote a mobile app but in order to make a business case, I'm going to have to either pitch xamarin, or something similar, I imagine. But I'm not going to pitch a technology that isn't stable. Considering we're mostly wpf developers, I've been thinking we'd find xamarin a reasonably familiar tech to pick up, but I suppose I'm a little unsure what to recommend. Are there other c# based stacks for cross platform mobile enterprise development? Or should I start dipping my toes in the xamarin waters even though I see a lot of concern about stability and unfixed bugs? These complaints remind me of the feelings (primarily regret) that we've had due to our choice to use telerik tools. Long-standing issues that go unresolved, poorly designed tools, UI glitches, etc all just have us wishing we'd spent our money on something else. Anyone want to point me in a couple directions that might be beneficial? We're willing to choose a js-based stack if they are better supported and stable.
Thanks for the link. Looks like a great resource.
You could try killing the "World Wide Web Publishing Service" in your services control panel applet.
So native is the way to go. Thanks!
Of course, who needs IOC containers, logging, or a garbage collector anyway? 
Why do you want to deploy the server with the client? Wouldn't it be advantageous to set up an internal server and have all of your clients point to that? Is this a requirement of your application?
one of the comments mentions: For those having issues with this part of the tutorial, if you're using VS 2015, the Movies connection string should look like this:&lt;add name="MovieDBContext" connectionString="Data Source=(LocalDB)\MSSQLLocalDB;AttachDbFilename=|DataDirectory|\Movies.mdf;Integrated Security=True" providerName="System.Data.SqlClient"/&gt;The key being changing "Data Source=(LocalDB)\v11.0;" to "Data Source=(LocalDB)\MSSQLLocalDB;"
I didn't even see the comments since they were collapsed. Awesome, thanks a ton!
Try the JavaScript ecosystem.
But the real question is when will management studio use a modern VS Shell?
Don't know why you are getting downvoted; I find manual dependency injection better. You still wire everything up at the composition root, so your code is still exactly the same. The main problem with containers seems to be moronic co-workers that badly abuse them and use them for things they were never mean to be used for.
I have tried that, it generates an api client for c#, but it does not have an api server generation support for c#
Yeah, I only found out about PagedList yesterday. I feel like there are a ton of little packages out there that would make my life easier but I just don't know about them and often nuget package description are obscure and it's not obvious what benefit they provide until you actually see someone do a writeup with code.
In VS 2015, if you right click a web project and go to "Add", there is a "REST API Client..." option that asks for a Swagger URL. I haven't used it yet, but it seems to be what you're asking for.
Oh, ok, I hadn't understood that properly. Then my answer doesn't make sense, sorry :).
I'm a former Visual Studio who's switched to Xamarin Studio. It's really pretty easy to work with after a while.
We actually had significant issues around projection when moving to 5.0. There are some breaking changes in there and bugs especially related to projection. The 5.2 myget feed version seems to resolve them for us. 
Really? I would have thought Javascript to be closer to the open source ecosystem with all the different frameworks and things.
Ah I see. Yeah that could be a fair point. I'm not totally sold on Javascript at all levels of the application; I get the reasoning behind it, but it kinda reminds me of when Rails first came out and you had Rails itself as the back-end, ERB (I think it was ERB, before Haml came out) for the HTML-esque markup and RJS for doing javascript, so everything could be handled in Ruby, and that didn't really work out well in practice.
Fantastic, thanks for the early patch, I'll give it a go soon, just busy finishing off a sprint, so I don't want to take chances with my dev machine.
Go Xamarin! :-) Having used Xamarin for 2 years, I still say there's nothing that comes close to what you can achieve with it. A js-based stack is okay if you don't mind having an app that feels like a website and have great JS , CSS, HTML skills.
Define "modern", then wait until "modern+1" or "modern+2" is released.
I'm curious, do you have suggestions on how to organize an F# Project? Finding opinions on how to organize a project is always difficult since it's more "put this here", and it often lacks a "why we put this here", even in C#. F# really is a huge paradigm shift. When i started learning it, I kept thinking to myself that I should treat it as if I've never coded before (so I don't bring any assumptions), and that has been super helpful in learning it; and apparently organizing an F# project isn't any different. 
Been looking for something like this for a while, but I'll check out The Morning Brew, too. Thanks.
JS this, JS that...... Those who have seen JavaScript's future in ES6 and ES7 knows that it's being heavily influenced by statically typed languages like C# and Java. You know what? Good. It's about time for JS to grow up, leave the wild wild west and join structured society. 
Most cleaning products will make you sick if you drink them but have no harmful effects if you just clean with them. If you prefer to drink them instead of cleaning with them, then I agree you should avoid them.
Guids are fine, if and only if they are never used as the clustered index on the table. why? Clustered index ensured the physical ordering of rows on the disk. every change insert or delete of the primary key will re-shuffle the rows. Not horrible if you have say 50k records or less, but the cost rapidly becomes exponential in scale over that. Guids should not be used for primary keys even if it isn't the clusteredd index of the table. Why? Guids are 4 times the size of integers.. but why do we care, HD's are cheap?!?! Primary keys are stored with every index node, and with every foreign key node, so going from 4bits to 16bits meaning your storage usage will suddenly be out of hand quickly. 
Guids
Does Syncfusion support asp.net core?
Yes [indeed](https://www.syncfusion.com/products/aspnetcore)
Oracle could learn something about this. In University we had a course that you already passed if you managed to install Oracle successfully.
Would prefer if this was done in VSCode rather than Visual Studio but the example is awesome. Thanks for the sharing link
Nice timing, was just researching angular2+dotnetcore for a new project at work, and was struggling with all the webpack + related configs to get things up and running. This has exactly what was needed in a premade project template, good job Microsoft :)
Looks like they have a yeoman generator. 
Excellent. Thank you!
&gt; Further, the "hackable" argument in favor of using integers is entirely wrong-headed; why should the user have to remember anything? It's nice for the users to be able to do this, but not necessary or vital by a long shot. Last night I had to read a GUID over the phone because my order was incorrect. So yea, "hackable" ids are really fucking important.
My point is that they don't "become trivial" over time. Either they stay trivial or they get worse over time, they never get better.
An order should have a order number that is distinct from the primary key.
&gt; Most of the junk I've seen has wide tables and bunches of indexes and tons of joins and various other things that keep the db from achieving peak performance. The thing is, GUIDs make those indexes more expensive. Not only is each index is larger, they take longer to read and update. I'll admit that GUIDs are a small tax, but why pay for an unnecessary tax?
https://marketplace.visualstudio.com/search?term=jenkins&amp;target=VSTS&amp;sortBy=Relevance Yes
"or its .net equiv" You want something prefixed with a time. So new ids are always mostly in order.
The performance impacts really aren't that big of a deal. But yeah. Same as any partitioned storage where you have to think about partition keys.
Angular 1. I'm actually working with the Angular SDK and have everything working as expected. The only thing I don't have figured out is why logout in particular is giving me a cors issue. 
I've been on a roll! Had to allow all headers and methods. Thanks!
nice example, but to be honest, the more I learn about angular the less I like it.
All are welcome changes. 
You're arguing about the technical merits, but and Order Number is also part of the connection between business and customer as well as something used otherwise by the business in general. I can't think of any good business reason to expose the thing that uniquely identifies a row in the database internally with the external uses of an order number.
Sure. I'm not saying that order numbers aren't unique. I'm also not saying an integer for a order number is wrong. I'm just saying there's usually more context to their use throughout a business. I've not had the luxury of working on many isolated green field software systems. There's **always** context and usually plenty of legacy cruft to deal with. I *do* agree that using a GUID as an order number is a bad idea. I am a fan of surrogate keys because I've been bitten so many times with software and their associated databases being merged or incorporated with other systems. If there's even a slight chance that there will be multiple instances or any kind of replication then GUIDs are what I'd choose. Once bitten twice shy.
You don't just leave VS open all the time? :p
I don't even know why this is a debate. Use an integer as a PK + a clustered index and be done with it. If you really need to, use longs if you're worried about your table getting bigger than 2 billion rows. Yes, you can reasonably use a GUID as your primary key *if* you use it with a *nonclustered index* but it's still slower than JOINing on an integer column, so there's really no point. Like the article mentions, GUIDs are ideal for horizontal scaling situations where you actually need the properties that make GUIDs useful (namely global uniqueness), but if your application needs horizontal scaling (or very good resiliency) I'd recommend a distributed NoSQL database like ElasticSearch or Cassandra over a RDBMS anyway unless you're worried about the clustering algorithm being a single point of failure. I say this because relational databases are not trivial to horizontally scale and if you manage to do it, you lose all JOIN capabilities (need to do them in the application layer) and basically end up with a RDBMS used as a document database anyway. See: how pinterest, instagram, etc scaled using MySQL
PSA for newbies: Do not install pre-release versions of VS if you are not willing to nuke your disk and re-install windows. They are released for testing purposes, not to boost your productivity. 
Supposedly starting with this version installers are self contained, so that should fix most of the issues.
&gt; RethinkDB .NET You might want to [rethink your use of RethinkDB](https://rethinkdb.com/blog/rethinkdb-shutdown/) &gt; LLBLGen 5 I've never heard of this one before. I just googled it and it sounds cool. How do you like it?
&gt; String interpolation &gt; Null Coalescing And also Null-conditional I think these are features that almost anyone using C# 6 can immediately get value from. They just make life easier. &gt; Dapper Also pretty awesome 
That was it. For future reference, is there an all-encompassing guide these things? Sometimes I'm not sure what to Google and tutorials don't always go through details like this.
Same, I'm a backend c#/db guy pretending to be full stack with the help of kendo. 
I've worked on a lot of e-commerce sites over the years and there always seemed to be business rules around the "OrderNumber". I think I've seen order numbers that started from 1000 and incremented by one from there but most times there have been business rules year-month-date- in front of the 1000+ or "web-" + some other stuff because it comes from the web site. 
I've been a full stack guy for 18 years. Still pretending. About everything. So is everyone else. 
I think there's two choices for front end as it stands right now; angular 2 or react. I don't think there's a wrong answer between those two, but they are different answers. Angular 2 has a lot of things that are specific to and will tie you to the angular stack/methodology/approach. React is a bit more "open" than angular and also a bit of a different approach. At the end of the day, you should expect that choice to change somewhere down the line. It's not like .net where you can run the same code base for years at a time. Expect these frameworks to have some volatility. Make sure management understands you may need to spend significant effort updating or moving to a new framework in a few years. Also understand that you need to make your code as reusable as possible, so beware of coupling. EDIT: also, embrace JavaScript. It isn't a perfect language, but it does have some cool things that it can do. Realize that you can't lean on the compiler to catch your errors, so implement something like jslint and test. TypeScript or ES6 are also good ideas.
As others have stated, start with Typescript. With a c# background, learning Typescript is an easier transition to front end development. Another great front end framework is Aurelia. 
What you should be aware of when coming to the land of front-end development from a C# developer perspective is how big of a mess front-end can be. We dotnet developers usually prefer M$ stack and some well-known 3rd party libraries/frameworks, but every problem in front-end has at least 10,000 solutions out there. And for a project with X problems, you will have 10,000^X choices to choose from. Sometimes I feel like the community tend to enjoy looking out for new ways of solving things rather than improving the existing ones. However, with that overwhelming of choices, you will definitely learn a lot. And who know, one day you can yield your own libraries.
I am pretending to be fullstack because it doesn't exist
Read "JavaScript: The Good Parts". That's the first thing I did and it helps knowing just how bad of a language JavaScript is, and of course the things that you can do to make it better. All the TypeScript suggestions are great too, or maybe Flow which is becoming more popular.
It's fine here
Ugh, I am trying to learn Angular 2 right now and in the same boat as you. I feel overwhelmed to say the least. Hopefully it gets easier.
I wouldn't use the standalone Microsoft release manager from 2013. We had a fairly large implementation of it and had issues with scale. Porting our release workflows to TFS 2015 release fixed most of the perf issues. Release manager 2013 is also written in wpf as a thick client with its own database and no support for multi tenancy. Where TFS 2015 release is a html 5 front end, tightly intergrated into tfs and is partitioned by team project. Upgrading to TFS 2015 is the best way to get the new release management capabilities and it will support Jenkins builds with out of the box tasks. To get it to work with release manager 2013 you have to make a REST call into a poorly documented API.
&gt;I think there's two choices for front end as it stands right now; angular 2 or react. There are aurelia, vue, ractivejs, ember, etc and many are just as good if not better than angular 2 or react.
If Angular 2 overwhelms you, then learn something else. There are plenty of alternatives. There is no point trying to learn a framework that is so complicated it makes you feel stupid.
tl;dl: He finds finalizers hard. 
I literally didn't mean only two, but that there were only two worth looking at for various reasons. Among those would be adoption, likelihood of surviving more than 12 months, documentation, and a sizable community. The libraries you mentioned are fine, but probably not as easy for someone with no front end experience to get started with. It's also not going to be good if his app has to be rewritten in the near future because one of those frameworks doesn't take off. In the case of Ember and libraries like knockout, they're already falling out of favor.
Say that to the people that adopted Angular 1
So it's a first party alternative to Plesk / etc?
Your core argument for Angular 2 and React is that they will last long and your app will not need to be rewritten. I am pointing out that that is not necessarily the case. Ember was the hype in 2015 and one could safely assumed that it was going to be the IT framework in 2016. My argument is simply, pick the framework you understand. Usually the smaller ones are easier to understand.
Thanks for the detailed reply much appreciated and glad to know there are a lot of folks in the 'this is overwhelming' boat. I have been following Deborah Kurata's Angular 2 course on pluralsight and so far so good. I can already see a couple of little apps I have built in the past could be redone using mvc to serve up the api and using angular to consume. Although after all of the ceremony required to build an app, I am not sure I have gained a whole lot in productivity. 
View.xmal &lt;Window x:Class="testappreddit.MainWindow" xmlns="http://schemas.microsoft.com/winfx/2006/xaml/presentation" xmlns:x="http://schemas.microsoft.com/winfx/2006/xaml" xmlns:d="http://schemas.microsoft.com/expression/blend/2008" xmlns:local="clr-namespace:testappreddit" xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006" Title="MainWindow" Width="525" Height="350" DataContext="{Binding RelativeSource={RelativeSource Self}}" mc:Ignorable="d"&gt; &lt;Grid&gt; &lt;StackPanel&gt; &lt;TextBox Height="35" Text="{Binding Model.x, Mode=TwoWay, UpdateSourceTrigger=PropertyChanged}" /&gt; &lt;TextBox Height="35" Text="{Binding Model.selectedElement, Mode=TwoWay, UpdateSourceTrigger=PropertyChanged}" /&gt; &lt;/StackPanel&gt; &lt;/Grid&gt; &lt;/Window&gt; View.xmal.cs using System; using System.Collections.Generic; using System.Linq; using System.Text; using System.Threading.Tasks; using System.Windows; using System.Windows.Controls; using System.Windows.Data; using System.Windows.Documents; using System.Windows.Input; using System.Windows.Media; using System.Windows.Media.Imaging; using System.Windows.Navigation; using System.Windows.Shapes; namespace testappreddit { /// &lt;summary&gt; /// Interaction logic for MainWindow.xaml /// &lt;/summary&gt; public partial class MainWindow : Window { public model Model { get; set; } public MainWindow() { this.Model = new model(); this.DataContext = this.Model; InitializeComponent(); } } } Model using System; using System.Collections.Generic; using System.ComponentModel; using System.Linq; using System.Runtime.CompilerServices; using System.Text; using System.Threading.Tasks; using testappreddit.Annotations; namespace testappreddit { public class model : INotifyPropertyChanged { private List&lt;string&gt; _a; private int _x; private string _selectedElement; public string selectedElement { get { return _selectedElement; } set { if (value == _selectedElement) return; _selectedElement = value; OnPropertyChanged(); } } public int x { get { return _x; } set { if (value == _x) return; _x = value; this.selectedElement = A[_x]; OnPropertyChanged(); } } public List&lt;string&gt; A { get { return _a; } set { _a = value; } } public model() { A= new List&lt;string&gt;() {"item1", "item2"}; } public event PropertyChangedEventHandler PropertyChanged; [NotifyPropertyChangedInvocator] protected virtual void OnPropertyChanged([CallerMemberName] string propertyName = null) { PropertyChanged?.Invoke(this, new PropertyChangedEventArgs(propertyName)); } } } 
Does anyone have any recent news on the status or timeline for .net native on coreclr?
Yet another one... Irc, slack, gitter, discord wasn't enough? We should split even more. At least propose something different like Matrix.
... what will the dotnet cli do if msbuild will perform the actions. btw. csproj is an abomination, project settings shouldn't be IDE specific.
&gt; ... what will the dotnet cli do if msbuild will perform the actions The same as it does now. &gt; csproj is an abomination, project settings shouldn't be IDE specific. Most stuff of the csproj is not IDE specific. In fact it even provides plenty of features that are not supported by Visual Studio (e.g. glob sources). They will also mainly simplify the existing content of the csproj file (I mentioned this) and also improve Visual Studios support for it. Overall XML is a much better choice than JSON for a critical project infrastructure as project files are.
Not found :)
It's just XML. They're removing things like listing all files as Net Core doesn't need that anymore, it's much more generic
http://www.discountasp.net sponsors my local meetup and seem great for a low complexity setups. 
Another option is to visit /r/microsoftsoftwareshop, grab a licence for the operating system of your choice, and then consider Vultr. Vultr have [documentation on their site](https://www.vultr.com/docs/windows-custom-iso-with-virtio-drivers) on going about this. Could also consider using my referral code, which will get you $20 of credit as well as credit for myself. http://www.vultr.com/?ref=6970633-3B
I read the article and came out wondering "why?". There's already Dapper (and other) lightweight and performant micro-ORMs. FluentMigrator.NET handles updates. What does the EF abstraction offer over the competition? _edit: spelling_
In ASP.Net WebForms, controls could maintain state, so that a dropdown list would retain its items after a postback. As you can imagine, this has its own drawbacks; most importantly, this state had to be maintained on the client side, that is, in the browser, then posted back in the next request. The better and simple way that you are looking for is output caching. It is available in the whole of ASP.NET, including ASP.NET MVC. 
You can always iterate through the list with a standard for loop and create a hidden field for each element in the list. Then that data will get posted with the request.
&gt;It seems odd to me that I have to query the database each time to view the same static data. I can cache the query, but isn't there a better way of doing this? It isn't odd. It's the stateless nature of HTTP. Do cache the query to speed-up subsequent calls if that's actually worth it but anything more would be over-complicating things. You can preserve things between the POST and redirect to the GET when it's not valid by sticking it in TempData. But I just stick the ModelState in it.
This is the right way because each the server is supposed to be stateless. You can cache the list on the server, so that you take some load off the database. I usually do something like this. var item = Caching.Cache(key, TimeSpan.FromMinutes(1), () =&gt; GetList()); public static T Cache&lt;T&gt;(string key, TimeSpan duration, Func&lt;T&gt; action) where T : class { var value = HttpRuntime.Cache[key] as T; if (cacheObject != null) return value; value = action(); HttpRuntime.Cache.Insert(key, value, null, DateTime.Now + duration, System.Web.Caching.Cache.NoSlidingExpiration, CacheItemPriority.Normal, null); return value; }
Well it's Microsoft's offering for one, which is enough reason for many developers, and the overall integration with other .Net tools will be much tighter.
Hey, thanks for the info. I will see if I can use a property-changed event. I actually don't have one selected element so will have to modify this a bit. I don't know beforehand which array element a particular value will point to or how many values there are. I have an array and collection of values pointing to different array elements which changes. But this is helpful.
It does, thanks again for taking the time to help.
No worries, I know how you feel. Deb Kurata's course is really great, it's where I started too :) good luck with it all!
Actually they're not removing that. MSBuild could do that same feature for a long time already - it was just Visual Studio that couldn't deal with this functionality. So for this feature they're adjusting Visual Studio.
:D Awesome. I will try it and get back to you.
Sure, but there's no real integration with any important tools. It's a poor abstraction that's less performant. Why promote it other than "Made by Microsoft"?
I just migrated to ASPnix.com because they support the .net framework 4.6.1. I'm only pay $13/M and its been pretty responsive.
Oh sorry, I didn't see he wanted Windows VMs.
You're right, most things can be SafeHandled. One circumstance where that's not true is when the handle is larger than a single pointer (which is all that SafeHandle can do). Then, you have to go down the full rabbit hole of constrained execution regions. If you think Finalizers are hard, CERs might as well be bloody brain surgery. 
Dapper is very low level, little more than just a mapper for single objects. EF maps results to whole object graphs. There is a low of power there that people love (though it does make the database work much, much harder). My opinion: EF is for newbies and people who are in a rush. Professionals should use Chain or Dapper.
I'm only mildly bitter. New blood and reinvigorated developers are looking at EF Core. I've had more than one drop it in greenfield after all the hype. Where's the ALT.NET:Core mentality in all this?
Hello Sir! Yes I know &amp; your are right it's not called Asp.net 5 anymore but in the Asp.net circles its still a more understood name than Asp.net Core. But thanks a lot for pointing it out:)
Then why not write about the officially released version, instead of a deprecated one? Especially since it's months already since the release. The purpose of the blog is to give a first time introduction, but you reference project templates **that don't exist anymore in this form**.
It doesn't look like they've upgraded to net standard yet which would make it easy but they do have PCL DLLs for it when are portable and I believe work on Mac. Have a look for how to reference PCL's in project JSON and try it I guess
It's for understanding some fundamental stuff which will be same whichever project template you choose. But you made a good point, I will accomodate that in the next post. Thank you for your valuable feedback.
There is nothing dumb about wanting to learn more about this field...cloud hosting moves faster than just about anything else in the tech space right now. So there are two resources here that would be most useful in your situation in Azure: App Services and Virtual Machines. Let's pick through what you're working with above and see if we can assign them out to those resources. An App Service is effectively the PaaS version of IIS running on dedicated VM resources, though you don't care about the configuration of the actual machine. You are basically given a sandbox within a VM of their choosing where you can deploy IIS applications, but won't have any real access to the underlying machine (though you do have Web Deploy, FTP and a few other extras). These are great for Web Apps because you pay for only the App Service plan, but you can host as many applications on that plan as you want (within the bounds of your resources). Since you're unlikely to need to install any extras on the machine, you're letting go of that control and letting Microsoft perform all the security patches/updates transparently to you. So for your apps listed above, you could safely run your 4 performant web apps, the WordPress instances and your Node site (these environments support PHP/Node out of the box, even though they're Windows-based). You can play with the performance tiers here a little; go to the pricing calculator (https://azure.microsoft.com/en-us/pricing/calculator/), add an App Service plan, and choose Standard S1 for now ($75/mo). This is probably more than you need, but it's less expensive than what you have and it's easier to manage. A Virtual Machine is basically what you're already using. If you need to run Raven and MySQL instances, you're going to have to have more OS control. This one's way more straight forward; head back to the calculator and add a VM at the Standard tier. Depending on your load, an A0 ($15/mo) might be enough for you...you could always bump it up later on (though IIRC that requires a reboot) while keeping the same image. Consider this: if you're only using that VM for these two types of DBs, you have a few other PaaS-y options. DocumentDB is essentially MongoDB on steroids, allowing you the ability to query it using SQL-like syntax, and it does some cool automatic search for you. That runs ~$25/mo at its cheapest, I think, but might be enough for what you're doing based on your data model(s). Also, if you're just reading/writing out some simple data structures, consider Table Storage (which is within Storage), which is extremely inexpensive relative to other storage options. For your SQL databases, Azure has a managed SQL DB resource that give you a few awesome extra features like point-in-time restores (auto-backups, you can go back and restore to pretty much any point in the past month or so, depending on the tier), geo-redundancy, etc. Cheapest DB you can find here is $5/mo, but if you have any real load, you're probably looking at $15/mo. File hosting is easy -- you're looking for a Storage Account (select Block Blob if you're still on the calculator). You can make these blobs public via URL or hide them behind credentials. In fact, you can even slap on a custom domain and a CDN if you're interested in such features. I don't think there's any managed email server on Azure, but I've always used SendGrid for that purpose. It's free up to a certain amount, and I don't think I've ever been charged by them. By the way, think we're up to ~$150/mo if you went with the A1 VM over the A0, and the S0 SQL DB over the B. You could probably safely scale those two back for now to bring you under $100/mo. Remember that you can change the VM/DB/App Service levels on the fly, so future upgrading is made easy. One thing to note on scalability (because you mentioned it) is that all of these listed services are able to both horizontally and vertically scale. Your best bet is probably to try vertical first, but if you ever need to run multiple instances of your sites, App Services can scale horizontally to multiple instances that Azure automatically load balances traffic between. You probably won't need it, but the option is definitely there. I think I've gotten everything here within your stack. Feel free to ask more questions as you poke through this, as I've spent quite a bit of time with each of these options I've listed above. Happy Azuring!
That pisses me off so much. I was up til 3am dealing with project.json issues last night. I still haven't figured out how to get it to use wildcard version numbers.
That's theoretically true, but .NET doesn't give the JIT enough time for advanced optimizations.
&gt;but in the Asp.net circles its still a more understood name than Asp.net Core. But thanks a lot for pointing it out:) No it isn't... I haven't seen anyone call it ASP.NET 5 on the github repos in a long time.
I like using Yeoman for something simple like this- specifically [generator-aspnet](https://github.com/OmniSharp/generator-aspnet). A nice clean Web API setup with a sane Dockerfile is as simple as ```yo aspnet webapi MyWebApi```
If you want anything beyond a developer / playing-around site, Azure gets expensive quickly. The tier required to run MS SQL, my own domain, and an SSL cert left me with sticker shock. (I'm migrating a PHP site from Linode, FWIW). In my case, it was for a low-traffic side business, which I don't think they're targeting with their business model, but it is something that people should consider before recommending Azure.
If you ran on linode, the shared tiers might make sense for you 
Only functions https://www.slideshare.net/mobile/ScottWlaschin/fp-patterns-buildstufflt
&gt;Other thing I don't like that this article does that our code fails to is relying on the global mapping configuration So having a handful of Mappers that you inject/initialize on demand? You'd have to be able to split your domain into very discrete sections to avoid them all just lazily triggering at the same time. The latest update claims to increase configuration performance by 2-3x too.
Ah the old "functional is everything, fuck oop" pattern. Seriously, functions aren't the fucking answer to everything. It's literally just procedural code with a few fancy concepts.
I'm talking about each ef model -&gt; dto having its own configuration. I'm honestly not sure how it would work in practice - one gotcha is we do have some "common" projection items configured such as how to handle DateTime -&gt; DateTimeOffset for example. But even if I was able to get all of the EF -&gt; DTO configuration in its own configuration that could be injected would be a good win in my book. We have a handful of places where I have unit tests around specific mapping configuration and having to test with the global config is a pain in the ass
I don't see how this is possible. I have a Car and CarDTO and an Engine and EngineDTO. I map from Engine to EngineDTO in one place, I map from Car to CarDTO in another. Mapping Car depends on having the Engine mapping available. So you'd need an IMapper for every combination? *** &gt;But even if I was able to get all of the EF -&gt; DTO configuration in its own configuration that could be injected would be a good win in my book Wait, you do know about Profiles and the new assembly scanning right?
Hmm, I'd never noticed this. I can't even think what Exception I'd actually expect in that situation. It doesn't quite fit any of the common Exceptions. Of course, they could just add a ResultToLongException
It wouldn't actually produce a real out of memory exception though, would it? As far as I can see the issue is that String.Length is an int, so it can't sensibly hold a string longer than that size (~2 billion chars) without being internally inconsistent Interestingly, trying to use StringBuilder with a capacity of int.MaxValue() also throws an OutOfMemoryException... or even with smaller values. int.MaxValue()/2 -1, for example, which I tried because of the fact unicode values are 2 bytes and I believe .NET objects max out at 2GB So I suppose it could be considered that it is running out of memory in terms of how it operates (even though the system has physical memory available)
I would love to see a GOOD real world use of a factory pattern if you can share it; I have yet to see one in the wild. I have joked before it is a purely academic pattern. And +1 on singleton's, I still don't get the hate on them, and most criticism I see is about how someone can use them badly. 
I think you're confusing things here slightly C#/Python/Ruby are languages Rails, Django, .NET/MVC are frameworks/platforms (the line between the two here can be a little blurred) A database layer is neither of those things, but tends to be part of the framework, or alongside it. Note that sometimes projects take different approaches to these things based on how they "see" application design, so they aren't always direct equivalents. The language is just the syntax and libraries of the physical code you write in a browser. The framework is built on top of those libraries to provide additional functionality. With C#, the modern technique is to use Entity Framework to represent the relationship between your runtime objects and your database
Not really sure what you are asking as it seems the question in the title and the question in the text are two different questions... &gt; What is the database layer for .NET? Typically Entity Framework will connect to the database, but there are plenty of options available if you don't want to use EF for one reason or another. &gt; Like Ruby uses Rails and Python uses Django. What does c#/.net stacks use? Since Rails and Django are mvc frameworks (although Django uses different terminology) I assume you are asking what is the equivalent in c#. ASP.NET is the closest equivalent but since Rails and Django also handle the DB side of things you will need to pair it with a database framework like EF. 
&gt; Just let the process crash due to a real OutOfMemoryException No, if you already know the OutOfMemoryException is bound to happen, better throw early and save time for everyone.
Dibs not being the guy testing the database being on fire
DI/IoC is something I cannot live without now.
Ugh. Tell me about it. I love LINQ, but it almost hurts dealing with InvalidOperationException { Message = "Sequence contains no elements" } and InvalidOperationException { Message = "Sequence contains no matching element" } when they should just really be some kind of `NotFoundException` instead of that stupid catch-all.
That's not an 'ideal' diagram TBH. There should be a layer of abstraction between the entities and the domain logic. This type of model leads to bleed of entities to the outer layers. 
Can you clarify this statement?
It sounds like you are describing a traditional layered architecture where each layer is independent. This is more of an onion architecture. Here the entities are the business objects, shared by all other layers, and persistence is treated as an outside concern and in the outer most layer.
DigitalOcean is fairly cheap, but they only provide Linux. The disadvantage using their service you must know how to manage your server. If you dont have ability to manage the server, then I wouldn't recommend them
I don't know if you've looked into .Net Core yet, but Scott Allen wrote a great ASP.Net Core course that includes Entity Framework (a SQL/DB library for .Net) talking to a database. https://app.pluralsight.com/library/courses/aspdotnet-core-1-0-fundamentals/table-of-contents
Great post. Some code samples would have been nice though.
Looks alive to me: https://github.com/Reactive-Extensions/Rx.NET
Thanks, I saw repository. How active the community is using the technology? Where to find them projects with Rx? Third-party libraries?
https://www.pluralsight.com/courses/aspdotnetcore-efcore-bootstrap-angular-web-app Its not a book, but i really recommend this course.
Not sure about that, I haven't looked at it in a few year. You should find something here: https://github.com/search?l=C%23&amp;o=desc&amp;q=observable.&amp;s=stars&amp;type=Repositories&amp;utf8=%E2%9C%93
I haven't finished your entire series yet . Do you have an extensive explanation on DDD ,aggregates etc?
Thanks I have just finished reading everything I am actually creating a project in aspnetcore I used mediatr, structuremap decorating my requests with fluentvalidation .. I don't have any ES yet though so your blog was a good start
docker is a set of tools for working with containers. What are containers? Containers are an alternate implementation of virtualization technology. VMWare, VirtualBox, MAME are all examples of virtualization. VMWare and VirtualBox are applications on a host kernel, while containers are a feature of the kernel. Because they are a feature of the kernel and use hardware technology like hyper-v, containers are more efficient than traditional emulation software. Until recently, Windows did not support containers at the kernel level. Because of that, Docker for Windows was based around VirtualBox and a Linux kernel to run the containers. That is no longer true after Windows 10 SP1 (anniversary edition) and windows now has [windows containers](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/quick_start/quick_start_windows_10), which means docker for windows no longer needs virtualbox or linux to work. I'm hosting a couple nanoservers myself this way using containers.
Ugh... I've fallen for a clickbait in an email from jetbrains - it's just an EAP... 
&gt; database being literally on fire Database must have been deployed on a Galaxy Note 7.
Ah, so you have heard of Microsoft's SQL Server on Linux initiative? It's been having some... interesting, bugs on smaller devices. 
Thank you for your interest, we are working on that.
Note that ASP Net MVC 6 has been renamed to ASP Net Core [see here](https://blogs.msdn.microsoft.com/webdev/2016/02/01/an-update-on-asp-net-core-and-net-core/) I would suggest starting from the tutorial on the ASP Net Core documentation [that you can find here](https://docs.asp.net/en/latest/tutorials/first-mvc-app/index.html) although it is not a book is a very comprensive guide for a start
Thanks. This looks really nice and well documented too. 
I don't know what you mean by "alive", but I (and our whole team) use it almost every day and like it very much.
ReactiveUI is a fairly popular and active project built on Rx: https://github.com/reactiveui/ReactiveUI
Why don't you want to use WebAPI?
Web API is nice for small projects but for large projects, it gets ugly fast. For example, writing controllers for 200+ entities is a lot of repetition. I would ideally like to use a framework that allows me to create entities (basically DTO's) that automagically map to their respective verbs to avoid creating controllers altogether unless needed since all of my entities essentially do CRUD operations against a custom business layer. Also, Web API doesn't automatically handle things like pagination, querying or sorting. Those can be added but I'm hoping to find a framework where those things are built in.
1. I generally use a master page to contain my form tag, but you should be fine. (I don't believe the form id really matters unless you're referencing it) 2. If you're doing a post back (You probably should) you won't use the Jquery Validation. When they click "Submit" they do so to save the information. Since you're writing that information to your database you only need to verify it on the back end (C#). When the user submits it, in order to do the front end validation you'll have to pause the post back, verify it, then send it to the back end and verify it again. If you want to do front end validation I recommend that on focus out of a field you verify it on the front. On submit you verify it on the back. 3. Yes Also it a nice looking form you have
Not really, doing local development will be fine on any laptop for learning purposes. I'd get a i5/i7 and no less than 8gb ram and you'll be fine. I build on a MacBook Pro running windows 10 as a virtual machine and works sweet
Have you tried https://github.com/jbogard/MediatR ? There are a few blogs around detailing implementation but you remove the ugly from the controllers by writing handlers. https://lostechies.com/jimmybogard/2015/01/20/integrating-mediatr-with-web-api/
Thanks! I appreciate it. I'm always paranoid of new windows versions.... :/
Hasn't it always been? I was using the developer edition a year ago at my previous job.
We are using Nancy and love it so far. Easy to customize if you need it. 
I really do...The fact there is a handful of php jobs and a massive pile of asp.net where I live is definitely helping me to do it..
Go for NancyFx, it's a pleasure to work with and also supports the .NET Core. 
Another +1 for NancyFx; it's proven to be extremely stable, reliable, and well-supported over the 3+ years I've been using it.
Looks very cool. Keep up the good work.
I am working on producing micro samples for aspnet core https://github.com/dodyg/practical-aspnetcore
Now I only need sql server on linux (2017) and VS (soon?)
on linux
Well, looking at the SDK most of the filters would actually be pretty easy to implement and a bunch of them are already in there. I'll add the missing filters to the list and I already need some image alignment/stitching code for some of my projects anyway.
VS code is pretty damned impressive actually. It's not full blown Visual Studio, but it's worth checking out.
I've tried on linux and it's great for small projects but meh for something professional
I thought you just meant SQL server on linux. But as noted below VS Code runs on Linux (but applies to different project styles). I can't see VS full coming to Linux anytime soon.
I meant both, now that .net core was released and MS has been hinting at being a full multiplatform system (since mvc 2.0 went open source and ran on mono), I expect VS soon on linux. To be honest, sql server was just a priority
Other than Nancy there really isn't much else in .NET land (there is Suave and Freya but they both use F#). That said Nancy is really nice (and a lot better than ASP.NET imho).
It's already on EAP 3. It has been fairly stable for me, but having been hosed by buggy EAP builds in the past, I completely understand taking it slowly.
Yes, it's really more for learning that anything else. It lacks a lot of the features one would look for in a large scale production application like replication.
Thanks, updated 
There are good number of videos on Channel 9 MSDN to get started esp ASP.NET Monsters series, also Microsoft Virtual Academy is releasing tutorial series on this soon. 
Visual Studio isn't even 64-bit last I checked. If they're porting to an entirely different OS, they'll probably be better off rewriting it completely.
This is not the case. There is a limit to the turnover of the company 
I think you would want to put something like Redux into the mix. React itself is just view rendering. But yes, using setState() whenever there is a update from signalr would work fine I guess.
just run the check on focus out https://api.jquery.com/focusout/ that way they get an immediate notice of an issue. The reason I don't recommend checking on the front &gt; do a post back &gt; check it on the back. is time and accuracy. Time: to check it on the front then on the back takes more time. I'm sure it's in milliseconds, but it's extra work. Also the post back shouldn't take more than a couple seconds. Accuracy: Any time you need to change your validation you now need to update 2 places, or if there's an error in your validation you need to check 2 places vs 1. The other option is do what jogi-san said and use ajax. It's faster and better for mobile users. 
I've done this using Redux and [Redux Thunk](https://github.com/gaearon/redux-thunk). Basically what I did is wrap the SignalR bits in a module that I would use in my actions files. I'd subscribe to SignalR groups inside my actions, then dispatch other actions when the notifications came in. For example, I'd submit some task for a server to run, and the server would return me a taskId. This would create a SignalR group on the server and the client would subscribe to this channel using a hub proxy. This was wrapped up in a module as mentioned above. In the actions layer I would use the module above to subscribe to a particular taskId group, which would be sending me started/progress/done messages. I'd case/switch over these, and dispatch other actions so the UI could respond nicely.
If the serializer is what's blowing up and you don't have server access, maybe try explicitly defining the returnType as text (which you could pass to a custom-rolled serialize type, chop in to pieces, stream into another object, etc.). If jQuery is available there are some good code samples to get started on their site (http://api.jquery.com/jquery.ajax/).
Use LibLog instead of common.logging so that you get structured logs.
Sure, I'd be interested in seeing how you handled it.
You are thinking about Community edition, which as far as I remember capped to $5 Mill and 10 employees. 
Ah yes I stand corrected 
Reporting back: I learned many things regarding Identity, the Owin Pipeline and all that good stuff, I also came up with some ideas, but ultimately I had to decide against the multi-tenant model. With this project I wanted to combine many technologies to learn how to build a larger application with a web-backend and multi-tenancy was just holding me back. I refactored the entire project into a more simple model where all the users are explicit from a single organization. Now I am doing very well with my project and I am very excited to bring now SignalR and a Xamarin App into the mix :) Thanks again for your help u/bigrubberduck !
Generally I create a "Logging" Project. I then wrap any object I want to keep logging on in a decorator pattern which simply delegates the calls on but logs what the parameters were, and sometimes what the response was. That said, the only objects that should directly reference it (maybe) are your services. It is considered good practice (where I work at least) to wrap all service calls in a try / catch and have the catch log. Just capture exception e and log e.tostring(). Same goes for button / UI clicks. But anything else, anything deeper than the UI or the WCF service should be done via the decorator. I would even argue the services themselves could be done with a decorator but that takes a good team. So I have something like... Logging OrderService OrderServiceLogging OrderServiceLogging references the logging, and in my hosting project, I simply inject the OrderService into the OrderServiceLogging. Now all calls to the OrderService will go through the logging wrapper and OrderService has no knowledge it is being wrapped (so you can extract it for other uses)
As a person that uses log4net, this didn't convince me that I'm in dire need of change.
Thanks for this. I'll explore into this!
This is the module with the middleware and signalr connection. It has some logic to keep the connection alive when it disconnects. The connection is started using `{ type: 'connection:start' }` and stopped using `{ type: 'connection:stop' }`: const connection = $.hubConnection('/signalr', { useDefaultPath: false }) export default function createSignalrMiddleware (actionDispatcher) { return store =&gt; { const dispatch = store.dispatch.bind(store) const stateConversion = { 0: 'connecting', 1: 'connected', 2: 'reconnecting', 4: 'disconnected' }; var keepAlive = false var wasConnected = false var currentState = null actionDispatcher(dispatch, connection); function onStateChanged (state) { if (currentState === state) { return } currentState = state dispatch({ type: 'connection:statechanged', state: state }); } connection.stateChanged(state =&gt; { const newStateName = stateConversion[state.newState] if (newStateName === 'connected') { wasConnected = true onStateChanged('connected') } }); // When the connection drops, try to reconnect. connection.disconnected(function () { if (keepAlive) { if (wasConnected) { onStateChanged('reconnecting') } else { onStateChanged('connecting') } connection.start(); } }); return next =&gt; action =&gt; { const { type } = action; switch (type) { case 'connection:start': keepAlive = true onStateChanged('connecting') connection.start({ transport: [ /* 'webSockets', */ 'longPolling' ] }) return; case 'connection:stop': keepAlive = false wasConnected = false onStateChanged('disconnected') connection.stop() return; case 'connection:invoke': const { hub, method, args } = action; const proxy = connection[hub] proxy.invoke(method, ...args) return; default: return next(action); } }; }; } You can use the middleware this like: const signalrMiddleware = createSignalrMiddleware((dispatch, connection) =&gt; { const testSessionHub = connection['TestSessionHub'] = connection.createHubProxy('TestSessionHub') testSessionHub.on('HandleTestEvents', (events) =&gt; { events.forEach(event =&gt; dispatch(event)); }) testSessionHub.on('Disconnect', () =&gt; dispatch({ type: 'connection:stop' })) }) const store = createStore( reducer, initialState, compose( applyMiddleware( thunk, signalrMiddleware ), window.devToolsExtension ? window.devToolsExtension() : f =&gt; f ) ) EDIT: Formatting
200+ entities sounds like it could be split into multiple medium sized projects. Still might not scale so well but might be more manageable.
For ASP.NET Core (which is the newest version ASP.NET) - Incredible performance - Amazing points of extensibility - Fun
[For more on Cake](http://www.hanselminutes.com/548/cake-build-a-c-make-cross-platform-build-automation-system-with-patrik-svensson)
I listened to that the other day. Not a great interview.
(continued) * Greater cross-platform compatibility * Open Source so the community can provide fixes to the official devs more quickly and efficiently, thereby improving the quality of the platform overall
https://github.com/dotnet/wcf/tree/release/1.1.0
Sensationalist title, the article assumes I want to do RESTful services. MQ (WMQ) works well with WCF and I'll stick to it.
&gt;january 6 2016
And WCF can do full on restful services also. 
Well now, let's not get ridiculous. 
I've recently introduced Cake at my job in hopes of scripting builds/tests/deployments/etc where there are none. I chose Cake instead of Psake, which I used at my last job, because I am more comfortable with C# than PowerShell and so are most of my co-workers. I do not regret choosing Cake.
Nice! Thanks for this.
Yes, they are the exception, but they are also in the process of adding AOT support. https://github.com/NETMF/llilum
Given Anders background (Turbo Pascal, Delphi) I was always disappointed that we only got a JIT/NGEN mix instead of having a full AOT compilation like on those languages. Thankfully the Singularity and Midori work wasn't completely thrown away and the experience is being used to improve .NET.
A good thing too. Interpreted IL never made much sense on limited resource machines, though I understand why JIT wasn't a good option.
Great article, u/grauenwolf, it touches plenty of real life topics - auditing, history, security. But where's the first part?
Where are you based out of? 
I had to unpublish the article so they can fix the content. It will be reposted on InfoQ once everything is sorted out.
GUIDs are not intended to be a source of cryptographically strong randomness, just use `RNGCryptoServiceProvider` to generate however many bytes you need. Also, you didn't mention the most important part of the scheme, which is the password hashing method and its parameters. PDKDF2 is included in the framework, but bcrypt or scrypt are preferable if you don't mind taking on an external dependency. Ideally you should store the function and its parameters along with the password hash so you can transparently upgrade individual users to a better function/more iterations later down the line.
Ah good idea. I didn't think of the "upgrade" process. Yeah I haven't decided on a password hashing method, that's why its not included. Good idea with the bytes[], also did not think of that. I can turn the bytes[] into a string and store that. Good call. Thanks.
Can you, or someone, explain why I would want to use one or the other? I originally had my controllers set up like this: public JsonResult SomeController() { return Json(variable); } I problably did this because that's how Shawn Wildermuth's course did it. At least at the time I watched it, not sure if he changed it at all. But, then I was looking at the samples on docs.asp.net and saw this. Also, I heard someone else mention it in a talk. I might have even heard/read/seen it this way somewhere else. So I've changed them to this: public IActionResult SomeController() { return new ObjectResult(variable); } Any reason to use one over the other?
Ideally, you should use: public IActionResult SomeController() { return JsonResult(variable); }
I have a ton of Microsoft certs but I swear to God, putting MVP on a resume is the mic-drop of IT.
Tldr have the right connections, know an mvp. It's just a title. 
It would be great if the author would mention he wrote Chain. Also: &gt; Projections can be tedious, but are needed to get the most performance out of your database. Yes, but projections aren't tedious in all ORMs &gt; Partial updates can be problematic for full ORMs. No, only in the ones you picked and only in that particular situation. Even in EF, fetching an entity, changing a field and saving it will simply update _only_ that field. 
I'm not sure about the others, but Entity Framework [already implements the repository pattern](http://programmers.stackexchange.com/questions/180851/why-shouldnt-i-use-the-repository-pattern-with-entity-framework/220126#220126). Wrapping it in your own is needless busywork. 
Yes and no. It offers a very crude repository, but in a way that pushes a lot of work into the client code. For example, you probably don't want to copy the update boilerplate over and over again every time you want to use it. And I would like to reiterate my point that properly using projections can dramatically improve performance. Which, in EF, means even more boilerplate code that the client shouldn't have to deal with.
Your comment about EF was handled with this line: "However, you can simulate it using the read-copy-write pattern for updates." The point still stands that you can't just tell EF to only update one column without bringing the object down first.
inb4 ef = repository pattern comments Yeah, but you should still abstract it etc.
P.S. This offer is open to anyone who wants to write about their favorite ORM or other library.
And by abstract it what do you mean? Throw a business layer on top of it? Create your own repository abstraction on top of it?
Does Chain has a way to configure mapping without littering my POCOs with 3rd party attributes (e.g. Fluent configuration in EF)?
Most of the attributes it uses are .NET standard: Column, NotMapped, Table, Key. If I recall correctly, only IgnoreOnXxx and Decompose are 3rd party. You don't need IgnoreOnXxx, as you can do the same thing with a rule. And Decompose can be replaced with a constructor materializer. So yea, it probably can serve your needs without 3rd party attributes. *** If you keep the property and column names in sync, you don't need attributes at all. By default it will ignore any property without a matching column or stored proc parameter; no configuration needed.
global.json, not project.json.
[What about this?](http://stackoverflow.com/questions/9832698/asp-net-mvc-polymorphism-inside-view)
Sorry, I'm dumb. I had project.json in my head and I replied via mobile so I missed it again.
Personally, I do think it should be abstracted. I can inject IEntityService or IEntityRepository all over the place with simple methods and not be tied down to EF or any other ORM. It makes swapping these out much simpler. If I pass around IAppDbContext all over, then I can only swap it out with something that treats my entities as DbSets. It also makes testing much simpler. I don't really like having to create mock DbSets (too much extra code -- at least with Moq) just to test code that has nothing to do with EF.
I have actually. In fact until recently there were some queries in EFCore that could not be generated using Linq statements. Because of this, I was able to easily swap layers temporarily. But even more than just the ability to swap interface implementations is that I don't have to Moq DBSets for any tests except those that test the service. It's a PITA (http://aikmeng.com/post/62817541825/how-to-mock-dbcontext-and-dbset-with-moq-for-unit). I also may start with EF as my ORM (bc it's so easy), but if performance becomes an issue it's much easier to make changes because I know ALL queries are going through the abstraction and I don't need to go touch every place in my application that needs domain data.
It's literally just an ad for a book. You can't even view a few sample chapters online. 
While I agree that it's an anti-pattern, I don't find his reasons to be compelling. I am more apt to be convinced by concrete examples of problems than theoretical discussions about layering and purity.
They aren't aren't compelling because his advice will lead to hideously slow applications. There is no architecturally pure performant database access model. Period.
Why should we wrap a repository (DbSet&lt;T&gt;) in a repository? Doesn't that defeat the purpose of using a database abstraction like EF in the first place? If I wanted my own repository and unit of work (DbContext) I might as well use dapper or straight up ADO.NET. 
Yeah I spent 2 days memorising the vce dumps and passed it almost perfect score. After this I realised it was pointless getting the cert as anyone can memorise the answers, even if they couldn't code a for loop.
The solution to all this is actually pretty simple - have a third layer that returns IQueryable. You have a Business Logic layer which handles the application, a Repository layer which handles storing and retrieving data, and you have a Queries layer with common queries in it which are all composable via IQueryable. As an example, you would have a query e.g. 'ActiveAccounts' with something like return DbContext.Accounts.Include( a =&gt; a.AccountDetail).Where( a =&gt; a.ExpiredDate == null); And then your repositories could use the ActiveAccounts query instead of using the context directly. If you have a repository method e.g. GetAccountById(int id) you could do return ActiveAccounts.FirstOrDefault(a =&gt; a.AccountId == id) This avoids bugs where you might forget to e.g. filter out where ExpiredDate == null or not included related entities, but still allows you to compose query expressions without materializing them, building efficient SQL queries. If you don't have the third layer, all of those ToList() calls in the repository layer are executing separate SQL statements instead of doing a proper join. 
&gt; It will certainly make unit testing your domain logic much easier if you return a IEnumerable. How? Is calling `AsQueryable` in your DB mock that hard?
Using an IQueryable plus an expression tree to get only the data you need. Queries in a database, especially if the right indexes are in place ~~is~~ *are* much faster than any query you can do in your application. Even if you do need to actually use the whole list it can be worthwhile partition*ing* the query ~~as~~ *since* if you push your resources it'll be much slower. The basic reality is that there's no framework in existence that allows you to completely abstract away your persistence layer and maintain any level of performance. It's why the layers architecture has a little in practice caveat allowing you to go more than one layer down for performance reasons. Edit:grammar
Thank you, I'm shocked every day when I see articles like this, that simply don't get this basic concept. The alternative of course is that your repository has a hundred attached queries for any piece of code that calls it. It becomes a night mare to maintain.
Having `IQueryable` leak out past your repository, just feels wrong. I'd much rather have a `List&lt;T&gt;` returned back to me. In the post, this example is given: `var getDoctorswith50Appointments = myrepo.GetDoctors().Where(d=&gt;d.Appointments.Count &gt;50);` What happens when you have one million doctors but only ten doctors with more than fifty appointments? I'd just let the database handle the filtering for me (e.g. `GetDoctorsWithAppointments(int value)`) because that's what databases are good at.
I agree with him that you shouldn't return an IQueryable but you can't just return a generic list of everything. That would be horrible. Instead, just make additional methods that you need like: - GetById(int id) - GetMales() etc... just make specific methods to do the high level filtering you need
There is absolutely no reason to have any layers that return IQueryable if your ORM already returns that. Just use the ORM with a layer that encapsulates it.
I prefer the DDD definition which is, paraphrased, "Repository: Used to store, retrieve and delete domain objects from different storage implementations." https://blog.fedecarg.com/2009/03/11/domain-driven-design-and-mvc-architectures/ Fowler's design patterns have always struck me as being simultaneously naive and yet overly complex at the same time.
I believe that you are correct.
This one is pretty sad. A really naive view of a layered architecture is being used to justify really awful code. EF has a huge number of advantages in the way you can achieve a reasonable separation of your database and business logic, but if you don't know what you're doing you can write some really nonperformant crap. There's just no escaping having to know and understand your data structures. I've seen juniors write code where the DB gets called prematurely, but usually even if they don't know why they pick up the huge performance problems. 
Yes. The database is part of the application. Use it like it is. Either merge the data layer and the business layer, or add a new method to the data layer for every unique query the business layer needs.
Razor in console app?
There is literally zero content here. It's a blatant ad. Why isn't this removed? 
You don't need to use the repository pattern. This is the best advice anyone can give you. 
You should check out the Voat source. I'm sure it has this feature on the profile page at least and it's an MVC app.
This is not a fact...
MySQL.Data not being prod ready has been driving me nuts. When I first tried to use it I went crazy trying to get AspNet.Identity to work until I realized, like you said, it is just pure broken.
Why not use FluentMigrator in place of built-in migration support? It works well in my experience, though not sure if it works with .net core yet
Yes, the query isn't executed until ToList. All the extension methods applied to IEnumerable are actually delayed from execution until that ToList, when the agglomerated operations are translated into SQL in the mother-of-all brilliant ugly hacks. Also, use the lambda forms of include or nameof rather than unrefactorable strings.
It's the table/view. Trying to pretend it's something more or less is silly. The "needless boilerplate" is the query, like any query.
I think there's a balance to be had, I'm still not a fan of SQL stored as strings, even parameterized, or of having DB specific syntax in my code, at least not unless the extra performance is really going to matter, but that doesn't mean you ignore all of the power of your DB. I still like EF as opposed to faster frameworks, I think the tradeoffs are worth it a lot of the time, but the performance impacts of this guys advice are just about the worst, especially since the same path will lead to including multiple one to many relationships. Imagine that bit of fun.
But that's true for traditional SQL-based code, too. I mean, a DAL, by its very nature, doesn't fit well with good unit tests- any time you're crossing a system boundary, fuckery is afoot.
Good idea, thanks!
Absolutely. But by encapsulation data access logic within a repository or query object you keep those concerns in one place. But if you return IQueryable then you expose the fuckery to anyone calling the repo. 
Woah, there, I return IQueryables even when I'm not using EF, but I use LINQ for pretty much any operation on collections. Sure, because the IQueryable represents a deferred query, you *do* have to be concerned about query-related failures propagating up to code that expects to have actual objects. At the same time- I often *really want* a deferred query.
I'm also guilty of this too. One of my favorite patterns in the past was generic repo or directly using DB context then using AutoMapper Project.To() to nap to DTOs. All right in the controllers. This is a fine architecture for CRUD apps. Or very small apps with small teams. But when things get bigger and you need the safe guards then it makes sense to start encapsulating and having specific query logic that returns IEnumerable. 
I agree- I just don't think that query logic always goes into the repository. It belongs in a class that represents *that* specific query. It can manage the IQueryable internally, deferring query execution until it actually needs the data, while encapsulating the exceptions. I've always viewed the repo as the minimal wrapper around core CRUD functions, and you write classes for the real operations.
&gt; you've got business logic in your repositories again, Why is that a bad thing? If I've got multiple applications sharing the same repositories, then there is going to be a lot of business logic that should be enforced across applications. 
Yes, using this package (https://github.com/Antaris/RazorEngine)
True, VS is still the best, but once the Project Rider is ready I guess that will be very powerful IDE as well. Although the other features such as native Docker environment etc. are far more important for me right now than just a VS for coding.
I have mentioned query objects in a few of my other answers. But whether the logic is in a query object/class, or a method on a repository, it's just semantics at that point. You still need a single place to integration test your query logic to ensure your tests are indeed correct. But treating the repo as a minimal wrapper around core CRUD functions sounds an awfully lot like a generic repository. There is really no need to create a generic repository around EF (can't speak for NH) because the DbContext is already an implementation of the Repository pattern. Wrapping it in your own generic interface is an exercise in futility. More often than not you're going to need something specific from EF and you'll wind up having to re-implement it in your own repo (e.g. like changing entities states, detaching/attaching, checking object state, etc.). It makes far more sense to have a concrete repository or query object that has full access to the DbContext for the very specify action. 
Docker can be added to windows with a single button. Add features -&gt; Containers.
You shouldn't have multiple applications using the same repositories. You're better off exposing this as an API instead.
This is 1952. Learn what code reuse is you useless twit.
Wow. That's a very thought provoking response. Ever heard of a bounded context?
The issue is that queries are part of business logic. What projections you use, what entities you include, how you filter, it's part and parcel of the logic. It probably shouldn't be, but until we find an ORM model that's not hugely leaky it's what we got.
Examples are a lot more useful than saying "don't do this" or "do that". Plus when you say things like "I don't mind", you're just pushing an opinion.
Visual Studio Code is amazing, I've been writing an asp.net core app solely on my Mac for 4-5 weeks now and not missed a single thing from full Visual Studio. Intellisense, step through debugger, quick code fix suggestions etc, it's all there.
I'm currently favouring a criteria object approach. So my repo's FindAll method might look like this: List&lt;Widget&gt; FindAll(WidgetCriteria criteria). The criteria object contains a bunch of possible filters, e.g. public class WidgetCriteria : ICriteria { public int? CategoryId { get; set; } public bool IsEnabled { get; set; } public string NameContains { get; set; } } This allows my business logic to author specific data requests without leaking things like IQueryable from the repo, and also without affecting performance, e.g. public List&lt;Widget&gt; GetEnabledWidgets() { return _repository.FindAll(new WidgetCriteria() { IsEnabled = true }); }
Dapper isn't the fastest (micro)ORM anymore, and hasn't been for a long time. See: http://pastebin.com/m4HC4twf (code: https://github.com/FransBouma/RawDataAccessBencher). Most recent results (with llblgen pro v5.1 which is currently in EAP) http://pastebin.com/SNRYPEcp
Yeah, show how I can use it in the same way and I'll probably be on board: just tell me not to, and I'll probably carry on doing whatever was convenient before
It involves leaking business logic down though. Not that that's bad, the point is that you always leak up or down. Your method means that if you need to access your data in a different way you have to modify your repository and you have to make sure your repository is fully testable, which can be messy. The alternative means people using your repository can fuck up the queries. The whole problem is that people don't know what an antipattern is and confuse it with code smell. An antipattern is a pattern that, when used correctly, doesn't deliver the outcomes it's supposed to. Code smell is something that needs to be investigated because it may violate best practice. It might not be. In this case, the only solution that doesn't violate separation of concerns doesn't work. It's also worth noting that IQueryable is not Entity Framework code. You can build a test context made entirely of Lists rather than DbSets and convert them to IQueryable so most of his argument is crap anyway.
I give Microsoft 5 years to make Windows desktop and universal apps debuggable and runnable on GNU/Linux.
Code editors are great if you have a small project. When things get bigger though and you start introducing significant abstraction and your code goes through a few dozen bug fixes then they are less nice. Things like race conditions are also much harder to debug as well. Visual Studio Code is not an IDE, and sometimes the overhead of an IDE is 100% worth the price.
I agree. I recall way too many "repository" examples that have them sitting over an ORM and saying it's right. The canonical repository pattern is what the ORM is meant to be! Like you, I feel it's fine to encapsulate the calls to the ORM in a repository or query (although I don't know that much about the query pattern), but not to create a generic repository that just wraps the ORM calls. I see the argument being "But what if you change ORMs" but let's be honest, how often is that something which you do, and how often would just changing the internals be the *only* thing that needs to get changed?
&gt; It involves leaking business logic down though. Not that that's bad, the point is that you always leak up or down. Agreed, I've never encountered a situation where this was completely separable. &gt; Your method means that if you need to access your data in a different way you have to modify your repository That's by design, if I'm changing a database call, lets say to optimise it at a later date or change the underlying structure, I want to be working in the data layer, not anywhere else. Preferably my services will not need touching although that bit is often a pipe dream depending on the changes being made. I personally don't want service layers architecting queries, I like my data layer as the gate keeper.
Oh I agree. If you need full VS then use it. Just for my uses, writing an asp.net core app with several class libraries, the usual layers, data, services, etc I'm finding it to be completely adequate. I'm not sure id agree that vscode is JUST a code editor though, more of a hybrid given true intellisense and debugging. It's a far stride from using Sublime Text for example. Out of interest, what enables full VS to debug a race condition easier?
&gt; and not missed a single thing from full Visual Studio I miss that the namespace in C# files is pre-initialized matching my project configuration and the file location.
Testing frameworks, while often used for unit testing, are often used for integration tests. During an integration test, order usually does matter.
Let the database handle it via unique constraint, and make sure you have code that will catch the exception and do something?
Been using windows 10 since it came out, not once has it ever forced a reboot on me while I was using it. Now the Windows 10 Linux Subsystem is out, so I'm running linux on Windows 10. Loving it, because I can write SH scripts now and run on Linux on Windows without a VM. Great for python projects.
And where do you write SQL? In Service layer? Doesn't seem right to marry your service layer to one specific sql dialect, that's why you have service and data access layers.
Being 'production ready' is really a very subjective metric. Microsoft and the EF Core team seem to think it is, provided you don't require any of the as-of-yet unimplemented features. I am using it in production on a couple of sites/web apps, so I tend to agree that it's ready. Granted, neither are overly complex, but I haven't run into any major hurdles since the release of 1.0. You should definitely consult https://docs.efproject.net/en/latest/efcore-vs-ef6/index.html before deciding either way. If you're comfortable with .NET Core itself, I'd say go for it. But again, my uses have all been fairly simple, so ymmv. 
&gt; so I tend to agree that it's ready. Are you using MySQL by any chance?
Nope, sorry. SQL Server on Azure for me. I can see how that could be a deal-breaker for you, though.
I was answering the questions posed in the article: &gt; Should I use Event Store or Read Database for a query? Where should I check that?‚Äù
The last paragraph answers those questions. His solution is using a Unique Constraint + Saga.
And how are you going to access the lame assed micro-services you insisted on adding? Oh right, through a repository. Because that's what repositories are for, to abstract away how data access is performed. And those repositories are going to want to include business logic that hopefully avoids unnecessary round trips across the network. Boom! Welcome to the 60's.
If I had tfs 2015 installed here at the office I'd give it a try maybe when I get some free time i'll look into it. 
That's interesting. Why can't you disable the Windows Update service on Home? Even if that's not possible, if I were stuck with Home with no hope of upgrade I'd still find a way. Tell Windows my wireless connection is metered. Or remove all permissions to the Windows Update executable file. Or block access to windows update at the router. Or whatever.
I posted about this last night: https://www.reddit.com/r/dotnet/comments/57tg6p/entity_framework_core_providers_dbms_choices/ So far only has my experiences with MySQL in there, but would love to hear from others
Chances are you're not going use a generic repository to talk to a "lame assed micro-service". And chances are you're not going to return IQueryable either. I never said anywhere that you shouldn't have some abstraction over your data access. Why you so angry man?
If you are using EF, what is the benefit you get from writing a generic repo? Why wouldn't you just use the DbContext directly in your concreate WidgetRepo? It's just added code/work.
vscode has no concept of solution, it's a folder based IDE