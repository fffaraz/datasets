This one sure seems like the easiest when compared to Amazon and Google Cloud. There's also a way to do it with VSCode if anyone wants to try it without Visual Studio. [https://code.visualstudio.com/tutorials/app-service-extension/getting-started](https://code.visualstudio.com/tutorials/app-service-extension/getting-started)
The first one is a function the second is setting the value of a field or variable.
This looks like a Concurrency Token (which is used for optimistic concurrency). IIRC, this is a timestamp stored as binary. If you're using Entity Framework, you should mark this field as a concurrency token and EF will throw an exception in concurrent scenarios. If not, there are plenty of answers showing how to sort it.
Here's an article from the official docs about optimistic concurrency: https://docs.microsoft.com/en-us/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/handling-concurrency-with-the-entity-framework-in-an-asp-net-mvc-application
Loving this may i ask how often/when, you will be uploading i have notifications turned on but sill im eager lol
If you have to work with some native libraries, x86 is necessary. 
Wow, this is very cool! I was going to roll my own but this looks like it does everything I need. Thanks for the post!
I‚Äôve had issues where sometimes two projects that have similar the same connectionstrings that run in the same website in IIS will throw these errors. What I normally do is have a put all of your connectionstring information within ur ‚Äúnew app‚Äù web config. I would then add another node in both the web.configs for shared settings: &lt;configuration&gt; &lt;appSettings file=‚Äò../Sharedconfig.config‚Äô&gt;&lt;appSettings /&gt; .... &lt;/configuration&gt; Then put all of your keys within the SharedConfig. This normally resolves my issues between two projects running in the same Website/App Pool
thank you
Title sounds interesting. Now I just gotta find some time to read the article! ;)
Wrapped up in a Docker container on a container orchestration platform -- we use DC/OS (hosted on AWS EC2) for this and evaluating Kubernetes on AWS' EKS for this.
Finally some guidance on how to use pipelines. So far I've been learning it by reading the kestrel source.
In MVC a View is just part of the equation. The Controller and Action serve up the View (and Model). Typically, you would use the [Authorize] attribute on your Controller or Actions. Use ASP.NET Identity; scaffold a new application in Visual Studio to see an example of how to implement it. 
Hot damn that seems like a way more complicated abstraction to understand than streams.
How do you like AWS EC2? I'm not familiar with it at all though. EC2 takes care of maintenance overhead such as scaling and patching right? Is it pretty simple to get up and running too?
It is and it isn't. Streams try to hide a lot of the complexity from you, and are halfway successful at doing so, but have a lot of gotchas and inconsistencies that the first part of the article describes. Pipes show you more of what's really going on so that you can create the right abstraction for your application.
I understand how it‚Äôs a more powerful abstraction, like how monadic structures are very powerful. However like monadic structures it‚Äôs going to be harder for the average developer to grasp.
Are they running as multiple applications under the same site or as actually independent sites? If the former, their web.configs will get merged (at least partly - I really don't know the details of the merging other than to say that I know it occurs).
I just love cheat sheets! Great job
Read [https://msdn.microsoft.com/en-us/magazine/mt842512.aspx](https://msdn.microsoft.com/en-us/magazine/mt842512.aspx) and [https://msdn.microsoft.com/en-us/magazine/mt763233.aspx](https://msdn.microsoft.com/en-us/magazine/mt763233.aspx) One option is to mix MVC views and Razor pages (into features/areas). You could, for instance, use Razor pages for simpler "page-centric" scenarios (like About, Contact, etc) and MVC controllers for APIs called from SPA ajax calls, or more complex scenarios.
Take a look at the ASP.NET core web application template in VS2017. It has a React.js variant which sets up a nice webpack build pipeline that you may find useful. I personally recommend React.js + typescript as the go to solution :)
This looks like a nice way to handle reading/writing crypto streams to minimize resource utilization. Will the next posts go into more detail of the memory pooling/recycling? 
 Am I missing something? Yes, it does produce a single less instruction, but its simpler instructions without the ternary. Without the ternary is still (marginally) faster. [https://github.com/Dispersia/TestTernarySpeed/blob/master/Program.cs](https://github.com/Dispersia/TestTernarySpeed/blob/master/Program.cs) Benchmarks: Method | Mean | Error | StdDev | --------------- |----------:|----------:|----------:| IsNullOrEmpty | 0.4197 ns | 0.0457 ns | 0.0836 ns | IsNullOrEmpty2 | 0.5269 ns | 0.0331 ns | 0.0309 ns |
Or even need.
Not quite. You have to scale and patch the underlying EC2 nodes, or in our case, we just destroy old unpatched nodes and add new patched nodes. They do have a more managed offering where they take care of even that, but we don't use it. Since we use Docker on top, the applications themselves carry their own execution environment, so you have to update them. Since we do weekly deploys, its as simple as just updating the Dockerfile. Deploys are really nice though. It takes many minutes to deploy our legacy .NET FX apps on IIS (webapps) or Topshelf services (backend services), but our .NET Core apps running in Docker are like magic and deploy FAST. It's a long slow migration, but its been well worth it so far.
I feel the best way to improve is to read more code. Kestrel is well-written too.
It is. But for edge behavior it's always nice to have solid documentation. And pipelines right now has nothing.
Recently someone asked me a similar question. I outlined a good solution at https://gist.github.com/sayedihashimi/813f42faa0ac4afeb5c617f72aad6909. If you check it out, and have questions please let me know. 
[dasblog core](https://github.com/poppastring/dasblog-core), I don't think scott hanselman uses it yet for his blog though 
except its experimental only. I wouldn't call that "releasing"
Is that Blazor? heard about it but never seen code for it. If so thats pretty sweet.
Wow, i did not know all this existed. I am üë§ ng it tomorrow. 
Excellent! Too many of the ‚Äúcheat sheets‚Äù out there do not fit on one page, which breaks the definition IMHO. This is perfect. 
Thank you!
It's hard to beat octopus desploy. They just released 10 dollar per month cloud solution which looks amazing. 
What he is suggesting is replacing the implementation for specific methods at compile time, not runtime. It would modify the resulting IL and pdb files to include the patched method(s). The original methods would no longer be in the resulting patched DLL (or maybe they could be with a different name scoped in a way that only the patched method could call the original). Version of a DLL would have to match the original, and none of the public APIs can change at all, and you can probably completely forget about strongly named assemblies working at all. Patching system types could theoretically work for .NET Core self contained builds, but that doesn't sound like a great idea. \&gt; is it just me, or making a custom version of the library is not that bad at all? That is fine if you are providing a PR and watching for it to get merged so that you can get rid of your custom version.
Proxies work great for adding wrapping logic, or modifying/translating inputs or outputs, but can't modify the code inside of a third party library without replacing it entirely. OP is suggesting selective replacement at the method level, which is much more fine grained and productive than what we can do today with existing patterns.
C# has a long history of reflection, dynamic code generation and production support tooling. What OP is suggesting is a compile time tool for patching DLLs included in a build...what does that have to do with Ruby/Python?
Cool sheet! Have you considered creating / contributing to something like [https://devhints.io/](https://devhints.io/)
I believe there is also "dotnet exec" which you can run on an assembly you have built.
thank you all
The application logic on your web server, wherever you decide to put the code might go something like this: If the username exists in the database, then Hash the password given and compare it to the hashed password in the database that is linked to the particular use. If they match, then Generate a token return the view and the token. When the user sends the next request, they use their token to authenticate instead of having to give their username and password again. After a certain amount of time, the token expires for security reasons. The token also can act as a unique key identifier so you can select data that‚Äôs related to that user in the database the next time around. 
A better sub for this type of post might be /learnprogramming
You are the best!
This is golden! Thanks!!!
Thanks. It is good to see nice comments.
Thanks a lot. I will create a request for [@rstacruz](https://ricostacruz.com/) but I saw that the site has a lot of requests. Maybe I can contribute this one to devhints.
Actually, I did not know it too before preparing it.
**ZetPDF.NETSDK is a versatile PDF component suite for .NET developers to implement PDF-related tasks in their applications. And display and print PDF documents on any modern browser. For more info visit here:** [**https://zetpdf.com/**](https://zetpdf.com/)
Once you have an adapter or proxy in place you can selectively override the methods which call suspect 3rd party operations with your own logic, via inheritance or strategy pattern or chain of responsibility or whatever else that makes sense for your needs. Functionally it would be the same as monkey patching, it just needs to be designed with that forethought.
Good catch! There is a discussion going on about 'dotnet exec' and it is still not in Microsoft Docs. [https://github.com/dotnet/cli/issues/2243](https://github.com/dotnet/cli/issues/2243)
Yes, totally agree. Thanks.
I like it, but it feels incomplete. E.g. I typically write \`dotnet publish -c Release\` to force release mode since the env var is set to Development all the time.
Thanks. 
When utilized in the right way you could even allow your callers to choose which version is used, or define their own, which allows for an easier transition when the 3rd party fixes their bug.
Try the course series for ASP.NET Core https://mva.microsoft.com/en-US/training-courses/aspnet-core-beginner-18153?l=VM5gy36dE_6611787171
Thanks I saved it and gonna print it. :D
Your arguments for why the status quo is bad, are bad. Your arguments for why it would improve the situation are bad. Just build the damn thing, fix the problem and post a pull request. Use the local dependency until they get their shit together, or dump them. Worst case rip the bit of code you need and screw them. Not everything revolves around GitHub and Nuget.org. You can set up your own Nuget with a simple drive mapping. The one possible use of monkey matching might be mocking, but in that case your code is almost certainly "wrong" it's moot. Never hated a suggestion so much, since that other guy with the stupid suggestion. There is literally nothing here we can't do with a facade ot adapter or something like that. Even the mock libraries already provide this exact functionality by allowing the mocked object ot pass through calls to the actual code. You'd argue "That's a mock", I'd argue you're a mock and stick my tongue out. 
The example (and solution) is contrived. Consider the case 'IsValid' is inspecting some private state of 'BadLogic'. There is no way to wrap it, because the state you need to inspect is private. I don't think monkey patching is a reasonable solution, but you can't 'wrap' away the bug.
I did the opposite, it's been 8 years till now, well... the first and most important thing, you will see many people in the dotnet world advocating specific design patterns/architectures, the most popular ones are the repository design pattern and microservice architecture. I would say learn those but don't fall like many others just because they seem fancy you might not need them. In Django, most of the stuff comes out-of-the-box, it's good and bad... good for agile dev, bad for not learning about how things work. Focus on the concepts more than the technologies: 1. Instead of thinking about how to implement a reservation system in ASP .NET Core, focus on how the reservation system is different in MVT vs MVC. 2. Instead of thinking how to persist data for the reservation system, focus on why EFCore ORM and Django ORM are built on the Martin Fowler's Repository and Unit of Work patterns. 3. Don't focus on implementing RESTful API in aspcore more than understand what RESTful really means (based on Roy definitions). 4. Here is the best part, learn TDD where most developers love it (ASP)... I've never seen any course for Django TDD, but you will see dozens for ASP. 5. Don't forget to get the enterprise-thinking, cuz most enterprise companies prefer C# and Java, so here is your chance to go enterprise. I would say this is the best course so far (covering most important topics by the almighty Scott Allen): https://www.pluralsight.com/courses/mvc4-building *P.S: it's for the old MVC4, but it has essential concepts than any dev would need. After you get the concepts in MVC4, you can switch into aspcore (the same great author): https://www.pluralsight.com/courses/aspdotnet-core-fundamentals While you watch the second course, don't waste your time thinking and dreaming... start your REAL project directly !
I am not sure if this functionality is baked into the identity framework. However, I would store the admin id used when creating a user account - just add another string or guid for admin id to the user model. Then whenever modifications are being made to the user by an admin, check first to make sure it is the right admin.
Your post led me down this rabbit hole that made me discover I don't need to build and publish and also that I was using the full docker sdk image in prod instead of the 1gb smaller runtime image. Thanks!
You can try Zetpdf sdk for pdf creation, edit and view. It is simple to use and provide great user experience. Follow this link : [https://zetpdf.com/](https://zetpdf.com/)
Thank you sir! That's a great suggestion. Unfortunately this is setting off a upgrade cascade. Oh, you want to get into react... you're going to have to really work with Node.js, Git and Gulp. Oh and you'll have to get into ES2015 and Typescript, and to use Babel or another transpiler to compile your javascript. Lots of tech that I need to know before I get started on react.
I recommend using a build server and a deployment process. You should take a look at Microsoft's VSTS - I've been using it for a few weeks now and it's very easy to use. It has both build and release features. My current setup builds and releases to Microsoft Azure. You could set up something similar.
There are some good options in the comments here but I'll add one more based on personal experience: Cloudscribe ([https://github.com/cloudscribe/cloudscribe](https://github.com/cloudscribe/cloudscribe)). It is a web site/blog/cms engine - you can choose noDB (ie just json files SQLite or a traditional database engine like sql server). I've been impressed with it so far. It's easy to use.
This article from 2016 is satire, but with enough truth that it hurts... [https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f](https://hackernoon.com/how-it-feels-to-learn-javascript-in-2016-d3a717dd577f)
That was hilarious and brutal. I'm going to send it to my manager and tell him this is the suggested approach...
Is xUnit the recommended way to test for dotnetcore? I also see mstest under dotnet new but don't read much about mstest.
This is great, thank you. I‚Äôll be printing this out for the wall by my desk :)
The only reason I ever used MSTest was because it was part of VS.
I use xunit personally. I think they're all pretty similar. A question is should you be using fluent assertions and the answer is yes because it's awesome. https://fluentassertions.com Some comments from another post. https://www.google.com/amp/s/amp.reddit.com/r/csharp/comments/4198ei/should_i_use_nunit_or_xunit/#ampf=undefined
**ZetPDF for .NETSDK is a PDF document creation component that enables your .NET applications to read, write and manipulate PDF documents without using Acrobat. Get more info visit here:** [**https://zetpdf.com/**](https://zetpdf.com/)
Automatic Engineer bruv. Well done bruv. 
&gt; if the writer is out-pacing the reader, such that the pipe is filling up with data that isn't being cleared by the reader, it can *suspend* the writer Does this means that code like one that's shown in the post, where reader only starts after the writer has finished can lead to deadlock?
Check if you have Anonymous Authentication enabled? Not sure. Just a guess
Came to offer the same advice. Storing the manager id on the user is there best way to go.
**Generate High Quality PDFs. ZetPDF is a .NET SDK is the next-generation multi-format document-processing component suite for .NET SDK for adding Pdf. Currently, it supports PDF files, bookmarks navigation, adding and removing file attachments and other features. Get more info visit here:** [**https://zetpdf.com/**](https://zetpdf.com/)
I am not sure if this can be accomplished any other way. I am curious to see if this functionality exists in the framework.
Unless you need a specific feature, there is no real reason to choose one over the other.
That [deleted] guy nailed it.
Be careful with EF's "in-memory database", it doesn't always act like a real db would. https://github.com/aspnet/EntityFrameworkCore/issues/2166
A token is just a key passed along with each request; used to access protected content. You have a separate layer in your app that validates the keys. This layer either lets the user in the app or rejects them. There are a number of ways to implement. I would suggest researching JWT. You can start here: https://jwt.io/ 
I recommend using the SQLite in-memory database. So far it's worked for me. It has Foreign keys. It does not support schemas so you can't have tables named the same across schemas. 
Yes. They both have the same virtual path in IIS. What's odd is I have another project, we'll call it Archive that also gets deployed inside the Dashboard directory like the new app, the odd thing is that it's web.config doesn't have any problems including it's connection string. It's just a drop and deploy. So I'm not sure why new app is having this problem when the Archive app doesn't. I'm guessing it's because it's in the same solution. I'm wanting to break these out, and if possible have them in the same virtual path, but the new app and Archive would be deployed side by side with the Dashboard app instead of inside it. But they both use the login creds from the Dashboard, so I've got to look into that. Basically, due to the way deployments work (i have no control over this) I would have to deploy both Archive and NewApp each time I deployed Dashboard because they're nested in its directory.
Interesting. That's definitely something I'll look into. As I stated in another reply, the odd thing is there's another app Archive, that exists on the same site, AppPool, IIS virtual path, but it has it's own solution. This app has no problems with its web.config or connection strings. It's baffling to say the least.
Thanks. Maybe they should have some built-in support for things like this. Its not very nice cross-platform when your code is full of if(os1) { Do1(); } else if (os2) { Do2(); } else { TryDo00(); }
Yes, I was expecting that too, but got really supprised with the zone not found exceptio. I hope they fix it in the next release 
Did MSTest ever add a way to inline data that is simple to use? [Theory] [InlineData(nameof(Users.U1))] async public Task User_Login_Works(string u) { var user = Users.GetUser(u); await api.Login(user); ... } 
In .net core you can create [razor pages](https://docs.microsoft.com/en-us/aspnet/core/razor-pages/?view=aspnetcore-2.1&amp;tabs=visual-studio). No need for MVC and all the fancy database stuff, but it allows you to create reusable components (such as the menu, footer etc).
&gt;Let's say you chose to use a Nuget library for a critical task. Time is of the utmost importance, and blocking issues are not an option. You've done your research, investigate the source on GitHub, reviewed the license, and even reached out to the author. You're feeling great about your decision. &gt;You begin to write your mission-critical code, and you realize there is a bug. This bug is easily fixable, but what do you do? If you have invested this much time in it already, you should do the standard/correct thing (righteous is a bit strong) and contribute a fix. If the fix isn't accepted quickly enough for your liking, fork it and state clearly that you have forked it to fix a specific bug and until it gets merged. Given that you have already taken some of the author's time when you reached out, maybe you could reach out and offer to compensate them for the time it would take to get your fix in. If this is really mission critical, maybe you have knowledge that would help the initial author improve the library.
I haven't looked at MSTest2, but the original certainly didn't. (There was a way to do it, but it was as clunky as hell.)
&gt; but still very readable Thank you :) It's called _"Hyperlambda"_, and it's a (very thin) abstraction on top of C#. The **[create-widget]** above is an _"Active Event"_, which is declared roughly like the following. [ActiveEvent (Name = "create-widget")] private static void create_widget (ApplicationContext context, ActiveEventArgs e) { /* e.Args contains the arguments, which is a graph object (tree structure) based upon Node.cs */ } The other parts are simply arguments to that C# method. This allows you to dynamically declare _"method invocations"_ through a simple relational file format (think JSON'ish or XML'ish). The paradox is, that even though it's literally just a simple _"markup file format"_, it's actually Turing complete, since it also contains Active Events such as **[if]**, **[for-each]**, etc ...
Clunky and sketchy even if you did get it working.
Is there a need to do it in a different library/framework (other than learning, I mean)? Given the complexity, it might be easier to starting off adding one new piece at a time. So maybe stick to jQuery and Bootstrap but use Typescript now. Once you are comfortable, maybe you could look at React or Vue or something else.
I like it when I need to write integration tests. There is a huge difference between `Assert.Failed("your code sucks")` and `Assert.Inconclusive("plug in the database server")`
I like being able to Assert.Inconclusive. I had a look at modifying the xunit asserts to support messages, but seems like a lot of work because the Assert Exceptions don't support the "message" parameter.
What I do for XUnit is download the source code for the Assert library and include it in my test project. Then I fix the worst of the mistakes like missing Message parameters.
I had a look, but got the impression that even if I extended the asserts, it was going to be a pain to get the messages into the exceptions.
You can do this with policy based access control. Take a look at PolicyServer from the ThinkTecture team https://github.com/PolicyServer/PolicyServer.Local/blob/dev/README.md
Maybe. I forgot exactly what I did.
[Part 2](https://blog.marcgravell.com/2018/07/pipe-dreams-part-2.html) was just posted -- it seems like they're working on some higher-level abstractions, they just didn't make the cut for .NET Core 2.1: &gt; Here we need a bit of caveat and disclaimer: the pipelines released in .NET Core 2.1 do not include any endpoint implementations. Meaning: the Pipe machinery is there, but nothing is shipped inside the box that actually connects pipes with any other existing systems - like shipping the abstract Stream base-type, but without shipping FileStream, NetworkStream, etc. Yes, that sounds frustrating, but it was a pragmatic reality of time constraints. Don't panic! There are... "lively" conversations going on right now about which bindings to implement with which priority; and there are few community offerings to bridge the most obvious gaps for today.
I always thought that if you are going to write an article trying to explain something programming related you should try and do it in less words than MSDN/wikipedia page on a subject or at least in a simpler terms. What the fuck is this one? War and Peace on IO systems?
I sent the article the other poster suggested to my boss and he had the same reaction... Isn't that a lot of work to just get functionality we can get with bootstrap and jQuery? I got it in bootstrap in about an hour. Where I spent the last day trying to get up to speed with react, node, git, Babel, es2015...
I just spent a month wringing an ssh tunnel out of my network team access Cosmos. Guess that was pointless.
This isn't a "baby's first design pattern" puff piece. Don't shit on somebody else's work because it covers an advanced topic you have no interest in learning about.
I've just built such a thing for a project at work : it takes a select query of an arbitrary type as input and an arbitrary key expression (containing properties of that arbitrary type) and outputs a groupby query like `query.GroupBy(T =&gt; {TKey}, T =&gt; {T - TKey}).Select(g =&gt; new T { ...g.{TKey}, ...g.Sum({T - TKey})}`, that gets correctly translated into the correct SQL by EFCore for any T. Using expression trees isn't too hard once you understand how they work, the tricky part being, like you noted, to build the {T - TKey} type at runtime. It's actually pretty easy to do with `dynamic`, but sadly it only works in memory and can't be translated by EFCore. I found on SO some guy who wrote a 300-400 line anonymous type factory for that exact purpose that i copy/pasted into my program and it works perfectly. It is still a bit of a hassle to use because you can't use any generic method directly with your runtime type, so you have to actually use reflection (again) yourself to find and build the correct typed overloads for Select and GroupBy.
This isn't even particularly long lol. Just a tiny hint of more depth than a "getting started" tutorial. 
Looks interesting. I think the static site generator thing looks like the go tho. Thank you!
I would agree you should focus on learning c# first, and then MVC later. As the other commenter said, if you dive really deep into .NET MVC, you find yourself going through the service layer + repository layer and everything. There's a great website that helps you create a boilerplate MVC project called: aspnetboilerplate.com. But it is INTENSE. For someone just starting out, it is absolutely massive - way too much to learn up front. The benefit is that you can build really robust, testable, and reliable enterprise web applications. Django is great - rapid development - agile, but you do sacrifice some maintainability for large projects. 
I'm considering using it myself for my personal blog because I have the same hangup that you do with umbraco and databases 
I cant imagine anyone using so many different APIs to process an order in such a way. Lots of code that isnt all that useful. Is this a real world or theoretical example? And the font colors need more contrast with the background.
I agree. Deploying manually is error prone, dangerous, a waste of your precious time and not scalable.
AdaptiveClient is quite interesting - I'd love to hear about some use cases. With our fallbacks, it's usually a load balancer handling that logic and just redirecting to a working cluster. I am sure there are use cases though where you test if one resource is up, and if not, use another, but drawing a blank.
It's really up to you. You can self-host it in your API, on its own, in a webform project, doesn't really matter. I put my hubs in the same project as my webAPI so the hubs can share some of the same functions as my API users, with the added benefit of being able to push to them whenever I need to.
This is really short, considering the topic he's talking about.
Pretty much what I thought. Just wanted to make sure there was no nuances. How would that work with load balanced api servers ? The api call would be round robin but the signalr connection is persistent with 1 server ? 
We used a redis signalr backplane to scale ours
That is what a plan on doing. One redis sub pub server While scaling out the signalr hub instances that listen to a single redis server. My question is then I would basically have to host my signalR hubs separate from my API to accomplish this? 
Your User ID element: is it a single column in the table, or is it that it can exist across multiple columns? Not very knowledgable of EF, but it should work similarly to LINQ. If it's single column, _context.Users.Where(c =&gt; c.id == desiredId); this will bring back all users with the id column == desired id that you're looking for. If it's multiple columns, _context.Users.Where(c =&gt; c.id == desiredId || c.id2 == desiredId || c.id3 == desiredId) and so on I may be missing something. 
it's single column. I'm trying to route it to a different view if that's possible? (I just started learning .NET MVC last week because they're requiring me to use it in this project of mine). This is a snippet of the code I'm using atm. public ActionResult ViewAccount(string UserID) { var id = db.UAR_ReviewItems.Where(s =&gt; s.UserID == UserID); return View(id); } 
This is a sample of the opening page. The plan is when I click that User ID at the bottom, it should query it to my database and find the other User IDs with the same User ID. In the second picture, I'm showing the URL of the supposed page if I click the User ID. I'm thinking how could I configure my view to receive different User IDs and display the information I need from my local DB. Pic 2 Pic 1
New development with AngularJS rather than Angular 6? Why?
To better isolate the optimizations in the test code itself, how about changing the input string to read from the parameter, like: [MemoryDiagnoser] [DisassemblyDiagnoser(printAsm: true, printSource: true)] public class MyBenchmark { [Params("test string", "again")] public string TestString; public static bool IsNullOrEmpty(string value) =&gt; (value == null || 0u &gt;= (uint)value.Length); public static bool IsNullOrEmpty2(string value) =&gt; (value == null || 0u &gt;= (uint)value.Length) ? true : false; [Benchmark] public bool IsNullOrEmpty() { return IsNullOrEmpty(TestString); } [Benchmark(Baseline = true)] public bool IsNullOrEmpty2() { return IsNullOrEmpty2(TestString); } } Result: | Method | TestString | Mean | Error | StdDev | Scaled | ScaledSD | Allocated | |--------------- |------------ |----------:|----------:|----------:|-------:|---------:|----------:| | **IsNullOrEmpty** | **again** | **1.0000 ns** | **0.0248 ns** | **0.0232 ns** | **2.80** | **0.09** | **0 B** | | IsNullOrEmpty2 | again | 0.3574 ns | 0.0101 ns | 0.0079 ns | 1.00 | 0.00 | 0 B | | | | | | | | | | | **IsNullOrEmpty** | **test string** | **1.0108 ns** | **0.0172 ns** | **0.0161 ns** | **2.88** | **0.18** | **0 B** | | IsNullOrEmpty2 | test string | 0.3522 ns | 0.0244 ns | 0.0228 ns | 1.00 | 0.00 | 0 B |
This blog is amongst the most unfriendly websites I've seen in a good while. It took over a minute to load the scripts, and in the meantime it was just showing a gif with gears spinning. https://imgur.com/WCj0rMa
I'm new to AngularJS. This is the easiest way i found to show how AngularJS work with ASP.NET. :)
Geez. Just use Medium if your blog is so slow. * ASP.NET for a simple blog * No HTTP/2 * No HTTPS * No CDN * 5MB page * 10s to 20s to download everything * Genius URLs like `http://www.samwheat.com/api/api/Blog/GetCommentsForContentItem?contentItemID=15&amp;noCache=043802108741487755` 
Your wording is throwing me off. When you say "element" do you mean a column in a table? Your database table has 3 columns that each hold potentially the same value? You would like a page to have a hyperlink representing some value, and when a user clicks it you want to display a page displays results of any record in the table that has that value in any of the 3 columns? Also, is "User ID" a table? record? column? Seems like you're using that name to mean different things.
Can't see any pictures. Can your first page get a Distinct() list of emails, then using that as a parameter to your second page, grab all the emails that match?
Ah the joy of working with TimeZones... :-/ [NodaTime](https://nodatime.org/) might help in future. It overcomes a lot of the short comings in the .Net Date/Time API. From my understanding it uses the Olsen DB notation for TimeZones (eg. Europe/London) for everything but will convert these to the Windows ones when you're running the code on Windows.
Attribute validatiors for most things, it's simple and clean. Usually for more nuanced validations we'll forego custom model binders for more direct approaches. Like when properties depend on each other we'll write an error message into the ModelState before validating normally.
A mix of annotations for simple stuff like required, and FluentValidation for more complex things. (But I can see why people might want to use just one of them to keep everything in one place) Then we use a middleware to auto validate before the data even reaches the controller and generate a validation error information which we return as a JSON with a &gt;500 code.
Is anyone running this in prod? Strongly considering migrating our php wordpress site to dotnet core. 
In .NET 4.6/4.7 on Windows, you still have the option to use the Olson notation. There's a method to convert to the windows timezone using `TZConvert.IanaToWindows()`. Going from Windows TZs back to Olson TZs is more difficult because multiple Olson TZs match Windows TZs (the Windows TZs are more coarse).
I want to say we're using Jekyll + Netlify, but I'm not involved with the project. It's also possible that we have a build service that does the work to creating the static site files. There's also GitHub Pages which supports Jekyll. Then you just push your files to your GitHub repo and it will build/deploy.
Static site generators are also nice because you can write your posts in a variety of markdowns (I usually just use markdown, but HTML is supported as well).
From one of the comments on the article: &gt;That's awesome! But can WP modify its files? People often use WP admin editor to tweak theme files, upload stuff, etc. Is that still possible? Also, im guessing auto-update will not works, since it needs to be recompiled into dll. I'd also be worried about those issues.
I think Hugo seems to support that also. Haven't quite got it working properly, but seems to have options for push deploy/build... One minor problem is I'm with bitbucket :| But, we'll see....
I feel like I'm always dealing with configuration issues with umbraco/databases...its always SOMETHING!. I've been looking at the static site stuff with what little time I have, and I sort of suspect its going to be the same thing, so many different technologyies and fingers at play....Part of the problem is that I don't do it full time and I don't do hundreds of sites......Umbraco Cloud would solve my problem but for a smallish personal site, it's more expense than I wish to pay :(...
Yeah that comment stood out for me as well. I also have a load of WP sites I'd consider moving but the ease of doing things in the admin is one of its USPs.
Thank you for the response, I did what you suggested and it worked :) after adding a string to the user model a SQL migration should be done first from the package manager console : PM&gt; Add-migration Give_it_a_name //This will generate a database script file PM&gt; Update-database //Run script file against database. then you're good to go. Cheers.
Take a look at this article - [https://www.strathweb.com/2013/01/asynchronously-streaming-video-with-asp-net-web-api/](https://www.strathweb.com/2013/01/asynchronously-streaming-video-with-asp-net-web-api/)
I wouldn't think there'd be anyone insane enough to do that. This just seems like an added layer of complexity to the hot mess of Wordpress.
Thanks! We're looking into that right now.
I‚Äôm still reading and thinking through this... I‚Äôm not yet convinced this absolves any security issues in PHP or in Wordpress. In fact it makes things worse as any code change or patch has to rebuild a dll.
Wow. I will have to try this. Sounds too good to be true
Apologies for my website. I am on the dirt-cheap plan so you had to wait for the VM to spin up too!
Guilty As Charged LOL!! You can try [this link](https://github.com/leaderanalytics/AdaptiveClient).
This is working code in at least one enterprise app. You can find a fully functional demo app [here](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon).
The classic use cases, which are the use cases that drove the design of the utility, are LAN connectivity and database provider. We have an app that is data intensive and we want to make in-process calls to the database. There are however, many remote users of the app who only have internet connectivity. In this scenario when the user starts the app AdaptiveClient attempts LAN connectivity and falls back to WebAPI if it fails. The other use case is database provider where we have users who are already using one platform or another and want to stay with that platform (for example MSSQL vs MySQL). Our app (still in development) will work seamlessly with either one and is dead simple to write. Entity Framework also makes this task much easier. 
Most of the WP exploits I've seen modify / add php code directly to the files which of course would not be a problem with a compiled .NET app. Obviously this would negate and entire type of attack. I don't think the server would even need PHP at all. But of course this would not mitigate all exploits. 
Except maybe the folks at peachpie.io: https://www.peachpie.io/2018/03/peachpie-on-peachpie.html
You don't use work locally or use source control?
Like the x commenter said, WordPress is often used in a way where everything, including security updates, is managed through the application. Having to recompile the binaries every time there's an update would break a lot of people's work flow. 
I know lots of WP and PHP has been subject to remote include issues though. Would something like a buffer overflow in PHP itself be mitigated though? Would the compilers IL or the end binary still have the buffer overflow? Not sure why I got downvoted. This seems the type of questions to ask.
DataAnnotations for most things. In-controller IF..THEN checks for the rest. 
I would recommend learning Angular 6 instead. Almost no one does new development with AngularJS anymore.
After some I've made it work and just wanted to share my experience. How I did it: * NET MVC with windows authentication * The user is in the page that has the inputs to be filled by the Card Reader * This page has a hub (signalr can call) * WPF application * This application is in charge to read the data from the card reader, it is hosted in the pc that has the card reader attached. * When the card is well read it calls the .NET (see 1¬∫ topic) endpoint that receives the information, processes it, and if all the info is correct sends it to the correct user. * This is done thanks to signalr and windows authentication. * In the wpf application i can get the user information and send it with the credit card info :D Well.... that's it :)
You‚Äôre welcome! Just found this very post on LinkedIn too. Turns out we work in the same city, small world. 
Do the ControllerBase::File(virtualPath,...) methods give acceptable results for files on disk?
Sounds like the opposite of enterprise trends, where we are all pushing towards microservices and distributed applications. Whatever you end up doing, it's going to be a lot of work reengineering these apps.
&gt; We get about 200 unique visitors a day Not today, guys. Not today
I'd say, for a start, that doing it in small increments has a much higher chance of success. For the implementation itself it's hard to tell what's best without knowing what all those applications are doing.
Can't you just CopyToAsync a FileStream to the ResponseStream?
As someone who has been through many projects like this in the real world, you are looking at years of effort to achieve this goal. There is no one size fits all way forward -- just prioritize.
&gt; I don't think the server would even need PHP at all. That is correct, at least according to their webpage. All you need is ASP.NET
Looks that way Server: Kestrel X-Powered-By: PeachPie 0.9.0-CI00996 X-Powered-By: ASP.NET
Out of curiosity, Why are you still using such an old version of the framework for a server side app where upgrades are basically painless?
Just fixed that; the project has been idle for a few years. It was a startup I worked on for 18mo, but has been idle. I did upgrade the code to 4.6.1 last year to keep it somewhat current.
I'm curious about this too.
Nice. Just nice. NuGet team if you're reading this. Try to reach out for feedback (especially for repeatable builds). I'd love to funnel some private time into helping testing stuff, maybe help with newcomer issues. Some of this stuff is desperately needed and I'm all hyped for these changes.
First, Microsoft has made it clear that Framework isn‚Äôt going away and will continue to be supported for quite a while yet. But the possible drawback is that Framework will only get any possible changes on a Windows schedule. Core on the other hand will advance much faster as it will not be constrained by the Windows release schedule. So it depends on what you want or need. If you want the ability to adopt new language or other API changes as they become available then Core will be a better choice 
Maybe they should see how Umbraco does it. It's a .NET CMS that you can update from the admin panel without touching Visual Studio or similar.
Meh. No mention about DTO to model mapping
&gt;I've already seen some limitations in .NET Core-compatible packages This is the reason not to go full .Net Core yet. .Net Core 3.0 promises more coverage for existing libs, but it is a ways out. You could go with a half measure now: using the .Net Core build system but target the full .Net framework. Makes it easier to do something like an ASP.Net Core site that references full framework stuff like EF6 (it is a scenario MS explicitly refers to in their docs). The benefit is that you can take advantage of some .Net Core stuff now, and are ready to transition to full .Net Core in the future when the libs you need come to .Net Core.
Yeah, when I started the original project a few years ago, I was gung-ho on using .NET Core but quickly realized it wasn't ready. (I was even using the project.json model before they reverted back to .csproj.) I was thinking about porting it over now that 2.1 is out, but it still seems premature for what I'm doing. Hopefully I ship this new pivoted project before .NET Core 3.0 is ready :) To my original point, I'm just not sure what .NET Core buys me. I'm happy to deploy on Azure Windows web apps anyway, and Docker/Linux isn't a concern here. I was just wondering if I was missing something which would tip it over the edge this time. Today I just updated everything to .NET 4.7.2, which wasn't too painful.
I'm not sure why you would want a single huge app. If it's duplication you are trying to avoid, try identifying common code and seperate it into shared assemblies. You can even setup your own internal NuGet server to distribute conponents to your apps. This is something you can do gradually over time.
Fluent validation all the way. 
Shit, not having to manage individual files in the .csproj files is enough reason to justify that. My merge conflicts have dropped significantly.
Not a problem for routing to a different view. For each row, you'll be foreach through the @model. On that column, it should look something similar to this: @foreach (var currentModel in model) { ‚Ä¶ other columns &lt;a href="@Url.Action("ViewAccount", "ControllerName", new { UserID = @currentModel.UserID})"&gt;@currentModel.UserID&lt;/a&gt; } 
Guesswork here, but first page has all user ids. Have a default page for this, and have links for each id that redirect to the page with the other ids. The first page will get a list of ids as the model (with any other info you need) then you foreach through them to create the list for the user to click on. Then use the Url.Action I put above to move to the other page where you show the details of your new query that finds all the other Ids associated. 
Yeah, I'm using the new project model with .csproj. So much easier. Seems there isn't an easy way to create a new .NET 4.7.2 project with the new project model, though? I just created some new projects, and had to start with .NET Standard class library, and then edit the .csproj to net472. My old ones had been upgraded over time, and I didn't realize it was tricky to create a new one. Maybe I'm missing something (using VS2017).
Framework is maintained by DevDiv, not WinDiv. It‚Äôs released on the same schedule as VS. UWP/WinRT are maintained by WinDiv and released on the Windows schedule. VS is released faster than Windows. 
I definitely will. Thanks.
I think the idea of AdaptiveClient is cool. OTOH, it's got some rough edges: - Locked into Autofac... I mean, It's my second choice for DI behind SimpleInjector. But I dislike that I'm locked into it, and have to keep my version of Autofac lock-stepped with it. - If I am to read the documentation correctly, using the `Try/TryAsync` methods in such a way that the call fails due to a logic error (let's say, a chance perfect storm of bad data in production), I'll wind up with every endpoint getting invalidated. Unless the documentation is off. That said, I'm having a hard time figuring out WHERE the invalidation even occurs in the code in this scenario. - To that end, I feel like some reorganization would really help me understand the codebase of AdaptiveClient better. - You should probably document how to safely use AdaptiveClient.Utilities (specifically in an HTTP context.) If I recall correctly, [this will work if a status code and no content is returned](https://github.com/leaderanalytics/AdaptiveClient.Utilities/blob/master/LeaderAnalytics.AdaptiveClient.Utilities/Http_EndPointValidator.cs) but could cause issues if any data is returned (due to the lack of disposing any potential response streams.) It's a valid way to do things, but again, really should have documentation to avoid people discovering things the hard way. 
ASP.NET Core is much more performant than ASP.NET Framework. However, it is somewhat immature.
Sorry, but USPs? User Story P...Points? 
I didn't realize that MS changed the guidance for Kestrel for net core 2.0+,[because if you look at the 1.0 tab it says](https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel?view=aspnetcore-2.1&amp;tabs=aspnetcore2x#when-to-use-kestrel-with-a-reverse-proxy): "A reverse proxy is required for edge deployments (exposed to traffic from the Internet) for security reasons. The 1.x versions of Kestrel don't have a full complement of defenses against attacks, such as appropriate timeouts, size limits, and concurrent connection limits."
Turns out the problem was with having GZip compression enabled. Disabling it for the relevant controller actions solved the problem.
Turns out the problem was with having GZip compression enabled. Disabling it for the relevant controller actions solved the problem.
That's the hardest part
I felt like I'd just finished the introduction and the article ended
Indeed, that was the introduction :).
USP = Unique Selling Point.
The problem with Odata is persistence is exposed, it has its own Pros and Cons. Its good for intranet based application. If there is a mapping solution to map EF core entity to another it can be a great option to consider. 
What about if we use [ASP.NET](https://ASP.NET) Core on .NET Framework 4.7.2 ?
Yep and 2.0+ &gt; Either configuration‚Äîwith or without a reverse proxy server‚Äîis a valid and supported hosting configuration for ASP.NET Core 2.0 or later apps.
We are using component one, but graphs are done in a separate code, just inserted to a PDF. I am just saying - you can investigate on your own if it is something worth grabbing. 
I am using MigraDoc to generate simple table views, articles, paragraphs and ClownPDF (which i do not recommend) for merging many PDFs to one, with adding e.g. paging, headers.
In my experience, there is never a clear yes or no with these types of things. Just like with having a kid, the right time is never. The important things are: * how resilient are you and your project against the little frustrations that will surely pop-up when doing a port? Most of which cannot be planned for in advance. It always takes more time than you expect. * Do you have time for a port? You mention that it's a startup. In that case, your focus should be on providing user/customer value, not your stack being on the latest and greatest (although I understand the pull ;-)) * Learning new tech could be a reason in and of itself to do a port, unless it conflicts with the previous point. Regarding a "better future", all things being equal, I would focus on .net core.
Hi birmsi, The full approach to this would be enough to span several books :-). My initial take on this would be to look at your team. Do you have the manpower, time, budget and scale to go for a microservices approach, then go that way. But ONLY if you can answer all questions with a resounding yes. If your budget is relatively small, your team could do with a few more engineers or your scale might not really require microservices, that stay away and create a monolithic application. There's nothing wrong with a monolith, even if the current trend is all about microservices. Many small-mid companies are finding out however that a microservices architecture boils down to maintaining a plethora of little app's, each with their own specs, roadmap, bugs, etc, extrapolating the issues you might already be experiencing, just with a different name. In a way, microservices are for the 1%. If you don't have the resources, you're probably better off with the traditional way of doing things. Like others have said, it's going to be a lot of work either way. Good luck!
I would suggest to first learn c#. If you dive into mvc without a basic understanding of c# it would be like asking for directions in English while being in Japan. Sure they got roads, road signs and traffic lights in Japan, but you're making it really hard on yourself not speaking at least a bit of the native language.
When can we expect next article?
plz, use pastebin and link the code My eyes... ;-(
One of the worst parts, generating pdf... Really, people changes after writing this.. They are never the same after... Quick question - ever used LaTeX? Maybe it will be easier to prepare template in latex, feed it with data from c# and generate pdf via some script?
Hopefully, will publish them on a weekly basis.
Sounds good!
It would be nice to have the ability to filter by licence type. Most of the project I work on cannot use stuff with copy-left type licences. 
The thing is, an example: * One of the enterprises is in automotive business, if he user instead of accessing 5 or 10 different apps, the user would access to one platform and from that platform the user could access the seperate applications. * It's an attempt to make the access to the applications easier. * Example, the Automotive Platform would include the car avaliation, car stock.... all the apps that are related to the automotive business. * One of my fears is the time it would take to make one application with all the others inside of it.... even if i maintained the smaller applications and the platform called them via API i would still need to implement alot of connections and all the views again.
[PASTEBIN](https://pastebin.com/NDPZv2bW) you are welcome -.-
hopefull it was usefull :)
I've just scanned through it quickly. And please do what another poster said and use Pastebin or anything that maintains a bit of formatting. First, it's hard to guess what your throughput is, but trying to throw more concurrency at it isn't the way to go. So don't use Parallel.ForEach and have that use Task.Factory.StartNew (You have a comment saying "Start a new thread", and this is not what happens even if you specify LongRunning) in each. Then there's a call to .Wait which is blocking, etc etc. All your MSMQ calls are synchronous, i.e. queue.Receive (with a five second timeout!) Finally you're doing everything else in a while(true) which is like driving with your foot full on the accelerator and using the clutch to control speed. So, the basic approach I'd take to fixing this would be to separate the receiving, sending and processing using queues. Each message is received and immediately added to a concurrent blocking queue (Have a google. It's also called a producer consumer queue in some areas). The thing here is, you get the message asap, put it on the queue and then get back to receiving more messages. The queue produces "events" or callbacks when it has work to do, i.e. someone put a message on it. You can then control how many messages you're going to process at the same time. More is not better here as your computer can only do x things at once where x is the number of cores (keeping it simple ok?) So trying to spawn more threads hoping it will go faster actually makes it slower because it has to switch between them more frequently. There are other things that I wouldn't do personally. In an error handling block you create a List containing a new Dictionary which always contains two kvp, which seems like more work than it's worth. Feels like one of those methods someone added because "It will work for EVERY problem and never need changing!" situations. That's just the ones off the top of my head. 
You could move the tls to a proxy. Basically you would send plain http to a process on the same box that handles the secure connection. You would be sending traffic unencrypted between processes on the same box so consider that before diving too deep. Something like Envoy https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/ssl#arch-overview-ssl can handle this out of band. The only other thing I can think to do in process is open the connection as soon as you can and reuse the same connection for your requests. Do this asynchronously if possible so you aren't blocking for the tls connection to be established. Some kind of health check method could establish the tls without requiring any processing.
Sorry, I should be more clear. My comparison was based on ASP.NET 5.0 running on Framework 4.6 vs. ASP.NET Core runnning on .NET Core 2.0. 
Thank you for your comments and for taking a moment to reading up on what AdaptiveClient does. Regarding Autofac, yes AdaptiveClient is highly dependent on Autofac despite my best intentions to not do that. The flip side of the coin, however, is that Autofac does all the work. AdaptiveClient is little more than a wrapper that makes some registration calls. I would like to make AdaptiveClient work for as many DI containers as possible. I hope to get the general pattern behind AdaptiveClient vetted by the community before I start down that road. `Try/TryAsync`: Yes if your app fails at a low level everything downstream is going to fail. There is not really much AdaptiveClient can do about that. I have, however, deliberately avoided trying to build error handling into AdaptiveClient. There are other libraries, such as [Polly](https://www.nuget.org/packages/Polly/) that handle that task. Regarding the codebase and utilities: I will continue to document and improve as the library matures. Thanks for the suggestion regarding `IEndPointValidator` - I will look into disposing those objects correctly. Additional feedback regarding use, bugs, suggested improvements, etc. are welcome and appreciated. 
I'll look into it, thanks!
Much appreciated
I use Aspose but it requires that you buy a license.
For me, it's mostly about allowing people with odata components to use .NET Core.
I think that's a great idea. I've sent that post link to the team.
Thanks Max, I've posted the suggestion before, maybe someone will notice it this time :D
If you add an issue on GitHub for this specific feature, I'll help you back it up: https://github.com/NuGet/Home I think it's a very good idea and I don't remember seeing that feature elsewhere.
It must be able to modify it's files, even the basic installation/configuration process for wordpress modifies the PHP files, that's how it stores and accesses settings. 
Will there be an example project in github repository at the end of the series? I've always been interested in how microservices architecture works all together.
yeah what you described is how I create projects targeting new csproj even if I don't use net core 
We're using the same setup as you (including HighCharts), and we've been using WkHtmlToPdf. It's not without it's quirks, but we've managed to make it work. 
Sounds cool but it strikes me as one of those deals where its not worth the trouble. Reading the page on plugins gives me shivers as all plugins are not indicated to be able to run. Whats the use case that isn't met by running WP natively in PHP? or using a .net app instead? It is geeky cool though.
Are you trying to use Ajax call back to the API from the page or submitting to the log in controller then call the API ?
Agreed, I go back and forth on it, and if it's worth my time. Right now, I've upgraded everything to .NET 4.7.2, and going to hold tight on the full framework. I have a day job (in software mgmt), and still have this idle startup on the side - have been planning to resurrect the code, and pivot it towards a different opportunity. So I'm not yet to the point of customers, etc. but don't want to waste time on a sideways move w/ little value either. I took two of my projects and put them into a .NET Core project over the 4th, and the only problems I had were with some changes to the Azure Service Bus client - missing functionality, which I did find on Github they'll be adding back soon. These are some of my main concerns - I'd go down this road, and then be blocked by things only in 3rd-party packages which support the full framework. (Since I'm talking to Azure and AWS APIs among others.) I need to review again the whole .NET Core vs Standard vs Framework thing, and referencing one from another. I might be able to port some of them over, which have less external dependencies. (80&amp;#37; of my projects are class libraries, and they link upwards into an [ASP.NET](https://ASP.NET) MVC API.)
Its more that I want the two to be able to act independently... So users accessing the webform/aspx pages have to authenticate through the login page/forms auth first... and then the webAPI controllers can be accessed anonymously/remotely without ever touching forms auth or a login page.
All of the repositories are a big solution, that works together :).
All of the repositories are a big solution, that work together :).
This kind gentleman /u/elRand0m did the pastebin for me. [pastebin](https://pastebin.com/NDPZv2bW) The throughput is at most 10.000 messages for a couple of minutes. Mostly it's just 500-1000 messages, so nothing major. Of course, every message has some crud actions. Regarding the parallel.foreach: It's created so that all of the queues can run side by side, instead of waiting for the for loop to be over. As you can see a bit later on in the code, we use Task.Factory.StartNew to create 3-4 threads pr. queue. I am looking at a former employees solution, so I am not the expert on it. Thanks a lot for your insight. And I will be looking into the product consumer pattern. And what I am getting a bit at, is that we maybe are starting too many threads, slowing everything down. 
Sorry if this is covered , but I didn't see how you deal with synchronous data requirements E.G In a shop app , a checkout micro service may be handed (via http from a web app) a checkout request containing items 1,2,3,4. However the basket must verify the prices of these 4 items (a sneaky user may pass prices of 0.01! ) Do you 1) call a microservice from a microservice (chained http requests might get slow) 2) share a service or repository (doesn't feel very microservice) 3) Request via a bus, which is complicated, Im just starting on microservices , so maybe Im looking for a problem that doesn't exist :)
The basket (Cart and CartItem models) is being updated based on the products that come from Products Service, therefore, it's not possible for the user to change the price - just send the product id, quantity, get product info from particular service and update the cart. Calling service A from B is totally fine and that's how it works in the world of microservices (these are very fast, internal HTTP calls, that can be e.g. cached). On top of that, let's say service A may contain some internal data from service B (e.g. a list of products so there's no need to ask for a product data each time).
I feel for you, it's horrible inheriting code. Check out the docs on StartNew as you can't really tell it to start a new thread, the options specified sort of hint to the task scheduler that it might need to be a bit more generous with how many threads it allows because some of them might take a while to run, which reduces the availability of thread pool threads that are only expected to run for a tiny amount of time. Good luck :)
Why would you share the messages across all services via library? This introduces coupling between the miceoservices.
There are a few choices, given that at least some services will be written in the same tech: -common package like we did (easy to implement and work with) -package per service (loose coupling but more painful when it comes to development, versioning etc.) -no packages at all - just API spec and each service is responsible for defining its own contracts
thanks
please continueeee
No worries, that's merely a beginning :).
Yeah, but if it should be easy, a microservices architecture maybe isn't the best choice ;) A common package hinders independent development of different microservices. Which is one of the key benefits imho.
You're right, and we're aware of that, but it's always pros &amp; cons of different approaches - in our sample project (besides making it relatively easy to understand), we wanted to show some different techniques, as there is so many of them without a single silver bullet (usually). On the other hand, in the real world, 100% decoupled scenario, most likely you would implement the services contracts independently of each other.
I recommend NOT using referer for generating links. There is no guarantee that this header will be passed by the client. In fact a lot of clients block this header from being sent. I just use an appsetting for the servers hostname.
[removed]
&gt; Thank you for your comments and for taking a moment to reading up on what AdaptiveClient does. I've been using a similar pattern for some time now, although typically without failover; moreso for local vs remote application logic. &gt; Regarding Autofac, yes AdaptiveClient is highly dependent on Autofac despite my best intentions to not do that. The flip side of the coin, however, is that Autofac does all the work. AdaptiveClient is little more than a wrapper that makes some registration calls. I would like to make AdaptiveClient work for as many DI containers as possible. I hope to get the general pattern behind AdaptiveClient vetted by the community before I start down that road. Opinionated statement: I'd start looking into this sooner rather than later. A library *requiring* a DI Container is in many cases a design smell to me. There's nothing wrong with requiring a DI Container for more complex scenarios (i.e. If I'm dealing with things that can't be instantiated with a parameterless constructor.) But, aside from that, you're already doing things with Autofac that would require rework to use SimpleInjector. [SI doesn't allow registering by keys, for example](https://simpleinjector.readthedocs.io/en/latest/howto.html#resolve-instances-by-key). I'd suggest trying to get AdaptiveClient working in SimpleInjector; As you can see from the link they usually provide guidelines for how to make things work in their container. In my experience, if I can make a design work with SI, it should work with about anything (even Pure DI). Waiting longer to do this increases the risk you wind up putting in more things that simply won't work in certain DI libraries. Also, it'll probably come out cleaner in the end. You may very well run into some good-sized refactoring in this process, but my gut tells me the library will be the better for it. &gt; `Try/TryAsync`: Yes if your app fails at a low level everything downstream is going to fail. There is not really much AdaptiveClient can do about that. I have, however, deliberately avoided trying to build error handling into AdaptiveClient. There are other libraries, such as Polly that handle that task. If I may suggest a happy medium... Consider an overload for Try/TryAsync that has a parameter like `Func&lt;Exception,bool&gt; shouldInvalidateEndpoint`. OTOH, I'm not sure if I like that either; it's a tough spot to be in, since one could argue such error handling would be in the client. On the third hand, I don't see how Polly would help in this scenario; sure I could use Fallback, but then I'd start asking myself whether I should just write a Q&amp;D Fallback ruleset for the endpoints themselves... &gt; Regarding the codebase and utilities: I will continue to document and improve as the library matures. Thanks for the suggestion regarding `EndPointValidator` - I will look into disposing those objects correctly. Additional feedback regarding use, bugs, suggested improvements, etc. are welcome and appreciated. Thanks. I'm actually (mostly) impressed with the documentation/samples as they stand. But documenting gotchas like that (or, even better, not having those gotchas if possible) are critical to ensure that people have trust in your library. If I wasn't the sort of person to peruse library source and ran into a leak like that in production, I'd likely have a hard time finding where the problem is, and could wind up blaming AdaptiveClient as a whole. 
Html to PDF docker service 
Looks similar to BlockingCollection
FluentValidation. We've used both; and attribute validation gets painful as your needs grow/change. FluentValidator is more flexible to handle things like conditional validators and more complex objects.
We're you able to get your project on github?
The microservices articles that pop up on Reddit from time-to-time are mostly using Java (bleh) or Go (beautiful but niche-y). It's nice to see fully fleshed out one written in C#, using Core no less. Fantastic job!
I think I need about 10 more articles like this to cover all of my Xamarin forms debugging nightmares
if you need to be on windows, you won't gain a whole lot at the moment. perf is better in many scenarios, but it's not a night/day kind of thing. what core gives you is options. linux, windows, raspberry pi if you wanted to. plus stuff like lambda on aws. if you're intetested in moving, I'd advise to port as much as possible to net standard 2.0 first. This is what we do. All logic is pure net standard and only the entrypoints target a real runtime like core or framework. additionally, you could target aspnet core on top of the full framework. then you're just a stones throw away from making the full switch. core is the new hotness and is the future, but dont pull your hair out porting. you've got lots of time to do it gradually, and it will get easier as core matures.
Thank you, happy to hear that :).
are you missing specific apis/packages? i thought i saw service fabric was working on a net standard 2.0 port, but i could be mistaken. net core 3.0 seems to have a lot of focus on windows ui, i dont know that it will fill the specific gaps you have. it wont be a seachange like 1.x to 2.0 as far as compat. I'd bet the best thing you could do would be to modernize what you've got. get rid of old cruft, migrate to newer apis, and with that you'll probably be pretty close to net core compatibility anyway. 
Being on Windows is more just familiarity right now; I already have the CI/CD stuff ready for it, etc. Your point about moving to .NET Standard 2.0 is spot on, though. I'll try and move my class libraries to that, and keep the web services and Service Fabric stuff on Framework for now. I started moving a few class libraries to Core this week, and ran into some limitations with the Service Bus client. I don't have a ton of third-party dependencies (mostly Azure and AWS client libraries), so I'll have to just test it out and see how hard it is. May be able to refactor a bit, and keep those pieces in a separate class library. At least that's a step in the right direction.
It's less about what's in .NET Core than third-party packages I'm using. I'm only doing web services and Service Fabric, no front-end work yet (and will be using Xamarin for that). Good suggestion, thanks. I will do a bit of cleanup towards .NET Standard support, before starting new feature work.
I have researched this myself, and found that there are no really good solutions. iTextSharp for instance, which some would argue is the best, does not respect _"pre"_ elements. If your needs are not very advanced, you might want to try iTextSharp. If you'd like pixel perfect support for everything, you're basically out of luck, unless you create a **CTRL-P** solution ...
Where do I find that version?
Lol 
I'm using MigraDoc but it's been a pain, specyfing heigths and widths instead of 'wrapping contents' was a pain. Also problems with fonts when deploying to Azure Functions made us waste some time.
[Browserless](https://github.com/joelgriffith/browserless)
Can't we just admit it is plain magic and end the explanation there?
Conditional means that if a child update panel has an event it won't refresh. The update panel will update on any post back event on a control inside the panel that isn't inside another update panel.
Add [AllowAnonymous] attribute to the controller or even just a specific action on the controller.
I tried putting the UserControl inside it's own UpdatePanel but this didn't work either because I think I would need to put manual post back triggers on every link I come across which is not ideal. It would mean any deeper level user controls also need Update Panels and manual triggers, and GridView, generated controls inside of rows cannot be assigned manual postback triggers (synchronous). I would have the Placeholder outside of the UpdatePanel but it needs to be in there to refresh correctly.
You won't win the debate, as long as the project is up and running and not facing any business-impacting reasons (performance, more users... etc).
That does not seem to do it at all... This is my web.config: &lt;authentication mode="Forms"&gt; &lt;forms name="TestAuth" loginUrl="login.aspx" defaultUrl="default.aspx" protection="All" path="/" timeout="10080" slidingExpiration="true" requireSSL="false" /&gt; &lt;/authentication&gt; &lt;authorization&gt; &lt;deny users="?" /&gt; &lt;allow users="*" /&gt; &lt;/authorization&gt; And even with AllowAnonymous on both he controller and the action, it still tries to redirect requests to the login page.
Hello, Tks for the tip. Obrigado.
It definitely seems like a WebForms bug because I noticed that if I add an UpdatePanel to the UserControl, the problem still occurs UNLESS I stop trying to load and add the UserControl programmatically. If I just add the control directly to the page, and have an UpdatePanel in the control, then the event handlers are finally being used correctly (only if using additional triggers as I previously mentioned in my last comment). The problem is that adding the user control programmatically is a requirement for my project. I don't know how to get the UpdatePanel behaviour to work after doing this step. It's as though the scripting it setup only during the initial load which means any asynchronous tweaking between the UpdatePanel setup is ignored.
I think holding tight to the full framework makes sense for you at this point in time.
 It definitely seems like a WebForms bug because I noticed that if I add an UpdatePanel to the UserControl, the problem still occurs UNLESS I stop trying to load and add the UserControl programmatically. If I just add the control directly to the page and have an UpdatePanel in the control, then the event handlers are finally being used correctly (only if using additional triggers as I previously mentioned in my last comment). The problem is that adding the user control programmatically is a requirement for my project. I don't know how to get the UpdatePanel behaviour to work after doing this step. It's as though the scripting is setup only during the initial load which means any asynchronous tweaking between the UpdatePanel setup is ignored.
I'm curious about this as well, maybe it prevents the method from modifying properties and variables of the object? I looked at the docs and can't find any mention of it with objects
For anyone near the south east side of the US, codeonthebeach.com is in August, and I highly recommend it. It's an awesome jam packed weekend full of learning and fun! Check it out!
If you pass a struct in it is copied, so we had "ref" which allowed you to change the originating variable to a new struct, and "out" which required setting the variable (via the parameter). With in you can say, "pass me that struct by reference but don't change it". This assumes an immutable struct, you can still change the values of a non immutable struct, but that's usually frowned upon.
I believe the question is, what happens when you use `in` with a class?
Got that. But I apparently can set 'in' for reference types too. 
In your global.asax Application\_Start method, you also need to add this to make the \[AllowAnonymous\] attribute work on your WebApi: GlobalConfiguration.Configuration.Filters.Add(new System.Web.**Http**.AuthorizeAttribute()); if you need to do the same with an MVC Controller (not web api) then you would also need to include this: GlobalConfiguration.Configuration.Filters.Add(new System.Web.**MVC**.AuthorizeAttribute()); If that doesn't work, you can try using the location element in your web.config [https://msdn.microsoft.com/en-us/library/b6x6shw7(v=vs.100).aspx](https://msdn.microsoft.com/en-us/library/b6x6shw7(v=vs.100).aspx)
I think it's worse than useless, it can actually makes the code slower. If you use `in`, then it will "pass the reference by reference". Or in other words, a pointer to a pointer to an object. Since you know have two pointers to dereference instead of one, it will take longer to get to your actual data. **** Disclaimer: optimizations may eliminate the cost of the double dereference. 
There are a couple of situations where `in` parameters are either useless or possibly even harmful: 1\. The parameter is a reference type (class). In this case it is at best useless. 2\. The struct is not a `readonly` struct and you mutate the struct or call *any* method (including properties!) of the struct. In this case, the compiler will perform a defensive copy of the struct within the method, so you're not actually gaining anything compared to passing by value. 3\. The struct is being mutated outside of the method. The readonly behavior of `in` is only enforced within the method with the `in` parameter. If you modify a variable passed as an `in` parameter (e.g. by another thread or via another reference), the method will see those changes. For example: int field; void Foo() { Bar(field); } void Bar(in int arg) { if (arg == 0) { field = 123; Console.WriteLine($"Surprise: {arg}"); } } 
Ah gotcha. Nothing of value I believe. It's not really the use case, but I'll admit the docs page is not fantastically clear.
Yeah those global config lines didn't help either :\ The location element works for other webform pages but I can't seem to get it to work for controllers. I'm almost leaning towards just having two separate projects now.
Please, if you are thinking about commenting with ‚Äúuse MVC‚Äù, please don‚Äôt. Thanks. 
That would probably require significant rewrites for Wordpress-on-.NET.
The use case is not having to write PHP.
Yes, but you can also use generic types which can be either value or reference types, so it would break that if they restricted it.
Yeah, it's only a debate with myself, since I own the existing codebase and will be working on the back-end myself. (It's not an active company/project - just bringing it back to life, and will hire others later.) I'm not a fan of sideways moves for not a lot of value. I just wanted to make sure I wasn't missing some obvious benefit with .NET Core, and this thread has been helpful to validate that I'm not.
My guess is that your events are not being called because your control is being lost after the postback. If you add a control via code then I think you need to re-add it on subsequent postbacks. Otherwise, the app doesn't remember that the control exists. I think this could be due to your viewstate not getting update with the new control or your viewstate not being sent with the postback.
I suspected dynamically adding your control to the placeholder as the probably the source of your problem before I got to this reply. On postback (including partial/updatepanel postbacks) you need to reconstruct the contents of the placeholder. There use to be some code on the web for a placeholder replacement that would reconstruct its child controls, but I can't recall the name at all. 
Agreed. If I swap out the placeholder element and have all the list views I need as user controls and set their visibility to false, does the code get added in the response only hidden? Or does WebForms not render it and send it to the client? Because there are a lot of lists in the real version of this web app and it would be inefficient if so. With the placeholder, all child controls do get constructed but I think this is somehow messing up the script manager. Just a hunch.
Completely irrelevant to your post, and for that I apologize. But god damn do I not miss WebForms.
I‚Äôm not sure if there is a best practice when it comes to that, but I've done similar setups with api calls before. It really depends on what kind of end product/output you need, and how the api handles returned data.
In may be useful in some scenarios where you want to assert that the reference isn't changed, e.g. to null but this would be highly dependent on someone's design placing significance on it, such as error handling.
Seems like if you know what you're doing there's a serious benefit: https://www.c-sharpcorner.com/article/c-sharp-7-2-in-parameter-and-performance/ 
Being 'optional' is nice as well.
I really hope some better inspection tools or code analysis will provide guidance here.
Right... And interfaces could be either.
With 'value' types the `in` modifier causes you to "pass a value by reference". Which means one cheaply copied pointer instead of a potentially very large value object that has to be copied in its entirety. Realistically though, that's a very unusual case. Few people are going to be dealing with large value types.
Thanks, I was wondering why they did it that way.
It's not really a pointer to a pointer. You are actually sending the initial pointer rather than a copy of the pointer. In would be the equivalent of const&amp; in c++. What IS true is that done wrong, it can harm performance
The `in` keyword is equivalent to this: void Foo( [InAttribute] ref object x); 1. Since `object` is a reference type, the variable always holds a pointer (or pointer-like data structure) to the actual object. -- pointer number 1 2. The `ref` keyword passes a pointer to the variable, rather than the variable itself, so that you can modify the variable. -- pointer number 1 3. The `in` keyword, or `InAttribute`, tells the compiler that you don't intend to actually change the variable being pointed to. But that doesn't change the fact that it is a pointer to a variable containing a [pointer | value].
Right so I was wondering why in the case of something like ```int``` is this even a thing? Isn't a pointer going to take up the same temporary memory as the int itself?
&gt; A pointer going to take up the same **or more** temporary memory as the int itself. An `int` or `System.Int32` needs 32 bits. A pointer needs 32 or 64 bits depending on the CPU architecture. 
OK!!! Thank you for validating my concerns. And obviously any value type that is less than int (like byte) makes no sense to use this. That said... Isn't there an overhead in making a copy? Is that the other side of this conundrum? 
The in parameter modifier is a performance construct. It‚Äôs far cheaper to pass a reference on the stack, than it is to copy a value. A struct with a few value types and it can reduce the call overhead to half or less. This however only matters in hot loops.
The question was in reference to using `in` in conjunction with a reference type.
Encapsulation and the net core ihttpclientfactory
&gt; Isn't there an overhead in making a copy? Depends on several factors: 1. How big is the value being copied? 2. How much data can your CPU copy in a single operation? (I don't know the answer to this one.) 2. Would the function just be inlined anyways, negating the need for the pointer or copy? 
The compiler will compile *in int* parameters as Int32&amp;, where a plain *int* would simply be Int32. It will also pass some other information about it being readonly. The last part is the only thing that is different from a *ref* parameter, and the reason why it was called *readonly ref* at the beginning. Because the bytecode contains the readonly information, there is no reason for the JIT to create a double ref, and for simple types it probably doesn‚Äôt even use a ref.
You call a highly situational performance feature worse than useless. That is not the case.
"The storage location of the object is passed to the method as the value of the reference parameter." Sounds to me like we are massing a const&amp;, not a pointer to a pointer. https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/ref If you have a source telling otherwise, I'm interested in reading.
If you want them independent of each other make them into two separate projects and then call back to the web API, using https and Ajax. I mean you said you are trying to make them totally separate and you need to have an anonymous. if you want some type of security wrap around it then you can also filter by IP address to make sure it's only coming from your server and you can do a call from the controller in the back end.
sadly not, but thanks a lot for your effort ^^ I tried also seperating each number with +, but google refused me to give me any result. Also tried some APIs for barcodes, was helpful, but not exactly what I was looking.
:-) That would be quite a short blog post!
So I'm guessing ```double``` is a good candidate for ```in```?
If you set visible to false, it shouldn't render any html to send to the browser. The timing of when things get added to the control tree on postback is critical for events being fired.
No cost (for pointer-sized) as it gets passed in a register. Passing `in` means it passes a copy of the pointer, which also means it uses the value on the stack; and has an extra deference. Passing a class by `in` is basically pointless. If the struct you are passing is &gt; 16 byes then `in` can be an advantage (as it skips a copy); however if its not a `readonly struct` accessing any methods or properties will cause a copy for anything passed via `in` (to maintain immutability); reading a field is ok. So the best use is for a `readonly struct` that is greater than 16 bytes.
Is it possible to put a dummy user control that bi D's correctly and then add the other programmatically? Is an update panel even really needed? Why can't the user control be it's own update panel?
But CTRL-P isn't all that ridiculous, right? I haven't done this recently, but it should be possible to create a (simpler) document for Word or Open/LibreOffice, and then have that application save it to PDF, right? You should even be able to automate the entire process with a bit of COM. Quick google search turns up https://gallery.technet.microsoft.com/office/Script-to-convert-Word-f702844d , which looks like a decent starting place.
Hi Ben. Good to see you. :)
&gt; Passing a class by in is basically pointless. This is important. But why is it allowed by the compiler then?
I actually start a new job at the end of this month and they don't use WebForms. I'm looking forward to the change as I prefer MVC/WebAPI any day.
&gt; This is important. But why is it allowed by the compiler then? Generics? Raised an issue for it: https://github.com/dotnet/roslyn/issues/23332
You'll love it. I had to stop reading your ppst about halfway through because the update panel complexity was literally making me nauseous. I remember battling those 8 years ago. I pray I'll never have to again!
And good luck with the new job!
You're just making up *possible* optimizations and then assume that the runtime implements them. That's not how it works.
Write a blog in C#
If you want to load a control dynamically in an update panel you need make sure that 1) The control is added on every post back to the page 2) The control has an ID, and that ID does not change between post backs. Your issue is not adding the control on subsequent post backs, as well as not giving it an ID. So, despite the fact that you can see it on the page, the server has no idea that it exists at all. Hence, no events.
Make the call to the API and create the file and then send it by email when finish. A simple http get /myadreess/mynumber can work. 
Again, since you seem to insist on making a fool of yourself. The question was about **reference** types. The situation where it can possibly be a performance improvement is when it is used with **value** types.
What part of "the question was about **reference** types" did you not understand?
This dates back to the original Visual Basic where there were four ways to pass parameters: * pass value by value `void foo (int a)` * pass reference by value `void foo (object b)` * pass value by reference `void foo (ref int c)` * pass reference by reference `void foo (ref object d)` The `in` and `out` keywords/attributes are essentially there to indicate *why* you are passing them by reference. Fun fact: VB ignores the `out` attribute. (Probably ignores `in` as well, but I haven't checked.)
I couldn't say without testing. But personally I wouldn't use `in` for anything smaller than the maximum recommended size of a value type. https://stackoverflow.com/questions/3860384/net-what-is-the-maximum-recommended-size-of-a-value-type
I'm having flashbacks.
I can read fine, but that doesn‚Äôt change the fact that you‚Äôre calling it worse than useless. Which might be true for using it with a reference type, but not for structs. We disagree about the wording I suppose. If you had included ‚Äúworse that useless for reference types‚Äù I wouldn‚Äôt have said anything.
Double can fit into a single xmm register so it's also pointless to use in with it.
the f# conference is at the end of september in san francisco.
It allows setting properties (because it is still a reference type), but doesn't allow setting the object variable itself (for example, setting it to a new instance, or null.)
Misunderstanding me the first time is reasonable. The second time, forgivable. But continuing to 'call me out' on your misunderstanding, despite the fact that you know damn well that wasn't what I meant, is just pathetic.
So it sounds like you are already working with a console application. My guess then is either you're looking to just make a simple Full Framework console app or class library? The web API. Is it [ASP.NET](https://ASP.NET) Web API, or something else? I think the service type would probably be a good thing to think about. What's your environment like? Do you already have a process to schedule code execution (Schedulers, Chron Jobs, etc.)? Also, is this going to be something that you are pulling on-demand or scheduled? That last thing is pretty important from an architecture point-of-view.
I would look at using c3, a d3 based library made for charts. [https://c3js.org/](https://c3js.org/). For PDF work in JavaScript, the best I've seen that's boxed and usable is PDF.js [https://mozilla.github.io/pdf.js/](https://mozilla.github.io/pdf.js/)
So just now realizing there isn't direct stamp capability like with iText, but I think with some canvas work, you should be able to port that to PDF with PDF.js. I'm drunk though. So probably should ignore this.
Porta-fucking-bility. If you can create a pure .NET Core app, you can deploy that on windows, linux, etc. 
Controls that are dynamically created should always be constructed in Page OnInit Event. Otherwise the event propagation / Viewstate will not occur properly. 
Since you already have HTML/JS you can perform HTML-to-PDF conversion on the server side. Typical tools for that are phantomjs (uses newer version of WebKit and more suitable for charts) and wkhtmltopdf. You can execute them with System.Diagnostics.Process or just use existing C# wrappers.
I bet that this is the problem. 
The events are prepared and fired after the control is loaded. So this usually occurs after the page load. So in your csse you contructed the object in the page load. So there will be no history from viewstate to track the changes and cause events to fire
We do this quite a bit on a product I work on. We use continuation tokens. Basically, you make the call that starts the work and it immediately returns a token (guid) and starts the work. You can then call for status against that token to see how far it is or even just true or false for done. Then you can use that same token to make the final call with the results. Just be sure that all your tokens expire.
If the API has a Swagger/OpenApi JSON schema file, I have very good results using the [NSwag Studio](https://github.com/RSuter/NSwag/wiki/NSwagStudio). This allows you to generate client and model files for easy access of properties and to ensure the datastructures are ok. I use this in combination is the ASP Core Web API to create the webapp and a C# client in one go - but it always uses the intermediary step of creating a json file, so there's nothing to stop you from generating only a client.
I‚Äôm excited to hear more about this, I had a quick glance at the source and need to look at this further. 
Regardless of the hosting solution, diagnostics and metrics are always useful. 
How big is the system this is used in? The client side analytics (e.g. GA, mixpanel, segment) have proven collectors able to withstand millions of requests per minute; where your servers can spend all of it's time on business functions. Co-mingling this (especially in a microservices environment, where each component would have to do analytics) would result in a (potentially) significant load on top of your normal business load.
Happy to hear that :).
The current system is really small but defer the analytic processing to a secondary system to handle huge traffic is really easy. The first option that comes to my mind is to implement a store interface that defer the analytics to an Orlean Silo. I will probably write that soon :) thanks for the idea
Ref in parameters were added for unity. They solve a very specific problem related to the common practice in game development of passing large structs around. In **literally** any other context if ref in is actually providing you a serious benefit your code has much, much bigger problems. TL;DR: If this feature is of value to you and you're not writing games, you should be taking a long hard look at your code base. 
I like.
Nice idea but to have valid statistics you need to give up on client side caching and any kind of caching in between (like a CDN).
Any idea how this would perform behind a load balancer? 
[http://openfsharp.org](http://openfsharp.org) for the curious. I'll be there (and speaking)!
You need to build a 404 page and call it I think.
What advantage does Cake provide over adding build and deploy tasks to my csproj file that invoke a PowerShell script?
Thanks for the valuable feedback. I started to think about second version of the cheat sheet.
Wow glad it helped and thanks for sharing your use case.
How would I call it? Perhaps redirect to it instead? Hmm... but that feels wrong somehow.
Call it by name using Page("404page"). Take this with a grain of salt. I'm on mobile and can't use a pc.
Please explain more
Not if you talk back to the server despite caching. You can cache big stuff and talk back the analytics server directly when pulling from the cache instead of relying on the loading of the content.
https://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.builder.statuscodepagesextensions.usestatuscodepageswithreexecute?view=aspnetcore-2.1 app.UseStatusCodePages(); app.UseStatusCodePagesWithReExecute("/Status{0}"); Putting somewher early into your pipeline (`Startup.Configure`) should look for pages in standard page directories named `Status404.cshtml` etc. If they exist, they should be displayed instead of the resource you were requesting, otherwise `app.UseStatusCodePages()` will display `Status Code: 404; Not Found`
This is probably the best answer yet. I *do* think there are others spots where something like this may be useful for lower level code (I'm thinking stuff like Buffered IO here where you're getting structs from a Native library.) 
As others have said, if it's an WebAPI you have control over, use some of the swagger tooling mentioned like NSwag. Writing all of this assuming it's .NET Framework and not .NET Core, btw. If it's a WebAPI you don't have control over (and can't get a swagger file for) you will have to write your own client. For what drawbacks it has, I am still a pretty big fan of [RestSharp](https://github.com/restsharp/RestSharp), if only because it is pretty damn hard to screw up in usage. Please note if you are doing many long requests at once, that the Defaults for `ServicePointManager` are absolute garbage. You'll want to consider a line like this at the start of your application: static void Main(string[] args) { ServicePointManager.DefaultConnectionLimit = 8; //Default is 2 in Console Environment.
Apperently good, the live demo is behind a load balancer :) (IIS 10 which is balancing between localhost and an another PC)
So this is an attempt at an open source version of Application Insights?
In my case clients have different domains. It is multitenant system, when one server might have more than one dns names, so we cannot rely on app settings. Regarding blocking Referer, do you have cases when browser might block it?
I'm going to disagree with your comment about it being about the "works on my machine" problem. It did work on your machine because the kernal that was making the http requests was fundamentally different. If you had tried to reproduce this on a Linux machine, you would have been successful. Docker on Windows is still Windows. The VM tries to do it's best, but really it still operates on Hypervisor/Windows Kernal
Sorry idk the DotNet implementation of pipes, but in golang a writer blocks until it's fully read - so you need to simultaneously read and write to the pipe to not deadlock. (Write without a reader deadlocks the thread)
Addins and dependencies to name a couple. Also cross platform (though powershell is now cross platform as well) 
Caches like CDN are for static assets while the statistics collected here are for the API calls. So there would be no interference there. Perhaps if the API calls themselves were cached (i.e. Varnish) it would impact the statistics but that's not very common. It's not an exclusive proposition either, server and client side statistics can be collected at the same time. It gets even more interesting when you can have a correlation id to link the client and server side statistics together.
yep, kind of :) 
To you and everyone who replied. This has been extremely helpful in understanding the proper use of this new feature. Being that `in` is optional. If you don't specify `in` when invoking a method, is it still problematic?
If you can afford eventual consistency, you can use CDNs also with dynamic content. At my company, we make extensive use of AWS's CDN solution, Cloudfront, to cache asp.net pages that we know can be served with outdated content. This allows us to spare 50% of the requests from hitting the servers.
The new catch phrase: `in`appropriate. :P
I'll give it a star. I am a huge App Insights fan so always interested to see what exists out there.
Here's the deal with coding interviews;they're all different. I left my previous job because they were absolute garbage about being transparent to their clients about the dumpster fires they were inflicting upon themselves and took a month to interview with as many places as possible. Some places did live code tests ranging from fixing unit tests, fizz buzz, algorithmic sorting, generalized white boarding, fixing production bugs they couldn't figure out, concurrency problems, building a full web app,...the list goes on. Some places just asked technical questions over the phone. Some questions were conceptual, some were strictly academic, and others were open ended in seeing what types of experience I had with a a specific technology. You can't study for an interview unless you know exactly the types if problems, technologies, and people that will interview you; right down to their personality. 
I tend to use an CSHTML template and then let C#/Razor render/create the pdf. No external libraries or nuget packages needed. If interested I could search for the snippet and post is.
I just skipped NetCore and went to 2.0. Yeah configuration ( derp new vs17 update of dotnet 2.01/2.0004 messes things up sometimes) &amp; support in MS tooling(cough cough efcore) is lacking but you have many more options when it comes to the glued together xml bloated templates in 4.5. THe Middleware is super modular + IHttpContextAccessor. If there is no real reason to upgrade dont .... but play around with it.
If the processing will take some time, I would consider using a messaging system such as a pub/sub message bus. That way you can publish a request onto the bus, allow the other API to pick it up and process it, then drop the result back onto the bus where you eventually pick it up and show the results. This will allow either app to go offline and not interrupt a long running process as it can recover by rerunning any unprocessed messages on the bus.
The point I'm trying to make is that this feature was added for a specific edge case, not because it's an overall improvement. It's not something you should be using by default, because it adds non standard behaviour for no benefit. In the unlikely event it provides you benefit you should almost always be converting the gigantic value type to a reference type rather than this. There are cases where this is useful, but they are the exception rather than the rule. 
In most places these days they will give you CTCI-like problems not really related to your .NET knowledge or real kind of coding you would need to do if hired. In few more reasonable companies you may expect to be given an interface for something more realistic an asked to write the implementation, then discuss performance and maybe other possible options. However, other then just *coding* inevitably for .NET position they will ask you about the the framework, language constructs, garbage collection, multi-threading, design patterns, unit testing etc. 
That's why I said to take a long hard look. This isn't a "never do this feature", there's a really good reason games do this, and there are other cases where the performance boost from not instantiating a class would be worthwhile. It is however a don't use this unless you know why you're using it. 
You should check out Stackify Retrace. It may provide everything you need. It does code level performance profiling. Can tell you which pages are slow and why, expensive SQL queries, etc. https://Stackify.com
So it was missing -t ? Should it be dotnet new -t console Is there an option for listing available templates? Or a template directory I can list to see what is installed? 
I wish Application Insights would just have an easy way to let me view my logs. Aggregating logs in a UI is great, but I just want the logs themselves. Not a terrible azure portal that is slow and requires a query language to show me just raw data
Is writing a CSV parser a rite of passage for dotnet developers? Everyone I've spoken to and myself included have done one.
[removed]
&gt; a) how do I advise somebody that this instruction page needs improvement file an issue here: https://github.com/dotnet/cli
If you do a \`\`\` dotnet new \`\`\` It should list the templates available. Also this page has some documentation on dotnet new [https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-new?tabs=netcore21](https://docs.microsoft.com/en-us/dotnet/core/tools/dotnet-new?tabs=netcore21) I don't have experience with dotnet sdk 2.1 on Debian 9, but when I do a "dotnet new -t console" on my Windows or Ubuntu it doesn't like the "-t".
Well now they don't need to
I made my own when I was a new developer as well. Never thought to check for an existing library, just rolled my own.
Just a little rant: This site sucks for mobile
I thought a JSON parser was a rite of passage.
Continuous Export?
I wasn't when I moved to .NET because this was 2008 Microsoft. I wouldn't say I'm a lover, but I definitely like MS now. I came from a PHP, Java, C background, mostly working in PHP and Java professionally, then I moved to a .NET job working for an Irishman who was &lt;3 &lt;3 &lt;3 with Microsoft and we were always trying out whatever RC / beta MS had for our projects. Not ideal mind you because all that taught me was to not bother with RCs from MS and wait for the release due to all the changes that could happen. Also we did a lot of Silverlight stuff. Anybody remember Silverlight? 
Visual Basic. When I started, Windows was just one of many graphical UIs available for MS-DOS, which itself was just one of many operating systems that I had access to. My favorite was whichever ran the video game I was currently playing. Then Visual Basic came out. It was the first programming language that made developing rich WIMP style user interfaces easy. No, it made them down right trivial. Around ver 3 it added database access and I could suddenly start making money by writing programs for health clinics. When .NET was released, I saw it primarily as "VB with real threads and inheritance like Java, but none of the stupid stuff like no properties or operator overloading". Later I switched to C#, which I still see as VB with a better paycheck. Am I a Microsoft lover? Yea, that's a fair statement. But it is because of VB/.NET, not the other way around.
 dotnet new --help
Because CSV is such an ugly format it's really common to have a use case where no library exactly fits your use case. It's also a problem that appears far simpler than it actually is so people are likely to roll their own.
I came in dotnet about two years ago for a traineship. I came from the PHP world, thinking Microsoft was evil, and I changed my mind when my technical expert told me about dotnet core and what Microsoft does for Open Source Softwares. Now, I like Microsoft and dotnet, and I am more than happy to have quit the PHP world. 
I tried --help and the --list option it mentions but couldn't find the right combination to get anything other than redisplaying the help. Will try again... 
I used to use .net and now I miss it a bit honestly, use python now but also happy to be free to use any linux distro with my work, have yet to try out .net core, maybe that will be a better fit for me :D
I don't even think docker on windows itself was the issue. The blog post sounds as if he was using windows containers on his ocal machine instead of regular Linux containers. Because he pointed at the use of curl on linux. To my knowledge the difference in framework logic itself is written using compiler directives for each platform. If he was using Linux based containers using docker for windows net core should have used Linux specific logic.
We have for something at work. Really useful, especially compared to wiring up custom metrics all over the place.
There's a guy at work who does that as well as the opposite. Writes "parsers" and "object to CSV deserializers" that then have multiple bugs...
Silverlight is an inside joke around our office. We have mostly entry level devs who never actually tried to work in it but we always say if it was good enough for Netflix.. 
I got into .NET for my first dev job, having learn PHP at university. I wasn‚Äôt keen on learning the Microsoft stack because I had the silly opinion I think a lot of newbies have that Microsoft is evil. However I had a great lecturer at uni who once told my class ‚Äúnever get emotional about technology‚Äù. This stuck with me so I gave it a chance and ya know what? I love .NET and think Microsoft are pretty awesome now too!
I dislike Microsoft's dominance (won through both fairly and unfairly tactics) in the desktop space. Have for 20+ years now and always preferred to run non-Microsoft tech where possible. Their support of Linux and better cross-platform tools for development has been an interesting mental whiplash. I came over to .NET a few years ago when I switched to working at a C# shop. Been using .NET Core for hobby projects now that 2.1 came out, working in both macOS and Linux. I like that I can leverage my C# knowledge as well as (most of) the standard framework that I know from my day job. With VSCode, it works as a development target and works a lot like I'd expect a Linux development tool-chain to work. Very low friction to jump between macOS, Windows and Linux. Looking forward to the day when I don't have to run a Windows VM just to do my C# work.
&gt; Writes "parsers" and "object to CSV deserializers" that then have multiple bugs... Ah, so someone that needs to learn the usefulness of BDD/TDD. Or maybe learn to apply automated tests more effectively.
Funnily enough he's never writing them because "its too hard to test" because everything is strongly coupled and there's no dependency injection going on. It hurts me.
Nothing personal about the company, but I strongly dislike using Windows ‚Äî favoring the Linux CLI, filesystem, package managers, configurations, etc.. Despite this, I really enjoy programming dotnet on and for Windows (as well as Linux and Mac). They do a damn good job keeping that aspect of their product line sensible and highly enjoyable.
After Ruby on Rails and Node.js I went back to ASP.NET MVC Core because of Entity Framework. EF is a great SQL ORM. Tried Sequelize and Bookshelf in the past and EF is way better. If not for EF, I would go with Node.js/Express/TypeScript for the back end.
**Why I'm a not-MS lover:** I still hate MS because they still force users to use microsoft provided technologies, while their only true support is only for big industries (their customers, microservices, etc) and it's never changed, even when they open sourced .NET aka dotnet, this whole show was an illusion(!) that microsoft opens it's arms to developers, while it's 2018 and dotnet is still not truly crossplatform (95&amp;#37; of the crossplatform coverage is all about functions that used by services that microsoft provide for it's customers, not for all developers. Java has more coverage (foreg: UI) MS don't have and don't give a fuck if individual developers want some UI interface on an OS other that Windows, because they don't get shit money from it, all they provide is a console in ya face) The only thing they cover is ASP.net core as for everyone... why? because they have microservices that support the runtime very well, so people will pay for their services if they chose asp.net core (azure and other shit) but as far as I know Oracle has way better could services / support / ecosystem. **Why I'm a not-MS lover 2:** Not long ago I spent 1 week to check my libraries/applications/dependencies if I can migrate them into kotlin libs (btw too much staph, can't do), why I checked? Because I saw the roadmap of the dotnet! Microsoft in 2019 will still focus on their services, they will not cover everything with coreclr (for eg linux arm is skipped all in one, only the community pushing some commits sometimes, which I truly appreciate), instead they pushing the dotnet to more microsservice compatibilities and more window-only support (XAML scheme, winform, UWP, self contained exe or what (CoreRT?) that runs only windows and more asp.net (so basically for one year, all they will do is support Windows stuph only with a "crossplatform technology"). **Why I love .NET and where I came from:** I love C#, it's a pure language that has no bullshit or misleading features in it. (I meant for misleading is that in other languages you can do 1 thing with 10 different ways, and 3 of them actually accepted/good way, while with C#, there is 10 different ways to do one thing, but only 1 is truly a good solution if we take care of design principles, dependencies, and usually the language will warn you by itself that what you are doing is okay, but how you are doing it is bad, atleast for me, it's forcing me to do clean implementations and use strict principles in order to create a nice framework foreg) The .NET is a good, easy to use (I mean if you know the language), and widely supported (on windows os). If you want cover everything on windows, you can go for .net and use some C/C++ via marshaling/pinvoke if you need low level access to stuph. Back then, as for web development I used PHP as backend, ASP.net (forms) was really really bad (MVC started looking good later but the ecosystem was still crappish). As cross platform development I used C or C++ (if I needed low level functionalities such as kernel staph) or mostly Java when linux distro came around and had to work with. It's almost 11 years I first touched .NET (VB) and there was years and years again I did not touch any .NET, but since then I slowly turned everything into .NET and learnt C# (mostly because of Roslyn and a hope of dotnet which is btw 5 years misinterpretation from my side of the meaning of "open source and crossplatform") and I'm still questioning myself every day why I couldn't go for JAVA and for oracles ecosystem... and it's all because of C# (It gave me certain principles that I comply every time I code)
Might also give Mono a try... we run several server and embedded programs on Linux (also on ARM boards) using Mono. Most work flawlessly without changes...
Silverlight was not a bad product, technically. It failed because of the "product/market fit" - the market for "plugins like flash, only nicer" was fading away at that time in favour of all-JavaScript things. It worked, but it did the wrong thing. Still, it was a proof of concept for .NET core and that's something.
I left one of the best contract gigs I ever had because they don't let you stay on contract more than 2.5 years at a time, and the only team I could move to did Silverlight. This was back in 2016.
Used to PHP a lot at home. Got taught Java at University. Used PHP for my first internship. Got a job at a large online retail company that used C#. Was optimistic that it'd be good to broaden my horizons early on in my career however didn't think I'd fall in love with .NET the way I have done. I even use VS Code on my MacBook in native MacOS (a Windows VM is too much effort even though I have parallels) for .NET Core and have no regrets whatsoever.
Unity
I do Linux and Windows and although I have no great love for MS. However I do have to say that the portions of .Net that run on both platforms are solid as a rock and perform well, and I could easily see switching entirely to C# in the future instead of flipping between C++ in Linux and C# on Windows. 
Hi! Let me know what needs to be done to go from "hobby" to actual business projects. My team are the .NET Cloud Developer Advocates and we're very interested in making it better for everyone. So if things doesn't work, either in Linux/Mac/Windows, let me know. I'll bring the feedback to the team and fix things. You shouldn't be running into obstacles but if you are, let us help you. 
Definitely something I will put to immediate use. Thanks heaps.
7 years of Delphi in the client-server space. Needed a tech that was modern and forward thinking. Once I saw the direction asp.net mvc was headed, my mind was made.
I‚Äôm a Mac dude, have been for years, and I love .Net. The first time I tried to learn programming was with Ruby on RAILS and it didn‚Äôt click. When I took a class I C# and I had WinForms to work with, I could focus on the programming side rather than the UI side and it made everything click for me. Now I don‚Äôt need to use WinForms for UI and I use ASP.NET Core for web dev, but C#, Visual Studio, and WinForms was so much easier to learn that programming everything from text using a simple text editor.
The problem is that platforms like Silverlight have an enormous buyin cost and inertia. I wouldn't say Silverlight was a smash hit, but a decent number of Microsoft shops invested in it just on Microsoft's assurances. These places are looking at man-years of work to untangle their products from Silverlight. Microsoft did not do its developer community any favors with Silverlight.
[removed]
I am a Java refugee. It was going in a bad direction under Sun, but Oracle has made a fucking mockery of its original goals. And the ‚Äúabstract everything‚Äù crowd has abetted its decent. There‚Äôs a line between philosophical purity and getting work done, and Java has been on the wrong side for a decade. Congrats guys, you turned Java into COBOL, and ironically Java is the ‚Äúenterprise‚Äù language since the 00s. C# is a great alternative, with the best IDE of any development ecosystem on the market, improved now by cross platform development through dotnet core.
&gt; asp.net mvc 6 projects That's not a thing. I assume you mean ASP.NET Core with MVC. Does it have to be MVC though, rather than a SPA? &gt; data-centric web applications These are painful to create in an MVC fashion. SPA's are a big advantage here. I recommend Vue either way.
I've been developing software on proprietary Microsoft platforms since the late 90s. Over the years I came to despise proprietary Microsoft products but was still a big fan of .NET. It was getting to the point where I was actively evaluating other languages like Go because .NET effectively tied me to Windows, which I generally refuse to use. Thankfully .NET Core came around to save the day. Not only is it FOSS but its pretty damn good and effectively works across a wide range of operating systems.
A long long time ago, I once proudly declared M$ to be the root of all evil. It was the common thing to think in this industry, really. I learned C# in 2003 and have used it ever since. I've taken up TypeScript and I love it even more. I honestly dread the day I'll have to abandon the .NET/MS ecosystem and go back to something horrible like C++ or Java. Ugh. Anyway, MS has made me a LOT of money. I'm really, really happy about this. The issues I've had with them as a company have been fixed with new generations of management. They do have an awful consumer track record still, but anything out of MS Research is generally pretty rad. 
I am a huge fan of c# and the direction they are heading as far as portability. I cam from ruby and php, so the type safety and package management features were new to me and they made my life easier. Windows 10 is a steaming pile. I run Windows VMs for dev work, but my base machine is Mint. It doesn't send all my data back to some central repo and doesn't try to force unwanted updates. Their stranglehold on DirectX is aggravating. 
I‚Äôm a Linux/\*NIX user who, on top of .Net/Mono, uses Java, C and C++. I started using .Net because it‚Äôs a lot nicer and Microsoft opened it up to non-Windows users.
Are you willing to give more detail regarding the amount of money you make/have made? Just curious
I use it for an in house web service. Great to save all logs, get error information, and get usage reports 
It made me hate web development. Well actually more than I already did. Javascript, CSS, DOM manipulation, etc
I love .net because of the paycheck 
If you're using MVC, then the pages will be rendered server-side, and Javascript will be used to add some limited interactivity. In this case, jQuery with a few plugins (like DataTables) will do just fine. If you're looking for more of a SPA experience, but still want to use MVC, look into Vue.js. It can be dropped into a page like a library and used to provide SPA-like interactivity without necessarily doing a SPA. Otherwise, your option is to do a full-on SPA with API, which requires much more tooling and setup, not to mention the learning curve.
* Fix the bugs. * Document everything. The hardest part of working with .NET Core is figuring out the library stuff. 
But you weren't supposed to get emotional ;) 
Do you know of any good Vue tutorials or resources? Especially any that use Vue with aspnetcore?
It's rough for large projects. You should try VSC on windows and see if that works for you. The experience is quite the same on any Linux distro. If it u want something that more resembles Visual Studio you should also check out JetBrains Rider. 
VSC is basically Microsoft‚Äôs version of EMACS. I tried it for a couple minutes when it came out and it didn‚Äôt really impress me. Some people swear by it, but I‚Äôm not one of them. I‚Äôd recommend MonoDevelop more, since it‚Äôs a pretty good IDE that‚Äôs probably available from your chosen distro‚Äôs official package repo. And even if not or the one offered is out-of-date, the Mono Project makes it available from FlatPak directly.
VSC has changed a lot from its first release. It is now much more than emacs and personally I even prefer it over MonoDevelop . If you don't mind proprietary software you should definitely give JetBrains Rider a go. It's not nearly complete but already provides a very good experience. 
VS Code is technically an IDE, but it feels more like something from the 1980's like TurboPascal. If you are a long-time Visual Studio user you're going feel like it is missing countless little tools that you use all the time but never think about. On the other hand, if you are already used to jumping between different IDEs and never really take the time to master one in depth, you'll probably be quite happy with VS Code.
Python/Linux guy. I got into doing actual dotnet framework stuff because that's what the main stuff at work is built in. I'm not happy about having to dev in a Windows VM but I'm not completely down on it either. We're pushing towards using core for new projects which makes my pipedream of a Linux work laptop less of a pipedream.
For what Ms has been providing to the comunity i see no reason not to love them and everything. Net. There was in the past. Not anymore, apart from some Windows 10 decisions regarding privacy. 
More documentation on everything if possible. There's some times I've worked with .NET features and the method documentation is nothing more than the fact it is in fact a method named "x".
I look forward to the day where I can use VSCode full time for .NET projects instead of VS. It's not quite there yet (mainly around debugging) but it's getting really close.
Oh yes, some people got stranded on the Silverlight platform, for sure.
Like others have said there are two pathways here depending on what you want to accomplish. * Full featured (highly opinionated) client side framework like Angular or Ember to make a pure SPA and use [ASP.NET](https://ASP.NET) MVC just for server side APIs. * Continue to use [ASP.NET](https://ASP.NET) MVC for server side rendering and use a smaller library like Vue or React for SPA-like enhancement. I could be wrong but seems React has better integration with MVC. See here: [https://reactjs.net/](https://reactjs.net/). If you don't like React, Vue is also good. Vue is so easy to use you can use it as just a jQuery replacement if you want.
My job is Windows/C#/.NET/SQL Server, but I prefer Linux. At this point, I'd probabally go for .NET core on Linux for new dev with postgres as the db unless someone else wants to pay for MS sql and maintain the OS for me. (I like MS, but I can't justify the cost or having to deal with Windows. MSSQL on linux doesn't seemquite ready yet, maybe with SQL Server vNext).
Silverlights code lives on in .netcore. it got the last laugh by ruining visual studio tooling for about 2 years ;)
Can you use the user's session to keep track of the site? Like have an extra property on the user that you can use to pull their site. The cases where it gets blocked is just from a lot of experience. Here is one example to get you start https://stackoverflow.com/questions/6023941/how-reliable-is-http-referer I suggest researching it more before you rely to heavily on it. On top of the issue with it missing, you'll also have to be concerned with people spoofing this value. Since its a user supplied variable. 
Do you have examples? We have tons of people documenting the heck out of everything. If we're missing good examples/samples/tutorials, I'd love to create them! Heck, we even have try.dot.net which is pretty cool on its own.
So you're telling me our public API don't have XML docs. Do you have an example? I'll do the PR myself if needs be.
The hard work shows. It's coming along nicely. Specific pain points I have struggled with: * Entity Framework Core CLI needs GUI integration. * ASP.NET Core LDAP integration is painful.
What a fucking garbage post. Here's the original source: https://blogs.msdn.microsoft.com/visualstudio/2018/06/06/whats-next-for-visual-studio/
My desktops have run MS products since DOS 5. I didn't use the Win GUI full time until Win 95. College was in Unix (Solaris) and Linux (Red Hat), and my college laptop dual booted Win 7 &amp; Linux (Slack). I spent the first year or so out of college writing Perl in a Linux (Red Hat) environment. I consider myself impartial to the OS, as long as it runs what I need it to run and doesn't lock me out of the low level detail I want. Why .NET? Because I used it during an internship and found it easier/faster to develop software than with C++. I felt the same way about writing Perl from the other side of the spectrum. C++ gets bogged down in the details and requires careful memory management. Perl's type system would happily let you hang yourself and not realize it until it blew up 20 lines down when shit stopped making sense. C#, to me, is the happy medium. Java is pretty much midway between C++ &amp; C# because of its verbosity. Objective-C can go drool in the corner, as I found it a nightmare to work with. Swift &amp; Kotlin are languages I would love the opportunity to try out in a small project. R fits cutely into its niche. Oh, and Javascript. Well, I'd be happy to see the Server-side JS movement fade. I've used it a lot, and it falls somewhere around Perl. I think I'd rather do Typescript, but I haven't used TS much to be sure and can't shift to it in my current project. And that's the story of ~~how I met your mum~~ why I think C# is one of the best languages to develop in. tl;dr C# is very nicely strict but still flexible. I know. I've seen some shit.
I am very platform agnostic. I use iPhone and iPad and Apple TV, but I have a Windows notebook and previously was on Android. I got into programming .Net circa 2000. My logic at the time was, companies paying to use Microsoft server and Development tools would also pay developers a little more. I figured companies using cheaper / free open source stacks would pay less. My career has been great so far. I made it from SQL Report writer to Application Architect. I have been on the MS stack the entire time, I haven't been out of work for one day since I started working in Visual Studio. And the code I was writing in 2000 still works. Microsoft's support for legacy code / applications is second to none.
Up until 6 months ago, I had been working full time on Windows with Visual Studio. Now for the past 6 months I've been working full time on Mac and I miss Visual Studio. Unless you **have to** switch your dev machine to linux, then I'd recommend just sticking with Windows and Visual Studio. VS Code is not really comparable to Visual Studio. JetBrains Rider is the closest I've found and it's what I end up using most of the time on my Mac. But I'd still rather be using Visual Studio on Windows.
You could give Rider a go. It is an IDE not just a text editor. https://www.jetbrains.com/rider/ I have not tried it though :p
The emotions thing is very important because it leads to fanboism. Another important thing is to remember that everything serves a purpose, even old tech.
Actually somewhat echo both of theses, though we ended up doing Azure AD the latter which is a pretty decent experience. But there are times I would just like to build a project, right click add migration, type in a box, and hit enter. Or right click remove a migration. Bonus points for showing the underlying command executing. 
Even Malebolge‚ÄΩ 
I develop in multiple different stacks. JavaScript, PHP, dotnet, Java and swift. Mostly the first two. I love dotnet, it's very well put together. But I hate doing anything in windows (for work. At home I love windows for gaming). I'd probably just be a dotnet developer if it was more feasible to do in Ubuntu or osx, but right now it's a pain because dotnet core and jetbrains rider just aren't where I wish they wete
Anything in particular you're missing? I'm asking because I'm pretty happy working on OSX.
I get all fanboy over stuff if it's actually good. WPF is dope as hell.
I wouldn't say I've ever been a Microsoft "lover". I rooted against them during the anti-trust lawsuits. However, I learned Asp.net and then .Net 1.1 in the military and that translated to very high paying civilian jobs ever since, so I can't really complain. C# paid for my house.
Do you have a couple of quick examples that I could point to?
I admit, being a Mac user, I died a little inside when the only job offer I got used Microsoft/.NET. My background is mostly in PHP (but even then it wasn't extensive so I was open to learning .NET). Four months into the job, learning C# and .NET MVC, I have to say I love it and am excited about the things I can - and will - do with it. But the Windows OS? Ugh. Hard pass. :)
I'm a Rails guy that is diving into Dotnet Core due to a side project with a buddy. If you revisit Rails after getting a handle on how Entity/MVC works on the Dotnet side, you will appreciate it more (and have a MUCH easier time). Rails does TONS of magic that can be hard to see when you're learning it. You're mostly memorizing rather than actually understanding, and you feel like you're grasping in the dark sometimes. On the flipside, Dotnet gives you 50 compile errors until you've crossed every T, dotted every semicolon, and manually imported every system-level library needed. The explicitness will give you a better understanding of what you're actually doing, but the elegance of Rails is really second-to-none when you're trying to quickly write code that works. 
Visual studio for mac not up to par either?
I can't find the links through Google on my home computer vs having the links bookmarked at work. I worked on this a few months ago, and it was working with the CRM Dynamics 2015 Audit SDK - looking at audits of our entities. Some of the methods or properties of the audit SDK were not fully flushed out in the documentation. Though I understand that the 2015 stuff is out of some supports like documentation. But it's a niche area that I feel the documentation was lacking, and may be something to consider in the future.
Yeah, you nailed the exact reason I struggled with rails. I felt like I was just regurgitating what the book or tutorial was telling me but I had no idea why. Coupled with the fact that I was very new to programming as a whole, it was really difficult. However, with my knowledge of how things work with MVC now, I‚Äôm sure I would have a better experience if I took a shot at it again today 
It's okay and very much usable but comparing it to VS or Rider shows it's much less useful.
Rider is fantastic, I use it on Mac and Windows.
I'm a recent grad and I've only ever worked with Node professionally. At my coop we used it to create something from scratch and at the startup I'm at right now, we write TypeScript and compile it to JavaScript for Node. My school was heavy on the open source and command line, but we had two classes that used ASP.NET and I liked the design patterns I saw with it. I just felt it was a shame it was only on Windows and not very open source. At some point during the second class I discovered .NET Core, then on 2.0, and I really liked the platform. I use ASP.NET Core for my own hobby projects, including a SaaS project my friend and I are prototyping. If my current job were open to it I'd recommend it to them, but I think they feel it's niche in the startup community and they want to tap into the stream of grads who cut their teeth on Node.
I just run VMWare Fusion so I can use VS
https://johnpapa.net/vue-typescript/ 
I started from ColdFusion and Delphi. Then .NET 1.0 happened so I switched.
Anyone using GQL with EF Core? How does it tie into your entity classes (eg. those generated with scaffold)? Also when you limit the query to certain fields, does that Select happen as part of the DB query or in-memory? Does it support navigation properties?
The download as PDF link doesn't seem to work. It brings me to the bottom of the page (the part that mentions the black and white version as an alternative) but then I click that download link and it brings me back to the top. It's a loop.
I use Rider on a daily basis on Windows and often in Linux. I actually prefer it to VS now.
I can't comment on your the DLL situation, but you may like [Costura](https://github.com/Fody/Costura) which can embed DLL's into your executable when you build. Makes the output a lot neater. I have noticed that it can mask some exceptions and make debugging dependencies a bit trickier.
I find VS code great for web development, but in that case most my debugging is done with powerful browser tools. But I found I really missed a lot of features of VS when I tried some small projects in Core in VS - though I didn't use it extensively and it was over a year ago, so things might have changed. I'd stick with VS unless, but there's no harm in trying it - works great on Windows, or you could spin up a VM to play with it for a while on Linux.
I tried that for a while before Rider was ready for prime-time. Overall, I found using VS in VMWare too clunky and disconnected.
I use this exclusively every day on professional projects. I can never go back. 
I second the LDAP integration. I'm somewhat new to the programming world in general and it was the first time since starting .NET core development that I really ran into any issues. 
I mean, .NET jobs are in high demand in high-salary industries, like banking. I can't throw a stone without hitting a recruiter trying to get me to take a job somewhere else. Pays about 20% higher than the average software engineering job in the area. I'm no millionaire, but I will be when I retire. 
Microsoft *was* evil from 1996-2006, then they changed their sinning ways. They were kind of neutral for awhile, but the current leadership (Nadella) has made them the good guys fortunately. They are better corporate citizens now that they live in a competetive world. Cloud has liberated them.
Sure, as a thought experiment and tool to exercise your brain (or exorcise your demons, take your pick).
&gt; is it because .net standard 2.0? Yes those are .net standard .dll's. &gt; And I can confirm that if I delete these files, the software works fine. You can't be sure unless you tested every code path that some set of unique circumstances wouldn't require these.
You picked a good time to start this journey. I would skip JavaScript and use https://blazor.net! It's the future of .NET! Note: I don't actually recommend this for production code, but a lot of .NET developers are really excited about this and hopeful that it will take off. Figured I should mention it.
Wish I could find a banking position for .NET. that would be amazing. Just a follow up. Would you say that you make over or under $150k a year? Just base compensation
There are very few companies that I love, and Microsoft isn't one of them. I don't think using words like "love" is appropriate when looking at technology. When evaluating what technology to use, there are aspects of the company behind it that matter, which are nothing emotional: * What is their support ecosystem like? Do they have paid support options? Is there a strong community support system? Does the company support the community support systems? * Do they operate in a responsible and mature manner when it comes to product lifecycles, security, and backward-compatibility? * Do they have a well-defined technology roadmap, and have they proven they can stick to it? * A sum of all of the above is what I would call the company's overall "credibility". An example of a company that meets the above: Microsoft. A company that does not: Google. All of these things lead to building a platform that is reliable and productive. Microsoft did not skimp when building C# and .NET, and have made very smart calls on some very difficult decisions over the years. As a result, I am confident in choosing C# and .NET for most projects for the kind of work I tend to do at the moment, and have been using it for about 8 years now. As usual, use the right tool for the job. There are cases where I would use Python over C#, Nginx over IIS, etc.
You can configure routing redirects to a 404 page in your startup.cs file. Microsoft has a fair amount of documentation on this - happy googling.
I used to always use VS, but about a year ago I wiped my machine and I only installed VSCode to see if I could use it full time instead of VS. I'm still using VSCode and have not yet installed VS. I like VSCode. If you are only doing dotnetcore you might be able to get by if you like CLIs. VS sets everything up for you and almost everything is done via menus and wizards. With VSCode you'll be running a lot of command line stuff. I do like that VSCode is much more light weight than VS. If you don't have a very powerful machine there's something nice about being able to pop open VSCode, make a few changes and run your app before VS even opens and loads a project. I also do not miss the VS update process. VSCode updates mainly in the background. If you need more features or dislike CLI then VS is the way to go. It has far better debugging, which can't be stressed enough. It also supports other things like WCF, WPF, aspx webforms, database projects etc. The biggest thing I miss from VS is being able to do a database compare. On the other hand if you do other stuff like PHP/Ruby/Go/Python VSCode is a great tool since a lot of them follow the same CLI based workflow. I might be mistaken having not used it extensively for a while but I don't remember VS supporting other stuff very well. Another good thing about VSCode is your workflow can stay almost the same on Windows/Linux/MacOS since VSCode is cross platform... but that has a lot to do with dotnetcore being cross platform too. 
Wrote something a while ago: https://medium.com/@matkoch87/6-reasons-to-use-a-build-system-92e6b67d0231?source=linkShare-b055055a1aac-1531103352
Nice spam. 
Ditto for rider. Faster too especially if you are spoiled and need resharper
Same boat (macOS w/ Windows VM running VS 2017 at work, but all Linux at home). VSCode works very well for .NET Core with the following extensions installed: - C# (a.k.a. OmniSharp) - C# extensions (YMMV) So far I've only been doing console / class library / xunit projects with the "dotnet new &lt;xyz&gt;" command from .NET Core. Been able to test/debug in VSCode just fine. I use GitKraken for doing the git stuff (as it's all open-source projects so far). Spent all weekend working on a C# side-project in VSCode. Next up on my list is using [Terminal.Gui](https://github.com/migueldeicaza/gui.cs) to do some basic console applications that can run cross-platform with full window UIs. I have a few ideas I'm kicking around where a text UI is all that I need. I'll also get into MVC/WebAPI in .NET Core, but probably not before we make the jump at the office. All of this seems quite easy to do in VSCode.
VS for Mac is basically Xamarin, from what I understand. Rider is probably the gold-standard, but if you're doing .NET Core then VSCode can be suitable.
Which parts of debugging are you missing when using VSCode? (I spent a lot of today in the VSC debugger on a .NET Core class library, so I'm curious.)
In general, what you're talking about (data-centric) should probably be split along the lines of: - A web API to serve up the data (in XML or JSON) and deal with business logic - Whatever you want for a front-end (MVC w/ Razor, Vue.js, React, Angular) which talks to the web API (We use Web APIs w/ Vue.js for our new stuff. But we also have a few legacy apps that are .NET 4.6 w/ MVC and Razor pages.)
I should've mentioned in the post that I did try to set up a new vanilla Ubuntu VM. But unfortunately, I couldn't repro it as well. So it's really "works on my machine" phenomenon.
What features did you miss? I use VSCode and just wondering if anything changed since VSCode has changed quite a lot in a year.
But do you understand WPF? When I accuse someone of being a fanboy, I generally mean that they not only are obsessed with a technology but also have no idea how it actually works so they can't even comprehend its flaws.
What about visual studio for Mac? 
I miss Silverlight. I still think there hasn't been a better technology for building browser based applications. 
I'm curious what CLIs you typically use that VS provides a GUI for. Things like csc, msvmon, msbuild, nuget, source control?
The CLI I use the most is dotnet. dotnet new, restore, build, run, test, add, ef and code-generator is most of what I've used for dotnetcore web apps. Another one is node and npm. I also use git via the command line often, but VSCode also has a built-in git client. A nice thing about VSCode is that it can open a terminal (powershell, WSL bash, cmd, git bash). I've used cmd in VS, but never tried using other command prompts since I usually stick to the UI in VS. 
What about getting a vm on azure with vs? 
thank you my friends
What is the purpose of having the API gateway, vs calling into the service APIs directly from the UI? Is this so the UI doesn't have to maintain N different endpoints?
Yes, the services should be kept internally. Any UI has to talk to the single API.
Doesn't this require changes to the API contract in both the gateway project and the service project if a service API changes, rather than just updating the request in the UI? And doesn't this negate some of the benefit with regard to teams working on and owning the development of a particular service if there is also this shared gateway? Not sure if this is a stupid question or not as I'm just getting into microservice stuff :)
I was going to write more about Gateway in the next post :). Having an API gateway has a lot of benefits - just think of authentication or authorization, not to mention load balancing, rerouting etc. You want your end-user to talk to the single URL, instead of having 10 different APIs and trying to figure out what are their responsibilities. You can implement API gateway as a separate application (as we did), simply map routes in your HTTP Server (e.g. Nginx) or make use of built-in services like Azure API Gateway.
I await the next post then :) The authorisation and authentication stuff definitely makes sense to centralise rather than spread between 10 services, but then you mentioned some sort of Identity service which I assumed was distinct from the gateway anyway. I'll have a think about some of the other stuff - I can see some of the benefits and drawbacks which seems pretty typical of microservices! Thanks for the article and coming and answering questions here
Identity Service - yes, it can be distinct (doesn't have to be), but if you think of configuring the same private keys or certificates for each microservice + setting up authorization policies, then one day, you want to change to another authentication provider - well, that's not to DRY :). 
Hi jxyzits, VS Code is more like a fancy Sublime or Atom in the sense that it's a generic code editor that can be extended with plugins, like the C# language extension. I have used VS code for asp.net core development and it went quite well. I have used VS on windows extensively for asp.net mvc web development and to be honest, it was refreshing not needing the great, but heavy, Visual Studio. When going with VS Code you do have to embrace the command line for many tasks that were GUI-fied by VS. But since you want to move to Linux on your home computer I have the sense that the command line will not be a problem for you, right? It will be more of a spartan workflow but then again, what do you really need? Proper code editing with some auto-completion, proper debugging tools and a straightforward way of interacting with your projects (like for adding NuGet packages, etc). All that can be done with VS Code and the command line tools from the SDK. Hope it helps!
If you really want to go full or semi SPA then I would also recommend Vue. In my personal, anecdotal experience I have found it to be the most comfortable to get started with. But, you should really think long and hard about if you really NEED a front-end framework. If so, know that you will be supporting 2 systems (front-end + back-end) instead of 1 (back-end with some html views). You need the skills and time to manage both properly. If your team, time and budget is anything but great, go for server-side rendering of the views with some javascript sprinkled on top, with something like jQuery or StimulusJs. Hope it helps :-).
Hi Peter02090, What you describe sounds liek a simple use case, regarding the api side of the things (don't know what's going on when generating the excel obviously). So, I would not worry too much about "architecture" as you only need 1 endpoint. I would use asp.net mvc or asp.net core mvc to receive a REST POST request to /reports/{yourNumber}, generate the report and return it with a 200 or 201 response. If your service will be used more than occasionally and you mention that generating the report takes "a while", then things get a bit more complicated. How often and how long does the report generation take?
Your post has been removed. Self promotion posts are not allowed.
What is this? There really isn't much here.
Have you seen the output?
Yes, interesting no doubt. Maybe add more explanation? Maybe it is appropriate to post snippets here, but I haven't read the rules in a while.
&gt; Not sure if this is a stupid question or not as I'm just getting into microservice stuff :) You're asking the right questions. Think about the complexity that microservices bring versus the advantages before embarking on a microservices or nothing endeavor. 
Good idea! I will rework this post :) 
There is nothing in software development that cannot be solved by another layer of misdirection.
Looking forward to seeing the result!
Except too many layers of abstraction.
What do you think now? :)
If I'm reviewing code and I see enumerables used with for(;;) instead of foreach, I'm already 90% sure that's a mistake. The whole point of enumerables is to feed then in foreach or other linq queries
You generally don't need to use "in". You'll know when you need it.
But you know that foreach is just a syntax sugar? From this foreach(var f in Fibonacci().Take(5)) { Console.WriteLine(enumerator.Current); } you have something like this: var enumerator = Fibonacci().Take(5).GetEnumerator(); while(enumerator.MoveNext()) { Console.WriteLine(enumerator.Current); } So you can write it like this for(var enumerator = Fibonacci().Take(5).GetEnumerator(); enumerator.MoveNext();) { Console.WriteLine(enumerator.Current); }
For the most part you can forward requests based on prefixes, e.g. `http://mysite.com/service1/*` will forward any requests with the prefix `/service1`, including `/service1/v1/toys` or however you lay out your service. As mentioned above, you can also terminate authentication/tls at the gateway so your tls certificate only needs to be installed in one place (if you want end-to-end tls you can even do internal/self signed certs since you own both ends of the communication btwn gateway and backend service). 
Yes but syntaxic sugar or not, what I mean is in 99% of cases the proper way to consume an enumerable is through the enumerator (foreach or not) 
Ok, I get your point. You are right. But first attempt even rewritten to foreach will still be problematic because of premature materialization of IEnumerable (`.ToList()`)
Well you shouldn‚Äôt be changing a contract if you‚Äôre adhering to OCP of SOLID, otherwise not making a breaking change when making a modification otherwise you‚Äôd be versioning the api. Yes it would be a change in two projects but there‚Äôs no reason in my mind to couple the two. 
Just add another layer of abstraction to abstract the abstraction away. ü§∑‚Äç‚ôÄÔ∏è
It's tricky, but we do it. We generate c# code with Roslyn using the fluent LINQ API on the fly to work on a flatten SQL view. It allow the sql operation like select, where and order to be done on the database. It supports navigation properties but it can be tricky for the in-memory joins (we didn't needed until recently as we use flattened views) we are looking on how to generate the includes as wells. It's a lot easier if you use OPENJSON but that's not our case, we do traditional SQL. 
Apple wasn't going to allow Silverlight on their mobile devices so it slowly got dropped by MS.
I came from GNU/C++, PHP and things like that in the .NET 2.0 era. I liked C# despite it came from Microsoft, and I really really wanted to hate it, but I couldn't. Turned me from a run of the mill Microsoft hating Linux goblin into a normal, grown up. But man do you learn stuff when you spend a lot of time in Linux, I'm happy I had that time. 
We use [ABCpdf](https://www.websupergoo.com/abcpdf-1.htm) at work for HTML-&gt;PDF but have [DynamicPDF](http://www.dynamicpdf.com/) in some places where we generate the PDF directly. 
Man... I get you. I did EF Core recently from the CLI and... it's painful. Totally agree. I think I need to find the issue on GitHub and see if we can get those prioritized higher. Another issue you will have with EF Core is scaffolding a DB Context from an existing database to a .NET Standard Library (not Console). #Fail. Example: [StackOverflow](https://stackoverflow.com/questions/48673987/trying-to-set-up-entity-framework-core-in-net-standard-project). There's tons of work that remains to do and every bit of feedback? We appreciate. We open issues and we push it forward. Never hesitate to hit me up directly (or tag me in a post) here on reddit or on Twitter on how we can make things better. I'm literally paid to receive feedback and make things better.
"Visual Studio for Mac" is based on MonoDevelop and completely different from the actual Visual Studio
Last time I tried VS Code I couldn't even get it to build/run my project. It seems like something Microsoft only created to silence people demanding a cross-platform "Visual Studio". But it isn't like Visual Studio in any way. If you want a good cross-platform C# IDE I can only recommend Rider.
Depending on timing of your project, Blazor may be production ready when you need it to be
Here is some positive feedback: * Core is fast. Stupid fast. * Cross-platform targeting is very easy (if you can remember the RIDs). * The new ML.NET library looks *amazing*. Kudos to the devs!
I'd really like to see some more day-to-day examples that I could use as a bog standard web dev. String manipulation particularly
Sounds interesting so please post it if you don't mind.
I'm not sure you really will. Pipelines are really for low level IO work. You'd probably benefit from Span if you are going to do string manipulation though. My only use case for Pipelines so far has been reading/writing to a TCP stream.
&gt; that I could use as a bog standard web dev. I don't think this fits most of your usecases then. This is mostly for backend/api services.
I was working with the MS stack before .Net because I used classic asp in my internship and it eventually led to a job. When C# and .Net were released, I started to really like the tech. One of my co-workers comes from a PHP background and he seems to be really liking C# and .core on the project we are currently building. He was even able to influence a dev at the client's offices to open up to C# instead of PHP.
This. Microsoft likes to pitch VS for Mac as way more than it is. It is for Xamarin development and can debug basic .NET Core applications. I lose intellisense constantly with VS for Mac... I am really hoping Rider adds better support for Xamarin development.
As a bog standard web dev, you're unlikely to ever use this. The JSON parsing library that you're using will hopefully use this in the future.
You don't really need this for string manipulation, but you could use `Span&lt;char&gt;` for that, which is somewhat related.
Yep, this would've been very nice when I was writing an RTSP server 5 years ago. :)
vs is a full blown ide vsc is a full blown text editor depending on what features you need, it may not work out. The old standbys may work too: windows vm, rdc to a windows box, etc
ASP technologies is simply gonna dominate the worldweb... every day it just becomes faster *_*
Just realised I meant span not pilelines.. doh.
Yes you are right. Something is wrong in the Cheatography website. You can still download the PDF by clicking thumbnail right next to the link. https://www.cheatography.com//oba/cheat-sheets/dotnet-cli/pdf/
I already see a use case for consuming web APIs on our backend services. We fetch data from various third party sources and cache it on our servers. The pipes would simplify and speed up what we do. In fact, the code examples are textbook demonstrations of all the things we get wrong in our application.
Ah thank you. Wish I'd known that before I printed and laminated the less detailed PNG that only covered one page. Thanks for the link though. :P
I'm trying to learn Microsoft's tech stack starting with .net core 2. I would love to hear the answer to your question too.
MS has done a good job with Azure and .Net Core which are good reasons to stick with .Net stack. It's hard to project too far into the future but I'm not terribly worried about the bottom dropping out on my career sticking with a lot of .Net right now. 
I've had a duplex stream class in my personal utility library forever that I'm hoping I can replace with the Pipeline class. That will hopefully also allow me to get rid of the reference to Microsoft.IO.RecyclableMemoryStream
Do a job search on monster.com or dice.com or indeed.com, compare tech and languages like azure, c#, python, ruby, aws, etc.
I've been told it's a regional thing. Around Kansas City the job market is as favorable as it gets, and a decent dev could find a new job within a week or two if they weren't picky. I don't see that fading any time soon.
The same in Houston. There's a ton of .NET positions and relatively little talent. Cost of living is low and salaries are high. It's a great time to be a .NET dev in Houston.
Hi, It is the right time to learn and implement what you have learnt. ASP.Net Core is the next generation open-source framework to help build dynamic cross-platform websites. With the ASP.Net, Microsoft has tried to create an amazing platform that is fast, flexible, and modern. However, learning ASP.Net is not exactly easy, especially with a textbook approach at the subject matter. In my experience, I really liked Projects in Asp. net course on Udemy. It had minimum theory and more practical projects which i can actually use in my portfolio or in other projects. Here's the link if anyone wants to try it out : [https://www.udemy.com/projects-in-aspnet-core-20/?couponCode=UJASP18](https://www.udemy.com/projects-in-aspnet-core-20/?couponCode=UJASP18) Hope this will also help you to learn new language and to create project. All the best!
Is this the kind of thing that underpins httpclient? And the new httpclientfactory?
[removed]
I don't think CRM Dynamics 2015 Audit SDK is a part of .NET or a .NET "feature. 
Is this an API to avoid using streams?
I would think so? But it probably doesn't since HttpClient as been around longer and changing its internal implementation would probably break compatibility. However, I'm expecting it to be revised for a future upgrade in Core.
Tried the sample for binary classification last night, replacing the data with a few lines of junk food nutrition info I got from Google, predicting whether a new food was or was not junk food based on its salt, sugar, and fat content. It worked pretty well!
I dont know about the states, but in europe .NET is doing great, we have amazing huge project everywhere, its the highest demand here.
Found this topic in my search for answers on this myself. So because of the changes to identity in [Asp.net](https://Asp.net) Core 2.1 you need to scaffold out all the pages you want to edit. All the code for the identity stuff is in a code behind page. Here's a screenshot example. [https://i.imgur.com/IlzWSmW.png](https://i.imgur.com/IlzWSmW.png)
Indianapolis seems to be a pretty good market as well, I've done decently here since 2013.
That is so cool! Did you happen to put it in a GitHub repository? Those makes amazing demos! üòÅ
All of my reports are done using SSRS and calls to the SOAP service from the server-side code to return byte arrays of the rendered report. Microsoft has not seen fit to provide a client-side implementation of the ReportViewer control for MVC, so ff you need to display the report in the view, you can use your scaffolding framework of choice (typically Bootstrap) for grids of data and a JS library like Chart.js to produce nice graphical charts for the reports. 
Got any examples/documentation of the apis?
My code is like 80% the same as their sample. Not much new brought to the table. In fact, I had an issue with the paths since I created a fresh project instead of working with their multi-project solution. The datasets must be in the same directory as the debug output right now. I put it on GitLab since that's what I use for keeping my own example projects. https://gitlab.com/mattwelke/ml-net-test-junk-food You can run it by opening it in VS Code on any OS and running the task.
That provides some context. Merging all those application will indeed be some work. Perhaps you can tackle the ease of use directly? Maybe a common navigation menu included in all apps, as well as shared authentication.
I use PowerBI for minor reporting, let's me embed and able to see the data underneath
Please note that there are also post compilers (like fody configureawait), which add the call to every async operation automatically (especially useful for libraries)
Olathe .NET reporting in
Tonne of options out there. But without knowing full scope hard to advise accurately. I created a dashboard for a production line could months ago which used elasticsearchand kibana virtualizations embedded on it. Which was quite powerful and easy to do. But so much stuff out there all specialising in things just a little differently. Another option Telerik controls that would give you a lot of design control but that's a bit pricey. I'd recommend trying to do your reporting as much as possible in the front end to give it as smooth experience as CV possible. 
Is there any use to adding ConfigureAwait to every call if I'm working on a service that runs in console or as a Windows Service? The way I understand it is it's only useful if there's a chance you could be consuming the async stuff from a UI thread. 
Looks interesting, I look forward to having a play with it tomorrow as I'm refactoring a bunch of stuff to async right now :)
Yes. PostSharp is worth mentioning too.
That's true, you don't really need this for console/service apps. You just cannot reuse this code in UI applications then.
The UI thread (or not) is not correct thinking. More precise would be presence of some SynchronizationContext instance. That said if you're strictly in console apps or windows services it's not going to change anything. But as long as it's a i.e. library and you plan to reuse it, it's a good idea to have it there.
You can reuse it. It's just going to be tiny bit less performant in general cases.
Or lock up on you. 
I just used [Microsoft's own documentation](https://docs.microsoft.com/en-us/sql/reporting-services/report-server-web-service/accessing-the-soap-api?view=sql-server-2017) on it. Generate the C# code from the WSDL, wrap it into a nice library that you can supply simple report parameters to and it's good to go. Of course, if you're using more than simple parameters, it can get tricky. The most difficult thing is pulling and setting parameters correctly when making calls to the ReportExecution2005 service. If you try to pass an incorrect value type/range, a value that's not possible (dropdown parameters), too many parameters, not enough parameters, forget a required parameter, etc....well the errors are not always descriptive. Sometimes it simply fails silently with a generic error. In a simplified nutshell, what I do is call the ReportService2005 management service and pull the list of parameters for the report (name, type, single/multi-value, nullable, possible values, default values, etc) and store them in a list. I can then cross reference the parameters list against what was returned in the ViewModel during POST. If any of the parameters are not valid, are missing, or whatever, I simply add the appropriate ModelState error and return to the view. If it all checks out as valid, I call the ReportExecution2005 service, set the execution parameters from my parameters array, and call the Render function to generate the bytes. My wrapper then returns a ReportExportResult item with the byte array, file format, filename, and some other data or errors. That can be used to return a FileContentResult from an MVC action with the report file for download or, in the event of errors, either a ModelState error or a safe redirect to avoid exceptions. 
https://old.reddit.com/r/dotnet/comments/8xrs83/net_core_july_2018_update/ why all the repeated posts with `?WT.mc_id=none-reddit-marouill`appended?
Hey, I'm a .NET dev in Houston too. If we battle to the death there will be even less talent. I mean, nice to meet you. 
Doesn't really have anything to do with IEnumerable, this is LINQ's [deferred execution.](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/concepts/linq/deferred-execution-example)
I've used this library in an old internal project. https://github.com/armanio123/ReportViewerForMvc It feels fairly hacky but worked fine for my purpose.
No, that's not correct. If you don't do any blocking, which you shouldn't anyway, there's never going to be a deadlock situation from this.
DevExpress has some nice reporting components. 
Ok! Sorry if this is not the reddit way, but instead of deleting my post I will reply with what I did to fix it, in case someone down the line comes here with the same issue. All I needed to do was enable SSL in my code: `SmtpClient mailClient = new SmtpClient("");` `mailClient.UseDefaultCredentials = true;` **mailClient.EnableSsl = true;** `mailClient.Send(emailMessage);`
*high five*
I've been told that Atlanta isn't a .NET town. That doesn't stop me from getting messages from recruiters daily to work in .NET.
Wait, even though 4.7.2 was released a couple of months ago, and this includes some improvements to it ("reliability fixes") that make it materially different, both versions are still called "4.7.2"? I must be missing something, because something in my head keeps going, "Well, obviously it should be called '4.7.3'. It's different than the thing that already exists that's called '4.7.2'." Can someone please help me understand the wisdom behind giving the same name to both 4.7.3062 and 4.7.3081?
Your post has been removed. Self promotion posts are not allowed.
It actually isn't, Java and Node are far more popular than .Net. I manage in a shop that uses all 3 and Java and Node tend to be tougher roles to hire for. That being said, we still have to work hard to get a .Net dev compared to none dev jobs in general. Market is just crazy in this town however you cut it. 
[removed]
Please what do you mean by self promotion posts ? This post is about implementing a functionality I don't see self promoting content... or if I'm wrong, let me know please. 
&gt; [WebAssembly](http://webassembly.org/) that executes normal .NET assemblies Can't wait until management tells me we don't have to develop for the web anymore. /s
Me thinks .net core is not yet mature enough to be used for large enterprises. What do you guys think?
How could i achieve the shared authentication? The applicationa would have to be under the same Identity Framework? Or should i create a new db only to handle the authentication? :p For the navigation is there a way to insert the same menu in the websites without replication code in each applications _Layout? Thanks!
I've been following and building an app using Blazor and have been having fun with it. I hope it continues to get developed, gets officially supported and develops a community. Everytime the Blazor team puts out something new I download and try it out right away. 
Have the support for hot reload implemented yet?
Yearning for this to achieve escape velocity. So tired of my company's hacked together webforms... I don't even know what to call it. Let's just say "mess" in the same way that the Titanic was "inconvenient".
Do you mean being able to change the code and have it automatically compile and reload in the browser? I use VSCode and as far as I can tell it doesn't. I never tried in Visual Studio even though they recommend VS to test things out. It can do a dotnet watch run and it will recompile when a .cs file changes but not when code is changed in an cshtml file. Seems like they are tracking this feature/issue here: [https://github.com/aspnet/Blazor/issues/5](https://github.com/aspnet/Blazor/issues/5)
dotnet watch is slow thought. Right now all the SPA framework supports hot reload so if you change the code, the UI on the browser will be updated without losing your existing states.
I don't think Blazor is intended to be a production platform, it seems more experimental at the moment. 
Me thinks one shall not simply call himself in third person. /s Also, i think it is definitely production ready, it always depends on what you want to produce. - Simple, Fast WebAPI for internal communication? Sure - Secure, WebAPI for Web-frontends? Sure - (Windows-) Service that runs in the background? Yepp - Windows Desktop Application? - Nope, nothing stable yet. -- Avalonia (Lookless, WPF-like) and UNO (Native look, WPF-like) look good though 
Not a great article. Sometimes refers to ‚ÄòBlazer‚Äô and shows an IE icon in the browser list supporting web assembly. Style of writing is too chatty for my liking as well, even without the grammatical errors 
I am aware of that. I am saying that the issue of hot reload is going to have a massive impact on programmer's productivity and adoption for Blazor. Having C# and WebAssembly is nice. but losing the hot reload is a high price to pay.
On Desktop app, I'll have to agree. Lots of thing will get fixed by .NET Core 3. https://blogs.msdn.microsoft.com/dotnet/2018/05/07/net-core-3-and-support-for-windows-desktop-applications/
Can't the same be achieved by a single database with more levels of authorization, and distinctly separated by their primary keys and foreign keys?
How is somebody going to log into the site to even make an account if everybody has their own database? What I think you want is something like a multitenancy setup. 
Interesting. That's my Youtube thumbnail, but that's not my video.
I could've sworn I saw one of the earlier Blazor videos where they hot reloaded in VS.
yes I already achieved this within the same database, but I imagine that if I had like 1000 admin accounts and each admin has 10 users underneath him it would be difficult to keep track of all the users and their data since Identity framework saves all the users in the same table, this is only to organize this whole structure. I'm not sure if this is a good practice, I'm a beginner in databases in general. what do you suggest? 
Is this just an update of the runtime or was there a bunch of churn in the APIs too?
only admins have their own database, the admin has access to a controller that allows him to create users underneath him in the structure, those users will be stored in the same admin's database. example of the structure : https://m.imgur.com/a/5GyUzri 
Basic simple way I found on the web... public class ModelStateValidationFilterAttribute : ActionFilterAttribute { public override void OnActionExecuting(ActionExecutingContext context) { if (!context.ModelState.IsValid) { context.Result = new BadRequestObjectResult(context.ModelState) {StatusCode= StatusCodes.Status422UnprocessableEntity }; } base.OnActionExecuting(context); } }
1-to-1 gif to text ratio means this is a garbage "article"
Thank you for the suggestions. For now I'm using my own csv generation, but I don't think that's a good long-term solution. Our applications that need reporting are fairly small and all internal-facing, so spending a ton of money on a 3rd party solution (looked at Telerik, DevExpress, and Crystal) seems like a ton of overkill.
A few guys I know are excited about this. They're primarily VB.NET Web Forms developers who have been forced to make the switch to C# and MVC after much resistance. I've been saying we should start thinking about React, Angular or Vue, but they refuse. I guess I'm not opposed to this idea, but I'm worried about siloing. These guys would easily use something like this as an excuse to never have to learn ES6 JavaScript and then become siloed in their careers.
Yeah but that's what I'm saying, I don't think Blazor is ever going to be a production-ready system or at least it's very far away from being that. Hot-reload is less of a concern simply because you shouldn't be using it to do anything productive.
exactly what I was thinking. Mutiple databases seems pretty inefficient to me.
But why create a new database for every admin? That has to be a nightmare. You can easily create roles to handle permissions for every admin. I can't think of a legitimate reason to make a new database for every admin.
I'm at a point where any company who's still dealing with Web Forms is a hard pass for me. Having gone MVC and then Web API with whatever the hell you want for the front end, Web Forms is just an exercise in frustration at this point.
Its servicing so no new API, I think 
It has released 4 versions so far (1.0, 1.1, 2.0, 2.1) so it should be fairly proven now?
Why would it be difficult to track the users? That is exactly what queries are used for, to filter down data in tables.
Here are some additional reasons why a database-per-admin is a bad idea 1. You have 1000 user databases (you mentioned you have 1000 admin accounts). 1. You are unable to scale 2. What if your organization changes and you need to add or remove administrators? 3. What happens to the users under those administrators 2. What if you need to facilitate a schema change to a database? Are you going to maintain and deploy changes to each one? 3. What if you are trying to run a reports that query against all administrators? 1000 queries for a single report? 4. What happens if an administrator changes their user name? Instead, you should stick to a single database and utilize indicies and keys that exist to solve these type of problems.. You can even extend your design and have a grouping table (say like an Organization). Then users and administrators belong to an organization (FK to an Organization), which keeps its flexible and allows for multiple administrators per organization.
Actually, it's not difficult to keep track of all users in a table if you have more fields to store their different data e.g. belonging to whom. It is the same as storing 10000 employees in your organisation each having different posts and supervisors. Usually we store all the records together. We can actually add fields to the AspNetUsers table too, via Entity Framework, and i think it is even possible to have another model for the Users themselves, either directly hooked up to Identity server, or, via a Foreign key from the AspNetUsers.
As not-a-front-end guy, I keep my knowledge of everything related to javascript and css very minimal.
Sorry about that. This came up in google search as "reuse". I will add the videos link to the image if you send it to me. Cheera
From what I read, it's a minor version which means bug fixes and patches. In this scenario, it's a security patch related to certificates.
And then there's my shop that includes all the MVC goodies in the project, but follows ASP.NET coding practices. That gives us horrors like 10,000 line controller classes, methods with 20 parameters or more, and all AJAX form submissions because apparently switching to MVC took the away the ability to submit forms normally. Did I mention that there are 80 tables with absolutely zero relationships or constraints, and in many cases just use varchar column types to store everything from GUIDs to date and time.
There are 3 gifs!
No worries, I assumed it was along those lines. Thumbnails have been a 'do better at this' thing on my todo list. If people are reusing them they aren't as bad as I think. Maybe a bit generic though. :) Here is a [link to the source video](https://www.youtube.com/watch?v=qDoSAnwNgQI). I think I still have the .psd lying around if you'd like to riff on it for your article, I'd have to check. 
There was an early prototype , but it was removed . Steve (the lead architect of blazor) explained that there were technical challenges and they would add it back eventually. 
It will be , and per the team hit reload is coming at some point . There‚Äôs just other bigger issues to tackle first . 
I think MVC is fine if you're working on something pretty straight forward, like certain internal tools or even a simple customer facing site, but beyond that I feel having completely separate front and back ends to be more desirable, especially if you also plan to have mobile apps that may use the same API endpoints.
Done. added the link now.
The default Angular template doesn't seem to have working HMR, all it does is reload the page when a file has changed (i.e. losing everything except the URL).
&gt; webforms I'm so sorry
Really? Damn they are missing out.
Sorry, default 'dotnet new' angular template. Didn't mean all of Angular (e.g. ng new...)
I manage an aspx intranet....i dont wanna touch it
Same with create-react-app
I‚Äôm still supporting some classic ASP apps, so working with web forms is a welcomed site some days.
There isn't a single SPA that supports it directly out of the box, and most solutions only store state in global state solutions such as Redux, Mobx, etc. Most things require re-loading, if it just automatically did everything by default, what would you expect to happen if you rebind a click event? Using Webpack you still have to tell hmr to remove and re-add that specific element for it to work.
Whatever is the latest is more desirable. We'll be circlejerking a new paradigm before you know it.
Probably, but at the very least I think most could agree that anyone still doing Web Forms is waaaay behind the times and will find it harder and harder to find people willing to work for them.
Are they strictly web developers? If so surely you need not piss against the wind for your own sake. However there are some of us that touch everything from stored procedures to windows services, to WPF, to WCF, etc. I'm sorry but I can't learn you latest flavor of the week web framework. Not because I don't want to, but because next week there will be a new flavor of the week.
Doing any sort of new development in webforms, you have to be nuts. However MVC is now basically undesirable as well, the view part specifically. God forbid you mention jQuery.
MVC is at least still tolerable. But yeah, I've moved pretty hard in the direction of Web API for the backend, and something with Angular/React/Vue on the front.
Even knockout.js is dated but better than native web forms 
I was approached about some contracts with my state recently and Knockout is one of the "desired" items. Yeeeeeeeah, 'bout that.... May as well just list Silverlight at that point.
Honestly I tried linking them to the Microsoft docs for [Client-side development in ASP.NET Core](https://docs.microsoft.com/en-us/aspnet/core/client-side/?view=aspnetcore-2.1) as well as [Common client side web technologies](https://docs.microsoft.com/en-us/dotnet/standard/modern-web-apps-azure-architecture/common-client-side-web-technologies) to show that hey, it's not like I'm asking you to learn Clojure or something, this is just how modern apps on the .NET platform are built. But the issue is some of them are still writing greenfield in VB.NET. I got the whole ".NET Core is not approved" and "this would take 100's of hours of training" runaround.
A database per administrator is likely a red flag, but a separate database instance per tenant is sometimes superior. Some industries require hard separation of data, for instance. Additionally, multi-tenant database instances can grow to enormous sizes and require where clauses on nearly every single query, greatly increasing the chances of data leakage. Separate databases merely require a connection string factory, which has a tiny footprint and is easily testable. 
That doesn't make sense. Of course the ambition is to make something that can be used for production. Why wouldn't it be? It is just a realization of WebAssembly in a C# context, and WebAssembly is very much intended for production. You are right that it's experimental right now, but I just don't get where you get the "ever" from.
Because it says [right here](https://github.com/aspnet/Blazor) on the github page for it: &gt; Note: Blazor is an experimental project. **It's not (yet) a committed product.** This is to allow time to fully investigate the technical issues associated with running .NET in the browser and to ensure we can build something that developers love and can be productive with. During this experimental phase, we expect to engage deeply with early Blazor adopters like you to hear your feedback and suggestions. It mi**ght become a fully-fledged product, or it *might* be shelved in favour of something else that comes from the learnings of it.
Yup. Replace "not ever" with "not yet", and we agree.
Well what I originally said was: &gt; Yeah but that's what I'm saying, I don't think Blazor is ever going to be a production-ready system **or at least it's very far away from being that**. 
Follow-up: FYI, that link only works to download it on mobile. When I click it on desktop it just brings me to the cheatography site with that loop.
This one's great. https://www.amazon.com/C-7-0-Nutshell-Definitive-Reference-ebook/dp/B076DMK61S
How big is the initial download for the user? I seem to recall it was a bit big, at least when it first started.
(Picard voice)
The project I'm on now has this custom MVP framework built by the original architect on top of web forms. Didn't even leave us the source, we had to decompile it. This is one of the better run projects at work.
I don't think they have done much to optimize that yet. It is about the same size as far as I know, but they hope to improve it.
This is going to be really helpful for blazor, I'm thinking.
Why are you worried about their careers? I'd focus on whether or not the technology is suitable for your purposes.
&gt;Why are you worried about their careers? I don't want to get into details about where I work but I'm not really concerned about their careers. I'm pretty good with AngularJS, but I'm not allowed to use it at my current job. jQuery only. The rest of the job is good, but I'm worried about falling behind. I feel like I could pick up React or Angular in short order, but as I started dabbling in React I started really enjoying it. But I know it will be a fight.
Thank you! Looks great! 
vue-cli configures HMR for you without even asking.
Then that template needs to be fixed. Doing SPA nowadays without HMR should be considered malpractice. 
It configures hot reloading, and only for template and style. HMR will still destroy the component and state for &lt;script&gt; changes, just like angular. [https://vue-loader.vuejs.org/guide/hot-reload.html#state-preservation-rules](https://vue-loader.vuejs.org/guide/hot-reload.html#state-preservation-rules) &gt; When editing the &gt; &gt;&lt;script&gt; &gt; &gt;part of a component, instances of the edited component will be destroyed and re-created in place. 
The state of VueX is preserved. That's the one that matters.
This would work for a REST api but this doesn't solve the problem the post addresses.
This post talks about Automatic ModelState Validation when using Views and Razer. Validation for ApiControllers and SPA's is pretty straightforward but there's not much content on doing the same thing when using Views.
This one is also great: [https://www.manning.com/books/dotnet-core-in-action](https://www.manning.com/books/dotnet-core-in-action)
Really good read. 
Thanx.. I'll check it out later... 
If I were writing my own Task returning methods in Core 2.1, should I always return a ValueTask? Are there cases where this is a bad idea, like using struct vs. class?
The last link in the article answers this pretty well, imo: https://stackoverflow.com/questions/43000520/why-would-one-use-taskt-over-valuetaskt-in-c
I clearly didn't read carefully enough :) Thanks!
You probably want hangfire https://www.hangfire.io/
That looks promising, I'll check it out. Thanks! 
Why not go directly against the scheduler or the cron?
I don‚Äôt have any experience with this sort of sokutuons at all and I‚Äôve never done anything with cron jobs before
Try some of these: http://nugetmusthaves.com/Tag/cron
I've used [Quartz.net](https://Quartz.net), but I'll be honest: it's a little wonky. And running scheduled background processes in a .Net web application has never turned out great for me in general. I've taken to building these jobs as separate processes. If something really needs executed at a given time I use Windows' Scheduled Tasks to make it happen. But more often my application triggers an event that requires heavy processing. So I push the work to a queue, and let a separate process pick up the word and process it. That scales well.
Great read, thanks. Can you recommend some more articles/blogs about .net inner parts?
Use Hangfire. We've been using it for 3 years without a problem. Just configure your App Pool and Website to be always running.
Hey there - I've done some work on JWT lately and one of the big wins is the way the service middleware configuration completely abstracts you from the process. Once you've followed that guide, it is purely down to the token itself (coming from a valid issuer) on the client-side. I think what you may be looking for is a sensible design pattern for standard user auth
I don't think there's any scope for dotnet to do this for you, but you may find some useful guides on best practice 
You can use IHostedService: https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-2.1
Thank you for the response. You are correct about JWT easing up the process but, I'm talking about implementing JWT in my API. The problem is that unlike MVC5 where the `AcountController` was auto generated along with all the code that handles user login, signup, roles and auth, Core 2.1 doesn't have that (at least in my wepAPI project). Following is a code snippet form the article I linked: public async Task&lt;object&gt; Login([FromBody] LoginDto model) { var result = await _signInManager.PasswordSignInAsync(model.Email, model.Password, false, false); if (result.Succeeded) { var appUser = _userManager.Users.SingleOrDefault(r =&gt; r.Email == model.Email); return await GenerateJwtToken(model.Email, appUser); } throw new ApplicationException("INVALID_LOGIN_ATTEMPT"); } It's a fairly simple controller that handles the login and returns a JWT token if the login is successful. The problem I have with implementing this approach is I am not sure if this is the *best way* of doing this. Since I don't know where the code is coming from and if it is even correct. What I'm trying to find is an *official* implementation of the Auth functionality provided by Microsoft itself, like we had in MVC5 (again, about how the AccountController was auto generated). Is this possible? Or the only way of doing this is to gather bits and pieces from different unofficial sources? Don't get me wrong, I'm not saying the unofficial sources don't know what they are talking about (I'm just a beginner myself). It's just that I'd be more comfortable working with a solution provided by MS.
Hangfire and IHostedService look viable, but I thought I'd throw this out there. If you're running a Windows server, you could create a task in Windows Task Scheduler. The task would send a web request to a specific endpoint on your site (e.g. using PowerShell's Invoke-WebRequest). The endpoint would then do all the things when it receives the request.
Do MS have an example to follow? They don't have an out-of-box solution in Core afaik
I was not able to find a MS guide for JWT implementation but the article I linked looks good. I wish there was an out-of-the-box solution for Core. But I guess I can live w/o it too ;)
Quick google: https://holon-platform.com/blog/api-development-securing-apis-using-jwt/
I've been using this for the past year in our site. It's working great. The process we use it for allows a user to generates a combined pdf from search results and then email that pdf out to the user. The process is kinda hefty so being able to run this in the background is awesome! Hangfire also gives you a very extensive backend for administration and monitoring.
I think you may be looking for [this link](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/scaffold-identity?view=aspnetcore-2.1&amp;tabs=visual-studio) The short answer, is the in-built controllers etc are precompiled and provided as a runtime so they no longer exist in the source. The good news is there is a way to scaffold them out to get the source files like how it used to be. I don't know if that helps what you're trying to do, but good luck
This is also what I do. Windows Scheduled Tasks is much more reliable.
I think I went through what you are doing. From what I can tell JWT is relatively new and there really isn't a "right" way or standard practice on how to do it with dotnetcore yet. You can try looking at this article if you haven't already: [https://auth0.com/blog/securing-asp-dot-net-core-2-applications-with-jwts/](https://auth0.com/blog/securing-asp-dot-net-core-2-applications-with-jwts/) That one helped me to get things to work although there seem to be issues when you need to refresh or revoke the token.
1. I'm quite sure you can run unit-tests without Test Manager (at least you can surely do this in Onpremise TFS ). 2. Not very hard. Gitflow is nice start, but you should tailor your branching strategy to support your release cycle.
* What sort of tests do you use in your projects? * Have you ever had any need for Test Manager? 
1. We have unit tests and manual tests. Unit tests run as a part of our build process, while manual tests are performed in Test manager by human test engineers. 2. Of course - for manual tests this is a nice tool.
We are still early in our new project and have only started to scratch the surface with our requirements. However I find those 52$ per month per user a bit pricey. I suppose you guys only have those licences for your test engineers?
If you are just starting you clearly can live without Test manager - it is good for running large test plans covering many requirements. If you have less than 50-100 test cases, I think, you could live with Excel or Google docs spreadsheet for tracking manual test runs.
Thanks for the answer. Yeah, we are still a long way from automating such manual tests. Other than what I mentioned above, are you satisfied with VSTS and ease of use? Do you find you are fighting the tool often?
Microsoft did really great job with VSTS and TFS OnPrem in last 2-3 years - it is very powerful tool with nice UI (i'm talking about web UI), supporting complete ALM (Application Lifecycle Management) lifecycle - from creating requirements to deploying application to final environment. There are many more lightweight solutions such as JIRA for requirement management, but you will have to combine those different solutions to work together, while MS gives you all of it integrated, supported and working out of the box.
PATCH is an http responsibility, so put it in the controller. Your service layer shouldn't have to know how to handle http verbs.
I'd recommend that you stick with a single database. Use unique identifier primary keys and avoid integer identity keys, so your database would be easier to "shard" in the future if you end up needing to scale out. Just create a "organization" table, and then a "organization membership" table that will connect your users to different organizations. Yes, you'll be using where clauses to segment data, but there are ways to write your code defensively to avoid data leaking between organizations.
Yes, I'll try it out, it seems to have the features I'll most likely need!
That article is pretty old. The gist of it is probably still okay, but Core 2.1 was recently released. So I'm wondering if they've changed stuff around identity and JWT.
Just to chime in with another experience. We used TFS and migrated to VSTS this year. VSTS has been amazing for us. We run plenty of tests and don't pay for Test Manager (I'm not even sure what that does if I'm completely honest). We run tests as part of our build, unit tests, integration tests, we run selenium tests as part of our release, load tests, etc. We pay $0 for VSTS (All our developers do have MSDN accounts, though). Also we do use gitflow. You can use whatever branching strategy you like with VSTS, it's just git. You can add policies to branches that prevent people from committing directly to it, forcing the use of PR's. You can also define criteria for approving PR's, such as a successful build of the changes and x number of reviewers (or specific reviewers for specific products). The *only* thing with VSTS and gitflow that is annoying is there's no easy way to merge a release branch into both master and develop, you have to do that as two separate PR's (if you've locked down the branches like we have). We really love VSTS.
Technically it wouldn't. The patch library for .NET core can function without anything HTTP. The service layer could take a strongly types json patch document as a parameter and apply the patch. 
Consider what you really need before going all-in with that git flow. http://endoflineblog.com/gitflow-considered-harmful http://endoflineblog.com/follow-up-to-gitflow-considered-harmful http://endoflineblog.com/oneflow-a-git-branching-model-and-workflow We do mostly web and don't support multiple different versions in prod, so we use a very simple (mainline style) workflow.
I would stick with task scheduler and a console application instead. It‚Äôs the most reliable solution. Otherwise you have to force something to constantly run on the web server which degrades performance. 
Perform the actual patching logic in an application-specific way in your service layer. Build an HTTP-ready wrapper method around it in your controller.
Test Manager is completely different than running tests during a build. https://docs.microsoft.com/en-us/vsts/pipelines/test/getting-started-with-continuous-testing?view=vsts 
AWS Lambda or Azure Functions as another alternative that's not been mentioned. 
Check to see if they are logged in and if they are not, prompt them to do so.
Pretty sure if you set up CI in VSTS you can as part of your build process get it to run all of your unit tests. In the build overview it will then give you an indication how many tests passed and failed
The things is I dont want that the users have to intervene in the process. I have the username and password,i just need to know how to log them via c# :D
a JSON Patch document is just a document irrespective of which HTTP verb was associated with the request (or for that matter whether HTTP is involved at all). OP: "it's clearly a technology built for the controller layer" It's a technology built to take a document and apply a changeset to that document. If it serves your purposes to put that functionality in your service/business layer then full speed ahead.
&gt; I have the username and password How do you have their password? If you've stored the password... well... that's a big big security issue. And I'm a bit unclear... the user is running the application but somehow hasn't logged into their machine? Is the app running on their machine? Here's a class to create a security token as a user: using System; using System.Configuration; using System.Security.Principal; using System.Runtime.InteropServices; namespace Whatever { class Impersonate : IDisposable { #region OS Calls [DllImport("advapi32.dll", SetLastError = true)] public static extern bool LogonUser(String lpszUsername, String lpszDomain, String lpszPassword, int dwLogonType, int dwLogonProvider, ref IntPtr phToken); [DllImport("advapi32.dll", CharSet = CharSet.Auto, SetLastError = true)] private static extern int DuplicateToken(IntPtr hToken, int impersonationLevel, ref IntPtr hNewToken); [DllImport("advapi32.dll", CharSet = CharSet.Auto, SetLastError = true)] private static extern bool RevertToSelf(); [DllImport("kernel32.dll", CharSet = CharSet.Auto)] private static extern bool CloseHandle(IntPtr handle); private const int LOGON32_LOGON_INTERACTIVE = 2; private const int LOGON32_LOGON_BATCH = 4; private const int LOGON32_LOGON_NETWORK = 3; private const int LOGON32_PROVIDER_DEFAULT = 0; #endregion private WindowsImpersonationContext _ImpersonationContext = null; public Impersonate( string userName, string domain, string password ) { WindowsIdentity tempWindowsIdentity; var token = IntPtr.Zero; var tokenDuplicate = IntPtr.Zero; if (RevertToSelf()) { if (LogonUser(userName, domain, password, LOGON32_LOGON_NETWORK, LOGON32_PROVIDER_DEFAULT, ref token)) { if (DuplicateToken(token, 2, ref tokenDuplicate) != 0) { tempWindowsIdentity = new WindowsIdentity(tokenDuplicate); _ImpersonationContext = tempWindowsIdentity.Impersonate(); if (_ImpersonationContext != null) { CloseHandle(token); CloseHandle(tokenDuplicate); return; } } else { throw new ApplicationException("Can't DuplicateToken"); } } else { var l = Marshal.GetLastWin32Error(); throw new ApplicationException(string.Format("Can't LoginUserA: {0}", l)); } } else { throw new ApplicationException("Can't revert to self"); } if (token != IntPtr.Zero) { CloseHandle(token); } if (tokenDuplicate != IntPtr.Zero) { CloseHandle(tokenDuplicate); } return; } #region IDisposable Members public void Dispose() { if (_ImpersonationContext != null) { _ImpersonationContext.Undo(); } } #endregion } } Here's how you'd use it: using (var Impersonation = new Impersonate("someone", "password", "mycompanydomain")) { // any code here runs under the "someone" user identity... basically you've switched the current process's identity } The Impersonate class is done as an IDisposable to try and ensure that the _ImpersonationContext.Undo() is always called because otherwise it is too easy to **stay** impersonated (if you see what I mean). I haven't tested this code very much at all (for my needs I ended up going in a different direction that didn't need impersonation).
Why is it wonky?
&gt;PasswordSignInAsync hey cs\_stud, I see you've used `PasswordSignInAsync`, do you want to login the user? If not, as far as i know the `PasswordSignInAsync` will actually login the user. So maybe it is better to use `CheckPasswordSignInAsync` instead, because all you want to do is **check if the credentials are valid** then **return the token**. 
If the only thing you need is running unit tests and having fail/pass depending on the result, you don't need Test Manager. VSTS can run unit tests in your Build phase, and make the build fail/pass depending on your test results. You would not get the Test Manager UI that is almost identical to Visual Studio's test tab, but to me that's not worth it. When your build fails you get the report that says which tests failed, open up your solution in Visual Studio and rerun your tests then and that's enough to get to work for me. The real value in Test Manager I think is the whole Browser testing and User acceptance testing. 
I monitor [https://www.alvinashcraft.com/](https://www.alvinashcraft.com/) everyday, but I use Google Feedburner to get a daily email. It has good articles all the time.
I second this. Ties very nicely into the Linux way. Reliable so far. And no other party dependencies
It may just be my use case. I used it on an API which is run from ten IIS worker processes at once to manage load. Juggling jobs and ensuring that job that should only be done once only gets done by one worker has been difficult.
If you can I'd suggest either running it as a Windows service. If that's not desirable then you could have it scheduled as a database job in your database 
Setup a total variable that stores the number of bytes on key press that each character takes up. System.Text.ASCIIEncoding.Unicode.GetByteCount() System.Text.ASCIIEncoding.ASCII.GetByteCount()
I think Unicode would be a better encoding because it will be able to handle most emojis and other characters.
Something is wrong in your spec; 140 bytes will ALWAYS be less than 160 characters (I am not aware of any character that takes less than one byte). Did you mean it the other way around? You will need to come up with some rules; it would be easier if you could get some samples of these messages and decide how you want to split them. If the only rule you have is "don't split inside words", for example, that can be simplified as "split on the nearest space". (This is not the best interpretation, but it's usable.) This would allow you to do something like this: var chunks = GetChunks(textToSplit).ToList(); foreach (var chunk in chunks) Console.WriteLine(chunk); IEnumerable&lt;string&gt; GetChunks(string text) { while (!string.IsNullOrWhiteSpace(text)) { var chunk = text.Substring(0, Math.Min(10, text.Length)); // break at the last space, if any var lastSpace = chunk.LastIndexOf(' '); // if the last space is in the first position, ignore it to avoid an infinite loop if (lastSpace &gt; 0) chunk = chunk.Substring(0, lastSpace); yield return chunk; text = text.Substring(chunk.Length); } } This should, of course, be tested.
You don't get to pick any arbitrary encoding, it's either GSM-7 (which is not the same as ASCII) or UCS-2 (which is not quite the same as UTF-16). 
GSM-7 is a 7 bit encoding, so 140 bytes * 8 bits per byte / 7 bits per character = 160.
&gt;I think what you may be looking for is a sensible design pattern for standard user auth Elaborate pls
I'm reading it on my way to and from work lately. Pretty good read so far. I wish I'd known a lot of what it covers, like MSBuild before I got into ASP.NET Core. It was just published out of MEAP today so good timing.
This is true.
"Split on the nearest space" is only okay if: 1. The message isn't one massive string without spaces (such as a really long URL) in which case I would be forced to break it anyway 2. The byte count is still taken into consideration 3. The split occurs so it takes the nearest space that results in the chunk being under X bytes and/or characters... AKA don't take the nearest space if the "nearest" is in the 161st position
You could use dynamic SQL or config files to define the rules, but that seems complicated. I would just make db tables for the relevant entities, and a rules table that referenced those entities. A rule table entry could look like: CustomerId | SurveyId | AnswerId | Action ----------|--------|--------|------ 1 | 1 | 1 | Next 1 | 1 | 2 | DoSomethingCrazy You could even design in interface to manage that table, so you wouldn't have manage config files.
Are you sure *you* actually need to handle this? Sounds like Twilio does this automatically and messages split into multiple segments are assembled into a single message on the receiving device, so split URLs shouldn't be a problem? In any case, from what I can tell you don't need to worry about bytes. You just need to determine whether all your characters are part of GSM-7 or UCS-2 so you know if you have 160 or 67 characters to work with.
would the action be a SQL script. 
1. Test manager isn't needed for doing unit tests. It's more for a tester to create a test plan and execute manual tests from what I understand. It can be useful for tracking what tests are needed for an application and track when they're automated. 2. Gitflow is a branching model you use with Git. VSTS supports Git, therefore it supports gitflow. There's no difference between using VSTS and something like Github or Bitbucket in terms of your branching strategy. Bonus: The build definitions are where you would run your unit tests in the build process. There's a preset task for running tests, and it'll look for anything in your solution in a unit test project. Has nothing to do with Test Manager. VSTS is nice. It's pretty easy to use, especially with .NET apps. Pretty easy pipeline to configure. There's nothing wrong with using it.
I'm using IHostedService in conjunction with [FluentScheduler](https://github.com/fluentscheduler/FluentScheduler) in a recent project and it's working well. I went with this over Hangfire since it was less setup (no database required.) 
https://www.atlassian.com/git/tutorials/comparing-workflows If it's just you, do whatever workflow you want (i.e. push straight to master willy-nilly). But if anyone else is going to work on the code, I recommend the branching workflow (branch off master, work for a bit, then PR back to master). In organizations, forking workflow is nice because developer feature branches don't clutter up the primary organization's repo. So the primary repo usually only has a master and dev branch (and maybe a release stabilization branch).
I've toyed with writing a hook that would run PowerShell or call an api to open the pull request from master to dev, and possibly give that account permission to bypass policy and automatically complete the request. But that's work and not work I get a lot of opportunity to do with everything else I have to do.
I'm not actually using twilio, they busy had good documentation on it
I use Azure Functions on a schedule to call my API for certain tasks. That way I can avoid having my App Service always be on.
Played with this tonight. It‚Äôs awesome! 
Did you read his code? It does what you want. First it splits the message into chunks 140 characters or smaller, then it finds that last space within the 140 characters.
You haven't provided much detail about what "action" means, but it should tell the UI what to do. If I were designing this, I'd probably send a json payload with either the next question or some other UI directive. 
unless it's in a tight loop, you probably wont notice anyway. its highly unlikely that the extra allocations are going to be your bottleneck. 
would this help to make corert becoming a real thing? i check in on that repo from time to time, and it is very active, but I've heard nothing in terms of a timeline. 
Action is is a custom block of code. apologies, the whole process's purpose is to generate a report. I have a table that has the report build laid out with a layout order. the customized actions based off of the section, question and customer, are rules to process based on an answer to a question. if you for instance, do you have allergies issues? answer=yes, then populate "potential allergy testing candidate." into that section I think the report. i am trying to figure out the best way to launch custom code blocks based on some data point. it doesn't have to go back to the UI. the report is pulled from a report viewer application, and SSRS. 
Follow up to my follow up. Tried on Firefox instead. Same problem. But this is my Linux boot. Windows boot works. I think the site has issues with Linux user agents.
You might consider using a url shortener on long URLs if your SLA allows for it. 
of course it belongs in the controller layer. Of course you shouldn't normally be writing all the logic in the controller action itself... you should hand that off to a service class that does it, which other non-http things can use as well in a non-http fashion
But the verbs have semantic meaning. PUT is different than POST is different than PATCH. They serve different purposes and should be treated differently.
Completely agree. I simply don't think this has anything to do with HTTP which I guess is where we are disconnecting. I would say that the message should arrive via HTTP PATCH but after that you just have two documents (original and changeset) and I think it is perfectly acceptable to pass off dealing with those two documents to another service/layer/universe 
That's quite neat. Will give it a shot myself, not really tech savy into ML yet but I might find good use cases here for my personal projects.
Is this a paid feature?
I'm starting to get ideas now and then about what we could apply binary classification to. It seems like the simplest form of ML in my opinion. In the realm of web dev, I was thinking user metrics like time on page and number of pages viewed in a session could be combined with a small portion of users completing surveys to guess whether the non-survey filling users are having positive or negative experiences. Maybe that could help with offering users browsing docs sites more help etc.
Doesn't sound like it will be.
I've used it quite a few times with my coworkers. Saved me a ton of time. Used to have to do a gotomeeting and give me control of their computer so I could poke around their project to see what was going on. Took forever and was slow and also had to deal with their IDE configs. I love that I can jump right in from my own VS with my own tools and layout and just fix their damn bugs. This is, in my opinon, one of the greatest new features MS could have added.
TypeScript does a pretty decent job at fixing that imo.
Indeed I'd be great for overall traffic analysis for websites. Ad click prediction was also mentioned on the article so there's definitely usage for those. I'd also say this would be great on sites like Github as well on it's issue tracker to predict what are recurring issues reported by users so that maintainers could learn where to improve. Be it better docs on those areas or improving the code parts to avoid misuse of the APIs. I also see quite a big application of ML on computer vision, it'd make discovering objects faster as the model gets more data. I'd bet this is already implemented on the upcoming Hololens V2 as well. Overall extra insights is always appreciated. I'll try out binary classification when I get home, as you say it's the easiest to try out.
This is probably not a good option but you could drop the username and password into the registry then reboot the computer and it will automatically login.
The funny thing is, we've been talking (me and my co-workers) about this kind of feature since Google docs became popular. If they continue to maintain this feature and releases it as a collaborative tool on github....oh man that would be something!
Did you in any case use the Visual Studio UI to configure the `langversion`? If you are not on the latest update of VS2017 (currently 15.7.5), older VS2017 will actually add a `Condition` attribute for `&lt;langversion&gt;latest&lt;/langversion&gt;` inside your csproj file which only enables the latest C# language feature under `Debug`. This causes everything to compile fine locally (because it's using `Debug` configuration), but fails when publishing (usually it's using `Release` configuration)
Check if you set 7.1 for release of as you probably only set it for debug. Right click your project, select properties, go to the build tab, choose release, choose advanced, choose latest minor or specifically 7.1
We will start simple, since there is only one dev on this at the moment (me). Off the top of my head: master branch, feature branches, occassional bug fix branch. I was thinking of moving in the direction of gitflow as things get more complicated. In essence, we would use gitflow more as a template and then see what suits us. Did you use gitflow yourself?
right click on project -&gt; Build -&gt; Advanced (bottom right) -&gt; Language Version 
Thank you so much! Even though I am running VS2017 15.7.5, the conditional was there. Removed it and everything works as expected. 
Comment for your valuable suggestion
rbobby, I have some robots that in order to work they must have their session, in the windows server 2012, active. At the moment I'm working on a platform that controlls all the robots that work on the server. To be able to start the robots i must guarantee that they have their session active before i try to run them. Thats why i have the user and password and i must be able to log them on the server.
I need something more smooth :D
Is there anything in that list, that isn't also true about a java site running on a Tomcat server? And most of it is also true for a PHP site.
According to the FAQ they‚Äôre committed to a substantive free tier for developers to use on an ongoing basis. They‚Äôll be evaluating the introduction of paid tiers with advanced features as they better understand the needs of the community.
Wish my company upgraded us from 2015 to 2017 so we could try this, but my boss for some reason doesn't like 2017 and keeping us on 15.
No, I don't use gitflow, I use what is essentially mainline. Master Branch and feature branches. If we start on a major revision, we create a branch for it that will live until the new version goes into production at which point it gets merged back to master. Sometimes I set up CI so that hotfixes applied to master get auto-merged back to other branches.
Controller, because PATCH is just like GET/POST/PUT. Your controller will make a call to a service layer, return a document, and for a POST you will update that document, from your Mvc Model, to your Service's Model, and it will handle the rest. Same with Get and Put. PATCH is more complicated, but should follow the same principal because it just shifts it from a horizontal object to a vertical object, but is still "Hey, I have this whole batch of changes, update stuff for me". If you pass the PATCH object to your service layer, you will either 1, expose your service layer's model to the end user's or 2, expose your MVC model inside your service layer. If you leave the PATCH document in your controller, your controller is solely responsible for translating an HTTP document to something your service layer can understand. 
In my experience ( just ended a job hunt in NYC), your CTCI/Leetcode will get you in the door, but you'll also want to have some C#/.NET knowledge prepared if possible. For me, I failed hard with a lot of CTCI interviews. That said, there were still a couple of companies that just did .NET interviews. I'm working at one now.
You'll have to roll your own accountcontroller to login, register etc MS really screwed up authentication shoving razor pages down our throats.
How would you compare Rider to the full features VS on Windows?
Has he ever given a reason for the dislike of vs2017? I can‚Äôt even think of a single reason to prefer 2015 at this point. The only time I remember keeping an old version installed is like when 2008 was the last to support the mobile windows stuff.
Are there reviewers out there that reviews these kinds of books?
Do you use EF Core?
I have a project that uses EF with SQL Server. We're switching to Postgresql and others have floated the idea of using Marten, but I believe we're keep the relational table structure.. so to me, Marten makes little sense unless I'm missing something. I'm going to take a look at Tortuga. Thanks for sharing. I've used Dapper in the past and liked the lightweight feel. 
Do you use an ORM? If so, what is your go-to for relational databases. I've looked at Marten, but it doesn't seem to help much with a relational model. 
Thanks a lot for your follow ups.
Besides what others have said, I'd like to add that building and running unit tests are free but limited to 240 minutes per month IIRC, if you need more builds you'll need to pay.
It largely has the things I care about. It's not as full featured as Visual Studio, but most all of the missing features are things that go unnoticed by me.
GoThinkster have been curating a set of front- and back-end applications that implement the exact same app using different technologies. See [Real World](https://github.com/gothinkster/realworld) for full details. The [AspNet Core back-end](https://github.com/gothinkster/aspnetcore-realworld-example-app) features JWT authentication. 
Tight loops would use the upcoming async version of IEnumerable where you don't need to deal with a task object unless you actually need to wait.
Tie it into TopShelf and make it a service. I love Quartz.
Sorry, you can't do it. While "drag and drop" works between browser windows (at least the last time I checked), you can't drag an OWA attachment because it doesn't exist on the client side until the user saves it to the local filesystem, and there is no automated access to the local fileystem available from scripting, for security reasons. 
I dont need local file system access I dont think. My thought was at the least the attachment details might have the location of the attachment on the exchange server. The web app could grab the file from there and load it to the db no?
Maybe if you're submitting data?
This sums up the problem I'm running into. I want 1 model that has every field and then I want to be able to hide fields that the client should not see but I don't want to take them out of the model. I'm generation this code so I don't know what fields would be hidden but I need to accommodate anything. To do that I need to patch the request and it just smells putting it in the service layer.
The web page should have some sort of resource ID for the file. If you can inject javascript into the OWA page, you should be able to download it and then upload it; however I don't think you can actually inject javascript into an https page. Maybe with a plugin like Scriptmonkey. Not sure. 
So typically the end user is on the phone with the customer when the email comes across. Right now, as soon as they get the email they forward it to another mailbox where our system grabs the attachment and uploads it. But right now that system will crash if they have any images in their signature or anything. My idea was that I might be able to save the end user a few steps if they could just drag and drop that attachment from the browser window they have open where they are signed into OWA into the browser window they have open with our web form app. I haven't looked at any code or exchange API's yet I just wanted to reach out and see if anyone else has done something similar. 
RFC 7231 6.5.7 &gt;6.5.7. 408 Request Timeout &gt; The 408 (Request Timeout) status code indicates that the server did &gt; not receive a complete request message within the time that it was &gt; prepared to wait. A server SHOULD send the "close" connection option &gt; (Section 6.1 of [RFC7230]) in the response, since 408 implies that &gt; the server has decided to close the connection rather than continue &gt; waiting. If the client has an outstanding request in transit, the &gt; client MAY repeat that request on a new connection. Basically, the client is allowed to keep polling on a new connection when the previous one timed out, so the client needs to keep track of how many times it has timed out previously to prevent an unescaped loop condition.
If you're on the same domain, Exchange has a whole API for this stuff. It's going to be a lot cleaner just to do it on the server. Also, if this is a decent size business with some cash, there are already a bunch of apps that will strip attachments for you.
I'm pretty sure it's all on the same domain, I'll check with our System Admin. There has to be some interaction because we have to tie the attachment to specific customer(s). So that's why I was thinking it would be so much quicker if they could just grab the attachment and drop it on the screen that has the customer information I need to associate it with. Thanks for your help.
Also, we tend to build everything we need in house unless it's something really complex like our phone system. 
Use the API It's a lot cleaner. You can have it trigger automatically and do it's thing before the recipient even knows the mail came in. 
I've had this training and might be able to help, but as with all things Udi, it depends :) Do you have a few sentences in the Systems Theory video where he's describing the timeout exception? My guess is it revolves around the concept of idempotence, and being able to submit the same data/request multiple times without creating duplicates (unless this is a requirement by design).
I think it is bad idea. Because users of IDictionary do not expect call to it to block or fails due to network reason. If there is network calls you should have Async methods
With a timeout you can‚Äôt really be sure what the other system has done. It may not have received your request, or it could have received and processed it but some network glitch means you never received a response. Depending on what you were doing, you may or may not need to ensure that if the request was processed that it doesn‚Äôt get processed again when you retry. For example, you could safely retry sending your GPS location, but you would want to ensure that a request that makes a payment is idempotent.
I'd prefer something like OData DeltaFeed over JSON Patch.
Have you taken a look at ChartJS? http://www.chartjs.org/samples/latest/
Bootstrap?
[amcharts is easy](https://www.amcharts.com/)
Our company uses Kendo UI charts.
You are right, and I agree. However, I focused on making simpler to use data store when I am developing small tools in the cloud environments. and I expect low-latency and low-load in use-cases.
Very helpful. That seems to be my experience thus far, but it's only been a couple of weeks. It seems to be feasible for me to use a mac for net core development now! What a day to live!
From Telerik?
Yes
I have been using morris(http://morrisjs.github.io/morris.js/) and flotcharts(http://www.flotcharts.org/flot/examples/) 
[https://d3js.org/](https://d3js.org/) can probably meet the needs but I'm no expert.
I think I‚Äôd look at EWS specifically.
Yep
Code first approach? Sorry, I'm new to EF. My old lead always liked having complete control over the database.. meaning we write the SQL and a lightweight ORM to map everything. Do you have to declare all of your foreign keys in code when you use EF core?
I had not considered the possibility that the receiver may have succeeded but the request timed out before response was sent. That's good to know. It makes sense and, at times, I get a little too literal with technology. So the thing that timed out may not be the processing but the response never occurred to me. Thank you.
Yeah, it depends.....X if this happens and Y if this happens. :) Udi is fast becoming my favorite tech talk person. His insights into the why of things are excellent and fit my style of learning. You always get a history lesson which I find to be awesome.Scott Allen is still up there. That accent is like no other! To your point, I do think he mentioned something about the processing completing and he definitely discussed the retry scenario. It just wasn't sinking in. Thanks.
That makes sense. Thanks for the info!
I don't see any mention of linux support though? How is no linux or OSX support being cross platform?
The applications themselves are cross platform however being that shared projects are much easier to work with when using SLN files you‚Äôd definitely need to use an IDE to make full use of Uno. The edit and continue functionality I spoke about is only possible because of the first class support for UWP too so even using Visual Studio for Mac would be a good experience but not as good as Visual Studio for windows. That being said this is still cross platform as the outputted apps are cross platform
Do you know if you can develop iOS apps without A Mac? I've not used Xamarin Live, but having the option seems nice.
Because 'cross platform' isn't a very helpful term. It just means that it supports more than one platform, and Uno seems to support 4. Their website describes it as a UWP bridge to web and mobile, so asking about desktop support might leave you disappointed.
No you need a Mac unfortunately. The most elegant solution MS have come up with is for Enterprise VS editions you can basically have an emulator window (kinda like Parallels) which shows the Simulator window from a remote Mac but besides that there‚Äôs no way on Windows to run Simulator natively. 
With Xamarin Live, you supposedly don't need a Mac while using Xamarin. That's why I was curious.
You still need an iOS Device so if you have an iPhone you‚Äôre good 
Yeah. It's an iOS app.
Amazing can't wait to try it. 
Isn't the language from FuseTools also called Uno? It's a cross platform language and compiler with a C# and Xaml style. 
This is what I'm using too and it's also used by Fortune companies so it's a safe bet. Reports from other Devs have cases where they run million jobs for banks over a time period with little to no issues. Setup is easy and with Net Core it works seamlessly so far on my project.
Yeah... this is how I mostly do things: https://github.com/0xRumple/dorm-portal?files=1 EFcore allows you to do modeling without touching your db, you just write normal classes (POCO), and it will do the magic for you See the early commits, I start with in-memory db... to make things just easy to start with
Is there any VB.NET support for this?
It also works like that for VS Professional (which is cheaper than Entreprise). I prefer to build the iOS app using VS for Mac.
Razor.Engine
Hello, have you already passed this exam? Was it about .NET Core or MVC 4? 
My coworker created the [same thing](https://docs.microsoft.com/sandbox/demos/nothotdog?WT.mc_id=none-reddit-marouill) with Azure a few months ago! We also have a Bacon not Bacon. üòÇ Those projects are always a blast!
You put "hotdog", did you mean open-faced sausage sandwich? üå≠ *** ^I'm ^a ^bot *^bleep, ^bloop*
I think this ‚ÄúNotHotdog‚Äù problem became the de-facto standard in computer vision :)
You put "hotdog", did you mean open-faced sausage sandwich? üå≠ *** ^I'm ^a ^bot *^bleep, ^bloop*
Most definitely! Our Bacon not Bacon was also done using the same tech. Relatively easy and awesome for demos. I even did a version with Azure Functions so as to avoid paying too much.
From what I saw, it was able to run [ASP.NET](https://ASP.NET) Core without much of an issue. I didn't try running it for days/weeks though. From personal experience I would guess you will need to optimize for RAM usage before worrying about CPU, depending on what kind of service / process you want to run. Which model of pi are you targeting? How much RAM does it have?
I have 2 raspberry pi's at disposal 3B &amp; 3B+, both have 1GB ram and quad core cpus (1,2ghz &amp; 1,4ghz)
I need one thread running http listener and invoking events and a few other threads sending get requests to some apis
&gt; It‚Äôs not going to be a singleton, but it‚Äôs not going to be a per request type thing either. .NET Core has magic sauce under the hood that means it will at times recycle the underlying connections when it thinks it should. iirc the HttpClient is transient but the connection handler is recycled on a time based period. So you always get a fresh client, but the underlying guts - dns cache, open sockets, etc - are reused. 
Yes I've passed the exam now. I'd say about half of the day exam was specific to Core and the rest was standard MVC
bad bot
Thank you, DC2SEA, for voting on hotdog\_bot. This bot wants to find the best and worst bots on Reddit. [You can view results here](https://goodbot-badbot.herokuapp.com/). *** ^^Even ^^if ^^I ^^don't ^^reply ^^to ^^your ^^comment, ^^I'm ^^still ^^listening ^^for ^^votes. ^^Check ^^the ^^webpage ^^to ^^see ^^if ^^your ^^vote ^^registered!
bad bot
You probably just want a JS chart library. Here's a table of features that might help you choose what you need. [https://cdn-images-1.medium.com/max/2000/1\*g9yUCa-NqAjwvuJUIzwV5Q.png](https://cdn-images-1.medium.com/max/2000/1*g9yUCa-NqAjwvuJUIzwV5Q.png)
I think if you need that server side logic you could use it. ASP.NET core It isn't bloated, so it can, in your case, do as little as possible and still have a scalable and sane way of adding complexity when needed.
Thanks, I'm very happy to hear it :)
Hi. I just moved my blog this weekend to Jekyll. Write posts in markdown and build static site. I made a small post about my move. http://topswagcode.com/2018/07/13/Moved-to-jekyll/ You can still make a dotnet core webapi and make webapp that uses your api. I have even shared all the code for the blog online, so you can see how the structure is. https://github.com/kiksen1987/blog And best of all there is free hosting at https://pages.github.com Just write me a message if you have any questions.
I created a my startup using .Net Core about a year ago and it's a fairly large web app. I couldn't be happier with that decision. Also, it's unbelievably fast. 
Raspberry pi is like a supercomputer of the 90s, thus Yes you can have many threads. And Yes, you have to take care about memory management and code it correctly. I have had some armv5 iot devices running 50 threads for years with no crashes. 
What is Qt qml???? And why does it need support for dotnet?
Qml is a widely used cross platform UI framework. There isn't a \*good\* cross-plat UI library for .NET, so I created a bridge to Qml, which has been battle tested for years in many industries.
Try out Wordpress with .NET core https://www.hanselman.com/blog/TheWholeOfWordPressCompiledToNETCoreAndANuGetPackageWithPeachPie.aspx
I think many people don't need dynamic features in their blog, that they just need it to display text. Commenting systems are usually not dynamic (on your server at least). So lots of people use a static site generator like Jekyll for their blogs. That said, as soon as you need dynamic web app functionality, I think there's no harm in choosing any framework. It depends on your taste. If you're the type who prefers doing everything yourself and having an explicit view of all the code, code something from the ground up with Express on Node. If you're the type who likes batteries included, and is already familiar with ASP.NET Core, use it. I'd choose ASP.NET Core for any dynamic web app I was doing, whether it was a blog or huge enterprise web app. I have a personal SaaS side project and though I'm familiar with multiple platforms including Express/Node, I chose ASP.NET Core for all the stuff it gives you out of the box. Pretty happy so far. The exception to this is that I prefer using preexisting solutions if I can. For blogs, there's stuff like Ghost and they have a Docker image, so I thought it would be simpler to just use that with my own custom theme for my blog.
Avalon does look promising though.
Thank goodness. Before it was almost like an "industry secret" that you had to learn on your own about the lifetime of HttpClients and had unfortunate gotchas depending on what you did. Glad to see none of this is a concern now.
Agreed with using github for hosting. Combined with a $10/year domain, it's the ideal and fastest way to get setup.
I also have a project I'm starting with ASP.NET Core. I was wondering, what database are you pairing it with?
I‚Äôm using PostgreSQL with the Marten ORM. It‚Äôs worked out well. 
Ah cool. I saw Marten posted a while ago and I thought it looked interesting. So far we're just using Npgsql and not treating our data as documents. The data in the app is relational by nature so we've found it useful. I figured if some of our data was unstructured we would store it as serialized strings or look into doing lower level JSON column operations, or even storing unstructured stuff separately in MongoDB.
It looks like the 90s.
It looks like the 90s.
I meant Avalonia. It's modern. http://avaloniaui.net/
Yep, and it looks (renders) like the 90s. It is good though. 
I have no idea what you mean. It looks just like modem UI controls look.
Guid? 
Almost all my sites are small sites. aspnetcore has been fun for me to use. I'm converting all my old webforms, mvc ([asp.net](https://asp.net) and RoR) and web pages sites into aspnetcore razor page sites and it's been going well. Razor pages are almost perfectly suited for this kind of stuff in my opinion. Just a few pages can get something workable and from there you can extend as needed. You can also do it using MVC or as a SPA using WebAPI + your favorite JavaScript framework. You can go as simple or as complicated as you want/need.
No, it created short, human readable IDs.
I feel that your question is a little too general and vague and people find it hard to provide valuable advice. Stating using something lead to a crappy other thing can happens regardless of using GraphQL or not. Can you provide a more specific example, a snippet of code, current folder structure?
You could load in a list of words from a dictionary, heaps on the net. 
The question actually does make sense to me. The solution we chose was to have two interfaces you can implement to hook into the root query/mutation: public interface IQueryProvider { void AddQueries(ObjectGraphType query); } and public interface IMutationProvider { void AddMutations(ObjectGraphType mutation); } Then in the constructor of the root query/mutation, we ask DI for instances of these: public Query(IEnumerable&lt;IQueryProvider&gt; queryProviders) { foreach (var queryProvider in queryProviders) { queryProvider.AddQueries(this); } } This of course requires that you use a proper DI framework behind the scenes, that support this sort of thing. Hope it helps
Esp. in the context of Qt (1995).
Last time I tried Qt even their own demos were buggy as hell so beta-release Avaloni may already be more stable. 
I would personally never recommend this for a bigger project, but in your scenario using [Razor Pages](https://docs.microsoft.com/en-us/aspnet/core/razor-pages/?view=aspnetcore-2.1&amp;tabs=visual-studio) would make sense. Doing a full-blown MVC app for your case would definitely be a overkill, so just having Razor Pages with a code-behind would be a lot easier and more productive.
I moved my blog to Jekyll too. Great little framework!
1+ for Marten, it's terrific!
I know this is a .net reddit but imo there are better alternatives than asp.net core for simple blog sites. You're generally better using a static site generator if you can. There is a good project in the .net space - wyam.io - but they don't support .net core atm (I've been tracking it for a while, it's on the cards but it's hard to do because the core framework was so interwoven with Windows APIs unfortunately).
[removed]
I like Jekyll but you're limited to blogging if you don't have a text editor or git installed. I'm currently moving from Jekyll to cloudscribe because of this reason. 
This worked great! Thanks :)
What's this supposed to mean?
&gt;this &gt;[th is] &gt;1. &gt;*(used to indicate a person, thing, idea, state, event, time, remark, etc., as present, near, just mentioned or pointed out, supposed to be understood, or by way of emphasis):* e.g **This is my coat.**
&gt;this &gt;[th is] &gt;1. &gt;*(used to indicate a person, thing, idea, state, event, time, remark, etc., as present, near, just mentioned or pointed out, supposed to be understood, or by way of emphasis):* e.g **This is my coat.**
As other have pointed out. For small, blog-like and relatively static sites, a static site generator like Jekyll is great. I use it for my site as well. That said, .net core is perfectly fine for small apps as well. It's modular design makes it not bloated in production. It depends on the features, not some much on the scale or size of the app/site. You can do big static sites with Jekyll, no problem. Or small, feature rich dynamic web app's with .net core. If you are building a blog, then I would just go static. Much fewer headaches in terms of deployment, hosting, security, etc. Protip: Jekyll + Netlify is a GREAT combo! 
Didn‚Äôt remember this one. Looks very promising. I‚Äôm a huge WPF fan and I‚Äôm not very impressed with UWP (mostly due to deployment limitations), so it‚Äôs good to see other projects following the WPF lead.
7 years later, finally https://www.elpauer.org/2011/04/a-wish-a-day-qml-wpf/ 
Maybe the right answer is don't use Graphql because it's not a proven technology yet? It seams to me it switches a little complexity on the client for a ton of complexity on the server. 
I‚Äôd recommend looking at Orchardcore cms. It‚Äôs an open source cms built with aspnetcore. It‚Äôs pretty easy to get running and offers multiple database options. Otherwise, there‚Äôs nothing that makes aspnetcore inherently bad for any site. 
Qt is as stable as they come. It is used on embedded devices, cars, medical equipment, etc.
Found it: https://www.reddit.com/r/dotnet/comments/8zbd5x/hashids_generate_short_unique_ids_from_integers/
Found it: https://www.reddit.com/r/dotnet/comments/8zbd5x/hashids_generate_short_unique_ids_from_integers/
NuGet package: https://www.nuget.org/packages/Hashids.net/
&gt;I ended up spinning a Ubuntu VM which wasn't convenient for everytime I wanted to blog. Docker can make this much easier. Once it's set up it's just one command to compile the website, which you can then define as your build action (in VS Code or whatever).
1. I don't have that option. 2. We're using it with success.
This is great, thank you! Can you share a few more details? How are you doing your DI? Are you using an IoC container, or home grown? Your queries are what are implementing the interface, correct? And then your DI framework injects those into Query(...) //root query? and Mutation(...) //root mutation? Is that correct? 
 Anonymous Authentication is enabled. still having the issue.
Looks cool, although anything you can decode is by definition not a hash... (Unless you're saving the original value in a hashmap?)
 What does this have to do with dotnet.
We use autofac. As you can see from the interfaces above, the query/mutation providers are given access to the root query/mutation. The only idea here is to split up what would otherwise all be in one huge file. Each provider specify fields on the root query/mutation, and the type of those fields are in turn also resolved through DI, when the field is used in a query. There's an example somewhere on the graphqldotnet repo on how you can plugin your DI/IoC container. It's pretty straight forward.
Just try to implement the RESTful/HATEOAS specs, not RPC. Think of your endpoints in terms of resources, make use of proper HTTP methods, status codes and headers, and use some well-known authentication &amp; authorization mechanism/protocols e.g. JWT (or OAuth if you need something more sophisticated). Speaking of GraphQL - I would use it only for a subset of my API and see whether it makes sense or not. Keep in mind that it's a rather fresh tech + difficult to implement from backend dev perspective.
You lost me at timeout. If the first point you raise is that your worried about a process getting shut down in 5 - 10 minutes you have bigger issues than trying to add a custom dictionary word.
Sorry to hear that. That was just a poor attempt to a little humor. The content becomes much more valuable after that. Would really love to hear your feedback on the rest of the article if you are willing to share it. Thanks for the feedback!
"difficult to implement from the backend"... Are you saying its more difficult to consume someone's GraphQL from non-frontend apps or Is it more difficult to implement a good backend -for- a GraphQL API
2 - you have to take care of e.g. proper query translation on your own if you're going to hit a database (unless you use cache, then it's a no-brainer). 
They are also the default project template now. Unless my install is different the default.
While I'm inclined to agree, I'm not going to argue with something that so cleanly solved my problem.
Thanks, previously I searched an ASP CMS but i found only old projects. I'll check it out.
We rely heavily on 3rd party APIs (REST, SOAP, etc.) for our business and I was impressed with one of our provider's Swagger documentation. Definitely recommend using Swagger if you're providing a REST API.
Razor pages a whole new concept for me but it seems interesting. :)
Netlify seems interesting, I'll experiment with all alternatives ([ASP.NET](https://ASP.NET) site built from scratch, static site generator, Orchardcore...) and if I decide to go with Jekyll or Wyan I'll transfer my domain from my current hosting company to Netlify.
I know Jakyll by name but I never tried a static website generator, in the next months i plan to experiment with all alternatives in order to decide what system is the best fit for me.
If I need classic .net just to compile an updated version of my blog is not a problem: If I write a website or a web app I want it to run on .net Core because Linux hosting often is cheaper than Windows hosting but if i have to launch Wyam just to update my website when I write a post I can do it from my Windows workstation so I'll give it a try. How I've already written I never used a static website generator so I'll give myself some months to experiment.
In all fairness they never actually claim it hashes it, the name is just a bit misleading. All it does in simplified terms is convert to hex values with a salt. It does a bit extra, but that's the basis of what it does. I guess it would be useful to avoid exposing id's in url's etc, rather than being used for actual security. 
Your post has been removed. Self promotion posts are not allowed.
I've been following your progress from the start. Just dropping a comment to say great work :)
I really like working with Odata endpoints 
Great article! One question though, why the async when capturing the message body?
I used this years ago for a short linking service. It was a good little library to bring in.
Any main reasons why? They all work the same...(low learning curver) Self documenting? Or ability to define results on the client side(rich query interface)? Or a mix?
Do you build your own clients or would you still use sdks if provided by the API author? Looking at other 'at scale' API service, I personally tend to use their clients/sdks if available (nuget, npm, etc...) Espically with players like Google, aws, azure
The answer to your question - "What type of API's do we focus on" - is "Any and/or all of them". I think, however, you are asking the wrong question. A better question to ask, or at least a preliminary question, is "How do we encapsulate our business logic such that exposing it via an arbitrary service architecture becomes a trivial exercise?". I've been working on that question for a couple of years. I wrote pattern and a small library called [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient) that may be of interest to you. I also wrote a [Demo application](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon). The demo shows a Web and desktop app calling services via an in-process client (MSSQL and MySQL) and also via WebAPI. AdaptiveClient can be used with any service architecture - REST, GraphQL, WCF, service bus, you name it. 
That's what I'm using it for. Yes, there will be permission checks too, but I don't want people thinking they can just id-walk my site.
Avalonia is bloated as fuck Nobody can beat the simplicity of WinForm
Versioning. Know that at some point, your initial logic will be deprecated and you need to be able to point the same restful calls to new logic without redeploying every moving part in your environment. I try to limit the complexity of the DTOs, usually I have two or three gets for the same object, one is just the PK and display, one is what I perceive to be the most important properties, and the last is the kitchen sink. This is where GraphQL is nice because it will limit the return subset according to your request, that said, it takes some more finesse to limit your select queries, so it will still be retrieved but not sent over HTTP which is where the serialization is more costly. If things don't change often, increase your cache duration server side to limit your database usage. If there is a huge subset of data, and you know the parameters you are after, a stored procedure is the way to go. Create some sort of analytics, know what APIs are being hit hard and which ones are not used often so you can know where to spend your optimization time when the server starts getting beaten up. 
Interesting, can ddd be related with this?
I suppose you are talking about CQRS? If so than I don't know if will work or not. I never really studied that architecture. Hopefully someone who is familiar with that approach and also n-tier can give us an answer. 
As a consumer I like being able to get back exactly what I need by using select, expand, filter, etc. As a builder of api's I like empowering consumers to be able to get the data however they need it. 
I wrote a technical article on my blog and shared it with the community. Why is that considered as self promotion?
This may not be too well received as lots of programmers don't like being asked to work for free unless someone's going to learn something from it, and with so little information this comes across as asking us to just write it for you. Can you give us some information on your background/ability, what technology you're using, what you've done so far...?
TypeScript addresses many (if not all) of the *syntactual* issues with JavaScript, in essence making it more OOP-like. But ECMAScript is becoming more and more OOP-like all the time. For example, we now not only have classes in ECMAScript, but properties as well. But neither TS nor ECMAScript ("modern JavaScript") address the clusterfuck of frameworks, packaging, dependency resolution and the like that make up modern JavaScript development. It's a pain in the fucking ass, and it has taken frontend development, a traditionally easy (compared to backend) area of web development, and turned it into the complicated bit that makes backend look like a dream. ECMAScript syntax is kind of neat, with things like the spread operator and a disregard for passing in too many arguments to a function (and other OOP restrictions) meaning I can focus less on writing correct code, and focus more on cranking out code quickly (albeit at the expense of readability/type safety/etc.). There isn't really a "right" answer, but in my opinion, if you're going to have the frontend be responsible for rendering the view and really taking a load off of the backend, well you shouldn't totally abandon all that an OOP backend language gives you- things like type safety. In a nutshell, we're transitioning from dealing with compile-time errors, to dealing with runtime errors, but I think most programmers would agree bad mojo should be caught at compile-time, before the app is running. Anyway, the cool thing about WebAssembly/Blazor is that we get the big pros of modern JavaScript development (things like reducing server resource usage by really only transferring data back and forth via JSON/SPAs), without the cons of JavaScript as a language and its frameworks and associated overhead/learning curve.
Agreed, I never much cared for mixing .NET and a JavaScript client-side framework. Looking at the code, it is often just like a controller with one action, serving as handoff to the JavaScript client-side part of the app. Same with the views, they just have say the React bootstrapper, but no real substance. It was like why use .NET at all? And in fact I don't, I tend to subscribe to the "use .NET for the Web API" philosophy, where the actual web app is pure JS with a Node base rather than a .NET base.
I've been doing a lot of research into this for some side projects I would like to pursue, and while my employer will likely be using React for years to come, I'm seeing WebAssembly as "JavaScript vNext". It seems like developers are getting tired of trying to keep up with JavaScript (ECMAScript), and the "honeymoon phase" of being free from OOP restrictions (like type safety) may be starting to wear off. I love Razor; it was so revolutionary when it first came out and seemed like the logical next step from WebForms, but web apps these days are so reliant on AJAX, especially if they want to be SPAs. Not really something you can do with Razor ("uhm, Akchewally"s aside). But it sounds like Blazor will solve this problem, and then some with the added efficiency/performance of WebAssembly.
Which supports the idea that NVidia is focusing less on consumer gaming hardware and more on the moneymaker (VR/Machine Learning). Or it could just be that a couple of key devs from the GeForce driver department retired.
In the banking industry, it hasn't.
This conversation I believe has been had many a time over the last few decades, with different languages. Things start off not explicitly aiming to replace other things, but they end up replacing them anyway.
Consider that JavaScript is expensive for businesses. They have to pay man-hours (expensive, dev man-hours!) to rewrite their apps in {latest JS framework} every couple of years. I can't begin to fathom how many hours were spent by devs at my current job moving from Knockout to React. We are still transitioning after over 2 years, and now I'm seeing signs Vue is the "next big thing" in JavaScript land.
Do we argue about C# frameworks?
Just to be sure, is the sticking point that you want to use MVVM with NoSQL and you aren't sure how to go about the NoSQL part, or MVVM in general is the issue and wondering if NoSQL will make a difference? 
You are lucky there there are a ton of libraries that you can use to do this, some already in .NET if you want to do some heavy lifting. Do some searching on PDF or Image Generation with .Net and you should find a ton of examples. I think doing some of this discovery yourself is beneficial, and if you get stuck on some of the nuance we would be happy to help!
I think this question has to be dealt based on business complexity. If there are lot of business rules than your entire focus should be on Domain. Investing in DDD approaches like Domain Event, Aggregates etc will help in clean architecture. You may or may not like DDD but some of the concepts are worth it . Build a solid domain layer and writes tests that makes sense. Testing is tricky investment area, there are so many tests one can write : Functional,Integration,Unit and System. Team needs to spend lot of time thinking what tests are making sense. To summarize focus on domain and testing. If you get that that right API and other cross cutting concerns will not be that hard. Lastly refer to .net architecture guide for your decision making: [https://github.com/dotnet-architecture/eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers)
It's mostly between the VM and the model. I haven't done a MVVM project before. Most of the examples seem to use a relational database with a ADO.NET provider. In my case I'm thinking of something like Couchdb lite or other embedded document store.
The interactions with the relational datastore should be a good placeholder for interactions with your document datastore. Not trying to downplay your question, but this seems straightforward from what you have stated. Your VM should really not know or care whatever type of DB you are using, ideally. 
When you're using a ADO.NET provider with the model, don't changes automatically get persisted back to the database if you have the bindings set up right? Without that, it seems like you would have to manually setup the event handling between the VM and the model in order to manually read/write the datastore. Or maybe I missed something in the relational database examples I was looking at.
Hmm, depends on what you mean by automatically, but I am getting a better idea of where your sticking point is. Can you link me to an example project you are working from, I can give thoughts on how I would approach it, and maybe that would help you get some ideas? 
Thanks, but I haven't set that part up yet. I'm still working with binding between the view and VM. Also, I have done an example with using the CouchDb Lite API to access data. It seems odd, but I searched for a while and couldn't find an example of not using a ADO.NET provider for data access and using a simple API (like CouchDb lite or even a REST API) for data persistence.
\&gt; Thanks to Commons Host, I was able to setup a manifest file that tells the server to pre-emptively push the dependencies of the index.html file on the web browser. You can read more on how to do it from [Commons Host Gitlab Page](https://gitlab.com/commonshost/server). ... do you mean like if you just bundled those dependencies with WebPack? or like if you just included them as `&lt;style src="" /&gt;` in your HTML?
 Bundling is an optimization technique used for reducing http request count that became obsolete upon the introduction of HTTP2 (anti pattern actually). I would say that its more like the approach of having multiple style links but instead of having the http request generated, the browser gets the dependencies on the browser cache. A http2 manifest file is not a bundle, it is a file used for defining the dependency graph of a web site so that the server can push the dependencies pre-emptively.
"magic button" sounds interesting, how will this button function? You should go learn the basics of development before you attempt to tackle something like this. If don't know how to do this part "button to gather information from text boxes" then "generate an image or a pdf file" is far outside your skillset. 
Awesome work. This looks promising!
&gt; Bundling is an optimization technique used for reducing http request count that became obsolete upon the introduction of HTTP2 (anti pattern actually Bundling or http2 manifest have exactly the same purpose. It is just a different way of achieving same result. I am wondering why not using bundling instead of http2. The idea is the same: Define the dependencies ahead and ship them together.
If something small the bundle changes (one js module) it causes the entire bundle to be flushed. If you have http 2 and files are served independently, only the changing parts of the website gets retrieved from the server
Using Docker to solve this did cross my mind and might still be an option. A few questions: 1. Does your build server shutdown the container service once it's done its stuff? (for me this is just jekyll build, uploading files to aws) 2. How much are you paying to spin up a docker container? I'm using Azure as my cloud platform of choice. I think given it'll be spun up once in a blue moon it should cost very little.
1. Yes. Once everything is set up the only real noticeable differences are using something along the lines of 'docker run jekyll build' instead of 'jekyll build' and that the command takes maybe a second longer to execute. 2. Nothing. I use Docker for Windows on my local machine, which makes use of Hyper-V internally. That said I also use Docker to manage all the services running on my webserver (Nginx, Mail-Server, Gitea...). I currently don't have my personal Dockerfile and custom build command at hand, but the [official image](https://hub.docker.com/r/jekyll/jekyll/) should work just as well and is well documented.
Good read! Simon Robinson also has a C# course on equality and comparisons on Pluralsight, it's quite lengthy too. Surprising how much boilerplate needs to be repeated to get this right.
Thanks for you answer, like you said yourself this may not be well received indeed. Wasn't asking anyone to write this for me all i was asking is giving me some hints. Actually i'am familiar with .net, i've worked on several projects. Like i said its a desktop app so i went for WPF. 
Look into [RenderTargetBitmap](https://msdn.microsoft.com/en-us/library/system.windows.media.imaging.rendertargetbitmap\(v=vs.110\).aspx)
"data source=(LocalDB)\\MSSQLLocalDB;attachdbfilename=|DataDirectory|\\DatabasePMS.mdf;integrated security=True;MultipleActiveResultSets=True;App=EntityFramework"
Oh I can see what I said was ambiguous, what I meant was is there a project using ADO that you are working from that you can use as an example of what you wish would be able to utilize nosql. I can give some thoughts on how I would move it to couchdb. Also, just to throw it out there, check out litedb if you want something scaled down and free from couchdb. I use it a lot when I don't need enterprise-y features. 
Thanks, I didn't know about the course! Link: [https://app.pluralsight.com/library/courses/csharp-equality-comparisons/table-of-contents](https://app.pluralsight.com/library/courses/csharp-equality-comparisons/table-of-contents)
I assume that is done because the data is chunked so you want to asynchronously collect into something that you can deserialize. If you had a stream-supporting deserializer (does Newtonsoft.Json have that?) you could, theoretically, pass the streaming body to it and it would handle that for you.
Not yet
If you have 0 experience with webdev overall, I'd say start with razor mvc. If you know how to use client frameworks (angluar, react...) then go with api instead. SignalIR is useful but certainly not something most projects need. My suggestion is make a crud todo list and then go from there. If you have your own idea or something else in mind, switch to that. If not, improve and perfect todo.
Is jumping straight to Razor pages as a web beginner cheating myself out of a lot of knowledge I would gain from going the MVC route, or is the MVC route the convoluted road to the same result?
An alternative implementation of `GetHashCode()` (or equals) that would be fairly quick would be to use reflection one time to determine the field layout and if applicable, determine a `ValueTuple&lt;T,...&gt;` that would have the same layout (and store this in a lookup table somewhere). Then coerce the type to this valuetuple and call `GetHashCode()` there. For example public struct Case2 { public byte Y { get; } public int X { get; } } GetHashCode could be implemented to determine `ValueTuple&lt;byte, int&gt;` is a structurally equivalent type and then JIT an implementation: Unsafe.As&lt;Case2, ValueTuple&lt;byte, int&gt;&gt;(ref this).GetHashCode(); 
I'd say learn MVC, it is a widely used pattern in the industry and it'll help you understand layered architectures better
That's all I need to know, just needed confirmation MVC is not a waste of time. I take it Razor pages will allow me to later apply the MVC knowledge is simpler ways and take certain shortcuts, where it will be useful to recognize exactly what is being skipped?
Well, most of the time you'll get your data from a model and you'll then use razor code to display it however you need. MVC is just the pattern, you can use razor in your Views, so learning razor can be a part of learning MVC. So basically with MVC you keep your code(c#) away from your presentation(html/css/js), and the data is most likely obtained from a source(t-sql)