The master page among other things has webform controls such as the login control, menu, breadcrumbs etc. And of course a bunch of other logic. I am sure I could find a third party menu solution that will utilize the built in sitemap and roles provider, I could do the same with the other controls etc. This will take time, and sadly time we do not have. The point of switching to MVC is to gain experience, keep developers engaged etc. IMHO using MVC with aspx view engine is better than nothing.
Ahh, well that makes things easier. They mention 3.5 client profile in a few spots but I was never able to get a definate answer. This would be why I guess. Thanks
Have an understanding of basic computer science / development principles. There are few expectations of a junior, outside of an ability to pick things up quickly, to be able to learn quickly the difference between inane / obvious questions to peers, and questions that *ought* to be asked. At some point, you realize that you'll be better off using Google to answer some questions, and you'll know what question types are best for asking the developer you're working with. Know what generics are. Know what SaaS is. Know the principles of some design patterns (MVC, Singleton, Factory), but you don't need to know all of them. Know the basics of linq - you don't need to be a guru, nor do you really need to know a ton about using lambda statements. A lot of that stuff will be learned on the job. Expect them to test your knowledge of the core OO / pattern / simple architectural principles. You should use the expression, "I don't know, but I'll read about it later today" when you are confronted with something you're unfamiliar with. It's a junior position, which generally means you'll be given small things and tiny projects to get you introduced, and responsibilities will be expanded over time. There isn't a high expectation for knowledge, experience, or leadership at that level... just diligence and a good attitude.
You also have to be in the right market. Some cities just don't have as much of a demand for juniors. Another good way to find junior positions is to go visit a college job fair.
All pages share the same master page.
But if you create a new master page and upgrade one module at a time, does it matter if the new page isn't a pixel perfect match? 
You can use MVC (and specifically Razor) side by side in an existing project. Numerous popular CMSs do this (sitecore and umbraco to name a couple). You simply need to reference the appropriate assemblies and modify your web.config to properly define how you want routing to be handled. A lot of this will be specific to your application, but it's for sure doable. Just be cautious of how you architect future development.
I am aware that is possible. However as I outlined in the post I am constrained by utilizing (sharing) webform bits via the master page. Which AFAIK rules out razor.
I think there is a lot more differences to the view engine anyway. Don't think you can reuse your ascx files by avoiding razor.
Thank you very much, I am actually going to revise everything you have listed before my interview.
Is it me or nobody mentioned events / delegates? Apart from those, and basic oop, I usually look for the persons interest and willingness, don't go there scared because that is a -
I would recommend rewriting all the controls the MVC way with Razor, or not doing it at all. I have some bad experience trying to retrofit existing stuff in MVC. It became an unholy mess that was worse than the old version.
I don't have a degree. But I do have 15 years of experience. I've never found not having a degree to be a hindrance. So, my point is this. get some experience in .Net and you'll be fine. Do some projects in your own time. Stick them in Git or whatever, and try and get an entry level programming job. Work your way up from there. 
Well the more advanced areas, are for those who are **more advanced**. A couple of universities in the UK are now using c# as the imperative teaching language, for people with three years academic experience, I would expect them to grasp the basics, maybe not the whole RWL vs RWLSlim thing. This maybe biased because of the nature of my past few years work being based around high performance, high scaling, high availability *real time* risk systems. That has the benefit of very high budgets, so if we're offering someone £10k more than the industry average, I'll be looking to be very selective. If I'm going to have someone working on a project that has a lot of Rx, I need to know they are familiar with the computer science foundation concepts. Funnily enough, I'm not doing that at the moment, what I'm doing is considerably more noddy (simplistic). However, anyone who has just come out of uni I'd certainly expect to know their SOLID principles. It's that bit where someone who has little experience must learn the textbook definition, I want every single class they write, for them to look at it, and think to themselves, have I applied each of the rules in turn, before they go to the code review. I've noticed that some people lack the aptitude to do a "solo code review", instead they rely on the more veteran developers to walk them through it, every single time. With very green guys, this I find can seriously impact productivity, in much the same way that a pair programming exercise between one experienced and one inexperienced dev, can either end up with the experienced one driving, and the lesser one completely failing to learn or understand whats going on, or the inexperienced dev going at an incredibly slow pace, himself giving it only minimal thought, as the other dev will tell him what to do. Now if I had someone who had 10 years of experience, I'm not going to be demanding they know the formal definition of say Liskov Substitution, but I would expect them to be able to demonstrate the skills in a test exercise, even if it is just explaining WHY resharper suggests a code change. &gt;Long story short I would probably not quality as a junior developer at your company. Then again I've been very successful doing what I do for many years, producing very successful software solutions. The problem is it really depends on your industry or niche subset of it. For the one I've been circling in and out of for a decade these matter. Even if you've only had to implement your own hashcode once before (I'll be honest, I normally just ask ReSharper to do it!) I would expect a graduate of computer science from a BCS accredited university to know that, to talk about it. I would expect the ones who are top of the class to be able to explain why you might grow via pseudo primes to boot. **They are thought this stuff.** My reasoning is simple, if someone can't explain how to make a basic collection, a basic hash set, and they've a peace of paper that says someone explained it to them, examined them on it, then I know they are very likely to be time consuming at best, useless most probably.
As you said niche areas will always require special skills. The vast majority of us are working on LOB applications, improving productivity of our businesses. We're not launching space shuttles. Everywhere I worked, it would be far more beneficial if a person knew how to design a 3 tier application rather than implement GetHasCode. YMMV.
Can you elaborate on this? As far as I understand there is no issues with utilizing existing webforms master page with a ASPX view. I am not looking to "mix" any more than that.
I guess it depends a bit, to me threading basics are such an essential, that someone who's been through the BCS accredited degree, I'd expect to be able to discuss them with. This is an upshot of what I've been doing day in, day out, my view of the world is skewed by this. If you had someone who was coming to do a WinForms or WPF app then yes, these are applied skills often specific to one framework, the general education of their degree won't really cover it. But threading to me is so important, it's where I see most people mess up too. As for doing 3 tier application design, I'm not so fussed. Here's why, I've never let a graddy run off and do greenfield on their own. They might be working on our green field project, and if they can demonstrate an understanding of SOLID principles, then the idea of splitting things into tiers will just make sense, they'll see what's going on. Furthermore, n tier application design often has quirks to the project you are working on. Some people it simply means having three csprojs, one called Model, BusinessLogic and UI. Fair enough. What I really care more about is the fact that each class is SOLID, I can just whack F6 and in fraction of a second move the SOLID class to the right area or namespace. A non-solid class will have much bigger problems attached, odd side-effects, something you expect to be an idempotent action really changing state etc. At the end of the day, it's a bit arbitrary deciding where to put something, hell this current project I've got Domain.DataBase and Domain.*SpecificSystemInteraction* for model objects. Having someone coming in expecting things to be split by some one size fits all distinction I don't think helps. The other thing is I wouldn't expect a graddy to really get IoC. Sure they might understand basic DI, but seldom have I ever met anyone who's degree course covered properly the concept of IoC. So when it comes for them actually adding say a 'business logic class' to our system, I'm still expecting them to need some hand holding on how to actually integrate it best. When to put a logging or replay interceptor on it for example. I would hope that when they join the project, they would see how it's layed out, and get enough of an impression to know where something should go. Frankly, if someone is doing SOLID, then they are doing n-tier, even if they don't split the classes into obvious namespaces, the classes themselves will have clear tiered interactions. If someone is doing n-tier, they might not be making SOLID classes. Hopefully that explains why I put such a strong emphasis on SOLID, it's a basic micro level pattern, that if applied well makes the macro much more manageable. But also, just to recap, I'll ask about hash codes *because a BCS accredited graddie will have been taught it* and it's a simple thing to ask, that has very practical use, even if they just choose to make a struct 'key' or just use default location.
No degree here either. I was in my late '20s when I started learning (while in other IT fields), and 30ish when I got my first paid development job (very entry level). By my late 30s I had moved to the top of that company, and then started my own ... 
When I started as a Jr I did not have my degree. I've worked alongside many who have and have not had formal degrees. It doesn't seem to determine whether you will get a position or not. It does affect salary negotiations. It also makes it slightly easier to get an entry level position having no experience what so ever. Your previous experience as a developer even from 10 years ago is probably more relevant than a degree. It is smart to take into account your local market. Your lack of a degree should not in anyway determine the technology you decide to invest in.
When I'm responsible for going through resumes looking for new hires the education is one of the last things I look for. Something that always catches my eye is a list of projects that they have worked on. This is much more preferable since it shows real material knowledge and interest. Joining an opensource community or starting your own small opensource project is a great way to gain traction. 
Experience &gt; Degree. I only have experience in the south of the UK, but we're so desperate for .NET developers that we don't even look at the education. It's all experience, not years but what they've actually done in those years.
&gt; That drop down does not seem to appear if you use the MVC5 project templates in VS13 AFAIK. Mvc 5 does support the WebForms View Engine. However tooling support for it is missing out of the box. StackOverflow discussion about it here - &lt;http://stackoverflow.com/questions/18703304/asp-net-web-forms-scaffolding-feature-missing-in-vs-2013-rc&gt;. There's a VS extension that restores the the feature - &lt;http://visualstudiogallery.msdn.microsoft.com/a6c3614f-83be-4749-afbc-8da394b6ea86&gt;. 
The degree only helps get you into the interview. That's it! My advice: Apply to a ton of positions and take whichever will accept you, just so you are back in the game. After 6 months, evaluate where you are at, and if it isn't what you want, start appyling to specific positions that you do want. If you are stuck and can't land an interview, don't be afraid to use head hunters aka consulting firms, at least until you get in.
You don't need Breeze. Angular's $http service is easy to learn and can accomplish any call against a REST service. The Web API side of things is agnostic to how it's called. Hope this helps.
Thanks, that's good to know.
&gt; I only have experience in the south of the UK, but we're so desperate for .NET developers We're in the North-East and it's exactly the same up here. It's been a real struggle to find .net developers of any decent quality, unfortunately.
Same here. Getting first job without experience or degree was tough. The first job without a degree will probably suck, underpaid, overworked and your lack of degree will hold you back. Get two years experience and now you have a resume built off actual work experience, move from there.
One thing people worry about when changing careers is starting over. However, software careers can be built almost overnight. Just 3 years ago I was an integration engineer making 35k out of college. Now I am senior .net developer make six figures. Why? Because every time I felt I had mastered my role I changed jobs and asked for 20k-40k more and I did that ever 6 months to a year and I have no intent of stopping. There are three keys to doing the above. (1) Dive fast and dive deep into every new role. Every time I start a new job it takes me 1 month before I know more about the code then anyone else. I spend off hours figure out where everything is, why everything is and what choices they made to get there. You don't need to understand every line (At least not in this phase) but you do need to be able to move around a code base as if you wrote it yourself. Learn your IDE F12 Go To Definition, Go to Implementation is your friend. (2) Understand the business. A lot of engineers want to write really cool code, Ohhhh look I got dependency injection! SMILES! Well if you can't take business rules and write business logic you are crap to me. If you can't justify your own existence to the ones writing the check (The business) you won't be getting that raise. Understanding CODE is really fucking important, but understanding *business* can make your career. Know why they asked for the feature, know how to keep customers happy, know how to make them money and they will pay you for it. (3) Know your worth. Someone once told me a good developer has their presence felt by their team. A great programmer has their absence felt by the team. If you leave for a week or a day and your teams development slows to a crawl or god forbid halts it until you get back.... then you are worth more and should seek it out. This doesn't address your questions. But it is some life lessons you may find useful in your career..
tldr: virtual and override
However, experience + degree &gt; degree Even if it's simply to bargain for more money. 
Oh nein! Op figured out ze international conspiracy! I'll tell you what, at the next illuminati meeting, I'll bring it up. Although as far as I recall ".NET - a bunch of libraries" was discovered by marketing to perform poorly in emerging markets. Apple has the corner there anyway; by having fruit related names (apples, cocos) they attract the coveted *malnourished* demographics. 
it's a platform (i.e. something that other things are built on)
http://en.wikipedia.org/wiki/Software_framework http://stackoverflow.com/questions/148747/what-is-the-difference-between-a-framework-and-a-library http://en.wikipedia.org/wiki/Inversion_of_control 
technically EF and WebAPI are part of the .NET framework. .NET is a lot bigger than just the set of class libraries it comes with.
From wikipedia: FCL and CLR together constitute .NET Framework. FCL (Framework Class Library) is of course what you are talking about when you say "a bunch of libraries". However, the second part is also important. The CLR (Common Language Runtime) consists of the JIT compiler (basically turns your dlls into binary in a lazy manner), memory management (garbage collector), thread management, and exception handling (utilizing Structured Exception Handling). I would argue that the C libraries are not a framework, but the Java Runtime + Java libraries could be considered a framework.
**.NET** is a platform. The **.NET Framework** is a framework. EF and Web API are also **frameworks** that build up on the .NET Framework. A framework is nothing else than a bunch of libraries that help you do a lot of things.
What kind of fruit is a Swift?
It's not. It brilliantly works in tandem with fruits as a call to action - "swift cocoa" means "get yer coconuts quickly!". This rates positively with the hungry youths, that flock to new buzzwords abandoning their old employers and codebases. 
Conceptually this is very similar to what I have. As you point out the issue here is the same as my code, IMapper cannot be easily configured given only a Type instance. 
You can target pretty much every windows machine .
All windows machines are per installed with .net Also you can do .net in Linux and I believe osx through mono
.net is distributed with windows. .net 3.5 comes with win 7 and 4.5 comes with win 8. Then look up market share of is to make your decision. http://www.netmarketshare.com/operating-system-market-share.aspx?qprid=10&amp;qpcustomd=0 
This is only partially true. While technically every Windows has .NET installed it is always a different version. - XP: .NET 1.0 - Vista: .NET 2.0 and 3.0 - Vista SP1: .NET 2.0 SP1 and 3.0 SP1 - Windows 7: .NET 3.5.1 - Windows 8: .NET 4.5 (in-place for 4.0) So you can't really 100 % assume .NET is installed on every Windows machine. Basic installation of Windows 7 only has 3.5.1, but not 4.0 or 4.5 - while Windows 8 only has 4.5 (and thus 4.0), but not 3.5.1. But **most likely** Windows 7 machines have 4.0 installed (not 4.5). But still... for clarification of your false statement. :-)
It is worth mentioning that **most** Windows 7 installations have .NET 4.0 installed. If I remember correctly it also came as an **important update**. .NET 4.5 (that ships with Windows 8) is an in-place upgrade for .NET 4.0, so it is (except of the few breaking changes) compatible to 4.0.
And Linux via Mono. If that is still in use.
Other versions are supplied through windows update, so most machines will have v4 or higher
While those are the defaults you can install .NET 4 all the way back to XP (SP3). 4.5 to Vista (SP2). If you use Click Once or some other installer it will auto install those for you on supported versions. 
I think there are other aspects of a platform that might be worth looking besides the install base, mainly things like updates/versioning and security vulnerabilites. In both of these respects .Net is going to offer a number of benefits over Java. As pointed out by others in this thread, you can generally count on v4 of the .Net framework delivered by windows updates. There are generally also patches that are delivered through windows updates too. Java on the other hand has far more problems than windows in regards to updates IMHO. Far to many people do not have Java auto updates on, and the frequency of Java updates generally means that IT doesn't allow them through each time. As far as security goes, the general reason for such frequent updates to Java is the constant security fixes. There are some power users that choose not to have Java installed because they consider it a security risk. This risk only increases as a specific version ages. .Net on the other hand has a much better track record as far as security goes, and since security updates are released through Microsoft they've got a better chance of actually being rolled out to machines. There are other platform options besides .Net and Java, but since you didn't give all that information about your project, so it's not clear what else might be worth looking into.
EF 6.1 now allows you to configure indexes via attributes. http://blog.oneunicorn.com/2014/02/15/ef-6-1-creating-indexes-with-indexattribute/
Yes, the problem I have is that tables created with azure mobile services.net backend have a default set of attributes that cant be modified on initial creation. Specificly the primary key is non clustered. And i just don't understand how to do a migration in .net to change them after creation.
Personal thought: I find php as it is just insane as webforms or webpages (brrr). Hint to find a place where php is used as it must be used: search for a job offer in which they specify a good php mvc framework like konoha, symphony, yii or Laravel. You'll start loving php instantly (well less than c# obviously, but still enough). For the underpayment, in my zone the salary is almost exactly the same but again I'm speaking about place where they use php-frameworks.
It depends on what he uses. Mvc5 is not supported and mvc4 only partially, async/awaint the same, wpf is not existent, etc etc 
This is true and I should have mentioned this. I was only refering to the standard installation that ships with Windows.
I would contact the guys who made the website for you. If the guy that left didn't make any changes tothe settings, then the documentation is insufficient, hence contact the designers.
lol. Maybe you should hire him back as a high paid consultant. I have no idea how your site is setup, but normally you login as admin, navigate to the page, click the little edit content icon for an existing html module, or add a new html module and edit it's content, etc. There are decent videos out there that will walk you through this stuff. You might want to get a trial pluralsight subscription and watch their dnn videos. 
There are a few things here: * As has been said, you messed up big time. Many managers are totally oblivious about their technical dependencies and it's one of the worst features you can have in this day and age. The mistake wasn't just letting that guy go, it was having that guy, exclusively, know anything about your site. You'll likely figure out this files issue, but, you're going to really get in trouble when it comes to normal DNN updates, hosting issues and renewals, SSL certificate issues, etc. * Look to the DNN forums, but don't ask a question like this. The forums are decent, and they have admin and superuser manuals. So, ask questions, but keep them simple - "I'm new to this - how do I upload a file?" or "I uploaded a file - and I think that worked, but it's not showing on this page like I thought it should. Do I need to do something else?" * Learn DNN. You can pretty easily install a site on your computer using the Web Platform Installer, or, there are tutorials for setting up a free site on Azure. In any case, if this is a major part of your job, or you're overseeing those for whom it is, you should take responsibility and learn DNN. That might involve some real work on your part - but do it... it'll make you a better manager. 
Putting all the stuff others have said aside and actually helping you. Start with the user manual. You will need to know what version of DNN you are using for they differ a bit. [Here is how to find out.](http://www.dnnsoftware.com/community/learn/video-library/view-video/video/360/view/details/how-to-tell-what-version-of-dotnetnuke-you-are-on) You can then download the Admin manual from [Here](http://www.dnnsoftware.com/community/download/manuals) Secondly you will need to know the host username and password. if its default (probably not) it is host/dnnhost. if not you will need to recover that. 3rd give your self time. In a couple fo weeks you will be a DNN admin with easy just give your self time to make mistakes and fix them. I would at this point be backing up the whole IIS folder for DNN and the DNN database from the SQL Server. That way if anything goes wrong you can quickly restore back to your original state. Lastly feel free to PM me for questions I can always login remotely and we can work on it together. If it needs to go beyond that I would suggest a professional company (not to large or to small, somthing like http://www.cmcweb.com) NOTE: I work for this company, this is an unabashed plug for it :) 
RTFM! Sorry, but DNN comes with documentation, you should at least familiar yourself with the system through the documentation and while reading it, look at how your site is configured. You first fucked up with this employee and now instead of doing the right thing, you're again as lazy as one can be and ask here instead of doing everything you possibly can to learn your own system. Sorry if I sound mad, but this kind of behavior is far from acceptable. More and more we slide towards a community where even the slightest setback is a trigger to first ask and let others figure it out instead of doing the work yourself. 
If you haven't used this for testing MVC apps, you should really try this out. It is a very good tool and is very simple to learn and use. It is absolutely perfect for doing UAT testing all through code. Disclaimer: I have contributed to this project. We thought it was fantastic, but it missing Windows Auth, so we kindly added it for everyone.
Cheers for the input and sorry for the delay. Found out the host had been keeping security setting details from me so I left them and went else where. My sites run now. ☺
Good point, have an upvote. But it is still installed with .NET alas different versions :) plus you can force an upgrade at install time Side note: There are people still running Vista :/
That seems weird. I have not run into this issue before, but I use IISExpress. I also wrote an extension KillCassini, that kills all local IISExpress instances. Are you using full blown IIS? Also, are you talking about Javascript caching, or something on the server side?
Much like the author, I use Resharper heavily all day long, but the performance implications of using it (esp. Solution-wide Analysis) are very real. As the article states, Microsoft hasn't included a lot of the major things that Resharper does, but if they were to take care of the top 10 most used "Alt+tab things" that Resharper users do, without the performance degradation, I could see a lot of Resharper power users jumping ship in favor of a vanilla VS2014 install.
Here is the [JetBrains response] (http://blog.jetbrains.com/dotnet/2014/04/10/resharper-and-roslyn-qa/) to 'Will resharper use roslyn'. It has a few tidbits about what resharper can do that VS won't be able to do. 
I'm currently working on a project which is utilising CQRS. Its just WebAPI atm. We have a service layer directly between the API and the Event Repository (Your Command layer). This allows us to shortcut read only calls, do a large amount of pre validation, and eventually host the Event Processing layer in its own process (with a coms layer in between). It really depends on your domain. I've seen them implemented both ways. A trading system implemented with CQRS will probably have the Command layer as the highest level. For Web Projects it maybe better to the service layer there, with a thinner command/event layer
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Betteridge's Law of Headlines**](https://en.wikipedia.org/wiki/Betteridge%27s%20Law%20of%20Headlines): [](#sfw) --- &gt; &gt;__Betteridge's law of headlines__ is an [adage](https://en.wikipedia.org/wiki/Adage) that states: "Any [headline](https://en.wikipedia.org/wiki/Headline) which ends in a [question mark](https://en.wikipedia.org/wiki/Question_mark) can be answered by the word *no*." It is named after Ian Betteridge, a British technology journalist, although the general concept is much older. The observation has also been called "__Davis' law__" or just the "__journalistic principle__". In the field of [particle physics](https://en.wikipedia.org/wiki/Particle_physics), the concept has been referred to as __Hinchliffe's Rule__. &gt;Betteridge explained the concept in a February 2009 article, regarding a [TechCrunch](https://en.wikipedia.org/wiki/TechCrunch) article with the headline "Did Last.fm Just Hand Over User Listening Data To the RIAA?": &gt;&gt;This story is a great demonstration of my maxim that any headline which ends in a question mark can be answered by the word "no". The reason why journalists use that style of headline is that they know the story is probably [bullshit](https://en.wikipedia.org/wiki/Bullshit), and don’t actually have the sources and facts to back it up, but still want to run it. &gt; --- ^Interesting: [^Betteridge's ^law ^of ^headlines](https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines) ^| [^List ^of ^eponymous ^laws](https://en.wikipedia.org/wiki/List_of_eponymous_laws) ^| [^Sensationalism](https://en.wikipedia.org/wiki/Sensationalism) ^| [^Sport ^in ^Birmingham](https://en.wikipedia.org/wiki/Sport_in_Birmingham) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cia67on) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cia67on)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
MS is so full of crap with all this C++/CX bullshit.
I used to be on stage for Microsoft TechDays in France for C++ sessions since 2011 and for the 2014 edition I have chosen to let attendees to discover the world of Windows Store Apps for ISO C++ developers. I have made studies for async stuff and collections handling. I have decided to write a technical article because C++ developers want to have power and control. Microsoft makes the promotion of C++/CX but the reality is that MS is building apps using C++ and WRL but it is poorly documented. I have spent some weeks about reading the headers and IDL files from Windows SDK and wanted a real experience like, 10 years ago, we had, with ATL or raw COM programming. WinRT and WRL programming is not very difficult but there are some tips &amp; tricks for example for collections, event and async stuff. Hope you will enjoy to read like I have discovered it. I have made several posts (emails) to Microsoft Visual C++ folks about the fact that Microsoft C++ developers rely heavily on C++, ATL and Win32 and that C++/CX will not certainly be a success... ATL was a popular library so why WTL is so poorly documented ? I had some MSFT responses but they wanted me to adopt it me and answer no question. I am a VC++ user since 1994 so those new guys won't change my mind. I want real C++ !!! So when I made some arguments, one of MSFTies added Jim Springfield (creator of ATL) in out private discussion when I asked them why WRL could not be the ATL successor ! I also had Microsoft support from creator of WRL and Visual C++ Lead to make correct use of dynamic type T using std::vector&lt;T&gt; matching for T as a HSTRING or T as a ComPtr type or another type. I would failed if they were not reviewed my article so I have to give them a great Thank you guys.
Control + W is reason enough to pay 149$ . Until Visual Studio has it, I cant even consider using it without R#
&gt;A Resharper favourite of mine is adding an interface to a class, hitting quick fix, and having it generate all the missing methods and properties. Much quicker than typing it all out by hand! &gt;The Visual Studio team don’t seem to have added any new quick fixes that I can see. What? this is in vanilla vs 2010 : http://i.gyazo.com/1fba1397c21b4e175986358fecc2c009.gif you don't need resharper for that. 
I read that when it came out, and it started as a fluff piece and ended as a fluff piece. There wasn't a lot of meat in it. Also, there's absolutely nothing that Resharper does that MS couldn't do inside VS, clean install. :-/ Ever since VS2012, and especially 13 -- MS has been making huge gains, leaps and bounds in the right direction. VS 2014 won't replace Resharper for those who strap themselves to it -- but it does hang the sign on the door. 
This is my main complaint against those who use resharper. They don't seem to understand how many features vanilla VS has.
Control + Shift + Left/Right Arrow is similar, but not as nice.
That's true, of course, but R# has a lot of extra you wouldn't get out of the box. A lot of people have been using the two so long it's no longer clear what's native, though.
I think CodeRush said they would use Roslyn, which should make things interesting soon.
The killer feature I want in vanilla VS is GNU-style indentation.
Is R# really a niche application, though? Considering the relative ease with which everyone can write VS extensions that use Roslyn for code analysis I suspect the writing's on the wall for ReSharper; who wants two continuous compilers running at once?
Wait, people who use ReSharper in general? Or just the jerks who over evangelize it like it's the second coming of Gates? It's not like VS is useless without it, but I do love me some dynamic templates, and namespace and signature refactoring have saved me so much time.
If you're coding web apps, you'd still want to have Resharper installed, from what I can see. Resharper's Javascript navigation and refactoring functionality is still way ahead of Visual Studio. Here's [a comparison I did](http://www.nilzorblog.com/2013/12/a-comparison-of-javascript-ides.html) between vanilla VS2013 and R# VS2013 at the end of last year, and as far as I can tell, it is still valid (see the table at the bottom). Correct me if I'm wrong.
Not helpful
Doesn't work. Change the language drop-down and it just goes to a default file.
Thanks! 
ASP.NET Web Forms is a serious mistake. MVC is the future. Want to use one of the thousands of open source Javascript and CSS libraries available? You can't do that with ASP.NET Web Forms easily, if at all. ASP.NET Web Forms is not a Web development framework, it is Windows Forms translated to the Web, it uses a desktop development metaphor. Yes, you can create Web sites with it, but it makes the developer ignore the mechanics of the Web by adding a proprietary event structure on top of everything. Not only is the cryptic and poorly understood by people experienced with Web technologies, what you learn using it isn't transferable to any other technology. If you need to supplement people to your team, any person experienced with Web frameworks can figure out MVC because it uses core Web technologies and terminology. ASP.NET Web Forms is only familiar to people who have developed Windows Forms desktop apps. Finally, look at this: [Google Trends, plotting ASP.NET Web Forms vs. ASP.NET MVC](http://www.google.com/trends/explore#q=ASP.NET%20Web%20Forms%2C%20ASP.NET%20MVC&amp;cmpt=q) I'm guessing that you are going to say "That's not an option", but I just converted a legacy app from Web Forms to MVC and the person I built it for is now starting to learn MVC so that he can jump in when necessary. It can be an option. Let your supervisor know that "the Internet" thinks building a new ASP.NET Web Forms site is a poor decision.
async/await work on mono; it's just the asp.net API that uses tasks that's not implemented.
If it were up to me I would probably go with MVC but like I said, I'm using VB.net and webforms because I have to. Maybe I can talk to him about using mvc the next time I see him, but as it stands they want this done in webforms. Thanks for the advice anyway though edit: and trust me, I would much rather easily integrate with javascript and CSS. I hate the fact that webforms does all of its own CSS inline on the fly, even turning auto-style off, it's still glitchy at best. 
Someone is going to have to sell me on Resharper. I have added it to VS 3 times now, and I have removed it every time. I don't think it does anything but get in my way, and it makes VS run a lot slower. Someone tell me why I should use it.
I'm glad you've seen the light here. I often use VB.net and that isn't an issue for me. I would prefer the ubiquity of C#, but there is a very active VB.net community, and at the end of the day, experience with the .NET framework is the most important thing happening. Don't piss off your boss, but he's taking the company down a lonely road as far as having a site / app that is easily maintained. 
To convert you need to select "Convert to VB.NET" option in Tidy UP menu: http://screencast.com/t/l461W6zQFs Changing options like Language/Project type on the left - are for creating new fiddles. I included a short description on top of the fiddle. 
They would have to also cover most of the code navigation aspects that R# provides. For me, at least all of the following: * Ctrl+N (go to type) * Ctrl+Shift+N (go to file) * Ctrl+Alt+Shift+N (go to symbol) * Ctrl+E (recent files) * Ctrl+Alt+Shift+B (go to implementation) * Ctrl+B (go to declaration) * Ctrl+Alt+B (derived types) * Ctrl+Shift+T (go to type of symbol) * Alt+F7 (find usages) * Shift+Alt+F7 (advanced find usages) * Ctrl+W (increase selection) * Ctrl+Shift+W (decrease selection) I would want most of the alt tab fixes, code reformatting, dynamic templating (esp surround with) and code generation options as well. 
Yes, the controller factory is responsible for creating and releasing the controllers. To be precise it's the [IControllerFactory](http://msdn.microsoft.com/en-us/library/system.web.mvc.icontrollerfactory.releasecontroller\(v=vs.118\).aspx) interface. You can override this element and instead of creating a new controller you could re-use an existing instance (if that's what you want (why would it be?!)). &gt; Interesting though, it seems if the Controllers are in an assembly in the same namespace/subnamespace as the Web application, it automatically loads the assembly. I don't know the default behaviour, but you can just look it up yourself: http://aspnetwebstack.codeplex.com/SourceControl/latest#src/System.Web.Mvc/DefaultControllerFactory.cs
Try converting C# -&gt; VB, then convert the VB code back to C#. No sanity :)
Yeah it is not 100%. Still requires a bit of manual intervention in some cases. 
There are many reasosn for not wanting to use C++/CX but they don't matter. What matters is that C++ developers didn't want to use Managed C++ nor C++/CLI, they don't want to use C++/CX now and MS is not documenting the WRL properly. That's stupid.
I don't think unit tests is the problem. They built a huge complex system that is dependent on some underlying data structures they have honed over the last half decade. Wiping out half their code base which holds the lookup information and re-writing the other half that does the lookups is more then a 'hey bro just unit tests that bitch!' scenario. 
What sort of applications are you developing?
You can push Alt-Enter,Enter and instantly turn a 20-line foreach loop into a 19-line Linq expression. :) Tongue-in-cheek but that's actually how I became fluent with Linq, by studying the more complex transformations that I would have never figured out on my own. I don't think there is a short list of killer features that could convince anyone that they must use it. It's mainly just a big collection of small conveniences. One that comes to mind is that it's only a couple of keystrokes to yank a class into a new file, automatically named, usings added, formatted properly, etc. Another one is that it has better inference for automatically adding usings *and* references. If you use it long enough, you may become dependent and start to feel naked without it. Or you may become frustrated with performance and all the "hints" it constantly throws at you, and feel liberated without it.
You'll be needing server controls. On postback, their values will be sent back to the server automatically. Many people may not like them, but don't fight the framework and just use them. The framework also provides you with a lot of validators, which make validating quite easy. I don't like webforms either and prefer mvc anytime, but as an asp.net developer, you're bound to encounter them sooner or later. You could just use html controls (and put a runat="server" attribute on them), should you need more control over your html output, but you will require more backend code then (like handling viewstate or fetching your data from the Request.Form collection).
You should be using pure HTML elements. edit: If you have to ....server controls. 
Well, Resharper does a lot of things. Value depends on your use. One example is dimming code that can never execute (such as after an infinite loop, or a case in an if that will never be evaluated due to an incorrect bit of boolean logic).
Slightly out of context - isn't that overkill for a small project? So many files, so many structures... it all ends up being more than the code the programmers adds!
As somebody that has a lot of projects in both languages, this could save me a lot of time doing it all by hand, even if it requires a little clean up afterwards. Thanks!
Well in fairness to those people, Resharper and Code Rush had them first and VS pulled them in. They probably just never paid attention to the editor updates in newer VS versions. That being said, I don't think anything will ever kill resharper, there are always new tweaks to make. I do like that MS is looking at those tools and pulling in the stuff that works best.
Unawareness, looks like I have some reading to do! 
That's like converting English into Hordor and back to English, you are bound to loose some elegance.
Not 100% sure what you mean, but you might want to take a look at this: http://blog.filipekberg.se/2011/11/14/using-the-c-interactive-window-that-comes-with-roslyn/ 
Uh, you mean "Convert VB.NET to C#"... No point in the inverse ;-)
Looks like Microsoft finally discovered dependency injection.
Could you make your question a little clearer? Are you asking what my applications are used for? What industry I'm in? Architecture? Programming techniques and patterns?
Just because the author uses link-bait does not mean it is definitely a no.
Also consider that while Roslyn on its own may not compete with ReSharper it does give us programmers the power to do so. There is nothing stopping us from making a community static analysis and refactoring tool.
I think that's true for at least some cases of unreachable code, but it's much nicer seeing that as you type rather than at compile time.
Been in MVC for a while. Been available through Unity for years.
Create a unit test project, then call it from there. Test for valid outputs.
What do you guys think about Seemann's opinion on ASP.NET vNext Dependency Injection: http://blog.ploeh.dk/2014/05/26/feedback-on-aspnet-vnext-dependency-injection/
Just those who act like they can't even code without ReSharper. It is definitely a great tool.
He's correct - having a generic activation hook is great, required, and applaudable - but component life cycle management and patterns like "session per request" often need finer control over scoping than a single hook provides. Thankfully the two things aren't mutually exclusive so its nice to see a low level hook regardless.
More in the sense of: are you developing web applications (MVC), desktop applications (WPF), ...?
Really no more than a webforms project - all those ascx, ascx.cs and ascx.designer.cs files add up. I personally think MVC works best for small projects and not so well for large projects that need a lot of view composition.
Fair enough, pity some people are too stubborn to learn new things.
There's a reason webforms is dying and shouldn't be recommended to new people. If people can use MVC then it's normally for the better.
Not sure this will supercede either the Telerik code converter or SharpDevelop's code converter - they both use the same engine I believe, but Sharpdevelop does entire projects whereas Telerik just does snippets. The issue in going from VB to C# however is the amount of sloppy stuff that VB allows that C# doesn't (passing uninitialised ByRef parameters, blank optional parameters etc) and and the fact that VB's Case doesn't exist in C# and Switch isn't as full featured. Not to mention that C# is Option Strict On (hey your VB projects are Option Strict On aren't they? Hmmm?) Source: VB guy who's just converted a 200KLOC VB solution to C#, because he wants to be a C# guy ...
...and make the calls that let you submit forms, and do Ajax calls to update controls, and build single page apps with angular, and use all the different jQuery ui controls, and make fully responsive pages? GOOD LUCK.
yep, waiting for official image though :)
I took this approach as being an attempt at solving the OWIN dependency injection story. Essentially if I want to inject things into other OWIN modules, there isn't a real good way to do that right now...
If you are eligible for Bizpark, that's the cheapest option for couple of years: http://www.microsoft.com/bizspark/faqs.aspx The next option is Action Pack: https://mspartner.microsoft.com/en/us/Pages/membership/action-pack.aspx https://mspartner.microsoft.com/en/us/Pages/Membership/action-pack-subscribe.aspx Detailed pdf on action pack contents: https://partner.microsoft.com/download/40196872 
Google uses a cached result.
Check to make sure firewall has a whole punched in it between servers and that your program is using the correct port when communicating.
Thanks. Just clarified this in my post. Yes firewall is open
So wireshark sees the incoming request on the cache server over port 22233 hugh? Interesting... Does it see a response from the cache server back to your app server? Does that response contain mroe information then the generic error?
All of the above :)
Well, basically binary serialized transfer. So, I don't see anything ascii in it. I can request and responses. Only weird thing I noticed is that last communication from app server is application/negotiate. Cache responds which I assume is the error code followed by RST (which is weird)
thanks. Author points out in ps this is specific to Azure cache only &gt; This is specific to Azure AppFabric Cache only Also, DNS isn't the problem. I can hit IP and name from app to cache over telnet to port 22233
Arrrrgh!
We are using RabbitMQ in .NET for this purpose.
MassTransit is somewhat close - distributed, high-level abstractions on top of RabbitMQ or Msmq. You are not limited to tasks though, and with request/response you get synchronisity (if you are unfortunate enough to need it).
I'm not a fan of RabbitMQ, but mostly because it seems a little processing heavy. One message queue that hasn't been mentioned, so I will mention it, is the one that's built into SQL Server -- SQL Server Service Broker, which is, essentially, just a message queue that you can access/control via T-SQL. Go with MSMQ unless your network architecture team gives you trouble, in which case go with Service Broker. 
what do you mean by 'processing heavy'? fwiw i've had nothing but good experience with rabbitmq and nothing but terrible experience with service broker. downsides to rabbitmq are: * the default client library (which is why i wrote chinchilla github.com/jonnii/chinchilla). * you have to install erlang and the rabbit-mq server * setting it up for HA isn't straight forward another option for distributed work management if you dont want a message broker is to roll your own using zeromq. it might sound like a lot of work, but it's not that hard and can end up being more flexible.
Look at http://hangfire.io, it provides a framework for creating, executing and monitoring background jobs in .NET without pain. What kind of project do you have?
Just a quick rundown of what I use the most, I've mostly been working in MVC lately: * CTRL+F12: go to implementing method of interface (if more than one, show list). * Quickly make methods in your business and data layers, just type out what method you'd like, and Resharper will generate a stub with the correct inferred datatypes, and even nice argument names (VS may do this stock, but Resharper definitely does it nicer). * Navigate to MVC views with F12, highlight views that don't exist yet, fills in the view's model in the Add View dialog, ... * General code hints that make life easier and code nicer to read, eg.: `list.Where(x =&gt; x.a).FirstOrDefault()` becomes `list.FirstOrDefault(x =&gt; x.a)` with the press of ctrl+enter. And probably much more that I can't think of right now, but probably use every day. And I keep finding new features! I can't comment much on how it makes life easier with WPF, it's been too long since I've last used it.
Why?
ASP.NET hates you and your synchronization context. And you only get one thread anyway. 
Great to know, thank you.
Please take with large grains of salt. Threading in asp.net is complicated. Awkwardly so. If you are on the happy path then everything is roses but step off and beware. Yes, i'm being glib but this topic is rather large and has many sharp edges. 
They introduced Hosting environment.QueueBackgroundWorkItem in 4.5.2 to make this a little easier.
ASP.NET kills your background threads on recycling process. You can register them in hosting environment as written [here](http://haacked.com/archive/2011/10/16/the-dangers-of-implementing-recurring-background-tasks-in-asp-net.aspx/), but ASP.NET can kill your threads anyway after a 30 seconds timeout by default. So, all background tasks must be persisted, as in http://hangfire.io, or you can lose them.
Give a try to http://hangfire.io and forget about all problems (disclaimer: I'm the author). It is backed by Redis, SQL Server, SQL Azure, MSMQ or RabbitMQ (coming soon).
QWBI is just a wrapper for ThreadPool.QueueUserWorkItem, that is being registered in a hosting environment. Look at the previous comment, 30 seconds timeout is still there. There is a great article that [describes QWBI](http://blogs.msdn.com/b/webdev/archive/2014/06/04/queuebackgroundworkitem-to-reliably-schedule-and-run-long-background-process-in-asp-net.aspx), but it has a wrong title – it contains the "reliably" word. See the limitations at the bottom – "Scheduled work items are **not guaranteed to ever execute**". QWBI is designed only for **short-running unimportant tasks**. I posted a comment to this article with question what reliability means for them, and with contradictory sentences from the article, but it was deleted.
While most of the terrible experience I had with service broker was due to the project being based off of External Activator (and that I inherited it), we also found that randomly messages and queues would get stuck and we had no tools to inspect the queue other than logging into the database. I'm sure if you know what you're doing there's nothing wrong with service broker, but I found it unintuitive and there are certainly better options out there.
I believe the article has been updated to reflect that it's actually 90 seconds, not 30. I agree that QWBI should only be used only for unimportant, light-weight tasks.
I work for a company in a similar situation, no MSDN subscription because we are only 2 developers. They had bought VS2012 before I arrived, but when the second developer was hired we took [Visual Studio Online Pro](http://www.visualstudio.com/en-us/products/visual-studio-online-professional-vs.aspx). I find it a relatively good solution, you have VS for $45 per month and it's always up to date (so when 2014 comes out, you'll get it). Then for Office, we went with [Office 365 Small business premium](http://office.microsoft.com/en-us/pc/compare-office-365-for-business-plans-FX104339483.aspx). You get Office Pro for $12.50 per month. We're missing SQL Server then, but we don't have any use of it for now, we're using SQL Server Express which is largely sufficient for our needs.
Msmq has a memory limitation of 2 gigs. That is 2 gigs total for hosting all your queues. Just be careful there. 
&gt; I believe the article has been updated to reflect that it's actually 90 seconds, not 30. To be precise, 90 seconds shutdown timeout comes from IIS, 30 seconds timeout comes from ASP.NET (to wait for requests and registered objects). 
AFAIK you are able to give users granular direct table access. In other words you can say userA can have read access to table Person, but userB has read/write access to the same table. No stored procs involved.
This looks great. However, I glanced at the docs and didn't see support for reliable task distribution. That is, send a task to one of 20 worker machines. If a machine fails, don't lose the task, move it to another worker.
I didn't know about Service Bus Queue, but it looks really interesting. It runs locally too. I didn't see support for distributed tasks though. 
Can anyone test this on RyuJIT?
The job is being deleted from its queue only after successful completion. HangFire has immunity to process termination: * MSMQ implementation uses transactional queues. * Redis implementation uses [BRPOPLPUSH](http://redis.io/commands/brpoplpush) command. Fetched list is being watched by a special component for terminated jobs. * SQL Server implementation uses `UPDATE` statement with `OUTPUT` clause for atomic fetch + update. There is a special component that looks for terminated jobs, as in Redis. * RabbitMQ uses `ACK` / `NACK` commands. This is an open-source project, don't rely on my shouts and look at the code ;) And I try to do my best with documentation, but this is a huge work. Thanks for the note!
You can have a look here; http://www.microsoft.com/learning/en-us/visual-studio-certification.aspx Of interest to you might be: 70-483 - Programming in C# and 70-486 Developing ASP.NET MVC 4 Web Applications. Check out http://www.microsoftvirtualacademy.com/ as sometimes they do offers. If you're going to sit one of the exams expect to pay around £100. Make sure you check out that site though because often you can get a voucher code which gives you a free second shot if you don't quite make the grade first time around. This is the site where you can book the exam or find a testing centre: https://www.prometric.com/en-us/Pages/home.aspx If your interested in 483 then I've found these videos particularly helpful. Although the official book covers a lot of the exam material, its presented in a funny order. However this guy does a great job of explaining things, particularly if you are new to things like async. The series isn't quite complete. But it's very good. https://www.youtube.com/channel/UCiA3tURI8dwCOUzCeM90c2g Have fun, good luck :)
TW: Patriarchy Please do not say guy 
&gt; Indexing into the MultiDictionary will never throw an exception (unless the key is null) and will always return an ICollection that changes as the MultiDictionary changes and vice versa. Are there any other built-in IDictionary data structures with this semantic? I am waiting for vNext mostly for the ability to write `d.TryGetValue(key, var out v);` instead of: var v = d.Get(key); //... somewhere else public static class DictionaryExtensions { public static TV Get&lt;TK,TV&gt;(this IDictionary&lt;TK,TV&gt; d, TK key) { TV v; d.TryGetValue(key, out v); return v; } } (reddit textbox programming; validity not checked) 
As far as I'm aware, the MultiDictionary is the only dictionary type with this functionality. Trying to index into any .NET data structure with an invalid key will throw an exception rather than just return null and there's lots of reasons for this, the most obvious of which is that for many of those data structures null is a valid value that can be associated with a key. Going off of your example above, this means that the first line of the following code is irrelevant since Get will return null regardless: d.Add(key, null); var v = d.Get(key); Generally speaking, we want to leave the option of null-values available and so exceptions are thrown when invalid keys are indexed. So why is the MultiDictionary different? If we view the MultiDictionary as a Dictionary&lt;TK,ICollection&lt;TV&gt;&gt;, then we know that there is never a null ICollection associated with any given key (because the internal ICollections are abstracted from the programmer and kept privately in the MultiDictionary). The MultiDictionary will let out a *view* of these internal ICollections through the indexer, but they can not ever be set to null (only removed or made empty).
Heya guys! I'm the developer for this collection, so let me know either here or on MSDN if you have any questions about performance or anything. Hope you enjoy :)
awesome. i'll try it out. 
The MultiDictionary is a bit of a strange situation in this regard, you're right. The current functionality is "working as intended", but it is a bit out of the ordinary I admit. One of the main reasons for indexation to never throw an exception is so that the programmer can index on that key to get a *view* of the collection associated with that key that they can then pass around and make modifications to. I reasoned that making Remove(key) disable this functionality would incur more possible confusion than it would provide usefulness. There is not currently a way to prevent further indexation or addition to any given key. Would that be something that you'd be interested in it having? Perhaps a dictionary.ImmutableKey(TK key) or something along those lines, where modification on that key is forbidden, and indexing on that key can have two different outcomes: 1. If the ICollection associated with a TK is empty, then dictionary[TK] will throw an exception 2. Otherwise, return an Immutable ICollection with all values associated with that key
What would you use for large projects if not MVC?
&gt;DoubleToNumber is pretty simple -- it just calls _ecvt, which is in the C runtime: Where do I find that source?! dotPeek just shows [native call] ..............
It's from the Shared Source Common Language Infrastructure (SSCLI) 2.0. http://www.microsoft.com/en-us/download/details.aspx?id=4917
System.Collections.Hashtable doesn't throw when indexing.
Wrote one ages ago: http://stackoverflow.com/a/380601/44991 Official repo (with updated version): https://github.com/SolutionsDesign/Algorithmia Nuget: https://www.nuget.org/packages/SD.Tools.Algorithmia/ Advantage is that it stores unique values in the contained collection, not duplicates. MultiDictionary uses ICollection&lt;T&gt; which doesn't filter out duplicates. Bonus functionality in mine: It has merge functionality :)
Re-routing feature is great!
Wow, yeah that's really useful 
Already added as a comment to the article, but I hope you don't mind if I repeat myself. Is there any reason why you don't return one of the new IReadOnly* interfaces when indexing? Personally I think it makes more sense since you (might) want to add values only through the real MultiDictionary. Or am I missing something?
Oh sweeeeeeet! Thanks!
as action pack is an MPN offering you may want to make sure the visual studio licenses are legit for your use - mpn has some rather nasty limitations that a lot of companies aren't aware of. &gt; MSDN subscription obtained as a benefit of MPN cannot be used for consulting services (only product development) 
Personally yes. I literally fell in love with IReadOnly* collections and I think they are a neat way to pass data around without the "deferred evaluation" semantic of IEnumerable.
I host my personal site on it. I am essentially wasting it, but I have a place to try stuff where security and reliability don't matter.
I run a VM with Visual Studio which I use as a development box. I work a lot with blobs and table storage so its location is helpful. 
Any advantages over https://www.nuget.org/packages/WebApiThrottle/? Besides the obvious self-advertisement.
Currently using it to try a new VS2014 sandbox.
Was thinking about this as well...Was it fairly easy to set up?
I don't want to say anything negative about the WebApiThrottle, so I just describe the differences. The way WebApiThrottle works - it implements static delay between 2 consequent calls. Although sometimes it is a desired behavior, it is not always. In most cases we want to allow certain number of requests regardless how often they happen as long as that number does not exceed the limit. Simple example: 5 calls/sec does not necessarily imply 200 ms in between calls requirement - it really implies "no more than 5 calls at any given sec"; 5 calls can be received within first 50 ms, and then nothing for 950 ms, or in any other pattern. So, the Throttling Suite handles all those different patterns by implementing 3 different throttling algorithms to choose from (please follow [the link for more info](https://throttlewebapi.codeplex.com/wikipage?title=Throttling%20algorithm%20considerations&amp;referringTitle=Home)). The concurrency: no comments on the other one, but Throttling Suite correctly counts all requests regardless how "close in time" they occur, even they go on parallel threads at the same time. Also, the Throttling Suite, contrary to other tool(s), does not use Cache - the .NET runtime can throw cached item away should the memory pressure raise (the cache priority can only delay it really) loosing all statistics that throttling relay on. I really meant the "performance" because it was originally implemented for such requirements and it's being used for a while at production site with 40+K concurrent users on it. The throttling tool is extremely fast and accurate. As well as it has self-cleaning statistics process taking outdated data out of memory to keep memory usage ALAP. BTW, the project source code has performance-related unit tests. Another benefit I'd like to mention is simple configuration: the main idea is that you divide the API on areas required different throttling and configure throttling for each area and, likely, HTTP method. This allows to handle the throttling faster overall, and avoid dealing with all possible routes; although it is achievable as well. Please feel free to ask more questions.
What size VM do you use and do you just destroy the VM when you're not using it (not counting the disk) so that you stay under the quota? 
When you go to create a VM it has some defaults one is for Visual Studio setup. I went with the default which creates a VM with 2 cores and 3.6 GB of RAM. I leave my virtual machine running I could shutdown I guess when I am not using it but that releases the ip and I would have to create a new rdp entry to connect. Running this configuration roughly bills at $43 for the 30 day period. Depending on your MSDN subscription you get anywhere from $50 - $100 - $150 a month credit to work with so for my level of access I can leave it running full time for a month and not go over my monthly credit. You can set your account to not allow you to go over your free credit so once you reach the max it will just shut it down till your next period comes up.
Build Server. VM with VS14 CTP. 
Build a Linux VM. Install a http proxy in it. SSH + port forward + foxyproxy your way past all of your employer's web restrictions &amp; logging.
Just because you can doesn't mean you should. 
And there responsible for most of the anti-patterns seen in the wild.
The statement "You should not create/rent custom threads in ASP.NET applications" is opinion-based. To make it impersonal you should write _why_ one should not do this, because the choice whether to create custom threads is based on risks: * Unexpected application domain unloads (background tasks should be prepared for unexpected termination). * Multi instance environments (threads should use distributed locks if necessary). * Unhandled exceptions may kill the entire app (each thread should catch all exceptions and contain compensation logic). * Custom threads may fight with request processing threads for CPU. * Background ThreadPool threads may lower the number of request processing threads. If I prepared for all of these risks, I _can_ use custom threads. There are a lot of opinions in the Internet about custom thread usage in ASP.NET, but very few of them contains _why_ I should not do this. Yes, this is dangerous, but it is possible to solve all of those risks.
How about "because there are much better ways to achieve what your attempting to do". I'm yet to see a valid use of threads in a web request that could not be handled better.
note: msdn azure credit is for development purposes only and is not meant to host any production site.
By better ways I mean some sort of message queue. They are much more robust and fault tolerant than background threads. Every time I've come across background threads in a web app a message queue was a much better solution. Things like sending email, credit card checks etc. Any examples of when a background thread is a better idea?
Bit coin miner?
Message queue is a communication channel between producer(s) and consumer(s). Background thread is an actor that _may_ use message queue to communicate with other parts of application. So, you can not compare these different concepts with each other. However, if you are processing background tasks inside ASP.NET application's background threads (and not in Windows Service or external console application, where you can own the domain), you should use persistent message queues to eliminate the first risk in [my comment](http://www.reddit.com/r/dotnet/comments/28lbk8/distributed_task_queue_for_net/cieovrj) written above. I'm not saying that it is ok to create custom threads while processing a request (it is awful), or you should use thread pool threads for background tasks (because you can loose them), I'm only trying to say that background threads _can_ be used, if the solution is built with care and considers all the features of ASP.NET and its hosting environment. The statement "You should not use background threads in ASP.NET application" is generally true, unless you know what you are doing, or using a solution that is intended exactly for this task, as Hangfire.
note to all those thinking about vms or other methods of persistent processor/memory usage. The base MSDN pro benefits do not really have enough to run a vm full time. Stick with per-request processing
getting a few days of worldcommunitygrid.org results
Some time ago I read this online book – http://www.introtorx.com. Nice language, interesting examples and good understanding of a topic.
this is an old post, but I thought i'd leave this here for you: http://visualstudiogallery.msdn.microsoft.com/59ca71b3-a4a3-46ca-8fe1-0e90e3f79329/
Does this implement ILookup&lt;,&gt;? I'm on mobile traveling out of country, so I can't check.
Are you aware that this is almost two years old?
Serious question. Why would I use this over Azure's new [API Management](https://azure.microsoft.com/en-us/services/api-management/) suite? It seems to do throttling and a whole lot more.
I second this. This is the site I used to get into Rx and it was very straight forward.
Hey, is there a way to sign up without entering in my credit card info if I only want to use my credits to pay for the VM?
 The Azure APIM is a great tool that is deeply integrated with all components within the Azure, including provisioning, scale-out, analytics, etc. You’re absolutely right. So, if you use it, then I recommend you stick to it. The Throttling Suite is well suited for standalone apps regardless cloud-based or not, but not even intended to compete with Azure. Please keep in mind - not everyone is on Azure. AFAIK, there is a small difference – the Azure APIM is aimed toward high-level throttling (calls per min/hour/etc.). In fact the smallest interval you can use is 1 sec. Throttling Suite allows to set and handles less than a second interval – 200calls/500ms – would be perfectly acceptable. "What's the gain?", one may ask. The answer is simple - smaller extreme spikes; think of it that API would react earlier on the flood of requests. 
But why writing another one if I wrote one already years ago which works fine, is OSS and available today?
The dates on all the sessions look wrong. I'm seeing 2012 dates for some of the Azure sessions
R# is for C# and VB. C++ support is coming. I would checkout PyCharm if you want something like R# for python.
I'm on the other side of the country, but thought it was worth saying: If you aren't taking advantage of local code camps (they are all over the country), and / or monthly INETA meetings, you are missing a great (and usually free) resource. I've attended both Tampa and Charlotte code camps over the years and always learn a ton.
Can't wait, any idea when the schedule will be posted?
Great CQRS, oh wait, it's the patterns and practices team... Go write some real software then you opinions on "best practices" might be worth something. 
You need a delegator and an executor. The controllrs will have an instance of the delegator. The delegator will be just smart enough to find the right executor and hand the command/query object over to it. The repository and service layer can then disappear. 
All pretty decent tips, except for the last one, involving server-side detection sending different content based on the browser's user agent. You really should stay away from this pattern, and instead rely on something like Foundation or Bootstrap to help you design "mobile-first", and responsively react to different display sizes, etc.
Yeah, that's been reported. We're looking into fixing that ASAP. The dates are Saturday June 28th and Sunday June 29th. The times on the sessions page are correct though.
Great! We're working on it at the moment, it should be ready by Wednesday at the latest. If you click on each session on the sessions page you can see a proposed time (though the date is wrong).
I'm not sure I fully understand your question (You want users to login but not register?) but, if you want to allow anonymous access to a controller just don't add the [Authorized] attribute, then any users can access that controller. 
yes, the system would consider an anonymous user as a registered user(store preferences/tool actions etc) but eventually when the user decides to register or login using Facebook/Google/Twitter I want that previous anonymous profile to be associated to the newly created account. 
I know your fond of the PnP team, what I'd like to know is why? The only reason you've given so far is that you like there IOC tool, despite being unfamiliar with the alternatives.
Uhg. Removing the unused references is asking for trouble. Certain things like some form designers are going to assume the base references like System.Drawing are there. It will be an un-fun experience the day you are debugging why visual studio is failing to open the form designer. I wouldn't get too carried away with moving and renaming stuff either, otherwise you break all the code-by-convention crap, e.g. AutoEventWireUp=true, so you must have a 'Page_Load' method, by convention. I absolutely hate code-by-convention, but I wouldn't be so quick to break it just because I'm unhappy with the naming scheme. When you say you sometimes rename Models to ViewModels, are you changing the design too? Models and ViewModels are different entities that serve different purposes. This isn't a bad thing to do, but wanted to point out it's more than just names. Those are different concepts.
Yes, they dropped the credit card requirement when you sign up now. Once you tie it to your msdn account it will ask what you want to do when you hit your msdn credit limit. You can choose the option that says do not allow me to go over limit which means you exclusively want to use your msdn credits or you can choose bill me where I imagine you would enter your credit card info. I choose the first one so it did not prompt me for any credit card information. 
I didn't understand why the mvc team built it. Honestly mvc 4 was a total mess. This + recipes ( a feature that zero people used). Glad they are rebooting and going hard in the other direction with vNext. 
+1 for messy features in MVC 4. But ASP.NET team also released Web API with MVC 4. Web API helped a lot to build OWIN. And OWIN helped to build ASP.NET vNext. So, MVC 4 was great, from the infrastructural point of view, but we didn't know about this fact :)
You are arguing for no reason. Did you notice they used many CQRS experts for their test application? I guess that isn't worth anything for the content.
Not a single mention of "ubiquitous language". The term "language" hardly used. This is not a good introduction to DDD.
&gt; Uhg. Removing the unused references is asking for trouble. Certain things like some form designers are going to assume the base references like System.Drawing are there. It will be an un-fun experience the day you are debugging why visual studio is failing to open the form designer. Absolutely. And ReSharper is not clever enough to truly figure out what is used and what not. Many things are also just resolved at run-time, this often happens with libraries and frameworks. What I sometimes do is whenever I start a new project I remove ALL references. Whatever gets added then is supposed to be used, no matter what tools like ReSharper tell me.
Just me personally-- I hate that the MVC "default/template" folder for Viewmodels is named "Models". Just a bad decision. I consider a "Model" to be something more back-endy, attached to a database object.
This folder is, IMHO, intended to correspond the first letter in MVC abbr for simple domain models. Models in "Models", views in "Views" and controllers in "Controllers" folder. When I studied ASP.NET MVC, it was very hard for me to understand what the "Model" mean in ASP.NET MVC application. There are controllers and views, they are easy to understand. But what is the model? Many tutorial showed me that "Models" is designed for EntityFramework (or other ORM) classes. But when the application grows, more layers appear between UI and database layer, blowing up the whole model. This is the time when view models come in place to keep views simple.
In what way?
I think it serves a purpose when dealing with older browsers. For some apps I am working on now, there is a view and path for modern browsers that utilize css3 and javascript frameworks. Then there are some older browsers where we return a different view with different functionality using the display modes feature.
Providing a separate interface is just what display mode speaks to. It keeps the separate interface fork down at the level of "different view for this page" versus "different model, view and controller for this page". The other thing it can speak to is if you need to support legacy mobile devices -- you can serve a modern responsive page to the modern devices and then serve a downscaled page to the legacy devices. This same tactic works for downlevel browsers.
[DevExpress Reporting](https://www.devexpress.com/Products/NET/Reporting/) comes to mind. Probably SSRS too. System.Drawing has basic, useful types for drawing in any context, not just a WinForm, types like Point, Color, etc. types that even if not used, are used as translation to designer UI calls.
Tip #10 - "You can’t create new elements and you can add any new logic for new use-cases." You can’t create new elements and you *can't* add any new logic for new use-cases.
Could I ask why you don't want to use an ESB?
&gt; Uhg. Removing the unused references is asking for trouble. Certain things like some form designers are going to assume the base references like System.Drawing are there. Why would you want to open the form designer in an MVC app? Asp.net MVC is the path away from that crap and that's a blessing. I like the idea of starting minimal and then adding things as you need them. It's a good way to learn how stuff works. Speaking of which, it could also be a good idea to learn how the conventions work. Which ones are necessary, which ones are not. What do they mean. If you feel like you can't touch anything because you might break something you don't understand... well, that sounds like an awful position to be in.
Just implementation time / phased approach. If it's really a must-have, I'm game, but I don't want to use that as a short cut to learning the appropriate technical solution (i.e. what's happening beneath the hood). 
It's not about understanding, it's about using it as-is, with as little fussing with as possible. In two years or less, it's all going to change, you'll be using the next New Hotness, and all of your 'cleanup' is going to bite you. My code is minimal. My classes are easy to understand. But when it interfaces with the boilerplate behind the tools, I leave the tools to their own devices. 
Rabbit MQ has several libraries .net libraries that cover everything from simple message queues to full blown ESBs. It also has the raw client library that would allow you to roll your own, but I recommend against it. It sounds like your confusing SOA and web services though, the two are completely unrelated. Web services are for public APIs, they won't give you much in terms of scalability. 
ESBs handle a lot of the traditional problems you have with SOA, such as reliable transmission, messages translation, routing, security, orchestration, etc. If you don't need those things (because you have services that all speak the same language, few in number, message are simple, etc.) and you just care about is being able to queue up messages so they don't get lost then a queue system (such as MSMQ, Rabbit MQ or Azure Queues) could work. Either way, I don't think you want to be writing your own ESB, there are a lot of very mature products out there. 
So its popular, how is it better? For the record I've used unity and I really don't mind it, I just found it lacking in many advanced features compared to the alternatives. And it has no reason to exist, there were/are better alternatives when it was created. The only reason it's around is because of MS old hostility towards open source. 
On a related note, can anyone recommend any other articles on good/best practices for asp.net specifically mvc? I'm in the weird position of being hired by a company fresh out of college to do some solo dev work and am trying to research best practices without any coworkers who are coders.
The C5 Generic Collection Library for C# and CLI also implemented multi dictionaries. I don't think it's about duplicating effort, but rather integrating useful collection types into the BCL so that we avoid yet another assembly when we want to extend the collections available for the average C# programmer. http://www.itu.dk/research/c5/latest/ITU-TR-2006-76-2up.pdf Section 11.12 discusses multidictionaries. This package is available on nuget. 
That's all fine but the additional libraries aren't included in the BCL, so adding a reference to another assembly is mandatory. As there are already more than one of these around, why creating yet another one? Why not instead invest time/effort in libraries which aren't there yet, or poorly implemented? (and why does someone downmod my post?)
Yep, this makes sense. Display modes as a cheap way to present mobile support for those who don't want to spend their time on building a separate web application for mobiles in a non-responsive past.
Thanks, all my projects does not provide support for old legacy mobile devices, so I missed this use case. It makes sense, however it does not tell us why it was introduced as a part of a web framework – IMHO this feature is being used rarely.
Personally I would probably use RabbitMQ, but if you're dead set against it you might consider something like [ZeroMQ](http://zeromq.org/) which is more of a wrapper over tcp/ip and doesn't really have a "bus" to speak of. It also doesn't have many of the benefits for a full ESB.
I'm complaining that such users are using Reddit plainly as an advertisement platform.
It can be but often I find that I only add one extra line of code to my public methods. For example, this constructor can have a contract block added simply by adding Contract.EndContractBlock(); after the checks. I'm going to check the values anyway, why not get some help from the tools? public Range(double min, double max) { if (!(min &lt;= max)) throw new ArgumentException(); Min = min; Max = max; } vs. public Range(double min, double max) { if (!(min &lt;= max)) throw new ArgumentException(); Contract.EndContractBlock(); Min = min; Max = max; } 
Very tasty, thanks! It is very hard to find information related to setting up code contracts for a NuGet-packaged library. Some time ago I wanted to plug in code contracts for my open source project, but faced with a huge amount of small difficulties, and that is why I refused contract usage. Your article will help me to reconsider my decision.
The Right Way(tm) may change soon, so keep that in mind. I have a library on NuGet using contracts, this may help too: https://github.com/aarondandy/vertesaur/tree/master/build
I'm pretty sure it has to do with cyclic nature of the reference. I have a reference to you, and you have a reference to my captured method. Of course, I could be wrong at the GC could eventually get it right, but I don't think that's the case.
Is guarding against invalid program state really code bloat though?
Thank you for the library example! &gt; The Right Way(tm) may change soon Do you have any information about the changes?
If you subscribe to an event, the class that raises the event has a reference to the subscriber. It needs this to be able to call the event delegate on it. Now, if you would dispose the subscriber (and unreference it), but the event raiser stays alive with its reference (the event subscription), the garbage collector would never collect the subscriber, because the raiser still has a reference to it. This object would even still receive the events whenever they are raised, and thus take up processing cycles. So both in terms of memory and in terms of performance, an improperly disposed subscriber is a problem you might want to avoid. The goal of IDisposable on resource-heavy classes is exactly this. Make sure all resources that are in use are unreferenced. That way the GC will find them and clean them up. I hope that made a bit of sense to you, if not, let me know what part you're not sure about.
dotnetConf is a free, online conference for helping developers create desktop, mobile, web, and cloud-based applications using the .NET Framework. Registration is not required to attend. For live streaming (June 25th and 26th), please check out http://www.dotnetconf.net/ For recorded videos (after they are completed), check out: http://channel9.msdn.com/Events/dotnetConf/2014 
Makes sense, thanks for the explanation.
The concept of adaptive rendering has been in ASP.NET since 1.0 -- it actually was a big selling point back in the days when you had to ask "how is this going to work in netscape 4" and hoped that people were using IE. Also keep in mind that frameworks tend to have pretty broad featuresets to support lots of scenarios. Adaptive rendering is very handy when you need it and certainly something to look at on a framework level. Especially in 2011-12 when they were building this stuff.
The CLR garbage collector has no issue with circular references, as it doesn't use reference counting or similar techniques. This doesn't matter to events anyway because the event subscriber doesn't hold a reference to the event raiser.
I can't find it but it sounded like there would be changes in packages so that instead of adding contract assemblies to a package you may instead have a deployment package with the release assemblies and a dev package with the contract assemblies and a dependency on the deployment package. Supposedly this makes it easier for deployments and minimizes what ends up on the server as you probably do not need or want contract assemblies there. The next thing to consider is the changes due to K and vNext. I have just been getting into .NET ecosystem again after Minecraft so I need to get into that more and see how that will change things for distributing contracts.
Then why the compiler doesn't complain that a class doesn't implement IDisposable if it subscribes to some events?
Really hard to help with out much more detail. I've been in a tight spot with Telerik controls at work as well, so I at least know how you feel about them. Apparently, dynamically creating Telerik controls on server side breaks their client end scripts when the rendered html gets slapped onto the page. I was tasked with moving a RadCalendar control off the homepage and it wound up breaking everything on the calendar and no one knew how to restore those client scripts.
Because it isn't always an issue. It only becomes an issues when the event source out lives the subscriber.
the object is a pdf viewer which i don't know where/how it's generated. Thus the only way i can think of inserting the wmode is by prepending it via jquery...but that didn't work. :/ I'll give the iframe thing a go. Thanks.
did you actually make sure that wmode is indeed the problem? it should be easy to see with firebug or whatever else you use, by inspecting the object. I would try other ways before hiding the iframe, it creates a nasty effect which is not user friendly and should be last resort. I'd search for wmode or params in the code that generates that pdf viewer and see if I could inject it somewhere during creation (modifying wmode on the fly doesn't work)
Nice explanation. Are these changes ( e.g. project.json, nuget, k runtime etc.) Applicable only to asp.net or to .net in general ( e.g. windows apps)?
Nice write up, I've enjoyed code contracts since they were initially released. The static checking they provide is really damn'd useful! As far as the code bloat this other gut is taking about... These are checks you should be doing already, just a different, cleaner, syntax IMO.
No this is basically the only one. It's actually pretty obvious once you know what's going on behind the scenes, however the syntactic sugar (simply adding the event handler method to the event and that's it) makes it a bit hard to see that under the covers there's a hard reference. It would have been better if .NET would have used WeakReferences instead, but alas... they didn't go for that. 
only if you implement your own observer pattern. The event system is an implementation of the observer pattern but with hard references, you don't add the references yourself in the .NET implementation of the pattern. 
Yep that's the problem, modifying wmode on the fly doesn't work. It has to be static from a source code. Problem is, the object element is not specified anywhere ...it's generated on the fly. (all you see is the opening and closing iframe tag and then in the code behind the src of the iframe is set by combining the report handling url + the date values into query strings) ..bummer..
You just have to think about how the GC works in .Net. It starts from the stack and it starts 'walking' the object references to the heap. Every object it can reach from the stack through the reference graph is flagged as 'still in use'. The rest of the objects are deleted. (In practice it works the other way around, but logically this it how it works). So ... the most common scenario is indeed a publisher/subscriber pattern where a publisher still has references to subscribers that are no longer in use. You have to think about events, ObservableCollections, INotifyPropertyChanged, etc. In my experience this is usually where memory leaks happen. But theoretically, any scenario where references to 'dead' objects remain in 'active' objects is a problem.
I had this problem with other object tags as well. IIRC it has to do with hardware acceleration of the object plug-in. As another user has mentioned certain object tag implementations will obey modes that can stop this behavior. The only one I remember using successfully was Silverlight. However, your performance will hurt. I also used the horrible work around that was mentioned. Before showing the drop down, hide the object/container, and then show it again after it closes/disappears. It sucks, but I never found anything that would draw HTML elements over hardware accelerated objects. FYI in my case, it was caused by embedding a .Net WinForms client into the web page. 
Here is another way: http://msdn.microsoft.com/en-us/library/aa970850(v=vs.110).aspx
What a shitty website. I don't want to watch a 50 minute video to find out what the hell the big dumb thing does or why I want it. *The first sentence* should say that; instead you wade through three paragraphs of how excited they are.. 
For my special friend: http://o.oz-code.com/features
Is the report a PDF?
I wouldn't call it painless, but you could use PowerShell to load all your project files' XML, manipulate, and then re-save. Or a C# console app. Just make sure your project files are backed up or are in source control before you start, because it is definitely the kind of thing that takes a few runs to get right.
You could potentially take advantage of the [MsBuild Import feature in NuGet](http://docs.nuget.org/docs/creating-packages/creating-and-publishing-a-package#Import_MSBuild_targets_and_props_files_into_project_(Requires_NuGet_2.5_or_above\) ). Create a package with your target and then do ``Get-Project -All | Install-Package MyMsBuildPackage ``
This is similar to what I've done. I used a third party library to reference csproj files and update their output paths in a custom WF activity. 
This actually seems to answer OP more than what I thought he was asking. 
The linked page is an announcement of the release, not the start page of the site. You could have clicked the logo to get to the start page, where *the first sentence* describes exactly what the thing does. Or you could have clicked the "Features" menu item, where they go into details. Instead of whining here.
&gt;Should I use a VPS for this, or something like the Amazon EC2 hosting? Same thing. EC2 is just a marketing thing. &gt;Each user will usually have their own database that they connect to (with the exact same schema however), and then the application helps perform simulations and querying the database etc. That's a little confusing... where were you planning on hosting all these dbs? Yes they can be on the same server, but depending on how many dbs you're talking about it sounds like you need a separate db server. I should warn you, this is a design that will NOT scale well. If you don't think it'll ever go above 100 users, then you'll be fine, otherwise you'll be in trouble. A better design would be to store each user's data in the same db. (i.e. add a user key to the necessary tables.) That being said, I don't recommend AWS. In your case I would make two recommendations: * Rackspace Cloud Servers * Microsoft Azure Both are WAY easier to use than AWS and have great support for IIS/ASP &amp; SQL Server. In your specific case I would recommend Rackspace. Why? &gt; I'll have to run this thing myself but I have no real experience with hosting a website, security etc. beyond getting a local instance of IIS up and running. Good luck getting help (other than website) support from Microsoft. Rackspace it's easy to get someone on the phone, and if you have very specific questions, it's not hard have them get an engineer on the phone to answer specific questions. Yes, Rackspace is a little more expensive, but for a small MS server, it's not that much. http://www.rackspace.com/cloud/public-pricing/#cloud-servers Disclaimer: Yes, I work for Rackspace. No I don't always recommend them. Look at my previous posts: I often recommend Dreamhost but they don't do much in the MS space. I play with Azure all the time too. Really easy to use but I'm not sure of the pricing. Also, if you don't get much response here, try again over in /r/webhosting
There are no such links on the mobile website - only if you're careful enough to notice the grip button do you then find a 'features' page. At that, if this is the first major release of your product, you probably should include some information about what the product does anyway. Nearly nobody knows what it is, and that page may be the first page they see of it. In my case, it was the first and last, and now they lost a potential customer. I'm not sorry, it's a shitty design.
There's Azure you could use, they have free packages IIRC. And as you said, there's EC2. There's GearHost (a colleague uses them); not free, but ~$10/mo. If you're able to compile the web app against mono and have it run as you expect, you can go with a Linux VPS (or web) host. If setting up the entire server (or vps) doesn't matter to you (I actually like setup myself) you can request a custom OS install through the provider. I like [SmartServ](http://www.smart-serv.net) myself. (They are really helpful and responsive with their tickets; they also are capable of setting up a mono host, but don't currently have the demand.) --- Overall, ~~Azure probably is your best option as it's free and it's the easiest to manage that I hear.~~ GearHost would be your best bet for ready-made ASP.NET hosting. If you don't mind setting it up yourself, EC2 or SmartServ would be just as good. edit: correction because of /u/AngularBeginner's comment.
Since you mentioned Sinatra, you should look into [NancyFx](http://nancyfx.org/) as it is inspired by Sinatra. With Nancy, you can do everything you've mentioned very easily and without much work.
I've created and deployed a web service / api (rest/soap) using vb.net and Visual Studio; i'm novice to intermediate in skill, and I found it pretty easy. VS creates all the stuff for you, so you are pretty much writing the "web" methods within 5 minutes of creating the project. I haven't used any other frameworks or languages, so I can't offer a comparison. I don't even know what "SPA" is, so I think I can only help you on your last bullet there... 
I think I am not going to be able to explain it better than this article: http://www.codeproject.com/Articles/29922/Weak-Events-in-C On the side note, even though I know about the weak events I am only using them in WPF/Silverlight scenarios when binding commands to the controls. Otherwise I make sure that I either have reliable Dispose method or staying away from the events all together.
There's a free Visual Studio extension which does pretty much the same thing - [http://visualstudiogallery.msdn.microsoft.com/6a667daf-be94-4be5-a92a-71732b8dd60a](http://visualstudiogallery.msdn.microsoft.com/6a667daf-be94-4be5-a92a-71732b8dd60a)
That's good to know. Thanks! Let me know if you ever have any feedback or problems from a Rackspace perspective. I mainly work on hosted email, but let me know if you're not getting the help you need on your VPS.
I think going with WebApi is a fine choice for what you are trying to achieve. I currently have it implemented in our own (mostly) SPA app, and it has worked fine for us both during development as well as in production. Now for your specific questions: - Hardly any. Setting up a fresh Web Api project pretty much does 90% of the work for you, as the defaults are quite usable. - Filtering routes is dead simple. It follows the RESTful guidelines so you have your standard Get/Post/Put/Delete, which are done for you. - This is kind of a grey area for me, and it's tough to find any open-source projects that explore options for this. What we ended up doing is simply decorating our api controllers with [Authorize] attributes (or the equivalent), which has seemed to work fine for us. - I'd say so if Microsoft is already suggesting folks go with this route. - Haven't done this myself, but from what I read around, Mono is getting a lot better, but still has a long ways to go. Your least painful solution would be to stick with a Windows deployment. - Development experience is fine. As I stated before, most of the work is done for you when you set it up. Hope this helps!
I did a SPA/Web API app last year. It was some of the most fun I'd had in a long time. To answer your questions: * Very little * Easy * Most anything that you have in .net * Yes * Not sure, likely not * Visual Studio is a bit of a resource hog but holy shit it's the best IDE available by lightyears. 
&gt;Haven't done this myself, but from what I read around, Mono is getting a lot better, but still has a long ways to go. Your least painful solution would be to stick with a Windows deployment. It's one of the main reasons I'm pretty hype for vNext. Deploying stuff like this to a Linux host without having to jump through hoops will be a godsend.
Couldn't agree more! Consider me equally stoked.
http://www.johnpapa.net/spa/ Check that link out for a great resource to get you started. This stuff is golden.
I have a SPA Offline capable webapp and we use WebAPI + OData. Together with KendoUI / JayData / Breeze etc. it's a piece of cake to implement.
I would use a targets file to centralize the logic. You can reference the project variables to make it generic. http://msdn.microsoft.com/en-us/library/ms171464.aspx Powershell was mentioned as a option to apply the target, which would work great as it has great XML support.
Create new webapi project Right click, add service reference Put in uri for soap service Find operation you want to start with, create controller with same or similar name. Map controller properties to service request. Map service response to controller response Use webapi goodness for json.
Last time I tried running Web API on Linux, it didn't run very well. Maybe support has improved now though. I was trying the ASP.NET hosted version, you may have better luck using OWIN instead. ASP.NET MVC, Nancy and ServiceStack all work on Mono pretty well. 
you can use azure websites... for free (including 20MB SQL Server DB)
Could you provide a link to that blog post about viewmodel builders please? Sounds very interesting.
If I'm understanding your question properly then use User.Identity in the Controller class like this for example: var currentUser = _userManager.FindByName(User.Identity.Name); in this case you would initialize user manager in the constructor like this : _db = new MyDbContext(); _userManager = new UserManager&lt;MyUser&gt;((new UserStore&lt;MyUser&gt;(_db))); This is using MVC5 in previous version you could find out using a static method. I think it was **System.Web.Security.Membership.GetUser()** if I remember correctly. But next time try Stackoverflow instead please. 
This was a nice summary of about 50 pages I just read in a book I have on this. Thank you!
Sorry about that, somewhat stressful situation handled poorly. Not being good at things isn't one of the scenarios I prepare for. And thanks for the example; worked out fine.
As others have mentioned, I would stick with Nancy or ServiceStack if you're looking to run in mono (self-hosted or OWIN and not asp.net). I hear MVC5's version of WebApi is better in mono, but I can't personally vouch for it.
Adding to the Linux thing, I've had no luck getting the latest stable versions of ASP.NET MVC5 or WebAPI2 working on Mono. vNext may change this. In regards to the boilerplate, relative to Sinatra or Flask, Web API definitely requires more initially, but a substantial amount. 
Oh! Thanks.. I have made on of those before, and didn't even know what it was called!.
Why jaydata and breeze?
Your DAL from JS to the server.
With regards to Mono, there has been a significant amount of effort that's been put forth by the F# community (and MS and Xamarin) in the past year to improve the F# story on Linux and OS X. Compared to where things were just a year ago, there has been a massive improvement across the board. In fact, this very question came up on the official F# mailing list not too long ago: https://groups.google.com/forum/#!topic/fsharp-opensource/AX-Mwwe0fGI. The only other thing I want to mention is that in addition to ASP.NET WEB API, you can use Nancy, ServiceStack, Simple.Web and even things like Frank, Suave for your back-end. 
Exactly this article was posted 3 days ago already.
Don't quite agree with some of those points..
I guess the URL was different.
Find a web app that you use that you wish had more features. 
Write a web app for collecting ideas for web apps.
Participate in an open source project that uses ASP.NET MVC, or port an open source project to MVC. 
Yes, you can cause a similar problem when using a Delegate to an instance method. This can be particularly tricky when using long-lived anonymous functions that capture the `this` pointer.
I'm reinventing computing (mostly the way we interact with computers). That's not a small project, though. But that's one of the few that matter. Think big. Aim high. Everything else is a waste of time.
Making a game in MVC is always fun, you have touch alot of subjects, think about business/data strategy and you tinker with new technology (HTML5, offline perhaps, web api, SignalR, etc).
View models also are a security strategy. If you use models directly on the page, a malicious user could try to add inputs to the page for model properties that were not originally included on the form, allowing them to set fields in the database meant for internal use only. This is why the bind annotation is used; view models present an alternate method of defense since any fields injected by a user will be discarded instead of accidentally being applied by the model binder. Did that make sense?
Write code, fuck shit up, fix shit, repeat. Lot's of people have opinions on best practices which are not necessarily valid, over time you will learn what works and what is crap as you mature as a developer.
The author considered System.Data unnecessary, which is absurd. Removing references can lead to weird dependency issues later on, when you're far enough into the project that you don't remember removing all those references. I doubt there are any substantial performance gains by doing this, and even if there are, optimization should be put aside until it's needed to focus on actual development.
I believe the default folder is meant for models in the sense of objects storing the state of the program, not necessarily view models. EDIT: I personally like to create a new project in my solution for my DAL, and if I am going to be using ViewModels heavily, I will also create a new project for them to help compartmentalize my code.
Why bother listing the minimum references for a dummy application? You can't do anything useful with that setup/. And maybe I'm just grumpy, but #4 seems so similar to #3 it isn't worth mentioning. I think it's worth mentioning how minification can break DI in some JS libraries, notably Angular, if injections are not annotated. Even if the web browser sends the content encoding, that doesn't help much if the user downloads an HTML file. I don't really get his point on Bootstrap, his logic seems really convoluted. As a developer, it kicks ass for styling pages since I lack the design skills to create my own layouts. I have never felt it adds a "layer of abstraction" to my hTML.I end up adding more &lt;div&gt; tags than I planned, but I'd like to see him give an example of a good layout that doesn't end up using more tags. Bootstrap is also available by CDN if bandwidth is such an issue. And why read user agent strings? I advocate using Modernizr to detect specific feature capabilities instead of guessing user strings. Overall feels like author doesn't really have a point...
Yep, this was the cause of a nasty memory leak I solved a few years back. There were thousands of garbage objects that weren't getting collected, because a long-lived object still held a reference to them via an event subscription.
Finish this: github.com/robertbaker/playitforward MVC Service Stack, Bootstrap. I had plans to implement AngularDart.
You I like.
What are you passionate about outside of computers/programming? What hobbies or interests do you have? I personally think it's going to be hard to work on an unpaid project if you don't have any passion about the subject. Another idea would be to contact family and friends who own a business and see if you can help them with a business process, project or system.
application for energy management 
Football Manager.
We've just started working with service bus for windows server. Essentially the same, but on prem. 
50 projects sounds excessive, how many have less than 10 code files? Wrapping this together will do wonders for your build time. How many of these projects are actual applications and how many are libraries? You really only need to upload the applications. 
Lib/framework for generating mock/random data.
As (I think) was explained in the post itself, there is no clear answer to that. Using project.json for creating general purpose class libraries works just fine, except if you add K-only features like Assembly Neutral stuff. project.json is basically just a replacement for .csproj, that has better handling of references, and allows targeting multiple platforms. Windows apps on the other hand are ... harder. It can be made to work (I've hacked a working solution), but it's not pretty. Hopefully it will be though. If it not, I'm planning to add features to Runt and probably a general NuGet package that simplifies this, but we'll see when K get's a bit more mature. Windows Store Apps I have no idea about, given that I don't even know if they have access to a proper file system. But if they do, KRuntime should be Windows Store usable, given that it's built against Core CLR which as far as I know should support mostly the same things as the Windows Store Apps does. Hope this explains your questions. If not, please swing me an email and I will try to answer better :). Also, remember to go vote on having project.json for all project types (link in the original post).
Change @ChangeType(@Html.DisplayFor(model =&gt; model.ChangeType)) to @ChangeType(@Html.DisplayFor(model =&gt; model.ChangeType).ToHtmlString()) 
This worked. Thanks!
This is a good quick solution. Keep in mind if you want to do this stuff more regularly or tie things like strings to strict types then you want to create your own converter class and use that instead.
We use a build server to publish so there is one line of code being published. I find it better this way so that if dev 1 publishes to production then dev 2 publishes without getting latest, he won't overwrite dev 1. You have to check your code in then publish
How big is your dev team? Can you create a nightly release every night? Did you ever come to the situation where your boss/colleague says: "Hey can you deploy this?" And you had to say no because you are miles away from having building code? Then it's good to think about a build server! We have dedicated testers and we don't want them to idle, so after every Mercurial push our TeamCity CI server builds the latest build, ups the version number and sends it to Octo Deploy, which then deploys it to the test servers. In a matter of minutes the testers can continue with a new build. I find deployment within VS handy for small hobby projects but I have had it once too many times somebody accidentally deployed to the wrong publish profile, overwriting a site completely. With a deployment server you can revert a build in a few clicks.
That's something I didn't think of. I will keep that in mind as it's a very good point.
One of my employees left and went to a new company that published from the desktop and said the first few weeks he was there it happened a bunch.
The build becomes the heart beat of your project. It compiles the code, runs the test and deploys the product to various environments. Don't make the mistake of thinking the build only happens on the build server. There are a lot of daily tasks that can go in there as well. Restoring/migrating databases, generating WCF proxies, documentation and code analysis are all part of a build. I'd recommend against TFS though, it's a collection of "worst in class" tools.
You can ensure that all tests pass before putting the code in front of customers. You have a build history, so if you need to roll back to a previous version, you know where to find it. If/when you start spinning up multiple sites/services as part of your deployment, you'll have an automated deployment, rather than relying on devs to correctly/consistently follow a manual workflow. As others mentioned, you don't have to worry about un-synced changes being overwritten.
Do you like knowing what's in production? Use a build server. Do you like knowing that what's in production has been tested? Use a build server. Do you want to consistently deploy the same product without error? Do you want to deploy the same product to multiple servers? Do you want to package a product for others to use? Use a build server. **Use a build server**. It is worth the time and effort.
Thanks for sharing! It seems like an excellent series and I've bookmarked it to check it out.
I'd recommend starting with MVC. Everything you need to get started can be found at [Asp.net](http://www.asp.net/mvc). You can get familiar with C# as you work on it. The lessons at Asp.net will walk you through using Entity Framework for SQL access. If you need any help beyond that, feel free to ask me. I'm not sure how else to direct you at this point.
Thanks as well!
I would say use the hell out of that 3 months of Pluralsight, one of the best resources you'll find.
Thanks for the advice and the offer for future help, appreciate it. 
Will do, thanks. 
The decision for TFS was already made and in place before I got here so it's something I can't change. Though I wish they would of went with something else.
This, we have a test and production environment and my boss refuses to lock down the testing and production branches (let devs merge and branch only) because they don't want a gatekeeper. Maybe if you had devs who understood how important workflow is it wouldn't be so bad, but since they were modifying stuff directly on production before I started this isn't the case. 
Why? PluralSight is always mentioned but I never see a comparison between what you can find there on ASP.NET MVC and what you can find on www.asp.net. Why is it a far better resource?
I have only used the TFS build service, but it has been sufficient for our needs (6 - 12 developers, many systems of various sizes). The TFS build service is integrated with Team Explorer in Visual Studio, and its fairly easy to setup builds, trigger unit tests, calculate code coverage and integrate deploy. The TFS build integration has builtin support for CI (build checkins), gated checkins (build before commit) and scheduled builds. If you need to customize the workflow things can get a bit more hairy, as it is uses Windows Workflow Foundation and the editor is cumbersome (IMHO) &amp; slow. As you already have TFS, I would suggest you start with TFS build and move to an alternative if it doesn't cover your needs.
Do you know if you need a build machine per collection? My system admin wasn't sure about this due to the wording in the info microsoft had on it.
Each Team Project Collection needs a separate Build Controller, but you can configure multiple Build Controllers on the same Build Machine.
Link please?
Thanks! I'll probably push for this. Hopefully it will help our work process.
Have a look at the type of object being passed as a parameter in the "return View(someObject);" line in the Controller's action method, OR have a look at the first line of the associated View.
Thanks - I posted this question and then promptly went back to putting out other fires at work. I will need to do some learning here, and teach my new assistant how to do it, too, going forward. I was able to figure out how to post a new article, so that was good. But yeah, going forward I may indeed PM you with some questions. Appreciate the advice! 
The TFS build service is based around what I would consider several bad practices, such as not having the build script under the same source tree. Cruise control and team city are the main .net ones. TC has much simpler configurations but it comes with licensing issues. CC is completely free, but last time I used it configuration was a nightmare. Jenkins is also nice, but I've had bad luck with TFS integration in the past. Then you've got your build tool to use. Nant is a bit nicer than msbuild, but there's not much in it. If your starting from scratch I'd suggest nant, if there is existing stuff with msbuild it's easier to just keep using that.
That selection is just for the auto scaffolds. It's just a quick template to get you started, all it does is set some method and properties. Look a the constructor to see what the data context is as it is likely hard coded. The class name will usually be named after the model type (if api controller) and each method will have a return type of the model as well.
If you can't find tutorials you are not looking, because MVC has plenty. Start here: http://www.asp.net/mvc/tutorials/mvc-5/introduction/getting-started
There was a question recently just like this.
Does this mean having a single controller action can serve html and json? If so great, I've been doing it for years with filters but it would be nice to have it baked in.
&gt; I've actually noticed that the foreign key indexes that EF generates do not appear to include the other columns... Generally I don't expect a tool to handle something that requires a bit of thinking to get the balance right. Creating indexes requires a bit of thought with some knowledge of your users workflows, table structure, profiling results etc. [I wrote](http://proactivelylazy.blogspot.com.au/2013/12/orm-vs-sql-part-3-filtering.html) a post a while ago illustrating just how finicky sql server can be about it's index usage.
Have a listen from ~5.20 for about 15 seconds, "full support for content negotiation". Sounds like a yes to me :D
Two options, as I see it: - Get a book. I love the Apress book "Pro ASP.NET MVC [N]". 4 or 5 is fine for N. - Get PluralSight, and look over the MANY MVC courses until you find the one you love.
I've been building web apps in classic asp and then web forms for well over a decade now, so I think I qualify as a "shit-hot webforms programmer". :) I was in the same boat as you and the OP recently. Just swallow your pride and do the tutorials linked to by /u/yesman_85. They're not as elementary as you make them out to be (they're not explaining variables, though I realize you were probably exaggerating), and you will indeed have that "ah ha" moment. You need to realize that you are pretty much starting over in terms of understanding how just about everything works on the web server. It's a completely different paradigm. Understanding how a webforms project translates to mvc is about as instructive as understanding how a php project translates to mvc. 
You can return Views, you can return Json, you can also return negotiated content. It's all baked in one.
thanks for the advice....i suppose i should pour myself another vat of coffee and give it another go. having used webforms for a decent time, would you say the investment in time is really worth it? Have you found anything yet that you couldn't accomplish FAIRLY trivially in webforms? Or is it just learning for the sake of it and some futureproofing?
&gt; I'd recommend against TFS though, it's a collection of "worst in class" tools. What? Why would you say that? Have you used a recent version?
TFS has a good built in build server. You might want to use a separate application for deployment though, https://octopusdeploy.com/ or http://inedo.com/buildmaster/
For those that don't know, [UnityVS](http://unityvs.com/) is a plugin for VS 2012/2013 that allows you to use VS to edit scripts and debug Unity projects. Unity only supports MonoDevelop at the moment. You can edit the scripts in whatever app you want, but you can only debug with MonoDevelop. MonoDevelop is great, but it's nowhere as great as VS. So this is pretty exiciting news. **EDIT** Err... I mean they're acquiring SyntaxTree, the creators of the plugin.
Even better than anticipated, this is great news!
Well, for the most part, it was just getting to the point where I was mildly embarrassed I wasn't well versed in MVC yet. :) So, mostly learning for the sake of learning, and future proofing. My current position hasn't really had a need for it, so I was content to just keep throwing apps together in webforms. The business expresses a problem, I throw together an app in half a day that addresses the problem, and they all think I'm a magical wizard. However, we're about to start a larger project that could include quite a few more developers. And it's getting hard not to admit that MVC is the future of web development with ASP.NET. It's something that needs to be considered for recruiting purposes, if nothing else. So far, I have to agree that MVC is a much cleaner, nicer approach to web development. However, I'm still convinced that webforms is a better approach if speed of development is a primary concern. 
I think of this as normal cms functionality. Just pick a cms and then go from there. 
I'd say that because I've used alternative tools and found them to be superior to the ones in TFS. Team city is a lot better than the build server. Nant is marginally better than msbuild. The task tracking system was horrible (most are). And most importantly Git and SVN are better than the sorce control. Even MS have seen the writing on the wall with this one and added git support. I forget the last version I used, whatever the latest was in 2011/12. I generally avoid companies that use it. What other SCMs have you used?
No .cs files? What files is in the solution? Are there other files related to the dbml file? If i remember correctly the dbml file usually consists of a cs file, a designer auto generated file and the dbml file. (i may remember wrong). Have you tried disassembling the compiled site? EDIT: What errors do you get when trying to compile the site? Is it only the dbml file that throws errors?
Is the dbml file registered correctly? If the namespace is defined in the cs file, and the dbml file cannot find it, perhaps the cs file is not being compiled? Check properties for the file.
&gt; try SharePoint. Nnnnnnoooooooooooooo
&gt; try SharePoint What kind of monster are you?
https://wiki.phpbb.com/Tables https://wiki.phpbb.com/Table.phpbb_privmsgs_to https://wiki.phpbb.com/Table.phpbb_forums_track https://wiki.phpbb.com/Table.phpbb_topics_track https://github.com/reddit/reddit/search?q=unread&amp;type=Code 
My last job switched over completely to Atlassian products. Bamboo was amazing. I also love the way Stash, git and crucible worked together.
We use bamboo to push all of our internal and production builds and it works really well. It's a little wonky to get setup with publishing to Azure since you need to use Power Shell, but regular MSDeploy works well to an IIS server. While we use Stash it works with git, so any provider like Bitbucket shouldn't be any issue. 
We do the same thing. I used teamcity in the past and I liked it a lot, but the whole combo of bamboo, stash and git makes for a nice workflow
&gt; No .cs files? Do you publish your source files to the server?
Back when linq2sql was released, I did, but not in a very long time. Started doing bigger projects and split them into multiple projects so its been a while since I used a website project.
But it would be great to not have to recreate the filters on every project. 
It's probably compiled already.. so, your options are - total rewrite or decompiling and putting it back together in something that builds - http://www.jetbrains.com/decompiler/ 
Start by forgetting about WebForms :)
A vote for TeamCity here. Sounds like the free edition will fit your needs. I wanted to like Bamboo, but Teamcity is just so good.
Using TeamCity interfacing with TFS building C# / ASP.NET projects. Works very good and has a very easy to use web interface.
There's always that 1 guy, isn't there? I have all of the CS files and it builds on the server just fine, I'm just having trouble convincing VS to build properly (VS ignores App_Code if it's a WAP). I *have* all of the cs files, at best I would simply rebuild the solution, at worst I would create another solution for any extra code I wanted and drop it into the bin folder directly (I've already experimented, this approach would work). But since all I'm doing is adding a small bit of functionality for a client of a client, I'm just going to drop in the functionality directly into the code-behind, and then recommend they pay someone to fix it up and send them the solution. 
Didn't know about this program until now. One thing for sure JMeter looks way more complicated than the one above. The Web Load Tester is only made to call web services and JMeter looks like it can call about anything in the world so there's more configuration to it. One things for sure calling a WCF service with a bunch of complicated parameters looks like hell in JMeter while it's basically out of the box with the Web Load Tester
I second TeamCity+Octo.
This was not entirely clear from your posting. And yes, VS does not build the App_Code directory, because the files in there are build dynamically - it's intentional behaviour.
This article is a lot of bollocks. It is true that async/await and IEnumerable are fundamentally incompatible.. And that's why you should not try to "hack" a solution. There **is** a solution for this, and that is `IObservable`. While `IEnumerable` is an active **I take something** the opposite `IObservable` is a passive **I get something** - the async variant of IEnumerable.
If you want beat in class, Visual Studio for IDE, Atlassian Jira for issue management, JetBrains Resharper for coding joy, JetBrains TeamCity for automated build.
and I quote: &gt; All of this leads me to believe this is most likely a solution-less "web site project". I've loaded it up as such in VS 2013, only it doesn't seem to want to build some of the things in the App_Code directory. In particular, this project is using LINQ2SQL and there's a .dbml file that isn't getting built. The rest of the app references the namespace for the dbml file and won't compile as a result. So here's where my difficulty comes from. I have always avoided the "web site" projects as much as possible. The few times I've had to deal with them, it's never been a good experience, so while I do know a bit about them, I have no real practical experience working with them. You opened your response with "that's a bad assumption to make", even though you're now telling me it wasn't clear that it was a good assumption make because VS works in exactly the manner I described it working in originally. As I said before, there's always that 1 guy, so lets just end this conversation here.
I've been using TeamCity for years and have been raving about to anyone who will listen. Had a bit of a play with Bamboo, but it just didn't feel as easy to use as TeamCity
&gt; One things for sure calling a WCF service with a bunch of complicated parameters looks like hell in JMeter while it's basically out of the box with the Web Load Tester Lol, everything is hell in jmeter, it's not exactly intuitive. Ill keep this in mind next time I'm dealing with web services though.
Not everything is black and white. Observables are one way to tame concurrency - they are essentiall LINQ to events. However, the use case in the article is also a valid concern. For simplicity, let's say I want to read the lines of file which is hosted in some remote store. I want to be able to do something like: foreach (var line in store.ReadLines("somefile.txt")) await TweetString(line); Now there are two approaches - read all the lines of this file and then process them: foreach (var line in await store.ReadLinesAsync("somefile.txt")) await TweetString(line); However, the obvious drawback is that you are reading in the whole file, which could be enormous. So let's say we want to chunk it. In the user's perspective, we are going to still want to present an interface that will work with `string` instead of `string[]` even though we are batching. However, how would this work with an Enumerable? We would need a MoveNext that was async in order to pull the next batch if necessary, and Current would simply return the current item from the batch. Now, why isn't Observable a natural solution here? First, Observables have synchronous OnNext semantics. That means that either downstream observers must all do synchronous work, or downstream async operators must synchronize (using blocking or caching). Blocking is obviously bad, and caching brings us back to the problem of large files. Let's see what the last paragraph means concretely. Let's say we have an observable that reads from the remote store somehow and OnNexts strings from the file. If all observers are synchronous, then we're golden. However, what if we want the observers to forward each stirng to another server asynchronously? We can either use Task.Result to convert all observers to synch (bad), or somehow make the OnNext on observers async. Further, all the discussion in the previous paragraph is somewhat orthogonal, since the implementation of the Observable in the first place is a pull based model unless the remote machine is pushing lines. If it is, then it would need to be able to determine when the receiver has finished processing the each OnNext before sending another OnNext (back to making OnNext on observers async). So neither Enumerable or Observable are right here because both are synchronous. Yes, Observables are synchronous in the perspective of the observers and even observables must use [Synchronize](http://msdn.microsoft.com/en-us/library/hh229583) to preserve semantics. And often these models are just fine. However, when we start talking about latency and throughput, they might not scale. TL;DR The real answer is using an asynchronous enumerable. Observables are not a natural solution to the problem. If the server supports a push based model (rather than the pull-based 'HttpRequest' style server), the asynchronous Observables are the answer. In practice, if observers are all synchronous, you can wrap the enumeration in an Observable.
Thanks! Just fixed it.
You use Pex? It does wonderful things.
I've toyed with pex and moles, both pretty awesome. Unfortunately I couldn't convince the team lead to implement it in our product.
You are correct. I have been going with mindset and I think this is why it was a bit hard. I think I have to go like a virgin. :)
Is there somewhere where you can get hosting? Preferably free. It's not for something serious, just want to play around with CI and get to know it :)
According to http://msdn.microsoft.com/en-us/data/gg577609.aspx, Reactive Extensions do what you're after, without the requirement to have everything be synchronous. 
It's all about TFS Build and Visual Studio Release Management, check the videos below (you will instantly fall in love) http://channel9.msdn.com/Series/DevOps-Visual-Studio-Release-Management
Yes, Rx does have [IAsyncEnumerable](http://channel9.msdn.com/Shows/Going+Deep/Rx-Update-Async-support-IAsyncEnumerable-and-more-with-Jeff-and-Wes).
Kind of redundant by now but another +1 for team city. As a community we really should get around to making a better OSS solution one day.. 
I have had to introduce MVC to MVP developers many times, and they always tried to find an equivalence, like from VB to C# ... and that makes things very hard haha. Questions like "how do I pass data from a page to another" ... or "why my dropdowns are reseted if I press F5"... or my favorite... "wait wait... there is no built-in webgrid?" ... ahhh ... god times...
What is the problem with this approach? public async Task&lt;IEnumerable&lt;Int32&gt;&gt; ReadAsync(DbCommand cmd) { var reader = await cmd.ExecuteReaderAsync(); return Read(reader); } private IEnumerable&lt;Int32&gt; Read(DbDataReader reader) { while (reader.Read()) yield return reader.GetInt32(0); }
While this is well written it's an extremely simple implementation, so much so that I'm not sure of it's value. It doesn't illustrate how to abstract the multi tenant nature out of the one controller it shows. It's also wofully outdated. Linq2SQL hasn't (and won't be) updated since 2007!
A couple of things: The Read method isn't async, but if you wanted all the db operations to be async then you should use reader.ReadAsync(). Typically with async you want it to be all or nothing. Also, you're using a single async method to return an IEnumerable. In my scenario I have to make an async call (it may be api, it may be DB) for each record returned.
&gt;Right now I have half a dozen or so applications that do minor CRUD functions, all being hosted on windows server/IIS (some of them are hosted by different parties). **Spaghetti hosting** does not sound fun. &gt; So I would be using the one Web.API for multiple projects (guessing not great practice) reusablity is a best practice &gt; Thinking this would be ideal also for when/if an application has to go to mobile. ????
For hybrid mobile application you pretty much need an api anyways. The projects are completely different and independent. but from what you wrote seems like a good idea
&gt; or hybrid mobile application you pretty much need an api anyways. Ooh right, the api controller is a must for non mobile clients.
#####&amp;#009; ######&amp;#009; ####&amp;#009; [**Multitenancy**](https://en.wikipedia.org/wiki/Multitenancy): [](#sfw) --- &gt;__Multitenancy__ refers to a principle in [software architecture](https://en.wikipedia.org/wiki/Software_architecture) where a single [instance](https://en.wikipedia.org/wiki/Instance_(computer_science\)) of the [software](https://en.wikipedia.org/wiki/Computer_software) runs on a server, serving multiple client-organizations (tenants). Multitenancy contrasts with multi-instance architectures where separate software instances (or hardware systems) operate on behalf of different client organizations. With a multitenant architecture, a [software application](https://en.wikipedia.org/wiki/Application_software) is designed to virtually [partition](https://en.wikipedia.org/wiki/Partition_(mainframe\)) its data and configuration, and each client organization works with a customized virtual application. &gt; --- ^Interesting: [^Athena ^Framework](https://en.wikipedia.org/wiki/Athena_Framework) ^| [^Cloud ^computing](https://en.wikipedia.org/wiki/Cloud_computing) ^| [^Hitachi ^Content ^Platform](https://en.wikipedia.org/wiki/Hitachi_Content_Platform) ^| [^Managed ^private ^cloud](https://en.wikipedia.org/wiki/Managed_private_cloud) ^Parent ^commenter ^can [^toggle ^NSFW](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot NSFW toggle&amp;message=%2Btoggle-nsfw+cip1aoh) ^or[](#or) [^delete](http://www.np.reddit.com/message/compose?to=autowikibot&amp;subject=AutoWikibot Deletion&amp;message=%2Bdelete+cip1aoh)^. ^Will ^also ^delete ^on ^comment ^score ^of ^-1 ^or ^less. ^| [^(FAQs)](http://www.np.reddit.com/r/autowikibot/wiki/index) ^| [^Mods](http://www.np.reddit.com/r/autowikibot/comments/1x013o/for_moderators_switches_commands_and_css/) ^| [^Magic ^Words](http://www.np.reddit.com/r/autowikibot/comments/1ux484/ask_wikibot/)
Exactly, the Read method isn´t async, because it does not need to be. I disagree with making everything async just because the sake of it, is not true you are supposed to that. In my code only the command execution is async, because it is the operation that takes time, once the DB is ready to return a result set, it is better to read synchronously if the DB is close enough. Making everything async is an unnecessary pain in the arse. I have should added a ".ConfigureAwait(false)" though. I recommend you to benchmark different approaches before making everything async, I did, and I got surprised. Only operations taking long enough are worth for async. For example, in network programming, I do the first read asynchronously because I don´t know when I am getting a request, so there is no point in having a thread hanging there, but once I get the first read, I continue reading synchronously till the end of the message, that gave me some performance improvement.
I know it can, but I highly doubt someone will want to run it under linux and fell like the "bastard son" while the 99% of the remaining community is using it under windows server, developing in visual studio under windows and with sql server. And just do not ignore the fact that libraries incompatibilities will pop out very soon
When you need the power of 100+ web servers, Linux and PostgreSQL are a great way to cut costs (I'm an MSDN developer, so I'm biased).
Agree, but in that case why use c# and mono while there are infinite alternatives more supported in that platform?
Maturity of product reasons. Why rewrite something that works great.
You can still develop in Windows with VS and just use Linux for production. I see what you mean though, Mono's been around for a while and doesn't seem to have built much of a following on those alternate platforms. Trying to run anything on it right now is an adventure. It is however an interesting development to see that Microsoft is warming up to it, that makes me cautiously optimistic.
I have a couple of very small websites I need a dedicated Windows VPS for, I can't wait to move them to my VPS linux server so I can get rid of one server.
You have no idea how many devs currently using windows would rather not be using windows.
Great!
Xamarin have always developed Mono, the company was called Ximian when they initially released Mono. 
The current version of ASP.NET can also run on Linux. My site/blog is built with ASP.NET MVC 4 and works fine on Debian Linux. I'm using a stack consisting of ASP.NET MVC 4, AttributeRouting, ServiceStack ORMLite, Simple Injector, ELMAH, and a few other components. All of them work perfectly on Mono. The difference here is that the core ASP.NET framework currently uses Mono's implementation rather than Microsoft's. With vNext, it'll be Microsoft's implementation running on Mono's core. 
I'm using a stack consisting of ASP.NET MVC 4, AttributeRouting, ServiceStack ORMLite, Simple Injector, ELMAH, and a few other components. All of them work perfectly on Mono.
I'm pretty sure I saw a demo on vNext by MS (was it Hanselman at NDC Oslo?) where they basically created a new MVC project and copied the source files over to a Mac and ran it. I also think they mentioned that they were gonna create plugins for Sublime and Eclipse... Anyway, I don't think some Australian blogger was the first to accomplish this.
With no understanding of the benchmarked performance of vnext on mono vs native, it's not really a sensible argument to make.
Have they fixed async support in ASP.MVC 4 yet?
I thought EF in current state cannot run on Unix. It is supposedly coming in EF7
That's the only thing that hold me back from doing all my personal websites in the .Net stack. Really excited about this vNext stuff
To me, another big difference is that Microsoft states it will officially support these scenarios.
Wow. What about running OWIN, SignalR and Nancy on a raspberry pi? http://youtu.be/s_ZQtiupyzE
Actually, Ximian was bought by Novell, which was in turn bought by the Attachmate Group. They fired the team working on Mono, including Miguel de Icaza, who went on to found Xamarin. So yeah, they have taken it over (or took it back, if you want), and yeah, it has improved since then.
P73-05759 pricing level C would be $44k. If you give the average server cost to be $6.5k, the performance level on linux would only need to drop by ~6.7% before you are at break even. Given the overheads this is not going to be an unreasonable assumption. Sans any bench marks, the argument is fruitless.
Exactly my thoughts, I love working in new technology too, but C# is not relaxing like Java is, good progress is made on almost all fronts of the toolset. It's just a matter of picking the right tool for the job, nobody can convince me that for example a NodeJS backend is better than C# webapi backend in terms of support, performance, structure, testability and maintainability. 
His [post comments](http://blog.jonathanoliver.com/why-i-left-dot-net/#disqus_thread) are more interesting than the conversation here.
What the was that crap? Bitching about NTFS? What does that have to do with anything?
Oh yeah, I forgot about that. Xamarin was founded a while ago though, right? 
Yeah, they're going to add Mono as part of their test suite which is huge. I'm really excited about it. 
It's his anti-Microsoft bleeding through. That's what this post is really about.
That's even possible as of now, but it should become way easier.
2011 iirc
I was sarcastic. Things are becoming more more open at Microsoft. It's just the beginning.
Being a senior developer has IMO not much to do which stack you are programming for. As a senior you are usually working with a team, writing requirements, making software architecture etc, right? Java -&gt; C# is a piece of cake, you will learn the new syntax in a week, but it shouldn't change the way write software right? You would still implement your algorithms, best practices and design patterns, if you write in Java, C#, Erlang or whatever. MSCD is a nice to have, but not an essential to start with.
C# syntax is one thing. I've written plenty of toy applications with C#, so I know the syntax very well as well as large parts of the .NET framework. But in Java terms, I would call this the equivalent of Java SE knowledge. My specialty is in large scale enterprise application development using Java EE. So, I have experience developing web applications, web services, messaging, etc. But the practices that I have learned are Java EE patterns for application architecture, build, change management, deployment, configuration and monitoring. I'm sure there are Microsoft conventions which are similar in principle to the architectural patterns of Java EE enterprise applications, but I'm just getting started so I'm not sure where to begin. The MSCD certification looked like a good way to get a jump start on the accepted Microsoft practices (through the training for the test) and earning the certification would be a good way to get credibility (to supplement my Java EE experience).
MSCD is a good thing to have, but which direction of MSCD you want to go on? I have done my MSCD webapplications, which is very focussed on the MS stack (except for the CSS/JS one), but even with 5 years experience it has a lot of pitfalls and hidden things from the framework, what I'm trying to say it won't be easy to get the certificates like it was 10 years ago. Are you in the progress of migrating to a new stack at your current job or a new position altogether? 
Every single issue this guy has I could fix in 5 mins with tech from MS or something I did my self. Just sounds like the typical antiMS junk. He is touting the java vm. Are you kidding me? MS made one that was better and got sued for it, then went and did better, c#.
I was thinking of doing the MSCD Web Applications certification, since the three exams cross-cut through a lot of my relevant Java EE experience (HTML5/JS/CSS or C#, ASP.NET MVC 4, Azure and Web Services/WCF). I would certainly hope that the certificate isn't easy to acquire because it would defeat my purpose for getting the certificate. I work at a solidly Java EE shop, but I'm considering other options.
The webapp one is good, but you need quite some MVC experience to get know all the ins and outs, and for Azure it's even worse, if you don't have any hands on experience with Azure it's a pain. If you can work with MVC/JS/Azure with some good books you can probably get your certificates within a year.
I write enterprise apps. I'm used to pain ;-) Thanks for the info!
I'm not sure why you're asking for advice and then challenging respondents. The advice from Alexis is pretty clear: No, it doesn't matter. I agree with his implied opinion on why you're asking in the first place. HTTP is HTTP; patterns are patterns. Code is code. Most anything is Java has an analog in the .NET stack. If you have automated builds and unit tests and know patterns, the switch from Java to c# really is negligible. Like everyone keeps saying. If you're an enterprise dev, you know well that it's nearly all CRUD and messaging and http abstractions. Right? Same stuff. Go for it. 
durp durp to you too :P
I haven't used anything buy webapi for so long I almost forgot wcf was a thing.
If there was ever a project that easily explained how "simple" applications never stay simple, this is the one. 
I've implemented multi-tenant solutions at a couple different jobs. In both of those, clients wanted entirely separate back-end databases for each tenant's data. While the article is basic, it does actually cover most of what you need. The missing bit is that the data store for tenant information needs to provide details about how to access a tenant-specific database. I initialize application layers with an implementation of a tenant context provider that contains that information. Depending on the context, such as a web application, unit test, etc. a different provider might be used. In all cases, the context provider identifies a tenant, allowing other application layers to fetch configuration information for that tenant. For example, a web application implementation of the tenant contact provider does what the example does. It parses out the host name to identify the tenant. Another implementation (for an SMS messaging app) identifies a tenant by a phone number. Etc.
Did he just give instructions on how to write a keylogger in C#? 
I avoid using Contract.{Anything} for preconditions in my public facing code to make sure that stuff is checked regardless of how the software is built. For internal or private code I use the non-generic Contract.Requires as it will be removed from my release builds; I rely on the static checker and tests for those. I avoid using the generic form of Contract.Requires&lt;&gt; so I am not forced into using the post processing tools to simply build &amp; run the application.
&gt; !(min &lt;= max) The reason for the double negative in this condition is because the input types are System.Double and Double.NaN can behave in a surprising way when compared, it always returns false, no matter what. 
Any reason not to go with Azure? It's pretty darn cheap.
Having the NuGet packages in the repo might work for some, might not work for others. I honestly didn't know that the NuGet package restore just sort of worked now, so this was news to me. My coworkers only put half or so of the referenced NuGet packages into our source control, so I've been trying to work out how it ever built in the first place. My builds code sign DLLs and EXEs using a timestamp server, so we have a dependency already on an external server, and removing the NuGet packages from our repo would probably work. I did write a signtool wrapper to automatically retry different timestamp servers in the event of a failure though. Re-downloading many NuGet packages over and over again from a clean workspace to do CI builds might take up too much time though.
You can easily google everything he wrote in this post. Writing keyloggers in C# is all over the internet.
If you can post the specific exceptions that would help a lot. One thing to check for is if the .dbml file is properly configured, IIRC it was a custom compilation target or used a specific tool (sqlmetal.exe iirc). That might be a path worth barking up depending on what the error is. As for the project, to control variables try this: * Load up Visual Studio 2013 * Create a new web site project targeting the appropriate version of the runtime, probably 3.5 as they were backing off LINQ2SQL in favor of EF by the time 4.0 came out. * Hopefully she will compile * Find the convert to web application button, put website in proper source control and deliver it back to them. One other angle here is linq2sql is so dead on simple that you should be able to generate the same l2s classes from the database given a copy of the database. So you could also take that route if you just can't get the dbml file to work for you. Final trick -- if you are willing to call the dbml static, you can just statically include the dbmlfile.designer.cs as a normal C# file and forget the dbml. That is just an xml file visual studio uses to let you drag and drop and sqlmetal can convert into the dbmlfile.designer.cs code file. There might be another dbmlfile.cs file which contains extensions to the classes that you probably need. 
Can you link us to what you think is the best quick learning resource for CER or write up your experiences with it? Also thanks for sharing the code.
Fascinating
For some reason I want to say that you can't have a list of HttpPostedFilebase. I believe you need to put each one into the method signature as their own parameter. If that is the case, I have no idea how this would work with a dynamic number of pictures. Binding files is nauseating.
Sounds awesome, will give it a look over tomorrow.
Why are you using SVN? Why not Git or Mercurial? I suggest changing from sourceforge to Github. Put this on github, use SourceTree for Windows and OS X as your client. The github app is sub par in comparison imo. Sourceforge has gone to shit. BTW Your "OpComm" didn't bother generating new certs since heartbleed. Might be worth paying for a trusted cert then having members install the self signed cert. The tour photos look like they need to be updated. http://imgur.com/a/QKPZr You got a nice project though. 
According to [this blog post](http://haacked.com/archive/2010/07/16/uploading-files-with-aspnetmvc.aspx/) it is possible to bind to an IEnumerable&lt;HttpPostedFileBase&gt;, but he is also using multiple input tags.
If each instructor can be assigned to only one department, why not put departmentID in the instructor model instead of instructorID in the department model? This would make it easy to assign a department to each instructor.
Hey, I appreciate your comments; they're a little off topic since I was hoping you'd comment about the project, and not its hosting. But I'll respond to them all the same. &gt; SVN Because I like working in SVN, it meets my needs well, and allows me to use svn:externals into the other (very large) repos I do work in. I tend to work solo, so I don't typically need half of the things that make Git great. On a mere personal preference note, I like having monotonically increasing revision numbers. &gt; Github I don't understand why I would use Github - I neither use Git nor need hosting. I'm quite happy providing my own hosting. &gt; Sourceforge has gone to shit You're quite right, and I considered a few other options such as Google Code or CodePlex. I'm open to suggestions if you have them. I really just need a project page / visibility, downloads, and links to my SVN repo on my server. I could just as easily host it off my own server too, I suppose, since I'm not particularly worried about the bandwidth. Know any good software I could use to do that? I don't want to spend hours working on a website that could be more easily spent working on the projects. &gt; Heartbleed You're probably right, that was something I overlooked. Thanks. &gt; Tour photos Thanks, I hadn't seen those in a while. Brings back memories. Not sure why they're relevant, but whatevs. Oh, and to answer your question, that OS was 'KVM switch'. :) 
The original data model has instructorID in department to assign the administrator. Instructors, in the original data model, have no association to department. That's what I want to add, I want to add departmentid to instructor, but department still needs an administrator (instructorid).
Would you be able to remove instructorid from department and maybe add an "is_administator" flag to instructor?
Tried to run server and client. Test impersonation fails with {"Failed to impersonate the client. Error Code = '80090301'."}. Client connect fails with System.ArgumentOutOfRangeException: Specified argument was out of the range of valid values. Parameter name: size at System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags, SocketError&amp; errorCode) at System.Net.Sockets.Socket.Receive(Byte[] buffer, Int32 size, SocketFlags socketFlags) at TestProtocol.CustomConnection.ReadLoop() in c:\Projects\SSPI\TestProtocol\CustomConnection.cs:line 138 at TestProtocol.CustomConnection.ReceiveThreadEntry() in c:\Projects\SSPI\TestProtocol\CustomConnection.cs:line 88 
Cert is identified as possibly malicious by my corporate firewall. Cannot look further.
Sure, I'd be glad. So, with rare exception, normal managed code can use try-finally blocks to make sure that some cleanup always happens - and this is good enough for 99% of the correctness of a program. If you have some resource-like object that you provide from an API and are worried that consumers might leak, you can implement Disposable to make it more obvious to your consumers that they need to clean it up, and you can implement a Finalizer to make sure your object gets a chance to release even if the consumer does leak something. There's a couple big problems though. Not all finalizers are guaranteed to run, and not all catch and finally blocks are guaranteed to run. There's a certain war occurring within the VM - the executee's right to perform cleanup during shutdown, and the executor's right to impose finite lifetime during shutdown. At some point you have to draw a line and say "I'm shutting down this AppDomain. Only the most urgent finalizers will be run", or you risk some nuisance finalizer permanently deadlocking the AppDomain kill. As such, only some things get executed. This StackOverflow post has some great answers on when finalizers don't run: http://stackoverflow.com/questions/3458177 So an executor (CLR Host) wants to ensure finite shutdown lifetime, and an executee (your object, perhaps with a finalizer) wants to ensure it cleans up resources. How do we balance these? What happens if your finalizer has problems of its own (see below). We'll come back to this. So now, consider the big-three exceptions that can't otherwise be anticipated or handled, at least, not easily. These are the 'rare exceptions', the 1% that ruin the correctness of your code: * StackOverflowException * OutOfMemoryException * ThreadAbortException And consider a situation where you must acquire a native resource, use the resource, and release the resource. You don't want to leak native resources, so you either want that operation to complete entirely, or never start. ThreadAbortException occurs when another thread calls Thread.Abort() on your thread. It's often very difficult to provide API that guarantees it will never see a Thread.Abort(). And what's really nasty about Thread.Abort() is that it can pop out of literally *any* instruction - an `i++` can throw a ThreadAbortException. You could try to catch it, but what if your backout code then suffers another ThreadAbort? It's not generally possible to reliably handle it. A quick aside - do you know how the compiler and debugger work together to allow you put breakpoints on braces? On x86, the compiler emits a NOP instruction, and the debugger sets a breakpoint on the NOP. Since any instruction can emit ThreadAbortException, NOPs can emit them, which means *braces* can emit them. In early days, .Net had trouble with the braces that make up try-finally contexts throwing TAEs. Continuing.. StackOverflow is also really hard to protect against. Your API could be getting called at the bottom of a very deep stack. You need to allocate room for local variables, try-finally contexts, your own method calls etc. You might just be the straw that broke the camels back, and at some innocuous call in the middle of using your native resource that you just acquired, you pop a StackOverflowException. You can't even try to use catch blocks or execute finally blocks with StackOverflowExceptions - it just skips over them, because then you might just cause another one in your backout code, and so the VM doesn't even bother. They are nigh impossible to deal with. OutOfMemoryException can also happen almost anywhere, though you can attempt to deal with it. If your methods haven't been JIT'd yet, the JITer has to allocate memory to store the native-compiled methods so that they can execute. If the JITer runs out of memory trying to do that, you get an OOM out of your method. Thankfully OOMs only can occur at well defined points - any time an allocation happens, which is either a regular managed allocation (`new` et al), or via JIT (method call). If you've entered a function and you can assume you have enough stack space to run any try-catch-finally blocks that might be in the function, then it's *possible* to implement 100% correct backout code. Still, you have to make sure your code is correct (doesn't cause any stupid exceptions of its own, such as NullReferenceException). So lets summarize. We need to protect against the 4 following problems: * ThreadAbortExceptions that may happen literally anywhere in your code * OutOfMemoryExceptions that may happen any time you call functions and they get JIT'd * StackOverflowExceptions that may happen any time you enter a new scope, including new try-finally blocks. * Finalizers that the VM may intentionally decide to not execute, in particular during 'critical' shutdown scenarios. CERs address the first three directly, and are used in fixing the Finalizer problem.
So what does a CER do? Sometimes people think they're magic, but really all they provide is few concrete guarantees: * The VM promises to hold off from manifesting ThreadAbortExceptions until after your CER is finished executing. * The VM promises to pre-JIT and otherwise prepare all methods that are in the static call graph of the CER. If the VM experiences a OOM during this, the CER preparation is aborted and it never runs (which is fine - either all or nothing is what we want). * The VM promises to analyze the static call stack of all methods in the CER **that are marked with ReliabilityContracts** and ensure that enough stack space is available for all methods, their local variables, and any try-catch-finally blocks that might be in those methods. If there's not enough stack space available, the CER preparation aborts and the whole thing never runs. So what exactly is a CER? It's a block of code that begins with a call to `RuntimeHelpers.PrepareConstrainedRegions()` (which acts as a marker to the VM), and then consists of an **unprotected** try block, and protected catch and finally blocks - the `try` does **not** have any of the guarantees of the CER. So for instance: IntPtr handle = IntPtr.Zero; int argument = currentFlags &amp; SET_MASK; // made up operation. RuntimeHelpers.PrepareConstrainedRegions(); try { // This code can still throw any of the big // three exceptions. handle = NativeApi.AcquireHandle(); } finally { // If we can assume that these functions never // throw exceptions of their own, we have very // strong guarantees that both statements will // execute completely. if ( handle != IntPtr.Zero ) { NativeApi.UseHandle( handle, argument ); NativeApi.ReleaseHandle( handle ); } } The above requires that at least the `UseHandle` and `ReleaseHandle` methods be marked with ReliabilityContract attributes (with the right values). The point of ReliabilityContracts is that the runtime will not bother with considering the impact of an unmarked method if it's used in a CER; the method doesn't say that it won't corrupt state, so why bother trying to make a strong CER if it uses a weak method? The following StackOverflow post demonstrates this: http://stackoverflow.com/questions/1101147/code-demonstrating-the-importance-of-a-constrained-execution-region The structuring of a CER depends on the nature of the code that's going to be called. If you have a method, like AcquireHandle, that might actually fail and throw an exception (maybe you don't have code permissions), you don't want that in the CER. The CER can still be built to handle that case, as I've shown above though.
One good example of this, is when .Net does handle reference counting (why do we do reference counting? I'll get to this later): bool got_handle = false; RuntimeHelpers.PrepareConstrainedRegions(); try { // This might fail - throw a code permission exception, or set got_handle to false if // something is wrong with the handle. fs.SafeFileHandle.DangerousAddRef( ref got_handle ); } catch( Exception e ) { if( got_handle ) { fs.SafeFileHandle.DangerousRelease(); } got_handle = false; throw; } finally { if( got_handle ) { result = NativeMethods.Foo( fs.SafeFileHandle.DangerousGetHandle() ); fs.SafeFileHandle.DangerousRelease(); } } If you want to invoke methods that are entirely safe, you may have an empty `try{}` block with everything in the `finally{}` block. Like I said, it depends on the nature of the code that you're invoking. So now that we can build CERs, we have most, but not all of the tools to solve the reliability problem. The last one is finalizers. CERs are great when you acquire and release handles all in the same block. What if your managed code acquires a handle and holds onto it for a long time? Plenty of handles are used like this - file handles, wait handles, window handles, API handles like the qwave/QoS API and in my case, the SSPI API. So in these cases, we need to capture the unmanaged handle in an managed object, and leverage the guarantees that the runtime and GC can make, in order to provide us guarantees on the lifetime of our managed object. If we can make guarantees on the lifetime of our managed object that properly captures the unmanaged resource, we can make finally make guarantees on the lifetime of the unmanaged resource. So to do this, we need two parts: One, a finalizer that when invoked is able to run in a CER, and does; Two, we need to ask the VM to execute the finalizer in more of the 'critical' shutdown examples. We accomplish this by having our class extend CriticalFinalizerObject. The runtime has explicit knowledge of CFOs. One, it'll create a CER for you, and then invoke your finalizer in that CER; Two, it'll promise to execute your finalizer in more of the 'critical' shutdown scenarios. It is able to do so because it performs the CER preparation *when the object is created*, which is usually looong before the finalizer is actually executed. It **has** to do this - Your finalizer might be executing at time when memory might have already run out. If the finalizer is prepared only when it's finally called, and you're already out of memory, there's no way for it to do its job. Objects that extend CriticalFinalizerObject have their finalizer prepared when they are allocated. So, as long as your finalizer has a static call path that can be analyzed, has all of its methods marked with ReliabilityContracts, and doesn't break the CER rules, you can be guaranteed that if you have a class that extends CFO, then if any object of that class is ever allocated, it can be finalized safely. Just to say it one more time, CFOs give us some of that lost finalizer power back. Since the finalizer CER preparation for the CFO is performed when the object is originally allocated, the runtime can guarantee that it can call its finalizer in all but the absolute worst scenarios. OK, so that's one big building block we can use. So how do we use CriticalFinalizerObjects? If you have some native API that looks like this: void* GetApiHandle(); void UseApiHandle( void* handle, int arguments ); void ReleaseApiHandle( void* handle ); You would normally p/invoke that by using `IntPtr` in place of the `void*` values. And now since you have an unmanaged resource (the IntPtr storing the API handle), you need to capture that in a managed resource with a critical finalizer (extends CriticalFinalizerObject). So now, if you, or your API consumers ever leak the MyApiHandle, since it has a finalizer and extends CriticalFinalizerObject, then that finalizer can call ReleaseApiHandle with some really solid guarantees, and now you'll know that you'll never leak it. Awesome. 
So here's what that looks like. First, we need a storage object to capture the unmanaged handle and wrap a CriticalFinalizerObject around it: public sealed class MyApiHandle : CriticalFinalizerObject, IDisposable { private IntPtr rawHandle; public MyApiHandle() { this.rawHandle = IntPtr.Zero; } // Remember that my finalizer is executed in a CER that was prepared // when the object was originally constructed. [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] ~MyApiHandle() { Dispose( false ); } public void Dispose() { Dispose( true ); GC.SuppressFinalize( this ); } // Note that this method is not protected or virtual; virtual calls in a CER // require special handling. [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] private void Dispose( bool disposing ) { if( this.rawHandle != IntPtr.Zero ) { NativeMethods.ReleaseApiHandle( this.rawHandle ); } } public void SetHandle( IntPtr newHandle ) { this.rawHandle = newHandle; } public IntPtr Handle { get { return this.rawHandle; } } } That's great. We have a place to allocate and store the handle, that once its captured, will be cleaned up in a critical finalizer. Here's where we actually perform the initialization and acquire the handle. Note that here, we have to perform the acquisition in a CER, since between the time from when we get the handle to the time we capture it in our MyApiHandle object, we could have a fault: public class MyApi : IDisposable { public MyApi() { MyApiHandle newHandle; IntPtr rawHandle = IntPtr.Zero; // Can't do allocations in a CER. Since a constructed // MyApiHandle is set to IntPtr.Zero, it's safe to leak this. newHandle = new MyApiHandle(); // This has to run in a CER since we have two steps: // 1) Get the IntPtr // 2) Capture the IntPtr in our CriticalFinalizerObject-based wrapper. RuntimeHelpers.PrepareConstrainedRegions(); try { } finally { rawHandle = NativeMethods.GetApiHandle(); newHandle.SetHandle( rawHandle ); } if( newHandle.Raw != IntPtr.Zero ) { this.Handle = newHandle; } } public MyApiHandle Handle { get; private set; } public void DoWork( int arguments ) { if( this.Handle.Raw == IntPtr.Zero ) { throw new InvalidOperationException( "Failed to acquire a handle during initialization" ); } NativeMethods.UseApiHandle( this.Handle.Raw, arguments ); } // We don't actually have a finalizer, but maybe someone would extend us and implement one. // Depends on how you design your API objects if you allow that. I do, so implement // the standard dispose pattern. Everything here is managed (MyApiHandle already is a managed // wrapper for the native resource), so we should only Dispose MyApiHandle if a regular Dispose // is happening; if someone extends us, and calls us during their finalizer, there's no point in // in attempting to Dispose MyApiHandle, since it might already be finalized; doing so would // actually break the regular finalization rules. public void Dispose() { Dispose( true ); GC.SuppressFinalize( this ); } protected virtual void Dispose( bool disposing ) { if( disposing ) { this.Handle.Dispose(); } } } OK, so now we've used a CER and a CriticalFinalizerObject to safely capture the handle and ensure that we will always release it.
What's next? Well, there's one more little problem we still haven't dealt with yet - finalizer races. It's possible for someone to get an initialized MyApiHandle object, and then call your object to use the handle, and for the GC to finalize your objects *while you are using the handle*. Lets say that you do this: void ConsumerFunction() { MyApi api = new MyApi(); api.DoWork( argument = 5 ); // long running. } The GC is very aggressive when it does finalization - it may actually finalize an object while a method is running in that object. It's perfectly reasonable and possible - if the method stops using `this` references, and runs for a long time after the last `this` reference, then the GC will collect the object (`this` becomes GC'd), and the method will finish running since it, perhaps, only uses local stack variables and no longer needs any `this` references. So if someone does the above, what happens? `DoWork` calls `NativeMethods.UseApiHandle( this.Handle.Raw, arguements );`. If that call takes a long time, then what happens? We made the `this` reference to pass in the raw handle value to the function. Once we start calling `UseApiHandle`, nothing needs `this` anymore. The `MyApi` object gets GC'd, which means nobody has a reference to the MyApiHandle that it owned, so MyApiHandle gets GC'd, causing the MyApiHandle finalizer to run. What does the finalizer do? Releases the handle. So we were executing a long running native API function, and in the middle of it, called the native function to release the handle - we released the handle while we were using it!! How do you fix this? You implement reference counting in MyApiHandle: * Reference count is initially zero * When the MyApiHandle critical finalizer is called, only call ReleaseHandle if the reference count is zero. Mark that 'finalization' has been done. * Whenever I make an API call, such as in MyApi.DoWork(), use a CER to increment the reference count, call the native method, and decrement the reference count. * If MyApiHandle.DecrementRefCount() is called, and it sees that 'finalization' is marked as done, and it decrements the ref count to zero, *then* it actually releases the native resource. That way, if our wrapper handle is finalized while the native method call is being used, we'll still 'finalize' the wrapper, but we won't close the native handle until the reference count goes to zero. But now that means that everywhere we ever use the MyApiHandle and its raw handle value, we have to perform reference counting in a CER. Remember that reference counting example I gave above? That's what it looks like when it's done manually. Ok, we're almost done. One last bit to cover. 
All of this crap sucks: * We have to use CERs to acquire handles. * We have to use CERs every time we use handles so that we don't release them while we're using them if a finalizer race happens. * We have to use CERs and critical finalizers to be able to reliably release handles. This is an enormous pain in the neck. Enter SafeHandles. The P/Invoke code understands SafeHandles. Whenever you pass a SafeHandle to an API call in place of an IntPtr, the runtime automatically performs CER-based ref counting for you. Whenever you call an API to acquire an IntPtr, you can change the signature of the native method p/invoke to instead your SafeHandle, and it'll perform the CER-based initialization and capture for you. If you ever leak a SafeHandle, they implement CriticalFinalizerObject and will always invoke your release-handle code except in the absolute worst cases. How do you use SafeHandles? Extend SafeHandle with your own class, then override the ReleaseHandle method to call the actual native release function. Mark the call path with ReliabilityContracts, and you're all good. So now, our native API wrapper that instead uses SafeHandles looks like this: internal static class NativeMethods2 { [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] [DllImport( "some-api.dll" )] internal static extern MySafeHandle GetApiHandle(); [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] [DllImport( "some-api.dll" )] internal static extern void UseApiHandle( MySafeHandle handle, int arguments ); [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] [DllImport( "some-api.dll" )] internal static extern void ReleaseApiHandle( IntPtr handle ); } public sealed class MySafeHandle : SafeHandle { public MySafeHandle() : base( IntPtr.Zero, true ) { } [ReliabilityContract( Consistency.WillNotCorruptState, Cer.Success )] protected override bool ReleaseHandle() { NativeMethods2.ReleaseApiHandle( base.handle ); return true; } public override bool IsInvalid { get { return base.handle == IntPtr.Zero; } } } public class MyApi_Safe : IDisposable { public MyApi_Safe() { this.Handle = NativeMethods2.GetApiHandle(); } public MySafeHandle Handle { get; private set; } public void DoWork( int arguments ) { NativeMethods2.UseApiHandle( this.Handle, arguments ); } public void Dispose() { Dispose( true ); GC.SuppressFinalize( this ); } protected virtual void Dispose( bool disposing ) { if( disposing ) { this.Handle.Dispose(); } } } We no longer need to create our own CERs, though we need to participate in one (MySafeHandle.ReleaseHandle), and we don't need to perform reference counting games because .Net does that for us automatically when we P/Invoke a SafeHandle, and if anything is leaked, we'll be guaranteed that it'll be cleaned up, even in all but the worst shutdown scenarios. Note - you can only use the SafeHandle tactic if you're wrapping an IntPtr. If your native handle is something other than a void* aka IntPtr, you have to play the CER-based ref counting game yourself. I happen to need to perform this in my SSPI library because an SSPI handle is *two* IntPtrs. Bleh :) ----- So with that, I hope you now understand the complete reasons why CERs exist, how to use them, what they actually provide for you, and why they're needed for performing native p/invoke. I've got a couple StackOverflow posts that are relevant - check out these two: http://stackoverflow.com/questions/24442209/how-to-deal-with-allocations-in-constrained-execution-regions http://stackoverflow.com/a/24390662/344638 Cheers!
Umbraco is nice to work with and I agree the docs are a little messed up but I know the core team are correcting that. Umbraco.tv is worth the subscription to learn the basics which will give you a running start
You make some good points. I'll think about moving the project hosting to one of those. Thanks.
But then any number of instructors could be the administrator and (more importantly?) it would take a join to determine who (all) the administrator(s) is(are).
My apologies - the org that I'm involved in is student run at RIT, and uses self-signed certs. If you'd like, you can currently download the source on sourceforge: http://sourceforge.net/projects/nsspi/files/0.1/NSspi-0.1-src.zip/download After reading some other comments, I'm considering moving to different project hosting.
Umbraco is a good choice. It's easy to run alongside your regular MVC site, and it's easy to plug. A lot of energy has been put into the editor experience. 
There appears to be a bug with the messaging that causes a sent message to fail decryption (MessageAltered), which then bubbles back up as a fault when calling Socket.Receive. Not sure exactly how yet, but there's at least 2 or 3 bugs happening. I'll let you know.
SVN is a dying VCS, yes it's still used by many projects (mostly on SF) but the transition to Git is happening, most new projects are using a newer VCS. I'm sure SVN does work for your needs, but Git and Hg are better. SourceTree (just a fan of it) is a great tool to easily use them. Mercurial is more SVN like, bitbucket is the closest thing to github for hg. I suggested Github because right now it's considered the best for open source collaboration. That is if you want other contributors, you'll have much better luck over at github then at SF. And SF is shit guys. Dice.com (a job site trying not to be careerbuilder but it's owned by them). Yes they offer binary file hosting, but they also wrap their own adware installer over your files! I hear you there, but unfortunately these days, your OSS is expected to have a nice little site, at the very least documentation. Check out Start Bootstrap, free bootstrap templates. 2-3 hours and you'll have a site fleshed out. Could easily be a one page site. The person who make ScheduleMaker (nice app) uses bootstrap and Git, some of your members can assist you. A few CSH projects if anyone over there needs something to work on: New main csh site using bootstrap. It currently uses some php, but I didn't see a need for it. They all look like static pages unless I'm missing something that needed php. The login screen for the CSH San should be updated too. The reason I am asking you these questions and "criticizing" because I know you are a student still learning. Learning a new VCS on a small project will be a lot easier than learning it on a new job. Plus it's your first FOSS project, IMO better to make changes to workflow now, then be really late to the party. The photos, I had a few laughs and was bored. --- The project itself is nice but I haven't taken a deep look at it. I could have used this in a prior project for my last company. From my understanding this is a library that can be used to easily use Windows Authentication in c#. So for example, I can make a public C# MVC site that uses Windows Authentication (AD backend) to have users login using their AD credentials without resorting to Kerberos or NTLM? 
This thread has been linked to from elsewhere on reddit. - [/r/dotnet] [/u/antiduh gives a great introduction to Constrained Execution Regions](http://np.reddit.com/r/dotnet/comments/2a682j/uantiduh_gives_a_great_introduction_to/) *^If ^you ^follow ^any ^of ^the ^above ^links, ^respect ^the ^rules ^of ^reddit ^and ^don't ^vote ^or ^comment. ^Questions? ^Abuse? [^Message ^me ^here.](http://www.reddit.com/message/compose?to=%2Fr%2Fmeta_bot_mailbag)* 
Few details: Windows 7 x64 in local (no domain) environment. UAC is turned off. No custom impersonation/authentication settings in group policy. Tried under VS2013 debugger. Hope this helps.
Umbraco - it is the only one where your model is about *content*. With DNN a page is the model. It makes a drastic difference (for the better) to how you construct sites. Your content editors only have to worry about the content, not the page layout. The UI for this is about as good as any general purpose UI can get.
Another vote for Umbraco. The docs are really lacking however.
I'm looking at this from the other side -- 10+ years in .NET looking at other things* and it is very daunting. I think a lot of folks here are missing some of the point -- yeah, after 10 solid years of java jumping to C# should not be that horribly difficult from a syntax perspective. The challenge is in the 10 years of carnal knowlege about the tooling, the ecosystem and community. How much of being a senior developer is based on being able to answer questions with statements like "oh, you are trying to do that, here are the 3 libraries you should look at. Here are strengths and weaknesses of each . . ."? Anyhow, I'm not certain how to fix this short of walking the walk and talking the talk and going and building shit on the new platform and fighting through the problems and gaining the carnal knowlege. Personally, I doubt if an MSCD will be that helpful as it will just tell you what MSFT wants you to know and not a bit more. And the bit more is where the good shit is at. * mainly because all the cool shit is happening in other things, not because I'm a traitor or some shit
I've had nothing but good results working with EPiServer. http://www.episerver.com 
Umbraco is my choice of CMS, templating is easier to work with and the new API is very easy to work with. The documentation is getting better every week. I've also heard good things about kentiko too although I've never used it and I believe it's a paid-for CMS. 
Orchard is hell, and Umbraco, while much loved, isn't much better - they both suffer from way too much reinventing of wheels. More people know umbraco though, so if you must use a general purpose cms then go that way. I'd always push for some markdown supporting remote resource loading thingum - if the users of the cms are technically able.
Nice, but I'll stick with ServiceStack
Most of my experience is Umbraco, and in the "Premium" side Sitecore, I'm not a fan of Episerver though (but that's cause the outsourced devs fucked it hard). Both Umbraco and Sitecore support MVC(ish) development, and can work alongside/integrate an MVC app to extend their backend. 
I just wrote an ASP.NET MVC website that uses WordPress database. The good part is that I don't have to recreate WordPress functionality while having the power of ASP.NET MVC to "do things" with the HTML content from WP. It worked so well, you might want to consider this route.
Umbraco or orchard
try Kentico, it's webform based, but supports MVC to a given degree, the code is clean, it's quite easy to extend, maybe you'll like it, also, their support is really good 
Personally i'd go orchard and work with rather then fight nhibernate. 9/10 your dealing with orchard interfaces, cos nhibernate is already abstracted by them. For those times it won't do what you want, just grab the session from orchards session locator and use dapper. We use orchard for a large scale website, and its held up very well under extreme load, also very actively developed.
What he said. . Id love to see this 
Maybe you can try KooBoo http://kooboo.com/ Github: https://github.com/Kooboo/CMS
Java/Android is a lot different from Enterprise Java. Mobile development adds new challenges in and of it self so moving from enterprise C# to Windows Phone would be just as much of a challenge. C# and Java are similar enough where I think it comes down to learning libraries and frameworks. Which is not something that is needed all at once and can be quickly learned as the need is presented. 
This is a great response and totally unexpected. I hope you can get this into HTML for a wider audience. If you have a blog I would like a link (for more reading), if not places like dzone and tech.pro can host content for you.
It looks like this is outdated ( 5 years old?)
Not a fan, huh? I respect your opinion on a few other topics. Why don't you like EF?
Looking at the code it is clearly a **very** old version of EF. `ObjectContext` should not be used anymore.
More of a dislike of ORMs in general. They lead to bad practices such as... * embedding SQL in the application * tightly coupling the application to the table structure * using heavy object graphs instead of projections (i.e. class===table) * fetching columns that are not used * unnecessarily reading a row when performing a write * inserting one row at a time instead of in batches (i.e. bcp) * updating one row at a time instead of in sets * moving lots of data back and forth between the application and the server when all the logic can be handled server-side via stored procs * property getters that unexpectedly invoke database calls * little or no support for immutable objects EF in particular tends to be really slow on top of that because of features such as auto-tracking. 
How do you suggest interfacing with a database if not for an ORM?
System.Data isn't too bad, though with a touch of reflection it is even easier. Or if you want to be buzz-word compliant you can use one of those so-called "Micro-ORMs". They are basically the same thing, but with a management-friendly name. Good data access without ORMs is not hard, just a little tedious.
Facebook (yes, reinvent the wheel, just make it shinier).
&gt; moving lots of data back and forth between the application and the server when all the logic can be handled server-side via stored procs Ooh... I was nodding along until you got there. Writing logic in SQL... not good.
Real world example from my current project. I have a Property table with a Parcel and a ParcelCenter field. Both are geometry types. I need to update Property.Parcel to equal what I have in IncomingPropertyTable.Parcel for the given property ID. Then I need to update Property.ParcelCenter as follows... if Property.Parcel.Type = Point Property.ParcelCenter = Property.Parcel else Property.ParcelCenter = Property.Parcel.Centroid() Are you really telling me that I should serialize the entire Property table, ship it across the network to the application server, deserialize it, run that little bit of logic, and then ship it back as a series of individual UPDATE statements? 
I don't follow. It's late and I'm tired. 
Here's the full SQL from the proc UPDATE Property SET Parcel = s.Parcel, ParcelCenter = CASE WHEN s.Parcel.STGeometeryType() = 'Point' THEN s.Parcel ELSE s.Parcel.STCentroid END FROM Property p INNER JOIN StagingProperty s ON p.PropertyId = s.PropertyId What would be the ORM equivalent of that?
You are trying to do multiple .First() on your result set. Try doing the .First() outside your object initializer (i.e. make sure there is only 1 .First() anywhere in this piece of code.
Why is that serializing the entire property table? What is the property table and could it be an enum?
&gt; Why is that serializing the entire property table? Every time you make a call to the database, any information that is returned has to be serialized for transmission over TCP (or UDP, named pipes, etc.). And of course you have to deserialzie that in the database driver running on the application side. People often forget how expensive that can be in terms of CPU and network utilization. *** And to make matters worse, most ORMs won't give you an updatable entity unless you request the entire row. (Though you can mitigate this by bidning to an updatable view instead of a table.) 
&gt; What is the property table and could it be an enum? For the sake of argument assume that it is... * PropertyId * AddressLine1 * AddressLine2 * City * State * Zip * Owners * Parcel (SqlGeometry) * ParcelCenter (SqlGeometry) And lets say 150 million rows.
So an enum is definitely out. Why does it need to serialize the entire table instead of just one row?
Why? It sounds like something that would get updated when something else triggers a change on property. How many records are we talking about? Edit - terrible English. 
Ok, but why is that a problem? 
I don't know much about your domain, but it sounds like a perfectly valid use for a stored proc. How is it meant to be an example of ORMs being harmful? 
That was in response to this comment: &gt; &gt; moving lots of data back and forth between the application and the server when all the logic can be handled server-side via stored procs &gt; &gt; Ooh... I was nodding along until you got there. &gt; &gt; Writing logic in SQL... not good. 
I wouldn't really call it logic, it's a bulk data operation. A single if statement (if it's even necessary) is very different from having all your business logic in the database.
If you enumerate the same collection multiple times you could end up repeating the same query. Their fix to avoid this was to just make it so that you couldn't resolve the enumeration more than once. If you call something like .ToList() it will resolve the queryable into a list of objects and then you can do whatever you want to the list. You may or may not want to do that depending on when you want the query to actually execute.
&gt; If you enumerate the same collection multiple times you could end up repeating the same query. Right, but that shouldn't throw an error.
Ideally yes, but that doesn't explain the error message.
Yes, it does. Your first mistake is thinking the line var u = contextDM.GetSinglePhonebookById(id); does anything against the database. It doesn't. It just defines the query. when you call u.First(); it actually executes the query and returns the first result. If you call it again, it errors out, because it already executed the query. Executing a query is very costly, and you don't want to do it several times in a row. If you call var u = contextDM.GetSinglePhonebookById(id).First(); it will actually execute the query and save all the results in the u variable. you can then use the u variable as user.FullName = u.FullName; 
There are a couple things to mention here. 1. GetSinglePhonebookById() &lt;-- this clearly says to get a SINGLE phonebook. For some reason you are expecting a collection or enumerable list of phonebooks. This does not make any sense. Is it single or not? 2. Since I'm assuming #1 should be a single element, you would then not even need to use First() on your entity because the "first" one would be the only one. You would just use u.FullName instead of u.First().FullName. The error you're seeing is from calling u.First() twice in a row. Normally I don't think this would be a problem but for some reason your DataEntities class has this limitation. I'm assuming the reason is because it changes it from an IQueryable to an IEnumerable and having two database calls breaks its immutability properties. If you insist on having your GetSinglePhonebookById() method return a collection, then you could rename your "u" variable to "phonebooks" then do "var u = phonebooks.FirstOrDefault();"
&gt; If you call it again, it errors out, because it already executed the query. Again, why? Maybe I actually did want to rerun the query. Maybe building u was complex and I want to cache it for reuse later. Or maybe I want to call u.Count() next to see how many items remain. (Useful just after a .Take(50) call). Is there a property I can check to see if u has been fouled? 
If you ask why the roof of your car always leaks the second day it rains, would you really accept "by design"?
I'm trying to figure out why you would want to run a query more than once within the scope of the IQueryable object in the first place. They decided to design it to prevent you from doing so because it has code smell on it.
Oh right, it's tied to the damn scope. Most of my work these days uses an DAL that doesn't require the use of IDisposable unless you are in a transaction. Otherwise it automatically opens and closes the connection like old school ADO.
People that dogmatic are going to screw things up regardless of the data access strategy :)
True, very true. Alas it seems that I can't avoid them in real life.
One of the good micro-orms is Dapper. It is simple to use and powerful. I will never have to hand roll something with System.Data again.
What a bunch of garbage lol.
It sounds like you have some other layer, and aren't just talking about something connecting directly to a DB. So yes, in your case, it might make more sense to do it directly in SQL. You can run raw SQL through EF too, by the way. 
You don't want to rerun the query in any cases you mentioned. All are runnable on an already executed on a query that was enumerated to an ienumerable in local memory and assigned to a variable.
It's a reasonable question. There's no requirement to throw an exception because the query results are enumerated twice. Any ORM with a linq provider will allow this, as they all do: execute query -&gt; enumerate datareader and materialize results -&gt; enumerate results. In EF however it's a bit different, because in EF6 streaming is the default: so the results aren't stored in-memory in a buffer to re-enumerate, the data reader has to be enumerated again. This of course isn't possible, as a datareader is a forward only cursor. And _that_'s the reason the error pops up. Not your reasons, because there's no reason to execute the query again if the results are enumerated again. A Linq query contains the provider which produces the results to enumerate when GetEnumerator() is called. The provider can just return the same results it fetched the last time. (source: I wrote a full linq provider for my ORM framework)
Those are not reasons why it's not possible. 
Those are nonsense reasons. The reason the query gives an exception is because by default EF 6 uses streaming when a query is executed through enumeration. This means that they materialize objects during datareader consumption. When the results have to be enumerated again, the datareader is at the end, not at the start, as there are no buffered results. You can't rewind a DbDataReader. If the results would have been buffered, like any other ORM does, the results would have been enumerated again, no query would have been executed. 
that will run just 1 query (the count). Take() doesn't enumerate.
&gt; Yeah that example makes sense. I'd probably want to evaluate the query by calling Count and then evaluate a new query with Take. Cumbersome indeed. why? It's just an expression tree. Appending another extension method call like Where or Take or Count at runtime creates a _new_ expression tree instance (as the nodes are all immutable, evaluating the tree is creating a new instance). EF evaluates the expression tree to their own internal command tree which in turn is converted to SQL. If you do: var q = ctx.Customers.Where(c=&gt;c.Country=="USA"); var q1 = q.Count(); var q2 = q.OrderBy(c=&gt;c.City).ToList(); it will mean that the tree created in the first line and stored in q is pulled into the expression tree created by the Count() call and evaluated (as Count() is a method which triggers execution too). During the evaluation it will create a new version of the tree in q. The ToList() in the 3rd line will also pull the tree in q into its expression tree and there too a copy is made during evaluation. I.o.w.: you don't need to create new queries. The linq provider does that for you. 
This is literally a "we can't even give them way" situation.
Thanks! Added var record = u.First(); Then get properties from record.
Thanks! Added var record = u.First(); Then get properties from record.
Thanks! Added var record = u.First(); Then get properties from record.
I don't want to run SQL through EF. I don't really want SQL to be anywhere near my application. SQL should live in the database with an encapsulation layer around it so that the tables and application code can be changed independently of one another. 
Yea. I plan on stealing some code from them because their object mapper is currently faster than my own.
Pertend I didn't forget the Select
Yes. I have product that uses a custom AuthorizeAttribute. The caller constructs a forms auth object and you decorate your receiving controller methods/class with this attribute that handles these requests. 
Use OAuth or something similar? http://openid.net/connect/
Here: using System.Net.Mail; public static class Emails // you should make this class static so you don't have to new { // it up just call Email.SetTestEmail(); public static void SetTestEmail() // you forgot to add the () { var message = new MailMessage(); message.To.Add("blah@blah.com"); message.Subject = "This email was sent using C#"; message.From = new MailAddress("sender@blah.com"); message.Body = "If you received this then the C# code works!"; var smtp = new SmtpClient("clientNameHere"); smtp.Send(message); // you were missing a semi-colon } } On another note, if you plan on scaling this functionality, you may want to use SendGrid... you can track your emails and ensure they made it that way for next to nothing in cost..
Thanks!
Put some parens after the method name like this: SetTestEmail() 
##Eligible startups must be: 1. Actively engaged in development of a software-based product or service that will form a core piece of its current or intended business*. To meet this requirement the software must: * Add significant and primary functionality to the integrated Microsoft software. * Be owned, not licensed by the Startup. 2. Privately held 3. In business for less than 5 years[1], and 4. Bringing in less than US$1 million in annual revenue[2] Microsoft may permit individual developers or others and/or separate technology entities who may not meet the standard eligibility requirements to join BizSpark from time to time. [1] Startups who are actively engaged in software development but have not yet completed the formalities of establishing a business. [2] This requirement has been adjusted to add local variances calibrated to local economic conditions in the startup’s place of business, below. If a startup's place of business is not listed below, then the revenue limit is US $1 million. US$750,000 China, US$500,000 Korea, Malaysia, Poland, Russia, Spain, Ukraine US$250,000 Egypt, Thailand, Turkey, and Vietnam ##* Not eligible for BizSpark * If you are developing dashboards, HTML editors, utilities, website, and similar technologies that are not considered primary service or application. * If you are a consultant *Copied from: https://www.microsoft.com/bizspark/signup/default.aspx* === **The answer should be yes according to their requirements. However, it may seem that you need a separate email address and microsoft account for this other business.**
&gt; // you should make this class static so you don't have to new BOOOOO!!!!!!!!!
Or mandrill -- which is free at many site's levels of email traffic.
I've had such terrible experiences with sitecore. I'm not saying the product is bad but this was built with three websites on top of each other with a bunch of spaghetti code. Was such a headache to work in. 
Are you using Forms Auth or some of the newer owin magic?
This has to be my favorite programming answer.... Ever
This is called federated authentication. You can do it several ways including oauth, open I'd, SSO with SAML. Look at Thinktecture identity server for an open source implementation
Yeah, Change the machine key in the web.config so the two sites match.
They might, at worst, be worried that you're going to use the accounts on your 5+ year business. If you sold it or dissolved it, they may (but probably won't) ask for documentation. That's as about as far as I can think into it.
It can take some work, but if the logic and schemas have overlap than it can make sense to consolidate into a single api. Here are a few patterns on schema and logic centralization. http://www.eaipatterns.com/CanonicalDataModel.html http://soapatterns.org/design_patterns/canonical_schema http://soapatterns.org/design_patterns/logic_centralization http://soapatterns.org/design_patterns/reusable_contract
Huh, I would have never expected that.
Eh. Why boo?
How would you unit tests a class that uses this e-mail send class when it is static?
I'm not sure I see a problem doing that, or a need to test a class that does nothing more than call framework functions. Surely, testing those is not your responsibility.
True. But why unit test something that doesn't do any custom logic that can possibly go wrong?
Stringly typed? Please, no.
Do you know where this class would be used? I'm not talking about unit testing this class, but the class where this would be used.
Note that making the *class* static doesn't actually do anything (except preventing you from declaring non-static methods). It's making the method static that does it.
Useless for programmers.
So.... your argument against using *static* is because you can't automate unit-testing? Booooooo. Worst reason, ever.
You can't unit test it, you can't mock it, you can't inject it. It's simply pretty bad design.
I think some folks don't quite realize you can "nest" ternary operations.
Using *Static* on a method is not bad design. Because you can't unit-test something is *not* bad design. Frankly, there's also nothing wrong with actually sending an e-mail because in true testing -- you want to be damn sure it does connect to the server and deliver the message. Mocking the method doesn't do crap in this scenario. In real world scenarios -- most servers have authentication and/or permission based on domain and in more secure environments, a whitelist held by the server. You will absolutely want to test that the email gets accepted by the server and delivered. If that means you use your unit-test to send to your own email address, then that's what you do. Saying it's bad design because you can't figure out how to wrap a useless unit-test (mock an email send? that tests *nothing*) is the laziest thinking I've ran into in awhile. That's only thinking of getting a greenlight on a useless unit test, and not actually testing the functionality. I swear to god, programmers are getting lazier and too codependent on abstracted tools these days.
I swear to god, it's hopeless to explain you. I don't care about mocking the send of an email, as I explained before already.
I can't find anything unusual in here.
In c#it doesn't matter anyway. The compiler is smarter than that.
My car gets 40 rods to the hogshead and that's the way I likes it! Call me old fashion... How would you like it handled?
Some of us old-timey structured programmers cut our teeth on case statements in FORTRAN and Pascal. Sadly it is hard to change. 
Put it this way: how many of those 7 arguments should actually be strings? Both the attachments and priority should absolutely not be - there is absolutely no reason to do it that way. You sacrifice compile-time checks, miss out on cueing the caller onto expected data, end up with your own "formats" (albeit trivial ones in this case), and have to process them yourself.
You are right and if I was handing this in for a class assignment it would not get an A... but this was just copied out of an old work project I have laying around. We reuse it all the time, maybe I'll get around to "fixing" it one day. Thanks.
It is. Upvote for you. 
I tend to install older stuff first, so VS, then SQL. 
I install SQL Server first, but in the end it shouldn't matter. SQL Express has it's own instance and I usually name my full SQL server as well.
This is best practice. Comes from the old days before the GAC, when you had to deal with different versions of a DLL file. Now with the GAC you can have multiple versions of the same library if needed and reference the one you want in your .config file. Although I have never that I can remember needing to use an old version of an assembly.
&gt;I install SQL Server first... That's what I've always done.. but, even though this is a virtual machine I HATE having to scrap messed up installations and starting over.
if you're using hyper-v, you can always make a checkpoint when you're at a good point. so worst case is you're going back to your last known point of goodness.
Yeah, I'm using parallels and you can do the same thing BUT.. I'm too stubborn and I want to do it right the first time (plus I've already Installed SQL Server 2014) :-)
GAC is going away in next version of .NET, although it will stick around for at least another decade (I'm sure it will be virtualized at some point).
Do not try to bullshit it. At all. It really depends on the question and your knowledge of the subject. If it's something you know absolutely nothing about, don't be afraid to say "I don't know", or "I haven't had any experience with that". If you partially know the answer, then give the partial answer but let them know that you don't know the entire answer. If it's a question on how to solve a problem that you don't know how to solve. You can say "I've never encountered that, but if I were faced with it, these are the concerns I would have and what I would research." For example once I was asked what the most efficient way to pull 1 million records for a database with millions of records. I didn't know the answer, so I told them how I would approach the problem for the first time. I would look into query optimization for that particular query, I would research the possible performance hit for any concurrent queries, and so forth. So, even though I didn't know the answer, I was able to show my thought process and my problem solving approach. For junior roles, I never really looked for someone with all the answers. I looked for somebody who can grow into the role.
1) *Do not try to bullshit through technical questions when interviewing with technical people.* 2) If you think you might know, ask for clarification. 3) If you don't know, admit it openly. 4) After admitting you don't know, express curiosity, interest, and a desire to learn by asking what this thing is. 5) If it is plainly obvious where this thing would be useful, demonstrate your ability to apply knowledge by mentioning how knowing it could make your life easier (but remember rule #1). 6) If you don't have any idea what they're asking about is good for, ask. Don't go overboard on this one. Once or twice max. 7) Mention you'll need to look into that subject later. 8) *Look into that subject later.* Fool me once, shame on you. Fool me twice... fuck, I'm an idiot. *Edited for formatting
Try to read the person, I don't know how your people skills are but get a feeling for the conversation. Being nervous is OK but when you have a non-talker on the other side then it can get awkward very quickly! I think the points mentioned here are very well. Don't be cocky, don't try to teach them, know your position. If you have a technical person, most of them like their ego being upvoted, so make sure to show an interest in things are you unfamiliar with, and let them explain how they do things and be impressed.
After developing in PHP (front/back-end) for 5 years, I decided to give the corporate world of .NET a try (back-end with a bit of front). During my interview, I was as honest as possible. For the questions I didn't have an answer to I just said "I don't know, I'll look it up when I get home". I got hired. First day was a bit intimidating but I quickly got up to date with WCF and other stuff that I had no idea I would be using. Make sure you know some patterns like dependency injection, factory pattern and singletons. Know when to use each one.
Dont bullshit. Period. Instead, explain how you might find the answer. Make a point to stress that you may be a junior developer, but you know how to find answers you may not know. A lot of people will just say 'I don't know', or sit there with a dumb look (I'm not being rude, Im just referencing the type of look on their face). Instead, I want to know what experience you do have with it. Have you heard of it at least? Where have you heard about it? Read about it? Are you interested in it? Why? Why not? Edit: Just for reference, it's ok to not be interested in something. I'd rather you tell me in the interview and I assign you to something you are interested in, than not tell me and do a shitty job bc you hate it. Telling me will not keep me from hiring you (unless it is all we do or what the position is for, which, in that instance, why are you applying anyways). Emphasize that you can figure a problem out. If they want you to use Web API and you don't know what that is, focus on how you're active on stack overflow, or you go to user groups. Focus on experience you have solving problems with no previous experience. If you have never solved a problem with no previous experience, then do that now! Pick a problem in your life, and solve it using something you've never used before. Then talk about that in the interview. Make a website for your mom's book club. Make an app with a little basic game. Doesn't matter what it is. Just do something. As an employer, I can teach you to program. It's really not that hard for anyone who has a little talent and a lot of drive. If the employer isn't a complete idiot, then he can too. Don't know the answers already? Show him you have the drive to learn and figure it out. That will make you stand out. (trust me, you will me in the minority) Source: Someone who has been hiring junior developers for a long time.
Big upvote to this. I'm interested in seeing if a candidate can do a deep dive on pretty much anything. It doesn't have to be the technology I'm focused on. If you've ever had a challenging project and really had to research and experiment to get the desired result, tell the story at your first opportunity.
I had not heard they were deprecating the GAC. Is that part of the vNext initiative? I have not even started looking at Visual Studio 14. Hell we just got up to 2013 in the last couple of months. It takes us years to move over the codebases for stuff like that. 
Especially for a junior position, you're not going to know *all* the answers, they're going to try and stump you on purpose if they can just to see how you'll handle it. They're looking for entry level knowledge, a natural curiosity, and good problem solving skills.
I agree about "puzzles." To clarify though: I separate technical problems from puzzles. Asking someone "why are manhole covers round" is a puzzle. It's not that great of a question. However, I do stick by asking tough programming problems. Yes, they aren't always exactly real world, but gauging someones programming and problem solving abilities in some way is important. Since doing a startup, here is a question that I used to think was stupid, and now I ask it during every interview: "How much water flows out of the Mississippi river every year?" This question is designed to completely make someone deal with a domain problem that they have never experienced. The "answer" to this question is the set of questions that a person would ask or think about to even begin to come up with a solution. (How wide is the river? How much rainfall is there annually in the united states? What was that weird equation I learned back in my differential equations class?) I loathed this type of question before. My change of heart came from the fact that we can't just be devs at our shop. The dev team has had to design RPG experience systems, virtual economies, anti-cheat strategies, deal with customer support, write status reports for business people, design booths and budget booths for conferences etc. I explain this to candidates after I ask the question of course. Funny thing is, that usually that sparks a ton of answers and creative thinking from them. It's like they are willing to answer it now that they feel it is a just question. Humans. :) I usually close the interview with this one, as it can elicit some discomfort for the candidate if they take exception to the question.
~deer in headlights face~ Google?
It might depend if you are using the Full .NET or ASP.NET. ASP.NET vNext will support true side-by-side deployment for all dependencies, including .NET for cloud. Nothing will be in the GAC. Basically, .NET will be distributed with your app (10MB for cloud). For desktop apps, this might be different.
Ask, that is what I have done before. If I don't know something I will ask questions about it and try to understand it. If I still am at a loss then I will admit I don't know.
&gt;Make sure you know some patterns like dependency injection, factory pattern and singletons. Know when to use each one. Good advice
Sorry, for postgres I had to buy [support with devart](http://www.devart.com/dotconnect/postgresql/). But it's pretty affordable!
In the case of singletons, "never."
I was interviewed recently and they were looking out for "I don't know", you'd be amazed how many people don't know how to say it! It means you're honest about your skills, it means you know to ask for help etc. So yea, bullshitting isn't going to work, but if you can show your approach to a problem then that is great, even if you don't know. Again, I had a couple of questions I had no idea about, but I'd voice my thought process and it turned into a discussion where they helped me get the answer, there is nothing wrong with this (and it scored the company some points in my book for being awesome). Companies ( in my experience ) want learning potential and engagement as much as skill, they have in all my interviews, because there is always something to learn. You're going in as a junior so you're not going to know everything, they are going to be aware of that and if you're honest with them it'l be much better. Interviews are great fun, you can learn a lot and if you're lucky, so can your interviewers. Good look in your interview, I hope it works out for you!
The approach taken by most interviewers is to try to find out what you know, what you don't know, and how you can be valuable to the company. The problem is that the way interviews are typically designed puts you into a catechistic position. I've had the most success when I've broken that mode of interview and allowed myself to just explain my position as a developer. Your goal as a developer during an interview is to position yourself where you are not answering, "well I've never painted a wall brown before, but I've painted one blue." Instead force them to let you explain what you've done and how you can be useful to their company. Try to find out what their problems are and be interested in solving them.
I've gotten a job before, and had my boss later tell me that a large part of the reason why I got the job was because I answered with "I don't know" (or something to that effect) when I was asked a question about something I was unfamiliar with. He appreciated my honesty. Fast forward a few years and I'm on the other end of the interview table, and I can definitely see where he's coming from. It starts to get annoying to hear that a guy has "dabbled" in everything out there. I typically don't believe someone when they say they've "dabbled" in a technology. I can honestly say I would've much preferred him tell me he's never heard of it. 
In case you still haven't had the interview, the best answer for something you don't know is is "Can I google this", unless the interview is at microsoft in which case "Can I bing this" is the best. If the interviewer for a junior role expects you to know every question they have, they aren't really expecting a junior. What's important is your direction, how fast you learn. Almost like your vector if you will, how much you currently know, what speed you are learning at. Those who bullshit give the impression of being dangerous. Those who you figure will ask for the important stuff, self learn for everything else are normally the kind people want to hire!
This. I recently did an internet for a junior ASP.NET developer at a company since I am trying to transition in to a role along these lines. I wasn't afraid to say "I don't know" but I did word it differently, something a long the lines of "I haven't had much exposure to it". I was straightforward and I wasn't trying to sell myself as an expert developer, much less ASP.NET since I have very little experience with web based stuff, but I did do some light reading up on ASP.NET and looking into the different models, frameworks, etc. EDIT: My recruiter would argue that I shot myself in the foot when they asked me some "Rate yourself on a scale of 1-10" questions, but I would rather be honest with them. I.E. "How would you rate your knowledge of C#?" I gave myself a 6-7. And, "How would you rate your knowledge of ASP.NET?" I gave myself a 2-3.
I was once asked in an interview specific namespaces for various stuff. It seemed extremely ridiculous to me that I should even commit things like that to memory. I said I'd Google it if needed. I did not get that job.
I'd start at the source: http://www.asp.net/web-forms 
Thanks man, looks good I'll definitely check them out. 
Java web start has been able to load the standard library piecemeal for years. I assume desktop .net will go the same way.
The code you will work on is almost definitely a terrible example of OOP. Don't learn anything from it.
FWIW: I solved this in a round about way. I disabled codeFirst migrations. I moved the seed method code into a CreateDatabasIfNotExists derived class and added the fields just as I tried earlier. Then with a little fluent I setup cascades and built a new database. Works like a charm. I still have to work out a cascade on delete issue, but the basic relationship I was trying to create is solved.
Very few languages provide language-level facilities for singletons. Singletons are useful/the best solution in maybe 5% of the cases they're actually employed.
Fool me once...
Learn the page life cycle and the events used to hook into it. As someone once put it "that's where you make the money"
I strongly suggest using the Model-View-Presenter (MVP) pattern when working with web forms. When I first started using it I didn't think the added layer was worth the effort, but the more you use it the better it gets. It sets you up to be able to use data models which I think is essential with programming now a days. And it splits up all your logic, so you are not exactly building your logic against controls like you typically see with web forms. You can find more info about this but here is the basic run down. You have your data model which you handle just like you would in mvc. You have your view which is an interface on your form. And you have your presenter which manages your data model on your view. You pass your view(form) into your presenter which modifies your data model. Your views properties getter/setters point directly to controls on the page. Well on second thought just read about this. Its not easy to explain in a couple paragraphs. Its really the only way you want to do web forms any more. It makes it super easy to manage and reuse code. Even if your working with an old code base, i would suggest writing new forms to use this pattern. And this will actually set you up to use better practices when you switch to mvc. 
At my first real job senior dev#1 didn't know it was more efficient to store date as dates instead of strings. He thought our monthly reports took days because our database had reached a whopping 400MB. Senior dev#2 insisted arrays were faster than lists and should be used everywhere. It's hard to be smarter than a rock in this industry. 
Yes, Amazon is good choice. Another recommendation for you is [cloud server plan](http://asphostportal.com/Windows-Cheap-Cloud-Server-Hosting-Plans) from asphostportal. Nice pricing
Wow... I'm bit surprised that you dont recommend AWS. As I know both Azure and AWS are good option. I actually dont recommend Rackspace because their expensive price. If I need cloud in future, I will definitely choose AWS, Azure or my hosting provider now, asphostportal
Have you worked with the WebFormsMVP, www.webformsmvp.com, framework at all? I'm trying to work through the demos but it all seems like black magic as I can't find where anything is instantiated. Granted I'm still getting used to .net and web forms in general.
[SignalR](http://signalr.net/) for real time connected features? 
You could easily ask the same question if you have been away for 2 weeks. Most of the new stuff is what /u/quadlix so eloquently refers to as "hipster conference fodder" I would focus first on what task you need to accomplish, then look for tools that fit the task. Don't just pick a tool and try to shoehorn a project into using it. When it comes down to it, there are so many different options to do any one thing that no matter what you pick there will be some alpha geek out there that will berate your choice of tool/framework. 
Asp.net is getting rebooted with vNext.
Your comment made my day, thank you. 
Yes, I totally get that. But, it's good to know what tools are available for given problems, so you don't re-invent the wheel. And, I'm not worried about any "alpha geeks" trash talking. Sadly, that's almost a given in this industry. 
Some people reading this, who don't work from home, might not realise that's is not always as cool as you think it is. You miss working with others. Not just the office banter but being able to discuss a problem you're having. Also, you can get bogged down with things that you would normally just "go and ask Dave about". You don't realise how much you need a rubber duck till you don't have one. Tl;dr: subscribed. 
Thanks.
The number one thing I can think to recommend to anybody who is trying to work a position where they may have the skillset but not necessarily the knowledge or experience is to **build a professional portfolio**. A professional portfolio, if done correctly, will completely annihilate anybody else contending. Nobody goes through the trouble of cataloging and dialoging their previous work in such a way that provides visual, demonstrated examples of competence. I recently acquired a .NET developer position with no experience coming from an extensive background in ColdFusion. I used my portfolio to show that I knew how to pick up languages; I included example code blocks from my past projects for ColdFusion, jQuery and SQL, and provided through descriptions of what scenario lead to the need for that code and how it addresses the problem. I really wish I could convey the response I received in something other than words once I posted my resume and started providing potential employers with my portfolio; I will say that from the time I finished my portfolio and posted my first draft resume online to the time I got a new job was exactly six days. I had to pull everything down because the responses were so fast and heavy that I was overwhelmed by the noise.
I made some ASP.NET Web Forms (C#) training videos awhile back, and have been procrastinating doing anything with them. So, today I uploaded them to YouTube and made a playlist: http://www.youtube.com/playlist?list=PLtMFeKNXw-GH0wC0tTM1Tb8wgOw7NV9EJ Hope they help! I had to learn ASP.NET Web forms very quickly at a new job in a very sink-or-swim type of situation seven years ago. I was right out of college with an education in OOP and Java. My only experience with programming for the web was PHP. I remember being very overwhelmed. Like other commenters have said, Web Forms feels a bit unnatural for OOP, and if you ever get a chance to work with ASP.NET MVC you'll probably love it for how much sense it makes in comparison.
WebAPI has eclipsed WCF almost completely. I still remember thinking how amazing WCF was and now its like the red headed stepchild.
Hmm. This one is interesting. I do (did?) a lot of backend work and used WCF a lot. Thanks for the pointer. 
Is your project referencing version 11 now of the Microsoft.ReportViewer dlls? You may have to revert back to version 10 until you can figure out the issue. Edit: Check to see if your referencing the same version of the Microsoft.ReportViewer as the 2008 solution was.
Yes it is referencing version 11 of the Microsoft.ReportViewer dlls. If I revert back it would have to be version 9.0, because the project was built using version 9.0. The problem also is that if I revert back to 9.0, I cannot open the .rdlc files in design view because it prompts me to upgrade the reports first, which it changes them to 11.0, even if my references have been changed to the proper versions.
When you have version 11 of the Microsoft.ReportViewer in your project and you open one of the existing RDLC files. Does it prompt to upgrade the report? Is so, just let it upgrade 1 of them and then run the report within the application and see if it works. If this is a web application, there are web.config settings you may have to check as well.
I have not. Normal MVP doesn't require any extra framework.
Can remember exactly when it came out but if you're not familiar with TPL (Task Parallel Library) including the new (in .NET 4.5) async &amp; await keywords I'd read up on those. Makes asynchronous programming much easier.
Make an effort to answer the question. If you can't came up with an answer, try to explain the path you're going down when trying to answer the question. Being able to show how you're trying to figure out the problem, even if you don't know the answer, will go a long way. But, like the others have said, don't BS at all. 
- ASP.NET has gone [open source](https://github.com/aspnet). Microsoft has really been making excellent strides in being more open - vNext is currently in development and will bring huge changes to the .NET ecosystem. [Example](http://whereslou.com/2014/05/28/asp-net-moving-parts-ibuilder/) - OWIN - Totally new (open source) compiler called [Roslyn](http://msdn.microsoft.com/en-us/vstudio/roslyn.aspx). Big enhancements here. - Visual Studio is on a much more frequent release schedule now. - [TypeScript](http://www.typescriptlang.org/), a strongly-typed superset of JavaScript. Very handy and gaining lots of traction. There's surely countless other exciting announcements but these are the ones that I've personally been most excited about. MS has really been stepping up their game recently.
Yeah, I had it convert one report first and tried running the web application. But it immediatly gave me a server error because I had two versions of ReportViewer.WebForm (9.0 and 11.0). So I went ahead and upgraded all of the reports and changed the ReportViewer.WebForms to 11.0 on each rdlc file. I also made sure that assemblies are version 11.0 for ReportViewer. Oh and also verified that web.config has the correct version (11.0). It's just odd to me because the web application runs, the reports work fine, but the debugger keeps prompting me about that line on all of the rdlc files.
ASP.NET going OS is great, but doesn't fundamentally change it (BTW, MVC was OS from the get-go, IIRC). Somebody else mentioned vNext. I'll take a look at it to see what kind of changes are there. OWIN? I haven't heard about this at all. I'll see how good my google-fu is. VS updates - I guess this might be a double edged sword. We'll see if they still maintain a fairly high quality bar. The UI isn't my ball of wax, so I'll take a brief look at TypeScript. But it's not something that I'll be interested in, probably. Thanks for the feedback. 
I've read a little bit about the new parallel library stuff. But I'll definitely look deeper into this. Thanks.
No. I want to see what db.SaveChanges does. When trying to debug save issues start at the lowest level, where you save to the DB. In there, if you can see your data using a visualizer in the IDE then you know the rest is working and it's either your SQL or something related to the SQL.
http://prntscr.com/430rsk &lt;- My project. http://prntscr.com/430ryp &lt;- The database. http://prntscr.com/430vfq &lt;- The database in ms SQL. http://prntscr.com/430sg4 &lt;- this happens when i try to post. http://prntscr.com/430t4u &lt;- apparently it's not even go in to the save, cause it's not "IsValid".. 
Fix the valid issue first. Also, do you have the source code for db.SaveChanges()?
You should spend some time cleaning up your code. You've got lots of extra white space, spelling mistakes, and non-consistent naming syntax. Assuming this is a school project you need to watch these things, because you'll lose marks, and in the real world you'll create something that can't be maintained. Look up Hungarian Notation. Discipline is the most important part of programming. Also for the love of SQL and all that is holy name 'middleTable' something meaningful.
This is just, learn to code on my own thing, before school starts so i maybe could do something without help all the time. And this is something i want to do.. at first i thought it would be easy but not so much. Yes i will clean up the code when i figure this out, and the "middleTable", any suggestions? the db.SaveChanges();, what do you mean by source code? 
He actually just needs to understand the Entity Framework in MVC and how changes are handled - here, look at this link http://stackoverflow.com/questions/22654785/what-is-db-savechanges-in-asp-mvc4 Asking him post the source code for db.SaveChanges doesn't make a lot of sense.
Soo.. ok, let's see if i understand you: I'll make 4 tables ? dbo.Class that has one to many dbo.Section(what should include in that table?) dbo.Section has one to many to dbo.ClassList (included, sectionId, StudentId ?) and dbo.ClassList has many to many dbo.Student ?
Put a break point in your post action and check what values student has before you save. If we know this we can halve the list of possible issues.
Basically yes. Class would be Computing 101 for example, Section could be Section 1 @ 3pm MWF, or something to do with when and where the class is offered. Class List would include the primary key for a section and a student, studentId and sectionId could be primary keys, but I'd use a row guid field, and have a more user friendly display Id's. Also your one to many relationships may be better as none to many. Class list is many to many with students, yes. A student could be part of several class lists. 
Nothing much jas changed, mainly incremental things in c#, mvc etc. If your writing desktop apps then no one has any idea on the direction at the moment. The one big (and ongoing) shift is the break up of .net as a monolithic platform. More and more is being moved to nuget packages. 
You should follow the MVC tutorials on Asp.net. The contoso university tut has been continually updated and the author is great about helping everyone who posts comments. I think this would help you have a nice consistent project to follow.
Just subtract DateTime startTime = DateTime.Now: ..something runs...; TimeSpan ts = DateTime.Now - startTime; Better is to use System.Diagnostics.Stopwatch though. 
Looking for MSBuild syntax... this would be a synch to do in C#.
I've experienced a very similar issue migrating a VS 2008 rdlc report to VS 2012. I *had* to let VS update my old rdlcs. I'm not sure if you're saying you've updated all your references to the new DLLs. All I can tell you is after letting VS update the rdlcs, I don't get errors like the ones you mention when trying to view the rdlc in design (and/or) XML view. All I can think is maybe you have an old dll version laying around in your project folders? Check "Copy Local?"
Wpf still has air space issue :*
I'll check again if there are any references that I may have missed.
Do you have access to Powershell? Maybe you could use an exec task, passing in your properties?
Okay, I ckecked the web.config file again and everything looks good. Went through the xml code on one of the reports and everything looks good there also. I created a new project with a report in it, and it works perfectly, no errors and no problem. Created a new report in the existing project and compiler comes back with the error for the new report. I'm just going to go ahead and see if I can keep the project at .NET 3.5 with ReportViewer 9.0.0.0.
Yeah, I don't understand exactly why they want me to upgrade the web application, but I will be asking and letting them know about the odd issues I have encountered. Good thing I have proof that spoke to the other programmers in the office about the issue. ~~Question for you. Will I be able to open the rdlc files without needing to convert?~~ EDIT: I just answered my own question. I did what you suggested and the web application compiles with no errors. But again, when I try to open an rdlc file in design view in Visual Studio, it prompts me to convert to RDLC 2008 format (The current format is 2005). When I convert to RDLC 2008 and compile the web application, the debugger gives me the following error message. Error 1 The report definition is not valid. Details: The report definition has an invalid target namespace 'http://schemas.microsoft.com/sqlserver/reporting/2008/01/reportdefinition' which cannot be upgraded. 
This will be decided when you create the table. If you mark your primary key column as IDENTITY(1, 1), then SQL Server will generate the value for you. In the example I used earlier, the ID column will start at 1 and increment by 1 on each subsequent INSERT. If you want to manually assign the primary key value, simply omit the IDENTITY property when you create the table.
And what if I delete a row and then insert a new row?
It will always continue where you left off. So if you insert two rows, delete one and insert a third, the new ID will be 3. If you'd like to fill in the gaps, you can implement something like this: http://technet.microsoft.com/en-us/library/aa933196%28v=sql.80%29.aspx (The script at the bottom right above the "See Also" section)
Looks like there are some useful topics here: Office 365, SQL Server, MDX, CRM, PowerShell. Thanks.
You are aware that its about a year since vs2013 was released?
- *CodeMap* was introduced in VS2012. - *JustMyCode* is already part since VS**2005**. - *Auto Brace Complete* is already part of god knows how long. Seriously, you mention a feature that is part since 2005 as a **new feature for 2013**? The quality of DotnetCodePress seems to be really low.
In that tutorial, the Id field was set as an identity. This is shown in the Initial migration file. ID = c.Int(nullable: false, identity: true) I BELIEVE that in this case the framework will decide what Id to give to the object. You could probably test this by injecting a high Id value into a Create call and see if it saves to the DB with your injected value. Additionally, "[w]hen a new entity is created, the Entity Framework defines temporary key and sets the IsTemporary property to true. When you call the SaveChanges method, the Entity Framework assigns a permanent key and sets the IsTemporary property to false."[1] This suggests that it doesn't matter what Id you give to your object, EF will overwrite it upon the SaveChanges method is called. I believe you can override this behavior through an override of the OnModelCreating method. I think what the tutor is trying to do is show how fields can be bound to a model and ensure that only fields that the developer wishes to be bound are bound. This can be useful if you have a more complex model that includes fields that you may not want a user to modify, for example fields that track an object creation timestamp or user field that shows who created the object. He could have, and probably should have, omitted the field and given a short explanation why but...it's a simple tutorial for the basics. [1]http://msdn.microsoft.com/en-us/library/vstudio/dd283139(v=vs.100).aspx
You are right about the first 2 but I think the auto brace thing was an add in. Productivity power tools I think. Actually, I think it says that in the article. 
Could be. But then, 2 of 5 is still a very bad ratio.. Especially considering that the one is already part of it since 2005.
Yes, you do. Professional is not enough.
I completely agree. The article is badly "researched" and poorly executed. 
while this is cool and all why is this in .net? other than the fact this explains how to run it on IIS and why you should switch to node from c# i guess, this really has nothing to do with .net 
OWIN, as the acronym implies, is an interface for web servers. It allows you to write your code and run it within multiple webservers. Right now, ASP.Net code isn't very portable between IIS, Self Hosting, or some other webserver. OWIN provides interfaces that can be implemented by any server so that your code is more "write once, run anywhere". The biggest benefit to me is the ability to integration test WebAPI code without IIS. We run on IIS in production, but if we want to set up a build server to spin up the API and run tests on it, we can self-host on the build server too.
Joey, I thought it was applicable to .NET developers whom visit this subreddit. From the looks of it, other's some-what agree looking at the point count. Nowhere am I telling people to switch to node, advocating anything about node. I'm merely demonstrating for those interested in blogging that are familiar with .NET/IIS setups, how to run the blogging platform if they wish. As stated in the post, I'm a predominant C# developer who got tired of dealing with a previous blogging solution and migrated to Ghost. Perhaps you can suggest a better subreddit sir?
I'm trying to tell what you're accomplishing here over the standard Lazy&lt;T&gt; class. It appears that your implementation prevents the developer from having to call .Value on it to get it loaded and access the lazily loaded instance. My usual paradigm is: Lazy&lt;foo&gt; _foo = new Lazy&lt;foo&gt;(() =&gt; { // long running operaton return new foo(); }); // accessor for the the lazily loaded foo instance foo Foo { get { return _foo.Value; } } And from then on, I just use this.Foo which hides the fact that it's a lazily loaded single instance. Forgive me, but what is your implementation doing that's better? Or am I missing the point entirely?
&gt; I'm trying to tell what you're accomplishing here over the standard Lazy&lt;T&gt; class. Functionally speaking, nothing in particular. My implementation only works on virtual members or interfaces, but other than that, it aims to be a transparent version of Lazy&lt;T&gt;. &gt;It appears that your implementation prevents the developer from having to call .Value on it to get it loaded and access the lazily loaded instance. Basically, yes. I'm not saying there's anything wrong with the example you posted, but I personally feel that it is more of a work-around than a "clean" solution. I mean if we were to use Lazy&lt;Foo&gt; in a collection, it would be certainly possible to apply your pattern in some way or another, but I think the result would be somewhat messy. With my implementation you don't have to do any unpacking or make any work-arounds. On the flip-side, my implementation is pretty useless if the lazy subject is an external component that does not have virtual members or a describing interface. If anything, this was more of a learning exercise for me; I had never worked with `System.Reflection.Emit` and CIL before.
 private Lazy&lt;T&gt; _lazyInstance = new Lazy&lt;T&gt;(); public T Instance { get { return _lazyInstance.Value; } } There is nothing messy about this.
vNext is the soon to be new hotness so a complete rebuild and open sourcing of .net is on the way 
answer the best you can but don't lie if you don't know it is a junior position the should be some expectation that there be things you need to learn one of the questions we ask all of our developers is if they have a Stack Overflow account and if they use it regularly. this demonstrates that you know where to go if you you don't know something to get the answers, also we have question about where people go other than SO to find answers 
The masterpage is simple html with a few extra tags. ANY, and I mean ANY html page can be made into a master page with almost zero work. So little work, I don't think there are really "templates" for masterpages. I wouldn't let you boss see this post, is this your first time using ASP?
Not exactly. I was looking for more with a master page project with page stacks...I.e. previous page next page and all. 
Aight. Thanks for your feedback man. Appreciate it!!
in case anyone has the same issue, the solution was to manually edit the generated XML, making sure that the in arguments were set correctly. The designer was setting the arguments as null or what it should be, seemingly by rolling a dice and multiplying that by the voltage of the cpu I couldnt figure out why the designer generated bad XML (it seemed to change its mind whenever i would add or remove parts to an activity), but there you go.
Take a look at bootstrap. This is where I usually start for my page templates. http://getbootstrap.com/getting-started/ 
Yeah for Line of Business apps bootstrap is awesome. I would spend some time by yourself figuring out how bootstrap and masterpages work. Maybe take one of the default bootstrap templates and convert it to a small asp.net website.
Being a .NET guy wanting to move my blog to Ghost. I think you good sir!
I guess no work today.
Turns out the issue was my "C:\Program Files (x86)\MSBuild\Microsoft\VisualStudio\v10.0\ReportingServices\Microsoft.ReportingServices.targets" file had somehow changed. The top of my file was: &lt;Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;UsingTask TaskName="Microsoft.Reporting.RdlCompile" AssemblyName="Microsoft.ReportViewer.Common, Version=9.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a"/&gt; And it should have been: &lt;Project xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;UsingTask TaskName="Microsoft.Reporting.RdlCompile" AssemblyName="Microsoft.ReportViewer.WebForms, Version=10.0.0.0, Culture=neutral, PublicKeyToken=b03f5f7f11d50a3a"/&gt;
If it's a stored procedure, you can run it directly from linq. Just pull it into the dbml and it'll become available. This can also be accomplished by doing joins in the linq query such as this: http://msdn.microsoft.com/en-us/library/bb311040.aspx You can also run a query without converting it into linq syntax by using executequery on your data context, seen here http://msdn.microsoft.com/en-us/library/bb361109(v=vs.110).aspx
I checked it and it is correct. So that is not the issue. This is a frustrating issue.
I'm on my phone so can't link, but you can use futures in EF Extended.
To answer your direct question, no neither LINQ to SQL nor Entity Framework can do multiple result sets from a LINQ query, although as /u/zers mentioned, you can shoe-horn it with stored procedures. That having been said, you may be overly worried about performance and in the process solutioning the wrong way. Pulling multiple result sets is not going to give you anything other than a *marginal* performance boost, which can be easily offset with better architectural decisions. I would suggest you stick to simpler constructions until you *know* you need to fine-tune your DB performance. 
This is a good point. Worry about performance when it becomes a worry.
It's also a wrong point. For small, frequent queries round trips are a performance killer. And since it is something that drags down overall network performance you can't "just optimize the slow queries".
Careful with that advice. Limiting the number of queries doesn't help if you balloon the size of the result set via imprudent joins. See fat parent-thin child.
You sure you created a WebAPI project? The default routing for the latest version of Web API looks like: config.Routes.MapHttpRoute( name: "DefaultApi", routeTemplate: "api/{controller}/{id}", defaults: new { id = RouteParameter.Optional } ); EDIT: Whoops, submitted early. The routing works basically the same way, only WebAPI will infer the method to be called via the HTTP verb that is included in the beginning of the method name (or the method attributes if your names don't math the default convention). Also, one thing to be aware of is that Web API was really only designed to have a max of 5 methods per controller (well, 5 that can be called from the API), one for each verb. If you find yourself needing more, then you probably need to reevaluate your application design.
geez, sorry, I was playing around with Area's and copy/pasted from the wrong thing. I've updated what I'm seeing. It's VS 2013, and I'm creating an MVC project and choosing the 'Web Api' project template. Is there some other way to do this?
Nope, that's the way I always do it, and that was how I got the aforementioned code snippet. See my edit above. The code you posted looks like regular MVC routing, though the only .Net stuff I do these days is Win Phone and Web API stuff, so I don't remember.
I've uploaded a series of images to show how I'm creating the project, and what I'm seeing. http://imgur.com/Uypq2Rq,omc8lJb,9Ar0zSs,ofDQ7cK I don't understand the interactions between the WebAPI routing and the MVC routing. When a request comes in, how does it decide to do 1 vs the other, and how doe Area's interact with this?
If small frequent queries are killing performance then kill the small frequent queries and do more set based operations. If the two items you are pulling from a DB don't have any logical DB relation to which you can pull them both into one set then the two items should not be operated on in the same process. 
If I'm assembling a web pages that needs the results of three queries what do you suggest I do? Perform a UNION ALL across the three sets with lots of nulls to account for the fact few if any columns are shared?
I prefer to think of it in terms of not writing slow queries in the first place. Most performance problems I see come from simply doing too much work in the first place. Too many round trips, too many columns, too many separate batches/transactions, etc. It's like saying "stop making unnecessary copies of a read only list" a premature optimization.
Seems like we're on the same page.
Should I measure this before 'optimizing' it? List&lt;x&gt; x = ... var y = x.ToList(); return y.Contains(z); Yes, I have seen this in real code.
That's not optimization. That's fixing bad code. 
It's a question of convention over configuration. When the routing engine matches the URL to a route in the routing table (RouteConfig and WebApiConfig both add routes to the same table), if it has a controller and an action to look for (including from a default route), it looks for an MVC controller named FooController. When it has just a controller but NO action, it looks for an ApiController named FooController. Note that namespace is actually unimportant, so I usually keep my ApiControllers in a sub-folder (and sub-namespace) controllers/api. That way, I can have both a Foo MVC controller and a Foo API controller. Regarding the second part of your question, the AreaRegistration file just adds MORE routes to the same routing table, but, like the API routes, they start with a literal string. Again, what determines what kind of controller gets used is just a matter of the class name (matching the Controller parameter of the route) and the type (e.g., whether there's an Action present or not). I know that's kind of abstract. Does that help?
Perhaps I'm using the wrong terminology out of ignorance. I have a Web API project, which is presumably ASP.Net I believe, but it has configurations for both MVC routing and the newer RESTful routing. My question is how they interact. Does the RESTful routing get resolved first, and then the MVC routing is resolved if nothing caught on the RESTful routing, or vice versa? I get routing in general, and convention over configuration. My question was specifically about how they interact in this instance. What determines who wins if a RESTful route and an MVC route both match the request?
I feel like there's been a misunderstanding of my question, I've tried to clarify it here: http://www.reddit.com/r/dotnet/comments/2b2sio/im_confused_by_how_aspnet_mvc_routing_works/cj1eh2y
so it attempts to match the MVC routes first? Part of my confusion is that while you can look and clearly see the order that the MVC routes are defined, or the order that the WEbAPI routes are defined, you can't see which set of routes is added first (or I should say it isn't obvious to me). so while they're all added to the same routing table, which gets added first?
Look in Global.asax. You have one RouteConfig class for MVC and one WebApiConfig class for WebApi but the code in Global.asax will show you which one is registered first. Either way they shouldn't overlap, at least not with the default config. Typically, the route that's registered first wins, but there are also times where "ambiguous routes" are detected and it just throws an error.
So is all optimization. It is a cancer of the .NET community that everyone assures each other to worry about performance last. This is terrible advice for any large project, as the performance fixes you identify after things are implemented are oftentimes impossible to achieve without fundamental rewrites. That might be good for job security, but it is bad engineering. Performance is a feature of your code. It directly effects how composable it is, in much the same way that SOLID principles do. The effects of algorithmic complexity can be planned for, and don't have to be measured retroactively. No developer should fail to test their code in all its dimensions, but that doesn't exonerate you from learning your tools and trying to get it right the first time. Also, remember that the OP started this thread to ask about performance. Let's do him the credit of assuming he bothered to do so because he was already pretty sure that his performance needed help.
Thank you, that's exactly what I was looking for :)
gotcha, that helps me a lot, thanks.
&gt;Also, one thing to be aware of is that Web API was really only designed to have a max of 5 methods per controller (well, 5 that can be called from the API), one for each verb. If you find yourself needing more, then you probably need to reevaluate your application design. Do you have a citation for that? (I believe you but I need stronger proof for my TL)
That's how I feel about a lot of these database access issues.
Hi! Looks like you've got yourself going, but I thought I'd bump this to introduce some other options, just in case you haven't been introduced to function properties and inline tasks, which will potentially save you a lot of time if you're using MSBuild a lot. Because there's quite a bit to cover, and in the hope I might be able to help others, I've blogged about this: http://www.davidmoore.info/blog/2014/07/19/using-msbuild-property-functions-and-inline-tasks-example-doing-performance-calculations/ The TLDR is: * Use Stopwatch instead of DateTime when benchmarking in the future, if possible. In this case, DateTime might be fine for precision, and easier to use from MSBuild. * Gist of the two options I posted is here: https://gist.github.com/DavidMoore/3bcc180796c116f55a2b * You can use **property functions** in MSBuild 4 to use .NET types like DateTime, string etc as well as do subtractions. More here: http://msdn.microsoft.com/en-us/library/dd633440.aspx * You can write a **C# inline task** in your MSBuild project file itself, without having to a custom task assembly. This keeps your project self-contained while still being able to use C#, and not have to worry about the headaches of building and versioning and linking in a custom assembly. More here: http://msdn.microsoft.com/en-us/library/dd722601.aspx
&gt; It is a cancer of the .NET community that everyone assures each other to worry about performance last. This is terrible advice for any large project, as the performance fixes you identify after things are implemented are oftentimes impossible to achieve without fundamental rewrites. If your architecture is solid you shouldn't ever be looking at a fundamental rewrite. Even if you've chosen the wrong tool for the job the proper abstractions should be in place that lets you swap that tool out without major changes to the rest of the system. &gt; The effects of algorithmic complexity can be planned for, and don't have to be measured retroactively. No developer should fail to test their code in all its dimensions, but that doesn't exonerate you from learning your tools and trying to get it right the first time. Sure, but we're not talking about algorithmic complexity. We're giving advice to a developer who is trying to cram multiple result sets into a query, making his code harder to understand and harder to maintain, so the first question we have to ask is "Is this solution worse than the problem you think you have, and is the problem you think this will solve even real?" If he's building an application that will be used by the 12 guys over in sales who will be running that set of queries once a day, he's just wasting time scratching an itch. 
Looks awesome! Might want to migrate to the new nuget package restore model though :-) http://docs.nuget.org/docs/workflows/migrating-to-automatic-package-restore
I read an article, I think on MSDN or an official MS blog a couple years back, but I can't find it (I'll edit if I do). For anyone who might question my claim, I want you to think about that logically for a second. Why would any well designed application provide more than one of the same HTTP verb for the same endpoint? The only time I stray from this is in the case of something like class UsersController { public List&lt;User&gt; getUsers(); public User getUser(int userId); } at which point it makes complete sense and provides an intuitive experience for the person consuming those endpoints. http://yourpage/api/users - returns all users http://yourpage/api/users/{id} - returns one user But trying to provide multiple methods, outside of the aforementioned convention, that take in a variety of arguments, who might have multiple return types? That, to me, would be poor application design and probably means that your controller is doing too much. **edit:** and to get that to work, you'd have to get creative with your routing, which is not something I am particularly fond of if I don't have to do it.
I agree, but I am dealing with a problem child who won't listen unless someone beats him over the head with a manual.
Done and committed, thanks for the info.
What's the difference between the two?
vnext rewrites the rulebook if you ask me.
Ok. If I look up c# tuts on msdn, will they also be relevant to asp.net? 
Go to http://www.asp.net/ and select the Learn button at the top. From there you can select WebForms and get a solid overview of how they work. 
Don't be afraid to say it. Web forms still has a place in greenfield dev, that place is just getting substantially smaller.
Seconding www.asp.net. w3 schools is not going to be a good place to learn. Get visual studio, run through the tutorials on www.asp.net, you should be fine.
Your api calls should use a different base url then the rest of your site, default is api/
How much of those details are necessary for accessing content from a sql server database and displaying it? I ask because my task for the project boils down to that. So far I managed to connect to a server, but I'm not sure of how to access the data and from where (aspx or cs code)...
Will reading about mvc be relevant to a project based on webforms? 
Please don't put business logic in code behind cs files... Keep those things as thin as possible and just have them wire events and call out to a business layer.
No. They're "competing technologies" more or less.
If you have the luxury of taking a bit more time, do the MVC tutorials you can easily find online. If you need to get up and running quickly, go the WebForms (the "traditional" flavor of ASP.NET) route.
Another comment said that MVC does not work the same as "vanilla" webforms. Are there any tutorials or resources on ado.net besides the msdn ones you can suggest? 
I still wouldn't, at a point you're going to start hitting compatibility issues. Even Telerik has stopped making controls for web forms, they don't even list them on their main page any more. Plus I just hate the fact that it's designed to be WYSIWYG oriented, because what I see is NEVER what I get, especially with Microsoft products (have you tried doing any layout work with Office Online? It's terrible!)
For real. And with things like asp.net, Microsoft put a bunch of effort into making their learning materials solid, why go through it in a shitty format like w3? It sucks that they still have top results for tons of things.
According to Microsoft webforms is still upwards of 70% of development in asp.net
That wouldn't surprise me. It seems that it's only been recently that asp.net has been an attractive platform for smaller, startup-like companies. More attractive I should say, but not quite as sexy as the MEAN stack, or whatever will be the next killing framework in 2015. So it feel like most asp.net development are at bigger companies, and more along the lines of web forms fitting well into an already completely Microsoft ecosystem. Just speculation from my POV, I learned asp.net via web forms right when MVC 3 came out, hopped on MVC as soon as I got a better grasp on front end dev, now I really only do JavaScript and a little C# when I have to. Been a fun path so far.
An aspx is the html side of a web form and each aspx has its own c# code behind file. In the code behind you'll be able to manipulate all the controls in your aspx and capture events like button clicks, drop down selection changes and page loads. Then you can put in your code to execute during those events. For a hello world, just have a label control display "clicked" by setting its text property on a button click handler. To create the handler, drop the button control on an aspx and double click it in the designer. That ought to get you started.
In general nuget packages are just zip files. Any zip extractor/viewer lets you do the same thing
Actually, they're not "just zip files". They're zip files following the [Open Packaging Conventions](https://en.wikipedia.org/wiki/Open_Packaging_Conventions). Also, a zip program may be good enough to examine the contents, but to create a package you need to follow the above OPC. And I doubt that you'd be able to publish from your zip app of choice.
I was surprised how easy it is to get my own NuGet server up and running and packaging internal libraries. NuGet Package Explorer certainly helps.
No but nuget command line is the main driver here. You can also pack a csproj automatically without a muspec. I'm not deriding the tool, but it should be mentioned that the GUI tool isn't the only way to do this 
Are you actually using a server or just a file share?
Well, I wish I could not learn it and code with it then because it sounds like a ginormous waste of time. Unfortunately the software dev firm I'm working with is developing with it and so I have no choice =\ Any "shortcuts" (where shortcuts is very loosely defined to resemble tips or resources) you can pass on for accessing simple databases and reading/storing data? 
An actual Nuget server. That's basically creating a new empty ASP.Net application, dropping the Nuget server package via Nuget and putting that up on a IIS server. Real easy thanks to the documentation on nuget.org.
What do you feel it offers over a file share?
We use it at work as the CMS we mostly use is webforms. I did try and use a MVC one (Orchard) but as it's maturity couldn't match DNN the designer/CMS guy got pissed off and I'll probably port it over to WebAPI with DNN later. 
We only host in-house packages right now, but I can see that being useful if we go to an "approved list" model. I wonder if there is a way to automatically sync a local NuGet server with the offical one for a pre-determined list of packages.
&gt; The primary advantage you get with NuGet.Server is Oh wow, I didn't realize that they made it that easy to setup. Maybe I'll look into it after all.
There's an extension for NuGet.exe that might help you it. I don't think it's published, but you could pick it up from the CI - https://nugetbuild.cloudapp.net/repository/download/Client_VisualStudio_NuGetForVS2013/3234:id/NuGet.ServerExtensions.dll. Essentially you drop the dll next to nuget.exe and run `nuget.exe mirror -Source http://nuget.org/api/v2/ path-to-packages.config http://my-server/api/v2/ http:/my-server` and it should sync packages for you. You can figure out what it does from https://nuget.codeplex.com/SourceControl/latest#src/CommandLine.ServerExtensions/MirrorCommand.cs. By the way, MyGet has a way to mirror packages (and it's dependencies for you). 
I'm a huge Umbraco guy. It does both mvc and webforms. It's a lot of work to get going but its got a lot of power behind it. Open source too.
You'll need to learn a decent installer system, I'd recommend the nullsoft installer, it's configurable enough that if it can be automated then the installer can handle it. I'm not sure what elements on your list can and can't be automated, you'll have to research each individually. For the database you should be using something like fluent migrator, otherwise your updates will break every other release. It's important to nail your branching/merging strategy as well as your build/deploy/package system. edit - Blogspam: [A database migration strategy that works in this situation](http://proactivelylazy.blogspot.com.au/2011/05/practical-msbuild-database-migrations.html), [Some general pointer on your build system](http://proactivelylazy.blogspot.com.au/2011/06/practical-ms-build-avoiding-spaghetti.html)
Awesome. Will reinvestgate. 
Write adequate but poor documentation and then charge a service fee for installation when nobody can figure it out. Works for Oracle and IBM. ;)
You might be able to run the website in a service using Owin/Katana, you could also install the database as an sql express instance which can be installed quietly from the command line.
All of that and more can be accomplished with [wix](http://wixtoolset.org/). 
Hey cool thanks, looks good I give them a go. Had a quick scan and they seem like there are the sort of topics that would provide a good foundation into the area.
I have used MembershipReboot (as well as ASP.NET Membership &amp; Identity 2.0), and found it slightly less frustrating than Microsoft's libraries. IdentityReboot is new to me, but looks like more of the same. My experience with these newer tools has largely been frustration and annoyance. Whenever I needed to customize something (to implement typical asinine/complex/stupid requirements) I found that I ended up writing far more code than I would have if I had just built a solution myself. The only experience I had that was positive was with Identity 2.0 and a toy application I made that worked with Google+ and Facebook because, well, I just had to hit a few buttons and it magically worked. For all my professional (B2B/B2C) work, my opinion is to build it yourself. Prior to Identity, I used the old membership providers for this (two classes + two methods and you had authentication &amp; authorization working). Recently I jumped over to Identity, primarily to modernize my applications against Owin. It was a bad time until I found this article: http://www.khalidabuhakmeh.com/asp-net-mvc-5-authentication-breakdown-part-deux. Gives you the bare minimum to set up auth with Identity. Combined with http://www.fluentsecurity.net/getting-started, I have a good starting stack. The rest of the "free" stuff you get with Identity is pretty trivial to implement: confirmation, lockout, password reset, password generation... I have not had to implement two-factor personally, but it seems OK: http://www.codeproject.com/Articles/403355/Implementing-Two-Factor-Authentication-in-ASP-NET Anyway, food for thought. Try them all and see which one fills you with the least amount of suicidal tendencies and meets your business &amp; technical requirements.
While there is a lot of web forms hate on here there are 2 key advantages in my experience. 1) Rapid dev speed (providing you don't get too complicated and start pushing against web forms natural capabilities) 2) complex controls/components available that are often used in business applications such as Telerik/Dev Express libraries. We use a mixture of techs depending on the requirement because neither Mvc or Web Forms is a one size fits all. You can also look at doing just the back end in asp.net/c# and the front in a JS framework like angular. Your choice is largely governed by your teams choice to allow you to pick a technology and the suitability of your project to certain frameworks. Web forms is definitely still alive and kicking and being upgraded by MS, I highly doubt they have any intention of dropping it any time soon. 
Probably says more about your workplace than the industry as a whole...
Is it meant to be rapid for experienced Microsoft people or for "everyone" (who is a developer) in general? The reason why I ask is that, as a newbie in all dev things Microsoft and mildly-savvy in semantic web/LAMP stack things, what would seem trivial for me to create in the latter is taking me a long time just to figure out with the former.. =\
I've found this to be representative of the industry at large unfortunately.
So what they did, is a love child of Grunt and Maven and called it a day?
Last time I looked at wix it was horrible. Over engineered, largely undocumented and a pain to configure for multiple environments. 
I've used wix, install shield, and plain ol' windows installer package and deploy project, and wix ended up being the best. None of them are very well documented as they all rely on the poorly document windows installer api. With that said, the op might want to look into newer technologies like [chocolatey](http://chocolatey.org/)
Ok, I agree wix is better than thise two, but that isn't saying much. An installer needs a real programming/scripting environment. Haven't tried choclatey yet though.
Well, since MVC is the "newer" platform, most of its tutorials will use Entity Framework or other newer database access techniques. Look for an older Web Forms tutorial (maybe something like [this](http://www.asp.net/web-forms/videos/aspnet-35)) if you want to learn more about hands-on ADO.NET. FWIW, even though I'm using MVC more and more, I still prefer to use .NET database objects (instead of ORMs and stuff).
IdentityServer is more interesting as you can move your authentication and authorization out of your applications. The WS-Trust/WS-Federation in v2 is quite nice and the OpenID Connect / OAuth2 for v3 is coming along quite well.
Provided you are dropping buttons and textboxes onto a page it's really pretty easy, no more difficult than winforms, do your data loading on the onload event and you should be right. MVC has more plumbing for easy stuff which is why they have the wizards in the first place. I personally prefer an MVVM style when using web forms, on its own it's very very prone to bad coding (which is why it gets all the hate), but no more than a c# winform.
I've worked with a wide array of devs and I've never come across any who didn't understand basic data types :-S I've got to belt unit testing and SOLID principles into seniors from time to time, but that's more laziness than ignorance.
Why this when there is the rather mature and rich [PSake](https://github.com/psake/psake)?
@codekaizen psbuild is not an alternative to msbuild like psake is. Instead it makes calling, and manipulating msbuild files, from PowerShell easier. For the manipulation piece, the idea behind that is that NuGet pkg authors can use psbuild to simplify modifying project files.
Thanks for the tips. I'm not sure I'm interested in rolling my own but it doesn't sound like a horrible idea. When you're creating your own auth systems, what library are you using for hashing and salting passwords?
PBKDF2. You can use SimpleCrypto.NET, but I would instead just recommend pulling his code and simplifying it into a helper class for your needs: https://github.com/shawnmclean/SimpleCrypto.net/blob/master/src/PBKDF2.cs
&gt; Who’s out there building a lightweight MMO in .NET? Minecraft did it with Java! [Blockscape](http://www.blockscape.com/v2/) is written in C#.
I said this exact comment to the guys in Jabbr a few weeks ago and they were so offended, as if I was attacking them personally. I agree with all of your sentiments.
This comes across as whiny rant to me. 
As a veteran of one of the top 5 trafficked .net platform based sites on the internet; when we created something bespoke for a need we didn't open source it. It became a trade secret. We didn't talk about it or blog about it; it was ours. Judging the .net community by looking at open source projects is like judging the value of Linux based on what you're paying for it. &gt; Do .NET developers really solve any hard problems? Yes, yes we do. We just keep the solutions internal; especially when the solution needs to scale. I have a patent, which was the only time my technique has been publicized. That's the culture.
But is that bad? My company earns its money by selling licenses for the .NET tools I wrote/write, why would I give away the 1million+ lines of code as OSS and with that also my source of income to pay the bills? It's good that people release code as OSS (I have done so too, a lot), I just think that the focus on 'it has to be OSS' is silly: most people who cry for X to be OSS never contribute a line of code. If you don't contribute, wanting something to be OSS has no meaning: you can use reflector to read the code, you don't need a github repo for that. 
I don't see it as bad at all. 
&gt; MS has always tried to kill popular .net OSS tools by providing there own, usually wirse offering. I don't believe that to be even remotely true. While Microsoft certainly did kill off other projects, it wasn't their intention but rather a side-effect of trying to meet customer demands.
Partly yes, but I do agree with some parts. The quality on CodeProject is lacking from what I've seen and I just feel like the general C#/.NET landscape isn't as comfortable as it could be. All the PHP/Python/Javascript goodies are mostly on Github and it's way easier to find stable or even actively maintained libraries on there. With .NET a lot of the time you're cobbling together copy pasta from years old forum posts or trying to get that library to work but it's eventually easier to write your own.
The Microsoft seal on developer tool means it was developed the Microsoft way. There was a thought put into edge cases. It's an exhaustive solution. It's made by (or at least overseen by) people that know the platform inside and out. They tested the hell out of it. It runs fast and works with all of the other Microsoft stuff. In small part, a billion dollar company stakes a bit of it's brand on it. I'm willing to risk integrating anything that's a production Microsoft product. I value the Microsoft seal. The "some dudes wrote this" seal is less valuable to me. I don't think you can use the term reinvent the wheel on this one. The wheel is a simple machine. I'd accept "reinvent the nuclear reactor" or "reinvent the passenger airplane".
Let's take them one-by-one. ## Nant Had to be done. .NET desperately needed a common build system that Visual Studio could natively understand. As a result, all IDEs that work with .NET share a common project and solution model. This is a lesson Java still hasn't learned. Without integration between the build scripts and the IDE every project is a royal pain in the ass to setup. ## nhibernate ORMs are for kids working on pet projects. You shouldn't be using them in any professional setting where performance or security matters. The fact that one especially crappy ORM was replaced with a slightly crappier ORM doesn't change that fact. ## monorail I don't know enough to comment on this one. ## eveey IOC container I find it fitting that you didn't bother naming any because everyone and their brother creates a new IOC container on a weekly basis. Generally speaking it is a unnecessary abstraction. And even when it does make sense, rolling a custom solution is much faster than trying to configure some overly generic library. &gt; Sure they didn't kill these projects, but they did everything in their power to discourage the OSS community on .net. Show me one instance where they said "don't use X" where X is a .NET open source project.
That's the theory at least. In practice... ## MSBuild Great integration with VS. Damn near impossible to work with when you just need to do basic operations like copy the output of a build. ## Entity Framework. Bloated and slow internally. Generates the same crappy SQL we see in other ORMs. ## ASP.NET MVC Ok, this one seems to have been well done. And the rewrite to drop System.Web should fix the lingering performance issues. ## eveey IOC container We call MEF the "Randomly Crash Silverlight library". I've never had a project where it wasn't ripped out after a year of pain.
Lol codeproject. What a pile of dogshit. I get search results that include code project too. They're in the same bucket as EE: literally worthless. That highlights a different problem: both CodePlex and GitHub aren't indexed by Google. The reason we feel underwhelmed about the quality of C# and .NET code on display on the web is that those shitty sites are well SEO'd, while the quality sites are invisible. I contribute a lot on GitHub with net languages. It's unfortunately not indexed by Google. It's got its own search though. StackOverflow obviously doing a great service to highlight answers to problems as well.
Oh I agree. That's definitely the direction Microsoft needs to go for many of its open source projects.
Thanks for the link, was looking for something like this. Just bought it.
&gt; The Microsoft seal on developer tool means it was developed the Microsoft way. There was a thought put into edge cases. It's an exhaustive solution. Your joking right? The guys that build these libraries build libraries for a living, they hqve no idea what problems they are trying to solve. It usually doesn't handle edge cases at all and it almost always does a fraction of what the competitors do. OSS libraries are built by real world usage, they are battle tested long before they get to that 1.0 milestone. nHibernate vs EF is a great case in point. EF is still catching up to where nh was a decade ago. Edit - your attitude seems pretty typical of .net devs. Just using the MS solution, not thinking outside the box. This is why OSS doesn't flourish on .net.
The good news is that part of Microsoft won the war -- MVC came out of Scott Guthrie's shop and he reported up through Satya. So there is hope. 
Quite a lot of those new features in EF6 where in nh before EF existed, which kind of proves the point. By the main point is that even if EF will/has overtaken nh, was it worth the effort? Could MS have not put the same resources into improving nh instead of taking a decade to reach parity?
This is not an argument, this is a real question. What's the point of MEF? Was there ever a situation where IoC saved the day? Has it made your life easier? It feels like unnecessary complexity to me, but having never used it I can't comment. I'm more than willing to change this opinion. I use reactive framework often to do something somewhat similar to how inversion of control is described. Crappy SQL statements: ORM is ORM. Crappy dynamic SQL is what they do. I liked Linq-to-SQL for three day project type internal tools, but 90% of what I had it doing was calling stored procs. Is there an ORM that allows multiple tables to be bound to single objects (for editing) yet? I remember that being annoying. Even asp.net web forms performs well when you know how to use it effectively. I have proof. I dunno on MSBuild. I've never needed to use it directly.
Yes, I would be more willing to empathize with the overall point if it didn't.
Entity framework had it's 1.0 release in 2008. It's also since been open sourced. I am having a hard time finding the initial public release of nHibernate, but it looks like it was certainly post 2005. Your numbers are a little weird. As far as your main point is concerned; it looks like they felt it was worth the effort. I can think of a few reasons why, but there's no point in enumerating them here. Between the two options, if I absolutely needed one, I'd still go with EF. 
Most OSS libraries I've found seem to have died due to the maintainers not working with the community. I've seen so many projects on CodePlex with numerous pull requests pending and you can see the community wants to contribute, but the maintainer doesn't care. Then the maintainer burns out or losses interest and thats the end. 
&gt; Entity framework had it's 1.0 release in 2008. It's also since been open sourced. Which means they started working on it at leaat in 2006/7, perhaps earlier. So a decade isn't far off. As for the rest, have you actually used any of the OSS tools I've mentioned. Or git? Or other languages eco systems? 
&gt; What's the point of MEF? It was designed to compose plugins for giant applications like Visual Studio. In my opinion you shouldn't consider using it unless you answer "dozens" to the question, "How many third parties are writing plugins for your program?" And even then you can probably get away with just using Reflection to dynamically load assemblies. &gt; Was there ever a situation where IoC saved the day? Inversion of Control is a useful technique to alter dependency chains. Inversion of Control Containers are for people who like to make everything unnecessarily complicated for everyone else. I has always made my job harder in every way from writing code to debugging to managing dependencies. 
&gt; I dunno on MSBuild. I've never needed to use it directly. MSBuild is fairly powerful once you learn the patterns. But the damn patterns are hard to understand. I need to find a book on it...
&gt; Is there an ORM that allows multiple tables to be bound to single objects (for editing) yet? Not that I know of. Hell, I would be happy with one that would allow the editing of a subset of the columns from one table without jumping through a bunch of hoops. 
That's actually one of the strengths of OSS. Projects get born and die off all the time. The eco system as a whole evolves. It's not one company deciding the one true way forward. 
&gt; Was there ever a situation where IoC saved the day? I am the main developer of a project at work that is a plugin-oriented tool for setting up management solutions for client-facing applications. It uses a IoC container to set up dependencies. We have saved so much work by using it, and now we consistently have a single point for every dependency. Changing from one authentication scheme to another is a matter of adding a nuget package. I did so not long ago on a project. It was a fork of another project to save time. The first project used a authentication and authorization scheme that didn't fit the new project however, but changing this around was a two-step affair: remove the old nuget package and replace it with the other. Everything worked automagically, and this was due to the plugin architecture and IoC. The project doesn't care how a user has been authenticated. It just needs to know if the user is authorized or not. A project doesn't need to know where the database is located or what platform it runs on. It needs a repository to pull and persist data with. This way the database can be substituted, or more importantly: it can always be faked which is useful when testing your application. So, yes. I have encountered several situations where IoC saved the day. It's just easy to miss them if you're not using a IoC container. Projects that doesn't use a IoC container tends to (in my experience) end up with factory classes littering the project as well.
No. I've written custom solutions that overlap with a couple of them though. I think the scale of our work may be different.
This makes total sense. I can see where it can really help in your case.
True! Github and StackOverflow are great and it's not all bad :)
If you haven't actually used these tools then your arguing from a position of ignorance. Your in no position to say which is better.
I never claimed to be.
You said &gt; The Microsoft seal on developer tool means it was developed the Microsoft way. There was a thought put into edge cases. It's an exhaustive solution. It's made by (or at least overseen by) people that know the platform inside and out. They tested the hell out of it. It runs fast and works with all of the other Microsoft stuff. And you really have nothing to base that on unless your familiar with other tools. MS could be the best or the worst and you would have no idea. 
Those are pretty common cases. Another oner is substituting authentication on a developer machine so they can just turn off production concerns. 
&gt; I value the Microsoft seal. &gt; The "some dudes wrote this" seal is less valuable to me.
You lost me on that one. That just sounds dangerous.
honestly, search in the nuget repository for libraries, not blindly google. 
Taking this to private message.
My point was that most of the interesting OSS projects didn't fail because the community didn't like them (judging from forks and pull requests). The issue seemed to be the maintainers not accepting input from the community. If bug fixes and new features from the community are ignored, the community will loose interest.
Around and kicking. They recently enabled its use in portable libraries and universal apps.
I'd rather mock out my dependencies during development than accidentally impact production or somebody else's testing, which is the real danger to me. IoC containers make changing dependencies easier, and enable changing them programmatically. However, the top benefit of an IoC container is object lifecycle management, not dependency injection. A container can make sure you get a new object every time, make sure you get a single, one for each user, each web request, or whatever criteria makes sense for you, all without burdening your classes with that additional responsibility. They can enable good OO design when those are significant concerns.
Can't you just have the database as a local db? Would that make installing sql server express unnessasary? Maybe that'll at least make part of the process easier. 
Check VS settings for ASPX file type. There should be settings that control reformatting on enter press. Or if you have Resharper, it has it's own auto reformatting settings. 
That's because jQuery is even more dynamically typed than plain javascript. Seriously: perhaps regular intellisense is interfering with your code and upgrading to VS2008 SP1 + JQuery hotfix could help with it. http://blogs.msdn.com/b/webdev/archive/2008/10/28/rich-intellisense-for-jquery.aspx http://blogs.msdn.com/b/webdev/archive/2008/11/07/hotfix-to-enable-vsdoc-js-intellisense-doc-files-is-now-available.aspx 
config data would normally go in the root level one, the view one is for stuff specific to that level such as registering namespaces to use directly in views (amongst other things)
Side note: check out MailGun if you are sending email a lot. 
As a more general answer, a web.config can exist at any level in your site hierarchy and contains settings that are applied at that level and below. You can use this to override settings for a particular branch, etc. In this specific case, the one under Views overrides the base security settings to prevent someone from accessing your views directly.
Thanks!
Thanks!
Thanks!
I already had installed the hotfix and the appropriate vsdoc.js and did all the tricks on the internet I could find but the code still jumps around and still no intellisense. Thanks though.
The 2nd one exists to do two things: * keep people from requesting your views directly, without it you could get http://www.example.com/views/home/index.cshtml without hitting the controller. * wire up visual studio to properly do some things with views and make things work with visual studio. This was more of a thing when one was using .ASPX templating versus razor though.
Actually yes on the ORM question. I wrote one a while ago that lets me do multiple tables from multiple data sources even. It's a part of the [utility library](https://github.com/JaCraig/Craig-s-Utility-Library) that I maintain. But no one really uses it other than myself, that I'm aware of, and it currently has limitations (SQL Server only since that's all that I needed at the time but I'm adding a couple other sources soon). Works well enough for my needs anyway but I'm usually only dealing with a couple hundred concurrent users and a couple thousand queries at any given moment. As far as IoC, my code/libraries use it a lot more now than I use to. The main reason being that my libraries tend to stick with me from project to project. So when a new project comes along, I may want to switch something out for something else (library A for library B) and the amount of code that I need to do that is a lot smaller. The larger advantage to me is how easy it is to add functionality to the system becomes. All I have to do usually is add a simple class to my app. For example my serialization code is such that I just feed it my object to serialize and tell it what type of serialization to use (based on MIME type) and it handles it for me. I wanted to add CSVs to the output so I just created a class that implemented an interface that did the CSV serialization and it was automatically picked up by the IoC container and added to the system. Nothing else in my code had to change. Everything else, the "testability", etc. is nice but what I care about now is just writing less code than I use to and IoC has helped me do that.
It's going to be very interesting to see how they incorporate forms into mvc. 
Note that on CodePlex, they don't really notify the maintainers that a pull request is pending. Or at least not well. So it may just be that the maintainer doesn't know about them. For instance, I had one that sat there for a couple months because I had no idea it was even there. If I hadn't just randomly clicked on the fork/pull requests section of the site, I never would have seen it.
Part of the problem is that any .NET OSS project is effectively cutting off a huge swath of potential contributors by not testing against or providing build processes/instructions that target Mono. Likewise, huge swaths of OSS devs (even ones that use .NET languages at work every day) don't work on .NET OSS projects because maybe they use Linux or OS X at home and don't know how awesome Mono is. Especially with Xamarin kicking as much ass as it is, there are more developers than ever ready to throw in on a .NET project, but the .NET community-at-large just isn't embracing Mono the way it should. To that end, I've started a campaign recently to try to dispel myths and get people to [usemono.net](http://usemono.net) (or at least acknowledge it). For more words from .NET OSS community members, please see the [OSS Panel Talk](https://www.youtube.com/watch?v=w6t0hP5wgPQ) from 2013's dotNETConf and the [OSS Panel from 2014's BuildConf](http://channel9.msdn.com/Events/Build/2014/9-013). One of the big weaknesses being identified over and over again is that Microsoft is just 'too competant' and they are creating these frameworks and libraries that overshadow community contributions. Not necessarily by being better, but by being good enough and being Microsoft. The ALT.NET movement is kind of losing steam (if it evey really had any), but I think if we as a community acknowledge and show a little care for Mono, we can have a thriving community yet.
The practice I am accustomed to is having a dev database (which the dev machines attach to), and which is separate from the production database. There may be more than one to fit needs (they are pretty easy to duplicate when needed). I don't know how I would feel about the development environment running different code in my application than production. I'm totally cool with a preprocessor directive to diverge specific debug configurations from release compiles; but the thought of having parts of the codebase that only run in production makes the hair on the back of my neck stand up. The lifetime management stuff you're talking about can also be accomplished with basic OO fundamentals; assuming you don't have a suite of tests and interfaces dependent on the layout of your objects not changing.
Can't all that be done just as easily without IoC?
Yes but the amount of code that it takes for me is a lot less than before when I wasn't using IoC.
This could be used to build a chat system, netcode for a multiplayer game, or simply as a learning resource. I'm hoping others will find it useful as I never did get around to utilizing it myself.
Visual Studio -&gt; Tools -&gt; Options -&gt; Text Editor -&gt; Look at the options under "HTML" and "HTML (Web Forms)". There's options that deal with auto-formatting.
I'm not 100% sure your first point is correct. I'd need to check but I think .cshtml isn't served (like .cs etc), at least by default. Would need to check though.
If I'm not mistaken they are not a part of Bootstrap, but CSS classes for the Boostrap-styled theme of [datatables.net](http://datatables.net)
cshtml files will definitely get rendered and served just like aspx files, that is what asp.net web pages rides. That and why would they add the following parts of the config if they could not be served: &lt;system.webServer&gt; &lt;handlers&gt; &lt;remove name="BlockViewHandler"/&gt; &lt;add name="BlockViewHandler" path="*" verb="*" preCondition="integratedMode" type="System.Web.HttpNotFoundHandler" /&gt; &lt;/handlers&gt; &lt;/system.webServer&gt;
Looks like a nice example if only it wasn't VB.net.
Looking for a C# version?
Bootstrap? Which Bootstrap? There are lots of things called Bootstrap.
I'm happy it's VB :)
Yup. The trick is depending on how one is using MVC one might have specific views that shouldn't be hit without a controller telling them to come out. Also opening up the views folder probably opens up a ddos vector -- exceptions are expensive, I'd expect most view files would crash in the absence of a model so you would have an easy way of firing off lots of exceptions in a .NET app.
Check out the post about Stackoverflow http://highscalability.com/blog/2014/7/21/stackoverflow-update-560m-pageviews-a-month-25-servers-and-i.html
&gt; I don't know how I would feel about the development environment running different code in my application than production. I'm totally cool with a preprocessor directive to diverge specific debug configurations from release compiles; It's the same thing but doing it via the container is a lot more flexible. It's also more testable than using the pre procesor. The code doesn't change, just the configuration. It also create different scopes for different customers if their configuration is different enough. 
I would be leery of that code too. I don't think these approaches are mutually exclusive. One way to handle configuration data like a connection string is through a configuration file, another is to be injected through a container. If you use good OO design as it sounds like you do, it's relatively easy to incorporate one. Most of the applications I write interface with web services and other items where mocks are appropriate. For example, on the application I'm working on now, we have some configurations that use S3 and some that use a local filesystem. I'm not yet at a level of complexity where an IoC container is warranted, but I can see the application heading that way. I also agree with your statement that you can handle lifecycle concerns with basic OO fundamentals. What a framework can offer is a well-tested implementation I don't have to maintain. When I start writing an application, I begin with poor man's dependency injection (constructors), then design patterns like Factory, and then I consider introducing an OO container.
Ok thanks. I'll take a look at that. 
Dude what you can't transpose it? Seriously its the same thing. Replace Dim with var
I usually like to think of it like you seem to be. MVC, WebForms, WebAPI, etc. are all the "UI" layer in a typical N-tiered application. This depends on your app though, and how complicated you want it to be. There isn't anything wrong with having a "fat controller" as it were except that it might not meet some of our coding sensibilities. Similarly, there isn't a reason that you CAN'T pass your domain model entities up through the controller to the view... I will typically do this for READ ONLY views, but for bi-directional ones (e.g. edit views) I will usually use the "one view model per view". Similarly for a WebAPI where I am supporting write operations, I'll usually have a "view model" that the WebAPI uses to communicate with. The reasoning is that this allows me to ONLY take back in the things that I actually want a user to be able to send me (e.g. the view model I use in a write situation will reflect SPECIFICALLY the form or API that I want to allow someone to send me and the bare minimum beyond that). That's more of a security thing though usually. The downside to using the same models and passing them around is that you end up passing around extra data a lot, or allowing someone to pass you data they shouldn't.
This is a hard topic to get good info on, all i've come across is various best practices e.g. use caching, reduce chattiness, load balancing etc etc
&gt; support injecting functionality into the controllers using inheritance No, this isn't the recommended way. Typically you'd use dependency injection, often an " inversion of control container" although I usually do mine manually. This basically means that your functionality is passed in via constructor parameters, which lets you mix and match what functionality is available. [This tutorial](http://www.asp.net/web-api/overview/extensibility/using-the-web-api-dependency-resolver) does things a bit weirdly (to me) in that the Dependency Resolver creates an instance of the controller itself, but it certainly works. Yes, if you want custom status codes you need to return an HttpResponseMessage since methods won't let you return two objects (status code and response body). The only other way I can think of is to make "Response" a property of the ApiController, but I think the way it's done now is preferable to that. Yes, unfortunately, that disables automatic content negotiation although the site you linked includes a section on calling it manually in cases like this. I think it would be good if someone could figure out a way to "have your content negotiation and eat it too" when it comes to custom headers, but with single return values it might be difficult :(
I'm just interested in IoC, I don't like the technique. I'm sure there are problems for which it's perfectly suited, but I've never encountered one. As far as I'm concerned, all it does is hurt the discoverability of a system for no good reason. rarely do you need things to be *that* dynamic.
 // create a server and start it: var server = new SocketServer(); server.StartServer(); server.MessageRecieved += (msg, clientsocket) =&gt; { if(msg.StartsWith("/say")) clientsocket.SendMessageToClient("/say Hi client!"); }; // create a client and connect it to the server: var client = new SocketClient("8989", "127.0.0.1"); client.ConnectToServer(); // send a message from the client to the server: client.SendMessageToServer("/say Hi server!"); // disconnect the client from the server client.DisconnectFromServer(); // stop the server server.StopServer(); I think this is how it would be in C#. I added the implied event handler though.
Not sure if you meant to say can't, but you can only mix languages in web forms projects AFAIK.
You can mix them in the same solution, yes, but not the same project as your comment. Could be confusing to those that don't know the difference.
Oh yeah. I was using project in the more general term and not as a component of a solution. Will fix in original comment.
I never did like the convoluted GPL license. Just changed it to MIT, hope that doesn't peeve anyone.
I did a full re-write in C# yesterday, but hit a strange bug I can't figure out yet. I'll let you know if/when it's ready. EDIT: Problem was the ReDim and WithEvents keywords which C# does not support.
Thank you (: I'll take a look at it later on when I have more time
God damn do you sound like a condescending asshole in this article. I understand where that comes from but it comes off profoundly unprofessional and makes reading the article uncomfortable. You need a neutral tone for this kind of writing if you want to be taken seriously.
Yea I've not found too much on the topic. A lot of companies have these issues but have proprietary processes for their design they don't want to share. 
Yeah, I feel the same.
Agreed and while some of the points are true as is always the case in architecture things are very situation specific. One must be careful not to dismiss a pattern entirely just because they have seen some bad implementations. What I see happens in a lot of these cases is a root cause issue of the original pattern or concept being under-developed or not adhered to properly. For example, if a company does standardize on a repo pattern (say because they trade out cache based and DB based data access depending on deployment environment) this is not a terrible thing if it is needed. What happens is when the pattern gets perverted or used incorrectly or ignored 1/2 the time after that things quickly devolve. IMO this is a side effect of most architects not being as engaged as they should be in the direct development of their software. While not true for all arch positions there is a sizeable cottage industry around the "Ivory tower" style arch which many devs are railing against. Good architecture is more than a set of axioms, designs and standards that folks must follow. Good architecture is trained into the teams, and an architect is as integral to the team and completing key tasks (as in the architect is actually doing those tasks) as the team lead is. It's simply folly to think that you can draw up a diagram, write a doc and some sample code and think that teams will employ it correctly, every time and make the subtle adjustments necessary for edge case deployments. 
well, I think we may just have different values as developers because I personally think if 'testing' is the reason why you're doing anything, then you're probably not doing it well. Testing is valuable, but creating all of that complexity just for testing is, as DHH recently coined it, 'test-induced damage'
How do I run the two apps at once from the debugger? Do I need yo just open the EXE's in the debug folder?
Yeah that's how I did it, run another instance of the EXE from the debug folder. EDIT: May have misunderstood you there...I thought you meant two instances of the client. The solution was set up to start both the server and client during debug; the server should start first.
I'm making posts on reddit, not writing a blog that's targeted at professional software developers. You write for your audience. I think this quote basically sums up the problems with the article: &gt; The cause of every one of these issues has been the same, someone with architect in their title. We really should reconsider what the role of an architect is and if the should be involved with technical decisions, or involved at all. You seem quite hostile towards a role you either don't understand or have never seen done well. It comes across as a naive opinion to the point that had you started the article with that statement, I wouldn't have bothered reading it. For what it's worth, RES shows me having upvoted you 23 times, so the opinion I'm expressing here is specific to this one article. Nothing personal at all.
Example parts: "On a related note, it is often claimed that a developers machine should resemble production closely. I can't think of anything worse." "The concept is simple enough, that's why every man and his dog have created a toy IOC container, unfortunately a lot of people don't seem to get to the end of tutorial 1." Your whole post reads like a light internet rage rant, not as a profession-related post that I'd bookmark for its value.
I finished re-writing this in C#, check out the [repo](https://github.com/perrybutler/csharpsockets/).
CIL is MSIL rebranded.
Json.Net, unless you want to spend time wondering why dates and other things that should work don't.
 CIL - Common Intermediate Language - is the term used in the International Standard. MSIL - Microsoft Intermediate Language - is the product term for the Microsoft implementation of that standard. &gt; http://blogs.msdn.com/b/brada/archive/2005/09/20/cilormsil.aspx
See also ServiceStack.Text. Current version is paid-for but version 3.x is still available for free I believe and is by many accounts faster than JSON.NET
Or he got sick of dealing with onion architecture fanbois and projects where you have more lines of code in the .csproj files than you have in your .cs files in the name of design patterns and ultimate extensibility with about zero real world benefit. Not that the article doesn't suck mind you.
Json.NET, no doubts.
Json.Net definitely, the asp.net team are increasingly making use of this library over their own because it is faster and easier to use.
A trick I re-discovered just this week is that you can change the baseviewpage in Views\web.config to a custom one, thus allowing you to put common logic in the view base class.
&gt; Your whole post reads like a light internet rage rant, not as a profession-related post that I'd bookmark for its value That's exactly what it was meant to be. Should I have used more rage? Edit - thanks for tge examples though. Ill try to keep things like that in mind in future. 
&gt;Firstly, none of these relate to enterprise architecture. They are strictly application design. Your right, it was meant to be enterprise *application* architecture.
I'd say it depends on your target audience and what reactions do you want from this post, Mr Frustrated. I think the top comment sums the reactions quite nicely.
&gt; Oh so somebody has never heard of interfaces or dependency injection God forbid that you use a design pattern. Constructor injection using IoC container? I've used IOC containers plenty, I said this in the article. I prefaced the section with: &gt;This isn't an attack on IOC containers, I think they're great, but in the wrong hands they are disastrous This was about people that use IOC containers badly. Not using constructor or property injection but littering the code with resolve calls everywhere. &gt;Maybe separate your implementations into seperate projects. Sounds like good design. Sometimes yes, sometimes no. Having a hundred projects with &lt; 5 classes is almost never good design.
That software is 6 years old. Try upgrading. 
I am using MVC 5 and I find it still using JavaScriptSerializer.
Boring. And way too emotional for this level of triviality. It's like shouting at random people because it's raining. Pick an umbrella, dude.
&gt; Boring. And way too emotional for this level of triviality. I'll accept that it's too emotional, as everyone seems to agree. Trivial though? This is big picture stuff that has a huge impact on an application maintainability and a companies ability to keep clients happy. 
Big picture is how you sell the need for improvement and how you improve the solutions and how you get people on board and document and lead and affect culture. ORM is not big picture, sorry.
Before you sell the need for improvement you need to identify if large scale improvements are needed. Even better if you can avoid them entirely. Some of tge companies I've worked at that fell into these traps ended up beyond saving. Cultural change is impossible if you can't retain the talent required to improve things.
This is what I ended up doing, but I wasn't sure if it was correct. Thanks for the links! I will use them to audit my code.
None of the above. ServiceStack.Text or Jil
Would I lose many of [these features](http://james.newtonking.com/json/help/index.html?topic=html/JsonNetVsDotNetSerializers.htm) with ServiceStack.Text or Jil?
Whoops, my fault! It indeed is only used by the WebApi (yet). Future versions will swap to Json.NET. But luckily /u/functionalcurious provided a way to use it now already. :-)
Json.NET &gt; DataContractSerializer &gt; JavaScriptSerializer (deprecated and recommended that you shouldn't use it)
Unless you've profiled the app and found json serialization to be significant just stick with whatever is easiest. 
Not sure how it fits here, when all he talks about is WinAPI and no .NET at all.
It's ridiculous to heap the blame for all of that on people in the software architect role. I've seen some of the same stuff but by and large it comes out of organizations that don't have a software architect. In my experience these problems turn up when a group of well meaning developers without an experienced architect build big ambitious solutions and (rightfully so) try apply the industry best practices. Without an experienced architect there's no one to pull back on the reins and say we don't need all this extra stuff. There's always that temptation lay the foundation for stuff you think you might need later in the beginning because it's so much harder to go back and work that stuff in later when you've got the weight of some massive application pressing down on you. I've been there and I've made some of those mistakes myself. Usually it's been the result of having too little time to exhaustively explore needs of the system you're building and all the implications of the choices you're making on the fly, some of them involving technologies and concepts that you might not have used in any serious capacity yet. You learn as you go and try to avoid those mistakes the next time but you've still built a system that needs to be maintained with maybe some bad decisions buried way down in the foundation. I've not worked many places where management has been very excited about spending a ton of time building out something that doesn't have a pretty quick ROI. It's just the reality of enterprise development. &gt; It was never intended to be a super serious, professional sounding article. I'd hoped to trigger discussion about the architecture that is seen in the wild, but apparently people are more interested in the writing style than the content. I think the comments here show pretty clearly that if that's your goal, you actually need to take it pretty seriously or no one else will either. It's not that people are more interested in the writing style, it's that the writing style gets in the way of your message.
Is NuGet an option here? 
NuGet is probably the *best* answer. That's what NuGet is for.
Right click solution -&gt; Properties -&gt; Debug pane Set both Projects to start when debugging.
I've been using it for many months now and I have to say, there really isn't any going back. Major perf boosts with very few downsides.
This uses C# and is a few years old, but seems solid. I am considering using this method for authentication on a project I am working on. http://www.daveoncsharp.com/2009/08/creating-an-asp-net-login-screen/
I've honestly never had an issue. You have the publish profile for the web application, and you created a database and it's listed in the Linked Resources? How did you create your web app in visual studio (what template did you base it off of?), and how are you publishing? If you right click on the web project and select publish, that's how I always do it. Based off the MVC4 or MVC5 project templates, you shouldn't be having any issues. Is the issue that your application can't connect to the database? Or are you not pulling back the expected data? Let me run through a quick test project deploy from scratch and I'll get back to you...
I have 3 projects: data.model, data.context, and project.web. Built in mvc 4. I click publish on the web project. I have data connecting on a lot of pages. I also scripted my db out in the cloud because I can query the db in the cloud and get results back...sorry first time deploying this type of project. Also to add to it, I can't even login or create a new user...and these were simple membership models included already in .net framework
It could also be something in my entity framework connection string? Do I need to add a connection string for entity framework to update database when the application runs?
What error are you getting, specifically, and when? It may be the connection string, check your config files, though that should be set up by your publish EF should by default be wired up to use whatever the default data provider is (sqlite in development, and your linked database in live.) Did you manually create the tables on your azure database, or was that done by the deployment? If the tables have changed since the last deployment, have you done an EF migration command?
If the pre-existing DB does not have user authentication bits built in I'd recommend setting up a second DB to be the application services database and treating that as a black box. As for working with the database you don't need visual studio involved much if at all -- setup connection strings as appropriate and your app will be able to hit it. 
Have a look at Log4Net 
Or Nlog, or SeriLog.
Or really _anything_ other than entlib.
I'm about 100% sure if you go to the ASP.NET website they have a tutorial of how to get the connection string for an azure database. Its considerably different that if you were going to use a regular localdb. I've testing connecting and using it multiple times on my local dev machine. 
Hey! So are you stuck with Enterprise Library for logging, or can you use other options? After years of using various logging libraries, primarily log4net and NLog, I would highly recommend you actually avoid any 3rd party libraries, and use what comes with the framework in System.Diagnostics. The tracing capabilities, and ETW support, has improved immensely over the last few years. You get very performant, full-featured logging support baked right into the framework. The main advantages of that is: 1) You don't tightly couple your application(s) to a particular logging framework and its style. This is actually pretty significant when it comes to logging "code smells", and implementing logging elegantly while having testable code. 2) You can use the trace listeners extension point to hook in 3rd party stuff if you wish (for example, you can configure the NLog listener to bring NLog to the party with its coloured console output or email support, as an example, without referencing NLog). 3) If you're using System.Diagnostics, you're using what the framework uses; this means your log file can aggregate any and all levels of logging from various frameworks in one cohesive log. For example, your client class for talking to Web APIs is having problems. Turn on the tracing for it, and then also configure the tracing for System.Net at whatever diagnostic level you want; now in your log file you can see what your code and the underlying .NET code is doing. 4) If you're publishing to the Azure cloud or want to get there eventually, the Tracing with translate immediately there with their diagnostics support. I could probably go on. I would love to blog about this at some point, but in the meantime I would definitely recommend you get comfortable with System.Diagnostics.
Is EntLib still a thing in the "modern" world ? Serious question, I've seen this library used a lot, but was never convinced by it or what it offers, so I'm wondering if I'm missing something.
There is already a simple windows application using that DB and I just want to make a website similar to the executable program that uses the same db, so you're just saying that everything I need it's commands doesnt matter whatever VS version I use
Yup. In that case you definitely want to separate the web authentication database. You might even be able to take this a step farther and use the same data access code. I would probably start there -- by separating the data access code from the thick client so I could leverage that. 
EntLib ties into all that and provides a really easy way to log System.Diagnostics logs into a database.
Unity came from the Policy Injection Application Block and that's pretty widely used atleast
There's a configuration tool that you can use. Just right click on your config file and its an option in the content menu. Basically, you set up categories, associate those categories with listeners like flat file, database, event log, wmi, email etc, and then optionally associate those listeners with a text formatter that will determine the message content.
Oh! It didn't start for me.....I must have done something wrong.
OHH! Ta!
Cool! Ta!
Unless you have an XML fetish.
Don't forget nested types - as in, classes within classes. This used to trip up Dotfuscator.
I've already tested this. Works fine.
Private, internal, protected as well? Nested internal classes jacked it up specifically.
Works like a charm.
I did some research and it seems that ProGet (http://inedo.com/proget/overview) is recommended for companies instead of NuGet. Would you agree? Also this may be because of my incomplete understanding of how TFS works but I was under the impression that custom controls would be accessible to developer PCs through the git repository? It would be great if you could elaborate on why NuGet is the right tool. 
Well, I'm friends with the guy behind ProGet, but it seems like a good tool. TFS is a source control and build tool. It has a lot of features bolted on beyond that, but that's what it's for. What TFS is emphatically *not* is a software distribution system. You *don't* want your developers grabbing library code out of source control, because you want to only have them using *signed off releases*. They could grab them from the build server's drop folder, but that's a manual step, and you're going to have a hard time keeping straight which version is the "good" one, and which version I have in my project. NuGet is a package manager and distribution tool. You put a package in your NuGet repository, and now developers can easily add that package to their project, or update their project to use the latest version of the package with a one-click upgrade. NuGet is like Ruby Gems, Python's PIP, Perl's CPAN, or NPM/Bower, etc. Yes, you can get the latest source for any library you would ever want from their git repo, but if you want an easy way to add the library to your project, you're going to use a package manager. ProGet is really just an enhancement that sits atop NuGet. It's essentially a better NuGet server for an enterprise environment.
What is the purpose of an obfuscator? (honest question)
The purpose of an obfuscator is usually to protect your source code. DotNet code that isn't obfuscated can easily be reverted to pretty much the original source code. So people use obfuscators to protect their intellectual property. Since an obfuscator basically "scrambles" your code, it can also be used to hide the inner workings to code, making it more difficult to do static and behavior analysis on assemblies (Which is interesting for people that make malicious software)
Hmm, I can hardly imagine anyone trying to steal undocumented source code (whiteout even local variable names) and forking it to create a successful product unlawfully. Security by obscurity is simply a bad idea and for licensing management, well we have seen again and again that if the motivation is high enough someone will find a way. I really can't help but to think that this is mostly useless paranoia . Do obfuscators address the problem of getting meaningful stack-traces from end users logs? 
The point is to make it less than trivial to reverse engineer your code. Of course it's still possible. It's impossible to write software that's completely protected because sooner or later it needs to be meaningful instructions to the cpu. And yes, it will also mess up your stack traces.
I think if your computer crashes enough to need to rely on AutoRecover then you probably need to reinstall or upgrade the computer. Also, VS put these backups in your profile. So, if work on a corp network where your profile sits on a NAS, VS will noticeably slow down every time it saves. You can change the location to the local disk 
The latest SQL Server release has in memory processing. Otherwise look at Redis, it's like Memcached but with disk persistance. I think you need to define if you want your data in process (some .Net data structure) or in-memory on a separate service (Redis, SQL, etc.)
I just commit frequently.
&gt; The latest SQL Server release has in memory processing. But only for Enterprise Edition. If they're looking to move away from SQL Server, spending Enterprise Edition money is probably out of the question.