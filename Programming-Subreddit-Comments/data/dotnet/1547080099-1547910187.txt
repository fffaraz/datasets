If it's a one shot background task, you will need the Pro version of Hangfire for batch continuations. If you need it for recurring jobs, you can apply [this attribute](https://gist.github.com/arxae/7741bae2c742d8fb3e64abfc89028dd8) to the class that contains the executing method. If hangfire triggers it again, but the task is still/already running, it will cancel the new one. But to be fair, i don't think a background task would be the correct approach for applying default settings (or get them for that matter).
So I've made some minor tweaks, and this is how you can invoke tools for different configurations: ``` var combinations = from project in new[] { GlobalToolProject, CommonProject, CodeGenerationProject } from configuration in new[] { Configuration.Debug, Configuration.Release } from framework in project.GetMSBuildProject().GetTargetFrameworks() select new { Project = project, Configuration = configuration, Framework = framework }; DotNetPublish(s =&gt; s .EnableNoRestore() .CombineWith(combinations, (s2, v) =&gt; s2 .SetProject(v.Project) .SetConfiguration(v.Configuration) .SetFramework(v.Framework))); ```
Indeed. I find some of the videos in the low-latency Java space (e.g.: FIX protocol stuff) really eye-opening. Control the garbage and your application can potentially run without performing collections for very lengthy periods of time. Pair this with data-oriented design, and awareness of concepts such as cache lines and false sharing, and you can achieve screeching performance that will scale with your CPU core count. On the other hand, write sloppy allocation-heavy code and you're spending most CPU time performing garbage collection, or thrashing your CPU cache, or thrashing RAM. The problem is that the conceptual object-oriented programming approach is taught without regard to the physical limitations of hardware and the characteristics of garbage collection. There's no regard for the underlying cost of things like garbage collection, and students learn to pay it no mind.
Can you link to some of the low latency java videos you are referring to? 
Not really. My bad. Thanks for your observation!.
Here's one you can start with: [https://youtu.be/8KJ2lNEDZxU?t=48m30s](https://youtu.be/8KJ2lNEDZxU?t=48m30s) That timestamp shows the difference that can exist between idiomatic Java and more performance-oriented Java.
Did they get image training working yet?
I used [ML.NET](https://ML.NET) with SigParser for identifying email signatures but I'm on version 0.3. Probably need to bite the bullet soon and upgrade. &amp;#x200B; My only complaint is that all the objects use fields instead of properties for mapping. Really non-obvious to me. Not how the rest of the language works. 
It does not support GPU. Sadly it’s not even on their map. 
Your post has been removed. Self promotion posts are not allowed.
Why is that a problem? You can also use the new embedded pdb format.
Hm?
Did you read the release notes? “GPU support for ONNX Transform (#1922)”
&gt;# ML.NET 0.9 Release Notes &gt;Welcome to 2019! For the past 9 months we have been adding features and improving [ML.NET](https://aka.ms/mlnet). In the forthcoming 0.10, 0.11, and 0.12 releases before we reach 1.0, we will focus on the overall stability of the package, continue to refine the API, increase test coverage and improve documentation. 0.9 release packs multiple fixes as well as significant clean up to the internal code of the package. &gt;### Installation &gt;ML.NET supports Windows, MacOS, and Linux. See [supported OS versions of .NET Core 2.0](https://github.com/dotnet/core/blob/master/release-notes/2.0/2.0-supported-os.md) for more details. &gt;You can install ML.NET NuGet from the CLI using: &gt;``` &gt;dotnet add package Microsoft.ML &gt;``` &gt;From package manager: &gt;``` &gt;Install-Package Microsoft.ML &gt;``` &gt;### Release Notes &gt;Below are a few of the highlights from this release. There are many other improvements in the API. &gt;* Added Feature Contribution Calculation ([#1847](https://github.com/dotnet/machinelearning/pull/1847)) &gt;* FCC can be used to compute feature contributions in addition to the overall prediction when models are evaluated. &gt;* Removed Legacy namespace that was marked obsolete ([#2043](https://github.com/dotnet/machinelearning/pull/2043)) &gt;* GPU support for ONNX Transform ([#1922](https://github.com/dotnet/machinelearning/pull/1922)) &gt;* GPU is currently supported on 64 bit Windows &gt;* Cross platform support is still being developed for this feature &gt;* `Permutation Feature Importance` now supports confidence intervals ([#1844](https://github.com/dotnet/machinelearning/pull/1844)) &gt;* Introducing `PredictionEngine` instead of `PredictionFunction` ([#1920](https://github.com/dotnet/machinelearning/pull/1920)) &gt;### Acknowledgements &gt;Shoutout to [dhilmathy](https://github.com/dhilmathy), &gt;[mnboos](https://github.com/mnboos), &gt;[robosek](https://github.com/robosek), and the [ML.NET](https://aka.ms/mlnet) team for their contributions as part of this release!
https://github.com/dotnet/machinelearning/issues/86
Every public website should have https by now. I mean, it's not too hard to get a Let's Encrypt certificate rolling.
This looks really interesting. I've been stuck using [MissingLinq](https://archive.codeplex.com/?p=missinglinq) ([nuget](MissingLinq.Linq2Management) which while better than nothing isn't exactly perfect. Definitely going to check out, and most likely swap to, your library instead.
My experience with stored procs is negative, at best - but that may stem from the fact that my employer uses them for tons of business logic, where what you as an integrations developer thought was a simple API call to update or insert a record, results in a massive cascade of database operations that you have no insight into. And once a stored proc is written and in production, no one dares touch it due to the fact that no one knows what parts of the system relies on it.
Okay but it doesn’t 
One actual bottleneck is the 32bit architecture, which only allows to address around 2/3 GB of memory.
It's a very valuable skill to have, as it can dramatically decrease complexity and improve performance when properly used. Yes, it can be abused. But these days you are far more likely to see someone underusing it then overusing it.
Not really used stored procs since the early 2000s. They damage the cohesion of codebases, and modern ORMs are normally "good enough", or equal in performance if you know how to wield them - especially with some of the optimisations the query planner in SQL server does these days. They have their place, but these scenarios are outliers in modern codebases.
Since taking over my current project, I instituted a "no new stored procedures" policy. So, no, I'm not. They're hard to unit test. Hard to debug. Hard to deploy properly in a good CI/CD setup. Hard to read. They centralize business logic when you should be pushing it to the edges (where you have many nodes). All around not a good thing for OLTP.
Not longer using SPs. But if there is a big data dump that needs to be exported, like monthly report or some long-executing data cleanup/update batch work, I use SP.
The compiler is open source. You're free to change the implementation to support emojis.
My IMHO is that all business logic should be in stored procedures (I know big companies which do it). My arguments are: \- your database becomes close to service or OOP class with interface \- business logic will not be sparse between client components and a lot of microservices - one point of true/maintenance/testing/refactoring \- each client (often on different languages) will not repeat the same business procedures and logic \- because DB now is "black box" with interface you get more possibilities for refactoring of the DB, business procedures and even structure \- views + stored procedures totally follow to encapsulation and single responsibility principles \- also I suppose such solution will lead to better performance But it has limits: such business procedures living in DB look like pure functions in FP - they will have limited effects support.
Please no :-)
What are they add?
But doing all that stuff in a database (Refactoring / optimising / testing / maintaining) is literally the hardest thing to do with the tools available. You're right to aspire to these qualities - but show me a single good refactoring tool for databases.
Absolutely. To not use stored procedures is to make your applications so much slower, as well as create any number of other issues. eg: I once worked at a place where all SQL was in the application (oh goodness NO!) and there was a problem that was fixed simple with an ORDER BY, but because it was embedded in the code and so many other issues were in the test pipeline it would be weeks before this fix was in the hands of users. If these had been in a stored procedure the result could have been fixed and moved to production in less than an hour.
Pretty sure I wouldn’t recommend sharing sprocs between applications. If two applications need the same data from a complex query, I would put that behind a versioned API to prevent accidentally breaking other applications.
Sounds like a deployment problem rather than a sproc vs ORM problem. A nice CI pipeline should can probably put out a code change faster than a database change. 
Where do you work so I can avoid any of the systems you’ve worked on? If you are writing the same business logic in multiple micro services, you are doing things wrong. Put that database behind an API with proper domain objects handling the business logic. 
I think that speaks more to a breakdown in your pipeline. If you can’t fast track a bug fix into production then something more fundamental is broken. (Which I get happens with legacy codebases / processes, but that external technical debt will continue to hold you back.)
I really hope you're a troll, because you're not even right about C#
Business logic modified DB model, should not live in microservices at whole. Do you know why?
Pointless discussion because everything you wrote is wrong.
you can use different database editors/managers or usual IDEs (for example, JetBrains has very good tools for databases, also they support SQL/DDL as language-in-language even with binding to real database). Sure, you can use SQL/DDL as code (under version control) or through some modeling tool. 
None of what I wrote is wrong. 
Because you are doing microservices wrong? I'm guessing you haven't figured out horizontal database scaling or something like that... The 'official' microservices example from Microsoft would seem to disagree with you: [https://user-images.githubusercontent.com/1712635/38758862-d4b42498-3f27-11e8-8dad-db60b0fa05d3.png](https://user-images.githubusercontent.com/1712635/38758862-d4b42498-3f27-11e8-8dad-db60b0fa05d3.png)
&gt; JavaScript used to be interpreted (so was C#) now neither of them are. You are either trolling or don't know anything. So no, C# (as a CLI language) was never interpreted. Java was in its very early stage, Microsoft wanted to improve it by a JIT-like technique but Sun didn't like it, that's why we have .NET today. JavaScript still compiles to horrible code and always will do becasue the language is so primitive. It was designed for simple web page scripting purposes but any attempt to use it beyond this is hopeless. You can still write applications in Visual Basic (not the .NET one) but the result is as-is :-) The C# "var" syntax suggar type inference happens in compile time and it is limited to local method and single call stack level scope only. You can not define a public "var" type variable. Both the language and runtime is fully strongly typed, except use of **dynamic** type (that is roughly equivalent of all JavaScript variable types). That's why it is safe because it happens in developer stage unlike the JavaScript **runtime** type inference that takes place in user's browser and it is processed by various different JavaScript engines. 
&gt;But these days you are far more likely to see someone underusing it then overusing it. Maybe outside the public sector.. I've seen some shit, man.
It's an amazing journey to see Microsoft transform itself from Windows-centric bullshit to embracing wider technology ecosystem. 
100% - if you're trying to do that, make a library, SDK or API. Database coupled applications are the literal worst to change.
Nobody can stop you to extract methods from the responsible class and to put them in some services/classes. This is a question of reasonable architecture. But microservices assume a certain infrastructure (containers, monitors, buses, etc like Kubernetes, RabbitMQ) they also need to be running and 99% depends on other microservices to be running. It's good known problem when often it's difficult to run and debug microservice infrastructure out of the office (licenses and other shit) and Docker does not help you very much. So, you have DB which is self-sufficient. It is such designed and implemented. Where did the engineers who created the database know that someone was not able to work with it without their PHP "business procedures"? :) And now nobody and nowhere can work with DB without all that Kubernetes, RabbitMQs, Dockers, licenses and other bullshit. But you, as code-monkey, should not have access to modify data, because your knowledge about data, relations, correct procedures/semantic is super small, so you should play role of a client of the data, not to be able to write and run some "business procedures". Big guys should do their job, without PHP/Ruby/other shit "business procedures" involving. It's easy to understand if you imagine a car with a reserve wheel. It will be very bad if your car can not work without the reserve wheel. The same here: database absolutely does not need your PHP/C#/Ruby/bullshit with all bells and whistles to manage, control, correctly modify data.
You *can* but these things are all not idiomatic in SQL, or really supported parts of the platforms. The problems I've had, for literally a decade, trying to introduce source control to database teams at relatively top tier companies has always led to deep scepticism around the adoption of more sophisticated database tooling.
Amen. Must be a nightmare. Reminds me of the place I took over. Literal fire drill every night. DB was dying and had to be put on bare metal bc if we failed over chances are there would be no hypervisor avail to host it. 20/40k Line stored procs that could not be i it tested. Logging was non existent. They literally just guessed at the problem. “I think this value is null and the inner join filtered it out. Oh and now we can’t trade. “ Case statement after cast statement. Oh you can’t divide by zero, who knew. It’s somewhere here in this 50k line proc but it could cone from any one of the 20 temp tables unioned together. Needless to say these guys don’t really do much anymore and most were just let go. 
One of my first jobs, I was the only developer, and I used SPs all the time, and thought it was fine, because I knew where everything was and what they all did. My last job also relied heavily on SPs. It was terrible. Not that the developer who wrote them did anything bad, other than putting A TON of business logic in them, but trying to follow call after call is a nightmare when you didn't write it. I don't know any way of debugging or testing them. And when you start putting any business logic in, how do you scale it or override it? I would constantly be told to just call an existing SP that did what I was looking for. If it didn't quite work, add an IF block... If that got too big, now it's OldSP_WithNewFeature as a whole new SP that also has to be maintained.
&gt; Where do you work so I can avoid any of the systems you’ve worked on? That's neither a nice nor a constructive way to bring the discussion.
very strongly depends on company standards. For example if company is Oracle-centric it can use some Oracle-specific instrument and it will be mandatory for all teams and members. I know such companies, all their businesses logic live in DB, most of teams are database guys and uses SQL and stored procedures. Java is for UI, client code only. And this solution is very old, they have a lot of clients (financial companies) and for me it looks very reasonable. Each instrument has own usage area. It's not normal to avoid use very good designed and implemented features of RDBMS only because you can implement them in C# or PHP ;) I don't see problems to have all SQL code under VCS. Testing is very simple and does not involve any microservices ecosystem deployment. You can do it on your laptop even.
I still use them (and have been since the late 1990s when I was using Oracle - now it's SQL Server only), but I only use them for the basics - read, write and delete. All business logic stays in the (drum roll) business layer, so the only stuff that's in my SPs is any INSERT, DELETE, UPDATE or SELECT operations that are required. My SPs are scripted and added to source control, along with everything else. I've tried using EF, and I just don't like it. It produces horrible SQL that's a pain to decipher ("Extent" and all that stuff). We use it at work and our current migrations folder has over 2400 items in it. How on earth are you supposed to find stuff in that? I like to keep my database and code separate. I don't see how using ORMs like EF helps with that. And which one do you choose? I've looked at Dapper (which at least doesn't look as inefficient as EF), and that has examples where you add SQL statements to your code - it's like VB6 all over again!
My job is literally to un-wire this exact vision of hell from company to company. Think we're gonna have to agree to disagree on this - but I'll throw this out there - all these companies are the ones that call because they atrophy and become unable to change and innovate because of this exact style of technology choice. 10/10 Sisyphean task.
Dotnet is run as MSIL which is JIT compiled to assembly, exactly like JavaScript. JavaScript is turing complete, contains first class functions, Closures, and whole bunch of other shit. It's not a primitive language, you can start write anything in it. You have absolutely no idea how typing actually works. The assembler has absolutely no idea about anything you put in and doesn't give a shit either. It's all pointers to memory, being statically typed makes absolutely no difference after compile time.
False. Part of microservices is drawing proper domain boundaries and having developers work with domain experts for the microservice(s) that they are working on. All of that other stuff you mentioned is fixed by having a proper dev environment. If you aren't able to debug a 'very close to production' implementation, again, you are doing something wrong. 
Could be worse. I've seen a few companies where the people who wrote that code are now department heads, CTOs, etc. Their view of every issue is 'back in my day we just added another case statement to a sproc or two and shipped the software'. 
100% serious. I would never work at a company with a philosophy of putting business logic in sprocs.
We use them sparingly in our applications. It's usually a case where we are doing a complex read operation that performs joins across multiple database contexts. In my experience EF can be dramatically slower in these scenarios as you lose the ability to defer all of the filtering at the database level as EF cannot perform joins across two separate database contexts. We try to use EF as the first approach but switch to Dapper when EFs performance is lacking. So far we have been using the following for our APIs and it seems to help with response times: Read Operations - Dapper, EF, Stored Procs (very sparingly) Write Operations - EF, Dapper 
Thats nothing to do with stored procs and everything to do with the proc lacking comments/documentation and the lack of knowledge needed to understand the code. This is a DBA perspective from someone who has had to deal with multiple entity framework applications causing chaos on my servers. If you are not capable of writing a book on EF and also SQL performance tuning then this applies to you as the skill requirement to use EF correctly is extremely high - you need to be atleast approaching DBA level on the SQL side aswell as having a solid understanding of EF. Do not use code first if performance is in any way a requirement. That rules out every single web application. Only bind to objects that you create in SQL. Bind to procedures and views etc. Yeah that makes all your direct DML linq queries hitting .Net table objects not work... well they probably were going to stop working once the table hits a few hundred thousand records anyway. But the upside is that when EF causes the SQL box to come to a crawl, the DBA will be able to crack open the procedures and views and tune them. Entity framework is to SQL as Frontpage was to HTML a decade or so ago. Yeah you can make it work but if you do it properly you always going to get a much better result. I've seen the queries that EF spits out and they are bonkers with pointless stuff like SELECT * FROM (select column from sometable where something) We actually have a ban on any new applications using EF at my workplace due to the number of issues it causes.
&gt; Dotnet is run as MSIL which is JIT compiled to assembly, exactly like JavaScript. Wrong again. MSIL level has all the exact type information while JavaScript don't. Period. My last reply as discussion with you is pointless. Learn the basics.
I think it is foolish to completely ignore stored procs and SQL in general. I've encountered a lot or junior devs who feel like EF is the only solution for interacting with the DB and it usually just comes down to their familiarity with SQL. In my experience I have used stored procs with Dapper when I've had to do complex joins on large datasets across multiple database contexts. EF in these scenarios usually takes a fairly big performance hit as you cannot defer all execution at the database level when joining on multiple database contexts. Generally we take the following approach and it seems to ensure that we have the best response times as possible: Read operations we use Dapper if necessary for performance sake, otherwise we use EF. Write operations we use EF. Very rarely have we had to use Dapper here. At the end of the day though poorly written SQL/Sprocs and Linq to Entity queries will always be slow. Once you optimize there though I feel like you can further evaluate what approach is going to be the best for your use case. 
you can find a lot of articles about such "wrong deployment" examples in the Web. No, mostly big companies needs some setup which usually is complex and slow. I will not mention company names but some of them I very good known in the world and they have the same problem. I understand position that you can move business logic to microservices (plural, yep, because mostly many of them will work with DB), but I have not heard a single argument why this should be done. Unlike my post, in which I set out the arguments. I will say briefly, leave the experts to do their work good and do not try to shove it into C#/PHP/Ruby/other microservices
I maintain that the database is part of "the" application. If you have 2 apps hitting a db, you really have 1 application.
You do not have to put business logic in a sproc. I'm honestly surprised to see so many people saying this. They can however provide a huge performance boost for certain things.
what approach is more preferable on your point of view?
I'm going to chime in because I am seeing so much I disagree with here: - You CAN
100% One of the greatest and longest running sins of our industry has been conflating application storage and general purpose databases. But that's probably a vaguely controversial opinion that needs more than a sentence to explain :)
That's a pretty drastic rule of thumb. As much as I dislike working with them primarily due to lack of unit testing...there are always those fringe scenarios in which you want to be able to use such a powerful weapon. SP's are much more efficient at heavy lifting complex SQL than any ORM. 
Obviously that's totally dependent on the problem domain - but in the abstract - "business logic" should be contained in versioned packages of libraries. Applications should be centered around a bounded context in a domain, and business logic should never escape the boundary of its application. Interactions with that application go via a well known API, that is either versioned, or version infinite, and the storage of data is an application concern - where the storage technology depends on the type of application you're writing. Medium sized, vertical slices, that communicate horizontally by API. From the perspective of code, whichever thing is most legible, maintainable and idiomatic. Relatively uncontroversial opinions really - don't fall into the "nanoservices" microservice antipattern, nor big ball of mud (or in this case, big ball of SQL) antipattern.
I'm going to chime in since I'm seeing so much I disagree with in these comments. I think it's necessary to use sprocs in some situations, especially if you're using an RDB that isn't quite as nice as MSSQL: - I operate under the assumption that the database is part of the application, therefore it should be versioned as part of the application. Anything that touches the DB I consider to be part of that application/system. You can pretty easily version a stored proc under these assumptions. If you have multiple programs using the database, it's still part of the same system and IMO should all be versioned together. Some people have/will disagree with me on this point. - Stored procedures can do things that you simply can't do in any other way. You can not always write efficient SQL using Dapper and other tools (particularly depending on which DB you're using). For instance, in mysql you can't use certain variable constructs, loops, etc, outside of a stored procedure. There is simply no other way to achieve this. If you need performance without doing web server round trips, this is your only option. - You can abstract data using a stored proc. For instance, one thing we do is insert a lot of data into a table then immediately move it to an archive table (there are probably better ways to do this). There are reasons for doing it this way (mainly to give it an autonumber ID before archiving). Doing this through a regular sql query efficiently is not possible since it is two operations. I wouldn't really consider this "business logic" so no need to put it into a business layer. In a normal circumstance this data would probably be left in the original table and then include a "archived" flag or something, that's just now how it is in this particular DB. The web servers don't need to know or care about this detail.
* Debugging sucks. * Unit testing sucks * Hard dependency on specific database structure. Most large applications map from ORM generated classes to domain classes allowing them to freely restructure the database without having to change the domain layer (only the mapping). * Vendor lock-in with whatever database all your procs are written for. ORMs almost always give you the ability to swap between vendors. * Inability to use best practices in certain scenarios. As an example, I don’t see how you could use the specification pattern using procs but it is trivial with EF and most other ORMs. * Tons of missing features you get by putting it behind an API like throttling or rate limiting, authorization, etc. That’s just off the top of my head. There are 1,000 more reasons which is why you won’t find any respected industry expert recommend using procs for business logic.
Being able to test, debug, and write maintainable code is not a bell or whistle.
Why did you bring up Entity Framework? It has literally nothing to do with my complaint. The same applies regardless of what ORM you use; having lots of logic in stored procedures obfuscates to the integrations developers what will happen when they interact with the solution, and it is a sure fire way to cement the business logic into something that no one will dare to touch due to the chances of something breaking somwhere, however trivial the change to the stored procedure. That's the real life concern in an enterprise retail solution catering to large clients.
With the years I’ve learned that EF is not the best option when you need performance. On the ERP I work in, we have SQL queries with more than 15k lines, that with EF takes days to run, and directly from Oracle it runs in a couple seconds. So using procs and even “normal” sql queries are still a thing, but obviously, there are application for all situations
There's really no reason for it either. A server can hold multiple databases. Just use a different database if it's a different application. :)
1. It's strange to listen because you have clean data and schema of this data. This means easy to test/verify/validate. It's very close to functional programming: pure code tests data without any side-effects. We can only dream about such case. You know, this is also close to Free Monads: all logic becomes data (which can be interpret later) and this is the best architecture for testing 2. XML -&gt; XSLT -&gt; HTML/JSON/etc allows to separate data, model, representation which is good too (like in MVVM, MVC, etc). As I understand, problem is only in big and complex XMLs, right?
According to them it's [actually beneficial](https://blogs.msdn.microsoft.com/ricom/2009/06/10/visual-studio-why-is-there-no-64-bit-version-yet/).
That’s from 2009?
Good luck processing 100K records in a matter of seconds or minutes. SPs have their place, especially for performance bottlenecks.
What's being discussed has not changed since 2009. Those lower level details work the same.
In general I think its the better choice and use stored procedures. It makes everything simpler in your code, improves performance, and sets you up down the road to easily share business logic between a wider variety of applications. 
If designed correctly why should you need to know what the stored procedure does under the hood? It should be a black box. Also not understanding whats going on in the procedure is no different than any other code you inherit. Sometimes you just have to dig into it and figure it out if the documentation sucks &amp;#x200B;
JavaScript has types too you idiot. It might be weakly typed, but it's not untyped. It has strings and ints and the compiler knows what they are. You're so ignorant 
JavaScript has types too you idiot. It might be weakly typed, but it's not untyped. It has strings and ints and the compiler knows what they are. Static strong typing isn't inherently superior and has nothing to do with safety or performance. 
It was mentioned on original post - 'code first exclusive' assume means EF. My point about your mysterious SPs still stands. If it were a proc in my environments I would just rip it apart line by line to understand it so the business is not caught with its pants down when it needs changing. I don't understand what is so scary about a stored procedure? Ignoring some aspects in terms of performance regarding plan caching etc it will function the same as just firing raw SQL strings from .Net. Stored procs are no more complex than any other SQL code - its just SQL encapsulated in an object. Just like HTML has no real visibility of the .Net code powering a website its correct and good practice to keep data changing code in the database. Why would you bother pulling the results set over the network to parse a few fields only to send it all back when you could do the whole lot just on the DB server? We have many different integrations from between multiple 3rd party products to 3rd party product integration into our bespoke applications and this is not an issue here at all however the SQL skill level in my office is generally very good so perhaps its a knowledge gap at your company? I am just trying to understand why someone would think stored procedures are a bad thing, its like saying using methods in .Net are bad because it obfuscates the logic to a HTML/CSS developer. 
I need to know what the procedure does under the hood and what other procedures may be called subsequently, due to the fact that what I might consider an innocuous API call may trigger a cascade of stuff that results in a critical performance incident that may cost the company tens or hundreds of thousands in SLA breaches. That's the real world problem in large enterprise solutions with obfuscated stuff like stored procedures.
If you use SSDT they're just part of your schema, so not hard to version and deploy. I use same SSDT to setup &amp; seed DB, then execute xunit tests against in memory db. Using optional parameters can let you have same procedure serve multiple consumer versions during rollout. There's scenarios where enforcing business logic in the db can make sense, but main reason often perf or control. A hand crafted tailored query can make an operation go from several seconds to sub millisecond, and if something executes frequently it really adds up to a huge resource saver. So even though they might not be for all scenarios, I think a blanket ban is way too harsh.
I get it man I work in that environment too with ERP systems. Those issues are no different than any other code you inherit though and are indicative of poorly designed stored procedures. It doesn’t matter where it exists if it’s shitty code it’s shitty code. 
Personally my only issue with them is they are harder to maintain. 
Ahh this old chessnut. Firstly - re: comments - comments are either lies now, or lies for the future - you get to choose based on modification frequency, failure to articulate or code drift. ORMs are an sophisticated solution to a problem that shouldn't exist - the impedance mismatch between relational and object data. They're also not that hard to use - nHibernate was generating performant queries for a decade before EF came along. Practically, as long as you understand mapping, and avoid N+1 selects, and structure your database across aggregate roots these things don't happen. I'll trust the query planner and indexes over fallible humans 99% of the time. With respect, just because a DBA or a developer doesn't understand the SQL generation of a query doesn't mean it's bad - even if it might look unusual to the eyes. That's like saying IL is bad because it's not readable - it's not meant to be. Good indexing absolutely supports high traffic web, but if that's enough scalability should be approached using web-native tech like distributed caches - this isn't a database problem. Banning ORMs will result in your developers seeking non-SQL storage. Or just serializing to JSON and faking a key-value store in your database.
&gt; but in the abstract - "business logic" should be contained in versioned packages of libraries agreed, except that my original point is to use SQL/DDL/T-SQL/PL-SQL/etc instead of client apps languages. But here also there are options, if tool's models can be versioned, you can use even them. Example is UML - obviously you will use tool's format for them (XML/whatever - depends on tool). And such point, that all should be written in PHP/C#/etc and usage of related tasks tools is wrong - IMO can not be accepted because such position eliminates very big area of IT and IT practices. There are good instruments written by experts in there domain areas, so why I want to use own solutions? Stored procedures and SQL have very strong interface, stored procedures form API of data, I don't need buggy microservices with timeouts, licenses, certificates and other shit. All is done by experts, it works, it's scalable, easy testable in pure-style, so why I should try to repeat it with help of some C#/Java/PHP code monkeys?! 
It's worth noting you can also do `Task.Run(async () =&gt; await functioncall());` which can be cleaner, but of course this will probably end up running on a new thread while your existing code keeps running so you have to be careful of multithreaded issues and race conditions in that case.
That is a very weak argument since the code for the sproc should be living in the same repository as your other source code. Seeing what the sproc does shouldn't be any different than looking at the query anywhere else.
Acceptable for complex queries, as a solution for performance issues, batch operations but not business logic.
If your shop has so much trouble with EF it's amazing your team can even write sprocs effectively. The skills needed to effectively use either are very similar. Stored Procedures are great when needed. But it can be hard to verify relational logic when used extensively in long complicated stored procs. Comprehensive testing with edge cases is much harder than in application code. These nightmare procs exist and probably are what gave stored procedures their bad name.
We have multiple repositories spanning various aspects of a mastodon solution that manages all aspects of wholesale, retail, company administration, customer administration, security, datawarehouse(s), article administration on all levels from legal down to individual stores, business intelligence, integrations frameworks, and a whole plethora of minor stuff that makes the thing go round. It's frankly impossible to know how it all works, and having stuff obfuscated makes it even more so. I'm speaking candidly here, and from a feeling of frustration, but I have the impression that most stuff that my fellow developer redditors deal with, is an order of magnitude less complex than the conundrums, catch 22s, conflicting requirements, and general massive codebase and database structures that me and my company deals with.
Yea, I feel like people who are saying you never need them aren't dealing with huge data sets or databases with a lot of tables. 
Btw why not just make selected tile a property instead of having whatTile? 
I would look at your database schema at that point. There's a tendency to normalize the shit out of everything to achieve that "perfect" data representation. If performance is a serious consideration, break up concerns of your app into read side and write side and then optimize the datastore for each. Look up CQRS pattern. This is often done on a large scale in projects with "transactional" database and "reporting database" - CQRS brings this pattern right down to the code level.
&gt;The skills needed to effectively use either are very similar. No they are not. To write a good SProc you need just good SQL skills. Period. To do the same task effectively in EF you need good SQL skills, good dotnet skills and also in EF. &gt;If your shop has so much trouble with EF it's amazing your team can even write sprocs effectively The majority of our devs spend more time in SSMS than visual studio and we also train DBAs to do their job more effectively - including big corporate clients whos names you do know and public sector. So appreciate you are not aware of our personal situation but we very much have plenty of SQL expertise here with probably a good 100 or so individuals with the organisation MCT accredited. &gt;These nightmare procs exist and probably are what gave stored procedures a bad reputation. Same for .Net methods. Same for freaking batch scripts or any type of code. If you write big long complicated code then its big long and complicated. Its your fault for writing it that way, nothing to do with the name of the type of object its encapsulated in. If you write all your .net code in one big method its also a lot harder to test. That's purely down to the way it was written and nothing to do with the underlying technology. &gt;Comprehensive testing with edge cases is much harder than in application code Is it? I have worked as an automation tester for a database only application (don't ask, it was awful code but...) and it worked fine and found plenty of issues. I can run a DML statement, select from some views or whatever then rollback to test changes. I can see before and after and compare the data. I can write code to do the comparison and spit the results out over an email without ever leaving SSMS. Nothing here is specific to stored procedures or even SQL. If you write long complicated code its difficult to work with regardless of the language. 
This reminds me of the time I had to give technical advice to IBM who insisted on writing a rest API generator driven from their database. Those two domains are not the same. They are not the same thing. In the vast majority of cases, your data is NOT your application (there are some exceptions to this). Trying to drive your application out of a language not designed to do that, with a poor interface, is just going to result in shoddy, arcane, poorly constructed applications. "the API of data" doesn't mean anything. Data doesn't have an API. It's data. SQL is a query language, not an application language. PL-SQL is an abomination for all but a super select set of circumstances. Programming languages exist for a reason. In fact, literally every single thing you listed there Oracle, PL-SQL, UML etc are all stale to dead technologies - and with that I mean that the vast majority of codebases using them are attempting to migrate away from them. They're entrenched, they've past the point of usefulness meaning they'll be around forever, but in no way are they fit for the vast majority of applications being built today. "Shitty microservices" exist in part to split up the unmaintainable mess of monolithic, expensive, difficult to scale, difficult to maintain databases - which is exactly what you're describing here. It's not just my opinion, it's literally the direction of software over the last 15 years as all the database driven monoliths ground to a halt - and I'm not sure denigrating software developers as "code monkeys" is particularly pertinent in a sub for programmers.
&gt; I'll trust the query planner and indexes over fallible humans 99% of the time Clearly you have not spent much time with query plans suggested indexes or dealt with some weird situation where its deciding to do some daft table scan instead of using a perfectly good index you created. Theres a reason why there are entire books written on indexes and query plans.
Sounds like an over engineered cluster. I sympathize with you. Either way I don't see how the problems you are describing are different for sprocs vs. code. How does putting the SQL or Linq in C# make this problem an easier?
&gt; Is it? I have worked as an automation tester for a database only application (don't ask, it was awful code but...) and it worked fine and found plenty of issues. I can run a DML statement, select from some views or whatever then rollback to test changes. I can see before and after and compare the data. I can write code to do the comparison and spit the results out over an email without ever leaving SSMS. That sounds like manual testing to me. What do you use to do the comparison?
Yes but stored procedures encourage long gnarly code because they don't have any mechanisms for abstraction or modularity. You can't break up a stored proc into many little stored procs that interact with each other very easily. In addition, stored procs aren't like functions which can be stateless. They depend on the state of the database so testing becomes much more difficult. &amp;#x200B; If your stored proc is just doing a query with joins, group bys, etc - no problem. But once you start getting into imperative procedures which have business and update logic it gets rough fast.
Bam - this - that data isn't stored correctly. The only real world scenario where this should exist, and not be factored out, are probably star schema data warehouses - and generally the SQL generation there is really complicated regardless.
Quite the opposite - I've spent plenty of time doing that for massive high performance sites in .NET But it's always the outlier - yes, sometimes you end up there. In the vast majority of the cases, we just changed our approach to storing that data to pre-computed views, caches or key value stores for those performance critical areas of the code. Cheaper, easier, cleaner in the software, less brittle.
We versioned our stored procs just like you would with any other public API. Though through the use of optional parameters and the fact that extra return columns are ignored, breaking changes were rare. 
I can believe it. 
In the old days maybe, but with SSDT it's no harder than any other source code. 
If you move that code to a shared micro- service then it's going to do fire same thing anyways.
It's difficult because tracing things that call procedures is impossible - unless I've missed a very modern silver bullet - if I have 10 database integrated applications and I refactor something missing one, I'm dealing with that at runtime.
If debug is packed and you're using this in your app, you'll unintentionally be using non compiler optimized code. Probably not a big concern given the size of the code base
Use the best tool for the job? No, let's do everything in app code do I don't have to learn how to manage database source code. 
Stored procs come into play on databases pushing 200k rows across multiple tables. Only when i worked for Sony and their membership services did i see good use of them. They had nearly 2m user profiles. 
Yep, but it does goes both ways. A lot of people who are afraid of stored procedures have similar problems getting database changes deployed. 
Using a web server baked into your application means your application is the only thing your website can run. Generally you want to use something like IIS or Apache as a web server and have it reverse proxy (with .NET Core's IIS plugin or NGIX for Apache) for only a specific url on the web server.
&gt; It's difficult because tracing things that call procedures is impossible XML comments. Whenever I call a SP I list it in the XML comments with a custom tag. Then I use a tool that gathers them all up for a report. Yea, it sucks that it isn't out of the box, but once you set it up it is easy to maintain. 
Absolutely. If you have a strong, well designed database then the model is best managed by the DB engine. Any abstraction of that into a different layer introduces problems. if your business logic lives in stored procedures then the DB engine has full access to what it requires to do the beat possible job it can. Optimisation becomes simpler and more effective. Transaction control is an essential part of ACID compliance and most ORM make it nigh on impossible to manage this. Now that modern tools allow easy version and release management of SQL projects there really is no excuse to blame the tool chain for poor architecture choices. The same applies to steering such choices by the availability of skilled resources. The lack of a decent DBA ruins a ton of projects, and developers that think they can do a better job of data management that a good DB engine should be sent back to school. ORM has it's place, I like EF despite its faults, but DB and Sprocs absolutely live on. 
Microservice APIs should be either versioned - in which case, we're good for changes (though deprecation is a pain), or version infinite (better) - consumers are responsible for managing changes, not publishing applications - so the responsibility is inverted (and normally backed by consumer driven contract testing). The XML hack is... ok... I'd probably take it further and have the application verify the contents of the proc somehow (hash check or something) on startup for every proc it knows about as a tolerable middle ground. Or perhaps even drop all procs that aren't explicitly referenced by the app. Certainly ways of making it more tolerable and cohesive with the application.
&gt; I would look at your database schema at that point. This is great in theory. If I'm building everything from the ground up awesome. But there are tons of systems out there where a complete redesign isn't feasible and you have to work with what you have. Plus, I'd still like to see a large system that could completely be maintained with simple queries and never require a DBA writing some custom stuff. 
..actually in the above scenario - I'd probably use an attribute, over a generated-ish class or dynamic proxy - sync the name of the method with the sproc - and verify on startup, potentially having the app reinstall its procs on run. At that point you've reduced the proc to no different than any other resource.
I built a .net application on top of NUnit to fire off my SQL commands and spit out the differences via email. It was automated even if the code writing was not.
Also, for every "you don't need to normalize your schema that much" example, I can show you a "that should have been normalized" example. 
Certainly better than manual - but wouldn't fit in a CD pipeline.
&gt;Yes but stored procedures encourage long gnarly code because they don't have any mechanisms for abstraction or modularity You can easily create smaller procs and call those from a parent procedure. You need to write your code in a modular manner for it to be modular. You can set up 'overloads ' using nullable parameters if you wanted aswell. &gt;You can't break up a stored proc into many little stored procs that interact with each other very easily. Really? Thats news to me. Whats wrong with parameters? What's wrong with a staging table with a range of data referred to by using an ID to get the data back out? You can even use a UDT to pass entire results sets from one proc to another but thats not a good thing to do. &gt;In addition, stored procs aren't like functions which can be stateless. They depend on the state of the database so testing becomes much more difficult. Again wrong. See below example - nowhere does it care about current table data and nowhere does it care about previous executions. Unless my understanding of stateless is lacking as I am no functional programmer.... CREATE procedure UP_GenerateDates AS BEGIN with CTE_Dates as (select getdate() as D union all select dateadd(day,1,D) from CTE_Dates where dateadd(day,1,D) &lt;= dateadd(year,1,getdate()) ) select * from CTE_Dates OPTION (MAXRECURSION 1000) END 
Sprocs are likely needed but look into Table Valued Functions. If they are single line sql in the function they get optimized like a view whereas sprocs do not. AND you can add the TBF to you EF model and join/access them in C# that way like a table. 
Interesting idea. I was using XML comments because the compiler automatically extracts them. Extracting attributes for a report would be more difficult, but I like your idea of verifying the procs exist on startup.
Maybe I can explain why his fix works and your code didn't. So, this is basic to how object oriented programming works, and understanding this can really help you code better I think. Let's say I make a new friend, and he asks me to borrow $20. Ok I loan him $20. And let's say later I want my money back. If I decide to make a new friend and ask him for the $20 back... this makes no sense. He's not the one who borrowed $20 and he'll tell me he doesn't know what the hell I'm talking about, ~~whatTile is null~~ he never borrowed money from me. He might also stop being my friend but fortunately .NET is more understanding of mistakes. And actually to extend the analogy further I messed up even more since I forgot who I loaned the money to right after I did it! So I have no chance to get that money back (whatTile was not null but now it has been destroyed!! The solution is for me to make a new friend, loan him $20, but then remember who he was and keep track of that until I need the money back. Then I use my memory of who he was to find him again and ask for my $20 back.
You'd probably need a tiny bit of metaprogramming glue - but a command line tool that loads your assembly into the app domain, reflects all types, and all members for that attribute and does whatever is probably only an hour or two of hacking. You could probably even invoke it during a build process to pre-flight-verify that your deployment artefact is going to work.
Your question is like asking "how do I make food"? It's extremely vague when the actual process could be any number of specific processes depending on what you're actually trying to do. I suggest you provide more details like, is there a specific application or framework that has "widgets" that you are trying to make one of? What exactly are you looking to make, or what problem are you trying to solve by making this widget?
Such as?
Can confirm. I've been maintaining 5 large apps that are database coupled, and I despise it. I'm currently trying to build a highly-available service API to refit everything to get out from under the hell that this is. I want to do so much database refactoring but I can't touch shit because of the apps.
So don't use an ORM. Just send a SQL statement.
You mean like just sending a regular SQL statement?
A hand crafted tailored query can be sent from code. Doesn't need to be stuck in the DB.
The best tool for the job is never a SP.
I wouldn't put business logic in them anymore either. I don't use EF anymore and just use Dapper though, and I've found they DO have a place still if you use SSDT projects already. "How do we manage our queries" always comes up with Dapper. Some people just embed them as inline strings or in const files. Or embedded resources, etc. But I've found that using SPROCs as simple named queries in an SSDT project for this is great. Your code base just executes a single named thing, you get intellisense and code highlighting when editing the .SQL SPROC files, you get compile time checking of your query against the SQL structure, version control, etc. The only real downside here is eventually you get someone who is a SQL-first person who tries to embed business logic, or touch too much in a SPROC that violates some artificial entity boundaries or something, and then things get ugly, but it actually FIXES some of the problems with query management when using relational mappers like Dapper, or ADO.NET directly, etc.
It separates the code from the data source making it at least easier to test. For LINQ arrays can act as a substitute for the dataset making them relatively easy to unit test. Stored procedures also cannot be overloaded so if you use them for general queries you'll end up with a whole bunch and theres no really simple way to tell whether it's actually in use or not, so they tend to linger. C# also have way better tooling support than SQL does. OzCode for example allows you to walk through a LINQ expression evaluation node by node and see what they evaluate to.
Why is this presumptuous? As far as I'm concerned it's a technical fact: The more memory is allocated, and the more often you almost hit the limit, the more often GC kicks in and VS gets suspended.
My data set is 60TB. With thousands of tables. &amp;#x200B; And since it's a project I inherited, and is over 20 years old, it has thousands of SPs. They suck. There's no reason for them. 99.99% of the cases can simply be handled by a simple EF query. 99% of the few that actually do something special (cursors, carefully crafted joins) could have been done just fine by issuing a SQL statement to the DB. It didn't need to be immortalized into schema.
It does not improve performance.
Yea, it's probably worth the effort.
Didn't realize you know more about Visual Studio then the people who build it. Who have also been very transparent about why they haven't ported it to x64. But I guess since you have no arguments you can make things out to be as simple as possible.
Says the junior dev that tripped into a leadership position.
I never said that. I just think that the article became rather irrelevant for the current circumstances. You think otherwise, that's okay.
&gt;Yes, it can be abused. But these days you are far more likely to see someone underusing it then overusing it. As I stare at an application with all its business logic in stored procedures... Hundreds and hundreds of them, some many thousands of lines long. 
They aren't going to be any shorter when you distribute them across a few dozen microservices. Bad code is bad no matter where you put it.
Oh I wasn't trying to argue that people don't abuse them, more so that saying that you can build a large scale application and NEVER need them is a little unrealistic. 
So you think people have this slick CI/CD pipeline for code and then keep some archaic process around for deploying database changes...?
Oh course changes were rare, you tightly coupled everything to your database. Kind of multiplies the work needed to make a database change by 10. In an app with proper domains, I could totally refactor my database, swap it out for Postgres, add a bunch of caching, and the clients wouldn’t know the difference.
I know that people do. I've worked with companies where I one of my tasks was teaching them to use source control for their databases. Not even CI, just the ability to associate DB code with a version. Many of these companies have so much bureaucracy around database changes that even adding a new column can literally take weeks. For many companies, this was the driving factor behind NoSQL adoption. They didn't care about its performance claims, they just wanted to be able to change schema without a permission slip from a DBA. 
It really depends on your needs and your data. SPs can do a ton much faster and sometimes with less lines of expression than code. In practice I have usually only done this with really big DBs (like 10gb and bigger) and with processes that require multiple joins/evaluations over the set that are also run often. 
I’m confused on your point... Regardless of how slow/fast your database deployment is, procs are still the wrong place for business logic. You was saying that CI/CD almost always includes the database meaning that changes get deployed all the time and it is no big deal.
You have it completely backwards. The point of using stored procedures, at my previous company, was so that the applications were decoupled from the table schema. They could literally take a table and split it into two without a single line of code being changed in the application layer. All of the changes were hidden behind stored procs. And the business analysts abused this. They would change business rules during the day, often multiple times per day, with zero down time because our websites and fat clients would just pick up the new version of the stored proc. *** Without stored procs, your application code is directly coupled to the table schema. You can't "totally refactor your database" because that means changing every application that touches that database.
Uh ya no. You have one application touching your database that is hidden behind an API. Your database is abstracted via a mapping layer. This is the only thing that changes with your database. And doing this in code gives you vastly superior refactoring tools. We have analysts changing rules all the time too. But guess what? They do it in a business rules engine instead of having to dig through SQL and break our database.
&gt; procs are still the wrong place for business logic That's a myth perpetuated by people who don't know how to work with databases. First of all, the very term "business logic" is nebulous. It has no real meaning beyond "code that isn't classified as something else". If we restrict that to be "logic needed to implement rules created by the business", then the best place for "business logic" is often inside lookup tables. Why? Because rules that are expressed as data instead of code are easier to understand and change. Often you can literally dump the table into a spreadsheet and have a business analyst alter the rules. They are literally coding without code. Views and stored procs then give you access to the logic encoded in the table in a way that is easy for the application to consume. (Or in many cases, an ETL job where there is no application code.) *** Where this goes wrong is when people who don't understand table-driven logic just translate their C#/Java code into SQL and dump it into the database. Instead of a few lines of table-driven logic, you get a few thousand lines of repetitive garbage. 
it appears that both stacks utilize the Model View Controller pattern. I use MVC because it is centered around using C#. I don’t touch Angular/React because I don’t want to use JavaScript.
There is also the issue of "storage logic". When you update table A, that change may need to also cause an update to table B. Maybe B is a history table. Maybe it is a pending work queue. Or it could be a denormalized table used for reports. There can also be translations. For example, in one company I worked at the application would send in UserName and the stored proc would translate that into UserKey. This avoided the need for the application to look up users itself. By encapsulating the storage logic in the database, the application can focus on the things that it does best.
&gt; You have one application touching your database that is hidden behind an API. Your database is abstracted via a mapping layer. Congratulations, you've reinvented stored procedures with an extra step.
Wow, interesting. Working mostly with very large C# and C++ projects, 2017 has been *so much faster* for me than 2017. I wonder what the difference is. (Note: I do run mostly without plugins. As much as I like Resharper, VS 2017 + Resharper was unacceptably slow, and 2017 has incorporated just enough functionality that I don't feel hobbled using it at base.)
Used to work where all database operations and business logic were implemented in stored procedure, this works, is very fast, more secure, but really hard to keep all the stored procedures organized specially once you get 100s of them. &amp;#x200B; The only spot where stored procedures shine is when you want to do updates to the database without having to bring the data to the application.
It certainly does for the things I work on. Applications I am working on routinely have to join 10 or more tables and are dealing with hundreds of thousands if not 1 or 2 million of records. For smaller things I concede that the gains can be negligible 
Thats a good way of stating that
How do you think large systems based on key value pairs work such as Cassandra. Those systems don't even have joins. The complexity is shifted towards orchestrating eventual consistency and data flows. World's largest systems are built on these kinds of architectures 
ASP.NET Core is the base framework on which MVC Core is built on. Check out my project at https://github.com/dodyg/practical-aspnetcore. You will see tons of samples just on ASP.NET Core. You can contrast it with ASP.NET Core MVC code.
Isn't that so much of programming LOL
Yes, data is not API, but you can bound it with API and this is right. Because clients of that data will get data + scenarios (setup'ed workflows, operations), like algebraic structure, i.e. set + operations on this set. This helps a lot. I had a bad experience when one team used a large database and various database clients implemented similar logic. It was in time when REST does not exist. Modern databases are class in OOP terms. Some of them supports REST even. Your ideas mean only one: elimination of this. Ignoring of the evolution of databases. For me it's difficult to get idea that Oracle, PL-SQL, UML are death technologies. It sounds like joke. What do you mean then say "SQL is not an application language"? Do you talk about its limitations? You can write in SQL even fractals generators. "Code monkey" is very important term. There are a lot of code monkeys today. Their work leads to a lot of problems. "Code monkey" = risks. Risks = money and failures. When you hit some problem, when you lost your data, when application is broken - this is a job of code monkeys. I've seen a lot of them. Their decisions are terrible. They constantly reinvent the wheel. They constantly invent, they deny existing values, they don’t want to learn, they want to deny and invent. Therefore, today in IT we are still marking time at the same place, repeating very old architectural solutions, but in new languages, again and again. Today they are a majority. But you don't answer me: why to have business logic in the "stack" with enterprise bus+caches+microservices+SSO+ceertificates is better then to have the same logic implemented in the database without all these dependencies. 
As one who had the same problem as you I hopefully give some clarity. It all depends if you want to be a pure back-end engineer or prefer to work full-stack. .Net core MVC is great if you want to focus on the back-end e.g. write most of your stuff in C# because you do not really have to write as much JavaScript in order to get a basic UI working and hooked up to your controller (=API) and you can even write C# in your front-end/ .cshtml-files or simply let the front-end be generated from your Models/ DataContext. So MVC is good for people who prefer back-end work but still also want to be able to generate simple user interfaces which do not have particularly bad but have a somewhat limited functionality or well it it harder to expand its functionality as it is not really meant to work as seamlessly as a Single-page-applicatin you could write using a professional javascript-framework like angular or react. Which is by the way a good translation to the other side of the medal. If you want to focus primarily/ more heavily on the front-end part then and only use the back-end sparingly or just as a simple means to host/ manage your webapp, have websockets (=bidirectional data transfer like whatsapp) or simply want to save data in a database or call rest-APIs, get the data, (save it into db) and send it to the front-end in a processed/ special format then yes.. you should start implementing JavaScript from the get-go and don't use MVC at all. The last option is by the way the preferred way or how have been using the dotnet/ javascript-stack in the past since I am a full-stack web-developer who focusses mostly on front-end and processing of data from external API's. As a JavaScript Framework I can recommend React.js if you have a project which requires a very powerful/ graphic heavy UI and Angular if your application is more basic e.g. more like a CRUD-application. ..okay that was a lot of text (I am using my smartphone by the way). summary: if you are serious with full-stack web development then go for .Net core (=back-end) and javascipt/ angular/ react (=Front-end). If you want to stick with back-end then mvc is a good way to at least get some "front-end air". If you are new to JavaScript you should not start with a framework like react or angular since they heavily rely on a deeper understanding of the technology e.g. it is frustratingly hard to find an error in angular if you have no idea how JavaScript/ typescript generally works. I hope my somewhat long comment could be of some use. Greetings everyone MoDX
For a total noob, Razor Pages is the easiest start probably
Right after I wrote that I realized that "serverless" programming is just stored procs stored outside of the database. I know that sounds odd, but serverless functions are deployed individually, just like procs. 
I remember the first time the circle of technology life really hit me. It was when the cloud started being a thing and thin clients were being talked up. I was like we've done this people it was called main frame this isn't new and it has down sides. All the younger programmers just looked at me like I was a crazy old man. 
Checkout the Blazor ☺️
To give credit, you can build a functional application in any of the three. Angular and React are more popular, but that's probably partly because they were open source from the get go. I would suggest either Angular or React because the experience you get would probably translate to a lot of jobs. Not saying that MVC wouldn't, but those are just more popular. 
Ok, let me reply in line: &gt; Yes, data is not API, but you can bound it with API and this is right. Because clients of that data will get data + scenarios (setup'ed workflows, operations), like algebraic structure, i.e. set + operations on this set. Nope, SQL doesn't have semantics. At it's purest relation databases are based around set theory. These are not domain operations. Sorry. This is a fundamental misunderstanding of both REST and OOP (and algebra). &gt; This helps a lot. I had a bad experience when one team used a large database and various database clients implemented similar logic. It was in time when REST does not exist. Modern databases are class in OOP terms. Some of them supports REST even. Your ideas mean only one: elimination of this. Ignoring of the evolution of databases. REST has existed in the mainstream since 2009, it was defined in 2000. Modern databases aren't "a class" - they're a service, with tables. They are *data*. REST over tabular data is *not rest* - Representational state transfer - it's HTTP-RPC over SQL at absolute best - and that's nothing more than brittley bleeding out your internal representation to the world. This is a bad idea, it'll cause problems as your software grows when external implementors will force you into a position where you cannot change nor control your schema. Do not adopt. &gt; For me it's difficult to get idea that Oracle, PL-SQL, UML are death technologies. It sounds like joke. This is not a joke. Go google the breakdowns of language popularity across both startup and enterprise. Look at the TOBIE index - they're dead. They're being phased out as fast as people can for multiple reasons - Oracle, it's cost, PL-SQL, well, same, but it's also *always* been a garbage language, UML is literally just a documentation markup language at best. Pointless. You'll still come across them in the wild, but no technology company worth working with or for is investing in these things in a meaningful way unless they're already stuck with them. Do not adopt. &gt; What do you mean then say "SQL is not an application language"? Do you talk about its limitations? You can write in SQL even fractals generators. SQL is an abstraction over set data. That is it's entire point. It's a query language, not a general purpose programming language. I can also write a web server in x86 assembly but I'd not recommend people do that either. It's a domain specific language - that is why it exists. &gt; "Code monkey" is very important term. There are a lot of code monkeys today. Their work leads to a lot of problems. "Code monkey" = risks. Risks = money and failures. When you hit some problem, when you lost your data, when application is broken - this is a job of code monkeys. I've seen a lot of them. Their decisions are terrible. They constantly reinvent the wheel. They constantly invent, they deny existing values, they don’t want to learn, they want to deny and invent. Therefore, today in IT we are still marking time at the same place, repeating very old architectural solutions, but in new languages, again and again. Today they are a majority. Most people are awful at their jobs. That's fine. People are shaped by what they know and it's the place of people with information to share it with those who do not have it. If there is reinvention, it's related to the serial brain drain problem in software development, or the failure in communication, or the failure in innovation of previous incumbents. I don't disagree - I've been doing this for 20 years - I see the reinvention - but slavishly hugging your RDBMs when it's way past it's sell-by date is foolish. There's a reason CosmoDB on Azure exists. &gt; But you don't answer me: why to have business logic in the "stack" with enterprise bus+caches+microservices+SSO+ceertificates is better then to have the same logic implemented in the database without all these dependencies. I didn't suggest a complex setup - just one that doesn't revolve around an RDBMs. I did - * Lack of cohesion * brittleness to change * Difficulty of tracing consumers * Poor development time and cycles * Poor deployability * Conflating data and application * Poor verifyability * Incorrect language to idiomatically express business logic and decisions * No flow control in SQL * Vendor lock in * Inability to change once published * Poor versioning support * Poor tooling I really could go on. I get that it's a bit of a scary world, and I know a lot of software in the mid-to-late-90s was built this way - but I've also worked across tens of organisations desperately trying to bail themselves out of a clusterfuck that looks exactly as you describe, with the benefit of hindsight, for all of those reasons. Build small systems around bounded contexts that own their own data-storage. And if you can, don't make application data-storage relational.
Whatever the equivalent of a Ubuntu desklete is. A frame that is constantly on the desktop. I've been looking round and can't find anything so I'm just gonna make a windowed app that can be minimised if I can.
Ah you are talking about Sidebar Gadgets, which I think got renamed to just Gadgets. I think they are deprecated? AFAIK there was never any official support for building them in .NET. You can make a windowed app and use some tricks to make it act more like one of those widgets though: 1. Hide border and titlebar. 2. Make dragging anywhere move the window (override WM_NCHITTEST message in your form's WndProc and return the title bar hit code). 3. Mess with z-order so the window is always under everything else OR adjust the window's parenting so it is parented to the desktop window (both require use of P-Invoke and Windows APIs to pull off). Not sure what quality results you'll get with these methods you may want to experiment a bit. Good luck.
I really don't trust EF Core. It can defer things to the client instead of SQL, leaving you with N+1 queries. And this can completely change between versions, fixing the N+1 problems or creating new ones. They say they are trying to improve that, but I'm staying away for now.
I don't understand how not using a stored proc would be any different? Any innocuous API call can end up doing tons of non-obvious stuff, whether it's a stored proc or just plain C# code? You would have to read that code to know what the API call does too, same as stored procs?
And now we've come full circle again with Angular apps being heavier than many of the "thick clients" that I used to write in VB. Stop the carousel, I wanna get off please. 
sure, SQL does not have semantic. But operations (stored procedures) gives semantic. If you have a set of 2 values A and B and two operations $ and % only implementation of these operations can show you the semantic of A and B (for example, 0 and 1 for + and *). So, this is also a reason why data should be bound with stored procedures. OK, I'll ask next question: if I have a big database, obviously (usually, 99%) developers of microservices will need consultations with database designers and their microservices very often will contain bugs. But agree, it is much better if you get a base with all the business procedures over this data. The same as in algebra: we have sets with defined operations on these sets! Sets without operations are dummy: can you define a group without definition of operations on it? And how to process data correctly, knows designers of the database. Not PHP/Java coders. But I see that you often argue the relevance of technology, its modernity. My question is: do you agree with the fact that we have seen the dawns of many technologies and the excitement around them, and how they were thrown into the dustbin of history literally in 5 years. At the same time, there are solutions that continue to exist for decades... Fashion factor. Today there is new trend: not NoSQL, but SQL + NoSQL, there are good examples how good old SQL can handle tasks solving today (fashion!) with ML :) Modern RDBMS's handle "NoSQL" data too: objects, trees, XML, JSON, geo-data, etc. Languages for non-relative data are more and more close to SQL, so we see that SQL is enough to handle them too. So, SQL and relational model is back. If we have set of objects and naturally relations between them, why we should avoid RDBMS? What architecture do you prefer if it's not RDBMS centric? Key-value, document oriented? In my current job one of departments hit a lot of problems with NoSQL data and currently they will migrate to RDBMS (with some additional storage for special kind of data). It's the experience. And today we see more and more examples when companies refuse to NoSQL 
It’s a layer of abstraction and you gain a ton of features by doing it (rate limiting, structured logging, authorization, etc). Features that you really want in 2019.
If you update table A and table B, you update your DAL and fix your mappings. You domain objects all get to stay the same and your storage ‘logic’ is still encapsulated.
When I say business logic, I’m talking domain logic. Business rules can live in a database but they shouldn’t live in a proc. They should be processed by a business rules eng be.
EFProf is your friend.
Yea, because adding unnecessary round trips to the database is always such a great idea. And who cares about ETL jobs.
React has a easier entry level, but a steeper learning curve. Angular is harder to start with, but Id say its easier. If you are Just starting - stick to mvc and jquery. 
Nice one. That's enough info for me.
You can still have all of that an the websevice layer and use procs for database access. 
You are wrong.
PDB are also emitted for Release artifacts.
Hosting fees are part of it. Azure is free with https or custom domain names, but not both.
It’s still just one trip. I don’t think you know what you’re talking about.
On dell, on most projects we use .net core with angular as ui, it’s pretty good
A great blue colored truck or a shiny red car for a homeless man? You should learn the basics first in my opinion. 
Glad I'm not the only one feeling this way. My company, which I've been at for about 18 years (minus a couple year stint elsewhere) was historically a Microsoft shop, but we got a new CIO about 3 years ago, and have gone completely the other way. I'm still the tech lead for a mission critical system (which I mostly built and continue to love...thank god), completely written in almost pure .NET, so am busy keeping that happy and have been isolated from most of the changes. But I have been to the various meetings where new products and being reviewed and discussed, and the sheer number of technologies being used now blows my mind. One of the new architects popped up a slide one day, and I swear there were dozens of technologies on it, and he described this as our new 'stack'. They are explicitly encouraging 'best of breed' tool/language/platform in each case, and now we have projects internally using Java, Python, Go, Ruby, and JavaScript. Another person mentioned they're likely to use Rust soon. Some teams are using AWS, others Google Cloud. Some are using Postgres and others MySql, and others document DBs, all with various flavors. Some web UIs are using Ember, others React, of several versions, and others are starting to use Vue. I could go on and on. Unless you literally spent all your waking hours learning all these technologies, I have no idea how a developer could be proficient in all of these various things, especially at the rate of change going on in the various technologies. We've had a couple high-level departures lately, so who knows if this could be contributing. I'd be beyond anxious if I knew that any given day I'd have to support some app that had all of it's layers written in technologies I had zero experience in. I agree that .NET development in 2008 was awesome, especially WPF (data binding!) and WCF. I was beyond productive, and I honestly do believe the lack of choices (WCF or ASMX...hmmm...hard one) helped with speed of development. I really feel that .NET has gotten really, really muddy over the last few years, as I've really started to lose track of what in the heck is going on with web development ([ASP.NET](https://ASP.NET) MVC? WebAPI? [ASP.NET](https://ASP.NET) Core MVC) or Windows development (WPF is dead...UWP...no, WPF is not dead...Core will support WPF and WinForms?!) anymore. I am hoping to migrate most of our software to .NET Core, as just the ability for an app to have it's dependencies localized, and not require machine-wide .NET Framework version, is pretty cool. I'm sure there'll probably be 3 more versions released as I'm migrating all of our code, and I'll have repeat the process forever. 
&gt; sure, SQL does not have semantic. But operations (stored procedures) gives semantic. If you have a set of 2 values A and B and two operations $ and % only implementation of these operations can show you the semantic of A and B (for example, 0 and 1 for + and *). So, this is also a reason why data should be bound with stored procedures. This means nothing - literally I have no idea why you're talking about maths here. It doesn't have anything to do to how expressive or suitable for application development SQL - a **query** language is. &gt; OK, I'll ask next question: if I have a big database, obviously (usually, 99%) developers of microservices will need consultations with database designers and their microservices very often will contain bugs. Citation needed. All software has bugs. There's nothing special about relational database design. It's CS101 stuff. &gt; But agree, it is much better if you get a base with all the business procedures over this data. The same as in algebra: we have sets with defined operations on these sets! Sets without operations are dummy: can you define a group without definition of operations on it? Yes, SQL is a *domain specific language* and it's domain, is set-theory based. &gt; And how to process data correctly, knows designers of the database. Not PHP/Java coders. Why do you think programmers don't understand a deliberately restricted and specialised syntax? SQL is designed to be easier to learn than a general purpose programming language. &gt; But I see that you often argue the relevance of technology, its modernity. My question is: do you agree with the fact that we have seen the dawns of many technologies and the excitement around them, and how they were thrown into the dustbin of history literally in 5 years. At the same time, there are solutions that continue to exist for decades... Fashion factor. Nothing to do with fashion - of course technology rises and falls. The existence of pencils didn't mean pens were useless. Just because the early 2000s featured a lot of incumbent databases, doesn't make them the right tool for the job. &gt; Today there is new trend: not NoSQL, but SQL + NoSQL, there are good examples how good old SQL can handle tasks solving today (fashion!) with ML :) This shows a fundamental misunderstanding of what NoSQL is - No SQL stands for "Not Only SQL" - and was a movement that encourage polyglot persistence to fit the shape of the data being stored. &gt; Modern RDBMS's handle "NoSQL" data too: objects, trees, XML, JSON, geo-data, etc. Languages for non-relative data are more and more close to SQL, so we see that SQL is enough to handle them too. So, SQL and relational model is back. If we have set of objects and naturally relations between them, why we should avoid RDBMS? Nope, this is database software behaving non-relationally. Nothing more. I'm a huge fan of hybrid databases like CosmosDB. All the approximations to cram SQL in there have been... less than ideal. People have been storing key-value blobs of data in tables since the mid-2000s while their bosses had no grasp of what object datastores were. &gt; What architecture do you prefer if it's not RDBMS centric? Key-value, document oriented? In my current job one of departments hit a lot of problems with NoSQL data and currently they will migrate to RDBMS (with some additional storage for special kind of data). Storage should be appropriate for the data stored inside it. If the data is fundamentally relational use an relational data store. If you're storing images? Files? Graph data? Don't. I believe that application data storage sits below the application, and it can change at the will of the application. The application **owns** it's storage, it doesn't use a monolithic, domain wide datastore with administrators and dedicated hardware. &gt; It's the experience. And today we see more and more examples when companies refuse to NoSQL Google trends disagrees. They rose to prominence in around 2010 - huge spike of interest - and has stayed consistent since. I can't think of a sophisticated business anywhere that doesn't use some form of a not-sql database. Do you use S3? That's NoSQL, BigTable? Cosmos/DocumentDb? Cassandra? EventStore? Mongo? Couch? Azure Blob Storage? A file system? Use SQL if it fits. You're advocating to cram everything into SQL - that's absolutely mental, and your software is probably awful for it.
We're still overusing it. The joys of a 20 year old application. 5,000 stored procedures
What Orms were you using in the early 2000s?
How many tables?
This. I've seen many p1 site outages resolved by DBAs changing a sproc.
Thanks. I am feeling insane reading the posts of "we don't need them" and wondering if these are new people who know NOTHING about database or.... what? The "deployment problem" responses again make me feel either insane, or again, do these people not have real world experience? Do they not know that other items in the que can log jam new code? Do they not know the values and abilities of databases? Anyway, have a good evening
Thanks for sharing you experience. Not sure why you are being downvoted.
My point was that you aren’t reinventing the wheel. You are making your data much easier to consume and adding support for dozens of features that any nontrivial app will want at some point.
nHibernate originally circa ~2005/6 - so early might be a slightly on the keen side, along with Castle ActiveRecord. Also used a closed source "DAO Generator" that was a proto-ORM at the time too.
I'd start here: https://app.pluralsight.com/library/courses/aspdotnet-core-fundamentals
IMO learn the basics first. Learn [asp.net](https://asp.net) MVC. After you have learned this and can make a good application with it, then learn a SPA framework.
Using EF for basic stuff, more complex is in stored procedures. Databases are in git using ssdt projects 
Ssms has a debugger. 
No, please don't suggest Angular/React use MVC - they are effectively MVVM but it's so transparent to the developer it's hard to really even call it that.
my bad. got my information from here https://medium.freecodecamp.org/a-comparison-between-angular-and-react-and-their-core-languages-9de52f485a76
I forgot to mention you can set TransparencyKey on the form and use a background image or custom painting to give it a custom shape. However this only gives you 1bpp alpha; I am not sure how some apps do it better unfortunately. Also items 2 and 3 I mentioned do involve messing with raw Windows APIs which is a bit of an advanced .NET topic so you may want to find code samples or libraries online if you're not comfortable with using the Windows API from .NET already.
Anything that normally requires a round trip. For instance, if you want to create a record and then immediately move it to an archive table except you need the ID to be generated, that is one instance I can think of on the top of my head
Sounds like there are no database tests in place. The tools for that do exist, although they're newer than code test frameworks
Sounds like nobody looks at the database management views either
FYI I feel your frustration. Our system has 30 million user records, 1000 tables, 5000 sprocs, 1.5 million lines of c#, 1 million lines of classic asp, and a main SQL db that is north of 6 tb. It developed before Orms were common, and we've never used them. We're extremely paranoid about what SQL runs in that DB. 
Yeah, the idea and implementation has some merit. For how crazy the idea can sound, it actually works. Problem is that the conversion from rows of data to XML can be tricky because you have to copy or modify the query to see the actual rows, otherwise they never leave the database. This can make it complicated to debug/maintain. There are obviously other flaws but it does kind of have a "separation of concerns" feel, if not for the XML conversion. With that said, doing it all in SQL is... probably not ideal, to be honest. You're much more limited on how you deal with the data. Lots of "tricks" to make things work as you want them to. Plus everything has to be stored in the database.
You're talking about the `///&lt;summary&gt;` comments?
Ah, I see the problem... you are working in an ERP system :)
If you start using optional parameters, you then need to consider indexing and performance and all sorts of junk. If you can be super strict, then maybe it's fine to treat it like a black box
I'd say go for react/vue MPA (not using SPA, just use as a library) for the data binding instead of using jQuery. Most of the stuff that is normally done using jQuery can be done in ES6, and using jQuery instead of a data binding framework is like using winform instead of wpf.
Essentially yes. I think we used `///&lt;sql&gt;` but really you can invent any tag name you want. 
Sure but you have to consider those things no matter where you do it. Thats why you test and the procedure should be designed with those things in mind. You shouldn't have to worry about anything blowing up when you call a sp IMO. If you do then somethings gone wrong in its implementation. 
I think you may be uneducated on this area. In Mysql you can not do any complex queries as a query. It has to be done in a stored proc.
As I mentioned above, in mysql you can not use complex queries as queries. You have to use them inside stored procs. It's your only option. Other systems like mssql might allow this though and that could be an example of where it makes sense to use just a query.
Do remember that SSDT only works with mssql and there are many other relational databases out there.
As mentioned above, this only applies to the one particular system that you have worked on. Other relational databases do not have this capability.
This shouldn't be a debate at all. Angular while being a great tech is way too hard to do many simple things that you can do in seconds with mvc scaffolding. It is easier to find real answers to the problems you will run into. I can see getting to it later but for a start? 
Since you've said this about 10 times already in this thread, and it's factually incorrect, I'm curious to understand why you keep saying this. It's clear to me you are mistaken but you must have some reason for thinking this and I'm curious what it is.
tl;dr; Not gonna happen in the near future. 
Because they don't. SQL treats raw queries the same as stored procedures. They both go through the same pipeline. End up at the same plan optimizer. Generate and run the same. The best you might obtain is some network overhead for the text itself.
There is absolutely no performance gain between making a stored procedure that joins 10 tables, and sending a query that joins 10 tables. The best you can hope for is a trivial amount of parsing overhead because the join is probably textually a bit longer than the EXEC call with parameters. But that's it.
Get a better database.
👌 
At least in MSSQL.
Ok man I’m not going to argue with you. That’s not been my experience for large queries that have a lot going on. 
Or the VP of Engineering with 20+ years of experience building multi-million dollar applications. But why bring that up? Right?
[https://stackoverflow.com/a/8559740/413967](https://stackoverflow.com/a/8559740/413967)
So, SSDT is better than nothing. But, for instance, it crashes my current application. Project is too big. Literally makes VS run out of memory, so we have to unload those projects while working on other projects. That's a fault of SSDT, yes. But in reality, it's not fixed, and it sucks. And also, our SSDT deploy pretty much takes down the DB. It wants to copy tables. Rerun CHECK statements. And since the SP contracts are pushed to a central place, you have to think about versioning during a deployment. My deployments now, for instance, are rolling across dozens of web servers. I can't ever change SP signatures, since half of the web servers would break, without a careful backwards/forwards compatibility dance. The point is it's HARD. It makes everything HARDER. And has no performance or maintenance benefits. So why bother?
A million dollar project at today's consulting rates is 3, maybe 4, full time developers for a year. As a C#/database developer I can personally support 4 to 6 UI devs, so we're already in to the multi-million dollar territory before the second backend dev is added. How many UI devs can you personally support? As in you are writing all of the middleware and database code for the rest of the team. 
P.S. bragging about your title is the worst possible response to someone who thinks you were promoted above your skill level
That's the point of using stored procs for encapsulating the database. It is one way to avoid tightly coupling the database tables to the application. I was taught that the application should never know the table names. I think that's a bit extreme, but I have seen it work really well. 
Home made dapper equivalents. That was pretty much the standard from C#1 through the introduction of Entity Framework. 
So the worst of both worlds? No thank you. 
Yes, and that makes me very sad. 
&gt; I have used stored procs with Dapper I absolutely love Dapper. The only limitation that I have come across is detecting if records have changed before I hit the DB (I know EF can do that). However, I was able to work around it, but honestly, it's my go-to for ORM in C#. 
EF Core doesn't support them. A huge oversight in my opinion and one of the reasons I made my own orm. Does EF really have them? They didn't when I last looked.
I like SPs for somethings, but not everything. For example, populating a dropdown from a SP. That way, I do not have to hardcode any values, and if I were to add another option in the DB, then my dropdown would reflect it automatically. At least, that how we do things at my job. I am still technically a junior dev, but my question for you, how would you avoid things like hardcoding and whatnot? 
Why are we talking about this? 
And judging somebody's skill level based on a single hard line position of disagreement is.... Well, it's fucking stupid, right?
Wait, you think... what, initializing an \`SqlCommand\` and putting some text in a variable and some parameters is "bad"?
Depending on your db engine, raw queries are not treated the same as stored procedures. This is a fact. Mysql, the 2nd most used database (above mssql and postgres) does not have this capability to use control flow functions within a raw query. You can say "get a better db" all day but it doesn't make you right. Of course you already know this since I already told you and you chose to ignore it.
Boredom. 
No, I'm judging it on a single hard line position that makes my job more difficult than necessary every time I encounter it. Ever notice how much time I waste on reddit? It's because I am allowed to fully utilize my database and my ORM without stupid rules dictates by underqualified management. Take away my procs and you cut the number of UI devs I can support by half.
SSDT (usually) verifies that the columns I'm using actually exist during the build process. And all of the db code is on one place, not scattered all over the app code. And I can break up long procs with sub routines. And they have names so I can easily correlate them to performance logs.
I can't speak to elegance but for me the answer to "Should I use AJAX to call a HTTPGET route that then queries the model and returns a true/false?" is yes. onblur = client-side = do it in the view Intermediary between the client (ajax) and the database (model) = do that in the controller database stuff = do that in the model.
Easy enough then. Thanks for the quick reply.
That's an interesting take on the matter. For single-application databases, the business rules are the business rules. If they're in the database or the one application that owns the database then so be it. But when then database is used by any number of applications, it becomes less and less likely that any one client application will precisely implement those business rules. The alternative, business rules enforced through the use of constraints, results in inviolable integrity. &amp;#x200B; I think the problem is that people treat data like they treat code. In theory your data is already the representation of some subset of the world, a series of facts in accordance with the closed-world principal, which means that it isn't subject to whimsy in terms of its structure (at least if you've designed it in accordance with standards such as IDEF1X).
Right but with the stuff I’m doing we’re shoving things into temp tables. Filtering data. Joining it with other crap and we’re doing this at scale Etc etc To try to do all that client side is less efficient and harder to maintain than doing it all in a stored procedure 
Testing, debugging and general tooling is much better in C#.
I don't have any experience with Metrocharts. I use Livecharts which are pretty easy to use, and there's lots of examples on how to use it. 
 thanks for the idea.. I'm checking it out now. In all honesty wpf isn't really my thing, never really tried to migrate from using forms. Just now with all these resolutions around now its hard to avoid 
Haven't got the code to hand, but it's easy to globally disable client side query execution in EF Core. I think it was a bad decision having it on by default though
&gt; This means nothing - literally I have no idea why you're talking about maths here. It doesn't have anything to do to how expressive or suitable for application development SQL - a query language is. This means that to have data + operations on it is better then to have data. DDL + stored procedures is such example. It's better then to have only data. Semantic is: - relations (remember frames and graphs as representation of knowledge bases, predicates calculus with their relations) - and this is exactly RELATIVE model (not NoSQL and other shit) - defined operations (their analogue is stored procedures). So, math says us that data + stored procedures is more correct and more valuable then data only. Or data + operations somewhere in PHP code-garbage &gt; Why do you think programmers don't understand a deliberately restricted and specialised syntax? SQL is designed to be easier to learn than a general purpose programming language. My point is totally in different area. I'm talking about domain area. Persons who designed database has different positions, knowledge, qualification, responsibility. OK, imagine database with all data of some big financial company. There are good payed specialists which knows "dead" and "outdated" instruments to manipulate or redesign/design data. They have knowledge of the domain area. And they can design business procedures correctly. For example, to transmit some money from one bank to another or to pay salaries, to lay off employees. This will affect a variety of tables and databases. Of course, you can trust to write this logic to PHP/C#/Java code monkeys. But better (trust me) is if code monkeys will write only client applications for this logic. And this logic will be written by professionals which usually don't know PHP/Ruby and other Node.js. And question here is: where will live their code? In stored procedures and views, sure. This is not question about complexity of PHP vs SQL. But if it was, you are not very right too: most "just for lulz"-coders don't know SQL on enough level. &gt; S3? That's NoSQL, BigTable? Cosmos/DocumentDb? Cassandra? EventStore? Mongo? Couch? Azure Blob Storage? A file system? I suppose you don't get my accent. I don't mean that these solutions are bad or should be avoided. I mean that modern relation databases don't lose the positions and there are areas where is good one model and there is another one - where is better another model. I accept you vision, but you don't allow that RDBMS are yet demand all the same. A huge number of providers in telecom used relational databases and continue to use. And the position of relational databases has barely been shaken and is already being restored. Of course, they have their own field of application, they are not "everything in the world". But I have at least one example, when NoSQL database is replaced by relational one, and I read about similar examples in the Web. If NoSQL is not only SQL (this is what I tried to pointed) this mean that it's NoSQL + SQL (let's call it relational). This was the point: Mondo is not SQL/relational database, and traditional RDBMS began to serve NoSQL data too, so trend is: SQL databases grow up and fill the NoSQL area too (they began to support XML, JSON, REST, etc - as I mentioned it early). For example, previously we have relational model only in RDBMS. To store objects we used Cache DB or ORMs. But today traditional RDBMS began to support objects too. Good example is PostgreSQL which supports also: - JSON - Arrays - UUIDs - key-value - geometric data - composites (structs) - etc
I've developing all my .net apps in Mac,2 in production and one is still in development. Also I've developed https://github.com/sqlkata/querybuilder with 45.5k downloads till now on Mac. using: - Visual Studio Code - SqlServer for Mac (before it was released I was using Postgres) - DataGrip (alternative to Sql Management Studio), DBeaver also is a good option, Azure Data Studio is also a good option. 
May be I did not get you very well, but this is very clean situation for testing and debugging if right architecture was chose from the start. At the end, you work with data only, and don;t involve any effects, so it's question of possibility to handle data. - you can log queried data in any format before to translate it to XML (for debug purpose). - you can process queried data with filters (that can log/acquire statistics, fire events on wrong/invalid values) before to translate it to XML (in the process) - you can have run-time checks (contracts) on generating XMLs which will fail (in the process) - when you have schema you can validate your data easy (even better then in languages like Haskell, at least because schema supports refined types also) - when you have schema you can even generate randomized values (similar to different test frameworks) You need only to remember that you can have some variability in the representation so comparison will be like of floats, with some acceptable epsilon. Sure, I dont know all details of your tasks, so these are only superficial assumptions that came to my mind 
Probably because my opinions don't chime with most of the others posted here (i.e. stored procedures = bad, ORM = wonderful).
But, do we need that? Thats paging basically, and normal paging with Skip and Take is more efficient than fetching row by row. If i understood correctly how iasyncenum should work
Specifically, which basics would you recommend learning first?
The goal is avoid calling `await DBReader.ReadAsync()` in a tight loop because it is inefficient. This isn't the only way to achieve the goal. Offering a ValueTask alternative to ReadAsync's Task based design would also work in theory. 
With ASP.NET Core you can primarily build one of two things; a) an *API* or b) a *Server Side web application*. An API (a) will simply handle HTTP requests and return data. This is useful if you plan on building a separate front-end (using React/Angular etc.) A server side web application (b) is where MVC (or Razor Pages) come in. In this scenario you create your front-end using ASP.NET. Specifically, you can create Views (or Pages) written using Razor (which will handle things like binding HTML to your models). ASP.NET will take your data, render it using your views (all on the server) and return the resulting HTML so the browser can render it. Now for the good news, a lot of what you learn for either a) or b) is shared with the other. The underlying fundamentals (routing, model binding, using models) are essentially the same (or very similar) whether you're just going to return the data or return a view. If you go down the MVC/Razor Pages route there are less moving parts to get something up and running (because you don't also have to run off and learn React/Angular/Vue etc.) If you already know how to build front-ends using javascript, the API route is equally viable. If it helps I tried to address this question [in a bit more detail here](https://jonhilton.net/what-to-focus-on-when-learning-aspnet/) :-)
[removed]
I hope you don't mind me asking you a question But I have given it a go. How would I change the value in one of the values in the button push. Code below. XMAL: &lt;Grid&gt; &lt;Wpf:PieChart x:Name="TestChart" HorizontalAlignment="Left" Height="268" Margin="165,78,0,0" VerticalAlignment="Top" Width="469" LegendLocation="Bottom"&gt; &lt;Wpf:PieChart.Series&gt; &lt;Wpf:PieSeries Title="Maria" Values="0.5" DataLabels="True" LabelPoint="{Binding PointLabel}"/&gt; &lt;Wpf:PieSeries Title="Charles" Values="4" DataLabels="True" LabelPoint="{Binding PointLabel}"/&gt; &lt;Wpf:PieSeries Title="Frida" Values="6" DataLabels="True" LabelPoint="{Binding PointLabel}"/&gt; &lt;Wpf:PieSeries Title="Frederic" Values="2" DataLabels="True" LabelPoint="{Binding PointLabel}"/&gt; &lt;/Wpf:PieChart.Series&gt; &lt;/Wpf:PieChart&gt; &lt;Button x:Name="TestButton" Margin="1,1,650,11" Click="TestButton_Click"&gt;Click Me&lt;/Button&gt; &lt;/Grid&gt; Code: public partial class MainWindow : Window { public MainWindow() { InitializeComponent(); PointLabel = chartPoint =&gt; string.Format("{0} ({1:P})", chartPoint.Y, chartPoint.Participation); DataContext = this; } public Func&lt;ChartPoint, string&gt; PointLabel { get; set; } public IChartValues Values1 { get; set; } = new ChartValues&lt;int&gt;(new int[] { 3 }); public IChartValues Values2 { get; set; } = new ChartValues&lt;int&gt;(new int[] { 3 }); public IChartValues Values3 { get; set; } = new ChartValues&lt;int&gt;(new int[] { 3 }); public IChartValues Values4 { get; set; } = new ChartValues&lt;int&gt;(new int[] { 3 }); public SeriesCollection SeriesCollection { get; set; } private void TestButton_Click(object sender, RoutedEventArgs e) { } /// } &amp;#x200B;
I'd say in your individual case you've ended up in a car crash scenario, and you should probably go back to brass tacks with SSDT or just try something else. There are a ton of advantages, and if you are not experiencing them, you're probably doing it wrong. Stability sucks too, I have to agree there, but carefully managing dev environments is a must anyway. Sure, the deployment can be a tad slow. I like a ton of people queried this amongst the community and I had to agree with the response: Sure it can be slow when it builds the scripts, but the advantages massively outweighs the slowdown.
sure it can, just add a ; in the web front end then 'tailor' your query right there for convenience. A million injection attacks can't be wrong ;)
SQLS certainly doesn't treat them the same, but the differences are subtle. One of the mechanisms is 'parameter sniffing' https://hackernoon.com/why-parameter-sniffing-hurts-your-sql-query-performance-d73c0da71fbc So stored procs tend toward having compiled statements with parameters supplied via variables. Some ORMs will use parameters with an 'anonymous' query i.e. one that gets compiled at runtime, because it got passed from the application. I've seen some ORMs inject the parameters right into the query string, which can cause cache hits and defies efforts to pin plans or optimise them. Understanding the difference and using the tools provided is the way forward, not blanket statements about what is/isn't the 'right way' 
They don't there is a nuget package someone made to hook up TVFs in EF though.
ah, so thats much lower leven than Linq skip and take. Thanks of explanation! 
A little unsure of what you're asking for here. If you want to change the text/value in XAML on the button you set the content="value on button": `&lt;Button Content="Value on button"&gt;&lt;/Button&gt;` If you want to change it using C# you bind the content value to a property in your C# code like: `&lt;Button Content="{Binding setValueOfButtonProperty}"&gt;&lt;/Button&gt;` &amp;#x200B; Does it answer you question?
Hey, Is it possible to see the code for the Update form and the MeetingUpdateDTO class? Might help shed some light on the issue :-)
Oh sorry, when I read it back it doesn’t make sense. I meant if I want to change the value of Maria from 0.5 to 1 when I press the button. I’m just playing around with the chart to see how different things work. 
&gt;You need to use databinding. You have to learn it to develop WPF apps. Instead of setting the values directly in your XAML code, you bind the value to a property that sets and updates the through C# code. &gt; &gt;So you'll probably need to bind the button and the value of Maria to each of their properties in C#. That way you can add 0.5 to Maria's value when the button is clicked. &gt; &gt;Maybe start by finding a simple WPF application that uses databinding so you can get a feel for it. There's tons of vidoes and examples out there. 
Thanks for the advice. I’ll have a look. Cheers 
Wait I thought this was the primary use case? To process rows as they came in? 
There's so many factual and technical inaccuracies and blatant lies in that article I'm kind of staggered...
I don't think so. It's just a bug. Sometimes sub queries get executed one by one (for each row) instead of just being a normal sql subquery
In EF, you always want to fetch in batches of 10/100/.. depending on how much you need in the app. Doing db trip per row kills perf, and even exposing IEnumerable is kinda anti-pattern because of SELECT N+1 problem. 
There's SQL test from redgate. There's tSQLt. They fully allow database testing. They are not however as widely used as test tools for application code. But they totally exist. 
Check out https://tsqlt.org "Output can be generated in plain text or XML – making it easier to integrate with a continuous integration tool"
And they're.... "Ok" - you're right though, it's a cultural problem.
Interesting debate, but we're certainly not going to agree. An eye opener.
You can change SPs signatures. You name the new one Foo_v2. You have new code calling Foo_v2. Old code continues to call Foo. Then you get rid of Foo after the deployment. I can't imagine why it takes hours to roll your code across web servers. That is an odd issue. What process do you use? Anyway, you should be able to deploy new code ahead of time into a differently named folder. Then you just need to switch folder names. You could do that in a powershell script or similar.
What size database? Do you keep your system live during deploy? I've seen those crazy issues even in SQL compare from redgate, where rather than change a data type it will copy everything into a new table. Not awesome when there are millions of rows
Our last CTO went on a stored procedures are evil kick. Terrible for the company. No new sprocs. So no efficient use of table valued parameters being passed in from app code for retrieving a specific set of 20000 users. Nope, let's do that in a simple query and use a 20000 item in list. That fucker
Stored procedures and prepared statements are similar. But raw queries are compiled every time. That's a big CPU hit on the database, though you may not see a big difference in time on per call testing. Stored procs being cached makes the DB act like c#, direct queries being compiled every time makes the DB act like classic asp. Don't make the DB act like classic asp 😭
My one caveat would be that since you are taking form input data and sending (POSTing? lol) that to your controller to do some checks, I think it should be a HTTPPost AJAX request. I typically leave HTTPGET for resources that are not taking user inputs. 
Since when does being a Javascript developer make you a fullstack developer? I have yet to see a single well working SPA, the best thing is to stick to Razor and use as little JS as possible
I'm a VLDB consultant so everything from a few rows to multiple billions. There was an update not so long ago where if you added columns to large tables, they didn't have to go as the last column, and it didn't trigger a big copy operation like you describe. If you simply HAVE to do that kind of table change, then unfortunately you just have to suck it up. I would script that in the PreDeploy and PostDeploy scripts anyway rather than just letting it fly at it automatically, but then per the above I'm well used to dealing with mammoth DBs.
https://pics.me.me/you-were-so-busy-wondering-if-you-could-do-it-3589967.png
Don't do it client side. Just maintain good SQL code in your C# or whatever, and send it from the app.
Right, and this is the end of scalability. SQL ends up always being a central point where horizontal scalability is limited. You don't want to put more logic in there than you absolutely need. You want to push it as close to the edges as possible, where you can scale horizontally. More app servers.
You can continue to put your runtime SQL statements in one place. Perhaps a shared library, like we do for every other type of code and which works fine?
Because it's big.
tSQLt can test them. Redgate has a SQL test app too. If blocks based on parameters are the devil. First time the sproc gets compiled if you have the less popular value in your parameter you then end up with a cached plan that stinks for the more popular option. Where clause conditions should be preferred to that. But two different sprocs would be better. If they share a block of activity that can probably go into a separate sproc or a view. Modular just like app code.
I forget how small databases are at most places. Ours range from 0.5 to 7 TB
anyway, thanks for posts
There is one case that's easier with queries in the code - deployment. Your code is the only thing relying on that query, so you can update it as you like as your needs change, while sprocs end up as shared resources. And you never have to deal with the window where the DB and code is out of sync. For us that's usually an hour or so, and we have to do lots of dancing to keep everything backwards compatible.
Making an API per sproc is often not feasible 😁
I wonder what that was like. nHibernate shards was still only halfway implemented when we looked at it a few years ago
Yes, but all applications end up as legacy. EF is probably seen as legacy for people who only use EF core
Yeah, its pretty normal for enterprise grade stuff, its been an adjustment to realize while enterprise stuff runs the planet very few people understand it (there just arent that many massive systems and millions of techs) in compassion to the number of technical folks running around with certifications. 
What database are you using with your 60 TB dataset? We have scalability issues with a database 1/10th that size.
I like it! I was gonna go with https://i.imgur.com/GtKvLYZ.jpg
As others a mentioned, MVC is a pattern we use in web development not unique to [ASP.NET](https://ASP.NET) Core. I'm a relatively young developer in the workforce having graduated two years ago, and from my experience thus far, there is a huge flux of new age developers flocking to JS frameworks like Angular and React (we use Angular at my company with a Java backend). &amp;#x200B; The fundamentals are all the same no matter what stack you choose in web development. From an ease of learning point of view, stick with learning Razor Pages first, and if you feel up to it, MVC alongside it as that, coupled with WebForms (at least from what I hear, before my time), were the primary motivations in the development of Razor Pages. The [ASP.NET](https://ASP.NET) team seems to be putting all their eggs in the Razor Pages basket going forward, so you'll put yourself in the best position of being at the forefront of new .NET technologies. &amp;#x200B; With the advent of Blazor, we'll finally get the sanity of .NET on the frontend. That being said, however, JS frameworks are not going anywhere. My advice would be to learn Angular 2+ after learning Pages, as you get TypeScript integration for free and you'll find plenty of similarities between the two making it a little easier to learn that React I think (if you have the misfortune of looking through a 1000+ line long JSX file, you'll leave with PTSD). 
Or you could just not use EF
EF has its purpose, strong and weak sides. Can be really good if you know how to use it (as any lib), or it can explode if don't, but its not EF fault, but yours. There are other mappers like Dapper if full blown ORM is not needed. 
Great, you take gazillions of manhours worth of exploration into garbage collection and smash it into.... smart pointers. I mean seriously. This project could be great, but don't act like this isn't going fuck any actual "Enterprise" application up like an 80s slasher movie.
Sure. And then we can add names to each of those SQL statements so that they are easier to find. And then maybe we can deploy that shared library to the database so that our applications can refer to those names instead of sending the entire SQL statement over the wire each time. Oh look, we've reinvented stored procedures. 
I just finished a project where a major corporation hired a third party to automatically port their cobol code to C#. And the framework that the C# used was written in C++. It was a disaster... 150 C# classes each between 15,000 and 25,000 lines long. They usually generated C++ for unix. But for this project, they decided to change their generator to create C# instead, since that was a requirement. There were all kinds of performance problems. For example, when copying or parsing strings in C++, you usually just write from one memory location to another using character pointers. This is very fast and efficient. But in C#, strings are immutable objects. There's an indexer you can use to get at individual characters in a string object, but it's read-only. If you want a writeable indexer, the StringBuilder has that. So that's what they used. All strings in the generated code were StringBuilders. What could go wrong? In the generated C#, any time they had to do a string compare, they had to call the ToString function on the the StringBuilder. So the code looked like Variable1.ToString() == Variable2.ToString(). The ToString on the StringBuilder is really slow. Normally this isn't a problem because you do it once at the end of all your processing. But they were doing it millions of times a second. To say it was slow is an understatement. Converting code may be a convenient way to port an application. But it should be considered a starting point for manual optimizations and enhancements from that point on. You shouldn't plan to get a finished product just by converting it.
Are you like doing it one server at a time? Without compression? 
We have plain SQL files applied for everything, almost never have to take the site down for deployments. Just have to be a bit fiddly to avoid things that take long schema locks - add as Nullable, fill in, quick small transaction to fill in New rows and switch to non Nullable, that kind of thing
Only 994 now. We cleaned it up! 😁😭🔫
Wow, that is a hell of a ratio. I don't know how many SPs per table is reasonable, but I'm pretty sure its a lot lower than that.
This is almost as good as the IRS trying to programmatically convert S/360 Assembly (1960s era IBM mainframe) to Java. Futile. Easier to start from scratch.
Code-converters are generally never worse the hassle as even if syntax can be similar, idioms generally aren't. Also, not sure of a scenario where you'd ever want to do this.
&gt; But in C#, strings are immutable objects. If it's not an interned string you can pin it and modify the contents in unsafe code.
Yeah, I did a demo of that for the third party company. They ended up not using it. 
Why would you *ever* do this? I bet you could count the amount of projects that have gone this route on one hand, and I'm sure it was the wrong decision.
I'm not following. Yes, they are. But you want the release dll to end up in your nuget package. We normally pack both, but only put the debug package in our sandbox, so devs can switch to it when they need to step through into a library
I put your code in exactly as is and the grid rows come out sorted properly. What is the actual problem you are having?
Yeah. This is big problem lmao
The list I'm binding to is unsorted. The view comes out in unsorted list order, not alphabetically ordered. Clicking the column header sorts the view in alphabetically order as expected. But that latter desired behavior has nothing to do with CollectionViewSource. I've been able to set the default view sort in XAML in other cases. But only when my column binding properties have names. And that seems to be the problem with scm:SortDescription - does not seem to like "." binding. 
Don't do that. Parts of the CLR assume that the string is immutable.
[removed]
"Xello World"
How beginner are we talking? If you know how to write code in other languages, just look at some examples of React and Angular and pick which one you like. I prefer React, but both are good frameworks. If you are total new to programming, I would recommend NOT learning a JavaScript front end framework at all and concentrate on the fundamentals. Just get your program to do something, anything, then keep adding to it.
your code does the sorting for me \[C# 7.3, .NET4.6.1\]. But try replacing "." with "/", which points to the current record according to PropertyPath docs \[[https://docs.microsoft.com/en-us/dotnet/framework/wpf/advanced/propertypath-xaml-syntax?view=netframework-4.7.2#current-record-pointer](https://docs.microsoft.com/en-us/dotnet/framework/wpf/advanced/propertypath-xaml-syntax?view=netframework-4.7.2#current-record-pointer)\] Also try omitting PropertyNamecompletely &lt;scm:SortDescription /&gt; Any of these versions of the code sort the unsorted underlying List&lt;string&gt; for me.
Its always something stupid. Bad: InitializeComponent(); ProductConfigList = LoadProductConfigList(_LastPCPath); Good: ProductConfigList = LoadProductConfigList(_LastPCPath); InitializeComponent(); Thanks for the help. The fact that you got it working made me try a few other things. ProductConfigList does implement INotifyPropertyChanged pattern, but changing it doesn't trigger the view to resort. 
Wouldn't you use a view for complex read operations or is there an advantage to a stored procedure for that?
If you don't believe me, just go look at StringBuilder's reference source. It fixes `string`'s to `char*`'s and modifies them.
I was referring as to the combination of .Net core and JavaScript/ other frameworks for front-end or if you should stick to front-end technologies which come "out-of-the-box" with. Net core (like razor pages)
&gt; If I test this case with regex storm or regexr it passes there Show proof because that pattern doesn't exist in the text, so it will never pass
Not true: the StringBuilder constructor takes the pointer and copies the underlying memory: https://source.dot.net/#System.Private.CoreLib/shared/System/Text/StringBuilder.cs,256
you need to escape special string chars like the "\" you have at the end there.
So double up on the \ at \\b?
yup or make it a literal string like you did the first bit. Personally I prefer using the $ and tokenizing the string. 
I am just figuring out Regex today. Can you show me an example? Or do you think I've got it here? string pattern = @"(.*?([" + QuickSearchTerm + "]\\b))$";
You need an @ on the string literal after QuickSearchPattern+. It contains a backslash.
First and foremost, you're using the wrong tool for the job. "2019 SLIM" does not exist in "2019 OTHER THINGS SLIM OTHER". You are trying to get modern search behavior, which is more than just a pattern match. To do it properly is actually rather complex. You could fake it with a regex by changing "2019 SLIM" into "2019|SLIM". This doesn't give you any of today's expected Googlish behavior like * ordering the matches based on the strength of the match * finding lower-confidence matches that were 1 character different. It would also match "SLIM something 2019".
I have found this to be a very handy tool for checking regular expressions: [http://regexstorm.net/tester](http://regexstorm.net/tester) &amp;#x200B; The bracket is used for matching characters, not strings, so you probably don't want that. "2019 SLIM" doesn't appear in your test string and it's not clear why you think it should. If you want to test for "2019" followed by "SLIM" then you might try this &amp;#x200B; @".\*2019 .+ SLIM" &amp;#x200B;
Yeah. My hope was for the system to be able to say, oh yes that string contains those words(regardless of order). Let’s call that a match 
I have over 350 servers in my app. Multiple regions. There are 40 individual databases involved. 6 SQL server clusters per environment.
Right, except, we don't need to do the last part because it just makes it harder.
MS SQL Server.
Look again: https://source.dot.net/#System.Private.CoreLib/shared/System/Text/StringBuilder.cs,369
See the updated edit above. I ended up going down the route of using a class that has the shared properties being bound to to circumvent this issue. My guess is that when model state validation is occurring, the page model is being evaluated rather than the individual properties (i.e. update/input dtos). But it would be nice to confirm if there is a way to prevent that.
You need to escape the \ in the \b like this `string pattern = @"(.*?([" + QuickSearchTerm + "]\\b))$";` also you are missing a `;` on the `MarketingName` line, but I guess that is just a copy paste error.
Should look something like this $@"(.*?([{QuickSearchTerm}]\b))$" 
It's only hard in your imagination. 
You forget, I have a project with thousands of SPs. It's hard in my reality.
WOW!!! That is COOL
I've worked in places like that too. And I've worked in places where they had just as many stored as const strings in a C# app. Simply moving them to another place won't reduce the count. If I may be so bold, are you using schema? I find that one of the biggest problems with even relatively modest databases is that they dump everything into one place (e.g. dbo in SQL Server) instead of breaking them out by namespace/schema.
There are over 40 individual databases involved in this project. Oddly, no schema's, though. But the app is broken down into pieces. Hey, I want you to understand something. This app is around 150 servers in it's production environment. The VS solution for the entire thing is about 250 individual projects. There is C#, VB, C++, COM components. .Net. MVC, Web Forms. Classic ASP. Each of the 40 databases are part of the source. Deployed with heavily modified SSDT (custom post deployment filters). The team is over 50 developers. It's run for 20+ years. We have market monopolies across a few ENTIRE STATE GOVERNMENTS. This isn't small shit. This is huge shit. The app can take upwards of 6 hours to deploy completely. The production environment has 6 SQL clusters per region. I can barely tolerate ANY downtime. Deploying a single schema change is a massive effort. Not because the tools are lacking, but because of the centralized nature of SQL itself. If I update a SP, I have to do it fast. Because schema locks. If I lock schema for more than half a second I have thousands of requests pile up. There's something like 60TB across our largest region (that's just SQL data). Database restores are unheard of. They'd take days. Everything we touch in SQL is a problem. Literally, any change at all, to anything SQL related, screws us over. Since deployments take 6+ hours, and I cannot tolerate any significant downtime, I quite often have multiple versions of the same code running at the same time. If I add a new stored procedure, I have to do it in the release BEFORE I add the code that uses it. Because the code roles out across systems. Half of the systems will be on the new version, half on the old. Changing a stored procedure's signature basically just isn't possible. Nor can I use SSDT on it's own, since we have to customize how it acquires locks (it can't lock the entire database while it applies updates). It has to move object by object. Which means I have half versions of SCHEMA running at the same time. Again, everything we do with SQL introduces some new problem or another.
To Java? Wow. Maybe to JVM bytecode but still. Yeah, write some tests then write a new implementation that conforms to the tests. 
Yeah, I think an authentication handler middleware would suit your needs pretty well. This was the closest example I found on GH: https://github.com/xavierjohn/ClientCertificateMiddleware
I know this isn't the answer you're looking for, but if you're building a proper search functionality into an app you should drop regex immediately and look into Elasticsearch (or any of the other lucene based data stores). You're headed down a bad path full of insurmountable tech debt. Elasticsearch is an absolute pleasure to use, on the other hand. 
Calling the string.Contains() method for every word is not advanced enough?
Personally I have long lasting love-hate relationships with MSBuild. It's very powerful but quite complex (and hard to debug ) scripting and build engine. From what I recall - I definitely can recommend this blog http://sedodream.com/ (and author's book, although I read only few chapters from it) - it has quite deep internal details of how msbuild works and what one can do with it. Most of the time when I had issues with msbuild I either end up on Stackoverflow answer by Sayed (blog author), his article or official MSBuild documentation itself. Once you get key concepts - documentation is good enough to answer technical questions, but to see what you can do with this engine - check the blog link. I haven't seen many developments in this area and it creates a sense that MSBuild either reached it's plato and won't be extended or everyone heading towards code-driven build systems - PSake,Cake, Fake,Nuke, Nake etc. Even dotnet is "dotnet build". Simplicity wins over complexity and MSBuild is no longer a hill I want to storm again :)
[removed]
&gt; The app can take upwards of 6 hours to deploy completely. So putting logic in the application is even worse than putting it in the database if you need a fast change. I have no doubt that SQL is a major problem for you given these circumstances. But at that scale everything is a major problem. Realistically, most of the people in this forum aren't at anywhere close to that scale. They are in places where stored procs can dramatically improve performance and reduce developer effort if they learn how to use it right.
\&gt; So putting logic in the application is even worse than putting it in the database if you need a fast change. How in the world would I change the SQL fast? Just directly edit it, circumventing the SDLC? I've heard this so many times. "If you put it in a SP, you can just change the SP without redeploying the rest of the app!" You're suggesting we architect the system in such a way as to make it easier to circumvent the SDLC.
Who said anything about the circumventing the SDLC? You can still go through the SDLC process and deploy just one stored procedure instead of going through a 6 hour application deployment process. &gt; And again, SPs do NOT INCREASE PERFORMANCE. AT ALL. They do compared to using ORMs, which is the primary alternative. Sending stored procedure like SQL down the wire with every request is very unusual. It may be the right thing for your company, but for the vase majority of the industry it makes no sense. 
Yes, that is on creation of the final string. But it when initialized with a string, it is *not* modifying the original string, which is what I thought you said. Also note that in this procedure a special internal method is used for allocation the string.
An idea I have been considering is setting up a database that can query a table on another database as well. I think it's called remote tables but would have to pull my notes. MS suggests using it to setup sharding so you can query multiple databases at once, which is nice if you have want to query a system with per tenant databases and want to query all of them at once for analytics. I've considered adding it to each tenants database so I can query for things in a master CRM database where other user info is stored instead of duplicating data between 2 places as another developer started doing that. But now saves are no longer atomic and the idea got put on the back burner for more time sensitive items. Essentially a database can query another database for you as if it was a table in that database and let you do joins on them and get the final result from a single database.
So like going down the path of domain driven design? But what about setups where you have per tenant databases? I'm afraid to eat up our SQL pool capacity. I don't even know if going down the path of having a read and write databases with db replication so locks don't become an issue for long running reports is something I can get the business &amp; infrastructure side to agree to.
I feel like with how powerful linq is, and I am just now getting really deep into it, I don't see how you can't recreate a SP with linq and with that most linq requests haven't an lambdo sytanx extension method. If you get a firm grip on how the SQL planner breaks queries apart and have a good plan for building linq like that upfront with smaller part I feel
EF 6 will run within .NET core in the next major release, and EF Core is expanding it's feature set quickly and I'm wondering what will happen when EF6 comes over. I can select entire business entities with their child entities included and limit it's select queries down to the where the joins will happen. Understanding how the query planner will break it down the query into smaller parts allows you allows optimize pretty well.
But \`dotnet build\` is using msbuild to do the actual building...
Sounds like you might need to start a blog :) &amp;#x200B; Have you seen [https://github.com/onovotny/MSBuildSdkExtras](https://github.com/onovotny/MSBuildSdkExtras)?
EF supports acid transactions with the use of the context?
Sharding wasn't really that common in the mid-2000s (and honestly, for the vast majority of enterprises still isn't, write and read replicas is often simpler) - the vast majority of organisations were running monolithic SQL datastores - you could roll your own identity + NHibernate session provider if you wanted to do sharding though. NH was really stable very early, the learning curve was a bit steep (people fell into N+1 traps easily - loading the whole database by accident, or didn't understand the session per request pattern) - but it was really performant, easy to tune and has plenty of features EF doesn't have today. The Alt.net movement was pretty big in ~07-08 and we had quite a lot of cool stuff.
I meant making an API instead of a sproc.
Quite the opposite actually. The general direction is deeper msbuild integration. The dotnet cli is merely a superset of msbuild which calls msbuild internally.
Why do you say that? Can you elaborate on why it would be a bad idea?
Good abstraction that he didn't notice that.
I found MSBuild quite easy to debug. Just turn up the verbosity level, up to even being able to view each statement and their result in the log. 
Msbuild structured log viewer is everything you need. It'll improve your productivity with msbuild by orders of magnitude. And it's open source, free... written by one of the visual studio for Mac guys. You just put /bl at the end of the msbuild call and take the output msbuild.binlog to the viewer. It shows how everything is evaluated.
It is actually! I’m splitting the search term on spaces, then counting the number of times that the marketing name contains the terms and if the match count / # terms is greater than 60% I’m counting that as a successful match
This stuff? https://docs.microsoft.com/en-gb/ef/ef6/saving/transactions I have to say I'd missed those improvements... that is cool. But the main issue I had with transactions was using stored procedure mapping to add apis to an existing DB where the sprocs already had transaction control written inaude them. When transactions rolled back they also ended the wrapping one started by EF which would confuse the poor thing. Ended up having to take that code out and rewrite it into the controllers. Not the end of the world but it was more work that crept out of the shadows on that project. 
Wow, didn't even know that. I love it, that makes it even easier.
You would want to do it if you develop a game using Unity3D. They have a compiler which compiles IL to C++ ([il2cpp](https://blogs.unity3d.com/2015/05/06/an-introduction-to-ilcpp-internals/)). &amp;nbsp; When using il2cpp there is a preprocessor which does a static analysis of the IL code and removes all symbols which are not referenced. This makes the final binary much smaller and is beneficial on mobile and especially browser (WebGL) platforms due to smaller download size and faster initial loading time. il2cpp builds offer greater performance over running a .NET runtime. (compiler can spend a lot of time optimizing: no JIT which needs to make compromises) Additionally, it is possible to take advantage of LTO for some platforms. However, there are some caveats: although reflection works fine, you need to be careful with symbols which are only accessed through reflection as they are (probably) stripped out in the build. If the symbol is in your own code you can put an attribute to prevent stripping, however, this is not possible for library code. (eg. `TypeDescriptor.GetConverter(typeof(TOut))` will throw a runtime error, as the ctor of `TOut` might be stripped away). Creating il2cpp builds takes a loooooong time, especially production builds. So a fast write -&gt; build -&gt; debug loop is not possible. However, as a Unity3D developer, most of the time is spent debugging in the Unity3D editor, which is not a problem as a .NET runtime is used. It only becomes a problem if you have a platform specific bug which you can only debug on that platform, then you need to wait for the il2cpp build every time.
So now you want me to customize my SDLC process per change. Basically two: one for DB changes, one for other changes. Every solution you come up with is just more work. You're asking me to do a thing, that provides no benefit, makes my life harder, removes capabilities, that introduces additional human costs. For reasons I can't divine. What if this new SP change breaks during a deployment. It takes down all nodes calling it. Where as before, if the query breaks sent from a node, I can just rollback that subset of nodes. In fact, that's why the deployments are so long. As we roll out across clusters, we monitor for increased error levels on nodes. And if they cross a threshold, we roll those nodes back. &amp;#x200B;
&gt; it is not modifying the original string, which is what I thought you said Not before claiming "Don't do that. Parts of the CLR assume that the string is immutable." You can new up a string (which is then not interned), fix it, and modify it all day long. You don't need to use the internal string.FastAllocateString to create the string. It's a common technique for high performance string manipulation in .Net. I've used it extensively in an app that serves over 1 billion requests per month. During the time it's taken me to write this comment, my app has unsafely modified probably about 1 million strings. No errors. I think it's fine.
So send all that as a single SQL statement. Just like you would do it inside of a SP. Problem gone.
False dichotomy. Sending raw SQL does not entail that it not be parameterized.
MSBuild (including csproj) is one of the most labyrinth, undocumented parts of .Net. I've learned the most MSBuild from reading through Microsoft's own build targets for various types of projects.
You CAN'T do this in a regular query in some DB engines. That is the point of stored procs and why people use them.
I'm so glad I don't have to deal with old dotnet. God how i hate that web config file.
David, I know :). And msbuild call csc.exe with truckload of parameters to compile, so its also an abstraction. I feel that abstractions in this area are moving up, from msbuild levels to more consumer friendly levels like 'dotnet build'. You may know and share, probably 😜😜😜
And turn MsbuildEmitSolution=1 ... And then scan all these megabytes of text with your eyes. Well, not my favourite debugging way.
Agree, that helps, still not on par with modern .net debuggigng experience. Back in time I even had step through debugger for msbuild configured and working, but it wasn't great experience. My comment just an opinion and everyone can have different views on msbuild or its usage experience
[Apparently they're not immutable anymore](https://www.reddit.com/r/csharp/comments/ad0hw1/its_actually_possible_to_get_a_pointer_to_any/edchy7u) in .NET Core but I got told not to get any ideas. Makes for funny results because of string interning.
Yes, i know. That brings lots of overhead that a sproc doesn't. And is roughly an order of magnitude more work even to get it going (depending on your infrastructure).
Oh, we have our own sharding libraries. Just the last time we needed to set up a system like that we decided to check out open source options. And they're abysmal for SQL server, while MySql and postgresql have mature options
Dear Lord, I wouldn't run SSDT on a database like that
It's not really badly documented. It's just that it changes to much.
But also a huge amount of other capabilities. With any mature modern infrastructure (PaaS style) setting up APIs can be measured in tens of minutes.
... I've literally never worked somewhere that didn't use some variation of blue/green deployment. Having two live versions is insane
Is it? The business rules (constraints) should largely be enforceable through the correct use of referential integrity, check constraints, not null constraints, and unique indexes. When people say 'business logic', they're often thinking of low-performance SQL code using cursors / loops / other constructs that are anethema to performant database systems - and indeed, I would argue that these constructs have no place in database systems. When I say business rules, I mean the existence of data, or the absence of data, and that enforced existence / absence through the proper use of the various constraints. Many people also do not architect their data in such a way that it allows the system to quickly satisfy queries. They chuck a clustered index on the table with an auto-incrementing number, perhaps some foreign key indexes, and then they call it a day. &amp;#x200B; On the topic of scalability specifically. Vertical scalability does have a cap, horizontal scaling less so. For many applications you could take it to the point where one customer gets one database thus achieving your horizontal scaling. In the post-GDPR world, most businesses will want to consider database sharding as a functional requirement from day one to mitigate as-yet uncharted issues surrounding data sovereignty. &amp;#x200B; That said, I concede that nobody ever says 'this database is performing queries and returning results too quickly!'. But if you accept that correctness trumps performance then it follows that the application of robust constraints within the database itself wins every time. The same conclusion occurs if you accept that the data outlives any one software application (which it does when taken as an enterprise asset in its own right).
I would strongly recommend **Vue** or **Razor** if you're a beginner in this area. It's the easiest to learn and docs are one of the best out there. &amp;#x200B; I wrote an article about integrating it with .NET Core, you can found it here. [https://medium.com/@danijelhdev/multi-page-net-core-with-vue-js-typescript-vuex-vue-router-bulma-sass-and-webpack-4-efc7de83fea4](https://medium.com/@danijelhdev/multi-page-net-core-with-vue-js-typescript-vuex-vue-router-bulma-sass-and-webpack-4-efc7de83fea4) And GitHub [https://github.com/danijelh/aspnetcore-vue-typescript-template](https://github.com/danijelh/aspnetcore-vue-typescript-template) &amp;#x200B; It enables you to use Vue, Razor or both depending on your needs.
Great video bruh’
hey thanks. I suggested using C#, but the company prefers to stick to VB.net as most of their old projects are written in VB.net
hey thanks. I suggested using C#, but the company prefers to stick to VB.net as most of their old projects are written in VB.net 
Anything beyond very basic "this exact string" is inside "this other string" matching that behaves the way most people are used to is incredibly complex and requires tokenization, dictionaries, indexes and math to calculate. This is hard and wasteful to write from scratch. There are some libraries that can handle it if your dataset is small enough but otherwise a proper search-focused system is what you need.
Agreed, Vue is an excellent beginners framework.
Oh, sure. It's blue/green. Per service. But not for the database.
We worked around it injecting filters into the SSDT process. It's pretty expandable. It's just a lot of hard work.
Source code?
Did you consider Azure?
I love the idea, I hate that you launched without macOS. I've been unsuccessful at getting Synergy working reliably between my MacBook Pro and Windows 10 Pro and was excited that this might be a convenient workaround. Is there at least a beta I could try?
Very cool
I did. Main reason for hosting on Amazon was the bandwidth included with Lightsail, since bandwidth can be very expensive.
My password manager (1Password) pushes data to the clipboard and I don't want that shared between machines. Is there a way to manage that? Can I set a filter to ensure that clipboard data from 1Password is not synced?
I just see an error when clicking the video ljnk
Thanks. The problem I'm having with the macOS app, which I consider complete from my end, actually seems to be an issue with Xamarin.Mac, which is in preview. The issue causes the app to hang randomly after it fails due to a null ref, which seems to be caused by the "native" code calling back into Xamarin.Mac for an object that has already been disposed. I have a theory that this issue will be resolved once I update one of my pages to cache and re-use controls rather than deleting them and instantiating new ones, and that's what I'm currently working on. If that doesn't solve it, I'm going to have to open an issue with them, and see if they will look into it. I could give you a build of the macOS app, however you should be aware that it will randomly fail, and you will probably be in a scenario where you need to remember to open the app again when you want to use it, rather than relying on it continuing to function in the background.
Not currently, though I understand your concern. Personally I use it myself sometimes to copy passwords between my devices. All of the data you copy will be end-to-end encrypted, but if that's still a concern for you, I could add it to my list to see if such a thing would be doable.
Sorry but it's closed source, though I can still answer technical questions, and if you're looking to verify certain things about the client apps, I wouldn't be against you having a look with dotpeek. Dotpeek tends to be pretty legible in my experience.
Not having that feature would be a deal-breaker for me. As you would expect, the workstations I use are not all in the same physical location (hence the reason this app would be invaluable) and while I try to always leave them locked there are occasions where I might have inadvertently left it unlocked and unsupervised. Actually, as I think about it, I probably don't want everything that hits my clipboard synced, but would rather selectively choose. Maybe some pattern matching would be involved, like always sync URIs but never sync anything less than 20 characters. As far as copying passwords between devices, I would never do that. That's what my password manager is for.
I was wondering the same. Without it, this thread just feels like an advertisement for the tool. 
Would you use Xamarin for the mobile apps if you start from scratch? What do you think of Flutter?
Sure. I use a password manager too. Still, sometimes I find it more convenient to copy the password from my phone, which has biometric login, and then paste on my PC. The reason I never added any kind of selective sync was that my philosophy has been that the apps should work without any user action, aside from logging in initially. After that, you will just know that your clipboard is in sync. I realize that not everyone will share my view. I'll add it to my list though. Thanks for your feedback.
Yes. Flutter looks like it will help you to make a good UI across Android and iOS, but I don't think it presents great prospects for developing actual application functionality. It doesn't look like Dart has a proper threading model, and seems to rely on Isolates, which seem like separate processes. If I'm understanding that correctly, it seems a bit overkill. Aside from this, Flutter doesn't seem to support desktop platforms. If I did a Flutter app for Android, I'd still need to make another Windows app, and macOS app, and I'd be back to three separate apps, only worse. When I had separate Windows, Android and macOS apps, they still shared a lot of code through a shared project, and even just having the separate UIs was enough of a pain point for me to merge them into Forms.
Just installed Clipboard everywhere on my Google Pixel. Seamless experience. Well done! Not sure I want to install it my Windows machine, as all my private passwords will leak onto the web in plain text. Not sure how to solve this. Perhaps have a "password mode" where it won't publish anything onto the web until you hit "authorize" on the GUI?
It's part advertisement and part me offering to answer people's questions, whatever they may be. Maybe you're new to .NET and you want to see something that .NET can help you to do, or you're looking to use something that I'm using, and you want to know my experiences with it, or some other type of question, I'm also here to share that. On the topic of sharing information, I develop my projects using my own private git server, for which I run Bonobo Git Server. This helps me to keep things in order, and private. I agree with the open source workflow, but particularly when you're working alone, I think it's easier to not go open source. For one, I have credentials in my repositories. If you're working with others, this or especially if you're working publicly, this can be problematic. Since I'm working privately, my commit messages, and my issue board (Azure DevOps), can contain private and personal information, which I would not be able to put there if these things were public. Particularly with the server software, I wouldn't want people to know about some of the decisions it makes, and the specifics of how things are enforced. In some scenarios, I grant users some leeway, and I wouldn't want anyone to know the specifics of this in case they attempted to abuse my good nature. Not security related, just me trying to give people the benefit of the doubt where possible, but I'd rather not go into that line further.
Glad it's going well for you. Remember that everything you copy is end-to-end AES-256 encrypted, so the server does not know the content of your copied text, it only sees the encrypted data. It is encrypted, sent to the server, sent to your other devices, and decrypted again client-side. As with anything you sign up for, I recommend keeping a strong password, which will make sure your clipboard items are very secure.
Considering the entire front end of Fuchsia is written in flutter, and dart is used heavily in the kernel, I would tend to disagree :P &amp;#x200B; However, yes, for desktop we only currently have: [https://github.com/Drakirus/go-flutter-desktop-embedder](https://github.com/Drakirus/go-flutter-desktop-embedder) which is not an official product (I'm not going to link googles because... its just not there yet) &amp;#x200B; however, this is hopefully coming out soon and should be absolutely amazing: [https://medium.com/flutter-community/flutter-on-desktop-a-real-competitor-to-electron-4f049ea6b061](https://medium.com/flutter-community/flutter-on-desktop-a-real-competitor-to-electron-4f049ea6b061)
Can you comment on some of the issues you faced moving to Xamarin forms? Did you ever consider using ASP.NET core with a DBMS like PGSQL and running the backend on Linux servers? It seems like you made lots of cost effective provider choices, but isn’t the SQL Server licensing quite pricey? What made you stick with it despite the cost?
Why use a relational database for this, as opposed to a document or key value store? One document per user in DynamoDB would seem like a natural fit
Great questions. This is a long one. &amp;#x200B; **Xamarin.Forms** Xamarin.Forms has been great for bringing things together. I felt that, when I was maintaining three separate UIs, and I had to make changes to the UI, it was really demoralizing, as I knew that I had to do three separate implementations of the exact same thing. I viewed this as a waste of time that I'd rather spend actually developing my software, or even relaxing! With Forms, it's not exactly a case of just doing one UI, and I'd describe it more as "one and a bit" user interfaces, however it's all more concise, and almost entirely in one project. The main issues I've faced are with the preview platforms, macOS and WPF. WPF has issues with complicated layouts, and often refuses to wrap text, font size doesn't seem to work, not everything can be styled from Xamarin.Forms. This means that a lot of my controls have custom renderers for WPF that do simple things like changing colours and font sizes to what they're supposed to be. Custom renderers sound more complicated than they are, and I don't think it's a big deal. I don't think I've had any stability issues at all with the WPF target. macOS has been a bit more problematic, because it has been exhibiting stability issues. NavigationPages are also a little inconsistent/annoying with their styling. When navigating through a NavigationPage, the back button doesn't always seem to show up in the window chrome, when it should be there. Not a complete deal breaker as you could add your own. Toolbar icons also don't appear unless the window is pretty wide, like 600 units or so, which led to me adding separate buttons to the page instead of relying on those. The main issue here is that I keep running into an issue that seems to be caused by Xamarin.Mac disposing of something that the "native" code is trying to call into, leading to a SIGSEGV error. I currently believe this could be resolved if I stop deleting views, and re-use them instead, so I'll have to see. I had an issue with Android where the app would crash when receiving a SuperDrop transfer whilst tabbed out, due to a null ref in the Xamarin.Forms.Core assembly, specifically the StackLayout class. To solve this, I read the guide on debugging Xamarin.Forms here: [https://blog.xamarin.com/debugging-xamarin-forms/](https://blog.xamarin.com/debugging-xamarin-forms/) I then applied my own simple fix, built my NuGet package, and that's what I'm now referencing in my projects. I did post the issue to GitHub and one of the developers says they will fix it. **.NET Core** The main reason for the back-end using full Framework is SignalR, which wasn't available for .NET Core when I started making this. I have used [ASP.NET](https://ASP.NET) Core 1.1 for my personal site, and with the breaking changes to Identity with [ASP.NET](https://ASP.NET) Core 2, and what I viewed as lacklustre tooling that relied on console commands rather than UI, I also decided to give .NET Core a break until they improved that by adding the UI back in. **SQL Server** The way I learnt about "advanced databases" was with SQL Server, though kudvenkat on YouTube. As a result of that, and since it worked with Entity Framework by default, I kept using it. I don't think there's necessarily anything wrong with using another database engine like PG, but I have no familiarity with it and I like SSMS. I'm currently using SQL Server Express, which doesn't have a licensing fee. It works up to 10GB in database size. **Linux** Regarding Linux, I didn't want to use it as a hosting environment as my Linux experience is pretty limited, and I feel quite comfortable using Windows, IIS, and sorting out issues that I encounter. I'm not so confident that I could swiftly resolve issues on a Linux system, so it's about my personal lack of experience there.
1. You mentioned you use a Windows Server instance with SQL Server installed on it, and that it has benefits over RDS. How big is the cost difference? What real benefits do you gain from the manual install of SQL Server that RDS couldn't give you? 2. Why did you choose SQL Server and not MySQL? 3. Why do you use AWS SES over just manually sending emails from code-behind? 4. Do you use any kind of scheduling software like Hangfire or Quartz?
There are many parts of the back-end that I feel suit a relational database, such as the [ASP.NET](https://ASP.NET) Identity tables responsible for storing accounts, my rate-limiting email implementation that stores emails in SQL Server until they are sent, to comply with SES rate limits; the log that I keep of internal exceptions, the log that stores user-submitted crash reports, and a bunch more. What would you say is the advantage of your proposed solution?
Have you encountered issues with emails with a + in them? Just signed up and it's telling me my email/password is wrong but have triple checked it.... 
1) RDS is a managed service, so you don't get access to the machine that the database engine is running on. For me, I wanted this access to implement a stronger backup strategy. I developed an application that takes full, differential and log backups of the database at appropriate intervals, so that I have all the files I need for restoring in a timely manner, and I run this app on the database server. RDS can do point in time restore, however apparently it's slow, as I think they restore a snapshot of the machine. If you're using the more advanced SQL Server features, e.g. FileStream, RDS doesn't support that, though I'm not using such features. The pricing of a t2.micro (1GB, 1 CPU), per month, is: * RDS - $15.84 * EC2 - $11.664 * Lightsail - $12 This doesn't include other bills like storage and bandwidth, except in the case of Lightsail, which has a much simpler pricing model. So that can be a pretty decent percentage, but I don't know if a full SQL Server license would tip the scales at higher instance sizes, with how convoluted the licensing can be. 2) Only because I had experience with SQL Server already, and it's what I used to learn about these types of relational databases, with kudvenkat's youtube series. 3) If you send emails yourself without an email provider, you will find that they always end up the recipient's spam folder. For this reason, it's better to use a trusted domain, which is the service you pay for with SES. 4) I have my own scheduling of sorts that I use to comply with my SES rate limit, using SQL Server to store my emails until they are sent, which was the biggest problem with integrating it. SendGrid doesn't have a rate limit. 
Did you check your password for illegal characters? "&amp;" and "/".
Thanks for the answers! For me I feel the convenience of RDS outweighs any small benefit you might get from self-hosting it in EC2 or Lightsail... especially if you have to scale up/out quickly, or need to leverage Cloudwatch to keep tabs on your databases. Also I only mentioned MySQL because the licensing is either free or significantly cheaper, and *allllmost* everything you'd written for SQL Server still works fine in MySQL
Thanks for the advice. If this gets big enough, I'll definitely have to consider that, as I know SQL Server licenses can cost serious money. My current scaling plan is to scale up rather than out, at least for as long as it's possible to do that. That keeps things simpler. I have read into scaling out SignalR however, just as a learning experience. I would like to move to [ASP.NET](https://ASP.NET) Core at some point in the future, but right now I don't think it would be worth doing, when put up against developing more functionality. The main thing I would want is Span integrated with the framework, as Span is pretty cool, but the full Framework doesn't have all of the Span-accepting methods that Core now has.
Thanks for the info. Are you using public/private keys?
Each user has their own private key, generated from their password and 100,000 rounds of PBKDF2. Each clipboard item gets a new initialization vector, which is to protect against two identical items being encrypted to the same result, which can be used to break the encryption by an attacker. Each user has a constant 128-bit salt, which can be public, but the one that is used is generated from your account info, so it's private and per-user-unique. This salt is used to prevent rainbow table attacks.
If you are looking to make your app DB platform agnostic consider [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient).
All good :) It's always a fun finding apps that don't allow my emails :D
I get the following when trying to register. I selected United Kingdom. {"Message":" is not a valid country."} 
&gt; For one, I have credentials in my repositories. Yikes! Also, I think I don't really understand your last paragraph. You don't want people to know the specifics of your back-end because they might abuse this?
I just tested the website and one of the apps with UK and registration is fine for me. Did you register through the website with some sort of JavaScript modifier?
The back end enforces things, like what features a user can access and how they can be accessed. When you enforce things, in some scenarios, you may decide not to enforce something in order to improve the user experience, even though the user may be at fault. I can't really go into that any further, sadly.
No problem, I think I kind of know what you mean. Thanks for taking the time to reply and best of luck to you! :)
Your post has been removed. Self promotion posts are not allowed.
I checked it and it's working at my end, could you please retry, if still getting error, could you please share screenshot on admin@studymash.com
Nice thread! Thank you for sharing! As a software architect who has been working for few years with aws technologies, I'd have considered moving your whole backend towards a serverless architecture. 1) authentication via Cognito 2) logic on API gateway + lambda (or ECS Fargate if you prefer the container world) 3) storage in DynamoDB (I don't see the need for relational access to the data, so DynamoDb or even S3 would lower your cost significantly! You mention signalr, but you use your own database as backplane. Have you thought an hybrid setup with azure signalr service? Or maybe playing around with a dynamodb based storage. Please don't take my post as criticism! I'm very impressed by your project! Good luck!
It doesn't sound like you're doing anything that requires joining lots of data together. That's where rdbms shines. Logs? Many solutions for that. Rdbms is a poor fit. We used to have one for that, now we use the ELK stack. Rate limiting - sounds like some sort of pub sub is what you need, where you only pull off as many as you can send. SQS since you're in Amazon. SQL can be used for all that, but none of them are places you need it. Right now a flood of errors or emails would impact the performance of your login process. Separate systems isolate each other. If 80% of your app is "pull clipboard data for this user" that is a super natural fit for a single document. Pulling a single document is fast. DynamoDB will abstract the storage and maintenance details. Plus more fancy buzzwords 😁
So.... Take 500 sprocs, replace with 500 "apis" and call it a day? Recipe for disaster without many other changes. If the DB changes, the sproc won't compile if it's incompatible. The API will instead tell you about it at runtime. Everyone knows how to see all the sprocs in the DB. They know what they are, what they do, etc. Who knows about your API? Who knows they should call it instead of a sproc it direct query? Design for sprocs tends to be: let's have 15 slight variations, and call the exact one we need. Design for an API should be: how do we want to expose this entity? What are the use cases? Apis successfully replace DB access when nobody can access the DB except through the API, and when they present the models people care about consuming and not the details of the storage layer, and when people have an easy way to find out about then and use them: swagger, maybe clients, etc. 
Thanks for the suggestions. My personal view of serverless, as it's called, is that it will be difficult to work with. For example, I used to have separate MVC and Web API projects, and I found that debugging the two at the same time was highly impractical, which led to me merging the MVC project into the Web API project. Once I did that, I could debug both sides of it at once, and it really helped with the earlier development of website. As well as complicating my debugging experience, these serverless solutions tend to be prohibitively expensive, in my experience, particularly with Azure. It looks like one SignalR "unit" would cost me a minimum of $49 per month, which seems a lot more expensive than if I provisioned a VPS. It looks like there's nothing between 0 and $49, as far as I can see. Comparing with that, I could get a 2GB Lightsail instance with 2 processors and 160GB storage, and 5TB of transfer, for $40. And I just realized that Microsoft's $49 is going to be on top of $0.09 per GB of outbound data. Cognito seems like it starts out pretty good, and if I reach 50,000 users I hope I'd be at a point where I could pay their rates, but Identity was part of the project template, and I know how to do some stuff with it. I would say that identity is probably a good thing to separate out however, as in the case of Identity Framework, it's a closed off tech anyway, and it's not something I need to debug, and it has also changed a lot in .NET Core. Not having to think about migrating my accounts in the event of moving to .NET Core would have been nice. Lambda does seem quite decent if you want to pawn off a few operations, but if I were doing a lot with it, I think I'd still take my own approach of scheduling things myself. The system I worked out for emails wasn't that difficult to implement, and would be usable for scheduling other types of work with a little modification. Further to this, I don't particularly like the idea of being locked into a particular hosting provider. I've never built anything that runs on Lambda, so I don't know how specific it is, but I would assume Cognito to be a lock-in. I like the idea that I can take my software and put it on my choice of machine. I do however recognize that it would probably be easier to update your applications. E.g. in my case, updating the MVC part of the website requires redeploying the whole thing, but it doesn't seem like an issue for me at the moment. Regarding Dynamo however, or NoSQL databases, I hadn't really considered it, and that's because I didn't really know about them. It looks like they're advised in scenarios where lookups are important, and I do make a lot of lookups, and some joins, but I'd say the lookups are more important at the moment. Thanks for pointing it out.
Yeah, I'm not joining masses of data. Joins between two tables are fairly common, but on a single-record basis. I was aware of Amazon's SQS and Lambda, but I just thought it simpler to roll my own, similar with logging. I have an ELMAH integration but I don't actually use it, and typically just use my own log table, though I don't want to slight ELMAH in any way. It's just that 99% of the time, all I want is a StackTrace and the exception type. I understand the benefits of separating systems into microservices, but my experience is that separating systems will make them more difficult to debug, so I personally like to avoid it, and would rather use my money to scale up. I feel that I have quite a lot of room left for that at the moment. It does sound like a NoSQL database is something I could work with however, provided Entity Framework and Identity Framework support, so perhaps it's something I could play around with. Thanks for the suggestions.
.gitignore generator azure tools c# c# extensions docker import cost nuget package manager prettier rest client sql server todo highlight vscode-styled-componets color picker react snippets
I think a simple hotkey + copy to not include this into the sync is the simplest solution without breaking to much code. So if you want to copy a password the user just needs to press the combination and you whitelist that clipboard :) let me know what do you think
UGH....signalr on mobile. I hate it. It is a core component of an app my company is working on. Don't get me wrong its great....when it works. But there have been some major hurdles that we had to cross and they sucked. One of the biggest hurdles was iOS and its restriction on network connections while back grounded or the phone screen is sleeping. 
Right, yes, I have experience with this, though I don't think it's fair to blame SignalR for that, rather I think you should blame Apple. I would like to release an iOS app myself, but due to background restrictions it's not feasible, because nothing will work unless you manually open the app. Maybe some day, Apple will stop limiting their users like this. They have to catch up at some point.
Does the server ever see either my password or the generated private key?
Try some ideas from here: https://github.com/viatsko/awesome-vscode#c https://blog.elmah.io/best-visual-studio-code-extensions/
Well, as with anything you sign up for, the password has to be sent to the server when you register and again when you log in, so yes the server does see your password, but it does not store it. It is hashed by Identity Framework and stored in the database (standard, secure procedure), and it is hashed again when you log in to check if it's the correct password. That's all done by Identity. The key is generated as part of the client-side login procedure and stays on the device, so the server does not see it.
&gt; I'm currently running an i7-7700 with 32GB of RAM and a Samsung 860 NVMe drive. I wouldn't describe it as slow, but a few projects take 2+ minutes to start debugging with local IIS / Chrome. That's a perfectly capable system. How big are your solutions? 2+ minutes is pretty long. You can spend a bunch of $ maxing out your CPU, but it may not improve the situation a whole lot.
Oh I define wilt blame apple for it. I love signalr as a technology really. Working with it on mobile.....correction, on iOS.....suuuucks lol. 
It’s not the system. RAM won’t help because VS is 32-bit. How many projects in your solution? What languages are they using? C# is super fast to load, don’t know about C++. Are you NOT caching your NuGet packages? Try turning OFF Browser Link. How many startup projects do you have set? Are you on the latest VS (15.9.x)?
I'd add gitlens and live share to that list.
That seems ... way slow? How big are your solutions (ie: # of projects, # files)?
Install the .NET Core SDK. dotnet new xunit -n MyFirstUnitTest Have fun! You don't need some application to write tests for. You can just write tests that test whatever you want (verify addition operator, etc.).
Entity Framework is an object relational mapping tool. It exists to deal with the impedance mismatch between objects and tables. But there isn't any such mismatch between objects and documents. You can just store the serialized object. I believe you can use it, but it doesn't add anything. Heard a DotNet Rocks podcast from a guest who was big in EF. She described getting that question and working out how to do it, before other people asked why 😁 Never used elmah... I thought it pretty much faded away. I wasn't talking about separating services. Just about separating data stores. We have a monolithic database, and getting a flood of action x can easily impact action y, simply because they share a database and ultimately the same disks and processors. DynamoDB, like all NoSql systems, has sharding built in. I'm quite sure you can scale up on SQL server. I just wish the app I work on had done something else years ago. You can even now use the MongoDb driver for talking to it, just in case you wanted to be able to move off of AWS sometime.
This is well worth a read ... [https://www.hanselman.com/blog/BuildingTheUltimateDeveloperPC30ThePartsListForMyNewComputerIronHeart.aspx](https://www.hanselman.com/blog/BuildingTheUltimateDeveloperPC30ThePartsListForMyNewComputerIronHeart.aspx) &amp;#x200B;
It's nice to see that people are still using same technology I prefer at work, got a couple of questions if you don't mind: 1- We are required to create databases in AWS, is lightsail cheaper than RDS somehow for MSSQL? AWS RDS cost is very high if we want to create MS instances. 2- To deploy the Xamarin app to iOS are you utilizing a physical macbook? is there a better way to do it?
Huuuuge possibility you have some project build settings that are messing this up, MSBuild is pretty quick. Either some pre/post build steps that are doing large file copies or some assets that are set to copy always. You should turn on debug logs for MSBuild and check what’s taking the time.
Yea that's very explicit though. Look at the unit of work pattern, which the DB context acts as along with persistence. When you make changed to entities tracked by the dbcontext it's building up a bunch of statements. When you hit save changes they all save or nothing. So don't put save at the end of a create or update method. Do all of your updates on the context and then save them all at once. It is now all or nothing on that change.
I think your last paragraph is what my answer was going to be anyway. The devil is in the detail here - consumer driven contracts protect from client facing changes, usually enforced by CI. Swagger, JsonAPI, verifyable test harnesses and Hateos all explain the interactions. Usually it's not "500 sprocs, 500 APIs" - as the vast majority of sprocs don't form the upwards facing API of most software, just implementation minutae - frequently a single API with GET/PUT/POST semantics will cater for a collection of use cases. The above addresses the brittleness of database centric design - where you're fundamentally leaking your internals outwards. Basically, don't expose your privates ;)
There is one that lets you use solution files in vscode. That is probably the most important one beyond the basic obvious ones
The *language* was based on C++, but the .NET runtime (CLR) is clearly JVM-like with garbage collection and such. And very clearly .NET was MS's answer to Java/JVM because Sun was being Sun and later Oracle was being Oracle.
Good answer. A+ :)
also * beautify * bracket pair colorizer * azure repos * git history * path intellisense and of course settings sync
I'm using this! I'll recommend all of them, especially C# FixFormat if you are a fan of \`same line brackets\` etc.
Thanks! Was on my mobile and didn't see the Single Database option \*doh\* 
Thanks, that makes a lot more sense. Haven't been able to look at it since but will give this a try later.
If you are using [SetClipboardViewer](http://msdn.microsoft.com/en-us/library/ms649052\(VS.85\).aspx) api to detect when something is copied to the clipboard and processing WM_DRAWCLIPBOARD message then you can use [GetClipboardOwner](https://docs.microsoft.com/en-us/windows/desktop/api/winuser/nf-winuser-getclipboardowner) function to find handle to the window which initiated the operation. Using the handle you can retrieve the process id and path of the executable.
The online market is growing, and qualified candidates have a lot of room to look for exciting job opportunities in the field. Microsoft Certified Solutions Assistant (MCSA): Web application certification verifies the expertise of professionals in creating and managing modern web applications. The Associate Level Certification Program is the first step in gaining expertise in web application building and management. This qualification reflects the ability of professionals to cross-local technology and cloud-based work to create and develop interactive application solutions. People often hang back if they get suspicious about the validity of [**Microsoft MCSA Web Applications Real Questions**](https://www.vceexamstest.com/mcsa-web-applications-vce.html), but VCEEXAMSTEST offers them their money back if they didn't succeed. Actually, clearing Microsoft MCSA Web Applications Certification is relatively difficult than other IT Exams. &amp;#x200B; &amp;#x200B;
One of the best series on .NET microservices out there. EShopOnContainers is ok but I've learned way more following your videos and is a cleaner implementation imo.
Thank you! It's one of the reasons why are videos tend to be so long - we try to describe our approach as clearly as possible. Speaking of EShopOnContainers our solution is a little bit different, as the command side is also fully asynchronous (API Gateway sends requests to the message bus instead of calling microservices API via HTTP).
The detail you guys go into is much appreciated and has definitely played a big part in solidifying my understanding. Its given me enough of an understanding that I've been able to switch out certain components and use their Azure equivalent such as Azure API Management for the gateway. 
That's just awesome to hear :).
I find it interesting that you actually enjoyed working with Xamarin.Forms. I personally think it's a dumpster fire of a product. 
iirc beautify and prettier fight over formatting pretty heavily. I remember having to disable / uninstall beautify for because of it.
Then you want something like this. bool match = false; foreach (string term in QuickSearchTerm.Split(' ')) { if (!(match = new Regex(@$"\b{Regex.Escape(term)}\b").IsMatch(MarketingName))) { break; } } This will look for complete words in the text (\b will match on word boundary which can be punctuation, whitespace, or the beginning/end of the text) that match your search phrase. Exercises for you: making it case insensitive, adding additional functionality (my search has special keywords like `after:2019-01-01` to match a date associated with my records).
If you end up working with a real dataset it will likely be in a database. Then you'll end up using SQL to do your searching anyway, not Regex.
Yes and yes. 
string.Contains() is too simplistic for what he wants, he really needs the \b Regex operator.
Oh wow! Huge help! 
I personally use https://regex101.com/, it's everything I need in a Regex tester. But the more tools the merrier.
The problem is that [] causes the regex to treat the enclosed string as a list of characters to look for. So any word that ends with any of the characters in the search string would match, which is obviously not what OP wants. The [] needs to go. Ultimately OP needs to split his search string into words and search the target text for each word individually, using \b on either side to ensure it only matches whole words. Also don't forget to use Regex.Escape.
Now just need a djungle-dboogie API.
C# Futures: Pointer Math https://github.com/dotnet/csharplang/blob/master/proposals/intptr-operators.md
Also good, but Microsoft .NET has a slightly different syntax
"Pseudocode, stub code, hypothetical code, obfuscated code, and generic best practices are outside the scope of this site. Please follow the tour and read "How do I ask a good question?", "What topics can I ask about here?" and "What types of questions should I avoid asking?". – BCdotWEB 2 mins ago" I do have a lot of hypothetical code...
I've never had a problem using regexs from there in .NET or JavaScript. I guess it's not surprising the syntax is going to be a little different. But at least for any Regexs I write it works great. I suppose if you start writing more advanced Regexs you might run into trouble but at that point I personally would make SURE a Regex is the way to go. Sometimes simple string manipulation is easier than a Regex depending on what you're trying to do.
[removed]
[removed]
Cool! I tried out the CLI and had two issues, but was eventually able to get a nice fast parallel download: * it crashed when trying to download something that requires authentication (HTTP 401 response) * it crashed when it got to 80% when the screen buffer size was 80 (ie: the progress bar didn't scale properly to the console window and eventually went out of range) Also, have you looked at [aria2c](https://aria2.github.io/)?
Yes, well, I had to parse ticker symbols from multiple international markets and those regex expressions were 100 characters. The differences between JavaScript and .NET were important
Thanks for your feedback. * I will added that feature in next releases * Got it !
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/learnjavascript] [Razor Pages\/JavaScript debugging vs production issue](https://www.reddit.com/r/learnjavascript/comments/afy56b/razor_pagesjavascript_debugging_vs_production/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
There isn't an official load testing tool right now. We have a port of Crank called "Crankier" in the repo (https://github.com/aspnet/AspNetCore/tree/master/src/SignalR/perf/benchmarkapps/Crankier) but it's not actively maintained outside of what we use to do our internal load testing. It should work as a base for a benchmarking tool of your own! (I work on the SignalR team)
I would think there's no reason why it shouldn't, but at the same time why do you need to run VS2017 on Windows Server? If you want to be able to debug applications you just need to install the Remote Debugging Tools and then connect to the server from VS2017 running on your PC. There's some other things you can install for VS1017 functionality, such as Web Deploy to Publish to the server, and there's also the Team components for that functionality (which I am unfamiliar with) such as source control.
We want to use it as a development machine on a server where multiple developers work over remote desktop and we have to decide between windows server 2016 or 2019.
When you use / at the beginning of your URL it will go up to the server root. Your web browser and the JavaScript running in it has no idea what your server is doing, what applications are set up or where, or whatever. That's none of its business. What you should be doing is using relative urls. So if your script is running off of /myapp1/index.html, you can simply specify Admin/Users/Create (without the slash) and it will map to /myapp1/Admin/Users/Create. However if you have the same script running in multiple places (for example, an AJAX helper script) for example, /myapp1/page1/somepage.html, now your path has to be ../Admin/Users/Create which is different from the other one. The way you seem to have resolved this is to hard code paths. Works, but probably not the best idea. You could look at the browser's window.location value but again this is hardcoding values and all it takes is someone on the server loading up the site with https://localhost/ to completely break it. There are two things I would do, depending on if I am using views or not. If I am using views, I can create a .JS file "view" and inject the View object's .ViewContext.HttpContext.Request.Uri.PathBase into the script and assign it to a constant. In your case the value would be "/myapp1". I can then include this view reference as a script in all my pages and use the constant to generate proper absolute urls for my AJAX requests. If I am not using views or cannot use views I can have each page have a relative url that references the application root, as a constant. Such as "../" or "../../" etc. Then I can use this to generate AJAX requests that resolve to the proper url as above.
Be sure the licensing for VS (professional or whatever version you have) allows what you are trying to do. Seems a bit unusual to me.
I believe we use @Url.Content("~" ) for the root path. This also works when hosting using an application name in IIS. In your case @Url.Content("~/admin/users/create"). 
Right... in that case I stand by saying that it is a lot more work to make an API than it is to add a sproc, regardless of how fast you are able to deploy it once you design it and test it
I would urge you to check performance is ok in this setup before you spend money. In the past when I've used a virtual device box I've found it to be very laggy, and that was with just me using it. Maybe things have got better in the last few years what with ssds etc but I use bare metal for my dev box. Maybe a stack of 2nd hand laptops might work better if it doesn't pan out.if it does pan out, great!
Seems odd - what is your use-case for wanting to do it this way?
Aren't Windows Servers limited to two concurrent RDP sessions? Also, I have had to do this before on Server 2012 with VS 2015, and there was no noticeable lag.
I usually use Azure Application Insights. Very powerful system and not very expensive. Storing the errors in a database is bad practice (in my opinion). The database should be for business relevant information, not for logging. An alternative would be basically any kind of key-value-store / blob storage.
My legacy system use database to save logs, its a bad practice and causes a lot of problems to find the log and is expensive in my case Now I use the ElasticSearch with Serilog, its worth, i can find any log fast and use the Kibana to show this Maybe graylog it's a good too but I never use
I made a module for ASP.NET Core to generate log files. It has the following features: * It implements ILoggerProvider and ILogger. * Can separate logs into separate folders based on year/month. * Can separate logs into separate files based on day. A new file is automatically created when the day rolls over. * If for some reason a log file is locked it creates a new one with a new suffix so it can continue logging. * It can automatically delete old log files. * It can log in text and/or csv format. Unfortunately it can't log exceptions that cause the program to crash, but I can check event log for those. I suppose I could try to have an unhandled exception handler but to date I haven't tried that.
Well if you want HTTP headers those probably have not been received yet when this function is called. Like the other guy said you probably want to use middleware which will give you access to the headers and should also let you access the certificate sent by the client in the HttpContext.Connection or HttpContext.Request properties.
The apps I write are desktop apps for internal use at work only. So, I have a generic errorhandler / logwriter class I use in most of my apps which capture errors and write them out both locally (local pc) and on the network where I can more easily get to it. It appends to a log file with a time/date stamp and as much information as I can reasonably capture. 
Thanks! I think this is on the right track! :)
Yeah, you got it. We're looking into the middleware solution. Thanks!
I have for many 20+ years posted the details to a database, that way I can sort by many areas (user, OS, ex number, etc) So I can handle higher priorities or help find the issue.
I have long stored my err/ex info in a database because I am a business software developer. What I usually develop lives in house and access to that info is important to resolving the issues, and sometimes when I get an error I can go to the user's desk and inform them I know about the issue. (I have had apps that lets me know when exceptions occur) This lets users know we are on it. I can understand your point about keeping errors in a larger production db, but that data does need to be stored.
for web stuff we tend to run log4net with a custom handler to reach out to stackify. This works wonderful due to the flexible nature of log4net. Then Stackify is great for remote servers we don't have access to. For other stuff, it depends on the environment, application and what exactly you are going to be logging. 
We use [Sentry.io](https://Sentry.io) to do all of our error logging, it's become invaluable.
One practice I started using was to use Diagnostics.Trace for errors/warnings/information. You can then create a custom listener to write the messages wherever you want. Then you register your listener in the app(s) config file. It decouples logging from your application so you can add/remove/update listeners without updating your app code.
Just this past weekend I started using MetroLog for it.
I cannot learn when laugh 😂 
Ahm, no? But you have to have CALs for all users. If they would be limited to two RDP sessions, terminal servers wouldn't make sense
It’s becoming a pretty messy language :(
Just wanted to say thanks to you and /u/Marco0546 for your answers, it's got me pointed in the right direction I think. I was having difficulty with @Url.Content("~") but now that I can identify why it's behaving that way, I think I can work with it better. My app front-end JS is weird because it's in a transition from a single massive jQuery file to trying to split it up and reduce redundant code using Vue. 
I don't think messy is a term to use for it. It is still very clean.
Every single windows server at my current client will not allow more than 2 people to be RDP'd in at any time. It is insanely infuriating. I just always assumed this was how it worked by default and was just some strange limitation of windows servers.
https://social.technet.microsoft.com/Forums/en-US/c52b6da7-cdf2-4e1f-95d1-6471c6f2f6b0/easiest-way-to-enable-more-than-2-concurrent-rdp-sessions-on-windows-server-2016?forum=winserverTS
I would definitely use an online service as other have stated for ease of access. For example [Https://elmah.io](elmah.io) that integrates with the most common logging frameworks for .Net both Framework and Core like log4net, Microsoft.Extensions.Logging or Serilog. If you already use one for your current logging, then it's easy to switch over to after getting a trial.
It's still just a proposal, so no telling if it will ever become part of the language. 
Have you seen C++?!
In my opinion better not having in the spec. In my opinion F# could be improved to write interop code
"add a single method". Feel like we're mostly agreeing though.
Any article, something about that? 
But wait, before where/how to store your logs, if you got a system would to change / add logging features, Then should go to every method and edit it's code to make it generates the logs? that's a lot of copy pasting, time intensive task! Isn't there more handy way? 
The important question is how you need to access or query your logs. Those answers will likely inform how you might store them.
https://www.codeproject.com/Articles/447238/Advanced-Tracing#_comments
A few http related .NET classes are instrumented with DiagnosticSource and may be what you're looking for. Here's the [DiagnosticSource user guide](https://github.com/dotnet/corefx/blob/master/src/System.Diagnostics.DiagnosticSource/src/DiagnosticSourceUsersGuide.md). Also related is the [Activity user guide](https://github.com/dotnet/corefx/blob/master/src/System.Diagnostics.DiagnosticSource/src/ActivityUserGuide.md). You can find pretty good example usage in one of the [OpenTracing C# libaries](https://github.com/opentracing-contrib/csharp-netcore/tree/master/src/OpenTracing.Contrib.NetCore/AspNetCore). Currently working on something similar. Happy to help out more if am able.
You can still just work with a subset of the language—like most people do currently anyway.
Ah sorry if I did not able to make, could you please suggest so that I can improve.
The video content is great. But it would be great if you work on your English pronunciation. 
Thanks, I will try to improve
No, you got it all wrong.
Well I do, obviously. Should I downvote you for thinking it’s clean just as I am downvoted for the other view now?
Sure... doesn’t mean it’s not messy (in my opinion). Happens to old languages in development. Just look at C++. One problem with it can be that’s newcomers use stuff incorrectly more often.
Yeah, true definitely. C# is certainly similar to C++ in a lot of ways. Just features on top of features on top of features. I don’t mind lol
Yeah, and I don’t want C# to remind me too much of it :p
Yeah, I saw that. I hope they take it slow.. in open source development “good” ideas will always pour in.
&gt;And pointer division and remainder? Seriously? I was curious too. [Here is a discussion about.](https://github.com/dotnet/corefx/issues/32775) Specifically, this post contains the rationale. &gt;&gt;Multiplication, division, mod, etc are much less likely to be meaningful. &gt; &gt;This isn't true. &gt;Detecting the alignment of a pointer can be very important for performance oriented code. Many vectorized algorithms will perform at most 2 unaligned operations (in order to become aligned) and then operate on the rest of the data as aligned, in order to achieve "optimal" performance. &gt; &gt;We are already doing the above in ML.NET and are having to work around the fact that you can't easily do address % 16. Many other operations, such as indexing the correct element, also rely on doing multiplication, shifting, etc (that is the whole purpose of things like the LEA instruction and the scale, index, base addressing option). I wasn't aware of this either, but my quick research suggests to me that it is is a use case of the **intptr_t** type in C99 also.
Yeah ok. Pretty esoteric, to be sure. 
Can't argue with that logic mate. For the Windows app, I've already implemented a hotkey combo to bring the window state back to Normal, which isn't actually in the current release as it's commented out, but that will be a great idea if I can figure out a good key combo, so I'll look into it. Thanks.
My experience has been that, even with some of the issues I've faced, and continue to face, it's a great help in creating a consistent UI across platforms, and overall has reduced my workload. &amp;#x200B;
Lightsail is cheaper at the small end, which is where I currently am. If you're starting out small, you can save about 25% by running Express. If you need to buy a SQL Server license, I'm not sure what that would mean for your pricing. I'm not planning to release an iOS app at the moment, due to platform restrictions, however I have developed a good chunk of one using a macOS virtual machine as my build host. I don't know how these are configured initially, however it is possible to find pre-configured macOS images on some websites that I wouldn't be allowed to link you to.
Right, well I'm not familiar with the workings of NoSQL databases, but it sounds like an interesting idea. Perhaps I'll utilize dynamo to run some tests. Thanks for the information.
Cool. This looks pretty sweet. This exception seems weird to me. &gt; Enum types: not all enum types contain the constant zero, so it should be desirable to use the explicit enum member. Literal zero can already be assigned to any enum type. Furthermore, it's the `default` value for every enum, regardless whether or not it's an explicitly named member. On the other hand, I would probably never use `new()` for an enum anyway.
I went looking in open tracing but didn't find anything but I guess I was looking at the wrong repo. Im gonna dig into this and see if I get better answers out of it. I saw the diagnostic source user guide but it seemed muddled to me. Maybe after seeing examples it'll make more sense.
What will people use this for?
&gt; Hard to read Then you are SQL illiterate.
Like a lot of recent additions to C#, this is a bad idea. Why? &amp;#x200B; XmlReader.Create(reader, new() { IgnoreWhitespace = true }); &amp;#x200B; What's the data type of the second parameter? The compiler can figure it out from the parameter list. But I have to delete that comma and type it back in to get intellisense to appear. Var also. Var is a bad idea. If I ever catch the guy who added var to C# I'm going to do mean things to him. I *like* the explicitness of C#. It seems like they're trying to undo that now in the name of innovation for innovation's sake.
Thank you! I thought I was the only one who hated var.
If it works anything like similar C# features, you’ll be able to hover your mouse over the brackets to see the inferred type. In addition, you’ll probably be able to hover over new() and see the exact constructor being called, on the exact class. In case you didn’t know, you can do the same with “var” ... you can always mouse over the var keyword to see the type, and all context menu items still function on it, such as “go to definition.”
Or, you know, people could just include the data type inline where it's plainly visible like they used to.
Elastic search .... just an ES endpoint and kibana.
 Dictionary&lt;CarIdentifier, IReadOnlyList&lt;CarPart&gt;&gt; myDictionary = new Dictionary&lt;CarIdentifier, IReadOnlyList&lt;CarPart&gt;&gt;(); That’s the nonsense the var keyword saves you. And that’s not an uncommon line of code...basically any generic collection initialization takes up way more space than it needs to without var. Not to mention that Linq would be about impossible to use while being readable without a concept like “var.” I agree that var can be abused, but so can just about any language feature. It’s trivial to turn on custom style rules that dictate that you get warnings when you use var and the type is not apparent from its usage. I think that’s actually built into Visual Studio now.
It's a preference thing. I hate var. While we're at it, I hate Linq also.
One difference being that to be able to use those features you need the "unsafe" keywords. newcomers should be scared enough to not use it. Or understand that it is at their risk.
From the C# Coding Conventions page &amp;#x200B; Implicitly Typed Local Variables * Use [implicit typing](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/implicitly-typed-local-variables) for local variables when the type of the variable is obvious from the right side of the assignment, or **when the precise type is not important.** * Use implicit typing to determine the type of the loop variable in [for](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/for) and [foreach](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/foreach-in) loops. * Do not use [var](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/var) when the type is not apparent from the right side of the assignment. * Do not rely on the variable name to specify the type of the variable. It might not be correct. * Avoid the use of var in place of [dynamic](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/dynamic). In this example the type is not important, the name of the type is noise that distracts from its purpose. Another example is var myLookup = new Dictionary&lt;string, string&gt;(); do you actually think it's easier to read this: Dictionary&lt;string, string&gt; myLookup = new Dictionary&lt;string, string&gt;(); and isn't it nice when you refactor this to change myLookup into var myLookup = new List&lt;KeyValuePair&lt;string, string&gt;&gt;(); that you don't have to make the change in more than one place?
The issue isn't the situations where it's obvious what the type is. The issue is when it's not obvious. The guidelines don't matter. I've worked it a lot of sketchy code written by weak programmers. They use var as a crutch. 
I'd only want to see var go if we get this `new()` for variables. I don't want to repeat a long type name like `Dictionary&lt;string, List&lt;int&gt;&gt;` multiple times in the same line. However, the `XmlReader.Create` example is really scary because an updated version of the `XmlReader` type can break existing code by introducing a new overload of `Create` that takes a different type for the second parameter. Suddenly, the compiler can't infer the type anymore and once-working code suddenly fails to compile. Suddenly adding overloads is as breaking a change as removing them.
var myString = “string”. Var is a great idea, but can obviously be misused and lead to lazy coding. No need to write more than I should and no loss of understanding. Again, that is when it is used correctly. It can obviously be overused.
Or worse, the compiler used to infer one type, then the code changes in a subtle way and now it compiles with zero errors or warnings while inferring the other type. I bet this inferring the data type thing leads to some obnoxious error messages: "Could not Convert parameter of type Namespace1.Namespace1.Namespace3.Type1&lt;Namespace1.Namespace1.Namespace3.typeA,Namespace1.Namespace1.Namespace3.typeB&lt;string,Namespace1.Namespace1.Namespace3.TypeC&lt;int, string&gt;&gt;&gt;&gt; to Namespace1.Namespace1.Namespace4.Type1&lt;Namespace1.Namespace1.Namespace3.typeA,Namespace1.Namespace1.Namespace4.typeB&lt;string,Namespace1.Namespace1.Namespace4.TypeC&lt;int, string&gt;&gt;&gt;&gt;" 
Woah woah woah wow. Linq too!!!????
It's the main interface to entity framework, which is an abomination. Also, while there are some performance gains to be made from things like delayed execution, I think the loss of predictable behavior negates any performance gains. Also, all of the syntax surrounding Linq... lambda expressions, anonymous functions... it's just a cumbersome and hard to understand syntax. At least they made it so you could step into anonymous functions with the debugger. Basically Linq allows you to replace 3 or 4 clear and obvious lines of code with 1 harder to understand line of code with probably zero performance gain. But at least you can claim on your resume that you used Linq. There's a trend in software development to be cool or clever. And this is always at the expense of clear and understandable. "Oh, you're not using blabla? *I'M* using blabla." as the developer looks down their nose at you for not being bleeding edge enough. If you can't tell, I usually take the role of a senior developer, with maybe 3 to 6 junior developers working for me. And I'd much rather have programmers writing clear, understandable, and easy to maintain C# .net 2.0 framework code that just fricken works. And if one of those developers leaves the project, I know that I can easily replace that guy, and the new guy won't have any trouble figuring out what the code does. If we're going to be professional adults about the job we do, then we have to ignore what's cool and trendy and focus on what's good for the business. Sometimes you really do need the hard techniques. But usually, there's a replacement for EF and Linq that performs better and is easier to write and maintain.
[Serilog](https://github.com/serilog/serilog) \+ [Seq](https://getseq.net/) . You're up and running in minutes. It's pure profit.
Your approach to languages is pretty different from the architects of c#. I really think you'd be happier writing go. 
Lambdas replaced anonymous delegates. Do you really think they're less readable and comprehensible? I don't know what to say. 
lol Never mind. You're right.
I'm pretty sure I'd be happy using a language that pays the bills.
Do you think it's possible to create a language that forces bad programmers to write good code. I'd rather work with a good team than a constrained language and team to match. 
The language can definitely encourage or discourage good or bad programming habits. Just look at VB6 and the option explicit command. Of course everyone would like to work with awesome qualified people all the time. But sometimes you're stuck with what you're given. 
I agree in a way. I think Var and some other similar features were added to support other features they were adding to the language to support more dynamic syntax (lambdas and the ilk). To me, that doesn’t mean the developers should go crazy with it. Use the typing methodology that makes sense. IMO Var should only be used if you’re developing something that you can’t control what is submitted. They are also fun in Lambdas IMO. I would cringe if someone started using Var everywhere just because they can.
Why? Linq is literally one of the more innovative things MS has developed.
You think Lambda and Linq are bleeding edge in C# 10+ years later?
Isn’t that what Ada tried to do?
I was ready to agree to mostly disagree, but you surely lost the remainder of your audience here. Linq isn't just a DSL for EF, it makes all the looping code much easier to read. Those standard functional-style combinators, though misnamed in C#, are standard in most languages now because they make complicated sequences of iterating functions easy to understand through successive transformations and aggregations. I predict someone might object they personally saw bad code with it, but the alternative (absence of Linq) can just as easily produce bad code. More easily, arguably. The Linq is nearly always more succinct than equivalent loops, so easier to verify for correctness.
I think the idea is that while the implementation of \`enum\`s allows for any bit pattern to fit in a variable, it's not "doing it right" for a lack of better term to be using the \`new MyEnum()\` syntax. If your \`enum\` is supposed to contain \`0\`, you probably have a name for it already, just use that. And if \`0\`-ing your \`enum\` is supposed to be a domain error, you shouldn't be using the default in code anyway. I've found personally that I prefer starting my \`enum\`s from \`1\` because accidental \`0\`s are so easy in C#.
There are quite a few of us, we just get drowned out and downvoted into oblivion by var supporters. Var should be seldom used, but sadly we live in a Javascript world.
Some call it explicit, others call it verbose.
When am I going to be able to use `var x = new();`?
They've been randomly doing that to me all week. I see it as a way to annoy me until I break my reddit addiction. 
&gt; ​ &gt; &gt; What's the data type of the second parameter? They type name doesn't matter. What's important is that you are passing `IgnoreWhitespace = true` to `XmlReader.Create`. The rest is just noise.
&gt; I agree that var can be abused, I don't. I've yet to see an example of `var` that I would consider abuseive. 
To some degree, yes. That's why I turn on "Warnings as errors" and crank up the static analysis. 
F# uses let everywhere and they do fine. var is not a problem.
If you check on the roadmap, there are 16 NEW FEATURES being considered for the 8.0 release. That is insane.
I bet you also write Perl scripts in as few lines as possible.
Ok thanks! Do you know if we will see a maintained version of Crankier in the future? 
That sounds like an IntelliCode feature :)
And then in those situations you want the language to work best for the lowest common denominator? Sorry but I work in good teams and I want a language for professionals. 
Not going to argue about the feature or var, features which I like. But you don't have to delete any comma, visual studio have the "Edit.ParameterInfo" shortcut, which is default bound to Ctrl+Shift+Space.
Meh, one really knowledgeable won't do a `(address % 16)`. Instead you'd do a `(address % 0xF)` to check the alignment. I really think that it's safe to assume that an alignment of something besides `2^n` isn't necessary. Multiplication is useful, but division or remainder?
Then it is a *dynamic* type :-) 
A combination of [Sentry.io](https://sentry.io) and ElasticSearch, Kibana and Grafana!
You think Linq is hard to read and bleeding edge? Hate to break it to you but as a senior dev you really SHOULD be keeping your knowledge skill set up to date. No competent junior wants to work in 2.0 and become obsolete if they ever need another job. I have worked at a Fortune 500 company on 20 year old legacy software and even they managed to update to .net 4.5 on their absolute mega behemoth insurance software. Thinking LINQ leads to hard to maintain code is a crazy position to take, in my opinion. If you think using LINQ is “trendy” I can only imagine how clunky your code base must read. 
Nlog with asp.net extension Store in database One table to requests One to unexpected errors The traffic is not over 1MM, so this free solution serves me well
I’m not even sure how often it can be overused. What is a good example of it being overused? You can always tell the type very easily.
We have been using Rollbar and its awesome. You get alerts by email, sms and slack when something happen.nIt also have an awesome search and detail page. Do not ever store logs in your database because you it will affect your performance at scale (uless you dont care of course i dont know your product). I've seen database and apps crash because of logs. Just the reporting is worth trying Rollbar IMHO
I'm not sure how I feel about it still, we already have `var`. This seems a little excessive? Mads has never let us down before though so 🤷‍♂️
DevExpress is my goto WinForms library [https://documentation.devexpress.com/WindowsForms/15074/Controls-and-Libraries/Map-Control](https://documentation.devexpress.com/WindowsForms/15074/Controls-and-Libraries/Map-Control) &amp;#x200B;
Hi Spinach_Explosion, can you tell me if MSSQL is included in A2 hosting offer without fee? what about the performances ? (is it a shared server with another websites ?) 
Personally I see it at interchangeable for var when declaring variables; useful for field initialise in classes, but I think it would be confusing and potentially dangerous to allow this syntax in method calls 
I am really interested in this too. I hope someone finds the time to point us in the right direction
I tried var and personally don't care for it. I prefer to see the type alongside the variable name especially if its declared before initialization. I think this new() might be more to my liking (we'll see).
I make mostly internal network apps so I use Windows Authentication through IIS. It's very easy to use. We also use Shibboleth to authenticate users. It sits as a layer between IIS and the application (using ISAPI Filters) and does authentication through a remote web service. It was a pain to figure out how to set up but once we did it was pretty easy to get working. Of course it's only useful if there is a remote web service in your organization to take advantage of. I'm setting up a Linux server next so I might need to learn .NET's built in Identity stuff and implement support for that in the application. Not sure if the project needs authentication or not yet. 
Using it is pretty easy, as seen here: https://dotnet.microsoft.com/learn/machinelearning-ai/ml-dotnet-get-started-tutorial Reusing existing models is very easy. Building your own model, or understanding how/why a model cane to a conclusion, requires knowledge of data science. If you go down that path, you may want to consider getting a text book on it, or watching open source lectures. 
dotnet new mvc Or the default template with individual accounts in Visual Studio. 
our external OAuth takes care of the authentication part, and returns a cookie. It is a matter of wiring up the bits and make it work across both core and framework that has me concerned. 
What do you think about this? [Register](https://github.com/makarychev13/Notify/blob/a20fdca00101640afd25980c9ce27d9aa7323934/Notify/Controllers/AccountController.cs#L35) [Login](https://github.com/makarychev13/Notify/blob/a20fdca00101640afd25980c9ce27d9aa7323934/Notify/Controllers/AccountController.cs#L82) [AuthService](https://github.com/makarychev13/Notify/blob/a20fdca00101640afd25980c9ce27d9aa7323934/Notify/Services/AuthUser/AuthUserService.cs#L11) &amp;#x200B;
Yeah that's how our Shibboleth setup works. It returns a cookie to the user to track their session and sends our app a HTTP header with the username. I've never used OAuth but I believe there is a package for it in .NET Core. Hopefully you will not run into too many difficulties. .NET Core in general tries to be API compatible with .NET Framework when possible. though ASP.NET Core tends to be the biggest difference and so OAuth's integration is probably different as well.
Close. Our authentication piece comes from our single sign on -- which I can check the cookie in the signin method -- it is the assigning and persisting for the life of the user's stay at the application of the user's role / permissions that I con't determine how to do across both. At present, we decorate the controller's method with authorize and the role and have a custom filter that grabs the user's role and permissions and validates them in each application. I'd like to centralize this, make it work in both core and framework, and avoid the DB round trip for each page accessed. 
If you are talking about .NET backend developer, they are going to ask how they can support GraphQL on their systems.
I develop at work on a HP Zbook with a i7 6500U with 32gb ram and an SSD. I would say it is a pretty good experience overall, however, when we update our machines I will probably ask for a "REAL" i7 or i9, not a mobile CPU. &amp;#x200B; I sometimes have 100+ tabs open, using VS2017, while running other apps like postman, Android Studio + emulators.
Just use one of the built in templates? Is also a generator somewhere... It's built in IIRC.
Are you confusing authentication with authorization. &amp;#x200B; &gt; I make mostly internal network apps so I use Windows Authentication through IIS. It's very easy to use. This piece is correct. &gt;We also use Shibboleth to **authenticate** users. It sits as a layer between IIS and the application (using ISAPI Filters) and does **authentication** through a remote web service. It was a pain to figure out how to set up but once we did it was pretty easy to get working. Of course it's only useful if there is a remote web service in your organization to take advantage of. &gt; &gt;I'm setting up a Linux server next so I might need to learn .NET's built in Identity stuff and implement support for that in the application. Not sure if the project needs **authentication** or not yet. &amp;#x200B; Did you mean to use the word authorization instead of authentication? I've created a custom authorization service for work. Is Shibboleth any good? &amp;#x200B;
DevExpress' Express Application Framework is pretty comprehensive. It's not the graphical 'nodes' tool you imagined like blueprints, but it does drive most of its behavior from your database model if you want it do. [https://www.devexpress.com/products/net/application\_framework/](https://www.devexpress.com/products/net/application_framework/)
Probably. Let me be clear that the remote server does the authentication. Then they send back an authorization to a url on our server Shibboleth is listening to. Then Shibboleth issues the browser a cookie so it can track their session between requests. I am only really familiar with Shibboleth and the specific internal web service I used. I did try an integrated .NET Framework library to bake support directly into our app but couldn't get it to work. I really don't have much experience in this sort of thing but I've figured out the basics of how it works. Shibboleth works well enough. Since it sits off to the side as its own service and intercepts request sto the application, that means it is mostly transparent and you can use it for authentication without any web application even being aware it's in use. And you don't have to worry about portions of your application being exposed to logged out users (eg login form and anything supporting that). It's all very simple and separated cleanly. Of course you need a service to actually handle the authentication.
Last pass? 
Another thing to check would be your anti virus, is it scanning your source and output files on each build? We have some large projects that take for ever to build with AV running because it tries to run scans on each individual byte change per file. Solution to this was exclude specific file paths for real time scanning and have them scan once or twice a day instead. 
this is interesting when you have to pass a mandatory object to a method like Foo(new ()); or Foo(new(){x=3}); the issue is the readability ;)
[removed]
Shibboleth I believe can do both. It makes claims (that need to be tested) and provides authentication.
MVC frameworks on top of the in-built tools are not as helpful as some others tend to be. The biggest get would be help in routing though honestly I have found my hands tied by extra frameworks here more often than not. Biggest suggestion I could make is don't actually plug your core business logic into your UI pattern. Keep your business rules and data access in separate stand-alone and testable libraries and use MVC to build your front end. This makes everything easier to work with, test and you are less married to your front end choices. 
&gt;Biggest suggestion I could make is don't actually plug your core business logic into your UI pattern. Oh yeah definitely not, agreed there, I suppose I used MVC just as a placeholder for some framework, I'm mainly speaking agnostic of design pattern but MVC is what I'm using as an example. For example, in laravel to create auth controllers, password reset routes and auth middleware, there's ``` php artisan make:auth ``` Which then generates a set of routes and controllers scaffolded with middleware all using best practices. Or, to create a model and automatically generate all the CRUD routes for it, it's just `php artisan make:model Post --resource`. I can definitely see the merit in building those controllers by hand for more complex APIs, but I'm mainly curious if timing saving tools like this exist in .NET. What drives me crazy about express is it's so un-opinionated that there's 100 different ways to do something and all of them are right.
Been a while since I have had to but IIRC you are going to need to override in-built styles for the control and use a converter or a behavior to get what you want. 
Hmm... In my opinion NotNullAtribute look like as crutch
It is. And honestly, so is the C# 8.0 nullable reference types feature.
I am on the fence, I think it should be defaulted on, but it wasn't. And I think for backwards capability is why it is "optional" but it forces good coding practices and stops bad practices. So I am in favor of it, just sucks you need to add that in to every class. The practice it stops is people who use property injection, which should rarely be used but in my experience is actually heavily used due to ORMs and frameworks like Spring. One company I worked for refused to expand the constructors because it would break unit tests, so they just injected all dependencies. Of course if you forgot to inject a dependency, you could get a null reference. 
IntPtr isn’t unsafe, though. Actual pointers in C# are.
So it looks like identity will do both for and framework
C# is a crutch. So is assembly language. So are computers.
Behavior is definitely the way to go. I wrote exactly this in a previous project based on some blog I saw. If I can find it I'll post. Only took like a day, too.
Nothing makes me feel like an old out-of-touch programmer like discovering there is a tool called 'swagger'.
Alright then mate 
What I'm saying is that all of these things help programmers be more productive, including non-nullable types. Calling it a "crutch" is a disservice.
If it's any consolation, "swagger" is the tooling around "OpenAPI".
Identity with JWT. Access for specific roles, etc is filtered by auth policies.
Can you provide a code sample where the error occurs? What library are you using for SQL access? Are you using Entity Framework or just using System.Data.SqlClient?
Yeah, it’s unfortunately hard to add to a language at this late stage and with a large framework and third party library base.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/latexandloaf] [How to add Swagger to ASP.NET Core 3.0 Web API using Visual Studio 2019](https://www.reddit.com/r/LatexAndLoaf/comments/agega9/how_to_add_swagger_to_aspnet_core_30_web_api/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
So... swashbuckle. That’s the whole article. Been doing that since .net core 2.1 it’s not new in 3.
By itself, no it is not. But unless you just pass it around, as soon as you actually want to access yourself the memory behind, you will need an unsafe context, especially when using the `ToPointer()` method. I prefer to "think" about IntPtr as also being unsafe, since eventually it might be used in such context. Just to be extra "safe" ;)
Just started to use signalR as a service for a real time chat project. How do you detect and send a message to all other users when someone has lost connection or gone offline ? Is there a built in event ?
So, same as 2.1 got it. 
Welcome to "How to do X in Y" pump and dump blog posts. They are looking for being the first "tutorials" on new beta software releases hoping to get that sweet advertisement revenue.
I prefer ReDoc UI (Swashbuckle.AspNetCore.ReDoc) as opposed to the swagger ui though swagger ui performs better when there’s a large number of apis. 
&gt; I seem to have a really slow webhost, and so when uploading it takes a while to upload all the files in wwwroot. If you are seeing issues already with FTP, uploading is going to be the least of your worries.
I must be the only web designer not overly concerned about DL speed, and more concerned about upload. :-) I am making sites that for the internal use in a business, and will not be challenged with users. 
What I am saying is: virtually all hosting has symmetrical speeds. That means if you are already experiencing issues with uploads, that’s just the tip of the iceberg. Expect a lot more pain in your future until you find a different host.
Be that as it may, are there any tools that can show me what I clear out from the root folder? 
I recall seeing a global project setting instead of the attribute... 
I'm really confused... so... * You're not a .NET dev * You are the creator of a graphing tool that has nothing to do with .NET * You're posting on a .NET sub asking .NET devs to comment on why your software is important to them? Why does this relate to .NET at all? Why are you presenting your tool in front of a crowd of .NET devs? Why would we know any better than you why your tool is beneficial to any group of people? You're the developer, you programmed a project to solve a problem, shouldn't you know best of all what it's strengths are?
Swashbuckle looks like it's not that active..? I've been using NSwag with Core and seems like it is getting updated more frequently. 
You can try a web scraper/crawler. It will download all files linked from others. What doesn’t make it there is not used.
Your post has been removed. Self promotion posts are not allowed.
There is, add `&lt;NullableReferenceTypes&gt;true&lt;/NullableReferenceTypes&gt;` to your .csproj below the 8.0 lang version declaration. Only works for new project formats now, but will work for once it's out of beta
&gt; Do I have to post the furniture first, get the Id of what was just posted and then pass it into the FurniteImage object and then store it? Nah, just populate your `List&lt;FurnitureImage&gt;` when saving your `Furniture` and then invoke `context.SaveChanges`. EF will generate the Id's automatically - but if you want to be very explicit, you can first save your `Furniture` and then use the ID of the saved `Furniture` as `FurnitureId` of your `FurnitureImage`s to be saved. I'm very worried about your method receiving a `dynamic` object as input parameter. Couldn't you just receive a `Furniture` object as parameter?
Right. I’ve used it without unsafe a few times, since it’s also used for marshaling handles in pinvoke (which for some reason is safe, or doesn’t require the keyword at least).
I don't know of a tool, but you could check if any unnecessary files are being uploaded. Such as style.scss, style.css, style.min.css. you could mark two of those as "don't upload" This would have an even bigger impact on larger library files such as bootstrap.
I recently migrated an old site that had a few thousand dead image files. I searched for a similar tool, but no luck - ended up having to write my own tool. 
If you like the idea of Optional, look into LanguageExt - it has this and so much more. There's quite a bit of a learning curve for those that don't have functional programming experience, but I think it is definitely worth it.
Maybe the problem isn't on the server side, but with OP's very slow upload speed? Our server has almost 1Gb up/down, but at home (and in the office...), I can only have 15Mbit upload, so it isn't the fastest, no matter what the server can do.
Doesn't FileZilla have an option to only upload files that are either missing or older on the server?
You could implement an IValueConverter and bind to a TimeSpan-Value. Getting the format could be as easy as following, or a little more complex, if it needs localization public string TimeSpanToString(TimeSpan timeSpan) { var result = new StringBuilder(); if (timeSpan.Days &gt; 0) { result.Append(timeSpan.Days); result.Append("d "); } if (timeSpan.Hours &gt; 0) { result.Append(timeSpan.Hours); result.Append("h "); } if (timeSpan.Minutes &gt; 0) { result.Append(timeSpan.Minutes); result.Append("m "); } if (timeSpan.Seconds &gt; 0) { result.Append(timeSpan.Seconds); result.Append("s "); } if (result.Length == 0) return ""; return result.Remove(result.Length - 1, 1).ToString(); } &amp;#x200B;
you could open source your tool?
Well, you can use `var x = new {}`, but then it is an empty anonymous type.
 public async Task SignIn(HttpContext httpContext, User user, bool isPersistent = false) { // ... Omitted await httpContext.SignInAsync(CookieAuthenticationDefaults.AuthenticationScheme, principal, new AuthenticationProperties { IsPersistent = isPersistent }); } Can be refactored to just (removed the "async" and "await" - we don't need that extra state machine in our code, since this is the only Task that is awaited and its at the end of the function we can just return it.) public Task SignIn(HttpContext httpContext, User user, bool isPersistent = false) { // ... Omitted return httpContext.SignInAsync(CookieAuthenticationDefaults.AuthenticationScheme, principal, new AuthenticationProperties { IsPersistent = isPersistent }); } pp: don't do this to `Task`s that are run in a `using(IDisposable dependency)` block
Are you saying that you used LastPass auto-fill? I've not used that before, but I could try to test it out.
Or use a tool such as Beyond Compare; it has FTP compare capabilities, and you'd be able to only copy over the files that are different/missing
Yes but... If you read a little further in the article there's a global setting that makes you have to specifically declare anything nullable with a "?" ex: "string? foo" and anything without a "?" after the type is considered non nullable by default.
&gt; I'm very worried about your method receiving a `dynamic` object as input parameter. Couldn't you just receive a `Furniture` object as parameter? You are correct to be worried, this was just something I was fooling around with when researching how to POST objects, it will be receiving just a `Furniture` object as param. As for the List&lt;Furniture&gt;, currently when I attempt to populate it as such - FurnitureImages = new FurnitureImage() { PicutreInfo = newFurniture,FurnitureImages.PictureInfo } It tells me that FurnitureImages does not contain a definition for PictureInfo, but I do follow that pattern in the above Entities?
Try `FurnitureImages = new List&lt;FurnitureImage&gt; { new FurnitureImage { PictureInfo = &lt;whateveryouwantyourpictureinfotobe&gt; } };` and you shall find great success!
thank you for help
I was asking if there is anything specific for .NET
I personally find languages that take a functional first approach to be poorly suited to most real world problems without performing ridiculous acrobatics that make the resulting code horrendously unreadable, undiscoverable, and generally awful. 
WHATS F HASHTAG AND C HASHTAG LOL JUST WRITE IN JAVASCRIPT
Really? Have you ever watched anything off of f# for fun and profit?
See https://fsharpforfunandprofit.com, then you can really make up your own mind
F# is nice for things that you want to do fast, but nothing really changes. A good example is running a SMTP server. You often get a request to send something, you aren't changing that request, you aren't storing the information, and you are just sending it along. F# objects aren't mutable, which means any state change has a high overhead. For what you are looking at, most business class apps involve state. Most want to know what the current state of the system is, and then wants to alter that state. Also, C# is an OO language, so you can do OO design (which many do not, most common is procedural, which OO languages are still better than functional). But if you do OO design, you can manage complexity much better, which is another reason large scale apps are often written in OO languages. This isn't to say you can't blend the two, but know the trade offs. F# is a functional language, and C# is an OO language, they solve different problems. 
I agree, never seen a functional language useful in practice.
It was throwaway code. Basically it boiled down to: for file in cshtml files lines = File.ReadAllLines(file) for line in lines check line for image extension // iteratively, in case of multiples parse out image file name add file name to hashset for imageFile in images folder if hashset does not contain imageFile move imageFile to obsolete files folder 
I like Haskell better 
I dunno, functional languages are very good at a handful of tasks (at least). Data processing, parsing, mathematical computation (naturally).
F# is a "niche" language which excels in a few narrow use-cases. Writing front end code (GUIs) is not one of them. It's possible but you will be working harder than necessary and you won't really be taking advantages of the F# goodness. 
Generally you see people do things like creating anemic domain models, writing a bunch of procedural classes, and then claim they are doing OOP because they are morons. At least that’s my experience at a lot of places.
If Blazor isn't going to be a client side replacement for JS anymore, then I've lost all interest.
Thanks. Fixed.
[https://stackoverflow.com/questions/2575216/how-to-move-and-resize-a-form-without-a-border](https://stackoverflow.com/questions/2575216/how-to-move-and-resize-a-form-without-a-border) Always use stacc.
It already does this, just with large load size. They are continuing to work on bringing load size down and the client side only story. This just allows the same framework to render on the server instead of the client for those who aren't bound to client only. Also, I expect client only will be on it's way out of popularity within the next decade or so, it's a temporary problem almost entirely driven by slow network speeds and will be largely solved by 5g cellular and other wireless network improvements.
I think this is dead wrong. Latency will always be an issue with server generated content vs client generated. As data becomes much cheaper as with processing and memory, downloading client heavy scripting seems like it will deliver a much nicer user experience. However, I do believe the server side generation of content alongside clientside under one code base, is very nice. 
Thanks very much. By "stacc" do you mean Stack Overflow?
So.. Blazor is designed to work on the client, compiling c# down to web assembly to run on the web client. It's on the server right now only because web assembly isn't as mature as it needs to be. The intention is for it to be web client first. Not too sure of your background, but the trend is to move to the client.
yes 
You can't use `var` if you don't initialize the variable anyway, that's how the inference works.
OMG! thank you! exactly. With ORM and things like spring, so many people just make data objects and "services" and try to claim to be a service oriented design, which it isn't. I had one company have dozens, maybe 50 "services" that just passed around data objects that had no methods. Software under developed for 5 years, mainly done by a software company hired, and I look at it and see this. We have the idea of an "instrument". I asked why the "instrument" object didn't have things like, "instrument.SendCommand", instead we had, "InstrumentDispatchers.Dispatch(Instrument, command)". Things like Entity Framework, and spring are so nice if you need the, but I think they lend themselves to anemic models as people forget to expand the other party. As a note, in EF, there is a reason why it is generated as a partial class, you are to write the functions, aka the other part of the partial. Anyway, thank you, I have this argument so often by people who do "OO design" that believe you can do OO with an anemic model. 
Wrong. If it's a return value from something else you're assigning the type name doesn't appear. Yeah you can hover the mouse to see the type, but you tend to have a few of these things in a row, and hovering doesn't work if the code is on StackOverflow.
This would only make sense to me if you meant purely functional languages that require a lot of ceremony to do any mutable state. But that’s not F#, where all you need to do is to add a single keyword. And the assignment operator is different. Otherwise, it seems that you could easily write Fortran in it without loss of readability. Unless you love braces, but I read you can do that too.
OCaml is widely used in fintech, among others. Erlang has been used to write WhatsApp and other server software. Perhaps you should read more? While I personally would prefer for the people evangelizing functional programming to be more pragmatic instead of pushing only using functions everywhere that I typically see them do, your claim seems more ignorant than anything else. 
Yep. Our company is fairly cloud adverse all this works fantastic for us. 
It's intended to be both client- and server-side. They are merely only able to officially support the server-side version because mono.wasm is really slow and kinda buggy at the moment.
this sounds like MVC with extra steps. 
&gt; There are benefits to using the async/await keyword instead of directly returning the Task: Asynchronous and synchronous exceptions are normalized to always be asynchronous. The code is easier to modify (consider adding a using for example). Diagnostics of asynchronous methods are easier (debugging hangs etc). Exceptions thrown will be automatically wrapped in the returned Task instead of surprising the caller with an actual exception. https://github.com/davidfowl/AspNetCoreDiagnosticScenarios/blob/master/AsyncGuidance.md#prefer-asyncawait-over-directly-returning-task 
Client side won't go out of style, it just might retreat to only being used where it makes sense (highly dynamic systems and web apps with large concurrent user bases) instead of being touted as a paradigm shift. Blazor is just one of the first implementations that actually seemed interesting as a possible shift away from JS there to a more sane language.
Some of us have gone a step further to deal with everyone’s bullshit. We just create a MyProject.Domains and map the EF classes to the domain objects. It gives you a nice layer of abstraction to encourage richer domain models. Still doesn’t completely stop the service bullshit but it does help.
Even the publish option in VS has this option
Think about it from an MS bean counter's point of view. They'd rather client-side blazor died on the vine and server-side blazor takes off - that means more CPU cycles on Azure, their big bet for future revenue generation. It's easy to see why they are putting eggs into the server-side blazor basket while letting the client-side blazor experiment languish.
I love these classes.
We won’t use Azure with server side blazor though, we’ll just carry on using JavaScript. I mean, this is basically just Runat Server but prettier...
I don't think MS cares so much about dotnet anymore, but they do care about finding reasons for people to use Azure and keep using Azure and then using more resources on Azure (which equals more revenue since there isn't a single thing you can do on Azure that isn't profitable for them).
How do you access KeyVault from an app not running in Azure? Is this possible? Are there any examples/docs on this? All examples I see assume the web app is running in Azure. 
&gt; server side &gt; blazor Do you ever stop and think, maybe this life **is** a simulation? Hmm 🤔
I feel like I'm missing the point. What the fuck is the point of server side blazor, we can already run whatever fucking code we want on the server side - it's our fucking server!
entire telecommunication systems are written in erlang. seems pretty useful.
would live to benchmark it against https://docs.unity3d.com/Manual/JobSystem.html A similar system (with unholy dependencies).
Sure, but my point is that I had two main choices 1. Dotnet with JavaScript/blazor 2. Some other platform with JavaScript While you say they don’t care if I use dotnet, the vast majority of azure is still focused on dotnet and related tech... if I don’t use dotnet, I’m MUCH more likely to use AWS. In fact this is exactly what I do: my C#.NET stuff is on azure, the rest is o AWS Blazor was one of the big potential differentiators that could make me use dotnet in future (and therefore use azure) rather than another platform (and most likely use AWS) Whether or not I utilise as much frontend JS isn’t even in question here: that’s going to happen regardless. Trying to focus blazor on the backend won’t force me to use backend hosting... it will just leave me considering MS competitors In fact the only reason I use MS platforms lately is because it feels like they’re actually trying to build the best platform. The instant it starts to feel like they’re trying to force me into using it, rather than simply that it’s the best option, I’ll be going elsewhere. Make it good, don’t make it the only option.
Efficient data processing and parsing often require mutable state. For example, reusable buffers to reduce memory consumption. 
You may find this interesting. It's not about f# in particular, but he discusses the benefits of functional programming. https://youtu.be/7Zlp9rKHGD4
This looks pretty cool. Historically I have solved problems like this using Rx - is anyone able to explain to me why this might be a better technique as I’d like to start using it?
&gt; F# is a functional language, and C# is an OO language That's not really true. F#, which is based on Objective Caml, was always a hybrid language. And while many would argue it didn't become a functional programming language until the introduction of LINQ, C# has had first class functions since version 1. The syntax has just improved over time. 
It solves none of the problems I am currently having and introduces new problems. Sure, it may have a few 'nice-to-haves'. But C# is an incredibly powerful language and its getting more powerful with each release. Meanwhile F# has some unsolvable problems such as significant file load order that can never be solved due to the design of the type inference engine. 
Should work roughly the same. The easier way is to make a dummy Azure AD app through the portal, copy down the client ID and secret key and then use them on start up to access key vault. I've done something similar in a console app before which uses KeyVault to get some settings. Would be something like this: https://stackoverflow.com/a/43732088/1267231
You're not. Blazor is experimental, and not representative of the final product.
*\*Screams in WebForms Postback PTSD\**
True, but when I published using the VS publish to FTP feature, it failed on some files while it copied others perfectly fine. That's why I would rather use FileZilla than VS.
&gt;F# is a functional language, and C# is an OO language, they solve different problems. You make it sound as if functional programming and object orientation were mutually exclusive or generally at odds. That is not necessarily the case.
These are mostly meant for high performance scenarios, and are quite low level. I wouldn’t normally use them unless specific reasons call for it. 
I agree with the part that says, that diagnostics are easier, since you see the whole stacktrace instead of just the one Method-Call, but then again, async-diagnostics are ~~bad~~ very verbose. 3(4, counting main) Method calls, lead to 6(7) stacktraces internal class Program { private static void Main(string[] args) { MainAsync(args).GetAwaiter().GetResult(); System.Console.ReadKey(); } private static async Task MainAsync(string[] args) { try { await new Program().DoJobAsync(); } catch (Exception ex) { Console.WriteLine(ex.ToString()); } } public async Task DoJobAsync() =&gt; await JobAsync(); private async Task JobAsync() =&gt; await ThrowExAsync(); private async Task ThrowExAsync() { await Task.Delay(1000); throw new Exception("Delay expired"); } } Leads to System.Exception: Delay expired bei HealthChecker.Program.&lt;ThrowExAsync&gt;d__4.MoveNext() in C:\PROJECT\Program.cs:Zeile 36. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;JobAsync&gt;d__3.MoveNext() in C:\PROJECT\Program.cs:Zeile 32. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;DoJobAsync&gt;d__2.MoveNext() in C:\PROJECT\Program.cs:Zeile 31. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;MainAsync&gt;d__1.MoveNext() in C:\PROJECT\Program.cs:Zeile 23.System.Exception: Delay expired bei HealthChecker.Program.&lt;ThrowExAsync&gt;d__4.MoveNext() in C:\PROJECT\Program.cs:Zeile 36. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;JobAsync&gt;d__3.MoveNext() in C:\PROJECT\Program.cs:Zeile 32. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;DoJobAsync&gt;d__2.MoveNext() in C:\PROJECT\Program.cs:Zeile 31. --- Ende der Stapelüberwachung vom vorhergehenden Ort, an dem die Ausnahme ausgelöst wurde --- bei System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) bei System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) bei System.Runtime.CompilerServices.ConfiguredTaskAwaitable.ConfiguredTaskAwaiter.GetResult() bei HealthChecker.Program.&lt;MainAsync&gt;d__1.MoveNext() in C:\PROJECT\Program.cs:Zeile 23.
I agree, diagnostics from async-code are easier, when using async-await i love how they finally managed to make async-stacktraces understandable, now they just need to be as lean as Synchronous ones // SYNCRONOUS System.Exception: Delay expired at HealthChecker.Program.ThrowExAsync() in C:\PROJECT\Program.cs:Line 31. at HealthChecker.Program.JobAsync() in C:\PROJECT\Program.cs:Line 28. at HealthChecker.Program.DoJobAsync() in C:\PROJECT\Program.cs:Line 27. at HealthChecker.Program.Main(String[] args) in C:\PROJECT\Program.cs:Line 16. // ASYNC System.Exception: Delay expired at HealthChecker.Program.&lt;ThrowExAsync&gt;d__4.MoveNext() in C:\PROJECT\Program.cs:Line 30. --- End of stack trace from previous location where exception was thrown --- at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) at System.Runtime.CompilerServices.TaskAwaiter.GetResult() at HealthChecker.Program.&lt;JobAsync&gt;d__3.MoveNext() in C:\PROJECT\Program.cs:Line 26. --- End of stack trace from previous location where exception was thrown --- at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) at System.Runtime.CompilerServices.TaskAwaiter.GetResult() at HealthChecker.Program.&lt;DoJobAsync&gt;d__2.MoveNext() in C:\PROJECT\Program.cs:Line 25. --- End of stack trace from previous location where exception was thrown --- at System.Runtime.CompilerServices.TaskAwaiter.ThrowForNonSuccess(Task task) at System.Runtime.CompilerServices.TaskAwaiter.HandleNonSuccessAndDebuggerNotification(Task task) at System.Runtime.CompilerServices.TaskAwaiter.GetResult() at HealthChecker.Program.&lt;MainAsync&gt;d__1.MoveNext() in C:\PROJECT\Program.cs:Line 21.
That answers my question, thanks!
Interesting read. It is globally well-written, which is rare for subject on threading and deadlocks. Allow me to make one improvement suggestion, though. Most examples use `Task.Factory.StartNew()`. In general, it is recommended to use `Task.Run()` instead (cf. https://blog.stephencleary.com/2013/08/startnew-is-dangerous.html). Interestingly, the last sample does use it. I'm just nit-picking, but since we have a good quality article here, let's make it even better. Can't wait to read the next posts. Cheers :)
In the first example task1 and task2 are executed at the same time on Task.WaitAll() ? 
Good example of deadlock what I maid in my code: [http://blog.stephencleary.com/2012/12/dont-block-in-asynchronous-code.html](http://blog.stephencleary.com/2012/12/dont-block-in-asynchronous-code.html)
Thanks for the feedback and tip! Changed to Task.Run()
They begin execution before Task.WaitAll() is called. The Task.WaitAll makes the initial thread to wait for those 2 threads, which causes a 3-Thread deadlock. Without Task.WaitAll(), we would still have a deadlock with 2 threads.
&gt;Thanks for the feedback and tip! Changed to Task.Run() I see three `Task.Run`, but there are still 4 `StartNew` on the page FYI.
I'm not sure why you think Azure has anything to do with dotnet, you can run any server, any tech stack, anything on Azure. As long as your ops budget goes towards MS they don't give any flying wazoo about what you choose to use. It's why they are so much more focused now on supporting your choice to use Azure in any way, as opposed to in the past trying to lock you into Windows, SQL Server, Exchange, etc...
I think you’re missing the point: it’s not that I can’t run other platforms on Azure, just that most people who do so associate it with the MS tech stack. It’s not so much “if I don’t use dotnet I won’t use azure”, it’s more “good dotnet is more likely to pull me away from AWS” I’m talking from the tempting people to azure angle, not the scaring them away from it. 
The MS tech stack doesn't really exist anymore. They don't promote "use dotnet" or "use Windows" or "use Visual Studio". They only promote "use whatever you want on Azure". 
Just because they aren't pushing dotnet doesn't mean it doesn't exist: it's still very popular. But again, you're approaching this from the wrong angle: I'm saying people use Azure primarily because they're on dotnet, not that MS try to push people onto dotnet using Azure... my point is that MS are not using a "make it good and people will use it" approach to dotnet People using dotnet are much more likely to use Azure over AWS. People on other platforms are much more likely to use AWS over Azure. This is entirely unrelated to what MS are/aren't promoting nor what technology you can use on Azure: it's just a simple fact that dotnet/MS stack users are much more likely to use Azure. If MS want people to use Azure, the best approach for them to take is to make dotnet good: because those people are far more likely to use Azure
If you don't trust your vendor, that is a bigger question about why you are using them. Look over the recent issues with malicious code in the NPM world and the same applies to any third-party library you import, reflection or not, c# or not. If you must use this library, and you are worried they will steal your code or you boss wants to protect the code, your only choice is to make a microservice that it's only job is to call into this API and return the results to you. It has to be hosted out of process from anything else, and if you are really paranoid about security, in it's own VM/Container. This is a lot of work that would best be solved with communicating to the third party or engaging a lawyer.
MS has made it easier to use dotnet on AWS which is probably against their intentions. I think one of the main reasons they don't really focus on dotnet in general any more is because they already know they lost that on a macro level. Sure the devs still working on it will keep fighting the fight, but at a corporate level - MS doesn't care about dotnet. They only care about Azure.
You're running someone else's code on your machine, theoretically they have access to everything. Some say open source solves that black box problem, but really you're going to have to pick your fights. Third party software containing malware is usually a risk businesses have to accept if they want to run efficiently. 
It's a feature. If the compiler can't ensure at compile time that the variable is assigned **in all control paths**, then you get the unassigned error. And your second example will **not** compile. https://sharplab.io/#v2:C4LgTgrgdgNAJiA1AHwAIAYAEqCMBuAWAChUAmdY1AZmxwDZtTaB2TAb2My+xtwdQAsmALIBDAJZQAFAEp2nbotwBOKQCJAPBuAQfbUzCRRQF8FXE7X5CAKgFMAzsFkcD3ScEzib9/WYBuosJgAxgD2UHDiwOKhmAC8mABmogA2ttb6igDuABbiSdZSIWERUVAyToqKAPSVcKEA5G61mLbBALbWwDlQAOaYGRFZ7p7AZlWVHnbAMRP2iKTezhWY1U0t7ZjBndYB9hDx8aPc1QB0p2bGRGbVEKlDk5hSkpiiUJjWAB4ADmB2tiUwpxkZgAym1rKgAKyRUKPYYAKlIemIVxqwQ2Wx2wD2B0Wy0qZyIF0UZj42CEoPakOh0lc7jkbAuhiAA
Right, apparently I didn't fix all of them before. Thanks
Thanks for your quick reply! And also for the link to sharplab! You're right about the second example, VS still marks the second use of iTest. Although I seem to remember commenting out the line in the loop in my original program removed the squiggly lines at one time or another, but that piece of code was *slighty* more complicated ;) I guess we can mark this as resolved. Too bad Reddit doesn't allow editing the post after it has been submitted :(
&gt; I guess we can mark this as resolved. Too bad Reddit doesn't allow editing the post after it has been submitted :( They do. You just can't edit titles.
All the vendors in this area can be dodgy, nothing illegal though, it is just the nature of the beast. I have read through the user agreements of other vendors and found that they openly insert clauses which say they have access to the code you write (but they use their own platforms so it is easier for them to do this). This vendor uses C#, but given the behavior of other vendors I don't want to take any risks if it can be easily avoided....so.... My understanding is that they won't be able to see unmanaged code, however my life would be a lot easier if I didn't have to write this code in C++. If the callback function calls an unmanaged dll, which in turn then calls a managed dll written and compiled separately, would that solve the problem?
These guys can be trusted to not do anything nefarious. I have found clauses in other vendors user agreements which state they have access to the code written (the other guys do not use .NET though). It isn't that uncommon in this field so I just need to avoid the possibility of them doing it through more above board means (for example they might say something like "Oh we need to see your code to avoid errors and for debugging purposes", or some BS like that). I certainly wouldn't expect them to install malware on my machine.
Nice article! Deadlocks are crazy to fix, especially in the wild, far away from a debugger : ) One comment about your opening statement: *For me, multi-threading programming is one of the most fun things I do as a developer. It’s fun because it’s hard and challenging. And I also get a particular sense of satisfaction when solving deadlocks (you’ll see what I mean).* While I give you points for enthusiasm (and that's really important for growth as a programmer!), the reason you've stated here wouldn't make me happy as someone that had to work on your code in the future. I'm sure you don't mean exactly what you're saying here. Meaning, you're not writing multi-threaded code just for fun and challenge. But instead you're writing multi-threaded code when it's necessary. Either for performance reasons or other important non-blocking issues (UI is the most common). My thought is to re-word this part to encourage readers to also make the right choices when considering multi-threading logic. The tricky part is to keep your excitement and not write dry prose, a trick indeed. Keep writing, it'll take you far in so many many ways! (I'm adding you to my RSS feed right now)
I was not aware of the existence of `Monitor`, I just thought lock was lock (I did know about how `using ()` resolves to a `try { } finally {} ` block with `.Dispose()` though so I'm not sure how this never occurred to me as a possibility). I wasn't aware you could put timeouts on locks. Very good to know, thanks.
Yeah, I definitely didn't mean I try to make extra-Threads just to challenge myself (though I see how it can be interpreted this way). Just that when a multi-threaded problem presents itself, I enjoy solving it. Many Thanks on your feedback, it's great to hear!
If you run someone else's .NET code (in this case, the classes supplied by the vendor) they can potentially access your code through Reflection. The use of callback functions does not matter, it just provides an obvious starting point, but at any time you can go and enumerate any loaded assembly and start looking through its types. The steps you can take to prevent this are to not utilize their classes and create your own for accessing their internet APIs. But then you are giving yourself a ton of extra work to satisfy baseless paranoia. Choose wisely.
So if I call an unmanaged .dll with my callback function, which in turn calls a different managed assembly to do the grunt work, does this solve the problem?
That would be the equivalent if barricading the door and leaving the windows wide open (even if it worked, which it might not). Like I said, any code that runs under your user account (such as this vendor DLL could in theory go around your system and hoover up any .CS files it finds and transmit them somewhere else. There's nothing to stop them from doing something like this if they want to. There's a few things that assure me that this sort of thing is unlikely to happen to me, and this I can continue to use VLC or Chrome or Windows in general or whatever other software I think to be trustworthy: 1. Nobody cares about my stuff specifically. I am not that noteworthy. 2. And if someone cared about getting everybody's stuff, someone would notice their program acting weird and catch them. If something feels sketchy I will refrain from being an early adopter and watch what other people think of it.
Ok thanks, as I said to other commenters, I wouldn't expect them to do something nefarious or illegal, but something with a plausible excuse like "we maintain a copy of your callback function for debugging and to avoid errors", etc, isn't out of the question. Just want to check unmanaged code would avoid that.
SQL Server Management Studio can export the database and all its contents to a dacpac file, which you can then apply to your local SQL Server instance. On mobile so I can’t get you the links, but a quick search will show you how.
MVC? sounds like WebForms with extra steps. -- Rick Snachez 
mysqldump will get you the database. Then just run the queries that it generates to restore. As far as file storage, that depends 100% on how your app is set up.
In my experience dealing with production data dumps for local development is rarely the right approach. The most glaring reason against it would be security. What sensitive data might be in production that you likely shouldn't have on every developer's computer? Instead, I've always found much more success with building seed files that contain representative data, without needing to touch production.
This is basically what I want to do. I'm just going to use a few old out of date records from each DB, not an entire data dump. The DB's online have like hundreds of thousands of records. By seed files do you mean randomly generated? Is there a way to do that SSMS, or do you have some separate code you wrote to fill in the values?
First, don't copy the existing DB data, only pull down the schema (this can be done with MySQL tools easily). Once you have the schema down and in your own local copy load it up with FAKE data. You can use the real DB as a guide so your dev build makes sense when showing it to stake holders but take pains to ensure nothing in there matches whats in the real DB. After that its a matter of you build and deployment scripts. In general I set things up so the default when loading my code is dev. for prod I normal put the prod configuration in a secure location that is even a pain for me to get to and tie it to an automation tool like Jenkins. The prod config is applied during release. This not only improves security but severely limits your ability to screw over the world when testing something simple.
First of all there is no Microsoft MySQL. There is Oracle MySQL and Microsoft SQL Server. If you have SQL Server, Visual Studio has SQL Server Express which would allow you to use a SQL Server database locally for testing. You would need to download the database, and the easiest way would be to make a backup as described here: https://docs.microsoft.com/en-us/sql/relational-databases/backup-restore/create-a-full-database-backup-sql-server?view=sql-server-2017 If you are using MySQL I believe you can also dump the database, install MySQL on your PC, and restore. Assuming you have phpMyAdmin it has that functionality in the UI to do both the dump and restore. You can install XAMPP for a quick and easy solution to get MySQL and phpMyAdmin running on your PC.
He basically means you should be using a database with fake test data. If you are going to copy a database you should be sure there is no information in there that is sensitive, and that you have the all clear from whoever is responsible for that database to do this.
Apologies, I'm using Microsoft SQL Server. I would change the post title but I don't think I'm allowed to.
If your first example used do while instead of while it would not get marked. While while loops it is possible for the loop never runs, do while loops always run at least once. So in the first example it is possible iTest is used without a value assigned to it.
What advantage would it have over using endpoint routing with ASP.NET Core?
It's almost certainly your font. Many fonts have ligatures to do this kind of thing. Use a standard system font or look at a programming font like FiraCode which has ligatures for programming: [https://github.com/tonsky/FiraCode](https://github.com/tonsky/FiraCode) &amp;#x200B;
Carter will be using the new routing in [asp.net](https://asp.net) core 3 too. The endpoint routing as a middleware is great but once you have more than 6 routes in a startup file you will want to move that to another class. To keep the syntax of the routing middleware you can use Carter and not use attribute or route table conventions in MVC. Carter also provides things like model binding, content negotiation, file uploads, status code handlers
General route for this is that the DBAs need to be involved. One approach is: 1. clone production database to second environment 2. trim out old records 3. scrub the personal information on each record (randomize each letter of everyone's name, street address, etc.) 4. create a .bacpac file for developers to use locally This gives you production-like data, without worrying about data breaches. Also keeps the data set smaller (below 8-9GB is best).
https://i.imgur.com/utzTCyo.png
A clear sign that NancyFX is a dead project.
Rethink your strategy, developing against a production environment is a big no-no. Too much stuff that could go wrong and even if you copy all the stuff to your local environment, your program should be able to handle ANY data, not just the data already in the database. And if this app is being used commercially, I doubt that what you are doing is even in accordance with the customer's privacy regulations. So tread lightly and make sure you are covered on the legal side also before doing something like this.
Probably could make something like electron with server side blazor. Basically a C# UI App that works everywhere. You can still do client side blazor right now. I think they are just taking razor components out of beta first.
Yep, this was it. Thanks. Stay away from the Space Mono font I guess.
Great write up. Been meaning to read up on all the new .NET technology. This definitely covered them and gave great advice. Thanks.
I'm not finding Nancy-esque in the reuslts. What name does it go by?
Nope. Carter provides OpenApi support. See README for examples. 😄
This is the first I've heard of CSLA.NET -- any chance anyone has experience with it? 
Microsoft Mysql? Running in IIS?
By providing the information **manually** using the `RouteMetaData` class, which can **easily** get out of sync with the real implementation. That was already a dumb approach with NancyFX.
No one that I know. It's some kind of application framework with data modeling and ORM built in if I recall correctly. But the only way to get the documentation is to buy his books. I couldn't even get a press copy.
This is a good write-up, but one thing to be aware of is that the advice to get on .NET 4.6.1 or above is a bit out-dated. The .NET team basically admitted they never should have said 4.6.* supported .NET Standard 2.0 (see for instance: https://twitter.com/terrajobst/status/1031999730320986112) From personal experience, you need to bite the bullet and move to .NET 4.7.2 if you want to consume .NET Standard 2.0 libraries. If you don't do this you are asking for pain.
Both are Turing complete, feature rich platforms with a variety of languages and paradigms and great extensibility. This is like saying, "Is an orange more fruity than an apple"?
A server has no ui. What's the point of a ui framework for something without a ui? The only crazy sense I could make out of this is that they're doing per-request generation of wasm code to send to the client. Then that means using a templating system, like how we use razor today, to efficiently generate the wasm to send to the client. No part of this is ui on a server, just code generation on a server.
A question that night give you an answer. Are there any features of the JVM which are not accessible by standard Java language code?
Thanks for your support
Yes, I think that was part of the point for Graal/Truffle.
The CLR had about 20 languages when it got released, VB.NET, C# and C++/CLI from Microsoft, followed by Eiffel, Cobol, Lisp, Oberon, Pascal, ... from invited universities and partners. MSIL is lower level than JVM bytecodes, allowing the same kind of low level programming that a programming language like C would allow, even straight functions. In fact, the programming language that is able to explore all CLR features is C++/CLI, not C#. Many of the recent performance improvements in C#, F# and VB.NET regarding handling of value types, slices and such, don't require any runtime changes, rather exposing existing MSIL features that were only accessible to C++/CLI.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/learnjavascript] [Razor Pages POST complex object model binding does not appear to work with JSON objects](https://www.reddit.com/r/learnjavascript/comments/ah3i11/razor_pages_post_complex_object_model_binding/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Why build a whole new routing framework and not just contribute to the .NET Core repo?
It’s not a new routing framework it uses aspnetcore routing underneath. 
And here's a fun example that uses Monitor for more than just a critical section. public class Example&lt;T&gt; { private Queue&lt;T&gt; queue; public Example() { this.queue = new Queue&lt;T&gt;(); } public T Dequeue() { lock( this.queue ) { // Block until the queue has something in it. while( this.queue.Count == 0 ) { // Drops the lock and queues the thread so that it can be woken later // by the writer, but all as a single atomic operation. // When it wakes up again, it'll be holding the lock. Monitor.Wait( this.queue ); } return this.queue.Dequeue(); } } public void Enqueue( T item ) { lock( this.queue ) { // Store something in the queue, and then notify any Waiting threads that // there's something for them to chew on. this.queue.Enqueue( item ); Monitor.Pulse( this.queue ); } } }
Keep in mind that, in DotNet, a Mutex is a heavyweight object, unlike Monitor. The following classes only support intra-process locking, but are faster: * Monitor * SemaphoreSlim * ManualResetEventSlim * ReaderWriterLockSlim Most of the other classes - Mutex, Semaphore, EventWaitHandle - use native OS WaitHandles, and tend to be noticeably slower as a result. However, they support inter-process locking, eg, named shared locks.
Very nice, easy approachable read. Threading and deadlocks, the subject is complex but you explained very well. Look forward to the next. As an aside. Do you have any good articles on how to use awaitany. I'm currently using awaitall to do thousands of web requests, I do them in batches of 10 with awaitall then get the next 10. I could make my code faster with awaitany since I can just put another web request in the task collection once one is returned. I had a lot of headaches trying awaitany so went with awaitall. I can't seem to find good examples on how to implement what I want to do correctly. Or if this pattern is appropriate.
Experience years ago. I hit frustrations with it along with other collections of libraries at the time. So went off and created my own wheel... Many wheels at this point in time. Been pretty happy ever since. But can go into more details on CSLA if needed and I'm not on a phone. But it's Lhotka's lib. He goes on .net rocks, etc. time to time if you want to look up info.
Fantastic! I really enjoy the ASP.net stand ups.
Good stuff, thanks.
This is the most doublespeak bullshit answer I've seen in a while. Saying that something is Turing complete is pointless - every modern language is Turing complete, because it's such a low bar for real world languages. Hell, the `mov` instruction on an x86 computer is [Turing complete. ](https://en.wikipedia.org/wiki/One_instruction_set_computer#Transport_triggered_architecture). That doesn't mean it's useful.
OpenApi is a dumpster fire used by idiots.
Why are you posting to a razor page instead of an API controller?
Is the python 3 isn't turning complete meme still a thing?
 To address each point... CLR did not have 20 languages when it was released. It supported [VB.Net](https://VB.Net), and then C# and v1 was released together. C++/Cli didn't come till much later, it was even MC++ before C++/Cli even existed. &amp;#x200B; I'm not sure what you mean by MSIL is lower level than JVM bytecode. MSIL is basically just Microsofts version. They are both mid tier languages that are compiled to by another language to be used by their respective JIT compiler. Neither is more "low level" than the other. Lower tier languages are meant as languages that provide less and less abstraction from the given instruction set. That's the entire purpose of MSIL, to be an abstraction that the JIT compiler can use to translate to the specific machines instructions. I'm also not sure what you mean by "straight functions", Java Bytecode can only compile directly to functions... &amp;#x200B; Take for instance: &gt; public java.lang.Object execute(); &gt; &gt;Code: &gt; &gt;0: aload\_0 &gt; &gt;1: invokespecial &gt; &gt;4: astore\_1 &gt; &gt;5: aload\_0 &gt; &gt;6: aload\_1 &gt; &gt;7: invokespecial &gt; &gt;10: areturn &amp;#x200B; And no... neither cl/csc/roslyn allows for tail-recursion, even though the CLR allows it just fine. No language can use all features of the CLR directly (however you can always manually write IL that would then allow this \[although you shouldn't... for good reason\]). &amp;#x200B; And finally, Spans (slices?) required a CLR change. Hence why .net version got a dumbed down version, as they only wanted to make the change in CoreCLR, the main difference between how they are treating the two languages.
Why the attack on the previous comment. Yes turing complete has been used to denote some ridiculous mechanics - like `mov`. But this is a technical forum, and any mention of turing complete should be interpreted as 'sensibly turing complete'. Stick to r/programminghumor for other interpretations. Secondly the poster noted 'turing complete and feature rich' - this belies your point about `mov` style turing complete - `mov` is certainly not feature rich - I am really unsure what your point is here. Thirdly - doublespeek? Seriously, I spend half my time reading research and the language the poster used certainly indicates he is more well read then you, and certainly more on point. 'Turing complete ... low bar for real world languages' - Where to even begin with this, a turing machine is not a programming language, it is an abstract computation device. Which the JVM and CLI being discussed fall under. But also - i am not aware of any research that indicates turing machines are 'low bar'. Thats not to say there aren't alternatives - other models such neural networks are much better if your targeting AI. 
Here's a sneak peek of /r/programminghumor using the [top posts](https://np.reddit.com/r/programminghumor/top/?sort=top&amp;t=year) of the year! \#1: [Junior vs Senior](https://i.redd.it/bgdpq3rjjyn11.jpg) | [5 comments](https://np.reddit.com/r/programminghumor/comments/9i781n/junior_vs_senior/) \#2: [Believe me, run it again.](https://i.redd.it/x3h0rv7s2tn11.jpg) | [3 comments](https://np.reddit.com/r/programminghumor/comments/9i0d65/believe_me_run_it_again/) \#3: [Machine Learning is Hacky](https://i.redd.it/bjlfoj9qzby11.jpg) | [6 comments](https://np.reddit.com/r/programminghumor/comments/9x24sb/machine_learning_is_hacky/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/afd0dd/blacklist/)
 No, neither is more "language-neutral". Both are intermediate languages that simply need a compiler to translate to. For instance: both scala and kotlin which are top supported languages, but there are plenty of other languages such as JRuby, Groovy, Visage, etc, that all compile down to .class files for JIT to run.
I know they're both intermediate languages but some VMs are more "polyglot" than others. Admittedly, I don't understand VMs too well but the Graal stuff seems pretty cool and a different approach altogether (you just need an AST representation). I understand how Graal works at a very high-level but I was curious to learn more about Microsoft's approach since it's considered to be "language-neutral" (from what I've read).
As an alternative to installing a local database server, consider using Docker. There's a Microsoft SQL server image available on Docker with instructions to setup your own container. I've had great success with this approach - it just works. I don't have to install SQL Server and can turn off the container when I don't need it, saving resources. If I mess something up, it's easy to delete the container and fire up another. Best practice: keep your data files outside the container so you don't lose them if you delete the container.
Graal can run java bytecode just fine, and still uses the JVM :) you can choose to AOT compile, but its not required. If you haven't read it already: [https://www.graalvm.org/docs/reference-manual/polyglot/](https://www.graalvm.org/docs/reference-manual/polyglot/) scroll down to "How Polyglot works"
Yes, it can. I wasn't clear at all but I was sort of referring to dynamic languages with respect to the AST stuff. https://stefan-marr.de/2015/11/add-graal-jit-compilation-to-your-jvm-language-in-5-easy-steps-step-1/ &gt;Graal is a just-in-time compiler for the Java Virtual Machine (JVM), written in Java. In combination with the Truffle framework, one can implement simple abstract-syntax-tree (AST) interpreters that can automatically use the Graal compiler.
I used CSLA back in .net 1.1, 13 years ago. It was shit. But that was 13 years ago. I assume it's changed since then.
Agreed. From an [OS standpoint](https://docs.microsoft.com/en-us/dotnet/framework/get-started/system-requirements), any OS (except Windows 8) that supports 4.6.1 supports 4.7.2. You'll need to install it of course, which often results in a reboot, but that's a small price to pay for not needing to test the edge cases in .NET Standard 2.0 support you find at lower versions. If you can't get your installation base to upgrade to Windows Server 2008 R2 or Win 10/7, you'll have a rough ride. Unless you're lucky and can start porting some code to an earlier .NET Standard version. Good news is most people on those outdated Windows versions aren't supported by MS, either. That's generally enough to incentivize an upgrade.
"virtual machine" is really misleading because neither of them actually are. There a both JIT compilers. And while Java bytecode was designed to be easily interpreted (as opposed to JIT-compiled), it is not in actual production code.
Expanding on the problem. jQuery isn't sending JSON to your Razor page, it's sending a form post. That's why your data looks like form data```person[firstName]``` and not JSON ```person: {...}```. Axios and other ajax libraries are going to send JSON. A controller using ControllerBase instead of RazorPage would be able to accept the JSON without problem. So in summary, you've got 2 ways to send, 2 ways to receive, and right now the ways aren't matched up correctly. You can use action to send form data, see: https://stackoverflow.com/a/47630754/186593 But, that's not the normal way of doing things and I wouldn't recommend it.
Yup. Been there, done that. It's nice for a starting point for a multi-tiered architecture. It won't satisfy everyone, but one can do much worse. https://github.com/MarimerLLC/csla https://github.com/MarimerLLC/csla/blob/master/docs/Getting-started.md 
Well, I'm trying to steer the ship from .NET Framework WebForms to .NET Core. Razor Pages seemed like a more natural transition from that environment, and my application was not envisioned as an API/SPA setup beforehand. Honestly, I didn't think the application was going to grow to the point that it has, but I was finding myself doing pretty wonky stuff like generating html through string concatenation in jQuery because I didn't know a better way. I'm still a pretty junior dev, and my mentors haven't really kept current, no offense to them, so I'm teaching myself progressively. So just to be sure I understand, 'person[firstName]' would be the first parameter of formData.append()? Thanks for your replies.
A long time ago, before LINQ was ever a thing. They had a decent enough system for generating data access code off your database. I never used it, only evaluated it but went a different direction.
I feel like it may be dead for .NET Core. It's been in pre-released (2.0-clinteastwood) for awhile now, and the developers seem to not give a straight answer regarding a 2.0 production release. That, and the fact that Carter is now being worked on existing (or current) Nancy developers.. Seems a bit suspect.
How does JWT authentication work in Carter? Is it going to be similiar to Nancy's Nancy.Authentication.JwtBearer?
Good question. I'm thinking of moving from Nancy to Carter for a project and was wondering how this is handled.
I'm in a similiar situation as you. We'll have to wait for /u/jchannon to respond. Good luck!
Dapper is pretty great for a micro-ORM, but I find the real-world performance benefits overrated when compared to: * Productivity - I can produce far more functionality with EF in the same amount of time. Not having to write a ton of boiler plate is a bit plus! * Migrations - Having the database migrated for you and stored in the same git repo is amazing. Makes deployments super easy too. * Ease of maintenance - Large inline SQL statements aren't really great to read. And I use LINQ a lot, so I find it far more readable and easier to maintain. * Tracking changes - EF change tracking is a huge benefit. And if it's not needed (just a one-way read) then disabling tracking using AsNoTracking() makes it bloody quick - good enough that you probably don't \*need\* to use another framework for reads. * Transactions - I like that SaveChanges() wraps all the changes up in a transaction. Yes, you could do it by hand in Dapper. But EF definitely reduces the workload significantly. That said, if I wanted to use something other than an EF-supported database, or \_really\_ needed to eek every possible bit of performance out of read/writes, then Dapper is a pretty good option. Although for bulk inserts I'd use something like SqlBulkCopy to get performance gains over either Dapper or EF.
Couldn't agree more. I just tried on 4.7.1 and it drove me insane. 
&gt; I'm not sure why your post is being upvoted, it's wildly inaccurate :s Right. Although - I have only passing and likely out of date knowledge of the JVM - but I'm under the impression it has nothing similar to a struct or value type, and also has no pointer manipulation analogous to unsafe code in c#. If my understanding is correct, you could make an argument that MSIL is lower level than JVM bytecode. Does the JVM have anything like .Net's hardware intrinsics?
To make a net core app that has a UI on all desktop platforms, you could give users a console app that hosts a blazor app on localhost, and then opens the browser. I think the server side one figures out what the change to the html would be, and sends it to the client. If the page is static, this would be more efficient for the server, so by having both server, and client blazor you can choose, or switch later without having to change how you are programming.
No. Both VMs are mostly aimed at supporting OOP languages with single implementation inheritance and multiple interface inheritance. CLR has value types and reified generics, which make generic functions faster when applied to values types, but its a marginal benefit. Both platforms have FP languages running on them (Clojure and F# are the biggest ones).
It's not exactly that simple. CIL was designed to be language agnostic (for use with multiple languages) while JVM bytecode was designed specifically for java. This means that there are quite a few things you can do in the CIL that you can't do in JVM, although you can for the most part hide this from the programmer at the expense of performance. I found a good stack overflow post that gives several good answers about the differences: https://stackoverflow.com/questions/95163/differences-between-msil-and-java-bytecode
umm... what? The bytecode was designer to run with the JVM, not Java. That isn't how compilers work. If its available to the byte code, its available to any language that wants to make a compiler to use it.
What I mean is instructions exist and supported by the CLI that C# doesn't even use (when compiled to CIL) and that don't exist in the JVM. These instructions allow greater optimizations and control over the runtime.
 That still makes absolutely no sense. That's comparing CLR to Java. That has absolutely nothing to do with the java bytecode, or what it is limited to. C# intentionally doesn't do some things - such as tail recursion, on purpose, as it ruins call stacks. Again, that is not how compilers work, at all. If Java bytecode supports it, it supports it, no matter what language compiles into it. That. Makes. No. Sense.
I have no idea what you're talking about. C# and Java compile to bytecode that is ran through the jvm/clr. CLI provides more flexibility in what the bytecode instructions available let you do versus jvm bytecode. Which part of this is wrong? 
Hi all, It will leverage the JWT security middleware in ASP.NET Core. You can then tell your Carter module to authenticate request with the extension of `this.RequiresAuthentication()`. Any unauthenticated requests will be returned with a 401 response. I have made a sample application for you. See the screenshot in the README for the data you need to post to get a token and then how the token is issued to get a list of tokens. For a test, feel free to make a request to the path of `/` before you get a token and notice you'll get a 401 response. Hopefully this answers your question! [https://github.com/jchannon/CarterJWT](https://github.com/jchannon/CarterJWT)
How does the compiler know when to use the use the "NETSTANDARD2_0" path vs the other?
[removed]
Is this because of valuetuple and/or behavior of binding redirects?
Because it's just empty lingo. &gt; But this is a technical forum, and any mention of turing complete should be interpreted as 'sensibly turing complete'. Stick to r/programminghumor for other interpretations. [What the flying fuck!?](https://giant.gfycat.com/AlertAltruisticAnemonecrab.webm) [Turing completeness](https://en.wikipedia.org/wiki/Turing_completeness) is a rigorously defined term. Either something *is* Turing Complete, which means that can compute the same shit as a Turing machine, or it is *not*. There is not even a debate what "sensible Turing completeness" is supposed to mean, it's just meaningless lingo. Even bringing Turing completeness up is kinda mood, because, duh, of course a VM is Turing Complete, otherwise it would be less useful than good old *brainfuck*.
Not really, it gets really tricky into permissions in user space. Most processes can read their own memory space, which if you have managed code call unmanaged code calling managed code, it still lives in the same process and you can search the process for loaded managed DLLs. It is similar to how Visual Studio can show you details of all the processes on your computer when you say "Attach to Process". Your best process is to make this code run on it's own process, and communicate using some sort of TCP based protocol or local pipes. example. Your App-&gt; TCP Socket -&gt; Vendor Microservice Then the call back would just respond, if its not long (less than 2 minutes), you can do this all through a MVC app. If you split the app into application pools on IIS, IIS creates unique users and processes per an app pool if you really have to have it run on the same server. If you are local, you'll need to run two processes. As a security example I used to right click a process in Task Manager and select "Create Dump" and then open the file in Notepad searching for "Password" to find sql connection strings developers thought were secure, same concept works here, so don't load your sensitive code in the same process, threads won't do.
Because it is accurate, many of us were there since the pre-1.0 days. Yes, Managed C++ got replaced with C++/CLI. I even have it listed on my CV. The only difference between Managed C++ and C++/CLI was syntax sugar as C++ developers weren't that keen with the Managed C++ syntax. Everything else stayed the same. Yes there were around 20 languages, not from Microsoft. As I mentioned `from invited universities and partners`. You can check by yourself getting .NET Framework SDK Version 1.1 and looking into the *samples* directory, or if you are lucky one of those CD-ROMs that were bundled with programming magazines back then, which happened to have test versions from Oberon.NET, Component Pascal, Eiffel.NET, Cobol.NET,.... No idea where you took the tail recursion from. MSIL bytecode is lower than JVM bytecode because it represents everything a language like C or C++ require from a virtual CPU, including pointer manipulation, value types, unsafe type casts, embedding actual machine code (mixed mode assemblies), functions/procedures, ..., whereas JVM bytecode mirrors Java semantics and forces everyone to generate code that just looks like Java to the JVM. Spans required a CLR change for better performance, there are also available in .NET Framework runtime. And for evidence: https://www.nuget.org/packages/System.Memory/ 
Primitive types are value types, but as of now you cannot have a compound type stored on the stack (structs). There is work ongoing in order to implement this, though.
Would be worth adding that CLR supports tail calls so that F# does need to emulate it at the bytecode level like the functional languages on JVM have to?
F# actually emulates it at bytecode level.
Thanks! And sorry, don't know any good articles on that subject
&gt;The bytecode was designer to run with the JVM, not Java. That is not a useful way to put it, even if it might be technically true. JVM was not designed separately from Java. It was designed as a runtime environment only for Java, and its bytecode was pretty much an implementation detail. The result are things like the assumption that all method calls are subject to static type checking at compile time. It wasn't until much later when people had already built a bunch of other languages' compilers targetting JVM that changes were made to make it easier for implementors of other languages, e.g., by implementing invokedynamic. So the bytecode was designed for JVM, but JVM was designed for Java at the time. If you argued that design purpose is a transitive relation, you arrive at the conclusion that JVM's bytecode was effectively designed for Java.
Why would it need to emulate it, if at the byte code level there is an opcode for that?
I think it would be best to write your own middleware to do this. [Here](https://stackoverflow.com/questions/36179304/dynamic-url-rewriting-with-mvc-and-asp-net-core) is a related discussion on Stack Overflow.
&gt; Honestly, I didn't think the application was going to grow to the point that it has, but I was finding myself doing pretty wonky stuff like generating html through string concatenation in jQuery because I didn't know a better way. Put handlers on your PageModel that return partial views? [Learn Razor Pages : Partial Page Update with AJAX](https://www.learnrazorpages.com/razor-pages/ajax/partial-update)
&gt; In simple recursive functions, the compiler will often optimize tail calls into loops rather than actual method calls. This allows F# code which is written using recursion to run as quickly as imperative code which uses explicit loops. Tail calls in CLR do not work in debug builds, so your functional programs would always blow the stack if F# didn't emulate tail calls with loops.
While I am aware that the runtime is free to ignore the opcode, your claim that F# always emulates them contradicts the source I provided (which does, in fact, mention that F# compiler \*also\* tries rewriting the function bodies as loops and only uses the opcode if that fails). Do you have a better source, then?
Is it lock in just because they sell tools and services?
No, and I could have probably used a better phrase if I'd had enough coffee this morning! I mean to say, how do the free parts of the MS ecosystem steer users toward the pay-only parts? What pay-only services/products provide a sufficient value proposition that they become must-haves?
I think VS is the one main piece is you look at developer productivity and large teams. If you are going to use [ASP.net](https://ASP.net) Core then you are buying into the web server platform.
I recently discovered vsdbg is not yet open-source, and it can be used only in Visual Studio or Visual Studio Code (due to its license).
The SQL Server business intelligence stack. SSRS/SSAS/SSIS and PowerBI, all widely used and only available with enterprise licenses.
Azure is where they fix their customers in place now. Its been creeping into every single aspect of their open source stack now. Azure doesn't lock you in per se, but their focus is to provide fancy pants versions of everything. RabbitMQ, NoSQL and similar technologies all have a M$ version that does an inch more than the open source originals - these can put you in a vendor lock situation. Not as bad as using a closed-source programming language for millions of lines of code, but it's definitely a lock-in creep.
How so? I could just use VS Code on an AWS Linux vm...
&gt; inside the loop you are adding two to the existing value of iTest without having set it first This was just an dumbed down version of the code I actually use, but you're right so I edited my example to say iTest=2 instead of iTest=iTest+2
/u/jchannon great, thanks for this. I'll take a look. =]
It always did that.
You are still using Kestrel Web Server. I assumed you were talking about using the platform without any of their server tools, but you are right, having the free platform independent web server is effectively the same thing since it lets you pick your underlying OS and infrastructure.
 I can't believe I actually went back and installed .net 1.1... anyway, [https://imgur.com/obCjJiZ](https://imgur.com/obCjJiZ) [https://imgur.com/kRtcGgU](https://imgur.com/kRtcGgU) no, no it did not, and 1.1 was not even the initial release, but sure. &amp;#x200B; What do you mean you don't know where I took tail recursion from? None of the compilers can compile to it, but you can write it manually using IL. I don't understand. &amp;#x200B; &gt; MSIL bytecode is lower than JVM bytecode because it represents everything a language like C or C++ require from a virtual CPU, including pointer manipulation, value types, unsafe type casts, embedding actual machine code (mixed mode assemblies), functions/procedures, ..., whereas JVM bytecode mirrors Java semantics and forces everyone to generate code that just looks like Java to the JVM. That's not what lower level means, and no, it does not. [A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture](https://en.wikipedia.org/wiki/Low-level_programming_language) [CIL is a CPU- and platform-independent instruction set that can be executed in any environment supporting the Common Language Infrastructure](https://en.wikipedia.org/wiki/Common_Intermediate_Language) The language name itself is "Intermediate Language". To break down each point: &gt;MSIL bytecode is lower than JVM bytecode because it represents everything a language like C or C++ require from a virtual CPU I don't think you understand what a virtual CPU is. Please go read up what that actually means. Regardless, [https://docs.oracle.com/cd/E50245\_01/E50249/html/vmcon-vm-vcpus.html](https://docs.oracle.com/cd/E50245_01/E50249/html/vmcon-vm-vcpus.html) &gt;pointer manipulation I'm assuming you mean unsafe? Because bytecode (and CIL for that matter) work almost completely off of "pointer manipulation". That's the entire \*point\* of reference types. Nearly every instruction does this by default. &gt;value types Java byte code does value types just fine, they are the back bone of literally every language? I don't understand what you mean by this. If you mean you can not manually create your OWN value types, they simply manage the heap / stack for you. &gt;actual machine code (mixed mode assemblies) Which has absolutely nothing to do with MSIL &gt;functions/procedures Already posted how this doesn't make sense &gt;whereas JVM bytecode mirrors Java semantics and forces everyone to generate code that just looks like Java to the JVM It's bytecode... it doesn't look like Java at all. We reflect it back to look like Java so it's easier for us to read. If " 03 3b 84 00 01 1a 05 68 3b a7 ff f9 " looks like Java to you, I think we are writing in two different languages.
**Low-level programming language** A low-level programming language is a programming language that provides little or no abstraction from a computer's instruction set architecture—commands or functions in the language map closely to processor instructions. Generally this refers to either machine code or assembly language. The word "low" refers to the small or nonexistent amount of abstraction between the language and machine language; because of this, low-level languages are sometimes described as being "close to the hardware". Programs written in low-level languages tend to be relatively non-portable. *** **Common Intermediate Language** Common Intermediate Language (CIL), formerly called Microsoft Intermediate Language (MSIL), is the lowest-level human-readable programming language defined by the Common Language Infrastructure (CLI) specification and is used by the .NET Framework, .NET Core, and Mono. Languages which target a CLI-compatible runtime environment compile to CIL, which is assembled into an object code that has a bytecode-style format. CIL is an object-oriented assembly language, and is entirely stack-based. Its bytecode is translated into native code or—most commonly—executed by a virtual machine. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Their BI tools are the new money maker for them so I doubt those will be available outside their ecosystem for a very very long time if ever.
&gt; MS SQL Server has a Linux version There's a version that works on Linux, but is it open source?
Well yes. AWS is no different with thier managed offerings.
My understanding is with OWIN you don't need to even rely on Kestrel.
Interesting...if that’s true then I guess that’s not a lock in
Correct
No worries. 
Microsoft is changing focus from selling licens too being a service company.
Using completely open source products can lock you in too. Every component of your system that isn't trivially replaceable is a form of lock in. The language itself, every library you use, every service or product you integrate with. Open, closed, free or paid there's coupling between every pair of components in your system and there's a price for breaking that coupling. That's what lock in is. It's not some magical property of closed source code it's coupling you can't afford to break. If you write a bunch of code that uses non standard features of MySQL you're as locked in as if you were writing for Oracle. You can pretend that if you use open source products you will be able to fork the code if the project you're using decides to change direction in a way you don't like, but realistically that's much easier said than done. Instead of obsessing about these things, worry about how you use the pieces you use and whether you've made appropriate decisions on how they are coupled. You can't avoid lockin completely because there will always be elements of your architecture that are hard to change, but you can try to keep the number of these components low. 
For whatever reason you seem to want to make a point about the JVM, disregarding the experience of those that were actually Microsoft Partners and entitled to take part in .NET alphas, before it got announced to the world. Conveniently ignoring that Microsoft wasn't alone when .NET Framework launched. All in all, you seem to need a degree in CS. By the way I majored in system programming with focus of graphics, distributed computing and *compiler development*. Now feel free to ignore my reply, I have better things to do.
Thanks! Going to give that a thorough read.
We use Visual Studio Enterprise. Our source code is in Azure DevOps Git repositories. Our build definitions and release definitions are also in Azure DevOps. Our build and deployment agents are still on-premises though. You could say we're "locked in" to Azure DevOps but it's actually a very nice ecosystem. We used to use separate tools for version control, build, and deploy (TFS on prem, TeamCity, and Octopus Deploy respectively)
SQL Server: it's available on Linux, but it's still damn expensive if you need anything Express doesn't provide, and SSIS/SSRS aren't cross-platform AFAIK
That's some nice *multithreaded code* you got there. Would be a right shame if someone were to put their GIL in it..
It would only take an open source project to do a job even close to their BI stack. Not going to happen any time soon. Mondrian is nowhere near as well tooled.
A growing app is a great opportunity to learn new things and refactoring as you go to tweak your architecture is normal. API controllers are easy to implement and even in one of the older webforms projects that I maintain I'm using API controllers to handle all of the client-side ajax calls. The reason you were finding it difficult to get answers to your problem is mostly that it's not the "normal" way of doing things. In .Net 4.7 and older you'd use an ApiController, in .Net Core you create a class from ControllerBase. Here's a tutorial in Core: https://docs.microsoft.com/en-us/aspnet/core/web-api/?view=aspnetcore-2.2 These days even if I'm not building a SPA I generally just use a RazorPage or a MVC view to serve up my Vue components and any static HTML that I want search engines to parse. All of the functionality of the app is performed via ajax calls using axios to API endpoints. Separating your API controllers will also make your code base easier to grow and maintain. Let me know if you have other questions, the majority of my work is on the .Net stack of various versions using Vue. 
Right, MSK, Kinesis, etc. They charge premiums, no magic here.
I think this is really their angle. They make it easier for C# devs to write to the Azure managed/enhanced services like Event Hub, etc. There are some C# nuget packages for opensource stuff, like Confluent.Kafka and such that are quite good, though. 
Microsoft has moved away from making money off software and moved to making money from services
I don't recall issues with ValueTuple but we don't use that in our codebase as far as I am aware. Even with 4.7.2 there is constant binding-redirect hell, especially because of the apparent screw-up where the 4.7.2 SDK has version 4.2.0.0 reference assemblies for System.Net.Http and System.IO.Compression but no actual assemblies with those versions ever shipped and the ones in the 4.7.2 GAC at runtime are version 4.0.0.0. But anyway, the main issue we hit with 4.6.* was runtime failures where none of our unit tests could run. They would fail to load neststandard20.dll at runtime and the only solution we found was to upgrade to .NET 4.7.2.
CLR was designed alongside VB.NET and C#. JVM was designed alongside Java. Neither system is directly tied to their respective high-level language. I feel the misconception may occur since JVM has Java in the name and CLR has Common in its name. I, being more familiar with .NET, know about more instances of people working on integrating other languages with CLR (including Java in more than one attempt) than I do about people integrating them with JVM, but I know it's happened. There are, of course, differences between how CLR and JVM accomplish their goals, but at the end of the day, their goals are pretty similar.
I have seen all of these same issues and agree 4.7.2 eliminates most of them but I still couldn't find much information bout why from the release info. Glad I'm not the only one...
I agree. GCP is the worst offender here in my opinion.
Of course they can. Lots of stuff from azure devops is open source - that doesn't mean that building on top of it doesn't lock you in. My fear with azure is really only that at some point Microsoft settles on consumers being overly dependent on these small tweaks that they provide. I just don't like how seemingly unrelated products get a forced azure integration. I don't want a dotnet code signing tool that only works around azure infrastructure, when there's open protocols and existing mildly dusted software already doing what we need.
Awesome, this is super useful
Who assigned a task to you that you don’t have the skillset or base knowledge for? Are they hiring?
"Turing complete" **is** a low bar for real languages - because almost every programming language in the world is Turing Complete - it says nothing about the design, feature set, or practical usability of a language. Brainfuck, Malbolge, \`mov\`, are Turing Complete; just as Java, C#, Python, C++, and C are; just as CIL, JVM Bytecode, x86 assembly are. Every single one of those languages can be examined in the context of a Turing Machine model. It's a vacuous statement, because it's a basic assumption required just to even *consider* the language. "Sensibly Turing Complete" haha! What kind of /r/iamverysmart bullshit is this? It is either Turing Complete, or it is not. There are no gradations of Turing Completeness. \&gt; Where to even begin with this, a turing machine is not a programming language, it is an abstract computation device. Which the JVM and CLI being discussed fall under. Turing Complete refers to a specific set of abstract properties in the realm of computation and computability - a Turing Machine is an abstract representation of computation. It does not say that said computation has *anything* to do with a virtual or physical machine. Programming languages are abstract representations of computation, the same as virtual or real machine code languages like CIL or JVM or x86 assembly. It sounds like your "well-read" mind doesn't even understand the basic premise of the question of Turing Completeness! Well, here, since you're so well-read, apparently, here's the Wikipedia article on Turing Machines and Turing Completeness: [https://en.wikipedia.org/wiki/Turing\_machine](https://en.wikipedia.org/wiki/Turing_machine) [https://en.wikipedia.org/wiki/Turing\_completeness](https://en.wikipedia.org/wiki/Turing_completeness) Happy reading.
People with the experience to see potential in others and who like seeing others learn more skills to become an even more important asset to the company than what they already are. Also, it’s call being self-driven. It helps a lot and takes you places.
But there is not much of a problem to use elastic and kibana instead of powerBi? Just push json from your db to elastic and make graphs in kibana... But that is my current idea for our company and I still migth be wrong, doing some research on side, as for now.
You define it in the .csproj file. Like `netstandard2`
\&gt; E.g. Visual Studio still has plenty of features that are lacked by VS Code \*Laughs in Community Edition\*
You could use Mulesoft as a proxy in front of your web app. It can be configured on a number of ways using different workflows to have decision based routing. 
Azure PAAS services and Azure Stack
This isn’t going to work. Firstly you’ve defined your image data as byte rather than byte[]. But secondly, net core web api doesn’t have a model binder for byte arrays like this. Your only option, assuming you want to keep to this api contract as best as possible, is to pass the image as a base64 encoded string. The best way to do this though is to think restfully and start splitting this up. I would post the image separately. I’ve designed a lot of restful apis for clients, especially those being used by mobile clients, and you’ll find that mobile clients often need to have multiple sync queues, where binary files, such as images are placed in a different queue that gets processed on improved data connections. 
When did PowerBI stop being free?
What’s stopping you using SvcUtil.exe?
Topshelf combined with Quartz.net. Installs as a windows service. Doesn’t affect the web host. Topshelf can run multipel service jobs in a single host so you can separate your concerns. 
You should maybe consider the Request, Poll architecture. http://restalk-patterns.org/long-running-operation-polling.html Or webhooks/callbacks. 
A bad workman blames his tools.
I still like the old FileHelpers. It’s been around for ages, but I still love the design. It’s still my go to for parsing and generating. https://www.filehelpers.net
By "as if it always did that" I meant: as if it never used the CLR opcode for tail calls, not that never in its history has it tried to optimise it away. Which is not the case according to the heading "True tail calls" in the article I linked. My "always" refers to handling of different cases rather than evolution of how compiler treated the subject throughout its history.
I’ve been using this and I love it. Also using NSwagStudio to consume my own services and generate my api clients. 
Do many open source shops use even the free ms stack offerings? Our enterprise shop has always been ms stack. Now they have open source stuff - awesome. But we are forbidden from using dotnet core on Linux :shrug:
Push json from the database? You can... But should you? We've done some data gathering in a DotNet job and then posted from there to ELK. Having a database server doing calls like that feels weird to me
I love that this topic is different from the usual recycled materials. This isn't meant for newbies that's great for us experienced devs. Keep up the solid work.
SQL compare, SQL data-compare, and SQL Change Automation from [https://www.red-gate.com/products/](https://www.red-gate.com/products/) sound exactly like what you need. I've never seen OSS that comes anywhere close though. Let me know if you find anything.
In a recent reddit post, I suggest Razor may make it harder to use another IDE brand with one's Dot-Net source code. It's titled "Razor Templating is Overly-Complicated without Significant Benefits. MS-Fail." &amp;#x200B;
You mean "trying to", don't you? Most of their revenue still comes from software. They are pushing services because subscriptions can be more lucrative than one-time software purchases, but that doesn't mean customers will mostly want to go that way.
His comments about other languages are verifiable. The first implementation for Oberon.NET was [in 1999](https://web.archive.org/web/20040228072217/http://www.bluebottle.ethz.ch:80/oberon.net/projectGoals.html), while the initial public .NET 1.0 framework release was [in 2002](https://en.wikipedia.org/wiki/.NET_Framework_version_history#.NET_Framework_1.0). Here's an [article on Eiffel.NET from 2002](https://www.codemag.com/article/0209091), though no notes on when the implementation started. [Boo](https://en.wikipedia.org/wiki/Boo_(programming_language\)) was released in 2003 and was one of the few languages that had community support for a while. [J#](https://en.wikipedia.org/wiki/J_Sharp) was released in 2002 and was meant to pull in Java developers. [A#](https://en.wikipedia.org/wiki/A_Sharp_(.NET\)) was released in 2004 as a port of Ada to .NET. Clearly your "definitive" statement about the available languages is wrong. Whether or not it was 20 languages and how close the release dates matched for all of them I don't care to research, but there were tons of them. The problem is that they were done as short-term engagements and most weren't supported afterwards, nor shipped via Microsoft. So the support for them languished in a rather short timeframe. I wouldn't be surprised if much of this was started through individual negotiations with graduate students.
PowerBI has business critical features hidden behind a paywall. The most obscene one is automatic refresh on the server app. It fails silently without a Premium licence.
No they did. Its azure, office 365, and dynamics 365
Does that have an option that outputs something that uses HttpClient? I didn't find any documentation indicating it can. Or if doesn't output clients that use HttpClient, will they work in .NET Core with Polly policies?
IMO VS Enterprise is a decent lock-in right there. There are some decent features you won't get on any other IDE, and once you're used to them it's hard to go to anything else for C#/.NET development. Not impossible of course, but I'm sure it's locked some people in (and VS Enterprise is damn expensive, maybe the most expensive tool for a team if you're using it)
I had no idea. We wanted to use it in 2014 or 2015 but it didn't let you print reports so we stuck with SSRS.
Seems like the cool shit is in the enterprise edition. 
Check out [Fluent Migrator](https://github.com/fluentmigrator/fluentmigrator). 
I've used dbup in the past combined with octopus for deployments. It is just a framework for the deployments, it doesn't do the compares and manage the objects. If you start writing a custom tool I would try starting with it for actually applying scripts to the db.
Although open source, their desktop offerings still lock you into a specific platform, even on .NET Core. My guess is that since desktop applications have nothing to do with what Azure can make a profit off of, they have no interest in supporting Linux/Mac in that regard. I hope I'm wrong about this though and that they are simply porting the existing stuff before working on a new cross-platform GUI library.
Start with ASP.NET Core with Entity Framework (all free and open source), easier if developed with Visual Studio and easier to have a SQL Server or Azure Database backend because of the driver support. Basically you lure in a developer to commit his time and knowledge to your technologies, then they sink down in the rest of the platform instead of going to a completely different path (Java or other similar frameworks).
https://www.liquibase.org 
Used this in the past with about 20 devs. Ran into tons of checksum issues especially initially when a lot of changes where happening. That being said it wasn’t terrible 
Great reasonable answer. 
Is there software already doing what we need though? Setting up and maintaining anything using certs is really painful, that's why let's encrypt exists. Setting up rabbitMQ is fairly simple, but a kubernetes cluster to give you real redundancy is not and hosting your own at the small scale is pretty inefficient. You can code sign with out Azure, but it's much easier and more secure in azure and it's cheap as chips. 
You can use your own view engine if you want. Razor is just the default.
Those are all pretty expensive. I'd love to use them (for how much making they do, and the price, there's gotta be something there), but I don't know that I could ever get budget for them. Thanks for the recommendation though!
I believe this was recommended in the book on CD from Farley and Humble. Will check it out, thanks!
But you are not self learning, you are expecting to find ready made plugins. Does this makes sense to you at all ? Try drawing diagrams, sequence diagram, class diagram. Then start coding. And then yiu start asking specific questions on stackoverflow. I think you are a bit arrogant on what you can do.
Thanks for the comments! I'll be looking into this this weekend. 
Just a regular Visual Studio dbproj compiling a DACPAC file which can be deployed incrementally.
Doc links?
Give this a whirl: https://docs.microsoft.com/en-us/dotnet/core/additional-tools/dotnet-svcutil-guide
There are x86 binaries too: [https://dotnet.microsoft.com/download/dotnet-core/2.2](https://dotnet.microsoft.com/download/dotnet-core/2.2)
not for linux there isn't lol
We have a homemade tool which is similar to dbup. It was the best looking option when we evaluated switching (we just didn't)
SQL compare is great in some cases. In others it will recreate a table from scratch instead of just changing a data type (which is safe to change). Which will work fine in qa, and take forever on a huge table in prod
Maybe you could use https://github.com/chucknorris/roundhouse
Will check it out, thanks!
I second this, used in every project till we discover it
[removed]
Seconding this. Roundhouse is a great tool for running migrations (the console runner can easily be added to a build script.) Check the documentation first, as not everything about it will be immediately obvious.
I failed to mention this, but we're wanting to use SQL extended properties for the version identifier. Does roundhouse have support for it?
Not database server on its own but put "data pump" between database and elastic. You want to do some batch processing outside of peak hours.
Why? Some security concern? Crusty Linux admins?
We are currently using DbUp but I do love the concept of Anytime scripts that get run if they are changed. Things like static data could be added to in a single file instead of single row accumulative adds in disparate files.
Honestly with 2019 every you really need is in community. Live unit testing is not that great a value IMHO.
Why? 
Would you mind elaborating a bit which features you are referring to exactly? As I'm a Rider user (have used VS since the beginning, and still use it occasionally for winforms stuff), I don't really have much to complain about wrt features that I'm missing, but as I haven't used enterprise since 2005 or so, I likely overlook something. 
Many people predict that core will overtake .net framework and therefore will lead to frameworks slow death. I suppose that would be the main reason in your case. I think you will see plenty of warning before that happens though so unless you are genuinely interested then I would say there's no need to bother
My understanding is that a lot of the new functionality in .net will be added to .net core and may not make its way into .net framework. &amp;#x200B; So no, there isn't a reason to switch today, but .net core has more devs thrown at it and will likely surpass .net framework in the near future.
Asp.net MVC 5 will be supported for quite some time, but there will be zero innovation.
Yes, it’s a great bandwagon to be on! Old school .net isn’t going anywhere, but ASP.net core is more performant, and doesn’t need to be hosted on Windows/IIS, both of which can save you money on hosting. You don’t even need Windows to develop for it, I currently do all of my .net development (including debugging etc) on VS Code on a Mac! Also it has some nice features you don’t get with the older stuff, such as a built in DI framework, and there’s really not that much to learn if you are used to Web API already. Unless you are using parts of the framework not available in core (such as System.Drawing) you could probably move a lot to core in a pretty short space of time, and start enjoying the new features which get added in each release and not backported.
Honestly it sounds like you lack the knowledge to even break the problem down into individual steps right now. And most people on the internet are usually unwilling to help unless you have shown that you've at least attempted or made a start on the problem. More learning is required.
I do not think there is any doubt that .NET Core is going to overtake .NET Framework, especially since .NET Core 3.0 will support desktop UIs such as WinForms, WPF and UWP. [Microsoft: .NET Core Is the Future, So Get Moving](https://visualstudiomagazine.com/articles/2018/10/10/net-keynote.aspx) &gt; "We will continue to update the .NET Framework, but you'll see it slowing down," Massi said, noting developers primarily will see new security protocols or critical Windows features they can take advantage of, along with bug fixes and other things that maintain a very high compatibility bar so things don't get broken. &gt; So **.NET Core will be taking over new workloads from the .NET Framework**, and developers can migrate to the new platform at their own pace. Emphasis mine.
Well you can't compare Visual Studio (IDE) with Visual Studio Code (Code Editor). It's not same :)
I guess the biggest factor right now is staying current with technology and the impact of that on potential future job searches. Microsoft has gone "all in" on Core so they've clearly signaled that's the direction they're taking. It kind of reminds me of the split in the early 00's between .NET and VB6 and Foxpro. Those who stuck with the older technologies found themselves having more difficultly finding jobs and those they did find were mostly maintenance work on ancient and ugly code bases. I've been working with Core about a year now. I've found it really works well for developing containerized Web API solutions and some web apps. The 2.0 release fixed a lot of the earlier weak points. The good thing is that if you know regular .Net well it doesn't take you long to get the hang of Core. It's worth it to develop a couple of pilot projects using it just to learn it. &amp;#x200B;
Yes. Get on the bandwagon. There will be an initial "hurt", but "classic .NET" already feels old compared to Core. Whenever I take look at a classic .NET project, it feels like comparing classic WebForms (or even classic ASP) to MVC. Migrating now is not going to keep you ahead of the curve — core is where it's at. It will just ensure that you don't fall _behind_ the curve.
is there a WCF core version too ? and if it's not there yet, is there any news of it coming to [asp.net](https://asp.net) core?
Some (one that I know of, default interface implementations) language features in C# 8.0 won’t or can’t be implemented in Framework. I can imagine that will be the case in 9.0 too. 
I haven't seen anything equivalent on the self-hosted side of things. It seems like if you want your secrets secured, you need to deploy to Azure. There is no way to encrypt the appSettings Json files either, like they way you used to be able to do with the web or app config files. 
WCF client, yes. WCF server is not on the roadmap, and is stuck on the full framework alongside Web Forms. (If you’re starting a new project, you should be using Web API instead of WCF anyways.)
I'm thinking a custom solution is in order but it seems weird that nobody would have come across even a product that fits the bill
Yes. That's like asking if you should learn a JavaScript framework or keep using jQuery. .net core is the future. Don't get left behind.
Do you have “force column order” turned on? That’s usually why it does that.
There is [Vault](https://www.vaultproject.io/) by HashiCorp you can run it locally.
Not starting a new project. but will need to maintain a complex project that interacts with external 3rd party enterprise systems.. think "SAP" and the like, via SOAP / WSDL / etc. Even more reason not to jump on the bandwagon now. I also found an answer here that it's not on the roadmap! Shame. WCF is being killed off, just like Silverlight boo ! [https://github.com/dotnet/wcf/issues/1200](https://github.com/dotnet/wcf/issues/1200)
Plus I find it's so much easier to configure a .net core project ! :)
I think the exact opposite, Razor strength and biggest selling point is its syntax.
No idea, haven't used it in about a year
Yes please do. The two main factors are: 1. Runs on Linux. Yes this is huge 2. It's much faster and more efficient Not adopting it is a mistake
The future? I guess you would not be saying this if your camp was Oracle / Java, or Apple / Swift, or SAP ABAP, or PHP, or Django, ... there's other non-Microsoft ecosystems out there... but since we are both Microsoft fanboys.. I will forgive you just this time :-)
Idiot vp who "knows someone on the dotnet core team who says it's not ready for prime time". Mainly the concerns are performance in prod - what will it be like, how do you monitor it (apm integrations), how do you look at a memory dump, stuff like that. And to be fair the blogs I read do post about his that is all still in the works. But the benefits... Man.
Can we borrow the rules from /r/csharp? Rule 1: No job postings (For Hire and Hiring) Rule 2: No malicious, intentionally harmful software Rule 3: Posts should be directly relevant to C# Rule 4: Request-for-help posts should be made with effort Rule 5: No hostility towards users for any reason Rule 6: No spam of tools/companies/advertisements for financial gain Rule 7: Submitted links to be made with effort and quality 
What is "data pump"?
When it got good
Elasticache - crippled redis ☹️