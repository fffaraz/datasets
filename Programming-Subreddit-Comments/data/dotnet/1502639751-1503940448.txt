At the company I work for, we use Professional as it provides all the functionality we need, and we are not exactly a small company. 
Thanks - I've actually fixed the problem now, with help from a suggestion made by /u/AngularBeginner, [here](https://www.reddit.com/r/csharp/comments/6teblw/install_visual_studio_2017_with_aspnet_and_web/dlk0fpx/?context=3) in /r/csharp
Have you used intellitrace? I haven't had the chance and am curious how useful it is. The idea that QA can report a defect that contains what I assume is a minidump that devs can just immediately step into seems really nice.
Useless spam bot.
There should be a tab in the installer to select individual components where you can deselect .net core components and keep asp.net etc.
I've used it for historical debugging during performance monitoring. It's quite nice for that. We got premium licenses for the dev licenses and stuff like intellisence came for free when they merged ultimate down into premium. What is and isn't included in each version varies somewhat over the releases, but there are generally some really nice features in the higher editions. Release management inside TFS is pretty nice, even if it is lacking a bit of maturity. Visual studio is getting really good these days, especially if you don't install resharper. Not saying everyone should buy ultimate, but it's not an open and shut case, and it's a decision that should be made in consultation with the team using the software.
Yep, found it now. I needed to deselect the whole ASP.Net workload first - you can’t deselect it in the individual components. Then I turned on ASP.Net in the individual components, and that had the desired effect. Thanks.
It was for 2015, which is what we are in now. According to this they moved code metrics to pro but code coverage is still Enterprise. About the clearest chart I've seen on 2017 edition differences. https://www.microway.com.au/microsoft/visual-studio-2017-compare-editions.php
I cannot imagine life without codelens
Never saw anything in enterprise that justified the extra cost. 
A new .net ide Rider was just released the other day that might be an option for you as well it's also significantly cheaper then visual studio. It's built by the company that made resharper which is a tool that I find invaluable in my .net development.
CodeLens is the first thing I turn off on a new install :) (After deselecting every auto-newline option in the text editor settings) 
See also the uservoice topic from **July 31, 2015** here https://wpdev.uservoice.com/forums/110705-universal-windows-platform/suggestions/9110134-f-support-in-net-native-for-uwp &gt; **After one year since has been flagged as WORKING ON IT , the support for .NET Native is not even in roadmap** https://github.com/Microsoft/visualfsharp/issues/2400. See also this comment from the F# creator Don Syme. https://github.com/Microsoft/visualfsharp/issues/1096#issuecomment-293248047 &gt; My impression was that F# libraries would likely work immediately in UWP (not coreclr native code gen) if tail. was ignored. I think it's up to the UWP team though, not the Visual F# Tools team. It's not a compiler problem, it's a runtime problem. &gt; To be honest, it appears **UWP is simply not implementing the .NET specs correctly** - tail. has been in all editions of the ECMA 335 CLI Standard... Mind you, generics have also been in that standard, with no mention of "limits of 7 deep" or anything like that. **I'm always somewhat surprised when I see adhoc limitations that incorrectly implement the ECMA standard - I really thought that was a standard which mattered.** Come on Microsoft, this is unacceptable, you're driving valuable existing and potential programmers away from your platform and F#. See this comment from a major F# developer https://stackoverflow.com/a/32000593 &gt; As far as I know, not possible. **This has been a disaster for Microsoft, BTW. A major client of ours wanted tens of thousands of tablets and they took Microsoft tablets off the table because they don't support F#.** See also comments here https://blogs.msdn.microsoft.com/dotnet/2017/08/14/f-and-net-core-roadmap-update/ &gt; **All this obsession with web stuff to write F# programs is a big turnoff to all the people I talk with. You can’t even write a simple UWP app without having to deal with yet another web framework.** This is really sad since the language is vastly superior to C#. &gt; **Apple is upping their game by pushing modern languages like Swift and the JVM has Kotlin and Scala** &gt; It’s great to see all those improvements, but **I’m still looking forward to see full F# support for developing UWP/.NET Native Desktop apps.** &gt;&gt; **It’s baffling that other platforms now have better F# support than Windows itself.** &gt; I have mixed feelings about this. &gt; On one hand congratulations on the work done thus far in spite of lack of support from management. &gt; On the other hand, **Microsoft not considering Visual Studio support relevant to delay the release or the ongoing lack of communication regarding UWP, isn’t a good message for F# adoption.** &gt; Agree on .net core being primarily for linux and ignorable for windows developers. .net standard though has wide-ranging advantages unrelated to linux. About UWP, yes it has minimal market share for phones, but it does have significant desktop share. If you code in Xamarin, you get a Windows 10 app for free with C#, but not with F#. https://github.com/Microsoft/visualfsharp/issues/1096#issuecomment-305949873 &gt; I found this thread tonight while **looking for information about how to build a UWP application using F# in Visual Studio Community 2017. I kept seeing lots of references to C# .NET and VB.NET for this purpose, but suspiciously F# was missing.** Which made me curious: &gt;&gt; ... CAN you build a Universal Windows Platform native application with F#? &gt; And then I found this thread. **I now regret investing any time in examining F# as a future development platform for any purpose. I'll probably never pay attention to it again, and I worry about the sincerity of any of Microsoft's future developer endeavors.** And comments here https://blogs.msdn.microsoft.com/dotnet/2017/07/24/get-started-with-f-as-a-c-developer/ &gt;&gt; Full tooling support, every single feature as VB.NET and C# have on Visual Studio. &gt; Is it your expectation that every programming language be supported across everything you can use in Visual Studio? &gt;&gt; As for .NET Core, **most of us on the enterprise hardly care about .NET Core beyond UWP**, until it gets feature parity with .NET Framework. &gt; I will challenge this position. Perhaps you’ve not seen .NET Core in your organization, but we’ve seen strong adoption and significant interest in the enterprise for .NET Core. And this also goes beyond Visual Studio tooling. Many enterprises have developer who wish to program on macOS and deploy to Linux machines. We’ve made that a priority for .NET Core, and F# is every bit as capable as C# on that front. This has also been a significant area of growth for C#, F#, and .NET as a whole. See also https://github.com/Microsoft/visualfsharp/issues/1096#issuecomment-320984143 &gt; No further news or updates on F# Support for .NET Native. There is no affinity between .NET Native, .NET Core, or VS 2017. It's an orthogonal area. &gt; There are three options: &gt; Use UWP Bridge, Deploy elsewhere than the Windows Store, Use Fable + React Native And now you have prominent members of the F# community who are [officially endorsed by Microsoft](https://blogs.msdn.microsoft.com/dotnet/2017/08/14/f-and-net-core-roadmap-update/) spreading FUD against UWP, such as this Ionide-Fsharp developer. http://archive.is/4htRV &gt; **Well, they focus on platforms that people want to use and have potential to growth... Not on the dead ones.** See more of his antics on twitter http://archive.is/c9Yks http://archive.is/rLeua http://archive.is/KPRK0 http://archive.is/JxE4i Also sad that to see prominent members of the F# community are liking those tweets. This is so disheartening, since F# used have such a great Windows friendly community back in the days. **F# is a mature fully .NET compliant Microsoft language, it should be expected and a high priority to get the .NET Native UWP runtime (your main client application platform) to accept F# code.** Most people I know invested in F# with the expectation to target modern Windows without fuss and workarounds. And no, UWP Bridge and Fable + React Native are not acceptable solutions.
If you don't know about it already, you should also check out[ Team Foundation Server](https://www.visualstudio.com/tfs) (TFS) or visual studio team services for source code control, task tracking and release management. There is also an express version of TFS if you're only a group of max. 5 devs.
I normally grab msdn platforms for SharePoint devs. Only 1k and covers most the stuff they need. I prefer octopus over RM. 
&gt; if you need to know that two databases are truly the same these tools will tell you that in minutes. Without them... it would take at least a day or two of soul crushing work. Or you could export the DDL for both schemas to text files and then... I don't know, diff them. Best of all - that solution will work with any and all products you'll ever use. And, if you ever use a storage product that can't export a schema (relational or otherwise), then run far far away. Yes, these tools are nice. But there are easy ways to do without them. 
Nice, but why did they let the designers redefine "black"?
I have not tried it myself but for those interested, [check it out](http://www.manjuke.com/2016/09/enable-code-lense-feature-on-vs-2015.html).
&gt; It's an orthogonal area. This (well, not just this, but I might as well key in on something brief) tells me they've lost the plot. It _should_ be an orthogonal area, in that so long as an assembly containing IL is produced, and so long as it only utilizes the available dependencies, it shouldn't matter which compiler produced it or from which source language it was produced. That _would_ be orthogonal. The current state of things is absolutely not.
See also the comment I added from Don Syme.
Which is a fair decision, but one you're making as a Dev. This guy is making it for developers. For the matter of that, octopus isn't exactly cheap either.
So riddle me this... In what situation are websockets actually *tangibly* better than polling? I can think of perhaps only 2 cases; streaming video or audio, or real time interactivity (shared coding, games, etc) both of which are things that the majority of people don't use. Compared to say https://serviceworke.rs/ with background push notifications, I've always struggled to see why websockets are a superior technical solution; it seems to me that they scale poorly, increase complexity (eg. manually implementing message handling and routing) and have poor debug-ability (since most browser tooling doesn't let you capture messages). Am I missing something? If websockets were a 'pure' socket like connection that you could open to an arbitrary host I could see them being useful as a generic server / client side common layer, but the basic websocket connection mechanism is: - open an http connection to a normal webserver - upgrade the connection to a websocket - use websocket Without a fully fledged webserver on the server side you can't even use them. :/
Octopus is a lot cheaper than upgrading all your dev's to enterprise. Heck it's cheaper to buy octopus enterprise than it is to upgrade one dev to vs enterprise. I don't think I'll outgrow Team edition anytime soon. Yeah he's trying to make a decision as a non dev which isn't ideal. It depends a lot on what his team is going to build as to whether they need enterprise or not.
It is very common that in a single application you use more than one message broker. For example, ZeroMq is an in-memory messaging queue and it is very efficient because of this reason; so, it can be used for logging purpose in a separate thread. Or the scenario could be using free message broker in Development Environment and paid one in Production Environment. So there are many possibilities using more than one message broker in a single application.
I was able to see socket messages in chrome when using signalr
Sheesh, but to be honest I'm not all that surprised. It is as if the industry needs to be reminded every few years that WinDiv hates DevDiv and only works with DevDiv begrudgingly. It has always been true, it will likely always be true.
Why not use something like: https://github.com/rebus-org/Rebus They support alot more :) 
The repo I shared is maintained by a close friend. But the one you shared seems quite developed already. Thanks.
If you have a small number of devs, but a large number of projects octopus costs a fucking fortune.
5k is unlimited no?
You can't officially (yet). While .NET Standard 2.0 is finalized, the support for it in existing runtimes is not. Later, when it's finalized, you can just reference your netstandard 2.0 project in an existing .NET Core 2.0 or .NET 4.6.1 project. You can not use .NET Standard as a "running" project (e.g. the console). For this you need to use either .NET or .NET Core (or later Mono once it supports .NET Standard 2.0). .NET Standard is just an API specification - you still need the runtime.
For one year of support and upgrades. The cheapest it gets to is 10k per three years. Plus your MSDN platforms with SA and you're pretty close to even on cost for enterprise VS licenses for a small team. Small teams at large companies don't get much in the way of discounts. 
Yeah it makes a bigger difference with larger teams. Giving 40 odd developers enterprise would hurt. 
Do people use workflow outside of Sharepoint development?
Another similar-yet-different framework is the Microsoft project Orleans. Halo 4 and 5 multi-player were coded with it. 
The lack of Code Lens in Community was my deal breaker. Other than that, they pretty much differ in licensing. 
Visual Studio has it's own built in versions of the schema/data compare tools (under *Tools* =&gt; *SQL Server*), I find them as good as the red-gate tools personally.
Needs an editor. There's a lot of grammar mistakes.
Wait. You said the old API system means as you target older platforms the fewer APIs you could use. But the new system opens up all APIs **for platforms that implement 2.0**. Well then the situation hasn't changed. For those platform targets that needed a restricted set of API, you'll still not be able to use all the API with them.
Can't this also be avoided by writing your libraries to target .net standard versions, instead of specifically .net core and .net framework? IIRC this is the direction they're wanting us to go, that way libraries are compatible between anything that supports that standard version. edit: This is of course referring to already released and supported tooling for .net standard versions. Of course somebody down lower in another comment has pointed out: It's a bit different if the standard you want to target is not yet supported in available tooling/frameworks (ie: netcore and the full framework) yet.
To me this makes pretty good sense, though I think their initial way of introducing all this was rather confusing. Make your libraries specify a standard so they can work between core and framework, and any other runtime/framework that comes up and supports it (ie: mono.)
Yes, but `Enumerable.Append` is not part of the Standard, so you can't use it in a codebase targeting that.
That's kind of what I'm getting at. Target the standard if you want to make sure your library works between framework and core.
It sounds like the GUI rendering and the OnClick are occurring on the same thread, and therefore you're unlikely to get rendered updates as the function is executing. The rendering stage will occur later on, after the click call returns (OnClick ends). The data bindings themselves should be updating due to INotifyPropertyChanged. If you were to process the input on a second Thread, things can render in the UI (on the main thread...) as your function is executing.
.Net Standard 2.0 provides the largish 32k APIs for all the main runtimes i.e. .Net Framework (Asp.Net, WPF, Winforms), .Net Core and Xamarin. This document: https://github.com/dotnet/standard/blob/master/docs/versions.md shows which version of .Net Standard to target if you wish to target Windows Phone or Windows phone Silverlight or one of the more niche platforms. From the perspective of .Net Core and its adoption, .Net Standard 2.0 is huge.
Yes that's what I'm understanding... It seems I have to review and rethink most of my code. On the other hand, I just discover that if I add MessageBox inside my MessageListener function, it triggers the Update of the UI after each MessageBox. I'm gonna give a shot with this. Since in fine I'd like the user to validate he has read each content. (A bit like in this demo: https://codepen.io/punkydrewster713/post/turn-based-battles-in-the-danger-crew ) Thanks for the help.
TIL. Thanks for that!
There **are** a lot of grammar mistakes.
It shows up in the little things; I run VS in administrator mode, so I can't drag files into it from Windows Explorer.
lol
[Muphry's Law](https://en.wikipedia.org/wiki/Muphry%27s_law)
**Muphry's law** Muphry's law is an adage that states: "If you write anything criticizing editing or proofreading, there will be a fault of some kind in what you have written." The name is a deliberate misspelling of "Murphy's law". Names for variations on the principle have also been coined, usually in the context of online communication, including: Umhoefer's or Umhöfer's rule: "Articles on writing are themselves badly written." Named after editor Joseph A. Umhoefer. Skitt's law: "Any post correcting an error in another post will contain at least one error itself." Named after Skitt, a contributor to alt.usage.english on Usenet. Hartman's law of prescriptivist retaliation: "Any article or statement about correct grammar, punctuation, or spelling is bound to contain at least one eror." Named after journalist Jed Hartman. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
"There's a lot" isn't grammatically correct? "There is a lot" sounds right to me.
The only real thing most people want in Enterprise is Visio on my team. But it is cheaper for us to do an enterprise office license. We are also looking at adding resharper with cost savings from not being on enterprise. Pluralsight licenses are included with enterprise but would have to be cheaper else where. It is all about bundling. Personally I could care less about the enterprise license I was given have requested it be reduced in the future to save the company money. I would rather they get me a new laptop every 2 years than give me this enterprise license. :)
That seems like a whole lot of unnecessary effort when you can just replace the session state memory provider with the redis provider. https://www.nuget.org/packages/Microsoft.Web.RedisSessionStateProvider 
Whilst
Indefinite informal pronouns like this are sometimes tough. Since 'a lot' here is synonymous with many (which is plural), you should use 'are'. [More info](https://ell.stackexchange.com/questions/114186/there-is-a-lot-vs-there-are-lot) (Obviously, we're getting nit-picky, but he started it!) So, if you can replace 'a lot' with 'many', use 'are'. 
Is this Azure only?
Reddit post vs. linking to a so-called "article." Different standards, my dude.
Nope.
&gt; I suck at Linux right now so kinda hesitant to check out .NET Core in Linux Any recommendations for Windows dev to Debian (or something eqv) to get started?
There are **many** grammar mistakes.
You need to use async and await, but it doesn't have to be intrusive. Your button click would need to be async and at any point if you were to say do a await Task.Delay(250); you should be able to see the UI thread free up, write the previous turn data. You are locking the UI thread. It can not update the UI until the click has resolved. By starting a task you can await anything and free the ui up to update. If you are writing syncronous code. You can do something like this. await Task.Factory.StartNew( () =&gt; Step1() ); await Task.Factory.StartNew( () =&gt; Step2() ); await Task.Factory.StartNew( () =&gt; Step3() ); 
I have done a little dotnet core on linux. Visual Studio Code has been my editor of choice (and text editor of choice). I mostly use Ubuntu. https://code.visualstudio.com/ https://www.microsoft.com/net/core#linuxubuntu 
Depends on whether the subject is plural, right? "There is a lot of pie" vs "There are a lot of pies."
You can use a completely separate file (such as config.json) and store your connection strings, emails, password, etc. in there. If you're using ASP.NET core, you can reference the file like this in your startup.cs: var builder = new ConfigurationBuilder() .SetBasePath(_env.ContentRootPath) .AddJsonFile("config.json") .AddEnvironmentVariables(); then grab the value like this: _config["ConnectionStrings:Main"] where `_config` is a reference to `IConfigurationRoot` Then your config.json will look like this: { "ConnectionStrings": { "Main": "string here" } } You need to be sure to include the file in your deployment as well, but exclude the file from git
It depends on whether the subject is _countable_. Pie is a bit confusing because it can be countable (tins of pie) or uncountable (... I'm not sure of a succinct way to describe this. Just image a big pile of pie in front of you, with no way to quantify the amount of pie). Water is uncountable. There is a lot of water. People are countable. There are a lot of people.
Will this be the same if I'm not using core yet? I'll be honest I've not looked at core - is there a big difference?
Relevant stackoverflow link: [link](https://stackoverflow.com/a/10029326) Although a super basic google search would get you to the same place. Took me about 2 seconds.
Oh, just change your webconfig to reference another xml file instead https://stackoverflow.com/questions/6582970/separate-connectionstrings-and-mailsettings-from-web-config-possible Also, the idea is mostly the same as the old ASP.NET MVC, but the implementation is a bit different. There's some great courses out there if you want to look into making the switch. Core 2.0 was just released today and it's looking pretty good, so it may be a good time to start looking into it.
I have taken the approach of adding connection strings and environment variables on my boxes. It helps solve this issue and ensures it is harder to screw up a deployment.
Anyone know if there are tooling improvements? With Entity Framework Core, there was no designer at all since they abolished edmx files. The "model" had to be generated using the command line. Has the Visual Studio tooling been improved in this area yet? Anyone aware of incoming improvements?
Has somebody a good example project to generate a visual studio solution file with cmake?
I doubt there are tooling improvements for EFCore https://github.com/aspnet/EntityFramework/issues?q=is%3Aissue+label%3Aarea-tools+is%3Aopen has 0 results and the closed issues under that label haven't changed since May. There is a doc page for EF Core 2.0, but it was last updated apparently in October last year? https://docs.microsoft.com/en-us/ef/core/what-is-new/index On the other hand, the roadmap page was last updated in may and states that EFCore will update in sync with .Net Core, so I'm less sure what to make of the doc page... https://github.com/aspnet/EntityFramework/wiki/Roadmap None of those links suggest incoming improvements to EFCore tooling.
That arrived a little quicker than expected. Good news indeed.
For a long time, I never stored passwords in the conn string, just used integrated security. I've been pondering putting them in the target machines, but it feels like the old days when I used a named ODBC connection that you would create on each machine. 
I'd like to try it. What's the easiest way to deploy and run on the cloud? Is there anything Heroku-like?
Azure
Is this the one people were saying would come on September 18th?
502 Bad Gateway EDIT: Thanks for shooting the messenger. Anyway, site appears to be back up.
Still no lazy loading of navigation properties...? Isn't that one of the main features of EF? Confused why it wasn't implemented in 1.0.
It appears to be yes. I had been planning around that date myself but it looks like I'll be changing my plans!
What I was thinking as well. I took the last few weeks off to teach myself Angular 2+ because I knew 2.0 was coming. Good thing I'm almost done with this ng2+ training!
I think they want everyone to go code first. And correct me if I'm wrong, but you wouldn't have edmx files if you're doing code first.
You may be correct. I personally prefer database first, but even code-first had some sort of "designer" with "old" Entity Framework.
I too prefer database first. So much easier to manage.
yes i much prefer database first and ef core has made this much easier, actually. Use ```dotnet ef dbcontext scaffold``` via the [cli](https://docs.microsoft.com/en-us/ef/core/miscellaneous/cli/dotnet). It's the equivalent of the right-click on an edmx and update, etc. So I basically change the schema however I want and re-scaffold. I don't bother with migrations, the ORM remains downstream, and I use another strategy to manage my database, and rescaffolding is far more reliable and customizable as a scriptable part of my build system instead of a right-click gui intense way. EF core has some things missing that EF 6 did, but I do like the tooling, and the direction they're headed.
Razor, more APIs and packages... aight, I can deal with this That said, you're gonna have to drag me kicking and screaming if you want me to use any version of Razor where I don't have Helpers or something that replaces them...
You could use LLBLGen Pro as designer for EF Core. (https://www.llblgen.com). It's not free tho. (disclaimer: I developed it) We'll add .net core 2.0 / ef core 2.0 support in the coming months.
Azure webapps are heroku like
Don't you need to run your app pool as a network user with access to the database then? That sounds kinda scary. --- Our deployment script runs as a user with access to a configuration database and uses a few variables to get a connection string from that database (namely the web site description in IIS for the location, the server name and the virtual directory name). With these the deployment user modifies web.config and stores an encrypted section for the connection strings using the server pool's machine key.
There has been discussion about [lazy loading being an antipattern](http://www.mehdi-khalili.com/orm-anti-patterns-part-3-lazy-loading) but more than anything the implementation in EF6 was not optimal (or not used correctly) so it has been held back while thoughtfully evaluated. Looks like there's been a recent update on this and it's been tagged for [EF Core 2.1](https://github.com/aspnet/EntityFramework/issues/3797)
I'd highly recommend Visual Studio Code with the Vim extension (vscodevim). They've done a wonderful job and the latest versions are moving towards neovim integration for ex commands.
Just a note, 2.0 release won't work in Azure App Service, only preview 2 is installed to the VMs
Can anyone recommend any eli5 article of core and standard? I've been out of the loop. Something very high level and easy to understand 101 before I dig deeper into it. 
Typically (if you're running a nontrivial environment), you'd look at putting your production environment into a domain and use SSPI/Integrated Auth. There are workarounds that allow you to use integrated auth without a domain controller if you have the same password on same-name accounts on both boxes too. Environment variables are an option too. Otherwise you can use configSource to put the connection strings in a separate file which doesn't get dropped as a part of the deployment. Or you can use EncryptedData as described here: (https://msdn.microsoft.com/en-us/library/dtkwfdky.aspx)[https://msdn.microsoft.com/en-us/library/dtkwfdky.aspx] There are a bunch of other options too, but these should probably* cover most of the easier options for non-core apps.
Makes sense. Thanks.
Super high level .NET Standard is a interface, that expands over time so each version is wider than the one before. .NET Core and .NET Framework are two classes that implement .NET Standard. In the case of .NET Core, both 1.0 and 1.1 implemented .NET Standard 1.6. .NET Core 2.0 implements .NET Standard 2.0. .NET Framework also implements .NET Standard but it adds a lot more on the side. .NET Framework 4.6.1 used to implement 1.6, but now also implements 2.0 with very little left out.
Thanks. So do I have this right? Net Standard is NOT a library that I install or deploy? I never download Net Standard. When I download Net Core, I can just know that it implements Net Standard? Did I butcher that?
"There" refers to "grammar mistakes," therefore, the verb must be plural. It must agree with the subject, which is referring to a plural noun. Examples: Singular - There is a grammatical error in the article. Plural - There are many grammatical errors in the article. To check, switch word orders. *A lot of grammatical errors is there.* - clear lack of subject-verb agreement *A lot of grammatical errors are there.* - no problem
Does anyone know how to use libraries in .net core that haven't been published to nuget? For example how can I use System.Data.Odbc? It is in https://github.com/dotnet/corefx. I understand it might still be unstable and that is why official packages haven't been published. Do I need to build corefx locally and then copy the built System.Data.Odbc.dll? Am I on the right track?
ha~ That's just a bunch of hand waving and a list of different technologies. The fact that neither you nor, it seems, anyone else, can articulate immediately the meaningful benefit of websockets over the alternatives, or show an example of an overwhelmingly effective solution built using them... ...just shows to me that they really aren't that compelling as a technology. If they were *really* that great, there wouldn't be a line like 'uses long polling as fallback when not supported' on every single websocket library; that there is makes it abundantly clear that all you're really getting is a convenience API; not actually anything meaningful in terms of communication layer. At least WebRTC has an immediately obvious use case; peer to peer networking. 
I think the idea is that you can build to a standard version and then have it work in both core and full versions as long as those versions support the same standsrd level.
nope, you're spot on. When you create a standard class lib, the project looks like this: &lt;Project Sdk="Microsoft.NET.Sdk"&gt; &lt;PropertyGroup&gt; &lt;TargetFramework&gt;netstandard2&lt;/TargetFramework&gt; &lt;---- This &lt;/PropertyGroup&gt; &lt;/Project&gt; When you call `dotnet build` it'll download the target framework files and build with it. You can do that if you've installed the standard .Net framework 4.6.1 on windows, or Core 2 on any platform it supports. ...but yes, basically, each runtime you download (eg. core 2.0) will have a list of standards it supports.
ah man, I'm going to have to go through so many csproj files now and remove the `-preview2-final` from the dep version. So good. FWIW anyone who's thinking of using Rider, take care; the .Net Core 2.0 support in it is still realllly shakey (can't build, cant run tests, etc).
Any support for n-tier architectures yet?
That's correct. You can see compatibility here: https://docs.microsoft.com/en-us/dotnet/standard/net-standard Think of .NET Standard like a Venn diagram. If each implementation of .NET (.NET Framework, .NET Core, Xamarin, Mono, etc etc) is a circle in this Venn diagram, the intersection of them all is .NET Standard. Thus .NET Core 2.0 includes all of .NET Standard, so if you download .NET Core 2.0 you *have* .NET standard 2.0. If you have .NET Framework 4.6.1 (with 2.0 tooling), you *have* .NET standard 2.0. The purpose of it is not to install or deploy .NET Standard. The idea is that it's an interface you can target. If you're writing programs, you don't target .NET Standard because it's not a platform - you install .NET Framework or .NET Core or something else and target that. The advantage comes when you're writing *libraries*. By targeting .NET Standard with your library, *any* framework that contains the version of .NET Standard that you targeted can use your library. Before .NET Standard, a lot of libraries in NuGet had separate packages for .NET Framework and .NET Core. Throw Mono and Xamarin in the mix and it gets even messier. Now, a library developer should *always* target .NET Standard unless they *need* an API not available in .NET Standard, in which case their library is still limited to a specific platform. .NET Standard 2.0 added a *lot* of APIs though, so this should hopefully be fairly limited to things like using Windows-only APIs in .NET Framework.
I'm no specialist on WebSocket protocol, but according to [this](https://en.wikipedia.org/wiki/WebSocket), it is full-duplex, not unlike what you get with webrtc, but probably much more lightweight as I only see it being used for simple messaging, and not much streaming, which makes sense given it runs over tcp and not udp. SignalR is supposed to be easy to use for web applications. Further, webrtc is not a protocol, it's a bundle of things. It relies on Real-time Transport and I don't know if it's been standardized by the W3C, so I would guess that while you can make good peer-to-peer software with it (just as with tcp), there aren't many guarantees w.r.t. cross platform or cross browser support, yet. But why poll when you have server-to-client messaging? The point is to avoid that. 
**WebSocket** WebSocket is a computer communications protocol, providing full-duplex communication channels over a single TCP connection. The WebSocket protocol was standardized by the IETF as RFC 6455 in 2011, and the WebSocket API in Web IDL is being standardized by the W3C. WebSocket is designed to be implemented in web browsers and web servers, but it can be used by any client or server application. The WebSocket Protocol is an independent TCP-based protocol. Its only relationship to HTTP is that its handshake is interpreted by HTTP servers as an Upgrade request. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/dotnet/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.24
Copy the packages to a local directory and add that directory to nuget.config as a package source. 
I know what websockets are. We use them in production; they've never been particularly better or worse than standard polling, and the surrounding code is simply more complex. That's my experience with it. My *question* was: **why** would you use them. &gt; The point is to avoid that. Why? You're just arbitrarily asserting that having a full duplex streaming connection is better... because reasons. It might be; that hasn't been my experience using it, but if someone can actually point out why that's better, I'm happy to listen to it. My experience? - Its complicated to maintain. - There's no tangible benefit over polling. We don't run a chat server. We don't stream binary data. We don't have realtime interactivity on our site. Most people don't. So why, would most people use a websocket? It's got a very specific limited set of use cases, that's my point. Having a full duplex stream isn't actually particularly any better than just polling if all you're doing is serializing objects as json and posting them back and forth. eh, I don't want to argue about it. I was just curious if anyone had **actually used them** and found them to be particularly useful for anything.
I guess I really don't understand the question. In scenarios like interactive gaming, communication, or streaming updates like with ticker info, etc, it can definitely provide a tangible benefit over polling. Especially with data-intensive web apps, handling poll requests from a small number of clients to a database to run a query to check for changes can be taxing, never mind for thousands of clients. In the right scenario, all of that time wasted on processing and I/O bogging down a server can be circumvented if the server can push messages out over tcp.
&gt; handling poll requests from a small number of clients to a database to run a query to check for changes can be taxing, never mind for thousands of clients. Citation need. What are you even talking about? Perfectly normal webservers can handle millions of concurrent users. The load placed on a server by a single stateful persistent connection is significantly higher, uses more bandwidth (typically) and scales worse than stateless queries. Never mind trying to sync server side state over multiple servers, or talking about heavy load and interrupted connections. It's not even in question. That's the most ridiculous claim I've heard today. Read https://docs.microsoft.com/en-us/aspnet/signalr/overview/performance/signalr-performance Websockets perform well for high performance and high throughput, not server side load. ah... nevermind. Forget it. If you wanna use websockets, go for it. It's all good.
Now you're just being ridiculous. I'm not even talking about a web server, I'm talking about data and databases. A web server is not a database, and web servers can be scaled horizontally more easily than data that needs to be shared and accessed in an ordered, predictable way. Further, if web servers can be scaled horizontally, so can the overall carrying capacity of a distributed system. 
Are you seriously suggesting that the way that a websocket service handles data and database interaction as it scales across multiple servers is so significantly different to the way a REST API does, that you have a meaningful difference in the way it scales? Or perhaps even that websocket services magically *dont need* a database? Have you ever actually tried this? Managed to keep the serve side in-memory state synced across multiple web nodes? ...because if you have, I'd love to know how you managed it, because we certainly haven't. /me shakes head. ...otherwise I think our discussion here is done sir.
Good thing my app will be ready for Azure in like 2 years!! /s
I'm not suggesting anything. I'm saying polling is the worst of the options for retrieving updates from a database shared by many clients, when the server can push updates and the messages routed as necessary via a framework like SignalR.
You are absolutely correct. Continuing with the interface/class analogy, when you create a library in "standard", you are programming against an interface without caring about which implementation you will actually get. Quite like accepting an `IRepository` interface in your controller. Who knows if you get an `XmlRepository` or a `SqlRepository`. .NET Standard 2.0 is like Microsoft went on .NET Framework 4.6.1 and did "right click, extract interface" and then applied the new interface to .NET Core :)
The dotnet command line tool has a number of options for scaffolding an app and Microsoft have made a couple of yeoman generators too. Makes standing something up quick and relatively painless. VS Code with the C# extension works well too.
Almost all of the code I've written about for my blog on .NET Core (I'll link to it, if folks are interested) has been writing in Visual Studio Code or JetBrains' Rider on Ubuntu 16.04 I'd definitely recommend either of those (VS Code or Rider) on Ubuntu, which is Debian based. Although, the current build of Rider (2017.1) doesn't have support for .NET Core 2.0 yet. 
Yes, we can use the Redis session state provider. It works in Azure too. I might also go with that one for upgrading existing solutions. However, as it is just another session state provider, the HTTP requests would still be served sequentially since the provider implements locks (unless in ReadOnly mode). This solution locks only when writing data. Also you can better utilize the many data manipulation and filtering features of Redis without session state. For instance advanced querying of data.
Ubuntu is the most Windows-friendly distribution in my opinion. Install VScode and the .NETCore 2.0 SDK (you'll want the .deb based packages) and away you go.
Thanks. Is there folder structure equivalents guide for Linux, you know like AppData Local, Roaming, Program Files, etc? And registry equivalent?
Yet, it normally takes them a couple of days. And a few weeks for support on VSTS.
You could consider using environment variables. You can set it manually in VSTS and reference it from you webconfig. So basically you would create an environment variable in VSTS called CONN_STR and assign its value. Then in your webconfig you would use the env var name as the value. &lt;add key="ConnStr" value="CONN_STR"/&gt; Getting it back is as easy as: var connStr = Environment.GetEnvironmentVariable (ConfigurationManager.AppSetting("ConnStr"));
There is, if you search for the "Linux FHS" you'll find lots of detail. But roughly: %APPDATA% -&gt; /var (Temp files go in /tmp) %PROGRAM FILES% -&gt; /usr/{bin, lib, share} Registry configuration -&gt; /etc 
X-Post referenced from [/r/programming](http://np.reddit.com/r/programming) by /u/ionoy [LiveXAML supports Visual Studio for Mac now](http://np.reddit.com/r/programming/comments/6tswi7/livexaml_supports_visual_studio_for_mac_now/) ***** ^^I ^^am ^^a ^^bot. ^^I ^^delete ^^my ^^negative ^^comments. ^^[Contact](https://www.reddit.com/message/compose/?to=OriginalPostSearcher) ^^| ^^[Code](https://github.com/papernotes/Reddit-OriginalPostSearcher) ^^| ^^[FAQ](https://github.com/papernotes/Reddit-OriginalPostSearcher#faq)
I had don't net core 1.1 docs up before I went to lunch only to find that 2.0 was released when I came back.
EF Core 2.0 and Identity 2.0 are so different from 1.1.2. Can anyone do a tutorial on migrating existing 1.1.2 to 2.0? The changes break a lot of things for my project. 
You want the [Migrating 1.x Authentication and Identity to ASP.NET Core 2.0](https://docs.microsoft.com/en-us/aspnet/core/migration/1x-to-2x/identity-2x)
We used domain accounts for the identity, not a built-in account. I remember using encrypted sections a couple times.
Do you know if Integrated Auth is supported on Core and/or on non-Windows platforms?
Yeah, I understand where you are coming from. I think the tooling has come quite a ways that helps cut down on this (read as VM templating and CI). Ideally you are running some service discovery system or at the very least some CI pipeline step that injects the correct variables into your web.config/environment variables.
What does EF have to do with your architecture?
&gt;Perfectly normal webservers can handle millions of concurrent users.The load placed on a server by a single stateful persistent connection is significantly higher, uses more bandwidth (typically) and scales worse than stateless queries. Yeah, if your web server is dishing out plain text it can handle a million concurrent users. But if you are making even a single db call have fun with those 1s+ response times. Web sockets aren't constantly pinging the client and the client isnt pinging the server, so i'm not sure where you are getting that bandwidth statement (needs citation). In fact with long polling, every single client polling you once a second with unnecessary http headers is going to chew through bandwidth like you have never seen and it will place unnecessary load on the server, since every poll request the server has to check if it has something to send (something that shouldn't really need a claim, fairly obvious stuff). As opposed to web sockets where no client pings, and the server just sends once it has something. Frees up loads of time. Hey, if you still dont believe it and you think almost every major company that has used long polling in the past that has now switched to web sockets are stupid. Why not run your own benchmarks? It is super to easy and will give you hard to argue with results. Can't wait to see them! Real talk though. I have seen plenty of people like you. You are not going to change your mind even if you do make a benchmark and web sockets turn out better. I guess this is my last comment here. I find arguing with people who want to be stuck in the past is a useless thing to do. /me shakes head.
Say I am someone who has worked in Ruby for many years, but enjoys Java and has fallen for TypeScript. Say I want to move away from Ruby because the lack of type safety and speed really irks me. Should .NET Core 2.0 be a contender for my next REST API? Why or why not? If so, are there any particularly good resources for getting up and running?
This is a amzing 8d sound which you will feel like it surrounds you here and there .You will be shocked after listing this song i bet you if you like it subscribe to our channel and like the video. "HEADPHONES ARE MUST"
One possibility is to use the Google Spellcheck API https://weblogs.asp.net/pwelter34/google-toolbar-spell-check-api
Haven't tackled the first but the second and third are really trivial. Check out https://docs.microsoft.com/en-us/aspnet/core/fundamentals/configuration and https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters#exception-filters I like IAsyncExceptionFilter for global filters.
code behind for view&gt; https://github.com/chebastian/Autonoe/blob/design/HexView/View/CloseableContainer.xaml.cs 
Razor has helpers. Or am I missing something?
Razor had helpers, unless I'm missing them being re-added https://github.com/aspnet/Mvc/issues/4127 If you follow links from there you can find more information 
Righto, you've clearly never actually used websockets in production, so I guess we don't have much to discuss. Fwiw, I recommend you have a good read of https://samsaffron.com/archive/2015/12/29/websockets-caution-required ...and possibly the related comprehensive discussion: https://news.ycombinator.com/item?id=10811838
Yes that will work too but requires a deeper understanding of git/ssh. 
New* Nugget*
Oh boy, welcome to Windows dependency hell. As I understand it, you only need the latest SDK (2.0) unless you have project.json projects or use VS2015. You need the runtime for any frameworks you develop or run on. So I would guess you need: * Microsoft .NET Core 1.1.2 - Runtime (x64) * Microsoft .NET Core 1.1.2 - Runtime (x84) * Microsoft .NET Core Runtime - 2.0.0 (x64) * Microsoft .NET Core Runtime - 2.0.0 (x86) * Microsoft .NET Core SDK - 2.0.0 (x64) * Microsoft .NET Core SDK - 2.0.0 (x86) Knowing how windows works, you may need to reinstall some of those, and uninstalling old versions of Windows software has a habit of nuking dependencies (looking at you Visual Studio).
HAHA I know the feeling. So does it mean that I can safely uninstall * .NET Core SDK 1.1.0 (x64) * .NET Core SDK 1.0.4 (x64) * .NET Core SDK 1.0.4 (x86) and my projects targeting framework 1.1 would still work? All my projects use .csproj, it should be fine right? At least in theory..
AFAIK you can keep all of them. The idea is that they are isolated and self contained so you can deploy apps side by side that target different versions. 
In theory, yes.
I recently shipped an app that was a migration from MVC5 to Core 2.0. I'm currently writing a blog post on the experience but stalled as there's so much there and I'm not sure what to keep! I'm also working on a sample app which I'll post on Github (actually quite a lot's done so I guess I should get it up asap even if it's not 'complete'). In short, it wasn't a trivial process. I had a moderately sized app and hit quite a few pain points - I shudder to think what sort of project a migration would be for a large enterprise app (and suspect most won't bother, a perfectly reasonable decision). To answer your points ... 1. Forms Auth. This was an issue as the API kept changing (I started working on 1.1). There was even a breaking change between Preview 2 and the RTM. If you go to the documentation you'll find it's out of date - you need to read the user comments as you'll find posted up to date code samples. 2. The ConfigurationBuilder works really well with Json allowing you to bind settings to a concrete class. See returntoheavendenied's link below. 3. Again see returntoheavendenied's link. Logging is also pretty simple and well integrated. 
For a very similar approach to REST clients, check out RestEase
Sadly, [still no .NET Native UWP support.](https://np.reddit.com/r/fsharp/comments/6tdrwq/after_so_many_years_still_no_net_native_nor/)
I really like it :)
odd this is getting press again all the sudden. I remember trying it out originally like 3 or 4 years ago. Nice library.
First time I've heard of it. Looks useful.
Yeah, I went with RestEase because of better query parameter support, but in the end, it doesn't matter that much which one you choose. I love the approach atleast and I could throw a lot of HTTP code away using this.
Oracle has announced support for .NET core later this year with EF core following next year. They had some dependencies on system.data which wasn't completed until the .NET core 2.0 release a couple days ago. I'm not sure there's any supported Oracle connectivity until then, but it would have to be a third party driver if it exists.
Does this mean that we can expect the System.Data.OracleClient api to work with .NET Core later this year? Assuming of course that everything goes according to plan. 
There is currently no official support yet. [Oracle has mentioned they are working on it](http://www.oracle.com/technetwork/topics/dotnet/tech-info/odpnet-dotnet-core-sod-3628981.pdf). The [Oracle .Net team on twitter](https://twitter.com/OracleDOTNET/status/841310384850067457) is where I got my info. From Oracle's PDF - ORACLE STATEMENT OF DIRECTION | MARCH 2017 "Oracle plans to certify ODP.NET, Managed Driver on Microsoft .NET Core around the end of calendar year 2017." I was able to do a couple of very simple calls to the OracleDb in net core with this open source https://github.com/LinqDan/oracleclientcore project. "Select * from x" but was not able to get the bind variables working. 
The managed driver is coming across only, not the unmanaged and system.data.oracleclient has been deprecated for years so I wouldn't expect to see that specific API, but I haven't seen anything more than the announcement back in March so I can't say for sure. You will be able to connect to Oracle databases, but I'd assume it'd be through the Oracle.manageddataaccess namespace, not system.data.oracleclient which was a Microsoft provider. 
The SDKs share a `dotnet` command and you can specify which version new projects use when you create them. 
Ah, yes. Forgot about the deprecation. As long as it's something that can talk to the database and can fill a DataTable I'm happy. 
Thanks. We would preferably want something that works now. So the official client from Oracle seems out of the question. I wonder if it's safe to bet mission critical stuff to that LinqDan project. We don't need much functionality from the Oracle client but the little we use is pretty important that it's correct. I guess nobody else can answer that for me though. :-) 
Retrofit is the go to REST library for Android apps, nice to see something similar for .Net.
Yeah, could see it being really popular for Xamarin.
 case ICard card when card.IsValidCard == false: When I moved from VB to C# this is something that I really missed.
REST police time! In 'real' REST, you can't just go calling endpoints freely, you need to navigate to them through other REST calls (see 'hypermedia'). This is not stating that this would be any worse that 'real' REST though
Is there an easy way to create a nuget package in VS 2017?
 == false Why would anyone compare with a boolean literal instead of simple negation as in `!card.IsValidCard`?
Believe that's the plan.
F# is leaking
Are you sure `IsValidCard` is a `bool` and not a `Nullable&lt;bool&gt;`? 
Yes, if you read through the entire article. The interface defines it as a normal `bool` not `bool?` or `Nullable&lt;bool&gt;`.
Not worth my time. The compiler is going to erase it anyways, so I wouldn't bother flagging it in a code review.
I recently fumbled my way through IdentityServer3 for a .net 4.5 app for a non-profit in the evenings. The code is up on github if you want PM me, I'll point you there. Although their example repos are numerous and useful, it took longer than I would have liked to get off the ground. If you're trying to learn it, I suggest following an example that has a separate process for the API and the Identity server, even if you don't have the thru-put. Keeping them in separate processes will make clear the conceptual boundary between the two. I started off jamming WebApi and Identity into an existing MVC app. I tore my hair out for a couple of weeks worth of nights and weekends before scrapping a branch and isolating everything. Then you'll start to see the relationships between scopes, clients, tokens, claims, etc. It's also more obvious in fiddler or charles or whatever you're profiling traffic with. I suggest getting used to Postman's auth2 client early. That sped things up a little, afaik, you can test client auth but not the token auth from the identity server. i may be wrong about that. I was using Angular and slapped a demo login page on the front just to get the token working. I used the oidc-client with typescript, from the identity server guys. It works well so far.
http://bitoftech.net/2015/01/21/asp-net-identity-2-with-asp-net-web-api-2-accounts-management/ is a pretty good resource that I used as a reference recently. While it's from 2015, not much has changed since then. 
readability. when scanning through code a small "!" can easily be missed but "== false" stands out clear and there's no doubt what the logic test is.
Thank you, definitely helpful advice. I was already planning on making the identityserver it's own project, on its own server, with it's own API. And then any other projects would include a authorize claims method. Any guides you would recommend for building out a minimal identity server that can login (verify credentials) and issue a user token with claims for getting started?
The one I'm focusing on getting better at using right now is `ctrl+T`, "Navigate To", in order to get more efficient at navigating around in the code base
You can miss the second = just as easily, which is the classic assignment instead of comparison problem, mostly avoidable by not using boolean literals in expressions.
As far as I know you cannot create WCF services with .net core. You might be able to consume SOAP services though. You should look up WebAPI. That is the recommended way to create REST services using .net core.
Great. Thanks!
For easier navigation, use [MiddleClickDefinition](https://marketplace.visualstudio.com/items?itemName=norachuga.MiddleClickDefinition)
Thats nifty!
There is no "official" way to do this in .net core, and this is a feature that a lot of dev's consider crucial. I've being doing some digging, and found this example. https://blogs.msdn.microsoft.com/dotnet/2016/09/19/custom-asp-net-core-middleware-example/ Basically, this article demonstrates a crude implementation of soap service as a middleware. Some kind guys created a repo based on this article https://github.com/DigDes/SoapCore I'm sure there are a lot of limitations in this implementation (WS-Security,tcp-bindings, sessions, etc.). Maybe it will work in your scenario. Please, consider trying it, and keep us posted.
This is indeed crucial functionality. Thanks a lot for the details. 
OMG... Doing Soap services in any way that does not include WCF is like jumping on a DeLorean and go back to 2002... Not a personal comment! Just that I couldn't see myself move away from WCF for anything less than WCF Core :P Hopefully we get Service Fabric in .NET Core...
Please add your voice to [Server side WCF](https://github.com/dotnet/wcf/issues/1200) on the WCF github repo to make Microsoft aware that WCF services on .net core is required.
I've been passively monitoring .NET Core buzz and I've seen a lot of talk about how fast it is but also that it's not ready for production. Examples being tooling, EF, and a few other libraries missing. Is Core 2.0 the ready for production release? It seems like the answer is yes if it implements the same standard that 4.6.1 does.
While .Net Core 2 and .Net 4.6.1 implement the same standard, they are not functionally equivalent. .Net 4.6.1 has a superset of functionality. Things like AppDomains and all of the framework UI are not in the standard and thus not in .net core. This is an important distinction that some people don't always grasp when learning about the standard. 
I just went through all of this two weeks ago ... ended up using Okta, but it was still hard to implement. Security is harder than I ever thought it would be ... the Bit of Technology Blogs definitely helped me a lot. Wish there was better .NET documentation on how to implement OAuth2/ OpenID Connect. If anyone knows of any good resources please let me know because I want to better understand these security frameworks in a .NET context.
I'm not using core. Its complicated. The Core core is production ready and I think there are a few versions with LTS (long term support). There are lots of libraries that are different and/or missing functionality compared to the full .Net version. This might or might not make a difference in it being production ready for you. 
What are you using WCF for? If it is just simple inter process communication, then you could roll your own using named pipes. I wrote a [blog post](https://johnkoerner.com/csharp/IPC-in-net-core-using-protobuf/) about this when .netcore was still in preview. There are links at the bottom to some other projects that are doing similar things. The code is available on [GitHub](https://github.com/johnkoerner/SimpleIPCDotNetCore) if you want to play with it( note: it hasn't been updated for the released .net core 2.0)
So, there are two main schools of thought on this, from what I've seen. I'll give my synopsis of both. Pro-3rd party There are very few programming problems that haven't been solved already. If you have a pattern that's already been implemented, and implemented well, by someone else's library, why not use that, instead of re-inventing the wheel? This will save you time, and free you up to code the custom code that you need for your project. It also makes it easier to adhere to certain standards of coding (separation of concerns, loose coupling). Pro-custom tooling Using someone else's library may be convenient in the short-term, but what happens when that library is no longer supported? Filling your project with dependencies on other libraries could create huge problems for you down the road, and may not be worth the hassle. Creating custom modules yourself, that only give you the functionality that you need, will make your project leaner, and less prone to break with updates. I think as with most questions like this, a combination of styles is probably best. If your boss wants you to use 3rd-party libraries, it's probably an expediency thing for them.
Named pipes are indeed nice. This will be for exposing a service to other systems though so something else is needed. We are currently considering if we need WCF or if WebAPI is enough. 
I think the library situation is going to need another 6-12 months to catch up before Core is truly "ready" for most people. The package compatibility option will help with this, but it isn't perfect. For example, I was playing with a Core 2.0 project this morning and found that the latest stable 4.x version of NLog installs using the .NET framework version, but reports that it "may not be fully compatible." I'm not entirely sure what the implications of that warning are, but it doesn't sound like something I want to test in production. NLog 5 however has .NET Standard support and is in beta, so I expect it will be ready within a few months. 
In the Identityserver3 examples, I used the JSAuthentication example combined with the AspNetIdentity example. https://identityserver.github.io/Documentation/docsv2/overview/jsGettingStarted.html https://github.com/IdentityServer/IdentityServer3.Samples/tree/master/source/AspNetIdentity The samples for Idsvr3 have a better readme than Idsvr4, apparently: https://github.com/IdentityServer/IdentityServer3.Samples A note on the bitoftech blog posts and Microsoft templates in Visual Studio: I started with the MS templates first, sort of got them working with an FOSS OIDC client, then gave up on those because they weren't quite lining up. If you're going with IdentityServer, I found success sticking with their examples. If you mix with bitoftech and MS examples, you're going to have a harder time. My code is here: https://github.com/savagelearning/machete/tree/online_orders There is a stub JsApplication in there (I stole from somewhere on github) for testing the tokens and basic authN. 
I personally am a fan of 3rd party tools if they are appropriate. It's better to have a team of people working on some small aspect of your codebase than to have to recreate the wheel. The important part here is "appropriate". Maybe the best thing you can do is research these tools and explain to your boss why they don't work for your needs. This... sometimes results in holy wars, but if you can't make a case for not using something beyond, "I don't like it" maybe you should use it.
Hi, loved the article. Any idea on the internal retargetting magic that allows us to use vanilla 4.5 .net portable libraries with .net standard 2.0 now? Will really love a good read on the internal works. 
Third party tools: Rapid development, less customization Your own tools: Slow development, more customization &amp;nbsp; I started out learning .NET by using Telerik as a crutch. It helped me get into development really quick, but it became glaringly apparent over time just how big of a crutch they were. Being able to solve something in your own code is both frustrating and relieving at the same time.
In the table "THE .NET STANDARD VERSION TABLE" (about half way down the page) what do the rows for "Windows", "Windows Phone" and "Windows Phone Silverlight" mean? Especially the "Windows" row. I'm pretty sure .net 4.6.1 runs on a lot of Windows OS versions (I'm running it now on Win7).
If someone else already did the work, use their sweat, not yours. 
Not the poster, but the table looked familiar. The one on DavidPine.net is actually a link to https://github.com/dotnet/standard/blob/master/docs/versions.md#net-standard-versions on GitHub by TerraJobST, which looks a lot like https://blogs.msdn.microsoft.com/dotnet/2016/09/26/introducing-net-standard/ from Immo Landwerth. Now it's possible all of those people could be the same person, but on the blogs.msdn.microsoft.com link, they have the sentence: &gt;You can use this table to understand what the highest version of .NET Standard is that you can target, based on which .NET platforms you intend to run on. For instance, if you want to run on .NET Framework 4.5 and .NET Core 1.0, you can at most target .NET Standard 1.1.
I suppose that's true if you're prone to writing that issue, but it has nothing to do with the readability.
AppDomain is in the standard. Indeed presentation framework is not part of net standard. For non ui code... hopefully most of your domain model, this is irrelevant. WPF can link net standard dlls without issue. EDIT: See 2.0 Diff with 1.6 https://raw.githubusercontent.com/dotnet/standard/master/docs/versions/netstandard2.0_diff.md
&gt; Immo Landwerth = TerraJobST He is the .NET program manager at microsoft. It's the official implementation table. The columns are the net standard versions and cells for each row show which version of the framework in the row implements that standard.
If the negation operator isn't readable in general, why use it at all, right? I think it's one case where syntax highlighting actually comes in handy to render the negation operator in a different color or weight than the variable. Which seems like a cleaner solution to deal with the readability issue.
There was actually an IdentityServer4 step-by-step pluralsight course released within the past week or so. It's probably worth spending $30 to learn. You can sign up for a free-trial, but I think you are restricted to 2 hours of viewing or something like that. Here's the course - https://www.pluralsight.com/courses/asp-dotnet-core-oauth2-openid-connect-securing
You can literally right click pack and publish nuget packages
That may be the version of .NET Framework that comes installed on Windows 8 and 8.1.
I don't think all of AppDomain is in .net standard, because not all of it is surported in .NET Core. unless that changed?
Good luck with that. On-premise WCF is legacy technology at this point.
Thank you so much, this is extremely helpful. The trail allows for, 3 hours and 20 minutes or 10 days, whichever comes first. Definitely going to check out this talk tonight.... 5 hours and 44 minutes of exciting OAuth talks! Recommend doing the whole thing or are you able to skip to sections of use? Also, noticed this course which is slightly older, but actually has sections completely dedicated to implementation of identity server. https://app.pluralsight.com/library/courses/asp-dot-net-core-security-understanding/table-of-contents Have you looked into this course at all?
Right, it's the old 8/8.1 Windows Store platform.
And often outside of the .NET world, the answer is "use an OSS tool and hack in the customization you need". .NET Core will only bring more goodness to the .NET ecosystem imo.
Yea, I took that one and it's pretty good. Short and to the point. Honestly, all the following are pretty good - https://app.pluralsight.com/library/search?q=identityserver Everything published before 2017, is obviously IdentityServer3 or other forms of auth, but I think they're all helpful to wrap your head around the whole thing. I get a pluralsight subscription through my work, but to be honest, if I didn't, I'd easily pay the $30/m in a heartbeat. There's just so much invaluable content on there if you are a full stack .net developer. I'm working my way through Angular 2+ as we speak!
As a pro 3rd party point: many 3rd party libraries are open source and available freely for use s long as you reference the original license or creator. You can always keep a current pull of the library in your codebase to ensure if support ends you can continue to develop or support your application. As an anri-3rd party point: there's always more code than necessary in any librarybecause they were not originally designed for your needs. YAGNI comes into play very quickly when using 3rd party code. Personally I like to avoid 3rd party for all the reasons you've listed, but I have nothing against learning the ways and means to eventually implement my own solution.
&gt; Does .NET have a native way to run the code on GPU? Not that I know of. The main technologies are going to be DirectX compute shaders, OpenGL compute shaders, OpenCL and CUDA (Nvidia only, but some people have developed translaters to openCL I think). All of them will require a wrapper around the C APIs and you will need to write the code that runs on the GPU in a separate language. &gt; If not - what would be my best bet if I want my code to run on both AMD and Nvidia GPUs? OpenCL is probably the most widely supported and can even run on the CPU.
Check out [Alea](http://www.aleagpu.com/release/3_0_3/doc/) and [CUDAfy.NET](https://cudafy.codeplex.com/) to start. What do you mean when you say 'native'?
Day 2.... you're not kidding when you said it wasn't a trivial process. Ugly in fact, nearly everything is different about mvc. Actually sort of disappointed with the mvc changes to be honest, tangibly worse in several ways already encountered, just different in most others, and DI seems forced onto you. I don't think I will migrate these sites after all.
Native as in build-in and not require the usage of third-party libraries. Alea is nice, but restricted to Nvidia GPUs and everything above single non-industrial GPU is paid. CUDAfy.NET seems pretty interesting, but again - commercial license.
Can you point me to some info on the whole 'wrapper' thing? I am completely clueless about using one programming language inside another.
Maybe check out Mono Game? Its for games, so maybe its too high-level for your purposes?
Mono requires Alea to run on gpu, doesn't it?
The thing with colors and styles though is they only apply to *your* workstation and IDE, not the coder next to you. I get you're trying to justify the use of ! from a readability point, but I don't think you'll win that argument. It's not necessarily a terrible thing, people don't write pristine ultra readable code by nature. This is just one of those things that if your aim is readability, this is a rule to follow to increase it. If you don't it's not a big deal. 
Like a lot of people said, third party tools are great, but I hate third party platforms mostly. And a lot of people have a tendency to turn a tool into a platform. Like a wise man once said, a majority of complexity in development comes when you try to make one thing do two things. My measuring stick is usually around the question on if we want to get rid of this tool and switch to something else, how much of a pain would that be? This can be alleviated with good architecture, but a lot of times not. For instance I worked for at a place that relied heavily on video streaming. We used a third party codec, a third party player, and we just managed the site that wrapped around it. We realized we were no longer in the development business, but the business of maintaining libraries. and when when the player went out of business, so did our product for 7 months while we tried to replicate. Would it have made more sense to make our own from the start? Who knows, but I think it is a key piece of your product, ownership needs to be in house. I tend to make more exceptions for open source software since you can bring that in house as you see fit. My big exception to this rule is around user management. Auth0, and Stormpath before them, used to be my saving grace on many a timeline. Most people will tell you that this thing, done in so many applications, will still be 30-50% of your work to dev, test, and maintain if you do it all custom.
Google "interop" "extern c" and "dllimport" and just start reading away.
I'm not sure if it is what you mean but you can look at this ([UWP](https://docs.microsoft.com/en-us/windows/uwp/get-started/whats-a-uwp)) library: [win2d](https://github.com/Microsoft/Win2D). &gt;Win2D is an easy-to-use Windows Runtime API for immediate mode 2D graphics rendering with GPU acceleration. It is available to C#, C++ and VB developers writing Windows apps for Windows 8.1, Windows Phone 8.1 and Windows 10. It utilizes the power of Direct2D, and integrates seamlessly with XAML and CoreWindow. 
Rendering is not an issuse right now - worst case I can see the result in Blender. There's just a lot of repetitive computations I'll have to do with matrix operations and things like that. GPUs work wonders on such things. In the worst-worst case I could write the whole program using MPI, but that would be kinda boring and useless.
In this case, I'd derped with my databinding of an asp gridview during paging so it was doing weird things. Boss told me to use a DevExpress GridView instead. Turned out all I needed to do was to remove the .Skip() from my LINQ query. I wouldn't say thats recreating the wheel so I figured that to go to the length of implementing the DevExpress control would have been "inappropriate". I told him as much and he was like "oh cool".
C++/CLI 
There is also ILGPU.
No there's no native library available.
Funny that you don't mention 'maintenance'. The largest part a piece of software is alive is after its first version: i.e. when it is effectively in maintenance. Adding your own DIY libraries is fine of course, but they do come with the cost that you have to maintain these libraries, and if you're not doing the maintenance of the software you're writing, the people who do will have to. If you add a lot of DIY libs while there are perfectly fine 3rd party alternatives which are actively maintained and more importantly: are written by people who actually are specialists in the topic of the library (and you're not, most likely), you increase the maintenance burden of the software you're writing, likely with a lot. That is a serious cost and consequence. It might even lead to a terrible end product because you write a library for a problem which didn't work out in the end and had to be replaced by a 3rd party working solution. 
&gt; Personally I like to avoid 3rd party for all the reasons you've listed, but I have nothing against learning the ways and means to eventually implement my own solution. And who's going to maintain that implementation? You? Or the person coming after you who you hand over the software to? Or you didn't really think about it yet? If you can 'outsource' the maintenance (fix bugs, implement new features, support for new frameworks etc.) of a library to a 3rd party (OSS project, company) you have more time to spend on your own code. IMHO it can be interesting to implement something yourself, but doing so isn't free: adding stuff to your own code means you have to take care of that too. 
I don't really see the point of your question: you made a mistake and indirectly blame the 3rd party. But what if there's no 3rd party? You then had to implement the grid yourself. Well, good luck with that ;) 
&gt; My measuring stick is usually around the question on if we want to get rid of this tool and switch to something else, how much of a pain would that be? This can be alleviated with good architecture, but a lot of times not. ok, now apply that logic to the alternative: your own implementation. 10 to 1 it will be as much painful, if not more, than a 3rd party (as a 3rd party is an external codebase and therefore that code is isolated from your code by definition). Sure there are a tremendous amount of stories known where people used a 3rd party tool/lib and it sucked and they had a terrible time. We've all been there. The big mistake to make here is to ignore the cost of doing things yourself. We developers think it's free and easy as we're all developers right? We can write code, so we can implement whatever 3rd party lib there is, we just need a bit of time and it's done. And most problems just take an afternoon of work, if not less, anyway, right? Man, the amount of people who thought writing even a micro ORM in an afternoon was 'easy', or better: maintain the code of a self-written tiny microORM in a project would be easy because it's 'so straight forward'. :)
When you think about it, it makes sense. A dictionary has multiple buckets, it has to aggregate the counts. Can only do that with locks (to be threadsafe) 
I don't think there are many general purpose programming languages that have native support for GPGPU programming. The closest thing to what you are describing would probably be using native interop and writing C++ AMP methods.
You can make use of SIMD. https://channel9.msdn.com/coding4fun/blog/SIMD--C--Parallelism-on-a-Single-Core Or if you can grok C++, C++AMP alongside C++/CLI. https://blogs.msdn.microsoft.com/nativeconcurrency/2011/12/21/how-to-use-c-amp-from-c-clr-app/
I am not Immo Landwerth (Terrajobst). I'm David Pine (IEvangelist). I'm confused how there was confusion on that... :)
Cool thanks. Do you know where I can find the documentation about configuring and automating it? 
Ah, yeah that puts everything in a completely different context. I was thinking of much larger problems than what you are speaking of. I don't even know how to comment of the level of stupidity of that response to your problem.
I wouldn't base my opinion about 3rd party tools on the experience you've had with large complicated packages like Telerik. Even though I saw a lot of problems in the last project I was involved in with Teleik controls (Silverlight), they really did save a lot of work.
I assume it's driven by a regular nuspec or the csproj. Besides that I don't know. Make sure to link it if you find it, I'd be interested too.
My existing app ran on Mono / Linux and the ASP.NET part hasn't received the attention it should (main focus being mobile) so I was keen to use something with better support (specifically with regard to memory management) - it's quite common to have to restart the web process when using ASP.NET on Linux which isn't ideal. So I had a specific need but the decision to migrate really depends on your use case. If you just want to upgrade so you're running the latest .NET hotness but you're still deploying on Windows I doubt it's worth the effort, especially if you have a significant code base.
&gt; ok, now apply that logic to the alternative: your own implementation. 10 to 1 it will be as much painful, if not more, than a 3rd party (as a 3rd party is an external codebase and therefore that code is isolated from your code by definition). At first, yes! But as time, needs, and life of the project goes on and it is a pivitol, non-negotiable, part of your process, I will bet you the same 10 to 1 it becomes the primary limiting factor in all future changes. Both ways are a bitch that can screw you in the end, I just like having the screwing on my own terms all other things being equal.
It could use an interlocked increment or compare and exchange instead. Though I hear that those can be even more expensive than a lock in some circumstances. 
Some ideas for supplemental learning: - SQL: Being able to go from Database to Web display of your data will make you a lot more valuable on the job market. Learn some SQL so you can create database(s)/tables &amp; start designing how you want data to move. Learn Inner/Outer/Left joins between tables to get an understanding of how you can structure your database to create tangible objects. One example we'd use in interviews was how you would design a course registration system within a database (i.e., you'd need tables for Classes, Teachers, Students, Student Schedules, etc. - you can start small &amp; keep making it more complex) - jQuery: I hardly use plain Javascript anymore, and jQuery is a lot easier to write (just my opinion). Also, once you get the hang of it, you can move on to... - AJAX: Want to move data between the server/DB &amp; the page/client without having the page reload, learn some AJAX! This will also require you to use some C# by building either API's or using WebMethods to handle the transfer of data between server/client. All these things I've already used today at my job. Good luck with the bootcamp! 
jQuery and Webmethods are pretty damn old school these days unless you're maintaining legacy ASPX stuff.
jQuery is powerful, but it also might make sense to learn ReactJs or Angular 4. Those are some really powerful frameworks that can make your life easier. 
[r/powershell /r/csharp](https://www.reddit.com/r/powershell /r/csharp/)
If the requirement of change over time is low, I would say write it in C++ (AMP) and wrap it in C++/CLI. 
I wanna second this suggestion. React, Vue or Angular will make any frontend better than using .NET with Razor. Use ASP.NET Web API or whatever they're calling it in .NET Core to model your data and provide REST end points, authentication and authorization logic, and validate and sanitize inputs, and leave all the rest of the client logic to a dedicated frontend.
Seconded, although you may be signing up for more than you're ready for. 
Interesting, this is good advice as someone just starting out with the .net stack. But wouldn't razor be better in some ways (or at least for some things) since it's server-side?
Maybe I'm completely off base here, but I think Roslyn just supports Visual Studio to be able to do things like Code Lens better faster and smarter. If you check the doc there doesn't seem to be any reason to move a working project to use Roslyn. https://github.com/dotnet/roslyn/wiki/Roslyn%20Overview 
Modular, self-contained, and portable. 
It doesn't even sound like you know what .NET core is or does and want to switch just because. It is just a framework, I mean, you are looking at having to rewrite almost everything. This is will be such a huge undertaking, I don't think you even realize it. If I were you, I would just find another job.
Does the FromSql() work without the complex where clause? e.g. doing something like "select top 1 * from dbo.supplies"?
Have you tried using Linq-to-Sql to see if you can get similar results as the raw sql?
Have you seen the price of Windows Server licenses lately? You have to pay per core with a 16-core minimum, making the entry fee over ~~14,000~~ USD. If you want a VM server, and don't want to pay for a separate OS license per VM image, then you need to Data Center version of Windows that runs close to ~~100K~~ (for 16 cores).
 So you work for a company that dictates what framework you use? And you have to sell them on the idea of how to build it? Sounds like micromanagement to me.
Wow, that's something I've never heard of. In fact, that might be even better, thanks!
In my experience most dev managers and IT managers want to know what they are paying for. That's not micromanagement, that's good business. 
I've migrated several large projects from MVC 5 to Core. It really isn't as much of a job as you are making it out to be. 
Docker and generalized lower costs for cloud hosting. Orchestration with best-in-class tools like ansible are much easier on linux. Plus, at risk of starting a flame war, linux is way easier to secure.
&gt; I think we miss out on talent because of our legacy vb.net/web forms environment. I recently left a company which had a VB.Net Webforms monolithic app, who are at present trying to re-write in MVC 5. **The technology isn't the issue, its the people.** If they've got themselves into this position you have to wonder if they understand technology/what your saying and will even listen to reason.
It's faster, it's cheaper, it's smaller, it's open source. If they don't really see the benefits of MVC though... well. You've got your work cut out for you. 
It depends. I've worked for companies that were scared of change and wouldn't adopt newer concepts like ORMs, IoC/DI, TDD, etc. It was hell, stifling, and their code base, devs, and customers are suffering because of it. It was mirco-managed hell, because the owner was from FORTRAN days who thought he was some genius that knew better than the community. I left to greener pastures, and while my superiors still care about what stacks that we use to some degree, there's a tremendous amount of freedom &amp; responsibility. The culture is so much better, the environment is progressive and not hostile; and I'm getting enjoyment out of my work again. I get to learn new techs that interest me and apply them to the jobs at hand. And everyone benefits. I get what you're saying, dev manager and IT managers want to know what they are paying for. But they also should be hiring engineers that know what they are doing, and have enough trust in them to use technologies that they see suit the application. They should be a check-and-balance to the developers decision, not the golden rule. 
I think he's talking about migrating away from web forms. Been there, done that. If you can, run away. It's a painful process. 
Wow! If your management understands what that means without you explaining any of it, I envy you!
They aren't even on MVC...
deploying to linux docker -&gt; no $$$ spent on licenses
They have a framework and application already built though. You can't blame management for not wanting to pay to unnecessarily rebuild it from the ground up without some good reasons.
Just shy of $7K for 16 cores of Windows Server 2016 Datacenter, not cheap but it'd also cost me around $3K/yr for 2 sockets of RHEL licenses. Licenses aren't necessarily money wasted, I'm looking at spending $30K/yr on OpenShift to provide an easy to manage environment to start deploying .Net Core (as well as Java and Python) apps on, money my team would easily spend in the form of labor handling deployments, server build and general maintenance that is dramatically reduced with the product. Could I use he free upstream builds? Sure, but now I get support when shit hits the fan and waiting for a response on a mailing list or IRC channel isn't sufficient.
I think it could be, but it seems like the use cases that truly really need to have a server-side rendered front end are getting fewer and farther between. Especially when you consider integrating SignalR and doing initial render and hydration of state server-side, you can almost do away with Razor completely. Although for a beginner, I'd definitely recommend learning Razor and how the .NET templating engine works. 
I've seen some stuff rebuilt from the ground up by people with no conceptions of best practices or how to develop maintainable code/architecture, it can turn out a lot worse than an old battle-hardened legacy application.
Linux support is huge, especially when paired with containers and orchestration tools like Kubernetes or OpenShift. The ability for developers on my team to point OpenShift at a git repository containing a web app and have it automatically build new releases and do rolling upgrades without having to set up anything is a huge time saver, we no longer have to ask "what server(s) do we deploy this on so we can configure CI/CD". Add in that MSSQL now has docker containers available we can run full end-to-end testing in a pristine, reproducible environment every time.
It's probably a good idea to dabble with some sample C# apps. Depending on what part of the stack you find most interesting, it will probably help you decide if you want to diversify asap or focus on getting some expertise on the front end first. Depending on what kind of dev you want to be, you can go after jobs that are full stack - i.e. you are doing front end work and back end work - or you can go after jobs that just focus on one or the other. That said, the entry level jobs are probably a bit more likely to be at smaller companies that need full stack devs.
I'm not sure where you're getting your numbers from. I'm seeing 6155 USD per core for Datacenter on the Microsoft site.
It's better if they don't understand and just buy into the buzz words.
**Datacenter and Standard edition pricing is for 16 core licenses.
Fucking Microsoft. Always making licensing more complicated than necessary. Why the fuck would they bury that in a footnote? If you look at SQL Server's page it's actually priced per core with a footnote that you need to buy them in 2 core packs.
Windows Server used to be licensed in two-socket bundles and that was the list price (since nobody deals with the expense of 4p/8p systems outside giant database servers), they just reused the same page design since 8-core dual CPUs are the most common configuration Windows environments are deployed on.
Oh God, this. If you are a .NET house, fuck the Hobsons choice of crazy Windows Server costs vs. Azure tie-in. At least leave the option of deploying on AWS/LINUX or LINUX on your own kit. Thats why. NET Core. 
As a consulting firm, they need to be able to bring in new talent and as the years go by, some clients will be coming to you asking for this. Depending on how much code you reuse, the new 2.0 standard will probably allow for transparent project targeting. As for the forms themselves being rewritten, the rewrite will be a hard sell to clients. It is more important to just start all new projects in the new platform. It will allow you start fresh, not worry about the clients complaining about excessive billing hours for you trying to make a mature webform work in core/mvc setup. 
Aren't those 3 words fundamentally describing the same thing?
Have you considered cloud foundry. Cf especially commercial version from pivotal is much less work day 2 then open shift. It may cost more in license but you'll have a fraction of operator's required to run open shift, and it supports running those older iis apps out of the box
I would go the route of maintenance. Argue that maintenance costs will be less due to he modular nature of ASP.NET Core. It crashes less. Microsoft supports it better. You can use the latest Microsoft tools to deploy. Better error handling. If you use middleware to appropriately log your errors, you'll reduce the cost of maintaining. 
We've already got Octopus handling old Windows stuff without much issue, I've been running a production OpenShift cluster (Origin 3.5) since November on CentOS Atomic and the Day 1+ maintenance has been negligible. Cloud Foundry is a real PITA to setup and admin, the Docker/k8s foundation which OpenShift lies on top of is much less headache for me to manage. Plus the built-in web console makes training new developers a piece of cake for most of what they need to use (I still end up writing templates after the fact for easy restore in the event of a DR scenario).
No. * Modular: Only pick the pieces you need, reducing bloat. * Self-contained: Doesn't require extra stuff installed on the server. * Portable: Runs on more than just Windows.
&gt;I think we miss out on talent because of our legacy vb.net/web forms environment. Yeah... Sigh... I get recruiters who say "OH WOW! You know .NET? Great we have a .NET website from 2003 that..." Sigh... Unless it pays crazy good, or its something really interesting I pass. 
Exactly. Explain it to them using luggage as an analogy.
Webforms is basically dead and won't really be getting any updates, not that your management will care too much about that. Asp.net MVC and core MVC will both give you similar benefits. Vanilla MVC won't require as much as you can tack it Into the current site. You can run core on .net 4.5 so it may be possible to use it side by side too. There's almost never a reason to rewrite from scratch, takes ages to get any value. MVC can be harder to do some things, I've never thought it was as good at composing sites with lots of widgets. I still prefer MVC over webforms though. Webforms using singular form for whole page can add a lot of co ststraints to apps, eg its hard to embed external subscribe forms. Razor is easier to read and debug due to less magic. I'd recommend trying to add your next major feature using MVC (not core) in the same site (unless you already have a oauth or similar logon). There will be hurdles to overcome that management won't like (no shared template) but a slow migration/rewrite is better than a big bang. If you do it right management shouldn't even notice - they really shouldn't be dictating technology (eg them saying no php is OK but not letting you use MVC over webforms isn't). Disclaimer: I mainly use .net for web services and have moved onto react for frontend. But I have done the whole asp &gt; asp.net 1&gt; 2 &gt; MVC &gt; webapi &gt; react journey. 
I don't believe in using core unless you plan to host outside the ms stack. It just doesn't make business sense. We are using core and angular 4 because we want to be platform independent. After nearly finishing up this project I have reaffirmed my beliefs that dot net core should only be used if your specific purpose is to not pay Microsoft so much in licenses. I enjoyed core but it comes with a lot of unnecessary headaches that slow development and hinder architectural choices. It has a very specific place in the toolbox and it should only be used as required... not just because.
Oh boy. Our place is a C#/webforms app, trying to re-write to MVC. But management (who are ex-developers themselves) are so resistant to changing their API layer they wrote originally which is a monolithic mess based on an obsolete ORM, and are of the opinion things lie design patterns, unit tests, database keys "slow development time". As long as the UI is shiny, it doesn't matter how much technical debt we accrue (technical payday loans is more accurate).
**Run**. It sounds like they are (slowly?) running out of money and just need sales. If they had confidence they would invest in the product and underlying code.
I'd say the opposite. It's not a huge company and they a have hit a nice niche thats very profitable, but are still in "new startup" mode where getting a minimal viable product out of the door is more important than clean code. Hopefully we get out of this transition phase sooner rather than later...
If the product or code is legacy and is currently being rewritten, how could they still be in start up mode? Wasn't the original product released a long time ago? Or is is this a partial re-write (but using the old API you mentioned)? Either way, if they don't invest in code there unlikely to invest in staff. I'd never say "Quit tomorrow!" as theres a ton of good reasons to stay in a job, but I'm still going to stick with **Run.**
This is true and always a point to bring up. However as you can also Docker legacy code, be sure to be stress its running .NET on Linux (using Docker) which could not be done before.
I assume this was 1.x? Have you looked at 2.x at all?
.net core &lt;2.0 should not have been used in production unless you wanted to be a beta tester. This was clear a long time ago. Judging by the current progress with .net core I'm pretty sure that it'll become just a frictionless as full .net.
Rebuilding is almost always the wrong thing to do. If a team cannot transform a working application piece by piece, then i doubt the same team will be able to write maintainable code.
[removed]
&gt; dozens of fields that interact with lots of complex business logic, needing server side processing If that's really the case, you're probably gonna have a bad time however you slice the cake... It's hard to say anything concrete without knowing what your company does, but it sounds like you'd be better off managing the field interactions in the frontend (perhaps using a framework like React, Angular, Vue, or similar), having the backend supply the total set of options and validate the submitted data. Very similar to a desktop client backed by an API, but without the deployment hassle.
For applications with complex forms, nothing beats Web Forms. If you are getting good performance, stick with it. The overhead caused by ViewState can be circumvented with careful selection of controls that need to be stateful.
Can you expand on that anymore? I'm curious and open to different tech ideas. Are you still talking about an MVC app that just use React / Angular instead of Razor? or something else entirely? Forgive me if it's an ignorant question, I really wish had a real architect at my disposal to buy beer for and pique their brain for an hour. 
Do you have any experience with scaling up web forms to handle large number of simultaneous users? 
If you rewrite in mvc you're going to have a lot of bugs. That's the nature of the beast when you convert. My recommendation would be to just scale out the web servers linearly for now. Rewrite is necessary long term, but not for short term scale out. That code sounds like a nightmare. 
You could do sql session state(will take some minor changes to code) , and then just spin up multiple web servers and put a load balance in front. As long as you don't have a data structure that grows based on the number of users to the point where it's taking most of the memory. 
I moved our tech stack off Windows a couple of years ago and feel totally vindicated. I had to skill up on Linux but now prefer it over Windows, I don't think I'd ever go back.
I recently shipped an MVC5 app to Core and hit a lot of pain points. It certainly isn't a trivial task.
Your main concern will be to make your presentation tier stateless so that it can be deployed onto many servers. When that is taken care of, you can use a load-balancer to route requests to the healthiest servers. Now, when I say "stateless", I don't mean that there cannot be web sessions; that is not necessarily a concern because a good load-balancer will be session-aware. Instead, your presentation tier must not have direct dependencies on physical resources (disks, databases, etc.) The aim is to make it possible to move your front-end from one server to another without having to reconfigure access to resources. Behind your presentation tier, you will need a service tier exposed as an API. Depending on the complexity of your requirements, this might be REST or WCF. For example, if you require transactions, WCF is probably the better choice; otherwise, you can go with REST and handcraft transactional features. The servers hosting this tier can be load-balanced similarly to your front-end, but with an internal load-balancer (unless you want to expose your API publicly, in which case a public load-balancer is needed). For a business application, it is unusual to require a lot of processing in the service tier; most operations are conversions of presentation abstractions to domain abstractions to data models, and vice-versa, with some validation. Therefore, it is likely that the brunt of traffic is borne by the presentation and data tiers. Assuming you are using SQL Server, you can set up a Failover Cluster with an internal load-balancer in front to distribute workload. For all the tiers, as traffic increases, you can scale by adding more servers where needed. All these are fairly straightforward to implement if you use Azure for Infrastructure-as-a-Service (IaaS), but if you use the purpose-built Azure services, such as Service Fabric, it is even easier. The main disadvantage of choosing the latter, IMHO, is that your architecture is then tied to the platform. Edit: grammar
Not a bad question at all: Yes, that's what I'm suggesting. Razor is great, but you'll have to hit the server every time you want something rendered differently. React/Angular/etc does this on the client for you. It's not an easy solution - you'd have to convert all the business rules for the form interaction to the framework of your choice - and you'll take a hit in productivity in the beginning when you're ironing out regressions (bugs you previously fixed in Razor). Personally, I think you'll end up with more maintainable code base (allowing you to introduce new features faster), and maybe scale more easily since som processing is now offloaded to the client. But again, this is very hand-wavy since I have no idea what kind of app you're building.
For SQL session state, it's usually a Web.config change with running the ASP.NET State service. But, it has been a long time since I have used sessions in our applications.
I think he's more talking about a backend API with a front-end UI written in a modern JS view-engine like React, Angular etc. Like /u/joelving said, the application is then similar in structure to a a desktop client backed by an API, but the desktop client is replaced with a web client. So no; it's not an MVC app anymore - it depends on which tech stack you choose but using React as an example, React is just the V, the API is the MC. There's big advantages in migrating to tech stacks like this; but there's also a huge cognitive burden in learning it and implementing it. It really depends upon your specific circumstances whether it's worth a re-write or you're better off scaling sideways and trying to mitigate the inevitable issues.
Are the fields that require complex business logic in the server validatable? If you can trust the client to do these operations and the final result can be validated (because you can never trust the client) I'd try to see if you can build a REST like API and push a lot of the logic to the front-end in a SPA framework like Angular 2, or React. These allow you to have a relatively stateless API implemented however the hell you want - complete separation between front and backend. The front-end can be as fancy as the typical client can handle and the libraries available are pretty well developed these days. The only thing I'd be careful with is if said business logic is something you can't trust to the client. In a web application I just developed my backend was pretty much just converting SQL to JSON for the front-end to do what it willed with it - but when it came to generating the financial stuff I had to have that logic in the backend - but it was pretty simple to implement in the end.
There are some things you can't serialize without changes I believe, but yeah, in general, just a web config change. 
Ah, true. For a business application, most classes will be built with primitive types and should be serialisable by default.
Wait. 40000 users. What are these users? Are they workers?
Do it in the label's resize event.
Thanks for this. I think the larger issue for the company is that the whole develop/test/stage/release cycle can be streamlined considerably. So .Net core is only one piece. It sounds to me like using .Net Core without also using containers etc would be losing half the value. I think that's the way I have to pitch it. 
Agreed. I had an old client come back with code my partner and I wrote in 2005 to try to get us to make changes. I said no thanks. 
Webforms is out. Microsoft is not going to support web forms in future. In any case you will have to rewrite at some point. With some careful planning, I believe you can do this with MVC. Do a small proof of concept with few forms end to end with all feature to see how it is like. I don't recommend desktop application in this day and age. Scalability is something you can do with both webforms and MVC.
Oh dear. I'm a newbie, I was hoping to avoid learning about events for the time being. Well, off to Google I go... Thanks for pointing me the right way :) EDIT: Yeah, this is completely unworkable. The tutorials may as well be written in ancient Sumerian, and when I finally figured out how to subscribe to the Resize event by half guesswork, Visual Studio wants me to write the code for what happens when the event fires in a place where the object I want to modify doesn't exist. This seems to be lightyears beyond my skill level.
Honestly while single page applications written in react, angular produce best experience for the user, the technical overhead of learning them, javascript and the right way to do ui work is probably going to be too much of a jump for you guys coming from webforms. My advice to you read up on microservices and strangling the monolith. Break up your single app into a few logical pieces. Start with just one, like an area of your app. Build that as a separate app with seamless translation in ui. As far as stateless app is concerned, all that means is that the state is external to your app. This way you can handle request regardless on which server it lands without requiring that subsequent requests end up on same box. Concrete recommendation to you is use mvc, avoid heavy ui framework for now as you going to get overwhelmed in learning to many new things and it's much easier to screw up ui architecture in single page apps. Given that asp.net core 2.0 is out, use that as starting point. It's api footprint should be big enough to allow you to do what you need, and you can tell management that new workloads can go on linux saving a lot of money in licensing. Good luck
OK, it really was a "dumb user" error... &lt;sheepish grin /&gt; It was not FromSql that was failing, it was what I was trying to do with the result... Sorry for a false alarm.
Literally every part about this sounds like an up hill battle. Frankly, no matter what the course of action, a year isn't going to be enough. You've got to start tempering expectations with that now. The challenge here is rewriting all that Web Forms logic, and this is the piece that is the biggest hurdle. I assume, that since you are working with web forms you don't have any test suite in place, so you're basically walking a tightrope without a net. I also assume that the actual business logic of the application is probably closely intertwined with web forms as well (which I'm assuming because that's how its been nearly every web forms app I've worked with). Before you start wondering with MVC is the right fit for you, you've got to prepare your code base from moving out of Web Forms. You should pick up [Refactoring by Martin Fowler](https://www.amazon.com/dp/0201485672/). And begin to move as much code as you can out of code behind files, and into pure C# classes that you can test. Interfaces are your friend. Facades and Adapters are your friend. Unit tests are your friend. You need to have your business logic as portable as possible in order to move away from Web Forms and not be constantly chasing down bugs and unexpected functionality. The more your code-behind is a facade that just takes inputs and calls other classes with them, the better off you'll be With that out of the way, now we can get in the framework discussion. First off, I don't think a SPA is a good approach here. It'll be too heavy of a lift for your team, and probably not worth the time and effort at this point in time. So you're pretty much left with MVC as an architecture, which is fine, but I think you're going to have some issues with "vanilla" MVC considering the load and the users you're going to have to service, so you're going to have to look beyond that to make this work. One of the key components to scaling this will be a good cache. If every read needs to hit the database, then you're done. I would recommend using Redis for caching, but an in-memory cache will help you here as well. I would also strongly recommend looking into [CQRS](https://martinfowler.com/bliki/CQRS.html). That stands for Command Query Responsibility Segregation. The high level idea is that requests that need to modify data gets handled by different classes than reading data. This makes it much easier to scale up/out the read side, and also makes it much easier to know when the cache on the write side can be invalidated (since that's coming from the read side). The challenge with ramping up on CQRS is that its generally presented with a lot of other solutions; MongoDb/Document databases, Event Sourcing, event messaging. Those are good and interesting in their own right, but they are way more than you need right now and CQRS does not require using them. The last thing I'll touch on here is that if you've got one big ball of mud, you're path to upgrading this going to be very tough. If you can find isolated parts of the application to split off, and test in a different framework, then you'll be much better off. The more isolated sales is from inventory (or whatever you have) the better off you're going to be in the long run. If you can find the time to isolate those components in the application now, rather than try and rewrite the big ball of mud, your future self will thank you.
Your controller's method should return a FileResult not an ActionResult.
&gt; VB.Net &gt;The technology isn't the issue, its the people. I absolutely could not agree more! Where I work currently, they are all former VB6/VB.NET developers who now write C#. Quite frankly they should not be involved in anything relating to software development. I never really knew if the "VB Developer" stereotype was true until I worked here, and now it's probably one of the most truthful stereotypes I've come across. They show a fundamental lack of understanding about technology and software development. There's no design patterns, everything is in 500 line to 1000 files. Multiple classes in every file. Hardly any interfaces, absolutely no effort to understand interfaces, LINQ, System.Collections.Generic (they prefer to use List and IList when IEnumerable would have been better), a belief that `int` and `long` etc are magically capable of changing size at runtime so they insist on using `Int32` and `Int64` for everything. Absolutely afraid of anything new or modern, totally butchering how you use Javascript and Typescript as well. Using products that are not designed to be used as a database as a database, failure to understand what ASP.NET actually is, all the code is tightly coupled, if you try show them a modern clean approach to something they will do the opposite of out spite, does not provide anyone with any tooling whatsoever and if you dare install an extension in VS you have hell to pay. I could go on, but jesus christ. Fucking VB developers.
How do you know what you have now won't work? Have you done load tests? Either way, I think doing a big rewrite is a bad idea. I would suggest incremental changes. WebForms and MVC can coexist in the same project. Start with one page/feature that you deem suffers the most from the WebForms model - either because of performance or complexity - and turn it into MVC. Then you can evaluate. Don't be afraid to experiment with a few different ways of utilizing MVC so that you find the best way for your particular system, before you continue. Then, every time you make more than trivial changes to old WebForms, convert them to MVC. In the end you'll still have some WebForms pages, but it's the ones that are seldom used and never changed anyway and it doesn't matter. Try to deploy the app, as it is currently, to 2 web servers. See what fails and fix it. I don't think it will be as bad as you think(?). AspNet Session can be put on a central SQL Server to make it independent of the web server for example. Once you're confident you can run the app on 2 servers, you can probably run it on 10 or 100 too. The SQL server is different, and you'll be best off adding RAM and CPU and SSD to a single server instead. I'm sure it'll handle the load. We're running 1.5M users on a single server and we have room to grow.
This sounds like your best approach. Take all of your business logic and slowly migrate it into another solution that becomes your API. Now your code-behind files will only have the logic needed to call into your API and handle the result. I second the use of CQRS and I would recommend looking at breaking up your implementation into vertical slices/features vs. the traditional n-tiered approach. You might struggle with code duplication for a little while when doing this, but once you have several features online you can start looking at refactoring common pieces out. [Jimmy Bogard](https://vimeo.com/131633177) has done [a couple talks](https://www.youtube.com/watch?v=wTd-VcJCs_M) on this. If you are willing to jump on the bleeding edge a bit, [Razor Pages](https://docs.microsoft.com/en-us/aspnet/core/mvc/razor-pages/?tabs=visual-studio) could be a great alternative to performing a full MVC implementation when you are finally ready to tackle re-writing the front-end.
That would cost every time any buckets are changed, instead of paying the cost only when you observe count 
This is horrible. Go submit your stuff to TechEmpower, if you want a better comparison: https://www.techempower.com/benchmarks/
I would highly suggest service stack. It is pretty awesome, fast, highly scalable, well documented and runs everywhere. I do not work for them but we recently standardized on their framework and vuejs for the front end and it has been a godsend. 
I don't know a lot about Go, but the one thing I do know is that everyone seems to despise Iris and its author Kataras.
Its hard to miss if you ask anything about sonething closely in the direction of iris :D
I didnt know go was THAT fast. Holy cow. 
That doesn't sound like great advice for OP's situation. Those JS frameworks add lots of additional complexity in exchange for a better user experience. 
Bad bot
Don't expect any real advice from this. It's just going to be people telling you that you need to switch to the latest technologies and frameworks. The reality is that you didn't provide enough information in your description for someone to give you actionable advice. My tip, unless you plan on expanding your team the skills of the people you have should be the biggest factor in your decision. 
I'd disregard anything the OP says; he's a well known liar and fraud in the Go community. 
Obviously, I think it's worth the trouble. Interconnected form fields are a pain in Razor, requiring constant post backs. In React (for instance) you wouldn't sweat them. I complete agree though, that it won't be an easy transition, if no one on the team has any experience with that sort of development. But then again, OP doesn't sound like he's necessarily looking for the easiest way out. 
&gt; dotnet run Probably should run it in release mode &gt; dotnet run -c Release Rather than debug 
I agree with this. OP, what is the motive? The scaling that you mentioned can be achieved using webforms itself. If you are pitching to your management to do the upgrade to mvc because it is more scalable, then no. If you simply want to rewrite because of maintainability or just a tech upgrade, then you can pitch it to the management for these reasons. Depending on how you have coded the webforms, some business logic may be reusable.
Good point. 
If I were you, I'd hire a good senior developer or consultant to come in and take a look at the current solution, hear your concerns and help you guys make a decision. I'd then get them to start coding some of the solution so your team can follow it and use it as a guide for implementing the next pieces. Regarding the cost of AWS, I'd bet you are overestimating what you need. I did the same when I first transitioned to AWS but I quickly realized I could run a lot of things on the smallest instances without an issue. 
Jimmy Bogard is great and has a number of resources. A couple things though is that he tends to suggest using MediatR a lot (which is understandable, since he wrote it). Personally, I don't think that's the best approach for this project, as it could cause more harm then good. Jimmy also is a strong Domain Driven Design (DDD) advocate which I agree with as well. My mention of finding "isolated parts of the application to split off" is informed by DDD. 
For the love of God, do not convert. Your project is going to fail if you convert. Your primary objective is to scale. So scale. * Create a load test script for your CURRENT setup so you know your baseline. * Get a bigger server for the DB with tons of RAM. * Start putting a second web server and use sticky session. * Change your session backend from ASP.NET State Server to SQL Or Redis (preferably) * Start checking in your code base for opportunity to cache result. * Put as much as possible on CDN. * Bundle your script and css. * Load test and load test and load test at every iteration. * You can do Async calls with WebForm. Starts doing it and measure. Again, DO NOT CONVERT. Choose your battle. Then start examining your read data. See if some of them can be generated client side. Start with that small panel on your screen. Implement a Web API off .NET Core 2.0. Iterate. Pretty much iterate, measure and repeat. Do not do big movement.
&gt; String interpolation in raw SQL methods I'm sure that will confuse the hell out of some security reviewers at work :D
&gt; the implementation in EF6 was not optimal (or not used correctly) You are referring to the fact that the navigation property needs to be virtual, instead of the entire entity like NHibernate does? EF's approach requires that every entity is proxied, and does not allow null checking of navigation properties because that will already load the entity.
Honestly part of me kind of likes this idea, at least better than MVC. How would the stack typically look like using .Net as a backend? Not that I need a terrify specific answer, but enough to perhaps buy a book on it and do some prototyping. In my head I kind of imagine something like a web-form app that's mostly WebAPI services that React / Vue / whatever calls...am I off base here?
Well I hadn't considered the approach, so just the suggestion itself is helpful. It gives me another book to buy and attempt to implement a prototype or two and compare / contrast to other methods for difficulty of implementation and overhead. When we implement new functionality, it typically is in production for 3-5 years or even longer. Is it silly to be concerned with the lifespan of these javascript frameworks? Sometimes it feels like they're only around for 6 months until they fall out of favor or have a new version that isn't backwards compatible. 
&gt; One of the key components to scaling this will be a good cache. If every read needs to hit the database, then you're done. I would recommend using Redis for caching, but an in-memory cache will help you here as well. We actually very recently implemented Redis on our Web Form app, moving away from rampant abuse of the In Memory Cache API. We also have a lot of legacy code tied to old ADO.Net datasets, that are slowly (over months and years) being migrated to EF + second layer caching mechanism running on top of Redis. So at least we got that going for us. &gt; The more isolated sales is from inventory (or whatever you have) the better off you're going to be in the long run. If you can find the time to isolate those components in the application now, rather than try and rewrite the big ball of mud, your future self will thank you. I think this is our second biggest problem. Our requirements (and I Use that term loosely) from management often want every department intimately interwoven, dependent, and tightly coupled with every other department. It *does* allow them to have a lot of automagic on the back end as the system does more for more people, and that results in less work on the staff, but it makes the tech stack a nightmare. I didn't mention it but the application is split up into multiple smaller applications, so it's not exactly a monolith, but no one application is really independent of the other, they all interact in a lot of various ways, so their deployments are always intimately tied together. Billing, Invoicing, Payroll, even HR and Recruiting all have logic intimately coupling one to the other. 
Who is a liar and "fraud" you @dlsniper?
&gt; WebForms and MVC can coexist in the same project. I've heard this before, but I'm unclear how this actually works. I know you can have an MVC and a WebForms project in the same solution - is that what you mean? I don't recall having the ability to add a traditional ASPX page to a MVC project, or add views and controllers to a traditional web forms project, so forgive me if I'm a bit confused :( Edit: Your reply is pretty down to earth and makes a lot of sense, so thank you very much for that. We do intend on more or less trying to go the route you suggest (getting to 2 servers and seeing what happens), but my concerns is with management not historically wanting to pay for servers, moving to a stack that would allow us to run more users on less servers would be a win. As a trade off, it seems like it'll be more effort and take us longer to implement complex functionality. 
It's running on production mode, see the `dotnet run` output and you'll understand, same as with `go run`. Iris is a famous package in the Go community, it's faster than the Go's standard library. The comparison looks fair to me, without include any other libraries like encoder and decoder. I read the source code of TechEmpower and it does not make a fair comparison, read the source code of .NET Core and Go and you'll remember me. Cheers
You're wrong, Iris has the biggest community around it, in the Go world. I learn a lot of him. @kataras, if you see that: listen us, the 98% of Go community and continue like this, don't stop! People like @dslniper stack in the past and spamming hate with fake accounts. @tweq is better to read or try his software before post these things, as you said, you don't know the Go community.
Part of our plan so far is to actually turn off Session state entirely. We've yet to proof of concept it, but with careful refactoring and custom authentication principals, I think we can get where we can turn it off entirely, which would be a big win. I don't know if it's true, but I read IIS performs better without Session since it doesn't necessarily need to answer resources requests in sequential order - so it seemed worth trying. 
Yes, employees. This system handles every possible aspect of the company, so it is used by nearly every employee all day, every day. 
I've literally never heard of this one before. I'll add it to my list to do some research on!
Any questions or want to discuss it PM me. 
&gt;We actually very recently implemented Redis on our Web Form app, moving away from rampant abuse of the In Memory Cache API. We also have a lot of legacy code tied to old ADO.Net datasets, that are slowly (over months and years) being migrated to EF + second layer caching mechanism running on top of Redis. So at least we got that going for us. That actually is a huge plus. First you were using caching before, and you've moved on to Redis. I think that's a good thing to hang your hat on. Personally, I would not worry too much about transitioning over to EF. Maybe its because I'm not scared of my database, but I think EF adds more complexity and more trouble than its worth for anything other than a CRUD app. I think in your case, you will wind up having to deal with EF's slower performance and that will begin to cause it's own stress and pain in your app. I have been working with microORMs like Dapper a bit more recently and feel that those are a good middle of the road in terms of eliminating boilerplate code, while keeping pretty good performance. &gt;Our requirements (and I Use that term loosely) from management often want every department intimately interwoven, dependent, and tightly coupled with every other department.... they all interact in a lot of various ways, so their deployments are always intimately tied together. Billing, Invoicing, Payroll, even HR and Recruiting all have logic intimately coupling one to the other. This definitely makes it harder, but does not necessarily mean that everything has to be tackled together. Basically, even though these processes are interwoven between departments, it doesn't necessarily mean that there isn't one thing that one department does before the next department does its thing. What is key and important for situations like these is that the logic that coordinates between all of the different departments exists out side of any of the departments that participate. You need to encapsulate the coordination so that you have all of that logic split out from the logic of the individual steps. Just because you have discreet pieces does not necessarily mean that it's not a big ball of mud. Having to have everything deployed all together is generally an indication that even though you might have a separate service for this and a separate service for that, that they are so tightly coupled they generally need to be thought of as one unit. My hope is that you're not sharing a database between all of these different services because if you are, your completely negating a lot of the benefits of having separate services, one of which will be the scalability of your application. --- Taking a step back and looking at things from a 1000ft view again. Right now you are in a situation where your dealing with a LOT of technical debt within your system. Its 2017, and any time I see web forms, its a huge red flag. Organizationally, you're dealing with issues too. Frankly I would not estimate that a 4 man person could support a system like this (especially if there really isn't a dev who's thinking about how all the pieces work together). On top of that, your organization thinks they can just have massive growth in the use of this system with minimal consideration for its maintenance and development. That is a HUGE business risk, and if they are betting on it, they are going to run into serious issues. After all that, they don't seem to want to commit to the hardware resources that running that sort of app will need. With all that in mind, a year is not enough. I know I already said that, but this sounds like a much longer endeavor, and if management thinks it won't be, then they're setting you, themselves, and the organization up to fail. As I mentioned above, 4 devs are not enough resources. Not only do you need more devs, but you are at the point where you need devs that have discreet rolls (front end, back end, DB, etc), and you need QA/Testers. Without more resources, and without more specialized resources that have focus, its going to be very hard to do this. Lastly. You need to do a lot of reading on Domain Driven Design, and you need to do it quick. It is absolutely imperative that you begin to understand how to organize and compartmentalize the different departments, their needs, and how they need to communicate with the rest of the departments. DDD tries to ensure this by structuring your application to respect those boundaries. You're getting into big enterprise system design whether you like it or not. These are huge undertakings that even big companies get wrong. Its hard, and its hard for a reason. There are solutions out there for a lot of the problems you'll encounter, but it will take a lot of reading, a lot of research, and you need to do proofs of concepts before you begin rolling things into your larger application. 
 Basically I've been asked what it will take to scale from 2,000 users to 40,000 users. Bare in mind I've been begging to replace a 5 year old web server for a year now, that they don't deem as "necessary". So honestly part of my motivation is if I can make the technology stack lighter on the server, that's less servers we'll have to fight to get management to pay for. Historically we've ran on a shoestring budget, and that in itself is a difficult thing to change. 
I mean. Performance wise I can't tell you shit since that's a measure only you can do but considering how much money it can cost if 40k workers have to wait for server maintenance I think redundancy should at least be an argument. I can't even believe that with 2000 clients this hasn't been an issue yet. Just one hour of downtime with workers on 10$ is a 20k loss. 20k is so much money that could have been used to buy server time. Has this never happened?
&gt; I would not worry too much about transitioning over to EF. Maybe its because I'm not scared of my database, but I think EF adds more complexity and more trouble than its worth for anything other than a CRUD app. Actually EF has worked out pretty well for us so far, but mostly managing a large number (hundreds) of database tables in ADO.Net became a nightmare. Code first EF plus some custom tooling to blow away and regenerate our model from scratch has saved us a lot of time. &gt; My hope is that you're not sharing a database between all of these different services This is unfortunately the case. Every department is intimately tied to every other department datawise, while it is organized by departments they're still in the same database to simplify transactions and referential integrity, which we would lose breaking them into separate databases. &gt; Frankly I would not estimate that a 4 man person could support a system like this (especially if there really isn't a dev who's thinking about how all the pieces work together). On top of that, your organization thinks they can just have massive growth in the use of this system with minimal consideration for its maintenance and development. That is a HUGE business risk, and if they are betting on it, they are going to run into serious issues. Yes, you are 100% correct this is the heart of the problem. I know some people would say go find another job, but it's never quite that simple. So I'm trying to make the best I can out of the cards that I'm dealt for the time being. I've gotten myself in trouble by being too loud / forceful trying to point out these things myself. What I *really* need is for them to hire somebody new with industry experience to just say the same things I've said, perhaps it would sink in. 
I'd rather they took that effort to make a web api over HTTPS websockets, like they are doing with Service Bus on Core over AMQP.
You are completely right. We have been lucky we've never had downtime for really more than a few minutes, so I don't suppose I've really quite looked at it that way. 
Thanks! I'm going to get a head start on SQL, as it's the first portion of the course. ( classes start sept. 11th). My JavaScript is pretty weak, so I'll make sure to read up on better use of jQuery and AJAX.
That last sentence hits the nail on the head. I guess I really need to drill down on C# and everything included in the course.
I don't think I would consider that lucky. Now, if anything major happens you are in for a treat because the first time for you now means blocking 40k people not "just" 2k.
Nah you're not that far off at all man - there's specific examples out there. Take these: https://github.com/aspnet/JavaScriptServices - that's straight from Microsoft; examples in dot net core in angular, react, react + redux etc.. So you have a dot net core "MVC" website which does nothing but serve up the react/angular, whatever JS, and serve API content. Once you've hit the javascript app entry point, routing is handled by the javascript app, and data is fetched from the API, then displayed using the view engine. I don't think you need to buy a book; there's so many articles out there you can read enough for free to get started. Pick a view engine (I'm heavily in favour of React+Redux+TypeScript, but whatever floats your boat) and prototype away! Where you'll see stack blow-out is in the JS modules needed to glue all this crap together.. Example might be: dot net core sql entity framework web api react react router react redux react router redux typescript babel webpack Some benefits to this approach, to get you excited: Type safe javascript with TypeScript! Hot reloading with Webpack HMR Single-page application with no page-refreshes Redux for state management makes life easy Re-using components across multiple views Damn quick render times dot net core is cross platform meaning you can deploy to a docker container, azure web app, whatever
Thanks for the gold, and I hope my advice helps. &gt;This is unfortunately the case. Every department is intimately tied to every other department datawise, while it is organized by departments they're still in the same database to simplify transactions and referential integrity, which we would lose breaking them into separate databases. You can scale out your application layer as much as you want, but as long as you are writing back to a single database, you will have issues scaling. You really need to rethink how much all of these departments need to be working all with the same data. I know that sounds weird, but here me out. You've got a customer, and there's a ton of information linked to that customer. Sales, Billing, Ordering, and who knows what other departments, all need a "customer", but do they all think of a customer the same way, and do they all need the same information from a customer. Sales needs to know "who" the customer is. They need to know who to pick up the phone and call to see if they need more widgets, they might need to know what their gross sales were last year (and if its above $100k, then there a "prefered" customer), that kinda stuff. Billing needs that customer's credit card number (which no other dept should have access to), it needs to know where to send invoices, it needs the orders too, but really only the final amount with tax and shipping split out (not that they ordered 300 widgets). Ordering just needs to know the customer's name and the shipping address (which may not even be that customer's address). The point is that if you combine all those use cases into one object, then you're going to have an object that is going to have a ton of data on it that you may not always need, and a ton of reasons for change (Sales says anyone over $250k in purchases is now a "gold" customer). What you should be looking into is segmenting that customer into separate objects that are used within each department. This isolates them away from changes that are needed by other departments, and it also allows you to split out the persistence of a given department's objects from the rest of the application. This isn't easy, and it brings some of its own challenges, but that's why you've got to dig in and do some research. There are decisions and trade offs that need to be made that you're going to have to discuss with you stake holders (you do have stake holders, right?), and get the business's input on. &gt;I know some people would say go find another job, but it's never quite that simple. So I'm trying to make the best I can out of the cards that I'm dealt for the time being. I've gotten myself in trouble by being too loud / forceful trying to point out these things myself. What I really need is for them to hire somebody new with industry experience to just say the same things I've said, perhaps it would sink in. Career advice here. Right now this project is set up for failure, and if you're in charge of it, you are being set up for failure. Given the constraints your dealing with, there it would be a herculean effort to get this done, let alone in the timeline of a year. You need to sit down and have a calm, cool discussion, not just with your manager, but your manager's manager. Being vocal/forceful about things tends to make it easy for others to discount your perspective. You need to try and present your plan to them, along with the challenges, risks, and issues that you see before you. Its not about you, its about the project, the app, and the business. Some would say its time to leave, but I understand you want to stick this out. If you can get management to see how hard it will be to deliver what they're asking of the team and commit the resources that are needed, this could be a very rewarding project. But if you don't get them to see it, then it will be incredibly stressful, and may end up with you being shown the door for not being able to deliver on their impossible task. Regardless of whether you leave, keep your resume updated, because this could potentially turn on you, and might need to get that resume out there quick.
Nothing changed. "result.location" is still undefined in the JS function. And that's all the code I wrote in a new project, so I can't post anything else.
Id say you need to scale using some sort of cloud solution, I'd doubt that a rewrite inside of 12 months without expect help will produce anything but heartache. Consider streamlining the use of viewstate and session . If the price of cloud based solution is causing problems , workout the cost of a rewrite and factor in the risk factor (I'd guess 50/50 at best). Then remind them that any plan b will also come with a price tag. 
CQRS might not be the best solution, as if sounds like the app creates a lot of transactions, which can cause overheads down the line. I've inherited CQRS applications that have been used for over 18 months and the backlog of recorded changes caused several technical headaches that were only getting worse Another problem is that the amount of support for CQRS out there is far more limited than vanilla MVC 
i had to do this recently, check out [this post on stack overflow](https://stackoverflow.com/questions/16670209/download-excel-file-via-ajax-mvc) (see the update on the top answer). this method worked amazingly well. there's a comment about a security risk. probably not an issue for you if you're using a local server.
That's a fair point. A cloud based solution is on the table, but with absolutely no on on staff that any experience with AWS / Azure whatsoever, at times it feels like we're just guessing how difficult it will be and how much it will cost. 
True but the risk is lowest. A failed rewrite (its a risk) could leave your company 100,000's out of pocket and still running on the same software. Honesty if it were your money you were spending , which would you choose 
&gt; What you should be looking into is segmenting that customer into separate objects that are used within each department. We kind of do this now, except with separate objects organized within schemas within the same database. Really I get sort of hung up on references, that if separating the schemas into separate database, perhaps sales has sales information pointing to a CustomerID that's in the Customer database. For whatever reason, that Customer in the Customer database gets dropped or the ID changed, then the sales database still has data pointing at a non-existent customer. Within the same database, this is easy because FK's will block it from changing. I know that's probably the wrong way of thinking, and you're probably right, I should probably be splitting what we have now as schemas in the same database into multiple databases, I guess its just an old hangup of mine. &gt; Career advice here. Right now this project is set up for failure, and if you're in charge of it, you are being set up for failure. I'm unfortunately aware of this, which puts even more pressure on. I'm not a lead / manager, nor do I want to be honestly, I was just asked about how to scale because I have more experience than my co-workers, but clearly still not enough. 
Look, you're not in the best position here at all. I want to help you get there, but right out of the gate, it seems you have your hands tied. I'll try not to go into too many unsolicited details, but feel free to ask for clarification. Here are a couple initial thoughts: ## Dont do CQRS, and DDD... Someone recommended CQRS and DDD. I think that is probably the worst idea for you in your situation. I'm currently re-building a large platform application using CQRS, DDD, and it is event sourced and I LOVE IT and I think it is the best long term strategy for a *software company*. However, I would NOT recommend this to you in your current situation. All 3 disciplines have a significant learning curve to be done right -- they are nothing like what you are likely used to. Someone suggested that it simply means you have different classes for reading and writing data which is incorrect. You have separate PROCESSES to handle each so that you can scale each independently which is the whole point. And if you're event sourced, then there's a whole 'nother discipline around denormalization which leads to all kinds of fun concurrency challenges. This leads me to the next thought: ## If you only have 4 developers that are not rock stars and a management team that is already balking at the costs before a plan has been presented, then chances are you guys will not be able to handle this kind of re-architecting let alone within a year. More advanced and scalable architectures are going to require some passionate and competent engineers to pull off and you're all gonna have to be on the same page. A team can't simply read a book and slap down a new system. There are entire cultures built around it. Judging by the way you're talking about your management, it sounds like you're a company with a software dept but not necessarily a software company, or the culture there is super stale. This does not lend well to making a complete 180 in terms of your software approach. What will likely end up happening is that you'll blow hundreds of thousands in resources (time and tech) and only make it about a quarter of the way through before the entire things gets scrapped, and obviously you don't want that. It sounds like the original system was either designed by engineers with no respect for clean layered code, or it was pounded out in a hurry. So everyone is going to have to change their mindset if you're hoping to make this rebuild worth the money it's going to cost, because if you're just gonna hack a new system together, it's going to be a complete waste of everyone's time and money. So my advice is to answer a few questions among yourselves and get everyone to commit to the answers. - What is keeping us from scaling right now? - What are the core issues of the the current systems? - Does this team know enough about the existing problems AND enough about some STANDARD BEST PRACTICES to fix them, or would we just end up reinventing the wheel yet again on a potentially pointless rebuild? I want to emphasize this point. Rebuilding the system in newer technology makes no sense unless a) the current technology is no longer supported by it's proprietor or b) the existing system is just so bad that it's about to fall on its face - Let's hire a few more people and include a competent application architect to ensure a cohesive rebuild. That doesn't have to be some super high paid maestro or anything. Just a developer with passion and a plan that they can back up. Talk about these things and then decide whether or not this team has what it takes to take you to the next level. If not, figure out what to do about that. # A recommended approach If you still decide to do this, I would recommend the following approach for the following reasons: - Stick to WebForms. I hate to say it, but with a team your size, this is probably your most responsible move. The entire point of WebForms was to bring front end development to back end engineers. It's a time saver. Keep it. Later on, if or when you all grow the team, you can consider moving to MVC, or NodeJs, or whatever UI-focused type framework out there. In the meanwhile, just do your best to keep the view and presenter layers as independent as you can to make it easier to port later. - Ditch the monolith design. You haven't explained your existing architecture, but it already sounds like a monolith. All the code is in a couple applications and it's just this behemoth that can be really hard to maintain. What's worse is that since it's all in one place, in order to scale one part of it, you need to scale the entire thing! Instead approach from an SOA point of view. Create coarse-grained services that focus on one aspect of the business. These are the services that will contain your business logic, not your WebForms app. Then you just have the web application interact with those services to get it's job done. As a transitional step, write all of the services with a RESTful interface, not SOAP, not RPC. Take time to learn what that actually means and what it looks like in practice. This will open up doors for you in the future since it's basically plain HTTP, so there are minimal complications when trying to integrate different technologies. - What's up with the statefulness? You can probably just write that right out of the application on rebuild. Think in terms of "do the thing" and "get the thing" and try to keep the lines straight and clean. Any time you run in to a case where you think you need to use the session to store data, criticize it heavily. There's almost always a number of ways to avoid it. - Allow each service to own it's own data. If you need data from the database, do not go straight to the database. This is the cause of many scaling issues as well as tight coupling that prevents you from making architectural changes in the future. For example, the fulfillment service talks to the fulfillment database. Nothing else is allowed to talk to that database. Nothing. This also protects your business logic by ensuring any data modification is run through the business rules first, in all cases no matter what. I find that many shops have a problem with this approach at first because it'll force you to do a little more work up front, but you'll find savings in the future for sure, as it has really great side effects. - Each service follows the n-tier design: A presentation layer, a business layer, a service layer (if applicable), and a data layer. The presentation layer for these services is the web API; the data contracts that are passed to and from the service. - Consider the introduction of a message bus. The more asynchronous your system is, the better it will perform and scale. If something doesn't need an immediate response WITH data, then it's perfect candidate for queuing which will allow the system to process as it finds time which means it can free up more connections at the head. It's also a great indicator for when it's time to scale up again. When the queues are backed up, you need more workers. Simple. The implementation can be a bit confusing at first though, so we use NServiceBus to help abstract away all the details of managing the bus. Ok, so SOA means more processes running independently with each other, so now you have to figure out how to host them... #Infrastructure Ok, so money. Always a sore spot. It's always hard to organize thoughts around the reality of building a system that can handle load, so I'll just dump a few super obvious thoughts (brainstorming -- it helps to just say the obvious out loud sometimes) - Scaling means more instances. - More instances means more machines - Scaling means more delegation (SOA, Caching, etc.) - More delegation means more machines - More machines means more money - If you don't purchase more machines, then you cannot scale - If you have a lot of users, then you need to scale or else the whole thing will just slug to the point of being unusable - Being unusable means no one will use it - No one using it means no one will pay for it and that is far more expensive. So, you're gonna have to compare the costs to the gains. If it's more than 10%, I'd say it's a no brainer. So, the problem with data centers is that it's your hardware. You have to pay for more stuff when it's time to scale. With offerings like AWS and Azure IaaS, you can simplify that. You still have to pay for it (obviously), but it's virtual, so you end up spending less money on manpower to scale once you have someone trained up on those services. Need a load balancer? Log into the portal and add one. Done. No need to purchase it, wait for it, go wire it up, etc. Your management's fear of the cost involved with cloud services is unwarranted imo. You're going to have to pay for it one way or the other. You just want to be as efficient about it as possible. #Ok on to a solution I'll speak to Azure since I haven't done much in AWS. And if you're a .Net shop, Azure has the benefit of being Microsoft serving Microsoft, so you're well supported on all fronts. 
Azure has PaaS offerings that make your life easy in this scenario, particular at this step in your evolution. Azure calls everything a service, but in short, it directly supports their own flavor of "container" that amounts to either a web application or a worker (aka windows service). You set up the project and have it implement one of these "roles" and then you deploy it to the cloud. Once it's deployed, you can control the scaling with a slider... literally. So if you're seeing that some worker service is struggling, add a few more instances until it catches up, then remove the new instances. Or you can set it to do that automatically. It'll take care of all the load balancing and other details for you. This is really great for someone in your position. You only pay for what is used with this PaaS offerings. So create a new service (container) for each of your uh... services (the coarsely-grained business services) and deploy them to azure. Then deploy your web application(s) to a similar container and have it talk to the others to get it's job done. That should take care of your application. # Other SQL is a whore. Unless you need ACID compliant databases, I'd consider moving away from it in lieu of a document database. Document databases have order of magnitude less development overhead and are far easier to evolve and maintain. People get a little hung up on the idea of denormalized data, but once you learn the practice around it, you'll start to grow your hair back. We use MongoDB, ElasticSearch, and Neo4j (we're event sourced so having multiple specialized databases is easy). We don't have a DBA so we use cloud managed services direct from the vendor to deal with the details. The price is about the same if you were to host it yourself. Maybe a little higher, but it's managed for you which is much cheaper than having that DBA. Also, document databases like MongoDB use an aggregate pipeline which is super expressive which makes complex queries much easier to write and much much faster than SQL in many cases. If you're gonna stick with SQL, then I recommend hiring a competent DBA to manage scaling. We're all smart enough to do it ourselves, but to really do it right, you're going to want to have someone dedicated to it. Also, keep your business logic out of procs. For the love of god. If you're using EntityFramework, I recommend sticking to procs for reports and leaving EF for simple queries. #Caching Find all scenarios where there's a high likelihood that the same data is being requested and cache it. We use Redis for this (it's a super fast key-value store). We just spun up an instance of it on Azure using a template. No problem. Basically we just use the query parameters and some string representing a context and generate a hash as a key and then dump the data as json as the value, and give it an appropriate expiration. For most things like UI elements, the expiration is no more than a minute. For other things like reports, maybe 4 hours to a day depending on the report. Doing anything more advanced than this is probably overkill in your situation. #Precomputing For common reports where the parameters are predetermined, consider running them on a schedule instead of on demand. For example, if you have a 30/60/90 day report, then write a service that runs on a schedule to compile these reports and then saves them to a denormalized database (like mongo) in it's final form, so that when it is requested, it doesn't have to be built on demand and bog your database down. All of these things can be taken care of in Azure fairly simply so it doesn't have to be this profound re-imagining of your company. So the challenge you have in front of you is documenting the cost analysis. Calculate after you have the architectural plan laid out (e.g. what services you will have, what databases, etc.). Estimate 3-5 instances of each service and at least 2 of the web application itself. For databases, at least 2. It'll be hard to estimate because they are billed on DTU, so it'll take some research to estimate your usage. Then once you have a solid hold on your practice going forward, you can estimate that the work will be front loaded and pad it for discovery and training. In other words, it'll be a slow start and you will have to learn a lot of things in the process, but the velocity of development will increase as the rebuild progresses. I'd say the first 3-4 months will be the slowest. Maybe estimate that by taking your effort estimate and adding at least another 50% to it and multiplying that by the average develop cost of the dept (your average salary). #My outrageous claims that have been proving true You can guesstimate the gains in productivity at completion. With a cleaner solution, you can expect to increase development velocity by 50% (anecdotally basing this on my experience from switching from a monolith to a well-layered SOA design) while keeping SQL and probably 75-100% (a full double) by moving away from it. On a side note, micro-service architeure using DDD, CQRS and event sourcing has a huge learning curve, but in the end the productivity is at least double and allows for far more complex application with the same gains in productivity :D. But you need a very dedicated team for this. The gains from this come from the strict practice of DDD and the compartmentalized approach of microservices. The side effect of the compartmentalization is usually cleaner, more focused code (SRP up the wazoo) and the ability to have developers working in different projects without stepping all over each other or requiring careful planning. Scaling with this approach is much easier and cost effective because you can scale individual areas of the system instead of the whole bloody thing. If you want to illustrate that, quite simply estimate the monolith hosting costs and multiply it by 2 for every step of the scale and point out that it doesn't include the database. I'm oversimplifying of course, but that's how it tends to turn out. # Summary - Figure out your team and if you're gonna have what it takes - Get management to understand the pros and cons of not doing this - Look into Azure since it'll simplify a lot of this - Try to grow the team by one or two more people at least - Choose an architecture, learn it, and have someone champion it – designate someone as the architect. I recommend SOA at the very least. I don't think you need microservices. - Choose an application model. I don't recommend CQRS for you; it doesn't sound like your team is ready for that kind of change. Keep to the basic n-tier model and apply to each individual service. - Nail down a methodology for the code. I don't recommend DDD for you -- hold on, let me clarify. Domain Driven Design simply means that you model your business layer around the domain and that the domain is walled off from the technical implementation of the system. The *Practice of* DDD is a set of design patterns that help you achieve it in a standardized way. This is the part I don't necessarily recommend in your situation because it has a steep learning curve. If you ALL want to do it, then you will benefit greatly from it. Just understand that learning it won't be pretty.
Oh I agree, I really want to go cloud myself, it'd be great for my resume if all else fails, and I prefer kind of more devops type stuff to actual development (god, I hate front end dev). I'm just mentally preparing for being told cloud is too expensive right now (I'm fully aware it's a near-sighted short term view), but I'm going to push for it nonetheless, meanwhile pondering what my alternatives are. 
If you can avoid session state, it's definitely the way to go. It'll be faster and scale better. And easier to debug 
&gt;separate objects organized within schemas within the same database. That's good. I always think that schemas are an under used feature in most database designs that I've come across. As long as you don't have one big customer table with 80 columns that each dept might try to take a lock on, you're at least not in horrible shape. &gt;For whatever reason, that Customer in the Customer database gets dropped or the ID changed, then the sales database still has data pointing at a non-existent customer. Why are you changing an ID? If the ID is just for uniqueness and doesn't have any relation to the customer, than it should have no reason to change. Also, you probably shouldn't be presenting that ID to the user either. This is one of the reasons that I like GUIDs because they tend to solve both these problems. Why are you deleting a customer? Soft deletes (i.e. active flags) are your friend here. First of all, if I've got orders for your customer, do you delete the orders with the customer? What if your boss says that they want to reach out to customers who haven't placed an order in 6 months? If you're deleting a customer, you may miss a lot of things here. Even if you genuinely do have to support deleting records, there are other ways you can handle that. You can have it be a legitimate operation that's supported in your system, just as you would adding an address, or placing an order. Its just that most of the time deletes tend to be the last thing to be handled, and adding CASCADE tends to make it far easier than maybe it should be. If you think of deleting as any other feature, then it can be handled. &gt;I should probably be splitting what we have now as schemas in the same database into multiple databases, I guess its just an old hangup of mine. Its a hang up that I've come around on myself. Once I started realizing that there really wasn't as many components binding these objects together, then I recognized it more as an opportunity. An odd thing that comes from things being truly isolated is that you now have far more freedom to change things than you did before because you know exactly what depends on that thing. Extending that isolation to the database means that freedom allows you to make schema changes or normalize a table with much more confidence than even separate schemas in single database. Also, resist all temptation to set up linked servers. They'll only become a crutch. &gt;I'm not a lead / manager, nor do I want to be honestly, I was just asked about how to scale because I have more experience than my co-workers I understand not wanting to be a lead or a manager, but if you're the most experienced in your team, then you are the lead, regardless of whether you want to be or not. This is something you need to be upfront with your manager as well. You need to be clear that this may be something beyond the skills that you bring to the table. There's nothing wrong with saying that, but if you don't, management may think you've got everything covered. They also may realize that if the best developer on their team is saying they need someone with more expertise, then maybe they need to go out and hire for it.
Not sure what CQRS has to do with MVC. CQRS is an application model and MVC is a presentation pattern O_o
Woah, I really appreciate all the time and effort you put into this post, so firstly I'll just say I read it over twice but will reply as a whole versus individual points. I think most every point you've got rings pretty true. I'd say our biggest enemy is the unwillingness to pay for things (server, infrastructure, or new staff). I will ponder and consider all or your points as I try and figure out and organize my thoughts and consider what I recommend we do (I'm not a lead or a manager, all I can do is recommend and seek buy-in from the rest of the team). I know this thread started with "Should I adopt MVC?" but ended with "Is this an unreasonable request from my employer?", which perhaps in this instance is all the same thing. It's still all useful conversation for me to consider going into the next month or two and the year to follow. I'm getting pretty overwhelming advice to stay a mile away from a rewrite, prepare what we've got for the cloud, hire people that are smarter than me to help figure out a long term architecture and design pattern goal and gradually move from one to the other. In the process, that means my company has to be willing to buy into a cloud infrastructure and the costs associated with scaling our current (even if inefficient) web form based solution, and getting staff that can help figure out how to target a different long term architecture. If this fails...we fail, and either they dont' get to scale, or I'm looking for a job. Man, some days I just think about working on a farm or becoming a bartender. This is one of those days. 
Sorry, what? 
I also wanted to point out that you're going to need to solve your problems responsibly. In order to do that, you should probably look at each area: - Presentation layer: This is the WebForms vs. MVC vs Angular/ReactJs/etc discussion. If you want to switch, then why? What are you solving? MVC is more manual work since it attempts to keep the view pure. You can use razor, but that's just a tool for getting back-end data onto the front end. The preferred approach is ajax anyway, so you'd likely wanna choose your poison on a framework (react, angular, or manual use of javascript/jquery, etc.). I suggest *cleaning* the project and sticking with WebForms considering your team composition. Clean it out! Get all that data access and business logic out of there! - Your business layer. Is it in your web application? It shouldn't be. This brings back out to the application architecture part of the discussion. The web application should only be concerned with user interaction and nothing else. The business layer and data layers should either be in a collection of libraries or in separate services. I recommend the latter, in which case you would break it up into areas of concern and have a service for each. - Your database. This is the data implementation part of the discussion. Migrating off of SQL can be ideal, but a massive undertaking. In terms of performance, you'll likely need to scale up before you can scale out depending on timelines. Long term, you should focus on scaling out. Looking into replication and sharding. Replication will help your read performance and sharding will help your write performance. Hire a DBA. It's more than something someone can just "figure out" when we're talking production here. All of these things (individually) affect your ability to scale. Someone suggested just throwing money at the database to scale up. That is not cost effective, particularly in the long run. It'll help performance of that one instance because a bigger chunk of the data will be held in memory, but that's really just covering up a root problem. Actually try to understand the problem before throwing money at it. Is the database bogging down because of general use (number of connections), or is it because the queries are too complex? Why are the queries so complex? Is it because they're just complex or is it because of the way the data is modeled? Or is it just bad SQL? This is why having a DBA helps because in addition to the management of the database, they know better how SQL gets executed and can help you write more efficient queries. This is why I love NoSQL. It eliminates most of this. Where people get it wrong is when they looks strictly at the database though. NoSQL isn't better out of the box; your application approach has to be different. But that's a different discussion.
Who do think you're fooling with this? &gt;Iris-go is totally great and very popular and all its users love it and also kataras is very handsome! &gt;-- /u/totally_not_kataras 
&gt; That's good. I always think that schemas are an under used feature in most database designs that I've come across. If I'm 100% honest I'm a database guy. I can design, query, tune, and administer in SQL Server, but I'm a crappy mid-tier developer, and even worse front end developer. I've just been at it longer, so I keep getting pegged with these sort of questions and tasks, which given a choice in the matter, I'd rather leave to smarter people with more experience (and interest) in architecture and design patterns. &gt; Why are you changing an ID? If the ID is just for uniqueness and doesn't have any relation to the customer, than it should have no reason to change. Also, you probably shouldn't be presenting that ID to the user either. This is one of the reasons that I like GUIDs because they tend to solve both these problems. It was a bad example, admittedly - we'd never change an ID on something, but if we were in two databases, I could see where perhaps a Customer got deleted from one database, and then you've got a Sales database referencing a non-existent CustomerID. And yes, we depend too much on physical instead of logical deletes - mostly because we struggle with adequate storage as it is. &gt; Also, resist all temptation to set up linked servers. They'll only become a crutch. This is one that I'd like to hear more about. Why do you think they will be a crutch, and in what ways do you approach things differently? I suppose right now I do use linked servers, but pretty rarely. Just when I want to copy some small subset of data from production to test or vice versa, but pretty seldom. &gt; I understand not wanting to be a lead or a manager, but if you're the most experienced in your team, then you are the lead, regardless of whether you want to be or not. You're right again, and I hate you a little for it. I hate leading because it inevitably leads to me being a manager type, which leads to me ending up doing more paperwork and politics than what I would consider meaningful work, so I end up just avoiding it like the plague. &gt; They also may realize that if the best developer on their team is saying they need someone with more expertise, then maybe they need to go out and hire for it. I really hope this is the case. It's an awkward situation where if our department actually gets too expensive (staff wise), it's not outside the realm of possibility they might just consider paying for third party software instead of in-house software, and we're all out of a job anyway, so it's always a pain point to bring up. Edit: my brain inserting the wrong words when I type too fast. 
Also, I could probably recommend writing all the new stuff in .Net Core/Standard. This will allow you to run on Linux servers which are literally half the cost of windows servers and 4x the stability imo. That's adding more learning curve though :/ Could be worth it depending on the commitment.
Oh man... my advice was based on external, paying users. It's gonna be 1000x harder to justify if they're just internal users D:. All the estimates are going to have to based on worker productivity gained. We've done a few of those. You have to do a study to see how much time is already being lost every day to inefficiencies. It's an annoying thing to conduct
It sounds like there is far too much woven into their WebForms implementation. I believe this is the case for many businesses out there so Microsoft isn't going to be cutting them off any time soon. They're lack of support will likely mean that no new versions will be released. Converting a beast like that to MVC would likely have to take place over several years with a team that size, and I don't think any amount of careful planning is going to make that anything less than daunting. Alternatively, they could contract the work though, but not without a backend redesign. If they could separate out the business and data logic, they could then just hire a small team to rewrite the front-end in a chosen technology. 
Absolutely, but it could be based on the current design of their system. When you have a BBOM like that, it tends to need more hardware for a number of different reasons, least of which has to do with what the application is *supposeed* to do vs. *how* it's doing it. If the solution was broken up a bit more, it's easier to pull off on smaller instances. I'm assuming this is the case; he probably looked at the size of the machine they're using right now and it's average utilization and then found the same thing on AWS which was pricey. A shitty situation for sure. They will likely have to split that beast up and that'll bring the hardware requirements down.
I'd point out that there's going to come a point where not being in the cloud is going to be far more expensive. I think you could probably bring those costs down with a partial rewrite as it will likely reduce the hardware requirements to meet your goals. I wish I had the time to offer direct consultation.
Yeah, I think the most important part is to get a clear statement of what you're being asked to do. Are you being asked to modernize the front-end or are you being asked to rework the system to scale better. Despite how simple each question is, it fans out into a number of things. Modernizing the front end could be a long, but somewhat easy task. Reworking the system to scale is much bigger because it digs into the very design of the thing. One problem leads to 5 problems which leads to 5 solutions which in turn lead to 10 more problems. 
Yeah, you can perhaps imagine now why there is resistance to buying things like new servers and tech. It's not as obvious as passing the cost along to a paying customer, or a cost per customer. You're already paying the employees regardless, so new servers to support those employees to a non-technical manager (all my managers are non-technical) just seems like pure, unnecessary expenses. 
&gt; I would also strongly recommend looking into CQRS. That stands for Command Query Responsibility Segregation. answering this
I think it MAY be expensive so maybe better to outline all possible cases and put together some costs e.g. Do nothing (cost/risk) Update existing hardware (cost/risk) Move to cloud (cost/risk) Rewrite (cost/risk) Be honest with this. This may show any flaws in your "gut feelings"
[removed]
&gt;If I'm 100% honest I'm a database guy. I can design, query, tune, and administer in SQL Server, but I'm a crappy mid-tier developer, and even worse front end developer. I've just been at it longer, so I keep getting pegged with these sort of questions and tasks, which given a choice in the matter, I'd rather leave to smarter people with more experience (and interest) in architecture and design patterns. There's nothing wrong with being a database guy (I've done a lot of DB work), and I think you're selling yourself short on your backend work. That being said, if your coworkers don't have the experience, and management is looking to you, you've got to have a conversation with them about it. &gt;mostly because we struggle with adequate storage as it is. I hate to throw the "storage is cheap" card at you, but it is. This sounds like the organization is being penny smart, pound foolish. &gt;Why do you think Linked Servers will be a crutch, and in what ways do you approach things differently? So I've used linked servers on a number of instances and in a number of different ways. The most innocuous was as an integration job where we'd run a query each night against the other database. The most cumbersome was actually against a Postgres database that was treated like it was just another table, and oh man did that cause a lot of trouble. There are caveats galore when it comes to linked servers, and depending on what your linking to, some things might come up that may not in other situations. The biggest thing is that transactions over linked servers are a big pain in the ass and are not worth dealing with. You've got to get distributed transactions set up, including a distributed transaction coordinator, its slow as ball, and generally is more heavy handed with the locks it requests. Just don't. Please, just don't. The other is that SQL thinks more in sets than it does row by row, but once you start writing queries against a linked server that treat the data like its just another row, that set-based model tends to fall apart, and row by row operations begin to take its place. I can't tell you how many queries I had to optimize because each row of data was executing a separate call to the linked server. Mostly this was solved by running the query against the linked server first, storing it in a temp table, and then doing the main query, but that incurs more disk IO as well. There are other caveats here too, but its not really about the caveats at all. At the end of the day, database integrations are a bad way to go down. It seems that the data is just as accessible as data in another table, but its not. It will screw with damn near everything if you treat it that way, and since its so easy to treat it that way, its best not to even go down that path. Despite the technical issues, if you're going down the road of splitting the data into separate databases, then you should think about them logically that way as well. If you think you need to modify these two separate things, then you need to encapsulate the logic and the process around that in code and in the application. When you let those kinds of concerns bleed across the databases via linked servers, you just get bad coupling. &gt;I suppose right now I do use linked servers, but pretty rarely. Just when I want to copy some small subset of data from production to test or vice versa, but pretty seldom. And that's a decent use case for them. Personally that's about all I do with them, and all I'd really recommend. &gt;I hate leading because it inevitably leads to me being a manager type, which leads to me ending up doing more paperwork and politics than what I would consider meaningful work, so I end up just avoiding it like the plague. My current position has far more non-coding task than coding and it annoys the crap out of me too. The worst is that its not even managing a development team, but dealing with the business side who constantly need to be coddled and can't follow through with a damn thing. I wouldn't go as far as saying it isn't meaningful, as I wouldn't be able to get projects done with out it, but it is both frustrating and not anywhere near as rewarding as development. &gt;It's an awkward situation where if our department actually gets too expensive (staff wise), it's not outside the realm of possibility they might just consider paying for third party software instead of in-house software Third party software always has a very tough time replacing in-house developed software. The reason being that there are so many customizations and accommodations that are present in in-house software that something off the shelf has a tough time recreating them. The business is also not nearly as interested in changing their processes, since the software has accommodated them for so long, that many times even if the third party software could do what's needed, the business won't adopt it. The last job I left was actually a Web Forms app that I knew they didn't have any interest in upgrading, but they were looking at a third party solution to take it over. I've got friends who still work there, and the situation hasn't changed and the third party solution never got off the ground. My point is that third party software isn't going to solve all the issues, and will likely bring a hell of a lot more of its own. Its a very tough sell for the business (it should be at least), so if that's what's holding you back on pushing for more resources, then I think you should reconsider. That other software has its costs too, and its likely far more than bringing in a lead developer (and buying more storage).
You give away so much with that comment. If you don't know Ben Adams (great work mate, very impressed!) you're not qualified to write .Net Core benchmarks. I don't know Iris, but AspNet Core MVC includes SO much functionality. It's not a barebones benchmark by any measure. Yet you use none of it. Also, you don't know TechEmpower which reveals you're not at home in the benchmarking space. Tl;dr: You and OP are full of sh*t. 
&gt;Someone recommended CQRS and DDD. I think that is probably the worst idea for you in your situation. I'm currently re-building a large platform application using CQRS, DDD, and it is event sourced and I LOVE IT and I think it is the best long term strategy for a software company. However, I would NOT recommend this to you in your current situation. All 3 disciplines have a significant learning curve to be done right That was me, and while I agree with your point, let me explain my thought process a bit more. OP and his company are at an inflection point right now. There is no path forward unless the dev team learns to build apps in a different way then they've been approaching it up until now, and the company realizes that regardless of what their main business is, this technology is an important component of the business and needs the right focus and resources. Yes, both of these approaches are a significant learning curve and they are far more suited to a software company than a CRUD based LOB app. It sounds like this app is far beyond that stage and necessitates far more advanced architectural approaches in order to achieve management's goals. &gt;Consider the introduction of a message bus. The more asynchronous your system is, the better it will perform and scale. If something doesn't need an immediate response WITH data, then it's perfect candidate for queuing which will allow the system to process as it finds time which means it can free up more connections at the head. It's also a great indicator for when it's time to scale up again. When the queues are backed up, you need more workers. Simple. The implementation can be a bit confusing at first though, so we use NServiceBus to help abstract away all the details of managing the bus. I don't disagree with you here either, but I'd argue that message busses bring along just as much of a learning curve and are just as suited for a technology company as CQRS and DDD are. &gt;Someone suggested that it simply means you have different classes for reading and writing data which is incorrect. You have separate PROCESSES to handle each so that you can scale each independently which is the whole point. I didn't mean to suggest that it was that simple, but I specifically kept my explanation short. The different processes are very much the whole point of CQRS so that you can scale out the read side independently of the write side. All in all, I agree with the rest of your suggestions. OP is in a tough situation for sure, and unless the business changes their approach, then I'm not sure how much can really be done here.
No.
First things first, you're certainly got multiple issues at hand and I'm trying not to give an overly complicated and overwhelming answer. Each of the bullet points below represents additional complexity and, as has been pointed out by /u/ThereKanBOnly1, a year isn't really realistic. I imagine you're looking at staring over again with a fresh codebase and copying business logic out as needed, as /u/ThereKanBOnly1 suggests. I know the notion of a rewrite is contrary to all the other advice you're getting, and I agree with other posters that it's not compatible with the timeframe you're siting and it would be a massively tough sell to management. But in my opinion trying to refactor into MVC is a half measure and at the end of the day you're just ending up with polished turds. * Yes, you want to move on from webforms. * MVC is probably passable, but if I were you I'd be looking at going the single page application route with a web api backend. There's a million javascript frameworks out there for doing the front end work, Angular is what I like (https://angular.io). I know /u/ThereKanBOnly1 is giving the opposite advice, but this is the route I'd go, especially if you're already going to be building web apis. * AWS or Azure is certainly a good idea too. Queues and load balancers everywhere. My limited experience with AWS and outdated experience with Azure says that Azure is probably a little simpler. Consider looking at Microsofts Service Fabric as a bit of a shortcut with getting scalable apps into the cloud. * Give some thought to your caching strategy too. Distributed caches are desirable but simple in-memory, in-proc caching is almost as good. * As you've already mentioned with MVC, you'll want to make sure you're going stateless. This will be a little more straightforward if you're already going SPA. * Don't assume you need a SQL database, particularly if you're following the new hotness that is microservices. Document databases will probably do the job just fine. * Don't sweat your job, the market is strongly in our favor :) You could find a new one in a few weeks. 
* As you've mentioned, it's more lightweight and has better performance. * The lighter weight/better performance means you may be able to use smaller, cheaper virtual machines if you move to the cloud. This translates into less expensive hosting. * Faster startup times lend it better to cloud scenarios where you want autoscaling and/or "serverless" architectures. * True cross platform support means you can use cheaper Linux VM's if you go to something like AWS. * True cross platform support is also nice if you have devs that like using MacBooks :&gt; * The modular and totally self contained deployments make it very Docker (or other container systems) friendly, which makes it yet again more cloud friendly. The one knock on going to .NET Core is you'll have to look at your technical needs and make certain you can meet them, since there are feature gaps between the framework and available 3rd party nuget packages.
Roslyn is the .NET compiler. Unless you're making some really low level tooling - e.g. something like PostSharp - it's not something you use directly. 
No. Look up "aspnet core" and "web api".
VB's biggest sin was being easy to learn. It let non-developers who had no business writing software write software.
I have no advice for you; I actually just wanted to comment and say "thanks" for introducing this concept of mutation testing to me. Can't say I know of any .NET tooling that serves this function.
I'd suggest you look at something like Okta or Auth0 and determine if standing up your own identity server is something you want to take on. We're in the process of implementing Oath2 with IdentityServer3, and it's definitely not been trivial.
I develop a 10+ year old web forms app and you are in for a rocky ride. Before you do anything, document the shit out of what you currently have in place. That means the hardware, the business rules, the domain architecture, everything. Only then will you fully understand the size of the beast. It is achievable but you will be eating the elephant one bite at a time.
Only a couple of your points I'm somewhat hesitant to agree on, so I'd be happy if you'd like to offer any further enlightenment: SPA - I don't really understand the SPA movement. I've spent all my career in government and healthcare, so all I know are applications with dozens if not hundreds of different pages, so I have problems wrapping my head around using a SPA for anything other than much, much simpler web applications than what I'm used to. Or perhaps I'm off base and what people are calling SPA's nowaday are little more than newer ways of having a "master page" and a ton of different "content pages"? Document databases - I've been kind of under the impression lately that document databases were starting to lose their "hotness". This is probably confirmation bias since I'm relatively skilled in relational databases, but I've read about a few companies lately (Etsy comes to mind) that abandoned and went back to relational. Also, again, I'm used to dealing with large, complex databases and most of my experience is with relational, so I fully admit my predisposition there. 
FileResult is a base class, you need to use FileContentResult, FilePathResult or FileStreamResult depending on what you're trying to send to the client. Also, don't do an AJAX call, just redirect to the target path. FileResult sets the Content-Disposition header so your browser will prompt for a download instead of pulling the document up.
Projection. He constantly uses sockpuppets, so he imagines that because you oppose him you must also be a sockpuppet of a guy who opposes him (or, rather, reveals his lies) often.
Don't convert. Get servers, Azure, AWS, whatever. Find what is slow... Too many db calls? Datatable traversals? Make a top 5 of really slow but IMPORTANT features. Fix em...The "right" way. Make another top 5. Rinse repeat. 
I agree that message buses can be complex which is why I only said to consider it :) I think the hardest part is thinking in those terms, but the implementation is pretty straight forward, particularly if you use something like NSB since it manages all of that for you. We sit that on top of Azure Service Bus where ASB is just the transport and NSB is what manages it. I think that was the easiest part of this whole ordeal. I'd still argue that DDD and CQRS are far harder to adjust to, however. We didn't go full HAM on DDD, only the basics (aggregates/roots, value objects, and domain services) that fit perfectly with the command model. Using each type properly is frustrating some people as well as leaving out technical details from the domain. With CQRS, the concurrent nature of denormalization is murdering brains. Then on top of that, because we're event source, some folks are really struggling with the difference between system events and domain events. We're getting people trained, but I think the message bus has been the simplest of all the pieces :) I do not envy his position. To be quite honest, I tend to bounce out when the management starts to work against themselves. On to better more interesting things, but that's just me.
There's really no way of doing this easily. Since your so heavily invested with webforms, my recommendation would be a two phase approach. Squeeze out as much as performance in the application that can be done as possible. Use transactions, switch from ef to dapper, use http compression on iis, etc. Finally, port what can be done easily and any new development to spa with a web api architecture. 
At what point do tuples make code harder to read and maintain? Don't get me wrong, I see their immense usefulness. Maybe tuples are just a good replacement for anonymous objects.
I know you probably want to remain private, but can you tell us what industry this is? You say it handles virtually every part of the business and from the #s this is a very large business. This sounds kinda crazy IMO. Are there any commercial packages in the enterprise? HR, FI, CRM, etc?
Holy jeez. Thank you both for going so deep into this publicly. I'm a baby dev, and I can't grasp the full scope of what you are discussing. Still an engrossing read, and it helps me and my tiny apps a ton.
They're not that much different. IMO, the best resource to start is pluralsight's mvc fundamentals course with Scott Allen. Then I'd read the Pro MVC 5 Book by Adam Freeman.
Something to consider is that you do go the js framework approach with an API, you can use the same API for desktop apps with very minimal changes (if any). And if you build a desktop app with electron, you can pretty much just use your same angular/react code...
+1 to Pro MVC5 by Adam Freeman 
What book do you have on Core?
On Udemy, the MVC 5 course by Mosh Hamedani is absolutely great!
Pst... It's the same guy. Also the creator of the Go framework he so kindly suggests in his article, but doesn't divulge his affiliation.
I took this course. mosh is such a great teacher. 
Awful examples of tuples. Better use a class for cases like this.
He always looks positively happy to teach too. 
Moving it to MVC and making it asynchronous could help in reducing the load on servers, but not so dramatically for 40k users. There needs to be some investment from the management in terms of new development and servers. I would say give them 2 options - to do a proof of concept by scaling present webforms across several servers. And another option with details on mvc, team size and all the risks associated with it and let them choose it. I'd say just state the facts, give them options and involve the management in the decision making and make them take part in the risks as well. You don't have to carry everything on your shoulders.
There are some very, very, good VB.NET developers (way better than me), but there are far far more terrible ones. I think its due to so many coming from VB6 and being used to declaring everything in the code behind (aspx.vb) files of forms. They also never really had an interest in OO principles (or even MVVP) since in their mind "this works fine". Basically they're a bit stuck in the past. Finally and most importantly : I actually love the VB.NET syntax and have no issues with the language (in some small ways I prefer it to C#). But the market has spoken and if your recruiting people you need to move to a more popular language. You cant stick with VB and complain you cant find good people.
If I am following correctly you would need to reactor the functionality from the exe into a different project (dll) that could then be referenced. Main app (exe) Model project (dll) Service layer (dll) Then you would be able to reference both the service layer and model from the main app and reference the model in the service layer.
&gt; Then I'd read the Pro MVC 5 Book by Adam Freeman. This is the one I read when transitioning to MVC 5 and I'd *highly* recommend it. By the time I was done with that book it was off to the races.
Production mode and build configuration are different. You still need optimized bits by building in Release. 
[Building Web Applications with Visual Studio 2017: Using .NET Core and Modern JavaScript Frameworks](https://www.amazon.ca/dp/1484224779/ref=cm_sw_r_cp_apa_UFBMzbZ042X3F) Have not read it yet. Just got it the other day. 
I'm curious what your age has to do with it?
Looks like a good course - but $200 for just one course, and I have my Pluralsight subscription for $250/ yr. On Pluralsight, lots of ASP.NET Core courses, but no clear path for MVC 5 etc.
With that course on Pluralsight, it seems you already need a working knowledge of MVC.
On Udemy, I got it for $15. Have to catch a discount. 
Wouldn't it be easier to generate a project file and run restore on that to get the right result?
You could extract the classes out into a common library shared and referenced by all, but with no references of its own. Ie. shopping cart and catalogue both need the product class, so you extract it to the models (or whatever name you like) library which you reference from both cart and catalogue. Makes sense? As an example, much of aspnetcore is built with references to *.Abstractions libraries which define common interfaces.
You can get 90% off by joining mail list from his [website](http://programmingwithmosh.com/join/) or by directly choosing [course](http://programmingwithmosh.com/courses/) you want. There is also his amazing 3 part series for mvc 5 on [pluralsight](https://app.pluralsight.com/library/search?q=Mosh+Hamedani). ~~You can also get free subscription for pluralsight if you are student or by making account on [visual studio online](https://www.visualstudio.com/vso/) website.~~ EDIT: Havent noticed you already mentioned having pluralsight subscription.
No, not at all. There will be stuff you don't know but that's why I suggest reading the Adam Freeman book after to clear up any questions you have.
My god, I keep seeing people recommend JS libraries I've never heard of, so adding this one to that pile. You can make a desktop app from js...? &lt;mind blown&gt; Am I the only one that ever wonders if the industry tries *too* hard to use js for everything? Nonetheless, an interesting approach to a problem I suppose. 
Healthcare industry, and we have hardly any commercial packages. The company has had them in the past, and has trouble getting past the 'reinvent the wheel' mentality. The first time a report or a couple of features they wanted but couldn't have come around, they'd want to develop an in-house solution and get rid of the commercial package. Fast forward a decade, it's enormous and covers almost every aspect of their business. And yes, I've never ceased to be amazed at how much we manage to get done with a small team and a small budget, however that has come at the cost of massive technical debt and scalability. 
Take a deep breath, lol. Right now there are three main frontend js frameworks: Angular 2+, React, and Vue. Angular and react are considerably more popular. React's learning curve is fairly low but it has a lot of "magic that just works". Angular with typescript makes more sense and feels more "normal" if you are a dotnet developer, in my opinion. Vue is a mix of the two, which is why it's gaining popularity really quick. In all honesty, you can't go wrong with any of them. Now when it comes to electron, it's basically a wrapper that allows you to use any of the above frameworks (and any other web tech) as desktop apps. It's also pretty cool when you consider that you only have to make one version of an electron app and it will work for all platforms (macOS, windows, Linux). So what you can do in the following: have all your backend stuff run on webapi, then your frontend can be a js framework that is used in the web browser and in a desktop app (using electron). You can also use these frameworks for mobile apps, but in my opinion, it's worth going native.
Thanks for the clarification, I'll get started on this course then tonight.
Huh, that is a really interesting approach. Let me ask a follow-up since it sounds like you've at least dabbled or used electron. If you wanted to make an app that had offline capabilities, but synced with web API's when there was an internet connection - would electron also give you that capability? Just intuitively I would think since it's a layer to js libraries there wouldn't be powerful abilities to store data offline and then sync, but I thought it worth asking. I would think at that point you're just back to native libraries?
You can persist data in embedded db's in electron. You can also use html5 localstorage. When connected to the API again, you then force your app to sync up.
Basically take the Scott Allen mvc 5 course as a quick dive in to get familiar, then the Adam freeman book will tell you everything you need to know (outside of auth, which is its own monster).
This course helped me get my current job as a junior!
Get yourself a copy of Working Effectively With Legacy Code by Michael Feathers. It details how to break up legacy software in as safe a manner as possible by identifying seams and being able to sprout off classes while adding retrospective unit tests. Book has saved my skin too many times to count.
The size and complexity of the app is a bit irrelevant in terms of "do I go with a SPA or not?". Any of the popular frameworks provide ways to decompose your application into modules and reusable widgets. If anything, I would argue that they push you to better decompose your applications, where as MVC (and other server side rendered) practically encourage monolithic architectures like the one you seem to have now. SPA's are not going to be related to any master/content page concepts from ASP/MVC. A single page app is an app where the state is completely contained in the client (browser) and navigation through the app is in the form of API calls and DOM manipulation rather than following links to other, separate pages (hence the "single" page part). SPA pros: * They generally dovetail pretty smoothly with RESTful APIs. * They result in a more responsive user experience. * Payloads from a API call are going to be much smaller and simpler for the server than server rendered HTML. This makes it much more suitable for bandwidth constrained scenarios, such as mobile devices. It also means lower overhead for your servers. * They pretty much force you into stateless application layers. SPA cons: * You've got new technology to learn. I think it's fair to say that it would be a sizable shift for you and your devs. * It's a foregone conclusion that you'll be putting more work into your front end code. Probably more work in your server side code as well, depending on how much you're relying on things like session state. In short; If you have a lot of work to get done and don't need the advantages of an SPA, then by all means stick with MVC. However, outside of taking a bit more work to build I think it's fair to say that they are otherwise better on all fronts and are a no-brainer for sites with very high volumes of traffic. As far as the document db's go, my views somewhat parallel that of SPAs vs MVC, and I think that the desire to stick with them is a case of "everything looks like a nail when all you have is a hammer". Lumping all your data into one big transactional database is antithetical to horizontal scaling. Yes, you can try to throw ever more powerful hardware at the problem, but there's definitely limits on how far that'll take you. I'm not sure where your impression of the loss of "hotness" is coming from, but I would definitely disagree. If anything, they're going to become more common as people adopt microservice architectures and/or migrate to the cloud. I know that it's possible to wring quite a bit out of a relational db when you really know the in's and out's of them, but to me that expertise is just compensating for SQL databases being a performance minefield to begin with. I'm not saying that there's no place at all for SQL databases, but I think the majority of use cases are better served with document dbs.
I don't see why that would be a circular reference? Exe -&gt; DLL1 (FormA) -&gt; DLL2 (ClassB) It would be a circular reference if your DLL2 had a dependency on DLL1 or your Exe: Exe -&gt; DLL1 (FormA) -&gt; DLL2 (ClassB) -&gt; DLL1 (ClassA) Your Exe can have a reference to DLL1 and DLL2, and DLL1 can also reference DLL2 with no problems.
I don't see your test project's AssemblyInfo. Does IntelliSense work for the public members and is just not working for internals?
As far as I know the Test project doesn't require a AssemblyInfo (Note: This is .net Standard library, not .net Framework). And you might be onto something, public members are not showing up in Intellisense either. 
I found a temporary fix by disabling R#. It seems to be the culprit. Does anyone know how I would go about configuring R# to work with InternalsIvisibleTo()?
This is pretty straightforward. In short, you need a third assembly (DLL) that contains the common stuff. The third assembly is referenced by both your .EXE and the other DLL. * MyApp.exe * MyApp.Whatever.dll * MyApp.Common.dll This is a pretty common (groan) pattern. Sometimes '.Commom' is '.Core' or '.Shared' or something semantically equivalent.
The only straightforward thing about this is the naming of the DLLs. Then the hard work begins. I can guarantee you the old EXE and DLL is a spaghetti of a mess and untangling to create meaningful separation of conserns takes months.
Is it possible to generate a project file off of a single .dll file? If so, how exactly would I do that?
Before automating create a .proj file manually (copy paste from existing). Remove all files listed and list packages as PackageReference and restore it. Make sure to set the correct TargetFramework. I'm away from a PC for a few days so can't give you an example for a few days. If you have a hard time just create a new project on Visual Studio, remove all .cs files and add the NuGet packages.
auth is most definitely it's own monster!
Thanks for the link, I picked it up for $25 :)
He has a good teaching style. Talks at a rapid, clear pace and seems to be genuinely in a good mood while delivering the material.
We are using that technique in dotnet new for install. See the code at https://github.com/dotnet/templating/blob/59ea5050650f0928e557d5fdaa80e894448ec671/src/Microsoft.TemplateEngine.Cli/Installer.cs. 
Hey, glad you solved the problem. Just in case it helps anyone else. The asp-for attribute is a "Tag Helper", added in .NET Core. It renders the appropriate input tag, with all the relevant attributes (including name). It does need the property to exist on the Model as you suspected (else it won't render anything). Form Tag Helpers also carry the benefit of automatically generating the validation token that helps prevent CSRF attacks. Inspired by your question, I put together a quick blog post on the subject if it's of any use to you... https://jonhilton.net/2017/08/17/how-to-get-data-from-an-html-form-to-your-asp.net-mvc-core-controller/ 
If I recall correctly, VB didn't even have classes until v4.
https://resharper-plugins.jetbrains.com/packages/ReSharper.InternalsVisibleTo/
Two things I'd ask you to expand on, as you seem to have more experience than me implementing these things... &gt;Using each type properly is frustrating some people as well as leaving out technical details from the domain Can you explain this a little further. Do you mean that some devs are not relying on DTOs as outputs from the query side, and instead relying a bit to much on the full domain models rather than the trimmed down DTOs that are tailored for that specific query? &gt;With CQRS, the concurrent nature of denormalization is murdering brains This is something that I'm still wrapping my head around because I don't necessarily think that denormalizing a query is a requirement for the read side, although it makes it far more scalable and also quicker. Can you expand further on how you're going about the denormalization process and how that factors in how you're designing the datastore on the query side?
&gt; You're wrong, Iris has the biggest community around it, in the Go world. Numbers please? Fortunately, I do have some https://github.com/labstack/echo - 87 contributors https://github.com/kataras/iris - 8 contributors But since we know that stars are more relevant to you https://github.com/labstack/echo - 8040 stars https://github.com/kataras/iris - 7354 stars &gt; listen us, the 98% of Go community and continue like this, don't stop In the same time, most of the posts from /u/rdmin /u/iris-go /u/gmgolang have more detractors than supporters. &gt; People like @dslniper stack in the past and spamming hate with fake accounts. Dixit the guy who has 3 accounts 
This post would be a good example for /r/schizophrenia
**Here's a sneak peek of /r/schizophrenia using the [top posts](https://np.reddit.com/r/schizophrenia/top/?sort=top&amp;t=year) of the year!** \#1: [When the hallucinations get bad](https://gfycat.com/MellowFatIndianpangolin) | [21 comments](https://np.reddit.com/r/schizophrenia/comments/6eaind/when_the_hallucinations_get_bad/) \#2: [I drew the demons I would see when things were at their worst. I called them dementors](https://i.redd.it/td4r9jqbmlxx.jpg) | [24 comments](https://np.reddit.com/r/schizophrenia/comments/5cwask/i_drew_the_demons_i_would_see_when_things_were_at/) \#3: [I finally tried to capture a hallucination in a photo. I've been trying to get a photo of him for weeks, but this time he posed because he saw me get out my camera. As you can see, he's not there anymore. At least I know he's not real.](https://i.redd.it/mzjyv9pt8izy.jpg) | [17 comments](https://np.reddit.com/r/schizophrenia/comments/6d4nar/i_finally_tried_to_capture_a_hallucination_in_a/) ---- ^^I'm ^^a ^^bot, ^^beep ^^boop ^^| ^^Downvote ^^to ^^remove ^^| [^^Contact ^^me](https://www.reddit.com/message/compose/?to=sneakpeekbot) ^^| [^^Info](https://np.reddit.com/r/sneakpeekbot/) ^^| [^^Opt-out](https://np.reddit.com/r/sneakpeekbot/comments/6l7i0m/blacklist/)
As others have mentioned, dependencies are just a noncyclical tree - so this is valid: EXE references dll1 EXE references dll2 dll1 references dll2 If you can't do that for whatever reason (ie dll2 needs access to a class in dll1 as well), then revisit how you've split stuff up. If you're not wanting to do that, you can get around it by putting interfaces somewhere (dll2?), tacking the interfaces onto your relevant services, and then using dependency injection to get at them. It's a bit of a misuse of DI though, so it's not a particularly good option.
Based solely on your brief overview, my recommendation would be on large to stick with the existing system and webforms for now. MVC's great, but it's not a panacea. Regardless, if management is "surprised" at the cost of what sounds like an essential business tool, you're unlikely to get the buy in (staff, hours or money) that's actually going to be necessary to rebuild it on a new tech stack. In the same situation, my 1 year plan would be: * Update the infrastructure to handle the application's reliability needs - AWS or Azure are good choices. This will take time, as will rejigging the infrastructure to get high availability/redundancy. * Focus on bringing the running costs down/increasing performance - Update the existing webforms app, or if appropriate rebuild a tiny subset of historically problematic functionality in a separate web tech (MVC if you want, though WebAPI may be easier to layer on top of existing functionality). * Focus on making development faster - Improve whatever internal processes you can, implement continuous integration, continuous deployment and increase test coverage (whatever's appropriate to your situation). * Tidy and refactor the main codebase with the goal of simplifying a future migration to MVC or some other technology. 4 people for a year is a decent amount of time, but if you're sitting on a huge and complex legacy codebase written for a specific technology, and you don't have internal experience with these sorts of projects, doing anything more than making multiple small iterative steps towards where you want to be in another ~5 years is likely a mistake.
Underrated by who? I use this all the time
Me too, but I very rarely see it in other people's code.
Also, `Array&lt;T&gt;` implements `IReadOnlyList&lt;T&gt;`. **This** is an underrated interface!
 public IEnumerable&lt;string&gt; GetFourSpecificNames() =&gt; new[] { "Martha", "Rory", "Bill", "Rose" }; // Which I think looks better than this: public IEnumerable&lt;string&gt; GetFourSpecificNames() { var list = new List&lt;string&gt;(); list.Add("Amy"); list.Add("Rory"); list.Add("Bill"); list.Add("Rose"); return list; } Comparing vastly different syntax styles is kinda silly, isn't it? Either compare this: public IEnumerable&lt;string&gt; GetFourSpecificNames() { var array = new string[4]; array[0] = "Amy"; array[1] = "Rory"; array[2] = "Bill"; array[3] = "Rose"; return array; } public IEnumerable&lt;string&gt; GetFourSpecificNames() { var list = new List&lt;string&gt;(); list.Add("Amy"); list.Add("Rory"); list.Add("Bill"); list.Add("Rose"); return list; } Or this: public IEnumerable&lt;string&gt; GetFourSpecificNames() =&gt; new[] { "Martha", "Rory", "Bill", "Rose" }; public IEnumerable&lt;string&gt; GetFourSpecificNames() =&gt; new List&lt;string&gt; { "Martha", "Rory", "Bill", "Rose" }; And suddenly the difference is not so big anymore. 
Yes. I made a mistake here. I should, and do, know better
And now your post says that you think public IEnumerable&lt;string&gt; GetFourSpecificNames() =&gt; new[] { "Martha", "Rory", "Bill", "Rose" }; Is "more readable and less fuss" than public IEnumerable&lt;string&gt; GetFourSpecificNames() =&gt; new List&lt;string&gt; { "Martha", "Rory", "Bill", "Rose" }; Really?
You can use similar syntax with any class that implements IEnumerable and has an Add method. Even your own custom classes! See [collection initializers](https://docs.microsoft.com/en-us/dotnet/csharp/programming-guide/classes-and-structs/object-and-collection-initializers#collection-initializers) 
- The class does not need to implement IEnumerable, the Add method is enough. - The class does not has to have an Add method either, you can also use an extension method.
Why are you always creating an array, just to create a list from it and throw the array away? Why not just create a list directly?
Good question. I can honestly not explain the reasoning behind it. Looking at it now, I have no idea why I did it that way. 
.NET core has no UI story beyond Universal Windows Apps and Web (Asp.Net Core). So if your project still require a windows native application (Forms/WPF) then stick with .NET fullstack. However for web api services it's fast, very fast, much faster than node.js for sever side implementations. It's also cross platform so you're not tied to any particular platform if you'd like to migrate or share your code among other platforms. So to management the sell is it's faster reducing hardware costs, cross platform so you're not tied to a vendor for all your needs. Also it's not a question if .NET Core is the future of .NET but when, migrating sooner than later will reduce migration costs down the line. 
After working a year with .NET core my only real slowdowns was availability of, ironically, Microsoft SDKs/Libraries that were .NET core compatible. For instance OData.NET was not .NET core compatible (it is now, but haven't tried it) and neither was the Dynamics 365 SDK. But since OData is just a rest end point I just wrote a simple http client to perform OData operations against the 365 Odata endpoint API. Also a lot of the Azure SDKs are not .NET core compatible. Also UserPassword pass-through isn't available in the .Net Standard version of the ADAL library which made auth against an Online 365 instance tricky. 
We had to access LDAP and they didn't have a lib until some 3rd part wrote one (which M$ references as the new standard so it's all good). We also ran into tooling with consuming a SOAP endpoint and auto-gening the classes. The tooling seems to be in place now. We also ran into trouble trying to get Sonar Qube attached to our build process so we could analyse our technical debt. This is just a short list. We ran into maybe 10 items we have had to work through, wait for fixes or work around. If I was to go back in time and chose the tech again.. *I'd still chose dotnet core*. The only point I was making is that just because core exists doesn't mean you should use core. There is still an expense with choosing the technology and you should only chose the tech if you actually get viable benefits which there are few short of 'OS agnostic' which is why I said I'm against using Core if you are not aiming for 'os agnostic'.
&gt; Can you explain this a little further. Do you mean that some devs are not relying on DTOs as outputs from the query side, and instead relying a bit to much on the full domain models rather than the trimmed down DTOs that are tailored for that specific query? Other than the denormalizers, the Query model is the most familiar part of the practice, so using DTOs is not an issue. The Query model has no access to domain models anyway, so they can't rely on them at all. The frustration come from wrapping one's head around the responsibility of the aggregate (enforcing invariants, generating events, etc.), how and when to use value objects, understanding that an "aggregate root" is more of a concept than a physical model, what is and what is not a behavior that should be in the aggregate vs. a domain service, what's the difference between a service context and an actual domain service, and all the patterns you have to learn to help keep these concepts pure (i.e. abstracting all implementation details). Our aggregates have zero public properties. Everything is a state capture (DTO). Folks are still adjusting to that. It's just a very specific way of doing things when average engineers are more used to simply solving the problem in the quickest way possible -- a habit that my shop is committed to breaking for large projects. Our domain has no implementation details. To accomplish this, we had to do some level of onion architecture. Abstract all the interfaces to an interface library and have the domain reference it as well as the implementation libraries. That way the domain and implementations know about the same interfaces but don't know about each other. This was confusing to many, but that's DDD; it keeps the domain clear and focused on the domain. The other challenge is modeling the domain like a domain. That is, it should be modeled in the same way that a BA would describe the business process. So for example, if a mailman places an envelope in the post box, it should be modeled that way in the code: mailmanEntity.PlaceMail(mailEntity, postboxEntity); eventStore.Save(mailmanEntity, mailEntity, postboxEntity); So the implementation of the PlaceMail() method on the aggregate (aggregate root in this case) would mediate the insertion of the mail object into the postbox object (acting as the transactional boundary and enforcing invariants) -- because the postman, the piece of mail, and the postbox all need to generate events about this insertion. The call to PlaceMail() would be placed in either a domain service or the service context itself. All of this is not the way typical engineers think. Typically, it would look like `database.Execute(newMailQuery);` Basically take the most direct route to make something look correct in the database -- but that is not DDD; that is data driven, not domain driven. So this way of thinking is a challenge for many and the stubbornness to accept the practice is real; for many, it seems like a waste of brain and development time. So even teams that have committed to the practice have a hard time making the shift when it's new to them. &gt; Can you expand further on how you're going about the denormalization process and how that factors in how you're designing the datastore on the query side? I think this is more specific to event sourcing, but I'd recommend it even if you're not event sourcing. But basically, I've written a base class that just needs to be implemented for each "projection". It has methods that know what to do with each individual *event* that comes in on the event stream. Every operation is written to be idempotent because the reality is that you have multiple instances of the same denormalizer processing events, potentially out of order. For the more order-sensitive projections, you can create a persistent subscription and it'll help ensure batches get processed by the same subscriber instance. The design of the data store(s) is pretty simple. We use MongoDB as our data warehouse which makes it really easy. Figure out what you want it to look like if it were a json object and model it that way. Done. The denormalizer will upsert all changes to that model as events are received -- hopefully in the right order. The same goes for any other projection or store, however they tend to be much smaller models. For example, the elastic store only stores searchable fields and the entity ID. Neo4j only stores entity IDs and relationships -- basically just an indexing service for complex (and undiscovered) relationships. We include bits of text here and there on each node just for exploration purposes. So we have 3 data stores' denormalizers listening to the same event stream and placing them into their respective stores: - MongoDB: our warehouse; complete data - ElasticSearch: a smaller, searchable model - Neo4j: an even smaller data model that's focused on relationships All have a different data structure and serve a specialized purpose. Each entity or specialized projection has it's own denormalizer. The actual query model itself is broken into services from which the API will query. Those services get its data from one or more data stores. We don't use the repository pattern here because it doesn't make any sense to since the Query model can only query and the command model doesn't even have a concept of a database let alone CRUD; it only looks things up aggregates by ID or saves them. So the query model really is a thin data access layer. http://i.imgur.com/JCvEHj8.png 
Some of these features are rather mess ... :-/
Serious question: How are we at C# 8 already? Does anybody use anything past 4.5 in prod? Are people constantly upgrading old projects?
Please do not listen to this advice. Using MVC does not mean you can't use Web API at the same time. Also, using server-side processing does not mean you can't use some decent lightweight javascript frameworks for things like data binding, such as knockout. Creating two completely separate applications built in two completely different ways just doesn't make sense here. Furthermore, I imagine a lot of this "intertwined" fields with business logic really is only a result of your web forms architecture, and if you were to rewrite it in MVC, you might be pleasantly surprised by how naturally you can separate things out.
This is such dangerous advice, and the exact reason my dev team decided to try this. It sounds so great in theory. You are decoupling your user interface from backend code, all view logic is no longer burden on the backend. SPA's give a desktop app experience. TypeScript makes programming on the frontend just like it is on the backend, etc... But what are you actually doing here? If one dev team tries to take this approach with a single application, you end up with two completely separate applications that have to be maintained, with completely different tooling, building, deployment, etc. Furthermore, each use case of the app involves two separate applications, with vastly different architectures. Your front end application is tightly coupled to the backend api, so any feature or use case changes in either application requires changes in both applications.. You are kidding yourself if you think the web app is just the V in MVC. What ends up happening is you have your business logic split between two applications. The front-end has it's own models in TypeScript representing the same models in the api. At a minimum, validation rules have to be duplicated on both ends. Each use case is a business problem, and involves business logic, so the UI is actually driving each use case and has to have business knowledge to do so, but then you are still treating the backend like a full DDD application that also contains all business logic. SPA's should be created for apps that are truly client-centric, and only use the backend for data. It does not work well when you take a traditional complicated business application that doesn't need a gmail-like user interactivity and try to split the front-end out into an SPA.
4.5 is the .net framework, not the language 
Oh. How does one change which version of the language they are using? I've never seen an option for that in VS.
In the advanced build settings of a project. The default value basically lets you use whatever the compiler supports regardless of target framework and runtime version, which is why it's a relatively obscure setting.
60 points. No comments. I guess I have to actually read the article now...
I didn't comment because I don't even know where to start in pretending I understand what they are doing.
Don't worry. No one ever does until they're forced into a scenario where they HAVE to understand... and that's because shit is hitting the fan, lol.
Are you guys governed by HIPAA? This sounds insane.. 
The flip side of that is libraries suddenly (or gradually, for that matter) being deprecated. In some cases, breaking versions of one dependency or another are released. I'll give an example. I'm developing a web app with Rails. I started the project with 5.1.1, but have since upgraded to 5.1.3. With that upgrade, however, some elements of Bootstrap no longer work. I've googled around, and a lot of other people are having this problem. While this will probably be solved in due time by the Bootstrap team, in the meantime I am forced to go back to the original version of Rails, or to use custom css instead. That's an easy choice, rolling back to 5.1.1 is easy, but I've seen cases which made life much harder for the developer. If it's an enterprise library, this is obviously much less of an issue, but for those of us who play in OSS land, this is a big concern.
I agree with the user management, absolutely. In a Rails app, I will pick up Devise every time. It's battle-tested, robust and just works in an idiomatic way, so why build up an auth system myself?
I'm weirded out by the idea of implicit cast operators getting extension-everything'd into something. Maybe I can just write a Roslyn diagnostic to error out on that kind of crap, though... it's not like that would be all that difficult of an opt-out.
&gt;In the advanced build settings of a project. The default value basically lets you use whatever the compiler supports regardless of target framework and runtime version, which is why it's a relatively obscure setting. C# 7.1 features like async Main and bare "default" were opt-in for me in VS2017 15.3.
I virtually never allocate a list or an array by hand: public IEnumerable&lt;string&gt; GetFourSpecificNames() { yield return "Amy"; yield return "Rory"; yield return "Bill"; yield return "Rose"; }
The article talks about GC implementation with no memory release. My guess this is useful for functional programming style with static objects/extensions.
This is awesome! * No more breaking interfaces and all the implementing classes when adding new methods with shared/same functionality. * Reduction in null pointer exceptions (having been in Haskell land for a short time, it's pretty nice not having to worry about null) * A obvious and familiar extension method (and property) syntax I don't have enough experience with async/await and streams though. So I'd probably leave that feature untouched for awhile. 
Thank you very much for going into all that detail. Its those kinds of insights from working with these things in the real world that are really had to come by from reading books and watching talks. Sometimes it just doesn't click until there's a context around it. I'm still ramping up myself, and the small little samples that I've put into place just aren't running into the same kinds of restrictions and parameters that I'd run into on an actual project. &gt;The frustration come from wrapping one's head around the responsibility of the aggregate (enforcing invariants, generating events, etc.), how and when to use value objects, understanding that an "aggregate root" is more of a concept than a physical model, what is and what is not a behavior that should be in the aggregate vs. a domain service, what's the difference between a service context and an actual domain service, and all the patterns you have to learn to help keep these concepts pure (i.e. abstracting all implementation details). I'm going to have to spend quite a bit of time with that paragraph. I'm still at the level where I'm probably making some of the same mistakes your devs are, but I've been getting lost in a lot of the terminology. This gives me some key issues to dive into and also frames some questions I should be asking as I work through the concepts. &gt;The other challenge is modeling the domain like a domain. That is, it should be modeled in the same way that a BA would describe the business process. It's very challenging, but also leads to code that is far more readable as well as communicates more about the underlying business process. I've found its easier to do when what's being modeled is at a higher level, and much more of a challenge when its at a lower level and has a lot more parallels with a simple CRUD operation. Thinking about it now, I guess its really about picking the right aggregate that's handling that business process and making sure that its really the right place for that logic. The details on how you've structured the read side of the model are great. Thank you. The diagram makes it much clearer how you're using those projections to take the events raised by the read side and push them into the respective data stores. I've been looking outside of the RDBMS world for some other purposes, but have come across neo4j and found that its a much better fit for both structuring and navigating highly interconnected data sets. The only thing that's not immediately evident from the diagram is what determines what read model is used for a given request. I assume that each query is associated with the model that is best suited to fulfill that query, but I guess I'm just wondering how that's structured. Thanks again for the response, and its clear I have some more digging to do to get to the bottom of these concepts. 
Point-releases will be on an opt in basis
After replacing resources in dnSpy, my exe fails at startup...
After I replace the resource and save the module, exe crashes on startup. I ctl-Z to get the original resource back and save module again and it works fine now.
Serious question: why is this guy getting downvoted? This is a common point of confusion. I'm glad we were able to help get it cleared up for him.
True, but it's like they picked the worst feature from the Java 7* GC (the Permanent Generation) and wrote a .NET GC that did only that. *The Permanent Generation went away in Java 8.
Because nowadays being modern requires us to solve problems we just invented on the spot. Because God forbid if we don't release something at least twice a year we aren't agile, falling behind and the platform is obviously dying.
Functional programming uses the heap just fine :-)
I don't mean to sound like a Wikipedia editor... but citation needed? Oh, and for reference, the [documentation](https://msdn.microsoft.com/en-us/library/czz5hkty) says: &gt; Starting with the .NET Framework 2.0, the Array class implements the `System.Collections.Generic.IList&lt;T&gt;`, `System.Collections.Generic.ICollection&lt;T&gt;`, and `System.Collections.Generic.IEnumerable&lt;T&gt;` generic interfaces. The implementations are provided to arrays at run time, and as a result, the generic interfaces do not appear in the declaration syntax for the Array class. In addition, there are no reference topics for interface members that are accessible only by casting an array to the generic interface type (explicit interface implementations). The key thing to be aware of when you cast an array to one of these interfaces is that members which add, insert, or remove elements throw NotSupportedException. You can make a read-only array by calling [`Array.AsReadOnly`](https://msdn.microsoft.com/en-us/library/53kysx7b) on it, but that actually returns a `ReadOnlyCollection&lt;T&gt;`
&gt; The only thing that's not immediately evident from the diagram is what determines what read model is used for a given request. I assume that each query is associated with the model that is best suited to fulfill that query, but I guess I'm just wondering how that's structured. No data store in particular. As I mentioned, each store has a specialty and of course you're going to want to take the most direct route to the data, but you can either grab your data from one source, several sources, or use one source to lead you to another. What I mean is, you can for example use the graph database to gather relationships and then use the IDs returned to retrieve what you need out of the warehouse, instead of writing a profoundly complex query to do it all in MongoDB. One example we're proofing right now is to answer this request "Show me all text messages that went out last month that were responses to customer reviews specifically talking about dogs and ford trucks for any store within 20 miles of the store where Jeff was working, but only when he was assigned to a register." That would probably be an intensely complex query. But when you leverage all data sources, you'd query elastic to give you all IDs that matched the text search "dogs AND ford truck" and only take results with a relevance above a certain threshold, and then look at each of those entities in the graph database and use them to find the relationship of "(jeff)-[:assignedTo]-&gt;Register" and use those IDs to look up the full documents in MongoDB "where reviewId IN(ids)" and then grab the SMS property in each document and return that. Effectively creating your own polyglottal aggregate pipeline. Sound contrived? It's fancy, but not contrived imo. For other, more simple queries, you can go straight to source. Use Elastic if you're doing a text search and use the ID to pull the full or partial document from mongo. Or if the query is pretty straight forward, then go straight to mongo. For reports and aggregation, it just depends what you're actually doing. Same ideas apply. Use which ever method will eliminate the most candidates first and then widdle it down from there. The multiple stores just give you more options to get to the answer as quickly and accurately as possible. 
Stop it with the featuritis already! The language is getting too big! I don't want to upgrade everyone's compiler every three months! Work on improving the libraries instead, where with "improving" I mean, "furiously deprecating old cruft"! Edit -- case in point: now that we get default interface implementations, are we going to deprecate abstract classes, which now are 100% superfluous? What's that, can't deprecate languages features? Let's do some careful thinking then before adding features! In a GitHub pull request, yes, that will certainly do!
But nobody forces you to use old compiler lol
Not sure what you are asking for. Anyway from .NET 4.6, `Array&lt;T&gt;` also implements `IReadOnlyList&lt;T&gt;`. Now I'm on mobile but once at the office I can prepare a quick example. 
Citation Needed means I'd love to see a reference for it. Arrays all derive from `System.Array` (not `Array&lt;T&gt;`) and the current (i.e. Framework 4.7) docs for `Array` don't mention `IReadOnlyList&lt;T&gt;` at all. Although, apparently `List&lt;T&gt;` references `IReadOnlyList&lt;T&gt;` which is crazy because not all lists are read-only.
Like which one? And why? - `in`-arguments: Great, finally read-only arguments. - nullable reference types: This will have the biggest impact in C# for a long time, perhaps even bigger than async/await. It will make the code more expressive, reduce a lot of boilerplate code and make the code more robust. - `IAsyncEnumerable&lt;T&gt;` / foreach await: Long overdue too. Finally a possibility to cleanly combine yield and async. - The only arguable options are Default Interface Implementation and Extension everything - both are very useful additions.
I beg to differ. When you write `string[]` you are creating a `Array&lt;string&gt;`. It is true that not all lists are read only but it is also true that all lists have the behavior of a unmodifiable list of items that can be accessed either sequentially or by index. Remember that, unlike inheriting from a base class, implementing an interface does not translate into a "is a" relationship, rather into a "behaves like" ;) EDIT: my bad, there is no such a thing as `Array&lt;T&gt;`. On the other hand, `T[]` inherits from Array and implements, among other interfaces, also `IReadOnlyList&lt;T&gt;`. Here is a small fiddle that shows it: https://dotnetfiddle.net/Re3yKq
It's a base level GC you can implement your own over. Memory allocation is required, deallocation is not. Not that you should roll your own GC, bug you could. You could potentially write code where this was the optimal GC implementation, but it's a pretty big question mark as to whether there'd be a substantial improvement over the default.
Default interface implementation? Oh you mean abstract class? If I want multiple inheritance, I would write java.
Replacing the resource leads to the pointer being deleted, figuring out how to find the inserted images' address
No, I mean default interface implementation. Abstract classes are not a suitable replacement for them. Also they're unrelated to multiple inheritance.
&gt; Stop it with the featuritis already! The language is getting too big! You really don't want non-nullable reference types?
Yeah, don't deprecate jack. I don't want to have to rewrite old projects currently in maintenance mode just because. That's one of the reasons people stick to .net and java.
&gt; The language is getting too big! Are you saying they should stop adding features now, or that they already should have? If they already should have, what features don't you think shouldn't have been added?
I should have been clearer! I'm not advocating this exact approach to solve this guys problems - without proper analysis of the problems they are trying to solve and knowing more about the existing code base and domain knowledge, advocating anything would be wild guessing. All I'm doing is clarifying earlier statements about new tech choices and saying "here's a valid approach to modern SPA design, go have a look and see what you think." &gt; But what are you actually doing here? If one dev team tries to take this approach with a single application, you end up with two completely separate applications that have to be maintained, with completely different tooling, building, deployment, etc. Furthermore, each use case of the app involves two separate applications, with vastly different architectures. Your front end application is tightly coupled to the backend api, so any feature or use case changes in either application requires changes in both applications.. What you're doing here is de-coupling the user interface from the persistent storage and application logic. That's it. De-coupling always comes with trade-offs, and yes, duplication of validation and models/interfaces is definitely one in this case. However, if you stick to an MVC stack are things any better? You should never leak ORM models to the view layer, so any MVC app ends up with a layer of ViewModels anyway - and they need validation (markup at the least) as do the ORM models. Also, there are distinct benefits to segregating the application in this way. If you then need to focus on a different customer market and produce either a mobile application or desktop application, you have a common API you can use, ready to go. I disagree that the front-end is tightly coupled to the back-end. If you use any sort of API versioning and as much as possible allow for backwards compatibility, you can deploy different front-end UI's targeting different API endpoint versions with confidence. &gt;SPA's should be created for apps that are truly client-centric, and only use the backend for data. It does not work well when you take a traditional complicated business application that doesn't need a gmail-like user interactivity and try to split the front-end out into an SPA. Sorry, I have to disagree again. Any business case can benefit from a better user experience - doesn't matter what the field is. At the end of the day we write user interfaces for **users**, not computers. The better you make their experience, the more they can concentrate on doing their job and not fighting your program. How can you make their experience better? In many ways! One of which is removing the now unnecessary legacy HTTP cycle of request -&gt; page load -&gt; response. Migrating to an SPA provides a smoother user experience, hands down. Yes, it does mean some duplication of effort. Yes, it may mean writing validation logic for both client and server (which you should always do, anyway). Do the benefits outweigh the drawbacks? Personally, I believe they do, in a number of applications. Is it right for your specific business case? That's something which needs to be worked out on a case-by-case basis.
For anyone reading this far, what /u/ThereKanBOnly1 is advocating for (roughly) is Service-Oriented Architecture or SOA - which more recently has found a resurgence in popularity thanks to Microservices, which follows the tenets of Domain-Driven Design. DDD says break your domain knowledge up into different domains (billing, orders, sales, etc). SOA says expose those domains as separate services. Microservices says make sure each service can be updated and deployed in isolation. Beautiful approach but comes with its own drawbacks and has a huge overhead. Definitely worth looking in to but not a perfect fit for every organisation. Especially those on the smaller side.
&gt;&gt; Stop it with the featuritis already! The language is getting too big! &gt; &gt;You really don't want non-nullable reference types? It's actually more like **explicitly**-nullable reference types as the real new language feature. IIRC, Mads really didn't like the proposals for **non**-nullable reference types, often phrased like "`string!` cannot be null". I don't remember why. Edit: maybe it was just that he strongly preferred this proposal instead because he felt like it played nicer with large bodies of extant code that tend to handle the possibility of null references correctly most of the time and didn't strongly suggest runtime support, at the expense of having to invest more in flow analysis for the new warnings to be appropriate in general.
&gt;Edit -- case in point: now that we get default interface implementations, are we going to deprecate abstract classes, which now are 100% superfluous? What's that, can't deprecate languages features? Let's do some careful thinking then before adding features! In a GitHub pull request, yes, that will certainly do! What do you mean, "100% superfluous"? As far as I understand it, you still can't have constructors, fields, or any static methods in an interface. The base members of an abstract class are actually part of the class, in addition to being part of any interfaces that it implements, so you can call them non-virtually, and they show up in IntelliSense for the class. The line between abstract classes and interfaces is now **A LITTLE** more blurry, but "100% superfluous" is a stretch, as things stand today. What's the big deal? 
They're actually calling the feature "nullable reference types", and the currently proposed feature is to make "string" non-nullable, and "string?" nullable (after opting in). But in my opinion this is the single most important feature that C# simply has no solution for currently. It will have such a profound positive impact on future solutions built upon C#. They should really get Anders back...
I haven't taken a look in awhile, can the default interface implementation cause diamond problems? If so, it's another thing for our junior developers to shoot themselves in the foot with.
Just throwing out a different option. (not necessarily a better option than has already been provided) Is it possible to split your customer base into pods (groups) where maybe clients A - D hit one server group and customers E - H hit another server group? That is if you could use a multitenant design. At a certain growth point just throwing more hardware on it won't fix poor software performance. But I really don't have enough info to tell where you are at on that curve. 
I've heard that Java is one of the reasons they want it. They cannot interop cleanly with some java apis because of that.
The current proposal requires that each interface member has a "most specific" override, defined as being either further down in the interface inheritance hierarchy or being implemented on a class/struct. A diamond situation without a most specific override is a compilation error.
Well, I'm glad you agree that the feature overlaps with abstract classes, and I'm sure you'll agree that the chance that these things will get added to interfaces in a future version is now decidedly non-zero.
this works but will be slower than an array.
That's rather a student question.
This is the correct answer, in the beginning at least. The problem is not going to be web firms, but the database, IME. Well, unless they are really handy with the DB work.
Look at JWT. 
I really don't see how this "X not available" won't be a problem for a loong time. Core is really not going into a lot of places .net went 10+ years ago. Core can only become the other world, where some things will be done because Unix allows/needs them.
Nah, the problem is rapidly decreasing. .NET Core 2.0 release had a massive increase in coverage (full coverage of .NET 2.0 standard). You can even import full stack libraries, though they will blow up on run-time if you call something not supported. Most of what matters is support for modern technologies. I don't need .NET core to support synchronous http for instance. Remember that many in house shops are stuck on very old version of .NET which .NET Core has already surpassed in many ways simply by supporting the latest C# language features. If you're supporting things like Active-X and visual basic interop or Excel OLE, then yea stick with full stack, that's not what .NET Core is trying to solve currently. 
Huge stuff is missing. WCF server side, EnterpriseServices, DirectoryServices, System.IdentityModel, C++/CLI, COM support is kinda incomplete, access to event log, windows service, WPF obviously, Workflow Foundation... At best, we'll get more or less good copypasta of all this into .net Core, or we we will run Core on the full .net - in which case, what was the point?
&gt; What you're doing here is de-coupling the user interface from the persistent storage and application logic. That's it. De-coupling always comes with trade-offs, and yes, duplication of validation and models/interfaces is definitely one in this case. However, if you stick to an MVC stack are things any better? You should never leak ORM models to the view layer, so any MVC app ends up with a layer of ViewModels anyway - and they need validation (markup at the least) as do the ORM models. You are missing multiple layers here. In a complicated business MVC app, you are going to have a business or domain layer (so business or domain models) in between your ORM models and your "View Models" that contains business logic. So what happens when you split out the "View Model" part of it into a client-side SPA? You now have a backend API, but you can't just return the ORM or Domain models directly from the API, so you have to introduce yet another layer in that application. You need DTO's or essentially the same "View Models" you had before, and these will still be on the backend, returned from the API. Then, you have multiple layers on the client-side, because the client-side is its own full object-oriented application. You have to exactly duplicate the models returned from the API on the client-side, but then since you want to maintain the illusion of being loosely coupled from the backend, you would have another set of models on the client-side that are independent of the models returned from the API, and then have a translation layer. So now you have taken that one set of models you had before in MVC, the "View Models" and simply duplicated them on the client-side. Since your domain logic is still all on the back-end, the front-end is tightly coupled to the backend, because any changes that aren't simple UI tweaks, will result in a change to business logic, which means changes on the backend, which will then need to be displayed on the front-end. You end up constantly having to change like 6+ different layers across two applications to do anything of significance in your application. Then even worse, your business logic ends up being both *split* and *duplicated*. It's *split* because some business logic will make more sense to put on the front-end, and has to do with use cases that start at the UI, while you have your core business logic on the back end. It's *duplicated* between two different applications because of validation logic, which should be able to be centralized in domain or business models. &gt;I disagree that the front-end is tightly coupled to the back-end. If you use any sort of API versioning and as much as possible allow for backwards compatibility, you can deploy different front-end UI's targeting different API endpoint versions with confidence. This approach is great when the backend is also conceptually split from the front-end, like when the backend is only used for storing/retrieving data, not also used for all of the business logic. But when you try to separate two things that are conceptually coupled, all you are doing is giving the illusion that they are decoupled, when in reality you will find that when you have a domain-centric application on the back end, most changes will require changes in all layers, so separating out the UI only makes those changes more complicated and the application more brittle. &gt;Sorry, I have to disagree again. Any business case can benefit from a better user experience - doesn't matter what the field is. At the end of the day we write user interfaces for users, not computers. The better you make their experience, the more they can concentrate on doing their job and not fighting your program. How can you make their experience better? In many ways! One of which is removing the now unnecessary legacy HTTP cycle of request -&gt; page load -&gt; response. Migrating to an SPA provides a smoother user experience, hands down. This is exactly my point. Everything you mentioned are problems the OP is not trying to solve right now. He has much bigger problems than user experience, and you are defending this over-engineered solution to solve problems that the OP doesn't have. &gt;Yes, it may mean writing validation logic for both client and server (which you should always do, anyway) First of all, with an MVC app it's easier to duplicate the validation logic inside one single application than maintain this duplicated logic on two separate applications with different architectures, versions, repos. Secondly, MVC has ways where you actually don't have to duplicate the logic on the front-end. You can use attributes on your backend models that contain the business rules for validation, and then using an unobtrusive js library, those rules are automatically translated in the generated HTML for the inputs, as attributes, which the JS library interprets and applies validation checks. So you have validation on both back end and front end, but no duplication of logic. All of the logic is centralized in the business/domain layer where it should be.
Publishing a Netcore 2.0 with Angular template fails.
I think it's rather unlikely because they deliberately did not implement multiple inheritance of classes because of high costs and little benefits: https://stackoverflow.com/a/995271
Some magic is used to make T[] implement IReadOnlyList&lt;T&gt; and IList&lt;T&gt;. Heres a comment in the source explaining it: https://referencesource.microsoft.com/#mscorlib/system/array.cs,27 And the class referenced in the comment: https://referencesource.microsoft.com/#mscorlib/system/array.cs,2712 
A lot of those are windows only centric technologies and should never be implemented in .NET Core. If you're writing a modern app, you're not using COM and the windows event log. You'll be writing for distributed cloud (Azure), with services exposed as REST API endpoints with something like an Angular front end with Typescript. You know what else .Net core probably won't support? DDE, basic, 16bit processors. Latest .NET Core bits also have preliminary ARM builds with support for Raspberry Pi, future looks bright IMO. 
Because Reddit. 
Not sure what Paket is. I haven't been in the game for more than three years. I do know that Nuget is pretty straight forward and it is easy to make your own package server. And in the core 2.0 world, you can always add dependent projects and dlls manually. I'm pretty sure that didn't answer your question, but I saw dependency in the title. Good luck m8
They mostly don't? It's just a case of 'use nuget until things break, then cry and mess around until you manage to fix it somehow'. I've seen this so many times its not even funny. Open an old project, try to restore packages... spend 1/2 a day fixing things. Most of the time it 'just works', but when it doesn't (those conflicting requirements..!) it's a real mess. Responsible folk will host a private or local nuget repository with the exact versions they want, but most don't. Paket is great, and there's no reason that I'm aware of not to use it... other than afaik it's windows only, so its probably not usable for new core projects? (investigating, I stand corrected; it works fine on osx and linux -&gt; https://fsprojects.github.io/Paket/installation.html) 
Some of the complaints that I see listed for Paket were valid in years past, but with the latest version of NuGet and the new .csproj format, they're mostly moot (e.g. transitive dependencies, package reference paths). I'll give them that taking the latest version of a package when given an option between two (typically because of mismatched transitive dependencies) can be a problem, but if your library author followed SemVer, it either shouldn't be an issue, or would be a large enough issue that your code wouldn't compile without assembly binding redirects, which should raise some red flags.
+1 for using your own repo. The bigger the organization, the more this needs to be curated. Internet makes mistakes, quality of packages varies wildly etc.
What you're really saying is what I am saying: the range of applications for Windows is more limited with Core. Not everything is a web or mobile app that feeds itself with JSON. Modern or not, COM is mandatory for a certain amount of Windows work (or event log, or whatever). Want distributed transactions? Gotta go through WCF or enterprise services. And so on.
The missing lock file is a pretty serious issue. You can compile the same commit of your source and end up with completely different results because the package resolution used newer packages.
Reproducible builds is reason enough on it's own to use Paket over NuGet.
You can't get true reproducible builds, because the C# compiler simply does not support this feature. But at least you don't get arbitrary versions of your dependencies.
Consider myself new at this (6 months, 10,000 lines written or so). I still don't know what this means. Does it mean I can tell VS2015 to use C#7 right now and still use the 4.5 framework? Aka, nobody who uses my software needs to upgrade?
I know the JIT compiler is non deterministic, but I haven't heard that the .net compiler is non deterministic. It embeds a guid, but apart from that, I'm not aware of any non determinism.
It is deterministic, but like you said: It embeds guids (and also timestamps). So while it likely is all the same, you can't ensure it via a checksum, you don't have true reproducibility.
What do you want that `/features:deterministic` doesn't provide? [Deterministic GUIDs in PDBs](https://github.com/dotnet/roslyn/issues/926) has been there for almost 2 years.
Zero out the guid and timestamp isn't that difficult. Everything is well documented. It is not as easy as checksumming a file, but is something you could give to a junior developer.
I was not aware of this flag. Will this really cause the compiled assembly to be the same bit by bit? Not talking about the PDB, but about the actual code.
To use C#7 you need VS2017, since the compiler has to support it (in theory you could use a newer compiler while still using VS2015, but that wouldn't be a very pleasant dev experience since VS would show errors all over your code for features it doesn't understand). But you can use C#6 language features like expression-bodied members or `nameof` in VS2015 even when targeting Framework 2.0 or whatever. There are some exceptions to this, as certain language features also need library and/or runtime support. `async`/`await` is mainly a compiler feature, but it does need the `Task` library to work so you can't (easily) use it on pre-4.0 frameworks.
Of course multiple layers are missing here. We're talking simplifications. I'm not advocating this as a magic bullet for every application. It makes sense for some. It's each architects job to look at the possible solutions to their problems and craft a solution which fixes **their** problems. I'm saying that a central API with de-coupled user interfaces (whether a SPA, mobile app, or traditional website) make sense if you need to target multiple platforms, want to provide better user experience, or want to move away from a monolithic app. &gt;He has much bigger problems than user experience, and you are defending this over-engineered solution to solve problems that the OP doesn't have. No I'm not; I wasn't even the one who suggested it. I was simply clarifying his understanding of it and providing links to resources. It's up to him what he wants to do. 
We're a small team and we use the free version of visual studio online which includes a private nuget repo. I don't use it for hosting existing nuget packages though, I've never run into any of the problem outlined here, we use it to host our own packages we use across solutions. Still, I thought it might be worth sharing that even the smallest teams can make the leap. 
Without knowing more your best bet will be to try EF code first migrations. If possible replace the existing entity models (db set properties) with your own classes extended or write your own db context class
I'm not sure I understand what you are describing, it sounds like the packages you use are unreliable.
Instead of public class NewDatabaseClasses: DbContext why not public class NewDatabaseClasses: DBSupportEntities 
Isn't that what the project.lock.json does or have I misunderstood?
&gt; If I want multiple inheritance, I would write java. That's a weird way to spell 'C++'.
Because it gives me the same error
Yes the assembly is exactly binary equal between builds if the dependencies do not change. Interop assemblies are still not deterministic, but deterministic builds that depend on them are still binary equal between builds. There are still some open issues: https://github.com/dotnet/roslyn/issues?q=is%3Aopen+is%3Aissue+label%3AConcept-Determinism
How sure are you that your application is using the new context? If you use a DI container, did you register the new Context? Or replaced the old one with the new one? What happens when you try to access it like this: var newDbContext = new NewDatabaseClasses(); var canread = newDbContext.Permissions.FirstOrDefault(x=&gt;x.ApplicationUserId == userId).canRead;
Same result. How can I check the context thing? 
This is great news and I totally missed it. Gotta play around with it.
There is no project.lock.json anymore.
It's the people and the stack. I've just started looking for work again and I'm seeing this over and over. Technologies are chosen seemingly at random. Companies want someone competent in ASP razor, perl, bash scripting, Erlang, Powershell and Brainfuck v2. If you don't build your application with standard tools, don't expect to be able to hire a single person to service it.
I don't think it is officially announced yet but I have been using it in production builds for almost 2 years now. It has reduced our average production build down to 2-4 file changes instead of the 40ish dlls it previously was. edit: in your csproj files you can use this node inside a `&lt;PropertyGroup&gt;`: &lt;Deterministic&gt;true&lt;/Deterministic&gt; to get deterministic builds. Or you can use /deterministic or /feature:deterministic depending on calling msbuild or csc or whatever (not sure exactly what goes where, the csproj option is what I have used the whole time)...
Yeah WPF is a serious bitch. Its so damn difficult to do basic things. You always have yo have SO open to constantly google things like how to get datacell to fit contents. Answer is always: you gotta write super complicated template code etc.... 
Are you using Blend? I remember it being much less bad than VS for visual design of things.
The WPF designer is still kinda useless as anything more than a preview window. And frankly WPF in general is a horrid mess to use for anything moderately complex. But I don't think using the WinForms designer actually saves time in practice. Sure, you can more easily drag a couple buttons and a text box on the form, but then you start manually setting all the anchors so the form scales properly, mess around with pixel values to get different types of controls to align perfectly, and the moment you need a layout more complicated than a single growing control in a row you need to place and move around all sorts of panels, awkwardly drag controls into them and configure them through five settings pages, can't easily see or select them when they're covered and so on. As for newer options, Microsofts 'new' big thing is UWP, which is pretty similar (though largely incompatible) to WPF. There are some third-party UI frameworks like Avalon (sp?), but I've never seen any of them in production.
Funny, as someone doing exclusively WinForms for many years, and only occasionally dipping my toes in WPF, I'm finding myself more and more avoiding the drag and drop part of the designer, and using GridLayoutPanels/FlowLayoutPanels instead. This is mainly to make sure the whole thing scales nicely with odd DPIs and other quirks of layout, but also because it just makes everything that much cleaner and more organized. Doing that, do many times I've wished I could just write out what I want the hierarchy should be, and pining for the day I can just move to WPF where I can do just that.
I like the pattern library at DoFactory. http://www.dofactory.com/net/visitor-design-pattern
If you use WiseJ you get all the benefits of Winforms but you are creating web apps.
Yay this is exciting news some of my favourite parts of swift are coming to c# 🤗
Default interface implementations still come with large trade-offs for tracking instance state. You can simulate it, but if that's something you need, you really need to implement the interface and allocate the storage as part of the implementing type, if at all possible. (The same trade-offs would apply to extension properties.) The value of default interface implementations is really in extending and maintaining existing interfaces, not in replacing abstract classes. I don't think anybody wants to make C# into APL, but I'm also not convinced that's really a concern, yet. Other than the syntax for anonymous delegates (unnecessary since statement lambdas were introduced) and extension methods (would be unnecessary when extension everything goes through), the only other actually redundant syntax I can think of is the new indexed initializer syntax, which is only redundant on keyed, unordered collections like Dictionary&lt;K,V&gt;--where it provides syntactic consistency with things like arrays and lists.
I think they may have mentioned (in the proposal) that type cast operators probably wouldn't be allowed as extensions. I'd have to go check, though.
I think most of our new development is currently 4.5.1, actually. Maybe even 4.6, but I wouldn't want to swear to that. Old stuff only has to be upgraded if it's changed such that it needs a feature that requires the newer framework. Even assuming this stuff all goes into C# 8 and gets released next year, I, personally, wouldn't expect to be able to use it at the office until 2019 or so, just because corporate standards grind pretty slowly. 
What in particular?
Looks cool, I'm excited for the async foreach. The nullable reference types change will take some adjusting... Still no mixins. I really wish C# would support mixins.
Default nullable ref types and default implementations for interfaces are both big parts of swift. Nullable ref types can make code safer and easier to write. 
$690 per dev.
What does "stringURL" look like? If it's just the base URL, then of course the query parameters are lost. They are in the Request object which isn't preserved by the Redirect. In other words, if stringURL is just "mypage.aspx" then it won't have any query parameters. But if stringURL is "mypage.aspx?param1=foo&amp;param2=bar" then those query parameters should be accessible in the target page.
What's the difference between mixins and interfaces with default implementations?
/u/Sasken yeah I've debugged and ensured that the stringUrl looks like the latter in your example. That's why I'm sort of baffled as to why adding false as that second parameter is making the redirect completely drop those query params.
mixins let you define functionality and then "mix them in" to existing classes. It would almost behave like multiple class inheritence... sort of. You can do much more with mixins than you could with simple interfaces/abstracts. Dart has a great example of mixins in their sample code. [Go here, and scroll down to the mixins section.](https://www.dartlang.org/samples) 
Honestly, I never could figure out what the point of WPF was. It was in all ways more difficult to deal with, and much, much more difficult to test via gui automation. And to do what, make the form look snazzier? Do users really give a shit?
I have not used or tried Blend. Should I? I looked at it as a stand-alone version of Visual Studio that was only designer-centric, in that it was for designers to do UI code, whilst developers hooked up the back-end, and that Visual Studio had everything that Blend did, and more. Perhaps I'm ignorant on Blend; I'm going to look up more details now. Did/does that change the WPF drag-and-drop experience? Anything else?
If you must have a SOAP Api, WCF can do that. -- Just curious, is this a homework type assignment? The description is "odd"
Is WiseJ just Visual WebGUI re-branded? I remember looking into it once and it did seem pretty straight forward for basic LOB web apps. Does WiseJ apps do full page postbacks though?
no its a work related assignment, but i'm not a programmer. i was just tasked to expand a program i wrote as a learning experience to learn more. and to hit on this a bit more, I'm not sure if i need a SOAP API or if i can use REST. I assumed SOAP because i'm going to have to implement some type of state. the client will pass an ID and values will be returned that match that ID type. the values are pulled down from a messageQueue which is updated every 3 seconds. so there has to be a session i think. currently it is a console application generating the data from the message queue. I considered doing a WCF project so the client could connect to the WCF web service but i still have the problem of connecting the web service to the console app to receive the data
Yeah it is a little on the expensive side. But if you need to port a winforms app to the web it would save you a ton of time.
WiseJ is similar to Visual WebGUI however it was created by a completely different company who was using Visual WebGUI and wanted a replacement. They use websockets and you can update the UI in realtime not requiring postbacks.
This is really exciting! A lot of the good parts of Kotlin and Swift are making their way into C# i.e. null safety, default interface methods and interface conformance via extensions. I really hope the "extend everything" feature makes it in. Can't wait for C# 8 to drop! 
WCF or WebAPI(rest) could both work. Sounds like you don't need state maintained as it relates to the calling client... (each request is unique and isolated) You need some "global state" in the app to hold the value read from you message queue. I'd look at a new "WebAPI" project, and use [hangfire](https://www.hangfire.io/) to run the message queue process. Hangfire would use a recurring job, running every 3 sec, to check queue.. dequeue message and update a global static value. the exposed "rest" method could just be a get/post to a controller method that would compare a passed in value to the global value. ... any help..? That was fast and messy
I just downloaded the free trial and tried it. If it deploys correctly, I think $690.00 is going to be a bargain.
Its been some time, so I don't know if VS has included more stuff, but Blend was the tool that the UI guy was used to using and it did lots of things that VS 2010 couldn't do.
Same here, I find WPF very cumbersome to use, hence why I've tried it multiple times but never really sticked with it. Sure it might provide more functionality and more freedom, but WinForms by default looks better in my opinion and also performs quite well.
Core is the way to go in my eyes. In reality MVC is MVC regardless of the version/name minus some basic startup code. Learning MVC 3, 4, 5 etc will all apply to core so the reverse is true too. The down side to core has been tooling and breaking changes were occurring often with new versions. 1.1 really started to fix that and 2.0 is looking good. So avoid asp classic and web forms and really all you have left is mvc in an old version or core Edit: to clarify web forms is also asp "asp.net web forms"
C# + mvc core 2.0 or bust. It's very different than classic asp.net mvc 5.0, and unless you're thrown into a maintenance situation, most new development with moving to core 2.0.
OK, so since no one else has mentioned this yet... There are really 4 major branches for ASP.NET * **WebForms** are the old way of doing web pages. These are not supported in .NET Core. * **MVC** is the new way of doing web pages. * **Razor** is the current technology used in the View layer of MVC. * Windows Communications Foundation (**WCF**) is used to do SOAP web services. This is not supported in .NET Core. * **WebAPI** is used to do REST web services. This is often paired with another framework on the front end, such as AngularJS or React to make Single Page Applications (SPAs). .NET Core is a redesign of .NET itself. ASP.NET Core is likewise a redesign of ASP.NET to be more modular. ASP.NET Core 1.x and 2.0 can be used on .NET Framework as well... however, there's no telling if later versions will be as it was an uphill battle just to get 2.0 supported on .NET Framework.
&gt; Windows Communications Foundation (WCF) is used to do SOAP web services. This is not supported in .NET Core. Actually WCF supports multiple protocols and standards. That's the whole idea behind WCF: having a single code base with multiple endpoints. It can also do REST as well as SOAP.
Have you used hangfire? Looks interesting but is everything running from the app pool identity or can it impersonate on a per task basis? Also wasn't the whole running background tasks from your site always frowned upon ? 
Surely inheriting the class and then adding an interface with default implementations would pretty much give you that.
The problem with WinForms is that no matter what you do, it's ugly as sin. The question is, do aesthetics matter to your application? If it's an internal application for techies, they probably don't care - but if it's customer facing, WinForms rarely looks good, and customers like things that look good.
&gt; Do users really give a shit? It depends on the user, but typically... yes. We ran a test on this, we took two applications that were functionally identical, and had the same basic UI (eg no trickery of moving important things) and UX considerations (eg both applications had green confirmations, red warnings etc). We gave them to two different, geographically separate, teams who fulfil the same role In both areas, the management loved the functionality. In both areas, we tracked the staff uptake and their opinions at the end. The pretty application had 3x as many users, and a significantly more positive reception. Of course, there are other factors, so this isn't scientifically perfect... but it was a strong enough indication to me that yes, looks matter
False as the second parameter causes the current request/thread to continue, rather than complete (like the next line does) I've no idea why that distinction stops the query strings passing through
That's true, it's just that WebAPI is usually used to do REST these days. I thought it was more important to point out that .NET Core doesn't support SOAP services because it doesn't support WCF.
Not really... mixins would be more like inheriting multiple classes. In c#, you can only inherit multiple interfaces, and interfaces do not implement functionality, they merely define it. There is nothing in c# currently that truly behaves like mixins. Unless you can prove me otherwise, in which case I would rejoice.
One of the features in there is literally interfaces with implementations.
I second blend. definitely try it out. Way better than the vs designer.
Yeah, it's odd... we've been having some issues with some form data not being saved, and I can't see anything in our logs that would point to the issue other than those ThreadAbortExceptions. I'm wondering if there is some sort of processing going on which gets killed when the thread aborts. I've been trying to test but it's super weird that the query params just get dropped liked that. 
Extension methods are close to providing mixin like capabilities.
&gt; interfaces do not implement I don't mean to be rude, but you're not paying attention. Interfaces that implement functionality is specifically what they're talking about.
So the other page does get called, but loses the parameters? Have you dumped enough breakpoints in the code to check that the `Reponse.Redirect(url, false)` you're expecting is actually the one being called? I know that sounds obvious, but it wouldn't be the first time I've come across a situation like this where the problem is actually that the call is actually being made somewhere else without the query string... Strangely, I had something similar earlier today, although without the query string. I had the below code and I'd noticed that I needed to change `/Error/NoRecord` to `../Error/NoRecord`. I spent ages trying to work out why I was still seeing that stupid "resource not found" page, until it clicked that the first Response.Redirect was throwing an exception, and it was actually the second error (which also needed the ../ adding), and I just wasn't noticing the different error page name try { // DB Connection code Reponse.Redirect("../Error/NoRecord", false); // ThreadAbortException } catch(Exception e) { Response.Redirect("Error/ConnectionFailure"); } I know that's a shot in the dark, but it's amazing how often we're actually looking at the wrong problem. If nothing else, a quick search for `Response.Redirect` may eliminate it as a possibility
&gt; Razor This is a template engine basically, and you can use it in either Core or "Regular" ASP.NET &gt; MVC This is a framework, and can be used in either Core or Regular ASP.NET too. I'll add WebApi here, which works in roughly the same way as MVC under the hood, but accepts/returns API data eg XML or JSON. If you can use MVC, though, you can probably use WebAPI without too much effort. ---- The two to be aware of are Core and the regular Framework. Core is newer, although is not a replacement for the general framework. Although they're technically different, actually developing on them is actually not that different, especially since Core 2.0 added a bunch more libraries. I use both Core and Framework, and 90% of the time I forget which I'm developing in. C#, MVC/WebAPI, and Razor are mostly the same in either. Other than a few use cases, there's little reason to worry about which to choose now: learn either, and you'll find switching easy enough if you discover later that you need specific features of one
ASP.NET Core (inc. MVC Core) by far. There is so much power in the framework and you can customize the hell out of it. I got about 101 for ASP.NET Core that I continue to work on (https://github.com/dodyg/practical-aspnetcore/) 
**[Edit]** I just noticed everyone is talking about the interfaces with default implementations in the article, which I somehow missed. Lmao, this is pretty hilarious. **Thanks for your patience everyone.** **I'll leave my mental diarrhea below as a testament for how cool that c# 8 default interface implementations will be.** _________________________________________ I don't mean to be rude either. I'm certain that's not possible in C#. How do you implement anything in an interface? Interfaces are merely contract. *Classes* implement functionality in c#. You must define a class, which inherits the interface, to implement the functionality of the interface. It's not possible for an interface to define default implementation... the most you can do is define a base class, which other classes can inherit. [See here.](https://stackoverflow.com/questions/30322008/default-implementation-of-a-method-for-c-sharp-interfaces_) That's where mixins provide a very different behavior than c# class-inheritence allows. While you can inherit from many interfaces (and then you are forced to implement their functionalities), you can not inherit from many classes, base or otherwise. You may only inherit from one, singular class. At most, you may only be able to provide the default implementation of a single interface through inheritance. C# isn't really designed for multiple inheritance. Instead, the approach taken in c# is to create composite classes, or controllers. If you inherit from multiple interfaces, you will need to define all their implementations in your class. But what if you could just slap classes together, and create composite classes automatically?. This is what Mixins can do. Taking after the Dart example, consider I have the following c# class: public class Orbiter : Spacecraft { public int Altitude { get; set; } public Orbiter(string name, DateTime launchDate) : base(name, launchDate) { } } My Orbiter inherits from a base abstract class, Spacecraft. Spacecraft defines the default implementation of the ISpacecraft interface. Now consider my c# class for the "IManned" interface: public class Manned : IManned { public int Astronauts { get; set; } public void DescribeCrew() { Console.WriteLine($"Number of astronauts: {this.Astronauts}"); } } It implements the default functionality for the IManned interface. Now, let's say we want our Orbiter to also be Manned. Through inheritance, I only have the ability to add the IManned interface to my Orbiter. I can't inherit from the Manned base class, because I'm already inheriting from Spacecraft base class. If I inherit from IManned, I am forced to implement Astronauts and DescribeCrew() in Orbiter, there are no defaults. I could go composite now, and either inject a concrete of IManned with a DI container, or I could simply new up an instance of Manned and use it. But I can't just slap them together w/ a keyword and the class name like I could in Dart. Dart provides the *with* keyword, which lets you slap on other classes onto your inheritance chain, providing that class with the default implementations of both the IManned and ISpacecraft interfaces. It would look something like this: public class Orbiter : Spacecraft with Manned { public int Altitude { get; set; } public Orbiter(string name, DateTime launchDate) : base(name, launchDate) { } } If only it were possible! I could try this though: public class Orbiter : Spacecraft, IManned { private IManned _manned; public int Altitude { get; set; } public int Astronauts { get { return _manned.Astronauts; } set { _manned.Astronauts = value; } } public Orbiter(string name, DateTime launchDate, IManned manned) : base(name, launchDate) { _manned = manned; } public void DescribeCrew() =&gt; _manned.DescribeCrew(); } But that's not the same as mixins. The new Extensions Everything in c#8 will be another improvement, I'm excited for that. But I'd be really excited for Mixins. You could create a library of tiny classes with parameterless contructors and mix them together to whip up composite classes with ease. **Tl:dr; I'm a dumbass.** 
Extension methods are *close*! Close enough that I'm pretty excited about the extension everything piece of c# 8. Doing it with properties too will be nice, basically extension classes. 
yep, yep they would. lol 
Very little, come to find out. haha 
[EditorConfig](http://editorconfig.org/) is a good start to cover some of the basics. VS has native support for it and there's a plugin for VS Code. **edit** Looking into VS support more, the native support is as of VS 2017rc1. There's a plugin available for earlier versions. 
I'd keep it as a web application and look into offline storage. If you definitely want an exe you could even look at creating a wrapper with cefsharp
In Swift reference typed variables are default *not* nullable. 
This is super cool look, will def dig into this deeper. We are using VS 2017 15.3 so no problems there. Thanks.
Much of Blend is integrated into VS2017, but it still has many tricks up its sleeves (that is why it is still part of VS release).
Yes, when used properly, WPF/UWP looks like 21st century tech. WinForms is lost in the previous century. Let's not even start talking about hololens, making WPF look antiquated.
With the release of Core 2.0, I just wondering if MVC 5 is dead?
You can use different c# version than the .net version
what? really? Honestly I find this massively disappointing; Mix the great work with .Net core 2.0 (eg. unifying ApiController and Controller) in with some this rubbish webforms backport. Great job guys. 'Let look at the direction the web is going in general with microframeworks and then decide that in-template-code-php is the model we want to follow as 'best practice'. Web forms! More web forms!'. 
&gt; I could see where perhaps a Customer got deleted from one database, and then you've got a Sales database referencing a non-existent CustomerID Take a look at SQL 2016 Temporal Tables.. your deleted data remains accessible for historic queries. &gt; mostly because we struggle with adequate storage as it is How big are you talking about? 6TB drives are insanely cheap, and in a RAID 60 config, quite fast.
&gt; I can't tell you how many queries I had to optimize because each row of data was executing a separate call to the linked server. Here is an example on how I get fast read-only data from a remote Oracle server. IF OBJECT_ID('tempdb..#MSF580') IS NULL SET QUOTED_IDENTIFIER OFF; SELECT * INTO #MSF580 FROM OPENQUERY(ELLDEV, "SELECT * FROM MSF580 WHERE DSTRCT_CODE='ACTR'"); SET QUOTED_IDENTIFIER ON;
Personally, I don't find doing anything with asp.net core (MVC) any harder than it was with asp.net (MVC). Core it a bit more flexible in my opinion. Configuration, development experience and deployment has all become much better especially outside of windows.
Does not compute How is it worse than MVC 5? Routing is better, binding is better, built-in dependency injection support, no bullshit split between "MVC and WebAPI" when they were really the same thing... 
So .NET Standard 2.0 implements everything in .NET Full 4.6.1, but .NET Full 4.6.2 / 4.7 implements some new things not in .NET Standard 2.0 (but is forward compatible with .NET Standard 2.0 assemblies)?
Seconded! Also, screw unit testing, back to spaghetti code.
What about Xamarin Studio or monodevelop for Linux? Are they less useful than VSCode?
Does that mean mono is .NET Standard 2.0 compatible? Writing multiplatform WPF is a grand idea.
http://try.buildwinjs.com/
Xamarin studio is great, if you have tonnes of hard drive space. I quickly ran out of hard drive space when I installed it on my mac (it only has a 120gb hard drive) along side an early build if VS for Mac (they're now both the same thing). I've not tried Monodevelop, if I'm honest. So I can't comment. I really like VS Code because it's bare bones, and you can install whatever you want for it via plugins. And it's highly configurable, open source, and cross platform. I *really* like Rider because JetBrains are, pretty much, the goto IDE experts. Also, it has ReSharper built in and is cross platform. 
Trying to migrate an mvc 5 app, I would call it worse. So many things are so much more disconnected and magical. DI is terrible to be forced onto someone, never have liked it much. Routing would say is pretty much the same. Cookie/Auth has been a nightmare to migrate a very simple use case of simply encrypting a value and then getting it back out at right point and attaching items to controller. It's a confusing overbuilt mess that as far as I can tell doesn't take into account actual use cases. For example cookie auth changes the cookie encryption key across application re-deploys and instances in a web farm. Solution for to this problem is to put the encryption key in a shared folder, you have got to be kidding me. I would also take issue with the overuse issue of extension methods. Literally everything is an extension method. I had always sort of liked the way they clean up the way stuff reads... but after this past week I see how they obscure where exactly the extension methods are located too much. You can't easily find them. Endless searching for the right ones, and had to end up guessing namespaces most of the time. I now would consider extension methods an anti-feature of the language and feel they are just a bit too magical. Totally agree with you on the MVC/WebAPI split. Maybe I'm a bit jaded, I've been working in some other stacks and know that this stuff shouldn't be so god damn hard.
Yeah, I've definitely verified that the url I'm redirecting to is correct. Not only have I verified after setting a breakpoint, but as soon as I remove that second parameter or switch it to true, I get redirected and the query params dont get dropped. It's super weird... 
It isn't so hard, but then I've been keeping up on it since alpha07 or whatever. It's nowhere near as magical as the old though, what with IIS and web.config and so forth. Funny thing about the Data Protection key thing: the previous solution was to have the key in web.config. Auth/Security are coincidentally the two areas in the new AspNetCore stack that I feel are a bit overbuilt, and I agree that the extension methods can be difficult to find in some instances. Once you get in the swing of it, the overall picture is 👌
I'd rather say that ".NET Standard 2.0 specifies everything in .NET Framework 4.6.1", just because .NET Standard is a specification and not an implementation. To be honest i have no idea about how .NET Framework 4.6.2 and 4.7 relate to .NET Standard 2.0... But yes, your point is theoretically correct. I believe Microsoft did something to avoid having a runtime not implementing the latest standard but i have honestly no idea what it could be.
IMO if you get used to writing XAML (not that different from HTML/CSS IMO) you get to the point where you are faster than drag&amp;drop of elements. But yes the Beginnings can be a horrible experience if you are used to the old RAD style
I'll be honest, I've been out of the loop for a while. When people were talking about Razor Pages, I had no idea it was something new rather than just another name for Razor views.
long shot, but you said you have an auto increment id at the database. Can you not pass any value for id and try again? The database should be the one that creates the id value for you.
None of above. We only use WebAPI and Angular where I work.
Perhaps you confuse the introspect and validation middlewares with the OpenID Connect frameworks? If you just use Google, Facebook, etc. authentication middlewares, then you don't need your own OIDC server. The Microsoft Identity Services are quite new. IdentityServer4 and ASOS (the framework behind OpenIddict) have been out for years - even tho the official releases are not that old. If you need a separate server that acts as a federated authorization service or not depends highly on your business needs.
&gt; Personally, I don't find doing anything with asp.net core (MVC) any harder than it was with asp.net (MVC). Sometimes when I can't figure something out in core, I just google for the full asp version and it works (almost) the same. 
Define modern, 2.0 is already out and it's not the same as 1.0 at all.
Still need something that loads the initial page with all the angular code on it. Could be a Razor Page.
I was just trying to suggest a simple solution. From what I understood the "api" would need to have a background task running, so that it could continually poll the message queue. hangfire is a real easy way to do that without getting in to manually managing a separate thread. This solution wouldn't be the best if they needed multiple running instances of the API (scaled out), but again its a simple solution to a somewhat simple problem. You could have a separate application polling the message queue, read the value and update a central database... Then the API could check the value in the Db. ... depends on what they really need
Do you need a slick UI, or are these internal tools that just need to provide functionality? For business, what ever tool gives you the highest return on the investment of your time. Maybe stick with WinForms for things that need to get done now, and keep learning WPF on side projects?
StyleCop is an alternative that we use for Visual Studio, but as mentioned here EditorConfig has native support so I'd probably go for that too.
Or it could just be a normal html page rather than some code behind clusterfuck. Honestly, viewing code behind as separation on concerns is so horribly wrong I don't know where to start. People can and do mess up controllers, but at least they're testable.
Down the road I might want a SSO between my different apps but that's not a requirement today. Right now I need JWT as I am writing the front end entirely in Angular. Most articles that I have read all say that to use JWT I should/need to use Identity Server 4 or openIdict. This is my point of confusion. Reading documentation for Identity server 4 JWT it seems it extends the Microsoft package. If i don't use Identity Server 4 then I can jump in to asp.net core 2.0. If I use a dependency then I must stick with 1.1. I am on the fence, not really sure the implications.
Yeah Bearer Token Auth via JWT is the way to go for SPAs. OpenIddict works fine with ASP.NET Core 2.0. Give it a try. The samples are great: https://github.com/openiddict/openiddict-samples OpenIddict is much easier to use than IdentiyServer 4. To use IdentityServer 4 you would need more knowledge about OIDC. 
My point is why would I use OpenIddict and not just use the built in middleware? Is there a major drawback to just using the default middleware or will it be fine? Edit: sorry, didnt realise openiddict already supported 2.0
This post can be summarized as *use async in front of your web api method calls*. This is an extremely low effort self-promotion post with what amounts to a copy and paste job from some microsoft docs.
I am doing same research. You can use OpenIddict in 2.0 but I can't really see why it's needed. There is built in middleware for JWT `Microsoft.AspNetCore.Authentication.JwtBearer` I think the benefit of using a library is you can add additional information to the token but I am new to using the token and can't really see if I need that yet. Edit: I think the confusion is that the Microsoft article in question (same one I read) was written before the official JWT middleware was released https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer/1.1.1 Identity Server 4 just wraps the official middleware and adds an authorization cache
Sonarqube runs against the code regardless of IDE and you can implement style rules.
That only allows you to receive the token in the "bearer" request header. What about issuing a Jwt Token? I want the user to login (with a login page shown by the Spa) and then be able to see the authorized pages in the Spa and use the backend api. At the moment I don't think this is all doable with core 2.0.
OpenIddict can do this for you.
What's really disappointing is that Microsoft spent a bunch of time developing this crap vs focusing on improving core compatibility and bringing it to feature parity. Ef core is not production grade imo, signalr too. Go put developers on this rather this monstrosity. All they are doing is breeding bad habits in new developers who need to be taught importance of separation of concerns, proper architecture, guidance. Instead they give 10 different ways to do same thing. Why? I look at Java spring and their toolbox is actually ahead of ours for enterprise grade development while Microsoft continues to focus on creating gimmick frameworks that don't work in larger projects. 
The JWT handler middleware is not functionally equal to a full blown OIDC server. The middleware is for introspection and validation of JWT. Let's say Google gives the resource owner (SPA) a token. Then the SPA requests resources from your resource server (WebApi) with that token. The resource server would need to confirm that the token is valid. That's what the JWT handler middleware is about. Google does the authentication step for you. Now you need to authorize the request. If you want to offer authentication yourself, you would need an auth service (OIDC) for that. The auth service can be implemented in your resource server or separated into an own server (confusingly called authorization server). Both approaches can be done with OpenIddict.
Why are you reposting it? Is there any new information or content available? https://www.reddit.com/r/dotnet/comments/6u50b8/disassembling_net_code_with_benchmarkdotnet/
For TypeScript I suggest trying https://github.com/prettier/prettier It'll force your coding style/formatting to follow a single guideline. 
TIL Razor Pages are actually an unwelcome reincarnation of 2003's awful WebForms. WT actual F. Edit: They're not actually that bad. In fact now I think they're similar enough to MVC as to be a little pointless.
Ive found TSLint good for typescript in VS code. Wish we had something like resharper for .net. 
Yeah, but without the R# bloat. It's SO slow in VS 2017 with a big project.
The main reason I have used IdentityServer 4 is for some Microservices that need to make sure the user is authorized. The Identity Server is then a used as a sort of hub to communicate between all the micro services to verify the authentication. In my own projects where this is not needed I just use Asp.Net Identity. One of my more popular OSS projects is all Angular 4 frontend with Webapi backend using JWT tokens if you need a reference. https://github.com/tidusjar/Ombi
To anyone who wants to actually learn full async you would be 1000x better off just reading the microsoft docs.
I personally welcome it! It gives a well needed upgrade path for webform apps. 
Wow a lot of unwarranted vitriol for this product. Talk about hating a screw driver because it doesn't hammer things well. Razor views are a perfectly fine tool for certain jobs. Not everyone wants to build a simple web form with the latest React form package or micro framework. If you don't like razor, don't use it. But don't shit on others who do.
I'll check that out! Thanks!
Thanks for your encouraging comments. async await should be used the way from top to bottom. This post shows how to do it in web api using EF Core. 
this post if not about learning async await basics, but how to use them in web api from top to bottom. Its mentioned that am not writing about async await. Thanks for the comment !!
Second this. Even with a small project 
The microsoft docs will also show you how to do async/await from top to bottom with EF core. They will also give the reader much more useful (and important) information about it than your article.
That doesn't make sense. How can you write an article about fully asynchronous code and then say the article is not about async/await? That is all your article is about.
Could you expand on your setup? Do you host the angular app within the web api project, or do you have separate projects? Is it one large web api that hosts all projects or many small apis? We're just looking into angular integration now and I'd be very interested to learn more from a production setup. 
Can you provide me the link of microsoft doc? Not able to find it. Thanks for your response.
I'm not saying I like the setup.... *** Server side is Java Spring Boot or C# WebAPI. RAML is used to generate REST stubs and DTOs. We're planning on using Swagger in the future. We don't do the microservices thing for REST servers. (There's really no point in doing so when everything is in one database.) We will use that pattern for secondary systems that run autonomously. **** Node is used to host the static HTML files, JavaScript, and other assets. An ever changing collection of grunt, gulp, and other Node tools is used to compile the website. TypeScript is not currently used, but that's changing in the near future.
This sounds like a good option for me. I'm not building applications, I'm building simple admin tools and hobby projects.
Thanks, I appreciate the info, some of this I hadn't heard of yet. Helps to have multiple paths to consider.
Thanks I'll need this for Core soon. As an aside, I set this up for my own projects a while back. You can create your own NuGet package that has those analyzers as dependencies. This will let you centralize your management of your analyzers for an organization, handle updates, ease setting it up across projects (since for some reason you have to enable this per project still....). See this repo for an example. Hosting the NuGet package you build is up to you though. I just use MyGet for my public stuff, cause it didn't seem like something I should pollute the public NuGet sources for. https://github.com/i8beef/I8Beef.CodeAnalysis.RuleSet
how do they fit in my toolbox? They don't. 
http://lmgtfy.com/?q=entity+framework+async first link. Basically the same thing as your blog but with more important information that people need to know given.
Thanks for the link !! 
Sounds like you are slightly bitter because your having a difficult time learning something new.
&gt; An ever changing collection of grunt, gulp, and other Node tools is used to compile the website. this line made me chuckle; help us web assembly...you're our only hope!
Care to elaborate? You can write as little or as much code in razor pages as you do in MVC views. When you need only to display something, a razor page is much simpler to use. Also the path conventions are easier to grasp. Testing the "code behind" is as simple as testing an MVC controller. Honestly I fail to see what is the issue here (except the fact that you have to learn a bit how to generate URL to razor pages).
The razor page viewmodels are also testable.
IS there other code after HttpContext.Current.ApplicationInstance.CompleteRequest();
You're definitely partly right, but what makes me bitter is that it's not really that much of an improvement and in several ways already encountered worse. I've seen that things can be so much better in other stacks and yeah that just frustrates me because several of my clients are .net shops.
[Rider](https://www.jetbrains.com/rider/) maybe? ReSharper engine inside, understands ReSharper's formatting settings. P.S. Isn't the ReSharper slowness problem solvable? What does JetBrains support say (provided that you've contacted them)?
I think Razor pages are fine but like anything else (including MVC) they can be easily abused. Case in point the examples provided by the blog's author. Many newbies will read that blog post and start putting database calls in the code-behind classes not realizing that the author was just trying to convey a quick and dirty example...at least I hope he was ;-) I always attempt to ensure that my business logic knows nothing about the UI logic. The code behind should only contain code responsible for programmatically manipulating controls on its related view and for passing data on to the business logic layer. Business logic and data access should not occur in the code-behind classes. As long as you follow those guidelines then using Razor pages should be a breeze. 
How is this in any way related to webforms?
No, it's nothing like it.
So Is not this the objective of Razor Pages ? That your controller is in the code-behind? I tried doing what you have suggested, building a utility class that can connect to the database but I’m having hard time doing this. 
Oh, god, no. It's nothing like it at all.
Thanks!
I use the jwt middleware that come with .net core fine. Works very well although I’m just developing for the sake of learning. 
When starting to learn MVVM/WPF I highly recommend you look into using an MVVM Framework to get you started. MVVM requires a ton of setup and an MVVM Framework will get you started without having to re-invent the wheel, so to speak. I really like Caliburn Micro or MVVM Light as starter MVVM Frameworks and Prism as a more advanced one. Once you choose your MVVM Framework, I highly recommend looking into and fully understanding the concept of Dependency Injection (DI) as it will make you life a million times easier. Once you're comfortable with DI look for a good IoC container to use. MVVM and DI are both just concepts, an MVVM Framework helps you implement MVVM in your application, while an IoC container helps you implement DI. I really like Autofac as an IoC container. They have lots of great examples and documentation. To answer a few of your specific questions, things like DragMove() can be implemented by using something called a Behavior. To you your question regarding if you should use MVVM or not, my answer is absolutely yes. You can do everything without using code-behind in your views and the separation of concern that MVVM gives you makes it so easy for you to write a solid, testable application that will be easy to update and maintain going forward. Sorry for the wall of text and unorganized thoughts. I'm on my phone at the moment and I can't even see your question as I reply - stupid reddit app. Feel free to ask anymore questions my response raised! Edit: all of the framework I listed above can be added to your project using the nuget package manager. Edit2: another reason for doing this in MVVM is because it is just an all around awesome concept and any opportunity you can get to learn it, I would take it and run. If you're looking to do web development, Angular 2/4 is based around MVVM and is really really awesome. 
If I get a chance this weekend (or next) I'll whip up a quick code sample and post a link here. But regardless of what technology you're using you should always strive to keep the "how" (razor pages/mvc/wpf/etc) separate from the "what" (your business rules, data access, etc). Think of it this way let's say you don't keep them separate and build everything out using today's fancy dancy brand new Razor pages. Now fast forward 2 years from now and MS announces a new Blazor-like web assembly based UI framework. It's cool as hell and you're just jumping at the chance to use it. But guess what? Now you have to re-write your entire code base! But why? I mean, your business rules haven't changed so why are you re-writing them? The only thing that's changed is the technology you use to interact with your end user and that's what you should be re-writing. So again, always keep your concerns separate and distinct from each other and you won't have to constantly (and un-necessarily) re-write your code base.
You don't have to remove every single line of code from code behind. DragMove can live in code behind just fine. I'd focus on learning viewmodels with binding, and then you can pick up behaviors, value converters, attached properties and such as you go. Don't frustrate yourself unnecessary :)
I agree with not frustrating yourself unnecessarily, however, stuff in code behind is usually pretty hard to test, and you may end up frustrating your client(s) instead if bug(s) start slipping through. I wouldn't say NEVER use code behind, but almost never and if you do, make sure it's very simple. :)
Thank you for the awesome reply! No need to apologize for the info! I'm going to take this and use it to my advantage. I'll make sure to post updates as I figure this out.
&gt; ViewModel is more of a translator of getting the data from the model to the view. No, data binding allows you to get data from the model. If you need translation, use a value converter. Some people have this idea that you should wrap every property on the model with a matching property on a view-model. **Don't do that.** It is an error prone technique that offers no benefit and often leads to memory leaks. &gt; The view is just that. It's a view INTO the data. It shouldn't contain any data itself. True. Data about the view is stored in the view-model. For example, which item is currently selected. 
So What Exactly is a View-Model? https://www.infoq.com/articles/View-Model-Definition
&gt; You can do everything without using code-behind in your views Uh, no there is no reason to go that far unless you are writing very simplistic UIs. There is absolutely nothing wrong with using code-behind to deal with UI interactions. That's how all of the built-in controls are created. I've seen people do things like MVVM Light's "Event To Command" in a vain attempt to have zero code behind. The result was really bad memory leaks.
I'm actually interested as to why EventToCommand would cause memory leaks? Are the handlers just never decremented/disposed?
Nobody has trashed MVVM yet so I have to step in. MVVM is just a trick to get people to follow some pre-determined pattern, as a way to get them away from putting everything in 1 huge file, which is what you said you're doing, and it's what happens a lot to new programmers. If you're confident in being able to move things to several separate classes and files to keep them nice and organized, then what you should do is just start moving them into UI-independent utility classes, and call into those classes from code behind, mostly as a response to event handlers. The ViewModel layer is worthless, data binding is worthless, and so is MVVM, it just creates a bunch of chaos and extra work.
If I recall correctly, EventToCommand listens for ICommand.CanExecuteChanged events. It then has a pointer to the UI control, so that when that event fires it can enable/disable the control. The net result is that the UI stays in memory for as long as the model holding the ICommand exists. This is especially bad if you keep your models for the life of the application and the user rapidly switches between views. Of course if you have multiple event/EventToCommand pairs attached to the same control, you get non-deterministic behavior as they all fight over whether or not the control should be enabled. So in the end, EventToCommand leaks memory in order to offer a feature that is broken by design. *** This isn't the only memory leak I've found in MVVM Light. It used to leak memory from its global messaging system. They fixed that with weak references... that can be randomly garbage collected so that your messages stop flowing. I've never tried Caliburn Micro, but they would have to go out of their way to make a worse framework than MVVM Light.
Cool, thanks for the insight. I personally use caliburn and haven't seen memory leaks in the event aggregation system so far YMMV
Well, no. In that case you would just use the model. You would use a viewmodel when you only need a subset of that data, or to include multiple datasources into the view and want to simplify things. Right? Perhaps you thought he meant translation a bit too literally?
There is a considerable body of work to suggest that the 'file per page' model for website development doesn't work at scale with complex sites; replication of logic, dynamic (xxx/[id]/yyy) style routing, all kinds of things). There is an even bigger body of work to suggest that 'code behind' aka 'code in templates' is harmful and bad practice. These aren't controversial points. Here we see two anti-patterns, which are widely acknowledged as anti-patterns, bundled together into a brand new package. This blog post *literally* demonstrates doing database access in the template code. And why? ...because its easy. There's blissful simplicity about one-file -&gt; one-url, and the file all of a template, the code, the get end point, the post end point, the database access layer... ...but that simplicity has another name too: Spaghetti code. Unfortunately, naive 'easy' solutions to problems usually mean that the solution doesn't scale to more complex scenarios; it's only actually useful at a trivial scale. So, given that we have this new wonderful .Net core 2.0 web stack, *why the heck* are Microsoft adding what is *known* to be bad practice as a new feature? It's literally a migration path for legacy asp.net applications that used the same model; to see someone decree it as an essential part of any modern toolbox is flatly absurd. 
To illustrate my point, do this: class CustomerViewModel { public Customer Customer { {get return _Customer.FirstName;} {set _Customer = Value; OnPropertyChanged("Customer");} } } Not this: class CustomerViewModel { public string FirstName { {get return _Data.FirstName;} {set _Data.FirstName = Value; OnPropertyChanged("FirstName");} } public string LastName{ {get return _Data.LastName;} {set _Data.LastName= Value; OnPropertyChanged("LastName");} } //repeat for all properties } Thanks to a very poorly written MSDN article, a lot of people were mislead to think the latter was a good design.
this ^ I'm very tired of apps that have duplicate copies of the same class, one for domain and one for view, so you always have to update everything in two places.
Are you familiar with web development at all? If so html + knockoutjs is a much simpler way to learn MVVM.
Sure, in that case, a viewmodel would be superfluous.
&gt; So many things are so much more disconnected and magical. Are you insane? This was was my biggest complaint with MVC 4/5
&gt; Are you insane? This was was my biggest complaint with MVC Aparently yes... but then I'm a bit of a contrarian. Honestly not feeling core MVC at all.. been using dotnet for a very long time, feels like it's time to move on.
yeah. me too.
I thought JWT were considered bad form? https://paragonie.com/blog/2017/03/jwt-json-web-tokens-is-bad-standard-that-everyone-should-avoid http://cryto.net/~joepie91/blog/2016/06/13/stop-using-jwt-for-sessions/
Nope, there is no code after that line.
I haven't used MVVM Light, but based on the name, I assume that Caliburn's [Actions](http://caliburnmicro.com/documentation/introduction#action-messages) would be more akin to an EventToCommand rather than the event aggregation system: http://caliburnmicro.com/documentation/introduction#action-messages I haven't had any memory issues using Actions in Caliburn, but I also haven't really been looking for memory issues.
You should consider that not every project is large and that having a breadth of different frameworks for different problem domains is actually good for the community.
I think this tool could serve you well. 
This is an awful lot of melodrama. Especially considering the audience who would use this sort of framework is probably not worried about the latest 'web scale' javascript memes. Chances are it's for developers building intranet sites with simple views or web forms. Trivial apps sometimes need to be written and a simple and easy solution is all the more welcome. Read: I don't want to learn 4 new frameworks 2 runtimes and a whole host of build tools in order to update some internal website used by 3 people to update customer information 3 times a year. &gt;There is an even bigger body of work to suggest that 'code behind' aka 'code in templates' is harmful and bad practice. Don't throw the baby out with the bathwater. Just because code-in-templates is not a great solution for massive projects doesn't mean they aren't used in other applications where view logic is trivial. Not every framework is going to be designed with 'web scale' solutions in mind and that's okay. 
Most of this guy's posts are merely plugging his blog. Not only is this one low effort, but his actions are not consistent with reddiquette and not in the spirit of the site. Is this guy really an MVP? The bar must be low (though some really are very knowledgeable). 
I'm not even going to argue with you, I'm just going to post the relevant quotes: &gt; I could make an argument that Razor Pages are the perfect solution to anything that is essentially a web page within your app. It would draw a clear line in the sand that any HTML “pages” in your app are true pages. ... &gt; Of course, I haven’t actually implemented this strategy yet. It could be terrible or brilliant. Only time will tell how the community ends up using Razor Pages. Yes. You see, it's easy to suggest things if you've never actually tried them. On the other hand, when you see people endorsing something that has been shown to be a mistake, time and time again, I don't think its unreasonable to say: - Hey, you've never tried this for anything serious *by your own admission*, you maybe don't really know what you're talking about. - What you're suggesting has been shown to be a really bad idea in the past; maybe before suggesting it's a great idea to other people you should a) try it yourself, b) do some background reading and at the very least acknowledge the history bad practice around this stuff, and why you think 'this time it'll be different'. Repeating the same practices thing time and time again and expecting (magically) different outcomes isn't helpful to anyone; especially if you're *just speculating* that this time there will (magically) be different outcomes.
You can use [this](http://imgur.com/09EuVUh) for free if you want.
I'm going to use this.
That would make my homepage more miserable than it already is.
You formulated that a bit weird so just for clarification: please be aware that MVVM is not aware/does not define persistent storage (database) and dlls/adapters working with that storage. The model is basically a wrapper around those adapters so that you can easily switch your storage type. Depending on the definition, the model may also hold the application logic.
No. The only time you should announce any of the technology you have used is if you want to get hacked.
One could also just use process.execute with a few parameters, but that's too easy I guess.
I disagree strongly. Reddit was written in python. Now that you know that, can you hack it? 
Did /u/d-signet say "If you know the tech a page is built with you can hack it."? Wouldn't you agree that if you wish to hack a page, knowing the tech it's built with, makes that job easier? I mean if it's impossible it's impossible. But if you have a list of 500 ways to hack a web page, knowing what tech the page is using probably means you can cross of 200 that are tech-specific (like weaknesses specific to myphp etc.) and perhaps put more focus on those known to that tech (like: people using X often forget to Y).
https://www.microsoft.com/net/images/learn/apps/web/asp-net.svg found: https://www.microsoft.com/net/learn/apps/web
They said the only reason to discuss what tech you use is a desire to get hacked. That's just incorrect. 
Yeah and I haven't seen an alternative good reason? Right now, (if you don't disagree with what I wrote) we agree that announcing the tech you use, makes it easier for hackers to penetrate your security. I still don't know any good reason to announce it. Making the only reason to announce it to make it easier for people to hack your page.
I am not really fond of authority arguments. That said, in the context of aspnet core 2: - replication of logic: razor page still has layouts, viewstart, viewimport, partials. For the business logic, the page object should only be a viewmodel in which you inject business services. I still don't see the problem and if there is one, it would be applicable to controller/views. - routing/URL parameters: I agree with you that controller are a better option here - the code behind argument is unclear. You cannot use authority arguments like this without being more specific about what is the problem. To me it seems that you have webform flashbacks causing a anxious reaction. But razor pages have not a lot in common with webform. - you can also put DB calls in views. I believe you can also find blog posts doing that. The important thing to me is that the framework still make its intent clear about cshtml files responsibilities - having an easy option is a good thing. Sure people will do insane things with it. But some of those things will be good. See the many useful projects built with phone python or go.
Can I get an SVG? Sheesh.
&gt; the code behind argument is unclear. spend two minutes on google with 'code behind anti-pattern' or 'fat controller' or 'separation of concerns code-behind'. If I can't even convince you that code behind is a bad pattern that is heavily abused, I doubt any argument I make about good practices vs. bad practices is going to even remotely dent your armour of indisputability. I guess... having an easy thing is good. ...but, if you look at every other web framework out there, written in say, the last 5 years... you'll struggle to find any which have a page-per-url mode. Why is that? If it's such a great idea, why is no one else doing it? Maybe there are some good reasons that other people aren't doing it... (I mean heck, even the king of one-file mode, php uses a router with all modern frameworks; and the php community when all in with that mode, and now they're all the way back out again, even for beginners its recommended against using that mode)
Most obviously because you want to show support for the people who made that tech. Alternatively, but along the same vein, because you made the tech yourself and are proud of it. It's all about pride in ones work and appreciation for the work of others. Expedia, Forbes, MetLife, and a whole bunch more use MongoDB. None of them chose to let us know that because of a desire to be hacked. Facebook uses React. Google uses Angular (sparingly, but besides the point). Microsoft uses .NET. None of them chose to let us know that because they wanted to get hacked either. And none of these companies are at significantly higher risk because of their choice to disclose this. 
I am really trying but you are not helping. Using your keywords I only found: 'code behind anti-pattern' - http://awkwardcoder.blogspot.fr/2013/11/mvvm-anti-pattern-view-code-behind-with.html : mandatory useless files. code behind in razor pages is optional. 'fat controller': I really only find results about the danger of having controller with too many action &amp; responsibilities. This is not specific to code behind or razor pages 'separation of concerns code-behind' I only found posts about how you shouldn't write db calls. Sure, but once again this advice stands for every framework. I don't see how razor pages invite the developer to do this. I am not being malicious here. You didn't provide any argument against razor pages. You are asking me to research and develop your point of view. I tried and I failed. About my armour, I think I agreed about the routing part and I motivated my other points with technical points. &gt; why is there no more page-per-url mode This is an interesting question. Let me try to answer. Fair warning: this will be highly subjective. At the beginning, web servers only served static files. "page-per-url" was an evolution from this: instead of accessing foo.html, you access foo.php using apache &amp; CGI or mod_php. Same with IIS &amp; aspx Now the issue is that the interface between you web server &amp; your application framework can be problematic. So nowadays the application framework embeds a web framework (asp net core 2 --&gt; kestrel; python --&gt; tornado, etc). Apache and the likes are now used only as reverse proxies. The upside: cleaner url, easier url parameters and better perfs. The downside: routing has become a bit more complex to grasp for beginner. In this new context, implementing "page-per-url" is now more complex for application frameworks compared to asking application writers to declare their routes using the framework api. So it make more sense for modern framework to manage routes with a code configuration mecanism because it is easier and provides some advantages. But it does not mean page-per-url is a bad idea.
https://softwareengineering.stackexchange.com/search?q=codebehind https://stackoverflow.com/questions/1477152/things-that-we-hated-about-classic-asp-but-still-exist-in-webforms-today https://stackoverflow.com/search?q=webforms+vs+mvc https://softwareengineering.stackexchange.com/questions/95212/when-to-favor-asp-net-webforms-over-mvc
I followed your link and found 0 post explaining why code behind is an anti pattern
/shrug All of these articles are pointing out the same thing, more eloquently than I can. I don't know what else to say.
Can you suggest an alternative for Spa apps that works in dotnet core 2.0 I'm open to suggestions !
So after you updated your post I read : https://stackoverflow.com/questions/1477152/things-that-we-hated-about-classic-asp-but-still-exist-in-webforms-today and https://softwareengineering.stackexchange.com/questions/95212/when-to-favor-asp-net-webforms-over-mvc since the other links are searches. https://stackoverflow.com/questions/1477152/things-that-we-hated-about-classic-asp-but-still-exist-in-webforms-today : This is mainly about webform, not code behind. Web form has many issue and the biggest one is not code behind but rather the choice to hide the web layer by adding a huge engine that will manage user state, page life cycle, component tree, etc Answer 1 is saying what I just said but with less details. The answer 2 &amp; 3 in your link actually states that mvc is not the answer to op's issues, has he can also get separation of concerns issues with it. https://softwareengineering.stackexchange.com/questions/95212/when-to-favor-asp-net-webforms-over-mvc The highest voted answer make several very good points against web forms. None of those points apply to razor pages. The accepted answer in this page also makes some good points against webform. They also don't concern razor pages. 
Now that is an argument I agree with. EDIT: Though I would argue that the main reason for none of those companies are at a significantly higher risk is because they probably have really good security to begin with. If you have a business page where people would have a reason to hack it and you don't have complete faith in your security, it might be reasonable to not disclose your tech. At least until you feel that you have a robust security and have had it pentested.
https://chrome.google.com/webstore/detail/wappalyzer/gppongmhjkpfnbhagpmjfkannfbllamg?hl=en
There is no logo for .NET Core. A Github issue is open with a request for a logo: https://github.com/aspnet/Home/issues/579
TIL, I'd suppose most hackers just use that, making d-signet's argument kind of null and void :) I wonder if there are technologies made specifically to obfuscate the underlying code base to tools like that one...
Thats pretty much what's already happening here right?
This looks like your best bet ... https://github.com/campusMVP/dotnetCoreLogoPack
I would suggest a WPF client (more documentation than udp) that uses SignalR to communicate in real time with a service running in the background (I like TopShelf). The service manages the data sync and cloud communications so the client can be focused on complex forms etc.
Feels like they try to improve all the fringe things few people use except the bloated core of actually developing software that most people use... I guess they'll save that for Visual Studio 16.0 (if we're lucky).
Mvc on .net is not hard, and it addresses a specific problem and creates a structure for your code that let's it grow. If you have been developing long enough you will find that you rarely can tell up front of project will end up growing. Something small may get added to over time and suddenly it's big. Having an easy to use while properly organized, TESTABLE codebase is vital to iterate quickly. Writing spaghetti code in the bringing to save a few up minutes upfront is not worth it. We have many gaps in the framework coverage on .net where Microsoft can provide guidance instead of creating inferior alternative solutions to problems already addressed. Fix ef core Improve signal r Create a proper dataflow framework for things like es / cqrs Create a good actor framework. Akka.net is immature port and tpl dataflow are too primitive Create something like spring boot initializer to template new projects in microservices Write hystrix implementation. Soooo many new problems to address in cloud vision without creating duplicate frameworks. 
Yes, you are right. The purpose is to demonstrate how you can integrate the latest version of Tesseract 4 in your apps and show a demo with working code in c#. The function ParseText expose it in c# and is thread-safe. Internally, it uses temporary files to deal with Tesseract 4. But the function hide it.
Ohh ok. Sorry I missed that.
Unless you have a specific reason not to just use normal session authentication, why not use that? The JWT style of token auth is only really particularly superior when you have multiple microservices on different domains you talk to directly from your SPA I thought... https://docs.microsoft.com/en-us/aspnet/core/security/authentication/cookie?tabs=aspnetcore2x
Have you seen the Spa templates ? How would cookie authentication work with these ? Also the reason I want to use a scheme like this (which btw is the same reason I want to use the Spa templates) is to allow different front ends to connect to the api for example an app vs a website. Otherwise I would just use the Mvc template and be done with it.
Thanks this seems to be the way to go I'll look into setting something up with this.
If they could just fix all the broken netstandard 2.0 to. Net framework tooling too, that would be great.
&gt; How would cookie authentication work with these? There's nothing special about SPA templates; that'll work fine. &gt; is to allow different front ends to connect to the api ok, well, that's more of a problem. Good question in that case. 
Interesting - I hadn't considered something like TopShelf, thank you for the suggestion!
&gt; There's nothing special about SPA templates; that'll work fine. Interesting, when making an api call though in JS you'd need to include the session id as a request header though otherwise the api call would fail correct ? IIRC cookie based authentication is essentially the same thing as Jwt the only difference is that the in Jwt the sessionId is kept in a "token" in cookie based authentication the session id is the "token" but I don't see where this is passed back to the server normally (say in an Mvc app) I know that you can inspect the request header and see the session id being passed back and forth. The Spa templates are basically a static frontend and a RESTFUL api so how would cookie based auth interact in this manner? Edit: Nevermind I think I get it the cookie is always sent regardless right I think for the moment this might be enough !
Yup, sorry, I wasn't comparing caliburn's EventAggregator to EventToCommand, I was just making a general statement that I havent noticed memory leaks in their EventAggregation functionality similar to MVVMLight's messaging system.
Edit and continue on a .Net Standard library in a .Net 4.7 project seems to be working in the 15.4.1 preview when it was broken in the 15.3 RTM
&gt; you'd need to include the session id as a request header though otherwise the api call would fail correct ? ? no? What do you mean? Cookies are always passed through, regardless. Have a read of https://stackoverflow.com/questions/3340797/can-an-ajax-response-set-a-cookie A static website can totally talk to a REST api and use cookie session auth; there's absolutely no reason that won't work, and you don't need to anything 'special' (eg. special headers) to make it work; the browser automatically does all that. JWT is the thing where you have to send a special bearer header or whatever manually, because its just some random cypto token you happen to have, and (eg. from an app) there is no 'browser' to automatically handle the cookies. (I mean, you can literally just use a session id by posting to an ajax 'login' end point and capturing the cookie manually, then having a fake app side 'cookie jar' that passes the extra header back across and you've basically got cookie auth for your app; it's just more annoying to do than having a stateless end point that takes signed requests... but technically, it'll work just fine; I believe some libs, eg. RestSharp even has some kind of built in functionality to do that for you) 
Just leaving this here: https://www.reddit.com/r/dotnet/comments/6uoypb/go_vs_net_core_in_terms_of_http_performance_by/ To quote /u/elsyms: &gt; I'd disregard anything the OP says; he's a well known liar and fraud in the Go community. (OP in this context is the author of the blog post.)
Sorry see my edit I get it now. The piece of information I was missing was that cookies are always sent through regardless even in ajax responses. I think I can actually use this. I didn't think they would be set for some reason (dat Friday feeling). On Jwt yes you just send the token in the bearer header but for the moment I don't need to support apps ... etc so really I don't care at the moment session auth is fine for me right now. Thanks again.
Feels good not having to use Azure for hosting/deployment. Both in my head and pockets!
PostgreSQL.
Cool ty
Regardless of the perf, its go, so its a no-go :P
I hear you there... for all my coremvc framework frustrations, deploying/running the clr in docker container was really nice, and clearly it's the future.
Definitely PostgreSQL
Does **anybody** actually use F# for production applications?
Check out PostgreSQL and Marten. Great combination. 
[Yes.](http://fsharp.org/testimonials/) @ OP: Don't be scared of new languages and techs because they are new. Try them out, develop a quick spike, and decide _based on experience_, not fear of the unknown. 
It's not a fear of the unknown, so much as a fear that Stack Overflow will not be there when I need help with &lt;INSERT OBSCURE PROBLEM HERE&gt;.
Really? F# is 12 years old. S/O will probably be of more help than you think. I was an early adopter of .NetCore, and being that it was bleeding edge, I ran into that issue a lot... but it's since waned because it's matured. F# has had a lot more time than .NetCore has... 
This is what, the 8th major version of EF? You'd think by now they'd have figured out WTF they're doing and stop breaking stuff.
Also the F# community is **extremely friendly and welcoming**.
There is no point in learning yet another new language when the one you already know is doing the job just fine. 
I just want IntelliSense to be more reliable and robust. I restart VS at least twice a day because it has completely stopped working, and simple things such as adding a using line if I use a type I haven't imported yet is AWOL. It happens more in .net standard projects. And don't get me started on the integration with dotnet tooling. It's as if the CLI dotnet is a different tooling than what VS uses. Hopefully it will be better before we see the next major version.
As far as I know in interfaces you can't maintain state but with mixins you could. 
Well if you are willing to deal with the performance hit from CWTs...
All the questions I've had on SO for F# have been very quickly answered. The community is very helpful, as well. Even on Twitter, if I asked a question I would receive help.
Using F# in production is no easy feat. It's not enough that I know it. The team needs to know it enough to debug it when something breaks. We stick with C# because it's well understood by the entire team.
8th major or 2nd? EF Core: Its not the first time I've gotten the impression that something from core "threw out the baby with the bath water" 
jet.com is a fairly large online retailer that uses F# for a lot of their services https://tech.jet.com/blog/2015/03-22-on-how-jet-chose/
Around 2010 I spent 9 months working on, now get this, a Silverlight application with F# code behind. Unfortunately the company got bought and the application never made it fully into production. That was the closest I've ever seen any F# in production. I think it's a shame as it really is a great language.
The way I look at it they've had 8+ versions between EF and EF Core to figure out what the provider model should look like. I know that they're going to revoke my MS fanboy card, but at some point they need to either get some grownups in the room or admit that EF is a fundamentally bad design.
PostgreSQL with NpgSQL
To follow up on my own comment... Anything you can do to bake Resharper features right into the product (refactoring in particular) please MS. I truly love you guys, but when you don't put out, I go seeking a bit on the side. If you would only install Resharper on your own machines, you would see why we two-time you.
It really depends on what you want. I started learning F# purely out of curiosity, and it's become my primary language since. I adopted it out of practical reasons, but the biggest lesson I learnt was that there was more than one way to write code. After 6 or 7 years of C#, I was pretty comfortable with it, and thought I had a good grasp of how to approach problems and design solutions. F# showed me that some problems could be handled beautifully using simple patterns I had never even heard of. Since then, I have started looking into other languages, to find out what else people haven't told me about programming. Could I be a successful developer without doing that? Of course. Did this approach help me grow as a developer? I am pretty sure it did.
Mongodb :p
To piggyback on this. Our group had a specific requirement dealing with some messy input, and in C#, the solution was this ugly iterative mess dealing with a lot of state information. I was able to rewrite the whole thing into this beautiful F# tail recursive function with half the code. 
Refactoring Essentials and Roslynator are both free with hundreds of refactorings and analyzers
Not a suggestion to use it but it's out there, SQL Server: https://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-ubuntu
What about better syntax highlighting like ReSharper? That and the decompiler are the only things I use it for. 
That I don't know, sorry. Never had an issue with the stock syntax highlighting.
Was surprised to see so many web forms related questions. Apparently many companies still use this. Bummer.
Here's an example. On the left is with ReSharper syntax highlighting. On the right it is off. Without it I have a harder time parsing what is going on. http://i.imgur.com/nYwzus0.png Methods, Properties, parameters, Constants, mutated variables, etc. are all colored. 
Sql Server! 
1. azure 2. more apps for windows
Doesn't do anything for me personally. I'm pretty sure all that can be done with just a theme in vanilla VS if you want that fruit salad 😉 without ReSharper.
I'm 99.999% sure it can't be done
If you don't install resharper, the core of VS is already pretty fucking good and less bloated than it has been in a decade.
Well, I think it might be possible, but I can't be bothered to prove it since I don't care enough. 🙂 At least consider looking into it!
mssql 2017
I think the global strategy shift has gone from 'our OS on everything' to 'our services on everything and for everything'. It's basically a loss-leader to get tonnes of users on their services. related: https://www.forbes.com/sites/greatspeculations/2017/01/27/in-microsoft-earnings-performance-growth-in-cloud-boosts-revenue-once-again/#191f27202bd1
Mostly Azure though. They are making tons of money with it.
For maintaining legacy code, sure; I can see the need. Sometimes something is so large and mission critical that nothing short of a natural disaster or the sale of the company itself will prompt its replacement. But anyone starting greenfield projects using web forms really needs to surrender their programmer’s license.
I don't understand how Microsoft hasn't bought JetBrains yet 
I used to think that this was a good idea, but MS has now replicated so much of Resharper on their own, they just need to carry on down that path. Also, Resharper is an absolute performance drain. MS would be better just to carry on baking the best bits into VS in a way that does not kill the dev experience. Another major plus would be replicating the "Errors in Solution" window.
Microsoft has had a long standing agreement with JetBrains that they won't fuck them and just clone ReSharper. They should've bought them before mono even 
[This extension](https://marketplace.visualstudio.com/items?itemName=AndreasReischuck.SemanticColorizer) is what you're looking for, I think.
Is postgres functionally usable with EF yet?
That's what I thought, they want more devs to learn dotnet and create things, sort of implied monopoly. It's a good thing, c# is a beautiful language. But I will be deploying my side project to heroku.
Every time I need to touch something EF related, I am happy that the majority of our .NET projects keep on using plain ADO.NET or Dapper. This whole .NET Core story, dropping all features and then having to play catchup (.NET Standard) as customers refuse to port their applications is a big joke, as if the grown ups left the kids alone to play with their toys.
[removed]
[removed]
I am the earliest adopter of Kotlin for Android so I do not mind adopting new technologies for a stack that really needs it. Our latest stack is TS, Ractive.JS, ASP.NET MVC Core 2, LLBLGEN 5+, SQL Server, Elastic Search, RabbitmQ, Bootstrap 4 beta, Swift, Kotlin, Unity, Xamarin and CSharp 7.1. So we are up pretty to date. For storage I am a big fan of RethinkDB for a long time but the company behind it went bankrupt. For Fsharp to be adopted in my company, I need one or two strong developers with the skills. That require training time in an already crowded training schedule in transition everybody to .NET Core and whatever happening on the UI front.
[removed]
I am speaking as a company owner. There is an external factor why it is hard to adopt Fsharp, which is the extensive technology churn on the other part of the stack. As a company, we continuously support for customers system. There is not an option of changing stacks or god, rewrite. So on the individual level, FSharp is easy to switch to. As a company, it is much harder.
Most of the questions are very outdated. I wouldn't even apply for a job that uses classic ASP :P 
I wrote my own automatic type-safe HTTP client similar to Refit. I was disappointed with the internals of Refit and its extendability. [TypeSafe.Http.Net](https://github.com/HelloKitty/TypeSafe.Http.Net) supports the addition of new serializers/deserializers for content/media types. It allows you to register and implement the HTTP client you want to use, right now RestSharp and HttpClient are already implemented. If you use a non-standard JSON serializer or XML serializer and want to use it with TypeSafe.Http.Net it's easy. You just need to implement the serializer similar to how the current one is implemented and then register it. This was not a possibility in Refit and it is another reason why I needed to write my own version. It doesn't have full feature parity with Refit just yet but it has features Refit doesn't. One issue is performance at the moment. Test coverage is nonexistant at the moment, but you can see the library lends itself well to testing, and once there is good coverage major efforts will be taken to precompile/cache request information to reduce GC pressure and improve speed. It's available on Nuget and you can pick and choose your dependencies which are registered fluently when you create the client.
Thanks, but I checked that out a while ago and abandoned it for some good reason that I can't remember right now. Just know I had it, and for some reason stopped using it. I even downloaded the source and tried to fix it but never got anywhere,for some reason... I had a good reason at the time. I know that much. 
The idea of a single meta package for Asp.Net core is great, but why have they bundled Entity Framework Core into it too?
different codebases, different functionality. Framework is windows only, Core is multi-platform. For the most part, Framework has more functionality than core. WPF for instance is framework-only. They are also licensed differently: framework source is non distributable, while core source has the MIT license. Historically, .NET Framework has been around many years longer than .NET Core, and therefore it has more libraries implemented. .NET Framework's source is here: https://referencesource.microsoft.com/ .NET Core's is here: https://github.com/dotnet/core and https://github.com/dotnet/corefx
To simplify further: * Both implement the .NET Standard. * Framework is the FULL windows libraries. You get full functionality your used to but it only runs on Windows. * Core is multi platform but missing many things you may expect. This should be less of an issue with each version and you're likely doing to want to use more Core as you can. You may also see some unique functions in Core moving forward.
Because most system will need DB access and EF Core 5 is gonna be great.
Damn you guys really go hard on those micro-services huh https://github.com/noordwind
Because of Mobile. 
We wanted to play with technology and learn something new, as it's a really vast and complex topic, so it's good to start somewhere :).
EF Core 5 :D
PostgreSQL
yeah, and this will be the last version of EF Core, as MS already finds the next best thing in the whole universe to replace it
From a pragmatic standpoint, it enables them to ship project templates that take advantage of it (e.g. a basic web application template with account registration / login).
Awesome - I hope you guys write down the experience of working in such architecture.
ASP.NET Core and ASP.NET Core MVC have a steep learning curve especially with people with previous ASP.NET MVC 5 experience. Everything is similar in the surface except nothing is the same. Last year once a month I would spend a day learning the framework except that I keep forgetting stuff and I had problems figuring out what goes with what. That's how https://github.com/dodyg/practical-aspnetcore was born. It's a brain dump for my learning. Other than the learning curve, I enjoy working on ASP.NET MVC Core 2.
Database Data Access has always been Microsoft's Afghanistan 
Ha. Too true... It's airways been like that. Back in the late 90s I was doing Access development, and every version came with a new data access library... Dao, rdo, ado...
Thanks, we will for sure but that's a story for another blog post.
Ahhhh! Nooooo. MS Jet Red vs Blue and their associated updates back in the day...
You are right. That distinction was confusing me at first, I assumed the middleware also generated a token based off identity. You need an authorization server to generate the token. It is possible in 2.0 but you have to use the dev build of openiddict.
In this article: https://wildermuth.com/2017/08/19/Two-AuthorizationSchemes-in-ASP-NET-Core-2 Sean Wildermuth says: &gt;for APIs, hanging on the top of the cookies for authentication is nasty. So we’d like to use tokens to authenticate the APIs I am serving an Angular SPA on my authenticated view but that SPA makes calls to an API. In this case is JWT good or bad? 
If it was you're best friend's company and you were in a foreign company where that was the only programming job, without internet... you might.
Windows only make up about 15% of their total revenue nowadays. They've moved to supply services instead. Btw. Sql server can now be run on Linux aswell. 
Looks good, thanks!
The only person I saw reference classic ASP was Pam Sanderson. Although her question ("How old is your experience?") was oddly worded, what she is saying is that she sees some candidates with experience in older tech who haven't kept their skills up to date. 
It's funny how nowadays you can't even joke on the internet :) 
Is Marten an alternative to npgsql?
It's not drastically different than previous versions.
It's not a replacement. In fact, Marten uses NPGSQL to connect to Postrgre. Marten is a data layer that uses Postgre as a document database.
See related GitHub issue here: https://github.com/Microsoft/msbuild/issues/1651 I haven't used this myself, but it looks like this is the really old build engine from VS2008, but you still can build things using Microsoft.Build.Execution.BuildManager. Hope that helps!
The full .NET framework still exists. Core is for cross platform.
A CSV file.
Roslyn?
I'm really not sure what you mean. I don't have Visual Studio installed and can run msbuild. I use macOS and have it installed as part of .NET Core.
"If you are looking to do self contained deployments (Which was a massive selling point of .NET Core), then you cannot use the meta package. You would need to reference packages manually." This doesn't correct to me. According to the docs, self contained deployments contain both the .NET Core libraries and the .NET Core runtime. 
Correct. When you use the meta package, the .net core runtime must be installed on the target machine. The point of doing a SCD is that the runtime does not need to be installed (Everything the app needs is in the deployment package)
Even if you referenced each package individually you'd still need to install the runtime unless doing a self contained deployment. I really don't see any disadvantages but the article makes it seem like one. 
Sorry, maybe there is a bit of confusion. If you use the meta package, when you deploy the runtime is NOT included in the deployment. When you do a self contained deployment, the runtime MUST be included in the deployment. Does that make sense? It feels like we are agreeing with each other here. 
We probably are. I guess what I was trying to say was that if you want to do a self contained deployment you can still reference the meta package whereas the article says you can't and you must reference each package individually. Correct me if I'm wrong.
&gt; if you want to do a self contained deployment you can still reference the meta package Actually. You can't. Check the documentation here : https://docs.microsoft.com/en-us/aspnet/core/fundamentals/metapackage &gt;When you use the Microsoft.AspNetCore.All metapackage, no assets from the referenced ASP.NET Core NuGet packages are deployed with the application When you use the meta package, it is not included in the deployment package therefore it cannot be self contained. 
It doesn't specifically mention what happens in the self contained deployment scenario though. It would be madness if that now requires the runtime (as that now includes the store) to be installed as that would negate the advantage of a self contained deployment. What I'm hoping for is that this only applies when not using self contained deployments, as I believe that is the scenario this was designed for. Another thing to mention is that using the meta package for SCD will give you a much larger artifact as you don't get the new trimming feature. It looks like trimming maybe performed based on use in the future though: https://github.com/dotnet/cli/issues/6653
&gt; Is there a defacto-standard library for creating OAuth servers? I'm not very experienced with auth, but I see [IdentityServer](https://github.com/IdentityServer/IdentityServer4) mentioned a lot. It doesn't support .NET Core 2 yet though. &gt; What about authentication libraries? See above and [this](https://docs.microsoft.com/en-us/aspnet/core/security/authentication/). &gt; What's the best way to generate API documentation straight from my code? I've been using [Swagger](https://docs.microsoft.com/en-us/aspnet/core/tutorials/web-api-help-pages-using-swagger). &gt; I understand that Entity Framework is like Hibernate, but is there something lighter, where i get to write pure SQL and just get some aid when mapping? Entity Framework Core was written from the ground up with performance in mind so should not be as heavy as it used to be. However, if you want to write pure SQL but still get mapping, then you could look into [Dapper](https://github.com/StackExchange/Dapper). &gt; Any library for API input validation? Look into [model binding](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/model-binding) and [model validation](https://docs.microsoft.com/en-us/aspnet/core/mvc/models/validation).
I just tested this out. Referencing only the Microsoft.AspNetCore.All meta package and publishing as a self-contained deployment gives you ALL of the assets (100MB~).
but I read the ASP.NET Webforms is dead and ASP.NET MVC won't be supported after version 5. No?
Try using the linker, see how it goes https://github.com/dotnet/core/blob/master/samples/linker-instructions.md
The article says: &gt; Once this is all setup, if we have an API, we can visit the API in the browser once we’re logged in and it just works... (shows how you can open the API url in the browser) &gt;This is pretty insecure. For users we want to have decent length cookies to make login easier, for APIs, hanging on the top of the cookies for authentication is nasty. ...? I have no idea what that means. If you're relying on the fact that users can't trivially hit your API endpoints using a browser to stop bad behaviour, you've got more serious problems than JWT will solve. Other than that... I dunno. CSRF? SSO? Something else? I mean, from my perspective he's just gone: &gt; Here's an example of an authentication scheme that works out of the box. ... &gt; But hey, I don't like it, so here's how to add JWT *on top* of that **as well** because reasons. I guess you'd best direct your question to the author of that article. From my perspective, it's using two authentication schemes on top of each other, with no actual understanding of the fact that they both achieve the same thing. Should I also OAuth sign my requests at well? Maybe I can also add a one-time CSRF protection token as well? The more authentication schemes the better? I have no idea...
Webforms is dead in the modern ASP.NET (aka core) as far as I know. MVC is very much alive and not going away. And neither is the full .NET framework. In fact, you can use ASP.NET Core with the full .NET framework rather than the .NET Core framework and you have access to everything. 
&gt; Say for a PDF generation/Image generation There are third-party libraries available that support these features. They support .NET Core. For example: https://github.com/SixLabors/ImageSharp. System.Drawing is also being brought back to .NET Core, but you shouldn't use it -- it's a very old library with a very poor API which doesn't map well to non-Windows platforms. &gt; socket based programming Sockets are fully-supported in .NET Core. Disclaimer: I work on .NET Core
Microsoft has a wildcard SSL cert for all *.azurewebsites.net instances. It should just work, assuming you're listening on the right port (443). What happens if you put https in your URL? 
If you are talking about compiling the C# code at runtime, perhaps you can take a look at Microsoft.CodeAnalysis.CSharp.* NuGet packages, especially Microsoft.CodeAnalysis.CSharp.Workspaces . I haven't used the latter package before, but hope it would be a way out of this.
you can run asp.net core on net core and net framework. learn that
Finally a word on .NET Standard 2 on UWP. Now I can start using it.
I'm pretty sure this depends on your payment level. Look at scaling up your instance and one of the levels will list https as an option.
if(IsValid == false) { ... } Makes me sad... Use if(!IsValid) { ... }
I wonder if they "fixed" the [high cpu usage](https://developercommunity.visualstudio.com/content/problem/36586/high-cpu-usage-while-vs-2017-editor-is-idle.html) again. I end up restarting VS multiple times in a day just because it's randomly decided to eat one of my cores alive. `IsAssertEtwEnabled` is often the top thread. But there's always a bucketful of MSFTs claiming it's been fixed with this one weird trick or update. There's no reason I should see VS at 25% cpu usage 3+ minutes after launch/load on a quad-core i7 with an SSD. It's worse than Chrome now. And then after all the effort I went through in disabling Javascript and Typescript support to stop the Node servicehub from running, the last update (15.3.2) brought it back and now I can't make it go away. It would make far more sense for some of the external processes to load on-demand instead of at launch. Like external language servers. If I haven't even opened a project with a .js or a .html file, why is it loading node? VS 2017 is the first release in which I've spent more time disabling and uninstalling features than I have discovering and enabling them. Honestly, it's at the point where I want them to bake in some internal performance and monitoring tools to tell me how to stop some processes from launching, and to determine the thread and source of the CPU hogs.
The power of .NET Core to confuse the hell out of people in incredible.
Is there anything like this for desktop framework?
Thanks for the tips! Is there an awesome-* list with .NET Core 2 compatible libs? Also, do you use Autofac or the built-in DIC?
Also [FluentValidation](https://github.com/JeremySkinner/FluentValidation) for validation.
I personally use structuremap 
Any special reason?
Look for libraries targeting the [.NET Standard](https://blogs.msdn.microsoft.com/dotnet/2016/09/26/introducing-net-standard/). As part of .NET Core 2 you can [reference .NET Framework libraries from .NET Standard](https://blogs.msdn.microsoft.com/dotnet/2017/08/14/announcing-net-core-2-0/#user-content-reference-net-framework-libraries-from-net-standard). We should hopefully see more libraries targeting .NET Standard soon as .NET Core 2 brought across a lot of missing APIs. So far I've only used the built-in DI.
Not experienced in auth servers but it sounds like IdentityServer is what youre looking for. For api doc gen, swagger is great. For a light weight orm, aka micro orm, check out Dapper. Finally, here are some libs I often use across projects: dapper, automapper, fluentvalidation, swagger, and for a command bus, if i dont roll my own, i find mediator to be easy to use and light weight.
Instead of using Mount directly as the data binding source, create a list (or other collection) that joins each Mount item with its respective image. Then, make that collection the data source.
Sorry, it is bound to my XML. Here is the XML layout. &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt; &lt;MountInfo xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"&gt; &lt;Mount&gt; &lt;Item&gt;Camel&lt;/Item&gt; &lt;Cost&gt;50 gp&lt;/Cost&gt; &lt;Speed&gt;50 ft.&lt;/Speed&gt; &lt;Capacity&gt;480 lb.&lt;/Capacity&gt; &lt;/Mount&gt; &lt;Mount&gt; &lt;Item&gt;Donkey or mule&lt;/Item&gt; &lt;Cost&gt;8 gp&lt;/Cost&gt; &lt;Speed&gt;40 ft.&lt;/Speed&gt; &lt;Capacity&gt;420 lb.&lt;/Capacity&gt; &lt;/Mount&gt; &lt;Mount&gt;
Since the XML doesn't have image information, you would still want to create a collection that includes both as your datagrid source.
OK, I can do that. As for the having a 0 value that I can add and subtract from with a button, how would I put that in the list? (under the Qty Column)
I'm guessing (though am not certain), that this might be due to using some source of random bits and then interpreting 64 of those bits into a `double`. There was some good reasoning for this with charts on Reddit or Hacker News a while back, but I can't find it. [This](http://mumble.net/~campbell/2014/04/28/uniform-random-float) seems like a good explanation though.
Darn it, thats a pain, I am on the free level. Thanks for that.
Personally I'd avoid auto mapping. Writing mapping code and tests is quick and robust. There's nothing worse than figuring out an obscure bug at runtime only to find out a property was added/modified and you didn't know there was an abstract coupling to another model in a different layer.
I do it, but I do agree that it makes it hard to find bugs. 
i like the following questions, which surprisingly so many people fail on, do this over the phone to not waste time with an interview if they fail on this. - what is an interface and what is it used for, explain the benefits - list the different sql joins (not .net specific but still relevant for most dev positions) - what is DI / IOC and why is it useful - what are generics and why are they useful - what is the difference between ienumerable and iquerable and why does this matter with entityframework or another ORM - what is polymorphism if they pass this, get them to come in and do an interview. here get them to do a technical test, ideally on a computer where they their screen is being captured, they can use google, within reason of course. ask them to do either or all of these - create a C# project that checks if two strings are anagrams of each other, they can use linq, add in some upper case letters to make it a bit trickier. you would be surprised how many people have no idea how to use visual studio - traverse a binary search tree and output the value of the string in the data structure, provide the data structure, provide a picture of the graph and ask them to output the value of the string in the order provided in the question. the solution should use recursion, if they struggle, you can hint, since devs dont normally use recursion often. class Node { public Node A {get;set;}; public Node B {get;set;}; string Value {get;set;} } - ask them to reverse a string, bonus points if they can do it in less than O(N) complexity - final questions, project history, what they have worked on, challenges they have faced, a general tech discussion to see how they handle themselves. What they think about new tech asp.net core, react, using docker containers, microservice architecture etc. follow that up with the usual behavioural questions avoid asking SOLID, everyone asks this and its easily memorised. please avoid using hackerrank a) easy to cheat if youre using an existing question from their library b) someone else can sit the test c) the type of questions asked there are not the type devs solve on a day to day basis. Also avoid asking the applicant to create a full blown application with perfect coding standards before they even get in the door. People have lives and creating an angular app with a webapi backend that calls a service takes a long time to do with produciton standards. I dont even bother proceeding with jobs that ask this.
It just doesn't connect. Everything is wired up correctly, the Slack, the Bot Framework, the Github, just Azure is playing silly buggers. Here is my most recent iteration http://stacko.azurewebsites.net Unfortunately the azure channel on StackOverflow is an empty room of people shouting help. Does anyone else do this :-)
I like the scanning in structuremap 
I'm fairly new when it comes to ASP.NET. However, I'm attempting to make a very simple application and its gone swimmingly thus far. Razor Pages are awesome since it makes development super easy. Only hiccups I've had thus far are: * Entity Framework Migrations in separate assemblies. Solved that problem thankfully. * Authentication and Authorization. Auth is the only thing I've not had much luck on. I've got a simple requirement: 1 account configured via the app configuration (Password is hashed immediately in Startup's Constructor). Everything I've looked at regarding auth is overkill for my needs with this app. I just need to be able to check the config options, and if they're good, give them a cookie that tells the rest of my app that they're authenticated as the owner. Any ideas on how I can accomplish that?
I personally avoid too much *magic* in general. Its like flying a plane with auto-pilot that you dont know or see what its doing and cant take control whenever you want.
Are you sure you're listening on the right port?
I use autofac but I assume most containers provide the same basic functionality. Register a connection factory interface as a singleton life style that returns an instance of *IDbConnection* and inject that to you repositories (or however is your data fetching/updating code). When you need to do a db operation, open/dispose a connection. Something like: interface IOracleConnectionFactory { IDbConnection Create(); } container.RegisterSingleton(new OracleConnectionFactory(myConnString))
Do I have an option?its not I the instructions anywhere. What do you mean? 
I don't like automapping because it's less explicit and the advantages aren't that much better than manually mapping. That being said, to answer your question, its safer to query (read) than command (persist), and on top of that, the magic in an auto mapper may go awry. Persistence via a command should be easily testable too. I recommend looking up CQRS
That's nice, but I haven't found a great tool for extracting localizable strings from code into the right resource files. That would be very helpful.
I'll think about it, right now LangTool translates prepared localization files
Am I missing something here, or isn't that exactly what the post claims as well?
This closes one error source by opening another. If you are concernered about missing properties, you could guard yourself with interfaces.
Missing examples.
Automapper has a method (AssertConfigurationIsValid) that validates that the configured map is complete. It complains if there are attributes that are neither mapped nor explicitly ignored. That, to me, is a significant improvement over manually written maps.
How would you organise the mappings? Add a `ToMyViewModel()` method to the entity class and vice versa? Is that breaking separation of concerns? I don't mind writing a bit of extra code but I want it to be neat. I like the idea of Automapper because I can make profile classes then pass them into `ConfigureServices()`.
Automatic translations are pretty much always bad. There's a reason why even the Google Translator is still very bad. And I as a user think it's better to provide **no** translation, than **bad** translation. edit: and looking at the German version of the website... It indeed is quite bad localization. edit2: There is also no information about the legalities. If you submit your text to them, are you giving them any rights to use them? Are they storing the text snippets you send them?
That's a great tool! I wrote an app for Windows Store earlier on that was a tool to send XLIFF files to collaborate on translation. I haven't updated it in all over a year so I doubt it works... We need tools of there to make it easier. Microsoft has a number of resources with translations of common terms like OK, Cancel, Close, Are you sure, etc in a number of languages. Drawing from a set like that is very helpful. Their native tool for XLIFF files uses Bing translate I believe and is a great help.
I play with hardware for IoT, lighting, etc. as a hobby and a lot of the stuff comes from Asian countries with poorly translated English instructions. I will hands down take poor English translations over having to teach myself another language enough to do an even simpler word for word translation and then guessing at the meaning.
It is indeed. (EF) Entities shouldn't know about view models and viceversa. You could help yourself with some extension methods available only at the highest common layer (in this case the view model). In reality you shouldn't because this will subconsciously bring you to have 1:1 entity/model. Having a proper model converter tool helps you thinking about the types separately. Each layer should have the types it needs, type conversion should be just glue.
Expand the "References" node for the relevant project in the solution explorer and see if you can find System.Data there. Right click -&gt; Properties and see what the path of the file is. It could be that it's not corresponding to the path you'd expect, which is causing problems for you.
Fair enough. My statement was aimed towards English being the primary language. :-)
This is an example of how I do it. If anybody has any remarks or improvements, please let me know. There are 3 main elements here: * The `Startup` * The `Repository` * The `Controller` --- // Startup.cs public class Startup { public Startup(IHostingEnvironment env) { var builder = new ConfigurationBuilder() .SetBasePath(env.ContentRootPath) .AddJsonFile("appsettings.json", optional: true, reloadOnChange: true) .AddJsonFile($"appsettings.{env.EnvironmentName}.json", optional: true) .AddEnvironmentVariables(); Configuration = builder.Build(); } public IConfigurationRoot Configuration { get; } // This method gets called by the runtime. Use this method to add services to the container. public void ConfigureServices(IServiceCollection services) { var connStr = Configuration.GetConnectionString("DefaultConnection"); if (connStr == null) throw new ArgumentNullException("Connection string cannot be null"); services.AddTransient(provider =&gt; new MyFirstRepository(connStr)); services.AddTransient(provider =&gt; new MySecondRepository(connStr)); // Add framework services. services.AddMvc(); } ... } // The repository: public class MyFirstRepository { private string _connStr; private IDbConnection _db { get { return new MySqlConnection() {ConnectionString = _connStr}; } } public SubredditRepository(string connStr) { _connStr = connStr; } public List&lt;MyModel&gt; GetAll() =&gt; _db.Query&lt;MyModel&gt;("SELECT * FROM table_name").ToList(); ... } // The controller (automatically resolved thanks to "services" set in Startup.cs): public class ExampleController : Controller { private MyFirstRepository _myFirstRepository; private MySecondRepository _mySecondRespository; public ExampleController( MyFirstRepository myFirstRepository, MySecondRepository mySecondRespository) { _myFirstRepository = myFirstRepository; _mySecondRespository = mySecondRespository; } public IActionResult Index(SearchResultsViewModel searchResultsViewModel) { ViewBag.results = _myFirstRepository.GetAll(); return View(); } }
Examples of files for translation? There are examples just below, for every supported structure
LangTool supports automatic and manual translating (after automatic), translations will improve over time
Not that I disagree, but too little magic can be just as bad. It is just as hard to find out why something happens if it is buried in mountains of boilerplate code.
So I create a model converter class that has overloads for different types it supports? Something like `var converted = MyModelConverter.Convert(modelA, modelB)`?
Also make sure all your nuget packages are downloaded. Tfs doesn't check-in packages so brand new check out they don't download. They should download on build, unless your vs has had its default settings changed
It is a viable option. I would prefer something like `IEntityConverter { Model ConvertToModel(Entity); Entity ConvertToEntity(Model); }`. Or, if you are using a split read/write model, like CQRS, the two methods should sit in two different interfaces. Anyway, the idea of the interfaces is that you can have an `AutoMapperEntityConverter` or whichever strategy you prefer. In this way you can easily jump from one to another with the help of the IoC container.
Do you create a connection only prior to executing a query? I don't need to close it or maintain it as a singleton?
&gt; Do you create a connection only prior to executing a query? I don't need to close it or maintain it as a singleton? Take a look at [this](https://stackoverflow.com/a/42938032/831138) StackOverflow answer: &gt; If you provide SQL connection as singleton you won't be able to serve multiple requests at the same time unless you enable MARS, which also has it's limitations. Best practice is to use transient SQL connection and ensure it is properly disposed.
It sounds like he added his Entity Framework references *not* through NuGet, but rather referencing the copies installed with Visual Studio - and you didn't install it yourself (or the path is different for some reason). Make sure that the packages.config file is showing the Entity Framework packages; if not, you or your friend will need to install Entity Framework from NuGet instead of relying on the VS-supplied version.
Right, it needs to be just the right amount. Reinventing the wheel is just as wrong.
I assumed it was about getting back to something that just works (like the older .Net Framework versions did) for a lot of situations instead of always chasing after dependencies of dependencies.
Hmmm.. looks like you're right. Not the right path. And it seems like he uses 4.5.2. But on my computer I can only find folders for 3.0 and 3.5. Downloaded .net 4.5.2 and tried installing it but it said I already have it or a newer version installed. But where are my DLL files for 4.x? Do I have to change the setting for every reference in the project? That's a lot. What has he done wrong?
It says that entity and mvc are downloaded. But still missing when trying to build.
It also breaks "find all references" and can make it hard for any other developer to know what is being used and what is not. 
I suspect /u/HamsterExAstris is onto something, so see if removing the reference and adding them through NuGet solves it.
in short things moved. you have to chase down all the right references to the right packages. welcome to dependency hell!
Your best bet is to remove the reference by right-clicking on it in the project tree. Then add it again by right clicking on the References folder. Choose the assembly tab and type System.Data in the search box and it should show up. This should be a lot easier than trying to find the actual DLL. Do this for any of the references in the reference folder that show a little yellow warning icon next to them.
I think I had the same problem few weeks ago, although I'm still junior so I don't really know if I got you correctly. My mistake was that I accidentaly created a project that was .NET Core, not .NET Framework, and that gave me much problems with references, then I created everything new with .NET Framework and it worked like charm. 
Exactly, it just isn't as explicit. Also, ideally your view models and entities shouldn't be a 1-to-1 mapping as your views tend to have more information (e.g. all the possible values for a drop-down in the VM but in the entity it is the selected option).
How do you dispose of it? Calling Close() on the repository destructor?
You could take advantage of the `using` to make sure that the connection is properly disposed: using (_db ) { var invoices = _db.Query(sql).ToList(); return invoices; } If you want something more "robust" way, I actually use Dapper via the following pattern: public List&lt;App&gt; GetAll() =&gt; _db.Using( db =&gt; db.Query&lt;App&gt;("SELECT * FROM apps")).ToList() ); public static TReturn Using&lt;T, TReturn&gt;(this T client, Func&lt;T, TReturn&gt; work) where T : IDbConnection { using (client) { client.Open(); try { // Return if possible: TReturn o = work(client); return o; } finally { client.Close(); } } } I can't help you further with the differences between `using`, `_db.Open()`, `_db.Close()`... If someone has more info about do's and dont's, I'm very interested in hearing them.
There is something shady going on here. I removed the reference from the list. Tried right-clicking&gt;add reference, but for example the mvc reference could not be found there in the list. Through NuGet it says it's installed. !!!!!
&lt;package id="EntityFramework" version="6.1.3" targetFramework="net452" /&gt;
thanks! It feels great being here.
I removed the reference from the list. Tried right-clicking&gt;add reference, but for example the mvc reference could not be found there in the list. Through NuGet it says it's installed. 
One of my gripes with it, although to be fair, it can be disabled, is the ViewState. HTTP is meant to be stateless, and the ViewState kind of violates that, on top of adding a lot of unnecessary bytes to each request/response. While saying why Razor is better isn't really a reason why WebForms sucks, I do prefer the way you can (usually) seamlessly mix HTML and C# when writing Razor views. It's usually much clearer what the view is doing.
There is nothing wrong with webforms. We've been writing/maintaining our system using webforms for more than a decade. It isn't sexy or new (any more) but it gets the job done. While I'm sure there are those that would find our code and techniques horrific, the bottom line is that it works and it is hard to argue with success.
Most developers hate on literally everything they aren't using on their current project. Don't let that bother you. There are pros and cons to every framework, and justifications can be made to use any of a thousand different approaches to a web application. In the end, if you can develop your app quickly and in a way that isn't annoying for you or someone else to troubleshoot, go for it. 
&gt;Only using GridView and ASP.NET charts on front end. The better question to ask is are you actually using Web Forms, or just a few of its components? Most of the things you've listed have nothing to do with Web Forms specifically and could just as easily apply to many other web stacks and frameworks. I don't think you'd view it so favorably if you'd taken the full plunge into Web Forms. The drag-and-drop web development equivalent of the classic WinForms experience that was the vision of Web Forms ultimately does not translate well to the modern web, and it will fight you a lot more than more modern frameworks when trying to accomplish "modern" things. It's just... more painful than alternatives. Also it's basically a dead technology, so that's reason #1 to not use it going forward on new development.
The thing about WebForms is people don't always understand the history behind them and what they were designed for. Most development prior to their release was for desktop but more were attracted to using the web for apps because of the simplified deployment and maintenance. At the time RAD was done using VB6 - a highly productive platform (I remember because I was one of these developers) with a drag and drop user interface. WebForms were designed to make it easy for VB6 programmers to migrate. On the whole MS did a pretty good job here. Critics cite ViewState as an issue but I don't think the large page size was necessarily a problem to begin with. A lot of business apps ran on intranets and also this was the time broadband was really taking off. It only became a really serious issue later when mobile started to take off and all of a sudden it was like we were back to the early days of the web when saving bytes was everything. I will grant that the page life cycle could be an issue but if you kept your UI fairly simple it wasn't so bad. I also think WebForms are far better at getting something out the door quickly than MVC. It really depends what you want to do. WebForms aren't perfect but the hate they generate is a bit much imho.
WebForms is great for intranet applications and such. But it's not recommended for 'modern' websites for many reasons, most of which can be summed up by the fact that WebForms was really designed to enable enterprise developers to quickly reuse their WinForms experience and apply it to enterprise web applications. 
It's been at least 6 years since I've done WebForms, but my biggest gripe was forcing the developer to code around the Page Lifecycle. Complex data-driven, edit-mode lists were terrible to code postbacks for because you wouldn't know the final state of the list until some obscure PrePostWhatever lifecycle event. Coding with Asp.Net MVC and an MVVM framework on the client is sooo much easier. But unfortunately WebForms were easy to sell to Middle Management because it's really easy to make a Northwind demo application.
OK so I made a list and bound it. The images now show up correctly, however, none of the DATA shows up now just the column names. Here is the new image. https://pasteboard.co/GHLP7EX.png List&lt;KeyValuePair&lt;string, string&gt;&gt; potimages = new List&lt;KeyValuePair&lt;string, string&gt;&gt;() { new KeyValuePair&lt;string,string&gt;("Image1", "Images/i_potion_of_healing.png"), new KeyValuePair&lt;string,string&gt;("Image2", "Images/i_bagpipe.png"), new KeyValuePair&lt;string,string&gt;("Image3", "Images/i_backpack.png") }; tab_potions.ItemsSource = potimages; XML DATA &lt;DataGrid x:Name="tab_potions" IsTextSearchEnabled="True" HorizontalAlignment="Left" Height="543" Margin="5,239,0,0" VerticalAlignment="Top" Width="639" FontFamily="/Player Workbench;component/Fonts/#UnZialish" AutoGenerateColumns="False" ItemsSource="{Binding XPath=/PotionInfo/Potion}" CanUserReorderColumns="True" CanUserResizeColumns="False" FontSize="16" MinColumnWidth="1" BorderBrush="{x:Null}" Foreground="#FFB3B3B3" HorizontalGridLinesBrush="{x:Null}" VerticalGridLinesBrush="{x:Null}" Background= "Transparent" CanUserResizeRows="False" RowHeaderWidth="0" FontWeight="Normal" CanUserAddRows="True" VerticalContentAlignment="Center" GridLinesVisibility="None" &gt; &lt;DataGrid.Resources&gt; &lt;SolidColorBrush x:Key="{x:Static SystemColors.HighlightBrushKey}" Color="#FFCAA201"/&gt; &lt;/DataGrid.Resources&gt; &lt;DataGrid.DataContext&gt; &lt;Binding Source="{StaticResource Potion}"/&gt; &lt;/DataGrid.DataContext&gt; &lt;DataGrid.RowBackground&gt; &lt;ImageBrush/&gt; &lt;/DataGrid.RowBackground&gt; &lt;DataGrid.AlternatingRowBackground&gt; &lt;ImageBrush/&gt; &lt;/DataGrid.AlternatingRowBackground&gt; &lt;DataGrid.ColumnHeaderStyle&gt; &lt;Style TargetType="{x:Type DataGridColumnHeader}"&gt; &lt;Setter Property="Background" Value="{StaticResource PrimaryBrush}"/&gt; &lt;Setter Property="Foreground" Value="{StaticResource PrimaryFont}" /&gt; &lt;Setter Property="HorizontalContentAlignment" Value="Center" /&gt; &lt;Setter Property="VerticalContentAlignment" Value="Center" /&gt; &lt;/Style&gt; &lt;/DataGrid.ColumnHeaderStyle&gt; &lt;DataGrid.Columns&gt; &lt;DataGridTemplateColumn Width="84"&gt; &lt;DataGridTemplateColumn.CellTemplate&gt; &lt;DataTemplate&gt; &lt;Image Source="{Binding Path=Value}" Width="74" Height="63" /&gt; &lt;/DataTemplate&gt; &lt;/DataGridTemplateColumn.CellTemplate&gt; &lt;/DataGridTemplateColumn&gt; &lt;DataGridTextColumn Header="Item" Binding="{Binding XPath=Item}"/&gt; &lt;DataGridTextColumn Header="Weight" Binding="{Binding XPath=Weight}" /&gt; &lt;DataGridTextColumn Header="Cost" Binding="{Binding XPath=Cost}" Width="100"/&gt; &lt;DataGridTextColumn Header="Qty" Binding="{Binding XPath=Qty}" Width="100" /&gt; &lt;DataGridTextColumn Header="Details" Binding="{Binding XPath=Details}" /&gt; &lt;/DataGrid.Columns&gt; &lt;/DataGrid&gt; &lt;/Grid&gt; &lt;/TabItem&gt;
HTTP has been made without state in mind, but applications definitely need state. Not to defend ViewState, of course.
They're great at hiding the stateless nature of the web, replacing it with its own leaky abstraction. If you just want to make some pages with a programming model similar to Windows Forms, that's what it's there for. However, if you want to learn and program for the web as it truly is, you don't want that (heavy) layer of abstraction, in which case you choose something like MVC. ASP.NET Core has introduced Razor Pages, which is kind of a middle ground between the approachability of Web Forms and the lighter abstraction and cleaner separation of MVC. Maybe give that a look too if that's an option for you.
In my experience, the most hate for WebForms came from people who were "using it wrong" :P I did a lot of WebForms apps before MVC appeared. I've been using MVC for a bit more than a year now. I never found the life-cycle that difficult to deal with, but I frequently encountered issues where other people had clearly not taken the time to understand it. I built lots of custom controls, even some with complex design-time support and I used IHttpHandlers for some things that didn't need forms. Having worked on a few old MVC sites, I am more convinced than ever that the first few versions of MVC (especially when using aspx views) were nowhere near as good as WebForms was at that time, but they are indeed different solutions to similar problems. I like MVC as it is now, I wouldn't suggest starting new apps with WebForms, but I don't think that being a WebForms app is sufficient reason to rewrite the app. 
Cookies are also adding state ;)