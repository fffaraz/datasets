I use Umbraco, Orchard, DNN, &amp; Ektron heavily and I would recommend them in that order. If you have requirements I could point you in the right direction. 
Avoid DNN at all costs
yeah hate it.
ah didnt realise you had that in your original post lol. I had the unplesant task of setting it up at work, so much pain
Also, don't use that Nuke one, it's not good.
mkay
With my clients I have used Orchard, DNN,nopCommerce, and Kentico. Kentico seems cool, it seems really powerful, but I am new to it so as i get more into it it may start to suck. nopCommerce is a good eCommerce package, but the CMS portion of it sucks. I have done some pretty awesome things with Orchard. So in order, my choices would be: Orchard, Kentico, nopCommerce and then DNN
Umbraco v6. You get to use MVC4 and WebAPI, also the new Belle admin interface coming in v7 is looking very nice.
We use Umbraco on the ASP.NET website and it's been working well for us. It has its quirks, as all CMSs do, but it's been fast and does everything we need it to. I've also been interested in N2, although I haven't tried it. Unlike other CMSs that take over your site, N2 is supposed to be more of a drop in that supplies CMS capabilities where you need them. http://n2cms.com/
The title for this is more accurately "Anders On Typescript with a few sidenotes on C# 6.0 at BUILD 2013"
Warning to everyone that would like to preserve their sanity, avoid DotNetNuke at all costs. If your employer is insistent that you use it quit your job. It will leave you looking like an idiot and your project will fail miserably.
Orchard, It's kind of silly to even call it a CMS, it's a complete application development platform. If it was up to me, Everything I built would be on Orchard.
Umbraco. 
[Even google thinks so](http://i.imgur.com/FLrqFFl.png)
Everything you say....
If money isn't an issue, I'd recommend Sitefinity. I'd say about half of my projects have been in Sitefinity, and the other half in Umbraco. Umbraco is really good, but the interface isn't really that pleasant.
Composite.net I am surprised that no one has mentioned this CMS. Maybe it is not as good as I thought? What am I missing...? I would put it up there together on the same level as Umbraco, if not a notch higher.
I've used Umbraco, Orchard, DNN, Ektron, and Sitecore. Umbraco is probably my favorite out of the bunch with Orchard being a close second. Ektron and Sitecore have their purposes... Or so I'm told. You can get what you want done with them, but it will make you want to cry a bit. DNN... I'm sorry you've used it. No one should be forced to do that.
Umbraco.
It's possible to basically use Orchard as some kind of framework around your mostly standard ASP.NET MVC app that you would create as a module, so in that sense it gives you a lot of freedom. At some point, someone got Orchard to run on Mono, but from what I understand that's no longer the case. I think some people are still trying to get that working. You can run Orchard with mysql even on Windows though or just host for the schools, that would mitigate the cost on the MS licenses. I don't know how much a Windows Server Web Edition license costs but it can't be all that much, especially for a school.
We used umbraco to do the Hoosier Lottery's [web site](http://www.hoosierlottery.com), and I really liked it. It's relatively easy to customize, too, if you need to go that deep into it. 
The author seems to assume .NET's failure, without actually explaining how .NET has failed or what "failure" means.
&gt;It was supposed to destroy Java Nope
This post must surely be link bait. .Net hasn't failed. The IDE for .Net (Visual Studio) is one of the best, if not the best, IDEs available. Microsoft SQL is an incredibly solid and powerful database. The entity framework and LINQ are also incredibly powerful. The language continues to grow and compared to many others is incredibly mature and well documented. Microsoft has admittedly taken a while to get their heads around open source and communities but they're getting there. Despite this .Net is still used by many. Source: I own a software house and we specialise in .Net.
I think you mean "insistent" but yes, I agree entirely.
Do they support dot net 4.0+ yet? I know they rolled back support for it at one point because they didn't have the development capabilities to support it.
Most definitely: .net 4.0 has been supported since v4.5, and required since v4.6 - and they were released about 2 years ago. I'm currently finishing off a site running v6.1.4 and I'm building against .net 4.5 Here's the minimum hosting requirements for the last major releases: http://our.umbraco.org/wiki/recommendations/recommended-reading-for-it-administrators/minimum-system-requirements
I've never used Sitefinity but people I work with loath it. Any particular reasons you like it?
Might have to switch back... My host runs all my domains under the same app pool, and everything else I did was in 4.0
It was just our preference. It seemed a lot more user friendly...for the developer AND the end user. We created tons of custom controls for the admin area too, which was harder to do in Umbraco...at least at the time. I haven't used version 6. But honestly, for side work, I use Umbraco since its free. 
why bother?
Check out the tutorials at http://ASP.NET (Learn button in the top menu) and there are probably some videos at channel 9: http://channel9.msdn.com/ 
After you exhaus what's on http://asp.net/get-started your next step is to head to http://www.microsoftvirtualacademy.com, there are a number of courses there which can help you.
The best resource for learning this stuff quickly that I've found has been pluralsight. It's not free but it's a justifiable expense imo. 
Intellectual property protection -- if they want it, make them work for it.
Yah, I was thinking about doing a plug-in with either Drupal or a Ruby CMS earlier (meaning I'll have to learn one of them -- at least well enough to work with a group of people on it). That way the only thing I'd have to do is test it with each new release and I wouldn't have to maintain as much stuff like overall security of the entire application. Thanks!
I'm not sure I'd suggest it for someone outside the industry. Don't get me wrong, I love Pluralsight and I'm really looking forward to seeing their recent acquisitions fold into the mix of their current courses but until you know that it's your career, it's kind of an investment I don't think most people would make.
\&gt;_&gt; I've been found out. lol
I found the MVC Music Store project to be really insightful and well rounded. I suggest learning by doing, and doing as in: download the project and dig into it, follow the tutorials.
Necro and me discussed a bit in message and he was under the impression that this version of the book was an updated version of the one he used. Please don't get this book, learning something as complicated as MVC requires good instructions or good examples, or both. This has neither. It does a decent job conveying a few big picture topics though. But please find something else to learn.
Pluralsight has the greatest learning content. It comes at a price, but if you're a professional developer, it's justifiiable.
Unless you really have to, you should learn ASP.net MVC instead of class ASP.net. Classic asp.net is really horrible. It tries to abstract away from how HTTP works but the abstraction is leaky and you run into all kinds of oddball things to work around the stupid abstraction. ASP.net MVC does not do this. You will learn more about how HTTP works and what you learn will be transferrable to other MVC web frameworks like Ruby on Rails. Maybe not directly but the general concepts. I liked ASP.net MVC in Action. It covers a wide swath of best practices and while I don't agree with all of them I do agree with most of them or at least find value in knowing them. My recommendation would be lukewarm though as I expect there may be better resources now. On a side note, not knowing classic ASP.net is actually a good career move in my opinion. If you do know it, you'll often get stuck working on shoddy old code that makes you consider another career choice. So it is a good filter for potentially bad jobs.
Still waiting for "Why InfoWorld failed to make a mobile site".
TekPub has a ton of free videos that they offer. Some of them are a bit old but they are still good resources to get started with: http://tekpub.com/blogs/tekpub-free-bin/tagged/asp-net-mvc-concepts http://tekpub.com/blogs/tekpub-free-bin/tagged/mastering-asp-net-4-0 http://tekpub.com/blogs/tekpub-free-bin/tagged/net-open-source http://tekpub.com/blogs/tekpub-free-bin/tagged/concepts
A new thread for each job.
The article is unreadable.
What was the point of that piece of shit "article"?
it's spam
MVC4? very tempting. I'm going to have a look!
So most of that was... meh... but [this part here made me laugh](http://www.youtube.com/watch?feature=player_embedded&amp;v=iA-TyEeBBio#t=132s)
Just some updates. I made some optimizations to reading huge repeated structures when doing binary parsing and the combinator is only 1.5 times slower than regular c++. not too bad 
It felt like they're trying too hard or something. The only part I laughed at was the part you linked.
It felt like they're trying too hard or something. The only part I laughed at was the part you linked.
As long as we got that one laugh...at the best part :) 
I was curious if anyone had interest in contributing. It's very basic at the moment (you can draw rectangles and ellipses). Mostly an educational project for me to learn about DX and more about C# and programming in general. I'm looking to keep it open source for use by anyone.
ah...cool....thanks for all the info...I'll add some more info Readme updated and MIT license applied. Hopefully this gives some more insight into what i'm after
I may be interested in knowing more. I have worked on several of my own c# graphic related projects. Never built a good functioning library since it was mostly college work.
[LINQ 101 Samples](http://code.msdn.microsoft.com/101-LINQ-Samples-3fb9811b) is my cheat sheet.
You could do a BeforeBuild on Project3 to copy the resources.
In addition to using a pre or post-build hook, you could add a reference to Project1 in Project3 or make the files actual resources. Unfortunately there's no simple switch to make it copy content items from indirect references.
How do you learn about this bit of VS? Everyone I work with doesn't know it.
&gt; But why do you not have admin rights on a developer machine? Ahhh, I sense someone who has never worked in an "Enterprise"...
Just a few, including the for the DOD... you should have admin rights on a developer machine. If they can't trust you... why are you writing their software? Edit: Also, you can have local admin without having to be a super admin for the whole network.
Yes. NuGet packages go in a /packages/ dir under the solution, so there's no need for them to touch anything your user can't.
DoD is basically "Enterprise" with the occasional trip to the rifle range.
All the bashing? Unless you're talking about private messages, only 1 person asked why you didn't have admin rights on dev machine. Which is a valid question. 
You can install nuget packages in vs2010 without admin rights. You just can't install the nuget package manager itself without admin rights.
First of all, major kudos for your dedication. One possible route may be to start with a sample .NET web appliaction, find some features it's missing (this will NOT be hard), and implement them. Numerous sample apps exist, including [this one](http://dinnernow.codeplex.com/), [this one](http://blogs.msdn.com/b/jmeier/archive/2011/08/02/sample-application-for-net-4-0-layered-architecture-and-ddd-patterns-sample-applications.aspx), [this one](http://www.asp.net/mvc/tutorials/mvc-music-store-part-1), or [this one](http://nerddinner.codeplex.com/). Alternatively, come up with your own idea for an application, and implement it using the frameworks, patterns, and coding style of one of these examples. The advantage to starting with a sample application is that most of them use real world patterns and frameworks. Another option to consider is asking if they could assign you a few low priority bugs that you could work on in your free time (you'd be spending time creating a website for your portfolio, anyway). However, depending on the size/complexity of your company's application, this may not be feasible (sometimes configuring a development environment can take days). You seem like a go-getter, so in addition to the topics mentioned in u/AbstractLogic's excellent post, here are some things you could learn to really knock their socks off! **Database** (In addition to what AbstractLogic mentioned) * Learn about the Query Analyzer, Execution Plans, and how they can help you improve performance of long running queries * Learn about indexes * HAVING clause * CTEs **Frameworks** You may consider asking exactly which frameworks they employ to focus your efforts, but here are some common ones. * ADO.NET (people aren't using this directly as much anymore, and are using O/RMs instead, but knowing ADO.NET is still crucial - it's what the O/RMs use under the covers) * O/RMs such as Entity Framework or NHibernate (Fluent NHibernate) * IoC containers such as Autofac, Unity, Ninject, etc. * Logging frameworks such as NLog * Automapper/Value Injector * ASP.NET MVC * jQuery * Underscore.js * Angular.js * Twitter Bootstrap **Architecture and Patterns** * Domain Driven Design (book by Eric Evans) * Read the ["Gang of Four" book](http://en.wikipedia.org/wiki/Design_Patterns_(book), paying close attention to Lazy Initialization (used by O/RMs), Factory, Singleton, Factory, Decorator (attributes in C#), Proxy (used by O/RMs). * Read Martin Fowler's *Patterns of Enterprise Architecture*. It won't all make sense it first, but re-read it every 6-12 months while you're working until everything becomes clear. Patterns to pay close attention to (i.e. in common use in modern .NET development): Repository, Service Layer, Data Mapper, Unit of Work, Model View Controller, Page Controller (used by WebForms), Data Transfer Object (used by web services), and Command * Read up on Automated Unit Testing and TDD (most shops are using these now) * Learn [SOLID](http://en.wikipedia.org/wiki/SOLID_(object-oriented_design)) * Learn the principles of Agile development (DRY, YAGNI, KISS) **.NET and C#** * Read Code Complete. It's dry, but it really does a great job of explaining how to write production quality code. * Another great book that will take your understanding to a whole other level is CLR via C#. It dives into the internals of how .NET works, and you will learn every language feature in great detail. Seriously, if you read and understand everything in this book, you will do REALLY well on the C# portion of phone screens, and it will improve your debugging skills by a considerable margin. Best of luck!
&gt; Hope this helps. Scott Gu? :)
And protected internal! Very few interviewees get that one right.
Ah yes lol. And when I am interviewing for VB (not for a while now) they never get (usually never even heard of) the Shadows keyword (which in c# is accomplished via the new keyword). Nice catch!
these ideas are all well and good, but they seem really focused on the "I can pass a multi level certification test" path, opposed to the practical application path. My humble submission for OP is to find a practical, yet manageable, problem you can to solve for your company. Maybe it's a small data entry system for a business unit like purchasing. I suggest you start small and find your niche. You'll never be a master of all tech, or even software, but taste what you can, then take the bites you're hungry for. You may end up finding your opportunities outside your current organization. FWIW, I started in simple Access &amp; Excel VBA for a small group in a mega telecom then joined a small firm doing TSQL, ASP.Net, VB.Net &amp; C#. Played at DBA, hated it, found my niche in RIA/UX with Silverlight, some WPF, ASP.Net MVC and loads of js. (edit added my path)
&gt; "I can pass a multi level certification test" For the benefit of OP, I would like to elaborate on this point. There's nothing wrong with certifications, especially starting out - it shows a "go got 'em" attitude for entry level developers, and it's particularly helpful if you don't have a 4 year college degree. For individuals WITH a degree (or 2), as a hiring manager, it doesn't tell me anything. I already know you can study for and pass a test. I would much prefer a blog, github account, or stack overflow account that shows insightful thinking and problem solving skills. &gt; You'll never be a master of all tech, or even software, but taste what you can, then take the bites you're hungry for Sage advise. &gt; You may end up finding your opportunities outside your current organization My recommendation to OP is that if you don't have a 4 year degree in CS/SE/EE/natural science, and you get a developer position, stick with it for at least 2-3 years. It'll make the next jump far easier and more profitable, because assuming all goes well, you should be a mid-level developer by then, and employers are less scared by the lack of paper on your wall. Conversely, don't let them string you along - if you don't have a developer position within 4-6 months, start looking. The job market is hot right now.
Some of the decompilers now have deobfuscators as free plugins.(Like JustDecompile) So really, you are talking about a couple clicks to deobfus, and another click or two to export that assembly to a visual studio project. 
Since this is C# if you want to show some solid skill some other things that will impress: * use of closures * use of the task parallel library * Create a class than can accept a Func&lt;T, bool&gt; type and use it internally * Create a generic class, show examples of covariance and contravariance * Use Reflection to load an assembly or query meta-data for a class * Use an IoC container * Create some unit tests * Create a custom .project or .targets file and use it to drive a more customized build
All good advice. Though half of that I can't even say I've done after 6 years of being in the industry. Its hardly junior developer stuff but it will certainly impress.
I can vouch for this book, it's good. They build the same sports store app in the latest edition. A plus, I think, is that they also include the use of dependency injection (with Ninject) and testing (with the standard MS test tools and mocking with Moq) while working on the app.
You don't create child classes from interfaces. They aren't inherited, they're implemented. Also templates are C++. C# the concept is generics.
Try your question on serverfault, but don't expect somebody to do it for you there: http://serverfault.com/
It all revolves around MSBuild. All Visual Studio project files are actually MSBuild files. If you look at their source, you'll see they reference special Microsoft .targets files, which is where the entire build process hooks in. So this means you need to get reasonably comfortable with MSBuild, and then you can dig into the Microsoft targets files. But they're pretty heavy. It can help to up the output level in Visual Studio to really let you know what's going on: Tools &gt; Options &gt; Projects and Solutions &gt; Build and Run, set MSBuild project build output verbosity to Normal or higher (then look at the Output panel after the build).
I cant get the drive to install property without getting it signed 
Isn't SS now dual license?
V4 will be. V3 will forever be free under a BSD License. It's pretty mature and hasn't had any major features added for months (ain't broke, don't need fixing.)
Where is this info on V4? The only thing I could find is [the current license on github.](https://github.com/ServiceStack/ServiceStack/blob/master/LICENSE)
https://plus.google.com/u/0/106787359118990653189/posts/g8TcZaE7bv9
So somebody offer up some cons for this.
I'm possibly in, what do you want adding (My DX knowledge is on par with yours but I do know XNA which is not a million miles away)
This looks pretty neat. For our current project, we gave Agatha a swing for our service layer. But I could have used this instead. Especially the multi-format support looks nice. Can anyone link to some best practices on how to implement authentication and security in this system?
Now that you've asked this, I definitely need more of a roadmap. If you read the readme file on GitHub it'll give you a decent idea of what I'm after, but it's not quite that specific yet. I'm gonna work on specifics this week. Maybe even write some skeleton classes. 
I wonder if I'll have to uninstall RC when the RTM comes out.
Be sure to check out [Peta Poco](http://www.toptensoftware.com/petapoco/) too. I like it a little better than Massive.
I've used [Dapper](http://code.google.com/p/dapper-dot-net/) and [PetaPoco](http://www.toptensoftware.com/blog/). I stopped using PetaPoco when I got StackOverflowExceptions with inserts that used a large amount of parameters: INSERT INTO stuff (column1, column2, column3) VALUES (@column1_1, @column2_1, @column3_1), (@column1_2, @column2_2, @column3_2), ... Can anyone compare Dapper vs. ServiceStack ORMLite and Massive?
This is awesome, will save so much time. Was hoping VS would have an answer to Webstorms Live Edit mode
It's a go live so in theory you should be good.
Good. Uninstalling VS can be a nightmare sometimes.
Re ASP.NET, I love innovation, but I feel I can't keep up with all the Membership Provider / Identity stuff changes going on. I'm not even sure what's the supported / recommended way of doing website subscriptions now.
1. Make a company/LLC 2. Sign up for BizSpark 3. Hope you get accepted
The description of the product mentions a 2 or 3 year MSDN subscription; what level of subscription is that for?
I did it and at the time the only requirement was that you were working on a product or service (not consulting) and are small. Also they wanted to charge you $100 after the 3 year period. They have recently waved the $100 I think and I never received a bill for it. The downside to BizSpark however is that I am now dipping my toes into Rust and Scala as I really don't want to spent ~$500 on VS 2013 Pro + ReSharper 8. It hurts when your access is suddenly cut. It's a really good program over all but I felt that the major problem was that the program needed to exist.
I have not used Massive yet but to be quick about it I found ServiceStack.ORMLite to offer more functionality with less typing than Dapper. On the other hand I had a lot of trouble getting the x86/x64 version of SQLite working with SeviceStack and have recently switch that back to Dapper with hand made create and 'upsert' statements.
Not sure on the Biz Spark, since I do mostly contract work and didn't qualify for it. I did qualify for Website Spark (a discontinued program), and got VS Professional through that one. My subscription actually expires this month, so I may have to cough up the $1000 next year. 
I believe you only have to pay if you've turned a profit.
I know they like to get paid for all their hard work on their products, but really, this tightening up on the pricing for development tools isn't a good idea. They've already lost a lot of market and mindshare to iOS, etc. and are facing resistance to Windows 8 adoption. This isn't going to make it easier for them to maintain their position, never mind advance it.
I agree to a certain point. They do give away a pretty nice set of tools with the Express Versions, and VS Pro (without MSDN) is &lt; $500 on Amazon. When the release cycle was every two years or more (2005, 08, 10, 12) it wasn't as bad, but when they are switching to a yearly build, it could get too expensive to continually move forward. 
&gt; A friend of mine recommended registering a DBA and joining the partner network. Does BizSpark require you to actual create a product (sucessful or not)? Technically no. When I had it, it was touted as $100 fee at the end which for a MSDN subscription for 2 years you would be silly not to accept. But then at the end they were like "congratulations - we waved the fee!". I was like "woah...awesome" and you get to renew the subscription for really cheap at the end (I think I paid like $1500 for MSDN pro for 2 years?). They also offer discounted Windows Server licenses. You need to call though...because I was supposed to get some number to use on the vendors site but when I called the guy at CDW was like "that's ok...just give me your CC and I'll hook you up". Honestly, the free VS editions is probably good for most indie people. However, which then I would have recommended technet...but they are getting rid of that program. There were others but I don't remember what they were. I really wish Microsoft had an open "home use" program. Where for some nominal fee per year ($200?) you can get access to Windows/Office etc for personal home use only. It's really a win-win - people can get cheap legal access to Microsoft tech and at the same time boosting their market share. Microsoft should be getting people hooked because when they go to work they should want employees to say "I use Windows at home, and that's what I'm familiar with, so please purchase me a desktop with that on it".
I started using a nice one back around .net 2. Worked well for all tag versions. I'm sure you've come across it while writing yours. Wish I could remember the name. By now i'm sure it's outdated though.
taglib-sharp?
Basic question, what does one need an MSDN licence for?
Good stuff :-). Any plans to convert this to a PCL so it can be used across .NET 4.5 / WinRT / Win Phone 8?
Kinda cool. I have been wondering how to do this. 
The first thing I look for, is a way to share objects and methods between the two environments, I don't understand why it's not the first code example Guess I'll have to click around to find that
It gives you access to all or most of Microsoft's products for a cheaper price than buying them separately.
Without having had a chance to actually play with this yet (my workday began, dammit), I would think JSON (.NET --&gt; js) and/or .NET Dynamics (js --&gt; .NET) would be a logical starting point.
It's probably one of these I've reviewed: * UltraID3Lib * TagLib# * IdSharp * ID3TagLib.NET * ID3Lib (puremp3) * Audio Sound Editor for .NET * C# ID3 Library * ID3.NET But I found those libraries to be lacking one or more of the following: * Open source * ID3v2.2 &amp; ID3v2.4 (sometimes limited to 2.3) * Ability to read all ID3v2 frames * Ability to show data in all ID3v2 frames * Bugs / inactive project
Look, I understand that we are all now trying to differentiate with blogs and such to impress employers and perspective clients. It's important to note though that blog posts like this are utterly useless to your fellow developer and anyone with a clue that takes a moment to read your postings will think you are blowing smoke. Please don't waste time and effort on a list that anyone can create with a google search on ".NET decompilers". Rather create an article doing an actual compare and contrast of a couple of the tools, walk through some examples and explain why/how one compiler may behave differently than others. Bonus points for some power user tips of the tools for new users. 
Yep, I found that eventually, pretty sweet
That, sir, is very good question! (and im afraid answer is "because"...)
If you think it's important information, try sending an e-mail to Microsoft about it - sometimes they change the MSDN layout based on feedback.
I had never even thought about it 'til now. Given that the function signature includes the return type it seems like a fairly glaring omission. I think I tend to click on the specific method that I'm interested in to read the remarks and examples. Gotta RTFM after all. 
I came to .NET from a Java background and this have been my pet peeve for years! 
The return type is actually not part of the method signature in C#. This is why you can't overload methods based on differing return types.
http://msdn.microsoft.com/en-us/library/yae1tyf0.aspx Clearly states what type the return value is...not sure why this is such a big deal...
As /u/jfx32 pointed out while the CLR may care about return type other languages do not. CRefs (code references) are used as the unique identifier for members in systems like Sand Castle. They *can* include return information but as I have seen only do for implicit/explicit conversions. Because of this implementers of the doc tools may have just thought about CRef as the required information and totally forgot about return type, I know I did! That is a great suggestion I would also like to see in MSDN now that you mention it. I hope this thread gets their attention.
I think OP means that in the methods section of the class page, there is no return type column, as there is in javadocs. When a given method is selected/page loaded, the return type is present.
Not sure what specific products you're looking for, but if you have a .edu domain email address you can sign up for Dream Spark. It includes VS Professional, SQL Server, TFS and some other software. 
The documentation is fantastic and is at least understandable.
In most cases you can infer it from the comments. In the case of StringBuilder, most of the methods return "this", allowing you to chain method calls.
While I always stop at MSDN first, there is usually some genius on StackOverflow that explains it better.
And those same geniuses will _also_ often link to the relevant MSDN article and/or C# specification while explaining it. It's always a good bet I stumble on some new tidbit I never knew before.
If you have ever have to use the KendoUI documentation it will further enforce this concept. 
There are tooltips if you hover over the icons.
Yep.
MVC is a great pattern for developing software. Basically what it provides you is a clear separation of concerns: * **The M in MVC stands for Model** - this layer handles the data logic in the application. * **The V stands for View** - this layer handles any presentation logic * **The C stands for Controller** - this layer handles the business logic - basically processing and preparing the model data for the view. And then you have the routing - the route provider actually provides a single point of entry to your application - all requests (for example http://exampleurl.org/Users/Login or http://exampleurl.org/Cars/Details) are being processed by the route provider and then dispatched to the appropriate controllers and their actions. The **ActionResult** is an abstract class that has a lot of implementations - it provides you with versatility so that you can return different kinds of data. Most used classes that implement it are View (returns the View that corresponds to that action), PartialView (returns a partial view - basically not an entire view, but just some html code that can be inserted anywhere - it does not contain any head, body, etc.), File (returns a file), Json(returns some data as JSON). Regarding **Razor Views** - if you have experience with C# and HTML they should be fairly easy to understand - in a nutshell they are HTML templates mixed with some C# code, that are compiled at runtime. These are pretty much the basics in the pattern. I can understand how you might feel a bit overwhelmed. However you can try replicating the demos that he does, because they provide you with the basic information that you might need - you will learn the rest of the abstraction behind the framework with practice and experience. I think you should read a bit more on MVC and the different frameworks that use that pattern (for example CakePHP, which, as you might have guessed, is written in PHP, Django in Python or RubyOnRails in Ruby). Once you get the basics of the pattern you will start understanding why it is so clever and how easy it becomes to organizes big projects. Btw Scott Allen's free course is hands down the best tutorial in ASP.NET MVC at the moment (not only on pluralsight) so I'd recommend trying to finish it.
The Apress books are great. 
Havnt tried it jet! But i think i would feel the same as you do. Looking also for a good tutorial or book.
It isn't so much about what MVC *is* or what you are *trying to reach*; but it's more about **what you are replacing** in the first place to understand **why it is a real world application**, when you grasp that you will be able to understand what it is and be able to decide if it is what you want to reach for. So, let's take a typical old style page; what we did back in the old days was a single file that had all this in it: - Queries to get information from the database. - Logic that determines what to do with the request and the queries. - The actual HTML code to present the results to the user. Everything thrown together in one file isn't clean; it doesn't separate concerns and also isn't allowing for code reuse, so what we want to do is split up the file. First of all, we want our presentation in its own file; so, we create a new file where we can pass our results to, we now have made a **View**. We can now do the same for our queries, also putting them in separate files in classes and functions we can pass parameters to; so, now we can call something like `getPersonsBetweenAges(lowerAge, upperAge)` wherever we need to get a group of persons between two ages. We have now made our **Model**; but, we can't house queries here, we can also put entities here and instead of writing bare queries put those entities in the database *or* keep them in memory if you don't want them stored in a persistent way but just used during the time span of the page generation. Now that we have Model and View split out; we are left behind with something, something that deals with the request and encapsulates the logic between the Model and the View. The logic we encapsulate become **Actions** of a **Controller**, and the part that deals with the request becomes the **Routing**; so, when a request is made the routing decides which action of which controller is called and passes along the request, the controller action then uses the split Model and the View to get information from the database presented to the user according to some logic. All the rest what you come across are just additions that might make the experience better; better ways to do the models using some other database paradigm, better ways to do controllers (not sure if there are, but I guess you could make multiple tiers for large scale applications) and better ways to do the views by using something that makes it easier to work in a templated way or use easier syntax (Razor) or something among those lines. If you have past experience PHP most of the above is explained in much more detail with PHP code examples at http://symfony.com/doc/current/book/from_flat_php_to_symfony2.html which you can use to get a deeper understanding; these examples are for the Symfony framework, but they are not too different from ASP.NET MVC 4 so what you learn there will be usable for understanding ASP.NET MVC 4. And as a bonus point, if you ever decide to switch or need to work in a PHP context; you have heard about at least one way to do it in PHP. It also kind of depends on what OS you run and which environment you are in...
Have you tried the Music Store? http://www.asp.net/mvc/tutorials/mvc-music-store/mvc-music-store-part-1 It helped me easily. That said I have always used WebForms in a similar manner, separating concerns of the 3-tier levels: presentation, logic, data access with object data sources so the transition really wasn't that bad for me...
I'm going through the exact same process right now. I'd suggest watching the MVC 3 series first (also by Scott Allen). It covers a lot of things that he glosses over in the MVC 4 series that probably aren't obvious if you aren't coming from a web background. I watched the MVC 4 series first and came away pretty confused. I went back and watched MVC 3 then rewatched MVC 4 and found a lot more of it clicking the second time through.
I also pretty much come up against the same issue. Been a Webforms developer for 2years since leaving university. Know C# and .NET pretty well, know jQuery and Javascript well and also very competent in HTML and CSS.....only problem is work is not that interested in going towards ASP.NET MVC, I really want to learn....but all the examples are too basic and I can't relate them to real world business applications....
Here is the magic piece that it sounds like the presentation glossed over: Controllers are classes, complete with methods. Each method maps to a URL, which means _any HTTP request for that URL is actually a call to that method_. That's the important part. There are a lot of details- each method thus exposed should return an ActionResult. If that ActionResult is a ViewResult, then MVC is going to execute a Razor view. There are all sorts of options- partial views, strongly typed views, etc. If it's a JsonResult, then MVC is going to do its best to serialize your return to JSON. And so on. But the important part: *Controller methods are mapped to URLs and return action results- HTTP requests invoke the method, the return from the method is the HTTP response sent back*.
This. It may take a while, but go through each step in the tutorial and actually write it out in visual studio and you will gain significantly more understanding than by just reading tutorials.
okay, thanks for the advice! I'm in this weird situation where I don't know the business too well and I don't understand a lot of the class structures yet and I'm also learning this new technology (MVVM/EF/PRISM). Unfortunately, the people who could help me out are obnoxiously too busy so I'm feeling a bit overwhelmed. It's just nice to see a frame of reference and others experiences. 
I've been in similar boats before. It might sound unlikely but at some point you'll probably look back on this time as the fun period of your job. It's frustrating because it seems so overwhelming but you also get to make more day by day progress in this phase of your learning than at any other point. Some years will go by and you'll find yourself no longer intellectually challenged by the work and then you'll want to be back where you are now. You have the advantage of having the original architect still there, even if his availability is low. Keep track of the questions you come up with during your learning experience and ask him/her why they did what they did. I've been an architect for a while now and I love when people start digging around in my old code and want to talk to me about it, even if a lot of the answers are "because I didn't know any better". It mostly only happens with new hires because after a while you'll stop caring so much about how it works and just be glad that it does (hopefully). I also find it a useful exercise for myself to try and support my decisions. Sometimes people will ask a question that never occurred to me and it will change the way I do things in the future.
&gt; All the rest what you come across are just additions that might make the experience better; ................ &gt; As someone who is also learning MVC... thanks for this! 
Check out edge.js for the reverse - interop with the CLR from nodejs!
As an alternative take a look at Nancy (www.nancyfx.org) 
In my experience the pluralsight courses are all over the map in terms of difficulty and/or speed through the course. So far I've watched: HTML5 From Scratch HTML Fundamentals Introduction to CSS Using HTML5 and CSS3 JavaScript from Scratch JavaScript Fundamentals JavaScript for C# Developers Introduction to Visual Studio 2010 - Part 1 Introduction to Visual Studio 2010 - Part 2 and I'm watching: HTML5 Advanced Topics jQuery Fundamentals Object-Oriented JavaScript Some of these have been, "omg, 2x speed and still I want to skip ahead" while others have been just right. 
You don't become a better coder by watching videos. You become a bettter coder by writing code and reading other peoples code. So start create a sample project for your self. Create a simple blog engine or look at some codeplex examples.
In all honesty, learning mvc via .net is difficult. I had a much easier time learning it with rails. Check out rails for zombies on codeschool.com
WebForms to MVC is a huge change. Be patient, and just try to build something simple (a twitter clone or something); it will help a lot. Once you "get" MVC, you'll likely never want to touch webforms again, so keep plugging away at it, but you really have to try to build something before it will click. 
&gt; many things from C# are not implemented There's not much that can be done in C# that can't be done in VB.net. yield for example. What else? Edit: Found this: [Comparison of C Sharp and Visual Basic .NET](http://en.wikipedia.org/wiki/Comparison_of_C_Sharp_and_Visual_Basic_.NET)
I imagine they'll be updating c# with ideas/lessons from typescript next. I can't see need for something else new.
Where are you getting your information? * [VB Lambdas](http://msdn.microsoft.com/en-us/library/bb531253.aspx) * [VB Yield](http://msdn.microsoft.com/en-us/library/vstudio/hh156729.aspx) * [VB Dynamic](http://msdn.microsoft.com/en-us/library/dd537660.aspx) In fact, VB had "dynamic" or late-binding from v1.0 of VB.Net, and doesn't need special keywords for it.
I ran away from vb after having to work in vb 6. Do peopke really enjoy using that language? Just remembering on error resume next gives me shudders
I'm a C# programmer. I love C#. It's beautiful. But I also love VB. There's something really honest about it. On error resume next is an abomination. But it's there for backwards compatibility with Vb6. So are many things. For example, this would still work in VB.net. 10 console.writeline("hello world") 20 goto 10 No one would ever actually write that nowadays. But it would work. 
Python/IronPython, perhaps. The Python Tools for Visual Studio (http://pytools.codeplex.com/) are pretty amazing, and VS 2013's "new project" page includes a link to download them.
My hope is that they invest more time with bringing powershell native to VS. ISE is just terrible. And even though its just a scripting language. Pretty powerful things can be built with it and be included with your traditional project libraries. There is nothing more frustrating than having to open a powershell file and have it just be plain text.
&gt; vb 6 Vb.Net is very different from VB 6
We tend to make heavy use of Knockout on the client side- 90% of our controller methods return JsonResult.
Check out powergui. Its far and away the best way to write powershell.
I think Microsoft is making a new big push for C++ for some reason. More specifically, in areas where you might not consider it such as web sites and apps. I've been a subscriber to the MSDN magazines for a long time and this year I've seen a large increase in the number of C++ related articles. The one sitting on my desk right now has "Using the C++ REST SDK in Windows Store Apps". There also seem to be a lot of new Visual Studio features for the language too. I don't have any complaints though. C++ was my first "real" language but I haven't touched it ever since I jumped into C#. It would be fun to go back.
We all felt a bit dumb at first getting into MVC. Things are being spread up to make it easier to maintain but that also makes it harder to grasp initially. It will take a bit of time but when you understand how routing works, what the controller's role is and how he calls up the view it will click. 
Its a shame, although the projects are not dead. If people keep working on them and more people start using them, perhaps they will be re-supported in the future.
I've only been doing this for a year and I can already understand a lot of what you're saying. The first days are the best because everything's new. I wish I could go back. The current project is just a spaghetti bowl now because nobody else felt like learning the new technologies. 
This has been tempered somewhat now that Sinofsky is out. C# devs are starting to feel the love again.
I found PowerGUI to be terribly slow, and the color scheme hurt my eyes. A text editor is preferred.
Really? What features does the paid version provide? I used the free version fine.
Well, it's somewhat needed. Microsoft are falling greatly [behind on C++11 features](http://cpprocks.com/a-comparison-of-c11-language-support-in-vs2012-g-4-7-and-clang-3-1/), but probably don't want to lose their users. So they have to do *something*. ^^Cause ^^you ^^know, ^^just ^^implementing ^^those ^^C++11 ^^features ^^wouldn't ^^do ^^it...
well considering they killed off ironpython...
VB.net love --&gt; select case statements, C# Switch statements are so clunky compared to that. Its the one thing I really miss from vb.net
Pretty much everything related to the "unsafe" keyword in C#. 
While unlikely Scala could be a possibility. CLR seems like a first class platform according to the docs I have read.
Any reason I dont see my post when logged out here?
I am so glad that I am not the only one that appreciates the value of VB.NET.
About time too. I don't mind the increased support for C++ or HTML/JS but that's no reason to actively not support C#/.NET. 
I think I see where you are at. You have a handle of each individual framework but you need guidance on how to put them together. Tutorials are usually horrible for this because they understandably go for a simple functional example. It is time you start reading about design patterns. These are recipes that apply to just about any object oriented language and help you organize more complex applications. As ASP.Net MVC/EF go, I highly recommend you check out MVVM (Model View View Model). You'll be able to find plenty of C#+ASP.Net MVC examples. I won't go too deep into it but basically it means you don't use your EF entities as models directly as that causes problems in the long run. There are some very useful patterns you can apply to how you use EF itself as well. I'd recommend you read about the Repository pattern. Do feel free to ask specific questions and don't forget there is /r/learnprogramming.
Oh I have and I do like Nancy quite a bit. ServiceStack really is the path of least resistance for most of what I do though. My job is not what you would call glamorous.
&gt; Plenty of code examples, clear explanations, easy access to previous versions of the .NET framework. Except for the really obscure class/method where a good doc and example would be most needed but that's my luck. Empirically learning how some fancy crypto class works was... fun. 
Those are the same icons used in Visual Studio. If you need to be *reminded* what they are the tooltip is fine but this is documentation clearly aimed at people who are at least familiar with what a method/property icon looks like.
It's not what you are asking for but how about instead you buy VS2012 Pro, rely on SQL Express (or some free NoSQL), Git instead of TFS? It will be cheaper to get you started.
thanks for your message. are these universal providers inherited from plain-old MembershipProviders? from a brief search it looks UniversalProvider inherits from MembershipProvider. I hope this is the case. While I don't like the structure of MembershipProviders, at least it'd be a tad less confusing. I thought they were phasing out MembershipProvider base class in favour of something else.
MSDN says it's an abstract class, unless we're talking about completing different things: http://msdn.microsoft.com/en-us/library/system.web.security.membershipprovider.aspx
Oh, you are definitely not alone. In fact, whenever I hear people say "vb is crap" I just think a little less of their opinion. I mean nowadays you would be better off learning C# than vb, but that doesn't make vb defunct, or somehow inferior. It just makes it more adept to the programming climate. 
My mistake, it is an abstract class and not an interface.
cache
It's a bit late so I'll be brief (but do ask more if you wish). How you make your classes and namespaces and how you separate those in libraries is mostly a matter of keeping things clean and easy to find. Remember, MVC/MMVM-ish is all about separation of concerns so if your repository doesn't know (i.e. has no references to) anything about your Web project, all the better. Think of it as "I'm making a web app to manage students... but hypothetically I would want to use the exact same data layer if I made a Windows app, or a phone app, or a plain console app, or whatever". So yes, make your Student POCO class in *****.Data, the rest of your classes that will make up your data, your repositories that will allow you to read/write student data and all the rest. Your web project references this so that ViewModels can have methods that call the repositories. More concretely, if your app allows CRUDing students, I would have a Student controller with List/Create(Get)/Create(Post)/Edit(Get)/Edit(Post)/Delete(Post) actions. These actions should contains as little code as possible as the heavy lifting will happen inside the ViewModels. List will instantiate the ListStudents ViewModel and pass that to the view. ListStudents will fire-up the Student repository and call the GetAll() method and stick the result into the AllStudents public property in that ViewModel. The List.cshtml view is strongly bound to ListStudents and will more or less loop over Model.AllStudents and show whatever values you want for each student (you could get away with AllStudents containing the EF entity but ideally that would also be another ViewModel). In any case the repository itself does not return your ViewModels (that would make the ****.Data dependent on the ****.Web project). You should better have ViewModels that take your entity class instance(s) as a constructor parameter so it can put it just the way you need it for your view. Writing code to move values from one class to a very similar one (and back in cases you are doing Adds/Updates) is a hassle which is why there are some very nice tools that let you do it. I'm very fond of ValueInjecter because defining my own mappings for special cases is very easy. Many like AutoMapper but I'm not fond of how you define special mappings (i.e. lots and lots of noisy fluid method calls). 
&gt; "In any case the repository itself does not return your ViewModels (that would make the *.Data dependent on the *.Web project). You should better have ViewModels that take your entity class instance(s) as a constructor parameter so it can put it just the way you need it for your view." This helped alot for my understanding. Thank you again for your help. I think I am beginning to understand more clearly. Please try to follow along here with what I'm saying if you can and let me know if I am misunderstanding or missing any steps. So lets say I have a page just to display one student, a GetStudent Action of my xyz controller. In this method I would instantiate my repository and then call my Repository.GetStudent(id) and get the returned Student into a Student s variable. After that I would instantiate a StudentViewModel passing in my Student s variable into its constructor, the constructor would "throw out" the values I dont need and copy the ones I do need to my StudentViewModel's corresponding properties. Then simply pass my viewmodel to my view and display it. If anything is wrong up until here let me know please. Now for CreateStudent (Post) action I would take in a StudentViewModel. Instantiate a Student class, use the StudentViewModel I took in to fill in most of its properties one by one and then fill in the missing Student properties myself? Then call my Repository.CreateMethod() passing in this student as a parameter and let it handle adding it to the db? Thanks once again for all the help. Its becoming alot more clear to me now. I have no idea what ValueInjecter/AutoMapper is but I will definitely look into it. I still have a few questions about viewmodels/models and repositories but I'll wait to see if my understanding of this is right so far.
So for this GetStudenViewModel(id) method the code would be something like?: public StudentViewModel GetStudenViewModel(int id) { Repository r = new Repository(); Student s = r.GetStudentById(id); StudentViewModel svm = new StudentViewModel(s); return svm; } Would this be a static method in my StudentViewModel class? And my CreateStudentViewModel.Save() method would be?: public void CreateStudentViewModel.Save() { Student s = new student(); s.Name = this.Name; s.Grade = this.Grade; s.etc = this.etc; Repository r = new Repository(); r.CreateStudent(s); } In this method do I fill in all the properties one by one to a real student model and then pass that to my repository method responsible for creating students in the db? Assuming I do not use ValueInjecter/AutoMapper.
new GetStudenViewModel(id) is my constructor for the Get view's viewmodel. StudentViewModel should be the type of the collection that ListStudentViewModel would have of for the single "Student" property the GetStudenViewModel would have. As for the repository, yep because it is already sort of an unit of work as it is, Commit will just call SaveChanges. This is important as what's using the repository (in this case a ViewModel) shouldn't be concerned with EF at all. For all it cares you could dump EF and make your repository work with MongoDB and anything using the repository shouldn't change or care about. Separation of concern is the golden rule guiding these patterns. Always ask yourself "should X care about Y?". The less each component needs to know about the other, the better. There is no hurry to do this but ideally you repository should be just an interface where one implementation would work with EF. Not that your web project will care, it will just call methods and properties based on that interface.
Thanks lol. Im an idiot and was blind to see the "new" in var model = new GetStudenViewModel(id);. Another question I have is: For the Student model, and CreateStudentModel, do we add Data Annotations to both of these? Like [Required] to the FirstName properties of Student and CreateStudentModel? Are all Data Annotations for properties repeated on both? Are some data annotations only on the model and left out for viewmodel, or vice-versa?
Data annotations are best on the ViewModel so you could in one form have birthdate being mandatory and in another not have it shown or edited at all. Frankly I found data annotations a bit limited and used FluentValidation instead. Much more flexible.
Any reason any data annotations would be placed on model class properties? Not viewmodels. I've read a little bit about FluentValidation, I'll look into it more now too.
I'm not so much against people posting blog articles... they just need to be interesting, concise, and accurate. This article just doesn't fit any of that criteria.
Don't get me wrong, your EF classes have data annotations because that's how you define how things will be stored in the database. But your viewmodels are the ones you'll use to validate user input and there you may need to be more/less strict (i.e. not ask for a mandatory field that you will autofill server-side based on Student age). So EF entity data annotation are good to say "I want this field in the DB to be of a string of size X". In your viewmodel you can get fancier and say it's an email and to make a form that validates for proper emails client-side via automatic unobstructive javascript validation.
So Data annotations added to EF entities would just be ones related to how stuff about the entity is stored in the database and related to the schema etc? And ones on viewmodels would only be about validation and display?
I prefer letting a filter handle the data type. The controller has no need to know. 
Don't worry about routes, selectors and filter for now. The default ones should be more than enough for you. Put the data stuff (CRUD) in the controller and put the html in the view, you generally pass data to the view using a model and that's the three key parts of MVC. Later on, when you want/need fancy urls, that's when you need to learn about routes and action selectors. At some point you will see a lot of duplicated code and that's when you might find filters useful. Forget everything you know about webforms, but much of what you know from classic asp and php will still be useful.
It's hard to say if it's lasagna code without more details, but knowing .net developers as I do it's entirely plausible. The call stack isn't the correct way tell if it is or isn't. How many actual layers are there and how many are doing something useful? Useful here meaning doing something, not just forwarding and transforming information.
The controller method is always typed as an ActionResult. It's just that most of the time, the actual type is a JsonResult. 
Just keep in mind that `MsieJsEngine` won't work on Mono. Is there a reason this doesn't just use the native .NET LESS port?
Exactly. Notice what you just said. It's as if each thing takes care of their own duties and that's the very definition of separation of concerns. It's not because business dictates that a valid student should have his date of birth provided that the field should be required in the DB. That would block you from saving partial student data and will quickly be too rigid. Plus cases where business says "field X is mandatory/oh we changed our mind" would cause problems if you have to update your DB later in. Much easier, flexible and safer to keep that kind of restriction in you ViewModel which is closer to business logic. Of course this is just an example and doesn't applies to all things. A student with no name might be more trouble (hard to find in a list/search form) than the flexibility is worth it so you may consider making that a required field. 
holy cow. I didn't realize that.
Your partial view is essentially a user control. Use Razor to render the custom control and inline it in your main view. @RenderPage("~/Views/Shared/UserControl.cshtml", ViewData.ControlData.Model) Then inside UserControl.cshtml just use standard control flow to change the control (eg. add additional classes) based on the model you passed in to the control.
You could try to make a razor helper function, read about them [here](http://weblogs.asp.net/scottgu/archive/2011/05/12/asp-net-mvc-3-and-the-helper-syntax-within-razor.aspx). If you want to add more classes to the &lt;p&gt; and &lt;input&gt;, just add a string or a string array as parameter to the helper function.
I'm not sure if this is blog spam or not. It's an android app written in what looks like fucking Pascal. Pretty interesting, but I'm going to downvote anyways
We have the same thing with custom settings changes but the web.config transformations all perform properly on publish, for us. But like I said in my other comment it was a horrible thing to setup and we ended up having to get a clean VM just to get it to work for production.
It's even worse with DBA's who think there is 1 correct way to handle names and addresses. After I become supreme ruler of the earth of enforce decimal time on everybody, assigning people GUIDs instead of names will be next on my list.
Can't wait to see the lasagna being produced with these fancy tools.
Agreed. This sounds more like a case for a helper method than a partial view.
In general, it's a good set of articles (it's in two parts, in August and September 2013). I do have one quibble with her approach: she's advocating using methods for all mutators. In cases where a property change is atomic or is used as a trigger for other changes in the object, I believe a getter/setter pair works just fine. I do hope she'll cover cross-aggregate references; many people get tripped up with that concept, especially as it pertains to data access (when does one perform an eager fetch, etc.).
What I did while teaching myself was build on a program. I wrote a call log program. It started with a simple program that simply took my input and inserted into access. By the end it pulled customer data calculated call time and length and was multi user. My point is seeing your app grow gives you an amazing sense of pride.
If they are that fresh to coding maybe start with a dynamic language like ruby, python, or JavaScript. Those are more forgiving and get people excited about programming. If you can't do that I think your idea is great too. 
Dude they don't program. Learning multiple languages as a qa isn't realistic in this scenario.
Give them the basics then have them start writing tests. There's a lot you can do with c# but you're training them for a specific job not to be developers.
Start with the [msdn C# tutorials](http://msdn.microsoft.com/en-us/library/aa288436%28v=vs.71%29.aspx). 
Prepare lots of exercises for them to do that progresses in difficulty, announce a pass/fail exam at the end of your course, 10 minute break every hour, free coffee available, daily quizzes, hand out some light material before the course starts (so you don't end up using half a day at the beginning setting up the IDE and they know the difference between an int and a decimal for example so on) and give them a bit of homework. This is my recipe for hosting courses, and its worked pretty good so far. 
&gt; Just remembering on error resume next gives me shudders This line of code still gives me nightmares. I inherited a project where the previous developer wrote ZERO error handling, and opted to suppress everything instead. FML.
Can I ask what framework you are using? We are using some old automation software. Would love to be able to suggest something more modern and with C# support!
LinqPad is amazing. I rarely have to deal with Sql Server Management Studio anymore. Its great for testing code snippets too instead of having random console projects lying around.
Linqpad has always been a good companion for VS and a great software. I highly recommend it (if you have doubts, try the free version).
Do you have a budget for this? If so, buy them some Pluralsight memberships, assign them the C# courses, and sit back. Then they can move onto the unit testing courses. 
ill get back to you on monday, i have it in an email at work. we actually went through a few choices and settled on one. it's quite good when i was given a quick demo
we have pluralsight at work, but not for the testers, id give them mine to use but that probably will get me into trouble hah. well unit testing isnt really whta they will be doing, its more automation from the UI, the principles do overlap though
I would, but they need to learn C#, if i was going to build them up as programmers i would start them on javascript.
The free version is pretty much the same as Premium, just without autocomplete. Premium/Pro is definitely worth the money, though.
Fair enough, best of luck!
The problem is that they are using C# to write the tests, these arnt unit tests they are tests to automate the UI, so given screen a they need to access the html elements via the id (or other selector methods), enter data then submit the page. given the next page do something else. they need to perform happy path scenarios and failure paths. so they need to really understand how to do basic c# programming. I dont need them to get too advanced and get them to understanding the merits of recursion over iteration, functional programming or inheritence, polymorphism etc etc. just basic get x and do y, if that do this. the code they write is going to be nothing advanced and probably just huge static functions of repetitive code at the start.
Much appreciated! We use TestPartner, an old VBScripted automation took that has easy EOL'ed. 
Autocomplete with this program is incredible. I love being able to easily whip up mashup-style programs at work. 
I now do 80% of my Dev in Linqpad. Once i'm happy, I port my workings into VS. It's epic.
They should offer a free trial of autocomplete to get more people hooked. I never saw the appeal of LINQPad until I got to use autocomplete. I bought it because of one of these sales. I do not regret the purchase one bit.
Bought it some time ago. It's a great tool. I'd love to have F# autocompletion too :)
This is a great tool and his book is pretty good too.
I use it as a substitute for scripting. I have a lot of one time tasks to do with file and database stuff that it doesn't make sense to make a full blown exe for, or even a script.
Yeah, free version works just fine if you are ok with occasionally looking up the syntax and arguments of some methods.
I think most people know the awesomeness of autocomplete through Visual Studio.
How about not doing it in the first place? Your trying to turn a bunch of testers into test engineers. Test engineering is a software development discipline just as much as software engineering is. Just like any other code, tests need to be maintained. The test code will end up just as poor as production code would if you asked them to write that.
because manual only testers will be redundant in a few years time. I'm trying to turn them into test engineers or whatever you would like to call them. Testing code doesnt need to be written as well as the actual code tested, it just needs to run the test. I also like to teach and do something a bit different then just smash out code all day. So it expands my own skill sets by helping others.
From your comment I'm assuming you have seen this: http://www.kalzumeus.com/2010/06/17/falsehoods-programmers-believe-about-names/ . But if you haven't, or anyone else that happens along hasn't, it's a good read about names.
You'll have a hard time getting developers to use WCF over ServiceStack or Web API. 
Or just use WebAPI...
Since you want to make your own control, I'd recommend you make your own custom HtmlHelper extension method (the "Html" in "@Html.InputFor(somethingsomething)"). It's quite easy: make a static class with a static extension method for HtmlHelper that returns an MVCHtmlString. Build your HTML with TagBuilder. I recommend you check out the MVC code and see how Html.InputFor() is written. That way you have your own controls in your app/projects and it makes it easy to change them all if later you want to add custom functionality.
Jesus people are still pushing this shit?
i never could find that debugger plugin.
This is very helful!!! Thanks
I share his opinion to some extent (asp.net is indeed not very web-developer-mainstream), although I'm pretty sure there is a market for that kind of framework. Sure you can do ugly stuff with it, but that's true for every languages and frameworks (yes, PHP developers, I'm looking at you mainly). Saying "The vast majority of .NET projects involve working with giant, monolithic, boring, poorly written, legacy enterprise products." is juste like saying "The vast majority of PHP projects involve working with giant, monolithic, boring, poorly written, legacy products." (I took off "enterprise"). Works the same if your opinion is biased against PHP (or Dupal in my case). Also, I'm not sure to see his point of view on MVC (a little biased maybe). Since I adopted asp.net MVC about 3 or 4 years ago, I never touched webforms again, and yet I worked for 5 different companies in that timeframe (but tbh, I don't even look at anything that says "webforms", or VB or sharepoint for that matter. Not my gig.). He has a good point on pricing (yet that's less and less true). But then, it depends what you sell to whom. If my customer is a small coffee shop that just needs to put out the menu of the day, a link to its FB page and a contact form, chances are that a simple wordpress would be enough. On the other hand, if I'm selling an intranet to a medium company that has windows all around the place already and will be used by dozens of users connecting through the AD and data will be merged from various inhouse SQL DB, well... And between those two examples, there's room for everything. But if you indeed feel that you have to make a professional choice to focus on one topic or an other, I understand his choices even if his post looks more like a rant written on the spur of the moment ("I QUIT !") than a constructed and rationalized decision. 
So, your saying that there are software, frameworks and platforms to suit different requirements? Exactly. That's why this kind of "ASP.Net is dead" "MVC is useless" "MVVM is overly complex" arguments are always had by people who are either a) new to software development or b) not very good at software development. One of the major things that experience has taught me is that you have to pick the right tool for the right job. It's the same when you see people say "I am only going to learn X language because only and idiot with too much money would use Microsoft". Those idiots with too much money are companies with support contracts with Microsoft who want supportable stable environments rather than some cutting edge platform. Can we all just stop with this kind of ill-informed, inflammatory, reactionary and childish comments? Edit: Fixed some illiteracy. More probably still exist.
This is a misconception about ASP.NET Web API framework that its going to totally replace WCF. Its just another way of building services that are non-SOAP based e.g. plain XML, JSON string etc. Also it has following advantages as: 1. It allows us to create resource-oriented services using full features of HTTP. 2. Exposing services to variety of clients easily like browsers, mobile devices etc. WCF is still the best choice for: 1. Messag Queuing scenario using MSMQ. 2. One-way communication or Duplex communication 3. If we intended to use transport other than HTTP e.g. TCP, UDP or Named Pipes.
&gt;The vast majority of .NET projects involve working with giant, monolithic, boring, poorly written, legacy enterprise products. These products always seem to be 4-8 years old and 2-3 major platform releases behind the current .NET framework, in part because of the sheer complexity of modernizing. Coupled with the insane licensing, capital, and labor costs to develop a product using Microsofts platform, these products are rarely modernized because the business has barely broken even on the initial project investment. Investing in new feature development always beats investing in the cost to upgrade to the latest .NET version in most enterprise organizations. So... your problem is that you don't like working for a large corporation and ASP.NET somehow embodies that. I can get in line on the first part -- large corporation bureaucratic bullshit is exactly that. But ASP.NET, for all its flaws, does not represent corporate bureaucracy, nor does Node.js somehow magically make it all go away. Believe me; I can attest to this first-hand, having been involved in several Node.js applications for a large corporation with a heavy-handed bureaucracy. 
I don't think anyone is implying that web API will "replace" WCF. However, I think everyone here is pointing out that if you wanted to build a restful service using Microsoft technology, you would be more likely to choose web API to build it.
&gt; *Saying "The vast majority of .NET projects involve working with giant, monolithic, boring, poorly written, legacy enterprise products." is juste like saying "The vast majority of PHP projects involve working with giant, monolithic, boring, poorly written, legacy products."* Too true. The same complaint can be made about Java, C++, and just about any non-new language. Something that might have a little effect on that: the stupendous majority of source code on this planet is written in the enterprise, by the enterprise, and for the enterprise. Decisions from 2 weeks, 2 years, and 2 decades ago forged the software that is moving untold billions around the global economy, and it isn't spontaneously improving. We sit here and *hundreds of billions of dollars* of investment into the software ecosystem later, changing our fundamental perspective. Time to market is a real thing. So are manpower issues - not many people in 2002 were writing things we'd look at as clean and impressive in 2013... And fundamentally, what were the alternatives? Bad coders make bad code in any language. I'd rather wade into a giant ASP.Net crapball than a PHP crapball, or Perl crapball any day of the week... Components, master pages, compilation, libraries, multiple language support, and reflection give you a lot of migration options... 
&gt; And fundamentally, what were the alternatives? Bad coders make bad code in any language. I'd rather wade into a giant ASP.Net crapball than a PHP crapball, or Perl crapball any day of the week... Components, master pages, compilation, libraries, multiple language support, and reflection give you a lot of migration options... Not to forget the tools you have at hand. Visual Studio by itself is great and can help you refactor code quite fast. Add it Resharper and then it's just a walk in the park (all things considered). 
Just ILSpy 2.1 source? I've tried that and still don't have the debugger option. Or should i be downloading the debugger plugin source from somewher?
Yeah, a nice comprehension fail on my part. I guess I see the word test and automatically think Unit Test. I spend a lot of time with NUnit every day. You are what you eat.
Ok agree to some extent :) but using words like "shit" has different meanings. Anyways. Guys, we must understand that technology evolves. We didn't have MVC support with initial versions of .NET framework but we can't say "ASP.NET is a shit". Can we? Secondly, still today we can't always use latest available techniques due to certain limitations i.e. We have already developed systems and so many others.... We are not investors all the time.
Our team uses TeamCity, but our techniques should still generalize. &gt;If both the build and unit tests pass, PowerShell is used to copy the working folder to an IIS site folder. We do this right within our MSBuild file, variously using xcopy, robocopy, or BeyondCompare. We don't really use Powershell at all in our CI process. &gt;Writing PowerShell scripts is a PITA. When we copy to the IIS folder, we have to be very explicit about what gets copied over and what does not. For example, we don't want to overwrite the web.config every time along with a few other items. We also have a SOA-project where a single solution has multiple projects and each project needs to be published to it's own IIS folder. Again, having to write an individual script for each target folder with an ignore-list and whatnot. Not a good experience. We would regard this as an issue. The whole contents of prod should be create-able with what's in source control. So if someone's tweaking the web.config in prod, capture those changes and commit them to your config in svn. &gt;We also have trouble managing configuration files. Our local configurations are different than staging which are all different than production. Our current solution is to duplicate every config file in SVN and add a location as the extension (web.config.local, web.config.staging, web.config.production). We then use PowerShell to rename files based on the target environment. This is what [config transforms](http://msdn.microsoft.com/en-us/library/dd465318(v=vs.100\).aspx) are for.
Really only drop support for NPAPI plugins that includes Silverlight. Microsoft should and probably will (think Netflix DRM) update it to native client.
Take a look at [Chef](http://www.opscode.com/chef/) this is what we are using for the deployment aspect of our CI tool chain. 
Netflix is migrating to HTML5, something that Microsoft's OS division is ecstatic over. While Microsoft's developer division loved Silverlight, and all of .NET, the OS division by and large hated it. And while things are changing, in the last few years the OS division held the throne as it were. I've reached out the Microsoft for a comment, but I'm not hopeful that they'll get off their ass and actually port Silverlight. IE for WinRT already doesn't support it. 
I've never, ever understood the hatred for silverlight. Tbh I much more enjoy writing silverlight than HTML, no matter which of the many abstraction libraries I use I've yet to find anything that is as elegant or relaxing to write. It is a shame, I don't think it's meritocratic, I would love silverlight to be replaced by a superior framework or technology, but frankly, I don't think it has been :( (Excuse me whilst I go back to googling how to do the sort of thing that is self explanatory in silverlight, but I can't fucking understand why I can't stack text ontop of my animated bootstrap progress bar.
To be successful, Microsoft would have needed to convince the browsers to natively support Silverlight and the CLR. Offering it solely as a plugin without even the IE team's support doomed it.
It is sad they didn't release an open source version. I mean Rotor wasn't as good as windows CLR, the license was awful. But making it an open platform, whilst making it run best on their OS would have been a better set of events than what we have now for 'rich' application web development.
I am stuck in a company using an old Stack. Why use Web API or ServiceStack over WCF?
Web API originally started as WCF Web API but was later renamed officially to ASP.NET Web API. You can build RESTful services with either WCF or Web API. The primary difference between the two is simplicity. REST is a paradigm for granting access to resources over a network using only the various HTTP verbs (GET, PUT, POST, DELETE, etc.). With this, we've already narrowed down our protocol to HTTP. Therefore, although we know we want to use HTTP, we would still need to configure WCF and probably do some IIS configuration to get the behavior we want. However, ASP.NET Web API and ServiceStack already have this in mind. Therefore, getting a RESTful API running with either stack requires less work than WCF. Personally, I have more experience with ServiceStack. It's programming model is great. You'll find yourself using POCO's to create DTOs, giving you a clean interface to code against. Check out this Stack Overflow article. One of the lead architects on the ServiceStack team goes into detail regarding the benefits of ServiceStack over Web API. http://stackoverflow.com/questions/9699083/servicestack-vs-asp-net-web-api I hope all of this information helps you! Please feel free to contact me any time! :)
Given that MS has an OSX version of Silverlight, we can assume they wrote it to be as self contained and cross platform as possible, which would be crucial to porting to NaCl's limited API surface. There is also a [Mono port to NaCl](https://github.com/elijahtaylor/mono/tree/master/nacl). This lends credence to the notion that it is possible for MS to port it if they so choose, though it would likely still require more than a few man years worth of effort. This announcement was only made in the last 24 hours - I imagine we'll hear something from MS in the months to come. However, I in no way mean to imply that anyone should still be targeting Silverlight for new development if there is any alternative - its days are still numbered. In the meantime, though perhaps good for karma, misleading title is misleading.
I also would be ecstatic. My only question is how they could guarantee the DRM restrictions with HTML5. Is there a mechanism for that yet?
I haven't heard any details.
True, it is possible. But Microsoft currently has no intentions to port Silverlight to the new version of their own web browser. I would be amazed if they decided to do it for Chrome.
&gt; because manual only testers will be redundant in a few years time Based on what? The industry or your company? Manual testers will always be needed in some form. &gt;I'm trying to turn them into test engineers or whatever you would like to call them. A test engineer is the equal of a software engineer in training and pay. Are you sure your company is going to invest that much in them? &gt;Testing code doesnt need to be written as well as the actual code tested, it just needs to run the test. This will be your fatal mistake. 
They probably just stop flogging that dead horse.
Chrome to drop support for NPAPI plugins is a correct title.
 I beg to differ. Xaml makes more sense to me. Its nice to not have to spend so much time fighting divs to make them do what I want.
downvote for the misleading title
I think you are confusing the 'run-time environment' with the language. Now in WPF/Silverlight whatever, if I want to stack two things ontop of each other, its simple, one way would be this. &lt;Grid&gt; &lt;ProgressBar /&gt; &lt;TextBlock Text="Look I'm ontop of the progress bar"/&gt; &lt;/Grid&gt; Now if anyone can show me an elegant way that I can do that with my: &lt;div class="progress progress-striped active"&gt; &lt;div class="progress-bar" role="progressbar" aria-valuenow="45" aria-valuemin="0" aria-valuemax="100" style="width: 45%"&gt; &lt;/div&gt; &lt;/div&gt; I'd be genuinely grateful. 
While I'm sure it's good and everything. I'm not logging in to get it. 
Microsoft know silverlight doesn't have a future. Lightswitch is a good example of that. 
I'm comparing dying tech. Silverlight adds nothing and is inferior to modern browser capabilities in every way. The same could be said for flash. It's as useless as VB6 was in 2002. Other than that, you should learn CSS. It's been around longer, and there really no excuse for your question. 
&gt; There are a very, very few things you can do with Silverlight you can't do with JS/HTML/CSS Like, you know, placing some shit in the fucking center of the damn screen. Well, maybe there is a way now. It used to be a major engineering achievement.
the only one useless here is you. anyone who has been around since the IE4 days knows that web stuff is a giant hack; sadly, the best technology doesn't always win, and Microsoft is determined to punch itself in the nuts with the way they've handled everything post WPF/Silverlight.
&gt; anyone who has been around since the IE4 days knows that web stuff is a giant hack I've been around since Mosaic. If you think that "web stuff is a giant hack", you probably just don't understand it. WPF is an improvement over WinForms, sure... but Silverlight no longer has a purpose. I can do anything with JS/HTML5/CSS that I can do with Silverlight, only more developers will be able to work on and understand what I've done. Can I work in Silverlight? Absolutely. It's not hard. It's WPF in a browser. But there's just no point. Why force users to install a plugin just because you don't want to learn a standard technology?
I'm not sure, I think I used version 2.0. Try it with that?
You're really going to tell me that things weren't tacked on as they came up in terms of the web? GiveUpAndUseTables used to be a popular site to link to for a reason. Now it looks like web is going to win the war, but it isn't going to be purely for reasons of being the best technology. And WPF and Silverlight will still be around for a long time, even if they aren't updated...hell, I still hear of new WinForms development.
Wow. You really believe all that crap is better than this? HorizonatalAlignment = "Center" VerticalAlignment = "Center" 
The OS division owned Win32 and COM, the stuff the developer division was replacing with .NET and the CLR. So on top of the usual corporate politics you have a language/framework pissing match. And then there was the Windows 6 fiasco where some asshat tried to rewrite the OS-level apps like Windows Explorer in WPF. Though Windows 6 failed for many reasons, a lot of the blame was focused on that aspect.
Please explain. I haven't been following that particular mess.
Considering the shit you just posted won't work in any browser that doesn't have some magical, soon-to-die-out plugin installed? Yes. It's a lot better. 
&gt; And WPF and Silverlight will still be around for a long time WPF will be. Silverlight will not.
Perhaps can we agree that HTML+JS is a pile of ugly hacks that nevertheless wins against sane component based frameworks like air and silverlight because you don't have to worry about client side deployment? 
The first release of lightswitch had a Silverlight client but they moved over to being a HTML client in the latest versions to gain better support for client devices. Basically an admission that silverlight doesn't have a future as a widespread client side technolgy and the eggs are better in the HTML basket. That would have been quite a bit of rework on their side. In saying that; Silverlight will be around for a while yet but the audience will be very much intranet environments for businesses tied into the Microsoft stack. A company would need to be mad to make a public facing Silverlight web application these days. Of course that is just my opinion.
I don't think dual booting is a good solution for programming -- it means you have to reboot to get into your alternative environments. And if something goes wrong in that environment then you are faced with somehow wiping the OS on that partition and reinstalling. Seems very cumbersome. You should get VMware workstation. It supports all kinds of operating systems, is very stable, includes a snapshots feature that allows you to easily roll back to a saved system state. Easy file transfer to and from the guest VM.
Yeah, the reboot can be cumbersome, but I don't expect to do it often. However, if VM's have really come of age (it seems so), then they really could be a better solution. Would you give this same advice for client/server development? I don't really understand how VM's work across networks and/or ports, etc. Can you have 2 VM's "running" where one is the server and one is the client you are developing and get "normal" network communication across them? That would be pretty cool.
Vmware workstation networking is quite customizable and powerful. Check out chapter 5 in this PDF manual for VMware workstation 9. http://www.vmware.com/pdf/desktop/ws90-using.pdf On your gues VM you can choose between 3 different virtual network adapter modes. NAT, Bridged, and Host Only. See page 144 and 145 for descriptions of those modes. You can have many VMs running at once all communicating with each other as if they were physical machines running on a local network connected by a switch.
If it was me I would just use a vm.
Thank you for the link! I'll check it out. 
Wmware workstation allows you to create VM and run VMs. It's the ideal tool for what you want to do. There is another application called Vmware player which allows you to run VMs but not create them. It has limited features. For example, I don't believe you can use snapshots -- and doing dev work you will want the snapshot feature. Also I'm not sure how much hardware customization you can do with vmware player. If I recall, you can't modify the memory usage, etc from Player. There used to be an offering called Vmware server, which I believe allowed you to create and run VMs, and it was free, but that is no longer supported, and it's out of date in terms of features.
So, in your experience if going with VM's is the way, is there a hardware consideration here too? CPU, HD, RAM all needing to be faster for throughput?
Thank you. So far you've single-handedly answered my question and pointed me in the right direction. I am tending to agree with you that VM's would work, especially given the networking options. I still need to read the pdf, but it sounds promising. 
You need hard drive space, memory, and processing power for each VM. It really comes down to how many VMs you will need and what will be running on them. I advise you to do some research on VMs and if possible download and experiment with VMware workstation before you buy it.
Good lord, it even has Virtual DHCP and NAT and you can have these virtual networks connect to external networks. Sold. :)
I would recommend using VirtualBox [https://www.virtualbox.org/] or if you are running Windows 8 Pro or Enterprise, you can run Microsoft's Hyper-V client which is included with the OS. Both are free to use and allow you to create/manage/run your virtual machines. I actually have a VirtualBox VM running on my gaming rig that I remote into from my Surface RT tablet since I can't run Visual Studio natively on it. 
I've heard good things about VirtualBox, but if I do get into VM's I'll stick with VMware since some of my work departments use it. Better leveraged were I to be required to work with them. Thank you for the info though. btw, I didn't think of Surface being ARM and VS not running on ARM. Thanks for the heads-up on that too. 
Agreed, I stopped dual booting about 6-7 years ago as machines got powerful enough and ram cheap enough that it became much more realistic to run multiple OS's concurrently. 
Another vote for VMWare Workstation. I have a complete development environment in a VM that I run for 8 + hour every day, and have for several years. It's as fast as running on my physical hardware, and things like snapshots and portability are icing on the cake. It's on an external SSD, so I can take the drive with me, and use it on my laptop when traveling. It's super simple to upgrade, since you can boost the physical hardware, and then just adjust the virtual hardware. I also have a number of clean installation and debugging environments setup, that auto roll back on shutdown, so that they "forget" anything done to them (great for testing installations and other things that may hose the machine). You can also create a VM from a physical machine (although it will reset your Windows license), so I dumped my last dev machine (retired 5 or 6 years ago), just in case I ever need something from that era. This was a 32 bit machine, so every now and then I fire it up to run some old DOS utility, or compile some PB3.5 for a client. I've found VMWares support of various hardware and software to be generally better than most other VM solutions (especially USB devices, and multiple monitors), although things may have changed over the last few years. 
&gt; auto roll back on shutdown is a great idea for testing installers. Thanks 
Sounds like you're on the right track then. Best of luck.
Surface Pro is full x86 though. I use a Pro as a development set up.
Hyper-V if you're a microsoft shop
With hyperV you get sound through RDP, but the system itself doesn't have an virtual sound card. So if you are on a console session to the box you wont have audio. Least that's what google told me. I havent ever tried it myself.
What does your current VM setup look like? What is your host software -- vmware or virtualbox? What processer, and how much RAM? Are you sure you have the correct virtualization technology turned on at the BIOS level? Are you using a 5200 RPM hard drive or 7200 RPM? 
I use a virtual Win 76 64bit for VS2010, and for Exclipse, and my main machine for Photoshop (better accelerated graphics than in a VM) &gt;Can you have 2 VM's "running" where one is the server and one is the client you are developing and get "normal" network communication across them? That would be pretty cool. Yes, they can share the host machines IP address or have their own. I use VMware Workstation 9, and Windows 7 64bit pro, with 16GB memory, and a couple of Xeon chips. I can quite easily have 4 or 5 VM's running XP in 1 or 2 GB's or Win 7 in 3GB's... and still have 7 GB's on my host machine left for photoshop, and everything else. =D No dual boot..... besides you might want to tripple boot one day, then you're really going to be rebooting a lot!
&gt;Is VMware workstation the "running" environment for VM's while something else is where you create them? **It's ALL built in.** You click "New machine"..... it goes through some Wizard.... hardware wanted, disk size, what OS you want..... **You NEED the ISO's of the OS's you want to install - but they're on Piratebay anyway.** If VMWare knows the OS in question (it knows LOTS) - it will do a "silent install".... at the end of the wizard, you point it at the ISO and it does a silent install, setting the OS up for you, and installing its accelerator tools! **It's fucking MAGIC... really. Others say "click and play", but Workstation really IS.** Here's a picture of it running XP..... http://technoace.net/wp-content/uploads/2013/06/vmware-workstation-windows-650014.jpg on the left bar, are the other VM's (you can run these at the same time, and switch between their windows by clicking their name in the tab bar at the top (only XP at the mo in there on the screenshot))... AND.... AND! The little window with the "diagram"? **Fucking MULTIPLE RESTORE points!** Say you fuck up an install by using V3, and you need V2 but you can't roll back...or something, you can TOTALLY (not like Windows "Maybe") restore to one of the restore points you made. You can even make a new restore point from THAT one... hence all the splits... Here's another piccy: http://blogs.vmware.com/workstation/files/2012/08/VMware-Workstation-9-Best-in-Class-Windos-8-Support.png Notice all those tabs at the top? **Clicking those swaps the display to THAT virtual machine - Linux, Red-Hat, BeOS, Ubuntu, Windows 3.1, Windows 8.... all running at once!** Workstation can run**MULTIPLE MONITORS for a VM! and also FULL SCREEN! or any size window you drag it too....** It's AWESOME for "Trying out" software.... add a restore point before the install.... fuck around with software..... restore point roll back.... no need to re-make the VM, it's good as new! Oh shit YEAH! Forgot. **UNITY MODE!** It's a bit slow unless you've got BIG hardware..... It works well for Windows.... your VM's Desktop vanishes from the VM's window.... Instead - the VM window Vanishes too... and the VM PC's own start-bar appears on your PC's own native start bar, and the VM's program windows appear ON YOUR NATIVE DESKTOP! Like they were installed or something! The glitch is - you sacrifice scroll speed inside an app window... but on a fast computer you wont notice. **3D!!! 3D!!!** Workstation 9 supports Windows Aero in full, and Windows 8.......... And quite a few 3D games too! 3D used to be really fucking big NO in the old days. I think VMWare were the first to get something useable... So yeah........ Workstation - it's the fucking bees bollocks.
read mine! read mine!
If you have access to msdn, the hyper-v built into win8 is very nice. If you're not familiar with the differences between type-1 and type-2 virtual environments, I would check out [hypervisors.](http://en.wikipedia.org/wiki/Hypervisor) Win8 &amp; Hyper-v have been rock solid for me with no problems. The Metro-UI crap takes about 5 minutes to adapt to, then it's irrelevant. Personally I just want the free type-1 VM environment. If my machine sleeps, the 4-6 VMs all sleep. If the machine hibernates and returns, they all return. The 1 -2 times I've had a power outage, the VMs have recovered fine. I have ubuntu, wk8r2, 2k12, win7, and other things running as VMs. I use win8 for games and specific VMs for everything else. If you're anti-win8, I'd recommend getting VMWare's type-1 hypervisor. However, I say that from a performance perspective. I'd check to see if it's easy to use, or if you have to configure it from some interface exterior to a running OS (web port). If that doesn't look feasible, I'd go with VMWare Workstation. The step up from VM Player allows some special functionality for testing in a multi-VM environment (though I haven't used it). I avoid virtual box. sometimes it just hangs for 60 seconds. 
I am not anti-win8 even though I have zero expertise with it. I am on a big push to get all my skills relevant again, so I I'll definitely include a win8 environment. Thank you for the hyper-v info.
&lt;it's a bit slow unless you've got BIG hardware. How big? Seriously, how big?
Interesting... External monitors?
Funny thing, my certs and interests are a Microsoft shop, while my work is *not*.
Next step is to read the vmware docs and then start making some hardware assessment. :)
I just got what you were saying. I'll look into hypervisors whichever route I take. Still leaving vmware.. thanks for the wiki link.
Yes, you're correct. You have to do remote desktop.
&gt; In a DataSet filled with DataTables, you can have relationships (foreign keys), primary keys, unique constraints, etc. just like in a database. In a collection of objects, that would require you to program extra logic around it How so? If the class contains an enumerable of the related records then the relationship is inferred hierarchically. In the example below, an Employee belongs to the EmployerEntity by the virtue of a property Employees in the EmployerEntity class. public class EmployerEntity { public int ID { get; set; } public string Code { get; set; } public string Name { get; set; } public string GroupCode { get; set; } private List&lt;Employee&gt; _employees = new List&lt;Employee&gt;(); public List&lt;Employee&gt; Employees { get { return _employees; } set { _employees = value; } } } public class Employee { public string Code { get; set; } public string Name { get; set; } } 
Data tables take up a good amount of memory and if you just need s collection of objects then you don't need a data table. It'd be like someone telling you that you have to use array lists instead of generic lists.
I was talking about unique keys and primary keys, which are not enforced on an object collection. As you say, foreign keys are implied in an object graph, but if your have more relations and they are not strictly top-down, you might need to remap some references between your objects after you deserialize, because you might have duplicate objects. This can happen if the same objects are used in multiple relations. For example: Person A could exist in a few places in your graph, ideally you want them all to reference the same object. This is something a dataset handles better than a deserialized collection of objects.
Thanks for taking the time to give the examples. I think people should remember reddiquette more, and frankly, not downvote you for being helpful (even if your views on SL are *wrong*). &gt; if you're developing for the web, learn the web technologies. Which is my point, I don't think that the technology won out in a meritocratic manner. Also thanks for the reply, it confirmed what I had read on some SO posts, but I hoped I was somehow mistaken. Methods 1 and 2 have a problem, the font is badly aligned (top aligned) so looks a bit fugly. Method 3 works (well started to after I added the left: 0. Obviously for my centered text I need to have the div which it is in, left aligned to 0... obviously) But seriously, method 3 is so god damned complex, convoluted and ugly. I am not disputing it might be the *correct answer in css*, just that compared to the Silverlight markup language, it is confusing, verbose, inflexible and ugly. The fact that CSS isn't just a simple Measure / Arrange paradigm like Silverlight means that it is complex, quirky, bug ridden and ultimately less CPU efficient. I am not disputing that I can write some HTML, have it on a desktop, an Apple flavoured tablet. It is more that I will bitch and moan that I have no other choice. Just to finish on the matter, do you honestly think there is any advantage in the way we do this simple task in Bootstrap + CSS vs standard Silverlight? Because frankly I see none. Hell I can't even make a toast notification for our desktop users. Guess the windows users (currently 100%) will get a little companion WPF app that does such things.
Oh I see. You are right that does require additional logic. var employers = new List&lt;EmployerEntity&gt;(); var allEmployees = employers.SelectMany(itm =&gt; itm.Employees).Select(itm =&gt; itm.Value).Distinct(); foreach (var employee in allEmployees) { Console.Write(employee.LastPayslip()); Console.WriteLine(LastPayment); Console.WriteLine(); } public class EmployerEntity { public int ID { get; set; } public string Code { get; set; } public string Name { get; set; } public string GroupCode { get; set; } private Dictionary&lt;int, Employee&gt; _employees = new Dictionary&lt;int, Employee&gt;();// public Dictionary&lt;int, Employee&gt; Employees { get { return _employees; } set { _employees = value; } } } public class Employee { public int ID { get; set; } public int EmployerID { get; set; } public string Code { get; set; } public string Name { get; set; } private Dictionary&lt;int, Payslip&gt; _payslips = new Dictionary&lt;int, Payslip&gt;();// public Dictionary&lt;int, Payslip&gt; Payslips { get { return _payslips; } set { _payslips = value; } } public IEnumerable&lt;Payslip&gt; AllPayslips { get { return Payslips.Select(itm =&gt; itm.Value); } } private Payslip lastPayslip { get { return AllPayslips.OrderBy(itm =&gt; itm.PaymentDate).FirstOrDefault(); } } public decimal LastPayment { get { if (lastPayslip == null) return 0; return lastPayslip.Total; } } public string LastPayslip() { return lastPayslip == null ? "(no payslip)" : lastPayslip.ToString(); } } public class Payslip { public int ID { get; set; } public int EmployeeID { get; set; } public DateTime PaymentDate { get; set; } public string TaxReference { get; set; } public List&lt;KeyValuePair&lt;decimal, string&gt;&gt; Additions = new List&lt;KeyValuePair&lt;decimal, string&gt;&gt;(); public List&lt;KeyValuePair&lt;decimal, string&gt;&gt; Deductions = new List&lt;KeyValuePair&lt;decimal, string&gt;&gt;(); public decimal Total { get { return Additions.Select(itm =&gt; itm.Key).Sum() - Deductions.Select(itm =&gt; itm.Key).Sum(); } } public override string ToString() { string itemizedList = PaymentDate + Environment.NewLine; itemizedList = Additions.Aggregate(itemizedList, (itmList, addition) =&gt; itmList + (" +" + addition.Key.ToString("0.00") + " (" + addition.Value + ")" + Environment.NewLine)); itemizedList += Environment.NewLine + "=================================================="; itemizedList = Deductions.Aggregate(itemizedList, (itmList, deduction) =&gt; itmList + (" -" + deduction.Key.ToString("0.00") + " (" + deduction.Value + ")" + Environment.NewLine)); itemizedList += Environment.NewLine + "=================================================="; itemizedList += Environment.NewLine + "Total: " + Total; return itemizedList; } } But then again the normalized version of RDBMS DB would too, not so?
&gt;This makes the immutable type very easy to use: OrderLine apple = new OrderLine(quantity: 1, unitPrice: 2.5m, discount: 0.0f); OrderLine discountedAppled = apple.WithDiscount(.3f); &gt;Two things are noteworthy: &gt; * The WithXxx methods try to avoid creating new instances if the new value is identical to the current value. &gt; * Fruits, especially apples, seem to be expensive. :) 
In my older boxes I didn't get this kind of experience, this has only happened in the last 18 months since upgrading the bare metal. My current rig: * i7 3770k * Z77 based MB * 16 GB of DDR3 1600 (8-8-8-24) * SSD for OS * SSD for VM * GTX 680 (no difference in VM performance from GTX 460 in the box before). Host OS is Win7 x64 Two things that made all the difference in this rig: * CPU has good virtualization feature set (and it's turned on in the BIOS) * VM image is on an SSD. When I first built this machine, I was still running the image on a HDD, and it was fast, but nothing like this. Putting it on an SSD (Sata 6Gb/s) made a world of difference. I run the VM with 4 GB Ram, 2 CPUs @ 2 cores (4 cores), and two virtual HDDs for 120 gb total (this VM came from a physical machine that had 2 HDDs, so it was easier to convert it this way). This is all in Workstation 7, so the newer versions may be even better. 
Thank you for posting such specific information. 
If you have to push data back to database there's change tracking. It keeps count of which rows were modified/added/deleted and column changes (original/current value) so you don't have to. You may also find some use for column metadata.
i've been thinking about this all day now and i can't think of a single scenario where i would use datatables.
DataTable is stupid slow. For one bulk insert process it took longer to copy the info into the DataTable than is took to then insert that DataTable into SQL Server. These days I just use a List of Dictionary&lt;string, object&gt; instead. Or rather, a subclass of that so I can add the interfaces that SQL Bulk Copy needs. 
There is no Database in this scenario so its just to store xml in memory and enumerate through it. I'm starting to think the use of datatables in this scenario actually is as crazy as I first thought it was.
I think you get the fastest load from XmlSerializer. If you have an xsd schema defined for your XML the xsd.exe tool can generate all the c# classes you need to deserialize into.
Not licensed for use with Mono :(
We're only just moving from 2 to 4.0 due to the development manager finally seeing the light (although some parts are written in 3.5 (SQL Server bits and bobs) and 4.0 (utilities I've written)). We can't move to 4.5 due to needing to support XP for another few years due to clients not upgrading their out of date hardware. We still had users using NT a year into Vista being released.
Just the other day I've had a customer asking me if a Win2003 deployment box was fine (we've asked for a 2008R2+) because that's all they had. Alright, paying customer, no 4.5 then!
I recently had to engage in a campaign to drop support for XP/2003 for this reason. I'm a little annoyed by the fact that MSFT wouldn't support those OSes (I find it hard to believe that they couldn't throw NotSupportedException if these was some OS feature they needed that was only present in Vista+ kernels), but I also understand why they'd want to use every trick in the book to get people off of 10 year-old OSes.
Since SP1 or SP2 for VS2012, you can have the VS2010 blue colour scheme back.
There was a fair bit of resistance in my team but those of us that made the switch were very happy to have done so in the end. I hate using 2010 now, everything takes so much longer to do. The search features alone justify the switch. If you're trying to use it the same way you use 2010 it's a fairly pointless change. You need to adjust your habits a bit to take advantage of the new features.
For one library, I'm still stuck on 2.0. The customer is paying for it though.
We're addressing this by shifting more and more of the actual work to run on the server, leaving the client pretty thin with the eventual goal of eliminating a deployed client altogether in favor of running everything from the browser.
The moaning is because of the interface? I think that can be learned in a matter of hours (minutes?). I just started using 2012 and not sure if there are "hidden gems" waiting to be discovered :)
GAH I'm stuck on 4.0 for Windows 2003 servers. "Why spend money licensing on new OS when the old servers do what we need!" I'm told. I do not agree with this assessment, but in small business, it's difficult to justify to the owner exactly what is wrong with .NET 4.0. There's *nothing* wrong with it. I don't think there's anything I can't do it it, unfortunately I'm stuck with doing things an older way. 
&gt; I assume that in Outlook it's done via the Outlook COM Add-in which does make sense. Don't assume that, outlook 2010+ has great web services.
I'll have a look, thanks!
Interface. I haven't actually used it yet being a Windows 7 user and my general method of having things be done on servers as telling someone what I outcome I want and then leaving them to do it. I think they're irritated that while it's OK they can't seem to work as quickly. I've bought them a nice big book and booked some online training so hopefully that will do.
Completely wild guess: The CTi tool installs an IE add-in that is looking at clicks and parses/scrubs the DOM to determine which contact is selected. The Dynamics CRM website may even make this easier by firing some JS event when a contact is selected. Again, a total guess.
What erks me is thay 4.5 beta ran on XP.
If it is easy and automated in you build scripts, don't sweat it.. you will end up having a 3rd for .net 5.
I wish. I've still got to maintain stuff stuck on 1.1. 
Fire up Fiddler and see what the browser's doing.
Why?
Why?
There's an easier way to do it through changing a registry key. Change the following value from 0 to 1: HKEY_CURRENT_USER\Software\Microsoft\VisualStudio\11.0\General DWORD: SuppressUppercaseConversion No need to install an extension that can potentially hose your VS stability.
Keep the 4.0 around; a lot of places are still stuck on XP and Server2003. We are upgrading later this year though I expect a lot of corps will hang on for a few more years. If you want your project to be relevant over the next 36 months it needs to run on the 4.0 CLR. 
[VS Commands](http://vscommands.squaredinfinity.com/features) is pretty lightweight and I would argue it's easier to click a checkbox than mess around in the registry. I haven't had any problems with it at all. Check it out, you might like it. I love the output window colorizer.
Because our current version of our (expensive) CMS system uses 3.5 and my job is to support and customize that. There are newer versions of the CMS which use 4, and we were going to upgrade this summer, but the upgrade also required a MS Search server installation, and so we needed to talk to our MS server team about getting another machine for that and they needed to look into licensing and how much it would cost and there would be negotiations about what department would pay for that, and so it got pushed to this fall but now we have another very urgent project that needs to go up using the existing CMS system so the upgrade is on hold until the spring. And of course in the mean time the vendor has released yet another version of the CMS, which doesn't require MS Search Server to run. So maybe we'll just jump to that, but we like to wait for a service pack or two before jumping on a new version so we'll have to take the time to evaluate if we can go to that or if we should continue on the plan to go with the older (but newer for us) version. Meanwhile waiting to see if that that one big project which has been jumping from in-house to vendor for the past year plus settles on in-house and if it does can we deflect it to a different team or make it wait until after the upgrade. **tl;dr** I work for a large organization and that's why we can't have nice things.
It's funny how this is cyclical over time. Many years ago the thin client was in, then everything was a push to a thick client, now back to a thin client. 
Ugh. Fine, be a douche about it, I don't care.
Thanks. Will give that a go on Monday.
As someone who has worked in small organizations his entire career thus far, you guys typically have far nicer things than we do. I scrape and claw for every single penny I get. :'(
Given the answers provided here, it would appear that the answer to question "Is anyone stuck on .NET 4.0?" I think the answer is: Yes, /u/grauenwolf is stuck on .NET 4.0. Good luck with your library!
You may want to consider supporting .NET 2.0, 3, and 3.5 as well. 
You're onto something. Initially I though that since it's on Dynamics Online it would have to use Dynamics API and I couldn't find a way to do it with the API so focused on the client side. I remembered that I had to install a solution on Dynamics. I've just checked and there is a plugin .dll embedded in the solution, but no service endpoints. I'm at home so it will have to wait until Monday until I'm able to decompile the .dll to see what it does.
4.5.1... the beauty of a closed environment. 
Perhaps the links in Outlook are of a special protocol type, for instance "dynamics:374897325982739535.html", and the application has its own url handler registered to deal with that type of urls?
This is what I was going to suggest - they've probably registered a unique protocol that launches the desktop app. More info on how this is set up here: http://msdn.microsoft.com/en-us/library/aa767914(v=vs.85).aspx. 
Here is some documentation. http://msdn.microsoft.com/en-us/library/gg334767.aspx
Without getting too deep into why I needed to do this, I use datatables when my app needs to return an Excel spreadsheet. ADO's SqlAdapter returns a dataset and they're easy to use with the office interop lib.
And every time you **do** ask for something, someone will pipe up to say "we could make than in house for less". Never mind the fact that every project is currently behind due to lack of Developer time...
We're still stuck with 2.0. Management, in their wisdom decided that the 3.5 upgrade wasn't justified from a cost / benefit point of view, and now that they know 4.0 is already effectively end-of-lined (and we have customers on XP/2003) they don't want to upgrade to that either. Net result - Dev teams are all working on their CVs.
Obviously, you understand my pain well.
It's because thin client is an awesome idea, but our WAN/Internet speeds always lag a little behind a lot of use cases. I work for a company handling audio recording on the client, and while recording device manufacturers have made excellent progress in compressing their channels, the people who cost thin-client infrastructure rarely bake in enough redundancy for 50% of their terminals to be up-streaming audio simultaneously. Not even sure it's possible at some scales. The upshot is that you start having to handle use cases with custom hardware, which ends up being even more expensive than a thick client route. Everyone then realises the thin-client ideal is very much conditional, switches back to thick-, until the cycle begins with fresh sales people and their marks. 
Async and await will increase your productivity if you do a lot of parallel programming. Source: I'm stuck on 4.0 and the vanilla Task Parallel Library can be a bit ugly as you end up with a lot of code in continuation passing style.
:(
Hey, I'm not knocking it. :) We have a reason (cost, network, hardware, speed, upgradability, end-user experience, whatever) for each thin/thick choice. It's the shifting of those parameters which cause the oscillation between thick/thin. It's just amusing after you've been in the industry long enough.
Oh no, I was agreeing with you: its hilarious how every cycle thin-client is promoted as finally viable, but is in fact always that little bit behind :-)
:)
It's a .net MVC controller that returns JSON instead of a web response. That also means its trivial to create an API that authenticates using cookies... Creating a client that supports that is another issue entirely.
He's just asking for circular references with that recommendation.
I was looking for criticisms of less class libraries approach he's proposing, but I don't see the circular references as being an issue. In fact, I'd think it'd be much more difficult to reach that point with this setup.
If you only have one class library in the whole solution how can you have circular references? ;) I am actually interested as to why phuber made this comment. 
I think it *fixes* the renaming problem because if you follow the routine from the beginning then you will not need to rename. I agree with using class library projects instead of sub folders but the video addresses this and says if you like you can use projects.
Can't you just type another solution name than the project name in the very bottom textbox in the "New Project" window instead of creating a blank project? Works fine for me.
I cannot wait to see what the next 12 months brings.
Yes. He could easily move them. I'm a bit jaded for this implementations because I work with commodity developers. If you open the opportunity for circular references it becomes increasingly difficult to re-factor it out. For this reason, I prefer to start up in separate projects so visual studio issues errors instead of letting things get out of hand. On teams of trusted developers where you can keep an eye on check-ins, I see nothing wrong with the approach.
The circular references I was referring to would be something from the 'Data' namespace referencing something from the 'Business' namespace and vica versa. If you go the folder approach, nothing prevents you from making this reference. It will compile and work just fine. You won't run into a problem until you try to separate out some of the code into an assembly. If you put your namespaces in assemblies, visual studio will prevent you from making circular references. Ultimately this is all about code maintenance over the long haul. Having fewer circular references decreases your maintenance overhead and makes it easier to write modular code.
&gt; Even some smartphones have moved to 64 bit, despite the fact that they only have 1 GB of RAM installed. heh.
Thanks for posting this kind of thing. Sometimes I feel like this type of sub really excludes people at the begging of the road for the technology discussed, but this is exactly the kind of thing that makes me happy I'm subbed despite my green-ness.
To host your .net projects I would look into Azure. You can host up to 10 websites for free, and easily. For the first part.. It kind of depends on what exactly you want... Do you just want the back end logic to be able to be used in any project? You could build a Class Library and have code in it to generate a blog database, and access and write to the blog database. Then each blog is mostly just the view with simple controllers like Blog.Display(blogPostId) or Blog.SavePost(blogEntry);
That doesn't sound too bad at all, I think I'm going to go for it.
I want to know more about this kind of subject but i find it so hard to find good information on it, can anyone recommend any good books articles or videos on the subject? Solution design is really somewhere i struggle at the moment 
If I have my MS marketing straight, newer fully-featured versions of VS have some architectural tools that will let you ensure that "Business" relies on "Data", but not the other way around, within a library... But, yeah: if you've ever inherited a monolithic program from someone and tried to separate it out into logical assemblies, or swap out its data layer, you'll gain n appreciation for keeping your layers isolated. 
It's somewhat dry, but IIRC the .Net Framework Design Guidelines book (http://www.amazon.com/Framework-Design-Guidelines-Conventions-Libraries/dp/0321545613), had some really solid advice - though it's also aiming at a larger domain than just solutions. Generally your libraries should reflect your points of re-use and architectural or license/copyright separation. 
I don't want to spoil the next 12 months for you, but I hear they're releasing a new and improved x64 JIT compiler. ;) 
Yes!
I agree.... But How?
No
Seriously. Gahh. I'm moving over to Groovy, that way there are no spoilers.
That sounds interesting. As someone who has never used F# before, could you explain a little more about how this is accomplished?
Have you tried the instructions found here? http://support.microsoft.com/kb/976982
Thanks for this - I'm actually working on a project right now that performs various validations on user-submitted sheets.
Sure, F# as you may or may not know is an impure functional language that compiles to IL, so it is fully interoperable with any .NET language. It's mature, and fully supported by microsoft. While F# is based on ML (so it's similar to OCaml) it does have some really interesting features. One of the main things is type providers. The goal of type providers (and F# in general) is to give you strongly typed access to random data sources. This means you don't need to create and map any types to and from your data source, the provider does it automatically. The gist of type providers is you give the provider an example data set (this is done in *code*), and while you are writing code it parses the data set and auto generates IL that represents access to the data and strongly typed data classes. So, you can imagine a CSV that has a header row. The CSV type provider will give you strongly typed access to the rows. It'll figure out what is the most general type that can represent a column. If you have a column of just one's and zero's, it may choose that column to be boolean. If you have a decimal value it may choose float, etc. Worst case, it chooses string. Some type providers come bundled with f# core, and some are available as extension libraries (mainly through Fsharpx, and Fsharp.Data). If you are curious and want to see real examples, check out this channel 9 video http://channel9.msdn.com/posts/Tomas-Petricek-How-F-Learned-to-Stop-Worrying-and-Love-the-Data that features Tomas Petricek who is a microsoft researcher working on F# and type providers. To answer your question, I am not familiar enough with the under the hood mechanics, but it is doable to create type providers to anything you want (though it's not totally trivial, or so I've heard). Out of the box, f# provides SQL, OData, and WSDL support. Other libraries like Fsharpx and Fsharp.data give AppSettings, Excel, Vector data structures, Regex, Registry and File System, WMI, XAML, JSON, XML, CSV, the world bank, and freebase. As an example (copied directly from f#.data), lets say you have a csv in a file called "MSFT.csv" that looks like this: Date,Open,High,Low,Close,Volume,Adj Close 2012-01-27,29.45,29.53,29.17,29.23,44187700,29.23 2012-01-26,29.61,29.70,29.40,29.50,49102800,29.50 2012-01-25,29.07,29.65,29.07,29.56,59231700,29.56 2012-01-24,29.47,29.57,29.18,29.34,51703300,29.34 This CSV is usually distributed via the yahoo finance API and you want to be able to access the api whenever you want but you don't want to have to deal with creating locally typed objects to map everything. You can access it with the type provider like this type Stocks = CsvProvider&lt;"../docs/MSFT.csv"&gt; // Download the stock prices let msft = Stocks.Load("http://ichart.finance.yahoo.com/table.csv?s=MSFT") // Look at the most recent row. Note the 'Date' property // is of type 'DateTime' and 'Open' has a type 'decimal' let firstRow = msft.Data |&gt; Seq.head let lastDate = firstRow.Date let lastOpen = firstRow.Open // Print the prices in the HLOC format for row in msft.Data do printfn "HLOC: (%A, %A, %A, %A)" row.High row.Low row.Open row.Close If you update the sample CSV, the type provider (at code write time, not even compile time) will update the data model and give you compile time checking and updating of the model. In general, there are a bunch of ways to "seed" the provider. I think some providers allow direct access to the web stream where it will check what the stream provides and update it for you. At this point, you can use LINQ and any other higher order functions to manipulate the collection. You can also expose the collection to C# since the data classes are actually emitted as IL. Here is an example of the Regex provider: type PhoneRegex = Regex&lt; @"(?&lt;AreaCode&gt;^\d{3})-(?&lt;PhoneNumber&gt;\d{3}-\d{4}$)"&gt; PhoneRegex.IsMatch "425-123-2345" |&gt; should equal true PhoneRegex().Match("425-123-2345").CompleteMatch.Value |&gt; should equal "425-123-2345" PhoneRegex().Match("425-123-2345").PhoneNumber.Value |&gt; should equal "123-2345" Gone are is the MatchCollection nonsense and all the regex bullshit that comes with having to extract captured values. And here is WMI. No more string typing! type LocalHost = FSharpx.TypeProviders.Management.WmiProvider&lt;"localhost"&gt; let ctxt = LocalHost.GetDataContext() let batteryInfo = [ for x in ctxt.Win32_Battery -&gt; x.Name, x.BatteryRechargeTime ] All of these properties are actually type checked and everything If you want to see examples of the excel provider, check the fsharpx unit tests https://github.com/fsharp/fsharpx/blob/master/tests/FSharpx.TypeProviders.Excel.Tests/Excel.Tests.fs which show pretty well how to use it. 
Indeed, something to at least be aware of!
Not a lot of commit activity lately. Project still going?
I had the same thoughts.
Thanks a lot for your detailed reply! Just for the fact that I will no longer have to use string identification for column names makes this sound really great. I am going to spend time this weekend reviewing the references you posted and learning more about F# :)
Holy shit. My hero.
This is great, will surely try it. However DocX might not be the best name to call it, could be hard to find related articles/help/docs on the internet.
Does anyone have a good guideline how to structure unit test and acceptance test (projects)?
It's Free. And Open Source. Those were two strong points in favor of DocX for me . . .
can this tool save the word doc as a PDF?
yeah. it's a fucking bitch. 
I haven't used Aspose Words, but it appears DocX will do most (but not all) of the same stuff. Specifically, I know it won't convert to .pdf. 
Thanks.
Are you using some kind of add-in/macro/package that could perhaps be running it's 'custom tool' during compilation? If that's the case, and the custom tool fails, maybe that's the error you're getting? Clearly I'm guessing here. Just something I'd check. Also, does it fail if you build using the framework compiler instead of the IDE? edit: By framework build, I mean msbuild.exe
Building a solution or a single project? Have you made any changes manually to the .*proj file?
There's definitely potential danger here... the process of searching for a code snippet usually involves vetting multiple examples and learning a lot about what you're doing. I feel like making it this easy could potentially be bad.
actually in the current beta version, the only source is SO. moreover, there's a ranking algorithm in Flow that rates every answer by a large number of parameters - one of them is the up votes... The better specific answer is ranked, it will be ordered first. thanks for your reply :-)
I've already seen way too many code snippets where it was pretty obvious the developer didn't know what they were doing, and copied and pasted random chunks of code until it worked. I feel like an extension like this may make these "copy and paste" developers even worse. :(
I have the same feeling. Every time I give a coding course, one of the point I stress the most is "do not copy/paste without understanding everything the snippet does". That being said, the issue is more between the chair and the keyboard that the extension itself and it always pains me to say "you should not use that" to someone because he's too dumb (read inexperienced) to use it properly. Great power, great responsability, etc. 
What percentage of those types of developers even know there's an extension ecosystem?
This is a concern for me and I have monthly issues with other developers I work with assuming that because code is online it's correct or even well written. I see it a lot with people looking for SQL to perform specific things and not getting it completely right but then people copy the incorrect solution and the fact that there's multiple incorrect identical answers creates and illusion of correctness. For example, look for t-sql for calculating when Thanksgiving will fall, then marvel at how poor many common answers are.
Agreed 
I see this helping me more as simply a refresher. I can't count how many times a week I have to do something that I understand conceptually and have done before, but simply forget the exact syntax or classes involved. I don't see myself using this as a replacement for researching and learning new topics that I've never dealt with before.
I feel like 60% of the times I find an answer to my problem on Stack Overflow, instead of taking the top answer, I use one 2-3 answers down that does it much better. Inertia seems to keep the first working answer at the top. People keep piking and voting for the top one instead of the best one.
Yeah, I've had that experience too.
This seems a little silly. You should simply build the strong types you need from the xml. If that means linq or whatever, then do that. Sure, maybe it is easier to convert the xml to a datatable and then build a list of your strong types. Do that and toss those datatables. Especially, if you are sending data over the wire datables take up too much bandwidth. 
We use async / await just fine on .net 4
I forget how to read/write a text file three times a week. Sad I know. 
I think it depends upon what your primary use case is. I do mostly desktop / service / library work, so my focus may be a bit different. My shortcut mapping isn't default, so .... I use F5 (debug), and F8 (step) , F9 (toggle break) the most as direct keyboard commands. I have shortcuts on my mouse for bookmark toggle, and next / previous bookmark, and those get used constantly. CTL ALT P to attach to a running process or service. beyond that ... I have dozens of custom shortcuts that are mapped to third party tools for templates and macros that help build boilerplate code, and insert helper code. 
Ctrl - is go to previous location. One of my favorites.
 Ctrl [ S Select current file in solution explorer Ctrl . Quick Tip menu (implement interface, rename member, etc) Ctrl , Quick Search/Navigate Shift+Alt+Arrows Box select
Ctrl+R,R - refactor -&gt; rename
Ctl-Shift-F "find all" with ability to filter by file extension
Ctrl E D - Format document. Great when writing HTML or XML. Just give things a quick tidy up.
Shift + Alt =&gt; Block selection Ctrl M Ctrl T to collapse all blocks 
[Ctrl Shift B] or [F6] = Build. May vary between versions.
Up, Down, Left, and Right.
If you are doing C# or VB without resharper you don't know what you are missing. I recommend learning those shortcuts over the default VS ones.
omg just learned about Ctrl [ s. You are my new hero.
Ah! There's a chart package involved in one of the pages - DevExpress about 20 DLL's!...... hm...... I shall check it out. msbuild? I'll have a look. What would it mean if it didn't?
&gt;Did you check the application event log? Does VS.NET crash? Or just throws the exception during compile a without crash? No event logged, VB.Net doesn't crash, it reports the error like it would when a variable doesn't exist. &gt;Or just throws the exception during compile a without crash? Yup. &gt;I've had this problem in VS.NET2008, in my case was caused by a 3rd party reference. Ahhh! Yeah, there's a fuckton of dll's for charting, reports, and things like that... I forget the name... hold on.... DevExpress! &gt;scroll down to the devenv.exe process exit and walk up the process entries for any failures that looks obvious (save the procmon and upload for review) Ah, this would help identify the DLL that caused the fuckup?
I'm going to check this out too: http://www.reddit.com/r/dotnet/comments/1nslxd/why_the_fuck_does_vs_2010_fail_compilation_with/cclmmjq I think you're right as well - custom tool....... dll... corrupt!
Ctrl K + C Commnet selected text Ctrl K + U UnCommnet selected text Ctrl K + S add code around selected text Ctrl K + D format all code F5 F10 F11 debug, step over, step trough code Shift Ctrl F find from all solution Ctrl F find in open file Shift Ctrl B build code Ctrl arrow keys move through code Shift arrow keys to select code Shift Ctrl arrow keys for better select code. 
Heh, I still call it devstudio if I'm not paying attention.
Is that not only for r#? If we are including resharper then ctrl+F12 for goto implementation
Ctrl+Break - Cancel building. Very helpful when you accidentally launched build/deploy of a large project. 
If you see an abnormal thread exist, then yes Can you remove reference to these DLLs, comment out relevant code in the same project and try a compile? This is how I was able to get to 3^rd party DLL Also, are you using any compiler directives? Build pre/post scripts?
Why would this question spur debate?
This comes very handy while working with web instead of F5 so you dont open a new tab every time you check something 
Ctrl + M, O collapses to definitions. Absolute favorite command.
Doh. My bad, it's become muscle memory for so long... Thanks for the correction. 
That Alt F12 will be golden. Thanks for sharing. I also like CTRL M Ctrl S for collapsing a region.
Nice, I need to learn this one. I refactor rename all day.
Nice one. Have to put this in my repertoire. 
I don't think the konami code has an egg in VS.
Is your life really that boring?
F2 does the same job
This is the stock VS hotkey/chord, no R# necessary.
Concurrent collections are just like ordinary collections, but they have additional code internally to synchronize calls, thus guaranteeing thread safety. Immutable collections are just like regular collections, except all methods to add or edit return a new collection instead of operating directly on the current collection. This gives you thread safety as a side effect, but otherwise the two groups are wholly unrelated.
Your use case is what determines which one you want. Do you have many writes to the collection or very few? What is the size of your dataset? For small datasets or use cases with few writes, you might prefer the immutable collections. For many writes or large datasets where you can't just merge the independent immutable sets, you may want to go with the concurrent collection. Personally, I dislike side effects, I use immutability whenever I can. 
Nothing overwhelms me more than a class file with hundreds/thousands of lines of code. First thing I do when I open a file is CTRL + M, O. If you're inside a collapsible region already you might need to hit it a few times to collapse everything.
I wish I wasn't the only one at my company who understood the usefulness of regions.
A world without regions isn't a world I want to live in.
Eeak returns a whole new collection? Sounds expensive.
Not necessarily. I don't know exactly how all of the new immutable collections are implemented, but as one example, here's what an immutable stack might look like: class ImmutableStack&lt;T&gt; { private readonly T head; private readonly ImmutableStack&lt;T&gt; tail; public ImmutableStack() { } private ImmutableStack(T h, ImmutableStack&lt;T&gt; t) { head = h; tail = t; } public ImmutableStack&lt;T&gt; Push(T item) { return new ImmutableStack&lt;T&gt;(item, this); } public ImmutableStack&lt;T&gt; Pop(out T item) { item = this.head; return this.tail; } } Now, a stack is probably the simplest collection to implement immutably and efficiently, but I'm sure that the developers responsible for the BCL immutable collections have come up with similarly efficient implementations for the other collection types. That said, not all collections are just as fast in their immutable implementation, so if you're working with a large number of writes, immutable collections might not be right for that application.
Disclaimer: I'm quite new to the CLR, but I do have some knowledge around the JVM and a few other languages. Immutability _can_ be expensive, but it's usually not. You generally don't have to recreate the whole data structure on read, you just have to recreate the parts that have changed from the existing structure and reference the existing subtrees that have stayed the same. And since each element of the collection is immutable itself (hopefully), you can guarantee those elements will not change either. Rich Hickey, the man much smarter than myself that designed Clojure, has [an amazing talk about immutable data structures and managing their references to the underlying data](http://www.infoq.com/presentations/Value-Identity-State-Rich-Hickey) that you should check out.
I'm kinda up in the air about them. I've occasionally seem them used where (I think) they are useful, but more often then not I see old code that #region Properties, #region Ctors, #region Public methods.. etc. In those cases I generally think they are either added automatically and not needed or they are present because too much is going on in the class. What ways do you find them useful?
You can also check the "Track Active Item in Solution Explorer" flag under Tools &gt; Options &gt; Projects and Solutions, but that functionality pauses if you have multiple items selected in solution explorer
And CTRL + M, M opens the collapsed code block your cursor is currently on. 
We helped Mike with some of the initial tech they were doing with this product. Very cool stuff.
Good read. Thanks for posting.
Msbuild.exe is the compiler that comes with the framework. It's a command line compiler that may give you a different error/result that could be informative. Set up a batch file to compile your project using msbuild.exe. I'm on my phone, or I'd give you an example. When I'm at work tomorrow I'll try to remember. It's not complicated. Let us know what you find out either way. 
Although this *is* great! and I am looking forward to it, I would love to see some significant improvement to the first launch time of an app _during development_. I'm assuming this improvement doesn't help in that case since recompiles would presumably require cold-starts. 
Conceptually, I can only think that there's some sort of issue with your connection string looking for a DB hosted on a machine, instead looking for a pure DB. MVC does require a bit of infrastructure setup too so there could be more stuff to change.
Remove tcp in connection string
It most definitely does work on Azure. I'm literally working on an MVC4/EF project being deployed to Azure Websites right now. That is may not be a valid Connection String for Entity Framework though. It usually has a lot more "junk" in it when using the designer feature. See this for an example: http://msdn.microsoft.com/en-us/data/jj556606.aspx I've seen this error that you are getting when the connection string is missing the "providerName" attribute. You have it on your connection string, but as you'll notice with an EF conn string they put the data inside too.
If you log in to the management console on azure, you can go to the DB instance, there you can copy the ADO Connection string to the clipboard, avoid any errors like that tcp I don't like the look of.
Hey. I'm kind of surprised a site like yours doesn't already have quite a formal way of uncovering emerging market needs. However, if this is an agile, low-fi, informal, crowd source type way of supplementing that, then all is fine and quite literally dandy. Apart from that if this is the case, you're being a bit disingenuous about the whole thing. I'd like to see case studies of fuck ups. CGI Group etc. 
One of the pain points is that some technologies are moving a bit too fast (examples: MVC, WebApi, or the new universal membership providers). It's hard to make a good tooling choice when tomorrow something else comes out, and who knows if it's actually better for our current requirements. The other pain point is MS pulling a Silverlight again (likely). Edit: oh, and fragmentation. WPF was an interesting technology. Now WinDiv is focused on XAML. I'm not sure I want to see technologies being ripped apart by different MS divisions. Hopefully the "One Microsoft" direction will help in this regard. 
Most years I can just hit a couple of conferences and have a pretty good idea of what's going to happen next. But right now the general vibe of "WTF are you doing Microsoft?" is drowning out all other discussions. 
Sure, that's definitely something we can look at.
So basically you're looking for recommendations? Maybe we should bring in people to do side-by-side comparisons of competing Microsoft (and open source?) libraries?
I was about to suggest that but it'd have to be fairly comprehensive and categorized. Something like "a constantly updated go-to resource" rather than an article. It would be extremely handy to have, all in one place: web services, web authentication (most of the focus around the web is on the fancy side of things like oauth, facebook integration, etc, but an intranet service rarely needs that), ETL (even a simple csv reader could fit, if it's well done), client UI (from winforms controls to XAML), web pages (a list of mvc-ready bootstrap packages, modern ui css). Something handy for the enterprise dev that has to quickly make a decision.
WP8 doesn't depend on .NET. Cut the crap.
Can't access it: &gt; This project is not yet published &gt; &gt; If you are a coordinator or developer on this project, please sign in to access the project. 
Thank you, it's always been in the back of my head to do something cool with reddit. I got to checkout to see how you did it. 
Make a new post, or update this one, because I would really like to see what you change.
For those interested in using C# to access data on reddit, I've found [this library](https://github.com/SirCmpwn/RedditSharp) to be very useful.
No, no pre/post directives in this project - that I know of. I'll go check. Thanks! xxxx
Ok, thanks! I've been off ill for a few days - damn cold.... I'm back in this week.
About bloody time.
1. Please help me understand how I can use this to develop for other platforms in C#? 2. What other platforms? 3. Is this of any practical use/benefit in an Apple/Android world? 
pcl are libraries that only allowed cross platform code so you can make a dll and use it in all. With monotouch
as a .net developer there some really great tools for test driven development. tools like resharper, ncrunch, nunit, and the list could go on for miles. there are also some very neat tools for behavior driven development (bdd) like specflow. there's always been a catch though: test runners like nunit are much too focused on the outcome rather than the behavior, and tools like specflow are hard to use unless you have a really awesome BA/Product Owner. to alleviate what i see as a huge gap in tooling i've created what i hope will one day be a comprehensive, test runner agnostic bdd framework for .net. its called Given. check it out at the link.
First world problem if I ever saw one :) . Besides, you're supposed to update comments when coding.
God, a wrong comment is worse than no comment.
 // do something public int DoSomething(string somethingElse) { return 1; } Even better: // Do not touch: Magic.
 // THIS IS A HACK. public int RollDice () { return 4; }
And yet still too expensive for my blood. I can't even get my boss to pay for Resharper. 
Would be nice for those who write public APIs and diligently write XML documentation for every parameter/return value/overload/etc.
Must have...
Humm, weird. The purpose of many (most?) comments is to make code more readable. I'd never ever want to hide them - it's the one place you can say exactly what you want, something important!
It's mostly intended for XML doc comments. These are very handy when you are the consumer of an API, since you can see the descriptions when you dot into the functions (autocomplete). For the writer of the API however they can become troublesome as they tend to visually crush the actual code, especially if you have many smaller, modular methods. There is nothing wrong with writing comments.
ReSharper is a bargain, your should put some more pressure on your cheap-ass boss. 
This guy may sound a bit... bro-ish but he's right. Don't check-in your NuGet packages to source control. NuGet will handle it and save you sine repository size.
what do you mean when you say you have 75 entities? 75 tables in the database? if so - thats fine.
No i mean 75 entities in a dBcontext file. Entity framework Database first. 
Complaining a Windows 8 app is too square and boxy is like complaining ice is cold. 
still not getting it. what do you mean with a dbcontext file? dbcontext is a class. if you are doing database first, you create a model of the database, which becomes an edmx file. this will have the tables from your database, which will be transformed into classes. are you saying you have 75 generated classes from your database? if so, see above, its fine.
Yes what I meant was 75 dbset statements in my dBcontext class. 
Almost all windows applications are too boxy. I started building rounded edge apps and my company thinks I am some guru programmer... Right guys.
Code should be self-documenting. Nothing is worse than a comment above a block of code that insists it does one thing yet in reality does something else.
I did some zig zag edged windows before, just for fun.
I would just like to point out that code-first is perfectly practical for existing databases. The easiest way to get going is to download the Entity Framework Power Tools (currently Beta 4 - http://visualstudiogallery.msdn.microsoft.com/72a60b14-1581-4b9b-89f2-846072eff19d) to reverse engineer the code-first model. Super easy and more maintainable than DB first IMO.
http://www.creativebloq.com/net-magazine
On the topic of concurrency, do *not* share a context between multiple threads. If you must, use a factory instead `() =&gt; new MyDbContext()` that each thread uses to create its own context.
I don't know what your design looks like, but I'd recommend that each user gets their own DbContext. When that user is finished, dispose the context and all of those resources will be released.
Will you really be using all 75 entities from the same piece of code (module, class, function, etc.)? The recommended practice is to break down the context into entities that form a coherent group (product information vs. localization data vs. user profiles, etc.). The more entities you have in a single context, the longer it will take to initialize a new instance of the context. You can find more information by searching for a "repository" or "unit of work" pattern.
I was relaying the first half decent one I could remember.
Requires free registration for the back issues but has some good tutorials. http://visualstudiomagazine.com/pages/topic-pages/net-tutorials.aspx 
I loaded a database from my employer, used EF to read the schema and build poco objects. I think they have ~750 entities in the DB itself. The main thing I noticed was initialization time. It took a long time. Maybe minutes. I didn't notice a huge memory spike, but I don't remember searching for it either. I think 75 will be fine.
I think the blogs listed on asp.net are a better option to get current information.
Nailed it. I generated all of these in a separate solution and just copy them into the main one as needed.
Potentially pedagogical pedantry: do you mean that there were 75 properties declared with type DbSet?
I'm charting new waters here (for myself and my company anyway). Previously all of this stuff would happen in a thick client that talked directly to SQL Server via an ORM that I wrote from scratch. This ORM worked really well for that model. Now we're beginning a transition to running everything from the browser and all of the ORM stuff happens on the server. We're also expanding the user base of our application to include 3rd parties outside of our company so we're increasing our concurrent users ten fold. The old ORM is not a good fit for this new situation so I'm exploring my options. I'm basically down to EF or building something in house again. I'm still a little shaky on how I'm going to make this DbContext pattern work with our architecture. It seems keeping lots of DbContexts open on long running processes is frequently advised against, I presume because of resource limitations. Also advised against is having shared DbContexts. What I see suggested everywhere is to create the DbContext, grab your data, dispose the DbContext, do your work, then create a new DbContext to persist the data. The thing that sucks is because all the change tracking stuff is in the DbContext, you have to manage setting up the new DbContext yourself, which seems like a pain in the ass. This was something my old ORM managed in the objects themselves. A big difference though was that my objects had to inherit from a certain base class, which wasn't a big deal since we generated them all from the database. I feel like I'm not getting something though. EF is tremendously popular (it appears anyway) with web based projects that should encounter all these same problems. I can't seem to find anything talking about a specific approach dealing with them, only guidelines on what not to do.
Worth the upgrade for the peek feature alone. Another highly underrated feature is the async loading of projects
yeepers
Great post. For round-trip state management, here is a suggestion based on an implementation of the [Memento design pattern](http://www.dofactory.com/Patterns/PatternMemento.aspx). My idea is that for each user's session, a unique context object will live server-side until it expires and/or is disposed. When the user is ready to save the changes made, a second DbContext would be used to query the related data and compare the user's changes with the current state of the database. If the state of the data has been modified since the user originally began making changes, alert the user to this and provide them with options for what should be done: revert, merge, abandon, etc. Sorry, I don't have much time to spend on this today, but I'm more than willing to continue this discussion. More examples: http://www.remondo.net/memento-pattern-example-csharp/ http://blogs.microsoft.co.il/blogs/gilf/archive/2008/08/01/memento-pattern.aspx 
VS2013 requires IE10
So... Is this meant to replace 2012, or live along side it like my 2010 install?
Pro: Synchronized settings is built in, no more SkyDrive/Dropbox hacks Con: Requires unlock through MSDN subscription purchase. My company isn't splurging for MSDN for every employee. *edit: also, still no way to sync what extensions are installed. I'd love to even have a list of "This is what's installed on your other machine(s)"*
Wow, what's that requirement all about? Did they give a reason?
It's Microsoft. Do they ever? [People have asked that it be backed off](https://visualstudio.uservoice.com/forums/121579-visual-studio/suggestions/4153040-remove-the-requirment-for-internet-explorer-10-to-) but no movement on that front.
Magazines are dead.
I just uninstalled 2008 last week.
I was kinda hoping they at least tried to give some excuse for it publically. 
It'll work side-by-side with 2012.
That's pretty similar to the one approach I was already thinking about. I could implement that with EF. The other option, the one I'm leaning towards, is a rearranged version of the earlier framework I already wrote where the objects themselves track their initial state and know how to persist themselves (via another layer of abstraction that I've already written). I'm sure the pattern I'm using has a name but I don't know what it would be. The big advantage here is that I already a have a bunch of tooling that lets us work database first pretty well. The whole shift to web based everything is going to be jarring enough for most of the department so the more we stick with things every one is already comfortable with, the less traumatic the change over will be.
Do you see any benefit to keeping 2012 around? 3 VS installs makes for a sad SSD.
Have you tried it with IE10 yet? Maybe it will work to. Same for IE11.
Use a vm with ie8 for those programs? I really dont understand companies that impose those kind of restrictions on developers. Normal corporate users are different.
Microsoft is trying to move everyone onto standards-compliant versions of IE. They want to stop all support for IE6-9 asap. It's part of a larger move on MS's part towards embracing standards to make future planning easier.
&gt;Use a vm with ie8 for those programs? It'd be considered as a last resort. We don't do much .NET development lately, so that amount of extra stuff lying around to support an occasional need will not be implemented swiftly. &gt; I really dont understand companies that impose those kind of restrictions on developers I have to be able to use/support the software I develop against. If I can't run IE8/9, I can't do my job.
There's a browser sniff built into the app that we can't disable. On top of a very old &amp; rickety ActiveX control. I was surprised that it worked with IE9.
It sounds like you have the right idea. You could focus on abstracting the data access layer so that your "client-side code" does not actually interface with the DAL, but rather passes through some sort of API, wrapper, or service. Maybe take a look at [this post on StackOverflow](http://stackoverflow.com/questions/6657740/net-rest-services-entity-framework-and-loose-coupling) This [set of slides may also be helpful](http://www.slideshare.net/wwegner/adonet-entity-framework-your-data-access-layer)
That's what I'm planning on. Web Client &lt;- REST -&gt; Web Server Domain Objects -&gt; Abstract DAL Interface DAL SQL Server Those are the major layers of my architecture. The web client application talks to the server via REST services. The service implementation uses my domain objects (or EF objects if I choose that route). The DAL layer from the domain objects perspective is just an abstract interface. The concrete DAL implementation translates between the domain objects and SQL Server (or whatever else I want to persist data through). I understand how to build all of that myself. In fact I have a lot of the logic already built, it just needs to be refactored a bit to fit a somewhat different interface. In my framework, the domain objects talk to the DAL via dynamic objects (not actually `dynamic` objects, but a class that functions a lot like the `Expando` class does now, but with some extra metadata attached). The DAL doesn't really care where the data came from or how it got produced, it just persists the data it's given, or runs queries or whatever. It's a pretty transparent veneer for SQL Server mostly, but I could easily make another implementation for a flat file or something. The whole thing works surprisingly well. The main problem is that it's structured to support a single user on a single process. I have a lot of static stuff that needs to not be static anymore. That means I'll have to carry around a lot more references with everything which is going to bloat up quickly. In some places I've been able to package the references up into descriptor singletons but that won't always be possible. Concurrency is going to be a big issue in this system. There are lot of long running UI processes that can alter giant graphs of data at the end. It's possible that two different users could be doing things on intersecting graphs. There's also a bunch back end processes that will do the same that could run at any time now since this is going to be a real time system instead of the batch processing systems we've always built. That's a mess in it's own right that's going to have a business rules controlling the outcome. I've got an idea there to write a record locking service that all processes can use to mediate access (like a logical transaction). There are still a lot of questions yet to be answered by people payed more than me there though. So anyway, I'm still trying to figure out if there's anything to be gained by switching to EF. I've been reading a lot about, watching pluralsight videos and building dumb little tutorial projects but it's just not quite clicking in my head. I get how things work now, I just don't understand why they're done that way. Part of the problem is that I've got this other thing that I've built 4 versions over the last 10 years that takes a pretty different approach to solve the same problems and of course in my head, that's the best way to solve the problem. I also know my own product inside and out. On the flip side, there are lots of people that know EF inside and out, even if I'm not one of them. So I kind of need to justify the learning curve to myself (and the rest of my team in time) or convince my boss that our in house solution is better. Right now, at best I can say is that they're different. I know there are things that EF can do that my solution can't (emitting `IQueryable` collections is pretty sweet) but I have a really poor understanding of whether or not the opposite is true. I guess my dilemma is that I need to become an EF expert before I can accurately compare the two but to become as competent with EF as I am my own framework, I would need to basically build a new version of the framework with EF. **TL;DR**: No real questions here, just an explanation of my thought process about EF currently.
Oh, I see. I misread and took it as unlocking the entire app requires MSDN.
MS embracing standards in a browser? Bahaha hahha 
All of the worthwhile features are on the insanely expensive ultimate edition. I see no reason to update from VS2012 Pro.
Oh god, I haven't had 2005 since my first year of university.
It takes a lot of energy to get a behemoth to move, but when it does it takes large steps.
This is an excellent analogy. 
Because it's a system we bought and migrating off it would cost millions of dollars and several man-years. 
Peek definition doesn't seem to be available in Express edition. Can anyone confirm this?
By energy, I think you mean a massive loss in browser market share.
The improved xaml intellisense is a delight.
[Apparently, not much.](http://i.imgur.com/FZw65xM.png)
First thing I read: create amazing applications in visual studio 2013. Oh good, I've always wanted to do that.
Scott Hanselman on ILNumerics: http://www.hanselman.com/blog/GeneratingComplexMathVisualizationsInSVGUsingCAndILNumerics.aspx
When I went to install, the default app folder was set to "Microsoft Visual Studio 12.0"... so even the team that setup the install procedure apparently didn't think it needed a new version number. 
I was confused by that as well. Then I looked more closely and realized that VS2012 is 11.0 and VS2013 is 12.0.
Good lord, nothing like really confusing people. Doesn't SQL have some mismatched nonsense like this too?
Yes... [it is a mess too.](http://sqlserverbuilds.blogspot.com/)
Yep, that's what I remember 2008 being "10". Grrr.
It's actually worse than you think. The latest version of the Business Intelligence tool only works for the previous version of Visual Studio. I don't know if they've fixed it but having to version of Visual Studio installed is always a pain in the butt.
And heaven forbid if you converted all your DTS packages to SSIS.
Yeah don't get me started, I could complain all day.
I discovered 1 reason to keep 2012 around: my resharper 7 gets no 2013 love, and the upgrade price is bullshit high.
Pass the parent dto you already made in the parent EntityToDto to the child EntitiesToDtos. You'll have to add a parameter to the interface as well. I'd probably add it to all 4 of your methods. Call it something like context and make it an object. Now its up to your implementation to know what that context is going to be. In this case your child EntityToDto is going to expect it to be the parent DTO. You will want to test that it's the type you expected and throw an exception if it's not. The context might be something else, depending on what you need. Maybe you want to map a many to many relationship so you're going to pass in a dictionary of DTOs that maps your entities to their already produced DTO. Let me know if you need more explanation. Edit: The poster Andrew on your SO question had a good suggestion as well, just handle both sides of the relationship from the parent mapper. After you get DTOs for all the children, you loop through and set all the parents to your parent DTO.
Man I really don't understand people's obsessions with videos. Technical discussions are a lot better in text.
The first option I gave would resolve that assuming you had a parent DTO available to feed it. I have another suggestion but I just realized another serious flaw with this code. Try writing the code to instantiate either one of your mappers. It's going to be endless chain of `new`s new ChildEntityMapper( new ParentEntityMapper( new ChildEntityMapper( new ParentEntityMapper... You can't actually construct one the way you've got it written. Well you can if you pass in a null or something but then it's not going to work. One of those two needs a constructor that doesn't care about the other. Are you realistically going to be dealing with the scenario where you need a child DTO with it's parent property populated but you can't start from the parent entity? You could either settle with leaving the parent property unpopulated or populate it manually after the fact. The parent is still going to want to populate all it's children unless you branch that code as well. If on the other hand it doesn't make sense to only have a child and instead you'll always want the whole graph from the parent down, you could just hide the child mapper away inside the parent mapper. The parent mapper would take care of instantiating the child mapper, which could pass itself as an argument into the child mapper constructor. I still think you should add the context argument I suggested earlier so the parent mapper could tell the child mapper what the parent DTO all the children will have (assuming your entities are in synch already, e.g. all children of the parent have their parent property pointing back to the same parent). All this mess is why lots of software just refuses to deal with circular references.
Let me explain a little bit how my repository works. I have a regular get function like so: public virtual IEnumerable&lt;TEntity&gt; Get(Expression&lt;Func&lt;TEntity, bool&gt;&gt; filter = null) but i also have this GetIncluded function public virtual IQueryable&lt;TEntity&gt; GetIncluding(Expression&lt;Func&lt;TEntity, bool&gt;&gt; filter, params Expression&lt;Func&lt;TEntity, object&gt;&gt;[] children) In the GetInclude i can specify pretty much any entity i want, i can get the child entity and it's parent, and just it's parent. The parent entity wouldn't include any other related entities. I can also get the parent entity and all it's child entities or the parent entity and child entities and the parent's parent entity(ParentOfParent). I already have a few scenarios where I'm retrieving an entity and i need to include a few fields from it's parent entity as well. At the moment I'm making a separate call for the parent entity and mapping the attributes i need from it to the ViewModel which is cluttering my code and making more DB calls than needed. However getting a parent entity along with It's child entities is the bigger problem. I already have the parent entity's Id(the foreign key) in my child entity and that works fine. If taking it the whole way and mapping the whole parent entity as well is too much trouble or too complicated I won't bother with it. Do you kno if it is more efficient if i somehow implement Lazy Loading into my repositories / dto's so that i can just ask the repository for a specific attribute on a parent entity? Is it even possible to lazy load through the layers (DbContext -&gt; Repository -&gt; Service -&gt; ViewModel) ? EDIT: After reading this i realized that for these specific scenarios i can just have a function in my entity service that maps it without causing a circular dependency.
That's so 20th century of you. Some people learn better by sound than eyes; or, in the case of videos, both.
I'm not completely following but I think I get the gist. I don't think it's possible to have lazy loading at the dto level without making your DTO aware of how to dig back down through the layers, something you probably don't want to do. I don't think that's a feature you'd want in your DTO anyway, it's supposed to have all the data encapsulated and ready to be serialized across the wire. EF is able to handle this somehow internally with the navigation properties it creates. It will only populate the navigation properties while the DbContext tracking the entity is alive though. I'm not entirely sure but I assume it's accomplishing this by caching references to all the tracked entities so that when you set the foreign key value on the child, it already has available the parent object with that key and sets the navigation property for you. If the context has been disposed, this doesn't happen. I'm far from an EF guru though, so I could be way off on what I'm describing. What you could do is have your mappers keep track of all the things they've mapped so far, so when you ask them to make a dto for some entity, if they've already done that, they just hand you back that instance. So say your starting from a child entity, the child entity gets to it's parent property and calls the parent mapper to make the parent DTO. The parent mapper does it's work until it gets to the children property where it asks the child mapper to construct the DTO. The trick here is that the parent mapper has reference to the same child mapper that called into the parent mapper in the first place. Since that child mapper has cached the child DTO that it's still in the process of creating, it simply returns a reference to the cached DTO. Parent mapper is happy and returns the parent DTO back to the child mapper, which uses this value to set the parent property. In the end you have the whole circular graph populated. One thing you'll need to change is the constructor definition of your mappers. You can't have the parent mapper in the child mappers constructor and vice versa. Just make at it public property and you can do something like this: var cm = new ChildMapper(); var pm = new ParentMapper() { ChildMapper = cm; }; cm.ParentMapper = pm; Now you've got your mappers pointing at each other. From inside the ChildMapper constructor you could do this if you wanted: ParentMapper = new ParentMapper() { ChildMapper = this }; Or vice versa from the ParentMapper. Your cache would look like this: private Dictionary&lt;TEntity, TDto&gt; _cache; and can live in your base class. Then you have your MapperBase implement EntityToDto like this: public TDto EntityToDto(TEntity entity) { if(_cache.ContainsKey(entity) return _cache[entity]; else { var dto = EntityToDtoInternal(entity); _cache[entity] = dto; return dto; } } Then you need a protected abstract TDto EntityToDtoInternal(TEntity entity); which you implement in the concrete mapper exactly like you have EntityToDto now. ~~This way all cacheing junk is under the covers and it just works.~~ Ok so after typing all that out and thinking about it, it's still got a problem. The entity doesn't get added to cache until it and all it's properties are fully fleshed out, but in the process of doing that, you're going to end up back at this mapper being asked to make a dto for the same child we started from, which isn't cached yet. To solve that I think you need to split the operation of populating the object and populating it's navigation properties, maybe with another protected method like protected abstract void PopulateNaviagationPropties(TEntity entity, TDto dto); Then immediately after you've added your dto to the cache, you go back an fill in the navigation properties so that when the parent dto is being created and comes back to the mapper to get the child, it's in the cache already. I'm not sure how you've implemented your GetIncluding method so I'm not sure if this approach is going to fit with what you've got. I'm just throwing some ideas around. Is this helpful at all or am I totally off base? Here's a partial of the base class I've come up with. You'd need to mirror these methods to go the other direction, form dto to entity. private Dictionary&lt;TEntity, TDto&gt; _cache; public TDto EntityToDto(TEntity entity) { if(_cache.ContainsKey(entity) return _cache[entity]; else { var dto = EntityToDtoInternal(entity); _cache[entity] = dto; PopulateNaviagtionProperites(entity, dto); return dto; } } protected abstract TDto EntityToDtoInternal(TEntity entity); protected abstract void PopulateNaviagationPropties(TEntity entity, TDto dto); Then your ChildMapper would look like this (only partial, need the reverse operations): public class ChildMapper : BaseMapper&lt;ChildEntity, ChildDto&gt; { public ParentMapper ParentMapper { get; set; } protected override ChildDto EntityToDtoInternal(ChildEntity entity) { return new ChildDto { SomeProperty = entity.SomeProperty }; } protected override PopulateNavigationProperties(ChildEntity entity, ChildDto dto) { ChildDto.Parent = ParentMapper != null ? ParentMapper.EntityToDto(entity.Parent) : null; } } 
[Learning Styles](http://en.wikipedia.org/wiki/Learning_styles)
And it is, has been, and continues to be heavily criticized since it has little scientific basis and usually produces no results when practiced. Read the wiki yourself.
C:\WINDOWS\Microsoft.NET\Framework\v4.0.30319\msbuild "c:\Work\MyProject.vbproj" /p:Configuration=Release pause THat's what you'd have as the contents of a batch file. Put that in a batch file and run it. The pause line will hopefully stop and let you see any errors/warnings/issues. I have a lot more lines than that in my build batch file, and you will too. Search online for more stuff. Here's a good start: https://www.google.com/#q=batch+file+msbuild Sorry for taking so long to get back to you. Hopefully you've already got it by now.
Glad I could help. This is precisely the kind of stuff I do at work. By the way, I didn't have to change too much of what you already gave me to make this work, so don't think it's any kind of magic that's way over your head. This is just a tweaked version of your code, which looks like a pretty solid architecture.
Once you get the hang of xaml it is awesome, and you can quickly build great looking interfaces that would take much much longer in forms. Also, it uses the mvvm pattern well, so you can get a proper separation of concerns between UI logic and business logic.
All the cool kids are doing mvvm right now. I'm in stevedoring, which is generally not a high tech industry, from an IT point of view. The company I work for is breaking some serious ground.
I was curious as to how to structure projects to use Dtos, entities and models from what i understand Dtos - any service or db call gets converted into a Dto which can be used by any project entity - dto are converted to entity when used in business logic model (in the case of mvc) - entites are converted ot models when they are presented to the view do i have that about right?
Well you're close. All functions/services in the BL that are exposed to the UI layer return DTOs. The DTOs are then converted to models/viewmodels in the UI layer before being sent to the views.
Not sure if 2010 qualifies, but upgrade pricing is reduced for the next few months ($499 for pro). Also, be careful upgrading SqlServer, as the licensing terms have changed to per-core $$$$$$$$
No. But if you have an iPhone it will.
Thanks for pointing that out to me. Now its going to bug me to no end!
How I've been doing it so far in my child DTO/ViewModels is not to point to the parent, but simply having the parents foreign key property on the child entities so if i need it it's easy to get. Fortunately for me there are very few scenarios where getting the ChildOfChild entities would be useful so i can avoid that headache :) I'm using AutoMapper in my project just for different mappings. I'm using it to map from my DTO's to my ViewModels. The reason I did my own mappings between the Domain Entities and the DTOs is because I read somewhere that using it to map between the entities and DTOs was not recommended but i honestly can't remember the exact argument why it shouldn't. I think it had something to do with the fact that the domain entities usually contains logic that AutoMapper often doesn't map correctly and can be unpredictable so it's safer to do it manually. But since DTO's and ViewModels are both dumb objects with simple relationships using it to map between those is fine and recommended.
We will definitely need the child of child include - I know I've seen it done before, it'll just require some (potentially major) refactoring. I agree RE: AutoMapper - I only use it to map domain objects -&gt; DTOs. For "mapping" in the other direction, I create/modify the domain objects by passing data from the DTOs to constructors and functions on the domain object so that the domain objects can maintain their own state. In your case I can't think of a reason not to just do two-way mappings between your VMs and DTOs - as you said, they're just dumb objects. Out of curiosity, what's your technology stack look like? We have an AngularJS frontend consuming .NET WebAPI web services, with a code-first EF + SQL Server backend.
Out of the reddit frame, everything loads. But I thought it was a nice parallel to VS 2013, a release that should have been a service pack for VS 2012.
Thanks, [this blog post here seems](http://typecastexception.com/post/2013/10/20/ASPNET-MVC%E2%80%93Azure-Websites-Azure-SQL-Database-and-EF-Model-FirstDatabase-First-Connection-Strings-when-Deploying-from-Github.aspx) to be just what I needed, if only it had been published 2 weeks ago when I was scratching my head with this issue.
Hardly top 5. The article is also just generally a piece of shit. You also continually link to this blog. Please stop.
How about bad formatting, useless content and not-so-good English? Your article is *much worse* than any of the official ones, thus it's useless.
What happened to BugAid name? It was much better. 
Good, I've been without for almost a week and I was missing it.
Oh thats good news!
Hey guys, I'm one of the devs on OzCode. Please let us know of any issues you might have on our [userecho](http://ozcode.userecho.com) site! Thanks!
I just want to say thank you.
My pleasure! Seriously :D
A ran into similar problems with apo harbor. Cloud providers don't seem to realize that there is a lot more to CI/D than just compiling and testing. 
In this case, I feel it has more to do with Entity Framework integrating all that meta data into the connection string. The result is a far from standard connection string (although, if you get the format correct per the article, it works just fine). If nothing else, the Azure team really needs to do a better job documenting this, and making it readily available. I was all over Google piecing this stuff together. Which is why I wrote the article . . . Beyond this, I found the Azure deployment process to be pretty smooth so far (albeit, I am dealing with some rather basic cases to this point). 
&gt; Beyond this, I found the Azure deployment process to be pretty smooth so far (albeit, I am dealing with some rather basic cases to this point). Can you do pre/post deployment steps? Can you deploy background services? Can you run nant scripts? I'm not familiar with azure do if the answer is yes to those 3 I'd be impressed, if not I'll stick to team city. 
&gt; Can you do pre/post deployment steps? Can you deploy background services? Can you run nant scripts? Yeah, I don't think so. Team City looked pretty cool as well, although I'm just learning my way around CI/D. There's a lot to learn. I should clarify in the above, I have been messing about with deploying to Azure Websites from Github or BitBucket. I don't think they (the Azure team) represent this as a complete CI/D solution, just a convenient way to deploy your site. I suspect that for the level of functionality you are looking at, you would want to set up a VM instead (azure VM's look pretty cool, too, and they have a gallery of pre-configured images. That said, Team City is still probably a more complete solution as far as CI/D goes, since it is purposed with that directly in mind.
If you're using the PAAS functionality, you can actually drop in whatever code you want to run during deploy: http://blog.amitapple.com/post/38418009331/azurewebsitecustomdeploymentpart2 I had trouble getting much done with it though. I'm not good with CMD files, and was trying to get it to call some node code to do some preprocessing without much luck. My biggest disappoint was the way the deployment is done (for PAAS) is they just update the changed files in place. I would much prefer it deploy to a clean instance then switch traffic over once all custom scripts say its ready.
Funny, I was just reading that very article and thinking back to this thread. 
I don't think there are any official MS channels left for buying 2010 directly, but there seems to be some versions on Amazon for $200 - $300. The best way to get VS (IMO) is with an MSDN subscription, but it's not the cheapest way. You'll generally get other tools, and have 2-3 versions of VS available. Also keep in mind, that you can still maintain and target older versions of .NET from newer IDE's. So you can use 2012 to work on code from 2010, without changing the framework version. Yes, it generally does run about ~$500. Amazon is a bit cheaper (2012 for ~$460), and other places like Component Source may be even cheaper. Also, do you need Pro? The Express versions do quite a lot for free. And .... have you looked into the Dream Spark / Web Spark / Biz Spark programs? I know that some of them have gone away, but if you can qualify for one of these programs, you can usually get a free MSDN subscription for a couple of years. 
Thanks for your tips! Yes, maybe I was focused too much on the fact that it had to be VS 2010, which is much more expensive than VS 2012. I think I will buy VS 2012. I'm a bit lost in all those MSDN subscription jungle. I'm a freelancer and have a contract with a client and have to deliver him VB code. Any of those subscriptions would be right for me? 
based on this comment: &gt;I'm a freelancer and have a contract with a client and have to deliver him VB code. You can get the express version for free. Though now its 2013. What is the motivation for 2010? you can still do .net 2.0 - 4.5 in 2013.
FYI, visual studio's license includes concurrent downgrade rights. If you buy 2012, you can install 2012, and 2010, and 2008, etc. (for your use only, of course). You have to get the installers yourself but one license works for all of them. Also, 2012 can open 2010 projects (2012 is much nicer than 2010!). Also definitely check out bizspark. That's what a friend and I use to build and pay for all our side projects (obligatory http://upboat.me reference). It comes with a sweet msdn (for windows, vs, etc) and a big credit in azure every month. 
2010 Express is still available, but you have to dig for it ... . It's at the very bottom of the download page... http://www.microsoft.com/visualstudio/eng/downloads#d-additional-software But I agree, you can fall back on the newer versions. 
If you need third party tools, or need to work in both VB and C# projects in the same solution, you'll probably need Professional. Unless you need something like TFS, or a specific feature of one of the more expensive versions, there's probably no reason to go beyond Pro. If I was buying one version, I'd probably buy the newest, since you can target backwards to the older versions. If the client still needs to work on older versions, just copy the solution to a new one and add the version to the name (I have a client that still uses 2008 on site, and I use '10 or '12, so we keep two separate solutions). 
Oooh, I really like the toggle through bookmarks; too many key combos in VS for that normally. I guess many of the VS "ThisKey + ThisKey, Then ThisKey" combos would be nice on the G13.
To be honest I would much prefer a mouse with hot keys to a keyboard. Most keyboard commands are already mapped to 2-3 key presses anyhow.
Very cool. It's been a bunch of years since I used an Express version. 
They've definitely came a long way. The only things they lack now are plugin support, extended refactoring support and the fact that web projects, Windows Metro projects and Windows Desktop projects are still separate (though I understand why Microsoft still do this.)
I have, and use, both. If I'm on the keyboard then I use the one touch keyboard buttons, If I have the mouse in my hand, then I have the same buttons on the mouse to use.
That's nice; very intuitive with sort of forward/back making sense.
2010 had a better UI than 2012 imo. That being said I've been on 2013 all week and I'm not looking back.
Agreed. At least they have a 2010 theme now. The only thing really holding me back at the minute is one commercial plugin that I need to upgrade for 2012. 
BizSpark is one of the most beautiful things that Microsoft has done.
Debugging JS in Visual Studio seems silly to me. Any browser worth it's salt has a full suite of JS dev tools these days that are more than capable. Doing it in VS just adds unneeded overhead IMO.
I don't think 2013 supports certain types of projects (setup/installers) that he may be using with 2010
it can be nice to step through it in the file that you would be editing to make changes.
Thank you! It looked like a dodgy DLL from DevExpress!
With web essentials, it can act as a central place for all JavaScript errors across all running browsers accessing the image. 
Have you considered using DropBox and coding directly in the dropbox folder/subfolder? DropBox would then sync for you.
You could develop in a Dropbox folder. Supposedly, they do versioning as well. I just use git.
i used Dropbox before, but it does not have versioning feature. And dropbox syncs everything, binaries and debug files. I would like to copy only source code, and propery version it.
Remote desktop into machine 1 from machine 2?
Github has some great tooling. As stated above you will have to comit your changes after you finish working. Git has git ignore files that exclude certain file extensions and folders.
I did this a lot back when I was in school and found it to be the simplest solution. File level solutions like git are great but they only keep the files, not the state of the IDE for example. With VS2013 they seem to be doing more with keeping settings and thing "in the cloud" but ... RDC works.
&gt; Whatever you change and compile, gets automatically versioned, and can be rolled back. I think a lot of this is normally handled through configuration management practices.
Before getting into the "how", are you certain that this is permitted? Especially the part where you're putting company code possibly on someone else's server(s), or remotely accessing the company network from a non-company computer?
I guess it should handle it. Consider updating to EF6 which handles large contexts faster. If thinks can be cut up in different schemas to make them easier to handle, EF6 supports that now.
yes it us permitted. Company does not care where I code, I can work home or in the office.
Literally, *just* released
Can you add line breaks to the code snippets or is that a part of the challenge?
Start with the source control the company uses. 
Sorry, I know that's annoying. Breaks added
I'm surprised about the "not svn" requirement. Why not svn? I would personally suggest svn or git. For online services you can try github (git, svn), codeplex (git, tfs, mercurial), google project hosting (git, svn, mercurial), bitbucket (git, mercurial) or sourceforge (i don't know). I personally prefer github. If you want to install your own svn server I have had good experiences with Visual Svn Server.
yeah, at the moment I am doing it this way. But sometimes I forget doing it, sometimes I have one dll referenced to one place in work pc, and to other place in home pc, and the references get copied all the time messing things around, sometimes components I am using in my work pc, are not installed or licensed in my home pc etc, some projects I am referencing might not exist etc.. what I was looking for is some sort of a virtual shared environment, very similar to RDP, but instead of transferring screen, it needs to maintain the same working environment on several PCs, perhaps some plugin to VS which works between SVN and IDE, to accomplish this task.
Some feedback... (hopefully I didn't mess up the question numbers) Q1: Easy but good question. Q2: Terrible code formatting. Tests a part of .NET only. Uses C# (F#, VB.NET and others exist..) Q3: This is trivia... Q4: What do you mean, "application state variables"? You said .NET, not ASP.NET. Q5: This is about C#, not .NET. Q6: Easy, but why not. Q7 &amp; Q8: **No.** No. No. There are plenty of explanations on the web; [here](http://blogs.msdn.com/b/ericlippert/archive/2009/04/27/the-stack-is-an-implementation-detail.aspx) is a good one. Boss Question 1: Again, WTF? This is supposed to be a .NET Challenge, not an ASP.NET one. Q9: This is a very weird question. While the *compiled* code is self-describing in a way, the *code* itself can be obtuse, weird and impossible to understand. Q10: **Wrong** again. An abstract class is a class that cannot be instantiated directly; any other description is wrong. You can create an abstract class without any abstract members. Q11: Weird phrasing, and the answer isn't the only one (NGEN exists too), but it's correct. Q12: Trivia, but OK. Q13: Too easy. Q14: **Wrong**, though most people don't know this. The `Environment.FailFast` method exits without running `finally` blocks, destructors, or anything else (that's its entire point). Q15: OK. Boss Question 2: Which GridView? There are classes named GridView in WinForms, WPF, WinRT, Silverlight and ASP.NET WebForms. Q16: OK. Q17: Oddly specific, but OK. Q18: OK. Q19: "Assert command"? Be specific. I suppose you mean `Debug.Assert`. Q20: OK. Q21: OK. That's a lot of question about switches... Q22: OK. Boss Question 3: I wonder how many .NET programmers have done that recently... I didn't go any further. *Please* make sure your answers are actually correct; out of 25 question there are 4 wrong answers. Also, there are quite a lot of mistakes (mostly grammar-related ones such as missing words) in the questions and answers. This quizz is anything but professional.
&gt; *...where you stop coding at work, and come home and continue from the same point you left at work.* Spin up an EC2 instance, slap visual studio in there, and remote in from work and home? One file system, no problems. &gt; *Whatever you change and compile, gets automatically versioned, and can be rolled back.* Whip up a post build command action to check in all changes to Git or SVN? ' git add -A &amp;&amp; git commit -m "Another successful compilation" ' That way you don't have to think about it, but also have a proper VCS underlying your project... File based solutions are asking for pain wrapped in hurt topped with some ouch over a long enough timeline. It's not a coincidence that all VCS systems have similar core functionality that extends well beyond the normal filesystem. &gt; *...sometimes I forget doing it, sometimes I have one dll referenced to one place in work pc, and to other place in home pc, and the references get copied all the time messing things around* This is the root of your pain, IMO. Project guidelines: all external references (outside of NuGet) should be copied into a 'references' folder in the project itself, to maintain dependencies and support. This way they get put into source control, and you'll always be able to recreate historic code. I mean, what's the point of auto versioning if you don't have the required libraries to build, test, deploy, or compile old versions? Solve that one issue, and I think you'll have a lot less need to synch between computers. Private NuGet repositories aren't too hard to slap together either, for dependency management. 
Dropbox actually does do versioning - you access old versions via the website, but it does do that. Also, you could exclude the bin and obj folders by going to Settings &gt; Advanced &gt; Selective Sync. I'd still recommend getting into the SVN or Git habit, though.
If your main requirement is to not sync build artefacts then SVN and a Visualstudio SVN plugin like Anhksvn would work, Anhksvn will only check-in project files so all the temporary/build files will stay unversioned on your local folder.
You answered 43 out of 66 Arena questions correctly, and scored 3607 points! Rank #1 at the moment. Got a bunch of the questions wrong because I was too annoyed with their formatting and/or length, and just clicked something... A bunch were vague and incorrect
I agree, a lot of questions were either weird, vague, or partially wrong. &gt; Q19: "Assert command"? Be specific. I suppose you mean Debug.Assert. I got this one wrong because I assumed it was about Assert.AreEqual, or similar
This is the classic issue with programming tests. I remember telling a prospective employer to *jog on* when they sent me such a thing. For a WPF role it was asking about WinForms development.... Incorrectly.
&gt; Whatever you change and compile, gets automatically versioned, and can be rolled back. I've used dropbox as source control before, but even though they have some versioning, rolling back is pretty hard. Their versioning and history also used to be limited to 30 days
thanks, that was quite informative. 
In addition to BizSpark, as was mentioned, if you have an email address with an .edu tld, you can sign up for DreamSpark. It gets you all the latest VS, SQL Server and Windows Server versions. http://www.dreamspark.com/
I might be able to help you. I'm a senior .NET developer and have experience in ASP.NET and have a relatively successful Windows Phone app. Sent you a PM.
All comments below are made after spending 10 minutes browsing your source on GitHub on my phone to take them with a grain of salt 1) why did you build this? Solving a problem you have currently at a job and were able to open source it, you wanted a challenge to build this or you want to make money with this? 2) quickly glancing at your code I don't see any tests, maybe I didn't look hard enough but you won't get a lot of respect as any open source tool without tests. 3) an expansion of 2, if one of the big pluses here is the performance you really should have a "load" test that people can run on their box to see if they see similar performance.
You do realize that the azure platform features a free caching system. It's just not OSS. Yes you can run azure services on your local system and network. The cloud service uses azure but use of the ms cloud is not a requirement.
Thanks! Concurrency is handled a few ways: 1. Ensuring that the TCP/WCF side is configured to allow enough simultaneous connections in that people don't get blocked by throughput. 2. Immutable (read only) objects where possible; they are inherently thread-safe. 3. When immutability cannot be achieved, usually a ReaderWriterLockSlim is used. It allows n readers but an atomic 1 writer at a time.
Yes, I do know this. My problem is that I tried to use it for a high-profile application and it didn't work well at all. This was built in response to that and other caching systems not "making the cut" performance wise. Also, not everyone wants to host in "the cloud" despite it being a super popular option right now. I do have Azure hosts in the works though. :)
you tried to use Azure cache server on a system you control or in the cloud? They are 2 different things. Azure caching is a service that is part of the MS cloud however it is also a package you can install and run yourself.
First, let me confirm that we're discussing the same technology: AppFabric Caching. We tried to run it locally (not in the cloud). It was brutal. 50% of system memory reserved for "internal operations", bad throughput, regular cache node crashes and tons of communication exceptions.
You're 100% right. I suck for not thinking to write tests... Can you tell it's my first open source project? It sounds like you know what's going on - would love any contributions to Dache you could make!
What's "Senior developer" where you work? Some places they want project managers, and other places some skilled programmer to help others.... 
Anyone else feel like it was more about ASP.NET and EF than .NET itself? List of stuff to add: * GC (most people won't care, but at expert level you should at least know if its non-deterministic) * System.IO * Generics * Threading Other things: some are just vague, others are just wrong ( can't recall which), the C# one has a bunch of errors as well.
Besides the other comments here I have a couple... 1. Is there a nuget package? 2. How does linking to gplv3 code work in .NET? I was under the impression that if I linked a gplv3 library in my non-OSS code I would have to open my code.
I keep reading that Redis is great (and often superior to memcached in direct comparison). Did you ever evaluate that?
1. Not yet but that's a great idea. I'll be doing that tonight! 2. As best as I understand it, you'd only have to open up your non-OSS code if it modified my code as a part of your code... If you just used the compiled library in your code I then it should be all good.
So true. So, so true.
That's not my understanding. If you want a library to be usable by non-GPL software, use LGPL (or MIT, Apache, BSD, ...). [This page](https://www.gnu.org/philosophy/why-not-lgpl.html) is called "why you shouldn't use LGPL" but its arguments are specifically targeted for people interested in restricting the use of their libraries to GPL projects. It implies that LGPL libraries are compatible with proprietary software, and GPL libraries are not.
This is good to know. Clearly I need to rework my license a little. Perhaps I'll get a lawyer on the phone for a chat and figure it out. Thanks!
I use them wrapped in a DataView as a DataGridView.DataSource because then you get sorting and filtering for free. I also use them when I don't have foreknowledge of the columns being returned from a database because it's easier and more maintainable for everybody than using a DataReader. Another use is for quickly being able to serialize multiple related data tables (in a DataSet) to a single binary stream for later (for instance - a reporting tool that runs user-defined queries on a database and saves the results to a file, so the original results are always available for viewing, even if the database changed.)
Why would you want to get 2010 instead of 2013?
What credentials does your application pool run under? And does that credential have access to the R stuff ?
Your presentation tier interacts with the Data Access tier using the Middle tier (WCF in this case I presume). Simply extend the interface of the middle tier with a method that allows changing the database: the Middle Tier can then propagate this operation to the Data Tier. Also, I'm not familiar with this specific MS implementation, but I would highly recommend introducing clear abstractions (interfaces) between the tiers if they are not there already, so that you can swap out implementations if need be (say you want to change the database from SQL Server to MySQL, or just an XML file, etc.).
Yes the DataService is WCF. Thank you for your help I am finding I am very ignorant about how to do this the 'right' way. Everything I built so far was tightly coupled. Do I have this right: Presentation references WCF which references DataAccessTier - so if I want to change something in the lowest tier I need interfaces at each step to walk that change from Presentation to DataAccess?
The libraries had a restriction forbidding their use on platforms other than MS. So I couldn't use the Immutable Collections in a library I am writing and then use that library on Mono for example.
The data service tier should have the connection string and create the connection. It controls the unit of work. Is this a web or desktop app? If it's a web app then don't involve WCF at all. Work out whether n-tier is worth it or not for you. To me it just adds a bunch of useless abstractions. Finally, don't follow Microsofts guidance on architecture, it's pretty useless. 
This is a winforms app but I want the flexibility to change to web later if I need to. I am primarily self taught and problem-driven. I want to build things the 'right' way in addition to building solutions that work. 
For your first project I think its fantastic!
I can't say if it was well organized or not (I haven't seen it). But having no classes sounds like your fighting the .net framework instead of working with it. t sounds to me like you never learned OOP in the first place, which is neccessary before you get into architectural considerations. &gt;I want to change that impression of my code. A worthy goal, just don't go to overboard int the other direction with a million classes that do virtually nothing (yes, I've seen it done).
I am heading in that direction I think. I plan to have a config file for the DataAccess where DataAccess will pick up the ConnectionString that it should use when it is run. The user will be able to change the config file and restart the application so they can change database connections if they need to.
&gt;You have to remember that in a WCF scenario you are sharing the server between users and if you aren't then stop using WCF cause you are wasting cycles serializing objects. And network latency. I really wish more people understood this. There seems to be a cargo cult of adding WCF services into web applications. 
I don't have a definitive answer, but I'd definitely post at the DevEx site. Their support has always been stellar. 
It sounds like you need to do some filtering of the data before you hand it off to be charted. I can't really tell you how to do that because it's going to depend on your data and what you want from it. Unless the chart control is doing some math for you, you probably don't need 100 data points per pixel.
I know I have too many points per pixel. My question is how do I do that filtering. What do I google for?
It's going to come down to math. You're going to need to sample your data somehow and combine data points in a way that makes sense to your data. If it's a pretty smooth line, you could probably do something like a sliding average. That might obscure outliers though, and those outliers might be important. So in short, I can't answer the question for you because I don't know your needs.
While not ideal, have you considered rendering to a larger than necessary bitmap and just rescaling the bitmap down instead of re-rendering the data to the bitmap?
For us, as a .NET/C# house, migrating from 2010 and 2012 was super-simple and almost a direct replacement, I believe the project file format had been tweaked very slightly so needed to upgrade the proj files which made them incompatible for earlier versions. Now with the release of 2013, I have myself testing with it and haven't run into any incompatibilities yet - the project files this time didn't need upgrading (though there are new, 2013-only project types available for consumption). The biggest hassle for us is VS extensions/addons. We use a bunch of Telerik libraries that need to be reinstalled/upgraded to 2013, and also our Resharper 7.x licenses are only valid for 2012, requiring us to upgrade to Resharper 8 ASAP. I think the maintenance/renewal plans that these companies use are pretty rubbish though and in the future, will avoid licensing terms like this where possible.
I agree. 50 dollars can get you a 2000 page reference book. Hell 30 dollars can get you a nutshell book http://www.amazon.com/5-0-Nutshell-The-Definitive-Reference/dp/1449320104/ref=pd_sim_b_1 But with StackOverflow &amp; great MSDN documentation is there a real need for books that gather dust?
PDF is dust proof :) But agree - doesn't look to be worth it
Holy cow. Migrating managed code: a couple of changes. Migrating unmanaged code: about 50 changes, some of which sound like they took a while to identify. And yet I hear people talking about how the industry is re-focusing on C++???
This is not a re-print of MSDN or a reference book. It's 100% original content that you are not likely to find in other books, or on the web - at least not in the format presented.
I did. It does not answer my questions. &gt;This book is intended for architects, developers, and other information technology professionals who design and build security components/layers of .NET solutions. That sounds like it could apply to 99% of .NET developers at some point in their careers. &gt;Many security developers do not know most of the material in this book, regardless of seniority or job title. Says the person trying to sell you a book... &gt;What many security developers believe to be the right-thing-to-do is often wrong and/or insecure. Such as? &gt;Even the most knowledgeable and aware security developers are often unable to produce quality .NET implementations that are sane, simple, reusable, and secure. Because? If the rest of your "book" is as light on information as your preface, I'm going to pass. Given your apparent lack of any kind of significant online presence, this "book" seems like it would better serve you as a series of blog articles that could actually establish you as someone worth listening to in the area of security.
There is preface, table of contents, full source code, and some "thumbs up" on Twitter from people who've either already read it, or bought it, or both. If you are still not interested, then you are not interested - nothing wrong with that.
I think they would argue that the industry is focusing on industry standard C++.
I have several mid size projects that I want to take from VS10 to VS13, without upgrading the .NET version. I also want to make sure that VS10 can still open that upgraded solution so I can still take advantage of my existing setup projects. Has anyone done something like this yet?
Thanks for the reply. My company is one of those 'hey we should upgrade! ya maybe some day.' type of places. So I am pushing this agenda on my own.
That's what I've got at work and it works fine. TFS 2012 gets you some nice features that you miss out on with TFS 2010. Mostly these revolve around work items, the source control part is the same.
My work is to stupid to use tfs for bug tracking and feature tracking. I am working on changing that but I don't have much sway. Its always 'Not now we have to much to do'. You'd figure after four years of to much to do they would realize that in software there is always a new deadline, new feature or critical bug. Oh well, i'll keep pushing. Thanks for the heads up on the work items though.
Hey I take it that VS 2012 Professional can connect to a TFS server just fine? Also do you have the msdn subscription and if so what does it offer that is worthwhile for a small team?
Yeah, no problems there at all. You just point your VS at your TFS server and off you go. MSDN is fantastic, especially if some one else is paying for it. You get access to a ton of software to use in development, though you still need licenses for things in production (Windows, SQL Server, etc.). Whether it's worth the cost to your particular team is something I can't answer. We get our money's worth.
So anything that Microsoft no longer produces is free. But items like Windows, SQL Server, Visual Studio, Office products all require licenses? Is it just the latest version of those products or is it the whole life. In other words would SQL Server 2008 be free but SQL Server 2012 require a license?
We have a guy that just deals with all this licensing stuff. It gets really complicated and also varies from product to product. None of the old versions of anything are free. With MSDN, you can get a whole boat load of MS software you can use strictly in your development environment. So you can have a server running Windows hosting SQL Server and IIS with you development database and web site on without needing to license the software. Same goes for any internal QA versions. If it's public facing or used internally for business operations, it must be licensed. That's just a very generalized overview though.
Perfect. This is exactly what I needed to hear.
Yes.
We are not exactly cash strapped but at the same time products we buy will eventually come out of the department budget and that effects bonuses. We are still small enough for a direct correlation. So I am trying to just get us upgraded and licensed (currently under the radar but working to fix that).
Nice advertisement, but I would rather read about something that makes it easier to use it.
It's just one developer's opinion. (And it's not me who wrote it.)
It's worth pointing out though that mono is a lot slower on the techempower [benchmarks](http://www.techempower.com/benchmarks/).
So, by buying 2012 I can have 2010 as well? You mean, the licence key is the same? My main concern about using 2012 is the fact, that I will work with solutions that include Setup projects, I mean projects that generate msi files for current solutions. I heard things, that 2012 no longer supports Setups, is that true?
Yes to your first question (and if the key doesn't work, ask MS for a working one). Regarding setup projects, I can't really say, unfortunately. I've only used out-of-vs installer tech. 
I use .Net every day for work, but I'm learning RoR for my personal projects. Mono isn't really a viable alternative. I've used MS stuff since MS began and there are always gotchas. It's fine for work where my employer pays for all the licenses and I even enjoy working with it but I want my personal projects to be more portable, open, standards-compliant, reusable, inexpensive, etc...
Not in demand...5 offers in one week in April, 3 when my contract ended in October. Give me a break.
The main problem that I have with ASP.NET is that it takes an faux-event-driven approach to what is a very serial problem. Unfortunately this leads to all sorts of data races in single-threaded code because it's literally impossible for an untrained programmer to follow the path of execution. We're constantly having trouble figuring out what code is executed when (OnInit, OnLoad, OnPreRender). Maybe it's just my coworkers who use it like that, but it's pretty aweful. The other problem is that it is extremely difficult to do any thing REST-ful, though that seems to be par for the course in enterprise web frameworks. I'm much more hopeful about ASP.NET MVC, though I haven't had an opportunity to try it (possibly the source of my hope).
It's also worth pointing out that those types of benchmarks are very hard to make fair because 1) different platforms have different strengths and weaknesses, and 2) unless you've studied their source code in detail, and you're deeply familiar with all of the platforms, you can't possibly know whether the benchmarks are correctly implemented or not. Also, pure runtime performance is only one of many factors. In the real world, you also have to evaluate cost of ownership, development velocity, robustness, scalability, learning curves, the third-party ecosystem, availability of enterprise support, and dozens of other factors. Of course, Mono is mostly identical to .NET, so most of those factors are the identical between the platforms.
What you're actually talking about is WebForms, which most .NET developers nowadays seem to shun, often for good reason. If you look at how the ASP.NET stack is structured nowadays (since 4.5 I think), WebForms is just one of several application frameworks (WebForms, MVC, Web API) built on top of the ASP.NET base architecture. Also, if RESTful is the way you want to go, the issue isn't that WebForms is bad, it's just the wrong tool for the job. WebForms is built around a Model-View-Presenter pattern, which for reasons that should be obvious is not a good fit for that. If you want MVP, use WebForms; MVC, use MVC (duh); and for RESTful, use Web API.
To be fair, I've never really heard anyone say that .Net isn't in demand, outside of this article.
The nice thing about the techempower benchmarks is that all the code's on github, and they take pull requests. Anyone who thinks their favorite platform hasn't been optimized correctly is urged to contribute, and they've been through six rounds of improvements so far. Performance is certainly only one of many considerations, but it's nice to know nevertheless.
Ha. Okay, that's good to know. I've always been surprised when people said that they liked ASP.NET, but I guess they weren't crazy, I was just uninformed.
&gt;Its Statically Typed It frustrates me to no end that people think this is a disadvantage.
A lot of hatred for ASP.net is actually a (justifiable) hatred of web forms.
&gt; Webforms is great if a: you know how to use it (REALLY know how to use it), and b: you're doing something complex. That's the exact opposite of my experience. I've found it's much better for people who like to drag'n'drop they're way through life and that it becomes a complete pain in the arse as soon as you want to do something complex. 
(a &amp;&amp; b) not (a || b).
How is RoR more "standards-compliant" than ASP.NET?
Bad programmers deride languages and technologies they don't use. Good programmers know why they don't use them.
I much prefer working with statically typed languages. 
If anything C# is more of a standard, being approved by ECMA and what not.
Right, actually we hate what we use and deride what we don't.
I agree. Just being able to choose the platform you host your application on is a huge advantage already. There will always be some kind of lock-in. I also like to work with modern js frameworks. I adore yeoman for example. But I hate using it on Windows. So I'm stuck with a fractured work environment (visual studio on Windows, yeoman on a linux vm).
I think when a lot of people think statically typed they think about C, C++, old C# and Java where typing everything forces you to type types everywhere..... type Type inference in some of the progressive statically typed languages like C# and Scala shows how much this isn't true nowadays.
The auto keyword in C++ is similar to var in C# and works nicely as well. And they are a real convenience. 
Thank you!
I've never used the designer. The only thing I find really hard to do in webforms is "real" ajax. You're not going to be able to access any of the stuff you just added to the page in javascript once you post back.
Luckily we have TFS 2010 which is why I wanted to know if 2013 works. I decided to just go with Professional. The devs around here to 'dev light' to know how to use most of the features in advanced versions.
WebForms is no where near as good as the MVC offering. That said, by using Sql Server as your view state store, and a strict regimen of not using &lt;asp: tags, you can leverage c# and make a damn good website in web forms. I don't know why you wouldn't pick MVC at that point, but I thought this was worth saying. 
I may be taking from my experiences in other areas with MS and applying them here mistakenly, but my experience with other MS technologies (such as Windows Media Server -- [where WMS created "their own version" of RTSP](http://bisqwit.iki.fi/source/ms-rtsp-dump/) -- or with [MSJVM](http://www.javaworld.com/javaworld/jw-10-1997/jw-10-lawsuit.html) -- where MS created a non-standard virtual machine while claiming it met the standards, and others...) leads me to believe that MS will continue this behavior where they can get away with it. Times before, I didn't notice it until I was well into a project and then gotcha! No turning back now... I'm a relatively new C#.Net programmer and I'm happy with it at work, where it's pretty much a complete MS environment anyways but I'm not willing to expend any energy in that direction in my own projects just to have that kind of stuff happen if I try to cross technologies. 
People who hate web forms usually forget that this technology was introduced to help desktop developers to use their existing skills (read - drag&amp;drop components) to make web applications. And they did that quite nice although it's not the best approach for web applications if you want them to be done right.
This probably depends a lot on where you live.
IMO all the frameworks sitting on the ASP.NET pipeline can handle those tasks well it all comes down to how you manage and design what you build. The only big plus for WebForms that I have seen is control availability though that is quickly eroding away with new controls for other frameworks and the explosion of JQuery based controls that are stack independent. 
Agreed, server controls are the strength of the platform; although I highly suggest rolling your own. I've yet to see third party controls that are efficient and worth the cost. Warning: custom server controls are difficult to write correctly (invest some time in documentation), but the results are pretty fantastic when done well. 
MS can create whatever they want in ASP.NET, C# or anything...they're the ones behind the technology anyway.
Im talking about complex enterprise apps. This is where MVC really starts to shine. MVC has some minimal architecture at least. With webforms you have to put together your own MVP or something similar. AJAX in MVC is a thousand times simpler. What struck me most the last time I worked with webforms was the sheer ammount of code I had to write that I wouldn't have to with MVC. If you haven't used MVC your arguing from a position of ignorance.
I suppose bad developers are everywhere but having a drag'n'drop gui just encourages them more. The main thing I disgree with is b though. MVC is much more suitable for complex applications. 
&gt;Clearly I need to rework my license a little. I'd recommend MIT if you're comfortable with something that permissive. 
If you spend your time on the surface level you'll never really get to know the framework. Web forms comes from a time when web development was more difficult. It's modular, nothing is set in stone, and everything can be replaced (if you know how to replace it). It's future proofed; there are no rules, you can adapt it to do nearly anything. It wasn't designed to be convenient and it wasn't based on convention. It was designed to let you customize it any way you need to eek as much performance and functionality as possible for your individual application based on the limited hardware available at the time. On modern hardware you can use it to work miracles. The first iteration of MVC was built on top of webforms. It's not difficult to mix Web Forms and MVC in a single application, it's not an all or nothing decision. As for which is better for complex functionality, it's really based on the functionality. If you're doing something really unusual that requires a ton of server interaction and has tons of permutations, it can be a life saver.
How do you end up with a race condition in single threaded code?
It was a bit of hyperbole, but what happens is that if you write code in the OnInit, OnLoad, OnPreRender callbacks, you have to be acutely aware of when different framework methods get called. Often you'll run into problems where you want to do something OnInit, but things like the value of controls in a form aren't available until OnLoad. You won't even get an error, it will just say that the Text property is null. It's a nightmare, in my experience.
Mvc wasn't around back in the day and now there's a shit ton of terrible code written in web forms out there that has no real structure or consistency to it. You can actually emulate mvc pattern in web forms, but like you said, at this point just use MVC...
This is a really good comment, and I wish we saw more like it in the tech community. Kudos.
&gt; The extension was Microsoft internal only until yesterday This looks like another great tool for the box. No dependency on SOS.dll, open source, personal interest from the developer... Seeing as updates to hard-core debug tools in .Net are hard to come by, this is a very welcome addition. The `wfrom` command looks especially valuable.
Oh and the .net code does encrypt and decrypt just fine, its just nothing else can decrypt it.. I get an encrypted string that looks like this on .net : WotTR7z/Bm2rTJwIxMpEoh00VIBtPXOxEQcdkzvNdcc= But in Perl (on linux) I get : U2FsdGVkX1+17c5WBAzvzc9yOqOuqQuKhbWgELx7OwG2Z7+ucPs5+JtJ2C1AMZMW 
Okay, this makes sense. Going to give a try and see what happens.. Thanks a ton!
Okay, so I added some code based on what I thought you were saying :) http://pastebin.com/xtReK5b8 I'm getting the longer encrypted strings, but they still don't seem right.. Can you look at the code I added and let me know if this is what you meant. Thanks.
I went very quickly through the code (on my phone atm) but there is a few things I would check: 1. The VB code is using salt and an IV where the perl code is not: if all the parameters (including the key) do not match then you'll get a different result 2. I'm not familiar with perl, but you should check how it converts the string to bytes to perform the encryption and how does it convert the encrypted bytes to string back. You can see that the VB code is assuming that the string is in UTF8 but perl might assume it is ASCII. The output in both cases looks base64 encoded but I would check that too.
There are high performance charting components you could get, but they'll cost you. SciChart can do millions of points with no prob, but it's pricey.
With .NET 45, there is no psscor capability with .NET 4.5 dumps. This looks promising
Try $(".txtBox_ClassName").keypress(function(event) {event.preventDefault();});
Haven't tried this with a datepicker, but occasionally I've used onFocus(this.blur();) to get around read only and disabled styles and quirks
I used a selector for the class to set readonly to true onfocus and false onblur. Worked like a charm.
Try a[ Douglas-Peuker algorithm](http://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm). Used frequently in mapping/GIS to reduce the number of points to represent a line. Here's a [JavaScript implementation](https://gist.github.com/adammiller/826148). Never tried that one so no guarantees but will hopefully point you in the right direction. Good luck!
I'm curious if you were able to get this working.
Let me know if you have any more problems.
&gt;Our code is comprised of 1400+ projects divided between C# and Windows C++ code How the hell do you get to 1400 projects? How many projects have less than 10 classes?
So they've finally realized how bad github for windows is amd they're hoping someone else can make a better one?
There were multiple ways to approach it, .NET specific approaches would be possible.
Did you actually open the service at some point?
no, I created the service as a project in the solution. When I run the application from the IDE, the DataService project is in the same solution. Does the WCF DataService need to be added to IIS as a web application?
Thank you for your help I have been struggling with this architecture! It seems like my application will be a Windows Form front end, using a SQL Server database, and the data will be provided by WCF running (as an ASP website) on IIS. Is this considered a good practice? (I am using this tutorial to get the WCF service setup: http://www.youtube.com/watch?v=mX8quq7MoeI ) 
I have successfully created the IIS service at http://localhost/WCFServiceSite/DataService.svc It runs in the browser. However, I am still getting the same message of "There was no endpoint listening at http://localhost/WCFServiceSite/DataService.svc that could accept the message" 
&gt; http://localhost/WCFServiceSite/DataService.svc So when you navigate to it in the browser it works? You should get a page saying the serviec has been created. 
yes, I do **You have created a service. To test this service, you will need to create a client and use it to call the service. You can do this using the svcutil.exe tool from the command line with the following syntax: svcutil.exe http://localhost/WCFServiceSite/DataService.svc?wsdl** I also changed my windows form web.config to show &lt;client&gt; &lt;endpoint address="http://localhost/WCFServiceSite/DataService.svc" binding="basicHttpBinding" bindingConfiguration="BasicHttpBinding_IService1" contract="ServiceReference1.IService1" name="BasicHttpBinding_IService1" /&gt; &lt;/client&gt;
&gt; Is this considered a good practice? It's pretty standard for a desktop app these days. I don't think winforms is a good pick for new projects anymore, but it's not like MS has given us a clear picture of what the future holds either way. And if your learning so much other stuff you probably don't want to throw WPF into the mix as well (It's got s steep learning curve). 
the exception is showing System.Net.WebException: The remote server returned an error (404) Not Found.
I have done a few Winforms apps but this is my first tiered one.
&gt; I also changed my windows form web.config to show web.config or app.config? Also, if your running from the exe you also need your config to be named {APPNAME}.exe.config. Also, does your app have a reference to whatever asembly IService1 is in? 
I've found [ServiceStack](http://www.servicestack.net/) to be way less of a pain in the ass to use than WCF. You might want to check that out before you get too deeply invested in WCF.
app has a reference to IService1 assembly appname.exe.config has this endpoint listed: &lt;client&gt; &lt;endpoint address="http://localhost/WCFServiceSite/DataService.svc" binding="basicHttpBinding" bindingConfiguration="BasicHttpBinding_IService1" contract="ServiceReference1.IService1" name="BasicHttpBinding_IService1" /&gt; &lt;/client&gt; 
I'm out of ideas then, I'm not that familiar with WCF. Hopefully google or someone else can point you in the right direction.
You should be able to run your WCF service project directly (some process started by VS should self-host it, thus making it listen as configured in the appconfig) and test it with the test client: http://msdn.microsoft.com/en-us/library/bb552364(v=vs.110).aspx I do hope this helps, WCF can drive a normal person crazy in a few days. Edit: oh, this assumes that you have a "client" and a "service" project - and that's the case, correct? 
It depends on your hosting environment. http://msdn.microsoft.com/en-us/library/cc668805(v=vs.110).aspx
I'm doing the Microsoft NTier Walkthrough, with a DataAccessTier, a DataEntityTier, a WCF DataService and a PresentationTier. This is the link - http://msdn.microsoft.com/en-us/library/vstudio/bb384570.aspx Unfortunately they don't describe deployment considerations.
A c# string is basically an immutable array of characters. So, generalize what you know of c# string handling to different types of collections containing different types of values, and you can see the advantages. The .net immutable collections also mirror strings in that there are also mutable "builder" classes, for when they make sense. 
&gt; Is this considered a good practice? It depends on your needs. Most things are going to way of the web unless you really need a thick client. If you don't have legacy concerns, then you can go straight to HTML 5 &amp; CSS 3. That will allow you to create a decent web experience depending on the application you are trying to build. WCF configuration is a major pain in the ass, they really dropped the ball. One typo/error and it might take a day to diagnose. The plumbing is great, but the configuration has a high learning curve and fails at some of the most trivial things (having http and https endpoints configured with a server that won't allow http - this actually breaks the application; thus your installer/configuration utility needs to have in the knowledge to set it up properly). If you are just learning, then it is good to have it in your bag/portfolio. However, thick client architectures have to deal with versioning (old clients talking to newer services). This can lead to headaches (APIs are hard to write correctly the first time). I liked what they did with WPF, but it is resource heavy (too heavy...). MVVM is one of my favorite presentation patterns; I use it on the web now with knockout.js. 
I gave it a quick read. So you added a service reference as described here: &gt; To add a service reference to the presentation tier &gt; In Solution Explorer, right-click PresentationTier and click Add Service Reference. &gt; In the Add Service Reference dialog box, click Discover. &gt; Select Service1 and click OK. And it all works when you debug from VS, right? Things stop working when you run the standalone client exe (outside of VS that is). Which OS, framework version are you using? You wrote: &gt; How can I deploy the service on the machine so the exe can find it? Usually these tiered applications live across two machines, the client, and an application server, which will expose the data methods through the web service. (for the sake of discussion, there might be a third machine, the database server, to which the app server will connect to), So I guess that's what you'll have when you end your testing. client &lt;----&gt; web service &lt;----&gt; database Right now I suppose the client, the web service and the SQL server instance are all run on your machine. Just like you would on a different machine, you must expose the web service through IIS (or a self-host in a custom app, but that's a more advanced scenario and out of the scope for the moment). This is done because the web service is "another application" and must be running somewhere to accept requests. http://msdn.microsoft.com/en-us/library/aa751792(v=vs.110).aspx IIS 7.5 Express: http://www.microsoft.com/en-us/download/details.aspx?id=1038 I don't know if you're familiar with IIS configuration (I'm really not that familiar either), or if any of this sounds obvious to you. Still let me know if this helps and if I can help you further I'll try my best.
I'm using Win Server 2008 with .NET 4.5 (?). The client is on one machine, the SQL Server instance is on another. The web service is not required to be a web service, I just wanted to go nTier in order to have more flexibility later. So far it sounds like the nTier route is adding unnecessary complications at the expense of getting the project completed.
&gt; So far it sounds like the nTier route is adding unnecessary complications at the expense of getting the project completed. I understand the feeling...! Are you able to setup the WCF web service in the SQL Server instance server's IIS? After that is done, you will have to reconfigure the client's appconfig to point to the correct url and make sure all non-80 and non-443 ports you're using are opened (intranet firewalls in the like). Also make sure to configure the web service to point to the correct sqlserver instance. 
I prefer native apps to web apps as a user and as a developer. HTML was never meant for building apps. It's for making documents. CSS and JS are just ways to **shoe-horn** HTML into a new role and they don't even work that well. Just give me a god damned byte-code browser already and let me build native user interfaces.
Shhh... there there...  (-) 
Aww... who am I kidding? ()8
Why not just buy 2012? To answer your question though, I see 2 options: * Buy an MSDN Subscription * Go through a reseller, like [this](http://www.softchoice.com/searchresults?Ntt=visual%20studio%202010&amp;dim=1&amp;scc=visual%20studio%202010)
I need it for professional use. I created [this](http://www.reddit.com/r/dotnet/comments/1p1jmy/best_way_to_buy_visual_studio_2010_professional/) thread some days ago to explain it. I'd like just to buy this fothermucking key and close this sad chapter in my life. 
When I signed up a few years back there were some restrictions on its use. For example I don't think it was appropriate to use for general consulting but they wanted it used instead for development of new products and services. That may have changed but be sure to read the fine print... If you are into that sort of thing.
Personally I would just buy the newest if I was going to be spending some cash. If not then I would look at amazon.
"What's that your trying to rebase with no conflicts? Better use the command line"
So, I completed the asp.net (the site's) MVC 4 tutorial on making a movie listing site, which at the end suggested I complete this Music Store tutorial, which definitely feels like the next step, but it's MVC 3, and I really feel like there should be an MVC 4 version out there... Do you know of one? Or any suggestions where to go from here?
Do biz spark. Or, buy 2012. The license and key give you *concurrent downgrade rights* to install BOTH 2010 and 2012 (or just one if that's what you're in to). It's in the license. This is one of the most sensible licensing things MS has ever done. 
There aren't many differences between 4 and 3. 4 builds on top of v3, with many features you most use would already be in v3
alright, cool. Thanks for the reply. :)
Working with a team requires SVN discipline. Combine good SVN habits this with a continuos integration / build server (Cruise Control/Jenkins). You always know your project compiles so you don't forget to check in things needed for building. Add in the ability to RDP into the remote computer from home in case you forget and that should be all you need. This is the process many teams use very successfully every day. All I have to add is check in early and often. Last thing you should do is check in before you leave and update before you start. Or... get a laptop you can take home. I use the above process, but I take my laptop home at night.
It's a good idea to have at least a model layer between your entities and the UX. Executing all your EF ops through a repository class. Look at Automapper for your translation.
I do this as well. Your business logic should never be working directly with EF objects. Have a repository class gives you the ability to change the schema (it will happen) without changing the business objects (a far better argument than "I might change my database provider"). I also makes your business logic testable because it won't depend on a database connection for the working objects. 
I think it very much depends. If I have a WCF layer I tend to translate the entities to DTOs. Not so much because they are so different (in my experience they rarely are), but because I don't want to have to update my service just because I added a column in the database and updated my entity. If I'm not using WCF, I might let my entities (pocos) flow all the way up to the UI. I do tend to partial class the entities from time to time to add some sort of trivial logic to them. However I am not as DDD oriented as some so the bulk of my logic still lives in my business manager classes.
Unfortunately all the time I'm allegedly going to save with MVC 5 is going to be lost trying to get it to work with VS 2012 because I don't have $1000 to drop on an upgrade.
I suppose that it depends on the project and personal preferation, but this is what I usually prefer: - Have a "Contracts" layer with models - Have a mapping layer in between other layers, if they need their own models. For example, if there is a DAL, a presentation layer, and a business layer I'd have these sections: - Contracts Layer - Data Access Layer - Presentation Layer - Business Layer - Data Access Mapping Layer - Presentation Mapping Layer The presentation layer usually has it's own view models, and the data access layer usually has its own entities. Note that its not necessary to instantly make a bunch of layers of the sake of having layers. In the projects I worked on so far, layers like the business layer or processing layers don't need their own models and can do their thing directly with the models. With this layout the separate layers don't know anything about the models, and the models don't know anything about where they are being used. If you project gets really big, you have clear separation of concern, and its not difficult to move the parts over to different servers, instead of being project references. If your project it not that big, usually one mapping for everything is enough. But if it is big, I like having multiple mapping layers too, so the mappings can be done on different services busses.
Executing all your data access operations through a repository is indeed a good idea. It may seem silly some times to make methods like *GetAllCustomers()* in a repository class, when you can just call *Context.Customers.ToList()* directly, but in the end it makes everything so much easier. You can make an interface for your repository, and you'll make testing a lot easier, since you can replace the database calls with mocks. When you have a test case that requires a specific set of data for example. Note that apparently this has been improved in EF6 ([link1](http://msdn.microsoft.com/en-US/data/dn314429), [link2](http://msdn.microsoft.com/en-US/data/dn314431)) but I havent worked with that yet.. Even if the context itself is mockable, having a interfaced repository still seems like nice thing to have.
Building a repository on a repository puts you firmly in Architecture Astronaut territory. With extension methods on IQueryable&lt;?&gt; you can get all the functionality with none of the drawbacks.
Only the business logic has the necessary context of what data it needs. The biggest performance problems I've seen are from abstracting data access to far from business logic.
VS2012 MVC5 support is coming next week. Edit: 2012.4 and 2013 will be release on the 13th after the official launch.
I don't mean for to list. To mock that you have to override the IQuerable provider. Though if you want to keep things simple an AsList extensions method is just as easy. But for other things the would go in the repository. Instead of having a GetProductWithTags method for instance you would have an IncludeTags extensions method. It's much more composable.
If you are just going to map your entities into models, why bother using an ORM at all? Why not save the CPU cycles and just map raw data tables?
This is a topic that bothers me a lot. The whole point of using an ORM is to avoid having to map raw data to models. My theory is that if I find myself doing it anyways then either the ORM is broken or I am using it wrong. I think a big part of the problem is using ORM Entities that are one-to-one maps with database tables. Entities should have the shape your UI wants to use, not just the result of blindly calling SELECT * across the relevant tables. I'm thinking that EF Code First + Stored Procedures/TVFs will give me the right results, but I haven't tested it yet. 
That last point doesn't make any sense. You don't need a database connection to create and populate an entity.
I hear rumors that the VS 2012 --&gt; 2013 upgrade will be cheap, like maybe a hundred bucks.
It depends on the needs of the business layer. If the dto object is some kind of adapter or facade then it may have properties that are derived from the db column but may not need to be exposed explicitly in the dto.
Example please
So say you have a car object in the database. The table has columns for engine type and fuel type and a bunch of other stuff. You also have a WCF endpoint that exposes a method called GetSimpleCarInfo(int carId). We don't want to return the entire car's info but rather some derived properties. In this case we want to return a property called IsElectric. The business layer derives whether or not the car is electric by evaluating the model and the engine type against some stored business logic. The business layer populates an intermediate object with the newly derived IsElectric property and that gets assembled at the service layer into a facade object dto. That dto gets sent to the consumer via the wcf service call.
$99 until Jan 31, for upgrade from 2012 pro -&gt; 2013 pro. $299 after that date (for an upgrade). Source: http://visualstudiomagazine.com/articles/2013/09/30/microsoft-announces-vs-2013-pricing.aspx?m=1
 You are a bucket of awesome 
I think I might disagree with you on this. But it really just depends on the situation. With EF, it all but eliminates the need for stored procedures. On top of that, I'm a stickler for separation of concerns. I don't like having business logic inside my data access layer. I'm also not a fan of serializing my EF objects. True you can add [DataMember(Name = "SomeProp", IsRequired = true)] or add a do-not-serialize attribute but you are still marrying your business objects to your DTOs. It doesn't promote flexibility.
&gt; Business logic in the database is mostly the thing of the past for the majority of modern applications Where in the hell did you get this idea from? As a side note, this entire thread is reminding me as to why I'm not an ORM fan for most cases. 
It's been mentioned already and a little bit of research into system design will yield approaches that cover this subject. Basically it's a separation of concern. The stored data may not reflect directly what's mapped on the UX. Testability and orthogonality are other concepts to consider. When thinking of the Model in Model-View patterns, this is always in terms of the UX needs. In cases of ViewModels, especially like Knockout or other javascript frameworks, this contains commands and other client specific handlers that have no business in an Entity and are in fact rendered/written in the client code. Or if working with a rudimentary WPF observer pattern by using INotifyPropertyChanged, you don't want to clutter your Entity with those operations. If say a DTO pattern was adopted in addition to a Model on top of your Entities, you now have the ability to decorate your DTO with validation attributes that are irrespective of the entities (keeping business logic out of the DB if that's desired) and leaving your Model free to only be concerned with presentation needs. Also clock cycles aren't typically a concern unless working with large datasets (millions of rows) or very limited resources (which might direct you out of managed code and into C/C++ anyway).
I guess everyone's experience is different...I used to see more app logic in the early 2000s, and see more stored procedure logic these days - granted, I'm in the client-server world more than most - and the attempts I've seen at ORM work tend to be quite chatty and tax the database a lot more than needed. I'm not completely anti-ORM, but to throw out stored procs completely seems like throwing the baby out with the bathwater to me.
I am certain it depends on the space you're in. I've been exclusively in the financial services sector building line of business applications. Whether it's Java or .NET nearly everything is DB--&gt;ORM--&gt;BL--&gt;Services--&gt;UI.
Storage logic still belongs in the database. 
&gt; In cases of ViewModels, especially like Knockout or other javascript frameworks, this contains commands and other client specific handlers that have no business in an Entity and are in fact rendered/written in the client code. Since the ViewModel is supposed to only model the view, e.g. tab states, which popups are open, etc., I don't see what that has to do with anything. &gt; Or if working with a rudimentary WPF observer pattern by using INotifyPropertyChanged, you don't want to clutter your Entity with those operations. Not an argument, you are just restating the premise. &gt; If say a DTO pattern was adopted in addition to a Model on top of your Entities, you now have the ability to decorate your DTO with validation attributes that are irrespective of the entities (keeping business logic out of the DB if that's desired) and leaving your Model free to only be concerned with presentation needs. Generally speaking the validation rules are going to same from the database up through the UI. Defense in depth and all that. By decorating the combined entity/model I don't have repeat the validation code in multiple places on the client, which would of course violate DRY and add more code to test. 
And I'm not a fan of having storage logic in the service layer. Thankfully EF+Procs seems to satisfy both of us.
What I've been seeing lately is ORMs that actually work with stored procs instead of pretending they don't exist. Or rather, companies I've talked to claim that's what they are doing with EF. I personally haven't tested the idea yet.
First of all, those are just "models". The MVC pattern is not "ViewModel - View - Controller". Secondly, is there any technical reason why you can't have the same validation attributes on a code-first EF entity?
Here is a preview of my report on the topic: http://www.infoq.com/articles/Microsoft-Stack-2013 If there is anything that you feel is missing let me know and I'll do a follow-up piece.
That's more of an ORM thing than an MVC thing, since the implementation of your model isn't an MVC specific thing. I'm assuming then that you're using Entity Framework as your backing ORM. I'm on my phone now, but if I recall correctly, using sprocs (stored procedures if you prefer) is totally doable. You're just likely searching for the wrong thing. Look for sprocs as they relate to Entity Framework instead of mvc.
One could think of it as MVC-VM. The Model from your code behind could be your entities. You're right. There's nothing technically prohibiting you from doing this. It's a matter of patterns and best practices. The VM comes from the client side javascript object.
This looks interesting... Code First Insert/Update/Delete Stored Procedures http://msdn.microsoft.com/en-us/data/dn468673.aspx EDIT: Better link http://msdn.microsoft.com/en-us/data/ee712907
How is your workplace dealing with persistence now? Active Record pattern? That is to say, do your domain objects encapsulate their own database access logic? 
Fuck you bot.
Eventually when your more confident you'll probably stop using the scaffolding altogether! (It tends to serve only very basic CRUD applications which are rare in the real world). If you delete the actions from the controller then you have to remember to remove the appropiate links from the generated views as well, which could be error prone if you're not sure exactly what to delete. If time isn't critical, have a go at doing it from scratch. It's really fulfilling and you'll gain an understanding of how everything works in no time.
Yup, that's exactly what I was talking about.
Ah yes, the Active Record pattern. Awesome for knocking out admin tools really, really fast. Horrible for complex applications.
Thank you. I agree that scaffolding is more gimmick than anything. But it's helping me understand MVC and how to start a project. I haven't done enough repetitions to get a real feel. Honestly, EF and MVC do seem like more overhead than I need. I was trying to use NancyFX and Dapper, but I just couldn't get my head around them, as there aren't a lot of tutorials or anything geared for a beginner. But for now I'm at least able to move forward. 
If you decide you don't like MVC, you may want to consider sticking with Web Forms. Microsoft still recommends it for smaller websites, especially ones that do a lot of reporting or simple data entry.
With most MS stuff, this is the case and I share your pain. When it comes to the Azure/ASP.NET team, they have more class than this. I just tweeted @shanselman with the question and he responded immediately.
He's asking where this code lives. If it live inside your data classes or perhaps some common base class, that would be the active record pattern.
In that case, you'd have to either: * iterate the data table and populate a model class, or * define the View as having a DataTable model
You're asking how you can call the stored procedure from the view? The answer is, you shouldn't. The view shouldn't have to call the stored procedure; the controller should take care of getting the data and passing it into the view. Did I misunderstand your question?
Oh, it's the worst. Completely untestable. 
In what object is this happening? 
Bull. While I don't enjoy, I most certainly can test active records. Kids these days, always afraid of the database like it has the aids or something.
Thank you. 
If an object is coupled with its persistance logic, there is no way for me to test the object completely without touching the database. That means my test will have to test at least two things. To me, that's untestable; if my unit test fails that means I should only have to look to the method it is testing to figure out what went wrong.
Well I'll admit that unless I'm writing some throw away code I'll probably be using MVC in the future.
You should look into using something like jTable and making a read-only API used by a single page. You can expose only the functions you need in one controller. If you want to export, it would be really easy to re-use the same API to build a CSV with jQuery http://jtable.org/GettingStarted
I think the second one is the answer that will keep him happy. Luckily this is only for legacy projects, the new projects I think hes going to let me run wild with EF.
Brilliant! I totally forgot about Web API. Was looking at creating a SPA version as well so this seems like a great opportunity to try it. Thank you!
Yes. This is the easiest way.
That's good (switching to something other than stored procedures for new apps). Here's something I've seen in real-life apps, multiple times. 1. App A is built with stored procedures (SPROCs) 2. App B is built, using those SPROCs for some nightly report/data transfer/etc. 3. App A has a change of requirements. Then, one of two things happens: The developer changes the SPROCs, not aware that it causes a problem for App B. App B keeps running, but getting incorrect values (for its purposes), and no one figures it out until months later. or The developer knows that App B uses the SPROCs, so makes duplicates of them, with the slight changes - filling the database with extra SPROCs, and no one knows which ones are used by which app. That's why I prefer to keep the query code inside the apps.
Thanks for the reddit gold haha! If you need help with jtable let me know
Agree here. I've just landed a desktop/vsto project after 3 years on a big asp.net (mvc+wcf) project and I nearly cried with joy. Can actually be productive again. I've been building stuff on the web for 17 years and it's just utterly frustrating and always has been. The project I'm working on is a massive financial platform that was supposed to be web this and that but to be honest they've had to give up now and start shoving desktop components out via ClickOnce and Office VSTO. It's just a rats nest from hell on the web side of things - millions of lines fo crap doing a job worse than Win32 would have done.
+1 to this. We used it for 3 days in production and it failed miserably. It couldn't GC fast enough for us as it was a managed process. Fortunately with some due diligence, we built a cache abstraction layer and substituted it with memcache spun up on some Ubuntu VMs and a light weight wrapper around Enyim memcache client. 6 memcache instances with 4Gb each stop us from having to make 500 million database hits a month (!!!) which means massive latency reduction and cost reduction on our SQL server cluster. These cache machines have been online for just over a year without a reboot or memcached restart.
Same experience here. We can build on up to 2013 apart from one VSTO project. We're still on 2010 though as R# and VSVN licenses we have are tied to it. We have partner agreement which means we can use 2013 without any cost but cant use VSVN or R#. I have suggested canning all this crap and our JIRA and migrating to TFS/git and telling R# to go away (it's unreliable) so we can manage simple upgrades.
Architecting isn't really about code or whether your code is right. 
Good for you for wanting to move your way up the ladder! Here is some serious advice. Think about this post from the reader's perspective. A letter with your post typed on a piece of paper with a return address just came through your door. You open it and read it. Which of the following do you do next? A. Immediately sit down at your computer and pen a reply B. Put it straight in the recycling C. Think "what a great letter - I should write one like that myself!" D. Put it straight in the bin Now, re-draft the post, aiming for A. and repeat. This kind of iterative approach is necessary before anyone will take you seriously as an architect. BTW, Here's what I immediately started typing in response before the red mist cleared: ------------ Wow dude, you'd be perfect as an architect! You are not exactly the completer-finisher type, are you? 1. The title! The grammar is incorrect, the spelling is incorrect and you used ellipsis. Don't give me "English is not my first language." The rest of your post indicates you could get it right if you only tried. 2. You didn't close your double quote. I REFUSE to believe that you are an experienced developer, and I dread to think what code review of that kind of sloppiness would be like. 3. You asked for something for free while offering nothing in return. Why would anyone agree to those terms? 
I helped move a friend to a new place yesterday for 10 hours. My back has been hurting all day. Most fat-ass software developers could not do what I did yesterday. So please excuse the fucking lack of quote. "You asked for something for free while offering nothing in return. Why would anyone agree to those terms?" How dense are you? This is the internet. If someone wants to help then they will or they won't. My opportunity cost is quite low to simply ask for help online. Go fuck yourself.
First rule of being an architect - everything is an opportunity. You had the opportunity to respond kindly and professionally, you took the other route.
Give it a try first. If you want to do software architecture start building from the ground up. Build an N-tiered Framework as /u/NecroSyphilis suggested; data, repository, service, and presentation layer(s). Use your framework to build whatever random application you can think of. Come up with strategies for publishing, deployment, installing...etc. App version control strategies, Service packing, unit testing, regression testing. How flexible and/or strict is your base architecture? Are you dependent on 3rd party assemblies? Are you following standard patterns and practices? Is the framework loosely coupled/tightly coupled/modular? Does the application need to be distributed? What core technologies are you targeting?(.net 3.5/4.0/4.5, EF?) What technologies do you need to integrate with? (MS office, Sharepoint, mobile Apps / non-.net; java beans, php, unix/mainframe systems) What level developer are you targeting to use your framework and maintain/develop your application? (junior/intermediate/senior) How much of the application can be maintained by a business owner and how much needs to be done by a developer? These are just a few(but not all) of the considerations I take into account when designing a new framework/architecture. If your a good solid coder then good for you, but that doesn't necessarily translate over to architectural ability, it takes years of experience and you can't just take a class or study for it.
Communication is one of the most important skills of being an architect, not coding or having books on your bookshelf. Making excuses for yourself will get you no where, and if you have that kind of an attitude when not on reddit, consider another job.
Good points there across the board. I'll back up your assertion that the Microsoft Patterns and Practices team are beyond useless. Not only that, design patterns as in GoF and all that are pretty much abstracted away to insignificance. I'd actually argue that there are no decent resources that cover practical solution architecture. It's something which is learned over time and assimilated from others, almost like an old fashioned apprenticeship. I'd spend some time cleaning up code inherited from less than great staff for that is the best way to learn how things should be fixed.
Wow what an awful person you are. Down votes for you!
I think the OP specifically said his R&amp;D manager does not want to use EF or Linq.
I am not experienced with MVC and I use EF and LINQ so I may not be the best person to speak up. Take this answer with a grain of salt. Here is how I would handle this situation: 1. Create a MyViewModel class that inherits from a Data Table. 2. In the constructor accept the params you need for the sproc. 3. Implement the sequence of events you talk about above. The difference being that instead of filling in *some* data table object you fill in the class itself (which is a datatable). 4. Have your controller implement your class as the model. The code for initalizing the MyViewModel class would look like this: using (SqlDataReader sqlDataReader = command.ExecuteReader()) { base.Load(sqlDataReader ); } **note** Please someone correct me if this won't work. As I said, I am used to EF 4 and never used an actual MVC implementation I mostly just write my own implementation of MVC since my company has some fucked up hack together partial implemented version of MVC.
Haha sounds like we work in the same environment! 
I wonder why they would recommend it for that sort of thing. It doesn't really make sense to me. I would only use web forms if I had a lot of controls already written that did exactly what I needed to do. If you become proficient in writing your own helpers and tying them together with your own custom or 3rd party JavaScript widgets, then it's actually easier to work with than Web Forms in my opinion. Microsoft should definitely focus on making a built in fluent syntax for their helpers to help beginners make an easier switch from web forms to MVC.
Override the events with functions that prevent default behavior. One question, why are you using a datepicker if it is read only? It would be easier to just output the text value or use a simple texbox in read only mode.
Honestly, my choice to use .Net came down to the development tools available to me. Visual Studio is the best IDE I've ever used. I also love SQL Management Studio. Any time I try to get into using a Eclipse, I just hate it so much. XCode is better than Eclipse, but still not as good as Visual studio. However, after learning MVC, I feel comfortable using almost any framework in any language that makes it easy to get the data from the database to the client and vice versa. So, like said, it comes down to the tools and .Net has very good ones. 
Yeah, you've got to do a lot of it client-side. Telerik's Kendo suite is actually very nice. I use it every day.
Ugh, I didn't realize which thread you were replying to. There are two caveats that need to be mentioned. * Don't use Web Forms for websites that target mobile devices * Don't use Web Forms for "mission critical" websites were testability is important The recommendation for considering Web Forms is specifically for "small/medium business applications" with a heavy focus on CRUD-style screens.
Definitely not for sites that target mobile devices. Before I got into MVC, I had a hell of a time getting mobile frameworks to work because of the stupid form around everything and view state divs and sometimes unpredictable markup. I would actually think that MVC would be more suitable for CRUD operation heavy websites than WebForms would be. Either way, I don't see myself choosing to use WebForms again unless required to by the money man.
I want to use a datepicker so that the user can edit a gridview textbox that holds the date. The best, and prettiest, way to do that is with a datepicker textbox. Initializing a textbox as readonly makes it so that even if datepicker or other javascript changes the value, the page will ignore it. My ultimate solution was javascript that activated readonly mode on focus and disabled it on blur.
Fantastic!
Read only can still be edited by javascript. It's essentially disabling manual input. The date picker still works just fine, but anything else that would input text into the box is disabled.
But you said you want them to be able to edit it. Do you or don't you want them to be able to pick a date? If you don't want them to pick a date or type one in at all, then why do you need a date picker?
I don't see what's so hard here... I already have it working. I don't want manual input. They click the textbox, the textbox goes into readonly mode while popping up a datetimepicker interface. They cannot manually input anything, but whatever they select on the datetimepicker interface goes into the textbox. As soon as they click elsewhere or otherwise leave the textbox, it leaves readonly mode so that the information can be sent to a database.
Why bother? You'll be throwing out the entire point of using an ORM in the first place.
It's the way you explained it. You should have said something like this: I want the user to only be able to choose the date by selecting it from the date picker and not be able to enter it manually. I don't see why you don't want them to enter it manually. Most date pickers won't submit an invalid date and you will have users that will want to enter the date manually because it is faster. I'm glad you got it working the way you want though. Sorry I had you explain it so many times.
That seems like an odd site to read on a mobile device.
Does this work on .net micro framework?
I'm a regular user of MSDN and various microsoft web pages, and I'm just pissed about the bad browser support. Something does not work in Chrome, something works barely in Chrome, and something doesn't even work properly in the latest version of IE... So I'm not surprised it does not work on your phone, unfortunately.
That isn't how you use the upvote button.
Damned good question. I use a TPL Task in exactly one spot that I could easily switch to a Thread... I'm going to check tonight!
Thank you!
MSDN also seems to steal my back button in Chrome
Yea, that's pretty typical of Web Forms. I'm willing to bet most of the bulk is from having too much garbage in the view-state. By default it includes far more than you actually need.
How is this different from SignalR?
SignalR is an [awesome] web-based technology that does HTTP client-server push notifications over web sockets. Web sockets are a construct invented for modern web browsers. You'd use SignalR to keep a webite in sync with a server which you use as a data source. SimplSockets does app to app (not web app necessarily) pure socket communication. You'd use SimplSocket to keep two server-side systems connected in realtime. It would be a suitable replacement for applications that use WCF over TCP however need even better performance than WCF offers.
Not that I actually would know a syntax if I stepped on it, but it actually seems cleaner using MVC (razor?) than WebForms. I guess, what I mean, you spend more time looking at code and then using Twitter Bootstrap CSS for UI takes a lot of guessing out so far. The thing I keep forgetting is that you can mix and match within a project. Not that I've built a large project doing this, so I can't speak to how feasible this is to maintain. If anything I see going more the WebAPI route. 
I an inclined to agree with you about OData, but you'll need a separate argument for Web API as it doesn't require the use of IQueriable.
I'm going to throw my hat in for ServiceStack as well. I'm not a giant fan of how much it leans on naming conventions to automagically hook things together because it makes it very non-obvious how or why things work the way the way they do, but it gets the job done with about as little fuss as anything else I've dabbled with. It has shockingly good performance from what I've seen but I've not put it anywhere that's really got the potential to make any weaknesses obvious yet. One of these days I need to dig around the source and see what kind of tricks they're using to make all the mapping stuff go so fast. It's definitely my favorite service framework right now.
Would you be interested in writing an article on ServiceStack for InfoQ? I'm sure our readers would be interested in learning about it.
I don't have a pony in this race, but I would like to point out that attribute based routing seems to remove a lot of the mystery and magic. http://www.infoq.com/news/2013/11/Attribute-Routing
Where are the unit tests? Where are the performance tests?
Explicit attributes do increase readability, if people use them. ServiceStack lets you get away without it if you name your stuff the way it wants. I wouldn't call it a deal breaker but it's my preference for code paths to be made as clear as possible. It sucks jumping into an unfamiliar project and trying to figure out when a certain method runs if you can't find anything hooked to it. I get that it's tradeoff though and what you give up in clarity to the novice, you gain in conciseness for the expert. In my current environment, without that clarity, the many novices will figure out their own way to make things work and those solutions are usually pretty messy. That's the context to my preference.
You guys did a massive interview with Demis (lead maintainer/creator) a while back...
Yea, but wasn't that before Web API 2 came out? I would assume that a lot has changed since then, but this isn't really my area of expertise.
Demis may well be up for doing another one once he's shipped v4 (soon!)
Clearly not there yet!
Winforms over WPF? Unless you never skilled up that seems like a silly thing to even mention. Xaml is still the dominant UI tech for the Microsoft stack. Really the only way to cross Windows 8.x, Windows Phone, Xbox One and the desktop (WPF). Learn Xaml once and it's only slightly different for each platform. Web: lean towards single page apps. Durandal is a great. Back those up with WebApi. Razor for the frontend if spots where seo is key. Entity Framework gets the data access love from Microsoft and seems like a good bet. It's turned out to be a good way to go. Use a micro ORM like Dapper if you dont want to use EF. Xamarin with portable class libraries works great for IOS and Android. C# is still king. Rightfully so. It's a nice language. 
How does ServiceStack compare to the likes of 0MQ and other event messaging stacks?
Over and over I keep hearing from developers that are still using WinForms or even abandoning WPF/Silverlight and going back to it. I haven't done a formal study, but it seems that they are citing the performance advantages and a lack of trust in Microsoft as the top reasons.
What developers are using NHibernate over a micro ORM unless strapped to a table with jumper cables attached to their feet? I'm just reporting what I hear. I'm not taking sides in this argument. 
That was surprisingly useful.
The StackOverflow answer you linked was written before the initial release of Web API (which is now a mature technology). And it's written more like a sales pitch for ServiceStack than an actual comparison. Web API is both open source and it supports OWIN (Open Web Interface for .NET) so you can use it with a wide variety of open source technologies or self-host it yourself. On the other hand, according to [this post](https://plus.google.com/app/basic/stream/z12tfvoackvnx1xzd04cfrirpvybu1nje54) 2 months ago, ServiceStack is transitioning into a commercial model which seems suspect considering it was developed in part using community source code contributions.
That blog post is kind of terrible. No one's API ever requires an IQueryable, so his core argument is sort of a straw man. Seriously, look at his example method signature and tell me what it could possibly be doing. And if your API provides an IQueryable, why does it matter how complex it is to write an IQueryable/IQueryProvider? You aren't writing the provider, typically. That's the point. There are lots of issues with common IQueryable implementations, and MS has done a terrible job developing the concept in the past five years. But all the points this guy raises are whiffs. And those wikipedia links! Does he think we don't know how to get there? :) Also, OData maps to IQueryable only as a convenience in the (old, weird, DB-focused) WCF Data Services. ASP.NET MVC4 has a more powerful library for making OData services, with no need for IQueryable. OData is a protocol, not a library, and implementations can vary. OData certainly has no innate relationship to IQueryable, outside of the basic concept of abstracting common data access concerns (filtering via expression trees, projection, paging, ordering, &amp;c).
Well **** me. If you ever need to learn something fast, ask Reddit. 30 seconds later you'll find the solution on your own. So we needed to declare the "otherwindow" as a "window" for some damn reason. It existing already wasn't enough. Then .show() showed up. Dim otw as Window = New OtherWindowName otw.show() Here's [the link](http://www.daniweb.com/software-development/visual-basic-4-5-6/threads/444732/to-open-a-new-window-in-vb-by-clicking-a-button-replacing-the-previus-windw) to what pointed me in the right direction if you wanted it.
Cool thanks!
read
No Automation tool can simulate an idiot user with a keyboard. Manual testing is very important. Things like asthmatics, usability, workflow cannot be automated. You can never take the human element away, humans break software in the strangest ways... 
Does WPF feel finished to you? It doesn't to me. The way that two-way binding to a property that isn't a string or an int works is terrible. I would rather not be able to do two-way binding to floats and doubles than have it as it is. Worst of all: globalization in XAML is a joke. ps-I'm still kind of hoping that I have made a terrible mistake with those two and there is a really obvious and easy solution. Is there?
I take it this a stream of a VS 2013 launch conference?
yup!
This is great news for all .Net developers! 
About damn time!
Technically, you're right. To be clearer, you shouldn't need Entity objects to test your business logic. If you do, you're creating an external dependency you don't want.
I see ORM as a convenient abstraction for accessing the database. I could just use ADO.NET and write all my SQL commands by hand, or I can use EF and LINQ and avoid the headache of writing all that plumbing myself. 
I wouldn't recommend this. Using EF Objects in your presentation code tightly couples the UI with the data layer. That's going to be a headache to maintain if you ever need to change the schema.
I disagree. EF is just a convenient abstraction for database access. If your business logic is directly accessing EF objects, you are creating a tight coupling between the business logic and the data access layers. By creating a repository class/library, you avoid the headaches of needing to change both when only one needs to change. As long as they continue to use an agreed upon contract (interface), you can modify them both independently without affecting the other. How many times have you updated the schema in a database? How many times has the business object required something that wasn't already in or could not be represented in the schema? Those are real world maintenance issues that happen every day. In addition, keeping them separate promotes code reuse. You can now reuse the repository in multiple projects, update it when necessary, and know that it's going to work for all. No need to update those applications at all since they all operate on the same contract.
I'm curious to hear an example of what you're talking about. A Customer class might use a CustomerRepository class for all its database operations. Adding a single intermediary layer that abstracts the business objects from the data layer doesn't seem like it would incur a significant performance penalty. It's all in memory, so it can't be that slow. Done right, the slowest part should always be the SQL query itself and the network transport layer (if applicable) to get the resulting data back. If the most costly portion is the repository layer, you're doing something horribly wrong.
Depends on how you use EF. If you are doing a straight table==class mapping then yea, that's a disaster waiting to happen. But if your EF objects are shaped correctly for the UI then the fact that EF populates them is just a foot note. Of course this all but requires populating them with stored procs.
I quite like [Atlassian's SourceTree](http://www.sourcetreeapp.com/) as a pretty simple, yet quite fully featured client. OK, I'm not the most sophisticated of Git users, but it enables me to easily follow projects on GitHub and manage my local repos. 
What? The whole point of EF &amp; LINQ is not needing to write any SQL, it's done for you. If you care that much about what SQL is generated by EF, then you probably don't want or need EF getting in the way. Write your procs and write your own ADO based repository to access it. Using EF in that case is overkill.
What you say is true. But which layer would you recommend removing from this above scenario? The auto-mapper and DTO objects? That's fine, for performance, but at the cost of tightly coupled code that prevents reuse. I still insist that translating EF objects to business objects is not the most costly part of that chain.
Don't use the phrase "tightly coupled code" around me. As far as I'm concerned that's a synonym for "code I don't like". As for performance, the extra layers are a secondary concern for me. What I'm more interested in is seeing how much extraneous data is being shuffled around. Chances are the entities and DTOs are bloated with data that the UI doesn't actually need. It does make the code more "reusable" in the same way a screwdriver can be reused as a chisel and an icepick. 
Say you have a product. The product has ExtraDetails, Reviews, Tags and Stock. All used in different parts of the application. The repository functions to do this efficiently would be: * GetProducts * GetWithDetails * GetProductReviews * GetProductByTags * GetProductWithDetailsWithTags * GetProductWithAll * ReserveStock And then you have to either return an object DTO with a lot of missing properties (violating it's contract) or create a different model for every possible use of the product and it's corresponding view model. And then the architecture. Is the ProductRepository allowed to query reviews or does it have to defer to the ProductReviewRepository? Hello n+1 queries. Or does the Business logic have to reference every repository it uses? Note that this is just for querying, there are a lot more considerations when the business logic has to do more interesting things. &gt;Done right, the slowest part should always be the SQL query itself and the network transport layer (if applicable) to get the resulting data back Yes, that doesn't mean that doing 10,000 queries to get a list of objects should be acceptable. And this is where the repository pattern always leads to once it's subjected to real world complexity, bussiness deadlines and human laziness. Doing the correct thing should (generally) be the easiest thing. 
&gt;If your business logic is directly accessing EF objects, you are creating a tight coupling between the business logic and the data access layers. First of all, there is a big difference between EF and EF objects (which are just pocos as far as the app is concerned). And business logic is ALWAYS tightly coupled with the data, it's entire job is to translate, add and subtract data in the appropriate places. A repository, without having a million specialized access functions has no way of knowing what particular piece of data the BL using it needs. This is where an ORM is useful. The BL can declaratively tell the ORM what data it needs to perform it's current operation. &gt;How many times have you updated the schema in a database? How many times has the business object required something that wasn't already in or could not be represented in the schema? Those are real world maintenance issues that happen every day. It really depends on the change. Adding a nullable column for instance has 0 effect except on the parts that need that column. Adding a non nullable column? Everything that access that table at all will need to be updated. The more complex the database changes the more complex the refactoring. Adding a repository layer doesn't save any of that refactoring, it only adds to it. &gt;In addition, keeping them separate promotes code reuse. You can now reuse the repository in multiple projects, update it when necessary, and know that it's going to work for all. No need to update those applications at all since they all operate on the same contract. Unless you add a non nullable column, then every application needs to be updated to pass the correct information to the repository. And having multiple applications accessing the same database doesn't sound like a good idea anyway.
I've gone through that heartache, my advice: use an index service like Lucene (Java, .net) or Windows Index Service to parse the files for you. You should find plenty if code examples too which should make it quick to implement.
It's really a good news 
Is there support for VS2012?
You could make an alias using sql network client utility. That abstracts a little bit of the config away. It's also handy for keeping things consistent between dev/qa/prod
I'm only halfway through the keynote, but I'm really excited about all that I've heard. Scott Hanselman's bit with the iPad and Droid tablet was freakin' hilarious. 2:06:30.
Scott Hanselman is just hilarious in general.
I wish Microsoft would just buy Xamarin. Seriously. I mean I understand that they probably will never buy Xamarin but wouldnt it be great. Now with the fast pace the VS and .net teams are moving.
You assume it's for sale. I think the Xamarin guys learnt a valuable lesson from the Novell purchase. 
I dont assume anything. 
Why? that seems like a very odd practise to do. Figuring this out is fairly key to coming up with a way to fix it. You may be able to cut down your pain a bit by using DNS that is updated anytime the machine is renamed.
You know they offer a free edition. The indie license is also very cheap for what it gives you. 
I believe so as I've had it working before. 
Any reasoning as to why books from this publisher are so expensive, compared to other tech book publishers?
Aside from the suggestions to your application, I would talk to someone "in charge" and get to the root of the issue. To me, the fact they are changing the database names at will is unfathomable. If one of our server/database admins did that, they would be at least one step from being fired. I've only encountered a server name change once and that was planned and our team was notified months in advance of the change. It didn't really affect us because we typically use the IP address of the server in our connection strings and our server IPs don't change.
&gt;A repository, without having a million specialized access functions has no way of knowing what particular piece of data the BL using it needs. So your argument is that because you can't know ahead of time how many methods your repository might need, it's easier (better?) not to use one? &gt;Adding a repository layer doesn't save any of that refactoring, it only adds to it. I disagree. You could decide that a table needs to be normalized to several tables and you could do so without breaking the repository's interface. The only things that need to be refactored are the methods that map the EF objects to the business objects. &gt;Unless you add a non nullable column, then every application needs to be updated to pass the correct information to the repository. Not true as I stated above. You can completely reimplement the repository without changing the interface. Having a single class library that provides a repository to a database is a benefit to multiple applications using it. It means that various classes aren't rewriting the same code. Imagine writing a non-trivial access query. Now that same query is needed elsewhere in a different module. Are you going to copy/paste it? Are you going to know where to look for that copypasta later when you fix a bug or the schema changes? 
But neither the free or indie edition give you the ability to use VS to develop your apps... Maybe that's why I forgot there was a free edition.
He's awesome in every one of his appearances. "Bing this with Google" "with the magic of windows 8 touch.. ..'Hi, I work for microsoft'" (Shows a fingerpainted StarWars Deathstar) His settle hints at his distaste for Win8 and bing is a comedy in itself!
I see what your're saying, but I don't think it's as bad as you imagine it to be. The way I would do it, the repository method would query the database via EF and get all the data I need as efficiently as I would any other method. Only then do I transform the data to the business objects. That doesn't create extra queries. Also, a repository can span all the tables in a database, there is no need to create a repository for each EF object, that would indeed be overkill. &gt;Doing the correct thing should (generally) be the easiest thing. If programming were easy, then all developers would be rock stars. I recently left a job because the code base had all the business logic in the code behind and there was copypasta everwhere. When I suggested we factor out business and data classes into separate layers I was told it was a waste of time and too much work. To them, it was the easier thing to keep the status quo of spaghetti code. Despite the extra work it took to add new features and the hundreds of bugs and features that simply didn't work because of the mess. I am of the opinion that doing things right is, in the long run, less work than doing the easy thing. How much typing is it to move your query from the business object to a repository? Not that much, really. The benefits far outweigh the minor hassle of putting the code in a centralized location. Unit/integration testing. Bugs stay fixed. No copypasta. Important schema rules are always followed. Reduced complexity. And probably my favorite reason is that the programmer creating or using business objects needs no knowledge of the database schema at all. Because of that, they aren't likely to get the wrong data or update something they shouldn't or forget some important rule when updating the data. Good encapsulation keeps bad stuff from happening. &gt;real world complexity, bussiness deadlines and human laziness. These are the best reasons for good encapsulation. Feel free to disagree.
This is great, but my only beef is BizSpark MSDN subscriptions aren't valid for the MSDN offers. It's too bad Startups can't get the same access.
By "tightly coupled code" I mean code that cannot function without an external dependency. If I can't use or test my business objects without needing a database connection or EF model, that is "tightly coupled". &gt;Chances are the entities and DTOs are bloated with data that the UI doesn't actually need. So change or add repository methods that return only the data that you need. 
YES! I was writing it.. and i was thinking.. "that word isn't quite right.. can't put my finger on it though.."
So what you're saying is that we should abandon WCF?
Well, either way, I don't know if he is actually distasteful of Windows 8 or Bing. More self-deprecating humor.
I think both. It's fun to watch and laugh with regardless though!
Yeah, I think one of the ServiceStack guys would be happy to write you an article.
Can anyone sum up what Xamarin is used for?
Do you happen to know where they post their announcements? I would like to add them to my RSS feed so I don't miss it.
Check the Google+ community!
iOS / Android development using C#
That's a separate issue. Both WCF and Web API have multiple options for exposing, not exposing, OData compliant end points.
pretty. Me wants. 
There's not nearly enough info in your post to answer your question. In general, individual fields are never garbage collected, only entire objects. The question of when exactly objects will go out of scope (and thus become eligible for garbage collection) in MVC is quite a bit more complicated than I am willing to try to answer here.
hmm...this any good to you? http://www.dotnet-tricks.com/Tutorial/mvc/4R5c050113-Understanding-Caching-in-Asp.Net-MVC-with-example.html
&gt; So your argument is that because you can't know ahead of time how many methods your repository might need, it's easier (better?) not to use one? No it's because you need so many permutations returning slightly different data that it's not worthwhile. &gt;I disagree. You could decide that a table needs to be normalized to several tables and you could do so without breaking the repository's interface. The only things that need to be refactored are the methods that map the EF objects to the business objects. But now the data your BL is working with is getting increasingly abstracted from the underlying structure which will have performance repercussions. In your example, normalizing a table but keeping the repository the same. Everything using that repository is executing extra joins whether they need the data or not. &gt;Not true as I stated above. You can completely reimplement the repository without changing the interface. But the data for the non-nullable column needs to come from somewhere. If your setting a default in the repository then you can set it in the database or BL just as easy. &gt;Having a single class library that provides a repository to a database is a benefit to multiple applications using it I don't disagree on the class library, I disagree that multiple applications should be using the same database. It should go through some sort of service layer with the BL behind it. 
&gt; I see what your're saying, but I don't think it's as bad as you imagine it to be. The way I would do it, the repository method would query the database via EF and get all the data I need as efficiently as I would any other method. Only then do I transform the data to the business objects. That doesn't create extra queries. Also, a repository can span all the tables in a database, there is no need to create a repository for each EF object, that would indeed be overkill. But then your repository is tightly coupled. Even worse, it is tightly coupled to the layer below it that it should have no knowledge of. &gt;To them, it was the easier thing to keep the status quo of spaghetti code. Despite the extra work it took to add new features and the hundreds of bugs and features that simply didn't work because of the mess. That was the easiest thing (short term) but not the correct thing. I'm saying the architecture should largely get out of your way and let you do work. If your fighting the architecture, or even thinking about it while your working then it isn't a good architecture. &gt;How much typing is it to move your query from the business object to a repository? Not that much, really But then you need data from several tables so you need to create a BO's for that specific case. Then you need data mappers for that specific case. And you've inverted your tight coupling. 