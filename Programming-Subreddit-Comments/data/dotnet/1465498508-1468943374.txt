Wow...that is some awesome info and has given me some new insight into overall approach. Can I ask, do you do this type of work professionally or is it a hobby? Do you have a site/blog? Thanks!
The focus is on MVC questions; there are some questions on azure platform though
I'm sure I'm completely out of touch, but is there any way a regular developer can get their hands on one?
OK. I was getting virtual applications crossed with virtual directories. Big difference appears to be virtual apps have their own unique AppDomain.
It's the developer edition so it's not even at the "early adopter" stage but the "build cool shit for the early adopters" stage. Microsoft need to make it cheap enough that dev houses will buy one, but also make it expensive enough to put Joe Bloggs off, lest he buy one and realise that there's nothing interesting he can do with it just yet.
Why not 70-480 and 70-483? This proves knowledge of C# and HTML/JS/CSS3. Failing that, there's always 70-515 (kidding).
Just a hobby - professionally just asp.net. No blog :) If you ever need any help or someone to throw an idea off just PM me
Nice!
Well that would have been nice to know about when learning how to use Wix...
Was about to say just that. Where is Resharper? This guy does all that and much much more!
Well there's always your brain. It can integrate into every step of your workflow and perform refactoring and optimizations with virtually no additional burden on the IDE. In certain benchmarks its performance in pattern matching outclasses even super computers. There is a known issue where it starts to slow down unless it's periodically restarted, but I just schedule mine during off-hours and it really hasn't been much of an inconvenience.
Please stick to Reddit's spam rules. You'll need to make sure you do to continue posting here.
I think 3 grand for it is a fair price given what the technology is. 
Pretty cool. Just tried it and it worked like a charm.
With the Stylecop analyzer, you just install the nuget package and it will automatically scan your code (similar to vs built-in analyzer). You can edit the rules being used in the same spot as the built-in analyzer.
Stuck on Skype here at work. I used to have an arduino controlled light indicating when I had a new message. But Skype killed that API so that no longer works.
ReSharper does many things but it doesn't replace Analyzers, it completes them
I had tried using $(document).ajaxComplete(function() {, with some success last week, but it still wasn't working on the second modal load. I noticed that the modal content still existed when closed, only hidden, so I set the modal-content to empty on hidden.bs.modal and that seems to have fixed the second modal issue I was having! I then used ajaxComplete again to set all tables up, and again it's working correctly most of the time, but it seems that the table is still occasionally being set as a datatable before the modal is fully loaded and it's not sized correctly. So now I'm trying to use the on shown.bs.modal event instead of ajaxComplete, I even added an alert to the function, and for some reason the event is not firing!
I cannot speak to the VS-style shortcuts as I've run the IDEA-style shortcuts since R# 1.x and have only heard from others that the VS-style shortcuts make less sense and/or involve (annoying) two-hit combos (annoying to me since the IDEA shortcuts do everything fine in one hit.) I imagine that the two-hit combos have at least some patterning. As for the IDEA-style ones, there are some strong patterns (and they translate really nicely between the two IDEs in nearly all cases.) F7 variations focus strongly on finding usages of the thing in question. N variations focus strongly on navigation based on what you type. F6 variations for method changes like moving methods, changing the signature in various ways, etc.. I could probably remember more if I were at my keyboard instead of chipping this comment into a phone but its probably okay since you already made the mistake of learning the VS-style shortcuts. ;) PS - I seem to recall the docs at some point in time covering or at least revealing the patterns. Might be worth a look.
ReSharper's main competitor, CodeRush, has a very nice snippet syntax; ps = property string, pi = property int, ms = method sting, etc. See [the cheat sheet](https://community.devexpress.com/blogs/markmiller/CodeRushShortcutsAndTemplates.pdf) for more details.
Yeah I've been using shortcutfoo.com to try and up my R# short cut game. Mostly it's just frustrated me. There's no pattern! It feels like memorizing random sequences. I had switched to the IDEA style shortcuts for a few years myself. I don't recall why I went back actually. I think I felt as wasn't as productive as I had been before. Anyway, I'll review the differences and consider going back I suppose. I don't relish the thought. 
Thanks for the info. I'm pretty invested in R# at this point though.
I'm not sure what you mean, but Resharper does static code analyzis and all the things mentioned in the article.
About the Generics point, and how the function doesn't extend to ints after you applied it once to string ... I discovered recently that you can overrule this behavior by defining the function to be inline. As in `let inline addFunc` ... It inlines the function where ever it's called and the type is restricted, essentially, to something like "any type that has + defined". However it sacrifices a bit of type safety for this flexibility so it's usually only used for mathematical libraries. (Considering there is no implicit casting between types in F#) 
I intended to introduce Roslyn analyzers in this post, so my post title is incorrect, my bad, but I did mention JetBrains (with Resharper) and DevExpress (with CodeRush) have analyzers.
No. RTFM. 
Really? Is there not even say a 500 page book that could do the same job? 
I think Genini WPF might be a good starting point. It has some built in stuff for just that: https://github.com/tgjones/gemini/blob/master/README.markdown
Conceptually you want a database and schema. Then you want a web service. Also you want a basic website to consume that service. From there you can build other clients and apps to consume that service.
Each visual component represents some class or method implementing an interface. Components know how to implement certain interfaces against themselves in order to interact. All you're really doing is breaking up your code and then allowing it to be plugged into itself. As far as UI goes. You'll need to add, remove, and move components in code so that the user can trigger events, to click and drag, join components with cables, etc. The basic principles are quite straight forward, so building a prototype can be as simple or complex as you like. What may be tricky is validating connections etc in real time so that the user can't connect two things which are incompatible, create feedback loops etc.
Each visual component represents some class or method implementing an interface. Components know how to implement certain interfaces against themselves in order to interact. It can actually be simpler than this if all you're doing is calculations "int Add(int, int)" could be one component, and those two arguments require two inputs on the UI component, and one output. All you're really doing is modularising your code that you'd otherwise write manually and then allowing it to be plugged into itself using the UI. As far as UI goes. You'll need to add, remove, and move components in code so that the user can trigger events, to click and drag, join components with cables, etc. The basic principles are quite straight forward, so building a prototype can be as simple or complex as you like. What may be tricky is validating connections etc in real time so that the user can't connect two things which are incompatible, create feedback loops etc.
You've set yourself one hell of a goal. You've got no professional development experience and you've never used C# in anger. On top of that you've got MVVM which isn't the easiest design paradigm to get your head around. Then you've got XAML to learn. Then you've got Xamarin which uses method names and design patterns from OSX so is different than the above. That's not even counting for the fact that you haven't worked out how to actually architect or distribute the application and it's associated data. Honestly you'll be lucky to actually be at a point where you can actually begin coding by the end of the month.
In the time you wait here for responses you could easily read the 1000 pages.
The literal answer to your question is: Yes, anything is actually possible. You probably want a more specific answer though, so you'll need to be more specific with your question and detail your WPF experience. * Have you worked with WPF before? * Do you know what a DependencyProperty is? * Have you made a XAML skin before? * Are you familiar with data binding? Events? * Are you trying to create a single custom button skin, or a multi-part button? maybe draw a picture * Top, mid, bottom is a very vague description of "values" * What are "his instances"
I don't know what issue you're experiencing specifically, but the VS Git integration has always been very shaky for me. I strongly recommend turning it off and just using Atlassian SourceTree instead.
I'm designing a Restaurant POS and each button represents a table, and the top and bottom "values" represent the waiter name and the time since the table has been occupied. The thing is that I want to add this WPF control to WinForms and I don't know how I might be able to access the top and bottom after I add this control to my project. I played a bit with WPF's settings and controls but didn't get too much. No, i don't know what a DependencyProperty is. I did one, but nothing really fancy. I'm familiar with events, not that much with data binding. http://imgur.com/hW5UKir "his instances" means that when i'm opening the order form with tables i will get the number of tables from an SQL db and i will programatically create a button for each table
Stick a stack panel on there with 3 labels in it. Bind the text of the labels to properties in your view model. 
I'm not sure what you mean by "add this WPF control to WinForms". WPF and Windows Forms are 2 different frameworks for creating user interfaces using .NET. Windows Forms was created in 2002, WPF in 2006 - generally more feature rich and modern. You can't mix and match controls between the two frameworks. You've probably got a lot to learn before you understand much of the following, but this is what you'd end up needing to do at some point, so eventually you'll either learn how to do this stuff, or quit. So assuming you want to create a WPF Button that displays 3 different textbox's you have a two basic choices: * Extend the Button class and add 2 additional DependencyProperties which would be called 'timer' and 'waiter_name' (you can reuse the main button label for the 3rd thing), * Then create a custom skin based off the base button skin to display the addtional properties in a couple of new TextBlocks * That skin would have to bind its additional text elements to the new dependency properties * You would then just use this new button class into your UI, binding the 3 DependencyProperties to whatever code behind or view model you are using OR - slightly easier * Add a custom [ContentControl](https://msdn.microsoft.com/en-us/library/system.windows.controls.contentcontrol) to your button's Content property * Inside the ContentControl you'd need to add 3 textblocks to display your 3 fields * Bind the Text of your textblocks to 3 properties in your code behind or view model In either case you'd need to learn about databinding and working with XAML. The first option is better long term, since you'd probably have a dozen or more tables getting displayed, and going with that method you wouldn't have to keep redeclaring your ContentControl (although I guess you could just extend the button class, set up the ContentControl there, and re-use it instead of creating a new skin). The XAML for your button would end up looking something like this (but with more formatting, and binding to the correct sources): &lt;Button&gt; &lt;Button.Content&gt; &lt;StackPanel Orientation="Vertical"&gt; &lt;TextBlock Text="{Binding timer}"/&gt; &lt;TextBlock Text="{Binding tablenumber}"/&gt; &lt;TextBlock Text="{Binding waiter_name}"/&gt; &lt;/StackPanel&gt; &lt;/Button.Content&gt; &lt;/Button&gt; 
Isn't there a winforms xaml container control?
Technically yes, [details for how to do it here](https://www.simple-talk.com/dotnet/.net-framework/mixing-wpf-and-winforms/). I'm not really sure why you would because - * Why make a new Windows Forms application in 2016? * Why make a WPF control just to 'Host' it in a Forms app? Extra trouble and you'll lose out on many things like data binding * Why not just make the control in Forms if you aren't familiar with WPF? (This is assuming you are more familiar with forms, if not, then why use it at all?)
Is there a reason it has to be VS2013? The integration as of 2015 Update 2 has got a lot better.
Business logic is more transparent with functional languages than imperative ones (that depend on changing a state). I read a nice simile in one book about this (I think it was Real World Functional Programming). Basically, if you want a cup of tea with two sugar cubes in it, the way you do it in imperative programming is : 1. ask for the cup, 2. put the cup in the table. 3. bring the tea to the table. 3. pour the tea into the cup. 4. Get rid of the tea jug (Grabage collection) 5. Get the sugar container into the table, you see where this is going. The table here in the analogy is the "state" you're constantly changing. With functonal programming you take a cup of tea with two sugar cubes in. The details are abstracted away and you focus on what you actually want to get done. It requires a certain mind shift if you're used to imperative programming, as instead of defining variable to hold your state, you define functions, and then you pass these functions to other functions. Your question is not about F# specifically (obvious you can't replace existing code bases with another language), but functional languages in general. Twitter is written in Scala, which is a functional-first language on the JVM. It is definitely a real world thing. Most F# books talk about using the right tool for the right job, and for many jobs F# is just easier to formulate and model the problem with than C#, for example. Or you could check this out: https://fsharpforfunandprofit.com/series/why-use-fsharp.html
Could you hint me to the names of the commercial controls? I can't seem to find them. Thank you very much!
[SyncFusion](https://www.syncfusion.com/products/wpf/diagram) [Telrik](http://www.telerik.com/products/wpf/diagrams.aspx) 
I completely understand your position on this, we still use Winforms for a variety of reasons and sometimes it's still the right tool for the job. However, by mixing and matching the two, you're only making your life more difficult. Stick with one or the other, trying to use both to make use of the parts where both are easier is ultimately going to be more work. 
While it's good practice to dispose them manually and free up connections early it shouldn't 'leak' as once the object falls out of scope it will be marked for eventual collection and disposal.
We moved away from the app.config and use Xaml files. Our software only runs on our own servers so security isn't a problem. Xaml &gt; XML any day of the week :)
Thanks, I see some goodies like the days I used to use lisp for "smart answer/question agents" and Im starting to see where this goes. Guess Ill jus have to learn more and put it to test to see where it could fit for me
Seems great, I will definitely give it a try!
Maybe it's just me, but I try to avoid server side view engines if possible. Makes upgrades, framework switches, separation of concerns so much better.
it really is as simple as putting the models and dbcontext in a separate class library and referencing those libraries from the mvc project. Actually the first thing I do when I start a new mvc template with Individual user accounts is move the dbcontext and the model classes out into its own project. Shouldn' take more than 5 minutes on the template. Edit: as a side note if you are using .net core and ef7 you will also have migration files, If I am not mistaken those will stay in the mvc project, but things have been changing with .net core so fast I could be wrong.
Hah... that "mvcarchitecture" project is *far* from "best practice", particularly regarding the over-abstraction of Entity Framework. I.e., don't try and hide it behind your own "unit of work" and "generic repositories": just reference DbContext directly. It makes it so much simpler and cleaner. Encapsulating business logic behind services is still a pretty good idea, however. Essentially you don't really want any business logic to live in your web controllers. Now as far as Identity is concerned... the current (pre-ASP.NET Core) implementation is not a panacea. It's rather poorly designed and documented. Frankly it's almost easier to write your own. Identity only does a few things, each of which is relatively easy to write yourself: 1) it perists Identity model objects with vanilla Entity Framework 2) implements a password hash function 3) implements some basic "one time password" functionality for password reset, etc. Maybe some other stuff I've never seen used. But to actually answer your question; I think if you just take sample code you find online and literally move the Identity context/model classes its own class library (don't bother trying to split it into multiple class libraries - they're exactly zero value to that), it'll just work.
I don't understand your question but the 2 ways I know of to make a tunnel on windows are: - Use stunnel - Use putty to set up an ssh tunnel
Why not just use bundle.config?
Point taken. I ruled out things like http context but didn't think about logging. The validator is definitely not a good example. Still people seems to get my point. Will take extra care to come up with a better one next time
Could you be more specific? The question doesn't make sense. If you are using a proxy then it should essentially be invisible to the website. There is no reason I can think of, that doesn't mean there isn't one, that you would ever need to directly connect to a proxy server. That kind of defeats the point.
for sockets vs http in general the advantage is usually speed: you dont have transaction overhead of opening and closing connections.
Ah yes, that makes sense. Sounds like in situations where an open API is not needed that Sockets is really a no brainier, especially with the ease of using Signal R
A few specific examples I was thinking of is on the project I'm currently working on. The user can type into a Input with an auto complete, that auto complete queries the server for results relating to the input; I think this is a perfect place for using sockets due to the high amount of queries. The other case is relating to the one above which is when the person selects an item and actually searches it, a request has to be made to get the data for that item. The user may only do one search for an item or they may search for many in a particular session so I think that it would again be best to use sockets in the case they want to search for lots. Do these sounds like valid use cases for sockets?
If your metrics show high use of the search bar, that sounds perfect for websockets to me.
I've never done it. Hope it works well for you.
I agree. This is the first time we get to see how the sausages are made and, surprise, pre-RTM versions demonstrate a lot of churn. None of this is new per se - it just used to happen behind closed doors before Microsoft opened things up.
I've been using 6.0 in Beta for a while. It's been good. The NuGet manager is a nice improvement. I hope the next iteration looks more like Visual Studio with tabs to break out the currently installed packages, updates, and then search to add.
https://channel9.msdn.com/Events/Build/2014/3-644 That is a video from a few years back where they use signalR as the API for a SPA. Really cool stuff.
Telerik, Ninject, you should really rething your tools.
&gt; chilkat Interesting components there and not even expensive. The website does not inspire confidence though. How it compares to others, are the components updated regularely?
We've used many of the tools from /nSoftware since the VB6 days, and make use of most of the DevExpress components. 
Totally agree on the identity stuff. Even the 3rd party auth it offers is less than ideal. WRT architecture: Something I've been thinking about is maintaining a base domain library with all the models, nice and portable. Then, business logic libraries that depend on the model library and describe repository interfaces that can be implemented in a DBContext subclass. That is to say: No inner platforms or redundant UoW patterns, like you say. Then, the repository implementations would either be in some kind of concrete EF6 library used by the web project, or within the web project itself.
This depends on the project, but these I call favorites: * SyncFusion (Big library with lots of components!) * NLog * RefactorThis.GraphDiff (when working with EF) * FluentValidation * AngleSharp * NancyFX * Humanizer * FluentScheduler * Hangfire * TopShelf 
&gt; Even the 3rd party auth it offers is less than ideal. The thing to remember about ASP.NET Identity* is that there are _two_ separate but related projects. [ASP.NET Identity](https://aspnetidentity.codeplex.com/) is a replacement for ASP.NET Membership: it's really just an abstraction over a user database with a password hasher. The authentication middleware components are part of [Katana](http://katanaproject.codeplex.com/) (which is MS' first implementation of OWIN). That project contains the cookies auth middleware which replaces Forms Auth, and the all of the 3rd party OAuth plugins, etc, implemented as OWIN middleware components. The really unfortunate thing about both projects is that work has effectively stopped dead about a year ago; they're both being written from scratch for ASP.NET Core. At least they're open source, so you can fork it yourself to fix the numerous bugs.
I think you can take help from experienced someone. You can also ask to be helped from any writing service. There are many services writes very well resume. But you have to find and select the best witting service to have a great resume. Few days ago I used a writing service to write a paper for me. They wrote a good paper for me. I think you could also have a nice paper from them. Source: [Best writing service reviews by Topwritingreviews ](http://topwritingreviews.com/services )
It helped me to create a new ASP.NET Web Application &gt; MVC. Then "Change Authentication" to "Individual User Accounts" just to see how the template looked. I called it my "IdentityScratch" project so I could write tests for it and mess around with it before I integrated Identity with my legacy project. You can do a lot of copy/paste from the template to get going. You can also have your own `DbContext`(e.g. `ApplicationDbContext` or `StoreEntities` in your example) implement `IdentityDbContext&lt;ApplicationUser&gt;` if you wanted to. It will add the entities necessary to your existing model.
Using a socket can be quite nice for cheap caching, too When workload is quiet, an open socket means you have the option of making a few calls to refresh local caches etc, ready for the next "rush" The usefulness of that depends on your application, obviously, but in some situations it can be very useful "free" performance That said, I find HttpClient to be such a tidy interface that I prefer it wherever network performance isn't a primary concern. It's not quite as fast, but it's nice for maintainability, especially when your application is being supported by more junior devs etc - it just seems easier for them to grasp and debug
So is there any mutation test tools for .NET that are alive? Visual Mutator, NinjaTurtle and Nester seems awfully dead :(
Don't forget Xceed! We've been around for more than 20 years! 
I want to communicate with packets. So It has basically nothing to do with flash.
It is still unclear what you want. It sounds like you are trying to basically hack this game with a man in the middle. Why is this needed? Is this just an experiment for fun? It sounds ridiculous and there is probably a better solution. I'm not trying to be mean I genuinely want to understand so I can help, if possible.
Sorry about that! Added
In the beginning I used TCP to connect to the Game and send packets there, but this Method became unsafe because the developers somehow developed a algorithm to catch ppl who are using this tool. So I try to connect to the game via Browser(as an overlay basicaly) and send packets under this to the Game.
Recently I needed a library for running tasks on regular schedules. I found there were not a lot of choices out there. Quartz.NET is a port of a Java library, but I knew a better solution could be devised. One which supported strongly typed jobs so you didn't need lines like ((MyObjectType)conext["MyProperty"]).ThingINeed Also, I wanted the a library to support jobs on multiple schedules and one schedule running multiple jobs. In addition I made sure to make the library fully compatible with unit testing and dependency injection for consuming code. The result is a great beginning with plenty of room to grow.
Request: millisecond scheduling support. Quartz won't let you go below seconds in cron triggers etc, but this would be killer and make me instantly switch to your solution.
Hangfire FTW
I just had it print to the console as fast as it can on my machine. Here's a piece of the output: 2016-06-14T04:33:22.5962554Z 2016-06-14T04:33:22.6002640Z 2016-06-14T04:33:22.6032665Z 2016-06-14T04:33:22.6052687Z 2016-06-14T04:33:22.6062713Z 2016-06-14T04:33:22.6092778Z 2016-06-14T04:33:22.6142832Z 2016-06-14T04:33:22.6162877Z 2016-06-14T04:33:22.6212977Z So, it'll easily do hundredths of a second. Also, it may have been slowed down by outputting to the console. Also, I have it SpinWait(5) each time through the main thread. So, it may be able to run even faster. When I had it print every second, it was accurate to 2ms.
Thank you!
This is fake. Reddit will obscure your password. For example, here is mine: ******
really? hunter2
More like https://github.com/fluentscheduler/FluentScheduler I think
Yeah it's just a issue with time really. We need this urgently but don't have the resource to do it ourselves. If I can buy something I would much prefer to do it that way.
Serilog with [Seq](https://getseq.net/) It is also possible to use ApplicationInsights.
This is just bad. It talks about using Repository and UnitOfWork if EF, that already implement these two things. And it does it with MVC3 and 4. Also adding 2 lines of region to hide 3 lines of code...
Yes, There are many such firms that not only provide the resume services and but also they help you in writing your thesis and essays. They learn you the way of writing and are always there to help you out. One of them is [Writing Daddy Service](http://writingdaddy.com/) which is best firm that provide essay writing.
In fairness, this was how we did things 5 years ago. Because we didn't know better. Though not everyone has learned from these mistakes yet, and there is _a lot_ of literature like this out on the web perpetuating the patterns to this day.
I don't test other OSes as much as I used to since we stopped supporting XP, but I've always just loaded my own VMs from scratch. I use VMWare and just keep a snapshot of the machine with a clean install (prior to installing any of our software), and just run updates and re-snapshot them a few times a year to keep them current. If you don't already have a couple of licenses from the older versions, it shouldn't be hard to justify the cost to get them. 
It's more lightweight than fluent schedule. For one, there's no reflection. Also, the schedule logic is abstracted to where it is fully customizable. Right now there are only 2 types built, but any type could be written and used now.
If you're getting the connection string from code not from a config file then you can use: AppDomain.CurrentDomain.BaseDirectory to get the directory for your executable and then dynamically form the connection string. The above property will point to the path containing your executable (.exe) Hope this helps and good luck :)
might want to make a note that retrieving all the records then applying AsQueryable kind of defeats the purpose of all of this functionality...
The images are free available for windows.
Hey, congrats on the first open source project, looks nice! Maybe just personal preference, but I would find the README example a bit clearer if it didn't jump straight into IoC (on seeing that I immediately started digging through the code to see if Chroniton has a Unity dependency of some sort...). Working with IoC is cool, but it takes longer to scan through to the meat of the example with it in there. Dropping the `Main()` method boilerplate, and marking the code block as `csharp` to get syntax highlighting would also make it a bit quicker to grok. (Just add `csharp` immediately after the three backticks declaring the code block in Markdown.) Probably sounds like nit picking, but great READMEs are a thing of beauty :-D Good luck with it!
log4net logging to a database. I notice you suggest using serilog in the docs, how much of a drop in replacement to log4net is it? Is there much work to get things logging into Seq? I'm not a developer but rather a devops guy supporting the developers so if I can get this implemented it will make their lives easier.
Thanks! I've not had to manage my own repository in Github before. So, that was some great feedback. Done :)
I dislike including role information in the name. This may come from PTSD as a result of too much Hungarian notation back in the day (*it was never a good idea!*), but practically- you wouldn't name it `SalesOrderModel`, would you? I *also* wouldn't tend to write a distinct DAO per model type. I prefer a larger DAO that exposes model objects as collections, something like `Entities` which might have a property `ICollection&lt;SalesOrder&gt; salesOrders`. &gt; Also, how do you all typically create objects of collections? Generally using generics. I'm not understand the question entirely here. If I have a list of `SalesOrder` objects, I'd just have a `List&lt;SalesOrder&gt;`. You only create your own custom collection types when you have unique business rules for governing the collection's behavior (or, if you've abused generics, and find yourself creating a `List&lt;HashSet&lt;CustomKeyType&lt;int&gt;, CustomDataType&lt;string, string&gt;&gt;&gt;`, in which case it may be worth inheriting from the Generic to make a non-generic type, but you'd still be better served by refactoring.)
For larger than trivial applications I prefer to use claims because they scale better than roles and make authorization via policy easier. This tutorial goes over a setup using mvc and Thinktecture identity server. https://identityserver.github.io/Documentation/docs/overview/mvcGettingStarted.html I believe they have a DB backed provider but the tutorial sets up an in memory store to help you get things working.
You get conflicts in the business layer because it doesn't know if you're referring to the data layer class or the business layer class.
That isn't true. You did something really wrong if you do get that, or you are misunderstanding me. I've done this dozens of times with no issue. Edit: Perhaps you are not qualifying the names properly, many people don't understand namespaces. That is just a warning that it is ambiguous and easy to fix by fully qualifying it. Imho this makes the code easier to read anyway.
Hi, Please take a look at my open source project https://github.com/spetz/Warden - if you clone the repository and open the Web folder you'll find the basic implementation of the authentication using the cookies and ClaimsIdentity e.g. https://github.com/spetz/Warden/blob/master/src/Web/Warden.Web/Controllers/AccountController.cs In the Core project, you will find domain entities such as User and IUserService where you can see the basics of registration and authentication. It's built on top of the ASP.NET 5, .NET Core and MongoDB but should be fairly easy to switch the underlying database if needed. Also, you can see here how it works (register/login) https://panel.getwarden.net or run it on your localhost and play with it. Just ask if you'd have any questions :).
Thank you so much for the response! I am definitely using entity framework and mostly understand how it works with code-first migrations. I will take a look into the EF tutorials today, because my spotty knowledge may be contributing to making this situation difficult. I should've mentioned in my original post that the divisions aren't completely separate from one another. I could potentially run into situations where a user needs access to multiple divisions. To my understanding, adding a division Id to my user table would complicate users with more than one division. Can I use the entity framework to set up a division table that has a one-to-many relationship from the userId as the key?
Thanks for the information! I definitely want to understand claims more so I will absolutely run through this tutorial, it sounds like it has a ton of good information.
Why wouldn't you use `SalesOrderModel`? In MVC apps I'll usually name view models similarly: `SalesOrderDetailsViewModel` and `SalesOrderEditViewModel` I totally agree it can get cumbersome and _very_ long, so I'm curious to hear other approaches.
Wrong sub, buddy. 
4. Reputable companies don't have to submit crappy spammy posts.
In part, because all classes are, by definition, a *model* of something. I want to suggest something to you, though- ViewModels all exist to *adapt* an underlying model object to your view, adding a few extra pieces of information about View's internal state along the way. So why are you creating unique types for these? Why not `ViewModel&lt;SalesOrder, BasicStateBag&gt;`? Creating new types for every situation has benefits- each property and method is enforced via strong typing, but for many situations, that's not entirely necessary and often causes you to write more code (and when underlying models change, often require code rewriting- why should your view model code need to change if the model changes?).
You could possibly use something like https://pdfsharp.codeplex.com/.
Thank you so much. It always helps to be able to read through a working implementation of what you're trying to do! I appreciate all the help so much, and I will definitely ask if I have questions.
Another itextsharp user here. It's old, so there's probably something newer and better out there, but it'll get the job done
The *easiest* way to do it is to generate a print-friendly html file that the user can make a PDF since just about every OS/browser has built in PDF generation. Check out print CSS a bit -- you can get a very good print report done that way. If you have to generate PDFs you'll need to use a library. iTextSharp is the venerable old, free one. PDFsharp is a little newer. I like DocoticPdf for heavier work where I have a budget (http://bitmiracle.com/pdf-library/).
Angular 2 or React seem to be the industry standard. However, they are both built for large enterprise websites. If you are planning something big then either of them will do. My personal beliefe is that Angular 2 will be easier to learn and stick with because React seems to have a release every week. But it really doesn't matter, you will find proponents and opponents for both.
From my perspective your choice of server platform doesn't have much of a bearing on which JS framework you select, as HTTP sits in between regardless (presuming Single Page Application). Purely subjectively speaking, Angular has served me well. It has strong community support and when paired with TypeScript it can feel very familiar to a .NET dev who has used MV* frameworks before.
If your whole app is JSON web APIs it doesn't matter if it plays well with .NET since that's all the backend. I highly recommend creating a basic WebAPI backend in .NET, and then a plain, static html page with some vanilla JS that talks to the web api, and go from there. Knockout is probably the easiest to pickup. React is nice but doesn't have a lot of support from Visual Studio, e.g. syntax highlighting for JSX files. It still works just fine though. I would look at all of them and then pick which works best for you and your application. 
Aurelia. Guy behind it has a lot of .net experience.
I didn't know there was *any* debate about licensing for any frontend frameworks?
I don't think that's a valid criticism considering the huge controversy around Angular 1 -&gt; Angular 2. From that same perspective, React has been far more stable.
How do you remove fields? My view models are subsets of my underlying data models, not supersets.
Good point. Mostly it's something I want to look into in my spare time, not for a work project (we are stuck on WebForms w/Telerik :() but just so I can broaden my skills, since last time I touched web stuff was when MVC 1.0 was out. So far it looks like Angular or React, I've heard good things about Angular (and bad things re: Angular1 -&gt; Angular2) but not much on React, so may have to look at that to compare. Typescript looks interesting, I had briefly looked at Coffeescript during a fling with Ruby on Rails, but didn't care for it. Typescript seems similar but more C-like.
I'm using Aurelia and I love it. My biggest problem with it right now is just that it lacks good documentation, and the tooling currently is a mess. Then again, the tooling for all javascript seems to be a mess to me.
The biggest reason for unique types is to have a strongly typed model for views. I'm starting to utilize CQRS on new projects so I can use task-specific query or command models as view models, but that approach isn't always feasible, especially on existing projects or really limited CRUD type of projects.
Vue.js. Very simple learning curve but a very powerful framework. 
Generics are creating new types. `ViewModel&lt;SalesOrder, BasicStateBag&gt;` is an entirely different type from `ViewModel&lt;SalesOrderDetails, BasicStateBag&gt;`. The weakened typing comes on the `BasicStateBag`- and that really depends on the implementation (and there's a reason I passed a generic type to ViewModel, so that you can replace the generic behaviors with specific ones as needed).
This is something to keep in mind. While I want to learn, I also want to learn something that is widely used just in case. Javascript is Javascript, of course, but I ultimately think that since Angular is popular, it makes sense to learn Angular.
Write your output in HTML then convert it using any of wkhtmltopdf wrappers, like: [NReco.PdfGenerator](http://www.nrecosite.com/pdf_generator_net.aspx) (free) [TuesPechkin](https://github.com/tuespetre/TuesPechkin)
At the cost of security though. Unless you make your code more fragile by adding exactly the same restrictions in validation. 
The experience I've got with OData is wonderful. I've only used it as a filter controller though. Together with [ODataAngularResources](https://github.com/devnixs/ODataAngularResources) you can set up a filter in no time. I think for filtering it's great, but for the other functionalities I cannot say.
I don't quite understand what kind of functioniality such a generic class would provide. Would you mind sharing a basic implementation?
Doesn't really matter. By the time you get your second project under your belt they're just going to change the framework again. Concentrate on learning the things that won't change such as JavaScript itself, HTML, CSS, etc. 
That's awesome. Where do I grab these? Google's failing me
&gt; At the cost of security though. It doesn't impact security *at all*. If we were crossing an application boundary, then yes, I'd want a custom structure that holds just the things that should hop the boundary.
I was thinking of the VMs they provide for IE testing over at https://developer.microsoft.com/en-us/microsoft-edge/tools/vms/linux/ It's been a while since I used one, but IIRC at least they used to be fully functional windows machines!
I didn't mean that I would implement my own version of it. Just that I would implement OData framework (the one you linked to)
You're not really implementing anything then. If you are using Web API Controllers, you can change the base classes to ODataController and it should work exactly the same. You'd need to then start switching over your calls to return IQueryable so you may need to refactor how you are using EF if that's what you are using.
True, all roles are claims, but not all claims are roles. The nice thing about standardizing an app on identity claims is you get the best of both worlds. The roles are just assigned with this claim type https://msdn.microsoft.com/en-us/library/system.security.claims.claimtypes.role(v=vs.110).aspx
You also get a bunch of standardized clients that can consume OData feeds. Excel, lightswitch, linqpad, etc. If you are using entity framework, it is really easy to get started. If you are trying to consume some external data feed and reproduce it as a OData service, you will run into a harder time. There are several ways to restrict capabilities of the service. I would turn on some sort of default page size for example.
I'm a big fan of hybrid apps. Use mvc for url routing, mvc/web api for data, and then Angular/react for UI/UX enhancements/state routing/etc. I do this because it's very beneficial for SEO and that's so important now days that if you neglect it, you might as well not even launch your app into production. To answer your question though. I use Angular 1.5x for production apps. I won't use Angular 2 on anything serious until it has a few releases under its belt (just like .net core).
I use [Breeze](http://breeze.github.io) which is a layer on top of vanilla OData. The client (there are JS and C# versions) make it really easy. Take a look at John Papa's [Building Apps with Angular and Breeze](https://www.pluralsight.com/courses/build-apps-angular-breeze) course. 
I still use knockout js. One day I want to learn angular but probably never will. 
Skipping all the sales marketing hype on the website, what do you like about it over simple other popular frameworks right now?
If you start with Angular 1.5 with upgradability in mind, it's not so bad. You can use classes and all the ES6 features and your transition will be better than before.
Vanilla JS. http://vanilla-js.com/
Why MVC + Angular? Just to serve up the index page?
OData supports actions as well. Here is some documentation http://www.asp.net/web-api/overview/odata-support-in-aspnet-web-api/odata-v4/odata-actions-and-functions.
Where I work we are migrating away from a Knockout/Kendo combo to a React/Redux/D3.js setup. Knockout works fine for a straight forward application but when you have a lot of views and are representing data in multiple ways and adding a lot of customization then you end up with a ton of similar but different viewmodels among other problems. We ended up going with React and Redux because we liked how lightweight it was. It doesn't dictate how things are done, although it has it's recommended patterns. It's also much easier than Knockout and Angular to debug in my opinion since everything has a circular flow and is less convoluted. For strict single page applications, it might be hard to overlook Angular. But for functional and unobtrusive JavaScript I find that React and Redux make a great combo with .NET. And also keep in mind it is possible to use Angular with React, they're complementary to an extent and not as direct of competitors as some may believe.
Thank you so fucking much. I love my MS stack and wouldn't reimplement in anything else because we run on MS and that's how we do. But you absolutely opened my eyes when you said it moves crazy querying into the hands of our users which is just a disaster waiting to happen. I hadn't thought of that feature in that way but it makes total sense. Cool I'll avoid it.
Mind bending
I think the requirements need work. You don't/can't change a user interface from wpf to webapi.
I can't see how having .net in backend makes a difference to frontend. So my reply is in general sense. If it were me, I would make some apps with [vue.js](https://github.com/vuejs/vue). Vue is a view layer library (like react). However It is quite intuitive and easy to learn. And it does not force you to learn typescript, functional programming etc. The most important characteristic of vue I find is that it can be used in both simple and complex apps. while react has a learning curve making it a bad fit for small projects and angular is designed from bottom up to be used in large projects.
You do know you can do REST and CQS together 
That's a bit unnecessarily sarcastic Okay so the other 500 pages aren't a complete waste, but he's got a valid point that a 1,000 page book likely includes at least a decent amount of information that's superfluous to his needs
Nope. But you don't have to make every endpoint OData.
I just tried it. If I have a class name "SalesOrder" in the business project and one named "SalesOrder" in the data project, it forces me to specify a namespace in the business project because it can't tell which one I'm referring to.
Because I want to name them similarly so someone else can come along and figure out what I did. If I have a "SalesOrder" class in the business project, they'll know to use that to grab SO stuff. But a I need another SO object in the data layer to do the actual SQL stuff. It'll only return a table, but the name needs to be at least similar so it can be reused in the future and what not.
There are multiple labels offered on each layer, and each layer clearly defined to prevent ambiguity 
Well, something else, clearly- you are neither operating on a `SalesOrder` nor a `SalesOrderCollection` (in the sense of it being a Collection object). You're performing a query, so you might do something moreâ€¦ ICollection&lt;SalesOrder&gt; salesOrders = Entities.GetAll&lt;SalesOrder&gt;(); Or perhaps you'd use a broker for that specific class of entity: ICollection&lt;SalesOrder&gt; salesOrders = new Broker&lt;SalesOrder&gt;().GetAll();
Also potentially true, but he appears to be asking for suggestions of a more focused, smaller book: a reasonable enough request, as someone's likely to know the answer off-hand if there is one
In reality neither C# nor Java has gone down in numbers that much. Instead the percentage of all other job types has gone up. The top is at around 2012 when most non development related jobs started to go up, especially around the time of the devops movement beginning in earnest. It's one of the reasons you'll actually see Python as having an increase in the percentage of jobs to where, depending on where you live, you will see it mentioned about the same frequency as C#. So the number of DBA, Analyst, etc. positions being advertised increased and thus the percentage for all development jobs went down even if there wasn't an actual decrease in the number of positions open. Also Scala and Clojure haven't really made a dent on the jobs front. Scala has slowly increased to about 0.1% from 0.02% and Clojure has remained around 0.01% of job postings. Where as Java went from about 2.87% to 1.69%. None of the increases for other JVM languages add up to Java's decrease. Nor do I think that Java's decrease is because of a drastic drop in demand. It's just an increased demand in some other set of skills.
Good post, thanks!
That makes sense, I just didn't realise it used EF for identity, I thought that was standalone. I have only recently just started looking into identity management though.
why would you try to announce this when it's clearly not even in a beta phase? 
ELK Stack (Can be hosted on windows or linux) Elastic Search [make your logs searchable] Logstash [Ship your logs to ElasticSearch -- use filebeat to ship the logs from individual hosts to your logstash servers] Kibana [Awesome Dashboard for searching, and visualizing your logs] http://elastic.co/
The dotnet world will soon realize that we shouldn't be moving the backend build process stuff to gulp. Gulp is good for client-side stuff (webpack, etc), but not for actual building of libraries and CI (dotnet, msbuild, etc). Use Cake instead. https://github.com/cake-build/cake
{"Message":"An error has occurred."} Edit : Fixed. For some reason just enabling CORS in WebApiConfig.cs wasn't enough so once I added it inside my controller it stopped giving that error.
Nice approach, just in time to help me implement an event-driven GUI system in Monogame :)
That's pretty close to what I have been thinking. Allow rolled over files to be virtually joined back together and displayed and filtered on as if it was a single file. Not sure how I will present the option yet but I envisage TB making a guess whether there are rolled over files but allowing the user to specify a rollover filename pattern in the event that TB gets it wrong.
Nice, thanks. I didn't even consider joining them. I was envisioning something like this: 1. Select folder 2. Enter regex filters (if any) 3. Tail current matching files 4. Auto-open, focus, and tail new matching files as they are created
But... why?
&gt; NodaTime : A better date and time API for .NET Ummm no. *J*odaTime is a better date and time API for Java because java's date implementation was notoriously bad. So bad that they rewrote the api for Java 8. But I don't see anything that this library does better than .NET's System.DateTime 
That's food for thought. I will definitely consider this
Great link! The distinction between "an instant in time" versus "time on somebody's clock" is a huge one.
So.... DateTime.Now.ToUTC(); ?
In which timezone is `DateTime.Now` to begin with? What if you want another timezone?
Use [DateTimeOffset](https://msdn.microsoft.com/en-us/library/system.datetimeoffset)
Does DateTimeOffset account for daylight saving time?
No, ESPECIALLY then Server time , or preferably UTC is all that matters, then you report it with a timezone offset. All built-in Again, people have been travelling and trading the world for hundreds of years, all of these problems have already been solved , and I am yet to find one that isn't taken care of by standard DateTime You tell the server to treat everything as UTC, it feeds relevant timezones out as required.....all built in if you just learn how to use it correctly. The "u" in UTC stands for Universal, because UTC is specifically designed to deal with all of these problems. It's a globally recognised and understood timestamp. Irrelevant of timezones or daylight savings times. That seems to be what this library is trying to do....but it's already built in , supported and recognised internationally over all languages. It's a port of a java library.....fine...but java has always had HORRIBLE date/time support. Nothing works in the java world as far as dates/times are concerned. They made everything zero-based and that broke everything. We .NET folks don't have those problems. DateTime is an awesome class/library that will do ANYTHING you want if you plan everything out first.
Time is [hard](https://m.youtube.com/watch?v=-5wpm-gesOY) to get right.
&gt; I've heard some things are in ASP classic (I hear that's bad?). Imagine all the worst parts of PHP, but now you're coding it in VBScript. And also, the technology died in like, 2002, but like zombies, they continue shambling along.
Yes. There's more though: * Timezones * Calendar systems (Gregorian, Julian, etc.) * Periods that only make sense in the context of an exact date (e.g. "one month") Also... http://blog.nodatime.org/2011/08/what-wrong-with-datetime-anyway.html
False! Despite the name DateTimeOffset is actually a full DateTime with timezone info. Edit: at least the sql type [DateTimeOffset](https://msdn.microsoft.com/en-us/library/bb630289.aspx) works that way
Reading that manual page... it doesn't seem to handle daylight saving time. What happens if I use â€œEastern Standard Timeâ€ in the summer? Besides, it seems very US-centric. I'd rather use the [IANA Time Zones](https://www.iana.org/time-zones) (which NodaTime uses).
It takes 5 seconds to get around that and write a test.
Well, you're on the .Net sub, so we're going to tell you to use .Net. However, think about the problem you're solving, the technology you're familiar with, and how much you want to learn on the fly vs how much you want to get done. ASP.Net still uses HTML, CSS, and SQL. The only (more or less) difference is Razor vs PHP. And just as a nitpick, if you're going to call PHP a "standard web technology", then ASP.Net is also a "standard web technology".
No matter what you choose, you will still have to use HTML, CSS, JS and SQL in one form or another. And even if you choose PHP over C# (.NET) - you'll likely use a framework aswell. For example - Laravel is a good php framework. I don't see any reason to write a website like yours with plain PHP (or any language). It's like reinventing the wheel. Just do some research by yourself about php frameworks, about asp.net and choose what you like more.
DateTime.ToLocalTime(); &gt;The local time is equal to the Coordinated Universal Time (UTC) time plus the UTC offset. For more information about the UTC offset, see TimeZone.GetUtcOffset. The conversion also takes into account the daylight saving time rule that applies to the time represented by the current DateTime object Is that too much for you to read, again?
Thanks for replying. I see now that my wording was not so clear. English ain't my native! :) What I didn't get to say clearly enough was that I know the involved languages (including C#), and I'm comfortable with them. What I'm mostly unsure about, is what ASP brings to the table (as a framework) - that would make it a better choice (rather than just "go old school"). 
Thanks! I'm thinking that I want the website to be able to deal with a good amount of concurrent requests. Also, I want it to be as secure as possible - without having to *really* dig deep into security details. It seems like ASP.NET MVC is a good choice, but it's difficult to be totally sure!
Well of course. According to this SO answer [he wrote the C#5](http://meta.stackexchange.com/a/9174) book before Anders Hejlsberg. (It _might_ be a joke)
I would suggest .net, mostly for security, as you mentioned uploading files and managing users. PHP can be secured but out of the box it does nothing for you. There are frameworks for PHP that help I'm sure but I've been out of that scene for a while now. 
Change Datetime.Now.ToUtc() to a Func&lt;DateTime&gt; call. Now you can inject any logic for what constitutes "now" and test that impossible to test code.
If you develop on Windows with Visual Studio, and host in Windows Server: **.NET Full Framework**, ASP.NET MVC5 If you develop on Mac and host in Linux or Mac: **.NET Core** ASP.NET MVC 6 
You have it totally backwards. .NET Framework for performance. The framework has a decade of optimization inside it for Windows. Core has no optimization at all. It's literally not even fully built yet 
If you use `TimeZoneInfo.ConvertTime` with a timezone that uses DST, it will adjust the `DateTimeOffset` as necessary. And as far as I know it isn't US-centric, but Windows-centric as it uses the OS time zone database. Are there major issues with Windows' time zones? In .NET Core the API uses IANA TZs, I believe.
IMO you are overthinking it. &gt; Store the OAuth tokens in a server side DB. Create and store a random hash/token to store on the browser/session. When a request to a resource server needs to be made, take the browser cookie, retrieve the OAuth token and make the request server-side. This is fundamentally what OAuth tokens do in the first place, you're just adding your own encryption scheme on top of it. It isn't any more secure than plain OAuth because you're still passing a token around, and it adds needless complexity client side. If your application is full stack this may not be a problem but if you're making an API this is just another layer that doesn't add any more security. &gt; Store the OAuth token in a session variable, which is then stored on the browser. &gt; Store it in the browser local storage, in one form or another. These two options are fundamentally the same. You can't do anything to ensure the security of the browser (or whatever web client) that is accessing your application; it isn't your application's responsibility. If the client successfully authenticates, it is authenticated from an application perspective and that is it. &gt; I could store it as a cookie, but then I'm at the same point, someone could grab the cookie and use it until it expires. A user could also get up and walk away from the computer, leaving the session and browser logged in. This is a client security issue, not an application security issue. &gt; I don't mind having my users log in every 20 minutes because of the type of service. Lower your OAuth token TTL to 20 minutes. 
No, I would be a resource server gaining a token from a third party.
&gt; DateTime.ToLocalTime(); I don't want local time. I want time that is in another timezone, not the one I'm currently in.
[Here](https://auth0.com/blog/2015/10/07/refresh-tokens-what-are-they-and-when-to-use-them/) is a pretty good article I've been reading on refresh vs. access tokens. There's a bunch of fluff around JWT which may or may not be relevant but the explanation of token types is the best I've found. Access tokens are designed to be self contained; that is, they contain all relevant information within the token itself, and therefore there would be no need for storage server side. The backend would just need to validate the claims stored in the token to determine access. Long story short, the resource server doesn't need to store anything. 
1) Check out RestSharp, it's a great HTTP library that you can use to interact with a remote API. Most restful API's are honestly pretty simple to use and consume, especially with RestSharp. As far as caching the response, you may want to implement a mechanism that would first check a local cache for a response and if it couldn't find one, make the API call and cache that response. If you use user accounts or session ID's, you could also associate the cache entries with those or you could use the username and URI path that you are sending to the remote API to associate the cache entries. You'll also want to ensure that you age out your cache entries so that the user doesn't get outdated data. Once you've added something to the cache, you could say that it is valid 60 seconds after which you would have to call the remote API again. 2) See 1, RestSharp will make it really easy for you to work with a remote API. Here's an example internal static class ExampleApiClient { public static string GetResource() { var request = new RestRequest(Method.GET); request.AddHeader("Accept", "application/json"); request.Resource = "/api/v1/test"; request.RequestFormat = DataFormat.Json; return Execute(request); } private static string Execute(RestRequest request) { var client = new RestClient { BaseUrl = new Uri($"https://api.com") }; var response = client.Execute(request); if (response.ErrorException == null) return response.Content; const string message = "Error retrieving response. Check inner details for more info."; var applicationException = new ApplicationException(message, response.ErrorException); throw applicationException; } } This example would return a JSON string which could then be deserialized using a library like JSON.NET. 3) I'd reccommend that you make your calls to the third-party API from the server-side. This will help you get around CORS limitations and allow you to implement some of the caching I mentioned above. Happy coding! :) 
A lot of people seem to be moving towards SPAs using Web API and SQL for the back end and then Bootstrap and Angluar/Knockout for the front end.
You can compare the various editions of Visual Studio [here](https://www.visualstudio.com/en-us/products/compare-visual-studio-2015-products-vs.aspx). The TLDR version is that most people will be fine with Community. Regarding jobs I've never had an issue getting them in the UK. I live in Wales which compared to London, Bristol, Manchester is very poor. If there are jobs here you should be fine! Finally, as a VB developer I would recommend using C# if starting from scratch. I wont get into it too much but its fair to say there are more jobs for C# and little downsides.
No worries about the reply. Regarding extensions none are mandatory however some should be looked into further (*Resharper for example is always mentioned*). .NET jobs are typically enterprise (read office) jobs but they do exist. Positions are typically full time but people do find part time / side projects. I'm unable to get such gigs (full time contract) so dont have much experience other than they exist. .NET may not be seen as new but as mentioned in the corporate world its very trusted and in demand. Microsoft is making good moves recently and honestly more people than ever want websites/software. I dont see .NET going anywhere for a long, long time. To reiterate : Dont feel worried about .NET not having hipster buzz , as its very in demand in the corporate world which is where the money is.
There is definitely demand for .net developers in the UK - I should know, I've just been hiring a couple (Sorry, you've just missed out on this round, though we're based in Warrington anyway). It's very much a developer's market, lots of positions going and many candidates able to pick and choose where they want to go. You can absolutely work with VS community. It's as featured as VS professional (With one or two very minor things missing). Legally speaking, as long as your "business" (that is, if you're freelance) has less than 5 people in it using VS Community, then you're absolutely fine. Just going off something that /u/AlienVsRedditors mentioned about extensions. Resharper (R#) is an excellent extension to use but these days it's less useful because Microsoft's own tooling has caught up. When you get the hang of the VS IDE, look into the .net analysers - they're free and give you a lot of the same "Lightbulb" functionality that Resharper does. This is coming from someone who actually does still use Resharper and is struggling to find the value in it. One last thing - do you still have your university email account access? If so, make use of it by signing up for the freebies from Github and Jetbrains - the people who make Resharper. Basically, you can get it for free just by being a student. All they validate is your academic email address and you'll at least get a license for a year. Finally, there's a lot of buzz around .net core, which has been a long time coming and is due at the end of this month. If it lives up to the Hype, it'll be a huge boon to .net developers everywhere and finally allow true cross-plat development, making it an ideal choice for many, many lines of business. 
Welcome to programming , the constant what tech to use that would be best? Not to answer your question but I have been in paying development world now almost 20 years. And if you can make a choice, choose a different as possible every project you do. If you go with the "what I know" you get feeling kinda stuck in just a ratrace making a paycheck. If you go with what is cool and current fad as you language of choice and never go outside that you become a 20 year .net /Microsoft programming LOL. Whatever you choose to go with this time, try to go with something else next time, .NET, MEAN, LAMP, MVC, etc. Always change for keeping it fun 20 years from.
Thanks for that! It's funny because I actually decided to build the project as an Android application instead. I know next to nothing about Android development, but I'm *"comfortable light"* in Java - and at least I know more Android today than I did yesterday. :)
Thank you for the kind words, right back at you :)
Every Microsoft staffer I've ever talked to push XUnit over MSTest
If you PM me I can link you to a twitter account of an agency I always see advertising jobs in the UK for c#
Your use case is actually a pretty good example. I don't use nodatime but I believe it uses interfaces thus facilitating dependency injection. The DateTime class you are using doesn't inherit from an abstract type and is usually accessed statically e.g. var now = DateTime.Now; if (a.Day == 1) { //do something } This makes testing difficult. Your code is tightly coupled to DateTime which you have no control over. Say you wanted to write tests where day == 1 or day == 5, how would you do that? You can't control what value DateTime.Now returns. With an interface you could pass in a mocked object in your tests where day == whatever you want. 
We were using several different solutions, but recently switched to nreco. In my experience it provides best results and has a lot more consistant rendering of complex styling since afaik it uses chrome under the hood.
Do you know C#? If not, I would start with that first before trying to learn ASP.NET. Here are a couple good courses to get started with C#: https://channel9.msdn.com/Series/C-Sharp-Fundamentals-Development-for-Absolute-Beginners https://channel9.msdn.com/Series/Programming-in-C-Jump-Start
I mean, that's a good point and this is something that I've run into, but I think it has a fairly straightforward solution w/o the use of a DI library or an extra interface: I usually just pass a DateTime (called dateToCheckAgainst or something) as an optional parameter with a default value to whatever method I'm writing. That way, my code will normally get the default value when it runs but I can pass any date I want for testing purposes. True, this still "couples" my method to the DateTime implementation, but I don't see that as a big deal for something as basic as a date library which probably won't be changed very often anyway
Good write up. I've been interviewing for a new job and have been getting this question everytime. This is a must know for any c# dev.
&gt; reactive css You mean in the project template? That's not meant to be used in a real application, it's more of a starting point/demo. 
Not sure if this helps. https://msdn.microsoft.com/en-us/library/ms753367(v=vs.110).aspx 
[removed]
+1 for doing this as an animation. You can use a ViewModel property to tell the view to turn on/off the flashing animation but the actual background color changing would be defined in your XAML as an animation.
Exactly how I would do it.
We switched from MSTest to NUnit when NUnit v3 came out last year as they added parallel test execution at the class level (MSTest only offered parallel execution at the assembly level). For our large solution (200+ projects), being able to consolidate our tests into a few projects and run them in parallel helped improve both build and test execution times significantly. Both the NUnit test framework DLLs and the NUnit console test runner are distributed as NuGet packages so we didn't have to install anything on the build server to get it working. I'll take a stable, popular, open source library over Microsoft's "flavor of the year" approach. Especially since the main features for this "V2" release is fixing a naming mess they inflicted on themselves and delivering the bits via NuGet.
Try to work with it instead of against it. That usually helps ;-) First two questions depends on a lot of (subjective) factors. For the last one look into architecture. A common model is like this: cllient &lt;=&gt; server (you) &lt;=&gt; services (your database or 3rd parties you call) * Server should do the bulk of the work. That way you are in control of it. * Client application can be as thin (fast) as possible. * You can mitigate changes &amp; errors in the 3rd party services (caching being a good example: serve caches when the service is offline). 
You mean this point? &gt;"MVC and Web API have been merged together, so itâ€™s not really any different add an API controller that returns JSON or an MVC controller that returns an HTML view;" 
It means you no longer have to maintain two projects to serve an API and web pages (or use the fiddly "Areas" functionality). eg you can have return a mixture of API resources and HTML web pages, depending on the request
&gt; But if you don't have to publish a project in the ~~next months~~ year Fixed that for you :)
[Head First Design patterns](https://www.amazon.co.uk/Head-First-Design-Patterns-Freeman/dp/0596007124)
Not sure if it is the best, but it's what I started with to get the basic understanding.
[Head First Design Patterns](https://www.google.com.au/search?num=50&amp;newwindow=1&amp;complete=0&amp;hl=en&amp;site=webhp&amp;source=hp&amp;q=head+first+design+patterns) It is Java based but translates to C# easily. Also IIRC the source code is available in C#.
[Agile Principles, Patterns, and Practices in C#](https://www.amazon.com/Agile-Principles-Patterns-Practices-C-ebook/dp/B0051TM4GI/ref=mt_kindle?_encoding=UTF8&amp;me=)
Design patterns are not like javascript libraries... they don't get "old" after a year...MVC was introduced back in the 70ties.
It looks like the downloadable code is Java only. The book itself has all of the code in C# though.
Writing a unit test for a couple lines that effectively assigns DateTime.UtcNow to a variable isn't all that necessary IMO. I'll give you the fact that DateTime libraries are often complicated because they need handle weird corner cases as you mention. What I meant by my poorly worded message is that my use case is very simple and doesn't involve any of those cases.
[removed]
looks good.. couldn't get the sample code to compile.. I emailed the author if it would be upgraded for the final .net core
Yay for the major version number of the next release being equivalent to the shorthand for the current release! The principle of least astonishment is lying dead somewhere in an alley in Redmond. 
Lol, how does that make sense?
&gt; Visual Studio 15 will let developers choose the components they want installed They didn't before? 
We should have had Windows 9 to add to the confusion. 
Robert C. Martin, a.k.a. Uncle Bob, initially wrote a book for [Java](https://www.amazon.com/Software-Development-Principles-Patterns-Practices/dp/0135974445?ie=UTF8&amp;qid=1466457870&amp;ref_=la_B000APG87E_1_3&amp;s=books&amp;sr=1-3) (2002) before the C# (2006) book was written. The only version he seems to maintain, or at least I can find links for on GitHub, is Java. The book has helped me understand concepts, and most consider Uncle Bob one of the essential authors of computer programming. The C# book has code throughout, especially Section 4, but I cannot find a downloadable source. You'd have to input by hand, page-by-page if you wanted the complete system.
So, the definition of a component has changed to be much smaller? 
Visual Studio itself is actually quite small. The vast majority of its functionality comes from plugin components hosted via MEF. So my questions include: 1. How are "components" as we see them in the installer mapped to "components" as in the plugins that VS actually uses? 2. How will that be changing? 3. Will we be able to pick and choose individual plugins or are they still only offered in bundles? 4. How much of this is being moved into NuGet/Visual Studio Extensions?
Then Visual Studio Code could be renamed to Code Studio Code. 
I find more often than not that patterns are rarely well represented in books; they are almost always delve *too* far into implementation specific details or simply gloss over *any* implementation details in favor of theory. Sometimes, all I want is an ELI5 about the pattern, and let me work the rest out for my codebase. Books on principles of coding, however, are dime a dozen. One I like in particular is [Adaptive Code via C#](https://www.amazon.com/Adaptive-Code-via-principles-Developer/dp/0735683204). It gives plenty of code examples (you can download the entire working solution projects from the website given in the book) and it does a fair job of covering several patters and gives specific examples of how they apply to the SOLID principles. Of course, there's always [Design Patterns: Elements of Reusable Object-Oriented Software](https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612). This book has been one of the programmer's bibles for the past 20 years. It is a more difficult read, since it is a more theory oriented book. One thing to note is that some of the design patterns discussed in the book have been rendered obsolete by aspects of some languages and frameworks, while a number of new patterns have come to exist based on those same languages and frameworks.
[removed]
In the js or html editor, I hit one quote and the editor inserts two. There is no way to turn off this misfeature. That's all I want. Let me turn that crap off.
I own all of [Robert C. Martin](http://www.amazon.com/Robert-C.-Martin/e/B000APG87E/ref=dp_byline_cont_ebooks_1#)'s books, except for UML for Java, as well as [Martin Fowler](http://www.amazon.com/Martin-Fowler/e/B000AQ6PGM/ref=ntt_aut_sim_1_1)'s Refactoring (the white one), PEAA, DSL and Analysis Patterns. I also have [Refactoring to Patterns](https://www.amazon.com/Refactoring-Patterns-Joshua-Kerievsky/dp/0321213351/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1466460934&amp;sr=1-1&amp;keywords=refactoring+to+patterns), [Code Complete](https://www.amazon.com/Code-Complete-Practical-Handbook-Construction/dp/0735619670/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1466461548&amp;sr=1-1&amp;keywords=code+complete) and [Design Patterns (aka go4)](https://www.amazon.com/Design-Patterns-Elements-Reusable-Object-Oriented/dp/0201633612/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1466461464&amp;sr=1-1&amp;keywords=gang+of+4) and a few others. I would suggest Refactoring to Patterns is the one you are looking for, but that expects you have a working knowledge of what the patterns are already (likely with your experience though you may not know them by name). I don't think that is strictly necessary but it will help you understand why you might want do do things the way being suggested in the book. The examples are in Java, but they apply to C# just as much. His site is here: https://industriallogic.com/xp/refactoring/ and he has the code up behind an online course / paywall (I have not looked into this). In a greenfield environment, tdding up an application from a blank project Uncle Bob or Fowler's pattern books are probably better, but from the perspective of an existing codebase I think Kerievsky's Refactoring to Patterns and Fowler's Refactoring are best. 
I'm reading this while waiting for VS 2013 to uninstall so I can install VS 2015 after that. Ugh... too right. FWIW - Assassinating various SDK add-ons through the Add or Remove applet and liberal use of CCleaner after an uninstall helps a lot to clean out the garbage. 
While it wasn't finished so didn't have all the components the last time I installed VS 15 using the new installer it took under an hour including downloading on a terrible internet connection, and absolutely nothing failed. Compared to closer to four for 2015 already downloaded and having either the Android SDK or Windows 10 fail most of the time. I also managed to remove it in even less time and without somehow using more space after removing components. I'm withholding judgements till I try one with feature parity, but still impressive.
Yes you can turn out off. Can't recall there name of the feature off the top of my head, but it's in the editor settings.
Seriously? No one has mentioned the .NET Framework Design Guidelines? As in the set of design patterns that .NET was actually written with? You disappoint me.
I'm sorry dad
What part specifically? It can get pretty annoying and it depends how deep you want to go but most methods are virtual so you can replace them with whatever you want. 
its so old but I might get it .. I am not that smart so it seems to be simple enough for me
Try http://stackoverflow.com/a/29696188
Will they have a x64 bit version? 
I seem to recall that most of the stuff we deem to be "Visual Studio" like Solution files, Intellisense (not the language specific bits, the whole thing), MSBuild, etc. are all plugins.
I just want them to improve where files are stored. I picked my data drive so my SSD wouldn't be full (c:) and it then proceeds to tell me 75% of the installation will be on c drive anyway.
So hard to find
Windows 8.72 for Workgroups
I'm not going to store the refreshToken at all, at this point. The uses for the authToken are minimal and the user session is likely to be a fraction of a minute anyway. Even if they need to log back in once more to perform the actions, it's not that much of a headache and saves me the trouble of ensuring the backend is secure enough to save a token which can give *forever-access* to a hostile actor if they were able to get it. I'm not a security expert and only a hobbiest coder so best to leave that to the professionals! I'm going to play around with encrypted DBs and tables, maybe once I have that down I'll consider saving the refreshToken. Even then, you're idea about storing it in a cache-style-system looks better, that way it would get auto-purged after a certain amount of time.
Yeah the other ones look more intense.. so even though its java I may get that.. I did see someone on GitHub converted some things to c#
From what I can tell in the original blog post it sounds more like they've done better at compartmentalising existing code. Nothing that has required major refactoring of the majority of the codebase. I wouldn't expect a major push for x64 until they are looking at dropping 32bit support for Windows. There's just a huge plugin ecosystem for the VS shell that it's going to be painful no matter when they do it. So might as well wait until they have to do it.
Indeed. SQL Server 2014 Management studio uses the VS shell. You can make your own tools that use the VS shell as well.
Holy shit, just like they announced and demoed during Build! /s
The latest release is a release Candidate (RC2), you should have really known this when you downloaded and tinkered with it. Either way, yes it's not officially "done" yet, with the RTM due later this month. I don't expect too much to change between RC2 and RTM though. As for what to learn, I mean it's hard to say, nobody will be using .net core in production right now, but it will be the future of asp.net at some point. The stuff you learn is generally applicable to both though, so if I were you I might dabble in one but concentrate on another. Which you choose to concentrate on is up to you. I'm not entirely sure what compatibility issue you could be having, jQuery is frontend, asp.net is backend. I suspect your compatibility issue is something else.
I guess my main concern is if Entity Framework is not mature enough on .NET Core platform is it worth starting development in .NET Core or sticking with 4.6 for the time being.
.net core isn't final yet, they are still to make breaking changes 
Great read. I'm trying to get my company to modularize components of our program. Maybe this article can convince them
Just let it die with the little dignity it has left. There are plenty of better unit testing frameworks, we don't need to keep the crap ones from Microsoft's "fuck the community" phase.
Ah, in that case can't you pass `Report` as a parameter to the `DiagnosticsTask` constructor? Assuming each task can pull child task results from the `AutomationReport` instance.
Isn't this really more of a "why is ravendb written in a higher level language"? I feel like the same arguments can be made for any jvm or clr based language (scala/f#/Java) 
The ms documentation shows the event handler being attached before calling start.
Nope. https://msdn.microsoft.com/en-us/library/system.diagnostics.process.beginoutputreadline%28v=vs.110%29.aspx?f=255&amp;MSPPError=-2147217396 You just can't attach it when the the process isn't started yet.
Or even just a higher level systems language like Rust. Personally, anything that needs performance I write in Rust or C++ these days.
See if this can meet your need: http://dotnetify.net
I know Rest is like a bunch of principles, but I want to know how to best implement these in an dotnet core application. And you are probably right, dotnet core is not that different from Webapi. For most part I like reading books, so if anyone know about a good book about Rest or Webapi reply here :)
Handlers don't get called until you call BeginOutputReadLine(). The moment when you set them doesn't really matter as long as it's before BeginOutputReadLine. Missing data is at the end, not at the start.
Places to start.. [ASP.Net Core](https://docs.asp.net/en/latest/) [.NET Core](https://dotnet.github.io/docs/) Not extremely helpful because significant parts just aren't written yet, but there really isn't a one stop shop for current information. If you try to exercise Google-Fu, make sure you date restrict your searches to the last several months. Much of the information you'll find in random blog posts, StackOverflow, and previews from 2015 is either useless or misleading due to being from 2-3 breaking changes ago. This is especially true for the build system and the tooling which is a complete clusterfuck that won't even be resolved on release. Most of the really relevant information that you need you won't find anywhere else but Github issues and source, so get used to looking through those to figure out problems and understand how things work and what is the current direction that the ASP.Net team is headed. The bigger issue if you have a pending project is to secure your dependencies before you start. You need to know exactly what functionality you need and if it's not supported by the platform you need to know where it's going to come from. Don't assume that the Nuget dependencies you normally use have been ported. Verify. Do this ASAP. Also, things you might assume still exist in .NET might not be there. If you need XMLS/XMLT support, LDAP &amp; AD support, and SMTP support, you don't have them. If you're used to working around some domain problem with AppDomains and the Reflection API, the former is gone and the latter is significantly changed. Good luck. It's an interesting new world. 
haha yes. Date restriction in google search is something I need to remeber ;)
Like other have told, you don't need something specific for .NETCore. So I can recommend the following books for .NET and REST: [Designing Evolvable Web APIs with ASP.NET](http://shop.oreilly.com/product/0636920026617.do) [RESTful Web APIs](http://shop.oreilly.com/product/0636920028468.do)
For a web app (in contrast to a web site): I'd say so. If all you need is primary presenting data (as in a traditional web site) MVC is probably easier and faster to develop, but the more you're handling data modifications and different data-depending views I'd go with React (or Angular 2). It depends on the complexity of your application.
REST is simpler than you'd think - a book would be massive over-kill for someone just wanting to implement a RESTful interface [This link](https://www.servage.net/blog/2013/04/08/rest-principles-explained/) explains pretty much everything you need to know. And even within that, it's mostly the first two points - Provide a uniform interface: retrieving Playlists or Songs, for example, should basically be exactly the same apart from one URI says /Playlist and the other says /Song - Stateless interactions: no cookies or session states, each request contains everything needed to make the request and provide the response.
I might try this again. I originally implemented it like this, but ran into an issue somewhere. I just can't remember at which point.
Seems bit like a religious argument.
These are interesting, perhaps consider implementing a skip list as well for performance comparisons. https://en.wikipedia.org/wiki/Skip_list article about implementing in C#: https://msdn.microsoft.com/en-us/library/ms379573(v=vs.80).aspx
It is more complicated, but on the plus side you can test the child class without needed an instance of the parent. 
Thanks, their SPA example is what I was looking for.
I saw that but it seemed to be more focused on a web app. rather than web API. First time making a site with .NET and it's somewhat confusing.
Depends on where you are located. When I did contracting I found a lot of work through the developer community. I went to every code camp , meetup , user group etc. 
This incredibly simple and extremely smart. This will definitely be on the list of possibilities. Thanks 
Another great reason for me to get out to these events. There is a pretty good community here and I have been to a few but I'm certainly not a regular. Thanks for the advice. 
&gt; as long as it's not too advanced GUI. Yes. Granular does not support DragDrop. ( https://github.com/yuvaltz/Granular/issues/28 )
For me it's all stemmed from relationships I built up with clients at previous jobs. From there, it just stems from word of mouth. I.e. I do a good job for James, James is talking to Tom, Tom mentions he needs some work done, and James recommends me because he likes me. It really does suck because to get started, it helps immensely to have existing clients, both as references and getting recommendations from them, but if you have no clients, it's a catch 22. I know there are some sites where you can bid on jobs. You'll more than likely have to do it for cheap, but you never know what customer might hire you for cheap, and like you enough to keep you on at a higher rate. 
This looks interesting: http://www.cshtml5.com
Shit man I do that already. Done about a dozen 
Why does this need to go to 200 different controls at one time, this smells of bad design to me. 
In addition to the suggestions you already got, try contributing to some open source projects. It will help you build up a visible profile online but it also expands your network. 
You may want to setup the viewmodel as a resource instance. Otherwise you might get multiple instances of it. Not sure if that is your intent.
Thanks for the input. I have looked at a couple sites but you are right most of the bids there are crazy low. Maybe taking on a client or two on the cheap and short term will provide a lead to something better. 
This. Well enough said.
I can see both sides of the argument, both for keeping and removing the keys. What I'm assuming is that they're addressing the fact that there's plenty of users leaving their API keys still open even when they're done using the service allowing possible attacks of malicious intents with them later on. The people claiming that it won't be doing very much most likely think like that due to fact that they're being responsible and are falling for the fallacy of believing in others would be responsible just like you are with the service. While I think this is a necessary evil to implement at this point, I'd personally think that they should make improvements like this; - New users(less then 60 days old) should have their keys reset/disabled once a month. - Regular members(60+ days old and not verified) should have their keys reset every 90days - Verified members(Perhaps members that have somehow proven they're actively using the API for their projects and such) should have a option to choose when they have their keys reset from 30-180 days
Find and contact local digital advertising agencies. Their line of work naturally ebb and flows as they gain new clients and complete work, which generally works well for freelance developers not looking for a steady stream of work (i.e. full-time). 
Almost done rewriting all of my data and time from the default API provided by .NET with this. I'm personally finding it easier to use now and pretty sure it's also making it nearly impossible for me misuse it in some way.
Key Expiry/Rotation - yeah, that's all good, but they need to make a way to automate this process. For companies that have highly automated processes, having to go in every 90 days to renew a key is a pain in the arse. 
Good food for thought, thank you.
Which is why they could also implement an type of confirmed business/company account which could verify and would offer the possibility of disabling the API key timeout itself. Right now I agree that without any type of automation, this would be very annoying for any company/business. Edit; I've sent them a email about this idea just encase they didn't think of doing something like this for a tier structured member system.
When I went freelance I had two primary routes for getting clients. One was I was well established locally in a strong developer community so between Devs unable to take on extra work and word of mouth I got local clients. When things were slow I'd also hop on upwork for small quick jobs. I was doing pretty well in both markets, end of the day though while I was really good at my work, a terrible jon really killed the passion of doing dev work for me, the freedoms of freelance helped, but I was still burnt out. Since I started teaching dev though I've started to live it again, I'm learning and teaching new things all the time, not just using established tech, plus it's nice to be able to discuss tech on a broader level and not always have a feature to implement or problem to solve.
I fail to see how this stops anything. For people who unknowingly publish their API keys to GitHub and the like, this won't prevent them from merely updating the keys and repushing them. For people that already recognized the mistake and expired the key manually, this is a non-issue. For everyone else, this change is little more than an annoyance. At the very least, it should opt-in only. This is what happens when tech groups lose sight of their user base. Big fat fucking fail.
IMO, .NET core is still evolving. Until RTM, it's not worth starting a serious project in regardless. Dabble, sure, but nothing binding.
&gt; HTTP calls together to grab a new key. We'd have to control access to that endpoint ... maybe with some kind of key?
I guess this is location dependant - here in the UK there's a couple of small web development style companies in each town snapping up all this kind of work
&gt; API, Caching the response? This is your call. The assumption is that you'll cache what you are able to - because it makes your application faster. They may enforce a certain level of caching by rate-limiting your requests, which may influence how aggressively you cache the responses. In general, though, you'll want to cache/store as much data as you can, and only retrieve things you expect to have changed &gt; Working against the API (frameworks?) You make a helper/client class which does the work for you, then you treat it like any other resource. Eg my class contains things like RetrievePlaylistsForUser(int userID), which returns a list of the playlists. The rest of your application can then ignore the specifics of the API and just call those methods as if accessing a database or similar: your application doesn't have to care where the data comes from. &gt; Server/Client side? This will depend on your use case and the specifics of the API. Generally you'll want to go via your own server so that you can process/log/limit permissions etc, but it really depends how much interaction you have with the data. There are times when you'll even have a mixture of both, for different types of data you're retrieving.
As you can see from my code example, I use events quite a bit. And I can't remember what problem I had when I wanted to implement an event for this issue, but I was against it because I thought it was a problem. Anyway, I just added an event to my base automation class and the call it from the children class and that seemed to work just fine. Thanks again for the help.
Kernel version 6.4.
You can choose whatever you want buddy. I personally love combining the Linux world with the .NET world.
Very nice! Also check out [Orleans.Activities](https://github.com/OrleansContrib/Orleans.Activities), which is Workflow Foundation for [Orleans](https://github.com/dotnet/orleans) (which originated the actor model used in Fabric), and [OrleansFabricSilo](https://github.com/OrleansContrib/OrleansFabricSilo) which allows you to run Orleans on Service Fabric.
Nice. I was not aware somebody did this. I played around with Orleans for a bit before moving to SF.
You shouldn't. The last OS that didn't have 4.0 installed via updates was XP. 
Alright then I will just update Windows and hope that everything goes well. Thanks
I'm pretty excited about this. I've been working on a small web app for work on .NET Core running on Hyper-V containers. Of course there are bugs but I like the direction. 
We just updated to RC2, and we're doing things about the same way you did. We're getting the same error. Edit: Do you suppose it's an issue with a backslash in the instance name? I wonder if the escaping is funky in Linux. Edit 2: Yes, it does seem to be an issue with the backslash. I switched from using Server=serverName\\InstanceThinger to Server=ipAddress,port And now it's working. Make sure it's a comma after the IP Address, not a colon. Edit 3: I opened an issue for this. https://github.com/aspnet/EntityFramework/issues/5851
Out of curiosity, what factors weighed into your decision to move to Fabric?
It has a bit of a bigger scope than Orleans. It's more than an actor framework. It's a distributed services framework, with an actor framework layered on top. I have non-Actor stuff running in it. Hell, I've fired up a few Node instances in it, because I needed to do something that was already done there. Also, it's an actual product that is going to be supported. I view it more like I view IIS. An infrastructure for publishing things into. Multiple applications. Some dependent. Some not. Some .NET. Some not .NET.
Beta 12 is being released. Core is far from ready
Unfortunately it's just some advertising for their VSTS plugin which only works in combination with their service. I would have loved some insight into implementation of these patterns instead of: here, take this plugin, pay us and it magically works.
I disagree, I think this was helpful to inform developers of the different ways a FeatureToggle can be used. It wasn't a deep dive, but it at least introduces the concepts and they can dig deeper from there.
I get your point, but I don't agree. This msdn article is still just an ad to me. "Effective patterns" would suggest some details or even some schematics and diagrams. All I get from that article is the names of the so-called patterns but not how to implement these. This isn't MSDN-worthy, IMHO. Let me give you a different example: I'll create an article called "Effective UI patterns" and all I do is mentioning MVP, MVC and MVVM in the title and describe how awesome it is for the user to have a fluid UI. I'll mix in some unrelated mock-ups from different UI ecosystems and call it a blog entry. On top, I offer you a plug-in that does it all for a ~~small~~ fee. You still think this is helpful?
I'm a few months into building a full-scale web application in Core. The tooling is working fine, and there have been some bumps with each release but I'm confident the RTM will be a stable product.
I definitely see your point and agree its more ad than educational content but for those who know nothing about feature toggles it provides value. For example, I had never thought about using them for "Subscription Plans and SKU Management" but now I may research that further elsewhere now that I know it is a potential use. Its a stretch.
It cam get a little tricky depending on the browser too. You might want to debug in a few different browsers. 
Browser, not even close for me. I would be open to trying the IDE, but I have yet to see any advantages.
I personally am not a fan of ORMs, and have moved completely to Document DBs in general; I just think they are the correct reflection of the modern data uses. However, ORMs are still widely used, but make sure the tool fits the project. My personal experience with EF was that it took more work that it was worth if I was doing anything more than rudimentary, but I am sure you will find others that disagree. 
&gt; May want/need to use stored procedures and/or functions to return the data and don't want to do something like db.Query&lt;Customer&gt;("select * from Customers;") in my code That's why I created Tortuga Chain. With it you'd write, ds.From("Customer" [, optionalFilter]).AsCollection&lt;Customer&gt;().Execute(); At runtime it will generate your SQL based on the properties in the customer object. This is a typical repository method for me: class Customer { T GetById&lt;T&gt; (int CustomerKey) { return m_DataSource.GetByKey("CustomerDetailView").AsObject&lt;T&gt;().Execute(); } List&lt;T&gt; GetById&lt;T&gt; (int CustomerKey, object filter) { return m_DataSource.From("CustomerDetailView", filter).AsCollection&lt;T&gt;().Execute(); } } The UI team then creates little single-use classes that match the fields they care about for each screen. They get convenience; I get good performance without lots of extra effort. 
You know, this is a very valid point and one I keep forgetting about. The UI does want to know all the details, you often don't want to have to go down 2-3 levels deep to get some property to display.
Chrome Dev tools
Fine, but you shouldn't be attempting maintenance work on the system without a copy of its database. Schema if not full data. You need to have functioning copy of the site before you can start work. Simple as that. If you can't compile and run the existing system, you can't possibly change it reliably. You don't necessarily need the exact database (although you're making headaches for yourself even then) , but you need the schema and the ability to recreate a similar DB, with an active user. As I said, it's a 2 minute job to create a user.....how long have you spent trying to work around it instead by creating an untested insecure system in the process? I, personally, have spent longer replying to you than you would have spent making a simple empty Membership-aware database and adding a user to it. Thanks for that. You admitted in your OP that you were being lazy. Stop being lazy. Create a new Db, use the asp-regsql utility ro create the membership tables, use the-presumably already existing-registration pages to make a new user, give that user admin rights, end of story.
I'm a big fan of Dapper.net (a micro ORM). I've tried EF, Linq2SQL and NHibernate before settling on Dapper. It's just easier to use and performs well. &gt; SQL inline, which is a big no-no at my current job Inline SQL isn't really anything to shake your head at anymore, as long as you are using parameters. With IIS and Application defaults, you basically have to go out of your way to allow SQL injection.
&gt; My personal experience with EF was that it took more work that it was worth if I was doing anything more than rudimentary Any specific examples?
Browser. 
Thanks for the links
&gt; Write you SQL to match your classes as closely as possible. Use joins to avoid multiple round-trips and write your select statements to avoid bring back fields they aren't going to use. Do you just write a bunch of "one-off" static repository methods to get these data sets? I feel like many web pages use different business rules to get data so I end up writing a unique LINQ query for each one.
Browser for sure. I like to use Chrome.
:( Where were you two months ago?
It's not annoying, it answers the question I was really asking. 
Document DBs (I'm using Elasticsearch and have played with MongoDB years ago) are easier to use and have usually unique interesting features, e.g. search and get the response as json which can be passed through to a web client. The reason why I'm almost always returning to RDBMSs is that in most projects I need a consistent source of truth in real multi user scenarios. This in turn is achieved best with normalsation and powerful locking mechanisms. And then this is where SQL Server, Postgres ... are far more powerful than NoSQL.
Damn, I think that's the first time I've ever agreed with him.
Read [Implementing Domain-Driven Design ](https://www.amazon.com/gp/product/0321834577/ref=oh_aui_search_detailpage?ie=UTF8&amp;psc=1). Customers and orders are in different [bounded contexts](http://martinfowler.com/bliki/BoundedContext.html), i.e. a customer object should not load all associated orders and vice versa.
Yes, referential integrity and lack of flexibility with atomicity is the biggest issues I have with NoSQL solutions. They seem better suited for "non-critical" read-heavy data. I always hear that mongo is better if you denormalise but that seems inherently wrong to me, especially if your going across documents and Mongo only supports atomicity at the document level.
I'm going to throw in my two cents. My opinion is formed after building multiple real world apps with entity framework that are out there in the wild being used. EF5 and EF6 code first helped me get things delivered incredibly quickly, especially with migrations enabled and deployment processes that automatically migrated the databases. You will however have to profile your application with a tool like the EF profiler from hibernating Rhinos, as you will have more than likely created a few badly performing queries or n+1 queries. However, this small additional bit of work out weighed the time saving we made during dev. For the current project I'm on, we decided against using a heavy ORM like EF. We're still undecided but are leaning towards dapper. The schema is a little more complex and contains several many to many relationships, which are difficult to define in EF7. By the way, EF7 has changed a lot and I believe is still not ready. Features have even been removed , like auto migrations 
That's not why inline SQL was historically frowned upon. Inline SQL ties your application to the physical design of the database. Stored procedures allow you to encapsulate the database schema, allowing for looser coupling between application and data tiers. Inline SQL also makes it hard for the DBA to tune the database. 
Really? I seem to recall a huge problem in NHibernate a few years ago that was causing this but everything was resolved in way that SQL server could reuse query plans from sql queries.
I agree with pretty much all of this. In today's fast NoSQL data store world, I only use SQL for certain kinds of data stores, and almost always manually write my SQL in-line with Dapper.
You could try this for Chrome: http://lostindetails.com/blog/post/JavaScript-debugging-in-VisualStudio-with-Chrome
dont even consider using IE 
Dapper plus sprocs. EF's code-first is interesting, but when you've got DBAs and DBDs at your disposal, why not have them write sprocs? NHibernate is popular but oftentimes not worth the hassle.
That's not true with parameterized queries. You only pay the query plan tax once, and SQL cache it for future calls. It's only a problem when you inline arguments instead of parameterizing.
The argument I have seen has always been that changing a database query will require a recompile and redeployed of the app rather than just changing a stored procedure somewhere on the database and it fixes it
Seems a lot of developers i've met is surprised when i show them that IE (and Edge) have gotten themselves a proper dev tool too. Then they do a few "but can it do this?" and i reply with "yes, of course". :)
chrome dev tools or bust 
I use Firefox developer edition browser and its dev tools are so good I never looked for anything else.
Same here. My coworkers don't get it. Can I work with you?
Yes, and if it remembered all query plans forever, we wouldn't call it a cache, but a memory leak. :-)
There is one big, SPECIFIC reason that I love the options that EF gives you, and it's tied to the LINQ querying: OData. That shit's slick as an API service for straight data calls for simple resources.
It honestly hasn't been that long since I've worked with web forms, but my mind has been phasing out all that stuff so forgive me if I give you wrong information. Without knowing what your requirements are, you could store your values as properties for your page. And on postback of your page you would evaluate and then perform whatever logic you needed.
Okay, no problem :) Thank you for your advice, I'll play around a bit and tell you how I did it. I just started asp.net and this probably exceeds my abilities by now. Have a nice evening!
can u please tell on what methodology shall be optimum here ?
Why not just expire unused keys?
There isn't a way.
&gt; Inline SQL ties your application to the physical design of the database. Stored procedures allow you to encapsulate the database schema, allowing for looser coupling between application and data tiers. That's such hogwash and most people have realized it. Use interfaces and you can swap out the implementation to match the data store. 
In the context of web apps, the whole recompile/redeploy argument is silly. And in the context of other apps, how often do you willy-nilly change things on the fly to make the argument stand?
Even if it where possible, you'd probably end up with a sub-par UI. Web and Windows are very different environments (stateless vs. statefull). Microsoft tried to make the Web as close as statefull with WebForms and it has a lot of limitations.
Let's walk through this. *** Given two pieces: * Database design A * Class A For whatever reason the DBA decides that the database needs to be changed. This requires 2 changes giving you: * Database design B * Class B *** You say to use interfaces. Ok, that changes the start to: * Database design A * Interface 1 * Class A1 When the database design changes: * Database design B * Interface 1 * Class B1 Well that didn't do jack shit. I still have to change the application code to match the new database design. Which means both have to be deployed at the same time. *** Now with stored procs * Database design A * Proc A1 * Class 1 Change the database * Database design B * Proc B1 * Class 1 Now we've done something interesting. So long as the procs' API remains stable, the class that calls it doesn't have to change. Which means you are deploying one thing instead of (1+N) things. (Where N is the number of affected applications.) 
Use database projects. Stored procs are a lot less scary when they are side-by-side with your JavaScript and C# code.
Well, Microsoft's out of the box implementation is tied to LINQ, ergo why an ORM like EF. You can actually take in just the OData object though do your own processing to return everything, but you essentially need to build a query builder to get the power of OData projections.
I'm a little confused on your example. With in-line SQL, isn't it the same? * Database design A * In-line SQL A1 * Class 1 Change the database: * Database design B * In-line SQL B1 * Class 1 The only benefits I can think of with your example are: * If multiple *applications* are reusing the same stored procs. But then again, there are probably ways to share in-line queries through code as well (albeit clunky). And I'm not sure whether people consider shared stored procs good design or not. * If the database changes, but the proc's API remains the same, only the database change needs to be deployed. Not sure how frequent this type of situation happens for this to become beneficial. 
&gt; In fact, the procs were used to share business logic between applications written in a variety of languages. Ok, in this situation I can see it make more sense. &gt; I've worked at ones where database changes were a daily occurrence and code changes every couple of weeks at most. That's interesting. Care to give your opinion on whether you personally thought projects in these situation had a good design (or are you indifferent about it)? I don't have programming experience to even envision how this type of application would work, unless the application is somehow highly data driven with lots of meta data to interpret given from the database.
Or should I look into docker?
Very cheap are Virtual Private Server on Digital Ocean, Linode, Vultr or Ramnode. However SQL Server is not available for Linux. (SQL Server comes to Linux) If you have &lt; 5GB out traffic, an Azure VPS is very cheap, too.
Amazon AWS gives you a bunch of resources for free for 1 year when you sign up. I'd go with Docker Cloud (free for 1 node) + a VM and Postgres DB (RDS) at AWS. So basically zero cost for one year. Another option is Azure web apps. For your needs I believe the Shared tier works. You can run multiple web sites in the same App service plan. Plus Azure SQL, or Azure Storage Tables for absurdly cheap (NoSQL) database.
I messed around with granular awhile ago... https://github.com/yuvaltz/Granular 
I did look around granular and that was the closest i could get.how did it go for you ?
What why do you have to change them that often for us ita like once every six weeks or so.
You could look at [AppHarbor](https://appharbor.com/). I think they will catch Core soon enough. Also, if you project is small, it will be free.
It worked remarkable well for that company. It does require that all of the developers actually know how to write SQL. Experts are still helpful, but you can't have a bunch of devs who can't write a join without an ORM. Even the BAs wrote SQL in that company. If you didn't know it, you were taught it in your first six months. **** The downside is sometimes they would get carried away and shove stuff like FTP processing into the database. 
Ahh that's it. Thank you. I thought the contentroot was only for the static stuff in wwwroot and that ~/Views/MyController/Index.cshtml would still work, relative the the directory that the code and program was running in. Thanks
And if you move a column to a different table? 
I tried to get a bizspark, but they kept declining me. Do you have any recommendations on how to get accepted?
It irritates me that people still bring up AWS free tier. It's been several years, almost everyone has used that up by now. And it all it does is cause hassle after a year when you realise their next tier isn't cheap.
No layer of abstraction is going to save you from work when you move fields around on your models, so I don't know why you're mentioning it ðŸ˜•. You have to update code and migrate the existing data if you move a field, with inline SQL or an ORM. Beyond that, I'm not even understanding the situation in which this would arise - unless you made a significant mistake in your table design.
Definitively don't agree with that. Yes tooling has minor issues, but nothing critical.
If you use Docker, the move to a different host will be pretty simple. Also, they're cheaper than Azure.
Not really sure i got accepted straight away. Not a student and all I said was i was making a game using websockets. 
&gt; No layer of abstraction is going to save you from work when you move fields around on your models, If you find that's the case, then you don't actually have an abstraction layer. 
Did you ever sort this one out? It's currently blocking me from getting VS2015 installed
Turns out my previous service pack update didn't install correctly and aborted. I don't think the dot net website was telling the whole truth. I don't think SP 1 works with it. But yea. Windows update service pack updates.
Sure why not. But, http://imgur.com/dXln1Ha
A yes, because putting business logic in an environment with no automated testing, limited to no version control and poor automated deployment so that it's easier to make off the cuff changes to the system is a great idea.
sql interview question for 3 years experience
Have you *tried* using dnu wrap? It's a project of *days* to get that to stop fucking itself over, and then you have to tiptoe around it so it doesn't fuck itself over again. You can't even use non-Core NuGet packages ffs!
Azure, they have free tier but you can pay if you want more horses
dnu has been replaced with dotnet cli.
Doesn't matter, they didn't fix the issue.
&gt; All db calls should be governed by MyService talking to MyDataService, so you need to check for a dupe email you pass in that information out of the db into the validation, not allow the validation to call out. Old discussion, I know, but could you elaborate on this please? Thanks.
Great idea!!!
will give it a shot, thanks!
Just because *you* haven't figured out how to do automated testing or version control with databases doesn't mean the rest of us are so ignorant. 
You can unit test a piece of code that's got a raw SQL string in it without connecting to a database? Or a stored proc, without connecting to the database? Not an integration test, a unit test. One where you can validate your business logic independent of your data source? Repeatably as part of a build? You can keep control of an entire application's worth of stored procs? Even after change requests have become changing a couple of bits in the database? No one ever just makes a change without a proper verified build? Your stored procedures work perfectly when you upgrade your DB version and you can verify that? Without needing a fully populated database? If so I am happy to learn from the master. Every SQL stored proc deployment I've ever seen is a clusterfuck of manually changed files that are god awful to change because even minor changes involve rewriting the whole stored proc.
"This product requires Visual Studio 2015 Update 3 or later" Then I install update 3 and get the same thing. Anyone else? 
I'm just installing update 3 now, I'll let you know in a bit.
I feel like I'm way to excited for a Monday morning. This is awesome.
If you aren't unit testing your core business logic, then you're not doing automated testing that's worth a damn. Integration testing is important, but it's not appropriate for testing the rules that make your app run. It's too slow and it's too inflexible. The whole reason the original poster wanted to move his logic to the DB was to make changes easier. That's the mindset that leads to shit. There's also a crap tonne of times you don't have full control of the database or even any control of the database. Customers can do what they like and there's always someone who wants to do an emergency hot fix. Then there's the DBA's.
Yup, this is the key question, I think. .NET going open source is a big step toward it reaching a lot of different platforms, and today is an important milestone in that story: .NET Core 1.0 [released this morning](https://www.microsoft.com/net/core). If by ".NET on Linux as a real thing" you mean "some .NET applications can run on Linux with official Microsoft support" then I think we're kind of there already. They have a long way to go toward porting APIs to core, and we as a community have a lot of contributions we can make toward providing good cross-plat core libraries, but .NET Core is going to make all of that possible, and the change is already starting. It's an exciting time.
I love your circular reasoning. You use ORMs because your database tests are too slow. But your database tests are slow because of your ORM's horrible performance. For me a typical database test takes less than 5 ms. That's a full CRUD test with over half a dozen database calls. Which means I can theoretically run 200 tests per second, which isn't bad considering every "test" actually consists of a whole set of individual sub-tests. 
does install update 3 do all the other updates (ASP.NET core tools/sdk)? Would make sense since they all came out together.
Mature enough to move from windows as a development platform to a linux one with all that .net offers on the former.
Having external customers is dysfunctional?
It looks like you'll have to wait quite some time to be able to develop on linux just like you can do it today on Windows. But despite lack of some tools that windows users have, it is possible today without a doubt.
Bullshit. Opening a connection to your DB will take at least five milliseconds and querying anything meaningful, even indexed will take that long running SQL direct on the server. If you're running a dozen DB calls in 5 ms you're either doing integration tests so trivial that they're not testing anything or you're getting cached results. I use ORMs because it allows me to separate my business logic from my data layer in a way that is meaningfully testable. I use them because if I fat finger a bit of the query I know before runtime. I use them because I have an entire ecosystem of tools that help me test, optimise, version control, deploy, branch, and merge code. An ecosystem that simply doesn't exist for raw SQL. I also get to enjoy out of the box protection from SQL injection attacks and the ability to support any database I can get a provider for without changing any business logic. I get all this for what is, in most cases, a pretty minor performance hit. ORMs are pretty shit for some things, in particular bulk operations, but they're not anywhere near as bad on simple crud operations as you seem to think. Nor is your carefully sculpted SQL that good. You can also generate some god awful queries if you don't know what you're doing, but you can do that with a stored procedure though too. 
I don't think so. I've completed Update 3 and the .NET core 1.0.0 is installing happily. Ninja: I have VS Pro.
sure, thanks
Crashes on my openSUSE Tumbleweed, sadly. Still hyped, though; will surely wait for proper build :)
As far one you can implement yourself for free I would recommend identity server (https://identityserver.github.io/Documentation/) It's a little more involved but really good and the documentation is pretty great 
I did that long ago with Mono.
I'm curious why you would avoid VS's account template? I'm an identity noob, so was wondering if there's something I should be aware of.
Ah, I think I understand: you pass the domain set against which to validate instead of having the validation method querying for it. Makes sense. Thanks :)
I've never worked in a place where that wasn't the norm...
Let's pause for a second and appreciate the fact that this release is launched at the "Red Hat DevNation" conference. It truly is a new Microsoft.
With this you can build dll's that work on supported platforms (windows, linux, osx, docker, xamarin (ios, android), arm, etc) So you can have a asp.net web project with one dev running windows, one coding on mac and host it in azure, docker, linux etc. .NET Core isn't chained to a OS like .NET was. And no more closed source dependencies as everything is on github.
So I have a question that's hopefully not too stupid. Didn't think it required an entire thread. So I know a .Net Core web application can run cross platform if it is not targeting the .net framework. However, if I decide to target the .net framework with an asp.net core web application, does that negate the performance benefits that have been touted when using .net core? I recall some blog posts praising the number of request .net core could handle now. Also, mention of how .net core was more modular and didn't depend on System.Web. Is this still true when targeting .net framework 4.5.2?
Those requests are indexed gets out of a table of 100 records and don't include setup or teardown times for the connection. If that's all your business logic is you don't need tests. They also show that the difference between a hard coded query returning a data array and entity framework is about .38ms. Which given that array will need to be converted to an object at some point anyway is pretty insignificant. 
I inherited an absolute mess of a legacy code base with 100+ solutions, 500+ projects and have been whittling away at it for about a year. We've got it down to a single solution, about 300 projects now, everything hooked up in TeamCity to do automatic builds and unit test runs, and now Octopus for automatic deployments to dev/qc/staging/prod/dr. There's probably 30-ish actual websites/webservices there along with a dozen or so server-side console apps and a WinForms/WPF fat client that has 100+ modules/plugins that we deploy (in various combinations) to client machines. One big tip I'd give is to remember that VS projects are units of deployment. I was literally able to get rid of 200 projects just by consolidating them together into larger projects and using namespaces for separation. This also applied to the unit test projects where they mostly followed the project breakdown of the actual source files. Switching from MSTest to NUnit v3 allowed us to combine a LOT of the unit test projects together and use the new parallel test execution features to drive down the build and test execution times.
Eh, for some things like simple domain objects, I think document databases are fine. For some other things, just plan blob storage or SAN storage is also fine.
Also, you can deploy an app without requiring system wide changes. This has always been an issue with the .Net Framework.
Coming from MVVM knockout would probably be better.
The website says it's a hundred records. Nominally true, but sort of beside the point. Connection pooling is application scoped. Depending on your test set up it may be providing you no benefit at all. It's certainly not always applicable in a production environment and is potentially counterproductive in some architectures. A thick client connecting directly to the DB from a large number of machines for instance. Connecting to the system you're testing against is a real issue with testing. If you're using the same context to avoid it you can have all sorts of weird artifacts in your test results, and if you are closing everything down to get a fresh known start state you pay that cost. My point still stands. You can't do twelve queries that actually do anything in 5 ms. If you're not including business logic into your queries then firstly you're not actually doing what I said was bad and secondly you're getting bugger all benefit out of running raw SQL in the first place.
Don't see a fix for the On-Prem gated build bug.
Write a website where people can register and trade panini cards.
I'm arguing that if your tests are starting with a fixed repeatable context which they should, that connection pooling won't help you because the pool will be shut down. I'm also arguing that if you have a thousand clients connecting to a DB and you connection pool that app you're going to need a lot of RAM to maintain connections you may not need. Connection pooling isn't free. It has a cost.
I use the Actipro suite of editors which you can pick up for $99. They have the benefit of coming with the Actipro Themeing component which also themes native WPF controls in your application. (http://www.actiprosoftware.com/products/controls/wpf/editors)
I still can't wrap my head around how to add this PDF that I generate as an attachment, not to download it through my browser.
Well I already was using the Response.Header that this post is suggesting? It will just download the file to the browser.
I managed to work meanwhile, had to put the renderedBytes into a memoryStream :) thanks anyway
Create something you'd find useful and be enthusiastic about it; that will go a long way with potential employers. 
Presumably you have some hobbies? Think about your hobby and find something you want, but that doesn't exist, or where the existing options don't include some things you need or want to do, then make those. For example, some projects I've done or looked at in the past - Twitch Bot - Personal budget tracker (back before such things were common on various App Stores) - Graphics packs for games The more interest you have in it yourself, the more likely you are to stick to it and make a "real" project, rather than just a showcase. Being able to show the source code of a real, working, "in the wild" application is much more attractive to an employer than "Here's a textboox example of a phone book". Plus, you might be able to make a little pocket money on the side from the project.
Not the person you're replying to, by my opinion is that if you've got a specific need to use Linux, skip .NET still. It's getting there, and in a couple of years it might be worth using in a production environment - but right now it's not complete enough for most use cases. The real questions I'd be asking 1. Do I need to use .NET? 2. Do I need to use Linux? Presumably you have reasons for wanting to use each, but somewhere there will be a clear "We MUST use .NET/Linux for XYZ reasons". Considering that there are alternatives to both, I'd simply identify which you can live without, then find a substitute. .NET is nice, but we're hardly short of options in development platforms
Great answer! Thank you so much! 
And you're still arguing that running fifty tests against a mutable database is testing. It's not. Period. Connection pooling follows application domain lifecycle. It's not always on. A lot of test frameworks will actually create a new application domain for new test projects so depending on how you actually run your tests you won't get a pool. NUnit does this as does xunit. Not every ado.net provider even supports pooling by default and it's completely up to the vendor. Oracle's provider didn't for years. Even if you've built one test project to rule them all, you should be running your tests against a database with known content and structure if you really are going to do integration tests. Otherwise you can't count on your results. The only way to do this is to construct the data sets from scratch before each test which will again reset your connection. That's why we don't test business logic in integration tests and why we don't write business logic into raw SQL. It's why you can't run meaningful tests in 5 ms because you need a known state before you start. Because your create will fail if the key already exists and your retrieve will fail if you're not specifying the key in your create and doing the create retrieve update, and delete in the same test is bad practice. You've spent all this time talking about how much you know and how ignorant I am, but you've yet to show how you can test business logic in a raw SQL statement without writing bad tests or testing to many things in one test. That's why we build repository patterns and ORMs and all that crap so that you can isolate and test your business logic. So you can verify that your app does what it's supposed to do.
If you create the record as the first step in your test, then you know what the key is. Really now, is that so hard a concept?
Too complex for my needs. Also, I dislike how all the authentication process is like a black box, I like knowing what is going on and since It's a personal project I want to try implement everything I can to learn the authentication flow.
Maybe not "all" but everything I need for web dev. 
Fair enough - that makes sense. It is pretty complex and I also don't like how most of the info about it is so vague.
Thanks for the article. The background image has a very bright white section which completely obfuscates the white, border-less text. Makes it impossible to read unless you scroll down a bit. ex: http://i.imgur.com/I2p0luC.png
YEah i need to make that button a little more visible, thanks
That would be great, thanks. My situation is that I am starting just a minisite that i am hoping to grow and we are somewhat close to launch, and I wrote my own auth backend and it works fine, but I am managing that piece more than other several times over. To the point where it is starting to feel brittle. And for the love of god I don't wan to manage social network integrations anymore. I have google/twitter/facebook login. Standard signup, forgot your password, and registration confirmation processes. I also have attached some meta data to the user, about 1-5K tops per user, but I can offload that to my own DB if needed. Like I said all works fine, but constantly needs attention to stay snappy. I am looking for something that would be hopefully free or inexpensive while I ramp up and see if this deserves more time and becomes fruitful. I also want to have my own branding on everything, nothing Stormpath or Auth0 branded. Something simple that would only take a few lines to integrate into my current auth pipeline, lets me use [authorized] attributes, etc. And a way to get a list of all users cleanly for mass mailing. Anything jump out as a better choice? Any other info I can provide to make it a more obvious answer? Thanks!
That's just wrong. I know plenty of bloggers have parroted that nonsense over the years, but it is unquestionably wrong. Even seemingly trivial unit tests usually need more than one assertion.
That's because you're not unit testing. There are some instances where you may need multiple assertions but those assertions should all be testing one thing. A failing test should immediately tell you what isn't working. If you're testing your entire DAL in one test you're only barely better off than not having them in the first place.
I've heard great things about http://nerddinner.codeplex.com/
How does this compare to Quartz or Hangfire?
With Quartz, you may write lines like this: ((MyType)fromContext["magicString"]).MyProperty It's not strongly typed. It is a full featured library, but can be heavy at times. Hangfire is very lightweight and features tend to be more static and less customizable.
Been there, here's a few things that helped me out. Is this ASPNET 4/MVC 5 or the latest ASPNET 5/MVC 6? That'd probably affect the details more than the general architecture for the questions you've put forward. Here is a pretty thorough link: https://chsakell.com/2015/02/15/asp-net-mvc-solution-architecture-best-practices/ While I don't 100% agree with everything, it is really good and complete for ASPNET4/MVC5 as code first and I learned a tremendous amount from it. It will be different from your db first solution in a lot of ways but there is a lot of really good info there. Especially overview on the repository pattern(I disagree on some details here but it's minor) and use of automapper and autofac(haven't had the chance to try this out but it's pretty established) dependency injection. There's a few things I'd recommend. The first is http://automapper.org/ as recommended in the link, it takes care of the viewmodel &lt;-&gt; dbmodel conversion pretty painlessly. It helps keep your db models and view models small and saves a lot of tedious coding. The second, if married to database first, is model extensions. This is really important if that's the case. Model extensions: If you look at your generated ef models you'll notice they are all partials. This is so you can extend them with custom code separate from the generated code so they will not be overwritten every time you change the database. The biggest gain from this is the ability to do markup via model annotations on a shared metadatatype class allow for common validations, usually as a result of db schema, to be housed in one place for both db and view models. http://www.asp.net/mvc/overview/older-versions-1/models-data/validation-with-the-data-annotation-validators-cs *Old link but the basic premise should be the same for model extensions, (Ctrl+F "MetaData"). Just reference this for concepts not arch. details. So you'd make another file that is a partial for your generated EF class and have it with the [MetaDataType(typeof(MetaDataTypeClass))] decorating the class. The MetaDataTypeClass class would just have same properties that need to be validated and the annotations to do it. Then your viewmodel can also have that same MetaDataType attribute so that nothing invalid gets passed to db model. This will also allow additional validation on viewmodels, since you may have different validation to perform based on the role or stage that an EF model is updated. Third: https://jqueryvalidation.org/ Works to generate clientside validation built in aspnet/mvc based on model's data annotation. This lets you save some time with server and clientside validation. Also, use modelstate checks for all submitted data but realize they won't catch invalid models on unit tests. A small aside but that can save you some frustration if you're expecting a invalid model response of some sort for your unit test results. I've used this some but very late in a project stage and it's seems very strong. Fourth: https://visualstudiogallery.msdn.microsoft.com/6cf50a48-fc1e-4eaf-9e82-0b2a6705ca7d I haven't gotten to use this to start a project but making a sample one to play around and see how this project is setup can also offer some insights. Whew that was a bit longer than I expected, hope it is clear enough and not just a wall of muddy text. Let me know if something is unclear! Hope it helps!
I'll check it out. 
You need to reference the package that contains the assembly containing the types you want to use. Writing the using statement does NOT import any classes - all it does is let you write the short name of types instead of using the fully-qualified name. If the assembly containing the type is not referenced, then the type can't be found. In .NET Core all classes are spread across many many packages, unlikely C# where you have a few monolitic libraries. By adding that line to the dependencies section you add dependency to a new NuGet package.
That's very nice. I used nitrous in the past to fool around with new dev environments quickly, but wasn't using it because .net was my primary language. I hope nitrous officially supports this very soon. :)
Take a look at the [Bot Framework](https://dev.botframework.com/) and [Cognitive Services](https://www.microsoft.com/cognitive-services/).
The namespace is just part of the types full name. It has, as you noticed, nothing to do with in what assembly it's defined. I can easily create an assembly named "FubuSchnubu" which defines types in the System namespace. It has its uses, e.g. - Providing extension methods in "known" namespaces, e.g. many middlewares provide a `UseXYZ` method in the same namespace where `IAppBuilder` is defined, so it's easily discovered. - Being able to rename the assembly, without having to change all the code.
Makes sense, I figured there was a good reason for it. Thanks for the comments!
Have you tried asphostportal? I initially signed up with thwm about 7 months ago and so far, I am very pleased with the service. The server is fast, the customer service is quick to respond to my inquiry. They are expert in ASP.NET
Isn't Aurelia moving from jspm to webpack, wish the tutorial would update that.
I'm still a beginner to this sort of thing. Would you mind describing what you mean by the repository pattern? Is that what I'm doing right now? I'm going to research all of it in the near future, but for now was curious on your thoughts. 
I'm kind of against automapper myself. If your viewmodel &lt;-&gt; dbmodel is so simple it can be automated, you did it wrong.
&gt; The namespace is just part of the types full name. It has, as you noticed, nothing to do with in what assembly it's defined Well, no need to be so absolute. In practice, it often has a great deal to do with the assembly in which it's defined, although the older, larger, namespaces are often spread across several assemblies. It's considered good practice to align assembly naming and namespaces.
I wasnt aware of Actipro. Thanks for the heads up.
I see what you're saying but using auto mapper isn't mutually exclusive from more complex conversions. You can just have the automapper handle common fields between vm and db models and then do more complex operations after that (like setting flags for viewmodel or calculated fielda for vm w.e). I did that as a helper in my current project but I'm not sure that's the best solution either. It also has very powerful configuration options that can handle many common exceptions, but I haven't delved into those as much as I'd like.
Are you aware of wix? It's a full featured well supported install package builder, that's open source. http://wixtoolset.org
I'm not sure I agree it's a beast to code - you should treat your installer just like any other part of your application. Spending some time coding it shouldn't be a waste of time. The documentation does suck, though, I do agree. 
I've never used either in true anger, but I've spent some time experimenting with both Personally, I'm not sure VS Code is up to scratch - it's fine for quick jobs/bug fixes when out and about when I only have my MacBook, but I still prefer to use my Windows install (either VM or Remote Desktop, depending on my network connectivity) for anything more complex. Maybe I'm just too used to Visual Studio "proper", but I just can't get close to the same level of productivity in Code
I'd actually say that it's slightly different use cases. Visual Studio Code will never replace my complete IDE like Visual Studio but it's a great alternative to Sublime Text or Atom when I need to work with random text files, something in another language, or just need some scratch space. To answer your question, not a replacement for me, I use both. 
Visual Studio code is just an advanced text editor with syntax highlighting, like Sublime or Notepad++, it can't compete with Visual Studio unfortunately. MonoDevelop is the best native alternative for OS X / Linux
So, cool because even just poking around for 5 minutes, I found some really neat libraries. However, does this link to the NuGet package source / how does the install process work? Having a list of neat libraries while still needing to manually update sounds painful.
Yep, that's what I do: &gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge"&gt;
I think you can download Xamarin Studio, which is a better alternative in Mac OS X
I use Xamarin Studio daily. It's not as polished as VS but it's not too bad. The only real complaint is debugging ASP.NET, it times out quite quickly (but that's using Mono rather than Core so it might be a Mono issue). I wrote a brief tutorial on my blog for setting up an ASP.NET website using Xamarin Studio [here](http://coderscoffeehouse.com/tech/2016/03/01/aspnet-with-xamarin-studio.html). There's also tutorials there for creating and running sites on a Linux server. I'm generally happy with my move from Windows to OSX. I work with technologies outside of .NET and found mixing them with Windows was like oil and water so that was the motivation for my move. I don't think I'd ever go back to Windows. I don't miss VS that much either really. I'm more a command line and light text editor type of person and found VS bloated. I really hate installing, it takes forever. 
MonoDevelop is a part of Xamarin Studio
I've done dev in windows and VS for over a decade before I started teaching dev. The company I teach with is a Mac operation as is my work computer. I still have windows vm'd with visual studio, but since everything else is on the OSX aide I tend to not bounce between unless I need to. Plus our courses are cross platform so I force myself to use VS Code as much as possible. I still prefer VS proper when doing anything involved, debugging, etc. For simple stuff, quick fixes, demos, etc VS Code works really well. (or if you're on an underpowered machine)
It fact, it is (should be) mobile optimised. What's your device?
Here are a few more. These may be debatable so don't take them as gospel. Always: -Always use ViewModels, rather than EF model (I know this is very tempting, but I think prototyping proof of concept projects is the only time you should do this). This separation is much easier to do up front than to have to go back and change when a problem arises. Also this helps prevent overposting attacks which EF can be vulnerable to if you're not careful with navigation properties. -Always use string resources for messages, labels etc. This will make Localization easier and prevent repetition or inconsistency of common messages. Though in general you should avoid hardcoded strings in code, but for messages it's a bigger deal. -Always use AntiForgeryToken validation for form submits. Try: -Try to do models for any parameters on public methods /actions (controller exposed actions). Meaning instead of having several id parameters, have a model that is specific to the parameters of the action. This adds a lot of files and may be overkill but it eliminates a lot of dynamics usage for redirects and allows you to leverage modelstate checks more effectively. I would say this one is minor, but I do feel it's optimal. -Try to use nameof() to avoid hard coded strings, particularly for routing. For example: return RedirectToAction("Index",... -&gt; return RedirectToAction(nameof(MyController.Index),... *Strongly typed routing here will help avoid typos and improve maintenance. Never: -Never use complex db model classes(even if not an entity) as properties in viewmodels. I know this is tempting for time saving, but it leads to issues. I made this mistake in my current project as a time saver and it backfired on me as my view became more complicated(with approving/denying changes). That's more of a code first issue though. -Never do inline styles in your views, always make it part of a css class. -Never put raw javascript sections in views, always have it as a separate file and reference that way to avoid bloat and keep good separation.
While I see there is a small need for them I think they are more niche now than mainstream. Sure if you are making an application that can run on mobile devices and a desktop computer with the same code on each I would say you could architect it in a way where you would write a "desktop" application as well. But I still think that the experience between the different devices makes it not worth it. There are most likely a couple of devices it wouldn't work out well on with this fashion and you could easily get those by just doing a responsive web application. So you could do it, but really depends on the needs of your system. 
Works fine for me http://imgur.com/Uo4nrsg http://imgur.com/vOOgJiP
Was this post inspired by my recent post asking for solutions to Quartz? I found FluentScheduler and it seems really good. How does yours compare to that and Quartz? What dependencies if any does it have? Quartz has a shitty dependency on common logging.
I think we should take a step back here - what are your goals? What's the scope of the project? Who is your market? If you want to build an application that can build installers for simple one-off projects ("hey, here's my Destiny stats calculator" was one I had recently), then that's one design. If you want to build an application that can build installers for something as complex as Microsoft Office, then that's a different, much more complex design. If your goal is simple one-off applications, you're probably 75% of the way there. But.. there's a contradiction: who would want to pay for installer software for their simple little calculator app that they want to release for free? If your goal is something complex enough to be able to correctly, completely, and efficiently model something like Microsoft Office.. you've got a long way to go - you've got enough to hammer nails into wood.. but you're building a Mansion.
I have made the switch, and I'm very happy with VS Code. There are some things I miss from Resharper mainly, like the awesome Go to Definition =&gt; Decompiled sources. But VS Code is quick and nimble and I use it for pretty much every editing purpose nowadays. Big disclaimer though: I haven't worked on any *big* solutions since I switched, and I'm only working on greenfield ASP.NET Core projects. Personally, if I have any say in the matter, the time for *big* solutions is kind of over anyway, but I guess that is another discussion.
The library was inspired because I had to use Quartz and I knew it could be done better. I wrote Chroniton to be fully integrable. It supports IOC, consuming code is fully testable, and it has zero dependencies aside from dotNET. All jobs and schedules are fully customizable. (I struggled with logging and taking a dependency. I chose to use an event based model instead)
Thanks for your response. My target market is smaller dev shops or indie making B2B software. I came from one of those and know the pain of installshield express. It couldn't handle downgrading and we didn't want to blow a couple grand for the full version so we were stuck with the hassle of uninstalling the dev build and then install the product build. The installer meets everything that we needed at my previous job and we sold specialty software to large clients. I'm not looking to get in on the companies making applications like Office as there is already a mature market for that and it would be a lot harder to crack 
If only it had ReSharper. 
Who said anything about writing on a phone? Here I was, sitting on my porcelain throne dropping a deuce, and I just wanted to catch up on certain things. Browsing that site would have been more efficient while I was on the john because I certainly wasnâ€™t doing anything else with my upper half at the time.
Many programs will always be native (e.g. wireshark) but those will be in maintenance mode. Line of business apps are all web based now.
HTTP at runtime
Date: 2007. Web forms. Not in the mood to refactor something almost a decade old to modern MVC; just looking for a quick fix that will take (by default, without any prompting) any querystring variables in an `@Html.ActionLink()` (everything after the `?` in a URL, as one unit) and encrypt it. And then auto-decrypt them for the controller with a simple decoration on the method. Either it exists or it doesnâ€™t. I mean, there are so many NuGet packages for ostensibly trivial things, yet a querystring encrypter falls through the cracks?
You shouldn't be encrypting query strings, that is not security. For high security demands you should be using short lived or single use only tokens. You use the tokens to look up the secured data server-side. For security concerns the tokens should be cryptographically randomly generated not a GUID or other guessable generator. 
That does the trick, thanks :)
Not sure if it would help you, but I have seen systems use a hash of the query string as a means of protecting it from tampering. The hash is appended as a param. The query string is sent unencrypted but it is essentially read only because the server computers a hash and compares it to the query string param prior to acting on the request. Then a salt is distributed to authorized end points along with whatever hashing algorithm is used. It doesn't obfuscate the parameters but it does make query string manipulation a lot harder. Does that help?
My main objective is GET-hacking by malicious users. Encrypting the querystring was simply the first thing I thought of. If using a hash of the querystring works, awesome. Do you happen to have any resources/examples at your fingertips that you could lob in my general direction?
Why do you want to encrypt the query string? If data is that sensitive, it should never be passed to client. Are you manually setting the query string server side? If not, then there is no reason to encrypt it, because you have to be able to handle an un-encrypted version of the query string, meaning a hacker could create a form on the client and just submit that with the values they want. Or they could even scraped values from the form. If you are setting the query string on the server, then you might as well make hidden inputs and use forms to GET from the server and use DPAPI (ala WebForms) If you are worried about a user seeing data they are not suppose to (aka userA is part of companyA and should not see companyB data) you should build security checks into your server side code. If you are modifying data in GET requests, you should change the behavior of you application so you don't modify data in GET requests (its poor practice). 
I have a page with a list of vendors. Each vendor has a link to a details page. I have another page that provides details on a single vendor. This page needs to accept a Vendor ID of some kind in order to bring up that specific vendor. How would you recommend I create a link from the first page to the second without providing a querystring with the vendorâ€™s ID? 20 years in web dev and I have never come across an answer to that. Mainly because web pages are stateless, and the server has no clue what link the user has clicked on until it gets the request from the user's web browser that includes the Vendor's ID; and ***MUST*** include the Vendor's ID because that is the only way the server can associate the Vendor with the link the user clicked on. Hence, with a bit of GET-hacking, that Vendor's ID can be changed to another one; especially if the primary keys are just sequential integers (which mine are not, but stillâ€¦)
If you say so, I guess the source code branch is a coincidence. Your attitude is shitty, I gave you a solution that works in asp.net, the framework you're using, have you even tried it yet?
&gt; What tech do you use? MVC 4/5? ASP.NET Core? Lookâ€¦ *UP*. To the top of the page. Your answer is there, waiting for you. &gt; How does showing a VendorId hurt your application? That was just a simple example I put together because the underlying situation driving my needs is far more complicated, but has no real bearing on why the need exists.
Okay, this just seriously tickled my elegance bone. Do you have any examples you could lob in my general direction?
The way it is implemented does not allow it to be integrated into `@Html.ActionLink()`. Especially not transparently or without a hell of a lot of work. My problem is that the resulting URL will also target different controllers based on the class of user seeing the link (it is a shared view page). So I need to have `@Html.ActionLink()` in order to avoid spaghetti code. &gt;Your attitude is shitty If being blunt is a shitty attitude, so be it. If you want coddling and unicorns and fancy heated air blown up the backside, Iâ€™m definitely not your best candidate.
 [Route("home/index/{query}")] public ActionResult Index(string query) { var protector = new DpapiDataProtector("myapp", "params"); var encrypted = Encoding.ASCII.GetBytes(query); var unecrypted = protector.Unprotect(encrypted); var vendorId = Encoding.ASCII.GetString(unecrypted); ///..... unecrypted = Encoding.ASCII.GetBytes(vendorId); encrypted = protector.Protect(unecrypted); query = Encoding.ASCII.GetString(encrypted); return View(); } This encrypted value will grow based on values... so there is a limit when encrypting data.
I am curious as to how the `@Html.ActionLink()` would encrypt the actual query. How would you attach an encrypted querystring to `@Html.ActionLink()`?
There is an example of the same principle being applied in web forms here. Most of the logic around how to do the hashing will work though. https://dotnetcodr.com/2013/10/28/hashing-algorithms-and-their-practical-usage-in-net-part-1/
oh no not downvotes
Yup, anything with ticking data, lots of data, complex user interactions, etc are still WPF. I doubt this will change in medium term. 
The identity bits actually aren't tied the the UI at all - when you create a new project however, just sets up some sample code to get you up and running and cover all the main concepts. You can throw all this away if you'd like and implement yourself. Your `DL` project will need to take a dependency on Identity as well, and then put your "User" class in there, extending the built in Identity user (this too is optional, but helps not expose you to too many concepts at once), and then you'll have all your data related bits in that single project.
Build a ViewModel for your table, encrypt the data in the controller and assign it to your ViewModel, then use the route values param to set the query string.
You click on the package and click on the link to their website or GitHub repo and you find each individual set of install instructions, just like any other way you'd find a library. It's a generic site that does this for different things, they're not going to make it .net specific. It also includes things like IDEs, which would have nothing to do with nuget packages.
I understand Quartz has some issues but I've never heard problems with Common.Logging. What's wrong with it?
&gt; you've got enough to hammer nails into wood.. but you're building a Mansion. Yeah from the screenshots this is very very basic. You might fool some business people, but any Windows developer wouldn't be interested in the least. 
So you could use the built in [data protection](https://docs.asp.net/en/latest/security/data-protection/using-data-protection.html) libraries for this. Take a instance of IDataProtectionProvider in your constructor, then create a local IDataProtector for your class using .CreateProtector("*myIdentifier*"); Then you can call .Protect(data) and .Unprotect(data). It's not going to be automatic, you need to do it in your action methods, but this encrypts and signs, and rotates keys. It'll also work cross platform, unlike DPAPI, and, with configuration, will also work across webfarms, unlike DPAPI. However because it's encrypting and signing the resultant data is going to be big. Do remember there's a limit on URL lengths. GUIDs however are probably enough, as long as you're not using sequential GUIDs (which exist for SQL). Note that whilst this library is targeted at ASP.NET Core it does work with older versions of MVC
Ok - so let's talk about this then. &gt; It's a generic site that does this for different things, they're not going to make it .net specific. It literally says on the home page &gt;A collection of Awesome .NET libraries, tools &amp; frameworks Ok, so its a collection, specifically to .NET. My question stems from possibly some of the other feedback users are leaving....the site design is...well off. I didn't know upon the first 5 minutes I was on the site if this was a simple list of links or something more complex akin to jQuery UI, where you can choose what "modules" you want and it creates a custom package. The View Source link blends in with the header and wasn't readily apparent. &gt;just like any other way you'd find a library. I usually search the library name directly in NuGet first to see if there is a supported package. I really try not to download direct bin deployable plugins anymore. Call it spoiled. So, let me rephrase and propose a few features for the site: 1. Provide a direct NuGet link where applicable 2. Is it possible to generate a PowerShell script (or equivalent) based off of a user selection to create a custom NuGet link to download a set of packages at one time? 
Calling language translators (such as Bridge.NET) a "compiler" really cheapens the work done by real compiler writers. There's nothing wrong with calling these things what they are - source translators. 
You are right, my mistake. Xamarin Studio is actually (partially?) owned by Microsoft, and should be well supported.
&gt; How does one provide a link that tells the page at the far end what specific piece of data to bring up, that does not make use of a connectionstring? Did you mean to say "connectionstring"? The "page at the far end"? Wtf are you talking about? You use the primary key. You use the damn vendor id like literally every single webapp ever made ever. &gt;Itâ€™s hacking the connectionstring Please, stop. This isn't even close to accurate. At best you could possibly mean "hacking the db query" and even then no one on the face of the planet calls that "hacking". &gt;If that variable could be encrypted, there would be no easy way to ensure that changing it wonâ€™t give you gobbldygook on the server side 9,999,999,999,999 times out of 10,000,000,000,000. Or you could use a GUID which is much harder to guess than 1/10,000,000,000,000. *["if you create tens of trillions of GUIDs in a year the chance of having one duplicate is 0.00000000006"](http://stackoverflow.com/a/184926)* &gt; I want the added assurance that they are encrypted and therefore even more difficult to hack. Just in case I missed something else. This is so *incredibly* asinine. Use the GUID if you want to try to use security through obscurity. Or you know, implement actual authentication/authorization for the resources you're trying to protect. YES, in theory, you can encrypt some identifier with symmetric encryption and decrypt it back out on every request. Of course, why you would do that for a GUID is beyond me. And no, there's not going to be some way of making @ActionLink do that magically. How would that *even work*?
Thanks! You are right. The categorization could be improved for sure. This is something I'm going to work next on - to allow everyone to improve/edit the listings. 
To make my point, I've added two files to a web directory. One of them is a sample (http://mickens.io/c1c9ca92df084c769e5cce14cd28838c.txt). The other, another random GUID, all lowercase, with no hypens, just like the example, has a Bitcoin wallet private key for 5 bitcoins. So, if you can guess it, it's worth ~$3000. Good luck.
Oh, man. This solution is so close it hurts. ~~What you are doing is encrypting the connectionstring in the Controller. Unfortunately, I need to have this done in the View. Why? Because I attach an entire resultset (table) to the model, and `foreach` it out in the View. That is where the primary key for every row in the resultset needs to be encrypted, within the `foreach` of the View, so it can be put into the link that surrounds other content of that same row. I donâ€™t know how to crack open the model to encrypt just the primary keys of an entire resultset before repacking the model to send off to the view.~~ And when trying to implement this example in my own project, it appears that `DpapiDataProtector` could not be found, even after I called all the same namespaces that you were. Not too sure what was up with that. I even tried to put it into my Extensions namespace, but then also got protection level errors. Edit: Looking further into this.
In my experience almost all .NET jobs today are for web development. There is a small niche job market for desktop development, but they seem to be steadily decreasing. Apparently building highly responsive applications is no longer high on the priority list for business today.
&gt; I feel like the entire rest of the world has moved beyond business objects Can you expand on that a bit, I'm curious what aspects of business object design - if you mean something different to domain models - is so bad? I thought they were more or less the same thing - creating separate classes with specific responsibilities in the business process.
You can find a variety of material design UI icons at [Material Design Icons](https://materialdesignicons.com/), these can be downloaded in a number of formats including XAML for directly including in your WPF. 
Fully owned by MS, AFAIK most of the Xamarin employees are now MS employees :)
Great idea, it can be difficult at times to discover good packages, particularly if they new and not yet highly rated.
What I like to do is create my own User model in my context and have it take the username of the identity model. That way I have my own User class to play with and I can always find the identity model using the same username. 
Another good place to look is [Modern Icons](http://modernuiicons.com/).
Did you add a reference to System.Security.dll?
Xamarin studio isn't too bad these days - especially cycle 6, it's pretty dope. Debugging isn't as good as VS but its getting there. Also, theres a dark theme now 
I haven't worked with any other business object frameworks, but I'm taking "domain models" to mean a set of classes that define data and implement their related business logic. The main problem I have with CSLA (and perhaps domain models in general) is that it encourages monster sized classes, and these monsters become huge PITAs. For example, my current employer has one that is over 10k lines long, and a few that are over 5k lines. This is a difficult project to maintain because ultimately everything in the application becomes tightly coupled, directly, or indirectly, to these monster business objects and when you want to change some aspect of how they work it's extremely hard to do without introducing umpteen bugs scattered about the project, many in areas you'd never have expected. This is what I call the "cash cow" syndrome. These objects can hardly be touched because everything depends on them. In a even a moderately sized application, this bloat is kind of inevitable if you think that a single class should be modeled after a real-life thing (let's say Widgets) and all of its business logic. One of the biggest sources for this bloat is the many different types of queries that are needed. In a large application there might be 25 different ways to list and identify Widgets. Regardless of whether or not you have a separated data access layer (CSLA allows for *not* having a separate DAL, making matters even worse), that's one method for each query. Furthermore, each query has to return the same results--meaning the SELECT list of your SQL statement has to return the same columns (if you're not using Entity Framework or the like). CLSA supports the notion of a criteria class allowing the specification of multiple search parameters, potentially reducing the number of factory methods needed but even this isn't usable for large numbers of parameters. CSLA (which stands for Component based, Scalable, Logical Architecture) simply isn't scalable. I suppose it's fine for up to a few hundred active users, but go beyond that and it becomes diffficult to achieve. Why do I say that? It could be designed with scalability in mind, but more often then not, performance is the last thing on the mind of the typical developer. So, by the time your application needs to scale beyond a few hundred active users and you have to refactor it for performance, you're f-d because of the cash cow syndrome described above. Any effort to refactor anything will lead to rewriting major aspects of the application. A big source of the problem here is child business objects and collections. In CSLA you have two choices: eagerly or lazily loading them. This is fine and dandy, but sometimes all I want to do is load up a Widget object, change the value of a single property and save. If I'm also eagerly loading its Widget.Facets collection just to change that one thing, that's a lot of wastted CPU cycles just to set a single flag. If I'm doing this in a loop, that loop slows to a crawl. Of course, I could lazily load the Facets collection and that would speed things up a bit, but what about that other long running processing I do which accesses the Facets collection of 10k widgets which now slows to a crawl because the child objects it needs aren't being eagerly loaded anymore? In essence, especially in scenarios where the a business object is a cash cow, there is not optimal choice between eager/lazy loading and you just have to make choices about which parts of your application will perform poorly. I suppose there are ways to architect your domain models so that these things doesn't happen, but it's very hard to have the foresight needed to do this because it's difficult to tell where optimization will be needed until it's needed. As an aside, just try using dependency injection with CSLA. You can do it but... it's kind of a huge joke. Of course, this stream of complaints about CSLA domain models would be remiss without a suggestion of an alternative. I prefer to use a simplified CQRS (Command Query Responsibility Separation) which is really quite the opposite from domain models. Note that usually when one mentions CQRS they are also talking about the concepts of heavy caching, eventing (pub/sub events) usually involving queing service such as RabbitMQ or MSMQ, and eventual consistency. When I say a "simplified" CQRS I mean literally just CQRS and none of that other stuff. Basically, under CQRS you have separate classes for each command (i.e. action that modifies data in some way) and individual query. For instance: interface IUserWidgetSearchQuery { WidgetSearchResult Execute(WidgetSearchCriteria c); } interface IWidgetDetailsQuery { WidgetDetails Execute(Widget id) ) interface IAddWidgetCommand { AddWidgetResult Execute(NewWidget w); } interface IDeleteWidgetCommand { void Execute(Guid widgetId); } interface ISetWidgetStatusCommand { void Execute(Guid widgetId, WidgetStatus newStatus) } One important thing about this which is kind of hard to see from my examples, is that each command needs be a complete database transaction in and of itself. The execute methods here should not be called consecutively in any operation that needs transactional consistency--in such case the entire operation should be encapsulated in a single command. Also note that each individual command or query has it's own POCO objects specifying the data it needs to function. The same for data returned from queries. Each command or query is independent of the others--they are NOT coupled to each other at all, and coupling should be discouraged unless there is demonstrable benefit to doing so. This lack of coupling provides the benefit of being able to completely change the details of how one of these commands or queries works without impacting even a single tiny thing in the rest of the application. Each interface is kept very simple, so if we did want to, say, implement caching we can write a wrapper class that implements the same interface but actually implements a caching layer on top of the actual query implementation. If we wanted eventual consistency, we can change the IAddWidgetCommand to stuff the contents of its NewWidget argument into a queue for later processing. Anyway, this architecture is *very* scalable. You could get to facebook scale with a full implementation of CQRS like this. 
What I have planned so far: * Regroup some code to split some solutions (some larger projects are really painful to build) * Modify the current structure so that we can move some code into separate repositories and then use a company nuget server to fetch these assemblies. * Upgrading from TFS 2010 to 2015 (this is actually work in progress). We will switch to git for many reasons and the new build system will help a lot with new processes. The problems I do foresee: Much code is more ore less spaghetti at its best. We need to refactor and for this people need to use patterns and unit tests. But currently many people are using patterns like single responsibility only when forced and the same apply for unit tests. This implies lots of problems in quality and effort. How do I actually convince them that these things are something good in a way they use it on their own? Do you have good online resources which can help me there? Its not about explaining patterns - there I have books and links I can hand out. A good source which explains understandable why we should do this that is missing. 
The purpose is instead of doing accountId=5 which is a private identifier you do token=a4bf28= (base64 text, crypto produced) You then have an in memory table to link a4bf28= to accountId:5 Guids are not secure, you can never use them as a means of providing security .
Connection string is != query string which is what you're referring to here. This sounds like a problem in your architecture. The application should not need to hide the vendor id to prevent people from querying. You should be utilizing authorization to prevent user JohnSmith from accessing vendor WeSellStuff if he is not suppose to be accessing it.
Not always true. I work for a large international enterprise and we have switched off of desktop applications. All of our new developments are web-based, mostly hosted in cloud services. 
Some of my functions have been around a long time, before they were available in the framework. I had a few MVC methods for producing dropdowns from enums, but Microsoft eventually implemented their own so they were removed. Maybe Concatenate is another that can lay to rest now :) I've only just got around to sticking the project into GitHub though
Microsoft have and excellent OCR API: https://www.microsoft.com/cognitive-services/en-us/computer-vision-api
Surely there's a better subreddit for shit like this?
Architecturally speaking you'd be right to want to stop those EF classes bleeding through to the UI layer, but in reality if it all works and you don't want to spend time on it there's no need to rewrite it. I always like to have a strong abstraction between the EF/Data Access Layer and the UI layer, some people are fine with using those EF classes in the UI but that leaky abstraction can cause you issues further down the line, especially if you're passing around IQueryables or have lazy loading enabled.
I recently used the MS vision services, combined with azure ML studio to take an image or document as input and scan for things like MICR check scan lines, signatures and such. Then I would feed that into the ML and it would predict which workflow to start with. 
I hate to break it to you, but migrations are a pain for even expert SQL devs/DBAs. On a more serious note, I've found a very reactive approach to learning things like this. If you have to do something, then become an expert on that particular feature. Learn it inside and out.. I.e. Backup and restore. After a while of doing this you'll eventually become a competent SQL person.
Just don't use migrations or code first. It's an awful solution and learning to create your own tables, relationships, keys, indexes, and queries will help you better understand entity framework as a whole. You can still use EF for queries but it's just awful at the schema part.
Yes, most RDBMS' have the same relative guiding principles with minor differences and features. With that being said, if you are looking to proactively learn as much as possible about SQL Server (or any DBMS at that) just understand they are extremely complex and it'll be a marathon instead of some other technologies that feel like quick sprints to learn.
After years of trying various client side-validations, we've adopted the server only approach. It just didn't make sense to keep trying to duplicate (or find libraries to duplicate) the server-side rules on the client when you realize that with modern caching, a typical full-page post is at most 3-5k. If you condense that using XmlHttpRequest to post the form data in JSON (like an angular app might do) and use a server-side if statement to return only the validation errors, it's even lighter (often &lt;1k). There's just no legitimate reason to do full scale client side validation **in our typical use case**. There are exceptions (e.g. username check, password strength, etc..) where we might want immediate feedback, and those can be accomplished using dedicated ajax requests also. That said, when we do return the errors from the server, we typically use something like jquery validate to apply the errors to the elements visually. It just saves time (when jQuery is already in use).
It's a shame that there aren't better practices used in the templates, but I can see why they would want to keep it simple and not get over-opinionated on architecture.
I think you're always going to have to make some compromises somewhere with an end-to-end validation framework. For me I don't mind a little repetition in the UI layer if my app requires a better experience. On a recent project I got pretty close to my ideal framework which was using webapi/mvc and angular. I'll try and outline what I did: First and foremost you always need to validate everything in your domain/business layer, because you should never trust any input from the UI layer. I use CQS inspired by [this post](https://cuttingedge.it/blogs/steven/pivot/entry.php?id=91) so my commands are lightweight c# pocos that just contain the information I need to perform an operation. I use data annotations to do basic validation, (using custom validation attributes if need be) and then do additional validation in the command handler if needed (permissions/uniqueness etc) and throw a validation exception if it fails any of these checks. In my web api I have a helper class that I use to validate the command and then execute it. If there are any validation errors on the command or validation exceptions from the handler I return these in the json with a http 400 error code. In my js UI layer I have a wrapper around rest requests that detects the 400 error, parses the response and then invokes a service to help displaying and formatting those errors, alongside properties where possible. How that actually works in my angular 1 code is a little complex, but a simple alternative is just to throw up a modal or toast notification to say there was a problem. Additionally in my UI layer I use the basic validation html5/angular provides on the form elements to catch easy validation errors, but you can put as much or as little effort in here depending on how slick you want your UI to be. The important thing is that invalid input always gets caught at the domain/business layer.
I can't recommend Syncfusion. The products I've tried are fine, but their licensing/sales dept is hyper-aggressive. For instance, if I browsed their user forums while logged in, I'd usually get an email or phone call from them. There are other companies with similar products who treat their licensed users better.
Bot Framework + LUIS (www.luis.ai)
No mention of .NET Banana 1.0 :(
No, you can run any T4 by right-clicking them in a project?
Logentries has a free tier, 5GB/month https://logentries.com/pricing/ They support the more popular .net logging libraries, i.e. Log4net, Nlog and Serilog.
Sorry, these do not integrate with Windows 10 Cortana.
Use AJAX POSTs for form submits, use FluentValidation on the server and return a JSON response with the model state. I think Jimmy Bogard has a blog post somewhere outlining this approach.
Why don't you use both? I've been using EF for normal stuff and dapper when I need performance.
You're welcome!
Do you know of any server packages which use MailKit so I could replace hMailServer with?
I do not know of one, sorry :(
Don't waste your time with tutorials, because the vast majority of them assume familiarity with at least one topic, which you won't have as a newcomer. The vast majority of them are also terribly-written and contain factual errors and broken code. Instead, look for a good reference book. C# 6.0 in a Nutshell by Joseph and Ben Albahari is a good choice, as are anything by Jon Skeet and Eric Lippert. The [Yellow Book](http://www.robmiles.com/c-yellow-book/) also seems popular, but I can't vouch for it. Even slightly older books will be useful, as long as they cover, at minimum, C# 4.0. The language hasn't changed significantly since then; most of what has been added is syntax sugar, as well as the introduction the Task Parallel Library async framework, and Roslyn, the open-source C# and VB.NET compiler and runtime compiler services.
Since I looked at this code with my phone (not very readable) I cannot really see a mistake here. Did you contacted Telerik to rule out a bug in the grid? Used Telerik some time ago and the grid had always little issues 
There are many useful packages that don't have a .net core version released yet, so it probably wouldn't make sense to migrate an old app until more are available.
I'm working on migrating my site. It's like programming glass... So smooth. Haven't had this much fun in ages :)
Yeah the full framework is available to you if you don't care about cross platform right now. 
&gt; I'd write new apps I'd actually go even further and say don't write new apps in .NET Core, if they are business critical. Even though it's at 1.0, there is still a lot of churn happening and the ecosystem hasn't fully settled yet. It's great if you are building stuff for fun or if you project has a long time-frame that allows you to keep up with the changes but otherwise, I would recommend holding off on it for a while. 
SignalR is currently worked on. In the meanwhile you can try the CI Release repo. It works for me. https://www.myget.org/F/aspnetcirelease/api/v3/index.json
Just uncomment the Facebook provider in this example http://www.asp.net/web-api/overview/security/external-authentication-services
Thank you, I will read into that more and hopefully it can do what I'm trying to do! 
Yes, maybe if you don't have many dependencies. 
Don't worry, read this [wiki entry](https://github.com/dotnet/corefx/blob/master/Documentation/architecture/net-platform-standard.md) and everything will be clear to you.
We have a new big business app on core. Started with beta 6. Main advantage is really simpler and more streamline middleware plus simpler deployment. This is to say the main reason for core existence. Ability to compile project once, deploy as docker container with env configuration. No more horrible web.Config merge and build per specific environment. Also you don't need to use npm, bower or any node package. But you are going to use same old Nuget. You should probably use those if you want to build modern spa app. Using angular 2 or react. Also misleading expectation- you don't need to run app on core, you still can use net 4.6 with all old libs you want. Signalr well no one really cares about it. All you need is pure websocket client really this days. But .net is really missing good implementation there similar to socket.io. You can try to use dev build of Signalr or just spend 1 day to were simple wrapper. Overall - you have no client side focus app with no problem in performance or Windows deployment, then keep it for a bit. Update to new mvc 6 with full net on kestrel server when ready. But if you need faster Api or more client side focus new app, try core. There is really few package that are not updated yet. Only cause ms really sucks with management or rc/rtm 
www.hockeyapp.net ?
Use database projects via SQL Server Data Tools. You get real source control and access to all of SQL Servers capabilities.
You can generate several best selling books trying to explain everything contained within that page.
I also like EF7. I avoid EF before this and now it looks half decent. 'dotnet watch' is a gem. The ability to change and reload is really nice.
Checkout the PetaPoco fluent mapping api
I'd be the first one to remove Node from his system, but unfortunately: When doing modern frontend development, there is nearly no way around node. The tooling that comes with node and NPM is just very useful, and there is no replacement for it in the pure .NET world. Bower is IMHO already out of the equation for a long time - it is a sinking ship, and was already so before support was added to Visual Studio. Tools like Webpack are incredible useful for building frontend code. You specify **one** entry file - and this file just specifies its dependencies, resulting in a dependency graph. You clearly see which file requires what file (JavaScript modules, CSS files, markup, json, etc). Webpack can then bundle all files that can be bundled, and make the other resources available in other means. The bundling was removed from the ASP.NET Core web template because it confused a lot of people. But IMHO the project templates were never really good. They were fine for showing "here, this works" - but never for starting a project on it. I hate Node - but I embrace the existing tooling that it provides.
**asp.net core** is the web framework which runs on both .NET 4.6 and .net core. If you use asp.net core on .NET core, you can only use packages which are compiled against netstandard1.6. If you use asp.net core on .NET Full (so .net 4.6) you can use all packages which target .NET full and all packages which target netstandard1.6 So it's essential you run asp.net core on .NET full to leverage today's packages. If you run on .NET core, things are different: you need netstandard1.6 targeting packages. Your answer was a bit vague, but that's not a surprise nor your fault: the whole netstandard crap is unclear and a big mess. 
that's something else than using an assembly which is compiled against .NET 4.6 in an asp.net core app on .NET core, as that won't work at all. You confuse netstandard versions with .net versions. Something that targets netstandard1.x can be used on all frameworks that support netstandard1.x. The lower the number 'x', the more frameworks you can use the dll on, but the lesser the amount of APIs are available to to you when you write that dll. 
&gt; You can run 4.6 packages on core if you target 4.6. With 'core' you mean 'asp.net core' ? As you can't run v4.6 packages on .net core. And it's then not 'on' but 'with': the 'on' suggests you use .net core as the platform where you run asp.net core on. Yeah it's nitpicking, but it's otherwise going to be more confusing than it already is... :/ 
With is more correct. But can't you target 4.6 runtime for console apps also? Or is it just asp.net that supports it? It's a bit confusing because netstandard is so agile and can be used a lot like PCL but much more powerful. 
You have .NET core as a platform. On top of that you can run .NET core console apps and asp.net core apps. Both of these apps can only target packages which target .NET core (netstandard 1.6). If you choose to run on the .NET 4.6 platform, you can run on windows only and run console apps, winforms / wpf apps, asp.net mvc5 apps, asp.net core apps etc. The 'netstandard' descriptions are api descriptions. So if a dll e.g. supports 'netstandard1.6' it means it can be used in an app on any platform that offers the APIs compatible with netstandard1.6. 
[Crosspost](https://reddit.com/r/csharp/comments/4r1arl/schedule_tasks_in_aspnet_web_application), so I'm crossposting [my response](https://reddit.com/r/csharp/comments/4r1arl/schedule_tasks_in_aspnet_web_application/d4xkxd8) &gt; DO NOT do persistent job scheduling in web apps, especially something running on IIS. Application_Start is not called until the first request, so if you have a slow day your tasks will not be scheduled until some random time. Not to mention that if you have to stop your site then everyone will be left wondering why they aren't receiving emails (or whatever) anymore. &gt;The intention of web apps is to respond to an *external* request. If you need persistent task running then write a Windows service, which is exactly what they're meant for. 
TIL you can use asp.net core with .Net 4.6. Is aspnet core RTM already? 
http://hangfire.io/ 
I actually have no issues with this as I disable recycling, and have a job set to iisreset and send a first request from a console app to restart the process. Honestly, I don't even need to do this except I use the imaging library for resizing and it doesn't release resources properly so I can start to get out of memory errors since my app has to be 32bit for one component. If you run a 64bit app, I would wager that you never need to recycle the worker process unless you have some other coding issues.
The point is I *don't* want to cram it into a column.
Huh, weird, dunno how that happened.
There are three major benefits to ASP.NET Core vs Framework (4.6) The biggest one is performance. ASP.NET framework performed so poorly it's actually not been included in the TCO benchmarks for the last few years, when it was included it rank it the 80s to 90s out of 100. (as in there were 80/90 other things platforms that out performed it, and it's performance was less then .01% of the fastest option. ASP.NET never performed well... With ASP.NET Core though they're expecting to make it into the top 10 the next time TCO does it's bench marking. Before RC1 even came out it was shoeing a 2300% improvement in performance over framework, and it's gotten even better since then. In other words if performance is an issue you're dealing with the switch could be in your best interest. Second reason is the middleware process is way nicer, you also don't have to go through a process to setup things like Dependency Injection. This makes a lot of your setup code cleaner and easier to manage. If your setup code is pretty ugly stuff going to core might help, that said I don't think the middleware improvements justify a migration on their own. (unless your app is small/simple or happens to be easy to migrate) The last in more nuanced which is the ability to effectively develop and run it on anything. Likely this provides little value to you, but typically Linux servers are cheaper for hosting, and some developers even .Net Devs prefer Linux or OSX over Windows. That said it's doubtful this would significantly impact you so isn't a good reason to make the move. Basically if you're struggling with your applications performance it might be worth the time investment, otherwise probably not worth the effort for the time being. New projects though I'd suggest just starting with core as while you might not be able to justify migrating existing projects, using framework for new projects only makes sense if you depend on libraries only available for framework.
CodeSmith CodeSmith is a template-based code generator tool. It features a syntax nearly identical to ASP.NET and can generate code for C#, VB.NET, ASP.NET, SQL, XML, or any other ASCII based language. I used to generate my custom DAL with CodeSmith, to this day I think it was the best DAL I ever had.
Tag Helpers alone are a wonderful feature. 
Hangfire + TopShelf Windows service works great. 
It will run on Linux. That is the reason I migrated ours. Customer wants Linux support, figured Core was a better move than recoding everything in Java. But you lose asmx client/server support, aspx support, and global.asax needs to be rewritten using the new entry point stuff. So there's still a lot of work involved, fortunately we didn't use a lot of that stuff. Plus a few libraries that are gone since they weren't cross platform. Biggest is System.Drawing for me.
The way I understood it, I need a separate resource file for each page
I've used http://hangfire.io for a couple years, it's great. Do not roll your own. 
okay thank you! will do. Enjoy the weekend
I've got 13 total years in .NET and would be interested. I'll PM you.
I usually create a `HangfireService` class with a Start and Stop method and configure TopShelf to use it and call start/stop when the service is started or stopped. The service class is pretty simple, it registers my IoC container, sets up logging, etc. Something like this: public class HangfireService { private BackgroundJobServer _server; private IKernel _kernel; public HangfireService(IKernel kernel) { _kernel = kernel; // Set connection string name GlobalConfiguration.Configuration.UseSqlServerStorage("{connection string}"); GlobalConfiguration.Configuration.UseNinjectActivator(_kernel); GlobalConfiguration.Configuration.UseSerilogLogProvider(); } public void Start() { _server = new BackgroundJobServer(); } public void Stop() { _server.Dispose(); _server = null; } } Then the TopShelf service is configured using: // Create Ninject kernel HostFactory.Run(x =&gt; { x.Service&lt;HangfireService&gt;((service) =&gt; { service.ConstructUsing(name =&gt; new HangfireService(kernel)); service.WhenStarted(tc =&gt; tc.Start()); service.WhenStopped(tc =&gt; tc.Stop()); }); x.RunAsLocalSystem(); x.UseSerilog(); x.SetDescription("{description}"); x.SetDisplayName("{display name}"); x.SetServiceName("{service name}"); });
Thanks for the sample code! I'll definitely give this a try!
&gt; You can create a asp.net core app and build it with dotnet build and run it in 4.6 (If I understand correctly?) .NET 4.6.3 (not out yet) yes. Because that supports netstandard1.6, the same api standard .Net core supports (and you compiled against ;)) &gt; You can build a dll in core and use it in a 4.6 project because the dll basically is a PCL. same as above: yes, because .net core supports the same standard as .net 4.6.3 &gt; You cannot create a console app in core and start it with 4.6 because you need to somehow bootstrap it somehow. (Again, if I understand correctly?) yes. a console app on .net core supports a different standard: NetCoreApp1.0. It's also not a .exe, but a dll with an entry point. The .exe you get from compiling on .net full is actually the same thing under the hood (a bootstrapper with a dll and an entry point) but you won't notice :) &gt; And can you use the asp.net core packages in a pure 4.6 project if you wanted to? I guess this would also be a way to use asp.net core in a pure 4.6 project. Yes, as these packages support the same netstandard as .net 4.6.3, namely netstandard 1.6 :) (edit) indeed I meant 4.6.3
yes. RTMed last week
While yes, I would think Topshelf + HangFire is the way to you should be doing this sort of thing, you can change the IIS settings so that the app is (1) started immediately, (2) is never automatically spun down and (3), only runs one instance with no overlapping recycle (that's probably actually the BEST reasons to move this to a service). In fact, these are exactly what HangFire's docs tell you to do when running inside a web app. The rest of your post is right though, it's ill advised for anything beyond the simplest commodity apps to do it in process with the website in an app pool. But sometimes it does make sense to take the IIS setup hit over the "separate background process" setup hit. Depends on the app.
The biggest thing against node is that it adds a large complicated dependency in an entirely different eco system to the project that we didn't have or need before. It's like having JEE or Ruby/rake as a requirement to build .net projects. Also, my experiences with node have been less than stellar. There are a lot of low quality libraries, windows compatability issues and aweful, aweful error messages. Apart from more universal packages I'm not seeing anything done with node that I wasn't doing in an easier and better way before.
Well, there's the whole notion of "best practices". One man's "best practice" is another man's anti-pattern. Good luck! 
Quartz.NET is originally a .NET port of Quartz for Java. Like many .NET ports of its generation, the source code is not always idiomatic C#, and rarely takes advantage of newer .NET features (NHibernate is another good example). That said, it is a *solid* library with years of maturity. But it's not as feature-rich as Hangfire. Hangfire and Quartz differ, I think, in goals. Quartz strives to be the most stable and accurate schedule-based timer for .NET and nothing more. While Hangfire also tries to solve the problem of durable asynchronous dispatch (i.e. job queues) and includes a built in UI for management and monitoring. While I think Hangfire is perfectly suitable to many projects, I personally do not use it. I don't like its method of serializing method dispatch - I prefer Quartz's style of requiring you to implement IJob and simply storing the classname. I think there are better and more robust message-based frameworks available (eg NServiceBus/MassTransit/LightRail), but they are necessarily more complex. TL;DR: Quartz is a more accurate scheduler, Hangfire has more features.
Is the software in need of protection? That is, is there any reason the public shouldn't see it? If not, you could upload the code to github and ask for code reviews.
Swap your special chars
Characters were right, I needed the http.
&gt; There is no reason we couldn't have all this in pure .NET. But who's gonna write it? Who's gonna write and maintain a less compiler usable from .NET? Who's gonna write and maintain a JavaScript minifier? TypeScript compiler? I think we have most of that already. We certainly have the js/CSS minifiers, several of them. Less and typescript I'm less sure of. It's just the retrieval part we seem to be stuck on. Now I'm thinking it might be better to come out of the rabbit hole entirely and stick to nuget packages and/or files.
Third and final solution: just a switch with another syntax.
I recently ran into a scenario where I needed to run jobs on a schedule. Quartz.net was the solution I went with, but I knew a better solution could be created. So, I've started my own side project dedicated to that exact problem. Firstly, I wanted the context within my jobs to be strongly typed. Secondly, I did not like that I had to take a dependency to a logging library. While the logging solution used by Quartz is robust and a common library, there are many options, and this could be an added workload for some people to take the dependency. Thirdly, I wanted to maximum amount of flexibility in how to schedule my jobs. The result is a library I believe will be an excellent solution for many developers out there. I encourage you to check it out. https://github.com/leosperry/Chroniton Also, for a bonus, it has a build dedicated to .NETCore
Meh
[removed]
Please name a pure .NET library that does what Webpack does (with the various loader), namely I import **one** TypeScript file, and I get out a bundled and minified JavaScript and one bundled and minified StyleSheet file out (because the TypeScript imports less files). And something that has no dependency on Visual Studio. Besides that, NuGet is awful for content files (and this feature is even removed when using NuGet for .NET Core). It just copies the files into a pre-defined folder structure. It looks like the files are part of **your** project, but they're actually not. Any modification is removed when updating the package. The ASP.NET Web optimizations framework is awful, because it just concatenates the files. It does not take care of order for you - you have to do this yourself. Annoying, error prone and just not necessary. 
This is pretty cool, but I'd probably not use this when I'm writing something new. But on the other hand, this will come in very handy when refactoring old code bases. It's a good tip nevertheless. 
correct
Can you expand? I'm trying to figure out how you use polymorphism with an ENUM. 
Meh, I find this misleading. The switch/if/case stuff is not gone, it's just moved into ResolveOperation(). The third solution is unconventional and indirect. It's not too bad, but it will be painful to debug and to understand for other developers. I'd need to have a very good reason for choosing this one. I guess the better takeaway message of this would be: Don't write huge switches or ifs. That is reasonable advice.
Doesn't this need to be a readonly dictionary of some kind? Switch/case/if is optimized heavily by compilers. A mutable dictionary will not.
The original is much clearer.
This open source side project of mine has taught me a lot. If you've got needs to schedule tasks on a recurring basis, I highly recommend you take a look. Throughout the design/build process, I've striven to make it the most robust and customizable solution while maintaining the smallest foot print possible. It has virtually no additional dependencies. I've even got a separate build + package created specifically for [.NETCore](https://www.nuget.org/packages/Chroniton.NetCore/). Tests are now automatically running. I've got a simple [tutorial](https://github.com/leosperry/Chroniton/wiki/Tutorial) page and [requirements](https://github.com/leosperry/Chroniton/wiki) listed now also. Next features will include serialization and more schedule types. High on the list is getting a cron string schedule. Check it out and let me know what you think.
I've been doing .net dev for ~5 years, but only recently switched to MVC as apposed to web forms (Which has drastically improved my life) When I first started with it I was a bit lost as Entity/Identity was definitely a bit out of the box from what I was used to, lucky I was working with a dev who knew it quite well and I got some interesting insights that I haven't been able to find online to the same extent. So anyway it might not be the best way to do things but I do a (Database/ Business layer) class library project and a web application project, both in the same solution with a reference in the webapp to the class library. The class library uses code first migrations with entity, I remove/move the IdentityModels.cs from the web app to the class library. Then add this to the webapps webconfig &lt;add key="owin:appStartup" value="ApplicationsNameHere.Startup, ApplicationsNameHere" /&gt; Then you will need to add references to your class library in your webapps startup.auth.cs and AccountController.cs like so using WebAppClassLibraryName.Classes; using WebAppClassLibraryName.Context.DB; using WebAppClassLibraryName.Models; And also the db context reference to your IdentityConfig.cs using WebAppClassLibraryName.Context.DB; I might be missing some stuff, but that's the jist of it, from there are if you want to add a WebApi to the solution for instance all you have to do is add the webapi to your project and drop out the IdentityModels.cs and add the references to your class library. It works fairly well for me all in all, and when I do DB updates it all happens from the class library. Also check [this boilerplate](http://www.aspnetboilerplate.com/) out 
It's in the pipeline ;)
Personally, I'm not convinced by this approach... it just looks less intuitive/self descriptive to me. Perhaps that's just because I don't use it so it's not "familiar" syntax that I can just "read" like I would any other code... but I feel like a new developer sitting down and looking at that would need an explanation, where they wouldn't for the other approaches. Get rid of case/switch/if: why? What's so wrong with a switch?
What's being achieved here is limiting the code to consider a value to determine a branch instead of considering an expression to determine a branch. This makes the code more difficult for subsequent maintainers to complicate. So; if A: doX() elif B: doY() Can become; if A and C: doX() elif B not C: doY() elif C: doZ() The initial state is... OK but it can grow into a horrible mess when requirements change. OP suggests that having the values mapped to procedures inherently resolves this. Of course we'd probably break out the X Y and Z procedures in the mapped method because A B and C demand composites of X Y and Z in this example.
Finally, can't wait for C# 7.
The upside to switching enums is that it will generate a warning (only in ReSharper?) when you didn't exhaust all the options. This will allow your buddies to discover they forgot something when all of the sudden your warning free project (we all have warning free projects, right!?) now has one warning. I would "hide" it by making a switch statement that returns the action to be taken from a function, much like the factory pattern. private FooAction ActionForActionType(ActionType actionType) { switch (actionType) { case True: return TrueAction; case False: return FalseAction; case FileNotFound: return FileNotFoundAction; // A warning here for not covering the Sometimes ActionType } }
Thanks for the idea. Another tool in the utility belt.
I'm still not getting his post. what other syntax?
As said above code is hard to read on a phone due to formatting. One thing I see is that your NeedsDataSource event should not be the button click event. NeedsDataSource is going to fire throughout the page lifecycle. Code that separately, then on your button click event call radgrid.rebind.
No problem because Node is *not* a "large complicated dependency" that exists in a vacuum. It's a tiny/fast/lightweight and simple dependency that integrates very well with other eco systems. There's really no comparison to Java EE or Ruby and if you can't see that already there's something wrong with your perception. *(Seriously, it's a 10MB installer. About a THIRD the size of VLC Media Player.)* &gt; Also, my experiences with node have been less than stellar. There are a lot of low quality libraries, windows compatability issues and aweful, aweful error messages. Welcome to open source. That's the way it is when project such as Node.js becomes *hugely* popular. And you can't deny that it's hugely popular. That's the very reason why you're here right now. So don't use the low quality libraries. Use the high quality ones. Problem solved. If the vast majority of web developers thought that .NET was the best ecosystem for their work, they would have built all the nice stuff that Node.js has on top of .NET instead of Node.js.
He's paraphrasing the article itself. He's basically saying "This article is telling you how to remove if and switch statements by basically using a different syntax instead of a switch statement". i.e. you've still basically got a switch statement in principle, you're just using different syntax for it (A dictionary lookup).
They built everything else under the sun while building .NET Core, including 3 different build and runtime systems - the whole concept of .NET Core was stupid from the get-go. Making .NET cross-platform - great idea, fracturing the entire ecosystem - bad idea.
Yes, under the hood you usually want to achieve the same outcome, just in a different way. Personally, I find the proposed solution better for some special cases e.g. creating a validation engine with some specific rules. I think it all boils down to your own preferences :).
[C# won't have `match` expression.](https://github.com/dotnet/roslyn/blob/master/docs/Language%20Feature%20Status.md) EDIT: C# **7.0** won't have `match`, C# 8.0 likely will.
True, you're doing a switch, but actually without using the switch keyword.
Under the hood it's the same logical outcome but not the same code. I doubt that IL produced is as simple as a jump or branch. 
Exactly. They built all of those other things. But they don't have infinite developers, so they have to draw a line somewhere. I think you're also over-reacting in that you think this is somehow "fracturing the entire ecosystem". It's not. These are simple tools to replace at some point. *(Oh, I see - you think .NET Core is fracturing it, not the Node tools. So you're in favor of BREAKING all of the .NET stuff that is working on Windows just to get .NET working on Linux? Fuck that. Microsoft isn't that stupid.)* Besides that, all of the stuff built on top of Node.js was built for free by open source developers. Nobody is doing that for Microsoft. Unless maybe you're volunteering? Time to get to work!
R.I.P. [*].
No thanks, I'll just use nodejs then since it's at least battle tested and actually has high paying work available. Thanks for pushing me there, Microsoft.
FWIW I think it's a neat solution to a potentially messy problem, I still prefer if/switch for simpler cases but I have got some cases that could definitely make use of that pattern.
While you are waiting for it, look at this TypeSwitch library: http://blogs.msdn.com/b/jaredpar/archive/2008/05/16/switching-on-types.aspx
Thanks! I'm aware that all of us have a different approach to such topics, so I don't want to convince anyone that it's the best way possible. It's just another tool that might be a good fit for particular scenarios.
Storing it in a database, easy/clean serialization, efficiency. I was just curious because I thought you had a cool idea that I wasn't aware of. If your plan is to abandon ENUMs that's fine. Just not a good solution to this particular problem. 
Spam. Nothing with substance, nothing with arguments.
Not necessarily. The objects might be more complex (I.E. complete processes) with a common interface. Selecting one instantiates a process object, which is passed back to be used as a UI view model. One could even argue for worker objects in this example, if you expected to need a number of implementations of these items. Or were just *really* excited about code separation.
I believe you could achieve this by using powers of 2 for your enum. A = 2, B = 4, C = 8 Then you'd set your dictionary as: [A | C] = doX, [B] = doY, [C] = doZ While possible, I do still agree it's not the best, as (1) you'll have to OR together all of your options to produce the value to look up, and (2) using the bitwise OR to combine two options that are more intuitively used as a logical AND will confuse any dev that isn't well versed in working with binary code.
Which is fine. There is nothing wrong with that approach at all. I use it myself. But I was thinking that this was a solution that works with the existing ENUMs. Not a completely different approach as an alternate to ENUMs.
Phonegap.
As much as the customer can afford.
Definitely start at 90 an hour so they can haggle down to 75. You are taking the risk here because they can cut you off at any time. You may have gaps between clients/projects and need to have enough income to weather those.
I know my billable because I sit in on plenty of sales meetings, and have helped put together SoWs and proposals. As well one of my internal projects is our crm. As for salary, I get 36 plus benefits back of the napkin on benefits adds 8, so 44. Thats low for the wider American market (IMO) but I work in a smaller market and I do OK. If I were to contract on my own, I would bill 100 minimum, with a stretch of 120. Edit to be clear, my numbers are all in hourly rate, not annual salary. 36=75000 annual. Edit2: spelling
Thanks for the info. How was your experience transitioning to independent work from a contracting firm in the midwest? I'm looking to do the same thing myself. 
That's our hourly charge rate per employee. Covers their salary and overhead costs.
&gt; All this because OP doesn't prefer the syntax the language offers. Do you see any advantages of the dictionary-based solution?
Follow the advice of /u/ikilledem - you need to price over 100$ an hour, whatever number you come up with, in order to cover your costs for travel, overhead, meetings, yada yada yada. The "business" - needs regular updates in the form MSDN/VS tooling purchases, it needs an Azure license of some sort (presumably), etc. Be mindful of chasing dollars to the bottom to get business - you'll be chasing the SMB market (Small/Medium business) where they will try to convince you to go lower and where you'll starve. If you have strong skills and can prove it, it's very reasonable to charge 165$ an hour.
Thanks for the detailed response, ill consider one of these options! Im more for binary serialization.
Props to you for writing a good question and not providing unnecessary details. First of all, setting fixed margins is often a bad sign as you are dealing with various resolutions when writing UWP applications (this is the point - they are universal). x:Bind is a new attribute that differs from Binding (https://msdn.microsoft.com/en-us/windows/uwp/xaml-platform/x-bind-markup-extension). The most important difference is the default binding type which is OneTime for x:Bind and OneWay (In most cases) for Binding. When using bindings, you want to bind to properties in your ViewModel. If you just want to display uneditable (to user) data, you want to use OneWay binding while you want to use TwoWay binding if user can update the information (for example, the name). View has no idea when you call the Devices method and it won't ever get notified. This is because binding mechanism is supported by INotifyPropertyChanged interface. This interface exposes a NotifyPropertyChanged method that is called in setters of properties (https://msdn.microsoft.com/hr-hr/library/system.componentmodel.inotifypropertychanged(v=vs.110).aspx). There are also collections such as ObservableCollection (MSDN: Represents a dynamic data collection that provides notifications when items get added, removed, or when the whole list is refreshed.) that help us in the process. In order to make your program work correctly, in your ViewModel, declare an ObservableCollection&lt;Device&gt; and in your Device class, implement INotifyPropertyChanged interface and call NotifyPropertyChanged method in all properties' setters. This way, when you bind to Devices and Devices collection is modified (see above what constitutes as modified), internal mechanism will immediately notify the view which will see that it was modified and refresh the data.
Thanks for the write up, it re-enforced what I thought I knew about bindings. I apologize as it looks like you started writing this before I edited my post. In my case the bindings were setup right and working, but I have NavigationCacheMode.Enabled and apparently that is stopping the page from refreshing. I use Template 10 which provides the base layer for things like INotifyPropertyChanged so that's taken care of, but I need a way to force the NavigationService to update only when a property has been updated. I'd greatly appreciate it if you have an idea on how to accomplish that. I've tried triggering NavigationService.Refresh() but to no avail. 
Sorry, I've never used that template, I used various MVVM frameworks.
$150/hr off site, $200/hr on-site. I have a 4 year degree and 15 years of development experience. 
&gt; doesn't require **learning**... Furthermore, you're lazy. It's good for programmers, but don't be lazy about learning shit. Take *a fucking day* and learn about gulp and bower. Done. You just expanded your resume and you know some extremely useful shit that is already built for you. You then also have a way *out*, if at some point you want to move away from Microsoft-only tech.
Nobody maintains that. Are you sure that you want to risk your projects on stuff that nobody maintains? No, it's not easier than installing Node and running 2 commands because you are using shit that nobody else is using and when you find that you have to do something outside of what that one tool does, you're going to be scrounging around again and not finding many options.
I charge $75/hour because most of my projects are just small time ecommerce shops and anything higher means I don't get the work. It's just 5 hours a week or so in the off hours from my FT job so I don't worry about it too much.
Look at Java enums. They are actually quite nice and I sometimes wish we had it's syntax, though I will do it manually as necessary.
Unless you are taking a survey, the intuitive scale is purely opinion.
If you are using CodeRush, just hit Tab for the next reference.
It won't. Boxing should only occur with the old non-generic dictionary.
Why host it when we have Roslyn?
It's not maintained much because there is not much to do and because Web.Optimizations was added to asp/MVC. Now it might get a second wind. And being able to checkout and run a project with no dependencies is simpler than using node. Edit - it also doesn't risk my projects at all, the worst case is I have to replace it with something else, even a naÃ¯ve implementation from scratch wouldn't take long.
Using resharper only affects the individual using it. This is adding a dependency (and complexity) on node for everyone working with a project.
.net core isn't going exclusively to the cloud, it works just as well on intranets. Corporate software is going to make up the bulk of .net code for the foreseeable future.
The problem is interoperability with all the js/CSS libraries. I can see the case for using a common repository, but the implementation is lacking. The more I look into it though, the more I'm leaning to staying with nuget packages and/or copying the files I want into the project.
Just a curiosity. How did u all start? I am already under my gf insurance and I am doing full time salary. I have been thinking about getting out either doing 1099 or w2. 
In a way, yes. The pull request triggers a build. The build runs the Roslyn based code analysis rules, which can trigger build warnings or errors. The build result is linked back to the pull request. If I recall correctly, our current stack is Jira + Stash (git) + Jenkins.
OK. Check out IdentityServer. They have examples, but none that have external OAuth support. I see no reason why it couldn't be added. The OAuth private communication should happen in your authorization server, not the client.
I see you have posted this question around in a lot of places, so I will assume that you are asking this genuinely. [Removed] EDIT: Oh nevermind you are a troll. Sorry everyone for almost feeding him. 
You have gotten into a semantic wormhole more than a technological one. First, Web API, and anything you write for it, will mostly be able to be ported over to Core just fine. Web API from ASP.NET is a framework they have to manage service calls. In simple terms, the MVC framework for core will handle both view requests and web service requests but syntactically will look almost identical to 4.6 from what I can tell. Previously, MVC and Web API were considered different frameworks. So long story short, Web API is not "dying" per se, but another framework that is the planned future for .Net web is utilizing it and wrapping it into it's MVC framework. No matter which one you do, your code will not be stale for the foreseeable future.
Nowadays I'd go for JSON-RPC web services using ASP.NET Core.
Yeah I meant current Web API is getting outdated, with the Web API in the new ASP.NET Core. So either way, WCF is not the way to go, right? Better to make a Web Service in ASP.NET Core?
Remote work is fairly rare in Denmark, where I'm located, but there is a few every once in a while. I would recommend signing up with on of the consultancy houses, who will match your competences with projects and pitch you to clients. The contracts are usually 6 months to begin with, but you will often get them extended (if you have the skills, which it sounds like you do). Take a look at ProData (http://www.konsulenter.dk), Mind4IT (http://mind4it.dk), Zenit (http://www.zenit.dk), or some of the groups on LinkedIn for the same purpose (I use "Danish Freelancers (For IT professionals and the like"). NB: I got my current gig through ProData, and am signed up with most of these, but other than that, I have no affiliation with the companies mentioned.
Go with WEB API until Core goes through a few versions. I did WCF for web services and it was too klugy to setup even the simplest things. I then went to Nancy which was way better and simpler. When WEB API was released I tried it out and saw that it followed MVC conventions and it was as easy as Nancy so I switched over to that. I am using WEB API for both XML and JSON endpoints. The only thing with WEB API and Nancy is you don't get WSDL generation like you do with WCF. So you'll need to find a library to help you with that if you need it.
What specific problem are you having?
Does x:bind work with WPF?
Bind to a bool property of your ViewModel and use a ValueConverter to convert the Boolean to Visibility property. 
&gt; Monolith Thanks I will fix this misspell. Today I hope I will publish 2 part.
It is an introduction. Today I will post next part. I hope you will find next part more useful
I honestly think MVVM and WPF really over complicates basic stuff like this (compared to using web technologies like React.js) and does a poor job dealing with typical issues you run into like this that MVVM is unclear about. YMMV with what I'm explaining and might be overkill for smaller projects, but when you want things fairly decoupled and don't want to be repeating code left and right, I think it's worth it. This is more of a run down of some techniques that could help in common situations like this. I'm running through this kind of fast, so ask any questions if something is unclear. ~~~ So, in this scenario, if each set of controls for each weekday are the same, I would create *one* View that would contain the controls for *one* weekday, and likewise *one* ViewModel for that View. Let's name it `WeekdayView` and `WeekdayViewModel`. To display each of the `WeekdayView`s in your window for example, you can use a `ItemsControl` to "template out" each of the weekday controls if they are all designed the same. If you're looking for a good MVVM library, I would strongly recommend using [CaliburnMicro](http://caliburnmicro.com/). And as explained in this [StackOverflow answer](https://stackoverflow.com/questions/7190137/different-view-depending-on-contentcontrol-caliburn-micro), it would look something like this where the `WeekdayViewModel`s are in an `ObservableCollection` property on the window's ViewModel called `WeekdayViewModels` . If you're using a different MVVM library, it probably has something similar to this feature of composing ViewModels within ViewModels which can have specific View to render with as well. In CaliburnMicro from the StackOverflow post, it would look like this: &lt;Window ...&gt; &lt;ItemsControl ItemsSource="{Binding WeekdayViewModels}"&gt; &lt;ItemsControl.ItemTemplate&gt; &lt;DataTemplate&gt; &lt;ContentControl cal:View.Model="{Binding}" cal:View.Context="Weekday" /&gt; &lt;/DataTemplate&gt; &lt;/ItemsControl.ItemTemplate&gt; &lt;/ItemsControl&gt; &lt;/Window&gt; ~~~ Now to deal with changing properties from your ViewModel, you can create an attached property on your View that you can databind the `VisualState` property on the View to and from your ViewModel. This allows you to basically change a `string` property in the your ViewModel that allows your View to use `VisualStates` to change properties on controls. Using this helper code provided in this StackOverflow answer works just fine in practice: http://stackoverflow.com/a/6002107/809572 In your ViewModel, you would define a `string VisualState` property. Then in your View, you would define a set of VisualStates that change the properties of controls depending on what `string` value the `VisualState` property is set to in your ViewModel (I have some example XAML below to get started). In your case, I figure you probably have the `SelectedValue` or a similar property on your `ComboBox` bound to a property in your ViewModel. In that case, I would make the property backed by a field so that you can add code in the `set { }` block to change the `VisualState` depending what value the property is set to. A naÃ¯ve example would look like this (and FYI you can omit `INotifyPropertyChanged` boilerplate by using the [PropertyChanged.Fody](https://github.com/Fody/PropertyChanged) package): [ImplementPropertyChanged] public class WeekdayViewModel { public string VisualState { get; set; } private string _weekday; public string Weekday { get { return _weekday; } set { _weekday = value; switch (value) { case "Monday": VisualState = "Show"; break; /* ... */ default: VisualState = "Hide"; break; } } } /* ... */ } ~~~ Arguably, the XAML for dealing with VisualStates is really verbose, but it keeps you from having any code in the ViewModel from explicitly controlling the View, and allows you to easily change the View without having to edit. It's then very easy also if you want to open up Blend and design smooth transitions between VisualStates instead of jerky, immediate transitioning between states. &lt;Window ... xmlns:local="clr-namespace:MyNamespace" local:StateHelper.State="{Binding Path=VisualState, Mode=TwoWay}"&gt; &lt;VisualStateManager.VisualStateGroups&gt; &lt;VisualStateGroup x:Name="Common"&gt; &lt;VisualState x:Name="Show"&gt; &lt;Storyboard&gt; &lt;ObjectAnimationUsingKeyFrames Storyboard.TargetName="MyControl" Storyboard.TargetProperty="Visibility"&gt; &lt;DiscreteObjectKeyFrame Value="{x:Static Visibility.Visible}" KeyTime="0" /&gt; &lt;/ObjectAnimationUsingKeyFrames&gt; &lt;!-- Use more `...AnimationUsingKeyFrames` elements to modify more properties --&gt; &lt;/Storyboard&gt; &lt;/VisualState&gt; &lt;/VisualStateGroup&gt; &lt;VisualStateGroup x:Name="Common"&gt; &lt;VisualState x:Name="Hide"&gt; &lt;Storyboard&gt; &lt;ObjectAnimationUsingKeyFrames Storyboard.TargetName="MyControl" Storyboard.TargetProperty="Visibility"&gt; &lt;DiscreteObjectKeyFrame Value="{x:Static Visibility.Hidden}" KeyTime="0" /&gt; &lt;/ObjectAnimationUsingKeyFrames&gt; &lt;!-- ... --&gt; &lt;/Storyboard&gt; &lt;/VisualState&gt; &lt;/VisualStateGroup&gt; &lt;/VisualStateManager.VisualStateGroups&gt; &lt;/Window&gt; ~~~ I've had to implement a fairly good sized WPF recently and can try to help if you have any questions.
Set the `ASPNETCORE_ENVIRONMENT` environment variable to `Development` on your system.
razor = render html from C# models. yes you need javascript. find an online course on JS and you should be good to go in couple of weeks :)
If you know the shape of it you can send it as a Json body using the FromBody c# annotation. You could send it as a list of items or as a dictionary. I think that you'll need to find some commonality so that your endpoints can consume it.
I'd like to know this as well. Maybe you'd want to start a new thread since this thread is a day old, and I think we're at the end of the bell curve of new viewers.
Maybe look on TypeScript, it has more clear syntax than JS and it more like on C#.
This is only for mobile apps?
You have no proof of that. You have to consider it, because that's how Microsoft always works. They leave hooks to change what they want later. Saying that "you know" that they aren't going to build their own toolset for web dependencies is like saying that you know the future. You don't. So, you have to consider it as a possibility because it is possible.
If you go typescript I'd recommend the typewriter extension which will convert and sync selected cs files into typescript. Useful for custom validation or enums.
&gt; It's not maintained much because there is not much to do You're not looking hard enough. How's it working with Angular? TypeScript? SASS/LESS? Do you get working source maps for everything? Does every dev need a special extension of some sort such as to generate CSS from SASS/LESS? Does it optimize images for you? Does it optimize CSS by removing duplicate rules? Can it pre-gzip things for upload to Amazon S3? Can it generate static HTML from Jade? Can it create bundle files that have a unique string based on the size of the bundle? Or...more likely...are you just not using any of those things? If so, you're living in the past.
Pass 'true' (end response) as the second parameter of Redirect(). 
Don't know where you are as far as your development skills but I moved from a LAMP stack to ASP.net and I just went straight to the source to get up and running. http://www.asp.net/get-started 
After the redirect, try this: return context.Response.WriteAsync(string.Empty); 
I don't know. I don't use Mac OS X, and it's a system depending thing.
Adding this return statement causes a 502.3 (Bad Gateway) error to be thrown. It doesn't appear that it helps the redirect fire as I'm still seeing the CallbackPath in the address bar.
Don't go with WCF. Recently I ask this a similar question regarding Web Services myself: https://www.reddit.com/r/dotnet/comments/4rb0ee/which_technology_to_use_to_make_web_services/
Please give more details. Did you placed 21 controls manually or maybe ItemsControl? Does each day have same choices? If yes it would really make it simple. Does your logic reflects two texboxes or is it just a visual effect? There are many places where this can be not so trivual. My suggestion is to bind something to textbox visibility use converter. In my opinion there shouldn't be a bool filed in VM that controls visibility of any control in view.
Just "shovelling the trash" to a new level surely doesn't help, you're right about that. A controller is a high-level abstraction that routes your incoming requests around in your application. Usually it either returns a dataset (XML/JSON/etc) or a View that gets built with a dataset depending on some decisions. The controller knows what to do, but not how to do it: - It should ask the security layer if it's allowed to do something (built in MVC) - It should ask the validation layer (often it's the model itself in ASP.Net in MVC) if the posted data is valid and reroute if it isn't. For more complex validations you can create Validator classes. - It should ask the next layer for the viewmodel it needs, but it shouldn't have a clue where that data comes from, what the table structure is or what the API looks like. I usually separate it into three layers: - A layer that handles the request for data and returns the view model, it will gather the needed data from different sources, it still doesn't do the actual fetching so it doesn't know about API's or databases - A layer that builds that data into a view model - A layer that maps your tables (entities), mostly for CRUD type actions and simple selects. This layer does know about the underlying structure of course. For API's there's a slightly different type of class but it should map in a similar way, a class per API You can do it in a different and better way if you want, it really depends on the application you're building what's a good fit for you. But the most important thing is that each layer does one single thing for your application and what's left for the controller is high level routing. Every function should look really small and readable by itself, unless you really can't avoid it (like building a simple but really large ViewModel). It basically boils down to applying SOLID principles in a predictive way. You want the next developer to find the same layers below every Controller every time. You want the functions inside those classes to be small and easy to read. No mix of higher level decisions with lower level boiler plate. If a class knows how to do something it only knows how to do one thing, all other things should be known by other classes. You're talking about removing the logic from the Controller to another layer. It kind of boils down to what kind of logic it really is. It shouldn't understand what a logged in user is, it should just know that if there isn't a logged in user that it should return you a AnonymousProductView instead of a AuthenticatedProductView. It should handle the routing of that call in as little code as possible, it should not start doing some kind of nested if...else if...else tree where it parses the inside of your UserAccount to figure out in what state the user is.
Ref: [Example](https://github.com/Azure-Samples/active-directory-dotnet-webapp-openidconnect/blob/master/WebApp-OpenIDConnect-DotNet/App_Start/Startup.Auth.cs)
The Netherlands is pretty short on developers, I bet some companies would take it for granted. At my current company some are remote but it's exceptional or in extreme need of devs. Pro: you speak Dutch.
&gt; It shouldn't understand what a logged in user is, it should just know that if there isn't a logged in user that it should return you a AnonymousProductView instead of a AuthenticatedProductView. It should handle the routing of that call in as little code as possible, it should not start doing some kind of nested if...else if...else tree Here you are proposing one *if* in a controller but other *if*s in a deeper layer. What if we have not a binary *IsAuthenticated* switch but some enumeration of states, some of them require additional data lookup, more *if*s and for every case we want to reroute to different views (the logic is pretty complex). We can leave all these *if*s in a controller, fattening it, or we can shovel everything into the next layer (thinning out the controller making it basically a redundant layer).
First of all, does it have to be a service? Is it self contained with it's own database etc? Secondly. Service != web service. If you're polling a might be a lot better off with a service bus like [mass transit](http://masstransit-project.com/), not sure if it's .net core ready or not. Then your interface is defined by message POCOs. &gt;A lot of the demos I see of a microservice for Core are using a web project, but this doesn't make sense to me if the interface for this should be extremely minimal if anything at all. This sounds like something that should be a console app or the equivalent of a windows service. Is Kestrel just al You mean user interface? Web projects can just be webapi now. &gt;This sounds like something that should be a console app or the equivalent of a windows service. Is Kestrel just always "on" (unlike the app pool in IIS) so having a repeating loop in the app.run something thats an option? I'm yet to experiment with this but you should be able to run kestral from a windows service if you want.
Have you heard of FontAwesome? I added it to my solution and it gives you access to a whole host of icons. I believe this will help you add it: http://blog.codeinside.eu/2015/01/07/using-fontawesome-with-wpf/
&gt; I love topshelf, whenever I build a Windows service in c# it's always my first choice! It only saves a dozen lines of code.
How is recommending open source libraries hail corporate material?
FWIW I've had a few experiences in the past couple days that have made me rethink my rate. 2 new clients I quoted $100/hr and they didn't even haggle, which is probably an indication I'm low (and looking around here I'm thinking I'm low as well). That is somewhat intentional, my goal is to make my clients look good by paying less than bodyshop devs but getting more, but at the same time I think I could probably go to $120 and still do that.
Why not wcf?
Do your clients usually try to haggle you down in rate? How many clients do you work with at one time and what's your typical billed weekly/monthly hour amount for each? Thank you much for fielding my questions on this.
If you're starting out sales is harder then you think. It is easier to undercut business, get contracts at a lower rate then spending time basically selling. I tried doing the higher rate thing at first and can get it now but I'd recommend getting any business at first.
You probably need to remove the call to HandleResponse() as well. if (context.HttpContext.Request.Query.ContainsKey("error")) { context.Response.Redirect("/AccessDenied"); return context.Response.WriteAsync(string.Empty); } I have similar code in my application which works, except instead of redirect I'm altering the response url, in my case forcing them to be https. 
Something LIKE a windows service, yes. But a microservice in Core that can be dockerized run on any OS, most specifically linux. The examples I have seen that have done this using Core use a web project which is the source of my confusion. As shown above it WORKS, but it does not feel elegant. 
Like I said, there's no reason to use Kestrel. Just add a Main method to your class and compile it. Have it loop infinitely, do the work, and sleep as necessary. Then package it in a docker container and ship it.
&gt; First of all, does it have to be a service? Is it self contained with it's own database etc? No DB, but in the same idea, yes. &gt; Secondly. Service != web service. If you're polling a might be a lot better off with a service bus like mass transit Though I am not sure why you felt the need to clarify the difference between service and webservice, mass transit would require a bigger overhaul than what we have in mind. There is a functioning, working, webservice which we want to store the JSON response from at regular intervals. &gt; You mean user interface? Web projects can just be webapi now. I am not following your question here. In either sentence. The app, be it console or web api, can run headless; there is no need for a user interface. &gt; I'm yet to experiment with this but you should be able to run kestral from a windows service if you want. Yes I have found this as well! But I need this to be contained in docker and not restricted to just windows since a majority of the machines that may be running this will be linux.
Oh I am sorry, I misunderstood. This is where my web focus may be leading me astray. I will give it a try! 
Multiple users responding in nearly the exact same way, all mentioning the same library and all but one linking to its homepage - that doesn't raise any flags to you?
Upvoted for hackish.
Furthermore, I think it's hilarious that you're here asking for a .NET version of a tool that is written in Nodejs and yet you're still arguing with me saying that .NET tools do everything Nodejs ones do. LOL!
How long would it take you to write an application similar to the features you will use from CRM? I don't see how it would ruin your career. You don't have a dev career, yet.
What does Jade do that I can't do with visual studio or snipmate? For that matter, just how much time do you actually spend writing static HTML? It's very little for me.
Take a look at [Azure Service Fabric](https://azure.microsoft.com/en-us/services/service-fabric/). It's currently being ported to Linux.
That does seem promising. Reading through it, it seems like potentially overkill. Have you used it before and was it a lot simpler than it looks? Managing nodes and such seems as though there is some overhead.
This gives me so much more context, and I am now officially embarrassed at how dumb my question is now. Things make a lot more sense now!
Yes, razor for MVC, it's kinda like php in that you're writing code inline with your HTML. You can also use the razor syntax in your aspx pages by using bee stings: &lt;% DateTime.Now.ToShortDateString(); %&gt;
Impressive how much has already been ported - thanks for sharing the list.
Thankyou very much for all the good info here. Its going to take me a little time to digest all this as Im new to WPF/MVVM apps. Having a collection of viewmodels (per weekday) within the main viewmodel sounds like it might work well. And thats a good tip for the CaliburnMicro framework. I typically prefer to avoid frameworks initially so I can hopefully better understand what is going on in the app. III definitely look at CaliburnMicro for my next app though. All the info here has been very helpful.
If you can get six different views from an enum that has six values the switch should be in the controller. If you can get two or three different views based on an enum of 12 values you might want to condense it into a new enum. (UserState --&gt; UserView for example). If they need different data sets you will handle that in the layer directly below, the layer that creates the right ViewModel out of the right data. There should be no data lookup at all in the Controller since your ViewModels aren't generated there either. Also if analysis needs to be done on that data before a decision can be taken this decision should be created outside the controller function, the decision to return a certain kind of view or another (or to continue looking for another route) should be a one liner ideally. 
Oh. Yes I had a feeling that I jumped into ASP.NET Core too fast. But I think I like it better, at least as a beginner. I read it merged the Controller and ApiController class in a single Controller class. Plus, I think even in older WEB API, we could use any controller action as Web API, by making the action/function/method return a JSON string. Not sure why there's even another ApiController class. Probably to handle HTTP POST and GET differently, I still have to look into that.
what if you want to protect the media behind a specific authorization rule? 
You have to do that in C# or VB.Net anyway, so you'll have either of those under your belt as well. I spent 3 years at a company as a Sr. Dev and while there I also did Dynamics CRM development. With that on my resume currently I get a req through LinkedIn about every two weeks or so for Dyn CRM, and I'm clearly employed in my profile.
Thanks for this. I'm new to SSMS, but back in the day I used AquaData a lot. Some of these features are things I was missing, like the execute highlighted query.
I tried signing up and was unable to login with GitHub. Sent them a message with the error. No response. I signed up using Bitbucket, auth'd my repos and then got an error message. Sent them a message. No response. Opened several PRs which their tool never detected. Deactivated my account.
who knows. maybe they have been on vacation, or better yet, earning a buck.
I would recommend learning MVC and Razor along with learning Angularjs. You can mix these together over time but it will keep you relevant in learning new technology. The company I work at is pushing away from .Net and moving towards more Java services with Angularjs html front ends. 
Bit tricky, you need to implement range download http://tpeczek.blogspot.com.es/2011/10/range-requests-in-aspnet-mvc.html?m=1 Maybe you can find a newer article. We did this like 5 years ago and stil works. 
If you have an impressive resume for your 7 years, I'd say low. If you have generic experience I'd say slightly low. I'd seek out negotiating a higher rate, sign on bonus, or moving reimbursement. 
Did you call `services.AddOptions();`? It seems to be required per https://weblog.west-wind.com/posts/2016/may/23/strongly-typed-configuration-settings-in-aspnet-core
$115k is a decent "average" salary, but your question is incorrect. It's not about how many years of experience you have but what skills you have. There are folks with 2 years of experience that _are_ senior developers (despite years of experience) and there are "senior" developers with 15 years of experience that shouldn't be allowed near a computer. Likewise, there are developers that are _senior_ that get more and there are developers that are "senior" that get less. Be honest with yourself about where you fall on the spectrum and decide if what you are capable of is worth 115... or more... or less.
Its not that protected. I just need to have admin users have access. I don't care about downloading 
15K is a big difference. I am living in the South where 100K that I get paid is good cos rent is cheap down here. I've never been to Seattle, and the only reason I am applying and stuff is to be with my gf who is also moving there. So, I have not accepted the offer yet - what is a good way to negotiate and ask for a raise? I do not have any offers yet and I do need to move.
What is a good way to negotiate? I have not accepted the offer yet. 
It is amazing to me how different the salary level is based on location - I know the reason, but it still feels so odd on paper (as in, the west coast seems more impressive due to the higher salary when in reality you need that extra income to make an equatable amount). 
Do some research on cost of living in Seattle vs where you live now. See what your current salary translates to. In addition to that, go to sites like Glassdoor or others and you can enter pretty detailed profiles that show you reported salaries for people similar to you. Once you have a firm number, and evidence to back it up, talk to the recruiter and ask if there is any negotiation room in the offer. From there, its up to you and him to decide. Just make sure you have your evidence.
So true. A good saying I heard a while ago is that some people have one 10 years of experience, and some people have ten 1 year of experience.
~~That was it, thank you.~~ NM, that wasn't it. It made no difference. However, qlaucode put me on to the issue, my AppSettings section was not in the root.
[This is my full startup.cs.](https://gist.github.com/anonymous/a436bd0ae710be7c9431df957bd4c5f8) e: Actually, the issue was your first question, whether or not my AppSettings section was root, it was not. Somehow I got it shoved into the Logging root element. Thanks
Talk about how you're interested in the job. Provide several reasons. The talk about how exact experience from your history matches their needs, several items. Then follow up with something along the lines "this is a big change for me, the commute/cost of living alone changes my life. For me to accept this position I'd really like...." Most of this depends on how much hard ball you're willing to play. If you don't *need* the job you can flat up tell them "for me to take this job i need ______". Which is an incredibly powerful negotiation position. The downside directly stating **you need** pretty much bars a counter offer. They can counter, but it will be unlikely given how definitive I need is. Compare this to the previous where it concludes in I'd like. This shows there's room for negotiation. An in between position is "if you do ______, I will instantly accept." This is stronger than I'd like, but weaker than I need. The corollary to this is how heavily it forces your hand. If you make this commitment and they agree, you look like a jack ass if you back pedal at all. The last option is you counter with a different org's offer. ABC offered me _____, I'm more excited about your job... reasons.. I wanted to give you an opportunity to beat their offer". This is once again hard ball. You can even literally make up the other offer as a bluff. Just be careful as you never know who knows who that if you're too specific you might get caught in a bluff. Assuming the 2nd offer is legit, you pretty much most be willing to accept it (or none). It'd be an immense sign of weakness to accept their original offer when you stated you have a superior offer standing. A tamer version is you express you're interviewing with other companies but would immediately accept the right offer.
Glad to help you get started. Like when I was just starting out, it took me a while to find the best way to deal with typical issues like this as cleanly as possible in WPF.
Thanks. Thats a good advice.
i came across its name a while ago, but never seen it on any job listings...
They're probably better than average
I second this. You may also want to do some research on what the average salary is for that area with the the same title. 115 seems fairly low for your experience but if the average is spot on, you'd be taking a pay cut. Is the offer negotiable?
Im a senior in Seattle with 10 years experience. That is low. you can do better elsewhere, Seattle has tons of work. I did .net for years, I do scala these days
I wonder if the HR team felt they offered a higher salary than the average '.net developer' instead of 'senior .net'. 
and you probably won't, but it's heck of a good tool. 
I agree with this as well. I like to use: http://money.cnn.com/calculator/pf/cost-of-living/
It looks very nice but it's not open source. I am a bit worried about adopting my systems on closed source app framework nowadays, especially since the target of this framework are long lasting systems.
I think what you're looking for is the concept of Row Level Security. The easiest way to implement this is often to have one or several RLS-fields in the tables you want to protect and use those to specify what users / usergroups are allowed to access the data.
/u/TambourineMan8 Glass Ceiling lists average at about 110k. http://careers.glassceiling.com/jobs/seeker/salary?skipDeviceDetection=true Salary is like art, it's only worth as much as someone is willing to pay. 
When requesting the page, the C# "stuff" is done on the server, and the results are sent to the browser. Any javascript "stuff" is done in the browser, and doesn't affect the server. You can therefore use C# to generate the javascript, but not the other way around. If you need to interact with the C# code from your javascript code, you'll need to make a new request to the server using AJAX
The framework includes features that fulfill the tenants of a robust microservice architecture. I think it currently runs Azure SQL Server. For a demonstration, I recommend the following [video](https://channel9.msdn.com/events/Build/2015/2-640) or searching channel 9 for demos and walkthroughs. 
Yes, that is always a concern with any closed source software, but I would also encourage anyone to take a look at it for ideas on how a Microservices architecture could be implemented. 
You would need to use the WebView control. To execute javascript you would call the WebViews invokescriptasync method. 
Ah! Thank you. I will look into this WebView control. 
I didn't think about NDepend in this case. But you are right it is a perfect tool for this. Thanks :)
Repost: https://www.reddit.com/r/dotnet/comments/4rpjrx/how_to_return_301_response_instead_of_login/
Look for a property on the command called "BindByName" or something, and set it true. ODP gets weird about parameter binding, and since you are adding the Paramus with their name and not their index, I would set this just incase
yes - I made a typo in the original title ;) 
Hope you will like it :)
You could be shooting your future self in the foot. Eventually you may decide that you want certain routes on your API to require authentication. This could be via client certificates (nice for server-to-server calls) and/or a token system like JWT. With that you would want to remove the "Authorize" attribute from your API methods and replace it with a different attribute that can deal with those kinds of authentication (and produce the correct response codes). It's not a lot more code than you wrote there, but it's more direct (a.k.a. less hacky). 
Code search. Finally. 
I think it is. But I am not sure how to proceed negotiations. I kind of blew my chance. What happened was - I had 2 interviews that day. The first offer came after the interview and the recruiter said what salary would I take to cancel all other interviews (I had told him about other interviews). I said I don't have a number and would like to not cancel interviews as they were all final interviews also. I haven't heard from any other companies yet except this one. And the offer they sent has a 7 day expiry date. I need to come up with something quick and I feel like I lost my position to negotiate. Any advice how I should handle this from here?
Glassdoor and Indeed show 110,000 for Senior .Net developer. Confusing.
This link says for $100,000 here the cost of living in Seattle is $130,00. I feel like taking a huge paycut :(
Using a task and a cancellation token is exactly what I was going to suggest. I don't know if that's the best way either but that's what I've done in the past for something similar.
Technical debt: a term for older solutions the managers want to replace regardless of if it functions well or not. See windows 10
At least you can make custom item stauses. 
Squash merge is really nice
A lot of good stuff in here but Package Management is definitely a welcome addition.
i'm hoping you can turn it off, we already have an app for that (ProGet)
If you are asking whether it's production ready in terms of performance, yes, Kestrel performs several times faster than the NodeJS server.
Call the library from its own thread and abort the thread if you reach your timeout. Using that mechanism, you need no cooperation from the library.
Everything is porting so fast. With introduction of .NET Core our community seems to be growing explosively. It's so exciting!
Do you have to enable it?
Yes of course, after it receives the JSON object it's stored in a file and loaded again when the server is not reachable. Since the JSON object can also be converted into a single base64 string, we can not only email them the file, but they can use an text-field in the client itself.. My problem though is about the actual server that replies to the request and that I need a version that can be distributed to our customer with its own licensing file and one that uses settings from our database..
&gt; If you need to distribute a licensing file for the licensing server, why not just distribute a licensing file for the original application? Because the server is supposed to handle access among of many machines and the license for that would include for example the number of allowed simultaneous uses. The client only cares if it received a valid license or has a non-expired license already stored, not how many other machines are active at the same time. &gt;I think you need to put your foot down with your boss, sometimes your job is to be the expert and say "This is a bad solution" I tried :/ I put my reservations and the associated problems and risks in writing and at least covered my ass..
I don't even know why we still have TFS installed, we switched to TeamCity for builds and Trello + YouTrack for issue / work tracking, and deployment is handled by TeamCity/Octopus/Puppet depending on the app. Having everything under one roof sounds nice, but it took TFS so long to even catch up with how easy it is to work with anything else that now it's still having to catch up on features.
Traditionally, you would treat the "customer" license server like a product that requires its *own* licensing using a separate key. So I'm going to assume that your licensing takes into account things like duplicate requests, if it's like our licensing then the license request contains some information to uniquely identify the machine making the request. Once that request is granted, the license cannot be requested again from a different machine unless the first license is returned (if using a floating license style system) or revoked in some way - fairly standard stuff. Okay, again assuming your system works a *bit* like this, the first thing to do is create a way of making that license request/activation possible "off-line", i.e by the requesting machine generating some kind of request code that can be manually copied (could be your JSON object serialised out and optionally signed so it isn't tampered with) and being able to accept the activation code (again, by pasting in some data or inputting a text file or whatever). Once you've got that bit done, the next bit is easy: Create a license type that authorises the generation of sub-licenses. So they'll install your license server and they'll have to activate that using the method above. You'll send them the activation response that's signed with your private key - don't give them this key. Instead, send a *second* private key that's shipped as part of the activation and sign the public part of this key with your own private key. This way if someone compromises your client's license server and starts generating licenses they shouldn't, you can just revoke *their* key but keep your own secure. You'll have to update your software to be able to check for the "nested" licenses, but this shouldn't be too difficult. Now, you should have a system whereby the client license server can be authorised to create licenses, without compromising your own licensing system. You could expand this further, by say limiting the amount of licenses it can actually generate (By product, or number or whatever), again just by signing the constraints with your own private key. Does that make sense?
In practice, Kestrel supports everything you need, especially if you're proxying it behind Nginx or IIS.
Thanks for the tip. I'll definitely check out MedallionShell too.
We use octopus too, RM in TFS has a long way to go. That said I do like build, work items and git in TFS. They still need work but they're pretty compatible with other thing IMO. 
Choniton is very lightweight. Hangfire was designed to operate in more of a server/client type architecture. Chroniton only needs a .NET reference. Hangfire has dependencies to SQL and/or other storage mediums. With Chroniton, consuming applications are 100% unit testable through mocking. Hangfire has some features such as a recurring job which cannot be easily mocked due to the static method calls. In Chroniton, your scheduling is completely customizable. Do you want a job which retries failures because of some connectivity issue and retries them in such a fashion where they exponentially back off so as to not bog down the rest of your infrastructure? You can do that with Chroniton. In hangfire, your jobs are really just delegates with no intrinsic state. While you can create jobs easily in Chroniton by passing a Func, you are not limited to that. In that regard, jobs in Chroniton are more like jobs in Quartz. Unlike Quartz though, the context of your job is completely strongly typed. The one drawback to Chroniton is that it does not have any serialization yet. If you need to track jobs between instances of your app running, you'd need to build that out. Serialization features are planned. For many implementations though, that is not a deal breaker. Finally, Chroniton has builds for both traditional .NET and .NET Core.
Tay was easily one of the funniest things to happen this year. I'm sure that's not what they were going for, but I appreciated the laugh anyhow.
Those bot platforms don't have AI baked in. Developers will have to implement those themselves. 
It's a completely different flow when using social logins. In the blog, he's using Postman to get access tokens. When using social providers such as Facebook, Twitter, etc., you bounce the user to those providers' websites to do a bunch of token handshaking. There's no middleware analogue to API authentication without a web browser that I know of. You'd basically have to fully manually authenticate with each social API using credentials stored against a principal (to your applicatio) user. 
I am investigating Azure, Microsoft has been ramping it up heaps now, its quite amazing, but expensive. I just made a mobile app that OCR's text, using Project Oxford, then uploads it to an Azure db, synced wit the phone, and will eventually come out on a website. 
Ah then cool
Check out [Auth0](http://auth0.com). Their service and documentation might be exactly what you're after. 
There's LUIS which is directly supported by the framework. So it seems like people will lean towards using LUIS. And tbh, LUIS isn't all that bad. It's fairly simple to get the bot trained well enough to work in most situations.
Awesome, do you know how xD
i think the "dotnet new -t xx" only has web template, but try "yo aspnet" Explained in this post: https://blogs.msdn.microsoft.com/webdev/2016/06/27/announcing-asp-net-core-1-0/ 
Thanks to all those that replied. For anybody following this thread here is what I ended up doing. I bound the textbox visibility attribute to the combobox selecteditem value and used a converter to do the translation from combo item to desired visibility attribute. Here's the relevant code:- &lt;Window x:Name="winMain" ... xmlns:vm="clr-namespace:ApplicationName.ViewModel" xmlns:md="clr-namespace:ApplicationName.Model" ... &gt; &lt;ComboBox x:Name="cmbName" SelectedItem="{Binding cmbName_SelectedItem, Mode=TwoWay}" ItemsSource="{Binding Source={md:EnumBindingSource {x:Type md:EnumVariableName}}}" /&gt; &lt;TextBox x:Name="tbName" Text="{Binding tbName_Text, Mode=TwoWay}" Visibility="{Binding cmbName_SelectedItem, Mode=OneWay, Converter={vm:ComboBoxVisibilityConverter }}" &gt; &lt;/TextBox&gt; public class ComboBoxVisibilityConverter : MarkupExtension, IValueConverter { public object Convert(object value, Type targetType, object parameter, System.Globalization.CultureInfo culture) { return ((ComboItem)value == ComboItem.RelevantValue) ? Visibility.Visible : Visibility.Hidden; } public object ConvertBack(object value, Type targetType, object parameter, CultureInfo culture) { // Not required return false; } public override object ProvideValue(IServiceProvider serviceProvider) { // ProvideValue allows you to reference directly from XAML code (dont need to also specify in resources) return this; } }
Yes it is definitely a steep learning curve! Ive added my final solution as another comment in this thread. Have also found this Udemy course was great to get a good overview of MVVM development.. https://www.udemy.com/mvvm-in-wpf-survival-guide-from-a-to-z/learn/v4/overview
This doesn't address your exact question but might I suggest using something like [Portable.Licensing](https://github.com/dnauck/Portable.Licensing) or [KeyHub](https://github.com/imazen/keyhub) instead of rolling your licensing scheme? They already have a lot of underlying infrastructure taken care, so you can focus on tweaking it to fit your needs rather than implementing all the functionality from scratch.
In the new MVC, there is no distinction between MVC and WebAPI. You just create a MVC project and return data instead of a view. Their documentation. Their [documentation](https://docs.asp.net/en/latest/) should show you how.
Read [this](https://lostechies.com/derekgreer/2015/11/01/survey-of-entity-framework-unit-of-work-patterns/).
Great article but it confuses the hell out of me. I have no idea what he's talking about in most of it. :/ Apparently I need to start at a more fundamental level.
I read in a few places that simply injecting `DbContext` is a good way to do it for very simple apps. [This article](https://lostechies.com/derekgreer/2015/11/01/survey-of-entity-framework-unit-of-work-patterns) linked above is an example. My app is eventually going to be pretty complex, so I'm trying to do it the right way from the start. If I do use this method with Autofac, what would be the correct lifetime? I'm not sure what's ideal to use.
viruss = viruss.Where(v =&gt; v.Species.Contains(searchString) || v.acronym.Contains(searchString) || v.virusnames.NamedProperty.Contains(searchString))
v.virusnames is reading as an Icollection. Since this is a many to one relationship, this method does not work. 
Was this called Perspex at one point?
[removed]
I've been intrigued with this for a while but haven't had time for a play. Has anything shipped on an App Store using it yet?
I did find a fix. The answer is posted in the OP. 
I'm on mobile, so the repo description is grey and not immediately apparent. It doesn't say so in the README, which is where I would have expected it. But thanks for being condescending, asshole.
For those like me who didn't know about the project, here is a nice 3min demo (under their former name "Perspex"): https://www.youtube.com/watch?v=c_AB_XSILp0 I wonder if it's able to integrates with "full solutions" that include models/controllers ... Otherwise it looks like a nice tool for prototyping. Again, I wonder how much can you re-use from the prototype into a full solution. Thanks for sharing.
I don't want to sound mean, but if you can't find any demo or examples you're not looking hard enough. I learned it with channel9 event talks and just googling Entity Framework Code First. Also dont confuse EF with your webservice (e.g. web api or wcf). 
You'd use a per-request lifetime scope, AutoFac will deal with disposing of it. The top comment on that article seems to echo my opinion that "DbContext is itself a unit of work" and I try not to create abstractions of abstractions, especially if they end up as *leaky abstractions*. This [SO Answer](http://stackoverflow.com/a/10588594) talks about using a factory to control lifetime which is fine, but I don't think it adds anything useful in the situations I have experience with. What he goes on to talk about is the [CQS or command-query separation principle](https://cuttingedge.it/blogs/steven/pivot/entry.php?id=91) which I am very much in favor of, but it seems like you need to sort the basics out first and if you can get a service with an injected DbContext and [CQS](https://lostechies.com/chrispatterson/2014/01/03/crud-is-not-a-service/) style methods then that's a good start. It's good to try and start of with a good architecture, but don't get too lost over-thinking it, you can always add more abstractions later on if you find a good reason.
The seemingly random bold and italics made this so annoying to read that I stopped. 
I use bold from time to time, mostly because many people (including me) at first scan the text by looking for such sentences and then decide if it's worth to read the whole. I'll keep that in mind and try to use it in a better way then :).
I can only agree with you. I had every intention of reading the whole thing but I barely made it through the intro.
Wow, what part of that came across as condescending? FYI, I was on mobile too when I clicked that link and it was plain as day that it was formerly called Perspex. Besides , with such a question...I assumed you didn't bother to click the link and i merely pointed out that information was available on Github. Take a chill pill bro.
I related quite a bit to this article. I've got my own fledgling open source project. https://github.com/leosperry/Chroniton working on it these last few weeks has brought me more joy than I'd expected. I even dreamt in code for the first time I a long time.
In your defence, it does say readme on the page which redirects to the page with a video on Perspex. Not sure if different browsers show different things
Excellent article. Thank you very much for taking the time to write it. I'm quite keen of the idea behind CQRS/ES (I think) but have had a hard time of grasping some of the details. A few things clicked when reading your article. Very much looking forward to part 2.
[Here's a long article on the subject](http://mehdi.me/ambient-dbcontext-in-ef6/). I haven't had the chance to test his proposed solution, but he makes some valid arguments, although maybe a bit purist. 
Perspex was a trademarked name for a piece of software somewhere so we changed the name to avoid any problems in the future.
[removed]
there are probably hundreds, but... they are pretty tailored for the deployment type. are you publishing local? MVC4/5/6 (and all code first?) I mean, the default one pretty much covers it all: http://www.asp.net/mvc/overview/deployment/visual-studio-web-deployment/deploying-to-iis 
[Not too hard to find](http://lmgtfy.com/?q=mvc+iis+visual+studio+2015)
I tried that one but i keep getting an error when ever i try to validate the connection when i try to publish the project. The error : Site 'Default Web Site' does not exist. and yes i'm trying to publish local
[*cough cough*](http://lmgtfy.com/?q=You%27re+a+twat)
[Here's another one for you](http://lmgtfy.com/?q=How+to+stop+being+a+twat)
No need to be an asshole about it - and yes, posting the LMGTFY link instead of the actual link means you're being an asshole. LMDATFY (Let me de-asshole that for you) &gt; [There's a nice tutorial here](http://www.asp.net/mvc/overview/deployment/visual-studio-web-deployment/deploying-to-iis), or search for `mvc iis visual studio 2015` for others if that's pitched a little too high/low for you.
It's not my fault that what you asked for is seriously a simple google search away.
sub for compare ok no
Instead of providing a method call `Scheme` with an enumeration, you might want to use some properties for that, e.g. `Scheme.Ftp.&lt;rest&gt;`. That way you can provide dedicated interfaces depending on the scheme, and prevent creation of invalid URIs. FluentUriBuilder.Create().Scheme(UriScheme.Mailto).Credentials("dsd", "sdfsd").QueryParam("asdgfsd", "asdf").Port(5) Creates invalid Uri: mailto:dsd:sdfsd@localhost:5/?asdgfsd=asdf
Those are sort of some words sort of arranged in something sort of like a sentence. Good job?
Great idea, thank you! I'm going to implement it in the next version.
bullshit. This guy injects exact dbcontext object instead of IDataContext abstraction. That's why his example not the good point to get rid of repo-abstraction layer. UPD: He should at least use DI princible + IoC frameworks to control context object lifetime, etc. 
You can buy perpetual :-)
Scott Hanselman could beat The Undertaker in a no holds barred match!
....no.
I'm not familiar with this specifically, but generally you handle this kind of thing with an event handler.
You're not the boss of me
https://en.wikipedia.org/wiki/Hasty_generalization
Which is $152.00. I could buy Beyond Compare more than twice for that money. I get that it's a cool algorithm, and a cool implementation, but $152 is too expensive for such a specialized tool. And to answer the question that begs itself, I could see buying it for 10% of that price. Also, subscriptions is a bad business model for this kind of tool. TBH, the only possible business model I could see happening for this kind of tool is getting bought by Microsoft - and I sincerely wish you good luck with that (if you're pursuing such a thing). EDIT: From your site: &gt; One easy payment and SemanticMerge is yours forever. Includes upgrades for one year. 20% "Product Maintenance" available after 1st year to upgrade to new major versions. This is not really perpetual.
Seconded. 
It's not a great super-in-depth technical article But for a typical "Which do I need?" approach, I'd say it covers the right amount of detail. GUID's take up more space, for one thing - 128 bits for every database entry adds up. There's also still a performance hit in calculating the GUID (although, as mentioned, it's vastly reduced lately). In the real world, I find performance differences are negligible - but GUID's are less "user friendly" to both me and the user. As anyone who's had to do support over the phone will know, it's easier to work in simple integer ID's - especially when driving at 70mph down the M42 on your phone's bluetooth to your car stereo and trying to help a colleague debug something. IMO, GUID's are great for true distributed applications, or HUGE data sets (scientific results etc), but are over-kill for a typical application your or I are likely to create in a given day. Many, many times I've been debugging something and scribbled down lists of ID's on the back of last week's team meeting agenda. That's a lot easier to do when it's a 6-7 digit integer, rather than a 32-character alphanumeric.
See my blurb about propagating keys (and that one shouldn't), I may have edited it in whilst you were replying.
It depends entirely on your application. Not all ID properties are equal within an app either. Are you running on a single webserver and db server where you can expect the DB server to be available at all times and you don't have hundreds of writers trying to insert thousands of rows concurrently? Then auto int is probably fine. If you are doing something like writing to the DB async but still need to have the ability to track that object, then guids are pretty much your only solution. There are drawbacks with using large PK key sizes in most DBMSs : your indexes will be correspondingly large. You can sometimes do both, and there are cases where this makes sense. 
Yeah I think the key propagation section was added while I was typing my own reply. I'd agree with that, but equally I still think the "GUID's every time" argument we often see (and I'm not suggesting you're a culprit) is invalid for many use cases
That's exactly why you don't want to do it. A really common class of security vulnerability is when you give out ID 1234 and they start punching in 1233, 1232, 1231, 1230 try to snoop at the other records for fraudulent reasons. A good application will prevent that, but security is never perfect (mistakes happen) and you can still infer things even if the full data isn't returned. I've seen criminals try to sweep millions of IDs based on a single one that was issued to them. YMMV :)
Use an auto-increment int as the PK and foreign key references , with a guid as a second ID for things like async editing and composure.
&gt; A really common class of security vulnerability is when you give out ID 1234 and they start punching in 1233, 1232, 1231, 1230 try to snoop at the other records for fraudulent reasons. Yeah, that's definitely not a database design flaw so much as an application flaw. The physical data layer should never be exposed in any way by the presentation layer to a user. This is also another major reason why some people (myself included) consider the requirements of a primary key to be: 1. Confidential - it should never be directly exposed in an application for the user to ever see or reference, OR 2. Multi-column - it should not be a single column key but rather a composite key which allows for a small part of the key to be made visible if necessary for user interfacing, while preserving a confidential half to prevent walking attacks. Of course, all different systems have different requirements. I saw GUIDs traded out of a very distributed system in favor of multiple integer column composite keys simply data was being sent over a very limited satcom connection and *sending two integers as text was amounted to less characters than sending 1 GUID!*
We changed from ints to guids to allow database syncing. Guids work great for that but they murder database indexes, inserting 10000 rows resulted in 80% fragmentation, can confidently say that using guids for pk is one of my biggest architectural mistakes. If you need to, use a sequential guid, gives you the benefits of guids without most of the indexing problems. In retrospect, there were better ways to solve that problem. Edit: I blogged about this last year [usesmallicons.blogspot.co.uk/2015/09/uniqueidentifier-as-primary-key-that.html?m=1](HTTP://WWW.CHANGEME.COM) Turns out it was 96% fragmentation!
I have to say, you've really developed an eye for API design.
I was actually really excited to read about the economics of modern municipal garbage collection, and the history of it over the past 50 years or so. Then I remembered which sub I was in =(
 &gt;Never store a GUID as a String Unfortunately this is exactly what Identity 2 did. I am a fan of GUIDs, but I prefer to store them as UNIQUEIDENTIFIERs, and not varchar(128). I would love to move or fork Identity 2 to a non-string GUID, but there is no way to do so (I tried). Apparently this issue involves the ability for a string to be used without being converted, whereas a true GUID as a GUID would have to have a shitton of conversion handling.
side note - I recently rebuilt my machine with some pretty decent specs expecting to see a decent boost in compile time. I was dismayed to see they stayed about the same. Then I realized I had left this setting enabled a while back. Reverted it to the default value and shaved about 6s off the compile time. It was was spending more time writing to the output log than actually compiling the code.
Doesn't perpetual mean you have a right to that license forever though, I've never seen a perpetual license include free upgrades forever as well as that's essentially a different product.
That's exactly what perpetual (or more commonly "for life") means. I've bought got at least three products with perpetual licenses (EmEditor, Collectorz.com - Movies and Collectorz.com - Books, off the top of my head). The "perpetual" license that SemanticMerge is just a plain vanilla license. Buy the product, and get updates for a year. In fact, it's probably less than a plain vanilla "buy" license, because usually plain vanilla licenses are good for all updates in the major version. Naming the "vanilla light" license as "perpetual" is a bit misleading, IMHO. Just call it "buy" license or "buy to own" or "buy to own with a free year of upgrades".
Take a look at this - it can help get the detailed error, rather than the (rather useless) "code 2". With that, you can start to debug it properly http://the-coderok.azurewebsites.net/2016/07/13/Resolve-the-Error-MSB6006-tsc-exe-exited-with-code-2-build-error-in-Visual-Studio-2015/
well, that's not my point at all. in this case the slowest thing is writing to the output window and it isn't even close. build set to quiet - http://imgur.com/L1GztIg build set to detailed - http://imgur.com/vZSDqz1
Possibly the worst powershell I've ever written but does the job: $str = Invoke-WebRequest -Uri "http://pastebin.com/raw/SWvZCkxN" ; $str -split("\r\n") | ForEach-Object {$filename = Split-Path $_ -leaf ; Invoke-WebRequest -Uri $_ -OutFile C:\Temp\Books\$filename}
But, are any of these worth reading?
Confirmed. This script works. Just change C:\Temp\Books\ to your preferred destination. Also, if your machine doesn't allow you to run PowerShell scripts like mine didn't, you may need to run this in PowerShell as an administrator: Set-ExecutionPolicy -ExecutionPolicy Unrestricted You might want to set it back to the default 'Restricted' when you're done.
Literally directly above this text he explains why he can't do that.
Or you can run PowerShell with a command line parameter, to enable it temporarily: `PowerShell.exe -ExecutionPolicy UnRestricted` There are 14 more ways to bypass ExecutionPolicy in the [first page I found describing them](https://blog.netspi.com/15-ways-to-bypass-the-powershell-execution-policy/)
The "millions" is the number of *downloads* of the books they're counting, not the books themselves. It's a little misleading, but not entirely uncommon. They are effectively giving away millions of *licenses* to download the books.
PS = Powershell?
Hey there, you've got a 403 on one of the links from your thank you for downloading page (/Download/Windows) -- the link is https://users.semanticmerge.com/documentation/ Edit: Not being able to provide proxy credentials during activation's also a bit of an annoyance (as in it's impossible for me to use the trial) :(
I did almost the same, but -MaximumRedirection 0 -ErrorAction Ignore to get the .Location of the redirects
What a shit title you used for the submission.
I like to learn by trying and expecting Do you have other suggestions ? thank you ðŸ’š
I'm not really fond of video... but this was totally worth it. Thanks for sharing!
.NET is backwards compatible (meaning something made for an older version will work on a newer one). This tradition breaks with .NET Core, so that won't do it, but the version that Windows Update suggests should suffice. 
Nope, he means that the new .NET core (technically .NET 5.0) is the first major release of the .NET framework that is not backwards compatible (since maybe the 1.1 days back in 2005). Up until version 4.6.2, you could run anything compiled for that version or earlier (back to 2.0 I think) without much fuss. Starting with .NET 5, this will not be true. Example: if I have .NET 4.6.1 installed, I can run apps compiled to 4.5.2, 4.5.1, 4.5, 4.0, 3.5, 3.0, 2.0 without having to install anything additional (sort of. The 3.5 and lower versions may require the client to install the .NET 2.0 CLR). If I run an app compiled to .NET core, I would have to install .NET core (version 5). Also, if I only have .NET core installed, I would not be able to run any of the apps compiled to the versions above.
&gt;Example: if I have .NET 4.6.1 installed, I can run apps compiled to 4.5.2, 4.5.1, 4.5, 4.0, 3.5, 3.0, 2.0 without having to install anything additional. Is that actually true, though? ".NET 3.5/2.0 support" is a Windows Feature that has to be manually enabled on recent versions of Windows even though it ships with .NET 4.5/4.6... Which actually winds up being a huge pain in the ass, at least in the case of my Windows Server instance in Azure, where it wants me to install the Windows install CD-ROM so that I can install .NET 2.0. So that I can use some shitty Windows tool. Really just amazing sometimes the choices that are made in Windows.
Thanks ðŸ˜Š May I ask ; is it a rule of Reddit to use the same title as the URL ?
No, but unless the actual title isn't the focus of why you're posting it, it's generally the one that makes the most sense.
https://blogs.msdn.microsoft.com/dotnet/2016/02/10/porting-to-net-core/ The link is describing how to port .net code to .net core code and telling some difficulties about it (still care, this was created at rc1 or rc2 times i believe). Also there are some namespaces listed which you should not use when you want to let your code be portable. Also some newer post which is also a lot shorter: https://docs.microsoft.com/en-us/dotnet/articles/core/porting/index
It's worth mentioning that you can run AspNet Core against the full framework - thus maintaining your lib comparability. Of course things that rely on Owin or similar or other older mechanisms might not work out of the box.
The way the new framework works is pretty different. You'll have a hard time migrating frameworks. On the other hand, targeting the full framework as someone said will enable you to use any package you want. And also move to Core only by updating the non compatible packages. 
The post had an error. The proper measurements are here: http://indexoutofrange.com/The-cost-of-garbage-collection.Part-2-rectification/
that doesn't seem to tell me anything about the video other than it's about asp.net and he's pointing out to certain content int he video.. your title suggestions is a lot worse imo
The link to part 1 didn't work.
http://indexoutofrange.com/The-cost-of-garbage-collection/ If you look at the href of the link on the Part 2 page, they mashed the current and previous URLs together.
"string" is an alias for System.String, however, in c# it is generally accepted as best practice to use this alias over the class. This is similar to using an interface in an assignment rather than the concrete class for the sake of abstraction. Consider that string is a keyword in the C# language itself, but you can technically call a variable "String" with no problem.
I've noticed most C# devs use 'string'. I know in Java you have to use 'String'.
You aren't going to believe this. I only know this because I've had this style argument many times with my good friends. One of them pointed out to me (about a year ago) that the C# spec recommended (at the time) to use `string` over `String` - I preferred `String`. I went through the most recent spec and I can't find that recommendation anymore. Hmmm. I love this topic/debate.
I don't know if there's any particular convention that most people use, but I always use `string` when I'm defining a variable or a method parameter, and I use `String` when I'm calling a static method like `String.IsNullOrEmpty (foo)`.
They are exactly the same
&gt; This is similar to using an interface in an assignment rather than the concrete class for the sake of abstraction. No its not. First of all, it's nothing like using an interface. Secondly, using an interface or a concrete class as the variable type is a far more complex decision than you make it out to be. Blindly choosing an interface has repercussions on forward compatibility issues (and to a lessor extent, performance).
&gt; I preferred String. So did I, but with the compiler warning the issue has been settled.
And be inconsistent with the vast majority of people who accept the default setting? There's not much point in that.
I think most people who like to use String instead of string probably have a Java background.
Yeah, same here for some reason.
Indeed. Using string or String make exactly zero difference. Interface/inherited class assignment has possible impact on which method will be called or which property setter/getter will be used.
Thanks
`string` isn't a keyword in Java, it is in C#. C# also has value types, so int and System.Int32 are exactly the same whereas Integer and int are different in Java.
Same here!
I think the point was that in the future the type that lowercase string aliases can change as long as it maintains the same contract, much the same way that if you use an interface, the underlying implementation could change without you even knowing. 
I do this too. I'm not even really sure why. It just feels right.
Int changes based on target machine. On a 64 bit build, int becomes int64. String only has one version but that could change. So if you want your program to not evolve, use int32, int64, and String. Sometimes you don't want to being up a legacy system and have it not change. 
It really doesn't matter. I prefer string over String and I also prefer int over Int32. 
Do they also change Int32 to int? Just curious (I don't use VS2015 or I'd just test it myself).
Based on your description, I think your structure could be simplified into 3 classes (no inheritance req or suggested in your case) Public Class User { [Key] public string Id {get;set;} public string Name {}//I'm going to stop typing out all the get/sets. public int OtherProperties {} public virtual ICollection&lt;WishList&gt; WishLists{} //foreign key down to wishlists } Public Class WishList{ [Key] public int Id {} public int OtherProperties {} [ForeignKey] //not req, but it helps the EDM out, increases startup performance public virtual User User{} //foreign back up to user public virtual ICollection&lt;WishListItem&gt; Items{} //foreign key down to WishListItem } Public Class WishListItem{ [Key] public int Id {} public int OtherProperties {} [ForeignKey] public virtual Item Item {} //foreign key to the product item (i assume. I would not suggest using "item" as a class, it is too generic) public virtual WishList WishList {} //foreign key up to Wishlist } - Having the Foreign Key ID fields are optional. You only need to use them if you want to control the field names (but i imagine you could just use the [Column] attribute anyway, never tried). Theres no difference in the select statement if you use the predicate on Wishlist.UserId vs Wishlist.User.Id, the EDM will generate "select where Wishlist.UserId = [x]" - However, if you DO need to user an int property for the foreign key, you MUST attribute them correctly (Add foreignkey("intPropName") to object prop) - if you wanted a list of items assigned to a person, you can query on the following: var userWishlistItems = dbContext.WishlistItems.Where(i =&gt; i.Wishlist.User.Id == [x]) - If performance is taking a bit of a hit (shouldnt, because EF will add all the appropriate indexes) you can add User to WishlistItems to jump a key right to the top, but you will need to assign it manually. var userWishlistItems = dbContext.WishlistItems.Where(i =&gt; i.User.Id == [x]) - MVC Scaffolding question: Not that I know of. The scaffolding tool is more for a starting point. Additional props added later would req rescaffolding or manually adding afaik Hopefully this was helpful to you! 
Same here.
Yes. They do that for all types that have a C# alias.
Method 2 IMO. Method 1 has a magic string. Method 3 just feels bad.
I tend to go for method 2 - dumping them into the model. Methods 3 and 4 would violate the separation of layers. :)
I'd go with 3, but with a helper between the view and the repository. I'd also remove "AsSelectListItems" from the repository, there is no need for the repository to be aware of ui details like that. Try putting an [interface](http://flukus.github.io/2015/07/24/2015_07_24_Simple-Abstractions/) on the data items.
Thanks again
Aight, let me know how it goes! 
Can't wait for Signal-R. That's the big one for me. Also, I didn't see anything in there about lazy loading (without `Include`) in EF. Although I have to say, not having lazy loading has definitely cause me to think more about my queries. It's probably better. When you leave too much to magic, it's easy to write inefficient code.
Introducing a new project system introduces breaking changes to pretty much all other existing tools and APIs, so they revert back. But they promised to clean up the existing csproj files and to improve functionality in Visual Studio (e.g. referencing globs is supported by MSBuild, but VS can't deal with it).
Most answers say method 2, but I think that doesnt work well for: * autocomplete lists. (Choose a company from a set over 200K) * non-relational data (Choose currency, persisting "USD"/"EUR" etc) 
It does have a few legitimate use cases, but I've fixed countless performance issues relating to massive N+1 selects. It'd be nice as a *non-default*, I think: ultimately, any ORM turns out to be a quite leaky abstraction once queries become sufficiently complicated.
I really hope they focus a lot more on tooling. I started playing around with .NET Core a month or so ago and abandoned it due to the terrible documentation around the tools and development environment. I hope I was wrong about it as I love both .NET and Linux and would love to use them together, but the documentation just seemed unusable at the time.
This was the generalization I was trying to make. I was not implying that always assigning to an interface was also considered "best practice". 
Seems good tool, but not free unfortunately.. 
When they mention "*View Pages (Views without MVC Controllers)*" do they mean what's currently known as "*ASP.NET Web Pages*"?
Should be fixed since last week. Thanks :-)
Well... there's no such thing as a free lunch Not all tools can be free and to be honest, best of them are not... It's the same for builders - you won't get a drill for free ;) We make money using them, so it's fair to charge for them I think.
Did you report that bug?
That is true of course. Highly recommend downloading trial - even if you won't buy it, you can find weak points in your code.
https://github.com/mganss/HtmlSanitizer is the best solution I know of. The built-in dotnet one is no longer supported. https://www.owasp.org/index.php/.NET_AntiXSS_Library recommends HtmlSanitizer.
I've *caused* countless performance issues related to massive N+1 selects. And then had to fix them later on. Ahhh, the joys of being a junior programmer back in the day. I wish I could shake that kid and say 'what the fuck are you thinking?'. Anyways, I have gotten used to and actually like having to think about every query and what data I'll actually need. It's definitely helped me to know my own code better. I liked some of those suggestions of turning it on for queries the same as no tracking. _dbContext.Posts.Include(b =&gt; b.Blog).WithLazyLoading().ToList(); Or something like that. I think this would be the perfect solution.
If you DONT want to use DI, you'll need to override your ApplicationDbContext's `OnConfiguring` method and place `options.UseSqlServer("ConnString")` in there, this way you can do `new ApplicationDbContext()`. Do note however, that it's rather pointless to try to avoid DI - the entire framework is now built on this, and trying to do things without DI (i.e. shooting yourself in the foot) is actually harder than not.
Sure, it's not terribly complicated, and I don't run a blog or anything, so I'll probably make a self-post on DOTNET shortly. /u/neoKushan ^^
And yeah, that's any extension. I've had an extension that reopened the start page after closing a solution crash my syntax highlighting when I opened a Javascript file. 
You on update 3? It made VS a bit crashy and there's an update that came out last week that sorted it. 
I don't run OS X myself so I'll probably be of limited help but I do remember that you need to uninstall previous versions regardless of platform. I did find this GitHub page https://github.com/dotnet/core/blob/master/release-notes/1.0/RC1-RC2_Upgrade.md . There is a link to a shell script for uninstalling all previous versions on OS X.
There's an ongoing discussion for the design of it here - https://github.com/aspnet/Mvc/issues/494#issuecomment-232534742.
The same whitelisted approach that AntiXSS used is now the standard approach in MVC Core. Using @ in an MVC view will use a whitelisting encoder. If you want to encode manually (why?) in a class that is DI hosted then you'd take an instance of HtmlEncoder (or UrlEncoder or JavaScriptEncoder), otherwise [System.Text.Encodings.Web.HtmlEncoder](https://github.com/dotnet/corefx/tree/master/src/System.Text.Encodings.Web/src/System/Text/Encodings/Web) and its friends is what Razor uses under the hood.
Huh, yeah, of course. What on earth is he talking about then, asp.net encodes by default doesn't it?
Thanks for the follow-up! I didn't notice `Data.HashFunction` in my earlier search for lightweight hash implementations, looks like a great collection of packages.
You can add asphostportal to your list. I have clients that use their service too and it is working fine.
I've been out of the dot net loop for about a year but last time I tried yeoman scaffolds for .net or any of the new dot net core stuff the experience sucked. Some of the scaffolds generated but vscode doesn't manage any of the files. Some of the templates didn't even work Frankly, I find all this rather confusing since the big sell of .net is world class tooling. I'm all for CLI and options, but to regress in the tool chain is just silly. It's funny that the Java world has this a whole lot figured out, for once 
And if you want to create a web project for F#? Having flags to define options is a common pattern for CLI.
No, I didn't want to go polluting the bug list if it's just a single machine on earth.
I didn't - I will give it a go, ta.
Really? Shit - I'll check today in work. It wasn't "my" machine, so I'm not familiar with what updates are on it. Ooooo, it could be fixed!
Single machine on earth? You overestimate the reach of this subreddit.
&gt;Regardless, there's cool templating tooling coming, I hear, but for now there's more there than I realized. I can't wait to see what's coming. &gt;yo aspnet: dotnet new -t web isn't the only way to make a new ASP.NET Core project from the command line (CLI). You can also use the Yeoman generator or "yo aspnet" to make very interesting projects, as well as create your own generators. I really hope that we don't have to rely on NPM and Yeoman to create our own generators or do anything interesting. 
Well, just build a VSCode extension implementing a nice GUI calling the underlaying CLI commands then ;)
Definitely recommend it for a new project. I love IIS, but the ability to use Kestrel + nginx makes a lot of things easier, as IIS has a lot of weird legacy behaviors. And the new HTTP pipeline architecture is a pleasure to work with.
ASP.NET, hands down. Stay away from Core if you want tooling that works and if you want to be able to use existing libraries.
Thanks, so using an ASP.NET Core project targeting ASP.NET is best for migrating to ASP.NET Core when it's mature?
ASP.Net Core, running against Full Framework. This gives you the new MVC, but you still keep all your library compatibility.
It's not about avoiding the command line. Perfectly fine if we get the necessary ability to extend `dotnet new` with our own generators. The objection is that "do anything interesting" then has a hard dependency on the NodeJS ecosystem. If I wanted to deal with that clusterfuck disaster of dependency management and maintenance I'd just work in Node to begin with. 
Hmm, I'll have to look into it again, but my understanding was that there were a fair number of breaking changes. I don't quite understand how their documentation can claim there aren't any, yet I know of at least one explicitly (razor helpers).
What are some things you've been unhappy with in .NET Core? I'm playing around with it but haven't done any serious work with it yet.
We aren't migrating our main web apps yet, but we have built several small projects and microservices in core with great success. We aren't against migrating the main web apps, we just know it will be a big undertaking, especially QA wise, and we just don't have time or need at the moment. I am not sure where the tooling complaints come from, perhaps they are using things that I am not, but the command line and VS is doing everything my team needed before and after core came out. I don't think either is a bad choice, there are some IIS niceties, but if there is a chance you might be working in an environment other than windows, I think Core is a fine choice. 
This is the exact reason I've been using ASP.NET Core, but I also have very few libraries that I depend on, and the ones I do work fine in .NET Core.
I would assume no considering .NET Core is still a baby.
Mostly what I've mentioned - that's been my biggest timesink. Apart from that, just a general lack of documentation and collective experience. Searching for solutions is difficult because the damn thing's had so many names.
There's ~~2~~ 3 reasons why you'd want ASP.NET Core: 1. You want a low memory footprint for hosting. 2. You want to host your site on Linux. 3. Console-based web hosting on Windows. (edit: thanks /u/theonlylawislove) If you don't want either of those things, you will have negative benefit from using ASP.NET Core. The tooling isn't mature. The libraries aren't plentiful. ASP.NET Core is missing libraries available to ASP.NET. There is a decade+ of good ASP.NET documentation out there. ASP.NET Core documentation is sparse, and there's a lot of conflicting information out there because of all the changes made in each beta and release candidate. That's a lot of downsides, and only 3 real upsides.
Depends on what you're doing. We are using it for new projects, but it hasn't been without hiccups. We build ASP.NET Core but target the full runtime. That has saved us some pain. If you're doing anything with WebAPI, ASP.NET Core is just massively better. The duplicated infrastructure/pipeline in MVC 5 and below is atrocious. 
Also, console-based web hosting on Windows.
Don't use ASP.NET core unless you are willing to spend a lot of time resolving tooling issues? Other than that, it does work
Because ive been in the Django / Linux world for a couple of years and I'm looking to expand my skillset. I like c# as a language so .net core sounded really exciting 
You mean /u/theonlylawislove :)