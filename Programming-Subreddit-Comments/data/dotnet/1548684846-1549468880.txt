Yeah, NServiceBus is the most mature and feature rich library for commercial usage.
It looks like maybe you need a more explicit model to bind to (not just IFormFile, but a class with a named IFormFile property) https://docs.microsoft.com/en-us/aspnet/core/mvc/models/file-uploads?view=aspnetcore-2.2
If you are concerned about this you should consider using the bundled mode where the .NET Core runtime is included in the publish. Then you are guaranteed the version of .NET Core you've tested on will be available.
Thanks for the links. I am looking through them but they seem more general tips. I will keep digging to see if I can find my specific fixes. What we are working on is this middleware that puts some logging stuff into the header. They have it finished for Core and I am the one who has to get it to work in Owin so I am hoping to not have to reinvent the wheel and use what is used in the Core version, but that may be more confusing.
Clearly, used to have a project around ActiveMQ. If I knew about it, would have loved it so much. Love how it manage the message part to let you with only the business related job. Really love it. Can't wait to find an occasion to try that.
[C# Digest](https://csharpdigest.net/) is a weekly that's always good.
[FromForm] will expect the file as a form field as you show in your screenshot. However I don't know if that is a supported way to process files? You can try [FromBody] and just send the file as the request body, without a form.
I see two things that are questionable to me besides what has already been discussed: 1. Use of IFormFile. There is no filename submitted in your request so .FileName won't ever return anything even if it works, I would think. IFormFile may not be the proper type to use. Stream or byte[] might work; I am a bit new to using DI for form fields. 2. This line is super sketchy ` item.PictureInfo = new byte[Convert.ToByte(fileName)];`. What it does is try to parse the filename as if it is a number, then allocates that many bytes into a new empty byte array. Your app will likely throw an exception here and interestingly it would be trivial to write a DoS attack to eat up all your server memory and crash your app entirely. If instead you wanted to store the filename as an array of bytes you need to use a System.Text.Encoding class for the appropriate encoding (UTF8 is typically a safe bet) and call .GetBytes with the filename.
Thank you for sending this link, very helpful! 
Hey, it's not my fault it's difficult to tell sarcasm on the internet. Though I do read a lot of /r/talesfromtechsupport...
The IFormFile does return .FileName, so that portion is working as intended. Coverting the fileNAme into byte, your right, is very sketchy and not something I had worked out at this point. It took me successfully passing in the file to be able to realize this was poor practice and have since removed it, and handled storing the fileName as a string rather than byte[]. This code was a product of trail and error while I was learning on how to handle files/images. 
You kicked it into the wrong net, my man.
From $8995? I feel like starting a project to write an open source clone of this now just out of sheer annoyance!
oh! yikes... I just got assigned a fair bit of certificate checking stuff for this next sprint, so I'm happy to have read this.
If you choose non-LTS version you'll be left without security fixes much sooner, so if you care about security and don't want to rewrite app soon, it's better to use LTS.
LTS means it gets fixes for a longer period of time for non-LTS; it doesn't say anything about when fixes are prioritized or released. But yes if OP is concerned about fixes he probably should NOT use bundled mode since he has to re-release to deploy fixes, while otherwise he can just direct the customer to install a .NET Core release. LTS is probably best for a work app unless OP needs features from the latest non-LTS.
What are use cases of this extremely expensive tool? C# and other similar languages and platforms don't suit when: 1) Even small GC pauses are critical to the business 2) You need to squeeze every nanosecond of performance. Both items listed are not possible with any code converter because only human can write idiomatic high-performance code (BTW they are state that converted codebase can be used 'in the same way as if they were originally developed in this language' lol).
I just ran K8 on a Ubuntu VM on server 2016. Saved me a ton of hassle 
Thanks! Much appreciated!
GCP's kubernetes is pretty cheap/reasonable, found it much better than bogging down my local dev machine, worth a try?
For what? Reads, writes or coder efficiency? 
I‚Äôve used Azures AKS to test with and there is not a lot holding me back from doing that, it works fine and for just testing that‚Äôll work. Main reason for running it locally is to make sure the other devs at our company can always work with a k8s cluster even when not having an internet connection (which happens more often than we like) So my recommendation would be to use a k8s from a cloud provider if you have a reliable connection and have a way to easily spin up a cluster (scripted, infrastructure-as-code style) for every dev without interfering with each other. Tbh it‚Äôs as much personal preference as anything else
I use [Ultimate PDF for .Net](https://www.componentpro.com/products/pdf). It is the third-party library. I found it useful. It is very easy to implement and have many great features to getting started with. You can easily generate pdf from template using this. I hope it will help you. Here is the simple example to generate pdf from html. using System; using System.Drawing; using ComponentPro.HtmlConverter; using ComponentPro.Pdf; using [ComponentPro.Pdf.Graphics](https://ComponentPro.Pdf.Graphics); &amp;#x200B; ... &amp;#x200B; // Create a new PDF document. This object represents the PDF document. // This document has one page, by default. Additional pages have to be added. PdfDocument doc = new PdfDocument(); &amp;#x200B; // Add a page to the document PdfPage page = doc.Pages.Add(); &amp;#x200B; // Options bool keepHeight = false; bool keepWidth = false; bool javaScript = false; bool autoDetectPageBreak = false; bool hyperLink = false; bool splitTextLines = false; string url = "[http://www.google.co.uk](http://www.google.co.uk)"; &amp;#x200B; SizeF pageSize = page.GetClientSize(); &amp;#x200B; PdfUnitConvertor convertor = new PdfUnitConvertor(); float width = -1; float height = -1; &amp;#x200B; //Calculates the height and width of the pdf image AspectRatio dimension = AspectRatio.None; if (keepHeight) { dimension = AspectRatio.KeepHeight; height = convertor.ConvertToPixels(page.GetClientSize().Height, PdfGraphicsUnit.Point); } else if (keepWidth) { dimension = AspectRatio.KeepWidth; width = convertor.ConvertToPixels(page.GetClientSize().Width, PdfGraphicsUnit.Point); } &amp;#x200B; Html2PdfOptions options = new Html2PdfOptions(); options.EnableJavaScript = javaScript; options.AutoDetectPageBreak = autoDetectPageBreak; options.EnableHyperlinks = hyperLink; options.Width = (int)width; options.Height = (int)height; options.SplitTextLines = splitTextLines; options.AspectRatio = dimension; &amp;#x200B; PdfGraphics g = [page.Graphics](https://page.Graphics); &amp;#x200B; // Draw rotated objects g.RotateTransform(45.0f); &amp;#x200B; // Translate to 20, 20 g.TranslateTransform(20, 20); &amp;#x200B; doc.ImportHtml(url, options); &amp;#x200B; &amp;#x200B; // Save the PDF document to disk. [doc.Save](https://doc.Save)("Html2Pdf.pdf"); &amp;#x200B; // Use the following code to stream the document to the browser. // [pdfDoc.Save](https://pdfDoc.Save)("Html2Pdf.pdf", Response, HttpResponseType.OpenInsideBrowser); // Response is an HttpResponse object. &amp;#x200B; // Close the document. doc.Close(); &amp;#x200B; &amp;#x200B;
Dapper IMHO; does most of what you need (projection etc...) but is pretty damn close to [ADO.NET](https://ASO.NET) in terms of raw performance. Plus it's battle tested (being the data platform of StackOverflow). First though ask why you can't just do DbDataReader...does it really save you that much time?
You can use [EPPlus](https://www.nuget.org/packages/EPPlus/),[UltimateExcel](https://www.componentpro.com/products/excel) for Excel,[ITextSharp](https://www.nuget.org/packages/iTextSharp/), [UltimatePdf](https://www.componentpro.com/products/pdf)for PDF and DocX, [Ultimate word](https://www.componentpro.com/products/word) for word. 
yup indeed. if we lose internet connectivity we‚Äôre pretty screwed, no deployments, no slack, no project mgmt system.. so we just tether to mobiles :)
Hard to say without knowing what your Dockerfile looks like.
My bad, check again. I thought it wouldn't matter. I don't really wish to get rid of the windows support altogether, but I do want it to be able to run on Linux for now. That's why I suggested just renaming the current Dockerfile and making a new one. 
Currently you target the Windows specific net core container. You‚Äôll want to change that to a Linux one, probably removing -nanoserver-sac2016 is enough (not tested though)
Okay, so assuming that I don't want to alter my current Dockerfile, would it work for me to just remove it from the root and then create a new one targeted to Linux? 
So I've been experimenting with doing a [MS Learn](https://docs.microsoft.com/en-us/learn/) module entirely within the browser, using the Azure Cloud Shell as a host, so the student can run `dot net new` and then edit in the [Cloud Shell's browser-based editor](https://docs.microsoft.com/en-us/azure/cloud-shell/using-cloud-shell-editor). I'm having to push back against some folks who are like, "But they're not using Visual Studio, so it's not the same experience!" I'd much rather be able to teach you how to write ASP.NET Core code without making you download anything, though. Then you can use whatever editor you prefer. The sandboxing around the Cloud Shell makes it a little tricky to do anything with a UI, so my co-writer and I have been using Web API for our modules. We're going to launch soonish (within a week or so - we're bottlenecked waiting on artwork).
I replied [downthread](https://www.reddit.com/r/dotnet/comments/ak8efx/learning_aspnet_core/ef5797g/), but I really appreciate this feedback. Explaining the hows and whys of scaffolded code is something my current project does really well, I hope. I'm looking forward to releasing it.
Oh, okay. It *is* a lot of info, but we definitely want to make it as easy as possible for you! Do you have any ideas on what might help? Please hit me up if you have any questions! I'm on Reddit, [Twitter](https://twitter.com/camsoper), or you can email me directly at Cam.Soper at Microsoft.
Maybe not the fastest, but very fast.. I would say Linq2DB. It have all of the benefits of strongly type queries, CTE, fluent mappings, window functions, and many more. I'm using it in all of my projects, definitely recommend it! 
Been a while since I needed to. Did you try flushing the stream before closing it?
DigitalOcean also has it for $10/mo per node plus cost of load balancer if you provision one.
Reads. Mostly for performing fast and stable CRUD on the database.
Okay, so this may or may not be helpful but I was playing about with image sources the other day and had a different problem that may be related. I found that I couldn't load an image from the code behind of the page I was currently on, I suspect the event sequencing was messing me up, so what I ended up doing was creating a second page that took a query string on page load and sent the image as a response. I then set the image source in the presentation layer of my main page to the url of the image generating page. So on data bind I set the image source with the database parameters in the query string, that calls the second page with responds with the image which is loaded to the place marker on the first page. Let me know if this doesn't make sense and I'll have a play about in the morning and post a code sample that may help. 
Your post has been removed. Self promotion posts are not allowed.
Hi, For resx files they are binary compiled and I don‚Äôt think you can load them at run time. I think you need a custom localization provider you can start from here: http://asp-net-whidbey.blogspot.com/2006/03/aspnet-20-custom-resource-provider.html And you can write yours file based. Hope this should help 
This is perfect thank you. We can either use the DB backed instance or have this be a static runtime that is read in from a local file .
[https://codefresh.io/kubernetes-tutorial/local-kubernetes-windows-minikube-vs-docker-desktop/](https://codefresh.io/kubernetes-tutorial/local-kubernetes-windows-minikube-vs-docker-desktop/)
That's good to know. As I have stated, my personal preference is to not touch code generation until *after* I know how the pieces fit together. &gt; I'm having to push back against some folks who are like, "But they're not using Visual Studio, so it's not the same experience!" That must be tough. But considering core is multi-platform it doesn't make sense to me to target anything other than the features of the language accessible to all platforms. At the end of the day, people like me want to learn how to use the language and its features, and are not necessarily interested in an 'experience',especially if that experience is only really available on windows. That being said, I look forward to checking out your module. 
Of course. 
Maybe a mutual link-sharing club can be put together that when someone in the club creates something, someone randomly selected in the club will post it somewhere from a requested set of sites.
Linq2Db is practically the fastest ORM today. 
This one is great too. https://github.com/EdwinVW/pitstop
Most of these are easily found with a Google or 2. Swagger and Core 2.1 are what you want to do. If EF Core doesn't work then add Dapper.
Just throwing it out there, but after using GraphQL for the past three years (very complex project, hundreds of object types), I would never choose REST for a project if I had the choice. GraphQL is also self documenting through introspection, so documentation sites can be generated really nicely from tools that are completely separate from your codebase. I personally use a combination of server-side session tokens as well as JWT. The JWT provides the application signature for extra security, and tokens can be revoked on the backend if need be. Best of luck whichever direction you choose!
That might work. I made this post in another forum and someone suggest that i use Handlers and abandon WebForms and move it to MVC. I'm researching about it now, and it's pretty interesting the way it works. I'm trying to learn it right now. I'll let you know if i get somewhere with this haha. Good luck and thanks for the reply!
No, actually i'm trying to move to MVC instead of WebForms. Idk, it seems to be quite better for my purpose. But i'll try this, thanks for the reply ! =)
You can compile .resx into satellite assemblies, especially useful for localization. More here: https://docs.microsoft.com/en-us/dotnet/framework/resources/creating-satellite-assemblies-for-desktop-apps
I definitely recommend mediatR. It's super convenient to be able to find everything you need in a single place. Combined with EF core it's amazing how productive you can be! Jimmy Bogard has some really good examples and other info on his blog/ GitHub. Also, core all the way!
Resx are just plain xml files, so you can certainly load them at runtime (as long as you don't compile them, which is the default behavior). A better mechanism is to use satellite assemblies (see my other answer).
I'll echo mediatr, though I use my own version of it. I wrote some middleware to handle routing. I write the model class and the handler, and add a line to configure it. \`options.Post&lt;CreateRoleRequest&gt;($"/subscriptions/:PartitionId/roles")\`
This is how I do it. https://github.com/codenesium/samples
Yes you are right but I think the Op was intend to find a way to not compile and just update the resource file and update the localization. The satellite file I can‚Äôt try but if they are in an exec can‚Äôt be overwritten because they are in use. Anyway your options are still good and are good to mention 
IMO MediatR is just a service locator pattern with extra steps. If you like it, that's great, but I don't see a need for it.
It is fairly close to the service locator, or mediator pattern. I agree. The implementation is super lightweight though, so I don't mind it. I agree that it solves a fairly simple problem, but I feel it does so more eloquently than I would otherwise.
Depends on business problem you are trying to solve. If domain is complex than use DDD. If query operations are more than post/put than use CQRS. 
Omg you probably should ask this question on stack overflow because reddit comments tends to just share their opinion and not thinking about your problem. But it's pretty easy to implement a REST API in .Net Core these days the documentation is pretty solid and fairly up-to-date the only problem you will face distinguishing between the .Net and .Net Core forums and tutorials but that's not really an issue. 
The "icon" is your browser's way of telling you it could not load the image. My first step would be to try and figure out what the server is returning instead of the image. My way to do this would be to open the Chrome Development Tools and use the Networking tab to see what the server is returning for the image request. If you're not comfortable with that you can simply right click the "icon" and view image though you might get a bit less information than the development tools could get you. But if you're lucky you'll get a useful error message. If there is an error message I would then try to debug it in Visual Studio. You can use the Exceptions Window and ensure the CLR exceptions box is fully checked to catch handled exceptions which will ensure the next time you try to load the image you'll be debugging the error, If the error is simply the file is not found it's likely the path is not being generated correctly in your view... of course it will need to point to your API which returns the appropriate image. Alternatively you can simply generate a data: uri right in the view to embed the image right into the HTML. As a side note... there is typically no need to store files in a database... I am by no means an expert but I do know the more data you store, eventually usage of the database slows down. Storing a path to an image file on disk and then saving the images to a designated folder may be easier. You could even skip storing a path and make the path deterministic based on other information from the record such as the ID.
OK so I have not used traditional ASP views stuff much so I cannot be 100% sure, but the way you are outputting the image looks suspect. The "icon" you see is an indication the browser is unable to show an image. It is probably because it cannot understand the url to the image embedded in the page or it tried to request the image from the url and an image was not returned from the server. The src attribute is expecting a url pointing to the actual image on your server, but it looks like you are returning an image object. I do not know how exactly ASP.NET deals with this but it may just be converting it to a string, resulting in the name of the Image type which is not useful. There are three approaches I think you could do. One is similar to what you are trying. You can create a url that contains the raw image data itself. You would construct a data: uri in this case. You do not need to use the Image type for this, you would encode the raw binary data into the data: uri and that's it. I'll leave it to you to Google around for documentation on the syntax. You will need to base64 encode the text (I will leave that up to you, I forget if .NET has an encoder built in or not). Be aware old versions of Internet Explorer do not understand data: uris. I forget if IE11 does. Be sure to research your options before selecting one to implement. :) The more traditional alternative that will work fine is to output a url pointing back to an API on your server. This API can then return the raw binary of the image as part of a separate request, which should display the image. The last option is there is PROBABLY some ASP.NET-specific way of doing what you want to do. I am just not familiar with it, as I said. As a side note... there is typically no need to store files in a database... I am by no means an expert but I do know the more data you store, eventually usage of the database slows down. Storing a path to an image file on disk and then saving the images to a designated folder may be easier. You could even skip storing a path and make the path deterministic based on other information from the record such as the ID.
Everyone answered your core question already but here's a couple extra things I have: It's usually a good idea to go for "separation of responsibilities" in OOP. A class should really only be aware of what data it works with and the task it has to do, and not really of anything else. This way the code is reusable since you can pick individual classes up and put them in new projects. This design Microsoft uses in ASP.NET encourages that since instead of typing your classes to other concrete classes, you tie them to much lighter weight interfaces which you can write other concrete classes for as needed. Also the way the nuget packages are structured, you can make a package that implements one of the ASP.NET Core interfaces and it only pulls in the appropriate interface package instead of a whole bunch of ASP.NET Core packages. So if you have some classes that are used by multiple projects you can factor them out into a library without pulling ASP.NET as a dependency on the library, so you can use it in a console app without needing to bundle all those libraries.
Ok, thanks i'll try thatüëç
I second the side note, a few years ago I was tasked with putting together a simple document management system and started off just storing the documents in the database. It is amazing how fast the performance degrades, especially if the end users upload power points with uncompressed bitmaps of screen shots. In your use case I can easily imagine management saying "our catalogue should be the best in the Web" and grabbing a high end digital camera and uploading some massive images. In the end I simply assigned a random file name to each upload and stored them on the Web server, with the database holding an index of the file names, it's simpler and quicker, way quicker. 
same here. graphql is a huge time saver and makes large APIs so much more flexibel.
You usually just have multiple docker files.
How well does dotnet play with graphql?
&gt; How to handle Authentication? Do yourself a favor and pick something pluggable that can evolve with you, supporting both API consumption and web applications/apps. Do you already have a user database with passwords? If so I‚Äôd look at https://github.com/IdentityServer/IdentityServer4 If not let someone else handle the heavy lifting like https://auth0.com
I'm sure SO doesn't allow "what tooling should I use" questions. As they are often opinon-based.
If EF annoyed you before it'll still annoy you in EF core. 
You are correct for the most part. A better place would be Software Engineering Stack Exchange, one of SO's many sister site. But even there you have to word your question carefully. 
SE is actually much worse. I'm sure SE doesn't allow such questions. Not so sure about SO.
I find it makes code comprehension harder (where is the handler for X) and removes my ability to see dependencies easily across the code base. It also makes life harder for new members of the team who cannot figure out where execution is jumping to. How do you manage this with Mediatr.? I see so many people love it but my experience with it at scale wasn't great for the reasons above.
REST API Best Practices: https://jonathas.com/rest-api-best-practices/
Also depends on where your deploying to? If you are hosting in azure for example you could also use functions.
Regarding documentation: it depends on what do you mean under "documentation". Swagger is good because it provides UI for QA and basic understanding of the API structure. Have a look at apiary and docfx solution. They pretty good at building documentation web sites for your API. 
What do you this for at SO?
.net core, apim and depending on scaling/Architecture service fabric or AKS for me. 
Your post has been removed. Self promotion posts are not allowed.
I would recommend you to watch lots of videos to build an understanding of the cloud + microservices. .net core has all the tools you need. even a sample project: [https://github.com/dotnet-architecture/eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers)
Migrated from 2.1 to 2.2.1 (Security patch) on prod. I have been using it for more than one month - no issues.
[https://github.com/graphql-dotnet/graphql-dotnet](https://github.com/graphql-dotnet/graphql-dotnet) make it simple but work/rewards scales poorly. Add [https://github.com/graphql-dotnet/conventions](https://github.com/graphql-dotnet/conventions) on top of it and it's a breeze. Your Mileage May Vary.
Not a fan of EF or dapper for enterprise projects.
You need to search for usage of the request type, just the same as you need to find derivations of the interface in a normal dependency. I'd argue they're around equally obfuscated.
What do you use? The raw ADO adapter? 
&gt; than
Microsoft as some best practices documents. But regarding library decisions consider open source projects with good support and high activity. For development practices consider SOLID principles because it will enable you to decouple dependencies to concrete libraries. 
This is probably the guy on the team that's writing the super in depth sql that's hard to replicate in one of those frameworks. I do understand the need for being able to write raw sql at times but in like 95% of use cases these are just fine. We had the same argument internally when we started our migration and ef (dapper is fine too) won out because the benefits outweigh the pit falls
I dunno if it's been other people's situations but we're migrating to auth0 at work and having a nightmare of a time. Awful support, buggy interactions, and the sdk for dotnet is terrible.
What problems with Dapper do you have for enterprise projects? 
I understand not wanting to use EF, but dapper should be able to handle whatever you throw at it. 
Nope. I do like procedures but that‚Äôs a different topic. In my experience EF and Dapper allow for big mistakes that are challenging to fix down the line. The biggest performance hit we get in .Net are from dependency calls and reflection. EF and Dapper make it too easy to say ‚Äúfuck it I‚Äôll just make 4 calls to the database‚Äù. EF takes a LONG time for the first call and is slightly slower for each call after compared to ADO. People like EF and Dapper for maintainability and readability but what I mentioned above about making a ton of calls to the database hurts maintainability because if you want to refactor for performance you then have to deal with all of these calls. Writing a datamanager that handles your database transactions is fairly straightforward and super easy to maintain. I worked at a very large c# shop like 1500 developers plus. They removed all EF. Just saying.
&gt; EF and Dapper make it too easy to say ‚Äúfuck it I‚Äôll just make 4 calls to the database‚Äù. No that's somebody who doesn't know what they're doing
Nail in the coffin for me learning GraphQL, I'll take a look tonight. Outside of pluralsight, any good resources you recommend? 
Stored Procedures. I‚Äôve worked at places that have like 3k plus stored procedures that literally contain all of the logic. They were literally checking for what day of the week it was in the SP. That is way too much. That being said SQL is a great language and is perfect for querying the database. It was literally written for that purpose. SPs are easy to maintain and if you build a database project they‚Äôre all right there and are controlled by source control. Used correctly stored procedures are great.
.NET Core is a given, RestSharp and NSwag for clients. Don't fear roll your own. It's pretty easy, but unless you've got legacy stuff that won't work with .NET Core, it's probably not required. I've solved a few problems because I started with a roll your own, so I sort of knew what headers were :/ Security, Authentication specifically will either be really simple or a total nightmare. I'll circle back on this. Anyway, invest in that sooner rather than later. It's dull annoying stuff, but you absolutely must nail it. Look at some supporting tech too. Redis does nice PubSub on top of its cache stuff. Consider how to secure your Redis or your Elastic or your Kafka, this whole unified authentication thing is a bitch to get right, but worth the effort.
All those concepts and tools can be great to push the quality of the app and the culture of the dev/devops team, especially if the project is about updating and/or upgrading the associated business needs. But this is a big step. The insight from the white paper related to eShioOnContainers is useful, but rely on a lot of things that aren't actually about just a REST Api (containers, design patterns associated/enabled by containers, messaging/eventing between micro-services). It's a lot to learn in one project. &amp;#x200B;
The issue I have with that is it makes it too easy to insert a little business logic here and there into your sql, and then it becomes a nightmare to maintain. EF (or other similar framework's) real power comes from the fact that for a lot of day to day DB stuff, it makes it painless to use, especially for a non sql saavy developer. If you want to maintain your data access that way, you're basically expecting all of your back end devs to have a decent, if not advanced level of sql knowledge, unless you're big enough to have dedicated DBAs. But being able to do db.Table.First(t =&gt; t.id == id).DateModified = DateTime.Now(); and update a record is so much easier than any sql counter part. I'm not saying that you can do everything in EF, and yes, if you can get some ugly sql generated if you write your query statements poorly, but you can get shitty code in any number of ways. 
&gt; I worked at a very large c# shop like 1500 developers plus So when you hit this size, I can see a valid argument for that, because you have the man power and budget to build and maintain it. I don't think a 1500 dev sized shop fits in with my 95% though. That's the extreme not the norm. That's like saying everyone should write their own frameworks or parsing engines because google and facebook have. 
And I would definitely agree with you that if you have the time, money, and man power to write your own anything, it's almost always better than using something pre built. That's just not feasible in a lot of cases. 
True but that‚Äôs the entire point of a lot of features in programming. To prevent you from making mistakes. Not necessarily the senior devs but the junior devs on the team.
Abstractions or bare metal code are completely independent of stupidity or inexperience. 
I don't. REST doesn't work. Working with entire objects on both the query and command side is nuts. You get way more data than you need on the client, you end up using the same endpoint for a dozen different operations, you end up with property soup since it's "easier" to add a new property to an existing endpoint. You lose all context on what the call is even for. You want commands and queries clearly separated and you want every query to be unique to a specific function on the UI. It only gets used multiple times if the UI requires that same data in multiple places. Otherwise you have tailored queries that ask for exactly the input they need and output exactly the data that needs to be returned. Commands are entirely separate. Again, commands indicate exactly what the context of the operation is and receives only the data necessary to complete that operation. At most, it will return the status of the operation, a new/updated id or validation/errors, never the new "object" in its entirety (another huge waste of "REST"). Yes, this kind of API is a bit harder to manage and maybe you end up with more endpoints, but you will thank yourself later for having a clean API that you can be confident about because you know what everything is for and what it is doing. This doesn't mean you are forced to use CQRS, but splitting things up this way allows you the option later.
I was doing my own benchmarks and **Entity Frameowork Core(**the latest) was faster than **Dapper.** The new version is very well optimized and there are almost none of the performance issues. There are however still pitfalls like lazy loading and some complex queries which result in less efficient queries but those are all things that as a developer you should be aware of and know which to use when.
Nice, that's what I was wondering. Thanks. 
You're the best. I'll definitely change how i store image, it makes a lot of sense. If i decide to up the database to a cloud server i would creaste a sort of directory to save the images and place the paths in the database ? . &amp;#x200B; And about the image error, some guy suggested i use MVC or CORE instead of the WebForms, and i'm trying to learn it now. It seems better then WebForms, and i hope to make my job easier haha. Thanks for the reply, it really helped me.
https://www.lynda.com/NET-tutorials/API-Development-NET-GraphQL/664823-2.html if you have Lynda access. That's what I used to get started then the rest was looking at the example on GitHub.
Definetely gonna do that. It amazes me how fast your database or program can change in high scale. Luck for me, my case it's a personal project to teach myself. Hope to find this managers and show then the beaty of the 'image paths' instead of binary . Thank you for the reply
We are using DO for our ‚Äúreal‚Äù clusters so it‚Äôs definitely an option
I've been using Windows Authentication at all my employers so Authentication has never really been a problem for me. &amp;#x200B; I'm doing a side project where I need to authenticate users. Do you use [ASP.NET](https://ASP.NET) Identity with IdentityServer? And is IdentityServer just for authentication or does it also handle authorization? &amp;#x200B; The UserManager seems so abstracted that I'm not sure how to create/update/get/delete user's permissions. Does a 3rd party library make this all easier?
If you have no relational database then it's fine, but if you do have a relational database then you are better off with a tool like Octopus Deploy when can automate everything, but you would incur a small amount of downtime.
I like to think I have a pretty open mind about technologies, but the advantages to Mediatr leave me scratching my head. From what I can see, it is a routing framework that lets you organize your code a little differently. I am sure there is more to it, but I am missing it. Would you mind giving some insight as a user of it? What could you not do before that mediatr allows you to do, or to do nicer/better?
&gt; If query operations are more then post/put than use CQRS. What would fall in this category? Could you give a quick example, please?
The images would remain on your web server so the server could easily access them to serve them to the browser. If you have other applications which need to query this database it may make more sense then to store the images in a more centralized location (like the database server) but then you need a separate service to serve the images over the network and it will add a bit of latency to the web server for serving the images.
Now with fewer helper tools! 
Good to know, we were debating making a move. 
You might want to try our Orchard CMS. You can reuse those skills when you work with some MSFT products that are based on it. For instance, the Azure API management developer portal uses Orchard.
Depends on what you're after. I'd tend towards static generators nowadays like GatsbyJs or Wyam for .NET, Jekyll...they are cheaper to host, free from the security issues like Wordpress; integrating them with GitHub for generation and version management is super compelling too. 
One thing I love Mediatr for is PipelineBehavior and company - its trivial to decorate all requests with unified form of logging, validation, performance checks, additional authorization that would otherwise require (potentionally complex) middleware, and all the infrastructure goodies that might be otherwise chucked into middleware pipeline or polute domain logic. After that, you can organize your solution in vertical feature slices which is a preference, but nice to have as an option, especially for more granular microservice-y things. The last salient point is that your controller can be trivial - often one liners and basically not requiring testing. This allows you to write subcutaneous tests that exercise almost the whole stack with more ease. 
Generally just basic information pages for small businesses. Home/about team/contact/services/etc. I got a few friends who want help building pages but I'm struggling with umbraco on the front end areas
I will look into orchard see if it's better for me. Thanks
Most of the applications we use in daily life fall in this category. We view more data and post less. 
I'd take a look at the ones mentioned above. GitHub Pages &amp; Jekyll [https://pages.github.com/](https://pages.github.com/) (with a custom URL) is probably the most mature solution... Doesn't sound like you need a full CMS for that; and all the patching, updates etc...that go along with it. 
Probably not. Realistically I could probably solve these things with wix or squarespace but as a dev my pride hurts so I cant bring myself to use them so figure I'd try a cms and learn a little in the process. I'll check those out . Ty sir
There is [https://www.codeproject.com/Feature/DailyBuild/](https://www.codeproject.com/Feature/DailyBuild/), it's not limited to .NET. There is also [https://www.codeproject.com/Feature/Insider/](https://www.codeproject.com/Feature/Insider/) for more broad news about Tech and Devs. &amp;#x200B;
Disagree, you have to set things up in order to prevent stupidity or inexperience. No one developer or team is good enough to find all potential bugs in code reviews.
With .Net Core 2.x don't worry about nuget packeges, you can use old .NetFramework Nugets as long you are deploying to windows platform, visual studio will complain about it, in the documentations they said there's no guarantee that's old dotnet framework packages will work properly all the times, and they plan to remove this possibility in dotnet Core 3.x But it's still a possible working solution.
Which is perfectly valid...:) As developers we need to learn new stuff and have a play every now and then. 
I second Orchard Core CMS. It's still in beta but great community with a very active gitter channel always happy to help out new devs get familiar with the system. It was a little bit hard getting started out but once you get the hang of it you will see that it is very modular and easy to extend, which is always what I look for in any CMS.
Why do think people who write stupid EF code are going to write any better SQL? Preventing merges without code reviews fixes that problem anyway.
Not sure that‚Äôs entirely true. A lot of the EF team are very talented and have decades of experience with ORM code, common pitfalls, etc. Having your in-house team could very likely end up with a much worse version of EF.
Well I guess I meant more so, better for your specific situation, not better for anyone else who wants to use it. I also mentioned having the man power, time, and money too. I mean yea, you're not going to take a team of 3 people and 3 months and have anything that's better than EF. But I'm on board with you that the people building EF are going to be better at delivering something more stable than anything someone else writes, outside of a scenario like OP mentioned, with 1500 developers on staff. 
Are you looking for a CMS that does the front end for you or are you looking for a CMS that allows you to easily add content management to an existing template e.g. a bootstrap template? If you don't want to deal with front end at all then I expect squarespace or similar would be the best bet, but that's not really taking advantage of your backend skills.
I feel you on the Umbraco pain - have been trying to learn it for a site I‚Äôm developing and it‚Äôs complicated and the documentation not clear at times (keep in mind, I‚Äôm new to .Net and c#). Am tempted to jump ship. Good luck with your decision, may the learning curve be short and sweet. 
We have used EF Core for several large enterprise apps... There is a learning curve, but overall, it has been great.
Look at Netlify and a static site generator (Hugo, Gatsby, VuePress, Next/Nuxt are good choices). Simple, secure, fast and low running costs. If you need a CMS for your clients/users to maintain content, Netlify have a very simple CMS UI. Alternatively there are several "headless CMS" (CMS UI with an API and webhooks to generate your static site content) available, such as StoryBlok ( see https://headlesscms.org).
You use good practices to avoid bad code and do code reviews to ensure good code.
Have you looked at DNN? Solid platform, open source. Lots of themes &amp; modules.
No I havnt. I will research it a little
A little of both. I dont mind doing a little css and such but I struggle with doing all the razor function stuff with umbraco and it seems a bit confusing. I would like my clients to be able to manage it a bit as well updating job postings and such
Thanks I'll look into netlify!
Let me know if you have any questions. (I don't work there.. My company develops 3rd party plug ins that work on the platform).
Ya it's the struggle of best solution for the problem but also developing new skills. First world problems haha
It's more a hosting platform, but makes it easy to hook up a static site generator and deploy from your source code/ control (similar to GitHub pages, but more). 
Well, fwiw I work on [Cofoundry](https://www.cofoundry.org/) which is a CMS that works fairly unobtrusively with ASP.NET core. It's doesn't provide any front-end code out the box, but it does allow you to use standard razor templates and C# classes to build your site in a way that should be familiar to .NET developers, so you can easily integrate this with an html template from elsewhere. The [Simple Site sample](https://github.com/cofoundry-cms/Cofoundry.Samples.SimpleSite) will give you a good idea of whether it's the right choice for you. I have also seen some demo videos of hooking up the bootstrap sample sites with Orchard Core, but I can't seem to find it now - someone else might be able to chip in with a good link? That sounds like the sort of thing you're after.
Do you want to make this one app a SPA, or do you want to make more SPAs after this one?
Thanks for the honest opinion. I'll look into it.
Good practices are worse at producing good code than a couple of talented senior developers reviewing code before allowing a pull request to go through. And you conveniently ignored my other question. Why would someone who writes shitty EF or Dapper code have any chance at writing better SQL?
Will do thanks!
EF Core has 147 contributors. I realize that isn‚Äôt an ideal metric but it would be quite the undertaking to write anything remotely close. If you only cared about small sunsets of EF functionality, you could make improvements for sure. I would still argue extending EF via pull requests would be a much better use of company resources.
It‚Äôs a huge timesaver in certain circumstances. If you have an API that is only used by one application and will never be used by others, GraphQL is overkill and a waste of time.
I thought blogs.msdn was going away. Seems I unsubscribed too soon.
Honestly I would just grab the AspNetIdentity/EF combo from the IdentityServer samples and call it a day. You can have that up and running in no time and it will be able to do anything you need it to.
Follow up - what's the point of a USING declaration if it acts like a variable?! Clean up your code on exit or use a using block.
give hot chocolate a try. it's so simple to get a GraphQL API up and running. https://github.com/ChilliCream/hotchocolate/
i wouldn't say that it doesn't make sense for a single application. GraphQL gives you the possibility to implement new features faster as you don't have locked your UI to the REST API. it basically allows you to structure the backend data logically without impacting the UI performance. REST requires you to build the API the way the UI needs the data and has not so great reusability for GUI evolution.
Yea, I was trying to be nice and say where this person's comment on EF might be valid. You'd have a hell of a time convincing me you're going to build a better application by doing your own, than by just using EF :)
Awesome! Will be adding this to my toolbox.
I've always been a fan of [Piranha](https://piranhacms.org/). It's maybe a little more barebones compared to other offerings, but it's completely open source, easy to extend, and (imo) got an easier learning curve.
I do plan on working on more SPAs in the future, but my priority is not delaying the release of this one too much, so a shorter learning curve would be better
Thanks for chiming in I'll check it out
We've been using craft CMS. Its not dotnet, but it seems to please the front end devs. In our use, it makes lots of calls to a dotnet api. 
I also don't like that syntax, but it doesn't "act like a variable". A variable becomes eligible for GC after it goes out of scope, this has `.Dispose()` called on it when it goes out of scope. 
Answer your other question first. They won‚Äôt but at least it‚Äôs happening on the database an extremely lengthy call. To your first question. Are you kidding me? Couple of talented developers. That‚Äôs my whole point. You need to put best practices to prevent bad practices and shitty code from happening. You can not rely on having good senior developers. That would be cool but not likely. 
 &gt; Clean up your code on exit That is the point- the compiler does this for you at the end of the existing scope as an alternative to introducing a new scope. Sometimes a new scope isn't needed (short methods) and just annoying (pre-declaring something so you can use it after assigning it in a using block).
Please front end devs how? As in backend devs can build front end stuff that they think is good?
AJAX all all the things in the webforms app, then never touch webforms again. Afterwards look at multiple SPA frameworks before deciding. Your .net skills can come in handy with asp.net core for the backend
I don't really know, I only work on the api.
&gt;From what I can see, it is a routing framework that lets you organize your code a little differently. Isn't that 90% of frameworks out there? ;) I don't use mediatr in my personal projects (from which the provided sample code was taken) as I want to handle routing my own way. My use of the mediator pattern is two-fold: abstract infrastructure from business code and (in a much more practical sense) remove boilerplate code. I've often found that developers either inject a lot of services into their API Controllers and end up writing business code through laziness, or move all their business code into another class and call it something fancy like Unit of Work. In the end, they are still writing 100 different methods in multiple API Controller classes to handle each of the requests. And it's a gigantic violation of DRY. My code has none of that as a result of connecting the mediator pattern to the aspnet request pipeline. I write the method that does the work and register my mediator (I call it a router) with the middleware and configure all the routes. It handles the rest.
Sounds like a plan So, for future SPA (or even rebuilding this app) you say that I should go to ASP.NET Core and work with it and a framework like React or Vue, right?
Not really. Before I could hit f12 on the method and get list of implementations.
Sure.. but you see my point, right? It's a lazy variable. Instead of a dev doing the dispose it just assumes the end of the block will do it. I don't know.. I just reads like a weird variable evolution. 
Sure.. it just seems like a solution for a problem no one really was crowing about. *shrug* Oh well.
Do you use WebApi, but just not in a "RESTful" way. 
&gt;Instead of a dev doing the dispose it just assumes the end of the block will do it. So... Like a regular old using statement? I'm not sure I see your point.
While I think the new syntax is bad for reading, I don't really get your point . Same as before using is not assuming anything, but makes use of the IDisposable interface. I don't understand what your problem is with that
I presume it will be wrapping it all up in a try/finally to ensure it's cleaned up, without it and manually calling .Dispose() if you throw an exception your Dispose() call won't get executed. I'm not sure I like this bit of sugar, I like the explicitness of the using block, just saving some indentation and some curly braces seems like an odd reason for this to exist.
The focus was at the end of a using block using variables were disposed.. now, as far as I am reading it, it looks like that responsibility is needlessly dumped to the current function or method. If someone wanted something disposed at the end of a method or function then.. dispose of it! Call the dispose and execute the intended responsibility. Using objects look like they subsume a regular variable's responsibilities with the added plus-plus of self disposing. Again.. seems like an expansion into variable territory when one wasn't needed.
And where will that web service get the connection strings from.. üßê
The Microsoft OpenID Connect library or some other SDK? My experience is with OIDC integration only and that‚Äôs pretty straightforward with .NET Core for both IdentityServer and Auth0. The OWIN implementation on the other hand has been a nightmare.
Oh, i though .net core supported arm64, it's core rt only for now ? ( just curious ) 
So, am I understanding you right, when I say that the following syntax is bad or lazy according to you? using (var v = new MyClass()) { // do stuff } 
You can handle basic authorization/permissions within OpenID Connect (which is the protocol that IdentityServer speaks). If you have more complex needs it‚Äôs better to keep authentication and authorization two separate concerns. My experience is that Authorization logic and business logic often overlap.
Ok so why is it better to have garbage code on the database server rather than on the application server? At least if my code is in c# I can write unit tests and use much better refactoring tools to improve it. What magical world are you living in where you have bad senior developers but are still making the best architectural decisions? The quality of your software depends on you senior developers. You can have them put great architecture in place, but without code reviews, junior devs will always fuck it up. 
Small applications don‚Äôt have that problem though. You ship the API and the application together for those kinds of projects. I like GraphQL but let‚Äôs not pretend it doesn‚Äôt take longer to build than a simple REST endpoint. Most applications are ridiculously simple and don‚Äôt expose their APIs for consumption by other apps.
Stupid question but I don't understand the using declaration advantage. Isn't the code disposed at the end of the method anyways, by default? What is gained here? Or does it explicitly call Dispose() before it exits the method?
I would say EF + Dapper is the magic combo. Sometimes EF is just slightly too slow and caching is complex enough that you can squeeze out some extra performance with Dapper and save on any caching complexity.
I'm talking about the management API provided by the `Auth0.ManagementAPI` nuget package. It's a hot mess.
Clearly from ANOTHER web service. And before you say anything... it will get its connection strings back from the first one!
I expect that there, but the syntax from the link indicate that is being shoved off on the function or method. Implying termination at the end of the function. Again - evolving the syntax into lazy variables.
Agreed!
It explicitly calls Dispose() before it exits the scope (next closed curly bracket) Is so you don't have to add another set of brackets and indent for the `using`; I imagine.
A variable (or rather, the object it references) actually becomes eligible for GC as soon as it no longer used, which can be in the middle of a scope. Whether or not it can be collected at that point depends among other things on debug vs. release mode builds. 
It creates a try-finally.
What are you doing with the managementAPI that's been bad? Some things are straight forward such as requesting tokens / fetching users. But other things like updating user data has been difficult as there is little documentation. I had to do trial and error on updating user app metadata as it's simply listed as a dynamic property. Although I am.doing minimal compared to what most other people might be doing. 
I use sitecore at my shop. Very powerful. Perhaps the best. Net cms. Huge learning curve and expensive, documentation online sucks. Had to learn it on the job
Is the point at which it is no longer used tracked somehow? I've never heard about this. Do you have any resources that explain how that works?
This is on .NET framework 4.7.2, but we‚Äôve done tests where we create an object, stop using it, call GC.Collect and have it garbage collected. In release mode, it doesn‚Äôt happen. Objects are tracked by either being on the stack or in a machine register, or in some static field somewhere. In this case it‚Äôll probably be in a machine register. So it can be pretty random when/if a machine register is reused. 
I can't say I'd be any help and ATM can't commit to anything but I'd definitely be interested in checking out the source code, just to see what it takes to create a language that runs on .net
Have you even bothered to read the *two lines(!)* about them? The variable is disposed at the end of the scope it is declared in. That means if you use the `using` statement inside an `if` block, then the variable is disposed at the end of that `if` block. You're *very* clearly against `using` statements, regardless of the syntax, as can be seen from your other comments: &gt;If someone wanted something disposed at the end of a method or function then.. dispose of it! **Call the dispose** and execute the intended responsibility. - &gt;It's a lazy variable. **Instead of a dev doing the dispose** it just assumes the end of the block will do it The whole point is to provide the developer with a guarantee that the `Dispose` function is invoked! So why are you so hell-bent on having the developer manually type out `myObject.Dispose();` and then having to wrap it in a `try-finalize` statement to ensure it will be invoked? Anyway, I suggest you read up on `using` statements, because it's very clear that you don't know what they do and how much code they save you - and how much they improve readability. [Read up here](https://docs.microsoft.com/en-us/dotnet/csharp/language-reference/keywords/using-statement). That link includes Microsoft's official stance on when to use `using` statements: &gt;When the lifetime of an `IDisposable` object is limited to a single method, you should declare and instantiate it in the `using` statement. - I don't think this discussion is worth continuing...
Did a quick test on dotnetfiddle, I can't reproduce that behaviour: https://dotnetfiddle.net/zaIBxt It however uses framework v4.5 rather than 4.7.2. Do you think its the framework that's different or am I misunderstanding how to reproduce this behaviour properly? 
Database 
Database 
I have to say ditto to this. It makes the code a lot harder to read.
So, kinda like Go's defer. Nice addition. Makes things less verbose. 
Formally, there won't be source code until the design is complete. The goal of the design phase is to produce a language specification. I said "formally", as I've written proof-of-concept experimental code, taking the Roslyn C# compiler as a starting point.
&gt;But if you try and write this class, you‚Äôll find that the compiler turns you away. `Span`is a `ref struct`, which means that it can only be stored on the stack. You can‚Äôt use a `Span` as a field of a `class`, or put it in an array, or box it (by upcasting it to an `interface` or `object`), or use it as a local variable in an `async` or `yield` method (because behind the scenes such methods copy their stack frame to the heap). (There are good reasons for this restriction, pertaining to memory safety.) &amp;#x200B; Couldn't this have just targeted `Memory&lt;T&gt;` instead of `Span&lt;T&gt;`? I was under the impression this was exactly the kind of scenario `Memory&lt;T&gt;` was for.
arm64 is supported.
A few concepts you'd want to support from the start: 1. Rate limiting 2. Load shedding 3. Retry logic 4. Idempotency
What's the "rule" or whatever you follow on when to use dapper vs EF? And so then you have both set up, i.e. if you make a schema change you have to update both? We still haven't quite figured out how we're going to handle our more complex queries as some of the use cases just make sense to be done with sql. 
Yeah it‚Äôs really dependent on several things. The framework includes changes to the runtime sometimes, and that cab affect it too. I‚Äôll see if I can dig up the code we used.
Declarations explicitly deal with the `IDisposable` pattern, just like the `using` block does. When you create an `IDisposable` object, nothing calls the `Dispose()` method on that object for you\*, you need to do it yourself. Previously, this was either done manually, or with the `using` block, like this: ``` using (SomeDisposableThing thing = new SomeDisposableThing()) { thing.DoSomething(); } // thing.Dispose() is called here automatically, even in the event of an exception ``` This is pretty nice, because it guarantees that the object is always disposed of, even of an exception occurs within the `using` block. Where this gets cumbersome however is when you have many objects that all need to be disposed: ``` using(thing1) using(thing2) using(thing3) { thing1.Hello(thing2.Something(thing3)); } // Disposes thing3, then thing2, then thing1 ``` What the new declaration does is effectively turn the entire method block into a using block, for that variable. It ensures that `.Dispose()` is always called cleanly at the end, without nesting crazy amounts of using blocks. \* The GC will call the objects finalizer (if it has one), which may call an internal `Disponse()` method to clean up unmanaged resources only. Managed resources cannot be cleaned up by the finalizer because thy may have already been collected, hence the need for a `Dispose()` method called by user code.
Ya to that point I might as well stick with trying to understand umbraco as it seems to be similar in that sense but free at least
And here I thought I understood IDisposable. Learn something new. Huh. Thanks for the detailed information!
If you are still at an early stage of your application, you should really not focus Kubernetes or anything complex. Make sure your application is stateless, meaning it can run on multiple machines at the same time and would not create any undesired side-effect. Then, you can easily put a load-balancer and have a nice scalable application. I love using Google Compute Engine for that, but you can do the same with AWS and Azure. Money-wise, your cost is going to be easier to manage if you keep it simple. Hope you the best :)
Martin Fowler kind of shys away from CQRS for some reason.
An open database connection isn‚Äôt disposed automatically. The using declaration is wonderful for working with these. 
Auth0 is also VERY expensive if you have a lot of users
More art than science IMO. For existing systems, I will literally just copy paste the procs into code just to get things moving. You can then starting going in and replacing the simple cases with EF while keeping an eye on any performance profiling you have in place. If there isn't any, you are stuck manually testing which still isn't too bad for most cases. &amp;#x200B; For new systems, I always do everything with EF and then just replace with Dapper as needed. I would say at least half of what I build either doesn't need Dapper or needs caching. Dapper lives in a pretty small window between EF and caching IMO. Nothing wrong with just eliminating that window if you prefer it. Some people prefer reporting databases or projections to handle those cases. 
Your first scenario is what I'm doing with a legacy system. For my 9 to 5 job we'll have to figure something out but I think that may boil down to pasting raw sql and running it with ef, at least in extreme cases. For reporting we have qlik on its own server
this is why people choose telerik. they do not want to be great. they want to make their customers great. &amp;#x200B; time to renew my subscription.
Ben Adams' posts never disappoint me 
"Better" ambiguous. It would be less performant, but centralizing these settings would make it easier on your company when it's time to change or add servers. I did this at a company years ago by referencing a shared config file.
Oh yeah, I figured Kubernetes is for major production heavy usage. I just threw it in there. I get what you mean. Appreciate the response. 
Is this supposed to be similar to Rust?
What do you recommend for Rate limiting and load shedding&gt; 
If you did, you would be able to lean on your existing C# and .Net proficiency, but if you have to really up your JavaScript skills... maybe diving into node.js head first isn‚Äôt such a terrible idea.
You are reading way too much into it, dude. Ease up. I simply didn't like the way it was presented or the reduced readability. Greenhorns have enough of a difficult time with scope let alone sprinkling in magic using variables.
Pure trash talk. Have a look at oData .
ASP.NET Core, EF Core, OData, Fluent Validator, Keycloak
Both are entirely arbitrary. But you want to strive for graceful degradation rather than straddling a fine line between being operational and systemic failure. The ability to curb excessive use (rate limits per tenant / user) and not execute lower-priority work (load shedding) are the means to achieving this graceful degradation if / when you're underprovisioned and otherwise ill-equipped to handle the load (i.e., either you're running your own hardware and you can't magically order a new server two months ago, or you're in the cloud but can't necessarily afford to provision more compute capacity). Here's a great read with respect to rate limiting and load shedding: [https://stripe.com/blog/rate-limiters](https://stripe.com/blog/rate-limiters)
Not really, only in certain aspects. I think the more similar part is resource management and move semantics, but if you followed pre-1.0 Rust development, my approach is more similar to the sigil-based smart references Rust had at that time. And reference lifetimes have more restrictions in its usage, but they don't need lifetime annotations. My language has proper class-based OO, and its implementation composition is based on mix-ins. ADTs are similar to Rust's in that they are fix-sized values and their variants form a closed set, but unlike Rust enums the variants are types forming a hierarchy. Also, although errors are values instead of exception heap objects, they have dedicated syntax for error handling and propagation. 
Go for razor components instead and build small vertical microservices fully in C#. Combine many together in a single portal UI as iframes and you can scale easily. Once the webassembly story becomes a bit better there will be a clear upgrade path to client side rendering. This will be the way C# LOB and Enterprise apps will be built 5 years from now.
I do not read too much into it. You said **multiple times** that developers should call the dispose function if they want something disposed. I straight up asked you if the current using syntax is bad and you refused to answer. I took that as a 'yes', which is the only logical interpretation, given your past statements. You made it clear that you don‚Äôt like the new syntax, but at the same time worded your comments so badly that people misunderstood them - and at the same time didn‚Äôt answer when asked for clarification. On top of that, you failed to read two incredibly simple lines, as you also made it clear that you didn‚Äôt know how the new syntax worked. This makes you seem ignorant or dumb, especially when combined with your previous very anti-using comments. You effectively seemed like an anti-vaxxer, in that you don‚Äôt know how it works, but are against it anyway. &gt; I simply didn't like the way it was presented or the reduced readability. If it was that simple, why make sweeping statements regarding all forms of using syntax? Why refuse to answer when asked for clarification? I even agree that the new syntax reduces readability in some cases! But I also think it improves it in other cases. Use whatever syntax makes the most sense at any given point, it isn't difficult. 
I have a feeling that CQRS is a significant investment, especially in the effort up front, while setting everything up. Do you encounter such projects frequently out in the wild?
Sorry, i meant windows arm64, i've seen in the doc it was only linux. as we've heard of arm64 support recently on windows, i was wondering. ( i don't see a use case for me now tough )
Yes, but you don‚Äôt necessarily know which implementation you‚Äôre going to get at runtime (although there‚Äôs usually only one!), and finding usages of a type is but a different keystroke away. 
Do you think this has any advantages over using postman?
I think this is for the people who love accessing the resource from the command prompt. &amp;#x200B;
Try this @stackoverflow answer [https://stackoverflow.com/a/41676797/2134604](https://stackoverflow.com/a/41676797/2134604)
ORM's like .net core can be pretty slow for querying, CQRS enforces you to view querying as separate operation then commands (Post/Put). I don't think its an upfront effort if you have followed CQRS well enough. This is a neat example of CQRS. [https://github.com/vkhorikov/CqrsInPractice](https://github.com/vkhorikov/CqrsInPractice) The second level of CQRS is having a seperate database, which is indeed a significant effort. Do it only when needed. In general used mediator commands to separate queries and commands. Use SQL queries or views for queries.
Thanks! You may also want to consider implementing Data shaping. e.g. : [https://jinishbhardwaj.wordpress.com/2016/12/03/web-api-supporting-data-shaping/](https://jinishbhardwaj.wordpress.com/2016/12/03/web-api-supporting-data-shaping/) 
Kudos for mentioning EshopOnContainers. The source code has implementation of ResilientHttpClient which relies on Polly. ResilientHttpClient should be default choice in most of the projects.
Why?
Thanks but I've already seen it. It doesn't provide an official or solid solution!
\&gt; The JsonDocument provides the ability to parse JSON data and build a read-only Document Object Model (DOM) that can be queried to support random access and enumeration &amp;#x200B; will it support LINQ querying?
Judging by the fact that you are using EF core, I am going to assume that you are user .netcore. In this case if you have many connection strings that you need to keep in a repository, then I have two ideas that may help. ## 1. Easy implementation, bad because storing secrets in config Create a private NuGet repo and an NuGet package that has a content item in it that is `connectionstrings.json`. add the strings in there as per a normal appsettings.json file. You can then include that in your projects and access the connection strings by just importing the config as you normally would. The benefit here is you can have your environment based config files too. ## 2. Hard because of implementation, safe because of environment variables. In your deployment scripts, pipeline, add code to set the environment variables on the host server to the required connection strings, you can import them the same way you do configuration. It is safe as the strings are in your deployment package and not in code. 
Yeah, we're kind of small still and pay over $100k a year for it. 
 L P Lp P Pop P√≥≈Ç Po Pl P√≥≈ÇüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜüèÜ P√≥≈Ç Po P P√≥üéëüéëüéëüèÜüéëüèÜüèÜüèÜüéëüèÜüéëüéëüéëüèÜüéëüèÜüéë PlpüéëüèÜüéë PüéëüéëüéëüéëüèÜüéëüéë üéëüéëüèÜ Plüéëüéëüéëüéëüéë LüéëüèÜüéëüéëüéëüèÜüéë üéëüéëüèÜüéëüéëüéëüéëüèÜ üéëüéë Püéëüéëüéëüéë üéë Ll PüéëüéëüéëüéëüéëüéëüéëüéëüèÜüèÜüèÜüéëüéë LLpüéë LlüéëüèÜüéëüéëüéëüéëüéë L üéëüèÜüèÜ LPlLüéëüéëüèÜüéëüéëüéëüèÜ LL P P PlLüéëüèÜ P LüéëüéëüèÜüéë P üéëüéëüéëüéëüèÜ üéëüéëüèÜ PpüéëüéëüèÜüéë PüéëüéëüéëüéëüéëüèÜ LlüéëüéëüéëüéëüéëüéëüèÜüéëüéëüèÜüéëüèÜüéëüèÜüéë PLlüéëüéëüéëüéëüéëüèÜüèÜüèÜüéëüéëüèÜüèÜüéëüèÜüéëüéëüéëüéëüéëüèÜüéëüèÜüéëüèÜüèÜüèÜüéëüèÜüèÜüèÜüéëüéëüéëüéëüéëüéëüèÜüéëüéëüèÜ üéëüéëüéëüéë üèÜüéëüèÜüèÜüéëüéëüèÜüéë LLpLpüèÜ L P LlL OlüéëüéëüéëüéëüéëüéëüèÜüéëüéëüéëüéëüèÜ ≈Å√≥≈ºka üéëüéëüèÜüéëüéë L LlL L üéëüéëüéëüéëüèÜüéëüéëüèÜüèÜüéëüéëüéëüéëüéëüéëüéëüéëüèÜüéëüéëüèÜüéëüéëüèÜüéëüèÜüéëüéëüèÜüéëüéëüéëüèÜüéëüéëüèÜüéëüéëüèÜüéëüéëüéëüéëüèÜüéëüéëüéëüéëüéëüèÜ Ll L LüèÜ L Ll L P P Ol Poll L Ol P L P Ll P LL P Ll P P Olol Ol P L P P LL L L Ll Ol L P P P Ol P Pllol P L Lllll Pllol PlLl Ol Ppl Ll P L Pllp P P Plp P L Lp L Ll P P Lp L L Ppl P P ll l P P P P P PlLl Ol L P Ll L O O Pl Pl L P L P L P Lll P Ol L L Ll P P P L L P√≥≈Ç P L P√≥≈Ç P P P Ll
Just had a look, not seen before. Nice effort though I notice it's SQL Server only - any plans to extend db support?
Thanks for the feedback. We [have an issue for it](https://github.com/cofoundry-cms/cofoundry/issues/171) but it's not something we'll have resource to work on in the near future. I'd really love to support postgres, or at least put the abstractions in place to allow for a community supported plugin. Much of the data access is EF Core, so it's certainly achievable, but it will still require a lot of work.
Firebase doesn't provide an official .NET SDK (at least not last time I checked). I have used https://github.com/step-up-labs/firebase-database-dotnet successfully. It's very easy to get up and running, and also returns an IObservable&lt;T&gt; so you can plug in Reactive Extensions as well.
That's reasonable but I wouldn't say that was particularly cheap, especially when there are excellent free alternatives like ImageMagick.
I can't imagine why this app would require admin privileges on my machine. After rejecting the request it ran just fine. 
The app can stream to a Chromecast device using OWIN framework server, which needs to have a port opened to communicate with. This is done through the firewall API of Windows and requires administrator privileges to do so, but don't worry everything else will work as intended. &amp;#x200B; The precise location fo the concerned source code is here: [https://github.com/bbougot/Popcorn/blob/master/Popcorn/App.xaml.cs#L109](https://github.com/bbougot/Popcorn/blob/master/Popcorn/App.xaml.cs#L109)
Would you mind quantifying/qualifying why this is "brilliant," please? It's not that I don't believe you, I just want a frame of reference and I don't want to have to dig through the entire repository just to find something.
Most of WPF applications do not follow the MVVM principles, Popcorn was mainly aimed to follow and embrace the philosophy of the MVVM pattern (i.e no code behind, extensive use of commands and data binding). Moreover, mainly WPF apps are LOB (line of business) and offer a relatively poor UI/UX, which is pretty sad in regards to the power of the framework. So, Popcorn was intended to offer a clean and efficient way to offer a good looking experience following the best practices possible.
I second you buddy. I want to learn WPF development and master it. How can I study PopCorn and see how things are done brother? 
Do not use DNN. You will regret it.
The big ones are `NLog` and `Log4Net`. NLog can be configured to handle lots of different things, and can probably can do all of what you want - if you set it up for your particular case. Log4Net is probably very configurable too though, you might want to check more on turning off the "extra text" etc.
MRW reading the title: *Ah look the monthly popcorn thread*
Absolutely log4net. Used very widely in enterprise and even medical applications.
Dunno how I feel about a title like this, praising your own project and not even mentioning what the project is, just that it is "brilliant". 
How do you know what most wpf apps are doing? I've seen a crap ton of mvvm frameworks for wpf and uwp. And why would anyone care if you are using code behind or mvvm? I'm not trolling, mvvm seems to have it's uses in terms of SOC (tho I've often considered it more of a pain than it's worth) but that means nothing to an end user.
Lmao so true. 
I'm admittedly speaking out of a fair amount of ignorance here as MVVM implementation isn't really my thing. However, based on what I've talked about with some colleagues, most MVVM frameworks don't really utilize the pattern as they have a strong coupling based around 1:1 relationships instead of letting the layers flow as they should be able to. For example, there will be a View for every ViewModel or a tight coupling between the Model and the ViewModel. This sinks the promise of MVVM out of the gate and strikes me as a gross anti-pattern.
If you peep [the namespace for System.Text.Json.JsonDocument](https://docs.microsoft.com/en-us/previous-versions/windows/silverlight/dotnet-windows-silverlight/cc626553(v%3Dvs.95)) then you'll see the Keys and Values are exposed as properties. This should support LINQ without any trouble.
But wait... That can't be.... That looks suspiciously like code-behind... Which we've been ensured this brilliant program doesn't use.
- You can use [CsvHelpers](https://joshclose.github.io/CsvHelper/) to handle the CSV serialization. - I believe file rotation is the same as a rolling file sink, which [Serilog](https://serilog.net/) provides out of the box. - The text which is written to the log is called a "Template". You can configure the template to say whatever you want or nothing at all. 
Can it be output without extra text? 
This looks interesting: http://element533.blogspot.com/2010/05/writing-to-csv-using-log4net.html?m=1
Thanks for including the final bullet point. I knew about IDisposable and Dispose() but I always thought ‚Äújust use the destructor already‚Äù. Ironically I thought Dispose() was necessary to release unmanaged resources, not the other way around!
Nlog has always handled everything we've thrown at it. 
This an example of running it on alpine. https://github.com/codenesium/samples/blob/master/ESPIOT/DockerFile
I can't really comment on any libraries - but if you have very specific requirements it may be worth looking into writing your own. That's what I have usually done in the past and they are pretty quick to whip something simple together, then you have absolute control over what you log, when and how. NB, I've never really used a logging framework so i don't know what kind of advantages they will offer if you are just writing files to disk..
These days I tend to make my libraries rely on Microsoft.Extensions.Logging. It's sort of become a defactor standard, since it's pretty much part of [ASP.Net](https://ASP.Net) Core. If the user wants to bridge in his own favorite platform, then he can. It's also not based around statics, but DI. So that's nice.
Self plug: my library lmao
It's brilliant I see sharp! Seriously now, I find sticking to the MVVM pattern is a must from my point of view not an extraordinary exception.
in case you haven't , the official docs are ok: [https://docs.microsoft.com/en-us/dotnet/framework/wpf/data/data-binding-wpf](https://docs.microsoft.com/en-us/dotnet/framework/wpf/data/data-binding-wpf)
NLog has treated me well too. We recently moved from log files to centralized logging in elasticsearch and it took nothing but a minor config file update.
Sure, you can specify the format on anything and even create new format types.
Various binding topics here https://wpf.2000things.com/2010/08/06/25-data-binding-overview/
Blame Google for this one. They are rather hostile to Microsoft. Kind of like the SDK relationship between Oracle and Microsoft. Google has been planning a dotNet sdk for years, I was hearing it was almost complete before the Google buy out though.
Yea I've kept coming back to that, but I was hoping for something a bit more concise and to the point. It's a bit long winded for documentation.
I'll take a look at the site. It might be a little fractured, but hopefully they topics that fit my needs.
[Head First](http://cdn.oreilly.com/oreilly/pdfs/hfcsharp3e_WPF_download.pdf) 
What is your particular situation? There are a couple of different NuGet packages that allow you to write calculated expressions in the bindings. IIRC I‚Äôm using [CalcBinding](https://github.com/Alex141/CalcBinding/blob/master/README.md) and it seems to work well. 
Problem is file rotation handling. Something I definitely don't want to reinvent the wheel on that one.
What library is that?
Old page about using a parameters file. https://docs.microsoft.com/en-us/aspnet/web-forms/overview/deployment/web-deployment-in-the-enterprise/configuring-parameters-for-web-package-deployment We used this on a project with a large team many years ago. I don't remember much, there was a dedicated deployment team. 
You'd have to write in csv, as well as the file rotation stuff. But it's at github.com/jmikew/waterlogged
Alternatively, those are things I should add to it anyway. I'll probably add those into it over the weekend now :) 
I bind to objects and display them for an internal debugging tool. Simplified my life for a non value added project. 
Linq2DB always looked promising, but I don't like how it pollutes the POCOs with mapping attributes.
The attributes are not necessary if your POCO fields are the same as columns in database. But I always use them anyway as I have better control over naming fields... Linq2db now also supports fluent mapping if you dont want to use attributes. 
`Dispose()` will (or *should*) also clean up unmanaged resources if there are any, but it primarily cleans up managed things (like calling `Dispose()` on other stuff). Usually when there are unmanaged resources in a class, it'll follow the following pattern: * It will implement `IDisposable` and have a public `Dispose()` method * It will have a destructor * It will have an internal `Dispose(bool state)` method. If the state is true, both managed and unmanaged resources are cleaned up. If the state is false, only unmanaged resources are cleaned up. * The `Dispose()` method calls `Dispose(true)` * The destructor calls `Dispose(false)` 
Like handles, right ? 
I‚Äôm going to go old school on you recommend a book called [Pro WPF](https://www.amazon.com/dp/1430243651). It has the best explanation of how the guts work that I‚Äôve seen in my ~10 years of dealing with it. 
Having been (forced) to use firebase for a project. I really can't recommend it. There are other way better document style databases if that is what you really want. Further, google (as they are wont to do) is no longer maintaining the project and have passed it off to a third party for maintenance. 
I am not sure what you mean by that. Perhaps you are misunderstanding part of my post but I can't figure out what. .NET does not use handles as it is object-oriented. Underlying OS APIs it calls may require handles but in those cases .NET takes care of that behind the scenes for you.
And no, by default, \`Dispose\` is not called at the end of the method if you don't use \`using\`. In fact, there is a chance \`Dispose\` is never called.
I had checked the github before posting to see if the situation had changed from what I remembered, and I didn't see any references to fluent mappings. Do you have a link to docs or samples? 
same!
This looks promising, thank you
Probably not exactly what you are looking for but I have found this cheat sheet to be helpful [WPF Data Binding Cheat Sheet](http://www.cheat-sheets.org/saved-copy/WpfBinding.pdf) 
Try [Ngrok](https://ngrok.com/) 
If you're having a problem with something on GitHub, yes, the proper thing to do is to create an issue. 
Which, many people found out if they ever used EF and forgot to close DB connections. 
You'll need to open up tcp port 80 on your windows firewall if you want to connect to it from the same network. If you want to connect to it from an external network you'll additionally need to forward port 80 on your router to your web server. [This](https://docs.microsoft.com/en-us/sql/reporting-services/report-server/configure-a-firewall-for-report-server-access?view=sql-server-2017) msft document about report server has directions on how to open port 80. But if you just hit start and type firewall you should be able to figure it out. The port forwarding is router specific but usually pretty easy to get to. &amp;#x200B;
[removed]
[removed]
Look at this : https://docs.microsoft.com/pt-br/dotnet/standard/microservices-architecture/microservice-ddd-cqrs-patterns/ddd-oriented-microservice
I was curious how people were liking it so far also, although I haven‚Äôt used it yet.
Thanks this gives me some direction. Sounds like separating pieces into classes is the way to go. 
 This is something I plan to look at this year. From the very little research I have done I think you can: Stand up a GraphQL endpoint in front (or alongside whilst transitioning) your existing REST API. Use custom GraphQL resolvers to leverage existing REST APIs to build response objects. Switch your front end to use the GraphQL endpoint (a bunch at a time so you don‚Äôt overwhelm yourself with bugs). Start removing the dependency of the legacy REST APIs. Custom GraphQL resolvers for nested objects can be very complicated and difficult to maintain. Eventually deprecate your legacy REST APIs and just use the GraphQL endpoint. 
League sounds like a value object. Does it have any behaviour or maintain any business invariants?
Our company rewrite one of these things once every two years over the last decade. It always tend to be outdated by the time we start a new one. Also since people have different opinions on which framework to choose, it tends to have adoption problem by the next bunch of devs. If you could solve these two issue (I.e. able to be easily updated as new frameworks come out, and dynamic enough that anyone can add their own thing without pain), then by all means, go for it. Also, there's quite a few existing templates based on Clean Architecture, DDD etc, how is yours different? I.e. Why would I use yours over these? https://www.google.com/search?q=dotnet+core+project+clean+architecture+scaffold
I think I must be missing something, but why don't you just talk directly to the database?! What is graphql giving you on top of that (other than a http/json endpoint on top of the database)? That missing security section would worry me a bit too.
[removed]
&gt;e if the situation Unfortunately their documentation is not very good. I came to found some issue tickets [\#1089](https://github.com/linq2db/linq2db/issues/1089) and [\#1160](https://github.com/linq2db/linq2db/issues/1160) where you can see the fluent syntax and how it can be used. Personally I never tried the fluent mappings as they're fairly new feature and I'm used to the attributes style mappings.
Why use GraphQL over OData? I'm just curious since MS seems to already has a lot of support for it out-of-the-box. You wouldn't need to create models from scratch. 
What do you mean by different framework?
If you're using EF Core, PostgreSQL has better support than MySQL : [http://www.npgsql.org/efcore/](http://www.npgsql.org/efcore/) And PostgreSQL is a great DB.
ORM, DI, unit testing library, auth library etc
I second this. PostgreSQL has far wider support for basically anything anyone might want, it‚Äôs always been stable and they care about standards. They might not always be the fastest in everything but data integrity, stability etc should be the most important thing anyway. 
You can try to install the Roslynator extension. Contains a lot of refactoring options.
the answer will depend on : what need are you looking to serve by moving to GQL?
I woild like maintan some behavior, like get all teams of a league, thw time with the beats position...
try this: Try this tool: https://github.com/icflorescu/iisexpress-proxy npm install -g iisexpress-proxy iisexpress-proxy &lt;yourport&gt; to 3000 then its just: http://&lt;your-ip-address&gt;:3000 
I think the problem is that Clean Architecture is just that, a organising principle, whereas a script (or solution template) is initialising a concrete set of projects which might or might actually meet your needs. Is there more to it that just creating a few default-named empty projects? When you say nuget packages, are you specifically setting up an [ASP.NET](https://ASP.NET) Core solution with EFCore, etc., etc.?
as I got WPF is now supported in Core 3 ?
We're in the situation where we could have at least 10 or more projects to start under this model, with more going forward. The existing templates are fine, but they're more of a copy/rename/search+replace rather than a scripting model. The goal for what we're building internally is to be able to generate the projects/solutions, and correct references/dependencies. We don't need the ability to support multiple ORMs, separate unit testing frameworks etc. We might need to be able to support different UI (Angular or API only), or different databases (Postgres, SQL Server, Cosmos). I've never released anything publicly, just trying to see if it will be worth it.
The requirement for us is to generate the solution/project structure ready for the actual domain logic. Right now, this is just the project structure, adding the references and nuget packages (MediatR, NSwag, etc), but could extend to hardening or similar. Examples of changes could be: separate angular or ASP. NET angular project, different RDBMS. We could integrate IdentityServer at some point. As I said in another comment, my question was more about whether there would be value in releasing, rather than what is required/desired when releasing. I've never released any code/scripts publicly before, so it could just be a learning exercise.
&gt;I am missing not having ReSharper in it, for generating Constructor injection. I don't see it as an option on the menu. Trying to see how I can get on without RS Are you even aware what ReSharper is?
If you have experience with SQL Server, try it on Linux. It doesn't support SSRS, SSAS, or machine learning services, but if you don't use these things then it's production ready. 
Sounds like the app will be primarily serving reads? Typical DDD patterns might be overkill. An ETL process to load the match results into a nice relational database schema and clear mapping from API requests to efficient SQL queries might be more important than modelling aggregates.
I've been using odata v4 when needed. It's been great
Im doing this only to study the ddd pattern
I third this. We use it now with asp.net core on Linux.
I'd go for Postgres. I started testing it out recently and it seems pretty cool. My main issue is that I've been struggling to get things done with Linux distros, but Postgres also seems pretty good on Windows. I did some simple tests with Entity Framework and found that SQL Server was generally faster, but not by that much, and considering how expensive SQL Server will become when you pass the 10GB Express limit, I now feel that it's best not to get involved with SQL Server. So Postgres is a pretty decent alternative. Not quite as good but pretty good overall.
So what happens if the log server isn't available at the time of a write? 
.Net Core 3 only supports WPF on windows.
I wouldn't say that EF the best idea if we r talking about fetching data from db (creating lots of object in memory, ect)... maybe better to have light-weight component to connect/fetch data directly from db. Feel free to check how we did it - [https://www.nrecosite.com/graphql\_to\_sql\_database.aspx](https://www.nrecosite.com/graphql_to_sql_database.aspx) 
the one big thing with postgres is it doesn‚Äôt have native fault tolerant, high availability clustering.
Sure I‚Äôd like to see it. I‚Äôd be more interested in a blog post on how it works and the methodology than actually using it though. 
Yep. There are official container images from Microsoft on docker hub too.
You can configure more than one target and write on both file and log server. I use serilog in every project and I prefer it more than others 
We're using a similar model but instead of a run-once script, we set up a "framework" git repo that you clone to start a new project. This way, if there are fixes or updates, you can pull these from the framework remote into your project repo as needed. It has been helpful for keeping up with latest .net core and Angular releases because most of the work to upgrade is done once in framework and then merged into downstream projects.
SQL Server isn't supported for Ubuntu 18.04 yet so I'm forced to use Postgres, which is a surprisingly good experience. I've even forced myself to limit interaction with the DB through the Terminal only, rather than using PGAdmin or DataGrip, so as to get better with the command line in general. Postgres will now be my chosen DB for any OS other than Windows.
Why not use [AdaptiveClient](https://github.com/leaderanalytics/AdaptiveClient) than you can use 'em all! 
Why are you forced to use Postgres? You can run sql server in a container
that AdaptiveClient has to connect with service layer (written in .NET Core) and that service layer has to connect with some persistence data source i.e. RDBMS here that can be installed on linuc? so ultimately again question is choice of RDBMS for linux server
We're using it on a side project that we plan to host on Digital Ocean if I'm not mistaken. So far it's worked great locally in terms of getting things set up. Main reasons we went with it were SQL Server costs too much for a project we can't really make money on, and since MySQL is now owned by Oracle, well, fuck Oracle. :P
Why would you ever use MySql? I've never understood it. Postgress is a real enterprise RDBMS if you wanna go open source, otherwise use SQL Server on Linux.
I fourth this. And using stuff like trigrams for fast search in strings is a godsend. 
Interesting. Does Sirilog handle file rotation?
Curious how this would look in terms of performance/scalability... wouldn't this feature be a nightmare for larger sites with thousands of hourly visitors? I'm sure when deployed in an intranet environment there wouldn't be much of an issue. Curious to see how this would scale with the server side diffing + storing the DOM snapshot. Does it store the snapshot for each session/user? Hmm...
Yes by date, size or date look in the following link, in my case I combine date (every day) and size 500mb maximum per file to not have file too much big to open. https://github.com/serilog/serilog-sinks-file/blob/dev/README.md
Yes my post is not a direct reply to your question. The intent of my post is to say that AdaptiveClient will allow you to implement a very thin layer of abstraction between your service layer and your RDBMS. This will allow you to implement several different RDBMS at the same time if you choose to do so. You may want to do this for no other reason than to try different platforms while you develop your application. You can see example applications that switch effortlessly between MSSQL and MySQL [here](https://github.com/leaderanalytics/AdaptiveClient.EntityFramework.Zamagon).
We use PostgreSQL Cloud SQL on GCP, and it has a high availability option just like their MySQL version of Cloud SQL. How do they implement that?
Most of us probably don't have this problem
I'm using https://mariadb.org. It's free and opensource made by original MySql developer. Been running for over a year on Debian server. No problem so far. MySQL and MariaDb do not support EF on core still. But there is this: https://github.com/PomeloFoundation/Pomelo.EntityFrameworkCore.MySql
i'd say that's a 100% incorrect statement. most dotnet (and java developers) are enterprise developers. and i'd also most are in finance and medical fields. so i'd say stuff like fault tolerance and high availability are extremely, extremely important. which is why those industries continue to use enterprise level solutions like MSSQL, Oracle, and Sybase. that aside. how would you scale the postgresql server instance natively? from what i know (forgive me if i'm wrong), there are only a hodgepodge of various/random commercial solutions that offer it, with unknown credibility. every modern/new database have clustering. especailly in the cloud/container era where scaling is as easy as spinning up a new instance. so why postgresql doesn't is so odd to me. hopefully it's something that comes in the future as postgres has a ton of awesome features and a huge engaging/active community.
amazon and google have their own custom Postgresql versions.
https://www.citusdata.com/blog/2019/01/24/microsoft-acquires-citus-data/ 
nice. that's awesome. but ideally the clustering is native in the open source free version of postgresql.
Gonna echo the Postgres crowd. We use it with RDS so we really only worry about ensuring it's on the right subnet and the schema.
It's just as available as my app is.
https://cloud.google.com/sql/docs/postgres/high-availability Looks like a custom solution; they use a "regional disk" which synchronously writes to two zones. When the primary fails, the secondary mounts the regional disk and takes over.
Thanks, great post!
Exciting News!
You don't need feedback. Just release it and see what happens.
Generally your models should be modeled around \*behavior\* and \*reasons for change\*. When would a league change? Would it change at the same time that a club changed? DDD is really focused more on behaviors than queries. If this app has very little behavior you may want to come up with a better example to learn from.
that's awesome and exciting. but that's not the same as postgresql postgresql. that's like comparing MySQL with MsSQL or Sybase or whatever. they are different products based on the original RDBMS Sybase.
postgres is great, but its not enterprise software. there are a bunch of commercial enterprise databases based or extend on postgresql though.
The league will change each year, could you suggest some example ?
Not for me with JS I have to go back to 2017, &lt;script&gt; tags strip all the code of its color even the comments are white. I guess I mistook CI tools from Roslynator for RS
Maybe start smaller,say a single game.. Think start match,start period,score goal,give penalty,send off,etc Then you only deal with game,team,player Then slowly add in league,round,etc 
Really enjoyed the talk, and the speaker was very impressive. Fast tempo.
Like hashicorp consul and/or vault? Sure. 
GraphQL isn't going to solve your CUD needs in any magical way, you'll still need to create a load of mutations yo represent these actions. What it will provide is a single endpoint for all your R (read) needs (query). I'm not sure how many endpoints you have for reading data, but I LOVE building my own queries on the client side.
I wouldn't. I'd use GraphQL instead. To quote a talk at NDC London this year: GraphQL will do to REST what JSON did to XML
Did they share any more information (or do you know of any existing details I may not) that describe what their plan/goal is for the WASM/client-side runtime? As I understand it the WASM prototypes are based on mono and I'm guessing that's still the case, but is that the plan for shipping 1.0? No plans to expand/consolidate on the Core runtime? I think Blazor has a lot of really cool potential but even with .NET Core moving as fast as it is these days I feel like we're 2 years away from having something viable running on the client. I'm choosing to use viable to mean package download size and performance competitive with angular or react. I know mono runtime bits end up in Xamarin mobile apps but those app packages are generally pretty large and a far cry from a reasonable size for a web app. Anyway, thanks for sharing!
I chose PGSQL specifically because of this middleware with core on Linux and because of its GIS capabilities. 10/10 would recommend.
What behaviors does a league have? Add/remove a club? Start/end a season? Are there invariants? Min/Max number of teams? Prevent starting a season when one is currently in progress? Is a season a dedicated thing? Does it have behavior? Scheduling matches, Rescheduling due to weather or other circumstances? What behaviors does a club have? Join/leave league, recruit/drop player, etc. What behaviors does a match have? Start/end period? Start/end match? DDD is about modeling behaviors, not data - model the things in the vocabulary of the users of your system. I'm not a soccer expert so its hard for me to answer this for you. DDD is very difficult as a pet project if you aren't the expert of the domain you're trying to build for. Your users should tell you what the behaviors are. &amp;#x200B; &amp;#x200B; DDD is more useful in situations where you have interaction - lets say a point of sale system. There you may have Aggregates like an Order, which contains value objects of Line Items. You may model behaviors of ScanItem, KeyItem, IssueRefund, AcceptPayment, etc. Each of these behaviors will have rules around what is valid or when it is valid for those behaviors to be issued. You can't accept payment for an order that has no items, or you can't issue a refund for an amount greater than the items.
We use octopus deploy - in our Web.Release.config we have placeholders for the variables and part of our deployment process replaces the variables in our web.config with the variables that should be used for that environment. You can also set it up so that variables match the setting names and automatically replace whatever is in the config file.
3.0 if it's a desktop app
Postgres has a lot of quirks that are annoying as hell if you're coming from MySQL or SQL Server. You have to run a bunch of commands just to give a user full access to a DB, for example, and the syntax for an identity key is obnoxious. Still a good DB, but it's annoying at times.
CITUS is an extension, not a fork.
been [here](https://docs.microsoft.com/en-us/windows/uwp/get-started/get-set-up) yet?
OP I know this isn't the most helpful response but we use Octopus to automate specifically what you're talking about.
Wasm runtimes are being built by mono team and corert teams. See, https://github.com/dotnet/corert/issues?q=is%3Aissue+is%3Aopen+label%3Aarch-wasm And, https://github.com/dotnet/corert/issues/4659 I don‚Äôt think they know which one will be used in the end.
After enough headaches with AWS, I have finally signed onto the Azure train with most of my peers. Your experience here is largely dependent on what exactly you need it to do. All of the cloud systems have their own strengths and it's not like you can't use both at the same time. Maybe you build your caches out on AWS and your platform and databases out in Azure. Docker is definitely interesting and _would_ work for a large scale deployment; it just depends if you want to build for it or not. In my experience, it's much easier to integrate with a .NET Core setup using some of the new models like the generic host with a service collection. You may run into trouble if you try to deploy something using IIS. As far as grabbing images and making containers? No problem at all. I am not any expert on it but it seems easy to pick up and Docker Hub has tons of images to work with. Chocolatey can also make your life easier here. I don't know about you but a big reason that the hosting / docker model is attractive to me is that I don't have to deal with getting the biggest servers. The goal is to hopefully scale your workers up and down as needed which not only saves money but is a lot less headache when one of them goes down. Containers offer a lot of QOL.
Yeah, the C# Inn run by /u/Kaisinell on Discord is a great place to talk about projects, be a mentor, answer questions, discuss useful resources, share your writing and all that. I highly recommend checking it out just because it's a change of pace from the usual Slack experience.
...besides using a better ORM. *ducks*
that's not an ORM issue. 
It is 100% an EF Core issue. EF 6 didn't have this behavior, nor do any other ORMs that i'm aware of. 
 third. postgres is what you want. npgsql is quite good. 
&gt; the syntax for an identity key is obnoxious ? ??? How is adding `GENERATED ALWAYS AS IDENTITY` to the end of a column definition any harder than adding MySQL's `AUTOINCREMENT`
&gt; You have to run a bunch of commands just to give a user full access to a DB GRANTs aren't exactly exclusive to PostgreSQL, [MySQL has them as well](https://dev.mysql.com/doc/refman/5.7/en/grant.html). &gt; the syntax for an identity key is obnoxious ? Making a column have the [`SERIAL` or `BIGSERIAL` type](https://www.postgresql.org/docs/10/datatype-numeric.html#DATATYPE-SERIAL) is obnoxious? You just make the column a `SMALLSERIAL`, `SERIAL`, or `BIGSERIAL` type... it's not exactly har
$ though. sql server works great. but is expensive. 
We use Microsoft.Extensions.Configuration with a custom provider that uses web.config as a fallback. We set config via environment variables on the server. This allows us to use the same config framework in both .NET 4.6.1 and .NET Core apps
You speak as if this is a side effect and not completely intentional. While I'm not saying the EF is the best ORM, using this additional functionality as a strike against EF is odd. Especially since it's easy to handle. I don't ever see a use case for me to use it, but that doesn't mean that one exists and that this functionality isn't useful.
[Webpack](https://webpack.js.org/) is fairly standard. We use it on most (if not all) our projects, most of which are ASP.NET based (Sitecore, Umbraco, AspNetCore, etc...).
Thanks. I've been researching Webpack but I don't know a lot about it. So can you please tell me if I understand this correctly? - With Webpack, do I pass in a bunch of JS files (that I would normally create a single bundle with) into it and it creates the single file at build time? Then that built, minified, file is referenced in our code. This way, our users don't have to retrieve these from their browser, therefore loading pages faster. Is that correct?
It sounds like you're "globbing" a whole lot of files together to produce 1 JS file (or multiple based on whatever rules). The difference with the webpack route is that it builds an output file based on a single entry file (you can have multiple entry files, producing multiple outputs, but for simplicity sake let's just focus on one). So you can tell webpack to build a file `bundle.js` where the entry file is `main.js` main.js may contain something like: ```js const $ = require('jQuery'); const moment = require('moment'); const app = require('./myapp'); app.init(); ``` Then webpack will process that entry file (main.js) and produce a bundle.js file output. You can use other loaders to do fancy stuff like babel for "transpilation" (ES6/Typescript to plain JS), style-loaders for SASS/PostCSS/etc... Don't let some existing configurations out there scare you off webpack, in its simplest form it's very powerful and straight forward. [This is a super basic webpack config](https://gist.github.com/brendanmckenzie/d7b40a3994ddef3173abb49e3b99c279) that supports babel with React (JSX) syntax
I'm new to the game, I've been using Rider for a bit now. In a Fedora environment, It is fast though compared to my foray with VS 2017 which seemed to be a slug. Even with R# off. And that's running on a pretty beastly machine. it does have it's issues. Randomly freezes on creating folders. As a Scrub and still learning sadly doing a follow along tutorial doing a MVC site and seeing features missing. The one thing that i'm finding to be a big pain is lack of scaffolding which J'Brains doesn't plan on implementing as people have been asking for it since last year. 
&gt;UWP apps use CoreCLR for Debug and .NET Native for Release Source: https://blogs.msdn.microsoft.com/dotnet/2015/07/30/universal-windows-apps-in-net/ There wasn't an anchor link for the section pertaining to your question, so just search the page for "CoreCLR".
You are only partly correct. EF6 did not give you the option of partially running the query in memory; it threw an exception by default.
It's a bad default. It should be opt-in on a query by query basis. This is the kind of thing that will look like it works, but fall apart in production when the database size grows. Using it needs to be a concious decision. *** And with EF 6 it is. Calling ToList mid-expression says "everything past this point should be processed client side".
Sure it did. Not by default, but you can opt-in with ToList mis-querry.
I‚Äôm not making that point. I‚Äôm saying it‚Äôs silly to say it‚Äôs a bad ORM because it has added functionality that can be dangerous. 
It's one of many reasons I prefer EF 6 over EF Core.
Sure. You can do that EF Core also. In fact, that's what this blog post itself says. Doesn't mean EF6 is good or that EF Core is bad.
Yes! Come join us, as we have people who love discussing architecture, helping others better understand. We need people like you üôÇ
Pit of success: APIs should direct people to safe and correct usage. Advanced features that are hard to use correctly should be available, but not the default. 
It's not a bad default, because doing query work on the client side **IS NOT A BAD THING**. In many cases it is actually desirable. What you are doing here is moving work from the database level where scaling is difficult and expensive to the client where scaling is easy and cheap. That's a **GOOD** thing. Now obviously there are certain kinds of actions which a database server can do better than your code can, but they're rare. You absolutely **should** be aware of what you're asking your ORM to do, but if you're defaulting to client side your performance problems are much easier and cheaper to fix. 
I too have encountered what I consider bad defaults in EF Core - for example, it's surprisingly easy to trigger a memory leak with the query cache, which you also can't easily control. Exactly as you said... My scenario worked fine... Until it hit production. 
For fucks sake people. Doing query execution on the client side is more often than not a **good** thing. It's **much**, **much**, **much** easier to scale your app than it is to scale your database. Stop obsessing about how to avoid client side execution and start learning how to write queries, ORM or otherwise that aren't shit. 
&gt; What you are doing here is moving work from the database level where scaling is difficult and expensive to the client where scaling is easy and cheap. Read the article again. Sucking down a whole table is going to be a huge hit to the database. 
With the query cache? What happened?
It‚Äôs too bad that this changes it to a runtime exception and not a statically-evaluated compile-time exception. 
Wrong. Pulling down the whole table is **not** a huge hit to the database. In many cases it will be substantially less work than running your query on the database. Now this isn't always true, a query using clustered indexes will be faster on Sql, and you need to be a bit wary if your query is drawing back particularly large varchars or binary blobs, but it's usually at least no worse. Sorts in particular are hugely expensive in SQL, whereas they're pretty cheap in app space. This doesn't mean you won't need more CPU to do this, but CPUs in app space are cheap and easy to scale. 
Sorting in the database vs client doesn't change the amount of data being loaded or transferred. That's why it's safe to move it client side. Downloading a whole table is a completely different matter. 
Not when it's at the expense of dramatically more I/O. 
You can only have one clustered index per database table, for obvious reasons. Heap index scans are still expensive. Database tuning is far more complex than you sem to think, and again database scaling is expensive and difficult. Scaling your app up or out is cheap. 
Yea, and its a hell of a lot harder if you can't efficiently use your cache because you are literally loading entire tables into memory. Also, I strongly suggest you read up on the term "covering index". A well written query with indexes not only dramatically reduces the number of rows that need to be read from disk, but also the number of columns. Then look up the terms "index lookup" vs "index scan" in regards to performance tuning. This is rather important. Finally, if your database is so small that you can afford to shift entire tables to the client with each query, then you have no reason to even be talking about scalability.
That would be nice, but I can't imagine how the compiler could figure that out. 
Client side resources are cheap, database resources are expensive. Even if you're using something open source and free scaling out your DB is extremely non trivial. That's not to say that you should always load the whole table and filter on the client side, but if you do it wrong it's really easy to diagnose and fix whereas fixing too much load on your DB is extremely hard. 
&gt; database resources are expensive. Which is why you need to learn how to minimize I/O and RAM usage.
Rust does it with type-based static assertions. C++ too (generics-based).
And you should look up the difference between a clustered index and a heap index and how they actually work. If your table has a clustered index of an appropriate type (int variants work best for MS Sql) it will cut the index in half and do higher lower until it finds the result. It's fairly efficient, but it's still going to get slower. If you have a heap index it has to scan the whole index, this doesn't scale well at all. Better than no index, but not well. All of these will cause contention and locks. Covering indexes can work, but you need one of those per query and too many indexes will slow down insertion dramatically. They also require more resources on the database side. If you're in a situation where your whole table isn't in memory on the DB anyway you're in a situation where you should be archiving. That's a much better choice for performance than any index. If you're in a situation where you're querying the DB for the same unchanged dataset over and over again you can cache at a number of other points. And no, small tables don't mean you don't need to scale, small tables are part of how you scale. 
&gt; &gt; &gt; If you have a heap index it has to scan the whole index, this doesn't scale well at all. First of all, it's just called a heap. It is not a "heap index". Also, WTF do you think happens when it copies the whole table to the client? 
If you are in a situation where your whole table is not in memory at all times, it means you have data which shouldn't be in that table. The best way to improve query performance at every level is to separate working set and archival data into separate tables. Your working set is small, memory usage is lower, and everything is faster.
Non clustered indexes behave as heaps, even though they aren't on heap tables. You get one clustered index per table **every single other one behaves like a heap.** And yes, a full table query will pull the full table, but it will do so quite quickly. 
Return different types based (but still subtypes of IEnumerable) based on whether the query would go to the DB or in memory. Then assign the result to a non-var variable
WTF? No. That's just, stupid. In most databases databases a non-clustered index is just another b-tree just like your normal index. The only difference is that it always includes the clustering key so it can do a join between it and the clustered index in case it doesn't have all the fields you want. By contrast, a heap is not a b-tree. It is just a series of pages without the tree structure above it. More info: https://www.sqlshack.com/sql-server-table-structure-overview/ 
P.S. Think about what you are saying. If a non-clustered index was a heap, then it couldn't act as an index. The whole point is to be able to jump to a particular record. Which you couldn't do if the "index" required you to instead scan every page like you would with a heap.
A "nonclustered covered index" creates a dramatically different execution path than a "nonclustered index"
Consider this query: var products = context.Products .OrderBy(p =&gt; p.Price) .Select(p =&gt; new { p.Id, p.Price, Tax = CalculateTax(p.Price) }); How would the compiler know that this cannot run in the database? The return type of `.Select` isn't going to change if you replace `Tax = CalculateTax(p.Price)` with `Tax = p.Price * 0.075`. And even if it could, how does the compiler know whether or not `CalculateTax` is mapped to function in the database? (EF 6 could do that. I don't think EF Core can yet.) 
Side rant: Far too many people who say "joins are evil" don't understand that their non-covering index has to perform a join to the clustered index.
Yes, but indexes slow down insertion and use up resources you can't just have a million of them. 
.net core or framework? There is a bundler built into both, and they are pretty straight forward. In the bundle config add the files to a bundle. Add a reference to the page. You really wanna keep a copy in your project too and have fall back. You can also set it to output the cdn reference when the debug flag is false or your bundled files which are minified and bundles. Or the full just file in debug mode so if you use a browser debugger it's easier. I add the original just file to my project for auto complete and there's additional code to load from my app server if the cdn is unavailable. I would suggest you don't bundle everything in 1 large bundle but a couple segmented by areas of the app. Use your cdns and if they've hit that cdn before they won't load the file at all. With http2 I am not sure if a single file bundle is that good as it has to download the whole file before processing and http is actually better with multiple files. But http1.3 only browsers really need a bundle. You should always minify production code.
Even a clustered index doesn't allow you to jump to a particular record. That's not how indexes work. A non clustered index involves scanning the entire index, which if it's a covering index may be a substantial portion of the table. An index can improve speed, but database indexes are not like array indexes. It's not an O(1) operation. That's why archiving data is so important, because databases slow down dramatically as you add rows. 
The page you linked doesn't even mention non clustered indexes. They're not stored on the table at all, which is why you need a link back.
perhaps they mean this https://github.com/aspnet/EntityFrameworkCore/issues/10535#issuecomment-375116203
Yeah, encountered this... When you're creating nearly 2k new entities a second, the memory starts to leak... Took me way too long to pinpoint it to EF, because it would take hours to get bad.
In my experience with ef core, many of the client side execution takes the form of what could be one sql query turning into potentially hundreds. What have you run into that didnt introduce that many network roundtrips, latency, overhead? 
The specific problem which bit us is this: https://github.com/aspnet/EntityFrameworkCore/issues/8223 (that's not my issue either, just some other hapless victim) Its still present in 2.2. To the best of my knowledge there is still nothing which evicts expression trees from the query cache. This makes it regrettably easy to miss bugs like this in stop/start conditions like debugging, which will bite you in production. 
I think you'd have to go read the code for a full picture. At a high level I'm pretty sure its just dotnet sending a call to node js saying "generate this site" and gets back a string which it then squirts out into the get result. In other words, nodejs does the heavy lifting running the js to produce the initial page html and dotnet simply renders. The exact Comms protocol I have no idea. RPC sounds a bit weird to me but possible. With regards to whether nodejs runs as a server I'd say if it's rpc then probably. At the end of the day it's just a program being executed and talked to by another (dotnet) program.
Edit: RPC does make sense I was mixing it up with something else
I didn't, but thanks for finding that for me! We also got burned by generating expression trees for EF Core to run (we ported this code from EF6, where it worked without obvious defect). The generated expression trees were another case where the query cache blew up in size, similar to what that reporter described. 
In our case, we had instance members captured as part of an expression tree, I've linked the github issue in one of my other posts. 
I've been using SQL Server on Linux for some time and haven't had any problems, if anything I'd say it runs better - it's certainly far easier to install than on Windows. That said, my use is because I ported a legacy database from Windows but I would probably choose PostgreSQL if I had the choice simply due to the license costs.
A covered index doesn't create indexes on every field included. You could have one nonclustered index that includes all the fields and it'll be efficient.
This sub gets one of these a month, about how to avoid client side execution at all costs. Not just in EF core, in every ORM. Client side execution is not always a bad thing. It is in fact often a good thing, but these articles always say it's terrible and a bunch of people read them and learn something stupid and wrong. If you're getting completely unexpected results out of your query in terms of performance or data it's because your query is bad. Learning **why** your query is bad and how to fix it is the answer, not rote rules if thumb which are wrong. 
Please take your advertisement somewhere else.
A covered index is efficient when it contains exactly what you need for your query. No more. No less. Every field you put into your covered index will need to be synced on any update, transactionally. Locking the row and the index while doing so (remember there's no key from original to index and non clustered indexes are not a B-tree, they're sorted but not a tree). Every field you put in your covered index will be retrieved by your query, just like if you were retrieving a full table row. Every field you put in a covered index takes up storage and forces data you might need out of memory if you don't have enough. You can't write an infinite number of indexes because they slow down other operations. Covered indexes are awesome, but you can't use them to speed up every query. 
Check out this YouTube video on this topic https://youtu.be/m3lg1Df8H0w
so, do you have some decent experience to share or not?
Usually the problem isn't with the ORM. If people would treat ORMs as complex abstractions that take some serious time to learn, you wouldn't have these kinds of problems. 
/u/scottgal I use Jekyll and generally I like static generators because of the amount of control I have over what they're generating. The downside I'm finding is the ease of use - as in - creating and editing posts. I'm aware of cms's for static generators and was wondering if you'd come across a good one? What do you use? Also, I just figured out I follow you on Twitter :-)
I have explained this about a dozen times in this thread. Sql resources are expensive. They've got high license costs and they're very difficult to scale up. App servers are cheap, and well designed Web apps are really easy to scale out. This basically means that work done in your app is dramatically cheaper than work done on your database server. Given that in most cases you will colocate your app with your database server transport between the two systems is cheap and fast. This means that rather than being something to avoid at all costs doing work on your app server is often desirable. Specifically, the example in the article is poor because the sort in Sql is much more expensive than doing the same thing in code. We don't talk about that though, bevause we're obsessing about the select not being done on the server side. However unless the table contains a really huge amount of data doing that sort of the server will cost **more** than doing the select locally. I understand the natural desire to make **your** code as lean and efficient as possible, I've fallen into that trap as well, but the performance of **your** code is irrelevant. The performance of the entire system is what actually matters. This constant obsession with avoiding local execution means we won't actually talk about or discover this. Instead we'll up vote some random who wrote something that's not even accurate, **again**.
EF Core can do that since 2.0 at least IIRC
Ah I see. I plain don‚Äôt agree in this case. Whenever Ive had a performance issue with EF Core, its been because of client-side evaluation. All in Azure. Its not obsession with the calling code, running lots of queries itself is just more often an overhead than not.
Again. The performance of your code is irrelevant, the performance of the **system** is what's important. Your code will be slower if you do client side evaluation because it is doing more work, but work in your code is cheap, and work in your code scales out more easily. That doesn't mean you should **always** do client side evaluation, but avoiding it like the plague is simply wrong. In particular, sorting in Sql is horrifically expensive. 
yeah, the system was slow. ive never encountered server side processing in sql being slow, but the team here runs into client side evaluation being slow all the time. many times. Sorting in sql is *necessary* if you want server side paging, otherwise you need to download an entire set of results, then sort and trim client side, which is massively inefficient. if you dont, there arent that many records. what are you sorting on? thats a very sweeping statement. 
Time it. Run a query which orders on the server and another which orders locally. Pick something that isn't best case too. You'll see that the server is slower. Try running a hundred of them on different clients simultaneously and see the Sql start to lock and the client side doesn't. Think about where and how you can cache that work or share that data. 
ok just get it over with. plug your own ORM so we can be done with this
Edit: I guess reading is asking to much ofc cuck dev.
I use this to throw errors for client side evaluation. Loading the whole product db probably isn't a good idea. If you can convert your method to an expression, you may be able to prevent the client side evaluation. public static Expression&lt;Func&lt;decimal, decimal&gt;&gt; CalculateTax = (price) =&gt; price * 0.15m; 
Nope. He wants one with LINQ.
Keep reading past the first page.
Again, non- clustered indexes are b-trees (usually). Learn how those work.
&gt; ive never encountered server side processing in sql being slow and have that fixed by client side eval, Scalar functions in SQL Server Joins in MySQL Sorting without indexes in any DB. (Not a performance fix, but it does reduce pressure on the DB) Those are the only ones I can think of. 
I am actually trying to improve architecture of a new application I am working on and have been reading pro [Asp.net](https://Asp.net) MVC 5. Imo the author does a great job of explaining separating the data access layer from rest of the application and he does it by putting it in a class library, which is what you are trying to accomplish. He also teaches repository pattern, dependency injection, unit testing and mocking data storage for testing. I think it might be worth to take a look at it. It explains things fairly well.
Thanks I will be picking this book up. That sounds perfect. 
I wonder if I should look into his Pro ASP.Net core MVC 2 book instead. I‚Äôm not using .Net core for this project but in all future new projects that‚Äôs what I‚Äôll probably be using. 
You can find a pdf of MVC 5 online, which will give you an idea of the content. Then, you can purchase core MVC, if you like MVC 5. I also plan on picking up the newer book just to see what has changed.
I like the colors, but what's with the extremely low contrast? It looks like it's meant for a dark background, or something.
I know I'm thread-rezzing a bit here, but I haven't seen anyone recommend Rotativa yet. Renders my HTML (with Razor syntax, etc) as a PDF, and I get to customize it with CSS. Haven't found much I haven't been able to accomplish yet.
I read the other page, they explain they're stored separately with a join back. Which I indicated. 
Any announcements that don't get me to [this](https://youtube.com/watch?v=P2PMbvVGS-o) will be very disappoint.
I know how a B tree works, I also know that if you don't have sequence friendly data, how it works is not very well. They also might tell you that even on a unique clustered integer index searching an index isn't O(1). Forget about non clustered indexes, non unique or non integer indexes all of which can cause all sorts of fun. You're massively overplaying the solutions that can be solved with indexes, massively underpaying the cost of indexes and generally talking a lot of shit. Sql does all sorts of fun things, and they're different even between versions, let alone products, and you're massively oversimplifying that. Indexes can, and often do, make things worse. Clustering on the wrong kind of data will actually reduce performance compared to a heap. Non clustered indexes even when they work perfectly, and they often don't can, end up retrieving half the pages in the table if you're unlucky. Covered indexes are costly, near, but costly. Sql optimisation is hard, which is why there's lots of cases where you want to do the work outside of Sql, where it's cheap. 
Check out something like Prose [https://github.com/prose/prose](https://github.com/prose/prose) there's a bunch of them around but they make it super easy. 
I still favour bringing the /r/CSHARP rules and banning them. Don't forget there's already a /r/dotnetjobs
Awesome! I didn't know about dotnetjobs.
For me it was the entity tracking/caching.
It should probably go in the FAQ or a related subreddits section :)
Not sure if this is relevant anymore, but back in EF6 days you could improve the performance by manually implementing equality on your entities. Didn't help with ram though. 
low contrast? are you sure your screen is not broken?
Disappointing.
It's not as if I can't read what it says, but it definitely looks oddly low-contrast for something as typically bold as a logo, you know? Also, for what it's worth, it has about half of the WCAG AA contrast ratio for large text.
Hey there, thx for the reply....Yeah makes sense. I see why the "Micro transaction" system is interesting. You can scale it up and down as needed from my understanding (remember I'm a n00b at this stuff). &amp;#x200B; As an Admin I'd automatically think of doing it in a ESXi environment with redundancy in house. So far I like Azure, love the UI interface. Just depends on how much it'd cost overall. Guess that depends on how much data/traffic it has. So much nicer than EC2, I know though performance ---&gt; UI prettiness lol. 
I remember when this video launched, it seemed far-fetched at the time. We are definitely nowhere near that IMO, unless you did it all in mixed reality. I'm hoping we're going to see an open-sourcing of Windows Core.
Good question! `Memory` is less general than `Span` - you can get a `Span` from a `Memory` but you can‚Äôt get a `Memory` from a `Span`. So it‚Äôs worth going to the extra effort in the implementation to provide a more general API.
Make me think: Commented out or Mostly Incomplete 
I'm pretty sure this is what the AsEnumerable method is for.
Any courses to recommend? I want to invest time one this since I'm on the C# for a while.. I did try Android Studio and Java and IONIC with TypeScript, but I feel using C# would be much more productive
But expression analysis has more than just return types, you can evaluate the expression body to figure out if you're calling random methods, see here: https://sharplab.io/#v2:EYLgtghgzgLgpgJwDQxAgrgOwD4AEAMABLgIwDcAsAFAHEkB0AMgJaYCO9AogB4AOCcKFGYB7TFEpVquAMzEATIQDChAN7VCm4nNIA2YgFYAPABM4AY2aQANkkJnLNuw6sRrAPkIAVQTFwHCAF5CAAoIJGAASiDPCEIAKkJgSS1tOn0efkFhMSN/UwtXW3tCm3dPJTdzdGsIeC8IbhIg0P5mczhowM82joTCfHoSAzAUrVl0wkyBIVFMPOMXJxLHN3LlKpq6uAbuRWCQ3s6Y719/Q4R2uDtB4bBIsc0NcZ0SfVwAFkIAWQhWEOi6ioqVSlWs1Vq9UaDAAQiITABPegAOXhOwRvDg9AAkuJMeYYADHiCwRDtrt5PQ4YiUWivBisbioPjCQ9npoAL7UDlAA===
Nice detailed article üòÅ thanks! Have you ever tried Apache Cordova ? I have moved from Xamarin to Cordova for ease of development and haven't found anything impossible to achieve so far 
Csharp do a monthly one where people post in.. however their 'automod' seems buggy and post half way through the month for the month.
Glass wall apartheid? 
Ooo, haven't seen that :) 
Considering the author works at Microsoft as "Mobile Developer Tools", this isn't an unbiased article.
Add New File. I can‚Äôt imagine having to use the New File wizard for every file.
My favorite: Resharper. IMO it has all the features you would want from a VS extension But.... I recently experimented with replacing it with free extensions from the store. The setup with these: Intellicode, Microsoft Code Analysis, Roslynator, CodeMaid, Sharpen, Visual Studio Spell Checker Also handy stuff I use: EditorConfig Language Service, Open in Visual Studio Code (I tend to like it more for json-s and large text files, csv etc), CSS Tools, Color Theme Editor For VIsual Studio
Honestly, the Xamarin Docs are pretty good, and they have a ton of examples. You should be able to get up and going in no time. The biggest learning curve is dealing with build issues. They are a lot less common now than they used to be. Just remember in most cases, cleaning, restarting VS, and rebuilding will fix the issue.
It's funny you mention Resharper. I tried it in the past and even with a pretty beastly PC I notice the lag. Was wondering if there was a way to get some of the functionality with stand alone extensions. 
Well, it's not the whole functionality. I haven't found refactoring tools like the ones that resharper has, also no fancy icons and tool tip windows (which as it turns out I miss more than I tought). Also haven't found a an analyzer which can warn about null objects the same way as resharper does (tough this problem I suppose will go away with c# 8.0 with nullable reference types enabled) 
\- AutoT4 \- Devart T4 Editor \- T4 Toolbox \- C# Union Types \- Code Maid \- DebugStudio Alpha \- DPack (I love the backup function) \- Move Type To New File \- Roslynator \- Viasfora \- Visual Studio Spell Checker
That's to advanced for me :) I've been using Rider on Fedora which is pretty speedy. But as I learn more on [ASP.NET](https://ASP.NET) Core MVC i see it's missing features like scaffolding. Found a work around though. Been kind of going back and forth deciding what platform I want to land on. 
I‚Äôm glad you posted this. I need to check this out 
That posture though
Your post has been removed. Self promotion posts are not allowed.
I implemented this via parameters and manifest files and some options in the csproj files. Pass those to the msdeploy command and it worked as expected. I found the whole thing to be clunky and unintuitive but it worked. Appreciate the advice!
All we want is to make ImageSharp a sustainable and successful library. We realized it's not possible without a working business model. Check out [my other comment](https://www.reddit.com/r/dotnet/comments/ajnb1q/betasix_labors_new_imagesharp_release/eeyq32i/) for more details. Personally, I'm happy to hear more about your business case. If you are providing software yourself, you are the one who needs to buy our license. Or are you consulting developer teams?
The reason he chooses is because he works for Microsoft... And before that xamarin... I mean maybe it is fine but this is an advert.
Do you use TFS? If so does it require you to check out the entire project every time you want to add a file? 
I do use TFS and no. In my experience the only pending change is the new file.
I think it's probably something in our settings, but I can't add anything to the project without checking the entire thing out. 
I mean, without that extension that is....not sure if that extension would help. 
Yea that sounds like some kind of configuration issue. Wish I could help more.
Is Xamarin free?
I tried xamarin anroid and put an app in the Google play store. Since then I've learned new languages and looked for other options for crossplatform frameworks and decided to go with flutter. So far I'm really impressed as the other option was react native and I'm no fan of JavaScript. 
I honestly think the only usable tools in .net nowadays are asp.net core 2+(for web api) and console apps. Uwp is a dumpster fire along with xaml in general. Ive never tried xamrin but id be very cautious due to the fact that android studio is a horrible experience on its own but has a method to it's madness. I cant even begin too imaging a wrapper around that.
It's free and open source now
Be a full stack developer in 4 months! Put on your resume you have tons of experience! No. Get your feet wet and get intoduced in your first hour, if you're coming from beginner. No more of this "learn in less than..." Bullshit. It's ruining the profession. Be a heart surgeon transplant master in 6 months!
What do you mean android studio is a nightmare? If you said gradle was a nightmare or something... that would make sense, but you don't have to use android studio at all with Xamarin.
Yup, I also stand by flutter. Fantastic to develop in.
 If by "paid evangelist" you mean literally works at Microsoft, yes :P [https://www.linkedin.com/in/jamesmontemagno/](https://www.linkedin.com/in/jamesmontemagno/)
Any good framework should abstract these out enough so you could BYO, if needed.
&gt; Move Type To New File I do not have this extension and I am certain this is built in to the lightbulb feature.
VSColorOutput - Color output for build and debug windows \ ILSpy - Integrate ILSpy (decompile .NET code) though the context menu can't decompile the thing you've clicked, just the document you have open, which is an annoying limitation. Most of .NET is open source or has equivalent open source version in .NET Core but the interface is better for browsing than github. Whack Whack Terminal - Integrated terminal, like how VSCode does it. Intellicode - Tries to be smarter about what completions intellisense offers at the top. Fix Mixed Tabs - One click to fix my coworkers' messes. Or Unity's default script template. File Nesting - Don't let built in types like WinForms and so forth have all the fun. Multi Edit Mode - Because four cursors means you're four times as productive! Debugger Image Visualizer - Not sure if your System.Drawing.Image was loaded/drawn/whatever right? This extension lets you see its contents while debugging.
Live Share. 
Exactly
That's the truth. I meet fellow developers that learned python from *Udemy* in 1 week and now claim to be great at it. Ffs, when I asked for client-server script, they didn't know what "buffer" means.. Neither they cared to search what it means online. Learning by searching, spending time &amp; rewriting own tools (no matter if they're already made before) is the only key to becone good developer.
Appreciate the article is perhaps unbiased, but I've had a very good and enjoyable experience building applications in Xamarin. I like it, definitely worth checking out.
Big Resharper fanboy and dev professional here. No, not any more. Free extensions do it all. Yes, you have to add about 5 of them, but one that's done you have a much faster dev. experience.
I pay for the Resharper suite to get the profilers. Every major release or so I'll install Resharper itself to check it out- and I usually last about 10 minutes before the performance annoys me and I turn it off.
you can get most features for free but i still think resharper intellisense and refactorings are much better.
Wait, what? Dude, that video was made in 2010 when Windows 7 first launched. It's 2019, I doubt Microsoft remembers of it or even maintains it anymore years ago.
What extensions are you using to replace Resharper?
!RemindMe 1 day
I will be messaging you on [**2019-02-03 08:33:13 UTC**](http://www.wolframalpha.com/input/?i=2019-02-03 08:33:13 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/dotnet/comments/amc6sx/do_you_feel_resharper_is_still_worth_using_in_2019/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/dotnet/comments/amc6sx/do_you_feel_resharper_is_still_worth_using_in_2019/]%0A%0ARemindMe! 1 day) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
I still love using it but sometimes vs gets so heavy on resharper loads on even medium sized solutions that I have no option but to turn it off. Their Rider ide is pretty decent though. 
Nope. Just to use it all the time. Got a devil device machine about a year and a half a go, didn't install it, haven't looked back. Use rosylnator and nunit runner to get the features I need
Absolutely. I regularly disable Resharper for comparison with the Visual Studio features. For me, the performance hit is worth it. One new feature I find useful is the in-line view of variable values when debugging. Another is the feature which shows parameter names in-line on method calls. Solution analysis is handy when checking whether your views in an ASP.NET MVC app are all valid. Resharper's refactorings and suggestions are still better than Visual Studio. I'm still sticking with Resharper, and I'm hopeful they can improve the performance eventually. 
I've used it for a long time and still feel something's missing when I turn it off. Is there a breakdown of R# features and replacements?
Absolutly. It is a resource whore when it starts up with VS but, the ease and practicality of its intellisense and refactorings are, imo, still just unmatched by other available nugets and extensions.
I went in another direction and switched to using a Rider exclusively. I‚Äôm very satisfied with that so far. 
Ctrl click decompiled sources is the one feature i am not ready to give up
Is there any free tool that let me iterate over usages of a item/function/method/whatever and have a good refactoring tool? I feel that's the only thing I use ReSharper for.
I have tried going all free extensions, but didn't quite got the full functionality. What I tried to replace it: IntelliCode, Microsoft Code Analysis, Rosylinator, Visual Studio Spell Checker, Codemaid, Sharpen, plus a lengthy editorconfig. With the new intellisese updates, and IF you enable nullable reference types you get about all the features you would need for editing code... However what you don't get: a lot of refactoring tools (ie.: ctrl + r + f for extracting varriables, invert if), renaming with search for strings, better code completition (even with Intellicode analyzing your solution), a lot of visual help (dynamically showing parameter names in method calls, when you inline the parameter you pass in, after . shows properties having get/set/get only), also debugging helpers ( live values around statements, so you can see at a glance what is going on instead of mouse overing or searching in watches) So yeah, if you can't/don't want to/inclined to replace it you can, but imo as of yet you can't really replace it even with a dozen of extensions. The biggest drawback I have encountered (without talking about it's price) is the speed but I can imagine it could only get better, also, even tough everyone freaks out about it being slow, on my home computer (Sata SSD, AMD X4 750K which is a rather old and slow cpu) it's bearable for middle sized projects, and for small ones its OK. On my work computer (Intel 8700K, with an nvme ssd) it's actually quite good although sometimes it does have hiccups. And obviously opening a large project for the first time does take a lot of time while resharper indexes everything. So yeah IF you have a fast enough machine OR you can be ok with the speed it's really the best tool/toolset I have found for VS, I tried going away from it, but I kept missing a lot of features for which I have not found alternatives.
I know that there are other flavors on market but, one thing i haven't seen brought up is some of the other apps you get depending on your account. I often use data grip, dot peek and pycharm. Jetbrains just has a ton of awesome stuff.
Go's defer queues a method invocation after method completion. this often leads to young gophers defering tons of stuff in loops and thinking that they properly managed resources. Only to realize, their endless loop keeps hold of all the resources because it never finishes
I‚Äôve personally never bought into the Resharper hype. I get the appeal, but I can‚Äôt remember the key bindings and by the time I‚Äôve worked my way through menus or even remembered that ReSharper can do something for me I can usually do the same as fast by hand especially as I‚Äôm a seasoned Vimmer. I definitely agree that in the last few versions of Visual Studio, with the introduction of Roslyn the reasons to use ReSharper have been mostly eroded. I personally ~dislike~ don‚Äôt use ReSharper because I genuinely find it slows Visual Studio and at times when I‚Äôve tried it I‚Äôve had it crash my VS. A big reason to use ReSharper is the test runner. I can‚Äôt believe that the VS built in test runner isn‚Äôt better, although even that is much more usable now that it has real time test discovery. I personally use CodeMaid, Roslynator, VsVim and a bunch of others and still find it snappier than ReSharper. I‚Äôll try to put a bigger list together when I‚Äôm at a computer. 
I have some news for you. Scroll down to "View decompiled source definitions instead of metadata (C#)" on [this page](https://docs.microsoft.com/en-US/visualstudio/ide/go-to-and-peek-definition?view=vs-2017).
Oh boy...! Thanks heaps, will give it a go!
Which 5? Resharper user here looking for that faster UI w/o losing functionality.
Yep Resharper perf and stability is just too poor for me to care about what it brings to the table.
One thing which I miss is IntelliSense including definitions outside of what is currently in the ‚Äòusing‚Äô. I.e if I have an extension method for IsNullOrEmpty ReSharper would include it in IntelliSense and automatically add the using in if you use it. I haven‚Äôt found any extensions for this yet :( 
Regarding "when an exception is thrown and I land in the catch clause, you tell me which line of code above threw the exception": go to Debug -&gt; Windows -&gt; Exception Settings, and make sure there is a "tick" and not a "black box" for "Common Language Runtime Exceptions" - the "tick" means it will break \_all\_ on exceptions on the line where the exception was thrown. This solution works for C#, not sure about other languages.
thumbs up for the dark theme &amp;#x200B;
I still use resharper and pay for it. The start-up perf hit is worth it as generally speaking that's the biggest issue. I can sacrifice a few moments of my time for convenience of having an all in one so to speak.
Yep that's me too. Spent a few hours trying different settings to recover perf but I just kept running into issues. The profilers though are great.
Hehe yeah... So there's that. 
I do not recommend Googling icollection&lt;string&gt;.
Tried replacing it with roslynator and others. Still not enough. ReSharper is still superior imo. In fact i use Rider Ide. Rarely miss VS.
This is the only reason I always come back to Resharper. It intellisenses with types you haven't imported yet. `var sb = new SB`+ Enter = boom, Resharper adds `using System.Text;` at the top of the file and expands `SB` to `StringBuilder();`.
The navigation features alone are worth it for me. The native searching in the solution explorer is one place where VS is slower than Resharper. Go to file and go to member are essential for surviving bloated solutions/classes. 
For me, the real question was is Visual Studio still worth it, compared to just using Rider. And, being entirely honest, I haven't launched Visual Studio 2017 other than to verify a build about a month ago in nearly a year. Rider is just _that_ good, IMHO.
I've tried dropping R# a number of times over the past 5 years and always missed something badly enough to reinstall. I tried again last week with Roslynator and have found a happy compromise between performance and features. I'll most likely be canceling my subscription.
I'm a bit ashamed that the biggest reason for me to use Resharper is the File Structure window (CodeMaid is ok, but reaaally slow when switching files). Also, can any free extension refactor selected lines into private method (Roslynator perhaps?)?
[removed]
Does extract method in visual studio not work for you? You select the lines, CTRL + . And select extract method.
Yeah, can't disagree that it would be much nicer if CodeMaid was quicker on its code explorer window. Perhaps I'll roll my sleeves up and see if there's any way I can contribute... I often struggle to get going with VS extension development though they often seem to need quite specific combinations of Visual Studio and various SDK libraries. &amp;#x200B; Vanilla Visual Studio will extract lines to a method for you: [https://docs.microsoft.com/en-us/visualstudio/ide/reference/extract-method?view=vs-2017](https://docs.microsoft.com/en-us/visualstudio/ide/reference/extract-method?view=vs-2017) &amp;#x200B; I think a lot of people love ReSharper and that's fine, but I think a lot of people don't realise that VS does a lot of what you want these days.
You have no idea how stupid you just made me feel. Oh god, how didn't I ever notice this.
I do find that often ReSharper fans as so wedded to VS + R# that they don't realise how much VS has eaten into it's feature set. Where I work it feels like the devs are split 50/50 on R# and either love it or don't see the point in it. The devs that do use it do make use of the advanced refactorings that VS doesn't have and obviously the test runner is fantastic. &amp;#x200B; I imagine that JetBrains will always keep R# features ahead of VS, but for personal taste for me, I'm not massively keen on it and I'm happy with the added features that I can bring in with free extensions. The only thing I'd really like is a decent free test runner either stand alone or integrated into VS.
Isn't that just CTRL + T in VS 2017? 
I agree on all fronts. I actually dislike most of R#'s functions, when compared to VS's implemenations. Like the variable renaming or snippets. Also the performance is just abysmal on pretty decent notebook.
This guy gets it. I wish more people would try Rider. I‚Äôve loved literally everything about it more than VS and haven‚Äôt found any reasons to choose VS over Rider. 
The fact that it is hard to understand to precisely why you *shouldn't* use it. It is easy to implement your own. https://github.com/pauldotknopf/react-aspnet-boilerplate Leave it to Microsoft for stupid crutches, ala WebForms.
Same as you! The only things that was missing in VS2017 for me were the clipboard history dialog (ctrl + shift + v) and a better go to all (ctrl + t) And it's in VS2019 now. With Roslynator, I get all the refactoring goods I need. I do think that the clipboard history need some work, sometimes it's missing entries, but it's there so. Only the "Go to Implementation" feature isn't working properly for me, that R# never had an issue with.
Yes, it does almost the same thing, but it's way slower to respond and puts too many things in one place. It's probably great for many cases, but doesn't work well for me in the code base I'm working with in my day job.
In source control explorer make click the workspace dropdown and click the settings item... It is at the bottom (I can't remember what it is called). Select your repo and click advanced... Make sure the repo type is server not local.. I am pretty sure that fixes it.
I almost hate saying it, because it feels like a cliche to say it's slow and crashy and the last time I tried it it wasn't as slow or as crashy as it had been previously... but for me it's a bit slow and a bit crashy and it doesn't give me any features to justify having it installed.
Upvote for Mads. All of his extensions are top notch!
Is there an extension to live highlight stylecop violations?
For some of the more bleeding edge MS tech I feel like VS plugins are essential or at least a huge help which I can't I again Rider supporting
I mean, if you type `StringBuilder` in Visual Studio and you aren't `using Systrem.Text;` then VS will offer to do that for you.
I use Resharper only for DotCover. It's a shame they do not sell it as a standalone product
How‚Äôs rider support for Xamarin?
The primary reason I use Resharper is the "introduce variable" light bulb/quick fix/suggestion, lol. Anyone know of a plugin that provides similar functionality? E.g. you type `new MyClass();` or `myList.Where(x =&gt; x.Whatever &gt; 10).ToArray();` and only takes a key press or two for Resharper to add the variable declaration.
Welp looks like I'm done with ReSharper now
I'm in the process of trying out rider and I'm pretty happy so far. It's going to allow me to completely move to Linux.
I haven‚Äôt worked on any Xamerin projects but https://www.jetbrains.com/help/rider/Xamarin.html
This is most likely true. I haven‚Äôt had to rely on any VS plugins. I‚Äôm curious to as what plugins people would need to keep them locked in on VS 
Same here. It's like using resharper without the massive impact on performance. I recommend everyone now try Rider if they like resharper. 
Why use resharper? Just buy Rider and ditch Visual Studio! 100% serious 
I got rid of it an am now using Roslynator, https://marketplace.visualstudio.com/items?itemName=josefpihrt.Roslynator
Big resharper fan. Never bothered by the startup lag tbh. I‚Äôve actually switched over to Rider since changing jobs and moving to .net core. I would definitely recommend at least giving it a try. Seems to give a much faster resharper experience and you can even keep using your normal VS keybinds. It even has a new feature similar to code lens so it‚Äôs a significant upgrade if you‚Äôre on community imo. So yes, for me resharper is a staple for whatever that‚Äôs worth. 
Last year I did a project using butterCMS; it provide a back end that is very plain and an API. Just query the API from the client or from you .NET backend; it was very nice simple and customizable in terms of it being a REST API. 
&gt; the in-line view of variable values when debugging VS has this
I hated my time with functions; but that was 3 years ago. Went to webjobs and was very happy; until some of the support docs in the last couple of major updates didn't get updated.
this..and Rider works on mac and Linux too 
3 years ago, I probably would have made the same choice, webjobs are a good option and Functions weren't there yet. These days they are quite good.
that's only on hover-over, right? 
Yes
Also switched to Rider entirely since I started using .Net Core. Really love the IDE, especially coming from a Java background (and thus using IntelliJ).
I use it on the mac for an ios product (Xamarin Forms app). It works just fine but I build and debug using VS ( just because I did not try that with Rider yet... but I am planning to).
Reshaped always got in my way. Faster without it. I had to disable it on every pc it came on:
You can pin them open though.
Well there's an extension just for that: https://marketplace.visualstudio.com/items?itemName=Dreamescaper.IntelliSenseExtender
This question gets asked on a.biweekly basis.
I have been professionally working with vs2017/xamarin and really haven't had any build issue for a long time. I'm not into forms, though. 
There are two things that I couldn't replace just yet: 1. An analyser for possible NullReferenceExceptions (which isn't really important with nullable reference types coming up) 2. A "references"/"usages" window that I can filter ReSharper-style: e.g. only write/set-usages. I'm using the [Roslynator](https://github.com/JosefPihrt/Roslynator)-NuGet-package, [Inheritance margin](https://github.com/tunnelvisionlabs/InheritanceMargin), [IntellisenseExtender](https://github.com/Dreamescaper/IntelliSenseExtender), [StopOnFirstBuildError](https://github.com/einaregilsson/StopOnFirstBuildError) and some features of [CodeMaid](www.codemaid.net)
Have they fixed in editor unit test run?
Does it support wpf yet?
Lmfao you weren't kidding
So much lag 
Can you develop .Net Framework or just .Net Core in Linux/Mac with Rider?
I'm sure you could write framework code in Rider on Linux, but if you can't build it what's the point. 
Rider is awesome but costs. 
Good example that comes to mind is service fabric mesh or web functions. But they only affect a small # of devs.
You were downvoted, but I want to thank you for pointing me to this. I wasn't aware of this Microsoft-endorsed reference architecture. It looks like a very useful resource.
Yes, it does, Rider has built-in WPF live previewer.
Not worth slowing everything down for that 
Install Ditto.. An awesome clipboard manager.
Or use Visual Studio Community, which is 100% free (for most people, large companies excepted).
I was just curious if the IDE maintains compatibility...and if I could then deploy to Windows. We have a few Linux machines at work, and I've got the Linux as an OS itch again.
Posted 1st February... Think you meant to on the 1st of April
You mean like a destructor in C++
I don't get it. Isn't that just var?
Not exactly, no. It adds the type and name of the variable. If you type `new MyClass();`, it adds the `var myClass = `, but then opens a suggestions box (inline, not a dialog), so you can choose between `var` or explicitly declaring the type (`MyClass`), then afterwards it moves on to the variable name, so you can change it to another suggestion or something completely different. It's difficult to explain, but if you have Resharper, you can just try it out.
Yeah, I haven't had any in my personal projects, but I took over a project at work and I started having issues with it. I think it was related to a dependency.
Actually, here's a gif: https://imgur.com/La7WZz6 :)
Theres nothing wrong with these titles or tutorials. They are clearly tagged for beginners. As intros to basic overviews its precisely the kind of training people need starting out so as to not be overwhelmed. 
Rider is indeed awesome, especially since it has the R#/IntelliJ goodness built into it and it's not sluggish. Unfortunately it has some IMO missing features that prevent it from being a full replacement to VS. :( 
&gt;." Bullshit. It's ruining the profession. I'll call BS on your BS then :) . IMO Its ruining nothing. The person who does not know his stuff isn't getting past the first interview so theres little harm. All that's happening here is that the tutorial is trying to make accessible a new world of programming to beginners. Its clearly tagged for a beginner and [asp.net](https://asp.net) needs more of these rather than less. Anyone who thinks they now are a seasoned developer will try to code something and realize there's more they don't know. Anyone that is intimidated from programming might realize its not beyond their reach by seeing some broad concepts in practice. We need to try and stop building walls and instead encourage and demystify programming. programming is not heart surgery. No one dies if a code has issues and the person going for a job in programming never gets the job without a degree or demonstrating said knowledge so the code will never kill a company. Us old crufty programmers forget real quick how we all were starting out - we all sucked even after we thought we no longer sucked.. Trial and error is how we all learn and all these kinds of training and titles do is say - hey get started trying with something simple, straight and direct. Now if a title says - become a pro in an hour, week or month then thats just lying
Indeed. IMO, It's not as expensive as some other packages. I got the whole Jetbrains suite for like 150 per year, which IIRC is a little more than how much I was paying for just the R# bundle. That's not a bad deal, to me anyway. I think one could spend a lot more on other software bundles. But at the end of the day, not everyone looks at it that way or they can't afford it, so I understand why the price makes it a non alternative to VS Community. 
Maybe in VS 2017 but not in 2015
Fair enough, I'm used to being on the latest and greatest and forget other might might not be for various reasons.
The best thing about this post, besides your bloody smarts, is the debugging information. Love it :) 
How about Live Share and Intellicode?
Is there any replacement for stacktrace window?
That's honestly fair. I just get bitter to see so much of these people talking big talk and wasting our time in interviews. Especially how some companies have bad interviewers and plant these people in a team and the team can't get them fired so just have to drag them along. But you're right. Good to have good learning resources and to help people learn! I am all for that. But not the "learn in 1 hour" gimmick. It just rubs me the wrong way.
I think I also tried logic apps, those are by far the worst.
And visual studio don't?:)
From my point of view all product pack is a great deal. For 200 you will get bunch of great developers tools
Never liked resharper but Rider is awesome. It is best option for Linux users
you can use Mono, but obviously things that platform specific for windows such as winforms stuff won't work
Did you read community license at all? Only up to 5 people can use 
yes you can build Framework binary, check out Mono project
Recently I stopped using R# and I really miss some features. R# has so nice formatting, e.g. it can wrap string if need. Does anybody know how I can do it without big formatting tools like StyleCop, mb there are some VS extensions?
Update - the code on GitHub is here: Update - I just posted the code to GitHub, here: [https://github.com/JoaoBaptMG/PathRenderingLab](https://github.com/JoaoBaptMG/PathRenderingLab)
Yeah that's what I do now. But it doesn't always work. And there are a lot of exceptions that are usually ignored that this catches when you don't want them to. Then you have to remember to turn it on. It would be better if they just highlighted the offending line. Also, when you're in the catch, any variables in the scope of the try are now unavailable. So you can't see what value they had which is usually the cause of the exception. Like it said, it could be a lot better.
At work we are still publishing using the one click deployments. Does Rider support that? I could use some PowerShell scripts to do that of course, I setup an automatic deploy process to our UAT environment but the live is manual. 
SQLLlite server compact toolbox Roslynator Format document on Save VS Intellicode Resharper Viasfora Productivity Power Tools Fix Mixed tabs and last bit not least PowerMode!!! That gave explosions every time you hit a key, but seemed buggy so was disabled. 
Nice! Some feedback you didn't ask for: Commit more. Complete a change that can be defined in &lt;80 chars and it compiles/works? Commit. If you let it sit and wait too long you end up struggling to get semantic commit messages.
I'm not following. I would hope your endpoints are talking to your services, which then handle your CRUD operations. If that's the case, then you simply create the GraphQL objects, queries, and mutations that leverage those existing services. Now if you have a lot of logic running in your controllers, then we're having a different discussion. Your endpoints shouldn't be doing any heavy lifting or processing. Separate your endpoints from your services, implement proper DI for leveraging your services in your controllers, and then you can jump back into using GraphQL. 
[removed]
I haven‚Äôt needed that, so I don‚Äôt know, actually. 
When you don‚Äôt have any references under your belt, a certificate might give you a leg up, but imo the time and money invested in getting one is not really worth it. 
So... I should just focus on improving my website? 
Yeah, I missed commuting on this project üòÇ Anyway, I want to adopt a more disciplined structure to the repository.
Did you try configuring the security settings on the binding, client side? I don't see them configured at all in your xml
of course, but it's nice not to have to do that. This feature can save a bit of time when debugging. 
Depends on your customer base. If you want to go corporate consulting then having certificates might be useful. If you target small or medium business they probably won't care about certificates. You probably would like to get all kind of customers but in reality you have to know your target audience. If you will be doing SMB's it will be hard to jump to big corpo. If you have big corpo clients then it would maybe be not that much of a problem to find SMB customer but then you will be disappointed with how much they can pay compared to big corpo. &amp;#x200B; So SMB's will not pay you enough that certificate will earn on itself quickly and big corpo will not hire you based only on certificate. Best to get big corpo contract is to have some other big corpo contract already. Then if you have big corpo contract rolling or in progress your cert might be useful as CYA for someone who hired you.
What if my plan for this website is for general purposes or community based like but not exactly, Facebook or Reddit?
It's mostly stable for me, but I know how to make it crash and can reproduce every time. If I start typing SomeString.Equals("AString", StringComparison...) it will crash. If you want to ignore case, you have to basically just copy and paste the code in because typing it crashes the IDE.
This is really useful, well done. Do you plan to support .NET Core Generic Host? It lets you specify a json file and a POCO where the properties can be overriden from the command line. But it is quite basic with no automatic help generation. 
Can flags have more than one character? I see that options can. I've had a problem where letters RSTLNE get used up very quickly and I'd like to use the first two or three letters of the full flag name instead of an unrelated letter. 
If I understand correctly, you would like to use NFlags to configure and run Host. The solution would be, to create 2 classes: 1. Arguments class with attributes and method to convert into HostOptions 2. Command class with run method that provides method to run the command and create hostbuilder, use the HostOptions converted from your type and run Host.
Are you asking for two letters abreviation? Flags are special case (Boolean) options. And can have full name on the same way. Matt all flags need to have abreviation.
Yes, two letter abbreviations along with full name like: -th 4 and -threads 4 instead of just -t 4. I have collisions with the current solution I use CommandLineParser and have to use non-intuitive letters for some of the switches. -T is a popular one same with -C. Why not allow for n-length arguments all aliased to the same command? Then it would be flexible for any situation. E.g. why can't I have -T -TH -THREADS -THREADSTORUN all configure the same variable?
I have a WPF project where I play and download m3u8 streams.
Firstly, that link returns HTTP error 500. Secondly, have you even tried downloading the actual .m3u8 file and taking a look at what it contains? It should be fairly clear what to do once you actually look inside of it. I have a working WPF application that plays videos from a .m3u8 file and optionally downloads them, so I can help you... but you gotta put in just a tiny amount of work yourself. At the very least, you should make sure your link works or (since you now know to actually look *inside* the .m3u8 file) post the contents of the file.
It's an interesting idea. It is not covered by any tests but abr is string not char and as I remember the implementation words should work fine as abreviations. Yet there is no way to create multiple allies for same argument. I've never seen anything like that in any cli tool. I wonder how help should look like in that case.
If you build something cool that people like, no one will all for your credentials or even references.
&gt;2 points I've just added new tests to ensure that word abbreviations are working fine and to ensure this will stay like this. If you would like me to add support for multiple aliases on the same variable please create "feature request" in github repository.
I was really impressed with Swagger and NSwag so will certainly give this a look.
Well, it depends a lot. Usually, a solutions architect continue to code. But code less and focuses more in POC and/or infrastructure as code.
The link returns error 500. Either way, try saving the .m3u8 file and opening it up in a text editor. Shouldn't be too hard to figure out what to do next. Basically, you don't want the .m3u8 file, you just want some of the info it contains.
That is one of the big challenges. To convince others when to go against the trends make sense and when don't. 
You can use third-party libraries in .Net core to produce pdf reports. I suggest you to use [UltimatePDF](https://www.componentpro.com/products/pdf) and ItextSharp for creating pdf in [asp.net](https://asp.net) core. You can easily generate pdf and give a option to create reports in pdf. You can simply create a way where someone select report and click generate you can create pdf from view. this is simple way to create reports in .Net core without using crystal reports. 
Visual studio don't, it has Community version which is free. 
You can't just download a stream. You send it http gets request for a specific chunk of time from the stream / video. It's not just a file you download. 
Net framework works only in Windows Net core can works in Linux and windows
Only if you are independent developer or working in veeeeery small company
and is there a way to run in both OS?
I you want to run it in both so you should create net core project I think
Yes, use .NET Core ( https://dotnet.microsoft.com/download ) and you can run it on Linix, Mac and Windows.
Can't agree. As an example we had 100+ developers and almost all of them with VS Community. 
Oh my, why put didn't put all together, the core with the framework? 
Ok. Then you are doing this illegally. Please check this link https://visualstudio.microsoft.com/license-terms/mlt553321/ Up to 5 people section
For push you may want to check out the [Reactive Extensions](http://reactivex.io/). Can't say much about it though since I've just started learning about them.
because .NET Framework is almost old enough to buy drinks without a meal and .NET Core is a completely new venture that deliberately leaves a lot of the legacy stuff behind
&gt; Net framework works only in Windows &gt; &gt; Well, there is Mono.
They are different things targeting the same .NET Standard.
Thank you! Looks like more to learn, but maaan my brain hurts...lol...I'm having a read of it all now...I've have at some point read of it before but glossed over it...I am not 100% sure if it will just introduce complexity without solving issues that basic messaging could solve...but I think you be right it warrants the energy researching it... If anyone else has any thoughts on if its worth going that way, I am all ears.
There's definitely some learning curves but IMO the code will come out cleaner than with using messaging. This may also interest you: https://github.com/joelpob/betfairng
By that logic, you could argue that Rider is free too provided you torrent it. VS Community is limited to 5 developers maximum (and an income restriction), as specified in the terms and conditions.
Ask them to write fizz buzz
That's a good way to weed out people who have zero coding skills but I don't really think it qualifies a good candidate.
I use [TestDome](https://www.testdome.com) and I can‚Äôt recommend it enough. It‚Äôs not free, but if it means you don‚Äôt hire someone useless it‚Äôs a bargain at twice the price. You can pick from a pretty substantial bank of test questions. Most of the questions are live coding and they test way more than a brain teaser or FizzBuzz. 
I honestly don‚Äôt do code questions like you describe - They tend to favor people who take online quizzes and understand terminology, but not necessarily coding practices and development methodology - the ‚Äúcommon sense‚Äù of development which I think is what your looking for. I prefer to do two things: One, I like to drill in detail on their current projects from their resume. If you developed something, I expect you to be able to describe its architecture in detail. The second thing I like to do is present (at a high level) a current project or problem we are looking to resolve - Nothing large, maybe a specific feature. I want to see how they would design it, to get an understanding of their thought process: Are they following a TDD model and laying out test cases up front? How are they separating tasks in their design? Etc. I‚Äôve found that is a better way to weed out candidates than specific language quizzes. I‚Äôll take a developer that can build a solid design and needs to hit Stack Overflow now and again than someone who knows every c# term in existence but writes spaghetti.
I second not asking them the kind of coding questions you find on Leetcode and whatnot. I work as a Rails dev now, but my current employer gives you a simple project to complete at home, literally takes less than 2 hours, and then in your in-person they have you modify that code with two devs watching and directing you to make specific changes. This happens after a technical phone screen with two engineers, and is preceded by a whiteboard session where they have you architect a solution to a problem they solved some time ago as a way to see how you think on your feet. All coding is also language agnostic, so I did the whole thing in .Net Core. They have you submit your solution via email as either a .tar or gitbundle. Overall I really liked it, and upon returning home after the final step I told my wife if they told me no, it's a no I would respect. I didn't feel like they tried to trick me at any point in the process, and learning the pairing session would have me making changes to code I wrote and was thus already familiar with. There were no gotchas, no tricks, no curveballs, literally an interview where if you've done the job you should be able to show it.
I really wished VS enterprise's live testing module was part of VS Pro or Community (unlikely). I feel that all in all VS has been great on a lot of features till the testing tools part. I get that VS Enterprise needs to have a defining feature to differentiate but I think it's additional testing and analysis toolset is good enough that they could down port the live tests feature to a lower tier VS. Even Code lens is now on Community which is neat, hopefully some testing tools would be as well (at least on Pro, and maybe on Community).
From a streaming perspective, you probably want some component that can take the data from their stream and push it into something that can manage it better. Either in some sort of persistent message queue (Rabbit MQ, Azure Service Bus, etc) or a streaming framework like Kafka. After that, something that's event driven seems like it'd make a lot of sense, but there's several different ways to go about an event driven architecture. Its really tough to make recommendations with limited info, so you're better off keeping it as flexible as possible. Keep your boundaries as loosely coupled as possible. Monitor things as closely as possible to get data and metrics to inform your decision. Set benchmarks on those metrics so you can explain what your expecting and say whether you're meeting it or not. Keep your options open so you can evolve the architecture, rather than being stuck with previous decisions.
Check your URL. It's a dead link.
Thank you. I have seen that before also (I actually use joelpob's old api-ng code prior to the stream api). Now that I look again, it defenitely, interests me :). Thank you kindly for the link. Looking like its the way to go. Thank you so much.
Beyond the questions about their experience and projects and culture questions, I usually have a prepared solution with a basic app and in-memory reposity implementation with a handful of unit tests that are failing. I tell them them running the tests is free and quick, and that some of the tests are failing. I hand them the laptop and ask them to fix the unit tests. I watch for a few things: do they run the tests? Do they add any tests to confirm their assumptions? Do they use debug mode? Do they rerun the tests after they make changes to the implementation? What is their thought process on finding the problems? I always make the problems fairly simple - never some obscure API in .NET. I do usually have a combination of bugs in the implementation and a bad test, to see if they'll try to grasp what the test is asserting. I want to make it as fair and objective as possible while understanding what their problem solving process is, and if in the future I hand them some code to work on, if they'll leverage the tests that other people have taken the time to write. I don't think there's anything much fairer than handing someone almost working code with covering tests.
Thank you. I suppose I dont want to bog down too heavily with details here (well...unless people are interested in the full details of things)...It's difficult to discuss without knowledge of the trading domain I suppose...(which I am also still learning)... I am the only developer so I'm my own metric of if I write crappy code or not (in retrospect its always crappy code I thinks). I am a tidy freak etc....so that's as much my motive as getting my application doing what its meant to be doing... I am off to have a read about kafka, rabbitmq, thank you for the tips.... Basically, I started writing loops for my algorythms...all in an imperitive way...running through phased enumeration something like; Phase1. Identify target/filtered market Phase2. Identify target runner Phase3. Identify entry point Phase4. Place entry point order Phase5. Place stop loss order Phase6. Wait for profit/loss Phase7. Exit trade etc All or some of those phases need to respond to price changes.... or other events to occur elsewhere in the application...the alternative being that I could only ever run one trade at a time and run it to complection.....which...is only good for starting out :) But yeah thats a little bit of the background of where I've been...
If you're wanting to do WinForms or XAML and want that format, you can do Xamarin Forms or Mono. It's well supported in Visual Studio. WinForms in dotnet will not be supported in Linux in dotnet core.
You've totally forgotten about mono.
I never understand the thought process behind the thinking that unrealistic homework questions, puzzles, riddles and trick questions (that old how would you move a mountain BS) is going to find the best programmer. And why ask software engineers to re-implement known algorithms from memory. If they did that while on the job, you would be paying them for the wrong thing. And the most dangerous thing is a lot of standard algorithm question "correct" answers are wrong. I often times see "correct" answers only favoring memory. And that is wrong because which way you want to go with the space time tradeoff depends on factors that the questions almost never cover. And sometimes those are wrong because they will assume things like C# strings always work like C++ strings. They don't. Ask them to provide a sample project and explain the code to you. Doesn't matter what the project is. If it works, the code looks good and they can explain it, they are probably good to go.
Mono isn't .NET framework.
No, but it does allow you to run most programs targeting .NET Framework on Linux or macOS. 
Such as?
Thank you 
So now we have basically gone full circle to the old web service reference based on the wsdl?
Well you don't have to use this, unlike SOAP where trying to hand roll it was a nightmare.
Excellent point, CodeLens was something I‚Äôd wanted to mention. Though on my lower powered machine I do find it to be a bit of a resource hog. 
You should read the documentation on how to install .NET Core on the various distros: https://dotnet.microsoft.com/download/linux-package-manager/debian9/sdk-current
I feel like nobody has given a decent answer, so I'll take a stab at it. In the .NET world, there are four different .NET implementations: **.NET Framework** The .NET Framework is the oldest, most mature implementation. This framework allows you to create a variety of applications for *Windows only*. **.NET Core** This is a new implementation of .NET. Unlike .NET Framework, it is open source and it also allows you to create applications for Windows, Linux, and macOS. The caveat is that it does not support building actual UIs. You can **only** create console applications or web applications. Note that when version 3.0 of .NET Core releases, you will be able to build desktop applications, but only for Windows. **Xamarin** This is used to build applications for Android, iOS, and macOS. If you're building mobile apps, this is what you want to use. **.NET Standard** This is a set of fundamental APIs that **all .NET implementations must implement**. It allows you to create libraries (*.dll files) that work on all editions of .NET, e.g. Framework, Core, and Xamarin. This means you can share code across programs built using the three other implementations. Keep in mind that different versions of the various implementations target different versions of the .NET Standard. For example, .NET Framework 4.5 might target .NET Standard version X, while .NET Core 2.1 might target .NET Standard version Y. 
I would advise to focus on high-level concepts rather than on specific technologies. You will be able to learn any technology once you need it. Decide what would you like more - client sever or mobile development and learn it.
Don't think so. It was all about open source. 
It is not limited to 5 devs. There are cases when it is not. Specifically, I am talking about open source projects. 
Then why are you saying that Rider costs? https://www.jetbrains.com/opensource/
Well, obviously, I haven't noticed it is free for open source projects too üòÅ. Thanks for the link. 
Yes, it is the foundation of the large majority of database access in .NET. Any SQL database related driver in .NET, ORMs like Dapper and EF, do build on top of ADO.NET. Now if you ask the industry as a whole, then maybe mobile might be a better choice, but then .NET is probably not the best technology for it.
Before I did into this series (It's quite a long watch) - For those who have watched the other parts, is it worth it?
Pick one or two actual projects from your backlog, and discuss them with prospective coders. Give them a real-world situation, with incomplete information, contradictory goals, and so on. See what their general approach is, whether they spot the problems in the project, flag up the technical difficulties, and so on. Ask them how they would go about implementing the project, how they would organize the tasks. For the coding part, I would give them something half-finished or broken, and see how they go about identifying errors and fixing them. See if they are able to pin down the point of failure in a large codebase, and to set breakpoints to identify the exact source of error. Then discuss different alternatives for fixing it. See if they have any awareness of broader architectural issues as well.
Definitely worth it.
Yes, ADO.Net is used all over the place. I still use it in some circumstances actually (where performance is the most important thing for example). 
Well, simply ask whomever is responsible for managing the Debian repos. If you want to install .Net Core on debian, simply follow [these instructions.](https://dotnet.microsoft.com/download/linux-package-manager/debian9/sdk-2.1.301)
All softwere that you and your colleagues develop under OSI licenses? :) If yes you can check this https://www.jetbrains.com/opensource/ :)
Hi, Because Your idea was quite interesting I've created the PoC for such functionality. [https://github.com/bartoszgolek/NFlags/tree/features/aliases](https://github.com/bartoszgolek/NFlags/tree/features/aliases) &amp;#x200B; The main idea is to allow multiple attributes when registering arguments using generic methods or allow to set target name when using non generic commend registrations. Of course there still is a littlebir confusion about precedence when i.e. two registrations are setting diffrent default values etc. and it require tests and coverage to ensure that everything is working as expected and is deterministic. &amp;#x200B; Check out new examples "NFlags.Aliases" and "NFlags.GenericAliases", and let me know if this is somethin You are looking for. &amp;#x200B; Best regards
It is advised against installing from a non Debian repo: [https://wiki.debian.org/DontBreakDebian](https://wiki.debian.org/DontBreakDebian) since it can break the system. And since MS have not uploaded the packages to Debian repo then I presume that they are not in good quality 
Thanks for the recommendation! The episodes are quite long, simply because we try to explain why are we doing things the certain ways as clearly as possible :).
ADO is faster but Dapper is damn close: [https://medium.com/@nuno.caneco/dapper-is-fast-as-hell-40531f894ed5](https://medium.com/@nuno.caneco/dapper-is-fast-as-hell-40531f894ed5)
My servers are running debian for nearly a two years and I'm using netcore since it was released. It works just fine without any issues. I guess MS is just being lazy to upload packages, sadly. 
It could be that they are that lazy. But I can wait for the packages are ready for prime time
.NET Core is ".NET: the next generation". It's much newer and cooler.
That‚Äôs the best part. We‚Äôre here to learn and I‚Äôm very appreciate your sharing of knowledge :)
That's a valid point ;).
Well, it says "Don't install packages from random websites", not "don't install packages from non-Debian repos, ever, at all". Plenty of vendor-provided open source software is via the vendor's own repos. https://packages.microsoft.com/ is not a random website. 
Also, one of Debian's own recommended things to do is run things in a container. Microsoft provides official SDK and runtime Docker containers that work just fine on Debian.
As an alernative depending on your needs, you could deploy the runtime along with your application. It will not be a shared runtime but it will work. When I build for linux-arm (Raspberry Pi in my case), VS2017 provides all the necessary support files so I just upload my app, put it in the right directory, chmod it, then it's ready to run.
As with some others, I feel coding 'puzzles' are nonsense that deserves to be consigned to history. They're artificial, super stressful and don't really reflect what ypu actually want them to do. The approach I take is telling them in advance that I want them to show me some code they've written (don't care what, where etc...) I then use that to explore how deep their skillset is / how they make certain choices when developing. 
No diss from me, I like how long they are I just needed an incentive and I got it!
I would do the same if I were you :). 
I've seen fizzbuzz used to a pretty impressive degree. First, it can be done in a very TDD way... Req 1 is to handle the first Fizz, so you can hard code that and write a passing test. Then handle the first Buzz, hardcode that, write test, and see the first test fail. Fix that so both pass, then handle the first fizzbuzz... And so on. Then, once the basic implementation is done, ask them to change the rules... Use different divisors, or maybe an additional rule. See how they create abstractions to improve testability and flexibility.
The packages *are* ready, these aren't "backdoor installs", they're completely valid repositories used by thousands of happy debian .netters. [Docker CE](https://docs.docker.com/install/linux/docker-ce/debian/) is also installed in this way, again, works absolutely fine.
If the packages are ready then why does Debian not include them?
You are not getting the real point of that. The real point is that Debian is ultra reliable because they take the time to test everything in their repos. Anything outside of that may make your system unreliable or make a future upgrade go wrong. So they advise against installing anything outside of that if you don't know what you are doing. Obviously there are things you can install without issue as tons of people have been without issues and dotnet core is one of those things.
If I had to guess, Microsoft most likely have to adhere to ISO9001 &amp; ISO27001, the Debian repositories probably can't be audited as easily as their own.
Choose based on your focus. What is the program? The Client Server Systems option seems like a good "general" course because it seems like it could apply to more than just mobile. Advanced mobile dev might be a better choice if you want to focus only on mobile. 
This is very cool! I checked it out with the example, maybe I'm not doing it right: dotnet run -- -fa -oa "testing" flag: False option: xyz Shouldn't it have the following output? flag: true option: testing 
I already did that. I got `.ts` files (the segments). What should be my next step?
Flag is OK because default value for flag is **true**. Flag when passed as the argument flag is changing value to oposite, change default values and try then. In option there was error in target value (set to string "flag", not "option". I've submited fix in the example.
Yes, its been used since the released of .NET 1.1 The only thing that I wished people adopted was F#'s type providers 
Absolutely. Some people have the ORM/ADO argument. And variations of. But it‚Äôs just stupid to me. Pick the right tool for the right job. 
Hi again, Here is an example of how I'm using NFLags together wit WebHost builder: NFlags.NFlags.Configure(c =&gt; c .SetName("AuthService") .SetDescription("Auth Service") .SetDialect(Dialect.Gnu) .SetEnvironment(Environment.Prefixed("AUTH_SERVICE_")) ) .Root(c =&gt; c .RegisterOption&lt;int&gt;(b =&gt; b .Name("port") .Abr("p") .Description("Listening port") .EnvironmentVariable("PORT") .DefaultValue(5000) ) .RegisterOption&lt;string&gt;(b =&gt; b .Name("network-interface") .Abr("n") .Description("Network interface to bind") .EnvironmentVariable("NETWORK_INTERFACE") ) .SetExecute((commandArgs, output) =&gt; { var networkInterfaces = commandArgs.GetOption&lt;string&gt;("network-interface"); var host = new WebHostBuilder() .UseContentRoot(Directory.GetCurrentDirectory()) .UseKestrel() .UseStartup&lt;Startup&gt;() .UsePublicIp( commandArgs.GetOption&lt;int&gt;("port"), networkInterfaces != null ? new []{ networkInterfaces } : null ) .Build(); host.Run(); }) ) .Run(args);
Debian has always been horrendously slow at updating packages. This is literally the reason that Ubuntu was created.
I get package upgrades very often. I can decide how well testet it is. If I want the bleeding edge I can do that with Sid. So why is it I cannot find .NET Core in Sid or in Ubuntu?
Another helpful thing to do is create some "mock" Pull Requests and have the interviewee walkthrough submitting some mock code reviews.
Okay, awesome! I think I was thinking the API would look something like this for defining a flag/option: .RegisterFlag(b =&gt; b .Name("flag") .Alias("f") .Alias("flag-alias") .Alias("fa") .DefaultValue(false) ) Could each flag/option have a single collection for the Name/Abbreviation/Alias? They are all the same thing to me. I don't think it should be possible to have an alias define a different default value -- If the alias default value differs from the target, which default value gets used? Also, by having aliases defined with the flag/option, it wouldn't be possible to accidentally set the wrong target.
&gt;The real point is that Debian is ultra reliable because they take the time to test everything in their repos. And that is why I choice to use Debian for anything important such as my desktop. When MS puts out things like 1809 or Ubuntu breaks with patches then I will skip those system. When I turn on my system then I expect it to just work. Maybe MS could learn a thing or two
don't use debian if you want packages made within the last two years
Are you saying that .NET would be in the next release if it is not even in experimental by now?
Legally speaking, Microsoft cannot maintain it in the Debian repos. There's certain requirement that companies have to follow. So, ask Debian why there isn't a volunteer maintainer to upload it. My guess is that no one finds it necessary, because Microsoft has a well tested vendor supported repository that they keep updated.
Why NSwag over [Swashbuckle](https://docs.microsoft.com/en-us/aspnet/core/tutorials/getting-started-with-swashbuckle?view=aspnetcore-2.2&amp;tabs=visual-studio)?
If they are well testet then it does not take that long to clone those packages. 
I'm not sure if all the answers here have deliberately misunderstood the question or not, but anyway. No, generally using raw ADO.NET yourself is not nearly as common as it used to be. There are newer frameworks and libraries (such as EF) that you can use. Internally, however, these libraries use ADO.NET internally. So technically yes while it's very widely used, it's not often directly used by devs.
I think it is possible to deprecarte abr method and use alias which can be called multiple times. The reason why I defined the target approach is to allow change first name and deprecate old one in description for backward compatibility. I'm aware of issue with multiple default values but I want to cover it by tests and document the precedence. I also have one concern for alias approach. How to define multiple aliases using property attributes. I'm not keen about using array type as attribute property.
I agree, this does seem possible.
&gt; I get package upgrades very often. All of them are security updates. Debian doesn't do any other kinds of updates for stable. Testing gets basically any package that has been in unstable for a certain period without any bugs reported; 90 days from memory.
I'm saying if you want to only get packages from the official debian repo then you're very often not going to find what you need to work with
What's your point in all this? You seem to have an agenda....
Dotnet core doesn't install like that. It would be dumb to have a Deb package that installs it like that.
Wired. Alle the packages/software that is install on my current machine right now is done from Deban's repo, with the only exception being games. They are installed via steam and dont have root access and I dont have problem with them crashing. Witch software should I miss?
Look at this guys post history. You are absolutely right. They are trying to instigate rather than actually ask a question.
So, ask Debian why they haven't cloned those packages?
because there must be an opsticles or someone would have done it already. Or maybe it is not important enough for be cloned?
Why should I not install .NET Core via my distro like all other libraries/compilers?
I am confused why .NET Core is not threaten like as a god setup as GCC/GlibC-setup. Unless the Debian is in freeze then I can find the latest version in Sid. And when a version of Debian is released then it is well testet and integrated in Debian. It looks like .NET Core is not even on the radar. 
I think it's more a combination of the following: 1. Most people DEVELOPING .NET Core apps on Linux just use the Microsoft repos 2. Most people DEPLOYING .NET Core apps on Linux are using the Docker image Essentially, the NEED for Docker to take the maintenance burden isn't there, because Microsoft has a well-maintained repo per OS they support (they have separate Repos for Debian 8 and 9 for instance) 
Wow.
There is also another difference between name and abbreviation. According to Gnu standards the shortform is prefixed with "-" and long form with "--". That alows to use calls like `netstat -tlpn` isntead of `netstat --tcp --listening --programs -numeric` or `netstat -t -l -p -n`. I'm supporting different prefixes torugh dialects and would like to allow such syntax in feature (depending on dialect).
It's meant to be distro agnostic and not actually take over.
well, the .net core runtime/sdk apparently! Also all video games!
That just mean I just use mono instead or gcc instead 
well, if you feel those tools are better suited to what you want to accomplish then I am very glad you chose an operating system that provides those for you!
The network limitation is why the top 10 or so benchmarks are all nearly exactly the same. I recall from a recent ASP.NET Community Standup that Damian said they were looking into upgrading the network to see where they really stand compared to these other servers.
No, looking at your post history, you are doing this same bashing all over multiple reddits. Just stop being a troll.
Could something like this work? [AttributeUsage(AttributeTargets.Property | AttributeTargets.Field,AllowMultiple = true)] public class Alias : Attribute { public string name; public Alias(string name) { this.name = name; } } So would look something like this: [Alias("-a1")] [Alias("-a2")] [Alias("-a3")] public List&lt;string&gt; Aliases; Then initialize the list from the attributes?
We are thinking similar, but to keep difference between name and abbreviation i would preffer something like: ``` [Flag("flag-name")] [FlagAlias("flag-name", "alias1", "abr1")] [FlagAlias("flag-name", "alias2", "abr2")] [FlagAlias("flag-name", "alias3")] public bool SomeFlag; ``` I'm not sure if I'm not complicating things too much. Need to think it carefully.
Yep, definitely up to you, nice project!
One solution would be to use PowerShell or the terminal on Linux to print something and call it using the class `Process`.
I'm currently testing that locally right now. Wondering how it'll perform when deployed. The fortunate thing is my output is going to be a number - doesn't need any styling.
Wow that is incredible! Also amazing that Kestrel is already the 12th most used Webserver, despite being only 2.5 years old. 
funny observation, this guy is an mra as well lol
I'm fairly sure a huge chunk of acute l azure uses it. Would maybe account for that figure.
reading stuff like this annoys me because business doesn't let us upgrade to core yet (even though we literally just started a new project) while at the same time drilling us on this application having to have great performance. Big enterprise moves very slow it seems!
Sheesh, dude. I only said "less than an hour" because the module takes 47 minutes. I'm not selling you anything, I'm trying to teach you how to use ASP.NET Core for free.
This is seriously incredible. This article does a great job at highlighting how awesome NetCore 3 is going to be. I'm already enjoying the preview version and would love to use it in production ASAP.
&gt;Big enterprise moves very slow it seems! I'm in a company made up of 6 people. It moves even slower than enterprises I've done work for. We've had no work for a month now, I could have completely finished a proposed project in that time, but instead it's sitting on a table stuck in "analysis paralysis" mode. Risk averse management is risk averse management no matter the size.
Assuming you're talking about .ASP pages or something along those lines, that's legacy stuff and probably not fully supported in 2017 any more. IIRC when I did .ASPX pages they didn't have a syntax like that (not sure it's been a while) and the next-gen stuff is .CSHTML which uses a @{ your code here } syntax.
I'm in a company with a team of 7 developers. 4 of the developers work in VS2008/.NET 2-3.5 and a core project still only opens in VS2003.
If by "form" you mean WinForms, WinForms is only available for Windows. However the open-source Mono project has ported WinForms to Linux I believe. So your program might run under that. I am not too familiar with the compatibility though. .NET Core is also for Linux but there will be no official WinForms package for non-Windows. I would not be surprised if someone ported the Mono package or some other package provided an implementation for Linux.
I think he just wants to make it clear Mono is not .NET Framework, in the same way .NET Core is not .NET Framework.
I think the issue is literally just politics tbh basically the IT ops in the company I work for are run by a different company, contracted (you have 100% heard this company's name). Currently we are stuck because they don't want to take responsibility of running a new server (Kestrel) without an ammendmend to the contract (which is obvious bullshit so negotiations on that are taking a while). The company wants to switch over to a different IT provider but, obviously, the negotiations for the contract are ongoing so we will be waiting for that a few more years :)
I‚Äôm so sorry
Fortunately my part of the team is using VS2017 with .NET Framework 4.7. Not the new hotness that is .NET Core but still a hell of a lot newer..
 So glad we've moved past this. Upper management decided that it's a waste of time just sitting around talking. Told the dev team they don't care what we use anymore, they just want results and everything to just work.
Seems like someone is trying to cook up a another package manager (like we need more of those).
This is all you really need to know.
This is the correct answer
This looks like the answer https://softwarerecs.stackexchange.com/questions/43018/how-to-join-and-convert-chunks-of-ts-files-to-mp4
Sorry, I couldn't resist doing a comparison to Tortuga Chain. using (var connection = new SqlConnection(_configuration.GetConnectionString("DefaultConnection"))) { connection.Open(); } This becomes var ds = new SqlServerDataSource(_configuration.GetConnectionString("DefaultConnection")); And because it is thread-safe, you only need one for the lifetime of the application. **** if (connection.QueryFirstOrDefault&lt;int?&gt;(@"SELECT Id FROM Contacts WHERE Name = @Name", new {Name = "Charlie Plumber"}) == null) { } Let's see if you can remove some boilerplate. if (ds.From("Contacts", new {Name = "Charlie Plumber"}).ToInt32OrNull("Id").Execute() == null) { } But wait, we don't really care about the ID at this point. if (ds.From("Contacts", new {Name = "Charlie Plumber"}).AsCount().Execute() == 0) { } *** Next up, the insert connection.Execute(@"INSERT INTO Contacts (Name, Address, City, Subregion, Email) VALUES (@Name, @Address, @City, @Subregion, @Email)", new Contact { Name = "Charlie Plumber", Address = "123 Main St", City = "Nashville", Subregion = "TN", Email = "cplumber@fake.com" }); Oh look, the field names have to be repeated three times. Lets clean that up. ds.Insert("Contacts", new { Name = "Charlie Plumber", Address = "123 Main St", City = "Nashville", Subregion = "TN", Email = "cplumber@fake.com" }).Execute(); But wait, I need the newly inserted key. And the way you do that is different in SQL Server and PostgreSQL. var id = ds.Insert("Contacts", new { Name = "Charlie Plumber", Address = "123 Main St", City = "Nashville", Subregion = "TN", Email = "cplumber@fake.com" }).ToInt32().Execute(); No problem. Chain's SQL generator figures it out for you. *** var charile = connection.QueryFirstOrDefault&lt;Contact&gt;(@"SELECT Id, Name, Address, City, Subregion, Email FROM Contacts WHERE Name = @Name", new {Name = "Charlie Plumber"}); Oh, you want that echoed back to you. Ok. var charile = ds.Insert("Contacts", new { Name = "Charlie Plumber", Address = "123 Main St", City = "Nashville", Subregion = "TN", Email = "cplumber@fake.com" }).ToObject&lt;Contact&gt;().Execute(); *** But really, this isn't what you want. You should be able to insert or update in a single request. var charile = ds.Upsert("Contacts", new { Name = "Charlie Plumber", Address = "123 Main St", City = "Nashville", Subregion = "TN", Email = "cplumber@fake.com" }).MatchOn("Name").ToObject&lt;Contact&gt;().Execute(); Can Chain do this? Nope, but it will soon. https://github.com/docevaad/Chain/issues/259 
Friends don't let friends publish from visual studio 
That seems totally fair to me. You usually specify what you are maintaining in the contract. So any changes to that would require an amendment.
Oh.. my... god...
A course focusing on ADO.Net sounds like a huge waste of time. It's rare to use it directly any more, and there's not that much to it. Unless this course is only a couple weeks long..
try [AtomicusChart¬Æ](https://atomicuschart.com/features/labels/) it enables one to describe any of the visualized objects by appending a label to it. With the label, lines or surfaces as well as a specified point can be described. 
Traditional ASP.Net is completely different from ASP.Net MVC. They share some commonalities since they both produce web applications, but there's nothing in ASP.Net that's a prereq for ASP.Net MVC. I did WebForms for over a decade and have been doing MVC for about a year now. It helped that I was familiar with the basic concepts of web applications but other than that it was starting over.
You mean they finally realized they hired you as experts and should treat you as such?! ü§£
You haven‚Äôt even heard of those 4 developers working off a network share and an in-house version control that logs changes on build üôÇ
Shit
Go direct.
I have to agree here. I am wondering why they are listing ADO.Net here. Maybe it implies they will be using something written on top of ADO.Net, but it's not clear. It might this is just an older course that hasn't been updated. Go for the mobile. 
Ozcode also has inline variable viewing.
That's crazy. There's so much value on running code. My company is 20 people and we are at 2.2.
Thanks! The built in options get values across multiple stages, which includes json files, environment variables, and cli arguments. The final overridden values then get injected as an instance of a POCO class. If I want to plug NFlags in this config chain, I'd need to: 1. Define attributes for the abbreviation, custom name, description, then add them to properties in my POCO class 2. Assign default values 3. Read it via reflection and register with NFlags 4. Read the config values and NFlags values, put them together, inject the POCO Is that correct or the best way? If so, would you be interested in adding standardized attributes and, if that's an option, something similar to \`.AddCommandLine()\` from the \`Microsoft.Extensions.Configuration.CommandLine\` package that does the above? Note there are currently two similar hosts, HostBuilder and WebBuilder, but they are planned to converge into a single host in Core 3.0 (I think).
Jesus, and I thought being stuck on VS2008 to do WinCE dev was bad.
Oh yeah, the network share version control system where you'd literally yell out what project you were opening to make sure nobody else was going to open it at the same time. I'm so glad we moved to git.
I would say in general when using REST for CRUD then you should try to limit a controller to one model. REST is what you make of it, there isn't anything stopping you from combining operations or implementing HATEOAS, or choosing a slightly different return code than what you've seen in a github example project.
Just to play devil's advocate, it would be nice if the title mentioned this is just plaintext responses. Each response is just the same string being sent back to the requestor. On the fortunes test, which is their test simulating real world web traffic, it doesn't get these crazy high numbers. Still, it gets impressive scores in fortunes.
I don't think its necessarily bad practice. Depends on the need for your API. I have 3 separate DTO (input, output, update). I don't want the user updating certain fields and I have extra properties I send to the user via the view DTO. I use automapper to map these various dtos to the base entity when the database needs to be queried. 
Yeah I use dtos and automapper as well, I missed the point of his question.
I‚Äôm sorry :(. I just decided to use core on my own and everyone had to accept it. There was push back but because things were getting done, it became a war between the doers and management rather than a war between two managers. 
What should be my [ASP.NET](https://ASP.NET) MVC learning path? 
Kinda misleading. [If you click the link it's a different picture](https://i.imgur.com/hmJLdoo.png)
The last job I had, that I left a decade ago, is still using VB6 and FoxPro.. whatever, 9 or something idk. I hear they still give me shit/blame for writing some stuff in .Net because they can't even manage to update form label text.
We're 30 people, 6-7 devs, and we're partly up to 2.2 but with a dozen year old code base with a couple tricky dependencies it's still going to be a year or two before we're fully targeting Core in production. We always look at greenfield work as opportunities to bring forward our tech stack, and tech staff has near complete autonomy over those choices.
Build stuff.
Thanks!
They also benchmark SQL (postgres, mysql) read and write access.
I like this design guideline for API: [Microsoft API Design Guidance](https://docs.microsoft.com/en-us/azure/architecture/best-practices/api-design)
well kestrel is typically used behind nginx / apache / other used as a reverse proxy so technically you could be using both at the same time
Will a deep understanding of Modern CSS &amp; Javascript help?
Uhm. That Good explanation Thank you 
It would be more efficient to use **OpenAsync** method. Also, Dapper will automatically open connection even if you don't call the Open method.
Yes and no? Unless you‚Äôre using MVC with angular or react, js and css is a bit irrelevant? Obviously you need those two to create good looking sites, but that‚Äôs not anything to do with MVC really. 
Yeh ,this is the most resumided post in the comments, thanks allot!!
Considering official templates use NPM for client side dependencies, I wouldn't say that Library Manager was a success... 
Some information on Library Manager: https://blogs.msdn.microsoft.com/webdev/2018/04/17/library-manager-client-side-content-manager-for-web-apps/
Jesus, that's a clusterfuck given the wide variety of free tools specifically made to solve those problems a decade ago...
And where I am uses old cludgie practices for greenfield work... Which hurts me inside in many ways... So many opportunities to bring knowledge forward and it just gets squashed because of familiarity.
Tried code rush?
Completely different items. .Net is the very old framework that has been fleshed our over many years, but for Windows only (there are some ways around this). .Net Core is completely new, leaves out a lot of items, but is completely gross platform. 
Do you have an opinion on Code Rush?
You absolutely can build UI‚Äôs with .Net Core, maybe not using items like winforms, but plenty of packages exist to build them with. 
While it is way better than installing frontend dependencies from nuget, it's still the wrong approach, that I would hope no one uses, except for small experiments. &amp;#x200B; Modern frontend development should use the tools meant for that (npm/yarn, webpack etc.)
&gt; Please consider implementing base repository pattern for common CRUD operations, writing all this for an insert or Select is exhausting. That can be quite a pain in the ass. You need to dynamically generate the SQL, paying attention to attributes such as `Column` and `NotMapped`. But reflection is slow so don't forget to cache. And if you are using SQL Server, there is a huge performance hit if you pass a nVarChar parameter to a varChar field or vise-versa. So some extra attributes, or database reflection, is needed to sort that out. This is why I created Chain. I want Dapper-like performance without the headache of writing all that damn SQL.
Not a strong one. I've tried it a couple of times and it seems like it could be a good lighter-weight alternative to ReSharper, but I didn't give it much of a try and I figured out I am actually pretty happy with my selection of VS extensions. I think it boils down to how well these tools suit you. A few extensions still feels fairly snappy (for an IDE) and the experience feels close to the original VS experience. ReSharper feels like it adds a lot of features and in a small way transforms the VS experience (e.g. refactorings, intellisense etc), but some people perceive "bloat". CodeRush seems like it falls somewhere between the two, going someway to transform your experience but without as much bloat. &amp;#x200B; I've not spent any significant time with either though. When I did try the CodeRush demo I installed it, but didn't find myself actually using any features before the demo ran out, so I uninstalled it. Getting to know ReSharper/CodeRush is one part of the battle for me.
Yeah, but if you know SQL (which you better do) and learn EF - then learning raw ADO is what you do on a lame friday afternoon. There really is not a lot there to start with.
I hope the answer was more useful than "yes ADO is used all the time everywhere" answers haha.
Thanks, but again, they're suggesting to use ffmpeg (external dependency). I needed something within C#, that'll give me full control in my application.
Why not go MVC Core? It's the future (and it's already mature enough to be used in most common projects)
We decide to take the bullets and bring forward all the MVC 5 projects. The lifesaver is that we use LLBLGen as ORM instead of EF. So that part just works.
In the area where I live 90% work is still being done in ASP.NET MVC, I will learn Core later after a few months.
There's nothing built in. You'll either have to write one or use a dependency. Why can't you use an external cmd line programme?
I would say C# is the only required knowledge. I suggest Pro [Asp.net](https://Asp.net) MVC 5, it will really help you build maintainable applications. 
The issue with that approach is that we always have to have some hackish build scripts to merge JavaScript and .NET build tooling and ASP.NET expectations where FE files should come from.
We have a bunch of code using Subsonic, which references the Linq2Sql library, which itself has a hard coded dependency on earlier full framework and will likely never be Core compatible.
If that's a problem, you can set that up to be a seamless part of the build, all baked in to your .csproj file. Try creating a project from the [ASP.NET](https://ASP.NET) Core/React template that comes with visual studio, it sets up an example of how something like that can be done.
If its a WebAPI + SPA I usually separate the projects. Trying to build it all in one project you do run into some weirdness, but if you keep all the JS / frontend parts in its own npm project, and maintain its build pipeline as a regular npm build pipeline that just copies its output back to the main deployable project as static files, then you don't mix build systems as much. Trying to do it all as a first class citizen of VS in a single project can be done, but I like the separation, and that way your front end only people who don't want to edit in VS can still run their normal workflow without having to worry at all about VS.
I'd also recommend keeping them separate. It's easier to manage. Besides, VS really sucks for JS/TS development. Use VSCode for that instead.
We usually do full stack, and WebAPI + SPA are quite rare.
https://github.com/FransBouma/LinqToSQL2 this might help if you want to port things over.
not yet! I'll give it a go!
This looks pretty neat, and definitely something I'll look into after .net core support is available. For what it's worth, though, [Flask](http://flask.pocoo.org/) is a very popular web framework for Python, and I wonder if that will cause confusion at all.
Thanks, .net core support will need a lot of rework. I am mostly a traditional .net developer with the full framework thing. I am still trying to figure out if this kind of approach to testing fits well into new microservices stuff or not. I mostly worked with some kind of microliths with lots of external integrations in one service so mocking them with weaving made sense. Yeah, the name is confusing to many however it stuck with me so it needs an effort to rebrand the whole thing. 
I'm thinking about something like: ``` public static class Program { private static void Main(string[] args) { var builder = new ConfigurationBuilder(); var configuration = builder.Build(); NFlags.Configure(c =&gt; c .SetDialect(Dialect.Gnu) .SetConfigurationProvider(new ConfigurationExtensionProvider(configuration)) ) .Root(c =&gt; c .RegisterOption(b =&gt; b .Name("asd") .EnvironmentVariable("asd") .ConfigPath("Profile:MachineName") ) ) .Run(args); } } internal class ConfigurationExtensionProvider { private readonly IConfiguration _configuration; public ConfigurationExtensionProvider(IConfiguration configuration) { _configuration = configuration; } public T GetValue&lt;T&gt;(string key) { return _configuration.GetValue&lt;T&gt;(key); } } ```
why don't separate the PDF generating/saving functionality from printing, let your .net core app generate / save pdf's somewhere in the file system then use automated printing solution sthing like [https://www.foldermill.com/solutions/automatically-print-incoming-pdf-files](https://www.foldermill.com/solutions/automatically-print-incoming-pdf-files)
I was using just for the simple task of downloading a package and placing it in wwwroot. I ended up switching to npm + scripts because the package sources Library Manager supports (unpkg, cdnjs) were completey unreliable. 
...... this makes me sad. String is now a nullable type... like it wasn't before! Part of me sees the desire, it will help with people to signal if they are expecting a string.empty vs a null to represent nothingness. I also feel like this will force people to use what fits the type better than what fits the process. We added nullable to value types so they could be treated like references, now we add nullable to reference so they can be treated like nullable value types? 
I think you've got something backwards. I think the name of the feature is misleading. Maybe it should have been "non-nullable reference types". As you point out, reference types have always been nullable. That's not changing. What's new is a way to refer to reference types in a way that can \*not\* accept a null.
Exactly. I guess it's called "nullable reference types" because C# team is targeting to treat reference types as non-nullable by default (so 'null' is not used normally for reference types, like it is today in F# for instance), so they are going to allow "nullable reference types" when you turn it on (maybe it will become default in the future).
Blazor is something I really want, but am scared/hesitant to lean into.
Just try it out, this is not LSD.
But then I might like it. Then I might want to sneak some into production. Then I might be screwed.
As someone that uses kotlin, knowing if something can be null or not prevents so so many bugs. 
Yeah, but all I see is either frustration in mapping between these two types (imagine working with 6 libraries where half use this). Also I am not sure what problem is this solving. Things like LINQ make sense to me, they make handling a list much easier by providing common solutions to list manipulation. What problem is this solving? that people forget a try / catch? 
If it passes your integration tests you‚Äôre fine :) Until you need to update and they‚Äôve changed the whole API. 
Pretty sure long is Int64 datatype, and int is Int32 datatype.
I don't really know if we should see this as real problem-solving feature... it's just changing some approach. Theoretically, in will limit the number of NullReferenceExceptions, so in theory the CLR will be less involved (by not handling them) and maybe we'll be handling our issues/cases in a different way, e.g. by throwing more meaningful exceptions. In practice - we'll see that in few years... External libraries you mention will be an interesting part ;)
Just saving null reference exceptions I'm sure. Also, try catch isn't what I'd recommend for solving those. Should be doing proper checks ;) 
Yea... that's what I meant. Unfortunately, cannot edit the title. :)
Has there been an announcement about when .NET Core 3.0 goes out of preview? I'd like to upgrade to 0.8 but I think I'd rather do VS 2019, DNC 3.0, and Blazor 0.8 all at once when it's out of preview.
You're fine just wanted to make sure you were aware just incase. Check out this SO post, and I also found a decent article about JOIN performance with int, bigint, uniqueidentifier as PK's. [SO Post](https://stackoverflow.com/questions/2124631/sql-server-int-or-bigint-database-table-ids) | [JOIN Performance](https://www.mssqltips.com/sqlservertip/5115/compare-sql-server-table-join-performance-for-int-vs-bigint-vs-guid-data-types/) Hopefully this may give you some insight you don't have yet. In my opinion depending on the types of queries you're performing there shouldn't be a very big difference. However that all depends on the scale of the application. How many user, making how many calls, etc.. You'll start wanting those millisecond efficiencies when you're transactions start deadlocking.
Thanks for the reply! I'm not necessarily worried about number of records, but more about the actual numbers on the primary keys. For example, I have an application that user create and delete items quite frequently, so the pk values are ever increasing.
Ahh I see, well check [this](https://docs.microsoft.com/en-us/sql/t-sql/data-types/int-bigint-smallint-and-tinyint-transact-sql?view=sql-server-2017) out. Essentially using the int datatype you have 4.2 billion numbers available (2.1 billion in your case as primary keys iterate in a positive direction.) So if it is possible for your users to actually consume all 2.1 billion of the lifetime of the application they you may want to go to bigint which gives you so many numbers in the positive direction it would be unbelievable if you ran out.
Yea, that's what I was thinking. Not all items have a bigint as a pk, except for the items that are created/deleted most frequently. Thanks for your input!
I AM interested to see how library developers have to work with this... Right now enforcement is at a top level project level I think... That said, I love this. I'd rather do a null check when I know I need to, instead of EVERYWHERE. It moves it from a runtime check to a compile time check which I am absolutely in support of.
Seems like a "whoosh" to me. It's a new compiler option where reference types can only be null on an opt-in basis. This is how you opt-in.
I can't decide whether the smiley face means you were kidding or whether you were being reassuring.
Last week I was at NDC at the likes of Scott Hanselman were saying it‚Äôs due for a release around ‚ÄòFall‚Äô 2019
Hm, that's later than I was expecting. Maybe I'll jump on the upgrade train sooner.
If it helps I‚Äôm running preview 2 of DNC3 and Visual Studio 2019 on my regular development PC and it‚Äôs absolutely fine. VS2019 feels really far along for a preview, and DNC3 is isolated well.
Those are used to help you exactly position controls. Most people don't use them because exact positioning and resizable windows don't play well together. I recommend learning how to use containers such as grids, wrap panels, and stack panels instead.
It's another non-used one. Check out the original github issue for it, there's a lot of well made points by people in there as to how this is inferior to npm etc.
If you're solving null reference exceptions with a try catch, you're doing something wrong.
I am glad they are taking the time. DNC 3.0 is absolutely humongous. 
You need to consider just how likely you are to hit the 2.4 billion mark if / when your application becomes super successful and you have a large number of users / customers. Working at a company right now that is grappling with this in some of our databases, I can tell you that it is a logistical nightmare when you suddenly need to double the physical space used on disk for a column for billions of records (without a prolonged outage). A lesson from others' pain: If it's even remotely conceivable that you'll have 2.4 billion records then go with \`bigint\` and call it a day. You'll save hundreds of thousands, if not millions, in software developer and DBA time in the future.
Yeah, I'm surprised they're doing preview releases this early. I guess there are a lot of things done and a lot still to do.
The ASP.NET Core 3.0 new features on endpoint routing alone makes it look like a new framework altogether. Add on top of that Razor Components, the blazing hot performance and other smaller features. 
Linq2Db is really fast too.
Just do it already!
I was hoping to grab it before my access to educational licenses expires. Work will upgrade me to Pro eventually but being able to just download and screw around with Enterprise has been awesome.
I was thinking of letting the server handle the printing- users could select a printer connection from the list. I‚Äôd need to test lag time for these types of functions, though. Any significant lag will bottleneck the overall process and start to reduce the cost savings we‚Äôre trying to achieve. Thanks for the tip, I will definitely look at this option. 
The problem this is solving is using the type system to document whether a value is intended to be null or not. I've been hoping for this exact feature since C#3.
&gt;this is not LSD. Even if it were, try it anyway! It won't hurt.
invokedynamic, constantdynamic, MethodHandle/MethodType constant pool entries. Less restricted goto (used just for breaking out of blocks) and jsr (used only for try/finally but supposedly it's slow so don't use it anyway.) All the other features can mostly be accessed by normal Java APIs.
Nullable to value types so you have treat them like refs. It was added so they could be nullable. Additionally the main change here is to enable reference types to be NON nullable.
A few hours of overtime and maybe some meetings will fix that ;3
Being explicit will solve a lot of problems especially on a big old code-base that's gone through a lot of developers. It can be a simple as knowing that a type is intended to be nullable or not, and most of the time it isn't. This also eliminates the old null checks on every reference types so it makes the code-base cleaner.
Depends on what you're trying to do. One build-in alternative can be [`System.Linq.Expressions`](https://docs.microsoft.com/en-us/dotnet/api/system.linq.expressions.expression). This only works for generating methods (not types) and is rather verbose, but the code is reasonably easy to understand for anyone. Dynamically compiling C# is also possible, the go to method is Microsoft's own Roslyn compiler and comes with [scripting/REPL](https://github.com/dotnet/roslyn/wiki/Scripting-API-Samples) capabilities.
I would suggest that you start with CQS. Starting with CQRS can be significant effort from day one. https://www.dotnetcurry.com/patterns-practices/1461/command-query-separation-cqs
I am generating methods that access methods/properties directly (without needing reflection). Would using System.Linq.Expressions work if the method uses types that are unknown but compiled?
Yes, you can obtain information via reflection and then generate the appropriate `Expression.Property`/`Assign`/ `Call` expressions.
Thanks! Any ideas of projects on GitHub that are currently doing this well?
TeamCity alongside Octopus Deploy is a dream pipeline to work with
I just wanted to repeat what /u/grauenwolf said. I wouldn't recommend using the designer to actually position controls and such. I've been using WPF for small hobby projects for a while, but we recently began using it at my workplace too. This meant that my coworker had to learn how to use it and he struggled with positioning stuff properly, because the designer sets crazy margins, wrong rows or columns in grids, etc. Once I convinced him to manually write the XAML, he had a much easier time positioning controls and making changes to the layout. Use the designer mainly as a tool to see how the XAML you write looks, not as a tool to write the XAML for you.
Totally agree. Have been using TeamCity for years. Previously, I used it for deployment too, but never really worked out well. With Octopus to handle that part, it is indeed a dream pipeline. To be honest, I've used Azure DevOps (previously VSTS) the last year. While the release experience is not as good as Octopus, it is still pretty good actually.
Azure Dev Ops minus point of "Long setup for first project." I find not so accurate. Those others take just as long in my experience. Cant wait to test out GitHub actions 
I mean there‚Äôs an Azure DevOps (through azure, not the newly named Azure DevOps) project that creates a repo, a build definition, and a release definition for you in 5 minutes that you can use. In terms of setup time, it‚Äôs hard to beat that.
Join the dark (or I'd say blazing) side, we have baked cookies.
I can vouch for this. Been using VS2019 Prev 2 for my work station for a while now haven't detected issues so far. Though I do have VS2017 installed on the side in just in case.
Totally agree. The integrations with Azure makes this piece of cake. I guess the minus is for people who use DevOps but doesn't host their site through Azure. Setting up a single project is pretty easy using the predefined templates, but the whole administration part of DevOps is a bit confusing IMO.
Come on, dude. It's 100% pure c#. Not the cut-up with Javascript garbage you're probably used to. We just mix the c# with some little stuff we call "WA" and burns any JS right out. Browsers love it, really gets them going, yknow? Just try a little tutorial, totally safe.
This comparison is very shallow. It doesn't take under account the possibility of using hosted/own agents, work management, test management and any of the specific features above just compiling code. For example, Azure Devops is a complete Application Lifecycle Management solution, so you can manage your environments and roll-outs in it.
It's an introduction comparison of the build server parts of the 6 products. Agree, there are so many other features available in most of the products, but for this post, the key focus is on the build part only. It would be interesting to compare more features and products like Azure DevOps release management vs. Octopus Deploy. Input for future posts :)
It‚Äôs like comparing programming languages by looking only at Main method signatures. While I understand the lack of insight in the text I do have to point out that less experienced engineers might skip in-depth research and pick one based on the fact that the article is posted on a well known website.
It's more like comparing web frameworks from different programming languages I believe. Build is a central part of all of the mentioned products, but I'm fully aware that there are so many more features available. Every product has advantages and disadvantages. The post doesn't dictate anyone what to use. People need to try out the different products and pick for themselves :)
I like CS Script for that. https://github.com/docevaad/Chain/blob/master/Tortuga.Chain/Tortuga.Chain.CompiledMaterializers.source/shared/CompiledMaterializers.cs
yeah im talking about the formerly named visual studio online / tfs
But when thinking about it, you do have a good point. Neither Kristoffer (the author) or I want to tell people what to pick. The post has been extended with a few words about that. Thank you for the input. Appreciate it.
I mean webforms is also C#, and it's pretty much a disaster to use and maintain. I'm thinking of trying this when I have some free time, I just hope it's not webforms redux.
Was the app done in .net or .netcore? If it was done in .net, than i think you can't host it. Maybe in Mono, i guess ( don't have enough experience in Mono to assure this) &amp;#x200B;
Tip of the hat to you sir.
You are correct. I don‚Äôt think you can host .net 4 on Linux. I wouldn‚Äôt even try. Maybe you could fire up a windows VM on there? Is that possible?
Uno already has a WebAssembly target: [https://platform.uno/](https://platform.uno/)
it confused me. not that bad but my immediate reaction was that i thought someone accidentally posted it here.
While Webforms are terrible, Blazor's concept is saner. Instead of trying to hide the stateless nature of the web, you basically run the app on the client (or on the server and automatically patch the client's UI). It's in ways closer to WinForms or traditional Javascript SPAs than Webforms.
Build kite is worth a mention
Can you write an asynchronous function that handles the database stuff? Then feed back the results once it's complete? 
Nice. I've never heard about that. Does it have built-in support for .NET?
In my experience Azure DevOps is fastest to setup, just need to have bit of experience and use all capabilities.
That's amazing, I had no idea. &amp;#x200B; How mature is this project? Is it production ready?
Agree. When you get to know the product, it is pretty straight forward. For .NET full framework, I think AppVeyor is actually faster. Maybe because I have more experience with that, who knows :)
And nothing's stopping you from using Azure DevOps with Octopus.
Nope. It's nice to be able no mix and match. Feels like an adult version of a candy store üòÇ
You mean using async/await? How will that help? The method called on page submission will still be called those many times.. 
Interesting read, thank you. But what really breaks my reading flow are the many typos and grammar mistakes. Maybe you should have a second look at it. :)
Thanks. Will have a look :)
Okay so I would create some sort of hasBeenSubmitted token and store it in the session cookie, then call off to an asynchronous function to handle the database stuff. Whilst that waits simply return this submission token to the user and use that to 1. Disable input and 2. Prevent submission again (validated at the point of call).
Wow. See your point. I've just made multiple corrections to the post. Thank you so much for that feedback. Will need to spend more time looking through the grammar next time!
&gt; Web forms Is this a legacy application you are being forced to support, or a greenfield project?
It's more of a javascript younger brother than silverlight in my opinion. It runs in the same secure sandbox and offers the same functionality.
Unless the web app was built in DotNet Core, forget Linux. Your only option is a Windows server. Period. Or, rewrite it in DotNet Core. Looking for a dev to do that?
It‚Äôs a bring your own build agent system, so comes with whatever you bring. It‚Äôs a super nice system if you prefer having build agents that have not much more than docker installed. You can then build your apps inside of a container. Works great with .net core and any docker friendly languages and frameworks really.
I see, sounds like a good approach. Thank you for the suggestion. Another new tech to check out üëç
 Checkout Syncfusion [HTML to PDF in C#](https://www.syncfusion.com/pdf-framework/net/html-to-pdf), it uses popular rendering engines such as Internet Explorer (IE), WebKit, and Blink (Google Chrome) and it is reliable, accurate, and high performance .NET library. [C# HTML to PDF example](https://www.syncfusion.com/kb/9143/how-to-convert-html-to-pdf-in-c-and-vb-net)
Legacy app.
I've only looked into their samples and read some of their blog posts on [medium](https://medium.com/@unoplatform), but it looks very promising. The WebAssembly support is still experimental, and it's pretty slow at the moment, but there's always room for improvement. Take a look at this Xamarin.Forms app running on WebAssembly using Uno: [http://bikerider-wasm.platform.uno/](https://bikerider-wasm.platform.uno/)
Maybe add it to https://github.com/thangchung/awesome-dotnet-core
I 100% agree about Octopus Deploy, but TeamCity is probably one of the worst build systems I have worked with. We are actually excited about moving to Jenkins.
Especially if you're familiar with either UWP (code-behind style with less boilerplate) / Vue / Angular it's pretty similar and is quite easy to pickup imo. Just like creating a somewhat local app (Blazor) that talks to an API backend.
Exactly, it's not exact copy or resurrection of identical Silverlight framework. But if you have C#+XAML in browser, what ever the underlying stack is, I'd say that's close enough. Lots of enterprise companies invested in Silverlight in early stage and lots of them had to migrate to HTML5 apps (as managers love to call them). I just find it funny that if in few years Blazor turns out to be technology that can replace Silverlight apps, it would turn out migrations were huge waste of money . My company is still working on migrating huge Silverlight backoffice system to HTML app. It's taking so long, it's starting to be funny. Especially because we keep getting new requests from business side to add more features to existing app, which means we have to add that same new feature to HTML app backlog etc.
dotnet will only run dotnet core apps. Yours is not. mono is a linux platform used to run old .net apps on linux. It basically mimics .net. It's an option but I don't know how difficult it would be to setup and you'd likely have to modify the app so it would work with mono. So it was probably a mistake to offer hosting. 
In .NET Core 3.0 you can compile and load assemblies which are lightweight and fully unloadable.
Here‚Äôs how you do it: https://benpowell.org/asp-net-disabling-the-submit-button-to-prevent-double-submissions/
&gt;What if, in the cases like our example, we knew before that in this place our object might be null? The problem with contrived examples: The type is string, so it can be null, so you should have know. 
You should also be aware that generics allow writing of code where the types are not known when you write it (but must be known at compile time). Eg List&lt;&gt; and Dictionary&lt;&gt;. Not sure if it's relevant to what you're doing but if so it should be a lot faster and cleaner than using reflection.
&gt; In .NET Core 3.0 you can compile and load assemblies which are lightweight and fully unloadable. Awesome, I've been wanting to unload assemblies in one of my projects. I usually have to copy the DLL to a temp folder, load it in, and then discard any references to it when I see a new version of the DLL to load in. Unloading will be much better.
Use HTTP POST for any request to the server that results in changes (in this case, records being created). The browser will not automatically allow refreshes of POSTs and will ask the user if they are sure as it might not have the intended result. HTTP GET should only be used for requests that do not modify server data.
I understand that, but the DevOps project I‚Äôm talking about creates the repo and build/release definitions in Azure DevOps (formerly VSTS) is what I‚Äôm saying. 
I'm aware of generics, but I truly know nothing about the types I will be accessing, what method/property will be using, etc.
Darn. i would go with the guy who recommended Expressions then.
Can't wait for the hot reloading, but until then I'll let Blazor marinade for a bit. lol
I don't totally understand what you're asking, but if you right-click on a method one of the options is "Find All References". That lists all the places that method is called.
It is. But I want to do it the other way around since the method I want to see if it‚Äôs used has around 1000 usages. Therefor I want to check from a specific point if that method ever calls the mentioned method. 
Wouldn't that just be a search within that method then?
Web assembly is a W3C standard. Currently implemented on all major browsers. It is a compilation target for any language that wishes to do so. MS can't nix webassembly, and if they tried, someone would just fork Blazor since the code is open. Sanderson started Blazor outside of MS anyway, MS adopted it. They'll like be big improvements with a separate runtime implimentation later when the wasm spec nails down a spec for garbage collection and other native hooks instead of sending everything through javascript. I think the likelihood of Blazor going away is slim. It is one of the truly innovative things that make .Net development compete with other stacks, and one of the only widespread uses of webassembly being developed right now. Currently the only major webframework.