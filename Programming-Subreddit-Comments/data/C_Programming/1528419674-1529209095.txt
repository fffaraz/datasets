Stack will outperform heap so if you're going for simple / minimal it may prove better if memory requirements are low
I believe C11 standardized threads. If you can get a C11 conformant compiler, you should be set I think.
This is great stuff! SERIOUSLY
libuv has excellent cross-platform abstraction for all kinds of things, including threads.
As a Unix clone, Linux supports pthreads. The C standard library has no concurrency features, so anything you use will have to be either provided by the OS or third-party library.
For wall-clock timing you can have two arrays, the first one randomized and copied over to the second for sorting. Sort the second array and re-copy the first array to second to re-sort over and over. Try 5000 cycles of copy and resort, for both methods with stopwatch timing. Of course the timings will include time spend doing the copy itself, so you should do 5000 copy's without any sort to subtract from the other two timings. Both sorting algorithms would be re-sorting the same original sequence which might be an easy or hard case depending on luck for that batch, but you can run the whole program more than once too.
It’s really easy to do a combination of pthread + Windows thread APIs to get native multiplatform multithreading without depending on some library. I think I could do a template project showing it.
OpenMP is stuck at like, version 2 in MSVC. and it's not even included with Xcode on Mac, tho you can install it from homebrew.
and standard library. Mac's standard library doesn't support it, and neither does microsoft's. you're pretty much limited to FreeBSD and musl.
oh...yeah...right... will update the code. thanks
That's very kind of you. and yes... I can use all the help I can get.
yes...much better... Thanks :)
it is supposed to be used as a library. And the input(the array) can be anything. 
The C standard library certainly does. It's just nobody bothered to implement C11 threads.
I once whipped up a basic (create thread, join thread, mutexes) thread package that uses pthreads or Win32 threads depending on if WIN32 is defined or not in... an hour or two? Most of which was spent reading MSDN documentation on the Windows functions. As long as you don't do anything fancy, it's easy to do.
It has to do with access time. Addresses are sequentially aligned so io is based on incrementing / decrementing. Heap can handle fragmentation so it can allow for larger address spaces
Never is a long time.
C won't die until there is a language at least as ubiquitous with the same low level control. Even LLVM doesn't cover a lot of embedded architectures, so writing a compiler targeting these platforms is an uphill battle.
What they fail to realize is that their beloved higher-level language is probably implemented in C.
Yup.
I know of Vulkan, but I have no idea how it fits in. Someone else said Vulkan may absorb OpenCL, and therefore doesn't/didn't? have it's own GPGPU API? 
I'm still waiting for c+++
c will never ever die. c++ is a hipster abomination so c programmers can take advantage of oop while retaining their ability to frustrate code maintainers with pointers and garbage collection. fuck c++. 
C still offers the lowest-level abstraction from the machine code. Rust attempts to do better but just creates more abstraction. D truly offers an alternative approach bit has failed to find popularity. C++ is like TypeScript for C - a templating framework. 
shh don't tell anyone, if they found out some good came of a reddit sub there would be heck to play, it has to keep its toxic reputation you know! but to be serious for a second, keep plugging away at it, should make a nice library... 
C**
Whenever you see someone say C is a dead language, ask them to program any embedded system or write any compiler, they'll be more than surprised
C--
Could a more experienced programmer than me comment on this viewpoint? 
yes please
Uh, it's actually (C++)++.
You can write the compiler for a language like Go in Go. It's actually a pretty common task for new languages to prove their worth.
Not c^2++ ? 
C will die when a new OS written in a new language will become as popular and widespread as Unix was. For example, if quantum computing does become available for industries (and probably banks), it will eventually get a good OS that must be written in something different than C. But before any of this happens, we kinda have to wait for incredible new physics discoveries, as powerful as the transistor was.
yes.
noobs. its ++c++
I'm pretty sure it's +cc
hahaha :P
C is a really dead language if all you've ever worked on is web based or just course assignments...
It is a somewhat extended view of C++. In fact, Linus Torvalds, the creator of Linux and git, [wrote his thoughts on C++](http://harmful.cat-v.org/software/c++/linus) once on a newsgroup and they were not very flattering. In my experience, C++ tends to overcomplicate things and I find the syntax somewhat obtuse. This has been aggravated throughout its lifetime as C++ has tried to support too many different ways of working: procedural programming, OOP, templating, and even with things like memory allocation (references vs pointers, smart pointers, copy and move semantics...). It's a bit of a Frankenstein language that wants to do it all grabbing ideas from all over the place, in direct opposition to the conservative approach C or Go follow, where keeping the language small and concise is a primary design goal. C++ is definitely not going anywhere but I reckon newer languages like Go and Rust are slowly gaining on it in those areas where performance is important but not the main priority.
Because in systems programming the low level control of C\+\+ benefits performance and you can make good use of templates and classes? I agree that for a kernel C would be a better option, but I can't say the same for a dbms, a key value store or a video game.
C\+\+ might die soon as Rust has everything need to replace it! C wont die! 
Why such a gem commentary was downvoted? C\+\+ == abomination!!!!!!!!!!!!!!
video games and accounting applications? really? bullshit has a use case too. its called fertilizer. :D
It’s a silly argument. The only people who argue about which programming language is the bestest are idiots. Use the tool to suit your task. C++ is commonly used when speed is important and/or memory is a concern. For example games or embedded systems. 
I think the thing is that C does it's job 'well-enough'. Sure it could be better, but nothing is perfect. New languages tend to be created to fill gaps in functionality, not replace olds.
C will never “die” in the same way that Fortran, Lisp and COBOL will never really completely die. There will always be legacy code that requires a working knowledge of these languages. With C especially given that it is ubiquitous in nearly all systems level software. 
&gt; C will die when a new OS written in a new language will become as popular and widespread as Unix was. Or they rewrite Linux in Rust. Which they are totally gonna do, and is, like, right around the corner! (/s)
Currently all the higher\-level features and types of newer languages are simply mangled away when programs are compiled and linked. They are just an arbitrary, artificial layer that is convenient for the programmer, but not really necessary and at the cost of complexity. There will always be demand for a language that omits all unnecessary stuff and is as simple as possible. C will die when some new computer architecture or programming language feature (maybe a new way to do memory management or concurrency) becomes so universal and ubiquitous that it has to be part of *every* new language and every modern platform's ABI, but cannot be added to C for some reason. C will then be replaced with a language that includes it, but not much more, and will also be as simple as possible.
C-- exists already. https://en.m.wikipedia.org/wiki/C--
Non-Mobile link: https://en.wikipedia.org/wiki/C-- *** ^HelperBot ^v1.1 ^/r/HelperBot_ ^I ^am ^a ^bot. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate. ^Counter: ^190448
This is painfully correct
I would ever recommend "c programming: a modern approach" by king. it's probably the best book i have ever came across to study C and introductory programming. There's also lot's of githubs with all the solutions by past students. As always google is your friend. 4 days gonna be really thight dude, GL:HF
My company writes web based applications, and whole web sites, in C and we are not alone.
&gt;popular and widespread as Unix \*\*is\*\*. FTFY
TIL 
You need to boot strap the whole process at some point, though, right?
&gt; seams weird to me to have POSIX threads on windows (From the main post) POSIX is just a standard, so POSIX threads is also just an interface which is just part of an larger standard. So there is an implementor and a user. You are the user of such interface while there is an implementation of the interface for (in this case) Windows and other systems (e. g. Linux, BSDs, ...). Neither Linux or Windows are (fully) POSIX compatible, but it doesn't stop you to implement (and use) some stuff. For example, Cygwin provides an POSIX environment on Windows by primarly implementing it's APIs.
Sure. At some point you have to write the initial compiler or interpretor in some language. But that doesn't mean that the new language only exists because of the language used to write it's tooling. There language used to make the tooling doesn't matter and can always be changed. Further, do you credit C for existing because of the language used to make the first compiler for it?
I'm an electronics engineer and we use C extensively to write compiler specific code in embedded systems. And if I'm not wrong the entire Linux kernel and by extension, Mac, Android and even Microsoft is based off of C. Sure, there are better languages like Java out there for application oriented development but no language comes close to even touching C when it comes to the machine level implementation. The level of memory management and speed C offers is unparalleled. Of course I am biased towards C because of it's simplicity and sheer power. 
I can only assume that mustve been some assembly compiler. So yes, its thanks to that.
It's actually a descendant of NB which itself comes from B. So by that logic, the B is a more important language than C because C was implemented in B through NB. So we should all be praising B.
Praise B.
I will give a specific use case - safety critical software. (MISRA) C and C++ won't be replaced any time soon in this field - this includes automotive, aerospace, medical machinery, etc. (SPARK) Ada is in use in some places too, but it's not been a full replacement in my experience. This encompasses large parts of the economy still. From what I've heard Rust could come into the picture, but in terms of maturity and proven track record it's miles off C/C++, and rewriting such large code bases, adopting new tooling and infrastructure, is also a massive investment and risk. In summary, my opinion is you are extremely unlikely to see the total death of C/C++ in that industry for at least another 50 years.
Well you need to write languages like Perl and ruby and python and C# also operating systems and MS office and Facebook web sites (which is converted to C/C++ from PHP using hip-hop) C and C++ is fundamental but because we don't think about it as often, folks think we don't need it. When is the last time you were conscious about breathing and questioned it's need.
Thanks, B!
It appears that you are shadow banned. Please talk to the reddit admins to fix this issue.
Lisp will never die since it's the best language.
How is this related to programming in C? Resources for programming in general are not on topic, only resources for C specifically are allowed.
[removed]
[clang-tidy](https://clang.llvm.org/extra/clang-tidy/)
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/programming] [Why C and C++ will never die](https://www.reddit.com/r/programming/comments/8pjgjr/why_c_and_c_will_never_die/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
[relevant XKCD](https://xkcd.com/297/)
It seems easier to agree on a coding standard tbh
Don't know about everything, at least not yet. But rust is moving fast, and getting a lot of traction so it certainly could happen.
One challenge for rust is tooling. They have to catch up with 30 years of invested manhours that have been invested into the ecosystem.
&gt; There will always be demand for a language that omits all unnecessary stuff and is as simple as possible. Which does not apply to C at all.
You only need some language that compiles to native code to do that. C and C++ are the most popular ones right now, but there's also Rust these days. You can also use Pascal/Object Pascal, Ada, Fortran and a ton of other languages. It's just that historically many, many tools and libraries came from the *NIX world, which was in C, so you were quite strongly dis-incentivized to write your apps in something else than C.
was... Most unix like systems aren't unix and the true decedents aren't really popular. 
It depends on what you mean by "die". If nobody writes new C++ code today, all of a sudden, those millions and millions of existing C++ code lines won't be all rewritten to something else, especially if it makes no business sense. I'd say that if 0 new lines of C++ are written in 2018 and after, C++ will probably last for decades, at least.
So, by recursion, praise FORTRAN ?
&gt; I can only assume that mustve been some assembly compiler FORTRAN is way, way older than C. Lisp too.
You get it
That's Linus link was beautiful to read. It perfectly described all my experiences with C++. I do have one more gripe to add though: a lot of "saftey" features in C++ just feel patronizing to an experienced programmer. For example, void* doesn't implicitly cast to other pointer types which is more annoying than useful (entirely annoying, not useful at all).
&gt; write any compiler, they'll be more than surprised Meh. A higher level language is way more suited for writing a compiler. Lisp, Haskell or OCaml are more suited to compiler writing... https://www.cs.princeton.edu/~appel/modern/ml/ 
UNIX by any other name is still UNIX and the most widely used operating system everywhere but the desktop.
I believe Fortran comes close, and often outperforms C.
Even with quantum, what's to stop the use of C plus some new intrinsics like set up? C already exists, is well used, and I'm sure there are plenty of researchers willing to be the first person to write a potentially successful quantum OS. Basically, the question distills to this: why reinvent the wheel when we can spiff up an already working one? All this said, I'm like pinky toe deep in quantum computing, so I'm very likely not correct.
blahblahblah
Go was written in C up until Go 1.5 when they switched to a self-hosted compiler. The bootstrapping process from scratch is a little convoluted because you have to go back to 1.4, use C, and then go forwards again, but in practice you can just use a precompiled binary to skip ahead to the end.
If you say so, you don't really know C\+\+
Yeah, absolutely. I think it's nice that rust already has such a popular movement despite the uphill battle it clearly has.
So is windows unix since it has WSL? Is OSX unix because some of it's kernel came from a fork of a fork of an independent rewrite of unix a long time ago? Is Linux Unix because it was written to work kind of like unix? No on all three accounts. 
Computer architecture hasn't changed since the 80s \- we get tweaks here and there, but essentially it work the same. And as it happens, C overlay perfectly over assembly and hardware memory layout thus is best for the job. **Unless new kind of computers will appear that changes everything, nothing will change.** FPGA for example: because it works fundamentally different that sequential CPU using other languages than VHDL or Verilgo will never be as effective. And although you can use C for it, you still need to know the hardware to do it effectively and with performance penalty \- there is no skipping it. Ps. I personally would like to see some minor tweaks to the C language, like more type safety and architecture independent standard library (like stb) but nothing as crazy as modern C\+\+ bloatware.
As far as I know are QC programs' results entirely statistical (I wrote small quantum computing applications on IBM Q and still learning)
It's not too far from the truth if you don't count STL
Ooh, this sounds cool! How can I test the waters on this, if I can?
Just an anecdote: I had to work with a huge commercial CAD library recently. Yeah, right, written in C\+\+. I've never coded C\+\+ and actually was afraid of it because of it's bad reputation. So I had to jump in (with the help of two great courses by Kate Gregory \- C\+\+ 2017 and STL). The first week was full of WTFs, the second week productive and from the third week it was fun. I can understand that large C\+\+ projects with lots of different devs can become messy. But I can also see why experienced C\+\+ teams would stick to C\+\+.
C\+\+ is primarily RAII\-driven language. And it doesn't have anything to do with STL: [https://akrzemi1.wordpress.com/2013/07/18/cs\-best\-feature/](https://akrzemi1.wordpress.com/2013/07/18/cs-best-feature/)
C won't die until we get rid of UNIX based systems, because they symbiotic. C++ is too deep in LLVM, gcc, Google, Microsoft, Apple and ARM OSes, so it will only go away if they cared to do an heroic effort to rewrite everything in something else. Now what will happen is that both languages will eventually decrease to a niche, kind of local minimum, with everything else being written in more safer, productive languages.
What they fail to realize is that their beloved C was only chosen as implementation language, because the language author was too lazy to bother with bootstrapping. 
Only because they took Plan 9 C compiler as starting point instead of rewriting Go fully from scratch.
&gt; C++ might die soon as Rust has everything need to replace it! First Rust needs to drop LLVM.
D can't really compare, since garbage collection still seems to be required.
Windows? Android? ChromeOS? Windows has been migrating to a mix of .NET Native and C++, including the kernel. Android, only uses C for the Linux kernel, everything else is a mix of Java and C++. ChromeOS does not expose the Linux kernel to userspace. But I am with you on quantum computing, then something like Q# might be it.
In controls, you could write everything in C but it is likely that using Matlab is much quicker to prototype and develop, and ends up being just as fast as C/C\+\+. Like the guy who tries to write his own FFT algorithm... point being there are people who have moved on because they can afford more expensive tools for rather than pay for engineers who need to know how to do low level programming. 
What C framework do you use, if any for the web server?
&gt;huge assumption that nobody writes new C\+\+ programs anymore It's giant! C\+\+ is my first choice. And I know many people who is hothead about it.
Why?
Rust and it's concepts are cool but they feel too restrictive, creating too much friction when writing code. Sure, it's awesome that I can't get a dangling pointer or reference but it's not a mistake someone does every day (and if I do make it, the debugger usually points me right to the problem). My other problem is the enforced RAII as some applications (in their nature) require resource disposing in aggregates, rather than in a FIFO manner. Still, I hope something will replace C++ in the near future (honestly, looking at JAI) and I still like and write C++. Don't get me wrong, I like Rust, I just don't think it would be a complete replacement for C++.
Definitely have to agree with you. Embedded systems have the same memory of a decent computer these days. So it's possible to wrtie some expensive programs. But I still think it's good to have a working knowledge of a "medium" level language like C to truly understand what's happening with your memory and how you can optimise it to the best of your abilities. That's why it's still the introductory language taught at all universities in my country.
C’s relation to hardware is an illusion. C is no longer a simple, direct layer to assembler. It’s a highly optimized, poorly defined, crashy, insecure language. I hope Rust replaces it within the decade.
For safety reasons, never make this kind of jokes before verifying that Linus Torvalds is not in the area. 
What is the question? for(int i=1;i&lt;strlen(c);i++) { if(i%2==0) { continue; } Strings (/arrays) start at `0`, not `1`. Also, the loop can be rewritten as: for(int i=1;i&lt;strlen(c);i += 2) Notice the `i += 2`, which basically defines the step to print, since it is executed after every iteration. Also, you don't need to create a new string out of the input. Just reuse the input.
As a side note, I've heard (and agree with) arguments that it isn't always a good test. When the early largest program is a compiler, the language features and implementation tend towards features good for writing compilers - tree traversal, patters, etc. If your language isn't designed for those things, writing a compiler becomes hard. But then people complain that "they can't even write their own compiler in this language, it must suck", even if that isn't the point.
Yeap, make C\+\+ reference\-driven, without implicit'iness on every step and with RAII\-primitive instead of new\-delete operator and call it anyhow you want. I am waiting for it too.
You should check out the IBM Q experience. I was introduced during a meetup where IBM Q was at: https://quantumexperience.ng.bluemix.net/qx/community They have a beginner manual which is great to get the concepts and get started. Then you can also proceed to the advanced manual. I'm seriously learning from this because I think it's going to be a skill of the future.
Rust backend is built on top of LLVM, which is implemented in C++. So until Cretonne reaches the same maturity of LLVM, Rust compiler depends on C++.
Thanks for the reply guess i misunderstood the question myself and thanks for pointing the mistakes i have now solved my problem.
Could you please point out what level of memory management I'd be losing if I instead wrote the same code in C++, D, or Rust?
Yes, but not necessarily by starting with C. The Rust compiler was originally written in OCaml, for instance. You could pick any language to bootstrap with.
Ok serious question... why? The only C web "application" I ever saw in C was an early "social network" thing back in 2000ish, and it was shitty as shitty mc shitfuck, compared to the dating portal we did in Perl in the same year (in like a 20th of the time).
That's very true. Really the compiler/interpretor test is only relevant to general purpose languages. From what I gather it's mostly done for bragging rights. One benefit that I do see to using the language to create it's own compiler is that it makes it easier for enthusists of the language to contribute. Take CPython for example. If I was a proficient Python decent and wanted to contribute to a section of CPython written in C, I now need to be good enough in C to make a meaningful contribution. It effectively raises the barrier to entry.
Computer architectures changed enough to completely change how one writes performant software. Cache hierarchies? Cache lines? Multi-core? RAM that's so much slower than the CPU that precomputing values makes your program slower? Etc, etc. It's a myth that C represents the hardware well. It did decades ago though.
The GC isn't required and is trivially avoided.
As long as you cannot get a USB driver programmed on a small ARM with 64kB flash and 512kB RAM with these compilers, it's not going to be a solution.
Thanks!
Here's how I did it, inspired by Neui's point that you can reuse the string, just stepping through it differently each time. If you wanted it to handle spaces differently, that would get more complicated. #include &lt;stdio.h&gt; #include &lt;string.h&gt; int main() { char string[] = "abcdefghijklmnop"; int step = 2; int len = strlen(string); while (1) { int count = 0; for (int i=0; i&lt;len; i+= step) { printf("%c", string[i]); count++; } printf("\n"); if (count &lt; 3) return 0; step += 2; } }
Indeed. Yesterday it took me 10min under -j9 to compile LLVM... I HATE IT! GCC &gt;
I won't stop using C/C++.
What does that have to do with anything ? Your thinking of running compilers on small embedded devices ? The compiler output your object files (or your final binary). It has nothing to do with what your flashing that binary on...
Youll have to excuse me but I haven't used these languages except for some very minimal amounts of C++. But in C you can access the microprocessor registers and cache memory and play around with that. I don't know if these languages support them. C++ supports it because again, it's built off C
OSDev wiki is a godsend. One of the only good resources on good OS dev.
Though I'd like to point out that the speed and efficiency doesn't come from the language itself but rather if the skill of the programmer. The issue with other languages is that a skilled programmer who knows his hardware from the inside out will hit the ceiling rather early. In C you can even try to optimize cache misses and scheduling.
https://queue.acm.org/detail.cfm?id=3212479
Well, C++ exists. 'Course, C programmers tend to refuse to use C++ and vice-versa.
Thanks for the example. Much better than mine.
I'm doing C++17 on AVR. Those constraints sound lovely.
Doesn't a large part of the standard library still require GC, or has that been changed recently?
Make *this* a reference instead of a pointer, make it a const-default language, and add __restrict to the spec.
Rust is not presently particularly competitive with C++. If/When it is, it would be able to replace C++ *and* C. It's really, *really* hard to beat C++ at what it does.
Ah, thank you. Let me try to rephrase. What I meant to say is that `&amp;(head-&gt;next)` is not the same as `&amp;(address)`. So OP's code doesn't have any instance of `&amp;(address)`. This would be incorrect.
i always asumed that, as windows isn't posix compliant you couldn't do anything posix-y on windows, but of course you're right. thanks again. have another upvote!
Don't know anything about D, but it's the same for those other languages (and easier, and safer). The limiting factor is vendor compiler support, and embedded engineers in my experience are resistant to change. I worked at a place where we were transitioning from embedded applications that were about 50% C and 50% hand rolled assembly to C++ on new projects. The biggest problem was that the engineers didn't know C++, didn't want to learn it, and thought it was going to be worse than C because they didn't know how to write it and didn't trust the STL. Thats not to say they didn't have reasons for that, just that it was an uphill battle. 
I am not talking about running compilers on this example platform above. Last time I wrote a hello world in Haskell it was over 1 MB in size. And there is not even a font or font renderer involved. The reason is that high level languages often bring a huge runtime environment with them. If you have a small platform and can write a hello world that takes a few bytes or a hello world that is 1 MB in size, the choice is obvious. And now scale it up to a USB driver.
I don't know why your wording sounds like you're disagreeing with me when in fact your post reaffirms my point. I didn't say you couldn't write concurrent code in C, I said there's no support for it in libc.
But thats your programming language choice, not your compiler programming language choice. You can write a C compiler in OCaml, and produce perfectly normal C object files. Which is why I cited the compiler part of your comment, not the embedded one. C is a perfect language for programming on small embedded systems... Althought 
Why would we need a "C framework"?
Windows has never been a UNIX, never claimed to be one, and has never strived to be like one, so your first question makes no sense. OSX is UNIX because it \*\*is\*\* UNIX. Linux is not UNIX but Unix\-like. It's not like your nonsense question about Windows being UNIX.
I see we understand each other bro, we should make a C+++ manifesto or something...
Yes, I know. I just want to be realistic. I know I could write hello world with a screen driver, font and font renderer with a couple of kB flash ROM, less than 1kB RAM. But already comfortable platforms like the one mentioned above is a problem on high level languages. One notable exception is Rust. I've seen people using Rust on small ARM platforms. The difference between C and C++ on embedded is just to know what you shouldn't do while programming in C++. Otherwise, it's all similar.
What tooling specifically? To my mind most of the important C and C++ tooling has only been developed in the last 5-10 years as an extension of the LLVM project. There have been linters and static analyzers of some level of quality for ages, but all of the sanitizer suites are pretty new compared to the language itself. If you mean MISRA tooling specifically, then I guess I don't know.
I am disagreeing with you. Don't confuse the standard library with particular implementations of it that fail to provide the full standard. The C standard library has support for concurrency. You said it doesn't. That's false.
C and C++ will be around as long as the hardware is a Von Neumann architecture.
What does this have to do with writing a compiler? You don't write program in a compiler.
No it doesn't. Every compiler (that is based on any modern compiler infrastructure) compiles to its intermediate languages. Rust is not a C++ code generator. Rust pays attention how to layout data efficiently and does not use much safety mechanism. Most of it is gone in the resulting binary code making it slim.
I was thinking something like this [https://kore.io/](https://kore.io/), mainly because not everyone is capable in implementing something like that. So I guess I'm wondering do you develop everything from scratch or use a framework like the one listed above? 
trust me..it's going to be a long time until any of it is dead. two reason: 1. yes its a great language for all the reasons people list here. 2. there is A LOT of code in the industry that no one is trying to re write. For example I'm in aerospace right now, and we literally still support fortran because that code has been worked on for literally decades and trust me theres not a volunteer line to re write that..lol.
Who said Rust generates C++???? LLVM is written in C++, so if rustc links to llvm.so, of course it depends on C++.
I think we have a misunderstanding. Maybe it's hard to understand, because I am not a native speaker. Shorter: Show me Haskell code for a USB driver or one dealing with interrupts and ports and we'll compare your hardware platform requirements and the ones of C compiled code.
Even if LLVM was written Java, I wouldn't care, if it generated nice binary code. The result is important not the tools to get there. Do you know that Git uses Perl?
We've been doing this for 14 years so we don't use other people's code.
Nope, C == system software C++ == Application level Rust competes with C++. 
I don't care what language is the compiler written in, only what language it is written FOR. I was never talking about programming languages for compilers. I am talking about programming languages for embedded programming.
Do you try to implement everything or do you just use a subset required for your application?
The top level comment was saying C is best for embedded systems and writing compilers. Someone replied saying that languages like OCaml are good for writing compilers (saying nothing about embedded systems). You replied to that comment connecting those compilers couldn't work for small embedded systems. Nobody was saying OCaml is a good choice for embedded software; they were saying it is a good choice for writing compilers (even compilers that compile embedded software).
You were replying to a comment about what language the compiler is written in, and that's what everyone else is taking about.
I'm a college senior majoring in computer science (technically software engineering), so I can relate. Is this an introductory C programming course? Also, seeing the past material would help me guide you better.
Subset of what? Depending on time, or what we've written in the past, we'll do it ourselves if we can. If we don't have time, and something else is good enough, we'll use something else and, maybe if we have time, write our own version if we feel we need to (cause ours would be faster, cheaper, easier, more secure, or whatever). Chances are, we've been there, done that already.
&gt; I hope Rust replaces it within the decade. It ain't gonna happen.
Analyers/provers, compliance checkers, compiler plugins, generators, IDEs, profilers, and not to mention vendor compilers for the more exotic architectures. We have a bunch of perl scripts here that nobody touched in 10 years because they still work.
This is one of the worst ways to compare languages.
hahahaha
Yes you can do it both in D and in Rust, the difference here is that for C and C++ you have existing headers provided by the hardware vendor, but for other languages you need to write your own. And not so many people choose to spend their time on this.
&gt; changed enough to completely change how one writes performant software Not really. Since first introduction of cache (1960s?), all performance optimization is data vectorization and batching \- no matter if its 2\-4\-8 way, 4\-8 byte lines or many hierarchies. **What I meant in my post is that** ***essentially*** **it didn't change.** Same with code, even with out\-of\-order execution, bigger pipelines, per instruction caches and other tricks \- code optimization rules didn't change at all, and is the same like 20\+ years ago. It seems that we get all this new technology, but we don't. More bits and bytes, more caches, more pipelines, wider data buses \- **no real revolution** that would require new language. Same with multi core devices \- its simply 1 core programming times 4 with choke point that is single memory again.
Really? How would I spawn a new thread using only libc?
I can fit the entire syntax, semantics, and standard library of C in my head all at once. That is simply not possible with C\+\+.
I think I'm close but it took years for C++.
[`thrd_create`](http://port70.net/~nsz/c/c11/n1570.html#7.26.5)
\&gt; C still offers the lowest\-level abstraction from the machine code. Honestly, though, it doesn't. You're not writing code for a PDP\-11. A C compiler *can* write code that is very straightforwardly translated to machine code, but only if you're on a von Neumann architecture with a flat memory model, a pointer representation that doesn't change depending on the object being pointed to, and hardware call stack. As soon as you add memory segmentation, Harvard architecture, any sort of vectorization, even link registers, etc, the correspondence ceases to be anything approaching one\-to\-one. ISO C and its various extensions help somewhat by forbidding things that would be hard to do on such systems (e.g. the way they specifie function pointers or the addition of named memory spaces), but it's not completely the same.
Linux kernel development
While it does not matter to ME what language a compiler is written in, it matters to the devs of the compiler, because if you improve the compiler code you improve indirectly the compiler and can compile it with a new version in the next round. And testing process is better because you are closer to test the compiler with its compiler run.
&gt; You're not writing code for a PDP-11. How dare you assume my processor!
Ah! Well, fair enough, you've got me there, wasn't familiar with that part of the spec. OTOH, OP's question was about usable APIs, so an unimplemented spec won't be of much use. Do you know what compilers implement these APIs in their libraries?
A more elegant architecture, from a more civilized age.
&gt; poorly defined, crashy I think C is pretty well defined. At least it's defined well enough for many people to be programming reliably in it. When my code crashes it's not because of the language.
C is the language wich interact with machines no other programming language is flexible especially pointers 
&gt; Fortran comes close, and often outperforms C Are you replying to, "...when it comes to the machine level implementation"? If so, I have been doing embedded development since 1978, and I can't remember ever seeing and embedded FORTRAN code. If it's so wonderful for this application I think there might be more interest.
I loved programming the 68000!
These problems were solved a year ago: https://dlang.org/blog/2017/08/23/d-as-a-better-c/
do you known all undefined behaviour by heart...?
How are you directly accessing the hardware caches? Generally they are below the ISA level, and managed at the microarchitecture level.
Nope, it's .c++
You don't need to, they're all ambiguous edge cases. It's straight forward to write straight forward code and completely avoid undefined behavior. I'd love to see good counter example
The base is C, so C is the most important language in those OSs. Java parts can be translated in C++ when they are performance critical, and C++ remains the best choice for real-time multimedia processing, which is basically everything those OSs are designed for. C is and will always be the best language to program any machine using bits at low level. Even if something scientifically better comes out, all of the market is already based on C and it will be hard to translate it all. My example about quantum computing comes from the fact that quantum computers don't use bits, they use qubits, which is an entirely different concept C is not designed to handle. As soon as we get an OS for a commercially viable quantum machine, the same language used to program it will become the standard for low-level quantum programming, the same way C replaced other structured programming languages as soon as it was released alongside Unix.
I was talking about its success at the time it was released. Both Unix and C are popular even now because C is used to write the kernel of virtually any OS and the OS design of Unix has inspired MacOS (OSX at the time) and Linux.
Subset of things like TCP/UDP, HTTP/1.1, HTTP/2.2, cross platform (Android/iOS/Embedded/Windows/Linux), websockets, \-\-\- It's just really foreign to me because I don't deal with it, and as a single person I wouldn't attempt it, but I can see certain companies having to do it.
Okay, but then you wouldn't know all the semantics as claimed in the parent comment.
&gt; C still offers the lowest-level abstraction from the machine code. Rust attempts to do better but just creates more abstraction. *Optional* abstraction is a good thing. That's why C++ is popular.
I think grandparent is right thought that a huge chunk of those are based on recent LLVM work, which is why Rust (which is also LLVM backed) has been getting them at similar rates.
I think Rust badly needs some kind of variadic generics support before it can claim to provide everything C++ programmers need. It's saddening to look at pages like [this one](https://doc.rust-lang.org/std/primitive.fn.html) where there a ton of boilerplate variations on each fundamental implementation, so much so that it's hard to tell what all is actually being implemented.
&gt; the same way C replaced other structured programming languages as soon as it was released alongside Unix. Being offered for barely $100 with source code helped a bit.
You can do a *whole* lot better than `rand()` and `srand()`. Not only is it impossible to seed `srand()` appropriately for this sort of application (especially not from `time()`), but it's not a cryptographically secure PRNG. To fill the initial buffer, you should read from `/dev/urandom`, `getrandom(2)` (Linux), `arc4random(2)` (BSD), or `CryptGenRandom()` (Windows). 
&gt; the pace at which embedded technology is increasing Yes, but you may still want very low power devices.
Rob, you are very much alone. Please take another sabbatical. 
&gt; I think C is pretty well defined Well, it has a bunch of undefined behaviors, which are, by definition, not that well defined. &gt; it's defined well enough for many people to be programming reliably in it. Come on, I love C too, but you can't deny it's famous for the huge number of vulnerabilities in tons of major C software.
That's true in the legalistic job interview sense, but in terms of knowing everything you'll ever need to code efficiently in C, your can keep that all in your head. (I haven't downvoted you.)
Aren't most C compilers written in C++ nowadays?
Agreed, 10 years ago, good static analyzers and IDEs for C/C++ were hard to find.
Professor told me that it is expensive for companies to switch to different languages. 
What topics are covered? Specifically what parts do you need help on? Oreilly books are always good
[This](https://github.com/snake8/Rsa/blob/master/code/rsa_gmp.cpp#L85) looks to be using PKCS #1v1.5 padding for RSA, which is [known to be insecure](http://archiv.infsec.ethz.ch/education/fs08/secsem/bleichenbacher98.pdf) [It also uses 1024 keys](https://github.com/snake8/Rsa/blob/master/code/rsa_gmp.h#L9), which have been discouraged from use since 2014 by NIST. The random key generation has also been mentioned. Crypto is _hard_, and there are many ways to mess it up. If you want to learn more, [Cryptopals](https://cryptopals.com/) is a great set of exercises to teach you about potential failures in crypto systems, including the very Bleichenbacher attack that your code is vulnerable to. If you ever wonder why people say you shouldn't roll your own crypto, this is why.
I know you weren't the person making the original claim, but I wonder why not say what you mean? What you're describing doesn't match what was originally said by /u/lorddimwit and I was responding to the original claim rather than this interpretation of it.
What was programming like in 1978? How has it evolved?
What does Git use Perl for?
Counterexample: Linux kernel bug about checking pointer for NULLness *after* it was dereferenced. More examples are on PVS-Studio blog.
&gt;What you're describing doesn't match what was originally said by &gt; &gt;u/lorddimwit It does, because I wasn't being pedantic but rather making a general statement more in line with u/georgeo's interpretation. :p
For orchestrating more complex processes with smaller Git primitives. The point is that programming languages are not responsible for quality of software. The devs are.
I'll go out on a limb and say that based on votes, there's a consensus that OP was clear.
Thanks, checking a pointer value after a possible dereference would *never* be allowed in my shop for obvious reasons like this. You definitely need explicit logic to keep track of it's state. It's waaay easier than tracking intermittent nightmare bugs.
Well said. There’s a great paper that goes deeper into how the C abstract machine is close to the PDP-11, which is very different from modern architectures. https://queue.acm.org/detail.cfm?id=3212479 I’d argue that, if you’re not a kernel developer, this is kind of a moot point. You’re still going to base your work on your OS’s abstractions. As long as kernels are written in C, the entry point to the operating system will still be the C API. Understanding C is crucial in this regard. Memory management, device I/O, networking, processes and threads... It’s necessary to understand C to be proficient in these topics. I’m curious to see if future OSes (and maybe unikernels) will break away from this paradigm.
For me it was a [formal definition](https://en.wikipedia.org/wiki/Big_O_notation#Formal_definition). When I started seriously programming I already had a heavy background in proof based mathematics. BigO Analysis was something that I had heard of, and I understood the gist of it based on the rate of growth of the function used; like, O(n^2) is more complex than O(log(n)) because the growth rate of the former is much higher than that of the latter. In general I still use that thought process when comparing algorithms, but performing actual analysis on my algorithms required me to understand the nature of BigO Analysis, so I went back to my roots and just learned the definition. Fortunately for me, computer science looks an awful lot like applied mathematics, but not everyone has that background. I'm posting this, not to discourage people from using other resources posted here, by all means use whatever you can to learn, but because I don't think anyone should discount the process of learning via strict definitions and analytical thinking. I've definitely found it to help me in the long run, even outside the realm of computer science, and I hope that in applying themselves others can too. Alternatively, you can tell me how far up my own ass I am, and I wouldn't say you're wrong.
Isn't C's `register` ignored by most modern compilers?
What would be the point of that? What you need to know is specified behavior, and use it to write programs. The concept of leaving some behavior undefined is a great idea, it keeps the language simple and limits what programmers and implementors have to know and pay attention to. Without it, the standard would probably be twice as big, full of complicated rules for weird edge cases that wouldn't be useful at all.
*this question will be on the interview 
Hello, fellow state machine enthusiast. Where do you find these test cases?
so what language *does* offer the lowest-level abstraction if it's not C? and no, assembly does not count 
It doesn't outperform C since C99's "restrict" keyword and strict aliasing were introduced.
One I see nail a lot of people: strncpy does not actually guaranteed a null on the end of the string. You won't go past the end of the buffer... but if you hit the end and there's no null yet, it just stops, it doesn't go back and set the last one to 0.
&gt;Honestly, I have tried this before, and you really shouldn't. Not unless you have a lot of spare time. That's like telling someone to not get into pottery because you don't think it's worth the time. OS Dev is not just a field of programming, it's also a hobby. I do it because of the satisfaction I get when something works, and the motivation I get to solve the challenges it poses. I have a higher purpose for my kernel, to introduce it into my university's Operating System curriculum. But even if I'm the only one to ever write software for my operating system, it will have been worth it.
Strange you should mention converting MATLAB code to C since I had a task to do this just this week and achieved a couple of orders of magnitude speed-up. C gives you the control to do optimizations which just aren't possible in MATLAB.
C is still closer to the metal than most languages, but given the points above, it's about the same distance away from machine language as any language of similar vintage: Modula, Pascal, etc. Their assumptions are often stronger, but end up being about the same as C's at the end. But for languages still in use today that has the closest correspondence to what actually ends up getting run on the hardware but isn't just assembly or something, probably FORTH. 
True, but that's *defined* behavior. ;)
OS, not compiler, is the usual way of thinking about what a standard library is bound to. FreeBSD supports C11 threads, and Linux distributions that use musl. There might be others, but not many.
What type of function did you rewrite and how did you determine it wasn't your MATLAB code that needed rewritten? I've seen some very poorly written MATLAB code (poor matrix management, mostly). Also, you can generate C code from MATLAB using codegen. (I have not done any of this beyond targeting a simple embedded device during a graduate class, so I'm wondering if you had and if you would share your experiences)
This wasn't just a kernel bug. The GCC/g++ developers enabled an optimization (which is now toggle-able) which presumed that *any* dereference, even one that wasn't a 'true' dereference (like a member function call) of a `nullptr` was invalid. Thus, a lot of software that was written in C++ stopped working, where they put their null-checks at the start of the member functions instead of before calling the member function. The optimizer saw the `if (this == nullptr) return;`presumed that since it was technically undefined behavior, it must be impossible, and eliminated it. Then the software started crashing. This happened across quite a few programs, and also happened in C software that acted similarly.
The optimization in GCC, after it was added, broke software which called member functions upon potentially-`nullptr` objects, and then checked for `nullptr` within the member function. Since the optimizer presumed that calling a member function on a `nullptr` was UB (which it is), `this` therefore could *never* legally be `nullptr`, and thus it eliminated the `if (this == nullptr) return;`. Thus breaking a lot of software.
Thankfully, I have documentation available at my fingertips, and thus do not need to keep the entire syntax, semantics, and standard library of a language in my head all at once, thus freeing up precious brainspace for the task at hand.
from what I read about Forth it doesn't look like it's any closer to the hardware than C, though I might have missed something. And it doesn't seem like it's meant to be compiled to machine code and run without a runtime, so that makes it rather more removed from the actual hardware in my eyes.
C++ is capable of performing literally *any* task that C can. The languages overlap completely at the bottom level. C++'s range just stretches higher. I do kernel development and embedded work in C++. Templates and `constexpr` come in very handy there.
Unexpected at first glance, but yes, defined :)
True. I guess my objection was that I've often written something in C++ only to find out later that there was a "correct" way to do it that neither I nor anyone on my team had remembered at the time. :)
Eh. C++ has additional features that are actually very useful in embedded, that C isn't capable of. Compile-time type and value validation (with a variety of mechanisms), strong typing, templates and `constexpr` (which I've used heavily to regenerate algorithms on AVR to minimize function-size based upon constant data tables), and such. You can't really do that in C. C is *simple*, and that's a strength. But C++ has a lot more capabilities, many zero-cost or even negative-cost, that make it useful.
I am referring to the top comment that mentioned embedded programming. Self-hosting compilers (also used cross-platform) have their advantages, as I mentioned already.
`C+=2`.
Is your team regularly using C++? I don't find I run into that problem very often, or I usually realize that there is likely a better way to do it and *then* look it up. It is very rarely after-the-fact. However, I *heavily* use templates and `constexpr` (exceedingly useful on embedded work and in game development, far more than most people realize), so I'm usually already using the more complex features that people don't think about.
Not OP, but no I don't. That said, it's pretty reasonable to for someone to have memorized the list. Afiak there are 193 cases of undefined behavior in C. Med school / law school students memorize much longer lists than that. Practically though, it's much easier to know the few cases that happen to be common landmines, and stick to language features you know. IMO this is much easier to do in C than C++. (I say this as a long time professional C++ programmer)
I like C++11 (and up) and I know what you mean, but one good strategy to lessen nondeterminism is to eliminate dynamic memory management. C++ is best used with a heap. C can be easily entirely statically managed and avoids this source of unpredictability.
I think people have the inability to read past the first sentence on this post. Really, look slight to the right and read, maybe you just wanted to lecture someone. I don't know but it is getting slightly annoying. In case you cannot, it says &gt; Not unless you have a lot of spare time. Unless you are saying that writing an OS is something that doesn't take a lot of time to do? In which case you are delusional.
My hopes are that WASM will bring back C on the front end. WASM games in C are much smaller (and therefore faster to download) than the other current source languages.
It's not uncommon in aerospace - a lot of satellites are programmed in it, for example.
No, is almost impossible to write code without undefined behaviour. Show me one your program and I'll be glad to find some for you :)
&gt; C++ is capable of performing literally any task that C can. hahahahahahahahahahahahahahah
Imo if you know what you're doing, and use the right tools. C is the most productive language.
Name something C can do that C++ cannot.
It would be very unusual to attempt to use a heap on embedded, simply because there isn't really memory available to reserve for a heap. I haven't run into any issues with static- or stack-only allocations with C++. The language in many regards lends itself well to it, if the standard library itself doesn't.
To be fair, this was all years ago, back when C++ was...less mature? The addition of move constructors in C++11, for example, was something that we would've welcomed a long time ago. ;) In the intervening years my work split into either very low-level (talking to hardware directly or with real-time constraints or code size was of critical importance, etc), in which case I use C, or time and resources don't matter, in which case I use Python or (more recently) Go.
If you use the language regularly, those problems become less - as I said, it's pretty rare for me to come up or find a solution *after* the fact. If I have to look something up, it's because I realized there was a solution and I just wasn't familiar with it or was having syntactical difficulties. Move constructors, lambdas, constexpr, expanded templates (including variadic templates) have really changed the language.
Interesting (stack-only). I'd like to know which parts of the standard and STL do you use. Do you use OOP at all?
It's true. This is why banks would rather pay people 500k a year to maintain COBOL systems rather than have their systems rewritten in a modern language.
Whoa, seriously, 500k?!
&gt; Move constructors, lambdas, constexpr, expanded templates (including variadic templates) have really changed the language. That's a little bit my point, though. That's a lot of stuff to remember. I'm a simple man. There's no "this would be better with a move constructor" if there are no move constructors in the language. ;) I think that's also part of it too with C: I regularly work with C code that's pushing 25 - 30 years old, and it still compiles more-or-less fine. There have been comparatively fewer things added to the language over the years, and those that have been fairly small and well-defined in scope (e.g. complex numbers). C++ has added things over the years but they often (though not always) *replace* things that were there. You read someone else's C++ code and they might use a feature that you've never used or even heard of. That doesn't happen very often with C.
"will never die" and "won't die for the foreseeable future" are different things. I'll bet money that nobody will be writing what we call C or C++ in 100 years, maybe even 50. C itself started becoming widespread _just_ ~40 years ago with K&amp;R, and it changed since then quite a bit. Near/far pointers? Variable declarations at the beginning? If you count from C99 then that's ~20 years and the proportion of C programmers is definitely not on the rise. And C11 is probably even more different from ANSI C than first versions of C++. Compare C++98 with C++17 code. They might formally be the same language, but so is English now and 500 years ago, but [you probably wouldn't understand much of it](https://youtu.be/tCckcTHWqKw). In a hundred years, everyone in the standards committee will die and be replaced by someone younger. If C++ will be around in 2100 and not just be a digital fossil, it will probably not look like anything we're writing today. Maybe curly braces will stay, but I'm not certain. At the same time, the proportion of C++ programmers is also on the decline, potential users (myself included) are leaking into other systems languages. Sure, it can't do everything now, but as tools/languages come and develop to cover the specific problem domains, those users will probably start leaking too. Stubborn to change programmers will retire and die and the new generations will not have the force of habit, formed by years of experience, to persevere. Zig, Rust, Jai, D, Go, Swift, Nim, some reincarnation of JS, who knows? C/C++ demise is inevitable and isn't far off. And so is our own.
Yes. There is a not-so-major, but still large enough to be in three or four states, bank HQ in the city I live in. They have an entry level COBOL position that starts at 320k with stock options. They also have a 620k position in COBOL. But here's the thing, they pay that much because maintaining 40 year old code is a soul sucking experience. Imagine having to maintain a codebase that is 40 years old...
I did not know that, are you in that field?
FORTH is often compiled and plopped into embedded environments. The "runtime" is often as small as two or three machine instructions and on subroutine\-threaded forths, even that goes away. Each word is just a list of jump or call instructions (or, in threaded forths, of addresses); it doesn't get much more "what you write is what the machine does" than that. There is the "FORTH environment" with defining words and editors and the like, but that's certainly not required to be there. Including that would be like saying IDLE is required to run Python because it's included in the standard distribution. ;)
I've got no dog here, just interested cause I like both languages. Why?
No. But I go to church with a guy who is a senior COBOL programmer at this bank. The dude is 30, looks 50, and his marriage is in a bad way. The enter system needs to be written, but the execs won't pay for it because it would cost them way too much money and would mean very mature systems would have to be killed and restarted, losing money while they gain stability.
OOP is a really, *really* vague concept, so you'll have to clarify what you are referring to. C++ itself supports more than one OOP paradigm. As per the standard library itself, it can vary depending on the platform. AVR, for instance, by default builds... almost none of the stdlib, including ones it really should (like &lt;algorithm&gt; and &lt;numeric_limits&gt;). I will happily use things like that - generic algorithms which can be specialized for specific cases, templates which allow me to derive traits/conversions/value ranges of types... if it's something I will have to do more than once, I will generally try to genericize it to limit what I have to write, and to maximize the compiler's ability to optimize it. I generally avoid the container libraries simply because they were *not* designed with static memory allocation in mind (though they *will* work with it using custom allocators). They were also not designed with embedded in mind. You can get better results writing your own similar containers taking those principles into account. Often, the container libraries aren't even part of embedded toolchains anyways, as they lack a concept of a universal allocator - they don't have `malloc` in a meaningful sense. Algorithms, basic functions, and informational templates tend to work fine. Sometimes I write my own (like a `min` that can take multiple arguments instead of just two, or a more detailed/useful version of `type_traits`/`numeric_limits`). This couples well with things like my `uintsz_t`, which resolves to the smallest unsigned integer *type* that can fit the value passed to it. I use it heavily on AVR as it's an 8-bit chip, so the larger the type you're working with, the more expensive computation becomes.
Yes i agree, but linux kernel guys (i guess much smarter and experienced than you) did it an got burned by it.
I've seen examples of rust creeping into embedded. They obviously have a long way to go, but so far it looks to be a viable contender.
&gt; That's a little bit my point, though. That's a lot of stuff to remember. I'm a simple man. There's no "this would be better with a move constructor" if there are no move constructors in the language. Sure, but they serve a purpose. Same with things like RVO (which is completely foreign, generally, to C programmers due to their preferring to pass large return types by pointer-reference as an argument). It reflects a different programming style, as well. C++ is *usually* backwards compatible. It's rare for an old C++ codebase that was standards-compliant at the time to not compile today, unless it uses one of the *very* few features which was deprecated or removed. The additional features in C++ honestly should have been there in the beginning, or much earlier. C++ stagnated for a *long* time. That's one of the reasons all the changes in this decade are so shocking is they all happened *at once*, though many had been proposed in the 00's or even in the 90's. Still, it's incredibly impressive that C++ is able to do *so much*, especially with negative or zero overhead in most cases.
Thanks for the info.
You may have noticed there's no one singular C compiler. You have to have the right compiler for your particular target. The x86 compiler knows which instructions can write to what registers. Same goes for ARM, AVR, MIPS.
Oh man, I never knew this dark side was intense. Sorry to bug you with lack of research, but how much money would be the cost to switch to a more modern language?
Everyone makes mistakes. And all the "linux kernel guys" aren't strictly better than everyone else anyway.
&gt;Ok serious question... why? Not OP, but if I had to guess, you can probably service *WAY* more connections, and faster, than an interpretated language. &gt; &gt;The only C web "application" I ever saw in C was an early "social network" thing back in 2000ish, and it was shitty as shitty mc shitfuck, compared to the dating portal we did in Perl in the same year (in like a 20th of the time). 
It was some reasonably complex matrix code which takes xray spectroscopy and converts it into an assay of component chemical elements. It was written in a fairly intuitive fashion by an applied mathematician. My task was to convert it into fast C++ code. The biggest gain was from multi threading but I also saw big gains from more efficient data representation, memory mapping of data files, optimised representation of FFT inputs permitting faster use of FFTW, a fast linear regression implementation, optimised spline implementation and general algorithmic restructuring. So some of these improvements could probably have been achieved in MATLAB but for instance there's a big difference between a naive call to MATLAB's linear regression and creating a custom C++ implementation which is accurate enough for your application while being as fast as possible.
C++ cant be called C....hahahaaha
Yes but you must still know *of* the thing you want to use. Unless you wanna go through the entire documentation every single time. 
&gt; it keeps the language simple No, it keeps the language and spec *small*. Not simple. It becomes extremely complex to actually then read or write a program, as you do not have a good understanding of runtime behavior in the presence of UB.
In this case, that's C++ not C.
I keep hearing that since around 1990, yet even Dennis was aware that it wasn't the case. &gt; Although the first edition of K&amp;R described most of the rules that brought C's type structure to its present form, many programs written in the older, more relaxed style persisted, and so did compilers that tolerated it. To encourage people to pay more attention to the official language rules, to detect legal but suspicious constructions, and to help find interface mismatches undetectable with simple mechanisms for separate compilation, Steve Johnson adapted his pcc compiler to produce lint [Johnson 79b], which scanned a set of files and remarked on dubious constructions. Dennis Ritchie -- https://www.bell-labs.com/usr/dmr/www/chist.html [Security Vulnerabilities (Memory Corruption)](https://www.cvedetails.com/vulnerability-list/opmemc-1/memory-corruption.html)
Well I've been a C programmer since the 1970's, I'm pretty sure I've made every mistake you can make, but do learn from them. These days my bugs are more in the design than the code.
Similar can happen in C, though under slightly different circumstances.
Generally, I'm aware of their existence. Just not necessarily how to use them or their syntax.
The expression `C++`, unless overloaded, returns `C`.
I know it's in the billions. Most banks in the US (maybe even all) are still relying on old COBOL programmers or enticing people to enter that field with vast sums of money. According to this (article)[https://www.reuters.com/article/us-usa-banks-cobol/banks-scramble-to-fix-old-systems-as-it-cowboys-ride-into-sunset-idUSKBN17C0D8] it cost a bank in Australia $1b Aussie and 5 years to get a new system. Probz just write that sucker in C and never have to worry about it dying, amiright?
&gt; Science advances one funeral at a time. Max Planck 
Assuming they are used correctly without introducing UB.
C++ is wrong, it should have been called ++C, that is why it still not added any useful feature to C! haha
Or, we could [kill the idea of a big OS](https://www.youtube.com/watch?v=kZRE7HIO3vk) altogether. More standardised hardware would do *wonders* to the OS and language ecosystems. 
What do you use variadic templates for? I know only of printf-like implementations, and Rusts can use macros for those.
No one knowingly invokes undefined behavior or plans for it. It’s a programmer error that implementations are not burdened with handling in any particular way - because in fact there is no good way to handle it.
So many things are written in C it’s ridiculous to think about 
&gt; safety critical software. (MISRA) C and C++ won't be replaced any time soon in this field Actually, they may be what's get replaced the fastest. C and C++ are hopelessly unsafe (especially without stuff like -fwrap), and MISRA doesn't really help. A low level language tailored towards safety (with MISRA-like features embedded, built in stack depth analysis…) could be adopted in no time, given the proper proof of concept. Ada is actually a good demonstration that C/C++ isn't the only game in town.
When machine code dies then C will. Until then, C is the best.
Some types of vectorisation are handled ok with intrinsics, although thats not part of the C language.. it's a retrofit that can still be used in a portable manner (provide functions that map to hardware, or are implemented by the closest approximation available on other machines)
[DasBetterC](https://dlang.org/spec/betterc.html) does not have garbage collection. It only requires linking to the C standard library.
For the function documentation check out Doxygen. You can have it throw warnings on Parsi g your code if there isn’t documentation for a function, macro, structure, and other things. 
The [DMD compiler](https://github.com/dlang/dmd) is mostly in D, and so is the [Digital Mars C and C++ compiler](https://github.com/DigitalMars/Compiler). I'm working on converting the rest of them to D.
[removed]
I was never talking about compilers, I'm not sure what that has to do with a language having (or not having) direct access to hardware\-managed caches. All languages are brought down to machine code at some point in the execution path, that does not at all imply that a language allows direct manipulation of registers or caches. C has no native way of pushing the current registers onto the stack, without using inline assembly. That is the beauty of C, the details of the ISA can be hidden from the programmer and yet you can still write very high performance code. Not to mention, even raw machine code has no real access to caches on current architectures that I know of.
C++ is off topic in this subreddit. Please post C++ projects elsewhere, e.g. to /r/cplusplus.
The best way to practice is to write code. Think of a program you would like to have and start to write it. Try to work out algorithms for all the things the program is supposed to do yourself, only asking others if you got stuck for quite a while. Afterwards, post your project here and ask for code review. Apply the advice you received and improve your code. Rinse and repeat with a new project. There is really no way to get good at C that doesn't involve writing at least a few thousand lines of code.
\+1, King \&gt; 
This is 100% the whole point. It has such a huge impact on business decisions. The feature-base of C++ is gigantic. The use-applications are the same.
*whooosh* the point goes over your head as you take the analogy too literally
Yes, similar things can happen in C, but I've never seen a case where undefined behavior was hard to avoid.
okay cool I'll do that. Thanks 
Build a webscraper from scratch, with the socket connection. Don't commit suicide during the process.
If someone wants to learn something, don't point them to a library that does it perfectly. This defeats the purpose. Just use normal system libraries and read the docs. Find a book that talks about this too. Google should be a last resort.
You implement your own TSL?
Let me know if you need help! I'm always there for guidance. Try to start with a project where you have a rough idea how to do every step, so you don't get lost before having learned development methods to cope with large projects.
The reason is that fork uses cooy-on-write semantics.
A forked process receives a *copy* of the virtual address space of its parents. This is typically implemented using a *copy-on-write* mechanism to avoid actually copying memory for as long as possible. There is a variant of `fork()` that actually shares the address space called `vfork()`, but it comes with a lot of problems due to the shared storage.
C represents the hardware _better than most languages_
&gt; entry level COBOL position that starts at 320k with stock options holy shit I suddenly want to learn COBOL now seriously hook up the job posting
Xray Spectroscopy, that's neat! I just took a class regarding FFT, hence my offhand FFT comment, and I recently saw in an /r/DSP thread that MATLAB actually uses FFTW for its implementations, if I'm not mistaken; so I am wondering (I haven't looked at it) but what kind of FFT inputs would permit faster use in FFTW?
You may have some confusion due to virtual memory, as well. The two processes are storing the variables at the same *virtual* memory address. However, each process has its own mapping between virtual addresses and physical addresses. So, the two variables are stored (in RAM) in different *physical* memory addresses. It is not possible for a process to see its own memory mappings or see any physical addresses. It can only see virtual addresses, and the kernel is responsible for managing the mappings on the MMU.
That doesn't bother you?
A user has reported this comment as containing personal information. Please do not post other users' personal information.
Read some open source project and contribute with them? Your submits will be available by maintainers
MATLAB's implementation of FFTW always uses complex valued inputs, doesn't reuse "plans" and always normalises the result. In my case the inputs were real valued so I was able to use a version of the call which is almost twice as fast. Also reusing plans and avoiding having to normalise every time makes things a bit faster. Having said that the majority of this program's time is spent in linear regression and spline operations so FFT was only a relatively small part of the total.
new[] has to track the amount of objects allocated so that free[] can work, that's an overhead that isn't required in C. In C I can change allocator by wrapping malloc/free. And then on the 'how people actually write code' front, classes are often massively duplicated in memory and in program code to achieve various C++ mechanisms.
My suggestions (these can be done in parallel) 1. Read &amp; do the examples of a couple of sites / books. Doing the examples yourself is very important. * [https://www.enlightenment.org/docs/c/start](https://www.enlightenment.org/docs/c/start) is a good start. * [Classic K&amp;R C Programming Language](https://www.amazon.com/Programming-Language-2nd-Brian-Kernighan/dp/0131103628) * [Expert C Programming](https://www.amazon.com/Expert-Programming-Peter-van-Linden/dp/0131774298) 2. Do some coding sites: HackerRank is one, there are many like it. 3. Do projects [This Github repo has links to project based tutorials](https://github.com/rby90/Project-Based-Tutorials-in-C)
&gt;\&gt; C still offers the lowest\-level abstraction from the machine code. &gt;As soon as you add memory segmentation, Harvard architecture, word\-addressable memory, any sort of vectorization, even link registers, etc, the correspondence ceases to be anything approaching one\-to\-one. The claim isn't one-to-one, the claim is it's closer than other languages, which it is.
Wait, are you saying yes that Vulkan doesn't have it's own GPGPU API, or that it's absorbing OpenCL?
Both, though the latter will probably take quite some time.
The last vestige of Fortran's relevance has been a tiny edge in performance because of ambiguous pointer aliasing in C, which if I understand the state of compiler technology correctly is now effectively a thing of the past. (I've written Fortran, so I'm not disparaging something because it's old.) Cobol... [has a mildly interesting PIC mask](https://en.wikipedia.org/wiki/COBOL#PICTURE_clause) but otherwise brings nothing to the table not done better by a dozen other languages, as far as I know. C and Lisp, on the other hand, remain unparalleled at what they do. Writing new C and Common Lisp (and Clojure, I'm sure) today is a good idea; writing new Fortran or Cobol today could only be justified in limited cases involving specific compatibility or codebases. 
What about WebAssambly impact on the ecosystem? Seeing as how the initial plan is C/C++ support?
I believe windows is mostly c++
I recommend K&amp;R and practice (throwaway code is important) on a site like codingame.com. I also recommend "Learn C The Hard Way" by Zed A. Shaw for a more modern take on C programming. 
&gt;Linux is not UNIX but Unix\-like. I think that "GNU is not UNIX" grew primarily out of legal concers. Both GNU and the Linux kernel have stived to be as SystemV compliat as possible. For all intents and purposes, Linux is a superset of traditional *NIX. 
\&gt; Why is the value only changed in one of the threads, if the pointers are the same? Others have pointed out the details, but just to be very explicit: Forked processes are \*not\* threads. They are independent processes, with independent memory. They will share virtual memory addresses up to the point of the fork (they have to, they are exact copies at that point and thus have the same pointer values, etc.), but they cannot directly modify each others memory.
&gt;What about WebAssambly impact on the ecosystem? You meant the thing that runs in a browser written in C/C++, which runs in an OS that's written in C/C++/ObjectiveC, running on a kernel likely written entirely in C? That's like trying to write a BIOS in perl.
&gt; When is the last time you were conscious about breathing and questioned it's need. I meditate every day, so daily?
There is a variant of `fork()` that actually shares the address space called `vfork()` Ooh, that sounds like fun... Maybe not too exciting, though, I guess it's similar to normal threads? (Except, wait, mustn't they have separate stacks? Now I'm intruiged...)
I think this is very true in C too, although less so
Don't know about most, but the most popular are. GCC, Clang, and VC++ all are.
It's just a fast Java with explicit memory control really
You question it's need everyday?
Java is too slow. C++ fills a gap in between java and c.
Anything involving arguments given to another function or constructor (whether forwarded immediately or stored) and representing a function signature are two big ones. Getting at all the types in a tuple is also useful. For example, this would allow you to easily make a function that applies a tuple as an argument list, which is very common in functional languages (C++ calls it `std::apply`). You can get into this situation a lot when storing or building up arguments for later. C++ uses variadic templates for its `variant`, and although Rust has a language variant, it's potentially useful to have an ad-hoc one in the same way it can sometimes be useful to have a tuple over a struct. Of course this is leaving out things like metaprogramming and tricks around inheriting from a bunch of things, which have their own alternatives in Rust.
Here's a short and sweet OpenGL with C. It's not much, but it's fun to be able to do some graphics with C. https://www.ntu.edu.sg/home/ehchua/programming/opengl/CG_Introduction.html
i totally agree. thats my point. sun fucked that up. it should have evolved but it didnt. 
Pretend you've never heard of vfork(). It's impossible to use safely 
Writing or calling anything in a vfork child process is undefined behaviour. All you can do is exec. It's for doing efficient fork-exec on systems without MMU/page faults. With fork, these systems would have to copy all the memory even if you're only going to replace it with exec.
It's the fact that fork copies the memory. When it happens (on write or immediately) is irrelevant.
Meh, I don't care about safety or keeping to well-defined behaviour. I just want to know what weird shit you can get up to, even if it might break in the next compiler/kernel release or change of wind direction.
http://www.algo-prog.info/ocapic/web/index.php?id=ocapic https://github.com/bvaugon/ocapic 
There's still people writing in Fortran, Pascal, PL1, LISP, COBOL and C, C++ and others. The reason is because there's code bases out that were originally created in the 60's and 70's! The reason why they exist is because the business model that those code bases are in doesn't warrant rewriting the software in a more modern language. Another thing is that the older the programming language your code base is in the more job security you have because there aren't many PL1, COBOL, Fortan programmers these days. It's expensive and timely to train those programmers up. C++ isn't any where near being dead. It's actually getting a resurgence. There's even [cppcon](https://cppcon.org/) That's a 900$ ticket event! So C++ isn't going anywhere soon. 
&gt; You're not writing code for a PDP-11. Yeah, I read that article, too. Most of its points have nothing to do specifically with C, though. It is the machines we have that keep emulating that PDP model as far as it can for the sake of the humans using it, even as the actual hardware is getting further and further away. It is a mental crutch, implemented in silicon, to help us keep our mental model of execution manageable and stable. With the exception of some esoteric niche languages, the non-C languages still assume the same execution model. Possibly with some more mental crutches implemented in software like garbage collection. Any change to hardware that significantly breaks away from this model will require re-training the minds of millions of developers. It will probably require using some new non-C language, but that would be the easier part, relatively speaking. 
C is old, mature and broadly used in all spectrums of software. Many languages are beautifully designed around C, while others like PHP are not, but still written in C. Theres OS's and microkernels, nuclear powerplants and aviation software written in C. Hell, we even have a rover on mars mainly written in C, and its been operation since 2012. C will most likely outlive languages like Fortran by decades.
Why would a programming language die? a programming language is just a tool. Use the right tool for the right job. The more tools we have, the easier is for us to to do our job. Even if a better C/C++ appears, most people won't switch. It's the economics of such a move.
fork() returns pid_t so you should save it into a pid_t variable for portability.
i don't want to learn how to build a system for concurrency. that is much to complex. i want to build concurrent programs.
As someone who studied physics, I'm fond of an apocryphal Einstein story that boils down to: why should I learn specific details when I know where to look them up? And details *do* load themselves into your middle-term memory if you use them regularly, like how in school I could tell you a bunch of weird integrals without hesitation one semester because a class was using them all the time, but would forget them by the end of the next semester.
Yes and we generate our own electricity. Any other stupid comments you want to make?
C\+\+ != C.
Hi all and thank you greatly for all your responses. Further information: By looking at the past two previous exams I have found helpful information: **Section A:** Questions as "What will the following code print?" These I will not struggle to much on, however some still confuse me; any tips of how to read the output of these type of questions? Other contents of exam heavily rely on pointers (I have decent understanding from YouTube tutorials). Also struct (I am familiar with). My main issues arise when having a statement, I have to code functionality to complete a certain task. I get confused as to... What am I supposed to put in to implement this and where does it belong... I find it immensely difficult that after having available resources as online material, lecture notes, etc... I then have to go into a paper exam that is closed book. Would anyone happen to know my best approach, considering my timeline? Practice exams? Watching videos? 
You should have read the rest of the post rather than jumping to your conclusion 
I dont C\+\+! 
Now, can you please point me to the part of my post where I make that claim?
hmm, the most hands\-down book on modern C for the real world I would say its 21st century C by ben klemens. It doesn't contain any code for trivial apps, and I wouldn't recommend it for novice users since it totally skips the usual introductory lessons about loops and arrays. But for intermediate and beyond I think it's a must. Maybe you will need to complement with other book, more introductory and forgiving like "C programming, a modern approach" by KN King or "pointers on C" by kenneth a. reek.
&gt; Ooh, that sounds like fun... Maybe not too exciting, though, I guess it's similar to normal threads? (Except, wait, mustn't they have separate stacks? Now I'm intruiged...) It's not really like that. `vfork()` was developed as a kludge to improve the performance of the `exec()` family of functions. The forked process does not have a separate stack (this is the core problem), so to make this work, the parent process is not scheduled until the child process calls one of the `exec()` functions or fulfils some other criterion I forgot. It's not really all that useful outside optimizing the performance of fork/exec and even there it comes with a lot of caveats.
Modern C by Jens Gustedt could fit the bill.
Reminds me of ... int ******************** _ptr;
What claim? I just said: C\+\+ != C. Can I?
/u/FUZxxl said it best so I'm just going to link some to stuff I think is helpful. - [Exercises for Programmers](https://pragprog.com/book/bhwb/exercises-for-programmers) - [Simple Programming Problems](https://adriann.github.io/programming_problems.html) - [Exercism - C Track](http://exercism.io/languages/c/exercises) - [Beginner Projects](https://jorgegonzalez.io/beginner-projects/) - [Mega Project List](https://github.com/karan/Projects) - [The UChicago X-Projects](http://chi.cs.uchicago.edu/index.html) It's important to note that all these will help you practice and improve, but nothing will compare to designing your own projects of interest.
&gt; No, it keeps the language and spec small. Not simple. It becomes extremely complex to actually then read or write a program Or to put another way, it keeps the language and spec *compressible*. I know maths has all sorts of questions that are similarly easy to ask, but ridiculously hard to answer or address the implications of, but I'm not sure if they have a word to describe that property. 
I didn't say build a system. I said use system libs to run a program concurrently.
&gt; I'll bet money that nobody will be writing what we call C or C++ in 100 years, maybe even 50. C itself started becoming widespread just ~40 years ago with K&amp;R, and it changed since then quite a bit. I'd take that bet. People are still writing Fortran and Cobol today, and nowadays there are an order of magnitude more systems than there were 40 years ago. As such, if we assume that code today and code 40 years ago has the same half life, then there should be more outliers. Also, "at least one" is a pretty low bar - some computer that "just werks", but has some minor bug/lack-of-feature that's easier to patch than to replace? Pretty plausible. Also, C has the advantage of *religion*.
&gt; Think of a program you would like to have and start to write it. Also, assuming that other people might like this program, you could even make a Git repo for it when you start working - even if nobody else will actually use it, writing the code as if other people will have to maintain, debug, and test it is a good habit to build the skills for. It also forces you to be exposed into how to set up the build tools and debugger, which will both add to your value as a dev and will make your life easier by allowing you to focus on the code rather than build issues once you've learned it.
I personally think that the book, PPPUC++ is not for C++ syntax. It is the best book ever in my life to figure out what it is "Programming" exactly. In the book, Bjarne Stroustrup teaches Programming with C++, not he teaches C++. Therefore, I don't know there is an equivalent book instead of it.
If you want to learn all of C++ syntax, I recommend you to read C++ Primer Plus.
A while ago I put this together: https://sourceforge.net/projects/linux011/?source=navbar It's a very "Apple pie from scratch" way of building ancient Linux kernels on Windows with ported era correct tools. This means no elf, it's all a.out cross compiling. I put in Qemu to run the built kernels, some tiny userlands and the full source to all the tools.. It's a bit much for a starting point, but it's always fun to edit a kernel in notepad, build it from a Windows desktop and run the resulting mess. There is a primative GDB, but I think Qemu is sort of confusing it. Maybe I need to break down and go ELF. But in the meantime it really does work. *to make Apple pie, the first thing you need to do is create the universe 
Thanks for detailed aswer, I'll look into those books.
I'll pitch for getting "Advanced Programming in the Unix Environment" and working through the examples that interest you. If you want a job in the embedded c\-programming world there's no better training. (Well maybe basic algorithms, but you'll probably get lots of that).
Closed book exams sucks, and I'm terrible in exams but for programming I've found a good starting point is to write out in plain text what your program should do in excruciating detail, then break each line of that down into corresponding code. Ahead of time do lots of simple programs in C, Make sure you know the basics, make sure you know how to alloc and free memory (so many classmates in college struggled with this concept). It's safe to assume your exam is only going to be on something that can be feasibly done in a couple pages tops, which limits complexity greatly. For read the output questions write out what the program is doing line by line, it's likely something you've seen before. Worry about functionality over time and space complexity, this works for interviews too, optimize (or describe optimizations) after you have a minimum viable product and only if you have spare time. Try redo any assignments / past questions without using any notes or resources, if you don't get it first try, try again. Finally there is no ideal way your code should look, but use some basic indentation and use the right kind of quotes! Best of luck!
I don't think that's true. Right now there are no true replacements, but that doesn't mean there won't be one in the future. I can possibly see Rust replacing C\+\+ eventually, maybe, and similarly a Rust\-\- with most of the features stripped out replacing C or just a completely different language. I'm not touting Rust here, it's just one of the candidates that could potentially replace C\+\+. The problem is existing codebases and ecosystems though. Something like Rust would be great for game development, but it's no good if all your third party middleware and libraries and stuff is in C\+\+, the preferred language in gamedev land. It would be a real long uphill battle.
yeah that's what pthread is. having different options is good though.
If you are interested in how C programmers implement object oriented code, this book is great. This book is not going to praise object-oriented programming or condemn the Old Way. We are simply going to use ANSI-C to discover how object-oriented programming is done, what its techniques are, why they help us solve bigger problems, and how we harness generality and program to catch mistakes earlier. Along the way we encounter all the jargon — classes, inheritance, instances, linkage, methods, objects, polymorphisms, and more — but we take it out of the realm of magic and see how it translates into the things we have known and done all along. [Object-Oriented Programming With ANSI-C](https://www.cs.rit.edu/~ats/books/ooc.pdf)
you can use array of numbers, and call round function inside a loop
C++ is a superset of C. It adds functionality that C doesn't have. Syntactically they're similar looking but if you're using C++ you're going to be writing a very different style or code. I wouldn't treat them as the same as doing so would be a huge compromise for both languages.
Use an array with a loop but this is also wrong: scanf("%lf%lf%lf,&amp;a,&amp;b,&amp;c"); Should be: scanf("%lf%lf%lf",&amp;a,&amp;b,&amp;c);
you might try this... https://www.w3resource.com/c-programming-exercises/
god damn it :) I hate myself and thank you 
&gt; c++ is a subset of C This is backwards, and a little wrong. C++ is almost a superset of C. This almost qualification is important for 2 reasons. The first is, there exists valid C code which is not valid C++ code. For example, C has some implicit conversions that must be explicit in C++ (`void*`), C has some language features C++ does not (e.g., variable length arrays), and more. Secondly, although a lot of C is valid C++, there is virtually no problem you would solve using modern C++ that you would solve the same way as in C. That is, if you wrote down some C that happened to be valid C++, it would probably be clear to anyone fluent in both languages that what you wrote is C. However, C and C++ are similar in some very important respects, making it easier to learn one if you know the other. Most importantly, they share essentially the same memory model: if you understand how memory works in C++ it is easy to understand it in C, and vice versa. They have similar syntax, some similar types (pointer types, for example, and structs). They also have essentially the same compilation model. So, learning C is relatively easy after learning C++, and learning C before C++ will allow you to transfer a lot of knowledge of the memory model with you (and this is the area the trips people up the most, so it's not a trivial part of learning the language). However, I will stress again, there is essentially no non-trivial code that you would write the same in C and C++, and C++ is an immensely larger and more complicated language that C. 
Chances are you will mix them up, because c++ is a subset of c. However, you can write c code in c++, no problems because of this. Many c++ headers have references to c headers, ctime, cmath... Plain C doesn't have objects, so you don't have classes.
Isn't Ada a perfect demonstration of that C is the only game in town? C kind of murdered Ada ^_^
At Uni we used Applications Programming with ANSI C by Johnsonbaugh and Kalin. It's pretty good. 
Thanks for making that distinction. Cleared up some questions I had of my own!
Given that, I believe there is little reason not to use C++ on embedded systems, unless the embedded system compiler doesn't support C++, which should be rare these days.
Looks good for a start. Still waiting to see something real-world like a USB driver and not just a fancy demo.
C before C\+\+, of course
Some of the micro controllers I would be using are 8\-bit which I believe means they would not have the power to have a c\+\+ compiler on them.
I think "immensely larger" is a good reason.
&gt; if you understand how memory works in C++ it is easy to understand it in C Is this still the case? I hear some people say that one shouldn't use new and delete anymore. I rarely use c++ so i'm not up to date regarding the latest standards but for example using the stl you don't have to. In c however you'll be using malloc and free for about every data structure you want to use. Personally i think it's good to create these by hand instead just to see how much work is done under the hood in other languages.
The compiler wouldn't be on the microcontroller. Compilation almost always occurs on a development machine and then binary code is flashed to the Microcontroller
I have limited experience coding on embedded systems, but I suspect the choice is less clear cut. C has benefits over C++. It’s much simpler, so it’s a bit harder to shoot yourself in the foot. It maps more directly to assembly, and the assembly produced is a lot more readable (no name mangling) so this means code can be more easily optimized. C++ tends to encourage writing more compile time generic code, through templates, which will generate more source code tending to lead toward larger binaries that may be unsuitable to embedded systems. 
Regardless of whether you call `malloc`/`free`, `new`/`delete`, or `std::make_unique`/RAII, the underlying model of dynamically allocated memory which is manually managed is the same, and has the same pitfalls when learning (having to consider lifetimes). The memory model also includes static allocated memory and stack allocated automatically managed memory which is mostly the same in C and C++. 
Ah i understand now, thanks for explaining.
Take your favorite Atari, NES, Game Boy, Arcade games ... and make those. Or maybe a Rogue Like, lots of tutorials to help start something like that and even a subreddit dedicated to it. Take a project on GitHub you use right now that is in C or has C components and try and contribute by taking one of the tickets on or even put up a pull request adding a new feature or bug you ran into.
Ahhh, I wouldn't say that, in java you are limited to runtime polymorphism (inheritance) because the generics aspect of the language is poorly designed, in c++ you are more encourged to write and use value types - which means "types that behave like int", and leaving the polymorphism to the compile time stage as much as possible. There's a lot to write about the differences between the languages, but the most important thing that I can tell you is *don't write a comments like this in c++ related subreddit, they will downvote you like maniacs*
[https://www.quora.com/Why\-does\-it\-seem\-that\-Unix\-is\-written\-in\-C\-and\-not\-written\-with\-any\-C\+\+\-Isn’t\-C\+\+\-more\-powerful\-than\-C](https://www.quora.com/Why-does-it-seem-that-Unix-is-written-in-C-and-not-written-with-any-C++-Isn’t-C++-more-powerful-than-C)
&gt; For example, C has some implicit conversions that must be explicit in C++ ( void* ), C &gt; has some language features C++ does not (e.g., variable length arrays), and more. As somebody who ported like 200k lines of C code to C++, there's a few C++ key words like "new" and "this" that also make it not a perfect superset.
Objects and classes are easy to be done on C with its own features. All that abstraction of C\+\+ is what I dislike it and any other PL with this disease!
&gt; the parent process is not scheduled until the child process calls one of the exec() functions or fulfils some other criterion I forgot. Ah damn, I checked the manpage and you're right here. This definitely limits the opportunity for mischief. Oh well, thanks for the new info.
I think it's *easier* to shoot yourself in the foot with C, especially with regard to memory. If the target can handle it and it fits the use case you might as well use C++ on an embedded system.
How come the powers that be did not maintain C++ in such a way that any C program could run in it without any changes?
OS? IDE/Editor? Compiler arguments?
Yeah I’m sure :) Wasn’t really giving my opinion, just simply stating facts for OP
How would an editor change compilation speed?
The common thing people say around here is that it is easier to shoot yourself in the foot with c than c++ but easier to cut your leg off with c++ than c.
how was that experience?
I dont get Reddit down vote. It is an instrument of oppression, one can just say stupid things and a bunch of clever boys smash that arrow down button! DAMN! 
some ide's use their own compiler I think? otherwise I don't think it matters
Sorry for the rambling random state of this. I guess if I had more ambition I could structure it properly, add amusing anecdotes and turn it into a book that no one would read. I can only speak for myself. I think a lot of people were programming on punch cards in 1978. I was programming early personal computers to do embedded type stuff. I worked for a little company and figured out how to do stuff pretty much on my own. I would write BASIC programs and write assembly language routines to make them run fast enough. I didn't have an assembler, so I'd write the program on notebook paper and then hand assemble it. If you haven't done that before, it's a two step process because you have to resolve all the forward references in a second pass. I'd take the hexadecimal machine code I'd generated and hand translate that to decimal in DATA statements inside the BASIC program. In 1978 I was writing mostly on 6502 and Z80 machines. Program storage was on cassette tapes. We had other weird technologies too, like [stringy floppies](https://en.wikipedia.org/wiki/Exatron_Stringy_Floppy). In 1979 we got [5 1/4" floppy disks](https://en.wikipedia.org/wiki/Floppy_disk). That was a big step forward. I don't remember when I got my first usable assembler, but it a wonderful advance. In 1982 I started working on IBM PCs. They were a huge disappointment. To me anyway, the processor was a big step backwards. I was really disappointed that IBM chose the 8088 instead of the 68000. The architecture was far inferior. But I continued to write similar programs for PC, with the parts that needed to access hardware or to run faster in assembly language and the glue in BASIC. Then in around 1984 I got a C compiler and started writing the glue in C. It didn't produce very good code, but at least it was compiled, not interpreted. It was also during the mid-1990s that I started making truly embedded products. Primitive PC-layout tools were becoming available which allowed me to design PCBs without having to use red/blue tape. Programming these boards was the traditional, "burn and learn". In other words, I'd program an EPROM, pop them into the board and use an oscilloscope and/or logic analyser to try to figure out what was going on in the code. [EPROM programmer](https://www.eetools.com/)s and [UV erasers](https://www.digikey.com/catalog/en/partgroup/uv-eprom-eraser-851/20358) were very important. Editors improved a lot during the later 1980's. Hard drives also became inexpensive enough to have them in every computer. That made development a lot faster and nicer. Keeping everything on floppies wasn't ideal. An important development during the 1990's was flash memory. Being able to reprogram systems remotely, without having to tear systems apart was a very big step. Even early flash parts that only had a few erase/program cycles greatly improved my company's products as far as our customers were concerned. Although I had an Arpanet account starting in the mid 1980's, there wasn't a lot you could do with the very limited dial-up access I had. My work provided an ISDN line to my house in around 1993. That was a big improvement over dial-up, but a year or two later I had a cable modem which was a giant improvement in speed. There were still limited resources available online. Most of my references were books. During the mid to late 1990s the fastest development environment I had access to was a Sparcstation. I used it until PC speed surpassed them, probably around 2000. Then I switched back to PCs. I briefly used Linux, but Windows tools tended to be higher quality. After working at the same place for 13 years, I was laid off in 2004 and worked as a short term contractor for about 10 years. The common denominator in all of those places was Windows as a development environment. There is a lot more I could talk about. If there is anything specific you'd like to know about feel free to ask.
windows 10 i use dev c++ and codeblocks 
No, it's *not* a superset.
I don't understand why anyone needs C++.
Your compiler is your friend if you turn on proper warnings, see http://c-faq.com/expr/seqpoints.html
The `++` bit of `p++` can be evaluated at any time before the call is made. And the 3 arguments can be evaluated in any order. So there is no one correct output from this.
I was talking about the critical software niche. Ada has yet to be displaced there, I believe. More importantly, it can be used to point out that safe software doesn't exactly mean C. That there used to be alternatives, and there could be more. 
The order in which the arguments to a function call, in this case `printf`, are evaluated is undefined. Because the third argument has a side-effect, the order matters. You're assuming they're evaluated in the order written. It's not an unreasonable thing to assume, but it's not correct when it comes to C. Specifically, you don't know whether the increment caused by the third argument takes place before, after, on in between the evaluation of the second and fourth arguments. One way to avoid this is to compute the values _before_ calling `printf`. E.g. change the code to something like: int v1 = *p; int v2 = *p++; int v3 = *p + 5; printf("%d,%d,%d", v1, v2, v3); 
Please tell us more. What IDE are you using? How long is 'a lot of time'? Are you getting any diagnostic messages? What else should we know?
[https://stackoverflow.com/questions/32017561/unsequenced\-modification\-and\-access\-to\-pointer](https://stackoverflow.com/questions/32017561/unsequenced-modification-and-access-to-pointer)
Such a cool perspective, thanks for sharing it! I was raised in the internet age, but I actually collect and maintain vintage computers, so I know more about computer history than the average person. Doing this since a young age really made me appreciate the technology we have now. I actually got a Borland Turbo C compiler running on my IBM XT a while ago, lots of fun! My latest project was getting Minix to run on my 1986 Macintosh Plus (Surprisingly fast for 4MB of ram and a disk image loaded via an AppleTalk network)
A superset contains the same properties with some additional ones of a subset. C++ contains the same functionality Ass with the addition of features like classes. Pretty sure that's the definition of a superset but I would love to hear how I'm wrong if you can provide good evidence.
You know, it's helpful to provide information when you ask questions. If you're running Windows, you're probably SOL. The slowest computer I have running right now can compile and link a small 30 line c program in about 13 seconds. It's a computer that was made in 1991. Is your computer slower or faster?
I don't understand why anyone needs anything other than Assembly.
&gt; through templates, which will generate more source code tending to lead toward larger binaries that may be unsuitable to embedded systems. This is not true. Enabling any optimizations will remove that redundant code. If this was true then [this](https://www.youtube.com/watch?v=zBkNBP00wJE) talk would have been impossible. Regarding name mangling, compiling with debugging symbols resolves that afaik for debugging. And that making optimization easier is not true based on, again, the talk I linked earlier. If you meant it's easier for compiler writers then that I agree with.
The atmega328p used in the Arduino Uno world is 8 bit with 2 kB of SRAM and 32 kB of flash. The Arduino environment is on top of c++. It can easily fit on even smaller and older systems like [this](https://www.youtube.com/watch?v=zBkNBP00wJE).
Eh, debugging memory and other resources doesn't take much if you sit down and plan for them. Same as in C++, most resource issues come down towards hiding naked allocations behind init/fini, with C considerations being edge cases when you put calls for init/fini intermixed in other code. We all know the classic example that RAII solves: int work(size_t space) { unsigned char *buf = malloc(space); ... if (error) return -1; ... free(buf); return 0; } One well known idiom in C is to use labels and gotos to make 'centralized exits', but this is very manual and error-prone. Instead, just avoid the edge-case of mixing your allocation with code: make your function pure of allocation by putting temporary resources in arguments, and preserve the ABI using stub functions: static int work_body(unsigned char *buf, size_t space) { ... if (leak) return -1; ... return 0; } int work(size_t space) { unsigned char *buf = malloc(space); int cond = work_body(buf, space); free(buf); return cond; } Obviously there's a lot more reasons to use C++ than trying to ensure correct resource allocation/free, I just don't think it's worth avoiding C for resource-handling reasons.
That's interesting, so if i write a C program it might not compile if I use a C++ compiler?
No? Why would it? Sometimes I don't recall the exact syntax for expanding a call upon forwarded variadic template arguments. But I certainly recall that I *can*.
UB is hard to avoid because it's not always obvious that something is UB.
I'm not sure how that's an analogy.
Any good C example? I haven't encountered any myself.
Right, more accurately, it's a hyperbole. My point is, it's not like we memorize the C syntax/spec/stdlib like the 10X10 multiplication tables, the benefit of C over C++ is that you can larger implement the same models, and solve the same problems, with a more manageable feature-set.
Oh window 10 on laptop? Yeah that explains it 
Regarding templates, I wasn't suggesting there would be redundant code that wouldn't be optimized away, I was merely suggesting that in C++ templates allow easy traps for growing the size of a binary because it becomes much easier to generate a lot of code. C doesn't have that capability, excluding macros. Of course, if you compare apples to apples, and you write all the same functions that would be caused by the template specializations in your C program, there won't be a difference. But, it would be easy to come up an example, say, where a templated function in C++ is specialized for `int` and `long` but in C only writing a `long` function causes less code to be generated. For name mangling, I'm just saying it's harder to read code generated from optimized compilation of C++ than C. I'm not saying any of the above is bad for C++ or C, I'm just giving an indication why someone would prefer C over C++ for applications where they really care about having more control over the code being generated. 
Sure. The easiest would be to have a variable called `class`. But barring variable names, this is not an unusual looking line in C: double* x = malloc(10*sizeof(double)); and it will not compile in C++. And not because of `malloc` not being in C++ (it is, but you shouldn't use it) but because it has an implicit conversion from a `void*` to a `double*`, which is not allowed in C++.
I don't understand why anyone needs anything other than addresses.
There are a number of C++ features that don't work well on small embedded systems. Dynamic memory allocation is something that most C++ programs use. In small embedded systems this is typically a very bad idea. [Here is an article](https://barrgroup.com/Embedded-Systems/How-To/Polymorphism-No-Heap-Memory) that discusses some of the issues and workarounds.
I think what you are saying is almost correct. I don't see anything that is theoretically wrong at the undergraduate CS degree level. After you learn pointers and memory management in C, you have a moment of realization why some types are immutable and so on.
There is nothing in C++ that forces you to use dynamic memory allocation. On the contrary, I find that C code tends to use more dynamic memory allocation than C++ code.
Larger **language** doesn't imply larger binaries or anything that would prevent it's use on embedded systems. I've seen C++ code better optimized than equivalent C code because C++ has a better type system, which allows the compiler to understand the code more.
You're 100% correct about C++ binaries. But what's that old jwz chestnut about people carving out their own little subsets? You can have 5 experienced C++ coders on a team who are using what are, for all intents and purposes, 5 completely different programming languages.
&gt; Aswell as this I am worried about mixing them up? Worry not. Compiler will make sure you won't mixing them up. Modern compilers usually will show messages good enough to point out where the syntax went wrong. Some even provide suggestions.
I don't understand why anyone needs anything else than abaci.
Equivalent would be: int y = *x; // unused if (!x) { return; } No guarantee it will crash *or* return. Also, basically any violation of the rather arcane strict aliasing rules.
Your conclusion is questionable. I can do the same in assembly. The advantage C++ offers is being able to better reflect the problem, being more explicit about what you are doing, and offering better compile-time safety and features. If you're using every feature of C++, you're using it very wrong.
C has some features which C++ doesn't, like VLAs, designated initializers, compound literals and certain implicit typecasts
I just think you're wrong. C is a superior language and it's proven by the quality of projects that use it over C++.
Your facts are wrong, any programming language with function pointers or equivalent features have object. Object-oriented programming is semantics, not syntax.
Mind numbing. It took a month to just get it to compile.
Look, I am sorry if I offended anyone, but to my knowledge C is not an object aoriented language and can't do objects in the sense of Java or c++ can. You can't have encapsulation of object methods, nor have methods inside of structs, something you can do in c++
In C dynamic memory allocation is explicit. As an embedded developer I never use malloc(). In C++, dynamic memory allocation is not only implicit, but often hidden.
If I'm doing anything that's heavy on polymorphism, I start looking at languages other than C. It can get a little tedious in C.
You're welcome to your opinion.
No it's not. Neither implicit nor hidden. If you call "new" you're explicitly allocating memory. Otherwise it's not allocating heap memory. It's only hidden if you use standard library classes that allocate memory or other classes that allocate memory. There is nothing in C++ **as a language** that forces you to allocate memory if you don't want to.
Tl;dr: C is statically typed but less strongly typed than Java. Python is strongly typed. 
There are a few things that are different, like sizeof(‘A’) which is 1 in C++ and sizeof(int) in C. char foo[3] = “abc”; is legal C but not legal C++. C++ doesn’t allow recursive calls to main(). Casting rules are different. C++ adds a bunch of keywords. Objective-C is a proper superset of C, therefor the syntax is adds is very different, New keywords are prefixed with @. 
You may read the arguments left to right but 
What features of c++ do you need for embedded development? Most platforms don’t support v-tables (so no OO stuff) and the language is immensely more complex and much more difficult to integrate with assembly. 
Looks like C++ and not just "C". Also, is this homework of some sort? It makes little or no sense to factor numbers like that for any given purpose I can think of ... then again who knows. could be inventory and you want to put marbles in boxes on shelves. Either way the question seems pretty trivial. However the integers could be really big and in that case factoring can be a nightmare and you need Mathematica or GNFS.
I don't really see your point. What you think of as an object, in an OOP, is just, at the level you are talking about, a pointer. So there isn't much difference. The compiler must know the class of an object, just like it must know the type of a pointer. 
That is a very interesting way of looking at... We give pointers types so the compiler knows how to interpret the stream of 1s and 0s the pointer points to. We instantiate objects from classes and we use the class to tell the compiler and/or interpreter how to store the data of the object (in addition to the relevant methods to query and modify the object for it's internal state). And, fundamentally, variables that "store" an object are really just storing a reference value (a pointer; a memory address) that points to the object in memory. I think I sort of see what you are saying. I guess my main point may have been to distinguish between the "OOP sense" of a data type and the "lower level" notion of a data type. I think the distinction is helpful though. 
What is your question? Are you asking how functions work? A function takes an input and return an output based on the input and the code defined. 
Not sure but having your includes in big bold text is sure to help
\[They also asked this question\]([https://www.reddit.com/r/cpp\_questions/comments/8pwlq6/need\_help\_urgently\_i\_can\_pay\_if\_needed/](https://www.reddit.com/r/cpp_questions/comments/8pwlq6/need_help_urgently_i_can_pay_if_needed/)) on /r/cpp_questions, but got fresh with some of the locals. 
That wasn't really the big idea that I was trying to convey. It was more how the data types communicate different things in different languages. In any language, data types communicate to the programmer a sense of the valid set of values (eg, can't store a string in an int) and a set of the allowe operations for those values (eg, floats have no concept of the modulo operator). In OOP languages, data types also evoke the sense of coming from a class; which defines an object that has state (internal data) and behaviour (methods). In more imperative/procedural langauges, data types may tell the programmer how exactly the data is stored. Or at least the was the core idea I was trying to convey, I don't know if I succeeded though. 
If you mean something like: foo(int *x) { int y = *x; // unused if (!x) { return; } dosomething(); return; } bar() { int *ptr; foo(ptr); return; } The original question was whether or not one has to be aware of the UB's of C. I said no. Unless explicit stated otherwise, it's the job of the caller to validate input, here the caller didn't do that. Whether or not the behavior is defined that is always a bug. It would be just like: void (*fun_ptr)(int); fun_ptr(10); // points to nowhere I've been doing this awhile but they seem trivially easy to avoid. And in these cases, I don't need any special knowledge of defined behavior. What do you think?
How large is your C project? Have you tried using Clang to compile it? How about MSVC?
Format blocks of code for Reddit by putting 4 spaces in front of each line.
You call your function 3 times with 3 different arguments... Also, format blocks of code for Reddit by putting 4 spaces in front of each line.
Thank you I know function call.However I did not know it was this way.
You need to know and follow strict aliasing rules so you don't run into UB. Lots of people compare pointers when they don't point to members of the same object or array. UB. There's also fun UB involving loops where the compiler presumes a loop that does not have a constant conditional always exits. Ergo, IIRC, this is undefined: void foo (int x) { while (x) while (1) }
Codeblocks with gcc shoud usually work and does not take long if you simply compile a simple program. We need more info on the compile arguments etc.
And you actually see/use code like this?
Of course there is nothing that forces you to allocate dynamic memory in C++. If there was, it would be completely inappropriate for embedded development. C++ is fine for embedded development as long as you know what you are doing. However, it is easier to avoid inadvertent memory problems in C. If your experience with embedded C++ is different, that's fine. There certainly are advantages to C++. True objects can be powerful. But most people with experience developing embedded C++ have encountered mistakes that wouldn't have occurred with C.
thanks.
If you have taken any programming course like Python or something, then learning C takes you probably like a week. C language doesn't have much actually. You have to learn pointers and memory allocation which is very tedious if you try to understand it fully. It is difficult but absolutely doable. Learning C\+\+ contains learning pointers, so you can skip this part. On top of C, C\+\+ has the object oriented programing component that is something you need to understand very well and there are quite a bit of things you will have to learn. The biggest problem with your statement actually is that Unity which is more popular than Unreal uses C# not C\+\+.
You can't envision times where you couldn't potentially see it in some form? And if you're going to say with a straight face that you *never* violate strict aliasing rules, I'm going to call you a bad liar.
Over the years I made every conceivable mistake one possibly can (though not as much these days). That's not the point. I don't think we're communicating. The original very specific question was: Do you have to know what's undefined in C. I say no. All those example should be correctly handled regardless of what is and isn't defined. You don't need to know exactly where defined behavior ends to program correctly in C. That's my only point.
printf() doesn't return text; it writes to standard output. snprintf() might be more what you're looking for.
I'm still not sure how to "grab" that from Python
There is presumably some way to turn a C string into a python string.
I provide more context here: https://stackoverflow.com/questions/50777214/python-ctypes-how-to-pass-row-outputs-from-a-c-function-into-a-pandas-dataframe You're right---in C, I pass the rows to a buffer...but I'm not sure how to access this in Python via ctypes
While few would argue that the same number of lines would have been faster to write the first time in Perl, is there any particular reason why you think the quality of the site was related to its implementation language? 
But your point is wrong. Software *constantly* breaks due to UB. Popular, heavily-used software.
But your point is wrong. Software constantly breaks due to bad coding practices. Popular, heavily-used software.
&gt; PDP-11, which is very different from modern architectures. I wouldn't mind seeing hardware Lisp machines, or [stack machines larger](https://en.wikipedia.org/wiki/Burroughs_large_systems) than [a microcontroller](http://www.excamera.com/sphinx/fpga-j1.html), or [High-Level architecture machines](https://en.wikipedia.org/wiki/High-level_language_computer_architecture) tried again, but nobody wants to do that. In fact, we've had less and less architectural diversity every year for the last thirty years at least. Possibly the sole, minor note of deviation in the march of convergence has been RISC-V. 
The single biggest problem is that when you're reading other people's code it's no longer optional. Hence the near-ubiquity of [tight style guides for C++](https://google.github.io/styleguide/cppguide.html) in sophisticated operations. Style guides that, correctly, eschew exceptions and tread quite lightly with templates. 
&gt; It's actually a pretty common task for new languages to prove their worth. Unfortunately so. Bootstrap GHC and you may find yourself lusting for a compiler written in C, and wistful for the day when they all were. 
D-Wave is said to be using Common Lisp for something analogous to an OS, but it's not exposed. 
&gt; They have an entry level COBOL position that starts at 320k with stock options. Options adds a variable, but this is very implausible. Universities were teaching Cobol after Java came out. More importantly, entry-level Cobol competence is not a high bar, so the supply-demand is implausible. It's not like bootcampers are using ECMAscript because they like it or it's the best language in the world -- they're learning it to make money. 
&gt; Probz just write that sucker in C and never have to worry about it dying, amiright? Embedded and kernels, so yes. 
&gt; The only people who argue about which programming language is the bestest are idiots. Those who think they're all equally useful are no less foolish. Picking a tool to suit the task is fine, but I find a surfeit of short-term "pragmatism" to be as big a problem in computing as the opposite. 
how similar is a lake to an ocean ?
There's a relatively compelling thesis that object-oriented programming only seems to have "succeeded" because its definition was *de facto* changed to match all of the things that claimed to be object-oriented. That the benefits ascribed to OOP in the enterprise have mostly failed to manifest, and have been accomplished in other ways (for example, code re-use through open source libraries irrespective of implementation language, not through OOP). 
&gt; nobody would be using c++ if sun had open sourced the jvm and allowed java to compile to native binaries so it could be used for real projects. Java was explicitly designed to be safe enough for enterprise business programmers to use. And it largely succeeded in the ways it intended, but at a cost. It's a dull blade with weaknesses of its own. It's only now becoming obvious what a vulnerability is Java's native serialization. I wonder what Rust's is? &gt; the windows kernel To the best of my knowledge `ntoskrnl.exe` is partially C and partially C++. I don't know if a version was leaked in that Windows source code leak years ago, but I bet it hasn't changed a huge amount since then even though that version would be quite old now. 
So, your code has no bugs?
We have to stop now, I said it already. Bugs yes, due to undefined behavior no. There's no reason to ever write code that would be subject to the possibility of undefined behavior including aliasing, bad pointer assignments etc. Your own examples are things you should never do. We'll continue to due them anyway but we can't blame UB. We're long past the point of just repeating ourselves.
Can we also emphasize the importance of putting stuff like strace outputs and encouraging people to read them? Especially if they don't want to put any code up. 
I'll just reiterate - if you honestly believe that you've never accidentally incurred UB... well... you're wrong.
Please explain why you used # instead of *
.
What platform?
Linux
Yeah I'm probably not gonna use unity cause I understand that c# is effectively more java related than c related. I will probably use the unreal engine. 
I'll just reiterate - I never claimed I did, I don't know how you keep taking what I'm saying and turning it into that but you do.
SDL seems like the obvious choice, sorry. If you can't get SDL working, getting one of the others working seems unlikely, but you do have a few thousand (probably no exaggeration) other options: Allegro, ClanLib, PLIB, OpenGL, svgalib, DirectFB, etc. Maybe you could help narrow it down a little. 2D or 3D? Real-time? Need any cross-platform support? Is it for game development? Do you need other media support (e.g., audio)?
OpenGL
Try [Allegro](https://liballeg.org/) But to be honest if your having issues with SDL working with an IDE, your probably going to have the same issues here. 
1) https://liballeg.org/ 2) http://www.raylib.com/ Raylib uses immediate mode OpenGL so it's not going to be the fastest thing on the block, but depending on what you're doing this may not be a problem. Not sure about Allegro. 
Simple. I’d be happy just to draw a line. It’s to help me learn, not for show and tell, so doesn’t really need be cross platform. The more barebones the better, so I can learn C, as opposed to memorizing funcs in a library.
Is this advice aimed at beginners? If yes, could you point to some examples of previous questions where this helped significantly? I'd like to get an idea of what kinds of problems this helps with, and how much context we would need to provide about what strace does and how to read it's output.
I have worked with SDL on Linux and if you can tell me what problem you are having with SDL I might be able to help you.
Sounds like you just want something to get you a framebuffer so you can start pushing pixels yourself. SDL and Allegro are probably your top two options.
Please edit your post to fix the formatting.
Much of the time it couldn’t find SDL header files. Even when I would go under Project/Build/Linker and specifically tell it as I had seen explained in many places. Other times, when I would go to build, it would say something like “there is nothing here left to do” but then it wouldnt execute. Think sometimes my problem may be my file management, as in I’ll open a new file under a project but it doesn’t really recognixe it as being that project maybe? I got pissed and tried doing some stuff through the command line and gcc as done in a LazyFoo tutorial, but couldn’t get my makefile to work. I cant really go into specifics right now because I’m really tired and burnt out right now. Plus I don’t really know what I’m talking about either.
What happened to something like graphics.h where it seemed you could just include the header at the top of your source and away you go, as opposed to having to deal with wrestling an IDE to work with SDL?
Improving the language is more important than holding on to useless baggage
Googling leads me to believe graphics.h was something that shipped with Turbo C for DOS in the 1980s? As it pre-dated the modern Internet, compiler publishers often tried to include as many libraries as possible (since you couldn't very well download new ones). Well now we have a lot of libraries to choose from, but it should be just as easy to get it going. I don't know what IDE you're using (or why you're using an IDE), but it should be as simple as a `-l` flag to get your IDE to link in a library once you've got it installed.
The header file for SDL2 is included using `#include &lt;SDL2/SDL.h&gt;`. To link the library with the resulting executable you need to use `-lSDL2` when invoking gcc.
Gcc graphic.c -o graphic ./graphic -l SDL ? That simple eh?
Learning how to properly install / link / setup libraries can be a pain. If you don't want to deal with this right now, I just did a quick search for header-only graphics libraries and I found this -- https://www3.nd.edu/~dthain/courses/cse20211/fall2013/gfx/ I just downloaded and tested it and it worked flawlessly for me. Simply downloaded the three files and ran the command provided in the terminal. 
Thanks. I appreciate it. 
-l goes on the gcc line. gcc graphic.c -o graphic -lsdl ./graphic (It's possible that you'd need extra `-l` flags if SDL has dependencies. It's been a while since I've used SDL so I can't remember)
[Here](https://github.com/fogleman/craft) is a wonderful example of a minecraft clone written in C, using modern OpenGL.
Awesome, thank you for the advice, especially the part of breaking down each line to code. :)
If you want to learn about how graphics works, then look up "Ray Tracing in a Weekend" by Peter Shirley. It's a really great starting point for learning rendering, while the example code is in cpp it's not hard to rewrite as C. Don't worry about any maths, if you know a little about vectors already then you're good to go, otherwise you can just look up topics when they come up.
Damn, that's really cool.
Your definition of superset is correct, however your notion about C and C++ is wrong. The C++ standards committee often tries to spread this bullshit, too, sadly. And if one looks at the languages from a not only grammatical standpoint, they are so different and shouldn't be mixed up.
I cannot agree, only if you try to compile C++ as C, but not the other way around sadly. Many thus think that C++ is a true superset of C and, even worse IMHO, think that they bus should be mingled freely which results in horrible code.
&gt; NEED HELP URGENTLY ! you realise that it costs more if you shout...
I'd recommend GLFW and OpenGL http://bedroomcoders.co.uk/modern-opengl-and-2d-sprites/ http://bedroomcoders.co.uk/learning-opengl-from-a-c-programmers-perspective/ these posts include working code for you to play with...
If using TI's code composer studio (eclipse based) switching the compiler output verbose mode to minimal makes a big difference on windows platforms.
A vtable is just a pointer to some table of function pointers, which systems (aside from GPU's) can't support such a thing?
It would be closer to: gcc graphic.c -o graphic -lSDL Or gcc graphic.c -o graphic -lSDL2 That's enough for basic SDL, but loading non-BMP images requires SDL_image, primitives (squares, lines, circles, etc) requires SDL_gfx, fonts require SDL_ttf, etc. Also, nowadays SDL recommends using pkg-config (on Linux) which should be as simple as: pkg-config --cflags --libs sdl2 sdl2_gfx sdl2_image (...) The exact package names would be distribution dependent. Either way, I would recommend OP learn proper linking over just throwing their hands in the air and trying to find an "easier" library. Linking is a fairly crucial step of C development, for anything but the most trivial self-contained one-file problems.
&gt; C++ is almost a superset of C. It absolutely is [*NOT* a superset of C](https://en.wikipedia.org/wiki/Compatibility_of_C_and_C++) as it fundamentally changes a number of things. If C++ were a *true* superset of C, a C++ compiler would have *no problem* compiling any non-trivial C program, but that is *NOT* the case.
Then it's not a superset. It's a **fork**.
&gt; I would love to hear how I'm wrong if you can provide good evidence. How about [this](https://en.wikipedia.org/wiki/Compatibility_of_C_and_C++), or [this](http://ptspts.blogspot.com/2010/12/it-is-misconception-that-c-is-superset.html) If C++ were a true superset, you would have no problem compiling any arbitrary C code on a C++ compiler. In the real world you are guaranteed to run in to problems. ObjectiveC is a true superset of C however. You can take any arbitrary C code and compile it with an ObjectiveC compiler.
&gt; I don't understand why anyone needs anything else than abaci. The string handling is *abysmal*.
I would agree with that terminology
He doesn't know how to backslash-escape \* and did that to prevent it from italicizing the text.
First, I highly recommend raylib. But using it on linux is a little more complicated than using sdl. So maybe keep it in mind for later on. I think one thing you might be misunderstanding is that working with sdl on linux usually doesn't require having your own copy of the sdl libraries and headers. If you're on a major distribution like Ubuntu, you can install the sdl development package, and then #include &lt;SDL/SDL.h&gt; and it'll just work, no matter what IDE you're using. All you have to do is tell your IDE that you want to link to the sdl library, which probably just means adding "-lsdl" or "-lSDL" (i'm not sure which) to the linker flags in your IDE. On Ubuntu, I think the install is: sudo apt install libsdl2-dev However, I haven't done this in a little while, so I might have some details wrong. A lot of SDL extensions are also available for install with apt. You can find a complete list here: https://packages.ubuntu.com/search?keywords=sdl2 
That doesn't happen inside code blocks though, and he correctly posted a code block
C++ is off topic in this subreddit. Please post C++ content to /r/cpp_questions instead.
FYI on certain (if not most?) distributions you can use the \`pkgconfig\` to output the proper include and linking paths. 
Have you taken a look at https://dashgl.com (GTK with OpenGL)?
SDL can be a bit touchy to set up, especially if you are hiding behind some IDE. Especially if it's Linux. Honestly you would probably find MS-DOS easier to work with, as you have direct hardware access. Does whatever ide you have work with cross compilers? I'd imagine a DJGPP cross toolset is trivial these days, and you can test it in DOSBox. Otherwise I'd just say get the source to Quake1, and look at the X11 code for Unix. It'll set up a simple area to memcpy to the screen.
If you're going to post a link to a blog, at least have the decency to provide a summary if you're submitting as a text link.
Please post links as links, not as self-posts. I have removed your post so you can try again.
on debian there's sdl2-config, running it will give you the proper options for it: $ sdl2-config --cflags --libs -I/usr/include/SDL2 -D_REENTRANT -L/usr/lib/x86_64-linux-gnu -lSDL2 So i usually stick that command in my CFLAGS, or when compiling things by hand on the command line itself like so `gcc -O2 -pipe $(sdl2-config --cflags) -o myprogram myprogram.c $(sdl2-config --libs)` I've just started using sdl myself and with version 2 several things have changed which make some tutorials rather hard to follow. I'm not sure if i'm doing things right but [here](https://github.com/ascheepe/sdlfun) are some examples to maybe get you started. Further more i'm also still kind of fond of the old graphics.h for these kind of things. Basic sdl hasn't got any line drawing commands and such if i'm correct so making a compatible one might not be that trivial. Anyway, hope this helps a bit and happy coding! :)
&gt; SDL can be a bit touchy to set up, especially if you are hiding behind some IDE. Especially if it's Linux. I don't agree to this. On Linux, there are two things you need to do in order to set up SDL: * install the library using your package manager * link with `-lSDL2` Everything else should just work™.
People rely on an IDE to determine what software library they'll use?!
Not bad. I compiled and linked a 27-line program on the Commodore 128 (in an emulator) with the Power-C compiler, and it took almost 3 minutes. :-)
Appreciate it.
I’ve tried Gedit, Eclipse, Geany, and CodeBlocks. And as I’ve said, I tried without an IDE as instructed in the LazyFoo tutorials. I just don’t have the knowledge to work with this stuff. 
Allegro and SFML are two nice alternatives to SdL
Modern C\+\+ has [many more ways to control how memory is allocated,](http://preshing.com/20130922/acquire-and-release-fences/) particularly in the context of multithreaded applications. 
Newbies do. 
&gt; Not sure if that will COMPILE my C programs into pure C (rather than something else like C++, or some kind of .net thing?) &gt; &gt; Whatever IDE I use, I want it to compile to PURE C! You don't "compile to C". C is the source language, which is typically compiled to machine code. i'm not aware of an implementation of C which compiles to run on the .NET [CLR](https://en.wikipedia.org/wiki/Common_Language_Runtime), but would be happy to be enlightened on this point. 
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://en.wikipedia.org/wiki/Common_Language_Runtime) - Previous text "CLR" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
You can "don't agree" all you want, it's still touchy look at all the missing main threads out there. It's not anywhere near as straightforward as -lSDL by any stretch. But like all the other platforms once you get it to link then it's straight forward.
&gt; Whatever IDE I use, I want it to compile to PURE C! An IDE is not a C compiler. Remember that. Many programmers don't use any IDE at all, including me. I recommend you to do the same in the beginning so you get a hang of what happens when you compile a program. I also advice you to ditch Windows in favour of a UNIX-like system like Linux as it's much easier to develop C code on these. UNIX-like systems usually come with a C compiler preinstalled. Type `cc`, that's your C compiler. &gt; I just need to perform the following BASIC things: This looks like you don't actually want a GUI library. A GUI library does things like drawing buttons and text boxes and reacting to the user interacting with these GUI elements. What you seem to want is a graphics library like SDL. &gt; So I would love to hear your thoughts on your favorite database platform to use with C? SQLite is very robust and the easiest database to get started with. As opposed to normal databases, a SQLite database is just a single file. You don't need to set up a database server and do complicated configuration. SQLite has very good documentation and is easy to learn.
It is exactly as straightforward as that.
[removed]
Use Visual Studio 2017 community. Free and easy to use since your on Windows. I would go to http://lazyfoo.net/tutorials/SDL/ next to set up SDL to do the basic graphics stuff you want to do. I have not connected c to a database or used speech. I will not recommend anything for that.
I know it's been a drought on Linux basically ever since Visual C++ was a thing on Windows. We all started as total n00bs once too. It took me using GCC with the -v flag and running each part of it manually to even get any grip on what the hell is going on. DOS is great for that low level access to hardware. Linux used to have SVGA direct access, although I haven't messed with frame buffer in a long long time. At the heart of it, all you need to do is edit a text file. Have you gotten hello world at least running? I'm old, I just use vi, gdb and a whole lot of terminal windows.. and Visial Studio. I don't know how or why people suffer through Unix dev, although the stuff I have primarily done is to revive dead code, usually on stuff like Solaris, where they want a Windows client and a Linux server. People would fall as they would jump straight to the targets without even trying the last known build to see if errors were from the port, or always there... Its all small steps. Drop into a command prompt and compile hello world with-v. You'll see there is a lot of moving parts
Thanks sorry for the mistake.
I just saw this on hackernews and thought it might interest this subreddit.
Feel free to repost this as a link instead of a self-post!
Lol ok great for you then
I believe this argument is about the design of SDL, which depends a lot on macros and things being setup in the right way for it to work. It can be quite difficult to add SDL to an existing project because of the assumptions it makes. If you use or plan for it from the start, than it's quite simple to use, yes.
skeetooooooooooooooo!
It appears that you have been shadow banned. Please talk to the reddit admins to sort out this issue.
Trust me, I’d like to. It just seems anything that talks about things like linking isn’t really beginner friendly. It’s not that I’m lazy. It generally goes like this-I see a term I have no clue the meaning of. I google this term. The google result explaining this term has two terms I lack in. I google those terms. Those two separate terms contain two more terms I dont understand, and before I know it, I’m in over my head just doing stuff I have no clue about and just copying code. 
What about Gedit?
Tell me more about using GCC with the -v flag.
&gt; IDE I hate to be that guy. I really hate to be that guy, and I'm asking for your forgiveness for making this post, but have you considered using Vim or Emacs? They work for any language or library, they don't eat all your system resources, and you'll increase your productivity by about 75% by avoiding the unending tidal waves of bugs and issues like this. 
I would just like to reiterate how sorry I am for making this post.
From what I’ve read, Vim seemed like something way more complicated than what most searches gave as a beginner friendly IDE. I don’t know anything about Emacs, but I’m willing to try either one.
[removed]
* Even Microsoft haters have to admit it's VS by far, even when you don't know how to use it. * Since you will get objectively better answers for this one, I'll have to go with esoteric one: Win32 GUI. It'll give you familiarity with Win32 internals and imo, it's fun (go on, mock me haters).
The only reason why your comments are visible is because I manually unblocked them. But you know, if you don't want to fix this, you don't need to.
The first half of [this](http://www.openvim.com/) is about all you really need to know to use Vim, there are lots of extra features that you don't really need (cons: it's easier if you switch your capslock and escape keys, I have a script that does that for linux if you want). Emacs is *slightly* tougher to learn, but a little more comfortable on the fingers, but much much funner to script and modify.
I'm trying to install it using the Ubuntu instructions on Debian but I'm getting the following error when I call cmake: -- The CXX compiler identification is unknown CMake Error at CMakeLists.txt:3 (project): No CMAKE_CXX_COMPILER could be found. Tell CMake where to find the compiler by setting either the environment variable "CXX" or the CMake cache entry CMAKE_CXX_COMPILER to the full path to the compiler, or to the compiler name if it is in the PATH. -- Configuring incomplete, errors occurred! See also "~/Craft/CMakeFiles/CMakeOutput.log". See also "~/Craft/CMakeFiles/CMakeError.log".
That’s a neat tutorial on vim.
&gt; Raylib uses immediate mode OpenGL so it's not going to be the fastest thing on the block, but depending on what you're doing this may not be a problem Raylib can use OpenGL 3.3 Core profile only. See their OpenGL abstraction layer https://github.com/raysan5/raylib/blob/master/src/rlgl.h
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [raysan5/raylib/.../**rlgl.h** (master → 817ae07)](https://github.com/raysan5/raylib/blob/817ae075050b348e42223a567e7deaff42b6bb24/src/rlgl.h) ---- [^delete](https://www.reddit.com/message/compose/?to=GitHubPermalinkBot&amp;subject=deletion&amp;message=Delete reply e0fn7z1.)
[removed]
I think I have raylib installed. Can you give an example of how I would compile my source from the command line now? On github it says “to compile one specific example, add flags as needed like this” make core/core_basic_window PLATFORM=PLATFORM_DESKTOP This isn’t how I’m accustomed to compiling my code. I just GCC code.c -o code for example.
Drawing widgets: gtk Drawing pixels: sdl2 IDEs are crutches, stick to an editor and a terminal window for cc and gdb. I use vim but admit that vscode is a really nice editor too.
Use emacs or nano
If all else fails you can output text-based PPM and convert using ffmpeg/ImageMagick/Netpbm or other tools. [[info](http://netpbm.sourceforge.net/doc/ppm.html)] If you can draw to the screen do that instead/also, this is the lowest common denominator. Something you might do in a command line terminal running on a remote host... It also means 100% CPU rendering of geometry hand-rolled from scratch. No shaders, viewports or even pre-provided line drawing. No fonts unless you emplement that too. Any result you make on your own with the help of books and research is going to suck. :/
1. This isn't C. 2. Your code calls both `userinput()` and `computerinput()` before they are declared or defined. That's forbidden in C++. It's allowed in C under certain conditions, which your code violates.
No one has said Cairo yet. I've used this very simple library to great effect to produce still images.
I've got Raylib installed. So how do I compile my source using this? Here's the example from github where I followed instructions to download. "To compile just one specific example, add flags as needed:" make core/core\_basic\_window PLATFORM=PLATFORM\_DESKTOP Can anyone explain what is going on with the above direction. The way I have been compiling any code I write is: **GCC codeexample.c \-o codeexample** That's what I do while in the folder that my code is contained in.
[imgui](https://github.com/ocornut/imgui) or [nuklear](https://github.com/vurtun/nuklear) might be good options for your GUI.
Thanks for your response MillerTheDeer! My apologies as I know my questions are highly newbie in nature! But yes, I just wanted to also ask you: I took a fast look at that SDL link you provided. It looks like exactly what I am looking for. The sample code in that tutorial seems to be using a lot of C++, while I would like to stick with C (which I'll be learning over the next few weeks/months). HOWEVER... I'm guessing I can pretty easily adapt that to C, and that the parts which call the SDL-Library, are simply call outs to functions? If so, then when I'm learning to use the SDL library, I just have to simply call those functions with C-code, instead of C++ code, right? 
Indeed, I wouldn't mind using a simple text editor actually as you suggest! I already do almost EVERYTHING in my life in nice plain simple NOTEPAD anyways! I like to keep the workspace nice, plain, simple, and minimalist! -------------------------------- So, yes... if I use notepad at first, and save the code, then I assume the next thing I have to do is just point a C compiler to the text file and run it? Immediately after that, will the C compiler create a .exe file for me to run? And that's it? ------------------------ As for the C compiler, does Microsoft Visual Studio come with one? Or is there another C compiler that I should download which you would recommend for Windows 10? (Apologies, as I know my questions are painful newbie questions!) I downloaded the CODEBLOCKS IDE earlier today. I wonder if that comes with a C compiler, that I can use to compile my c programs (without me having to use the CodeBlocks IDE)? I'll have to check that out. 
Ah yes, as a complete-newbie I totally used the term "compile" incorrectly. Apologies about that! So yes, if I understand correctly, the compiler takes the C-code that I write (usually as plain text), and then turns that into machine code. In the more distant past I once tried to write a C program, and the compiler that processed it was for C++, rather than purely being dedicated for just C alone. So I wondered, at that time, if a C++ compiler like that was adding "a bunch of extra stuff and junk" to the machine code, in a C++ kind of way? In other words... if I compile my C code using a C++ compiler, will it compile it in EXACTLY the same way that a dedicated C-only compiler would do? 
Thanks! I'll check that out! 
It's interesting you mention Win32, as I found a book just a few days ago, that is all about the Win32 API. So... if I'm using Windows 10 64 Bit, I assume I can still call the Win32 API, right? Also... I've never called API's before in my life. I'm assuming that API's are a bunch of functions, that you call, and pass values to? So the book I downloaded will probably just teach me a series of API functions, that I can call with a programming language like C? Or maybe I'm totally misunderstanding how interfacing with the Win32 API works? (Sorry again for such Newbie questions!)
Oh yes this never gets old. So fucking clever.
DOS (real or emulated) means dealing with pallette registers, video modes, pixel planes, sometimes only 16 colors... No, just no.
Thanks for all your tips FUZxxl! SQLite sounds EXACTLY what I'm looking for. As you said, at this stage, for now, I don't want to have set up a complicated database server, and go through a big series of painstaking, frustrating, hairpulling configurations! So SQLite it is! Also you recommended a Unix-like platform. I hate to admit that my only in depth OS experience is with Windows. But I've seen youtube tutorials on things like fun hacking projects with Kali Linux, and I've been itching to dive into a Linux-Unix operating system. I also just got a book about learning the Bash command line. However, for this C project that I want to do, I would have to be on a Windows machine. Off hand, not sure if you know of any good C compilers for Windows 10? If not, then no worries. I downloaded the code-blocks IDE for Windows earlier today, so I'm not sure if it comes with a compiler... I'll check that out later tonight. Finally... in the very distant past I had written a C program and compiled it with a C++ compiler. But I've been wondering: will a C++ compiler, compile C code in the exact same way as a stand-alone C compiler would do? Or does the C++ compiler add a bunch of extra C++ related "junk" to the compiled machine code? (In other words, I want the final machine code to have been derived from "pure" C-code only.) Sorry for being such a Newbie with my annoying questions! 
`getc` is not deprecated. `gets` is, though. And you really don't want to use it. You could use `getline()`, but the effect is subtly different. Namely, `%s` stops at the first white space character whereas `getline()` reads until the next newline. You might get the desired effect with `getdelim()` though. You could also use `open_memstream()` to get an automatically resizing buffer and use that to accumulate the input. You could also just implement your own dynamic resizing code, it's not that difficult and you only have to do it once.
Yeah you have the idea. SDL is in C. The tutorial is wrapping it in C++ but you can ignore it.
Perfect!
No, it's not easy. We are talking here about a complete beginner who would just like to toy with some very basic graphic output. In the 80's: * switch the computer on * type `LINE (50,100)-(200,150)` * press ENTER Linux (and other contemporary OS) in the 2010's: * boot and start X11 * figure out which graphic library you should use * install SDL library * figure out you the development version * open an editor/IDE * #include SDL header * figure out you need to `init` the lib * figure out if you should use SDL_gfx or not or yes or no * figure out you need to create a window * figure out you need a renderer; what's a renderer? * figure out how to specify a colour * save * try to compile * hmmmpfff, time to learn about linking * compile, link and somehow run * figure out why you don't see anything or it disappears in a blink * learn how to 'pause' the result, or learn about SDL 'events' * end up with a 30-lines program * compile, link and run That's like several orders of magnitude more complicated. Of course it is relatively easy for someone with a bit of experience. NB: MS-DOS / Borland would stand in the middle of the scale. More complicated than on 8-bit computers, but once you have the Borland Turbo-something IDE installed and lauched, more much straightforward than current OSes environments.
&gt; Finally... in the very distant past I had written a C program and compiled it with a C++ compiler. &gt; &gt; But I've been wondering: will a C++ compiler, compile C code in the exact same way as a stand-alone C compiler would do? C and C++ are different languages. If you compile code with a C++ compiler, it is interpreted as C++. If you compile code with a C compiler, it is interpreted as C. Some times, both compilers do the same thing, but usually there are a number of differences that can break your code. Never ever compile C code with a C++ compiler. Just don't do that. Most modern C compilers have both a C and a C++ mode. Make sure to select the appropriate mode when compiling your code. Check out your compiler's manual for details.
&gt; So, yes... if I use notepad at first, and save the code, then I assume the next thing I have to do is just point a C compiler to the text file and run it? Yes, exactly. &gt; Immediately after that, will the C compiler create a .exe file for me to run? And that's it? Yes, exactly. Depending on what compiler you use, it might merely produce an *object file* which you have to *link* using a *linker* to get a binary. On UNIX-like systems, the compiler automatically invokes the linker for you unless you tell it to abstain from linking using the `-c` option. Microsoft's C compiler has the two commands `cc` and `cl` for just compiling and compiling/linking. Check out your compiler's manual for details.
One last annoying question for you MillerTheDeer: I was taking a look at some Python tutorial videos recently (although I haven't yet learnt Python. I'm going to learn C first. I really like the minimalism of C... something about it speaks to me!) Anyways, some of the interesting Python GUI libraries are written in C. So does that mean I could use those libraries with my C program? In other words: can I just watch the Python tutorial video, and look for the parts in which they pass values to the library using functions... and then just call those exact same functions with my C code (instead of using Python)? If yes... does that mean that a lot of other libraries written in C, for Python, can be called using C code instead? If so, that would be pretty awesome! That way I could just use all those Python libraries, but call them with C code instead (since I seem to really like C code better than Python!). 
Cool. Do you have an example? Do you risk that hierarchies get artificial (grouped by arbitrary metric)? I want to see a problem where OOP definitely is a no-brainer. If some classes happens to need a common functionality, does the functionality get duplicated or does more primitive/orthogonal things get to live their own lives? If functionality can stand on its own, we must they be grouped in classes? I have obviously done very little OOP (except some introductory courses), but I'm trying to understand how a problem cannot be analyzed into smaller parts, let each thing do one thing well, and just combine it *when needed*.
&gt; So... if I'm using Windows 10 64 Bit, I assume I can still call the Win32 API, right? Yes. &gt; I'm assuming that API's are a bunch of functions, that you call, and pass values to? Yes. &gt; So the book I downloaded will probably just teach me a series of API functions, that I can call with a programming language like C? Maybe. I haven't seen your book. &gt; Or maybe I'm totally misunderstanding how interfacing with the Win32 API works? Doesn't seem like it. &gt; (Sorry again for such Newbie questions!) Instead of asking these questions, it might be more productive to start with the book first, asking questions only when you get stuck. Most of your questions should be answered within the first few chapters of any decent books and most of your confusion is going to be cleared up by then.
I guess my question is then: Let's assume I'm using `getline()`, and attempting to build a dynamic resizing code; the parameters for `getline()` include knowing the buffer\-size for max output; how can you maintain that input past the buffersize (so I'm not always allocating, say, 10000bytes for the purpose of argument) while reallocating to increase your buffersize? Because for some types of input, it's quite possible, and `getline()` would work beautifully; but reading through documentation, I'm having a hard time not believing it would just truncate anything past the buffersize, and I'd have to realloc before accepting input; because the code would look like this, if I understand it correctly; `getline(string, 10000, STDIN);` If I could omit buffer limits and build dynamically from that, storing it later in a malloc'd location, I'd be golden. And please tell me if I'm missing the point and looking at it from the wrong perspective, because at this point I may have tunnel vision.
Is it a superset of C89?
The original proposition was &gt; SDL can be a bit touchy to set up, especially if you are hiding behind some IDE. Especially if it's Linux. which I believe is wrong. I don't see how your comment is relevant to this proposition. Yes, doing graphical stuff is more complicated today, though you conveniently left out the part in your “In the 80's” section where you had to drive to the library and hopefully get a book on graphic programming for the exact home computer you bought. And then, when something goes wrong, there is no forum to ask on, no debugger to help you understand your problem and usually no good documentation for the internals you could look at. If you were lucky enough to buy a popular home computer, you could by expensive books written by people who had reverse-engineered your computer's graphic processor to gain at least a slight understanding of how this actually works. Oh yeah. And if you did something wrong, your computer would crash and all your progress since your last save to cassette would be erased. Is this really any better?
I'm not sure if you can use python libs in c. But you will be able to find anything you need for C or at least c++ with a binding back to c.
&gt; Let's assume I'm using getline(), and attempting to build a dynamic resizing code; the parameters for getline() include knowing the buffer-size for max output; how can you maintain that input past the buffersize (so I'm not always allocating, say, 10000bytes for the purpose of argument) while reallocating to increase your buffersize? You have misunderstood how `getline()` works. Please re-read the documentation. &gt; because the code would look like this, if I understand it correctly; getline(string, 10000, STDIN); No, that is not correct. Please re-read the documentation.
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions.
Use a library that fits the purpose. There are a lot of different c compatible hashmap implementations.
&gt; Write a hashmap for every project, or use libraries? If I need a hashmap, I write my own. This has rarely been the case though.
More so than C99, but still not quite. https://stackoverflow.com/a/1201840 has some examples
You may implement a tagged union with enumeration and union type. You can then use it "almost" like generics using void*.
Thanks FUZxxl! 
That's true. I'll try to minimize the newbie questions, until I've read a lot more. But thankfully the questions and answers here really cleared up a lot for me. So I now feel a lot more grounded and confident as I am about to dive into the books. 
Phantastic! Let us know if you hit any more roadblocks! There is no fundamental problem with newbie questions, but it's better for you to first follow the tutorial for a bit as many of the answers you get here won't be immediately helpful without some context you learn in the tutorial. Also, I can really recommend reading the manual of all tools you use. That's a very good way to get familiar with them.
Still requires you write your own hashmap right?
That's interesting. You rarely need hashmaps? 
Yes. Edited my reply.
nullprogram.com has lots of fun shit! I like it. &gt; The library mustn’t call malloc() internally. &gt; Define at most one structure [in the interface] I don't know why this is so pleasant.
Yes, indeed.
&gt;OK &gt; &gt;[Crap](https://i.imgur.com/pCX3L6U.png) 
It looks like you might need to build the dependencies in the `deps` directory first.
The header file is called `&lt;SDL2/SDL.h&gt;`. You are being intentionally obtuse.
A string type that lets you append to the end, automatically growing its storage space as needed, is a useful thing to have in your toolbox. Easy to write or you can use an existing one...
Short answer is https://dashgl.com. It uses GTK as a Window library and provides basics examples for building Brickout, Invaders and Astroids using a very minimal matrix library for drawing with OpenGL. If that's not your cup of tea. Then there several other resources worth taking a look at: 1. This is a really basic example that uses GLUT and OpenGL 1.0 to draw some basic geometric shapes. If you want somewhere to get started just to get something on the screen, this is the place to start: https://www.ntu.edu.sg/home/ehchua/programming/opengl/CG_Introduction.html 2. The wikibook for freeGlut also has some examples for starting with a triangle and working up to a cube. The tutorials are technically written in C++, and only the matrix manipulations are done with a C++ library called GLM. Translating the C++ library into C code is good programming practice, and a good way to learn about the matrix transformations used in OpenGL programming. https://en.wikibooks.org/wiki/OpenGL_Programming 3. If you want to just use the C version of the above tutorial, you can check out my repo on Github here: https://github.com/kion-dgl/3D-Cube 4. OpenGLbook.com provides some basic examples. I haven't gone through these personally, but the code is in C and it presents a good balance of providing enough concepts for the reader, and providing enough code to make everything understandable. http://openglbook.com/ 5. If you want to learn concepts, then you should check out this page. It has tons of graphics, diagrams and explanations of how the pipeline works, how cameras work, vertex buffer objects and all of the fundamental concepts for working with OpenGL. http://www.songho.ca/opengl/ 6. Lots of people on here recommend SDL, And I find it weird that despite SDL arguably being the most widely used library, tutorials in it for C are few and inbetween. So far the best resource I've found for SDL in C is this page: http://www.parallelrealities.co.uk/2011/09/basic-game-tutorial-1-opening-window.html, but I haven't had the chance to go through it yet. Lastly for a programming environment, if you're on Linux I recommend you use a basic text editor, like gedit, mousepad, vim, emacs, nano, and a command line. You can make a short .sh or make file to save the command for building your project. And then you can type, run the command to build and test. And potentially use git or something so you can always go back to when something was working in case you get stuck.
I can't come up with any examples off the top of my head, but it's usefulness is pretty self\-explanatory. Strace is a Linux tool and an equivalent will be available for other UNIX systems. It basically attaches itself to a process and watches what it does in terms of interactions with the operating systems (system calls, signals, etc.). For a beginner, all you have to tell them is to install strace and run it as it needs to be run, and post the log somewhere. For example, if I build my .c file to some executable called MAINEXE, I'll do \`strace \-o logfile\_output MAINEXE\` and have a general overview of how the programs interacts with the OS. Possible \[equivalent for windows but never used it\]([https://docs.microsoft.com/en\-us/windows\-hardware/drivers/debugger/logger\-and\-logviewer](https://docs.microsoft.com/en-us/windows-hardware/drivers/debugger/logger-and-logviewer)). \[strace explanation\]([https://stackoverflow.com/questions/174942/how\-should\-strace\-be\-used](https://stackoverflow.com/questions/174942/how-should-strace-be-used))
It is nonsense. bp is uninitialised so dereferencing is not allowed. And casting it to a different type makes no sense either.
Actually, it's initialized, but I can't craft a good example since I can't understand how it works. I found this line in the CPython source code. https://github.com/python/cpython/blob/222f7f40339238b3d2c803849c75e682725449d7/Objects/obmalloc.c#L1364
It looks like you don't have a C++ compiler. Try 'sudo apt install g++'
If you can live with its restrictions, there's always [hsearch()](https://linux.die.net/man/3/hsearch) from the posix standard.
* It was renamed to Windows API because of that confusion, since in summary, it is what you use if you want your program to work on pretty much every NT version of Windows (in practice, it's more nuanced than that, but it's true in general). * Pretty much, yes. * Yes. A little different from regular APIs since it's not object oriented, so not every interaction is through a already given function, but yeah. * No, you got it right.
I think you should read about memory management and virtual memory. Even if a pointer's address value is same in 2 processes, they are 2 different physical addresses. In certain implementation the pointer in a process is an offset from starting address of The process in RAM. To oversimplify just to explain this, If you process p1 at address 1000 and process p2 at 2000, then if a pointer ptr1 is inside both of them and has value 200, the actual address for ptr1 in process p1 is 1200 and for ptr1 in process p2 is 2200. I repeat I have oversimplified . This is handled by MMU.
We were talking about embedded systems. Vtable implementations in cpp are non trivial because of multiple inheritance and anonymous classes, they aren’t just references to function tables like you might see in other languages. I’ve done development with TIs embedded systems which have an implementation of gcc (and so they support cpp). But there’s no reason to use it because like I said, no vtables, no OO stuff, no functional stuff, no exceptions etc... so you may as well just use C
You can also look at apache portable runtime. A library that provides threads and related primitives for both window and linux Https://Apr.apache.org
Ive had issues with openssl allocating memory internally that blows up valgrind because of whatever internal trickery it's doing. It's out of my control and irritates me.
I like it simple: IDE: Notepad++ or VIM, gnu make, gcc GUI: Iup lib from tecgraf. I wrote a UI for my Software Renderer and Image synthesis algorithm for procedural textures. All of them statically linked inside of 2.3 MB Binary, i like this lib.
Exactly. That's why it's "almost" a superset.
Thanks for that tip on IUP Portable User Interface. I'm checking out the website and sample code-projects right now. That one is very interesting! Nice and simple as you said. 
The objects in Python are pointers to structures that are allocated on the heap. The data that they reference is a pointer to that piece of data, in whatever representation. They are no different, except in how these abstractions are classed in your mind. 
I use `gedit` as my "IDE" ith a terminal window to run compile scripts/see debug output
I chuckled. Now you can see yourself out.
I've found this document by the designer of C\+\+ that describes how multiple inheritance could be implemented ([https://www.usenix.org/legacy/publications/compsystems/1989/fall\_stroustrup.pdf](https://www.usenix.org/legacy/publications/compsystems/1989/fall_stroustrup.pdf)). It doesn't look like it needs any special runtime handling. Are there more complicated cases that do require some kind of special runtime support? I don't agree that C\+\+ would be worse than C if you can't use exceptions/vtables/OO, you'd still have a bunch of features that make C\+\+ great, like RAII/classes/function overloading/templates/better type safety/constexpr.
Thanks!
[Here](https://blogs.msdn.microsoft.com/oldnewthing/20140403-00/?p=1333) is an "explanation".
I have heard it said that you should avoid using anything more complicated than an array wherever you can. Small size arrays and maps have relatively similar performance, so use an array until it starts to become a performance issue.
What you've said is true, you can still organize code in classes without using inheritance/virtual functions etc which would probably result in better/more organized code. C++ definitely has a lot of good features, but when you switch to a language as large as it you get the good with the bad... The major issue with objects is the runtime support for dynamic allocation, in my experience with embedded systems you try and allocate as much as you can statically. I will still stand by my earlier statements though. I dont see any features of C++ that would make embedded development easier/better than using C. I find that a lot of people try and use newer and more fully featured languages for no purpose, if all you are going to do is control the state of something, call functions, use primitive types, there isn't a reason to add so much complexity. This is especially true if you want your code to be tightly integrated with assembly.
There is no way to represent polymorphism in C*. So you would basically have to use another language that supports it. OOP is a simple way to apply abstraction to a given problem. If you are programming a graphics library, at some level you dont need to worry about exactly what shapes you are drawing. You can program around the assumption that you are handed something drawable, and you draw it. It makes programming/organizing large projects easier because you can program around certain abstractions of the problem.
CXX compiler is the cpp compiler. otherwise known as [g++](https://packages.ubuntu.com/trusty/g++-4.8).
It seems that your comment contains 1 or more links that are hard to tap for mobile users. I will extend those so they're easier for our sausage fingers to click! [Here is link number 1](https://packages.ubuntu.com/trusty/g++-4.8) - Previous text "g++" ---- ^Please ^PM ^/u/eganwall ^with ^issues ^or ^feedback! ^| ^[Delete](https://reddit.com/message/compose/?to=FatFingerHelperBot&amp;subject=delete&amp;message=delete%20ID_HERE) 
It's called using the debugger to step through the code and I don't even have to donate.
We know that, and we're catching up fast. In fact there's a [rust-clippy issue](https://github.com/rust-lang-nursery/rust-clippy/issues/2227) to create lints for many MISRA-C rules (when interpreted in a Rust context).
what about busybox?
&gt;One common way to avoid complicated types in an API is to make them *opaque*. \[...\] However, this is difficult to pull off when the library doesn’t allocate its own memory. Exactly. Which is why avoiding malloc() in libraries is generally a bad idea. Of course you could avoid malloc() and still have opaque types by having custom allocators passed to the library, but this again makes the API unnecessarily complicated.
Search easyx, I am using it ,simple, very small and easy to install. You need installed VC++6.0 or VS2013∽2017 first, then download here https://www.easyx.cn/down.aspx?id=9&amp;no=0 only 627KB, when it is completed clicking that .exe file and follow steps, or install manually open that file with rar release files .h in include folder to your_vs/vc/include, files in .c to your_vs/vc/lib. After finished, you can open VS and create a c++ console project, include "graphics.h" include "easyx.h" then start your drawing use functions they offered.
You might like "boringssl".
GNU Emacs! haha [https://imgur.com/a/zVqSVTI](https://imgur.com/a/zVqSVTI)
16 colours? Vga mode 13 is 256. SVGA VESA goes to 32bit deep. Palllette registers are hard? Lol, are you kidding? Get out of CGA and VGA is where it's at. 
No mention of sdl-config, that include is going to bomb.
&gt; int #p; wat
You're going to wrap around...Uhh, I'll just show you what prints &gt; 2 bottles of beer... 1 bottles of beer... 0 bottles of beer... 4294967295 bottles of beer... 4294967294 bottles of beer...
&gt; skeetooooooooooooooo! Is this a reference to something? I've seen other people post similar comments on skeeto's articles, but I don't understand what it means.
Nice tips! Minimalism is at the heart of C so these are useful things to keep in mind. Reminds me also of [certain](https://www.safaribooksonline.com/library/view/masterminds-of-programming/9780596801670/ch04.html) [principles](http://wiki.c2.com/?SimplestThingThatCouldPossiblyWork) from Forth. This line confused me a bit though: &gt; Putting `FILE *` pointers directly into an API mingles it with the C standard library in potentially bad ways. Do you mean bad in terms of minimalism?
Your Welcome, some parts are a little bit tricky, i think, but if you will spend a little bit time it would be great. 
Not the author so IDK :).
Oops, sorry. Cc /u/skeeto (I think that's how you mention someone on reddit)
it is working fine... I tried it with 12.30 and 12.34...
That part doesn't have anything to do with minimalism. It's almost the opposite: Since minimalist libraries are generally embedded, this issue doesn't affect them. Occasionally a library may be linked against a different C standard library than the application that's calling it. They will use the same `FILE *` pointers, but these actually point to incompatible structures. The same issue comes up when the library allocates with `malloc()` and the application calls `free()`. That problem can be avoided by having an API to pass the pointer back into the library for freeing (e.g. some sort of destructor). This issue is a lot more common on Windows than Linux, where the operating system doesn't provide a C standard library (though there is now the "Universal CRT"). An example on Linux is a shared object dynamically linked against glibc and an application statically linked against musl. Each could even statically link the same version of the same libc, but they're still incompatible since they don't share internal static variables. Historically this has been a frequent issue with C++ since there is no standardized ABI outside of `extern "C"`. Passing a `std::vector` between library and application requires that they've both been carefully compiled compatibly. This is easier to avoid with C. 
Dont mind! I am a long time skeeto stalker! haha
Please consider not reposting your projects in quick succession.
Apologies... will keep that in mind
How does C do better at representing the hardware than D, Rust, or C++? 
One wouldn't use `new[]` in modern C++ anyway. And custom allocators can be used for all the STL containers - actually easier than wrapping/#defining/whatever malloc/free. And this is C++-specific - it doesn't apply to D or Rust. I don't understand what you mean by the "how people actually write code" paragraph.
Depends on how you define "large". For the sake of argument, let's say that *all* of the D standard library requires the GC. So now you can't use any of the standard library if the GC is unacceptable for you application. Ok, but if the alternative is C then all you have is the C standard library anyway. And since you can call the C standard library from D, you can do all you'd be able to do in C anyway and *more*. And that's based on the false assumption that none of the standard library is available with no GC. Stick a `@nogc` on your main function and that's that. You'll know if any of the code you're trying to use requires the GC since it won't compile.
I don't think the point is to claim that some non-assembly language is better at C at representing hardware. Is that the alternatives can all represent the hardware at the same level of abstraction. i.e. C isn't the only choice, and there's nothing you can only do in C that wouldn't be possible in D, C++, or Rust. I'd love to be proved wrong.
How is it closer than C++? Objective-C? D, Rust, ...?
Your claim was that "no language comes close to even touching C when it comes to the machine level implementation". C++ isn't C. And neither are the other two languages I asked about.
C++ isn't C, and the claim was that no other language comes close to C.
Because most modern languages include hasmaps, I feel like people go too far in using them as a "solve all your problems with this one neat trick" data structure. In most cases where they are used, I would consider it premature optimization (which ironically slows down the code in question).
I've noticed that file system access and process startup on windows 10 is extremely slow. The fastest C compiler I've found for win10 is MSYS2's version of clang. If you install that you might have a better time. That said, compiling my C engine (about 12,000 loc) on my new 7700k + NVMe drive + win 10 machine takes about 9 seconds. On my 2015 MacBook with an atom processor it takes about 3 seconds using clang. 
&gt;I don't understand what you mean by the "how people actually write code" paragraph. If you ignore new/delete, you could write a hell of a lot of C from within a C++ compiler, which people do to various degrees. So it depends how you write your C++ as to how much you end up triggering bullshit C++ behaviors.
Oh, okay, thanks for clearing that up. Those are good things to keep in mind for writing cross-platform APIs.
I'm not sure about D, but Rust and C++ generally generate more asm per line of code (unoptimized, and using a breadth of features), so to me thats a farther abstraction away from hardware as well.
%s is for a string %c is for a single character `printf` won't print a newline if you don't supply it.
"massivebrain" and can't fix a simple C logic error?
BTW, you can't print your char with a %s. 
You have numerous problems. For a start, initialize your board, `x`, presumably to contain all spaces - i.e. `' '`. Next, use `%c` instead of `%s` in your `printf` format string.
`x[0,0]` This is not the correct syntax for dereferencing a 2D array. You have it right in `userinput`
So, ok, a few general things to point out about the code: 1) Be explicit with your function prototypes. fun() doesn't mean that 'fun' takes bo arguments, but fun(void) does. 2) Use srand only once at the beginning of your program. Each call to computerinput would likely produce the same two random numbers, because you're reseeding it with the same value (assuming, this happens in less than a second). 3) Please, get rid of the global variables where possible. You don't have to declare variables like 'randomnumber1' in global scope, because they are used only within one function. Declare these variables closer to where they are used.
A recommendation I'd pass on is to not typedef function pointers. It totally obscures what a 'compare function' signature should look like. I understand that it looks prettier but it is an unecessary abstraction. 
Jedwardsol mentioned the problem with arrays in printboard, but I think it's worth going into that a bit deeper. The "expression result unused" warning is likely because of the comma in the bracket. Commas in C, when used outside of function calls and list initialization, etc, evaluates to the value of the last entry. So, x[0,1] is the same as x[1]. That leftover 0 is what the warning's indicating.
Could be a joke name. Are you actually a Tyrannosaurus Pizza?
Let's play Global Thermonuclear War.
Please be civil.
One of the more unfortunate possible outcomes of undefined behavior: global annihilation.
No, I'm actually a pterodactyl pizza but that was taken.
So true.
Not related to the problem but you should only call `srand(time(NULL));` at most once per program. Also the computerinput function writes out of bounds; array indices start at 0 
`gets` was removed in 2011. It was deprecated between 2007 and 2010.
The `printf` causes undefined behaviour due to incorrect format specifier. You should cast `w` to `unsigned long long` if using `%llu`. 
 int counter Doesn’t put zero in counter.
I buy my guns from a guy named T-Rex. He's a small arms dealer.
If you write assembly directly (e.g., in a .s file), you're realistically limited to only writing complete functions. Using gcc's inline assembly allows you to write only part of a function, or at the very least skipping the usual function preamble/postamble (e.g., setting up the stack frame). It also opens up the possibility that the compiler can inline the function as an optimization.
It's not a bad try. Not a big deal not using a `for` loop either. For a grid this small that was simple but effective. The other commenters pointed out minor things probably tripping up on. I would consider maybe putting numbers in empty spaces to make input easier... That might not be a good idea actually. :/ Oh, could do end of game checking... Is the game guaranteed to finish by 5 moves? Input (both human and computer) should check if space is already filled. C arrays are zero indexed so you don't need those `number % 3 + 1`-'s, it's supposed to be zero sometimes. Don't forget to put a `\n` at end of lines or they'll run together (unlike Python's `print`).
on the contrary, I actually find it difficult to read without typedef. let's see what others have to say on this
The signature tells people what the comparison function should be like. I can't see anyone agreeing. But it doesn't have the exposure so I guess we will never know. 
A strange game. The only winning move is not to play. How about a nice game of chess?
Remember that if unaware developer use STL containers he will have dynamic memory allocation that is "hidden". Of course if you work on small embedded system and you want to disallow of dynamic memory allocation you can just simply create your "new" implementation and just give compiler error inside. In small embedded systems is good to stay with static memory allocation. Another "hidden" C\+\+ case is when you call object's method on platforms like ARM and you are aware that you can only pass 4 parameters with registers, but you forgotten that one of them is **this** pointer. From the other hand there are plenty of C\+\+ features that you have "out of the box" like memory mapped devices, oop that experienced developer can use to build better system. Of course you can implement "classes" in C with use of structs and function pointers.
Very True there. I mean borrowing your phrase, "the reincarnation of JS", look at how JS is widely spread in back\-end technologies. A language never intended to talk to the database is now a chasis of Popular back\-end technologies. I think , C\+\+ or C(i dont know about C), will will keep on evolving....and probably be a digital fossil......when writing code for device drivers will be a WYSIWYG technology....
I know they're allowed, but I don't like function declarations without parameter names. Take this line from sort.h: void quick_sort(void*, size_t, size_t, compareFunction); As a user of the library, I don't know what any of those arguments are. I can make an educated guess given the function name. In this case, I would assume that: * the `void*` parameter is a pointer to the array to sort, * the two `size_t` parameters probably are size of the array and size of an element, but I have no idea in which order, and * the `compareFunction` parameter is a function pointer to a comparison function. Looking at the implementation, you keep the same order of arguments as the standard library's `qsort` function. That's a good habit to have when implementing functionality that exists in the standard library. However, the argument order should be immediately obvious from the header file. Re function pointer typedefs: IMO, they're fine as long as they are declared somewhere obvious.
Do you have any data about D, Rust, or C++ generating more asm per line of code? If they do, in a way that ultimately matters?
You don't need empirical evidence to know that, it's part of the compiler. The C you write reflects assembler more closely. It's less abstracted from the hardware.
This subreddit exists to discuss the C programming language. You will likely have better results over at r/asm. As I don't tend to write assembly directly, I'm not aware of any advantage to using `gcc` (the GNU C compiler) over `as` (the GNU assembler) other than that `gcc` appears to invoke `ld` (the GNU linker) for you, while as far as I'm aware `as` will only assemble. I suspect this because `gcc` uses `as` for its assembly. Any optimizations that `gcc` performs likely occur during the compilation stage, which comes before assembly, when it is converting C to assembly. Bear in mind I'm not an expert here, so if I've said anything wrong or misleading please let me know. Hope this helps.
When the header is this short and the typedef is in plain sight, I think it's fine, especially considering it's repeated a few times, however I don't think function pointer types are harder to read when inline, and especially in larger headers, not having to go look for the typedef it also welcome. As far as not naming the function arguments as another user pointed out, I also think it's fine when a group of function take the exact same arguments, as long as it is documented above the function group, but in this case it is not. something like: /* The following group of sorting functions take a pointer to the array to sort, * the number of elements in the array, the size of each element, * and a pointer to a comparison function that can operate on array elements * If (compareFunction(a,b) &gt; 0), a will be placed before b in the sorted array. */ Although good variable names would make most of this obvious, which improves usability.
Definitely. Not doing *THAT* again.
&gt; You don't need empirical evidence to know that I always need empirical evidence. &gt; it's part of the compiler What's part of the compiler? Are you saying that if you use LLVM as the backend that it'll generate different asm for C, C++, D or Rust code with the same semantics? Again, where are the examples? How would that even make sense? &gt; The C you write reflects assembler more closely. It's less abstracted from the hardware. Try compiling `int add(int i, int j) { return i + j; }` with optimisations turned on. 
I've tried your suggestion and u/dumsubfilter 's suggestion, and while the errors have been eliminated, it just... doesn't... run! my program is impossible (emphasis on the "pos")
thank you so much. But, it still doesn't run, even after trying this and replacing the "%s" specifiers with "%c" specifiers. Help??
You're not initializing counter in your main(). Change "int counter;" to "int counter = 0".
I would like to share my works (all written in C for C), but since most of them are still in progress except math library, it is very early to announce them I guess :/ 3D math library: http://github.com/recp/cglm Other project can be found on my Github profile (recp). There are some cool projects e.g. render engine, importer, image, Apple Metal wrapper... I'll announce them when finished ;) After that, I think working with graphics, games and related-stuff will me more easier and fun for C developers.
I saw an image of ls with icons somewhere. Got really excited about that. So, last weekend, I started creating my own version of ls which includes coloring and icons. There are a couple of implementations of this already available - one made in Ruby, another with a modified (small modification) version of GNU Core Utils (no, I am not bashing any of those). This program, however, is made purely in C, and compiles to a simple, single, binary which can be placed in the user’s PATH, and it’s a rather small program, consisting of ~1k lines of code. Over time, I will be adding more icons and features (feel free to create Pull Requests and Issues on the Github link). And, it probably will have bugs, so, be wary of that. Hope you all like it :) GitHub: https://github.com/Electrux/ls_extended
Love this kind of project! A modern replacement for something used every day. The idea of icons is sweet, however, in order for them to be meaningfully visible to me, I have to increase my font size to way larger than I want. Kinda wish there were maybe some config options to the build, to adapt the formatting somewhat. Would also suggest adding some instructions to your readme, although the build process really was a breeze when you just run `build.sh`.
That would be very nice.
are the icons emoji or bitmaps? 
you might be surprised to learn that the c standard actually doesn't specify the actual 'bit pattern' of the basic types. for example, it doesn't actually force 2's complement for signed integers. this is left up to each implementation. also a python variable actually does specify how that variable is stored as data, under the hood - the python implementation (cpython is the most common) of course handles that. every python variable can actually be thought of as a pointer to a variable of that type, and every python variable has a type, and this type can never be changed, unlike in C (many people are confused about the python type system). therefore your revelation of why pointers need to have types is true for python as well as java and any other language that is implemented in C or C++. you aren't wrong in saying 'python is more oop than java'. 
they are part of the fonts - nerd fonts
Thanks for that insight! What kind of instructions would you want me to add? And ideally, you would have different fonts for your normal characters and the icons (Non-ASCII) characters. But, if that is not possible, you probably will have to find icons which have correct size of icons and characters :)
This is not a fruitful argument because, among other things, you are continually misreading my comments
Well I just had the case today while trying to write a linear algebra library, if you try to malloc or calloc a block of memory that is too close to the amount of physical memory installed on the system, the call will fail and return NULL, because malloc and calloc require the memory to be contiguous and even with overcommit enabled it cannot possibly give you more contiguous memory than is physically installed. In my case, if you're working with large dense matrices, like 100k*100k (a reasonable use case), that would be 10 billion elements which is 40GB if they're floats, 80 if they're doubles. By the way, calloc doesn't actually zero out all the allocated memory at least on linux, at first all addresses point to a single zero'd out page and writing to it will generate a new zero'd out page before writing to it if necessary, so you can actually calloc much more memory than currently available.
Wow this looks great! Is there a way to patch another font, say `consolas`, to add the icons you used here?
Your calling swap alot, and when you are each time your making a sys call to malloc which takes some time..
I'm interested in seeing how you implemented it, got a link to it?
&gt;you might be surprised to learn that the c standard actually doesn't specify the actual 'bit pattern' of the basic types. for example, it doesn't actually force 2's complement for signed integers. this is left up to each implementation. I actually am surprised to here that! I've learned about 2's complement in the last few days and it seems by far the best way to store signed integers in binary. But, I suppose that is quite the spirit of C: the whole freedom to do just about whatever you like. It's a pretty maverick language I've found, if that makes sense, haha. &gt;also a python variable actually does specify how that variable is stored as data, under the hood - the python implementation (cpython is the most common) of course handles that. every python variable can actually be thought of as a pointer to a variable of that type, and every python variable has a type, and this type can never be changed, unlike in C (many people are confused about the python type system). therefore your revelation of why pointers need to have types is true for python as well as java and any other language that is implemented in C or C++. Yes, this is I think a key concept I am just really starting to understand. I always new in languages like Java (for the reference types) and Python (for *any* type) that what variables actually stored was reference values that pointed to an object value. I always vaguely had the notion that these were basically pointers, but I never really knew. Now I understand, as /u/guynan put it, that variables are really just "pointers to structures on the heap". One thing that eludes me though is how exactly C implements that, because aren't functions in C not first class? Like, to implement an object, you could use a structure to store the data -- but how do you store the associated methods? I have not really delved into structures, so maybe they can store functions somehow, and I'll learn about that later. I've also seen talk that what we call variables in Python should really be called *names*. [Some people even go so far as saying there is no variable declaration or initialization in Python!](https://stackoverflow.com/questions/11007627/python-variable-declaration). Now, I don't know if I'd go that far, but the underlying idea is certainly correct -- variables are really just "names" for object. Aliases, if you will. Also, you say that "every python variable has a type, and this type can never be changed". What do you mean by this statement? I can easily change the type of a variable, I believe. Take the following time at the shell. &gt;&gt;&gt; a = 10 &gt;&gt;&gt; type(a) &lt;class 'int'&gt; &gt;&gt;&gt; a = True &gt;&gt;&gt; type(a) &lt;class 'bool'&gt; Or perhaps you mean something different. &gt;you aren't wrong in saying 'python is more oop than java'. Err, I only hesitated because I definitely think there are arguments to be made for each. For starters, I was only picking on java for it's distinction between primitive types (eg, int) and reference types (eg, a String). Java does have the wrapper class Integer for representing integers -- and that *is* most certainly an object. Furthermore, literally all Java code goes in a class, which is pretty evocative of OOP if you ask me. However, you could make arguments for Python as well. For example, as I mentioned, literally every single value in python is an object. And, python functions are objects too (which is not the case in Java). I think you could argue your case either way. 
You should have a look at function pointers! If you go on to do a bit of assembly, functions are just addresses in memory which in C you may take advantage of by passing this address around. It is syntactically different to how it is implemented in other OOP languages
I am starting to learn that, yes. I have a question for you though. We can't store functions in structures in C, right? So, how do we arrive at the concept of a pointer to an object in C? Wouldn't we have to store both data and functions to create an object?
You can store a function in a structure. It is just an implementation defined width pointer to an address in memory that contains a function. 
You can store a function in a structure. It is just an implementation defined width pointer to an address in memory that contains a function. 
OH, I see. So you store a function pointer, essentially? That's pretty neat! So, let's see if I have this right. Say I have a class called foo that has two data attributes, say a and b, and a method that returns the product of a and b (eg, return self.a * self.b). In C, would this look like a structure that stores two ints, and a pointers to a function that takes two ints and returns their product? So back in python saying a = foo(3, 4) would essentially be equivalent to initializing a structure with 3 and 4 and then storing a pointer to that function in the variable a? 
That's pretty awesome! In Python, we have first class functions. You think of a function as fundamentally just being a grouping of instructions, just as a string is a grouping of characters or a list a grouping of elements. Saying something like def f(): return 10 my_f = f in Python just stores the function f in the variable my_f. You can then invoke the function like my_f(). So would this mean that both my_f and f just store an address (a pointer!) to the sequence of statements in f?
Exactly right. While they are _slightly_ syntactically different, you can in effect achieve the same thing. I guess with slightly different paradigms the utility is different as well. I've delved into a lot of C-Python API stuff but I haven't looked into how they wrap this. I'll add it to my to do list. 
That's where the complexity heightens a little since there is no construct of the `self` pointer to the instance in C natively. You would have to build that by yourself somehow. 
You're not wrong. However, I think for some languages this site is actually really helpful. For example, for python scripts with lots of functions and recursions this site gives a really nice *visual* way of seeing what happens (especially the aliases created as the program executes). [Take this example](http://pythontutor.com/visualize.html#code=def%20binaryStringToDecimalRecursively%28s,%20count%3D0%29%3A%0A%20%20%20%20'''%0A%20%20%20%20Takes%20in%20a%20binary%20string,%20eg%20%2210001010101%22,%0A%20%20%20%20and%20returns%20it's%20decimal%20representation%0A%20%20%20%20'''%0A%20%20%20%20if%20len%28s%29%20%3D%3D%201%3A%0A%20%20%20%20%20%20%20return%20int%28s%5B0%5D%29*%282**count%29%0A%20%20%20%20else%3A%0A%20%20%20%20%20%20%20%20return%20binaryStringToDecimalRecursively%28s%5B%3A-1%5D,%20%28count%2B1%29%29%20%2B%20%28int%28s%5B-1%5D%29*%282**count%29%29%0A%20%20%20%20%0A%0AbinaryStringToDecimalRecursively%28%221010100010%22%29&amp;cumulative=false&amp;curInstr=0&amp;heapPrimitives=false&amp;mode=display&amp;origin=opt-frontend.js&amp;py=3&amp;rawInputLstJSON=%5B%5D&amp;textReferences=false). It can be helpful in understanding recursion I think. But yeah, once I learned about the debugger, this site did lose some of its awe for me.
That's pretty amazing. One of my goals in learning C is to one day be able to contribute to CPython. I still have a long way to go but it's nice to be heading that direction :).
I'd definitely have a look at the C-python API. The documentation is incredible and it's relatively easy to set up. Here is one of my projects which I started when I was semi getting the hang of the API. It's a very simple example that could get you up and running. www.github.com/guynan/finance
facepalm
&gt; char* strings are immutable because they are part of read-only data in memory. Actually, string literals (when not used to initialize an array) shouldn't be modified since it is undefined behaviour (they could be placed into ROM, Read Only Memory). They just have the type `char*`. &gt; char *myStr = (char*)malloc(strlen("Hello")+1); &gt; myStr = "Hello"; &gt; &gt; Using malloc, a new object is created in the heap that "indirectly" points to the same address in the pool as str1 and str2. myStr is equal to str1 and str2 when compared because they all point to the same address. I'm not so sure what you want to say. `malloc` allocates memory in the heap, and a pointer to it is stored in `myStr` pointing to the block of memory. It can't guess what you want to store, so it's never a pointer to a string literal. (modifying string literals, as said above, shouldn't be modified anyway, defeating the purpose of `malloc`). In the code, you then then you overwrite the *pointer* with a pointer pointing to the string literal. You actually never modify the memory returned by `malloc`. Since the pointer `malloc` returned is lost, it's effectively a memory leak. &gt; From here on out, every time a new object is created in the heap and it is assigned a string which already exists in the pool, the object will contain the address of the string in the pool. If the string does not yet exist in the pool then a new string is added into the pool. The newly created object points to the address in the string pool. (See above) Basically, C doesn't do such things by itself. There is no thing as "string pool" in C that is automatically managed. The compiler might (and probably) place strings together in one place in memory (which you could call pool, but it isn't managed at runtime). &gt; String literals are enclosed in quotations, but would a character array be considered as a string literal? No it wouldn't, but both can be classified as strings. &gt; Are ONLY strings declared by using pointers to char sent to a string pool, and not char arrays? See above. char arrays get their data at runtime and since you can change then at any time, they are in memory that can be written to. 
Thank you so much for your clarification.
Rob Pike's [rules of programming](http://doc.cat-v.org/bell_labs/pikestyle) are a close match: Rule 1.&amp;emsp;You can't tell where a program is going to spend its time. Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you've proven that's where the bottleneck is. Rule 2.&amp;emsp;Measure. Don't tune for speed until you've measured, and even then don't unless one part of the code overwhelms the rest. Rule 3.&amp;emsp;Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants. Until you know that n is frequently going to be big, don't get fancy. (Even if n does get big, use Rule 2 first.) For example, binary trees are always faster than splay trees for workaday problems. Rule 4.&amp;emsp;Fancy algorithms are buggier than simple ones, and they're much harder to implement. Use simple algorithms as well as simple data structures. The following data structures are a complete list for almost all practical programs: * array * linked list * hash table * binary tree Of course, you must also be prepared to collect these into compound data structures. For instance, a symbol table might be implemented as a hash table containing linked lists of arrays of characters. Rule 5.&amp;emsp;Data dominates. If you've chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming. (See The Mythical Man-Month: Essays on Software Engineering by F. P. Brooks, page 102.) Rule 6.&amp;emsp;There is no Rule 6.
You have to implement inheritance yourself. Are you sure you need inheritance?
Most of this could apply. &gt; &gt; Beautiful is better than ugly. &gt; Explicit is better than implicit. &gt; Simple is better than complex. &gt; Complex is better than complicated. &gt; Flat is better than nested. &gt; Sparse is better than dense. &gt; Readability counts. &gt; Special cases aren't special enough to break the rules. &gt; Although practicality beats purity. This is iffy. &gt; Errors should never pass silently. In robust systems you could have integrators that process m of n errors and can tolerate certain unfavorable conditions. You also don't want errors to reduce functionality unless specifically designed to. &gt; Unless explicitly silenced. &gt; There should be one-- and preferably only one --obvious way to do it. This seems wrong for most circumstances, even in Python. There are a lot of design decisions that choose between two good or sufficient solutions. If there is an exactly right way, it might not even be worth finding. &gt; Now is better than never. Now can easily be worse than never. Let's say that you have a feature that is nice to have but will kill your schedule. 
I've found that SDL works best as a bare\-bones graphics setup, but Allegro is a great alternative
The C language does not have a “heap” or a “stack”. Those concepts are specific to individual implementations. There is no “string pool” in the language either, and though I won't rule it out, I don't know of any implementations that use one, although it would be of limited use since strings are directly modifiable. String literals are usually intermingled with other preinitialized data, and may or may not be read-only.
I’m the person that made the version that was based off of coreutils, even-better-ls. Let me tell you that what you’re doing by rewriting it is a way better idea than my route. Nice job mate.
Which one of the Python list are in any way Python (or in many cases even software) specific? Extra points for Rob Pike’s rule four listed in another post where aside of some syntactic sugar to make `*(p+i)` look like an array reference C requires you to roll your own every time to use any of those structures. Unfortunately many of the millions of hand coded versions of these are broken which is why I’m doing over a hundred security patches a week on stuff in my house.
rule 2.1 : rule 2 only matters on a given implementation and risc Power systems and risc sparc systems can throw all measurements out the window.
thanks a lot man! Your project did gave me the inspiration. Without it I didn't even think of a possibility like this :)
I believe yes, nerdfonts.com has font patcher for patching any font (according to their website) here: https://github.com/ryanoasis/nerd-fonts#font-patcher
i should clarify - the underlying object cannot change its type. your python snippet isn't changing the type of the underlying object. what you are actually doing is making a variable called a (which in python is nothing more than a name, but when implemented can be thought of as a void pointer), setting it to 10 (which makes the underlying object an int and therefore a is an int pointer), then making another object, setting that to true (which makes it a bool), and pointing a to the new object, which when implemented makes a a pointer to bool. the first object has no more references and will be garbage collected (you basically killed it). actually, python is even more strict than you might imagine - int, bools, strings, etc are all immutable, and therefore you create a new object everytime you set it to something new (for optimization, many common values just reside somewhere in memory and can be pointed to as needed). the variable name in python is just a name, which is kinda confusing. but it is basically implemented in cpython as a void pointer and can point to different things as needed. function pointers is basically how a lot of languages implement classes (you can think of a method as a function pointer that is part of a struct). you can pass function pointers like any other pointer. oop languages like python basically provide semantics around that to support functions as a first class entity. python hides a lot of the implementation from the programmer. remember, everything that can be done in runtime in any of these vm languages can also be done in C, and in fact is often times implemented in C (or C++, which in terms of runtime, also can be distilled down into C). this is true for most jvms, cpython, and even pypy emits jit c code. if you are really interested in this, take a look at how cpython is implemented. in python, everything is an object. this includes your modules (which includes random scripts). what this means is that you can pass along your scripts like anything else. java has primitive types (and the existence of wrapper classes doesn't change that) so if you were to do a comparison (however meaningless it might be), most would argue that python is 'more oop' than java. 
I am not sure why you would need inheritance. Usually aggregation is better. Inheritance works best in two cases: 1. When you have common interface 2. When you use polymorphism In term of second case you can use common structure that contain pointer to function, that one of the parameters have to be **this** pointer (pointer to that structure). Your "implementation" structures will have to have your interface structure as a member. Then you would need to create functions for each implementations. At the end you would need to create "constructors" for each implementation, which will allocate memory, set the function pointer to appropriate function and init some other members. I did that in the past, so if you need I could show some code examples.
&gt; ideally, you would have different fonts for your normal characters and the icons Wait what? How would you do that?
You could base the danger level on the relation between the turning (left/right) velocity and the forward (front/back) velocity, assuming the accelerometer is mounted somewhere near the tires or in the cab. The velocity is just the accumulated acceleration over time. I don't think the gyro can help you much. Disclaimer: I don't know jack about physics or sensor components.
In professional solutions (ESP systems) IMU unit is mounted in the middle of the car. It contain gyro and accelerometers. To calculate what you want you will also need information how much wheels are turned (or steering wheel and calibrate it for each car type). You will also need to have the speed of the car. In professional systems all of those information are then calculated and passed to module that breaks some wheel to get the car back on track.
For the next time: Please do not delete your posts after receiving answers. Future readers might have the same questions you have and might have found this post to help them. By deleting your post, you deny them this resource and erase the work of all the people who helped you. Don't be a dick. Don't delete your posts.
Take a look at this little example I put together: https://godbolt.org/g/nKNY33 This shows how to implement C++ style vtables in pure C. Note that we can also implement something which doesn't exist in C++, namely "static virtuals", i.e. a static method (in the C++ sense) which is polymorphic on its type.
Rather than printing out a generic "Couldn't bind to a socket" error, perhaps it would be more useful to output `errno`, or better yet its `strerror`-decoded counterpart? That'll help you track down why `bind` is failing.
If I output strerror after failed bind it says: address already in use; If I add the following code before issuing a call to bind if (setsockopt(sock, SOL_SOCKET, SO_REUSEADDR, &amp;yes, sizeof(int)) == -1) { close(sock); continue; } It says: Success, even though bind failed.
&gt; It says: Success Take note that `errno` only has significance after a failed library or syscall. You can't rely on it being zero at other times. So here's another question: under what conditions do you expect that loop to terminate with a non-zero `res`?
Oh wow. I'm an idiot. Your last question immediately asnwered it. I never terminated on success, so loop kept on going until res becomes NULL.
What is VS?
Visual Studio
It works now, except for the fact that the computer occasionally overwrites a part of the grid, so I added an if statement to make sure that doesn't happen. if (x[randomnumber1][randomnumber2] == 'O' || 'X') but now it gives a warning: "use of logical || with constant operand" and tells me to change it to |. but I do that and it says something about == taking precedence or some stupid shit. What am i doing wrong?
oh...both IDE and editor are neat, indeed!
You cannot write "X equals a or b", you have to write "X equals a or X equals b"
Which bit do you need help with?
the binary search issue i have the sorting working
&gt; binaryoutput = binary\_search(int numarray\[10\], int end, int target); ^ is that where you call the binary_search function? I think you forgot to pass actual arguments in there (e.g). binaryoutput = binary_search(numarray[], numarray.size(), 5);
Thanks! just one bug left: the computer doesn't always print a character. I have now updated the code at the top.
From your print statement I'm guessing you want to return the index at which the number was found in the array (if it's found at all). I would structure it like this: while (first != last) middle = (first+last)/2; if (middle == target) return middle; else {change the range depending on whether the target value is lower or higher} return -1; (value not found) Doing: return target == numarray\[mid\]; tests whether the element indexed by mid is the same as target so **I THINK** it'll only return 1 or 0 as it's a test wheras you want the position in the array. 
Yep I'm positive. I'm using polymorphism in a python C binding project.
If I may bring my point of view: &gt; This is iffy Well, it's a Zen, not a specification, so fair enough. &gt; In robust systems you [...] can tolerate certain unfavorable conditions. Hence the "Unless explicitely silenced", the language and its ecosystem is designed to give you all errors and have you decide whether they are important or not. Of course in practice some libraries get written that don't follow that, but then they're not pythonic which means people don't tend to like them. &gt; There should be one-- and preferably only one --obvious way to do it. That's actually one of the real strength of python compared to other languages included C (and like all strength it doesn't apply in every scenario and works better if built arround). C programmers has a tendency to reinvent the wheel and when switching projects you often end up with different functions which do almost the same thing but slightly differently, with slightly different interfaces for the same functions (memset anyone?), with lib1_strcmp, lib2_strcmp, lib3_strcmp etc... I like reading C very much, but I've never had that same feeling as in Python where every codebase could be my own, everything looks familiar, and even domain-specific functions implement standard although conventionnal interfaces so you can conveniently use them with standard functions. In practice there are many ways to do many things, but the right one to choose is almost always obvious so people choose the same one in the same condition, and that ends up creating a global uniform style that we call "pythonic". In that sense it's definitely a strength. That said, this is definitely a point where I can see other languages go other ways (and they do). That's why it's worth stating for Python (so that new pythonistas know what is the common philosophy) and why the Zen isn't for every language. &gt; Now can easily be worse than never. Which is why the very next zen element is "Although never is often better than *right* now" ;) I read it a bit like Rob Pike's rule n°2: don't over engineer at first, get things working and reassess later if necessary.
Afaik Ada was required for DoD projects up to a certain date, after which they realized they were swimming against the stream, as Ada programmers are quite rare. After it stopped being mandatory it was practically dumped as C was/is the bigger player.
Unfortunately this code will most like bind an IPv4 socket only, instead of one handling both IPv4 and IPv6, at least on some versions of getaddrinfo. Someone messed up the spec and you can't count on it returning IPv6 first even when AI_PASSIVE is used. The only way to make sure to bind IPv6 first is to set ai_family. If IPv6 fails for some reason you then retry with IPv4. These kind of problems aren't helping the IPv6 transition...
I believe the method you are using for binding the socket is used for UDP datagram sockets, however you are trying to make a TCP (SOCK_STREAM) socket. Try looking at and dissecting this program. https://www.cs.cmu.edu/afs/cs/academic/class/15213-f99/www/class26/tcpserver.c
This was kind of fun. #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #define N 10 int arr[N]; void print_array(int *a, int len) { char *s = "["; for(int i = 0; i &lt; len; i++) { printf("%s%d", s, a[i]); s = ", "; } printf("]\n"); return; } // we all need to jog our memory sometimes... // https://www.geeksforgeeks.org/function-pointer-in-c/ // https://www.tutorialspoint.com/c_standard_library/c_function_bsearch.htm // https://bytes.com/topic/c/answers/218702-typedef-function-pointer // ...oh screw it. //typedef int (*)(const void, const void) cmp_t; int cmp_int(int *x, int *y) { return (*x - *y); } int main() { int i, sf, c, num, *found, idx; printf("\nrandomizing array... "); for(i = 0; i &lt; N; i++) arr[i] = rand() % 100; print_array(arr, N); printf("\nwe better sort that array first... "); qsort(arr, N, sizeof(int), cmp_int); // &lt;stdlib.h&gt; doesn't have a "bsort()" print_array(arr, N); while(1) { sf = 42; do { if(sf == 0) { printf("\nlet's try that again..."); do c = getc(stdin); while(c != EOF &amp;&amp; c != '\n'); }; printf("\npick a number, any number: "); sf = scanf("%d", &amp;num); } while(sf == 0); if(sf == EOF) { printf("\nbye.\n"); exit(0); } found = bsearch(&amp;num, arr, N, sizeof(int), cmp_int); if(found == NULL) { printf("not found!\n"); continue; } idx = (found - arr); printf("%d found at arr[%d].\n", num, idx); } return 42; }
Trust the programmer.
Uh, no. OP's got the right idea, it's just his logic was a bit off. And the code in that link is seriously outdated in that it doesn't use getaddrinfo() and getnameinfo(), assumed ipv4...
It's the last value. The weasel words are designed to allow compiler-writers to avoid loads of variables if it knows their contents. Of course, it doesn't actually *know* the contents in the face of setjmp/longjmp, as a simple example can show: jmp_buf env; volatile int a = 69; int b = 42; if(!setjmp(env)) { b=0; a = 0; longjmp(env,1); } else if(b != 42) { /* compiler is allowed to remove this branch */ } else if(a != 69) { /* compiler is not allowed to remove this branch */ } 
Like DereferencedVoid said, you really shouldn't be seeding your random number in the computerinput function. Do it once at the start of the program. For up to a second, you'll be getting the same random value, and probably overflowing your stack as the recursive function tries the same square over and over again.
&gt; is all about restoring the values at the time of setjmp() It's all about restoring the state of the abstract machine, not "values" in general. So, while it will restore the value of the stack pointer, it can't actually restore any values within the stack itself. &gt; but isn't it against the goal of setjmp() and longjmp() No. They're designed to provide control flow. &gt; By just insinuating that it shall have a well-defined value, isn't there a chance where some people expect it to have the value at the time of setjmp()? Maybe. I take it as them just telling you that accessing those values will not result in undefined behavior. 
Your comment helped a lot. Thank you! =D
What is iffy about keeping things simple? The more complicated you make things, the more likely it is to break or be hard to test. The point of one obvious solution is that you should have enough information about the problem for a solution to be obvious. You should also be able to use a well known, existing solution instead of making your own or fitting in something that doesn't make sense. For most people in most situations this should be true. If you think it's not then you should really think out why your case is different because you're likely wrong. I read "Now is better than never" as a way to justify doing things right the first time. At work people often say we can do such and such later and if we're lucky a ticket is made for the work but usually nothing is ever done and we continue to kick the problem down the road till it never gets done. This problem exists at every place I've worked. I've been working with people to just assume that we need whatever problem fixed tomorrow so we actually get it done today by showing them the huge backlog of work that we said we would do later that never happened. 
If this is python specific, you may find useful resources in /r/python. Others there have likely had to solve this interfacing problem 
Good man I've been wanting something like this for awhile but had zero ideas how to render the icons, how did you accomplish that?
You could add a flag/config file for specifying icon size as well as maybe specifying another icon font to use maybe specify color as well. 
It's not, it's C specific, but thanks!
Linked list should be vector
You still have to prioritize your work. If you have a backlog of minor work, it could be better to not work on it.
C++ is a multi-paradigm language, not just OO. And if by OO you mean everything is allocated on the heap and accessed polymorphically via a pointer or reference, I agree. That kind of programming style is for Java. But C++ is so much more than OO in that style.
In my experience, it's usually easy to deprioritize most work to new features even if that other work would be really beneficial. "Minor work" might be cleaning up all the kludge that has built up in the code base. There's no immediate benefit to doing that work but it is important to do.
But that cleanup can have a very high risk and cost. Now that you've changed your code, you have to pay the cost of your QA and testing. 
This is a mindset that I rarely see outside of C specific fora. I agree totally. 
Sure, any change has risk but if you have good, automated tests with high coverage and overlap then that risk should be manageable. If you're scared to change code then that's a good sign that it's not healthy code. My team manages a code base that has this problem right now. It was originally made with no tests and has just had features added with no cleanup ever done. When I took over as a feature lead I halted new features so that we could work on adding tests as well as tooling for monitoring code health. With that in place we've resumed adding features at a slower rate while also rewriting and cleaning up code. People enjoy doing the work now since more certain that their changes work, the code isn't a hodgepodge, and adding features takes less time. We don't have a QA team so getting that testing added was a necessity. Code changes and clean up shouldn't be a bad or scary thing.
You make some good points, but I think that most of the things at my shop get pushed because our managers want us working on other things. 
That seems to be a common issue for most places unfortunately
No. Vectors are a form of arrays. Linked lists have their place and are fairly useful in their niche.
what's the diff between constant and amortized constant? I take it amortized means average?
So what should I do? use VLA, instead of malloc?
point...thanks
well, I will look into it perhaps, but honestly I have no clue whatsoever about rendering haha
iTerm2 has settings for that. I don't know about other fonts. Although the nerd font collection probably has some fonts which would look correct?
Try allocating an array once..maybe at a parent scope..then reuse.
I think compilation options such as `-fwrap`, and `-fno-strictaliasing` have more effect on safety than most of the MISRA rules. Safety is best addressed at the language level, not with static analysis on top of something broken. 
What is "turning intensity"
&gt; char* strings are immutable You have that backwards, generally `char blah[]` strings are immutable, tho you should really use `static const` to enforce that, because they're still mutable otherwise.
With constant time, each operation takes the same constant time or less.With amortised time, once in a while, operations might be much slower as long as it takes a constant time on average in the long run.
I believe he mean how fast car is turning or can be turned without collision. This is common problem with ESP modules in cars that use IMU to measure inertia. I was working in that domain for some time. Imagine that dispersed heavy weighted truck is trying to take a turn with too acute angle. Inertia could cause that it will rollout. But this is very complex problem and not easy to provide solution. There are many variables that have to be taken into consideration.
Yes, it means some operations are very costly but affect the entire duration of the operation minimally. See [this](https://en.wikipedia.org/wiki/Amortized_analysis#Examples) for an explanation of this particular case.
Pre-processed assembly (.S, compiled with gcc) allows #including C header files for the constants #defined there. Linux kernel uses this trick a lot. But only makes sense for mixed C/assembly projects, not for pure assembly. Same with linking, mixed projects may benefit from gcc knowing how to find -lgcc. GCC does not optimize assembly. Thank god.
The magnitude of the vector sum of velocity of the truck and the centrifugal force. A driver must turn slowly to minimise the stress he will induce on the truck. 
Please show us your code. What compiler and possibly IDE do you use?
Have you tried `va_arg`?
See https://github.com/torvalds/linux/blob/master/arch/x86/boot/printf.c
[Permanent](https://help.github.com/articles/getting-permanent-links-to-files/#press-y-to-permalink-to-a-file-in-a-specific-commit) GitHub links: * [torvalds/linux/.../**printf.c** (master → 2837461)](https://github.com/torvalds/linux/blob/2837461dbe6f4a9acc0d86f88825888109211c99/arch/x86/boot/printf.c) ---- 
Which compilers? Which compiler error? What do you mean by "open a project in C"? graphics.h is not standard C. I do know Borland used to ship it with their Turbo C/C++ compilers, but it could be some other vendor too. If you want to use vendor-specific stuff, you need to keep to that vendor's compiler. There is no standard way to do graphics in C.
As far as I understood, your post can be split up into 2 questions: 1) How do I extract variadic arguments? 2) How do I know the number of variadic arguments? Here are some answers: 1) You can use va_arg method. In your variadic function, declare a variable of type va_list, initialize it via va_start, extract arguments with va_arg and finalize with va_end. All of this is accessible via manuals. 2) You can get the number of arguments via number of specifiers in your format string. Unfortunately, there is no way to know the exact number of variadic arguments passed other then rely on non-variadic ones, at least I don't know of it. If you're using gcc, you can use a special __attribute__((format, _, _)) to instruct your compiler to check the number and types of arguments passed to your printf-like function according to format string.
Code updated. The only problem now is that after 3 runs it prints "ok my turn." three times and then prints the board three times
Now although it always prints, it often malfunctions and repeatedly goes "ok, my turn. ok, my turn. ok, my turn. ok my turn" and prints the board a bunch of times. Now what?
You might want to think about your array indexes for computerinput. What range are your random numbers producing? Also, you should probably think about what your recursive function call is doing. What happens when it's called, and when it returns?
I fixed that already as you can see in the code, it now produces from 0 to 2. After reading your recursive function comment, I made the code below the if statement in computerinput() start with an else statement. This way, if the computer sees the space is not taken up, it will move onto the else statement and NOT recurse, unlike if it saw the square was taken up and did the if statement, in which case it would try again. program eventually will print "ok, my turn" ad nauseum and then it will crash with a "segmentation fault"
You could port the C++ code you found to C
Sorry, I don't fully understand. Are you trying to code new Atari-esque games in C or get existing Atari games to work with a C emulator? Stella [https://github.com/stella-emu/stella] is written in C++ and it would just be a matter of a lot of work to port it to C, though I'm not sure that this would be a useful exercise. 
That's quite a lot of code to port.
Wouldn't (x%2) produce 0 and 1? If the top left 4 squares are full, there will be nowhere left, and it would recurse indefinitely. You might want to limit the number of times it tries to pick a square.
It works!
Is that realistically feasible in a non\-stupid amount of time?
I have a bit of hardware that can run small C files to interface with neural networks. I need to be able to have the Atari games in C for it to be able to run on the hardware. I need Atari games as they're a standard test bench in the field.
It depends on how C++ish it is, with luck it could be just C with classes.
I really struggle telling the difference. Would you mind having a quick look to see how feasible it is? I think this is the main stuff I would need to port [https://github.com/mgbellemare/Arcade\-Learning\-Environment/tree/master/src/games/supported](https://github.com/mgbellemare/Arcade-Learning-Environment/tree/master/src/games/supported) 
here is some psudo code for a shell #define SUCCESS 1 #define FAILURE 0 int running; int built_in_commands(char *input) { if(strcmp(input, "history")) { show_history(); return SUCCESS; } //other built in commands return FAILURE; } int main() { running = 1; while(running) { print_prompt(); wait_for_input(); char *input = get_input(); record_history(input); if(built_in_commands(input)) { continue; } //progs from bin and so on if(system_commands(input)) { continue; } //progs from the current directory if(directory_commands(input)) { continue; } } return 0; }
It is definitely feasible. I looked at Adventure.cpp and Serializer.cxx, there’s very little that needs to be rewritten. These two files do look like C with classes. Those two files can be ported to C in less than an hour, I think.
we just need to call the history functions when a user enters "history", that's the part where we are stuck
You already have `if (strcmp((const char *)args[0], "exit") == 0)` for the exit command, just do the same thing, but with the string "history", and instead of exiting call history.
Simply don't declare your "private" functions in the header file. Also, ship a lib file instead of a source files.
Awesome, cheers for all the help man. Wish me luck converting it all!
How do you think C++ was first implemented? Structs can easily be used similarly to 'objects'. You can even store function pointers in them. It's not true "Object Oriented", but it's close.
There was a topic recently about "inheritance" that had some comments on the subject. I'm not where I can look for it. Essentially, you declare a struct whose pointer is passed to all the "methods" -- where a method is just a function that takes that struct pointer as a "this". That struct gives you access to a vtable or equivalent (a vtable is just a list of pointers to functions). 
Typical OO in C involves putting struct foo; in your header file. Then adding one (or more) function returning a `struct foo *` and several others that take it as the first parameter. Then you put the actual definition of `struct foo` in the `.c`file instead. That's all there really is to it. C is not big on inheritence, but you shouldn't be using that to too much in C++ either anyway. Encapsulation is much more popular nowadays, and that's done the same way in both languages. If you _really_ want inheritance in C then you simply put the superclass as the first member of the subclass like so: struct foo { struct bar super; }; then any `struct foo*` can be cast to `struct bar*` and still be valid. It's rarely used in practice though. 
https://www.cs.rit.edu/~ats/books/ooc.pdf
It isn't, there are lots of programs that do not do this. This is just for most Operating systems where an application or executable returns an exit status. An Exit status of 0 just means no error. 
not a direct answer, but if you're asking this kind of thing you would probably appreciate having harbison + steele's book on your desk. it answers this and a bunch more questions about C programming - i use it all the time.
Use `va_args`. There used to be something called "vargs" or something but it doesn't exist anymore in modern C. It's one of those things you have to [look up](https://www.tutorialspoint.com/c_standard_library/c_macro_va_arg.htm)... Not used often in common code but nice it exist. The last (one one of the few) times I ever tried to use it funny things happen if one of the arguments are NULL. But I was doing something a little weird. :/ Just follow the examples and test your code works!
It isn't 
The returned int is an exit status. It isn't really used that often anymore except by task automation, but informs the operating system of how a program exited. Zero tends to be no error. You can also examine it yourself in say a bash or a windows batch file, which can be useful for some automation tasks.
In addition to what the other commenter said, you should use the EXIT_SUCCESS macro instead of 0. http://en.cppreference.com/w/c/program/EXIT_status
Thou shalt not left-shift cout
It's not OOP, it's a hybrid. You can do OOP in assembly if you try really hard. 
I figured it out for the project I was working on. I found this site to be most helpful, as far as the struct diagrams. [https://www.codeproject.com/Articles/108830/Inheritance-and-Polymorphism-in-C](https://www.codeproject.com/Articles/108830/Inheritance-and-Polymorphism-in-C)
Your main function can return whatever value you want. It is just convention to return 0, as in 0 errors.
Habit. It just looks _wrong_ leaving `main` without a return value, even though that's perfectly valid.
Habit. It just looks _wrong_ leaving `main` without a return value, even though it's perfectly valid C99 code.
&gt; In addition to what the other commenter said, you should use the EXIT_SUCCESS macro instead of 0. Just to nitpick this slightly. The C specification says that _both_ `EXIT_SUCCESS` and 0 indicate a successful termination. Most implementations will defined `EXIT_SUCCESS` as 0; you are correct that some implementations may not do this. However, using 0 is perfectly valid, and will always have the same effect as using `EXIT_SUCCESS`.
Interesting. You've read [https://swtch.com/\~rsc/regexp/regexp1.html](https://swtch.com/~rsc/regexp/regexp1.html), right?
One gotcha in using `va_arg`, which may not be immediately obvious... you have to make sure you extract arguments as if they had undergone "default argument promotion". So if somebody passes a `char` into your variadic function, for instance, then the type of the argument you extract is `int` or `unsigned int` (depending on whether `char` is the same as `signed char` or `unsigned char` on your platform). Similarly, when somebody passes a `float` you need to extract it as a `double`. The parameter referenced in the `va_start` macro call must also be compatible with this application of default argument promotions.
This is in the references of that very page but https://dl.acm.org/citation.cfm?doid=363347.363387 is worth having also. I may or may not have a copy that might be able to possiblely be uploaded somewhere as long as that doesn't break any rules, in theory.
I've got a dumb question, dumb because I've never tried it, but what does `main` return if you don't give it a value? (I've never tried it, because I feel you have to return _something_; I only do pure C for console programs, where knowing success/failure is critical to workflows.)
&gt; I've got a dumb question, dumb because I've never tried it, but what does main return if you don't give it a value? As per the [(draft) C standard](http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1570.pdf): &gt; If the return type of the `main` function is a type compatible with `int`, a return from the initial call to the `main` function is equivalent to calling the `exit` function with the value returned by the `main` function as its argument; reaching the `}` that terminates the main function returns a value of 0. In other words, _if_ you defined `main` to return an `int` (other forms of `main` are permitted, but are implementation-defined), and _if_ you neglect to explicitly return something from this function before reaching the `}` that terminates it, then the C implementation will behave as if you had executed `return 0;`. Furthermore, this is then equivalent to calling the `exit` function with 0 as its argument.
Rob Pikes's *Structural Regular Expressions* should be invaluable http://doc.cat-v.org/bell_labs/structural_regexps/se.pdf Considering he invented them... &gt;The current UNIX® text processing tools are weakened by the built-in concept of a line. There is a simple notation that can describe the `shape' of files when the typical array-of-lines picture is inadequate. That notation is regular expressions. Using regular expressions to describe the structure in addition to the contents of files has interesting applications, and yields elegant methods for dealing with some problems the current tools handle clumsily. When operations using these expressions are composed, the result is reminiscent of shell pipelines. 
I mean you could just do return 69; or something but it wouldn't make a difference. Whatever you return in main is irrelevant because all of the calculations and actions are complete. If it's a function outside of main, then you won't always want it to return 0. I.e. #include &lt;stdio.h&gt; int addfunc(int iNum){ int iOut; iOut = iNum + 2; return iOut; } int main(void){ int iOut; iOut = addfunc(2); printf("\niOut is %d\n\n", iOut); return 0; //main will print 4.
Thanks!
&gt; There should be one-- and preferably only one --obvious way to do it. I don't think I understand this particular one. If it means a way to implement a specific algorithm or data structure, then sure I can agree with that. But if it means a way to solve the problem overall, then I would have to heavily disagree with it. As my math teacher in high school once said, "There is more than one way to skin a cat."
The `realloc()` to shrink the buffer is a pointless optimization. It doesn't accomplish anything useful. You've also go an off-by-one error and you're shrinking it one byte too much. You've got another off-by-one error in `reverse()` causing you to print the terminating null byte, which is now technically outside the buffer due to the `realloc()`. You go through the trouble of defining your own assert or including the standard one, but you don't actually use it anywhere. 
if i don't use `realloc` won't they be unused byte. For example the supplied string is 12, but the allocated memory is 10000. What about the unused bytes ? . Won't it cause issues?
I tried this, but you end up referencing the structure in the function call anyway so it seemed pointless for my application. Maybe I was just doing it wrong, but I found it to be much simpler/more maintainable to just avoid that pattern together.
This is not what `rev(1)` is supposed to do. It is supposed to read its input (either the files whose names were specified on the command line, or, if no files were specified, the standard input) line by line and output each line individually reversed. Your program either reads a single line of up to 999 characters (line feed included) from its standard input, reverses it, and prints it preceded by a NUL character, or it reverses and outputs its command line arguments, each preceded by a NUL character, all separated by spaces.
This is true, but it is how you would implement "OOP" in C. myStructPtr-&gt;doAThing(myStructPtr); It's not pretty but it's not _that_ bad Or, just call doAThing: doAThing(mystructPtr); It's basically the same thing.
Typically that memory won't go back to the operating system. Instead that memory is retained in the process for a future `malloc()` allocations. The only reason to shrink a buffer like that is if you're going to 1) have a lot of them 2) keep them around a long time, and 3) make lots of allocations that are likely to be carved from the returned (to the allocator) memory. Unless you're writing code for a microcontroller, 10kB is nothing. It's so small that returning some really doesn't matter. The overhead of managing that tiny bit of memory (e.g. by calling `realloc()` to shrink it) far outweighs briefly holding onto a few kB you aren't using. 
Yeah I ended up using the second method because it avoids what I felt was like extra obfuscation.
I'd advise against using the return value from main as a count of errors. A couple reasons: 1. Who cares how many errors there were? 1. It's not specified by the C standard how the return value is interpreted, unless it's 0 (success), EXIT_SUCCESS (also success, which may or may not be 0), EXIT_FAILURE 1. On most hosted systems, the returned value is clamped to the last 8 bits, which makes things awkward if you ever have 256 (or 512 or ...) errors in your program
Ahem, RP did not invent regexp, it is a mathematical concept of one Stephen Kleene. In any case the first computer scientist in implementing (partially) was Ken Thompson » https://en.wikipedia.org/wiki/Regular_expression
**Regular expression** A regular expression, regex or regexp (sometimes called a rational expression) is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern. Usually this pattern is then used by string searching algorithms for "find" or "find and replace" operations on strings, or for input validation. The concept arose in the 1950s when the American mathematician Stephen Cole Kleene formalized the description of a regular language. The concept came into common use with Unix text-processing utilities. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Can I please have my Pike appreciation thread :(
no test, no negation, no `{}`, ??? je, je, you are welcome to the world of persons who make your own "library", you are welcome to C I do not understand much about theory, but I also have a small engine. In case you find [it](https://github.com/nasciiboy/RecursiveRegexpRaptor) useful. I do not speak much English, so the documentation can be confusing ... and also has no comments
So the linter doesn't complain "Path with no return in function declared to return int." It is so much more satisfying to see "Finished checking --- no warnings."
Yep. i've never seen the return value used as an error count; i've far more often seen it used as a status, with the value somehow mapping to a specific error. For example, it might be a number whose binary form represents which error(s) occurred. So the least significant bit of a number might represent whether a file exists; the next bit might represent whether a file is readable; the bit after that whether a file is writable; and so on. Thus, a value of 6 = 110 = "file not writable" and "file not readable". Alternatively, one could a non-binary mapping, such that a value of 1 might be "file not found"; a value of 2 might be "file not readable"; a value of 3 might be "file not writable"; and so on. This approach can be helpful to users of your program, whether human or another program. A human will get a more specific error message than "something somewhere went wrong"[1], and a program that needs to use the output of your program (because it's e.g. receiving it via a pipe) can decide what to do based on the particular error it gets. [1] Notwithstanding the "an experienced programmer will usually know what's wrong" joke.
argv[0] is the program name. If you don't want to print it, then don't.
The "name" of your program is always the first argument. Just check to make sure your argc is greater than 1 and then start your for loop at 1 instead of 0. This isn't a C specific concept by the way.
`0` and `EXIT_SUCCESS` might have different effects if the operating system has multiple success codes 
Huh? It is fine to write `char blah[] = "hello"; blah[0] = 'j';`
The unix operating system, when it runs programs, received an "exit code" when the program exits. Exit code 0 is regarded as success - everything went as planned. Any nonzero exit code indicates some kind of error, but the mapping of exit codes to actual errors is not very standardized or portable, so it's often ignored. The main function in a C program should be written with a return type of int. This integer represents the exit code of the program. Because returning 0 from main indicates success in a C program, it has become a standard practice to have other functions within a program return an integer, where 0 also indicates success, unless the integer is supposed to actually represent something, like a file handle.
Make it work with multibyte encodings like utf-8. Especially with utf-8. And Unicode combining characters and bidi text. 😚
There is a very **Strong** convention to always return 0 "on success" and some other positive value "on error". This is extremely useful in many situations. Scripting is one **Major** reason. Let's look at the UNIX command grep as an example: `$ grep "hello world" /tmp/somefile.txt` `hello world` `$ echo $?` `0` In this example, the string "hello world" is in the file /tmp/somefile.txt (so it is outputted), I can then check the return value of grep (with bash's $? variable), which is 0 (success) Here's an example where "hello world" is not in the file: `$ grep "hello world" /tmp/somefile.txt` `$ echo $?` `1` In this example, grep returns 1 (error) because it couldn't find the string. In this final example, the file (/tmp/nofile) doesn't exists: `$ grep "hello world" /tmp/nofile` `grep: /tmp/nofile: No such file or directory` `$ echo $?` `2` In this example, 2 is returned to signify that grep couldn't find the file. At first glance, this might not look that useful but consider this used in a script. Let's say your script depends on the fact that "hello world" is in the file /tmp/somefile.txt: `grep -q "hello world" /tmp/somefile.txt` `if [ "$?" != "0" ]; then` `echo "ERROR! string hello world is not aavailable in /tmp/somefile.txt"` `exit 1` `fi` In this script, we grep for "hello world" (we use -q to make grep be quiet and not output anything), then we make sure that the return code from grep was 0, if it's not zero we output an error and exit (note that we exit with error code 1 so that if our script is used by another script, it can also detect errors). Every UNIX command that has ever been written follow this convention which enables us to write robust scripts. Windows have a similar mechanism called &amp;#37;errorlevel&amp;#37; which can be used in batch scripts. Also, various system tools like for example inetd (and probably systemd, when a service is in simple mode) will use the return value to determine if processes exited successfully or if there was some error
I tend to do something like: CC = gcc STD = c99 STZ = -fsanitize=address -fsanitize=leak -fsanitize=undefined DBG = -g3 -Og $(STZ) INST = -pg --coverage CFLAGS = -std=$(STD) -Wall -Wextra -Wpedantic LDFLAGS = -lm dbg-build: main.c $(CC) $(CFLAGS) $(DBG) -o $@ $^ $(LDFLAGS) prf-build: main.c $(CC) $(CFLAGS) $(INST) -o $@ $^ $(LDFLAGS) with variation based on what I need. This obviously isn't a full Makefile, nor is it perfect, but I just wanted to demonstrate how I differentiate debug and instrumentation options from my `CFLAGS`.
Tests are currently being written (I've been using a little console app for "testing" purposes, removed now), I'll definitely continue working on this little project in my free time. Basically everything in the "cons" list should be implemented (including the arbitrary quantifiers you've mentioned - "{m, n}"). Give me time, man ;). I think it is fairly decent for a first try at something like this as it isn't that trivial to write it all yourself. Your lib is awesome, by the way.
I did, yes but I've primarily been reading the [Dragon Book](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811) and [Parsing Techniques](https://www.amazon.com/Parsing-Techniques-Practical-Monographs-Computer/dp/1441919015). Regular expressions appear in both of these books, of course.
&gt; 0 and EXIT_SUCCESS might have different effects if the operating system has multiple success codes Good point. I hadn't thought of that!
&gt; It's true that returning 0 to signify success is just convention It's more than convention. It's _specified_ to signify success. &gt; **7.22.4.4 The exit function** &gt; _[...]_ 5. Finally, control is returned to the host environment. If the value of `status` zero or `EXIT_SUCCESS`, an implementation-defined form of the status _successful termination_ is returned. _[...]_ 
Do not post these. Do not impersonate other people on the internet.
Right, good point
I don't have any project online where I needed hashmaps. I have a few toy projects on my computer but they aren't really interesting. Basically, you pick a hash table size (should be a prime) and a hash function (I like PJW hash) and then make a hash table with a linked list of entries for each buckets. Easy as pie. If you want to be able to resize the hash table, it's a bit more involved, but you don't need that very often either. If the amount of data you need to keep track of grows, it might be time for a database (even if it is just SQLite) anyway.
I think this is a little bit overengineered. You might find it easier to _embed_ an object of your base class inside the sub-classes, i.e.: typedef struct { /* parent class's fields */ } DynamicArr; typedef struct { DynamicArr parent; /* subclass's fields */ } IntegerArr; typedef struct { DynamicArr parent; /* subclass's fields */ } StringArr; You can refer to the fields of the parent via each object's `parent` object. For instance: bool dynamic_arr_ensure_space(DynamicArr *darr, size_t elms_size, size_t n) { /* expand *darr if necessary */ } bool integer_arr_append(IntegerArr *iarr, int i) { if (!dynamic_arr_ensure_space(&amp;iarr.parent, sizeof(iarr-&gt;arr[0]), 1)) { return false; } iarr-&gt;arr[iarr-&gt;parent.len++] = i; return true; } Note that we can only have single pointers here, not pointers to pointers, and we avoid needing to cast anything.
You almost certainly do have undefined behavior here from the casts. The way one typically does this is they have the "base class" be the first element of the struct of the "derived class". It is valid to convert between a pointer to a struct and a pointer to the first element of the struct.
You might want to look at [http://www.deleveld.dds.nl/inherit.htm](http://www.deleveld.dds.nl/inherit.htm)
true, it's nothing trivial, and the theory and syntax have some obscure and irreconcilable nooks with a "pretty" code, enyway, good luck
If you know all your integers will be within some FIXNUM range just use an array of native int/long and a bit mask to test/set the type.
What if I want to read from something that isn't a file?
its the only way your OS will know if everything is okay!
Thank you for your comment, I hadn’t really thought of that, so I’m going to add this to my to do list. Could you help me understand the types of non-files you might be thinking of? Are we talking file-like interfaces like stdin or something different? You’ve also reminded me that I want to support compressed read/write. I’d just have to figure out a sane way to enable support without forcing dependencies on the end user. I’ve seen libraries allow a char* input but in my data analysis experience I find this counterintuitive. If you’re going to be passing around data in memory, I’d expect you’d be using something like JSON or google protobuf so you can preserve your type information. 
I'm not a good C programmer, and I didn't study your code in detail, but I'm going to quote a recently popular article by /u/skeeto: [https://nullprogram.com/blog/2018/06/10/](Minimalist C libraries) &gt; Communicating IO preferences to libraries can be a real problem with C, since the standard library lacks generic input and output. Putting FILE * pointers directly into an API mingles it with the C standard library in potentially bad ways. Passing file names as strings is an option, but this limits IO to files — versus, say, sockets. On POSIX systems, at least it could talk about IO in terms of file descriptors, but even that’s not entirely flexible — e.g. output to a memory buffer, or anything not sufficiently file-like. Just use a buffer, I guess.
Thanks guys for your interest, i'll solve the problem !! :)
Thank you for sharing this, I found it very insightful. I’m not a good C programmer either, but recently I’ve gotten more curious after hitting a performance wall on a data problem at work. At the bottom of that blog post the author linked to a GitHub repository which looks interesting. I might try to integrate this as an interface for my next version. [Growable memory buffers for C](https://github.com/skeeto/growable-buf) 
A possible approach is allowing the user to provide their own callbacks for IO. Basically make your library take function pointers for read/write etc. Also consider this for memory allocations. 
For example, instead of a filename string to open, a user might have * an already openend FILE\*, e.g. stdin * a unix file discriptor of an opened file or socket or fifo from another process * the complete csv file contents in a memory buffer (memory mapped, or already read/decoded/decompressed from somewhere) * part of the file in a buffer, sequentially read/decoded from somewhere. I think ideally there should be two alternatives in the library API: * a simple interface that uses a FILE\*, or maybe a filename string, * an advanced interface that can be used for everything else, and requires the user to provide callback functions that act similar to fread/fwrite, e.g. &amp;#8203; csvreader csvreader_init_callback(csvdialect dialect, int (*read_fn)(char *buffer, size_t size, size_t *read, void *userptr), void *userptr);
Thanks for the suggestion. What I really like about this idea is that it allows the user to use system specific IO, or their own compression library without making me impose a dependency on others. I’d like to provide an ANSI interface because a lot of the time that’s good enough but that could also be implemented as a set of callbacks, which attracts me more to this idea. I’ll have to do some thinking on the public API though. Internally I’d need to work with the chars one by one in order to follow or apply the configuration rules such as character escaping and field quoting. And pulling too big a buffer between means I’d potentially have to maintain a dataset between calls to “next record.” In any case, many of the objects such as the dialects, readers and writers are only going to exist on a one to one basis with datasets so if they expand to 16 or 32 bytes I don’t foresee a modern machine having too much a problem with that. The memory management focus I have is on the objects this API would be generating a large number of like the fields (tokenized string containers) and the records (field arrays). I’m adding this idea to my list. Thanks again!
&gt; Downvoted WHY? No idea. &gt; Was my answer wrong in any way? It wasn't.
Thank you for the comment and the example declaration. I think I see where you’re going with this example. I had an idea of how I could implement this project with C++ templates, but now I’m getting an idea of how much more flexible the C approach is. Prior to this project my only experience with callbacks was doing a intro to multiprocessing class where we touched on openmp and pthreads, and pthreads was my first exposure to callbacks. This was about 5 years ago, so I didn’t remember them let alone think to apply them here! I do have a follow up question for you regarding an implementation strategy for using callbacks. Would you think it wise to base the API around providing callbacks and implement callbacks for the data sources I might like to directly support? Would a callback centric API be a type of loose coupling in the C world? For instance, I’m thinking the `csvreader_init` function would be the advanced constructor and another constructor like `csvreader_filepath_init` would be a simple constructor for end users. This second constructor would use the `csvreader_init` function and provide callbacks opaque to the end user. Then, I’d change the `char` fields in the dialect object to `int` and voila I can support 8, 16 and 32 bit character strings, and I also have a consistent internal implementation which would likely be more maintainable. 
Thank you to all who provided suggestions, I really got some good ideas on where to go from here from this conversation. I see how my current design has painted me into a bit of a corner and how I might redesign this project to support 16 and 32 bit characters with a consistent API and no need to compile the library again. I will continue to watch this thread and try to reply to all who provide insights!
Your `csvdialect` and `csvfield` unnecessarily complicate APIs. I would use something like the following: typedef struct { int len, max; char *str; int m_fields, n_fields; char **fields; // fields[i] points to str } csv_record_t; /* option 1: have to expose csv_record_t */ csv_record_t rec; csv_file_t *fp = csv_open(fn, '\t'); while (csv_read(fp, &amp;rec) &gt;= 0) { } csv_rec_free(&amp;rec); csv_close(fp); /* option 2 */ csv_record_t *rec; csv_file_t *fp = csv_open(fn, '\t'); rec = csv_rec_new(); while (csv_read(fp, rec) &gt;= 0) { } csv_rec_free(rec); csv_close(fp); Because `csv_record_t` is small enough, I would rather let users directly access it. If you prefer better encapsulation, you may have `int csv_rec_n_fields(const csv_record_t*)` and `const char *csv_rec_field(const csv_record_t*, int index)`. &gt; Suggestions I'm looking for: *String libraries* I wouldn't use any string libraries in this case. Plain C strings give you a lot of flexibility. For example, with `csv_record_t` in my definition, you don't need to separately allocate for each field. &gt; Is it wise to try to support Unicode right away? Others also mentioned custom readers and allocators. You should really get the basic working first before thinking about those minor features. 99% of users would care about those.
fmemopen
Forget wide characters, UTF-8 is all you're going to need
You can install git and whatever else you need with homebrew.
It is possible to clone a git repository to your local machine using `git clone &lt;url to repo&gt;`. I don't prefer to do this, because there may be implementation differences between your mac book and your school's servers that result in your project not performing uniformly. The better option, in my opinion, is to ssh into your school's servers with X11 forwarding enabled. On macOS you will need to install XQuartz, which will set up an X11 client on your mac book. Then when you connect to the server use `ssh -Y &lt;url to server&gt;` in order to forward X11 connections. This will allow you to continue to develop on your school's server, and all the windows or GUI aspects of your project should appear on your mac book when it comes time to test. It is worth noting that your school's servers will need to be set up to support X11 forwarding, but if they're not I recommend asking your instructor for help to change the sysadmin's mind. Hope that helps.
[Xcode](https://developer.apple.com/xcode/downloads/), Apple's development environment, is a nice environment to use for C development on the Mac. When you install it, it will also install git which you can use from the command line or from within Xcode.
The Mac has a half decent unix environment with bash and some dated utils. There are a few ways to use them for programming. One is to get Xcode which last I saw was free. But that is a bit heavy for someone with low disk space if you don't need all the mac and ios sdks etc. For command line stuff just install the brew package manager. I haven't had a mac for awhile but you should be able to use it to install clang, make and git and anything else you need. Running a stripped down linux in a virtual machine isn't going to take as much space as you think. Only install the packages you need. Grab something like virtualbox or docker and your favourite distro install image. A vm will be faster than remote access but doing things natively with brew might be a better experience until you run into differences between the two environments. 
I can think of only two reasons why you would use a macro over a static inline function: 1. Your code needs to be portable to systems that don't conform to C99 or C11 (e.g., ANSI or pre-ANSI compilers) 1. You need text substitution. Text substitution has a *lot* of uses: new syntactic constructs, polymorphism, generated string literals, etc. Mostly because of #2, I suspect macros will continue to be much much more popular than inline functions. Inline functions have some advantages, too, namely that you get type-checking for free, they're much easier to reason about, and you will likely get clearer error messages in case you made a mistake. They can't come close to the power of text substitution, though.
The buffer approach is going to run into the same point of friction as [Expat](https://libexpat.github.io/), an XML parsing library. A single value might straddle multiple input buffers, and either the library or the application calling it needs to be able to manage that. In Expat's case, it [passes the value to the application in chunks](https://www.xml.com/pub/1999/09/expat/reference.html#chardatahandler). It's convenient to let the library grow a buffer to concatenate all the pieces before handing over the full value. That [also has potential pitfalls](https://en.wikipedia.org/wiki/Billion_laughs_attack), so ultimately it's a trade-off. 
**Billion laughs attack** In computer security, a billion laughs attack is a type of denial-of-service (DoS) attack which is aimed at parsers of XML documents. It is also referred to as an XML bomb or as an exponential entity expansion attack. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
That makes a lot of sense. I've been using macros to implement optional features enabled with a configure script, but I will probably switch over to the inline functions for error checking and type safety.
Thanks, I think I’ll go with this option! It seems like the easiest to setup with my current workflow. Thanks so much! 
&gt; The Mac has a half decent unix environment with bash and some dated utils. There are a few ways to use them for programming. Mac OSX is certified UNIX so you get way more than a half decent UNIX environment. 
Thank you for the reply! I see what you're saying about the custom string type vs. using plain C-strings, but I'm still going back and forth on this one because your post forced me to consider another aspect of this library. Let's use a Machine Learning dataset as an example. These datasets can be tremendously wide, thousands of columns sometimes, and often enough contain open entry fields which can be thousands of characters wide (ex. customer reviews). I'm now considering having the \`csvrecord\` act as a lazy evaluator, only parsing a field after it is requested. I could see a potential optimization case for a background thread maintaining a small buffer of fields to offer some potential speed benefits as well, but only if it proves absolutely necessary. Adding threads is never trivial if I recall correctly. I'm also wondering if this logic would be better suited to a callback API as well, because the end user will have a better idea if they prefer simplicity or need to tightly manage their memory. &gt;I wouldn't use any string libraries in this case. Plain C strings give you a lot of flexibility. For example, with csv\_record\_t in my definition, you don't need to separately allocate for each field. I kind of got this picture as I was trying to define the \`csvfield\` as well, but I kept going because I felt it wise to have an API that returns a character array paired to its length. I also may have been spooked by the \`strncpy\` documentation though, so I'm going to take your advice and scrap this part of the API as unnecessary. &gt;Others also mentioned custom readers and allocators. You should really get the basic working first before thinking about those minor features. 99&amp;#37; of users wouldn't care about those. This is what I was planning to do, and why I'm so thankful people like you are helping me take a look at my design before I dig into implementing it! Thanks again!
Thanks for the advice! I was worried that sometimes you get a Windows file with UCS-2/UTF-16 encoding which might throw a wrench in the works. Do you think it would be reasonable for a library to expect a custom callback to convert from wide character formats to UTF-8 for the library?
Have you tried forwarding x over ssh? The command is ssh -X. That should let you get any x windows from the remote server.
Thank you for the insight, I have to say my general strategy with programming is to stick to principles except when they drag me down. I'm quite satisfied with documenting that the return value of a function must be freed by the caller. That being said I do see the benefit of minimizing allocations in the library. Unfortunately, there are a number of cases where a CSV writer needs to add characters (ex. escaping, quoting) and therefore might not fit in an externally allocated buffer, or a CSV reader needs to discard characters (ex. delimiters, escapes, quotes) and needs to shrink the buffer and move values around. I'm happy to consider generic buffers as an I/O option but I'd expect that the library would need to call `realloc` at some point, or spend an inordinate amount of effort moving chars around.
No problem! I did this for a couple classes in college with my mac book air; it was a real time saver. If you run into any problems I'm always happy to help.
Yes, it is a full certified unix environment. And a half decent one at that.
I don't know what tools you consider dated but age does not make a tool bad or worse than any others. 
This is the correct answer. Also, use vim and you don't need to forward any windows!
Well .... doesn't the version of GNU Emacs shipped with OSX predate the first iPhone? i'm not an OS X user, but i believe the version that comes with OS X is around version 22 (due to licensing issues, i think?), and Emacs is currently at 26.1.
Don't worry about whats fast or not until you can spot it in a profile. Too much time spent optimizing the wrong shit. Literally just had this come up in code review about using templates (C++) or std::function type declarations for the purpose of inlining. The choice can change whether that function gets inlined, but then the compiler would just choose to outline child function calls for zero performance difference. It's better to let the compiler decide when to inline vs outline. If you inline too much, you get crazy register pressure and binary bloat. It's also good to outline uncommon error handling code so that you can fit more hot code in instruction cache. The functions also help provide type safety, and can usually be more legible than macros, as with macros, one begets another. LTO and PGO help the compiler make more informed inlining decisions, too. Re: the other comment on portability, gnu89 supports them. You'd really have to be reaching IMO for a compiler that doesn't support static inline. Macros do have their place, but only when what you're trying to do can't otherwise, or doing so would be extremely verbose otherwise (thus helping readability). I work on a compiler and the Linux kernel though, so I might be biased.
Variable Length Array In Structures are not and will not be supported in Clang. The final patch to purge them from the Linux kernel just landed in mainline today. "auto growing arrays" sound like "vectors" to me.
Assuming you don't mean the return from main(), zero is pretty easy short encoding to put into a return register, which is probably what you want for the common case. So much so that ARM has a register that's always zero, %xzr.
Sometimes you only need a minimal amount of assembly and it's easier to work mostly in C or C++. Also, maybe there's a inline extended assembly constraint that's tricky to implement without knowledge from compile time (can't think of one off the top of my head). I prefer intrinsics in C code, or separate assembly files to inline assembly, but I'm sure it's the best choice (for something surly horrid).
&gt; It also opens up the possibility that the compiler can inline the function as an optimization. LTO to the rescue.
This is a great answer and probably the best for your situation, I do this all the time. Note that since you're on macOS, you must launch this command from an xterm window in XQuartz, which you can install here: [https://www.xquartz.org/](https://www.xquartz.org/)
use -Y to avoid having X Forwarding suddenly stop working
Macros are also very useful for codegolfing etc, but in normal use cases they shouldn't be over used, as sometimes happens
&gt;Variable Length Array In Structures are not and will not be supported in Clang I don't use clang, just installed a few moments ago (from Debian Stable repos) to test the code and seems to compile just fine using the same flags of gcc: $ clang --version clang version 3.8.1-24 (tags/RELEASE_381/final) Target: x86_64-pc-linux-gnu Thread model: posix InstalledDir: /usr/bin $ clang poly.c -std=c11 -Wall -Wextra -Werror -pedantic-errors $ ./a.out Hello World! A B C D E F G H I J K L M N O P Q R S T U V W X Y Z Hello Again! $ &gt;The final patch to purge them from the Linux kernel just landed in mainline today. What a coincidence! &gt;"auto growing arrays" sound like "vectors" to me. Basically what I mean is this: https://en.wikipedia.org/wiki/Dynamic_array I think in C++ they are `vector`, in Java `ArrayList`, in Python `list` etc...
&gt;Reallocation can change the pointer yep, that's why I use the double pointer to e.g. `IntegerArr` object, also that is the one returned from the allocator and that is the one I should give him back!
tbh I think it's a bit overkill for a CSV library. I guess you could use the wchar stdio functions for both UTF-16 and UTF-8 support, then wctomb and friends to convert it to UTF-8 for API strings.
&gt; yep, that's why I use the double pointer to e.g. IntegerArr object, also that is the one returned from the allocator and that is the one I should give him back! Sure, but if you make the pointer _internal_ to your object &amp;mdash; i.e. make `arr` a pointer, not a VLA &amp;mdash; then you can use your data structure using only a single pointer... or even _directly_, such as by allocating your data structure on the stack.
Yeah, I see your point, a single pointer would be more easy to handle. Maybe I could use `void *` and define `DynamicArr` like this: typedef struct { size_t len; size_t cap; void *arr; } DynamicArr; and then cast `arr` to the right type if I need to (e.g `int *` inside `integer_arr_append`). `realloc` can work just fine with `void *` and the elements size passed as argument (I could even hold it inside the `DynamicArr` struct maybe) and I'll avoid double pointers casts and all that "ugliness".
&gt; and then cast arr to the right type if I need to Well that's one approach. But you can still use part of your "rudimentary polymorphism" idea: typedef struct { size_t len; size_t cap; } DynamicArr; typedef struct { DynamicArr parent; int *arr; } IntegerArr; Again, no casts are necessary. You can simply use the parent object's fields via the `parent` field, as in my earlier post. (And completely off-topic, but I find your chosen naming a little bit... blech. It would be easier to read if you spelt out "array" completely. I know I'm being fussy, but it's only two more characters!)
This subreddit is about programming in C. Please post .NET content elsewhere.
&gt;Again, no casts are necessary. You can simply use the parent object's fields via parent, as in my earlier post. But then how can I realloc `arr` from a `DynamicArr` pointer? I mean, what would be the implementation of `dynamic_arr_resize()`? &gt;(And completely off-topic, but I find your chosen naming a little bit... blech. It would be easier to read if you spelt out "array" completely everywhere. I know I'm being fussy, but it's only two more characters!) I know, I suck at naming...
Have it receive the old pointer and return the new pointer, or return NULL on out-of-memory. Assign the new pointer as required inside `integer_arr_append`.
&gt; Have it receive the old pointer (as a void *) and return the new pointer (again, as a void *), or return NULL on out-of-memory. If it's not NULL, assign the new pointer inside integer_arr_append. &gt; &gt; Of course, if no resize is necessary you can just return the old pointer again. Oh yes, something like `dynamic_arr_ensure_space(DynamicArr *darr, void *old_p, size_t n)` and `dynamic_arr_resize(DynamicArr *darr, void *old_p)` and then inside the append functions: ... int *new_p = dynamic_arr_ensure_space(iarr-&gt;parent, iarr-&gt;arr, 1); if (new_p == NULL) { return false; } iarr-&gt;arr = new_p; ... that would be neat, thank you! 
Well we can guess all day but, if what you say is true, you're talking about one application that not everyone uses, plus he's talking about command line tools. The poster makes it sound like a number of these are out dated so that's unclear. I don't use a Mac on a regular basis but, when I have for work, I've not found any issues with such things at all.
Note that `termios.h` is not a library on its own. It's just a header file containing declarations for a bunch of functions in the C standard library. Your platform's vendor should provide you with documentation for all the functions supported by your platforms C standard library. For a list of portable functions, refer to [POSIX](http://pubs.opengroup.org/onlinepubs/9699919799/) or the C standard. For other libraries, there is no central directory. You need to do research libraries yourself. Always assume that there is a library that does what you need, but beware that each library is a dependency you need to keep track of, so don't overdo it. Prefer to implement functionality yourself if it isn't too difficult.
If you look at the "graphics with C thread", someone posted advice on using graphics.h https://www.reddit.com/r/C_Programming/comments/8pzb7c/graphics_with_c/e0gkkf2/
I plan on doing that as my next step, thanks! For my initial implementation I wanted to control as many variables as I can, so forcing the filepath makes sense but a few other suggested implementing a callback interface. I’m kind of trending in this direction because implementation wise, I can force the callbacks to figure out how to efficiently maintain their buffers. For processing the data all I really need is a way to open and close a stream, and request the next character. I could also use callbacks to put a character into output buffers of the users design. I like this because the library can then implement the simplest cases like those provided by `stdio` which would satisfy 95% of everything in the real world. I’m also considering adding an ‘extensions’ folder and some CMake variables to control this so i could allow bindings for zlib, gzip, etc. and keep the core library dependent on stdio alone since this shouldn’t cause any issues. Off the top of my head, I believe `getchar` from stdio returns an `int` so you can check for EOF so alignment with the standard isn’t too far off. 
It is good that you ask these questions. Such design choices are very important but are rarely discussed in this subreddit. On `csvfield`, firstly, when we read a CSV, we want to see fields most of time and rarely need lazy evaluation. Secondly, separating fields is fast: that is one pass through a line with no memory operations for the second line or above. It will be faster than many types of downstream operations, and even faster than careless line reading – in C, it is hard to properly read lines of arbitrary length. Thirdly, you can still implement lazy evaluation when calling, say, `csv_rec_field(record,index)`. You don't need `csvfield` to achieve that. Tbh, you lack experiences in API design and in C programming in general. I reiterate that writing code down is more important than overthinking about every possible feature at your current level. It takes years to become a good C programmer.
Use a macro only if you can't use an inline function. Someone said text substitution already. Needing to make the caller return is another example. Alloca. Creating delcarations. Macros are much, much easier to get wrong. If you pass in an expression as an argument, and the macro refers to that argument multiple times, the expression is evaluated multiple times. If you forget parens, you can get really surprising results. But what do most programmers do? They overuse macros, and write buggy code as a result. Try to be a good programmer instead.
If you know where your `man` pages are installed you can browse the folders with section 3-'s in it. I actually haven't always had man pages installed... But if you got them use them! Browsing the directories themselves is so you wouldn't need to know any keywords to search for. The tech world has moved away from local documentation and towards online search, Q&amp;A, extensive encyclopedia things... But local documentation still has a place!
&gt; Always assume that there is a library that does what you need I'm not OP, but sometimes I don't know what I need until I see it used. 
Much of it will be caught by just watching what others do, like the tutorial you're following. You'll see some frequently used libraries, and pick up on some key words for searching. Occasionally you'll see here curated lists of libraries, those can be good starts. Look at the dependencies of other projects (e.g. something might depend on nspr) and see what those do as well as what other libraries do similar. You'll soon see some that seem more popular (ncurses, sbcp, etc.). But, as /u/FUZxxl said, find the balance between writing your own (fewer dependencies to manage) and reusing others code (reinventing the wheel can be buggy). A couple of classic cases of *not* writing your own are encryption and randomization. But writing your own hash map is likely a wise choice. 
You can always ask about how to achieve something.
(I accidentally deleted my last comment, so rewriting it) * There are occasionally curated lists posted here. Those are useful. * You can pick up a lot by just watching other discussions, and doing tutorials like you've done * Look at popular projects, or ones that interest you you, and see what they use. For instance, one may use nspr - what does it do, what alternatives are there, and why was it chosen? * You should eventually see some big library maintainers - like GNU, 
True. 
Reason #3 (though rare and should be used with care): 3. If the code needs to return from the function. A good example is parser authoring (when authored by hand). The buffer bounds test can be wrapped in a macro, allowing the function (not an inline function, but the actual function) to return in cases where the buffer was consumed (preventing overflow).
Thank you for your advice! My lack of C experience is precisely what I'm looking to remedy with this project. I've learned a lot from the comments so far, enough that I'm going to refactor the API a bit more before I dig into implementing. Earlier in the post I mentioned most of my last few years has been spent with Python, as such my intuitions are 'adjacent' to C but I'm studying everything I can get my hands on, like K&amp;R, and I'm very appreciative of all the advice I've received so far. At the same time in Python I regularly use patterns similar to callbacks, like passing functions and classes as arguments and calling them. You just have to be a bit more specific about the arguments in C. i.e. no using `callable(*args, **kwargs)`, but if I remember correctly, C's `void*` is a nice way to allow arbitrary struct pointers to be passed to callbacks. One more question on why I should or should not implement a Field object -- in my experience CSVs often represent a flat file version of a database table. I wanted a way to represent the difference between NULL as being distinctly different from an empty string. Would a normal representation of a empty string be similar to the pointer returned by `calloc(1, sizeof(char))`? 
Do not post these. This is your final warning.
Why are you multiplying `i` with `sizeof(t_symstruct)`? Recall that `a[i]` is shorthand for `*(a + i)`, not for `*(a + i * sizeof a[0])`. If you increment a pointer, the pointer is advanced to the next object, not the next byte. Multiplying the offset with the object size is wrong.
You are correct, that is what was wrong. Thanks!
`char *p = NULL` is a NULL string. `char *p = calloc(1,1)` is an empty string.
Not sure this is the best place for this, you might have better luck in computer science or data structure subreddits. BUT:. My initial thought is something like gcc's std::deque, where it allocates chunks of arrays and then links them in a linked list ish fashion.
I don't know the answer so I'd approach it like this: 1. 2O(n). O(n) for this loop-through-once to get counts in a link list queue; then O(n) to copy those into an array. Prolly 2(n+C) memory. Feels silly to iterate twice, especially when one of the loops creates a usable memory structure for you. 2. Periodic resizing. There are a wealth of implementations of things like this. I'm pretty sure nearly every standard lib in the higher languages, with an Array class that supports resizing does something like this. So... I mean... I feel like the periodic resizing thing is a well worn route for a reason. That first idea seems... silly. Your link list based queue is going to use up extra memory... why not just loop through and get a count, then make an array, and copy them in? Still two loops, but saves you all this waste memory moving?
I like the last idea of looping through just to get a count, and then just declaring an array of that size. Mind you, it would be based on the number of lines in a file, so the way this would probably work is counting up the lines using fgets(), declaring the array of the appropriate size, and then using fseek() to go to the beginning and actually read in the data to store into the array.
ahhhh ok well the file thing brings in a bit of an interesting wrinkle, cause youd love to not have to read the file twice, for that IO. so the memory thing might be more desirable, than avoiding it. trade of - more memory at lesser io. or, could do some quick math based on the filesize, and create an array that is large enough (ie, estimate high) and then use that and lop off the stuff at the end - going through only once this way.
I need a lot of random access and good search runtimes, so I don't want to leave them in the linked list. The file size idea is interesting, but it's too variable of a factor to reasonably accurately predict the number of elmeents in my case, so I think I'll just go with the original idea and use extra memory for lesser IO like you said.
I think an array is usually the best way to store this type of data. Just don't resize and copy *every* time the data exceeds the capacity of the array. Try doubling the size of the array when you reallocate it -- this will reduce the number of copies etc. In C++ there is a type called a vector that does this. I used to write something similar in C as a structure I called a "buffer" which did something similar. If I was to write it today, I'd borrow the behavior and nomenclature from Go's slices.
Array resizing is probably the way to go tbh. You mentioned that the data comes from a file, so reading from a disk is probably quite slow compared to other operations, which is why I wouldn't loop over the data first. Anyway, here is some analysis of array resizing: A linked list for n elements will require at least one call to `malloc` for each element. Then you would have to `malloc` the new array once you know the total size, copy over the elements, and `free` once for all of the list elements. The total space would be at least twice the size of your array (the space for the array plus the space for the list), and you need to copy your data twice (once from the source to the list, and once from the list to the array). Not to mention a linked list has O(n) memory overhead. The alternative: a resizable array. Let's say you create an array with 4 elements initially. If you want to add a fifth element, use `realloc` to double the size of your array to 8. when you're inserting the 9th element, `realloc` to make space for 16 elements etc. `realloc` will automatically copy your data over to the new pointer. Note there may be enough memory when you call `realloc` so it doesn't necessarily even have to copy your data around! It could just allocate new memory where it is. Memory: If we keep doubling your array size, when it becomes full, then the absolute maximum memory we need to use is twice the size of the array. The minimum memory required for the linked list is double that of the final array, so the worst case memory performance is at least as good as the best case for the linked list. When we're done, we can free the excess data after the array using `realloc`. `realloc` should be able to free the excess memory after the array without having to copy it. Winner: array. Performance: * We only have to call `realloc` O(log n) times for the array. We need to call `malloc` O(n) times for the list, Winner: array. * The array needs one call to `realloc` to get rid of any remaining memory, the linked list needs O(n) calls to `free` to get rid of all of it's elements. Winner: array. * If we only have to copy our structures when the size of the array doubles, then potentially half of the list doesn't ever need to be copied. 1/4 will need to have been copied once, 1/8 will have been copied twice, 1/16 will have been copied three times, etc. In the best case scenario, the array wins because `realloc` didn't need to copy anything. in the worst case we have to do up to 2 copies per element, so it needs more copy operations. Winner: toss up. All in all, I'd probably use a resizable array myself. Most implementations use this. If you're curious, you could test this to see which one is better.
Wow thanks for the breakdown. It makes sense now. Thank you.