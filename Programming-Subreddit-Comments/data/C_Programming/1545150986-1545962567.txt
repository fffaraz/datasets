I've seen a lot of things in my time.
AFAICT your asm code is exactly equivalent to `migration_sql = full_name`.
Please indent your code with four blanks so it appears readable.
Done, thanks.
Looks good now. But you do realise that you wrote L”%ls” instead of L"%ls" in some places?
Thanks - ya I’m posting from iPhone, so not in full control of that.
&gt; I fear I’m missing something fundamental: wchar_t myArray[wcslen(myString)]; Strings are terminated with nul character, so you must always allocate 1 more character than strlen says.
Where is your call to `GlobalUnlock()`? It would help if you provided an [SSCCE](http://sscce.org/) instead of just snippets.
I do have that call - was just hoping I was missing something glaring, like “Of course your string is being overwritten in memory because of xyz.” It haven’t been able to reproduce my issue as a SSCCE on account of the issue occurring within a .dll, but I’ll keep working on it. Thanks for the reply.
Good call - thank you!
Why are you using assembly for this? This is......just the entirely wrong way to solve this problem. &amp;#x200B; https://xyproblem.info
I'm sorry, but I don't really understand how your `__asm__` statement is supposed to work. This is just equal to `migration_sql = full_name`. If you want to access these variables from C, it's as simple as declaring: extern const char migration_201812001[]; extern const size_t migration_201812001_size; If you want to load the data “dynamically,” i.e. without having to hard-code the name, consider making a data structure containing triples of names, sizes and pointers so you can later traverse the table and access the record you need. Something like this: struct sql_record { const char *name; const char *record; const size_t *size; /* pointer to size */ }; const struct sql_record records = { "201812001", migration_201812001, &amp;migration_201812001_size, ... NULL, NULL, NULL }; The last record terminates the array. I believe you auto-generate the assembly for including the SQL data in your binary, so auto-generating this table should be easy, too. 
I was asking about `GlobalUnlock()` because I suspected that you were calling it prematurely. Now you tell me you aren't calling it at all, even though the [documentation](https://docs.microsoft.com/en-us/windows/desktop/api/winbase/nf-winbase-globallock) says you are required to. For accessing the Windows clipboard, the sequence of steps is supposed to be: 1. `GlobalLock()`. 2. Make your own copy of the the buffer. 3. `GlobalUnlock()`.
Would it be easier to read the data from static/migrations/\*.sql instead ? No recompiling :)
Did this fix it?
The I idea is to ship the static files content with the binary, after some digging I ended up using assembly to copy the file binary into the C code
&gt;R The I idea is to ship the static files content with the binary
There are so many better ways to do this. &amp;#x200B; [http://www.tutorialspoint.com/unix\_commands/xxd.htm](http://www.tutorialspoint.com/unix_commands/xxd.htm) &amp;#x200B;
Thanks - playing with the positioning of that call does affect the results, so it is definitely a pertinent observation. I’ll do some more reading on what that call is doing. By ‘making my own copy of the buffer’ - is what I did valid? swprintf(myArray, wcslen(myString), L”%ls”, myString); Or is there a better way to accomplish this? Again thanks! 
If you’re typing on the iPhone, hold down the quote key for a second to bring out the secondary characters. The straight quotes are rightmost for me. https://i.imgur.com/eOvU0FX.jpg
I had no idea! Thanks!
Drach88 provided a rather nifty solution to the problem
You can draw out a table similar to my one taking that change into consideration.
That's just an algorithm (not even the best one by the way), not a library.
I more wanted to know why I couldn’t just dereference a pointer to a string from the clipboard at any given location in my program. I believe the answer has to do with the proper use of GlobalLock, copy text, GlobalUnlock - as indicated elsewhere in this thread. Still unclear on whether my choice to use swprintf is the most appropriate for copying the buffer.
You’re welcome!
It's unnecessary and inefficient to use a string-formatting function of all you're doing is copying the string. Since you're on Windows, I suggest you use [`StringCchCopyW()`](https://docs.microsoft.com/en-us/windows/desktop/api/strsafe/nf-strsafe-stringcchcopyw) instead.
I don't understand why such a requirement. Not my business I suppose. Have you considered just appending the data you have to your binary, and appending the size of that data last. Open argv\[0\], seek to end - integer, read the size, seek again and so on...
I found [this file](https://github.com/robertbachmann/openbsd-libc/blob/master/include/namespace.h) which has some useful comments.
 $ cat -n sp.c 1 #define _XOPEN_SOURCE 600 2 3 #include &lt;stdio.h&gt; 4 #include &lt;stdlib.h&gt; 5 6 int main(int argc, char *argv[]) { 7 int looper = 0; 8 void *foo; 9 printf("This is a string.\n" ); 10 while (looper&lt;32) { 11 char *str0 = "hello"; 12 foo = realloc(str0, sizeof(str0) + 1); 13 printf("addr of foo = %p and str0 = %p\n", foo, &amp;str0); 14 looper+=1; 15 } 16 printf("This is also a string.\n" ); 17 18 return (EXIT_SUCCESS); 19 } 20 $ gdb -q sp Reading symbols from sp...done. (gdb) break 11 Breakpoint 1 at 0x4008f9: file sp.c, line 11. (gdb) run Starting program: /usr/home/dclarke/pgm/C/stackframe/sp This is a string. Breakpoint 1, main (argc=1, argv=0x7fffffffe930) at sp.c:11 11 char *str0 = "hello"; (gdb) print str0 $1 = 0x0 (gdb) print &amp;str0 $2 = (char **) 0x7fffffffe8a8 (gdb) step 12 foo = realloc(str0, sizeof(str0) + 1); (gdb) print str0 $3 = 0x4009d2 "hello" (gdb) print &amp;str0 $4 = (char **) 0x7fffffffe8a8 (gdb) step Program received signal SIGSEGV, Segmentation fault. 0x000000080075f59e in realloc () from /lib/libc.so.7 (gdb) quit A debugging session is active. Inferior 1 [process 17714] will be killed. Quit anyway? (y or n) y $ 
I just deleted it. Seems to be some gcc specific linker parameter!
Perfect! I knew there must be a better way. Thanks again.
If your talking about google test then I am using it for C++ project only. I happen to like the framework.
`wcsdup` does pretty much exactly what you want, as long as you `free` afterwards. If you’re declaring the dest variable on the stack, you’re risking a crash if the string is very long.
There’s an older, non-GCC way to define a weak symbol via pragma, either #pragma weak a=b or equivalently _Pragma("weak a=b") Those define that `a` is a weak alias for `b`. The GCC attribute `__weak__` does the same thing, and it can be used with `__alias__` to create an alias. Weak symbols allow anything linking against the object to override them, rather than erroring out because there are two definitions of the same symbol. So e.g., if you wanted to override `malloc` and `free`, you could just define your own normally (strongly or, as long as things are linked in the right order, also weakly) and the linker would hopefully use yours. Similarly, if you wanted to provide a default `main` to anything linking against your object/library, you could export it weakly.
I'm sure there are better ways to do this, but here's a function that dynamically grows an array of strings, void grow(char ***buf, int *len, int *size) { if (*len &gt;= *size) { *size *= 2; if ((*buf = realloc(*buf, sizeof **buf * (*size))) == NULL) err(EXIT_FAILURE, "realloc"); } } &amp;#x200B;
Oh, is this the game we're playing now? Editing our comments when others point out errors in our code, then pretend the error was never there? Anyway, your code invokes UB, so predictions such as “you are going to get a massive `SIGSEGV`” are meaningless outside the context of a specific implementation, and possibly not even then. I have a `malloc()` implementation (part of a test harness) which will throw `SIGABRT` in this case. Other implementations may deterministically throw `SIGSEGV` as you say, or they may return `NULL`, or they may mistake `str0` for a valid allocation and trample all over the program's pre-initialized data segment, which on some systems (DOS .COM files, for instance) is identical with the code segment. You don't know, you *can't* know, and you shouldn't assume.
game. There is no game. I was attempting to provide a workable example. Go away.
Questions like these can be answered with the man pages: https://linux.die.net/man/3/realloc
Read the documentation and learn the syntax of the language properly. [https://en.cppreference.com/w/c/memory/realloc](https://en.cppreference.com/w/c/memory/realloc) `memoria = realloc(memoria, sizeof(void *) * size);` Is how you would properly reallocate your memory block.
You're missing a parameter. [Documentation for `realloc`](https://en.cppreference.com/w/c/memory/realloc). There's an example at the end. One thing to note, `memoria = realloc(memoria, size * sizeof *memoria);` contains a subtle bug, in that if the `realloc` fails you've lost the pointer to the original memory. That's why the example assigns to a second variable. After checking for errors you'd usually assign `memoria` to this new pointer.
If your best answer is "just look at the man", what is the use of this subreddit...
To tell people to RTFM.
This makes its argument a weak symbol, allowing others to override it.
While there are some circumstances where code might be able to usefully recover following an allocation failure, it's much more common that programs will need to do a tasks that require a certain amount of memory. If they can't get the memory they need, they simply won't be able to perform the indicated tasks. If code which uses `realloc` would be prepared to cope with the fact that the allocation hasn't been adjusted, then it would make sense to save a copy of the old pointer so it can be used following an allocation failure, but in the more common scenario where code cannot meaningfully precede after an allocation failure it's unlikely to matter whether the program frees up the old block before it exits. Further, effective handling of low-memory situations generally requires a more powerful API than the one provided by the C Standard, including support for multiple memory pools or defragmentation. Some operating systems may provide such features, but code which uses them typically wouldn't be using `realloc`. 
I'm glad to see that the prices are reasonable. I've configured (very close to) the same machine that I'm currently using and the delta was only ~30% (from $1500 to $2000).
You will get multiple definition errors, say, if you include the same c file in 2 files. 
You can, in fact, #include any file you want (.txt, .pdf, .html, etc). Wether doing so results in a valid C program is another matter. Writing everything in .c files and #including them into one another is very much possible. However, doing so will probably end up with you having .c files which look pretty much like .h files.
If you include .c files you would waste a lot of memory. If you just include function declarations and later link them you use less memory. C was written in the 1970s and memory was valuable.
Thank you I’ll play with that too!
What other commenters said is relevant. TL;DR multiple definition error. Not an expert, but I've been using C and C++ for some time. If I'm factually incorrect I'd be thankful if anyone corrects me, learning more and more about C is always good. Let's say you make a file called five.c: int five(){ return 5; } It's so useful and amazing, you make eq.c: #include "five.c" int eq_five(int n){ return n == five(); } And use it in main.c: #include &lt;stdio.h&gt; #include "eq.c" int main(){ printf("%d\n", eq_five(2)"); printf("%d\n", eq_five(5)"); } And everything works dandy. But then you make another file neq.c: #include "five.c" int neq_five(int n){ return n != five(); } If you include both eq.c and neq.c in your main.c file, you will get a **redefinition of 'five'** error. That's no good. So let's make a five.h: #ifndef FIVE_H #define FIVE_H int five(); #endif And let's include five.h instead of five.c in eq.c and neq.c. Now the program compiles. But why do we need this? Include statements, essentially, copy-paste the contents of the included file. So, going back to the not-fixed code, this: #include "eq.c" #include "neq.c" int main(){ return 0; } turns into this: #include "five.c" int eq_five(int n){ return n == five(); } #include "five.c" int neq_five(int n){ return n != five(); } int main(){ return 0; } As you can see, five.c is included twice. So the preprocessor will paste the contents of five.c twice, thus defining the five() function twice. Let's get back to the five.h header: #ifndef FIVE_H #define FIVE_H int five(); #endif ifndef ... endif is a conditional preprocessor block - ifndef means **if** **n**ot **def**ined, therefore this block will 'execute' if and only if FIVE_H is not defined. At first FIVE_H is not defined, so we enter the ifdef--endif block. And the very next thing we do is **#define FIVE_H** - it's no longer undefined. Since we're already inside of the block, it will continue running, and hit the next line - **int five();**. And then we exit the block. When, as was said a few lines up, the file gets included the second time - FIVE_H would already have been defined, so we won't enter the **ifndef** block, and we won't define the function twice. Problem fixed. That's a long comment.
You can if you want, you can even put code int .h and compile them and put all you declarations on .c files and make a project where everything is backwards, it'll work just fine. It's all just convention, but that being said you don't gain anything by going against convention and you do add more confusion.
Here are some better options: [Baking Data with Serialization](https://nullprogram.com/blog/2016/11/15/)
You could write your entire application in one .c file too, but anyone who has to work on it later will make a voodoo doll in your likeness to get their revenge. It's mostly a convention adopted to help find parts of the code such as defines and declarations. You can and often do put declarations and definitions that are only needed internally in a .c file. You can and often do implement the body of simple functions in the .h. Things like this you choose on a case by case basis of what makes sense in context. Normally for things other code may need to use, like the public interface other code NEEDS to know to use your code, you separate those into .h files so that what they need is in one easy to find location and separate from the implementation because you don't normally need to know that detail to use it and it just makes the interface harder to read with it all lumped together. Compilers have added the ability to precompile headers since it's a good bet that you'll change the .c files more often than the headers (declarations, defines, etc) but it's not required. That's just something added later once people had standardized on what .h files were being used for and it can speed up builds. It's just a convention like using indentation to make the code more readable. You could write one long, long line correctly and never need a carriage return, but that would suck to read. You don't have to use .h files, but it's expected by normal society.
&gt;f you include .c files you You're confusing "memory" with "storage", there are two different stuff. A header file is a just way to make your code more organized, that's the advantage; The disadvantage is in order you create a header file, you've to write the function's prototype and definitions, e.g.: main.c: #include &lt;utils.h&gt; int main(void) { print("reddit"); return 0; } utils.h: #include &lt;stdio.h&gt; void print(const char *restrict format, ...); void print(const char *restrict format, ...) { printf("%s\n", format); } main2.c: #include &lt;stdio.h&gt; void print(const char *restrict format, ...); void print(const char *restrict format, ...) { printf("%s\n", format); } int main(void) { print("reddit"); return 0; } Now, look what we have: The source is greater when you're using splitting method (header file) $ ls -lh main*.c utils.h -rw-r--r-- 1 slayer slayer 199 Dec 18 21:43 main2.c -rw-r--r-- 1 slayer slayer 74 Dec 18 21:37 main.c -rw-r--r-- 1 slayer slayer 144 Dec 18 21:41 utils.h The binary have the same size even though they're compiled seemingly differently: $ gcc -Wall main.c utils.h -o main $ make main2 CFLAGS=-Wall cc -Wall main2.c -o main2 $ ls -l main{,2} -rwxr-xr-x 1 slayer slayer 15504 Dec 18 21:42 main -rwxr-xr-x 1 slayer slayer 15504 Dec 18 21:43 main2 $ sha256sum -b ./main{,2} bc95f72939a019bba51c7ca730430798acdcb17a0bffee7c5757471f80013feb *./main ae8dbd41370c988ed1ed105fc6a7d17981514f677fe6a791d844a48b9f6f1f36 *./main2 $ ./main &amp;&amp; ./main2 reddit reddit Note: I'm using VM with timezone unadjusted.
I wasn't alive in the 1970s but if you separate files I think you could then load some into RAM and save the others into the disk instead of loading everything at once into RAM. You are not helping either by stating what is the difference between "memory and storage".
You really have no idea. Working in the finance and insurance industry for some while now. Also there is no such thing as "close enough" in payment computations. Otherwise balloon payments end up at the bottom of the amortization table. Go away .. don't reply. 
This is the "dynamic linking" and the IBM already used this technique at this time.
This is a great comment, but it doesn't explain to OP why header files are used over plan c files. There's nothing stopping you from using #ifndef FIVE_H #define FIVE_H int five(); #endif in a .c file
And that is why I am suggesting header files were used in the beginning and therefore made it into our modern C days. I really don't understand all the downvotes. I guess Ill stay ignorant since no one has corrected me.
Are you talking about the memory of the compiler itself? Because that is impossible to generalise about. Including a file in C is just telling the compiler "insert the contents of this file right here". It is done by the preprocessor (before the compiler proper) and has no bearing on linkage or even compilation. Including another C file is the exact same thing as opening that file, copying all of it, and pasting into your first file. Memory usage of the resulting application has no casual connection to how you decided to arrange your source files. The reason you usually only have function declarations in H files is that the functions are defined elsewhere. &gt;C files also need declarations of functions to work anyways. Strictly true, since a definition is also a form of declaration. If a function is only used in one compilation unit, the definition is also the definition, as long as you only use it after it was defined.
I haven't dope too far in it. If i can at least get the code code to work then I can abstract in such a way that I can use various algorithms from a debug standpoint and get started with that. If the algorithm proves good enough for the user than that's a plus. Of course I'd be looking into updating the algorithm in the future.
If your software is closed-source you don't want people reading your .c file. You give them a stripped, compiled .so, and in the .h file you have your prototypes and whatnot so they know how to use the functions you are providing them in the .so. Your black magic is safe.
Also GNU #pragma once
I am stating that it takes less RAM memory, which was important in the 1970s, to just copy and paste function prototypes than to copy the whole function prototype and source code into the file and that is why headers are used. 
To be clear: Are you talking about the memory usage of the compiler or the resulting application? It seems from this post to be about the compiler, whereas your first post just said that "you" waste memory, and it was unclear at which step that wastage happened.
The files are gonna be copied and pasted, only god knows when and its gonna take more memory to copy function prototypes and source code than just function prototypes. That is my point.
So... The compiler's memory usage. That was the thing that was unclear, is *my* point.
Does it matter? It is wasteful and unnecessary to copy functions source code regardless of when it happens.
He's not entirely wrong. If you include `.c` files with function definitions in separately compiled object files and then try to link them, you'll end up with linker errors. To avoid the multiple definition errors, you would then need to make the functions declared in the `#include`d files `static`. Once they're static, there will be one "copy" for every object file, and this will bloat the binary size and the runtime memory footprint.
What? How could it happen anywhere but for the compiler? Binaries don't copy their own source code?
Does it matter in this discussion? No because we are talking about RAM usage. Take care.
&gt; RAM usage You still seem to not understand (or more likely, willfully misunderstand for some point) that both a compiler *and* the resulting application are using memory.
The main reason I don't include a C file directly is because a header file is effectively an interface that can be swapped out at compile time. E.g.: if are building for two different system types, then the system specific code can go in separate C files but from the point of view of the rest of the code, all the details are in the header file. The linker will determine the final code. 
Im just talking about RAM usage during compilation stage (preprocessor, compiler, linker whatever needs to be done). Nothing of what I have said has anything to do with the memory consumed by the compiled application when ran yet you keep bringing it into the conversation. 
No, I kept asking, because you seemed to miss that you never actually specified it in your original post.
The C preprocessor is a powerful tool. In fact, it's not just limited to C, you can run GCC in preprocessor only mode to transform files. I've done that with linker scripts for embedded systems so that address definitions for code and linker can live in exactly 1 file. (without the C code having to play tricks with external symbol definitions).
Pragma is a demon spawn . 
You can absolutely just put everything in a .c file. Those #include directives do precisely that as part of preprocessing. At some point though you'll look back and realize that separating headers and source is one of the best things about C/C++ because it encourages expressing interfaces between modules (files) separately from their implementation.
Code is divided into smaller files for: 1. better code organization, 2. parallel compilation, 3. quicker recompilation (compilation of only changed source files). Including entire .c files breaks parallel compilation and quick recompilation. For small projects it does not matter. For medium to large projects, breaking code into units is a necessity.
'pragma' isn't C standard, it's a compiler thing.
That's a great question actually, and I thought of two reasons: First off, let's say you have a 1000 line .c file and a 10 line .h file. When you include the .c file, then in the compilation process those 1000 lines will be copied each and every time the file is included, which means you'll need more RAM to store the entire thing. Compilation may take longer because of this. Instead, you can include the .h file - it only has 10 lines of code, so those problems don't appear. The second, and IMO more important reason, is distributed compiling. Two situations: One, you have one big file you've written perfectly, that takes time to compile, and you want to include it somewhere. You could include the entire thing and re-compile it each time, but you don't have to - you can just compile it into an object file once, and use that already compiled object file. But you can't include the object file - it's a compiled binary - so you need a liaison between the compiled object file and the file you want to include it in. The header has all the plaintext function definitions in it, and it can be connected to the object file by the linker - thus you're able to include an already compiled file somewhere else. Two, let's say you want to give your project to someone, but don't want them to see the code - making a closed source program. You can just give them the already compiled .o object file which they can't easily read, and the .h header file so they know the names of functions and, of course, so they can include it. Those are my thoughts, if you have any better explanations it'd be awesome to hear them.
Then pragma once ia a nice &amp; tiny demon that saves me from repetetive include guards.
What modern C compiler doesn't support it?
You're getting downvoted because you are wrong and you refuse to listen to people who are trying to correct you. The division between .c and .h has nothing to do with saving storage or memory. In fact, you need *more* storage and memory if you divide your code into several files than if you keep everything in a single file. It has to do with organization, modularity, and separation of concerns. A header file contains information which is shared between multiple source files, typically types, macros, variable declarations, and function prototypes. A library will usually ship with one or more header files which describe the interface that the library offers to applications. Without those header files, you would not be able to use precompiled libraries, but would have to recompile everything into every application every time.
A few drivers are designed as several modules working in concert. How about an API module with RTC_Init(), etc so your interface is stable and not too verbose, then a module as needed per device that registers with the API module. The API module knows which device is associated with which driver and passes it to the right driver module and as long as the device modules are loaded it can report that it's too busy to be unloaded. Essentially the function pointer table, but it solves loading both hardware drivers if you don't need them.
No, conforming C implementations are not allowed to return a non-null result if allocation fails, and I would be shocked to hear of one that does. They *are* allowed to return a non-null result for an allocation of 0 bytes, but that is a completely different matter.
*Rubik's
Quite a number of AVR compilers, also some ARM compilers. &amp;#x200B; When you're into embedded programming you generally can't use things that aren't strictly in the C standard.
Considering how `#pragma` is a preprocessor directive, you can either run the program through the gnu cpp before the compiler, or replace the one provided if seperate.
Both parts support setting and getting date and time. NXP part has additional features, which would be EOPNOTSUPP on Dallas... I would start with something like struct rtc { i2c_device *location; int (*set)(rtc *, struct tm *tm); int (*get)(rtc *, struct tm *tm); struct rtc_alarm *alarm_option; struct rtc_timer *timer_a_or_b_option[2]; }; struct i2c_device { i2c_bus *bus; int address; }; struct rtc_alarm { int can_use_ymd_hms_bitmask; int (*set)(rtc *m, wday,h,m,s); /* or maybe struct tm here too */ int (*get)(rtc *, ...); }; &amp;#x200B;
The context of that statement was "some Unix configurations". From a Linux [malloc manpage](http://man7.org/linux/man-pages/man3/malloc.3.html): &gt; when malloc() returns non-NULL there is no guarantee &gt; that the memory really is available. In case it turns out that the &gt; system is out of memory, one or more processes will be killed by the &gt; OOM killer.
That's like saying that `fopen()` should have predicted that your disk was going to fail while you were reading from the file and returned `NULL` instead of a valid stream pointer.
And what are you attempting to avoid -- is it code duplication, difficulty in retargeting to different architectures, inefficient code paths, binary size, etc? That answer will also drive your decision. I'm also in favor of the pointer table with a registration call on the included file. That's essentially how the kernel does it. I'd also default set all the pointers to functions that immediately crashed with a stack trace. 
I'm torn between correcting your analogy to something like &gt; `fopen()` didn't bother to check if the disk even exists before returning non-NULL or simply pointing out that we live in the real world and have to deal with our tools as they currently exist. Lazy allocation is a thing. Since it originates in the OS layer, you must mentally step outside the C sandbox if you want to handle it.
Aren't header-only **libraries** like what is available [here](https://github.com/nothings/stb) basically the same thing?
So this is definitely what I was looking for, thank you. In case anyone references this, I had to use `_wcsdup`, not `wcsdup` to achieve desired results.
Sounds about right for what I want to achieve. In this case, would a sensible flow of operations be to: 1. initialise the device-specific driver (in my case, _PCF8523_Init()_) 2. have the device driver's Init return a pointer table containing its own API functions 3. pass that table to the _RTC_Init()_ API module which keeps an internal copy 4. use only _RTC_...()_ functions onwards in the business logic layer or am I overcomplicating it?
Thank you, looks like a good starting point. I wasn't aware of the _EOPNOTSUPP_ value, I see it's a part of _errno.h_ - I'll have to dig into this further, as my current development environment (not the IDE, mind you) does not really use a structured or elegant way of standard error handling. It doesn't get more standardised than a C standard library.
&gt;Unfortunately, the formatting got mangled, and I'm having trouble reading it. For example, the asterisks were consumed turning some of the text into italic.Could you try indenting everything by four columns and try again? Backquotes don fixed it.
Are you programming on Windows? If so, recall that on COFF targets like Windows, C identifiers are decorated with a leading underscore when turned into symbols. So if you want to refer to a symbol as an identifier, remove the leading underscore in the C source. Additionally, your types are wrong. The type of `binary_baked_txt_start` and `binary_baked_txt_end` should be `char[]`, not `char*`. For `binary_baked_txt_size`, special precautions need to be taken as the size you want is the address of the symbol, not the content of memory at the symbol. The easiest way to cope with that is to declare extern const void binary_baked_txt_size; #define BINARY_BAKED_TXT_SIZE (size_t)(&amp;binary_baked_txt_size) This should yield the desired result.
Thanks for the answer, butcould you please elaborate what the difference between `char *` and `char ...[]`?
I'm trying to avoid high binary size (by not compiling or loading things I do not need for a particular application) along with trying to avoid sloppiness and spaghettification in my own modules.
Remember that a symbol refers to the address of a variable, not the variable itself. If the variable has type `char *`, then the symbol refers to the address of a variable containing a pointer to a character. This does not seem to be the case in your example. If the variable has type `char[]`, then the symbol refers to the address of a variable containing a bunch of characters. This seems to be more like what you have. To illustrate the difference, consider these two declarations: char *ptr, arr[]; Now it is possible to assign a new value to `ptr` because the variable `ptr` stores where `ptr` points to. The same is not possible with `arr` because `arr` is not a pointer and you cannot change which array it refers to: ptr = new_ptr; /* legal */ arr = new_arr; /* illegal */
Thanks again!
https://en.cppreference.com/w/c/memory/malloc That means that they are non-conforming. 
I would expect the API module to be loaded and init'd before any driver specific modules are init'd. Especially because if you load two driver modules then you'd end up initing the API module twice and that seems dodgy. I would have that API module define registration and deregistration functions. It gets loaded and init'd ahead of the device specific module because modprobe handles dependencies. Then the first device specific module works out its function table and passes it to the registration function when it is init'ing. Then the second device specific module does the same. These de/registration functions are good points for the API module to update its state about if it can be unloaded or not. Then yes to #4. The only thing that talks to the device specific modules is the API module and everything else in kernel and userspace talks to the API module unless there's a *really* compelling reason.
It's one of network errors, forgot that, sorry. ENODEV "Operation not supported by device" could be more appropriate. Maybe defining one's own errocodes would be better, with REFER\_TO\_ERRNO when appropriate.
Thanks, that's very thorough. I'll have to read up on how modprobe works, but I think I've got a good idea of how to organize it properly.
is `char arr[]` equivalent to `char * const arr`?
No! The `const` qualifier does not change how the datum is laid out in memory.
Exactly, glad I asked :)
The Standard gives implementations essentially free reign as to what sorts of translation limits they may impose, and what they may do if those limits are exceeded, provided only that they are capable of processing at least one possibly-contrived-and-useless program in a fashion consistent with the Standard. The authors of the Standard expressly acknowledge in the published rationale that a "conforming implementation" might succeed at being totally useless, but regard the ability to process useful programs as a "Quality of Implementation" issue. What the Standard should do, but presently doesn't, is recognize categories of programs and implementations with the following traits: 1. A Safely Conforming Implementation must document a set of environmental requirements and failure modes, and if it is given a Selectively Conforming Program and its environmental requirements are met, it must either process it in accordance with the Standard (hanging without side-effects would be considered indistinguishable from processing a program very slowly) or fail via one of its documented means. An "implementation" that unconditionally fails when given any program would be of such poor quality as to be useless, but a guarantee that an implementation would fail in defined means when given any program it can't handle would be far more useful than a guarantee that an implementation would be capable of processing at least one (possibly contrived and useless) program. 2. A Selectively Conforming Program is one that will not invoke UB when given any input. Such programs, however, may include directives or tests to ensure that it will fail on any implementations that don't define optional behaviors and guarantees it may require. For example, if a program contains a directive specifying that integer operations must never have any side-effect other than yielding a number that may or may not be representable in the integer type, or failing in Implementation-Defined fashion, an implementation would be allowed to either reject it or process it according to such semantics, but integer overflow would not invoke UB. Under the present Standard, because of the One Program Rule, there's almost nothing an implementation can do that would make it non-conforming. If the Standard were to adopt the terms above, it could meaningfully and usefully specify the behavior of non-portable programs on all systems. Even if feeding a program to a compiler that rejects it wouldn't be very useful, a guarantee that any compiler that couldn't handle a program would reject it would be *extremely* useful. Returning to the malloc() situation, implementations on systems that overcommit memory could be accommodated either by documenting a requirement that the environment not kill them off (or regarding forced termination as an environment's failure to meet requirements), or by documenting the possibility of an out-of-memory-termination as one of its failure modes. At present, however, the Standard regards such issues purely as a Quality of Implementation matter and would allow implementations to handle any malloc() request in any manner they see fit. 
Thank you for a relevant C Programming post
You're hacking the compiler essentially. For a basic program sure but for writing kernel mods or production stuff, especially for cross compiler, bad.
How so?
What I would do is have a single high level API, and then have these high level API methods call a common, more technical API that each driver implements. For instance if the high level api has a function “Write”, each driver would implement a “write_bytes_sync” function. The high level API could also dynamically load the device specific driver on the fly and store the device level functions from the device specific shared library in a table of function pointers for the high level api to call into. 
Dynamic loading is something I'm not sure how to do. Plus, I'm working on an embedded platform, with no requirement for hotplugging peripherals, so my target is to have it easy to minimally change and compile, not to have it swap peripherals at runtime.
Aren't you modifying the behaviour of the compiler in the end? Pragma once is fine but from what I learned, pragma is a preprocessor command that works as a nasty hack and isn't supported similarly across compilers
In that case I would probably do the same thing with API layering but do what you initially suggested using ifdef’s. I would have something like an rtc_driver folder containing rtc_driver.h and rtc_driver1.c, rtc_driver2.c, rtc_driver3.c. In each of the .c files I would then have ifdefs around the entire body of the .c file like #ifdef RTC_DRIVER1 and so on. Each of these .c driver files would implement all of the functions in rtc_driver.h but only the file with its specific ifdef defined on the command line would be built.
In that case I would probably do the same thing with API layering but do what you initially suggested using ifdef’s. I would have something like an rtc_driver folder containing rtc_driver.h and rtc_driver1.c, rtc_driver2.c, rtc_driver3.c. In each of the .c files I would then have ifdefs around the entire body of the .c file like #ifdef RTC_DRIVER1 and so on. Each of these .c driver files would implement all of the functions in rtc_driver.h but only the file with its specific ifdef defined on the command line would be built.
https://gcc.gnu.org/onlinedocs/cpp/Pragmas.html It's part of the C standard, although once isn't.
No one has explained me yet how copying both memory prototypes and functions' source code consumes the same memory during compilation stage than just copying prototypes. I am talking about RAM memory, not disk memory. As far as I know RAM has been historically scarce when compared with disk memory. OP is probably wondering why you can't just import a class or package as you would in Java for example. All of the answers so far make no sense and do not answer that question. I was not alive in the 1970s but again, Im sure it had to do with computing constraints of the time.
It's impossible to perform most embedded programming tasks without reliance upon things that aren't in the Standard. While it would be practical and useful for the Standard to add enough optional features to support the needs to 90%+ of embedded programs (such behavior would be defined on all implementations *that accept them*), at present it doesn't specify any behaviors for freestanding implementations other than "abort processing for a #error directive" or "wait forever without doing anything". 
I agree, this does not seem possible.
Sizeof give you the memory size, just do a main with printf("%d", sizeof(typer));
Do not spam.
Your question is irrelevant because neither memory nor storage were ever the reason for having header files. I am astounded that you are unable to grasp this despite having had it explained to you repeatedly. Header files are used to describe code that exists elsewhere so that your code can interface with it. Their purpose is not to conserve memory or disk space but to allow a clean separation between different bodies of code that come from different sources but need to interact with each other, especially if a certain amount of flexibility in mixing and matching versions of those bodies of code is required; for instance, fixing bugs in the operating system without having to recompile every application. This concept is not unique to C and is found in many other languages, either explicitly (e.g. Modula-2, whose “definition modules” are generated by the compiler but stored separately from the object code) or implicitly (e.g. Java, Python, where the bytecode files include a metadata section that serves the same purpose).
*" fixing bugs in the operating system without having to recompile every application "* I'm pretty sure not having to recompile every application saves memory...
really ? this is just one post...
Many programs are sufficiently large that compilers would be incapable of processing the whole thing at once. Instead, compilers would have to process individual parts of the program (which the C Standard calls "compilation units") independently to produce object files, and a linker would then be used to combine those files. While a linker would need to be able to process an entire program, the operation was sufficiently simple that it wouldn't need to keep the whole program in memory. Instead, the file could be processed using a sequence of merge passes on disk. If the code in one compilation unit needs to access a function or object located in another, the compiler processing the first compilation unit won't need to know the address of the function but can instead insert a place-holder along with a request for the linker to patch it once the address is known. The compiler will, however, need to know the type of the object or function. While implementations of some languages can process type information stored in object files (or their equivalent thereof), C implementations generally expect receive the appropriate information in the form of declarations written using C syntax. All of them can receive the information in that form, and many of them don't implement any other means. While it is possible to write a programs that are built as a single compilation unit, possibly made up of a group of source files that are included form one main file, such a design would often still require header files. Suppose that function foo() in module 1 calls bar() in module 2, and that function in turn calls boz() in module 1. In order to generate the code within `foo` that calls `bar()`, the compiler will have to know the return type for `bar()`. In order to generate the code within `bar` that calls `boz`, the compiler will have to know the return type for `boz()`. Thus, the compiler would need to encounter the things described below in a manner meeting the specified ordering requirements: 1. A declaration of the return type for `bar()` (from module 2) must be encountered before the code for `foo()` (in module 1). 2. A declaration of the return type for `boz()` (from module 1) must be encountered before the code for `bar()` (in module 2). It is thus not possible to process all of module 1 before processing at least some of module 2, nor process all of module 2 without processing at least some of module 1. While it would be possible to have the preprocessor scan through all of the source text for both modules twice, skipping much of the content on the first pass, it is generally more efficient to have a file for each module whose content is limited to information which other modules will needed, and which is not reliant upon the content of other modules. Although there are times when a module won't be able to supply information one module needs until after another module is processed, for the most part it's possible to design header files to be independent by using struct tags and union tags instead of type names. For example, if module 1 includes `typedef struct { int x; } foo;`, and module 2 contains a function that accepts a `foo*` as an argument, the definition of that function would need to know the type of `foo`. If, however, module 1 included `struct foo {int x;}` and module 2 contained a function that expected a `struct foo*`, that function definition could be processed without having to know anything about what `struct foo` contains. 
&gt; I'm pretty sure not having to recompile every application saves memory You're really not interested in listening to anyone but yourself, are you?
extern char \_binary\_baked\_txt\_size\[\] is also ok, if somewhat confusing, at least with GNU ld 2.17.50 \[FreeBSD\] 2007-07-03 &amp;#x200B;
What have you written so far? This sounds like a guided homework project?
yes it is part of an assignment I'm working on. &amp;#x200B; what I have written so far: &amp;#x200B; \#include &lt;stdio.h&gt; /\* printf, fgets \*/ \#include &lt;stdlib.h&gt; /\* atoi \*/ &amp;#x200B; void setup() { Serial.begin(9600); Serial.println("Please input an integer:"); } &amp;#x200B; void loop() { if ( Serial.available () &gt; 0 ) { static char input\[16\]; static uint8\_t i; char c = [Serial.read](https://Serial.read) (); &amp;#x200B; if ( c != '\\r' &amp;&amp; i &lt; 15 ) // assuming "Carriage Return" is chosen in the Serial monitor as the line ending character input\[i++\] = c; else { input\[i\] = '\\0'; i = 0; int number = atoi( input ); Serial.println( number ); } } }
Man page is your friend. "open(), openat(), and creat() return the new file descriptor, or -1 if an error occurred (in which case, errno is set appropriately)"
No, open just returns a file descriptor. Given that you are always writing on creation could you read the contents to perform the check that you need? The test/create race is common and there are standard ways to solve it. Linux has openat() Windows has NtCreateFile().
Ignore me, misread the question
You can do `O_CREAT` in combination with `O_EXCL`. This causes the `open` call to fail if the file did not exist before. That said, you can easily check if an open file is empty: Just `lseek` to its end end check if you are at the beginning. Design your code to treat an empty file like a file that has just been created. I had a [similar problem](https://stackoverflow.com/q/35554607/417501) before and that's what was suggested to me.
`lseek`, why didn't i think of this... Thanks!
Why don't you use another metric to decide on what to do, like file size? Thatis: don't only care anout gile existence, but zldo, file contents.
That's not what you've written, it's what you've copy/pasted from here: http://forum.arduino.cc/index.php?topic=103511 Forgive my bluntness, but do you know how to program in C at all? If not, you're going to have a really, really hard time with this project overall, and you should focus your time on learning the basics of C rather than just trying to get the right answer on an assignment. Once you've done that, the assignment says that you need to write a function with the signature `int readInt(void)`, so go do that. I don't see that function in the code you've listed above. Next, research the function that gets input from Serial, and try to understand how it works. A good portion of that is already in the code you've pasted. Go through that code line by line to understand explicitly what every part of it does. Don't just copy/paste -- you're kidding yourself if you think that's learning. Once you've read each available byte, and written each byte to your char[] buffer array, and you have detected that the input is finished, you need to turn the char buffer into a null-terminated string. (This just means setting the character *after* your data to 0). (In C it is absolutely essential to understand what is meant by a a null-terminated character array. it is one of the most important facets of understanding string manipulation) Next, use `atoi` as indicated in the instructions to convert that character array to an integer, then return it. That's about as helpful as I can be without writing your program for you. As a word of advice: If you neglect learning C right now, and simply look for shortcuts or copy/paste code that you find elsewhere, you're doing yourself a disservice. Start learning for real right now or the deficit is going to keep snowballing out of control and you'll end up failing the class.
Thank you for this. And if i am honest I have chosen computer science as my degree, however after trying out all the modules, i have already found my knack or passion for Human computer interaction module and want to become a UX designer. It’s hard to be studying this degree as a whole simply bcos my passion for coding whether that be in java (which i have more practise with) or C (little to no practise) has completely diminished. Find it a very frustrating process once you have found out what you finally want to chase, but having found it in the second year of a three year degree sucks. So i find myself going forum to forum needing help with coding as I don’t have any interest was never there so i never learnt doing it properly. This degree is very focused on programming a lot of the time, but my interest revolves around a branch of computer science that steers clear of coding completely. :/ 
Yeah, I get ya. One of my biggest regrets is *not* being a CS major, and 10 years down the line, I'm kicking myself. Let's be real -- you need to work in C for this project, which means you need to devote time to arbitrarily learning C outside of the context of this project. That is going to require a time commitment. My best advice: get a C tutor, or go to office hours, or set up some time to meet with the professor/TA (after you've done some basic tutorials) and be frank with them about what you don't understand. For the love of god, don't ask them (or anyone else) to do your coding for you, and don't go to them expecting them to give you a catered "everything you need to know about C" explanation. (unless that's part of the course material) Here's where you can get started: https://www.tutorialspoint.com/cprogramming/c_functions.htm https://www.tutorialspoint.com/cprogramming/c_strings.htm https://www.tutorialspoint.com/cprogramming/c_arrays.htm https://www.arduino.cc/reference/en/language/functions/communication/serial/
Cheers for the direction, i’ll give C a go then. Hope everything works out for you too, seems like u don’t even need a cs degree by the looks of it but i do get how it is in the real world blahblah so i hope whatever u got going on is doing well. Cheers again✌🏼
FreeBSD has *st\_birthtim* Time (nanosecond resolution :) when the inode was created. Similar mechanisms on other systems ? Not a recommendation of course, just exploration.
malloc without a free nearby. Screwing around with ultra "clever" things. (3["no"]) Long functions -- there is very little reason for functions to be longer than about 20 lines. 
I would not rely on birth time as not all file systems support it. Also, use this renders your code unportable.
really\_long\_variables or equally single or double letter variables lines over 70 chars over long functions (no hard limit here especially if there is boiler plate) lack of readability
- Unreadable code. - Incorrect code style. - Shit commit message. - No free() added with malloc(). - No function comments. Pretty much everything mentioned by /u/which_spartacus .
Is 70 char limit more advantageous over the commonly used 80 char limit?
I use 72 characters for comments, 80 characters for code.
\- Too many levels of indentation. \- Poorly named functions. \- Hard coded magic numbers in the code (use a define or an enum)
&gt; `3["no"]` Henceforth, this shall be the normative spelling of `'\0'`.
&gt; Mucking with the iterator in a for loop. Sometimes there are good reasons to do this, for example in a parser with lookahead.
&gt; Long functions -- there is very little reason for functions to be longer than about 20 lines. John Carmack\* seems to disagree with this one, and I'm starting to see it the same way. If you only call your functions once, it is reasonable to inline these, even if you might get one very long function. \* http://number-none.com/blow/john_carmack_on_inlined_code.html
Sure, CREAT|EXCL is the answer, just trying to find goofy alternatives
 git commit -m "updated stuff to fix that error we talked about last week during the sprint review that Mike said need to be fixed by the end of this sprint so I took the story and decided on this fix thanks happy coding holiday merry kwanzaa"
&gt; lines over 70 chars Thas very debatable and certainly not a red flag if the whole codebase adheres to the same style.
&gt; 20 changed files with 2,790 additions and 8 deletions. &gt; "Updates"
I don’t understand why line lengths like 70 or 80 characters are such a big deal. Do you do all of your coding on an 80x25 monochrome terminal? No, you probably have a 27” wide screen monitor in front of you. Actually, you probably have two or three of them. I’ve heard the argument that it would be nice to have two reasonably sized terminals side by side. That makes sense. How about we update max line lengths to something more sane like 120 characters? That solves the side-by-side problem and is not ridiculously short. I personally use 120 characters when working on projects with other people (work). Personal projects don’t have a set limit. 
when I see this if (foo == 0xba2) { return 1; } else { return 0; } instead of this return foo == 0xba2; 
I pretty much disagree -- it is solely due to readability that I'm looking at smaller functions. I'm not advocating for OOP style here, just smaller functions. ``` int processFileToMap(char *filename, MapType *m) { ReadFileToBuffer(...); ProcessBuffer(...); } ``` Now, imagine all of the error checking around those as well. And the declarations, etc. Sure, I can expand all of those functions, but now it doesn't self-document the steps I'm performing as well as breaking it into simple functions. 
I have a 13" screen. So using 80 char limit means I can have side-by-side view without any word wrap and have little wasted space on the screen. Having a small character limit means that I'm less likely to write code pyramids too.
And magic numbers in themselves are problems. You should make all of these macros or constants (preferred), and comment where it comes from if a non-obvious constant. 
Sure. It isn't a "absolute no", it's simply a red flag that makes me look closer. 
It's not really the answer because it does not actually allow you to open a file that already exists.
Exactly. One fun way to do a logical-not: #define NOT(e) ["0"]((e)==0) 
I agree, but I would put a fat asterisk next to that. Lately I've been involved with lots of numerical code and binary parsers. At some point putting everything into macros is too much work. Magic numbers are fine in a limited context if there is a comment with a reference to the paper or specification where these numbers are described.
Mind explaining that? 
I really don't understand the "too much work" thing in this case -- mainly because I've been bitten way too many times by magic numbers that have no reference and are just sitting in the middle of an expression. &amp;#x200B; If all of the numbers are from one paper, putting the constant names and then referencing that paper seems quite fine and sane. But I still would prefer using the constant names to the numerical values.
It comes down to the focus of the code, but I'm not a fan of your solution either. I tend to maintain embedded code with a LONG life, so clarity is king for me. I agree that multiple returns is bad, but I hate putting a Boolean decision in a return statement. I've had so many time where that decision is doing something off and I need to put in a temp variable since the debugger won't show the results of a decision that immediately goes into another, like a return or if. Your example is simple, so probably no big deal. But for more complex ones, ug.
Inconsistent indentation, including mixing spaces and tabs. I just start twitching.
I think the worst thing is when they try to use C like it's another language. For example, using C and trying to adhere to, say, Java's "coding style".
Yes! I don't care\* about tabs vs. spaces, or exactly how far to indent at each level, just...be...consistent!!! &amp;#x200B; \*I do have preferences, but I don't care enough to argue about them or push them on others.
Magic numbers are okay only if used once or twice within the same scope. If it's in a couple of functions or files, make a constant of some sort.
Same. Several of my colleagues have a habit of writing really long functions, and it makes it next to impossible to grok the high-level steps being performed. It also then leads into the habit of using variables to track little bits of info here and there and then use their values much later. It's very difficult to understand code if you have to use search over several pages worth of code to find where a single local variable is declared, assigned to, and used all in one function.
80 cpl works for C quite well and I can have more windows side-by-side. I have 3 monitors (2x24", 1x27") with FullHD and still I'm always annoyed when the CPL is much higher. Also, for my projects, I sometimes do printouts, which also have a limited CPL.
Don't put source code in a header unless you know exactly what you are doing. Place your source code in a source file and deliver a fitting header file with it. This can be compiled into a library, but you can also simply bundle it into the project you want to use it in.
&gt; fail if the file did not exist before. You mean if it *did* exist before. &gt; That said, you can easily check if an open file is empty: Just `lseek` to its end end check if you are at the beginning. Design your code to treat an empty file like a file that has just been created. This makes sense as long as writing to an existing file isn't a security risk. You could also use `fstat()` to check the size of the file without seeking.
If there is a block of code whose behavior *including all corner cases that might be relevant in future, whether or not they are relevant at the moment,* can be described in English more compactly than in source code, it may make sense to factor it. Pulling a chunk code out to a function can make it harder to see whether its assumptions about corner cases match those of the surrounding code. I find it interesting that some people are opposed to the idea of marking and using telescoping/collapsable regions within a function on the basis that it encourages long functions, when the biggest problem with long functions is the difficulty of focusing on outer sections of code together--which need not *be* a problem in editors which allow a chunk of code to be visually replaced with a comment describing what it does [but which can be expanded to show the code when necessary]. 
Long lines also indicate the logic is meh, like an if statement with 5 conditions etc.
Tell me, just tell me it does not save memory usage, you can't because its not true. Sure, it also organizes code, but it saves memory usage. You can't deny that copying function prototypes takes less memory than functions' source code everytime you include a file or that headers allow you to create object files separately to later link them without having to recompile whole applications which also saves memory usage. This is getting boring, see you all. 
A good algorithm for solving Rubik's cubes is [Kociemba's algorithm](https://en.wikipedia.org/wiki/Optimal_solutions_for_Rubik%27s_Cube#Kociemba's_algorithm). The [official](https://github.com/hkociemba/RubiksCube-TwophaseSolver) implementation is in Python, but I found a C implementation [here](https://github.com/muodov/kociemba) as well.
I’m not asking “how” to make a C header file into a library, I’m asking whether or not I “should” make it into a library binary to be used with other projects.
I rather enjoy this one... It seems like the type of thing my last prof would put on a written test to mess with us.
Writing functions that already exist. C's standard library isn't *that* big, even if you include POSIX. And yet people don't seem to know that it has some pretty basic functions in there. Re-writing `abs` is by far the most common, but also most of the string functions seem to be ignored. I even had one coworker (fresh from college) who didn't seem to know that `printf` could handle numbers with multiple digits, and was doing the decoding with `%10` manually. Another that's more of a pet peeve than a red flag: not using a boolean type for boolean results. C's had `_Bool` for almost 20 years now, and even before than it's been common to define some sort of enumeration or `typedef` to represent booleans. And yet programmers think the best way to handle boolean values in C is *still* to use `int` and define your own `TRUE` and `FALSE` macros for 1 and 0. I'll give a pass to old code, but this is nearly inexcusable in anything new.
Does anything in the Standard actually specify that `strcmp` won't read past the first mismatch?
So what's the advantage of using a bool type as opposed to 0 and 1? I'm just curious cause I've read that multiple times but for me 0 and 1 are intiutive enough and I'm wondering why I'd need to include an additional header for this.
One of my pet peeves is return (Foo); instead of return foo; 
IMO it reads much better, and shows intent. `value_found = false` reads better than `value_found = 0`, which is also ambiguous. 
I don’t know... seems like it just takes to read the variable name. And once in a condition `if (value_found)` can’t be misinterpreted.
Type safety and clarity. If you declare a function as int it could theoretically return -1. What does that mean? It may be hard to tell. A function that returns bool has only two possibilities. It is easier to reason about and clearer to read.
If a function returns it you should do an explicit comparison if (condition_is_true() == 1) { // ... }
I see, but then what I don’t like how it’s written with an underscore and a capital letter. I’d much prefer having a simple enum so I can use bool instead of _Bool all the time.
20 lines? Are you high? That isn't even enough for options parsing for a mildly complicated program. One page is a good point to stop and consider whether you should redactor, but frankly error checking, logging, and whitespace can quickly add up. I think indentation / conditional nesting is a better metric for code complexity.
Using the preprocessor to try and morph the language into a different language. &amp;#x200B; Things like `#define END }` and `#define BEGIN {` Even worse, `#define CASE3(c1, k1, c2, k2, c3, k3) \` `case c1: \token.kind = k1; \ stream++; \ if (*stream == c2) { \ token.kind = k2; \ stream++; \ } else if (*stream == c3) { \ token.kind = k3; \ stream++; \ } \ break;` Also type generic structures through macros. &amp;#x200B; I generally like the preprocessor.
I think it is heavily implied by the term lexicographically order. Once you come to the first mismatch, you know the result; no possible characters after the first mismatch will make the strings equal. So why read the other characters? &amp;#x200B; Not sure what this has to do with strncmp vs strcmp, I might be missing your point though?
If multiple compilation units are going to refer to it, you start dancing with odr issues pretty quick. At that point, a library you link into is better than a header. 
Depends on the purpose of the code review, I guess. In typical dev-team code reviews for a pre-check-in or pull request process, I look for a statement of purpose (what problem are we trying to solve? why is this code changing?), fit and integration (how does this new code affect the existing system? Have you told Dave that you're changing FunctionDaveConsumes()?), and coverage (what does this code do when things go wrong?). I want to see comments and/or documentation. If the answer is "the code is self-explanatory and doesn't need comments", I walk out because you're wasting my time. I want to see something for testing. For code reviews that help spread knowledge on the team -- used to share a solution or explain how a module works, for instance -- I look for many of the same things. There's much more priority for statement of purpose, though. And we really have to understand why all the decisions were made: what's the performance contract? Why did we decide this business decision instead of that one? What known shortcuts were taken in the interest of time or simplification against the normal expectations? The main thing that freaks me out is people who pay attention to things that don't matter. Indentation, formatting, function length, stylistic pet peeves, naming, coding idioms ... it really doesn't matter. Maybe these things are appropriate for a personal code review when I'm mentoring a junior developer, or working on code that's going to be formally published (like in a sample for documentation or an SDK, not just plain old open-source). People are pretty bad about giving feedback. For example: (D = did say, C = could say) D: "Line 35, You have a magic number here." D: "Line 114, Another magic number." D: "Line 281, Another magic number." D: "Line 292, Another magic number." D: "Line 305, Another magic number." C: "Several magic numbers in this module. Our standard doc says to use an enum for most of these." --- D: "You've changed function foo() and I've always hated it. Why didn't you fix it to be more blah?" C: "Thanks for getting the baz feature into foo(). It reminds me that foo() could be more blah, though, so I'll get into Jira." --- D: "Why didn't you baz?" C: "I think it might be better to do baz instead of bing here. baz has the advantage of being more fippy." Of course, if the code review was clear about "What problem are we trying to solve", then it might be obvious that we *don't* want to be more fippy, and bing is fine, and this wouldn't even be necessary. 
Say that you want to place a breakpoint on that function when it returns 1. How do you do so?
Thanks for the tip, I ended by using this way 😁 
Set a conditional breakpoint on the RET instruction only if RAX/R0/whatever == 1
I'm with you. I hate long functions, of course. For example, I've got a code base now which does things to data. The collections are heterogeneous, so any function that visits something has to decide what the data is and handle it differently. switch/case everywhere. Except the case blocks are all inline, so we have one giant Handle() function with a loop and a switch/case in it. Two thousand, five thousand lines, sometimes. Why not break those handlers into functions? Then, it's easy to look at Handle() and see the structure: oh, we loop over the list, switch each item to see what type it is. The handlers are in these other functions. Maybe you need to fix the loop or the error handling. Or, maybe you're messing with a handler. In the suggested arrangement, it's easy to see how HandleBing() is different than HandleBaz(), which is different still than HandleFoo(). Isolation, less indentation, and so on. But to me, twenty lines seems like an academic rule, applicable only to trivial projects. If a quantitative rule must be placed, 200 seems like it might be a better limit. But why not descriptively specify the desired behavior instead of guess at a quantitative limit that might (hopefully) prohibit a symptom of undesired behavior? Why not: "Complex ```case``` implementations should be placed into their own function, named like ```&lt;CallingFunction&gt;&lt;CaseName&gt;```." drives the specific improvement we want and leaves room for people to do the right thing at the right time.
That would work. But wouldn't it be a lot easier to leave the if/else in place and drop a regular, unconditional breakpoint on the ```return 1``` line? The if/else pattern has the benefit of being visible while stepping, too, without stopping to evaluate something else.
Show more code. The code does pointer arithmetic but without more code we can't tell where its pointing to. You can think of pointers as numbers, so you can add them, subtract, multiply, divide etc...
It's hard to say without context, but here is what I can assume from these five lines of code. `w` is clearly an array of `float` variables, of length `nw`. Easy enough. I assume `b` is similar (dunno how many elements, though). `x` is different, though. It's not itself an array, but instead points somewhere inside a different array. In this case, it points to some element inside `w`. It happens to point at the `nh * n`th element, whatever that means. But I can take a guess there, too. `nh` seems like some sort of size, assuming similar naming as `nw`. This pattern feels a bit like `w` is actually intended to be a 2D array, but allocated as if it's just 1D. Each row is `nh` columns. So `nh * n` is having `x` point to the `n`th column inside this 2D matrix. I'll reiterate my caveat: I'm making a lot of assumptions and guesses here, so I could be very wrong.
&gt; Hard coded magic numbers in the code (use a define or an enum) # YES!
That makes x point at a element within w. Say nw is 10. nh is 5. n is 1. w points at an array of 10 floats. x points at at the 5th one w x | | | | | | | | | | | This is often done to create 2D arrays. If nw was 100, nh was 10, and n was in a loop from 0 to 9. Then w would point at 100 floats. But the 10 other pointers would point at the 1^st, 11^th, 21^st etc, So you could use them like a 10x10 2D array
Sorry for the vagueness, I probably should have just linked to the code I'm attemping to port. Modified my original post, but the link is https://github.com/glouw/tinn
Sorry for the vagueness, I probably should have just linked to the code I'm attemping to port. Modified my original post, but the link is https://github.com/glouw/tinn
The result of the equality operator is bool, so you don't need to go all the way and write yourself 1 and 0.
Thank you! 
20 lines is simply a thumb rule. If you look at my code, you'd find plenty of functions that were longer than 20 lines. But every time I have one of those, I twitch a little. I start looking at better ways to structure it. I start looking for why it has that scent. Sometimes, it's pretty unavoidable -- state machines with lots of nodes, for example. But in plenty of other cases, questions of "hmm, maybe this would be cleaner if..." Pop up. The original question was "red flags". I interpreted the question as "code smell", while others interpretted it as "hard no!". 
Thanks for that. Good read. I’ve never touched c++ in my life (never need to)
What do you have against type generic structures through macros?
If you have a function that lexes a file and translates identifiers into tokens in a vector, how is that supposed to be less than 20 lines?
Out of curiosity, could you provide an example?
"Fixes issue #23 and some more stuff"
I totally like your view more than OP's, also it's easier to then add more logic or debug prints or whatever and the code is clearer. Maybe set up a r\_v variable before the if, assign inside, and ser a single return after the if to make it easier to analyze by linters and static analysis tools that don't do optimizations.
Looks like aliasing to me. It is used to share a single block of memory between multiple arrays. Can be tricky (you have to be extra careful with boundaries), but minimizes allocations and simplifies deallocations. Let me explain that with code: ``` #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; /* * Let's say we need three arrays of int, each of length 3. * It can be anything, floats, doubles, chars etc. */ #define LEN_A 3 #define LEN_B 3 #define LEN_C 3 int main(void) { int idx; /* We will allocate memory dynamically, so we need pointers */ int *arr_A, *arr_B, *arr_C; /* * We are allocating one, big memory block, that will be shared * by all three arrays. Pointer to it will be assigned to our * first (arr_A) array. */ if ( (arr_A = (int*)calloc(LEN_A + LEN_B + LEN_C, sizeof(int))) == NULL ) { perror("calloc()"); return 1; } /* * The remaining two pointers are aliased to the middle of our memory block */ arr_B = arr_A + LEN_A; arr_C = arr_B + LEN_B; /* * As a proof it's a large memory block, we print all of its elements, * which are 0 (because we used calloc(), not malloc()). Notice that we * used a sum of all our arrays sizes, which is the size of entire memory * block. */ for ( idx = 0; idx &lt; LEN_A + LEN_B + LEN_C; idx++ ) fprintf(stdout, "%d ", arr_A[idx]); // prints "0 0 0 0 0 0 0 0 0 " fprintf(stdout, "\n"); // Just a newline for clarity /* * Let's assign values to elements of our first, 3-element array */ arr_A[0] = 1; arr_A[1] = 2; arr_A[2] = 3; /* * Now, let's see what is inside it. Notice that we used LEN_A as a size */ for ( idx = 0; idx &lt; LEN_A; idx++ ) fprintf(stdout, "%d ", arr_A[idx]); // prints "1 2 3 " fprintf(stdout, "\n"); // Just a newline for clarity /* * Now, let's set different values to elements of our second array */ arr_B[0] = 11; arr_B[1] = 12; arr_B[2] = 13; /* * And again, let's see what's inside (notice the LEN_B) */ for ( idx = 0; idx &lt; LEN_B; idx++ ) fprintf(stdout, "%d ", arr_B[idx]); // prints "11 12 13 " fprintf(stdout, "\n"); // Just a newline for clarity /* * Finally, let's set other values to elements of our third array... */ arr_C[0] = 21; arr_C[1] = 22; arr_C[2] = 23; /* * ...and see what's inside (notice the LEN_C) */ for ( idx = 0; idx &lt; LEN_C; idx++ ) fprintf(stdout, "%d ", arr_C[idx]); // prints "21 22 23 " fprintf(stdout, "\n"); // Just a newline for clarity /* * To see that the block of memory is in fact shared between all three * arrays, we print all of it's elements */ for ( idx = 0; idx &lt; LEN_A + LEN_B + LEN_C; idx++ ) fprintf(stdout, "%d ", arr_A[idx]); // prints "1 2 3 11 12 13 21 22 23 " fprintf(stdout, "\n"); // Just a newline for clarity /* * To release the memory taken by all three arrays, we need single free() * instead of three */ free(arr_A); /* And that is it :) */ return 0; } /* vim: set ft=c sw=4 sts=4 et: */ ``` If you run it with valgrind, you will see there are no errors or leaks: ``` 0 0 0 0 0 0 0 0 0 1 2 3 11 12 13 21 22 23 1 2 3 11 12 13 21 22 23 --3374-- REDIR: 0x4901f20 (libc.so.6:free) redirected to 0x4838940 (free) ==3374== ==3374== HEAP SUMMARY: ==3374== in use at exit: 0 bytes in 0 blocks ==3374== total heap usage: 2 allocs, 2 frees, 1,060 bytes allocated ==3374== ==3374== All heap blocks were freed -- no leaks are possible ==3374== ==3374== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) ==3374== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) ```
&gt;Long functions -- there is very little reason for functions to be longer than about 20 lines. With experience I've found this is generally not true if you are trying to be useful to the guys calling you and gracefully managing different types of errors without adding like four or five unneeded functions that then need you to jump all over the file and add complexity to your mind model of what the current code does. I don't have a preference for either short functions or long but \_readable and clear\_ code, but find the latter harder to write. In my experience the latter is also better till you don't tend to remeber "oh that easy to read function was very long" but I do have some anecdotes of "oh this Jr. coder had me jumping all around the file and writing stuff on paper and thinking about wether those functions are static or not and I'd had preferred a 40/60 line single function with a switch of few ifs".
&gt; really_long_variables What's wrong with really long variables? I like to use really long variable names where it will make things clearer.
He was anything but civil. 
I presume it's printing a NUL. What did you want it to print?
It's like the brown M&amp;M test. If you can't even get the indentation right then you can pretty much guarantee that there are deeper more significant issues.
got me So it prints nothing...thats a NUL?
ASCII 0 is NUL.
So, is there a 0 in 2 places in the array? How could I print out the hex in each location of the array?
Read the man page for printf. You can use the formatting specifier 'x' for hex output.
I may misunderstand, but "finnish" in gdb gives you the returned value, and you can always "print foo==bar".
Of course. Super short functions, avoiding globals like the plague, despising goto, too many compilation units etc.
Gdb isn't always available.
Why in the world would that be one function? Why would you not simply link in flex? 
Because generally a multi_butts_supporter is a sofa!
Prefer my own lexer instead.
That's great!
Totally fine not using flex -- but I still have no understanding of why one function for all that work is a good idea.
It's a 2 char at a time lexer that uses a switch statement to lex the starting chars with either specific functions to process into a token or a simple output to their correct token representations. Almost every token type uses a case
Thanks. I tried man printf but I think that's the bash one.
Whenever the default is not for C, usually man 3 &lt;thing&gt; works
So the switch statement is one function -- like a table. Sure. No reason to break that up. &amp;#x200B; But then it's called from the main routine.
thanks!
It's called with the function lex(TokenVector* tv, FileVector* fv) that reads from the FILE* in the last element in fv to lex tokens into tv.
&gt; - Hard coded magic numbers in the code (use a define or an enum) Reverse side of the same coin: having something like this in your code: #define ZERO 0 #define ONE 1 #define TWO 2 #define ZERO_FLOAT 0.0 #define INDEX_1 0 #define INDEX_2 1 #define INDEX_3 2 And in case you're wondering, this is from real code (at least, the best I can remember it). Somebody took the "magic number" advice to heart and removed *everything*. But sometimes you really just mean 0 (like when initializing a counter) so the only meaningful name is just ... `ZERO`. Of course ... then someone else came along and reused `TWO` for their own magic number instead of giving it a real name with its own constant. Ugh. And yes, `ZERO_FLOAT` is actually a `double`. The index constants were used as constant array indexes for small arrays that really should have been a `struct` (like an 3D point in space). So it didn't *really* matter that they redefined the array to start at 1 ... but you can bet that got frustrating.
Just to add to this ... and I know what sub I'm on ... but those parentheses can actually matter in C++.
Yeah ... those are all good things for C as well, and not "Java's 'coding style'."
I would NOT pass that in a code review.
That's why I think quantitative rules aren't great. Semantic or descriptive rules tend to work out better.
 fscanf(file, "%5s", reds[i]); This reads "words" (anything separated by whitespace) from the file. It knows nothing about lines. Instead, I would suggest `fgets` or maybe `getline` if you can use POSIX functions.
how bout `*""`, ooh almost temptin
the "review" part is an instant red flag for me, like I gotta sit through this BS :(
&gt; Having a small character limit means that I'm less likely to write code pyramids too. This is such strange thinking. Line length should have no bearing whatsoever on how deeply you nest code. Whether you write 70 char lines or 150 char lines, if you see something indented 5 times you should immediately start trying to figure out if it can be refactored. I can't stand it when people argue that a longer line length means that they start writing more nested code for no reason
Honestly, if it's consistent and you _did_ care enough, some quick regex could probably get you to your preference.
I don't care how long a function is as long as it does *one thing*. In one C project I wrote (and still use nearly 20 years later) I have one function that is over 430 lines of code. This function parses some text into a data structure. I did not want to use lex and yacc (or flex and bison) due to threading issues [1] and breaking up the function to sub-functions just to appease some rule would be more problematic (not to mention the problem of naming the 14 logical steps, some of which exceed 20 lines themselves). [1] The project is just a single process, no threads, but it's just the idea that making thread safe code in lex and yacc (flex and bison) is so unreasonably hard that I'd rather just skip using the tool.
Well written C code is boring and easy to follow. Whenever there is weird trickery in someones code, a closer look is warranted. Almost certainly, this code should be replaced by boring code.
How so?
lawl
Writing `return (foo);` is actually mandated by some formatting styles, including the one I prefer to follow (KNF).
Note that you can also use the `objcopy` utility to do the same thing you do with assembly.
Starting globals with underscore (which would have been unnecessary if we didn't really like singletons just because). This was from at least one young C# developer and a few other older developers that I don't know what they programmed in before.
Not OP but if overused it makes finding type definitions very frustrating because they don't exist in your source code. You can't just right click the type in the variable declaration and click "show definition".
"Hmmm, I need to change all the initializers here to five... What the hell, let's just do `#define FOUR 5`. There, fixed it!" 
Okay, two points: 1. The question was, "Red Flags", not "what practice do you ban outright." If I see long functions, I don't immediately say, "There's no way this could be a good design!", instead, I say, "Hmm, I wonder if it has to be this long." or "I wonder if there's a better logical grouping of parts." 2. Having said that, my inner-Monica really wants a shot at that 430 line function. Any chance you can paste-bin it so I can have a refactoring whack at it?
It comes down to return type deduction from C++14: int i = 0; decltype(auto) foo() { return i; } decltype(auto) bar() { return (i); } Here, `foo()` has a deduced return type of `int` while `bar()` has a deduced return type of `int&amp;`. This is because `decltype` has different behavior depending on whether its argument is a variable (as in `foo()`) or an expression (as in `baz()`). Fortunately, this isn't a very commonly used feature -- it's really only useful in templated code where knowing what type certain expressions are can be difficult.
I gave it a quick read and didn't see anything about the compiler. Can you tell bake which compiler to use? 
Also, your buffers are too small. Wouldn't it be easier to just use char reds\[13\]\[MAXLEN\] ? Delay the fopen, you check argument count \_after\_ using argv\[1\].
Good point. Currently, bake defaults to gcc (or g++ for C++) on Linux, and clang (or clang++ for C++) on macOS. As long as other compilers use gcc-compatible command line options, it is trivial to make this configurable (maybe parse the CC environment variable). Support for compilers that aren't gcc-compatible would require writing another bake driver. 
1. Fair enough. 2. Better yet, I can point you [right at the code](https://github.com/spc476/mod_blog/blob/master/src/wbtum.c#L149). It's a function to parse text, and there are gotos, so beware 8-) 
Well, looks like I'll have something do play with on vacation...
thanks!
`char x = 97;` aieeeeeee
You can't control the size of _Bool though. In environments where size matters you've got to use a 1-byte type instead
Why? The parentheses achieve nothing.
Do you plan to incorporate any of that? Perhaps allow users to load their own textfiles as a "driver"? Or did you plan your code to be easy for others to write drivers?
Yeah I will likely incorporate this very soon: [https://github.com/SanderMertens/bake/issues/2](https://github.com/SanderMertens/bake/issues/2) &amp;#x200B; This would be a simple feature to add to the C driver. This is the function that decides which compiler to use: [https://github.com/SanderMertens/bake/blob/master/drivers/lang/c/src/c.c#L96](https://github.com/SanderMertens/bake/blob/master/drivers/lang/c/src/c.c#L96) &amp;#x200B; It would be easy to make the driver read the CC environment variable (and CXX for C++). Since bake has an environment variable manager, you could do something like this: `bake export CC=my_compiler --env my_compiler_env` If you would then build your project and select this environment, it would load the CC environment variable, and use the correct compiler: `bake --env my_compiler_env`
Use `getline()` to read entire line.
I would rather expect it to be the spell for calling the nasal deamons. 
Nice attempt. You've got a bunch of problems here that are going to bite you later. First to answer your question explicitly: 0. Use `fgets` rather than `fscanf`. If you know the maximum length of the line, then awesome. Just remember that `fgets` stores the newline character, so strip that out (by setting it to 0) if the newline shouldn't be included in your output. Your use of `fscanf` is causing `RED` and `A` to be read separately. 1. You're using "magic numbers", namely the number 13. Do you know for certain that you're expecting 13 values? If so, you can clean up your code by doing something like `#define numInputs 13`, then use numInputs throughout your code, so you don't have to worry about wondering why the heck you hard-coded 13 in there.... 2. THAT SAID... it's still a crap idea to use a fixed number like this for storing input. Why not use a while loop? The cool thing about `fgets` is that it returns `null` if it gets EOF, so you can do something like `while(fgets(buf, sizeof(buf), input)` or similar. 3. I strongly suspect that the purpose of the *dynamic* array is that you *don't* know the number of values to expect? If that's the case, you absolutely can not hard-code magic numbers. 4. Your malloc size for the individual strings is wrong. First off, you're using magic numbers again. BAD. Secondly, you're not allocating enough space for any of the lines. `RED A` requires 6 bytes. `RED 10` requires 7 bytes..... which brings me to.... 5. ... why malloc the inner char arrays but not malloc the entire structure? If the outer array goes out of scope, you're leaking everything you've malloc'd... which begs the question: 6. I can haz `free`? Remember to build in such a way that you can free whatever you malloc. Good programmers clean up their toys when they're done playing with them. 7. It's bad form to re-use `int`s for loops like you're doing. It'll work, but it's somewhat unmaintainable codewise. Much better to declare your `int` to apply to the scope you need it for, then let it go out of scope ie. for(int i = 0; i &lt; sizeof(some_array); i++){ //do something with i } Good luck with the edits. Feel free to poke me for more feedback once you've made updates.
Interesing, I'll check it out. Without commenting on `bake`, I'll say that my wishlist for a build system would basically be something along the lines of CMake without using CMake's god-awful language, and a clearer model for configuration and caching. It would be great to somehow avoid projects developing monster build systems that they tend to do by enforcing simplicity somehow, e.g. using a non-imperative language or something along those lines. It sounds like you're thinking in that direction with the JSON configuration files. Combining it with a package repository is an interesting concept. I think though that then you have to consider a lot of things: it should use upstream versioned sources pinned to a release and/or git version, and support patching like debian packages, as well as downstream patches. It should support ways to instruct it to use pre-installed (system) versions of libraries instead of the repository version. It should support multiple versions of libraries installable in parallel and possibly configured with different options. Basically, it should ultimately be something like nix, I suppose.. and doing this with cross-platform support is going to be quite a big project.
&gt; You can think of pointers as numbers, so you can add them, subtract, multiply, divide You cannot multiply or divide pointers. You can add an integer to a pointer, subtract an integer from a pointer, or subtract one pointer from another pointer.
Congrats on your build system: You had an itch and scratched it. If you don't mind me asking, tho: If you could fix Autotools, what would you change?
Seems interesting! I guess it would be cool if C (and embedded C) had a build manager similar to rust's \`cargo\` and a library and embedded driver repository similar to [crates.io](https://crates.io) . There are similar efforts for these that I've seen posted in this subreddit but it would be nice if someone who knows C committee talk about them about reaching a standard for this. I also wonder why they used toml instead of json.
They make the code easier to read by analogy to `if`, `for`, and `while` statements.
There are no *defined* cases where the behavior of `strcmp` depends upon anything beyond the first mismatch. That does not imply, however, that there couldn't be situations where that could affect whether or not behavior *is* defined. Is there anything in the Standard that would disallow an implementation like: int strcmp(char const *s1, char const *s2) { return memcmp(s1, s2, strlen(s1)+1); } In cases where strings match for most of their length, such an implementation might be able to outperform an implementation that never looks at anything past the first mismatch if run on hardware like the 8088 that features instructions for scanning to a certain byte value, or comparing a sequence of words until the first mismatch. 
I honestly don't understand the dislike for Autotools. M4 *can* be a pain in the ass, but it also has very few hard limits once you wrap your head around the syntax.
my personal belief is that people would stop thinking that autotools is a nightmare, if GNU published a quick guide on how to get a basic setup going fast. Cuz, in the classic sense of Unix tools^[1] , autotools right now is not exactly quick to set up: You got to read about six books to get things going, which, judging from the direction o'reilly is taking, is not currently a favorable approach to building an app. [1] *RTFM, Luke!*
Arguments are for you to pass different things to functions, so they can do something with the things you pass them. If you include `int n` in a function definition then that function will have access to an integer `n`, which it can then refer to. e.g. #include &lt;stdio.h&gt; void Add42(int n) { printf("%d + 42 = %d\n", n, n + 42); } int main(int argc, char **argv) { int i = 8; Add42(i); } This program will output: 8 + 42 = 50 BTW, if you start each line with four spaces, reddit will format it nicely like in this post.
I agree about the CMake language. How do you feel about bazel?
Thank you, so in the new function (void Add42) , we don't have to declare the int n and we can put it straight in the argument line ? And also, in the main function, the argument is int argc? Why is that, couldn't you put int n in that part or is it because you are going to int n and int i, it wouldn't be good as the int i will not be read ? Hence you included int arg ( as in all integers that are going to be must be read ) ? And could you have done add42 + i // or is that wrong because you can't add a function and a variable but can include the function which number specifically you want to add, hence you added the brackets with a number or variable in this case which could be (8) or (i) and both of these would produce the same result (50). 
&gt; Thank you, so in the new function (void Add42) , we don't have to declare the int n; and we can put it straight in the argument line ? Yes, you don't declare the `int n` inside the function, as it's already declared in the argument list. &gt; And also, in the main function, the argument is int argc? Why is that Because that's how the standard defines `main`. `argc` is the number of arguments passed to the program by whatever has invoked it, and `argv` allows you access to these arguments. Here's a quick example: #include &lt;stdio.h&gt; int main(int argc, char **argv) { int i; printf("argc=%d\n", argc); for (i = 0; i &lt; argc; i++) { printf("argv[%d] = %s\n", i, argv[i]); } } I've compiled this in to a program called `args`, and here's what I get if I run it a few different ways: $ ./args argc=1 argv[0] = ./args $ ./args foo argc=2 argv[0] = ./args argv[1] = foo $ ./args foo bar goo argc=4 argv[0] = ./args argv[1] = foo argv[2] = bar argv[3] = goo $ &gt; And could you have done add42 + i No, but with some changes we could do something similar, e.g. #include &lt;stdio.h&gt; int Add42(void) { return 42; } int main(int argc, char **argv) { int i = 8; printf("%d + 42 = %d\n", i, Add42() + i); } So instead of just printing out the values in `Add42`, we've changed it to return an integer, and not take any arguments (as we're not doing anything with `n` any more). This example is very silly though, as it would be much simpler to just do `42 + i`, but it shows what you can do if you return an integer. 
Don't know it other than that it's one of the biggest sources of headaches in making a Tensorflow debian package. So much so that the packager built his own CMake-based system to replace it. I'm not up to date on the issues but I can tell you that a build system based around downloading everything on demand is a serious problem for packagers. It should at the very least be possible to disable and override paths to system-installed versions of dependencies.
You can. Not the brightest idea but you can. At least I could do in this online C compiler. [https://www.onlinegdb.com/online\_c\_compiler](https://www.onlinegdb.com/online_c_compiler)
The very compiler you linked to refuses to multiply pointers: const char * p = "hello"; const char * q = "world"; p * q; &gt; error: invalid operands to binary * (have ‘const char *’ and ‘const char *’)
If you declare and define the "methods" all on .c file they are not visible from others modules
What makes you think the solution at the bottom is undefined behavior?
Why not try a union, where one item is an array of two 32 bit ints, and the other is a 64 bit int. 
I said you need to deference them. Try this: int nums[5] = {1, 2, 3, 4, 5}; int *ptr = &amp;nums[4]; ptr = *ptr / 2; printf("%d", ptr); Or in your case `*p * *q`
Here's what I dislike about GNU Autotools: * It promises a lot more than it actually delivers. It does a bunch of platform/implementation tests to, in theory, allow the build to adapt to different environments — originally a bunch of different unixes with varying degrees of brokenness. In practice, Autotools builds only work out-of-the-box on the platforms where the developers have tried it. Take it into a slightly foreign environment and everything falls apart. * *Everyone* makes poor use of it, even GNU projects. Why is it wasting my time using a single core to make hundreds of checks for the existence of basic things like `stdlib.h` and `strcpy()`? If these are missing then the compiler is simply broken. There should be no attempt to work around it. A project should pick a specific C standard (C89, C99, C11, etc.), document this choice, and then assume this is what's provided. If it needs a specific platform (POSIX.1-2001, Win32, Linux 4.x, etc.), then also document this, then assume the platform is working correctly per the standard. More recently, projects using Autotools now produce GNU Makefiles, so you need GNU Make to build the project. Why am I bothering with Autotools if the resulting build requires a GNU environment anyway? GCC has a dozen or so sub-configure scripts that get run during its build, mostly doing checks that are entirely unnecessary. As a result, a significant portion of the GCC build is not parallelizable. * m4 is ugly and annoying. The only reason it still sees any use is because of Autotools. * Configure scripts are a dirty hack. I really *do* like that the script can be run on any system with a bourne shell (any unix system). Unlike CMake, you don't need Autotools just to build a project that uses it. That's really clever, so this isn't all bad. However, this comes at a cost. The generated script is part of your source release, and the specifics of the script depend on the version of Autotools that built it. This causes reproducability issues, and its cropped up in Debian on occasion where they really care about this stuff. It's only tolerated because it's so established: old and widespread. Here's what I like about it: * It's a standard build configuration interface that I can rely on. I can often build a project without even checking the build instructions just cause they're following the standard Autotools practices. I can set a prefix and know that it's probably going to work correctly (again, even some GNU projects manage to break their `--prefix` option!). I can even cross-compile (the GNU way) since Autotools often supports this fairly well. However, as I said, *everyone* uses Autotools poorly, so a lot of the time cross-compilation is pointless broken when it really shouldn't be. * Configure scripts are a useful hack. While it has its problems (see above), not needing Autotools when building from a source tarball is a really nice feature over CMake and others. 
But `*p` and `*q` are not pointers, but characters? Whether or not you can multiply characters has nothing to do with pointers.
I agree about the packaging dilemma. I just meant in the context of a personal build. IIRC, the reason bazel has little packaging support is that it is based on Google's blaze build system, and Google builds everything from a huge monorepo. In practice, that makes it hard for bazel to address the more normal use case involving system packages, etc.
He says "Strict alignment requirements", I'd have to check that though to make sure if he's right about that. /u/skeeto if you really have to, you might be able to do some compiler trickery by doing an aignment check or something, your code here: void inc64(uint32_t a[]) { a[1] += !++a[0]; } Is checked by clang's UBSAN for UB, and so is your first one (overflow). The last one, GCC catches the UB, but clang doesn't. If you're really that worried, make an assert or two or just use /u/which_spartacus 's answer.
This behavior is implementation defined. C89: &gt; With one exception, if a member of a union object is accessed after a &gt; value has been stored in a different member of the object, the &gt; behavior is implementation-defined. C99/C11: &gt; If the member used to access the contents of a union object is not the &gt; same as the member last used to store a value in the object, the &gt; appropriate part of the object representation of the value is &gt; reinterpreted as an object representation in the new type as described &gt; in 6.2.6 (a process sometimes called "type punning"). This might be a &gt; trap representation. As a real world example of an implementation where this would break: any system that uses a big endian representation. 
What I am not clear on is what part of that “strict alignment” requirement OP is concerned about. c spec requires elements of an array to have no padding, so is the concert that the first element might not be on a machines preferred boundary? If so I am pretty sure the spec also requires the compiler to emit code that will work properly for unaligned inputs if the are possible. (I am on my phone so my copy of the spec is not at hand, but there are a lot of situations in normal code where this is pretty much required). 
&gt; If so I am pretty sure the spec also requires the compiler to emit code that will work properly for unaligned inputs if the are possible. I'm fairly certain they have to work for *Any natural alignment*, so... anything with an alignment of less than or equal to `alignof(max_align_t)`
These are not compatible types, so it's a violation of strict aliasing. Compilers (GCC especially) get weird about this situation. 
I admittedly haven't read more than just your post, but this sounds an awful lot like bsd's make.
Then, in that case, do the opposite. &amp;#x200B; Keep a 64-bit value as the array, and access it as two values, for the two words you want to play with.
Some architectures require a 64-bit integer to have 8-byte alignment, and the pointer passed to `inc64()` may not be 8-byte aligned. uint32_t *a = malloc(...); inc64(a + 1); For example on PowerPC in big endian mode, this example would generate an alignment exception for the aliased definition of `inc64()`. 
So, backing up -- while uint64\_t \*x = (uint64\_t\*)a might not work: &amp;#x200B; void fiddleBits(uint64_t *v) { uint32_t *a = (uint32_t*) v; a[0] = a[1] ~ 0x7; } &amp;#x200B; That has no reason to not work. &amp;#x200B; Now, the C standard is written in the most computer sciency abstract language way possible. In theory, uint64\_t can be kept in one list and uint32\_t can be kept in a different list, and thus accessing from one to other would break. &amp;#x200B; In reality, this isn't the case. You do need to worry about byte order -- but you were already screwed in that case with the uint32\_t if you were doing bit twiddling. 
Ah I see. Well in any case my answer is just that, no, I haven't personally used it.
Anyways if you really are proposing a new packaging system I think it's a really good idea to consider all corner cases as much as possible and think hard out what you can offer over existing solutions like, eg nix or even a homebrew-style solution. Remember that initially developing such a project, while hard, is way easier than maintaining it and coordinating a team over the long run to review and integrate upstream work. Especially that projects already have to do quite some work to keep up their build systems and package files. I've got projects, small ones, that have files for autotools, pkg-config, now cmake installed files, etc, all to support compiling and installing like, 3 cpp files .. and that's not including the work that distro packagers do. there pretty much is more 'infrastructure' code than actual code.. it gets frustrating to satisfy everyone's favorite systems all at once, and keep them all working.
&gt; Autotools builds only work out-of-the-box on the platforms where the developers have tried it. Take it into a slightly foreign environment and everything falls apart. That's fair. Are other similarly complex build systems such as CMake or ANT--I've really only ever used Autotools so I'm not exactly familiar with what else is out there--able to handle foreign environments better? &gt; m4 is ugly and annoying. The only reason it still sees any use is because of Autotools M4 *is* fucking butt-ugly as well as reprehensibly horrendous to debug, but I still use it in my day to day work--completely independent of Autotools--because I have yet to find a more powerful and flexible preprocessor: it may take me longer to write an M4 macro than whatever it's replacing, but in the end that macro's going to save me a lot of typing--once I work all the kinks out. That said, I used to write a lot of Lisp code and I thoroughly enjoyed that, so I'm probably a little biased. 
What is the data in the dword array ? Would the compilers have easier time elsewhere, coping with dword accesses to qwords, like which\_spartacus said?
I have heard the only way to handle this sort of thing is to perform a memcpy to a uint64 or to handle the shifting yourself.
For learning purpose, start simple, worry about the rest later. (This does almost the same, except simpler output and no new line (\n) in the output). #include &lt;stdio.h&gt; void Add42(int n) { n = n + 42; printf("%d", n); // will print 50 } int main() { int i = 8; Add42(i); }
Initializes all array elements to 0. Integer value of 0 is equal to the NUL `'\0'` character so when you try to print it, it prints nothing - as it should.
Not a bad idea. However, that just seems to invert the problem itself. All the 32-bit accesses (the majority of operations) become unoptimized: /* Increment first 32-bit integer */ void inc32a(uint64_t a[]) { uint32_t lo = a[0] + 1; uint64_t hi = a[0] &amp; 0xffffffff00000000; a[0] = hi | lo; } /* Increment second 32-bit integer */ void inc32b(uint64_t a[]) { a[0] += 0x100000000; } GCC makes a mess (-O3): inc32a: movabs rcx, 0xffffffff00000000 mov rax, [rdi] lea edx, [rax + 1] and rax, rcx or rax, rdx mov [rdi], rax ret inc32b: mov rax, 0x100000000 add qword [rdi], rax ret Here's what I want to see: int32a: inc dword [rdi] ret int32b: inc dword [rdi + 4] ret Clang gets `int32a` right but not `int32b` (same as GCC). However, you did give me another idea: void inc64(uint32_t a[]) { uint64_t x = (uint64_t)a[1] &lt;&lt; 32 | a[0]; x++; a[0] = x; a[1] = x &gt;&gt; 32; } GCC produces the optimal assembly. Unfortunately Clang (7.0.0) makes a mess: mov rax, [rdi] add rax, 1 mov [rdi], eax shr rax, 32 mov [rdi + 4], eax ret 
&gt; Is, in fact, STRICTLY CONFORMING, so long as the length of the array &gt; passed is, in fact, 2, which we are assuming it is. Accessing a `uint32_t` via a `uint64_t` is undefined behavior and it *will* bite you someday unless you're consistently using `-fno-strict-aliasing`. I brought up alignment because that's one example of where this code goes wrong even when no compiler shenanigans are involved. Alignment is just *one* of the problems with that pointer aliasing hack. Also, here's an example where the array size is 2 but it's still probably not 8-byte aligned: struct foo { uint32_t x; uint32_t a[2]; } 
For learning purpose start simple and worry about the rest later. (This does almost the same, only difference is simpler output and no new line (\n) in output. #include &lt;stdio.h&gt; void Add42(int n) { n = n + 42; printf("%d", n); //will print 50 (this is a comment btw and does not matter for the code) } int main() { int i = 8; Add42(i); }
This is also undefined behavior. There isn't an alignment issue, but accessing a `unt64_t` through a `uint32_t *` violates strict aliasing and compilers will punish you for it. &gt; but you were already screwed in that case with the uint32_t if you were doing bit twiddling. My first two definitions of `int64()` do not have any dependence on the byte order, so it works identically on big and little endian systems.
To be precise, I am talking about using macros for big type generic DATA structures like HashMaps or Trees. You end up using multiple macros that need to include the type. I find it to be a strong case of trying to change the language. There's C++ for that. As with most things, there are legitimate cases, like the stretchy buffer concept.
https://en.cppreference.com/w/c/language/type Yes, uint32_t[2] is compatible with uint64_t so long as you cast it possibly. If you're *THAT WORRIED* about alignment, just make your structs you're having have a set alignment (which requires that your compiler support DR444): #include &lt;stdalign.h&gt; #include &lt;stdint.h&gt; struct thing { uint32_t thing; alignas(8) uint32_t thing2[2]; // type now also has an 8 byte alignment };
That's not what I'm talking about -- close, however. &amp;#x200B; void inc32a(uint64_t *a) { ((uint32_t *)a)[1]++; } /* Increment second 32-bit integer */ void inc32b(uint64_t *a) { ((uint32_t *)a)[0]++; } &amp;#x200B; which becomes, on gcc with -O3: inc32a: .LFB0: .cfi_startproc addl $1, 4(%rdi) ret inc32b: .LFB1: .cfi_startproc addl $1, (%rdi) ret &amp;#x200B;
I don’t think this method does what you think it would Declaring a function as static means the code for the function is local to that translation unit. This means the functions are “private” to ALL FILES - so not like a private method If you want to mimic a private function, simply define a static function in the implementation .c file
The array is everywhere else treated as a 32-bit integers, with a 64-bit counter in the middle. It's very similar to ChaCha: https://github.com/skeeto/keyed/blob/master/chacha20.h#L115
Okay, I'm confused. Either these 8 bytes are a 64-bit integra, in which case all this is pointless. Or this happens to also be 2 4-byte values that you are using for an unholy purpose. If you need to increment by 1&lt;&lt;32ULL in some case, why not just do that? Why do you need individual access to each word?
`memcpy()` won't help since the results depend on the byte order, but [shifting does seem to work for GCC](https://old.reddit.com/r/C_Programming/comments/a8aeb4/how_to_convince_compilers_to_produce_a_64bit/ec9a31e/). I hadn't thought about shifting until this thread.
Attempt #1: void inc64(uint32_t a[]) { uint64_t b = ((uint64_t)a[1] &lt;&lt; 32) | a[0]; b++; a[0] = b &amp; ((1ULL &lt;&lt; 32) - 1); a[1] = b &gt;&gt; 32; } GCC produces the output you want, but Clang does not. It comes close, though, so there might be a slight tweak to get it to behave how you hope. Attempt #2: void inc64(uint32_t a[]) { union { uint32_t a[2]; uint64_t b; } u = {{a[0], a[1]}}; u.b++; a[0] = u.a[0]; a[1] = u.a[1]; } Interestingly, this produces the exact same results as attempt #1. Clang recognizes that it can read the data as if it were just 64-bit, but then can't put it back the same way. I wonder why it's doing that? Attempt #3: void inc64(uint32_t a[]) { uint64_t b; memcpy(&amp;b, a, sizeof(b)); b++; memcpy(a, &amp;b, sizeof(b)); } This, however, produces exactly what you want, at least with GCC and Clang. See [here](https://c.godbolt.org/z/Ot4bny) for where I played around with different options. As it turns out, the (illegal) type aliasing produced the most consistently optimized results across compilers (especially when I started to play around with the more exotic architectures). Otherwise, \`memcpy\` actually did pretty well. Also, it could very well be that one of my first two attempts is acceptable in a more real-world scenario.
Thanks. Does this mean I should put my static const data in the .c file too? 
&gt; Yes, uint32_t[2] is compatible with uint64_t so long as you cast it &gt; possibly. No. Both GCC and Clang disagree with you on this. Simple demonstration: uint64_t foo(uint32_t *a, uint64_t *b) { *b = 0; *a = 1; return *b; } Both compile `foo` to this: foo: mov qword [rsi], 0 xor eax, eax mov dword [rdi], 1 ret So the following call returns 0 when optimization is enabled, and it probably returns 1 when optimization is disabled (-O0): uint32_t a[2] = {0, 0}; uint32_t x = foo(a, (uint64_t *)a); // x == 0 That's strict aliasing in action. 
If the data is private, yes If you want to make the constant public, that’s a time when static data makes sense in headers, since most compilers would inline static constants
If you type a&lt;enter&gt; b&lt;enter&gt; c&lt;enter&gt; d&lt;enter&gt; e&lt;enter&gt; Then the 1st 5 characters in the input stream are `a&lt;enter&gt;b&lt;enter&gt;c` 
&gt; Why do you need individual access to each word? Because the rest of the algorithm operates on the whole array as individual 32-bit values. Here's a real world example of this: https://github.com/skeeto/keyed/blob/master/chacha20.h#L115 That's ChaCha20, a cipher likely used to [encrypt this very comment](https://tools.ietf.org/html/rfc7905) on its way to you.
That's one reason I said this is a pet peeve rather than a red flag. There's definitely good reasons to have a different type to represent a boolean condition.
So should I be typing them all then press enter?
Your code doesn't do anything or prove anything: #include &lt;stdint.h&gt; uint64_t foo(uint32_t *a, uint64_t *b) { *b = 0; *a = 1; return *b; } int main() { uint32_t a[2] = {0, 0}; uint32_t x = foo(a, (uint64_t *)a); return *a; } On -O3 GCC 8.2 generates: foo: mov QWORD PTR [rsi], 0 xor eax, eax mov DWORD PTR [rdi], 1 ret main: mov eax, 1 ret So it's very clear that GCC thinks it's O.K: -O1 also generates: foo: mov QWORD PTR [rsi], 0 mov DWORD PTR [rdi], 1 mov rax, QWORD PTR [rsi] ret main: mov eax, 1 ret
You're making the common mistake of assuming that if it works in one instance, then it's not broken. Here's why it doesn't work: #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; uint64_t foo(uint32_t *a, uint64_t *b) { *b = 0; *a = 1; return *b; } int main(void) { uint32_t a[2] = {0, 0}; printf("%d\n", (int)foo(a, (uint64_t *)a)); } Running it: $ gcc -O3 example.c &amp;&amp; ./a.out 0 $ gcc -O0 example.c &amp;&amp; ./a.out 1 Your program's behavior shouldn't change when you change the optimization level. 
It depends on what you want to do. That would be one way for your code to read the input. Or you can modify how you read input (maybe by having the format string ignore whitespace?).
Try it. Changing your last line to printf("%d %c ",dizi[i],dizi[i]); will also give a clue about what's being read.
If my data is inlined does that mean it is duplicated? 
As someone who actually likes autotools but acknowledges its warts, I would like to say that this is one of the most well thought-out criticisms of it that I've read. So thank you.
&gt; able to handle foreign environments better? The main problem I have is that Autotools encourages users to make all sorts of unnecessary tests and then rely on the results. The problem is that a lot of the tests are flawed and the configuration will be wrong, sometimes in subtle ways. For example, when you're targeting Windows, they'll use tests that only work correctly within an msys environment (backslashes in paths, etc.). That really messes things up when cross-compiling. Another one that comes up a lot is they'll have a test to see if the host can execute .exe files, even if I'm explicitly cross compiling. I sometimes have the Wine binfmt enabled on Linux, so this test "passes" and produces an invalid configuration. That test shouldn't happen. 
It does, but static const int RED = 1; Is good to inline because it’s so small. That’s why ‘static inline’ functions are generally small - the duplicated code makes the executable larger but the code is (generally) faster
Yes, Im sorry. I don't know what I was thinking about lol. You need to do it this way: int x = 10; printf("%p\n", &amp;x); unsigned long long int *mult = (unsigned long long int)&amp;x / 2; printf("%p", mult); I used this hex calculator to double check the result and it was correct. [https://www.calculator.net/hex-calculator.html?number1=7ffce2aec90c&amp;c2op=%2F&amp;number2=2&amp;calctype=op&amp;x=58&amp;y=17](https://www.calculator.net/hex-calculator.html?number1=7ffce2aec90c&amp;c2op=%2F&amp;number2=2&amp;calctype=op&amp;x=58&amp;y=17) Again I don't know why anyone would find this any useful but it is possible.
Ah I definitely dont want that then, the data is big arrays
That won't work; the "static" keyword has different meanings in different contexts, and is different in C and C++. In particular, a static method in a C++ class is still "public" as far as the linker is concerned, even if it's private. But a static function in C is truly private to the .c file it's defined in. If you're trying to emulate a static class method in C, your only real option is to give it some sort of obfuscated name, typically starting with a couple of underscores. Typically, when I do something like this, I have a "public" header file "mystuff.h" that API clients include, and a second header file "mystuff_internal.h" that starts with a comment that says "The stuff in this header file is private to the 'mystuff' package. You should not include this header file". Also, if you have a structure that represents your object, put "struct mystuff {... };" in mystuff_internal.h, and put "typedef struct mystuff MyStuff;" in mystuff.h. Then all your functions accept a pointer to MyStuff, without the client app ever knowing what's inside that structure. When publishing an API, it's common to produce two packages: the "runtime" package containing the library mystuff.a and/or mystuff.so, and a second, "developers", package containing mystuff.h but *not* mystuff_internal.h.
It can be annoying to learn many things about autotools, I can agree, and way too much knowledge and best practices have to be learned by googling (possibly wrong) examples and solutions, but the [amhello tutorial](https://www.gnu.org/software/automake/manual/html_node/Creating-amhello.html#Creating-amhello) doesn't fit your description?
I think you're a little confused; You can multiply `chars` together because they have the semantics of an integer (typically `unsigned short`). Pointers however belong to an affine space. Although they may be _represented_ by an integer, semantically they are _not_ the same. This is because some operations simply _do not make sense_ for types with these semantics; `1 + 1` is fine, right; `0xDEADBEEF + 0xCAFEBEEF` where the hex numbers represent pointers, does not, because there is no origin. Subtracting them, however, does, it returns a type with different semantics, an integer. Similarly, multiplying a pointer with an integer (or a 'scalar') makes sense, because you have a complete vector.
The problem with both `union` and `memcpy()` is that the results depend on the byte order of the host. The bitshift solution seems to be the most promising: good results from GCC while being 100% clean and portable.
&gt; Your program's behavior shouldn't change when you change the optimization level. Congratulations! You've found a compiler bug in GCC! The optimization levels at O1 to O3 all generate 1, at least on the online compiler I'm using, and works (GCC 8.1.0) It also is gaurenteed to work if you change it to: #include &lt;stdio.h&gt; #include &lt;stdint.h&gt; __attribute__ ((noinline)) uint64_t foo(uint32_t *a, uint64_t *b) { *b = 0; *a = 1; return *b; } int main(void) { uint32_t a[2] = {0, 0}; printf("%d\n", (int)foo(a, (uint64_t *)a)); } The noinline attribute makes it so that the compiler doesn't mess with it.
Accessing a `uint32_t` via a `uint64_t *` violates strict aliasing. Plus the result depends on the byte order of the host.
In my example, both GCC 6.3.0 and GCC 8.2.0 give different results depending on the optimization level. I doubt 8.1.0 is any different. If you think it's a bug, please report it to the GCC devs. I bet $1000 they close it and tell you that it's not a bug. Using `__attribute__ ((noinline))` doesn't guarantee it to work. Again, you're just getting lucky because you disabled an optimization.
Actually, that entire algorithm seems to be treating everything as individual bytes that happen to be stored in 32-bit chunks -- I'm not seeing any 32-bit math on it.
I already corrected my point in the last post on this thread. 
Yhea, I tested it on my OWN computer now: (GCC: (Rev3, Built by MSYS2 project) 8.2.0), I am getting the same results, but JDOODLE was giving me different results. GCC is actually using an optimization here that has NOTHING to do with "strict aliasing" outside of , GCC assumes (incorrectly) that the two parameters passed NEVER alias internally, IF you're on -O2 and -O3, however on -O1 it actually recognizes that it doesn't (technically) alias, as it's the same thing being assigned twice. The generated code for the function as if it were a library is correct, however, the inlined code takes different assumptions based on compiler optimization. So yes, congratulations! You have found a compiler bug, report it to them.
In my experience NUL \_can\_ be printed, but I guess it depends on the system how far it survives or if it is visible. main() { printf("%c", 0); } hopo $ ./a.out | cat -vet ^@hopo $
Oh, it does, it is an excellent tutorial! I am just playing Devil's advocate on OP, while making the observation that in my days we had a bookcase full of o'reilly ant InformIT books, and nowdays the reading part seems to have gone sideways.
Packaging systems are complex. One thing that is unique to bake though, is that it does not have a global repository of packages, like brew, npm or apt-get. It therefore does not (and cannot) guarantee compatibility between random git projects. First and foremost, it provides you with a packaging system for *your* projects. If you use 3rd party bake projects, you'll have to do the extra work to make sure that new versions don't break the code. While this is not a full packaging solution, it still offers a lot of convenience since those 3rd party projects would use the same build system &amp; conventions around naming packages, specifying dependencies and storing include files / libraries / other resources. 
You don't need to put "private" details in header file. Put that in implementation file and make them static. Header is for public data while implementation file is for private data. This still doesn't help you hide struct fields. Look into opaque structs for that. 
Good point. The best example of this I have seen is the redis siphash implementation: https://github.com/antirez/redis/blob/unstable/src/siphash.c#L77.
&gt;multiplying a pointer with an integer (or a 'scalar') makes sense Typo, you mean offsetting?
I may be missing something, but I believe Skeeto is right on this. I see him getting downvotes below, but if you check n1548, the C1X standard (a little old, but the copy that happen to have lying around), and look at S6.5 P7, it says &gt; An object shall have its stored value accessed only by an lvalue expression that has one of &gt; the following types: &gt; &gt; * a type compatible with the effective type of the object, &gt; * a qualified version of a type compatible with the effective type of the object, &gt; * a type that is the signed or unsigned type corresponding to the effective type of the object, &gt; * a type that is the signed or unsigned type corresponding to a qualified version of the effective type of the object, &gt; * an aggregate or union type that includes one of the aforementioned types among its members (including, recursively, a member of a subaggregate or contained union), &gt; * or a character type. S6.2.7 defines compatible types, and it doesn't consider int32 and int64 to be compatible. It's also not a qualified version, it's not a signed/unsigned variant, it's not an aggregate type that includes the type, and it's not a character type. This is known as strict aliasing, and it is quite strict. Any program that violates any "shall" is undefined by the definition of C. An undefined program may or may not run correctly in any given implementation of C, but the language permits a strictly conforming implementation to crash, do nothing, erase your hard drive, etc. 
Mainly its usability. With bake, I was really going for something that would be *really easy* to use. I'd like junior engineers to be able to just pick up bake and start their first project within minutes. Autotools, and even other build systems like cmake, rake and premake have steeper learning curves. To give you an idea, this is an (admittedly trivial) bake project file: ```json { "id": "my_app" "type": "application", "value": { "use": ["foo.bar", "hello.world"] } } ``` That's all the information bake needs to build a project. As a bonus, if it also needs to build `foo.bar` and `hello.world`, it will make sure to build those before building `my_app`. This example is a bit more interesting, and links the application with `pthread`: ```json { "id": "my_app" "type": "application", "value": { "use": ["foo.bar", "hello.world"] }, "lang.c": { "lib": ["pthread"] } } ``` Since you'd only need to do this on Linux and not on Windows, you could add a filter like this: ```json { "id": "my_app" "type": "application", "value": { "use": ["foo.bar", "hello.world"] }, "lang.c": { "${os linux}": { "lib": ["pthread"] } } } ``` You could even abstract this in a separate project, like so: ```json { "id": "threading" "type": "package", "value": { "language": "none" }, "dependee": { "lang.c": { "${os linux}": { "lib": ["pthread"] } } } } ``` And then simply use `threading` as a dependency in `my_app`: ```json { "id": "my_app" "type": "application", "value": { "use": ["foo.bar", "hello.world", "threading"] } } ``` I hope that gives you an idea for what I am going for with bake, and why I didn't use something like autotools. I think autotools is really powerful, just more complex than what I wanted.
Thanks a lot!
It's not really about alignment, it's about aliasing. Please see my reply below (I should have made it here, but I'm too sleepy and messed up).
https://stackoverflow.com/questions/5240789/scanf-leaves-the-new-line-char-in-the-buffer
The aspirations of bake are a bit more humble ;) Bake enforces a certain structure and convention upon projects which I won't see adopted or even endorsed anytime soon by the language committees. The best that bake could ever hope for is to become a defacto standard for building code, much like GitHub has become a defacto standard for using git. Realistically though, I think building code is an inherently complex process, and there'll always be a bunch of different tools for different purposes.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/cpp] [Meet bake, a new build system &amp; package manager for C\/C++](https://www.reddit.com/r/cpp/comments/a8d7ny/meet_bake_a_new_build_system_package_manager_for/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
What about this? #include &lt;stdint.h&gt; #include &lt;assert.h&gt; int64_t increment(int64_t hi, int32_t lo) { assert( hi &lt;= INT32_MAX ); return (hi&lt;&lt;32) + lo + 1; } int64_t foo( int32_t value ) { return increment(UINT32_C(0), value); } Generates the following assembly: increment: sal rdi, 32 movsx rsi, esi lea rax, [rdi+1+rsi] ret foo: movsx rax, edi inc rax ret
It is essentially a vector with a magnitude changing, I think multiplying is fine here.
This is not the same thing, you are doing an operation with a pointer and a scalar. Dividing by 2 works, as was discussed, but dividing by a pointer does not.
&gt; and it doesn't consider int32 and int64 to be compatible. I keep trying to find this, but it seems impossible to find exactly. But, you should also look at the first section I had posted: 6.3.2.3 paragraph 7 &gt;A pointer to an object type may be converted to a pointer to a different object type. If the resulting pointer is not correctly aligned69) for the referenced type, the behavior is undefined. Otherwise, when converted back again, the result shall compare equal to the original pointer. When a pointer to an object is converted to a pointer to a character type, the result points to the lowest addressed byte of the object. Successive increments of the result, up to the size of the object, yield pointers to the remaining bytes of the object. So long as they are correctly aligned in this case, *AND* the format of the integers is the same, this program, is, in fact, correct. So, this predicates us on two issues: Can we gaurentee that they are aligned? And can we gaurentee that the platform's format of the intergers are the same? The answer is: so long as we are converting/initializing from the same thing, yes! uint64_t val = 12; uint32_t a[2] = &amp;val; uint64_t b = *a; The previous 3 lines shown should be 100% valid based on this.
&gt; GCC is actually using an optimization here that has NOTHING to do with "strict aliasing" outside of it's own internals The code I used is the *classic* example of strict aliasing. It's [literally what John Regehr uses when discussing strict aliasing](https://blog.regehr.org/archives/1307). The behavior you're seeing is well known to both the GCC and Clang devs. Neither consider it a bug. &gt; You have found a compiler bug, report it to them. I don't consider it a bug. If you do, you report it. My offer of $1000 bet is still up that they dismiss it as not a bug.
Awful. Also, misses support (or second-class support) for too many platforms. Maybe it makes sense if you're Google and your whole world revolves around it or you have a team that does nothing but develop for it.
`CHACHA20_QUARTERROUND` does 32 bit addition, left shift, right shift, and XOR. There are no 64-bit operations, though there is a 64-bit counter that's operated on like a bignum. Initially there's a step to read 32-bit little endian integers from a byte buffer (key and IV). After a block is computed, there's also the reverse operation. Otherwise there are no byte operations.
Your `increment()` is very close to [my `inc64()` here](https://old.reddit.com/r/C_Programming/comments/a8aeb4/how_to_convince_compilers_to_produce_a_64bit/ec9a31e/), and GCC generates perfect code for it. The only difference is that you used signed integers, which isn't quite what I need.
When something isn't mentioned in the semantics, then that thing is undefined by way of omission. So the fact that they don't say "int32 is compatible with int64" means that it's not. In the bit you quoted, all they say is that they can be converted---the pointers can exist. They don't talk about whether you can actually use those pointers, which is what the bit I quoted does. Even if the platform's format is the same, it has no bearing on whether or not this operation is defined in the language C.
Ty
Imagine a was 0x00000004. This would be 32 bit aligned but not 64 bit aligned. Which means if you were to perform a 64 bit instruction at that address it would crash. No matter what code you come up with it will not be safe to perform a 64 bit address of a, that's why the compiler won't do it.
If it doesn't support msvc, it's half-baked.
According to the authors of the Standard, a major aspect of the Spirit of C is "Don't prevent the programmer from doing what needs to be done". UB was expected to be used as an opportunity for implementations targeting different purposes to work in a variety of ways, so that each implementation could best serve its intended purpose. Compiler writers and programmers should be better positioned than the authors of the Standard to judge the costs of supporting various features on various implementations, and determine what makes sense. The portable way to increment a number stored as two 32-bit halves is to do each half individually. If the values might not be stored in a way that satisfies the platforms' requirements for 32-bit values, the portable way will be the only reliable way. On implementations that are configured to be suitable for low-level programming, addition using a 64-bit lvalue will be reliable if alignment requirements are met. There are three approaches one can take to ensure things are handled sensibly: 1. Quality compilers like icc will recognize, even with type-based aliasing analysis enabled, that if a `uint32_t*` is converted to a `uint64_t*` and immediately used, they should make no assumptions about whether the action would affect things of type `uint32_t`, even though they could assume the actions wouldn't affect things of other type like `float` or `double`. 2. Quality compilers like icc will recognize, even with type-based aliasing analysis enabled, that when a volatile read of any type is performed, they should make no assumptions about how such a read would interact with writes (volatile or not) of any objects, of any type, that would be accessible to the outside world, and when a volatile write is performed they should likewise avoid assumptions about reads or writes to other objects, 3. When using gcc or clang, it is necessary to use `-fno-strict-aliasing`, which may degrade performance *unless* one takes care to use `restrict`-qualified pointers when practical. Using `restrict` will make it possible to inform gcc of most of the places where it could benefit from assuming that things won't alias, including many places where type-based aliasing alone would be insufficient. Note that a construct like: uint64_t assemble2x32(uint32_t *p) { union { uint32_t h[2]; uint64_t full; }u; u.h[0]=p[0]; u.h[1]=p[1]; return u.full; } void decompose2x32(uint32_t *p, uint64_t value) { union { uint32_t h[2]; uint64_t full; }u; u.full=value; p[0]=u.h[0]; p[1]=u.h[1]; } void inc64(uint32_t *p) { decompose2x32(p, assemble2x32(p)+1); } uint32_t test2(uint32_t *p1, uint32_t *p2) { if (*p1) inc64(p2); return *p1; } might generate decent code on some compilers (tested on x-64 gcc 8.2), the generated code is pretty horrible on many others. The Standard was never intended to throw the Spirit of C out the window, but instead relies upon compilers to uphold it without regard for when conformance would require it. Jumping through hoops to accommodate compiler writers who interpret the Spirit of C as "Don't allow programmers to do anything the Standard doesn't require" will simply increase the rift between the extremely useful and powerful language Dennis Ritchie invented, and the much more limited dialect processed by gcc and clang with optimizations enabled. 
x86-64 has no alignment restrictions for the `inc` or `add` instructions, so there's nothing preventing a compiler from using these. They're just not smart enough (yet) to optimize this. However, I *did* work out a definition where GCC produces basically what I want: void inc64(uint32_t a[]) { uint64_t x = (uint64_t)a[1] &lt;&lt; 32 | a[0]; x++; a[0] = x; a[1] = x &gt;&gt; 32; } Which [GCC 8.x compiles to](https://godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(j:1,lang:___c,source:'%23include+%3Cstdint.h%3E%0A%0Avoid%0Ainc64(uint32_t+a%5B%5D\)%0A%7B%0A++++uint64_t+x+%3D+(uint64_t\)a%5B1%5D+%3C%3C+32+%7C+a%5B0%5D%3B%0A++++x%2B%2B%3B%0A++++a%5B0%5D+%3D+x%3B%0A++++a%5B1%5D+%3D+x+%3E%3E+32%3B%0A%7D'\),l:'5',n:'0',o:'C+source+%231',t:'0'\)\),k:50,l:'4',n:'0',o:'',s:0,t:'0'\),(g:!((h:compiler,i:(compiler:cg82,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',libraryCode:'1',trim:'1'\),lang:___c,libs:!(\),options:'-O3',source:1\),l:'5',n:'0',o:'x86-64+gcc+8.2+(Editor+%231,+Compiler+%231\)+C',t:'0'\)\),k:50,l:'4',n:'0',o:'',s:0,t:'0'\)\),l:'2',n:'0',o:'',t:'0'\)\),version:4): inc64: add qword [rdi], 1 ret Unfortunately Clang and GCC 7.x miss this optimization, but things do look good for the future.
Prime numbers have always interested me. A few months ago, I sat down just for fun to see if I could write a prime number tester in Python. I succeeded, but as you can imagine, the program was unbelievably slow when the number grew to any significance. Since I was just starting to learn C at the time, this seemed like a perfect project to work on while I was learning the ins and outs of C's syntax and standard libraries. After a couple of iterations using various integer types, from `int` through `long unsigned int`, I started looking into the `gmp.h` library. This proved perfect for this project, and the result is a small CLI utility that can test arbitrarily large n-digit numbers to see if they're prime (and tell you their first divisor if they aren't). I had a lot of fun working on this project and learning things along the way. I hope some of you find it useful!
Just make it a lot simpler to fix this problem, instead of using scanf, just use: dizi[i] = fgetc(stdin); To avoid the problem
| Anything; it doesn't care because it's not a C program. Where do people some up with such nonsense? Quoting N1570 4p7: "A conforming program is one that is acceptable to a conforming implementation." [meaning that is accepted by at least one implementation--not that it accepted by all]. While the Standard imposes tighter requirements for *strictly conforming* programs, the notion that only strictly- conforming programs have any kind of legitimacy is directly contradicted by the Rationale for the Standard. Instead, according to the Rationale, the authors of the Standard viewed an ability to usefully process various non-portable programs as a "quality of implementation" issue. Incidentally, they also view the ability to usefully process *any programs whatsoever* likewise. "The Standard requires that an implementation be able to translate and execute some program that meets each of the stated limits. This criterion was felt to give a useful latitude to the implementor in meeting these limits. While a deficient implementation could probably contrive a program that meets this requirement, yet still succeed in being useless, the C89 Committee felt that such ingenuity would probably require more work than making something useful." Quality implementations will uphold the Spirit of C, including among other things the principle described in the published Rationale document as "Don’t prevent the programmer from doing what needs to be done." If the Standard fails to define the behaviors required to accomplish some task in practical fashion, but an implementation could facilitate the task by processing some action "in a documented manner characteristic of the environment", quality implementations claiming to be suitable for that task should make a bona fide effort to process such actions in that manner when it would help facilitate those tasks. Those claiming to be suitable exclusively for other tasks not requiring such behavior, however, have no obligation to do likewise. I'd have no beef with gcc and clang if they more openly stated that they make no effort to make their optimizers suitable for many of the purposes for which C programs are most useful, without trying to suggest that code targeting compilers that are suitable for such purposes is somehow "broken". 
That is a very primitive way to check for primality. Why not try something like a Miller-Rabin test?
If you can divide by 2 you can divide by the numerical value of the pointer. I also tried that. int x = 10; printf("%p\n", &amp;x); /*Too lazy to check if pointers are 4 or 8 bytes so just to be sure.*/ unsigned long int *mult = (unsigned long int)&amp;x / (unsigned long int)&amp;x; printf("%p", mult); //Will print 0x1 &amp;#x200B;
Strict aliasing is just one of three reasons why `((uint64_t *)a)++` is a bad idea, so `-fno-strict-aliasing` wouldn't help anything even if I was willing to use it. The other two are: 1. Alignment issues, which, outside of aliasing, is another reason why these compatible type rules exist. 2. The result depends on implementation details. A big endian host will get a different result than a little endian host. I want to solve all three problems while also getting the compiler to produce the ideal assembly. I figured out how to do so with GCC 8.x, but not anything else.
cool, and it works the same on Linux
In many cases, it may be necessary to process a significant chunk of data as an array of uint32_t. GCC, however, is unable to handle constructs like the following: union BLOB { uint32_t ww[16]; uint64_t ll[8]; } blob; uint32_t test(int i, int j) { if (*(blob.ww+i)) *(blob.ll+j) = 0x123456789ABCDEF0; return (*(blob.ww+i)); } Note that there isn't actually any *aliasing* in this example, since every pointer that is used to access any lvalue is derived directly from `blob`, used, and abandoned without any intervening reference to `blob` or other constituent thereof. If compiler writers would honor the stated intention of the footnote to N1570 6.5p7 which says the purpose of the rules is to say what things *may alias* [as distinct from preventing programmers from doing what needs to be done with freshly-derived lvalues], that would support most of the things programmers need to do while eliminating the need for the "character type exception", "effective types", and other such obstacles to simple and efficient optimization. 
Those would probably be interesting to add into the next version of this. I've no illusions of surpassing GIMPS, or anything like that, for serious mathematical research (I'm a cybersecurity tech, not a mathematician). This was more of a fun project to tackle something that wasn't really doable in the languages I typically work with (Python, PowerShell, bash). Still, thanks for the suggestions. Both should make for some interesting research.
Many debuggers are limited to putting breakpoints on line boundaries. Writing the code with returns on separate lines will make it easy to set a breakpoint on one or the other. Having the program stop only on cases where the function would return 0 will generally be much less efficient, if it can be accomplished at all (typically, the debugger would need to interrupt program execution unconditionally, test whether breakpoint conditions are met, and resume execution if not--a course of action that could cause an order of magnitude slowdown if the function call occurs within a tight loop). 
Some comments: * `input_error_1`, `input_error_2`, `input_error_3` and `input_error_4` need more descriptive names (perhaps `input_error_no_arg`, etc.) * `return(value); break;` is redundant. Since you're returning you'll never hit the break statement. * Unless you're using pre-c99, it's better to only declare your variables once you can also initialize them. So instead of writing: int is_prime; // ... many lines // is_prime = ... you should write: // ... many lines // int is_prime = .... * Learn about `for` loops and use them instead of `while` loops when appropriate (which is true for both places you use it). * When dealing with boolean like returns, just check one way and use else. So instead of writing: is_prime = prime_test(...); if (is_prime == 0) { // not prime } else if (is_prime == 1) { // not prime } you should write: is_prime = prime_test(...); if (is_prime == 0) { // not prime } else { // not prime } since `is_prime` is now used in exactly one place, you can get rid of the temporary variable: if (prime_test(...) == 0) { // not prime } else { // not prime }
Thanks very much for the comments! Which of the `while` loops do you think would benefit from a conversion into a `for` loop, and why? Is there a performance differential in doing so, or would it just be syntactically cleaner in one of these spots?
If you cast a pointer to an int, it is no longer a pointer.
If I cast a pointer to an integer will it still be composed of bytes? Memory addresses are numbers and they can be manipulated as such. My posts prove it. If you divide mult's memory adress by its own memory adress it will point to 0x1. 
The rules in 6.5p7 include a footnote indicating that the purpose of the rule is to say say what things may alias. Interpreting the rule as applicable in cases that do not involve aliasing would render the language essentially meaningless. Among other things, the rules do not allow an aggregate to be accessed using an lvalue of a member type. This is deliberate, because allowing such aliasing would compel a compiler given something like: struct foo {int offset,size; int *dat; }; void store_next(struct foo *f, int value) { if (f-&gt;offset &lt; f-&gt;size) f-&gt;dat[f-&gt;offset++] = value; } void store_several(struct foo *f, int value, int count) { while(count--) store_next(f, value); } to allow for the possibility that an access via `f-&gt;dat` might alias `f-&gt;int` or `f-&gt;size`, thus severely limiting optimizations. Since it's very rare that programs would need to use an lvalue of member type to *alias* an aggregate, the Standard makes no provision for it. If one recognizes 6.5p7 as merely saying what things may alias, then something like: void inc_as_uint64(uint32_t *p) { (*(uint64_t*)p)++; } shouldn't pose any problem because any compiler that isn't being willfully blind will be able to see that the operation affects storage which is identified via `uint32_t*`, and thus treat the operation as a potential access to an object of type `uint32_t` or to one or more elements of a `uint32_t[]`, and those objects would be allowed to alias other objects of type `uint32_t`. Note that if a pointer is derived from an object of another type, any use of that pointer following any operation which involves the original object without going through the pointer would represent aliasing, and 6.5p7 would restrict such accesses. In code like the above, however, there is no aliasing and thus quality implementations that uphold the Spirit of C (which *the authors of the Standard* describe as "Don't prevent programmers from doing what needs to be done") shouldn't try to obtusely apply 6.5p7. 
Op expressed an interest in portability. That requires a strictly conforming program. If you follow the chain of definitions: 1. A *conforming program* is one that is acceptable to a *conforming implementation*. 2. Both forms of *conforming implementation*s must accept any *strictly conforming program* (give or take complex and other parts of the stdlib), but otherwise have no requirements. 3. A *strictly conforming program* shall use only those features of the language and library specified in this International Standard. It shall not produce output dependent on any unspecified, undefined, or implementation-defined behavior, and shall not exceed any minimum implementation limit. It happens that the standard does have limits on implementation-defined behavior and unspecified behavior with requirements restricting/guiding what conforming implementations can do. (E.g. for unspecified, the implementation has to document the behavior so the programmer knows what they can expect, to be able to write programs that behave like they expect). But, the C standard says undefined behavior has absolutely no requirements. It is &gt; behavior, upon use of a nonportable or erroneous program construct or of erroneous data, for which this International Standard imposes no requirements There's a huge divide between undefined and unspecified. Non-portable programs are fine; the standard provides a set of options for implementations and requires choices to be documented. However, programs with undefined behavior aren't even considered. They are undefined by omission. I say, they aren't C programs. Maybe another language called "GCC C" provides semantics for those things, but conforming implementations of C aren't required to. Ultimately it's just an argument about the semantics (pun intended) of "C program". Define it however you like. I consider relying on undefined behavior in a program a mistake, and feel justified by the standard for thinking so (the rationale is non-normative). We've seen over and over that implementors agree that undefined behavior gives them carte blanche to do whatever they want (usually really unexpected things, that differ from one version to the next, from one compilation option to the next). Unspecified and implementation-defined? Sure! But undefined? Never. 
If you want compilers to generate a 64-bit increment, you need to be targeting a platform where that will have the desired behavior. If you want to use a 64-bit increment on platforms that support it, and accomplish the operation via other means on other platforms, then you should use conditional compilation or selective inclusion to produce different code for different platforms. In most of the cases I've found where chunking optimizations are useful, I will either know that I'm targeting a platform with which requires alignment only to the smaller type, or know that the actual address I'm using will be aligned to the larger type (e.g. the data is in an array which I've forced to be 8-byte aligned, and the index of the lower-numbered element is even). C became popular because it allowed programmers armed with *simple* implementations to achieve reasonable levels of performance by exploiting whatever useful features their target platforms happened to support. The notion that compilers should search heroically to find chunking optimizations that programmers could have expressed manually if they thought it appropriate seems contrary to the principle of C being a "simple" language. 
Both of them because it's idiomatic for the specific pattern you're using: init; while (check) { next; } for (init; check; next) { } One of the major benefits is it brings all the steps in controlling the loop next to one another so it's easier to see what is going on. 
I agree: [https://github.com/SanderMertens/bake/issues/1](https://github.com/SanderMertens/bake/issues/1) &amp;#x200B;
I understand the confusion, but I'm not optimizing a chunking operation. This is essentially a bigint embedded inside a 32-bit state array. ChaCha is a real life example of this: https://github.com/skeeto/keyed/blob/master/chacha20.h#L115 Since this is a practical matter, I'm really only focused on the *is*, not the *ought*. Ultimately my goal is to express it in such a way that it is: * Portable: The results don't depend on any details of the implementation (byte order, etc.), nor does the program use extensions or other non-standard constructs. If it's not fast, then at least it's guaranteed to work correctly. * Simple: There are multiple ways to express this operation, and I want to pick a simple implementation that achieves my other goals. * Performant: Mature compilers (GCC, Clang, even MSVC) targeting common architectures (mostly x86-64) produce essentially the same, ideal assembly that I'd write myself. What I like about C is that I usually get to have all three, or very nearly so, at once. If I'm careful, I still get them even when I don't know platform I'm targeting. I *did* find a solution for this with GCC 8.x (with Clang likely to follow in the future), and I put it at the bottom of my post. This solution compiled with GCC 8.x produces perfect assembly for both x86-64 (`add`/`inc`) and x86-32 (`add` then `adc`), it's correct regardless of the host, and I didn't have to violate strict aliasing, either *de jure* or *de facto*. In fact, violating strict aliasing would have defeated my goals anyway. 
It is highly possible. I made the mistake of not reading your comments. The problem with signed integers is if the upper side and lower side have different signs, so maybe my implementation is bogus.
OK, that makes sense. Thanks for the explanation.
From the point of view of the Standard, the difference between Implementation-Defined behavior and Undefined Behavior is that implementations are *required* to specify a behavior for the former even in cases where doing so would be expensive and useless. The notion that the authors of the Standard intended anything near the huge distinction is contradicted by the published Rationale. &gt; The terms unspecified behavior, undefined behavior, and implementation-defined behavior are used to categorize the result of writing programs whose properties the Standard does not, or cannot, completely describe. The goal of adopting this categorization is to allow a certain variety among implementations which permits *quality of implementation* to be an active force in the marketplace as well as to allow certain *popular extensions*, without removing the cachet of conformance to the Standard. Informative Annex J of the Standard catalogs those behaviors which fall into one of these three categories. Does the author's use of the phrase "popular extension" refer to the notion of implementations defining things which the Standard requires them to define? Or the fact that implementations specify that when given a choice of specific behaviors they'll select a particular one? Or to the fact that in places where the Standard allowed implementations to behave however they see fit, many opted to behave in predictable and useful fashion that allowed programmers to efficiently accomplish a wider range of tasks than would be possible in strictly conforming code? Why do you suppose the authors of the Standard mentioned the fact that commonplace implementations would treat an expression like `ushort1 * ushort2` as equivalent to `(unsigned)ushort1 * ushort2` unless the result was used in certain specific ways, *even in cases where the result was between `INT_MAX+1u` and `UINT_MAX`*, if they did not intend that programmers should expect such behavior from commonplace implementations (see page 44 of http://www.open-std.org/jtc1/sc22/wg14/www/C99RationaleV5.10.pdf for precise details). I think the authors of the Standard expected and intended that implementations would process things in such fashion absent an obvious and compelling reason to do otherwise. They'd probably have regarded the fact that some ones'-complement platform would require extra cleanup code when processing unsigned multiplies as a compelling reason for unusual behavior on those platforms, but the possibility that `unsigned foo = ushort1 * ushort2;` might by happenstance only be used for products in the numerical range 0 to `INT_MAX`, and dead-branch elimination on that basis might happen to be useful would seem far less compelling, I'm not sure who invented the notion that the authors of the Standard intended a huge gulf between Implementation-Defined and Undefined Behavior, but such a notion is not supported by anything whatsoever in the rationale that I can identify. It's true that the Standard never requires a program to behave usefully if it invokes Undefined Behavior, but the Standard also makes no attempt to require that implementations behave usefully when given *any* program. As the Rationale notes: &gt; While a deficient implementation could probably contrive a program that meets this requirement, yet still succeed in being useless, the C89 Committee felt that such ingenuity would probably require more work than making something useful. Note that part of the mandate for the Standard was: &gt; Don’t prevent the programmer from doing what needs to be done. If many programs rely upon a particular behavioral guarantee, but there might plausibly exist some platforms where upholding it would be expensive, someone targeting such a platform would be better placed than the authors of the Standard to weigh the costs and benefits of defining the behavior on that platform. That in no way implies any judgment that the implementation shouldn't be defined by quality implementations that claim to be suitable for low-level programming on platforms where such behaviors would be useful. 
If the Standard were to define macros which allow an implementation to specify whether it consistently uses big-endian or little-endian storage and what kinds of type punning it can support with freshly-derived pointers/lvalues, then such code could easily be handled in portable fashion in such a way that even simple implementations could process it efficiently. The notion that it's somehow better to have programmers guess at what complicated constructs compilers will be able to render into simple code seems crazy to me. 
Writing code where the byte order matters [seems crazy to me](https://commandcenter.blogspot.com/2012/04/byte-order-fallacy.html).
Game words? I'm not sure I understand, but it's a distinction that needs to be made, types exist and have constraints for a reason. The fact that in C you can have a variable or rvalue that is the value of some type aliased as another does not change it, you are not multiplying two pointers, because you cannot. 
Try RayLib, great little c library :) 
For one, neither uint32_t or uint64_t are guaranteed to exist.
We're obviously coming at this from two different perspectives. While I've read the rationale a number of times, it holds no real weight for me---it's not normative. It's not part of the C standard that defines any language we call "C". It's interesting, in the same way that an artist's thoughts on their painting might be interesting, but those thoughts are not part of the painting. The languages C99, C11, C18, etc., are standardized languages. They permit all sorts of extensions, but those extensions are not part of C. This is important because implementors are, in fact, taking extreme advantage of undefined behaviors in their optimizations, which have led to some huge security vulnerabilities in major pieces of software. This is allowed by the standard, and so a practical, pragmatic programmer who is interested in portability and correctness must avoid undefined behaviors. That GCC or Clang deals with undefined behavior in such a way may "prevent the programmer from doing what needs to be done", but those words are not part of the standard that defines the language C. Perhaps one day they will be added, but it is not part of any Cx standardized so far. If it were ever to be added, it would be in a new language, maybe called C37, which still wouldn't be C18 or C99. I think perhaps you might get more traction if you tried to argue that the standard needs to change to include those ideas, or simply to reduce the number of undefined behaviors. Others have started work on this, e.g., [Friendly C](https://blog.regehr.org/archives/1180) and [Boring C](https://groups.google.com/forum/m/#!msg/boring-crypto/48qa1kWignU/o8GGp2K1DAAJ). I could certainly get behind those kinds of ideas. But then you may want to read the [Friendly C Followup](https://blog.regehr.org/archives/1287).
I think it's weird that this would optimize on big endian, because your type is conceptuqlly little endian.
You might want to consider the name of your tool, as it could cause confusion with https://github.com/esrlabs/bake
I haven't read your code yet, but setting aside things like what u/FUZxxl said about better algorithms, I notice that your algorithm description says it tests divisors up to n/2. It's only necessary to test up to sqrt(n) (possibly + 1 to fudge rounding error in the sqrt method). My guess is that's what you were after with the /2 limit. Using 15 as an example, we don't need to try dividing up to 7 (or 8). The sqrt is ~3.872. Dividing by 3 will find that 3 and 5 are factors, so we don't need to also test 5 separately.
I'd expect it's slower on big endian for this reason, and that the best option would be a 32-bit add-with-carry. The primary target is little endian systems.
You can use `uint32_t a[static 2]` to avoid the objection of possibly being called on only a single int. Whether your compiler uses this info is another matter. If the compiler doesn't optimize when it should, submit a bug report.
I'm confused; is there some sort of requirement that the increment be done in an atomic fashion? Is this a multi-threaded application or is this array of memory part of a device that might change its contents outside of cpu control? (I swear I've seen this before as an interview puzzle.) Also: your post implies that you can't control the alignment of your two 32-bit words; can you confirm that? In other words, why is this broken? void inc64(uint32_t words[2]) { if (++words[1] == 0) { ++words[0]; } } Assuming that this is a multi-threaded environment, or for some other reason, the increment has to be done atomically: If your CPU requires 64-bit values to be 64-bit aligned, and you can't guarantee that your input is 64-bit aligned, then I think you're screwed. You'll have to copy your input to a 64-bit register or to 64-bit aligned memory to work on it, and there's almost certainly no way to do that if your CPU requires alignment. In fact, if you have a 64-bit data bus, there's no way to do it even if your cpu *doesn't* require alignment, because the hardware will still break the memory accesses into two accesses. If this is the case, you may as well walk away now because I don't believe there's a solution. If this is a multi-threaded issue, then you can solve this using a mutex. There are probably other multi-threaded lock-free algorithms to do what you want. Now if you *can* guarantee that your data will have the proper alignment, then you have a better chance of solving this. It's possible that the new _Atomic class made available in C11 will do what you want. It's somewhat restricted, and I suspect it doesn't work with pointers to arbitrary data in memory. I had to deal with this exact issue with the Solaris compiler back in the day, programming a device driver for a device that required 64-bit writes. The compiler kept insisting on splitting the writes into two 32-bit writes, even on architectures with 64-bit data. My answer then, and possibly your answer now, is to switch to assembly language. Use the ASM macro for your compiler (I think they all have one) and do it by hand. I'm afraid this will be your only solution. As the saying goes: Here There Be Dragons. There's pretty much no way to prevent the compiler from optimizing this any way it wants. Even if you were to discover the black magic fuckery necessary to get the compiler to generate the instructions you want, there's no saying that the next version of the compiler won't go back to doing what you want. There is a little bit of hope in the more modern compilers. As of C11, there's the `_Atomic` keyword that affects how the compiler might 
&gt; is there some sort of requirement that the increment be done in an atomic fashion? Is this a multi-threaded application or is this array of memory part of a device that might change its contents outside of cpu control? None of that. This is a hot spot, so I want the popular compilers to implement this operation in a single x86-64 instruction instead of needlessly using four. Some expressions are easier to optimize than other equivalent expressions, so I'll prefer them. I also wanted to be sure I wasn't accidentally blocking this optimization like in my example with `if`. Compilers have to consider scenarios I'm not expecting, so it's easy to miss. &gt; Also: your post implies that you can't control the alignment of your two 32-bit words; can you confirm that? Alignment doesn't matter since my main concern is x86-64, and what I want is a simple, non-atomic increment. I was just using alignment as one reason (of several) as to why the inevitable suggestion of a pointer aliasing hack isn't a valid solution. Mentioning strict aliasing causes everyone to lose their minds, so I wanted to avoid it. Unfortunately that happened anyway.
SDL is quite easy to use and to get running. OpenGL with GLUT can be only a few lines of setup code for doing 2D or 3D graphics. OpenGL is a lot less to setup compared to Vulkan and also easier to get started with. Starting with Vulkan will only get you confused, OpenGL is a better entry point if you wanna do some 3D graphics. If you are just wanting to try to do something on your own, you can write a function to export [.ppm](https://en.wikipedia.org/wiki/Netpbm_format)-files, which you then can view in your image viewer of your choice. Then you only need to allocate your image in memory and then write your code to manipulate the pixels you want.
**Netpbm format** A Netpbm format is any graphics format used and defined by the Netpbm project. The portable pixmap format (PPM), the portable graymap format (PGM) and the portable bitmap format (PBM) are image file formats designed to be easily exchanged between platforms. They are also sometimes referred to collectively as the portable anymap format (PNM), not to be confused with the related portable arbitrary map format. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
OpenGL is great for 3D work, but if he only needs 2D surfaces, cairo get be another good option. https://www.cairographics.org/
Yeah that's another alternative that I did not think about. Great addition!
&gt; I want the popular compilers to implement this operation in a single x86-64 instruction instead of needlessly using four Frankly, I would leave it alone. Beating yourself up over one specific compiler/architecture combination gains you very little and costs you portability. Plus you should consider the possibility that the folks who designed the compiler know more about optimization than you do. For all you know, that single 64-bit operation you envision takes longer than the four operations the compiler generates.
I'm plenty familiar with x86 to know that most compilers are getting this quite wrong. It's a straightforward missed optimization. Also, in this particular case, if it works well on x86-64, I expect similarly good code generation on other targets (i.e. the compiler recognizes this particular pattern as a specific higher level operation). And, indeed, that turned out to be the case with x86-32 even though I hadn't considered it. Besides, I *was* ultimately successful with GCC at no cost to portability (see the edit on my post), and I expect a future version of Clang to pick up this optimization, too.
Definitely! This library needs more press and love, such a nice library to learn Computer Graphics and Game Programming.
If you just want to generate static images you don't even need a library. Write a .ppm P6 ([https://en.wikipedia.org/wiki/Netpbm\_format](https://en.wikipedia.org/wiki/Netpbm_format)) writer (format is simple enough that it's trivial) and then generate images as pixel 2d arrays and just write them to .ppm. &amp;#x200B; Maybe try writing a raycaster, it's my goto project when learning new language.
**Netpbm format** A Netpbm format is any graphics format used and defined by the Netpbm project. The portable pixmap format (PPM), the portable graymap format (PGM) and the portable bitmap format (PBM) are image file formats designed to be easily exchanged between platforms. They are also sometimes referred to collectively as the portable anymap format (PNM), not to be confused with the related portable arbitrary map format. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
Well, there's several problems this code. But let's start with the one that's probably causing this bug. Consider the loop where you're reading the input file. What is the maximum number of lines that could be read within that loop?
Honestly, this is what inline assembly and architecture specific headers are for. Even if you find a layout that works today, it's very probable that future changes to the optimization schemes may break it. It's a fun thought experiment, but if this is for production then you're approaching the problem from a very difficult angle.
It reads 13 lines. If the input has more than 13 lines it only stores the first 13 lines. If the input file has fewer lines it prints out an error.
#defined macro variables are just textually replaced in your source code before any semantic analysis. Const is part of the type system and participates in all semantic checks. It effectively means read-only but since it can be easily cast away, there are no guarantees about a const variable's immutability during runtime. They are really different things in many aspects. You should study them separately.
Nahh the best way to define a constant is to use `typedef enum NAME { blah } NAME;` then you can even have the compiler do type checking.
&gt; If the input has more than 13 lines it only stores the first 13 lines. OK. So your "troublesome" input file. How many lines does it have in it? &gt; If the input file has fewer lines it prints out an error. How can it do that? Your code doesn't even _have_ any error messages in it.
enum is an int in C, so there's nor more type checking than there would be with a defined value. You'll have to use c++ to get typed enums, or possible some non-standard extension.
What do you mean #define can only do "simple" constants?
&gt;OK. So your "troublesome" input file. How many lines does it have in it? &gt; &gt;Don't forget that an empty line is still a line. 32 lines. Wow I guess my approach was completely wrong. &gt;How would it do that? Your code doesn't even *have* any error messages in it. Where would it get an error message from? Here: if(!match){ printf("error"); return 0; } &amp;#x200B;
Many programs that invoke undefined behaviour produce different output under Valgrind, since the environment provided by the synthetic CPU provided by Valgrind is not the same as that when the code is run directly.
I am pretty confused. If the input file has for example 12 lines. error get's printed out. 
What would the contents of `reds[12]` be if the file only had 12 lines? `fgets` isn't going to fill the `malloc`ed string, so the contents are undefined. The fact that you're getting an error message and not simply crashing is nothing short of miraculous. 
&gt; Here: Yeah, having an error message in _a completely different part of the code_ isn't really the right way to validate your input.
I think all the typedef above (in C) does is let you refer to it as "NAME" instead of "enum NAME". In C++ the typedef doesn't provide anything and by default does minimal checking.
Have you used any other similar checker? I just lately learned about *scan-build* (thanks to people here) and using it against programs that had been trouble free for years... Whoops, better fix that.
Okay that makes a lot of sense. I think it must have something to do with this error then... ==13193== Conditional jump or move depends on uninitialised value(s) ==13193== at 0x4F6572A: PKCS5_PBKDF2_HMAC (in /lib/x86_64-linux-gnu/libcrypto.so.1.0.0) ==13193== by 0x407E2B: yaxaKDF (passmanager-1.23.0.c:2703) ==13193== by 0x40691A: openEnvelope (passmanager-1.23.0..c:2220) ==13193== by 0x402B17: main (passmanager-1.23.0..c:641) I use the \`PKCS5\_PBKDF2\_HMAC()\` function to generate the key OpenSSL's HMAC function uses, so that would make sense such an error would effect that. I'm just a little mystified why it's not a problem for any of my other functions that utilize it. I am guessing there must just be some small difference that makes the undefined behavior of the uninitialized value work in the other functions, but not in the one that prints my file--but that works for all functions outside of valgrind. I cannot figure out for the life of me what value it is claiming is uninitialized. Off to gdb I go. Here's the \`yaxaKDF()\` function I'm using \`PKCS5\_PBKDF2\_HMAC()\` in. It derives a nonce and key for a counter-based stream cipher. Just to learn with; I'm becoming intimately familiar with Schneier's law. Anyway maybe someone else can spot some way I'm using it erroneously while I debug the input values. /*Derive a cryptographically secure key from the supplied database password*/ void yaxaKDF() { int i; unsigned char bigSalt[16]; for (i = 0; i &lt; 8; i++) bigSalt[i] = yaxaSalt[i]; for (i = 8; i &lt; 16; i++) bigSalt[i] = yaxaSalt[i - 8] + yaxaSalt[i - 8]; originalPassLength = strlen(dbPass); /*Generate yaxa nonce*/ /*Must generate +1 byte for iterators to reach 64th element*/ /*Must be able to access yaxaNonce[64] because iterating only to 63 results in periodic keystream after only 262400 bytes*/ PKCS5_PBKDF2_HMAC(dbPass, -1, bigSalt, 16, keyIterationFactor * originalPassLength, EVP_get_digestbyname("sha512"), 65, yaxaNonce); //PKCS5_PBKDF2_HMAC(dbPass, -1, bigSalt, 16, keyIterationFactor * originalPassLength, EVP_get_digestbyname("sha512"), 1, &amp;yaxaNonce[64]//); /*Generate yaxa key*/ for (i = 0; i &lt; 16; i++) { PKCS5_PBKDF2_HMAC(dbPass, -1, bigSalt, 16, keyIterationFactor * originalPassLength++ + bigSalt[i], EVP_get_digestbyname("sha512"), SHA512_DIGEST_LENGTH, yaxaKeyChunk); memcpy(yaxaKeyArray[i], yaxaKeyChunk, 64); } memcpy(yaxaKey, yaxaKeyArray, BUFFER_SIZES * 2); /*Generate a 1025th byte for yaxaKey, because the iterators need to be able to read yaxaKey[1024]*/ /*If the iterators only reach yaxaKey[1023] the keystream generated in yaxa() will become periodic after only 262400 bytes*/ /*If the iterators reach yaxaKey[1024] without this byte, it is always 0, but technically undefined behavior*/ /*Going to generate 1 extra byte to prevent undefined behavior and predictability of that final byte*/ PKCS5_PBKDF2_HMAC(dbPass, -1, bigSalt, 16, keyIterationFactor * originalPassLength++ + bigSalt[i], EVP_get_digestbyname("sha512"), 1, &amp;yaxaKey[1024]); } &amp;#x200B;
&gt; Okay that makes a lot of sense. I think it must have something to do with this error then... The way you worded your post, you made it sound like Valgrind wasn't actually reporting any error, that you program was just producing the wrong output. But if Valgrind reports an error, then yeah, it's a good indication that your code is wrong.
I'm somewhat suspicious of this: "Must generate +1 byte for iterators to reach 64th element" "Must be able to access yaxaNonce\[64\] because iterating only to 63 results in periodic keystream after only 262400 bytes" &amp;#x200B; Zero to 63 is 64 elements. What is the size of yaxaNonce?
I guess I was assuming that was a false-positve or merely a warning, since all the other program functions which use the HMAC function were working successfully despite it. You ought to install the debugging symbols and source for OpenSSL. That will let Valgrind give you the line number in the OpenSSL code where this use of uninitialized value is occurring. From there you can work backwards to your code to work out why the value is uninitialized. Oh that sounds like it would be super helpful, I'm not sure how to accomplish that though. I'm on Xubuntu 16.04, can I install that from packages in apt? This is the first time I've implemented a software library like OpenSSL.
Nope, I only became familiar with valgrind after learning about gdb. I was having some "Core aborted" messages that I couldn't deduce the source of with "where" like I could with gdb, and googled the issue (heap corruption). That's where I learned that valgrind can detect and report issues that would cause this, and that it was also good for finding memory leaks... 'Whoops, better fix that', indeed.
I'm not really sure. I don't do Ubuntu. I'm going to _guess_ that it's something like `apt install openssl-dbg`. You have to hope that the package maintainer actually bothered creating a debug symbols package though.
Yeah, which is why I do it. Good thing we're in /r/C_Programming and not /r/Cpp
Sorry, unclear comments. yaxaNonce is allocated to 65 bytes, so that this code can reference yaxaNonce\[64\] without undefined behavior... /*Encrypt file and write it out*/ for (i = 0; i &lt; fileSize; i++) { fputc(yaxa(fgetc(inFile), yaxaKey[ii], yaxaNonce[n]), outFile); /*yaxaKey and yaxaNonce are allocated to 1025 and 65 bytes respectively*/ /*See yaxaKDF() for details*/ if(ii &lt; 1024) ii++; else if(ii == 1024) ii=0; if(n &lt; 64) n++; else if(n == 64) n=0; } yaxaKey and yaxaNonce were formerly allocated to 1024 and 64 bytes respectively, and this code would then reference yaxaKey\[1024\] and yaxaNonce\[64\], causing undefined behavior. When I changed the code to limit the highest elements indexed to be yaxaKey\[1023\] and yaxaNonce\[63\], it ended up imparting a period in my resulting keystream ( which I was intending to be non-periodic ). So I wanted to change it back, but avoid the undefined behavior, so I simply allocated yaxaKey and yaxaNonce each one more byte, and derived one more byte for each as well. I was in error thinking this prevented the period--it really only changed it to a 3411200 byte period. I know, pretty clunky and confusing. I'm going to scrap this ultimately, but I figured trying to keep this all in tact and get the program I implemented it in "polished up" would be good practice.
Why not build your debug versions with UBSan, TSan, etc?
Clang-Format is a far better idea.
Okay, looking at the pieces and parts that you're showing, I have one other question -- do you get the same valgrind error when you limit the key to 64 bytes instead of 65 bytes? I'm wondering if something isn't setting that final byte, and that's what valgrind is noticing in the library.
I'm doing the same for my format string handling... Why no dependencies? because no dependencies...
The problem with the abs function is that by definition it will only ever return an unsigned integer, yet the standard library has it set up to return a signed interger, which really messes up with type checking.
gdb output of the "read" function that valgrind fails the HMAC on, watching the values used in \`PKCS5\_PBKDF2\_HMAC()\`. That value for \`EVP\_get\_digestbyname("sha512")\` looks interesting. Starting program: passmanager -r allpasses -f ./examplepasswords1.dat Enter database password: Breakpoint 1, yaxaKDF () at ./passmanager-1.23.0.c:2683 2683 /*Must generate +1 byte for iterators to reach 64th element*/ 1: dbPass = (unsigned char *) 0x60e300 "pass" 2: originalPassLength = 4 3: bigSalt = "\251^b|py\000NR\274\304\370\340\362\000\234" 4: yaxaSalt = (unsigned char *) 0x60f480 "\251^b|py" 5: evpSalt = (unsigned char *) 0x60f4a0 "\210SCjX\030\342|" 6: keyIterationFactor = 1000 7: EVP_get_digestbyname("sha512") = -136573472 (gdb) c Continuing. sagacitys.com : uncover : q+v}?9,vV96_ve5@ shrivelled.com : preserve : MZ8U2fBd1GUL8jvh executives.com : matriculated : pass sleazinesss.com : shameful : +(t{}{:4#Fte3KA9 synchs.com : misjudgments : r3;pB"%vTsg|}c"6 snails.com : reenact : 2EhY]j.f!c%&lt;nwL+ missions.com : lenoras : c&amp;heD|NWEhvsQ2Ke apples : 5\-WLPXmPbrV1O[2 apples : )|MZI:o^JsWkge() [Inferior 1 (process 29869) exited normally] (gdb) quit Via OpenSSL man pages [https://www.openssl.org/docs/man1.1.0/crypto/EVP\_DigestInit.html](https://www.openssl.org/docs/man1.1.0/crypto/EVP_DigestInit.html) const EVP_MD *EVP_get_digestbyname(const char *name); const EVP_MD *EVP_get_digestbynid(int type); EVP_get_digestbyname(), EVP_get_digestbynid() and EVP_get_digestbyobj() return an EVP_MD structure when passed a digest name, a digest NID or an ASN1_OBJECT structure respectively. &amp;#x200B;
&gt; without any word wrap ???? you're just hardcoding the word wrapping instead of letting the IDE take care of it...
You always have dependencies. Not using other stable libraries just means you have a good chance of finding all the errors they already did.
I'll Google those, but to answer your question, I'm too new at this to know what they are :/
argc and argv are arguments to the program. like inputs and outputs for it to manipulate. argc is the number of arguments (ARGument Count) argv is a 2d array of arguments as strings (ARGument Vector) 
Looks like your suspicions were right-on. I changed the program back to my previous configuration (1024 byte key, 64 byte nonce, index maxed to 1023 and 64 respectively) and it now runs just dandy in valgrind, no more warning about uninitialized values either. Interesting, I thought I was fixing the undefined behavior, just added more. This cipher seems hopeless though. In this configuration it will induce a 1024 byte period in the keystream :/
Are you rolling your own crypto algorithm? Unless for educational purposes, I would advise stepping away slowly.
Yep just educational. I'm going to branch off the program and strip the cipher out before I make any actual use of it, but it's been really useful for learning cryptographic terminology and learning the nomenclature. Not to mention gaining more programming skills. Now I know how to use valgrind! I didn't even know about gdb before writing this. Flying by the seams of my pants.
No blog spam please.
Thanks [which\_spartacus](https://www.reddit.com/user/which_spartacus) :) kenny@AMD:~/passmanager/devtools$ valgrind --leak-check=yes ./passmanager -r allpasses -f ./examplepasswords1.dat -x password ==23542== Memcheck, a memory error detector ==23542== Copyright (C) 2002-2015, and GNU GPL'd, by Julian Seward et al. ==23542== Using Valgrind-3.11.0 and LibVEX; rerun with -h for copyright info ==23542== Command: ./passmanager -r allpasses -f ./examplepasswords1.dat -x password ==23542== marzipan.com : illustrated : `YYDpkc;(cPNDl;| addresses.com : supremacists : Q2jI*+&lt;XmIUs&lt;cj3 interments.com : wilts : CE4f`~OxIgWzTlNm overworks.com : lampreys : ILa?$d!:J02hW`Kc workaholics.com : fannys : y4`c?6?4%vimec11 hostilities.com : washtub : &amp;v-q!P.oW&gt;z$`*e: noxzema.com : severances : Xv35}4SuFSU6bq&amp;' divisibilitys.com : noontimes : B*%v~mu,+mZ[}CjQ compassed.com : uncivilized : &amp;L~D^%GRWAsvGhf' watts.com : diplomacy : -6Vc@&gt;3@Ecp_dc45 ==23542== ==23542== HEAP SUMMARY: ==23542== in use at exit: 25,759 bytes in 553 blocks ==23542== total heap usage: 985,825 allocs, 985,272 frees, 198,322,540 bytes allocated ==23542== ==23542== LEAK SUMMARY: ==23542== definitely lost: 0 bytes in 0 blocks ==23542== indirectly lost: 0 bytes in 0 blocks ==23542== possibly lost: 0 bytes in 0 blocks ==23542== still reachable: 25,759 bytes in 553 blocks ==23542== suppressed: 0 bytes in 0 blocks ==23542== Reachable blocks (those to which a pointer was found) are not shown. ==23542== To see them, rerun with: --leak-check=full --show-leak-kinds=all ==23542== ==23542== For counts of detected and suppressed errors, rerun with: -v ==23542== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0) &amp;#x200B;
Duly noted. That makes perfect sense too.
Valgrind changes timings, a lot, so a threading issue can show up, or not, when run with Valgrind or not. I take you are using Valgrind for memcheck, is there anything reported? Though timing stuff will of course will generate nothing, bar timeouts. Even stuff that is all correct can fail when done through Valgrind due to slowness and time outs.
So you mean No link post??? Should I delete it ?
Your website is a blog spam site. I do not want this kind of content on this subreddit. Please don't post anything about it.
No blog spam please.
Ok. I will delete it. 
In his description he links to his configuration/website which leads to his dotfiles on github [where you can examine his configuration](https://github.com/LukeSmithxyz/voidrice/blob/master/.vimrc).
&lt;p&gt; I looked at the dmenu source code a few days ago and it uses the x11 to make simple guis, more info about it is in&lt;/p&gt; &lt;a href="https://en.wikipedia.org/wiki/X_Window_System"&gt;wikipedia&lt;/a&gt; &lt;p&gt;. https://en.wikipedia.org/wiki/X_Window_System#User_interfaces may also good to read. &lt;/p&gt;
UWP on Xbox one and also android :) 
Basically the debug symbols packages are in an another repository. For Ubuntu: https://wiki.ubuntu.com/Debug%20Symbol%20Packages 
Luke has tons of videos about how he sets his stuff up. Probably at least 5 on vim alone. He also leaves his dotfiles in the description of every video.
i think the square part is just reffering to the final product being 100% different than how it started.
Nested loops (;
Nested loops each running n times per input. The inside loop will scale the columns, outside scales the rows.
I get that it needs nested loops, I don't get how I'm supposed to read XYZ and write XXYYZZ on two seperate lines. Right now, I read the current pixel, then enter a loop that writes it n times, and that completes the first row, but now my code needs to know.. Hey, read the input file from line 1 again. The for loop increments i/j, so it starts reading the second row of pixels from the input. I don't know what a good way to handle that is.. 
I get that much, and can do nested loops. The thing I don't understand is how to handle reading the input, I guess. I use a for loop for height, and inside of that, a for loop for width, using i and j, but the thing is, I currently read a pixel, then write that pixel n times until the line is over. After that process is done... The for loop incremented, so my read is reading the second line of the input, which is not what I want. I don't know how to handle reading each line n times. My brain might just be too melted to think about this for right now. Trying to wrap my head around all of the code surrounding this problem alone was very taxing to a new programmer. 
You'll have two choices: 1) Remember the position within a file where a line starts with [ftell](https://pubs.opengroup.org/onlinepubs/009696799/functions/ftell.html), transform the one line, and then if you need to make more vertical copies, go back to where the line start with [fseeko](https://pubs.opengroup.org/onlinepubs/009696799/functions/fseek.html). One problem is that wouldn't work if the stream isn't seekable (such as when it's from a pipeline). 2) Buffer the entire line into memory. Once you do that it's easy to write it out multiple times. To handle this properly, you'll need to understand dynamic memory ([malloc](https://pubs.opengroup.org/onlinepubs/009696799/functions/malloc.html), [realloc](https://pubs.opengroup.org/onlinepubs/009696799/functions/realloc.html) and [free](https://pubs.opengroup.org/onlinepubs/009696799/functions/free.html)] to handle being able to buffer the entire line. If that feels too much you can create a static buffer with the maximum size a line can be although choosing the maximum can be hard. 
Thank you so much for giving me a more informative answer, this helps me a lot, actually. Would you mind giving me a basic breakdown of what being from a pipeline means? Thankfully, this task is laid out as a project, and it's simply a file in the root dir of the project, so I doubt that will be an issue, but still curious. So, from your explanation, I kind of have two ideas in mind now, and would appreciate if you could tell me which would be.. optimal. So, I'm dealing with BNP 24 files, so I'm reading 3 bytes per pixel manually, and checking if there's a buffer, just to give you some more basic information. Do you think I should parse my line into a new malloc'd location (not sure how I would store this actually, but I suppose a hex), or would it be best to somehow create a temp variable that stores how many times I've done the loop, and repeat the loop until temp is n. I guess I would handle a temp variable by simply increasing temp when I'm done with a line, and if temp does not == n, then repeat the code. Not sure how I'd make temp 0 again, however, not sure if setting temp to 0 again right after seeing it was == n, and then using break would work. I am still trying to understand some basic things. This is me learning C through Harvards online course.. and this is what they're making me do on my third week of learning, which feels maybe a tad cruel. I now understand a lot about bits, bytes, and padding, and handling that, as well as file pointers, etc, but this is so complex to me right now!
my guess is that you don't have the input image in memory and so can't access it randomly (arbitrarily), but are instead reading it from a file. so do that first (read it into an array). then nested loops are trivial.
There are (at least) two ways to do this: 1) seek backwards in the file so that when you read it again from the file you get the same data. This is inefficient because you’re reading the same data from disk multiple times, but it’s probably just one extra line of code. 2) read the data to a buffer inside your program (either allocated on the stack or heap) and then your loop can repeatedly use that instead of reading straight from the file each time.
The Linux kernel is in large parts very poorly written. The GNU source code always reads rather clunky. I can highly suggest the source code of FreeBSD, OpenBSD and Solaris. Though the nicest source code I've ever read is that of TeX.
A define macro is a blind find-and-replace operation and can lead to apparent order of operations issues if you’re not careful. Definitely a good idea to add a liberal amount of parenthesis. Yes, it would be evaluated at compile time but only after the substitution. 
The #define statements are just instructions for the processor (like any other macro), their behavior is equivalent to copying and pasting the corresponding text where they find the macro, so they are not evaluated at compile time. If you do: ``` #define SUM 10 + 20 ``` then ``` SUM/3 ``` will result in: ``` 10 + 20/3 = 16 ``` instead of the 10 that you would expect.
the umbrella term is sanitizers, they're mostly a Clang thing.
`define` is just a find and replace. so your function expands to `printf("%d/n, SCREEN_WIDTH / TITLE_SIZE + 1 * TITLE_SIZE")` which causes an error in the order of operations since it will evaluate `1 * TITLE_SIZE` before and addition. 
I'm interested to know is there any available source for Solaris? 
OK TeX and BSD's source code! Thanks!
Yeah, no. also the FreeBSD code is written badly. the variable names are complete garbage, they're still use C90 style "initialize all variables at the top of the function including loop iterators", just nahh.
When the macros are expanded, you end of with something like this: TILE_SIZE * MAP_WIDTH TILE_SIZE * SCREEN_WIDTH / TILE_SIZE + 1 16 * 240 / 16 + 1 240 + 1 = 241 MAP_WIDTH * TILE_SIZE SCREEN_WIDTH / TILE_SIZE + 1 * TILE_SIZE 240 / 16 + 1 * 16 15 + 16 = 31 Notice how the equation changes due to order of operations. By parenthesizing MAP_WIDTH, you ensure that you maintian the proper order of evaluation. It's almost always a good idea to encapsulate any macro longer than a single statement in parenthesis.
The last true OpenSolaris release was build 130. Development has been continued in Illumos. Check that out!
So, I should abandon fread, and take the loss of assigning all of the relevant data to a new array? I don't know much in the way of best practices right now, but I think I could definitely manage doing so, I just worry about how much memory that could mean using, if I took in a huge input file. With the current file, it would only be 36 bytes, I believe, so it's not a huge loss, but if I wanted to read a 1080p file with 3 bytes of data per pixel, wouldn't that make this less optimal? I guess it would still be easily small enough for any RAM i'd realistically run this on, but just curious if this is a good idea to do when an input is larger. 
This is a perfect example of why you should always wrap defines of compound expressions in parentheses. TLDR: The substitution is textual (copy-paste) and so the compiler sees the order of operations on the operators as it would have been typed in. The multiply of \`TILE\_SIZE\` by \`MAP\_WIDTH\` is multiplying the value of \`SCREEN\_WIDTH\`. Lets look at this simple example: #include &lt;stdio.h&gt; #define FOO 1+2 #define BAR (1+2) int main(){ int a = FOO * 2; int b = BAR * 2; printf("a=%d, b=%d\n",a,b); } This prints: \`a=5, b=6\` Why? Lets look at this with \`gcc -E\`. With -E we can see the effect of the preprocessor: /* I stripped out all the stdio.h junk*/ int main(){ int a = 1+2 * 2; // FOO int b = (1+2) * 2; // BAR } Here we can see what the compiler sees. Our substitutions are "correct", but not what we intended. We intended the expression \`(1+2)\*2\`.I'm using parenthesis here to emphasize order of operations. This works with b, because our substitution of BAR is literally \`(1+2)\` so we get literally \`(1+2)\* 2\`. But with FOO, we get \`1 + (2\*2)\`, because the compiler sees the string "1+2\*2" and correctly interprets the order of the operator evaluation to be \`1+(2\*2)\`. &amp;#x200B; In your case: printf("\n %d", TILE_SIZE * MAP_WIDTH); printf("\n %d", ((16 * 240)/16) + 1 ); printf("\n %d", MAP_WIDTH * TILE_SIZE); printf("\n %d", (240/16) + (1 * 16) ); If you place parenthesis
"take the loss". wtf are you doing? is this some hyper-optimized competition?
So, since this is only my third week into programming, I'm kind of new to some fundamentals. When I define a variable of a set size, it's in the stack, and if I define a pointer, that pointer is pointing to the heap, right? I just want to make sure I understand how that works, so I can understand your comment properly. Thank you so much for your time being given here as well. 
Thank you!
It's not pure C, more like C++ lite, but the DOOM 3 source code is pretty stellar in terms of readability and cleanliness. Source code: [https://github.com/id-Software/DOOM-3](https://github.com/id-Software/DOOM-3) A great review and walkthrough of said code: [http://fabiensanglard.net/doom3/index.php](http://fabiensanglard.net/doom3/index.php) And I highly recommend looking at the rest of Fabien Sanglard's stuff on his website. He did walkthroughs of the original C DOOM source code, Quake, and others. 
Thanks alot!
I've found Postgres to be well-written and readable.
Raylib is very neat, make sure to check out the subreddit and read documentation. The author replies to every post!
Aw man, I like the C90 style
https://kotaku.com/5975610/the-exceptional-beauty-of-doom-3s-source-code
 for ( row loop [i] ) { read line to pixel_array for ( n times ) { for ( column loop [j] ) { for ( n times ) { print pixel_array[j] } // each pixel repeated n times } // end of row print newline } // row repeated n times }
Anything written by me.
Ooof.
Yeah, me too. Declaring variables all over the place is super annoying because it's very hard to find the declarations. 
SQLite, too.
Have you read either of his "Game Engine Black Books" for [Doom](https://www.amazon.com/Game-Engine-Black-Book-Doom/dp/1987418433/ref=pd_sim_14_1?_encoding=UTF8&amp;pd_rd_i=1987418433&amp;pd_rd_r=1adcfd56-0619-11e9-90c0-f9307a5a46b8&amp;pd_rd_w=RJAq7&amp;pd_rd_wg=a9YDN&amp;pf_rd_p=18bb0b78-4200-49b9-ac91-f141d61a1780&amp;pf_rd_r=CY47NDZQNKAXNGV39GB8&amp;psc=1&amp;refRID=CY47NDZQNKAXNGV39GB8) or [Wolfenstein 3D](https://www.amazon.com/Game-Engine-Black-Book-Wolfenstein/dp/1727646703/ref=sr_1_3?s=books&amp;ie=UTF8&amp;qid=1545504038&amp;sr=1-3&amp;keywords=wolfenstein) yet? I just stumbled across those yesterday, and was thinking of picking up the Doom one.
Wow, source code walkthroughs... never knew or thought about these. Thank you. I'll google but do you have any others that are good to pay attention to?
Thank you, this is genuinely very helpful, and informative to me, and you're the only person that actually helped me by providing a pseudo-code example, as asked for. Reading this has helped me a lot, and I will be sure to actually analyze it, and learn from it.
I'll only define one variable at the top, and that's the return variable. also, I'll put iterators for do/while loops above them, otherwise, where they're used.
I've read good assessments of the Plan 9 code quality, but I assume that the style is pretty outdated by now.
There are a lot of interesting ideas and well done things, but if I remember correctly there are a bunch of little quirks about their chosen styles I didn't particularly like.
Oh yeah, Plan 9 is really nice to read. However, they actually use their own dialect of C with their own standard library, so the entire code base feels a little alien to the casual reader.
What is the reason the Linux kernel (still) is poor? Backwards compatibility? Technical challenges because a libc-implementation cannot depend on itself? 
I thought TeX was written in some Pascal dialect.
The code quality is poor because all developers to their own thing and write in their own style. While the worst code is usually rejected, many terrible things get through as people are not willing to reject code for being crappy as long as its correct. The FreeBSD sources on the other hand all follow the same design rules and documentation standards, so once you understood the rules, it's easy to find your way through the code base.
TeX is written in literate Pascal; i.e. it's Pascal code embedded into a TeX document. It reads like an essay that can also be fed to a compiler. You can buy it as a bound book. Knuth restricts himself to a fairly sparse set of Pascal features, all of which are also available in C. He doesn't use dynamic memory allocation and doesn't even use pointers. 
Even more PVS Studio spam.
Redis is known for its [excellent code quality](https://news.ycombinator.com/item?id=6552680).
“I need help constructing pseudo-code” != “I want you to write out pseudo-code”. In fact it sort of implies the opposite, that you want “help” but not actual code. Ask for what you actually want. This: #define BUFFER_SIZE 4096 ... char my_buffer[BUFFER_SIZE]; would allocate a 4KiB buffer on the stack, and `my_buffer` would be a pointer to the beginning of that memory area. This: #define BUFFER_SIZE 4096 ... char* my_buffer; my_buffer = malloc(BUFFER_SIZE); would (attempt to) allocate a 4KiB buffer from the heap, and `my_buffer` would be a pointer to the beginning of that memory area. If you do you should check that the buffer is not NULL and call `free()` on the buffer later when you’re done with it. In a simple utility program you could just exit from main without explicitly freeing it, and it will get cleaned up along with the rest of your process’ memory, but in general you don’t want to “leak” memory like that. This: #define BUFFER_SIZE 4096 ... static char my_buffer[4096]; Would allocate a 4KiB buffer “somewhere” (most likely in the data segment of your executable) that exists from program startup, but not on the stack or heap per se. This can also be done outside of a function, creating a globally scoped static buffer.
It looks awesome and it actually works nicely (on Notepad++), but I just don't get it to run on VS Code aka I can't get it to compile through cmd.
Came here to say this. It's some of the prettiest C I've ever seen.
Yeah, this. The FreeBSD kernel source is much cleaner and better engineered, imo. There are tons of portions of the Linux kernel that are great examples of C done right; but the hodgepodge of some portions brings it down a bit.
The linux kernel has guidelines: https://www.kernel.org/doc/html/v4.10/process/coding-style.html Well written would probably most consist of just bring consistent and documenting your code
The OpenBSD project places emphasis on code correctness and simplicity. Reading through any of their [code](https://github.com/openbsd/src) would be a good starting point - you can start with simple utils (cat, netcat, etc.) and work your way up to the harder stuff. They have rewritten pretty much everything at some point, and all code has to be reviewed and OK'd by a peer which makes it harder for crap code to sneak in.
&gt;the entire code base feels a little alien to the casual reader. Well, Plan 9 *is* from outer space...
&gt;The code quality is poor because all developers do their own thing and write in their own style. I thought this is one of the things Linus was famous for being a tyrant for. 
I haven't been around Postgres' source for a while but it used to have a block based memory allocator/manager that was really fun to read and play with.
Definitely try to add support for CTest and possibly Install-RPATH support. I can link some examples tomorrow since I’m on mobile now
See also: r/osdev
I haven't been that deep in its guts. I was looking at some of its query optimization bits and remember being pleasantly surprised with its readability.
Waiting for Quake, myself.
OpenBSD is always my goto (pun intended) for how to write good C code. So much easier to understand than the GNU equivalents. 
You're like a vegan at Subway who gets a meatball sandwich -- wrong sub. :) The big thing to check is where all of your libraries have been installed, however. 
Anything from Fabien Sanglard is gold in my opinion. I haven't found anyone else that is as detailed and good as him though.
Name checks out.
I wouldn't call this comment well written.
Wrong sub. But simply explained: you need to update your settings Json file with the correct path to the header files you are using. 
I would call your mom well ridden.
Undefined behavior doesn't have to make sense.
What if I'm asked to write the output of the program?
Running that program made my computer catch on fire. It's undefined behavior; there is no correct answer for what it outputs.
Does not make any sense to return the memory address of a variable stored in the stack either.
You're*
It's, "your" dumbass. You're = you are. God you are stupid.
If God was so stupid how did he make the Earth?
What about programs by https://suckless.org/ ? I use dwm, st and dmenu and I enjoy the simplicity of the code and their style guide.
The authors of the Standard never intended it to fully specify everything an implementation must do to be suitable for any particular purpose (or even--as evidenced by the One Program Rule, any useful purpose whatsoever). They didn't think anyone would care about the difference between an non-conforming implementation that is of such poor quality as to be useless, and one which is equally useless but happens to be conforming. The Standard makes no attempt to specify what is necessary for something to be a quality implementation that is particularly suitable for a particular task, because it is obvious: A compiler that allows a programmer to readily accomplish a given task is a quality implementation suitable for such task; one that makes it more difficult or impossible to accomplish the task isn't. I am well aware that compilers which are only suitable for the narrow purpose of performing certain kinds of processing input from non-malicious sources perform optimizations which make them unsuitable for many other purposes. I'm also aware that the authors of such compilers believe that the Standard not only entitles them to produce specialized implementations which are only suitable for a narrow range of purposes, but also to regard conformance to the Standard as justifying a belief that their compiler is suitable for all purposes. It would be literally impossible to write *any* strictly-conforming source text whose behavior would be defined on all implementations. Indeed, it would be trivial to construct a pair of implementations such that at least one of them would respond to any Strictly Conforming program by outputting "Hello Fred!" and terminating with an exit code of 42. For example, there is no required minimum level of nesting for functions that contain an even number of automatic objects of type `int`. So an implementation where that number was zero could be conforming if there was at least one program it could process which exercised the Standard's translation limits (and where every function had an odd number of `int` objects). Likewise, an implementation that couldn't handle any functions which contain an odd number of `int` objects could be conforming if it could process at least one program that exercised the Standard's translation limits. Since the number of `int` objects within `main` must be even or odd, at least one of those implementations would be allowed to process any strictly-conforming source text in any fashion whatsoever. Anyone competent person making a bona fide effort to write a quality implementation should read the rationale and abide the principles therein *without regard for whether the Standard would require them to do so*. Only an incompetent person or one seeking to produce a poor-quality compiler would use the fact that the Standard doesn't require them to behave in useful fashion as justification, in and of itself, for their failure to do so. I see no good reason for the authors of the Standard to repeatedly include the Spirit of C in the charter and rationale documents, but not bother to mention it within the text of the Standard itself. On the other hand, there's not really way way the Standard could be written in such a way as to preclude the possibility of useless-but-conforming implementations. Compiler writers who want to make their implementations suitable for various tasks will do so whether or not the Standard requires them to do so, and those who don't care about making their implementations suitable for various tasks could find ways of making them useless no matter what the Standard tried to say. 
Plan 9 had some interesting concepts, but it made a number of needless breaking changes which sunk the whole thing. The time is long overdue for a documented offshoot of C in which most existing C source will run without changes, some will generate compiler errors but can be easily adapted to run identically in both languages, any only a very tiny fraction will silently behave differently, or would need to be written differently in both languages. Consider, for example, an expression like `uint16a - uint16b &gt; uint16c;` In C, the behavior of that code would be required to vary depending upon the size of `int`, but a good language should not have behavior which is required to silently change between implementations in that fashion. Consequently, if I were writing an offshoot of C, I would include a directive to specify how code like the above should be processed, with the recommended default choice being to regard it as a compile-time error. If a signed comparison is required, `(int32_t)uint16a - (int32_t)uint16b &gt; uint16c` or--if the values are small--`(int)uint16a - (int)uint16b &gt; (int)uint16c`. If unsigned wraparound is required, (uint16_t)(uint16a - uint16b) &gt; uint16c`. In the absence of a cast, neither the compiler *nor any human who sees the code* would have any way of knowing what behavior was intended if they didn't know about its intended platform or purpose. Having a compiler reject an expression like the `uint16a - uint16b &gt; uint16c` would be a slight breaking change, but it could easily be fixed by adding one or more casts that would make the intention of the code clearer, and would thus represent improvements. Plan 9, however, imposed breaking changes that were far more serious and could not so readily allow code to be compatible with both Plan 9 and normal C. 
You say: "the output is potentially unpredictable/unreliable because `y` references a value that is beyond the stack pointer". Where did you get this from? Do you have a prof that gave this as an example?
\&gt;I'm now expecting linux kernel source code itself and GNU coreutils are answers &amp;#x200B; These are probably bad examples. Better code can be found in the musl and plan9 trees.
&gt; musl and plan9 trees. OK! Thanks! :)
I don't think that C is an high-level language.
Wait, the DOOM 3 source code is open? This is an amazing discovery for me. *Here go I diggin'...*
OMG, that is awful.
I'm still chuckling from the initial try. It walks you through your source, in a browser window, showing the detailed program steps that lead to the detected problem. All practically without RTFM. If there are even better options, how sweet is that.
Hello world :D
shure it is, you must write more assembly! she so high she token'isin
&gt;All popular cross-platform programming languages and scripting languages, such as **C++, Java**, **Python**, Perl, Ruby, PHP, Lua, and Bash, are implemented in C I'd have to double check, but I know Java is written mainly in C++, C++ is probably self hosting these days (surely) , I seem to remember something about perl moving to C++ python, php and probably lua fair enough...
Well, no. It means you'll find a lot of bugs, but they won't be eploitable when big libraries are found to have severe bugs, you'll be fine. or maybe you'll find exploitable bugs in yours, but since your target is so much smaller, you're less likely to be hacked. as for dependencies, you're right. I should've said external dependencies.
why are you saying it's spam
Is this for a 'programming languages' class where you are working through how stack frames work? Not a horrible assignment in that case -- make the frames in a fairly arbitrary way, and since the two functions are similar, you should have a clue into what the memory layout is doing. 
What about a classic read? https://en.wikipedia.org/wiki/Lions%27_Commentary_on_UNIX_6th_Edition,_with_Source_Code Old mean small. It's a good read. I loved it. Busybox is a better read than the core utils. It chooses size over features which keeps it simple. Core utils is all about features so is bigger and less clean. Linux is a mixed bag of good and bad. Again, real world feature rich code is hard to all be good. If you know Window's Win32, WINE is a good read. Not all clean, but how could it be and do what it does? SQLite is good too, again, small and clean over features.
The quality of your article is very low and your site has an unhealthy amount of advertisements. Many things in your article are factually or conceptually wrong and the grammar and layout is just atrocious. Nobody is going to learn anything from this article. All people who read your article are learning wrong things from it. Wrong preconceptions people in this subreddit later have to clear up. That's really shitty. Don't start a series of blog posts on programming in C unless you know the language in and out (which you clearly don't) and have enough experience in web design to make reading your articles a pleasurable affair. You should also have some command of the English language so your spelling and grammar is correct. Right now, the grammar in your article is a hot mess and an insult to the reader.
Do you link to libc? Do you call any OS functions, like read and write? If your program is minimal, sure, write it all. But the minute you do anything close to complicated, why would you slow yourself down by not using a good library that already exists? 
I'm very close to removing it, but on the other hand the site is ad-free, so the definition of “blog spam” I set for myself is not fulfilled.
Lion's commentary is pretty nice but one has to keep in mind that many design patterns used in 6th edition UNIX are rather archaic these days.
gcc I have, made g() like this: g: pushl %ebp movl %esp, %ebp subl $16, %esp movl 8(%ebp), %eax movl %eax, -4(%ebp) movl $0, %eax # NULL leave ret Compilers can have sense of humor.
How so Solaris’ source code?
OK, I try to improve it. Can you do a favor of me. Can you elaborate mistakes in my article. Whatever the mistake I did, Please mention that line with remark. I have a Knowledge of C Language, I am an Assistant Professor in a University. Well, Thanks for your advice. &amp;#x200B;
Irreparable harm to the reader should override all other concerns, ;-)
&gt; C++ is probably self hosting these days (surely) At least clang is definitely written in C++.
&gt; Though the nicest source code I've ever read is that of TeX. Literate programming in Pascal, right?
SQLite: [https://github.com/mackyle/sqlite](https://github.com/mackyle/sqlite). Pay attention to how they deal with platform-specific eccentricities. They also have a detailed memory-allocation policy. XV6: [https://github.com/mit-pdos/xv6-public](https://github.com/mit-pdos/xv6-public). Annotated like 6th edition Unix, but rewritten for educational purposes to teach x86. Avoids the overuse of macro preprocessing by relegating any magic to code-generation at build-time.
Yes. See my other comment on this matter.
Yes, but it's what people build on and evolve from. So it's useful archaeology.
Why one needs to post this here and tell a bunch of C programmers why we need to learn C is just ...
r/codereview?
So first of all, get rid of Turbo C. That tool is about 30 years old and obsolete in every way. Your post seemingly tries to describe ANSI C (but does so incorrectly) which is obsolete as well. The current standard is ISO 9899:2018 also known as C18. You can find the [latest draft](http://www.open-std.org/jtc1/sc22/wg14/www/abq/c17_updated_proposed_fdis.pdf) of this standard on the internet. You should read it. Unless you are familiar with the content of this document, you have no business teaching C. You also need to improve your grammar. It is terrible. Here are some factually and structurally wrong things I saw on my first pass through the article. All references to sections and paragraphs refer to ISO 9899:2018: * your article talks about *tokens* but fails to mention that there are actually two different stages of tokenization: the C compiler first splits the source code into *preprocessing tokens* which are then preprocessed. After preprocessing, preprocessing tokens are converted into *tokens* which are then parsed. It is important to make this distinction because it explains many of this quirks the preprocessor has. See §6.4 for details. * your article completely fails to explain what a token is. There is no example for splitting source code into tokens. You say “In the C programming language, tokens are very small unit in C and it is very important to the compiler.” which is about as useless as a description as it gets. Why bother explaining tokens when you can't even explain what they are and why they are important? * you then introduce different "types of tokens." Note that the standard calls them *token categories,* not *token types.* This is important to distinguish the *categories* of tokens from the *types* and *name spaces* of identifiers. Always use exact terminology. * the token categories you introduce are: variables, constants, keywords, identifiers, operators. This list is incorrect and self-contradictory; you say that variables are a category of token and then say that identifiers are a category of tokens representing variable names. Neither is correct and every careful reader is immediately going to ask “is a variable name a variable token or an identifier token?” * you also claim that constants and literals are the same thing. That is absurdly wrong and shows that you have failed to understand the important difference. Constants represent values, literals represent objects. The difference is immediately apparent in that you can take the address of a literal but not of a constant. If you do not understand this yourself, how do you expect your students to understand this? * the correct token categories are: keyword, identifier, constant, string-literal, and punctuator. There are three things to notice: (a) there is no variable token. In fact, at the time where the C compiler translates preprocessing tokens into tokens, it does not know whether an identifier represents a variable name, type name, label, or something else. This is only done during parsing. This detail is one of the fundamental things one needs to understand when one wants to know how the language works. And you get it wrong, indicating that you have not understood it yourself. (b) constants and string literals are distinguished which you do not (c) there is no operator token. Instead, there are punctuator tokens. Indeed, whether a punctuator represents and operator or not is only distinguished later during parsing. Again, this is something fundamental you got wrong. * your program example is not formatted as code (even though I'm sure the blogspot software can do that) and the output is shown as a picture. Don't do this. Never show pictures of code. Also, your one example program is completely irrelevant to the concept of tokens. Variables do not exist at the time the compiler cares about tokens, so why is there an example of variables? * your explanation of integer constants fails to explain the size suffixes (e.g. in `1L`) which are rather important in many situations. * perhaps the single useful thing a beginner would get out of your article is the syntax for floating point constants which is slightly unintuitive. And you then go on and do not explain it at all! You also miss hexadecimal floating point constants, which got introduced with C11 * your description of character constants fails to explain: (a) that character constants can be longer than one character (b) how to escape characters within character constants * the same applies to string "constants" * your list of keywords fails to contain any of the keywords introduced in C99 and C11. Keywords do not have a special meaning to the compiler but rather have a special function in the language. You do not explain this any further which is sad. * you say that all identifiers are variable names. This is wrong. Labels, structure tags, type names, and structure members also use identifiers in their grammars. This would have also been a good time to say which identifiers are reserved, but you fail to do so. * you then proceed to introduce "operator" tokens without even giving a complete list! How is the student supposed to know what these tokens are if you don't list them? This is useless * your precedence table is incomplete, wrong, and useless. It should specify all 15 precedence levels and the corresponding associativity rules. It should also list all operators so the student doesn't have to guess. See [here](https://en.cppreference.com/w/c/language/operator_precedence) for a good example. I feel bad for your students, honestly.
I didn't know this was a subreddit, and apologize for posting this here, rather than there.
Indeed! I can't say that I didn't enjoy reading it.
1. You never check if the call to `strtol()` fails. You also assign the long value to an int value. 2. When opening a file fails, you may want to use [`perror`](http://man7.org/linux/man-pages/man3/perror.3p.html) or [`strerror`](http://man7.org/linux/man-pages/man3/strerror.3p.html) to get and print the cause of the error (e. g. "file not found", "permission error", ...). 3. You never check if the calls to `fread` fail/read less than intended. 4. When you `malloc()`, you don't need that cast. Also, there is a potentional that the multiplication would overflow to something smaller. The error handling can also be improved with a better message (e. g. `"Out of memory\n"`), and you missed an `\n`. 5. While thinking about the above, you may want to check if the scaled size can be stored in the BMP (since the width and hight are stored as LONGs) 6. Using `abs()` for something that is normally positive anyway seems wrong. 7. Like with `fread()`, you may want to check for errors after calling `fwrite()`. I also would have said that reading and writing to/from a structure isn't the best idea due to padding and internal representation of numbers, but since the [complicated example code](https://docs.microsoft.com/de-de/windows/desktop/gdi/scaling-an-image) from Microsoft also does it, i'll let it pass.
Code review is on topic in /r/C_Programming.
Your post is not on topic. /u/andrewcooke suggested another subreddit, but code review is fine here, too.
Thank you so much for the time you spent evaluating my code, I wanted to say that before anything else. You're right, and I was a bit of a derp for not checking if strtol fails, I'll be sure to bare this in mind when I inevitably use it a lot in future, and will update my code accordingly. With how new I am, I don't know about perror, or strerror, but I'll look into this right now! Using abs for the height was actually for a reason, as apparently having a BMP with a negative value for it's height is 100% valid, and changes how the file is read. I don't know why a height can be negative, but it apparently can in bmp files. I also didn't know how else to handle the headers than to use structs, but I did account for padding in the structs themselves, so it won't fail, thankfully. (I think.) which did make it a little convenient for me, since I could just replicate the structs Microsoft's own documentation used. Thank you so much again for the time you spent on this. You gave me some unbelievably helpful advice, and things to look into, and you giving your time like this really is appreciated. 
So I argued in [this other post](https://www.reddit.com/r/C_Programming/comments/a8kz4x/tokens_in_c/).
&gt; Using abs for the height was actually for a reason, as apparently having a BMP with a negative value for it's height is 100% valid, and changes how the file is read. I don't know why a height can be negative, but it apparently can in bmp files. I just took a look in [Microsofts documentation about the structure](https://docs.microsoft.com/en-us/previous-versions/dd183376\(v=vs.85\)) and sure enough, it's valid: &gt; * biHeight &gt; The height of the bitmap, in pixels. If **biHeight** is positive, the bitmap is a bottom-up DIB and its origin is the lower-left corner. If **biHeight** is negative, the bitmap is a top-down DIB and its origin is the upper-left corner. &gt; &gt; If **biHeight** is negative, indicating a top-down DIB, **biCompression** must be either BI_RGB or BI_BITFIELDS. Top-down DIBs cannot be compressed.
Thank you very much, I'm glad that my post isn't off-topic, or against the rules of the subreddit, I was a bit concerned due to how he worded his message. 
Yeah, I find it kind of weird that it's valid.. And it would probably just be better if it wasn't, since I think that means the first bit is always dedicated to it being negative, or positive, but it is an old file format, and I'm not savvy enough with programming yet to actually know if what I just said is true or not. It just seems like it unnecessarily limits the file's scale.
Performance, speed, oldschool: try [Emacs as C++ IDE](http://martinsosic.com/development/emacs/2017/12/09/emacs-cpp-ide.html) 
Using a buffer that includes the padding would simplify things a bit. Byte order would need to be handled for big endian boxes.
Netbeans
I had to look into what that exactly meant, but I do now see what you mean. That is definitely something that's good to know, and bare in mind in future, thank you! 
Since someone recommended Emacs, I will recommend Vim! &amp;#x200B; It really depends on what you want and what your environment is. If you are working at a job where everyone uses Visual studio, I would recommend using Visual Studio so that your collegues can help you a lot easier. If you are still studying, figure out what you really want. &amp;#x200B; You say performance and speed? I said the same once. Therefore, I use Vim, Make and GCC on Linux. Linux boots fast and the programs do not have startup time either. Vim can handle large files easy and has many, many keyboard shortcuts to make programming faster. Emacs also has these features, so it is not really an argument in favor of Vim specifially. It is an argument agains Visual studio and Atom, though. &amp;#x200B; Linux, Make, Vim and GCC also have a very steep learning curve, something Atom, Visual Studio, Code::Blocks and Windows do not have. But I see that as a one-time investment and therefore I am willing to make the investment. &amp;#x200B; The reason I package Vim, Make and GCC is that almost all modern IDEs package the functionalities of these three programs in some way.
I don't use an IDE. I recommend working with two terminals: one with a text editor, one where you compile your code. Add more terminals to open documentation and reference code at the same time.
Visual Studio. Jesus said to use it if you are on Windows. Anything else is a sin.
Use a text editor (Sublime, VSC, etc) and the command line.
Emacs for C. 
QtCreator seem very good these days. I personally use Eclipse but I have a workstation with almost infinite RAM and 12 cores of CPU.
Your code is full of magic numbers, replace those with defines that describe their intent
`vim` + `cscope` + `zsh and POSIX utils`
Maybe they used some kind of algorithm that draws bitmaps with the origin at the bottom left but making the height negative would move the origin to the top as a side effect.
gedit + terminal for me
I know you're trolling, but I laughed anyway. 
I know you are trolling, its hard to accept Windows did something good for once.
Except it wants to install 100Gb of crap. Why does a C++ IDE need am SQL server installed? 
As I see it, there are four general approaches one can take: 1. Have every piece of code that wants to assemble or decompose a value from/into smaller chunks do so explicitly with a sequence of operations at the place where the load/store is supposed to happen. 2. Have every library include its own set of functions/macros for byte packing/unpacking, requiring that anyone trying to read, understand, or adapt the library code understand the requirements thereof (e.g. what alignment do they expect, etc.) 3. Have the Standard finally get around to defining such things, on the premise that whether or not 8-bit is the universal size for `char` it is certainly the standard size for information interchange, and that the usefulness of systems with 9-bit, 12-bit, or 16-bit `char` would be *enhanced* by having standard routines to e.g. convert the bottom 32-bits of a `long` or `unsigned long` into a little-endian sequence of four `char` values with 8 bits stored in each, do likewise but big-endian, etc. 4. For code which will only ever be run on systems that lay things out properly, simply use type-punned reads and writes to let the hardware do what it naturally does. If I know that code was written for a 68000 which requires 16-bit and 32-bit values to be 16-bit aligned, and I see that it casts a pointer to an `unsigned long*` [32 bits on such platforms] and xor it with `0xFFFFFFFF`, then I can see based on just that code and my knowledge of the processor, that it is going to invert all the bits in two consecutive 16-bit words, and that it is not expecting any allowance for the pointer not being 16-bit aligned. If the that were replaced by a call to `store32lh(ptr, load32lh(ptr) ^ 0xFFFFFFFF)` I'd have to look at those functions to see what they do. I'm not sure why compiler writers should be proud of the fact that their optimizers can take code that requests an inefficient sequence of operations and replace it with an efficient sequence, when simpler compilers would have allowed programmers to specify the efficient sequence of operations in the first place. 
Just by quickly scrolling over your code, I'd say you should validate the inputs from the user, making sure they're within a set of desired options.
I used prints to see where the problem is, like a user recommended me and the problem is within \*(lab+pointerpoint(ex-2, ey)), for example. it recognizes the key but it does not enter the if.
Seems like someone does not understand how checkboxes work... I just reinstalled Visual Studio and it was 6GB. You need to get out of your way and tick the database option to download it. 
The problem with the various "Friendly C" concepts is that they--like the pushers of aggressive optimization--ignore the fact that *different kinds of programs need different things from a compiler*. It is not reasonable for programmers to demand that all compilers process something like int foo = INT_MAX; long long bar = INT_MAX+1; in such a way that `bar` gets `INT_MIN`, but it's also not reasonable for compiler writers to demand that every programmer needing a function which behaves like: int muldiff(int x, int y, long z) { return x*y &lt; z;} in cases where `x*y` fits in `int`, and which either yields 0 or yields 1 in cases where it doesn't, must add extra code to handle force overflow cases to yield 0 or 1 in deterministic fashion. When C89 was written, a philosophy that implementations intended for various purposes should be expected to uphold guarantees that would benefit those purposes at nearly zero costs worked well, since the usefulness of many guarantees was inversely proportional to the cost. There really weren't a huge number of "close calls" where a behavioral guarantee would be useful even though upholding it was expensive. There may have been situations where offering a loose guarantee was much cheaper than offering a tighter one (e.g. on many platforms, a compiler would often have to include an otherwise-unnecessary sign-extension instruction to guarantee that the function above would be consistently processed as: int muldiff(int x, int y, long z) { return (int)((unsigned)x*y) &lt; z;} but wouldn't have to do anything special to ensure that the function will never do anything other than yield 0 or 1 with no side-effects) but in many cases the looser guarantees are more valuable than the tighter ones anyway. The problem is that compiler writers have come to view a theoretical possibility that a guarantee might impede some optimizations as an unacceptable cost, even if the circumstances where a compiler could *usefully* exploit the lack of a guarantee would be far less common than those where programmers could benefit from its presence. The Standard's failure to mandate a guarantee represented a judgment that there might exist platforms where the cost of a guarantee would exceed its benefit--not a judgment that the cost exceeded the benefits on platforms where the only "cost" would be the loss of optimization opportunities which were very unlikely to be useful outside contrived scenarios. 
Yes, but I only actually use fread/fwrite, calloc and free.
The only time I've really had problems with VS is when the anti-virus keeps fighting it. I got rid of my anti-virus in favor of Windows Defender and Visual Studio works great. 
Visual Studio Code has a nice balance of simplicity, ease of use, scriptability and extendability. There are good plugins for c/c++ and you are up and running debugging an application with a few clicks. 
Yes. Avast would always check my C executables time before running them, I just added an exception to all files located on Visual Studio folders and I was good to go.
Adding an exception didn't work in my case. I was using Trend Micro. 
You learned to do this in a month in C, with no prior programming experience? You're gonna be all right.
I work in IT, I grew up with computers, I spend hours and hours outside of work doing all sorts of things IT related. I dabble in Windows and Linux network configuration, etc. I would love to learn to program, but I just don't "get it". I have a hard time grasping the big picture of it all. Why does a program need to loop? Why do I need to know how to add/subtract/multiply and divide in python? Why are floating numbers important? I lack the fundamentals. Granted, I have a pretty severe learning disability (never got past algebra even as an adult). I've dome some tutorials, but they are pretty much the same. They all seem to start out with writing the print "hello world" statement and progress from there. Can anyone suggest a "programming fundamentals" course/tutorial? I'm in my 40s now and I'm still eager to learn, but am pretty frustrated and don't know where to go.
I hate having to configure all the crap to make Visual Studio build projects, but I also find that it's debugger is worth it. Although, that's just for application code. For headless server code, I just VIM, since a debugger rarely helps you there and you mostly have to rely on good logging and critical thinking skills to find issues. 
That is really, really flattering of you to say, thank you! I really do appreciate hearing that, and I'm kind of relieved to hear people saying that it's just that the challenge is very difficult, given my experience, rather than me being inept.
You could give Clion a shot. https://www.jetbrains.com/clion/ 
Well I just mean to say if this is your progress in a month, keep doing what you're doing. There's a lot of pretty complex concepts going on here that most people have trouble understanding in scripting, let alone a low level language like C. Have you been teaching yourself from a book (or books)?
Visual studio code is probably the best editor as emacs and vim are not for beginners. But I cannot figure out how to set up debugger at all...
How do you set up a debugger (gdb, llvm) ? I cannot ever figure it out...
VSCode and the C/C++ plugin has debugging capabilities builtin. Works sort of fine for me with GDB.
My mayor recommendation is to split this code into many smaller functions. Basically every line or small block that you have annotated with a comment can be separated into its own function. This makes the code a lot easier to read, understand, alter and build upon :-).
I’ve had very good success with Eclipse CDT. I’ve worked with on small( couple of thousand lines) and big (millions of lines) projects. It’s free. Open source. And cross platform. 
Thank you so much again! My first week of learning was simply watching a LOT of Youtube videos. I found a lot of playlists that attempt to teach you C from start to finish, some being quite good! This was how I learned Syntax, and the very basics of C. On my second week, I discovered Harvard's cs50 course. A course you can take online, but is apparently not designed for people doing it online. I started to strictly start doing their assignments, and googled a LOT of concepts to try and understand the basics of C that they didn't explain. I'll give a basic breakdown of all of their assignments I was given, and chose to do (Some were optional) that you can look over, so you get some idea of how difficult jumps. Imo, difficulty is jumping too damned fast. Week 1: * My first ever assignment was to take the user's input, and construct a half pyramid of N height (Their input). This simply involved placing an amount of spaces, and then a #, and repeating until I print the bottom of the pyramid. (I struggled with even this, back then) * My second project was to take an input, and tell the user how many coins it would take to give them that amount in change. For example, an input or 0.75 would return 2, because you could be given a 50, and 25 cent coint. * My third project was optional. I had to calculate if a credit card number was valid using an algorithm, it was difficult for me, but I LOVED this task, and it challenged me like crazy. Week 2: * My first assignment was to encrypt the users input by incrementing each char in their input string by n. N was also user inputted. In this, I learned to us the Modulo operator to make sure the user input wasn't redundantly large. I then just.. incremented the value inputted, and if it reached Z, I set it to a, and it if reached z, I set it to A. I was also required to make the program able to decrypt it again, which I did. * The second assignment was to "crack" a password! This one incredibly difficult, but incredibly fun. I was given an encrypted password, and was given the task of bruteforcing what it is. This was done by taking the salt (First 2 values of the encrypted password, in this case) and using that to encrypt every possible password that can exist within a 5 character limit, only alphabetical characters were checked though. If my string encrypted, and matched theirs, I found their password. Week 3: * A music generating algorithm. Now this one was actually a hellish experience for me. In this task, they gave you some code to start with. They wrote a valid .wav file for you, but you had to handle one of the .c files that took the user input, and converted it into a valid, machine readable format. This required me to learn to read others code, and add to code, rather than write it all. I had to read the other files to understand what input I'd receive, and what output I needed to give, and then test it myself. Fun challenge, but it was very difficult for me, and took me two days. * Whodunit: I didn't name the challenge, but it was REALLY fun, by far my most enjoyable task to deal with. They gave me a few .c files to start with, and all they did was duplicate an input file, and I needed to modify it to find a hidden message in the input image. I loved this one so much that I'll include the input and output I managed to make for this one. They gave me this file, and told me to make the secret message inside of it as visible as possible. https://i.imgur.com/Iwr8AQ1.png My output that I was very proud of, was this! https://i.imgur.com/XMUIMII.png * This exact challenge! It was a challenge to take an input file, an amount to scale it by, and output that file. They gave us a small amount of code to start with. They gave us code that duplicates a file, which I soon realized isn't even half of the work required to get this done. Harvard's cs50 course has done nothing in the way of teaching me things, but they've given me the assignments I've used to challenge myself, and make me do research on my own. I'm sorry if this was way too much of a wall of text.
What text editor and terminal emulator do you use? I use vim and urxvt.
nano and whatever I have installed.
Wow that sounds like a very accelerated course. I've always always self-taught myself from books, and avoided their exercises because they were so boring. I appreciate the break down, those challenges sound much more interesting. Seeing what kind of progress you've made in a month, I might have to follow your lead. Have you heard much about steganography? Those last few exercises reminded me a lot of that. If you enjoy that, you might get a kick out of cryptography. There's a pretty popular online course available for it. https://www.coursera.org/learn/crypto
vim/vis/neovim or emacs. I personally use micro though.
Have you done much bash scripting? It might be easier to learn the basics of programming with a simpler language, and then apply them to C. Understanding the concepts of loops, arguments, functions and all that can take a while. I would Google "basic programming concepts" and study them for any language. Two books I really understood well were "C For Dummies" and "Practical C Programming"; "C For Dummies" teaches you about the fundamentals of programming very well, and "Practical C Programming" avoids "Hello world" pedantry and shows you how to make actual use of the language in ways you're more likely to want to do, like file input and output. I'll try to nutshell your question about loops and arithmetic (adding/subtracting/multiplying). We like to harness computers for their ability to do repetitive tasks really quickly, and that's precisely what loops do. Say you want to copy a file from USB drive to your desktop: the data is processed from one drive to the next with a loop because each byte ( more likely a block of bytes ) has to be moved sequentially one at a time. The loop just does the same function (copies a byte) over and over until the whole file has been copied; but how does the computer know when to stop copying? You have to tell it. You need to know how to do basic arithmetic functions in any programming language because you have to know when to make those loops stop. Copying a file is fairly simple, because the computer copies one byte at a time, checking how many bytes it has transferred, against how many it had to transfer in the first place. The loop would know which 'condition' under which to stop by 'evaluating' the 'expression' \`amountOfBytesCopied = fileSizeInBytes\`, and if it is 'true' then the loop will stop. That is what is known as a 'conditional test'. We 'evaluate' the 'expression' to determine if it matches the 'condition' we describe, and tell the loop what to do based on that. Generally we want the loop to stop when the condition is true; but we can tell the loop to stop when the condition is false too. Okay, but what if instead of copying the file, you wanted to strip the footer off of it? You would have to be able to tell the program, "Stop this loop this many bytes less than the file size." Using arithmetic in loops can very easily be thought of as converting word problems to expressions. Say, "Copy every byte in this except for the last 20," and you could check that by 'evaluating' the 'expression' \`amountOfBytesCopied = fileSizeinBytes - 20\` and if that 'condition' is 'true', the loop stops. 'amountOfBytescopied', 'fileSizeinBytes' are called 'variables' because their value may vary; '20' can also be considered a variable, but because we know the value is 20 it's just a 'constant'. That's an algebraic way of thinking about it. For our purposes in programming, we just think of that as a 'value'. All values can be 'variables' or 'constants' because the computer just translates each into a 'value' anyway. Likewise, you can use 'functions' inside of an 'expression' and the computer will translate that to the funcition's value. That's a fairly simple example, but it can get more complicated if you have several different things you need to test the conditions of, and for that you may need to evaluate expressions with many different values. For example with the file copying, if you wanted to copy most of the file in 1024 byte blocks, and then the remainder of that byte-by-byte, you would most definitely need some division in there. Anyway, if that explanation helped, pick up the books I mentioned at the beginning of the post. "C For Dummies" explains these concepts very redundantly as I have, and "Practical C Programming" relates it to examples a little more realistic than "Hello word" examples. I know personally how much learning disabilities can suck. I just turned 30 and never learned algebra either. There's a ton of learning resources online though. Check out "Kahn Academy". 
It's all relative to your other numbers. I've found it better to keep the numbers small, or use a tested, open game system like D20. If you know what you want your power curve to look like, you can plot a bunch of points and then do the appropriate regression to generate a formula. 
Excellent question for game design -- you'll probably get more on-point responses in dedicated game-design subreddits, as this really isn't particularly C-specific. THAT SAID The biggest question you should ask yourself is whether you want damage be a somewhat even distribution with regards to your random variable, or whether you want a distribution that tends to center around one particular place. Let's take a (very) simple example: Rolling a 12-sided die vs rolling 2 six-sided dice. Both systems end up with a maximum of 12, however in one system you have an equal chance of getting each value (ie. 1/12 chance) but the other system, you have a higher chance of getting a 7 (1/6) compared to getting a 12 (1/36). Your damage equation can't simply be determined in a vacuum -- you need to look at it in the context of hitpoints, armor/defense, chance to hit etc. My best recommendation would be 1) straight-up copy a system from a tried-and-true pen-and-paper RPG system. 2) build out a spreadsheet, and play around with some "what if's" of different damage systems. Of course, you should be building your program such that it's somewhat trivial to tweak these values/equations later -- so just go with something simple, and then playtest, tweak, and balance from there.
srsly, i had to write a driver one time. looked at openbsd's version, 6 pages of clean and simple code, looked at linux's, 60 pages of hell. i know it was pages because i printed it (i was going through a printing phase).
I have been finding Geany very nice to use on Linux. It makes dealing with larger source files easier. Emacs is pretty powerful, but it is so clunky to use I never bother anymore.
&gt; Why does a program need to loop? Why do I need to know how to add/subtract/multiply and divide? If you do want to continue learning to program, it's probably best to stop worrying about questions such as these. Just try writing programs that do simple things. I've got a project idea for you. Try solving it and reply to my message with your answer. You know how in school detention they make kids write a sentence 100 times on a blackboard? Write a program that prints a sentence 100 times, like "I will be a good program. I will be a good program, ..." etc.
Vim is very cool.
Since no one else commented about the comments, I think I will. Most of your comments repeat the code, not adding any value. I can see make out you are reading `infiles`'s `BITMAPINFOHEADER` just from the variable and type names; there's no need to say that in a comment. However, the comment "ensure infile is (likely) a 24-bit uncompressed BMP 4.0" is good, as it describes *why* the code exists (that even if you replaced the constants with descriptive names it still might not be apparent why the code is making those checks). I usually only comment either working around bugs in other code [1], convoluted logic (sometimes known as "business logic") [2] or the often times weird input one can get in a program [3]. Try to avoid comments like "close infile" or "success" because it doesn't add anything to the code. [1] I recall coming across one implementation of `memchr()` (a standard C function) that [failed when the count argument was the maximum value allowed](https://github.com/spc476/CGILib/blob/master/src/Pair/PairNew.c#L48). [2] I have [code that will swap two dates](https://github.com/spc476/mod_blog/blob/master/src/backend.c#L186), and I have to handle partially specified dates. The comments for that routine give a lot of examples of input and output. [3] [You can't trust input from the outside world](https://github.com/spc476/CGILib/blob/master/src/Pair/PairNew.c#L53). 
Visual Studio is IMHO a really bad C IDE, simply because it tries you to move away from C to C++ and urges you to use crude language extensions. I think *especially* for learning an IDE is bad, because you do not see the steps involved in building a program, which is inherently needed when programming in C.
Why did you reinstall it?
It *is* slow. Even on an SSD with my Ryzen 7 1700 and far enough RAM. At least compared to an editor. And it's easily the worst C IDE as it wants you to code in Visual C++ all the time, urging you to use crude language extensions. A beginner will be fooled by this. No way.
Mhm. Yeah I agree with the part with learning language first rather then vim/emacs. I think vim/emacs are used by advanced users. Even if I know how to program (I am web programmer) and I started to program in C/C++ I would still use and IDE, but I would try emacs/vim, because I want to and why not :) Anyway, thank you for your input!
I programmed in Visual Studio long time ago, so I dont remember how it performed. &gt; I think especially for learning an IDE is bad, because you do not see the steps involved in building a program, which is inherently needed when programming in C. So you recommend using vim/emacs terminal? Thanks for your input!
Allright! I will take that into consideration! If you have some links for good tutorials on terminal/vim I would gladly accept to learn. :) Thanks for your input!
Thank you for your input!
Mhm, that is interesting. Thank you for your input!
Thank you for your input! 
Thank you for your input!
I recommend using a simple editor (nano, vim, pico, emacs, notepad++, gedit, ...) and compile the program on the console using `cc`. You may or may not want to specify the exact compiler `gcc` or `clang` s.t. you can use compiler-specific flags. Try then to automate the build process using `make`, it's a rather simple concept, really useful, and on *nix systems used almost exclusively in some way or another.
All right! I see many people recommend vim/emacs. Do you have any links that I could start learning vim/emacs and terminal of course? Thank you for your input!
I use notepad++ for web stuff and currently code::blocks as IDE. I used sublime, coda 2 (its really good, but I am on windows/linux(soon) now), visual studio code. But you want to say, to write in text editor and compile it via terminal? Thank you for your input!
Snap up? What do you mean by that? do you have any links for learning emacs/vim? Thank you for your input!
All right! Thank you for your input!
Thank you for your input!
Hey give us some ram, pls :D For me eclipse is somehow heavy, i dont know. But thank you for your input!
Mhm. Thank you for your input!
I think that that vim/emacs are for advanced users. Thank you for your input!
Is it heavy weight? Thank you for your input!
Since I am newbie for vim/emacs do you have any links so I can learn them? Thank you for your input!
Clunky? In what way? Thank you for your input!
For vim, you can just run `vimtutor`. For emacs, you can find tutorials online: - https://david.rothlis.net/emacs/howtolearn.html - https://www.emacswiki.org/emacs/LearningEmacs For vis, you can use the man pages or `:help`
Oh, thanks for this nice explanation! :)
Thank you very much! :))
The comment you highlighted is correct: once you invoke undefined behaviour _anything_ is possible. You can't even rely on the code crashing.
It's very command driven for being a graphic interface. For example you can open many windows, and have them all perform a useful feature but you can't do it in a point and click, but instead have to remember keyboard shortcuts. I'm sure some Emacs fans are real snappy with them, but for me they were cumbersome to actually perform right and hard to remember without a list handy. Nevertheless it is still very powerful for debugging because you can run gdb inside of it and track the source in a split window. It's just getting those windows open might be more of a hassle than just following in gdb's limited terminal view.
Thank you for this detailed explanation :)
Are you into functional programming? Then you might find [Cló: The Algorithms of TeX in Clojure](https://www.youtube.com/watch?v=824yVKUPFjU) interesting.
the comment you highlighted is correct, but it's an obvious point that vd linden would have known. my guess is that the poster didn't understand the point the book was making.
Following could be used on intels for demo/testing. First time I tried it, seems to do what it says on the tin. #ifdef i386 asm("pushf; orl $(1&lt;&lt;18), (%esp); popf;"); /* enable AC check */ #endif
What's the difference between r/C_Programming and r/cprogramming btw?
Different subreddits for the same subject matter.
I’ve used with the Linux kernel. Is that enough? 
It looks like it converts a native float into the IEEE format.
Related part of the conversation: &gt; &lt;Jibz&gt; and it should be &amp;f instead of &amp;p &gt; &lt;geezer83&gt; Could fix it like this? float f = 100.13; float *p = &amp;f; fwrite(&amp;p, sizeof(p), 1, fp); &gt; &lt;jp&gt; wut &gt; &lt;Amun_Ra&gt; nope &gt; &lt;atk&gt; no, that's not how you serialise floats &gt; &lt;jp&gt; atk is our resident cereal expert, yes &gt; &lt;lyptt&gt; now you're serialising a pointer &gt; &lt;lyptt&gt; to a float &gt; &lt;atk&gt; and anyway, there's a better way of doing this &gt; &lt;oxymoron93&gt; &gt; Compiler reports syntax errors in situations such as: x) Trying to use an declared variable. &gt; &lt;geezer83&gt; what's serialize and what's wrong with this? &gt; &lt;atk&gt; http://ix.io/1wH0 - you can find examples of this kind of code all over the place, this is my version &gt; &lt;atk&gt; use this to turn into a number, and then you can use standard serialisation methods to write this to a file It looks like the function liked converts a float number to an IEEE754 floating point number representation as an long integer that you can more easily write to a file. The reason to do this is float in C can be anything and IEEE754 float is a reasonable standard to convert to/from. Note that current modern systems and compilers float is already a IEEE754 float, but it doesn't have to be.
certain? what is so special?
It's worth noting you can use Clang with Visual Studio now, so you're not limited to MSVC and it's terrible C support. It's still not great though, and there's lots of odd C specific problems, like 'wchar_t type unknown' when you include windows.h for some reason. But really wrestling with VS+C has just made me appreciate MinGW-w64 more. I don't know if I could ever recommend command-line GDB in good faith, but it does the job when you need it.
Not sure about this, for the letter 'd' move your if check is if(*(lab+pointerpoint(ex+1, ey))==track) notice it is add 1 to **ex** whereas all other checks are adding 2 to either **ex** or **ey**, but inside the **if** block you move the player to **ex + 2**. 
Unaligned reads come at a cost. Unaligned reads [may also be slower](https://lemire.me/blog/2012/05/31/data-alignment-for-speed-myth-or-reality/). Or what if the read straddles a cache line? To execute that instruction, the CPU will need to detect this, make two separate memory fetches then, stitch the results together. Your CPU has to be more complicated (more transistors, etc.) in order to manage this, and instead some architectures just don't handle it.
Neat idea. Incremental improvements without having to break all sorts of existing things. I wonder if this could be used to add units to C as well. 
Usually you separate code in multiple files because of mantainability. Imagine in an year you found a bug in one function, now you have to go over the 1000 line single file to find the function you need to change. It would be better if you had to skin over a 100 line file wouldn't it? Now you have a new problem: how to split the functions among multiple files? You try to group the functions by some "theme", for example one file for all I/O functions, one file for you main program, one file for the "business" logic. Having said that, since you still haven't written anything, start at a single file, and when you have a couple of functions, try to group them in a logical way in different files, but don't sweat over it, if your program is simple enough, one or two files may be enough.
Thank you very much for that explanation, it's very much appreciated! I'll definitely be referencing this in future for how to split things up sensibly. 
When you start emacs the first time, it starts with a tutorial. Snap up: nothing, really. I’m not native English speaker and just wanted to use a term that I felt appropriate in the context of “start quickly without much delay”. I’m sorry if this was confusing.
Or just a *nix environment rather than Windows :) Eh, GDB works quite fine for me, especially with conditional breakpoints etc.
bro. am the poster. And am not a God in C. 
Thanks. You gave me some great resources.
It's not always about looking pretty or even being readable. Linus definitely trades off readability for speed of instruction. https://medium.com/@bartobri/applying-the-linus-tarvolds-good-taste-coding-requirement-99749f37684a In general this blog post will help. I still haven't made it all the way through. https://blog.regehr.org/archives/1393
auto is mentioned twice in the list of 32 keywords.
Along with that, having smaller files typically makes for less merge conflicts when multiple people are maintaining the same codebase.
I don`t get how that is "supposed" to generate a bus error - alignment things are entirely architecture-related. x86 will happily do an unaligned access for you (for a performance cost).
Thanks
The definition of tokens in the Standard is a bit different from what's suggested here. The Standard, for example, forbids compilers from being smart enough to parse 0x1E-4 as three tokens (a hex integer constant 0x1E, a minus, and a decimal integer constant 4). The rationale suggests that this was done because it wasn't worth requiring that compilers be able to handle such constructs, though none of the pre-standard compilers I've seen had any difficulty with them.
The auto keyword exists and I still haven't seen an actual use for it in all these years. ; )
Yes
Is it too much to ask that those who write tutorials in the C Programming language actually have some expertise in the language?
Reformatted computer.
You just need to change the extension of any files you create from .ccp to .c and that's it. When I first opened Visual Studio I thought it was crap, but that was because I did not know how to use it. What do you classify as slow? I installed Visual Studio on a normal hard-drive and it takes 5 seconds to open up and 10 seconds to open projects. I had it on an SSD before and it took less to open up. 
You separate your program into parts and each part represents a .c file. I am writing a Lexer with a Parser and I have different .c files each one with it's own header file. * File that takes care of memory management (I implemented my own malloc) * File that takes care of data structures, it basically provides a LinkedList struct and several functions that work with it * File that takes care of reading and loading the alphabet into memory * File that takes care of lexing. * File that takes care of parsing.
Would rather read this a 1000 times: [https://www.lysator.liu.se/c/ANSI-C-grammar-l.html](https://www.lysator.liu.se/c/ANSI-C-grammar-l.html) [https://www.lysator.liu.se/c/ANSI-C-grammar-y.html](https://www.lysator.liu.se/c/ANSI-C-grammar-y.html) &amp;#x200B;
Do one thing and do it well. Use .h as your public API.
A good example is certain microcontroller operations: function calls on ARM, for example, need to be aligned to a 2-byte boundary, whereas flash writes need to be aligned on an 8-byte boundary, yet flash erases sometimes need to be aligned to 16 bytes. And in some cases unaligned memory reads/writes are not possible (e.g. register memory space).
Looks like a potentially useful trick to ensure portability, but it may be hard to enforce alignment in the same ways that a target architecture might. What is the flag you're setting there?
Alignment Check, EFLAGS.AC. CR0.AM must also be set for it to function, but nicely, FreeBSD has provided that. I've been combing old programs with the check enabled and, strange, that rep movsl in memcpy is about the only thing that has come up.
Sure, it compiles with `/Zc` then, but it still has some problems correctly parsing `.h` files and is one of the least compliant compilers. And when I wait for some file to open I classify it as slow. I have a small Thinkpad E470 which for sure is no beast, but it opens arbitrary files instantly. Waiting is not part of my workflow and shouldn't be. I want to get something done.
These are the source files for the recipes in the recently released `CMake Cookbook`. It's a great book for those not familiar with modern cmake. You can also find it on certain russian sites with libgen in their name.
none ! I hate having to spend ages digging around in a GUI looking for some option thats buried in some unexpected place.... I find putting together a Makefile much easier and quicker, even enabling me to auto detect files in a specific directory and compile them into an executable, picking up library locations using pkg-config so it will work on multiple \*nix's unmodifies (even Linux for Window :0 ) I think especially for beginners intelliguess(tm) hinders learning, I've seen students guessing what they need from a drop down too many times... It really is worth learning how your compiler and linker work...
I tend to group code of similar functionality together... any utility functions that are unlikely to change will get separated out fairly quickly because if I don't need to edit it I don't want to keep scrolling past it, if a particular file takes too long to scroll through, then I'll likely split it... 
repost and most of the issues mentioned by u/FUZxxl here https://reddit.com/r/C_Programming/comments/a8kz4x/tokens_in_c/ece4xun?context=3 still stand. This is spam.
I agree! When I was learning to program I thought the autocompletion is enemy of newbies and I still think that! Thank you for your input!
Its okay, I understood it :) Thank you!
haha yes it is :D
Thanks! :)
What problems have you found parsing headers? You are not waiting for a file to open. You are waiting for a whole project which involves several files to open. 
I would either recommend vs code or clion. I’d also recommend that you learn how to compile code: gcc, goo, ld, auto tools or cmake, etc. 
Awesome thanks! 
Vim
What's the point of C and C++ when there is already so much you could do with CMake?
both 'word' and 'replace' only refer to the first entry in each array, you have to repeate this process for each entry in 'word' and 'replace'.
There are a lot of issues here. To start, you are using the standard string functions (strstr, strlen, ...) incorrectly half the time. char word[MAX][MAX] = { "hello", "what" }; char replace[MAX][MAX] = { "salut", "quoi" }; These are arrays of character sequences (or arrays of strings for simplicity) &amp;#x200B; char string[MAX]; This is a normal string declaration &amp;#x200B; strstr(string, word) This generates a warning on *any* compiler/IDE. strstr takes two strings a parameters, not an array of strings. If you want to access each string element in the array you need to use a loop and iterate through the array comparing each element to the "string" &amp;#x200B; Example: // Get the number of items in the word array int word_count = sizeof(word)/sizeof(word[0]); // Iterate through each item to compare. for (int i = 0; i &lt; word_count; i++) { if (strstr(string, word[i])) { // Add code here following the same idea. // Break out of for loop when a match is found break; } } &amp;#x200B;
Because the design is encrypting a single character a time, only the low byte of the counter will ever be XORed with the input character. If you look at the behavior of the low byte of the counter, it repeatedly counts up to 0xFF before wrapping back around to 0 - the same modular arithmetic behavior as if your counter were just a single unsigned byte. I am not well-versed in crypto, but IIRC the modular arithmetic behavior preserves the desirable qualities of using a counter. Reading chunks of four bytes may be doable as well, but you'd need to carefully do the conversion in a way that doesn't rely on the endianness of the machine. And I'm not sure what if any effects there would be on the cryptographic properties.
 I am not well-versed in crypto, but IIRC the modular arithmetic behavior preserves the desirable qualities of using a counter. I'm really no expert either, but from what I understood the counter was supposed to create an apparent one-time-pad. I can't explain it very succinctly. But when you do keyByte XOR nonceByte the resulting value is the "keystream" byte, and that's XOR'd against the byte of the message. The counter variable is intended to make the "keystream" non-periodic, so that it appears to be a one-time-pad (though technically not since it's not truly random). So if you do keyByte XOR nonceByte XOR counter, then the resulting "keystream" byte is modified by the counter variables value; even if the keybyte and nonce byte roll over to the same value, the resulting keystream value will be different. It doesn't necessarily make it insecure, since write now the keystream will only repeat after 3.4 megabytes ( more than enough for a password file ) but I would still like to be able to implement it as the design intended in the book. Reading chunks of four bytes may be doable as well, but you'd need to carefully do the conversion in a way that doesn't rely on the endianness of the machine. And I'm not sure what if any effects there would be on the cryptographic properties. Yeah that's what I was worried about too. Because if it's little-endian or big-endian, it will discard a different sit of bits off of it right? I believe big-endian does the left-most bits ( higher values ) and little-endian discards the right-most bits (lower values)?
I compile stuff from the command line only. I use it on a machine with no Notepad++. I think you downloaded the [Windows Installer package](https://github.com/raysan5/raylib/releases/download/2.0.0/raylib_installer_v2.0.exe); this locks you into Notepad++ and MinGW. I download just the header and lib and use them like any other C library with any IDE or editor. Linked to the respective libraries, if you're using VS Code with * MinGW - [32 bit](https://github.com/raysan5/raylib/releases/download/2.0.0/raylib-2.0.0-Win32-mingw.zip) - [64 bit](https://github.com/raysan5/raylib/releases/download/2.0.0/raylib-2.0.0-Win64-mingw.zip) * VC++ - [32 bit](https://github.com/raysan5/raylib/releases/download/2.0.0/raylib-2.0.0-Win32-msvc15.zip) - [64 bit](https://github.com/raysan5/raylib/releases/download/2.0.0/raylib-2.0.0-Win64-msvc15.zip)
Neat trick! I thought I'd throw it into a `LD_PRELOAD` library on Linux and see what happened. My first test was `cat`... which failed immediately due to an unaligned read in `getenv` (via `setlocale`). :-p
I highly appreciate your help! I've tried your idea, but I failed it and I think I'm to blame for it... could you, pretty please, provide me with the resolution to this problem? 
Want to post what you tried?
 while (!feof(fp1)) { strcpy(string, "\0"); fgets(string, MAX, fp1); int word_count = sizeof(word) / sizeof(word[0]); for (int i = 0; i &lt; word_count; i++) { if (strstr(string, word[i])) { ptr2 = string; while (ptr1 = strstr(ptr2, word)) { while (ptr2 != ptr1) { fputc(*ptr2, fp2); ptr2++; } /* skip the word to be replaced */ ptr1 = ptr1 + strlen(word); fprintf(fp2, "%s", replace); ptr2 = ptr1; } break; while (*ptr2 != '\0') { fputc(*ptr2, fp2); ptr2++; } } else { fputs(string, fp2); } } } &amp;#x200B;
It happens on FreeBSD too :) ... -&gt; setlocale -&gt; __getCurrentRuneLocale
It parses them as C++ or incorrectly. And sure, it opens a project, but what the hell takes so long, I often dance between multiple projects and it's a hassle managing all the instances of VS. And source files within VS even don't open that fast. So it boils down for me to * Too C++ focused, C parser and linter is crappy (e.g. doesn't warn about missing prototypes or K&amp;R style function declarations) * Slow as hell, I don't like waiting for a program to open. Why should I? I don't understand this should be acceptable. * solution+project files are a horrible system that doesn't translate well to anything else * It builds slow. Hardly concurrent and it tries to even continue building when it found an error, and you cannot change that behavior when you want concurrent builds. What the fuck? Sure, MS fixed a bunch of issues like that they have mostly separated IDE from the compiler, introduced better warning systems, integrated other build systems, but it's still shitty at the core and not flexible at all, just like Windows itself. I have a workflow and I want to use that workflow and not adjust my workflow to the tools. I'm currently forced to be working with VS again regularly, and yes, I'm positively surprised how good it is... compared to VS2008. With VS2010 I already noticed improvements but it was far from VS2017. I still want to go back to my setup though.
You have a nested for loop iterating through world_count * replace_count times. You only want one loop (If i understand your goal)
In the while loop that reads the symbols and strncpys it, you never increment i. As i is set to 0 just prior to the start of the loop, each string is copied into symbols\[0\], overwriting the previous contents. Also, n and token are not used for anything within the loop, which is odd.
So, you're saying that this code works -- what is the change that makes this code not work?
Code re-use is a good reason. Having your project organized into separate files with clear purpose makes it easier to re-use functions. It also makes it easier for other people to understand your code. 
That was it! Thanks and sorry for not figuring out reddit formatting for inline source code.
And even with only one loop, the issue of the other word not being translated still persists.
Do not repost [removed posts](https://www.reddit.com/r/C_Programming/comments/a8kz4x/tokens_in_c/) without asking. Also, while I appreciate that you tried to improve your post (and the lack of advertisements makes it much nicer to read), you addressed less than half of the mistakes I listed, making your article still a bad resource for beginners.
I removed a lot of stuff prior to posting to cut down on size
Indeed, that could have been improved in the grammar.
&gt; —- — - - — —- is this morse fuckin code? Format your post, damn.
There isn't really a need to split your code into multiple source files, but often it is useful to do so. Here are some reasons: * each file can be seen as a kind of bookmark into your source code. You can group your code by what part of the program it is for and put each group in its own file. This allows you to quickly find where a piece of code is if you forgot the function's name * when linking, often only entire object files can be linked in. If you write a library of which only parts might be used by a program, this forces unnecessary code to be included during linking if it was in the same source file as a function the user wanted. For this reason, many libraries like the libc split up their source code into many small files such that each file contains only a single function (or a strongly connected component of the call graph to be specific). This allows the linker to pick individual functions, reducing the amount of code needed in total.
Thank you for your input! :)
Oh, I will take a look at that! :)) Thanks!
Thank you
Lol gave me an idea for an awful text editor plugin. What if there was an editor that with every compiler error or segfault, it opened a Reddit post and then displayed the replies in your error window. 
What do you mean "the worst plugin ever"? This would be hilarious. Someone make it happen!
That would be better than posts like "it doesn't compile, please help [a snippet of a 2-line, unformatted code without context or even any relevance]"... OK, maybe not every time the compiler cries, but a simple button "paste error and relevant code to Reddit message" could help beginners... and us. But also could make people lazy and not use the debugger. For some people Reddit is the debugger even now. 
Yeah. It could automatically format the code to end the difficulties with getting it right in Reddit. Though it would still be tempting to sneak away a feature to make terrible posts. IDK maybe this would actually be a useful project. 
Yes, hard-coding the word wrapping by limiting each line to 80-characters is great. Take a look at these [two screencaps](https://imgur.com/a/EDWAZCw) which compare word wrapping long lines in vim vs limiting each line to 80-chars. You'll notice that "hard-coding" the word wrapping to 80-chars is much better.
If I have a line of code that's difficult to break down to 80 characters without reducing readability, it's most likely because my code is nested deeply. It's not a "hard rule", it's just an observation.
/r/errorsofshame?
It's sooo funny Xd i cant hahahah rofl
I just use Vim. See my .vimrc in [here](https://github.com/p1v0t/dotfiles/blob/master/.vimrc)
The return in the loop causes the function to finish immediately. It won't continue to the part after the loop.
yeah actually I don't know how I missed that, think I have to take a break from programming. Thx anyways
You'll want the fgets function (documentation: "Reads characters from *stream* and stores them as a C string into *str* until (*num*\-1) characters have been read or either a newline or the *end-of-file* is reached, whichever happens first.") So for each line you'll need to (a) have a buffer large enough to store the line (this will probably require that you set a finite max line length), (b) read the line into the buffer, then store the contents in your array.
What you need is an explicit step to serialize your counter into a byte array (i.e. `unsigned char` array). Once serialized, you just process it as a buffer. Here's a rough outline. unsigned long counter = 0; unsigned char key[KEY_LEN] = {...}; struct cipher cipher; cipher_init(&amp;cipher, key); unsigned char block[BLOCK_LEN]; while (get_input(block)) { /* Serialize the counter (little endian) */ unsigned char stream[BLOCK_LEN] = { counter &lt;&lt; 0, counter &lt;&lt; 8, counter &lt;&lt; 16, counter &lt;&lt; 24 }; counter++; /* XOR with stream */ cipher_encrypt(&amp;cipher, stream); for (int i = 0; i &lt; BLOCK_LEN; i++) block[i] ^= stream[i]; write_output(block); } This big thing missing here is an Initialization Vector (IV). Instead of starting from zero, you should start from a securely-chosen (`/dev/urandom`, etc.) random value. This value is not a secret, and you would include it alongside your ciphertext unencrypted. Otherwise reusing a key will completely break this system. Further, a 32-bit IV is also *way* too small since it's easy to brute force guess all possible values, and accidental collisions are too likely. Even a 64-bit IV is on the small side. Suggestion: Use a 128-bit counter made of two 64-bit integers. You'll have to increment the upper integer if the lower one wraps around. You're also missing a Message Authentication Code (MAC), which prevents someone from manipulating the ciphertext undetected. Stream ciphers and CTR mode are trivially malleable and particularly vulnerable. You'll need another crypto primitive (a hash function) in order to construct a MAC. It's very easy to screw this part up. 
So, something like [this?](https://github.com/drathier/stack-overflow-import) Except, instead of importing your code, it would just make posts on your behalf to SO.
Has science gone too far?
Yeah. Essentially the reverse of it. But no one steal my idea yet. I don't have very much internet Fame and this might be my ticket. LOL
Thank you!
Could you please show me some code because I am beginner?
If you return i instead of 1 you’ll have your square root
I doubt there are any non-contrived programs that rely upon the grammar specified by the Standard, but unfortunately it would be hard for a compiler to handle all strictly conforming programs in the manner required by the Standard without requiring a whitepsace after hex constants that happen to end with "E" or "e". For example, stringizing 0x1E+CAB+CAB*CAB+CAB in the presence of a macro for `CAB` should result in the two `CAB` tokens being macro expanded, but the occurrences of `CAB` within the `0x1E+CAB+CAB` token not being expanded. If the `0x1E+CAB+CAB` token doesn't ultimately survive the preprocessor except in stringized form, it could appear within strictly conforming code, meaning compilers would be required to recognize that as a single token. Even if a compiler was willing to interpret that as separate tokens after preprocessing, that wouldn't result in the first two `CAB` instances being expanded. 
Hey thanks a lot for your reply. I have a couple quick questions 1. Isn't the way I implement the yaxaNonce basically an IV? 2. If I initialize the counter variable to anything besides 0, doesn't that just reduce the overall entropy of the key stream since it has less values to use before hitting 0 again? 3. In the source you provide it says for little endian, but what if the machine is big endian? 4. If I serialize the counter variable to be an int, but leave the other components of the XOR operation char, won't that ignore the upper bits of the int counter and make it effectively a char too? 5. If I must serialize all component char values to integers, can I save them to file and be able to (deserialze) then into the original char values without worrying about endian-ness too? Thanks for pointing out the lack of MAC. I'm using OpenSSL's hmac function with sha512 and a separate key in the password manager implementation. I just stripped it down to this to make the smallest demo of how the algorithm is working, and how the keys are derived for it.
The Linux Programming Interface
Thanks, I'm fairly new to linking non-standard libraries. However, I always get **undefined reference to `InitWindow'** and various other errors. My .c file contains the first [example](https://www.raylib.com/examples.html) from raylib (the one with the blank window). Also, if it helps. My folder structure looks like this. raylibtest -include (the files from your linked MINGW download) -lib (the files from your linked MINGW download) -main.c -Makefile If I, after successfully setting up SDL, understand linking correctly, **gcc -I include main.c -L lib -lmingw32 -lraylib -lopengl32 -lgdi32** should work. But it leads to the said error.
Please make this happen
you use a loop with `fgetc();`. use `fgets();` in combination with a loop that checks wether you reached the end of the file. (actually quite the same as you already did) Write the char pointer into a char array. `char row[i] = fgets(BUFFERSIZE, FILE);` for example. Here is i the variable of a potential for loop that you could use to go through the whole file. &amp;#x200B; Have a nice christmas everybody
&gt; Isn't the way I implement the yaxaNonce basically an IV? I didn't look much at your code, so my comments were about my example. Your code is hard to follow since you're passing everything around via global variables, but, if I understand correctly, you're doing something like this: unsigned char nonce[] = {...}; unsigned char passwd[] = {...}; // user-supplied key (passphrase, etc.) /* Derive the cipher key */ unsigned char key[KEY_LEN]; kdf(key, passwd, nonce); /* Initialize cipher from derived key */ struct cipher cipher; cipher_init(&amp;cipher, key); If so, you're right. This should work fine as an IV, so it should be fine to start the counter at zero. However, you're vastly over-complicating things, and the size of your key and nonce is excessive ("Generate 8192 bit yaxa key", "Generate 512bit yaxa nonce"). IMHO, if `RAND_bytes()` fails, then the program should try to delete its incomplete output files and then immediately exit with failure. Continuing with a warning is dangerous. Also, your `cleanUpBuffers()` won't necessarily work like you expect. It's probably being optimized away by the compiler. Zeroing sensitive buffers is actually a tricky problem, and you generally have to call a special function to do it. However, you're only calling this function immediately before exiting the program, which serves no purpose. When the process exits, all that virtual memory will be unmapped and destroyed anyway. This is something you'd do if the program was going to continue running after the sensitive information is no longer needed. &gt; If I initialize the counter variable to anything besides 0, doesn't &gt; that just reduce the overall entropy of the key stream since it has &gt; less values to use before hitting 0 again? A 32-bit counter can take on 4,294,967,296 distinct values. For any properly-designed cipher, none of those values are special, and so wrapping around to 0 isn't special. What *does* matter is wrapping around to your start count. It doesn't matter where you start along that sequence, just that you never reuse any particular count with the same key. &gt; In the source you provide it says for little endian, but what if the &gt; machine is big endian? Study my example carefully and think about how those shifts work. It serializes to little endian format regardless of the host machine. On a big endian host, that code will still store the integer in little endian byte order since it writes the LSB first, and so on. I chose little endian because doing so will (generally) be more efficient for little endian machines, which are, by far, the most common. Here's an example of how to do it incorrectly: /* Write out in host order (don't do this) */ unsigned char block[BLOCK_LEN] = {0}; memcpy(block, &amp;count, 4); Unlike my example, the format of this buffer will depend on the host. You could do even worse: *(unsigned *)block = count; // undefined behavior! Which won't necessarily do anything useful at all. &gt; If I serialize the counter variable to be an int, but leave the other &gt; components of the XOR operation char, won't that ignore the upper bits &gt; of the int counter and make it effectively a char too? I don't understand your question. Once the counter is written to the byte buffer it's just bytes, and it's manipulated the same way as any other bytes. That's the point of serialization. Don't XOR anything with the counter integer itself. &gt; If I must serialize all component char values to integers, can I save &gt; them to file and be able to (deserialze) then into the original char &gt; values without worrying about endian-ness too? I don't understand this question either. You should never need to recover the counter value from a byte buffer (e.g. to deserialize it). When you're working with individual bytes (`char`), there's no byte order since they're just bytes. That's the nature of being the smallest addressable unit. Order matters when we can address the inside of something (e.g. like an `int`). We're generally not worried about *bit order* since individual bits are typically not addressable. 
I wouldn't want to encourage the "go to Reddit at any and all signs of adversity" mentality. We already have enough posts sitting at 0 karma wish answers akin to "read the man pages".
Thanks a lot so far. Sorry the source is so unclear. To explain my last two questions, this is how I'm (incorrectly) handling the encryption/decryption function /*Encrypt file and write it out*/ for (i = 0; i &lt; fileSize; i++) { fputc(yaxa(fgetc(inFile), yaxaKey[ii], yaxaNonce[n]), outFile); /*yaxaKey and yaxaNonce are allocated to 1025 and 65 bytes respectively*/ /*See yaxaKDF() for details*/ if(ii &lt; 1024) ii++; else if(ii == 1024) ii=0; if(n &lt; 64) n++; else if(n == 64) n=0; } yaxa() unsigned char yaxa(unsigned char messageByte, unsigned char keyByte, unsigned char nonceByte) { return keyStreamGenByte++ ^ nonceByte ^ keyByte ^ messageByte; } keyStreamGenByte being the counter variable, just gets incremented with each call to yaxa() the for loop makes. But since it's being XOR'd against char bytes, and returned as a char byte, doesn't it then become reduced to only 255 values? Won't any integer XOR'd against a char just have its upper bits zeroed? I think what I'm gathering from your source and your responses is that I don't really want to be XOR'ing these components as individual bytes in the encryption function, but rather want to serialize them all into one large data set, and then XOR that against the message data? Hopefully I've got that right. My hope was that I could make the other bytes in the XOR operation all integers, and return an int, then it would again allow the counter to reach its full size as an integer/long. I'd never really conceived doing it like you've demonstrated in your source, which I find very eloquent, but a little hard to understand just because I'm still pretty hazy with programming in general. I tried to read some of the source in Applied Cryptography for an idea of how I should be doing things, but that was all just mystifying and I wondered if I should try to read much into them since it was published in 1993. So I very much appreciate seeing some fresh and modern source in the proper convention, but it's taking me some time to digest it. Anyway I hope that might clarify some things (besides the fact I barely have a clue what I'm doing). My interest in programming kind of ebbs and flows, and so I'm trying to get back into it by tackling cryptography since it's always been a very interesting subject to me. Maybe bit off more than I can chew.
Looks okay to me. You're getting linker errors. There's a detail, however, which stumps many. Does your library's bit width match that of your linker's? If you're using a 64-bit toolchain, you need to use libraries that are also 64-bit; same for 32 bit. Check with `gcc -v`. If the target has something like `x86_64` then it's 64 bit. Download the right one and install them in your `lib` and `include`.
I'm still pretty confused about what you're trying to achieve, but I'm also realizing that I didn't read your initial question well enough. I saw AES-CTR and then that you're using OpenSSL in your code, then thought this was your AES-CTR thing and you were encrypting your counter with a block cipher. So you've invented your own stream cipher. You fill some buffers with key-derived bytes, then you have two indices (`ii`, `n`) into these buffers that loop around over and over. And you also mix it with your counter. That last part doesn't really make any sense, and the idea can't be salvaged, which is why I think you're stuck. You're fundamentally misunderstanding how this works. Plus your cipher would be very easy to break anyway. You're also misunderstanding how a nonce is used. It's not something you continually use to produce ciphertext. You use it when setting up / keying your cipher. It's used as an initial input (IV), or it's used in deriving the key. Then you just use the cipher like normal, forgetting the nonce. If you want to see what a real byte-at-a-time stream cipher looks like, check out [RC4](https://en.wikipedia.org/wiki/RC4) or Spritz. (Though both are now considered broken.) Both are very easy and straightforward to implement. Their indices move around in very complex ways, and their states get mixed up as they generate the keystream. If you insist on using a counter to produce a keystream, stick with CTR mode and a good block cipher. Try to write your program without using global variables or dynamic allocation (`malloc()`). You don't need dynamic allocation for this. Use fixed-sized arrays instead. Collect your cipher state up into a struct, and write your functions to manipulate this struct. &gt; I tried to read some of the source in Applied Cryptography for an idea &gt; of how I should be doing things, but that was all just mystifying and &gt; I wondered if I should try to read much into them since it was &gt; published in 1993. You're already on the right track with *Serious Cryptography*. It's up to date, starts from the basics, and the author really knows his stuff. Just focus on that. *Applied Cryptography* is a tour of some cool stuff from 20–30 years ago, but it's very dated and is no longer useful. 
**RC4** In cryptography, RC4 (Rivest Cipher 4 also known as ARC4 or ARCFOUR meaning Alleged RC4, see below) is a stream cipher. While remarkable for its simplicity and speed in software, multiple vulnerabilities have been discovered in RC4, rendering it insecure. It is especially vulnerable when the beginning of the output keystream is not discarded, or when nonrandom or related keys are used. Particularly problematic uses of RC4 have led to very insecure protocols such as WEP.As of 2015, there is speculation that some state cryptologic agencies may possess the capability to break RC4 when used in the TLS protocol. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
People that use more than simple vi or ed really deserve what they get. Damn atom. Damn vscode! All ya need is an editor. No color. No context smarts. None of that jazz. Do the code in your head and then drop it into a file with ed. For flair and special flavour you can use vi. Not vim. Just vi. And get off my lawn!
I've been using Sublime Text 3 with some linter plugins to highlight code that triggers gcc or cppcheck warnings/errors.
&gt;I'm still pretty confused about what you're trying to achieve, but I'm also realizing that I didn't read your initial question well enough. I saw AES-CTR and then that you're using OpenSSL in your code, then thought this was your AES-CTR thing and you were encrypting your counter with a block cipher. Oh that makes sense, I thought it seemed a little similar to how OpenSSL creates its EVP\_CTX cipher structs, but I've only been using their high-level EVP interface. The reason I chose AES-CTR is I wanted to cascade this stream cipher I've "invented" into AES, and I read in Applied Cryptogrpahy that a cascade algorithm is at least as strong as the strongest algorithm if they are both stream ciphers The only practical benefit of the stream cipher in the password manager at this point is that I haven't figured out how to modify password entries without the use of temporary files, and so I figured I could get away with using the weaker stream cipher to protect temporary files, and then have it still be secure afterwards because it was being cascaded into AES-CTR. &gt;So you've invented your own stream cipher. You fill some buffers with key-derived bytes, then you have two indices (ii , n ) into these buffers that loop around over and over. And you also mix it with your counter. That last part doesn't really make any sense, and the idea can't be salvaged, which is why I think you're stuck. You're fundamentally misunderstanding how this works. Plus your cipher would be very easy to break anyway. Okay, I was starting to get that sense. Essentially I just saw the diagram for a counter-based stream cipher on page 215 of Serious Cryptography and thought it would be as simple as adding a counter byte on to the usual "Simple XOR". So C1 = P1 ^ K1 C2 = P2 ^ K2 C3 = P3 ^ K3 C4 = P4 ^ K1 C5 = P5 ^ K2 C6 = P6 ^ K3 Would be... C1 = P1 ^ KS1 = K1 ^ Ctr C2 = P2 ^ KS2 = K2 ^ Ctr C3 = P3 ^ KS3 = K3 ^ Ctr C4 = P4 ^ KS4 = K1 ^ Ctr C5 = P5 ^ KS5 = K2 ^ Ctr C6 = P6 ^ KS6 = K3 ^ Ctr When I read about the nonce best being 64 or 128 bits, I wondered how that could be done, and didn't know about serialization, so I figured it must be done with a buffer of bytes equal to the desired number of bits. I guess this is functioning more as just another sub key, and the salt that I've been using in PBKDF2 is more like the IV. &gt;"Try to write your program without using global variables or dynamic allocation. You don't need dynamic allocation for this. Use fixed-sized arrays instead. Collect your cipher state up into a struct, and write your functions to manipulate this struct." Is that how OpenSSL does it? Would it be worth it to take a peek at their source to see how they're implementing various ciphers? I'm not really sure I'd understand what was going on. I have been liking Serious Cryptography so far, but wish it had some full C source examples like Applied Cryptography did. Thanks a lot for all the help and input so far. I've had a feeling that I've been really misunderstanding how this was supposed to work. But it's been a good way to learn the terminology. With that in mind before I give up on it once and for good, I'm wondering if I at least described how it should work in cryptographic shorthand well enough? C₁ = E(P₁ ⊕ N₁ ⊕ KS₁ = f(K₁ ⊕ KC₁ = g(KC₁ + 1))) C₁₀₂₄ = E(P₁₀₂₄ ⊕ N₀ ⊕ KS₁₀₂₄ = f(K₁₀₂₄ ⊕ KC₁₀₂₄ = g(KC₁₀₂₄ + 1))) C₁₀₂₅ = E(P₁₀₂₅ ⊕ N₁ ⊕ KS₁₀₂₅ = f(K₁ ⊕ KC₁₀₂₅ = g(KC₁₀₂₅ + 1))) My goal from here will be to address some of those issues you mentioned with global variables, try to cut down on malloc usage, and figure out how to get past my need to use temporary files so I can just use pure OpenSSL. Doesn't seem to a lot of sense putting much more work into this even just as a learning tool. Thanks!
Now, I'm a little bit confused. I did `gcc -v` and it had indeed `x86_64`, to be exact it was `--build=x86_64-pc-linux-gnu`. I'm using `mingw32-make` though, to compile, if that makes a difference. I tried to compile by simply replacing the 64 with the 32 bit versions. Now the error looks like this:`lib/libraylib.a(init.c.obj): In function 'vsnprintf':` `C:/mingw-w64/i686-6.3.0-posix-dwarf-rt_v5-rev1/mingw32/i686-w64-mingw32/include/stdio.h:560: undefined reference to '__ms_vsnprintf'`. I'm confused with the last part, since I didn't even have that directory. Since I didn't really know what to do with it I downloaded [that](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/6.3.0/threads-posix/dwarf/) so that at least the said directory would exist, however the error remains the same. Also, just for clarification with "install" do you mean just copy and pasting it into my work folder? 
Now, I'm a little bit confused. I did `gcc -v` and it had indeed `x86_64`, to be exact it was `--build=x86_64-pc-linux-gnu`. I'm using `mingw32-make` though, to compile, if that makes a difference. I tried to compile by simply replacing the 64 with the 32 bit versions. Now the error looks like this:`lib/libraylib.a(init.c.obj): In function 'vsnprintf':` `C:/mingw-w64/i686-6.3.0-posix-dwarf-rt_v5-rev1/mingw32/i686-w64-mingw32/include/stdio.h:560: undefined reference to '__ms_vsnprintf'`. I'm confused with the last part, since I didn't even have that directory. Since I didn't really know what to do with it I downloaded [that](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/6.3.0/threads-posix/dwarf/) so that at least the said directory would exist, however the error remains the same. Also, just for clarification with "install" do you mean just copy and pasting it into my work folder? 
Now, I'm a little bit confused. I did `gcc -v` and it had indeed `x86_64`, to be exact it was `--build=x86_64-pc-linux-gnu`. I'm using `mingw32-make` though, to compile, if that makes a difference. I tried to compile by simply replacing the 64 with the 32 bit versions. Now the error looks like this:`lib/libraylib.a(init.c.obj): In function 'vsnprintf':` `C:/mingw-w64/i686-6.3.0-posix-dwarf-rt_v5-rev1/mingw32/i686-w64-mingw32/include/stdio.h:560: undefined reference to '__ms_vsnprintf'`. I'm confused with the last part, since I didn't even have that directory. Since I didn't really know what to do with it I downloaded [that](https://sourceforge.net/projects/mingw-w64/files/Toolchains%20targetting%20Win32/Personal%20Builds/mingw-builds/6.3.0/threads-posix/dwarf/) so that at least the said directory would exist, however the error remains the same. Also, just for clarification with "install" do you mean just copy and pasting it into my work folder?
lol. No joke, my professor used wordpad. He's color blind and at the time it was one of the only editors that didn't have syntax highlighting. Since vim, emacs, and notepad++ would render most of the words invisible to him. 
I made a thread on some related topic recently: https://www.reddit.com/r/C_Programming/comments/9g0kpv/get_updated_on_new_c_standards_after_the_kr_book/ So far I've done well with K&amp;R 2nd edition and using the internet (SO, reddit) + manpages for documentation and answers to problems. I think a second reading of K&amp;R will go fine, it's not too complicated ([I've ranted about this recently too](https://www.reddit.com/r/C_Programming/comments/a4pyqx/is_the_c_programming_language_by_kr_a_good_book/ebi0by3/?context=3)).
``` g++.exe -Wall -std=c++14 -fexceptions -g -ID:\\Work\\Libs\\raylib-2.0.0-Win32-mingw\\include -c D:\\Work\\TestRaylib\\main.cpp -o obj\\Debug\\main.o g++.exe -LD:\\Work\\Libs\\raylib-2.0.0-Win32-mingw\\lib -o bin\\Debug\\TestRaylib.exe obj\\Debug\\main.o -lraylib -lgdi32 ``` 1. You're looking at the `--build` flag which is immaterial; it's evident from the sub-string `-linux` while you're running it on Windows. Look for the `Target` field; it should be something that ends with `-mingw32`. Depending on what it's, copy the library that's of the same bit width. If it's 64, use 64, else use 32. 2. Above are the commands I get from my CodeBlocks setup: so all you need other than `-lraylib` is `-lgdi32`. 3. Yes, I meant copy and pasting. That's pretty much how C++ libraries work :(
I strongly suggest you find a project and write a program. You can't learn to program by reading a book any more than you can learn how to repair a car. The practice is what is important, the book helps give you pointers along the way. Your bottleneck clearly isn't the books, it's the practice. "Scratch an itch" is the common phrase. Find a problem that you want to solve using a program, something not impossibly complex, given that you are coming back to it you might already have something. Then write the program. When you get stuck, read stack overflow, pull out your books and do some targeted learning. With a specific problem to solve you will be more engaged, absorb the information better and learn more. Repeat the last sentence until(complete). If you are completely stumped for a program then finding an open source project that you use and is written in C is a good alternative. Their bug tracker will have a list of issues that you could take a swing at and other developers to critique your work. Be aware that the first one will probably take a while to understand the code base, that is normal.
I prefer cppreference.com for this if you are an experimental programmer. Do some projects or just write console applications which serve some purpose (eg. a benchmarking code for finding algorithm runtime) , this might really help you.
I'll echo the "learn by making something" as it will help frame your reading and research. For books, I always tell people to start with K&amp;R and then immediately read _21st Century C_ by Ben Klemens. The former is the basis for everything, the latter will teach you what's important and what isn't 40 years on. 
I sat in on a developer meeting with MS folks way back when and the most popular dev tool was notepad. Not saying it was the ultimate tool but it was always open on every devs computer back in the days of NT 3.51 or so.
Yes. But it hasn't made it into gcc or clang afaik.
And maybe you are running on a 64 bit computer or maybe you aren't. Or maybe you are using a long endian or maybe you aren't. Or maybe you are using a binary computer or maybe you aren't. I use %p. &amp;#x200B; There is no battle, and how is using headers not a standard way? 
I guess I am just expecting that a 32-bit pointer will always print with a set width and a 64-bit point will always print with a consistent width but leading zeros are not printed. Thus a stack address looks like 0xFFFFFFFF7FFFF490 whereas heap looks like 0x100101A70 and those are not the same width at all but both 64-bit. I'll hunt around and see if I have a 32-bit linux box handy that is little endian.
Use `%p` and cast to `void *`. Am I missing something?
I was just looking for consistency. Sometimes you get a leading "0x" and sometimes you don't. As I commented below there are systems where you don't get leading zeros on an address and thus 0x100101A70 is a 64-bit ptr but doing a series of printf's with heap and stack gets messy as hell. 
[Here](https://github.com/Skyb0rg007/doit4me) is my attempt. It's a shared library, not a plugin, but the essence is the same. Please excuse my terrible coding style. Hopefully pastebin doesn't hate me for testing this because I triggered their spam filter a lot.
This was for my Operating systems course
can cast to void * unless it's a function pointer
back to k&amp;r2 :)
I think this is what I'm supposed to do... I'm a bit confused about what you said.Are there any reference materials to go through to understand this? Thank you!
GDB is run on the executable to be debugged, not individual source files. If the executable adequate debug information, GDB will load source files as required to provide listings.
I've been a member of the noconst revolution since I read K&amp;R2 and POP. sorry bud, but that keyword just ain't worth the hassle to me hehe
Modern C by Jens Gustedt. it's free, look it up.
It's already been answered, but to follow up; if you compile a C program using multiple files (headers), it collates everything into the executable. You can actually still make breaks in other files or "until" lines in other files by typing the filename first. I.e `break secondFile.c:255`.
You might want to supply some more information. What are you trying to do. What code are compiling etc.
&gt; Another question: what if I have one patch and it doesn't align, do I have to manually edit the patch or can I do it automatically somehow? `patch` will attempt to find where a particular hunk should be applied, even if the line numbers aren't correct. It can also deal with a certain degree of incorrect "context". Take a look at the description of "fuzz" in the `patch(1)` manpage for details. Only if all that fails will the hunk be rejected. Other hunks in the patch may still be able to be applied.
Interesting, I'll check it out. Thanks!
King's C book still a great book (2008), lots of questions, verbose as a class teacher but well paced!! 
https://idownvotedbecau.se/itsnotworking/
Huh? Either the binary is x86_64 then it will only run on 64-bit and the address will be 64-bit or it's x86_32 and it will run on both 64-bit and 32-bit but the address will still be 32-bit... . You can specify the field with, like this: printf("0x%16p\n", (void*)foo); or if you want to be independent of the architecture: printf("0x%*p\n", sizeof (foo) *2, (void*)foo); You can possibly even do that using macros in compile-time instead of passing another parameter to printf.
Oh, if you always want the same prefix, just write it yourself (yes, that's the way you do it, I like it): printf("PREFIX%p\n", (void*)foo); Where PREFIX can be 0x of course.
BDS and its famous C packages: Tmux...
Hey /u/gordon2222, this subreddit is about programming in C. Your post is not really about C so I removed it.
Emacs...
Emacs + Eglot
You can always do `%0.8p` or something like that to enforce leading zeroes.
Note that on some platforms, only `%p` is really useful for printing pointers. For example, on the 8086, `%p` might print a pointer as `1234:5678` which is the useful form for a programmer whereas `%u` might only give you `0x96696` which discards information about which segment the pointer is in.
So you question is about modularity rather than c files That is a big and important topic
You got something known as a *merge conflict.* This happens from time to time. There is no automated way to resolve this conflict in its entirety, manual work is likely required.
The best -- and potential only -- way is to do it manually. Hopefully you already have tests in place that reduce the chance for regressions in your software; those are really important to combine work of multiple people well. 
Related Question :How do people print e. g. a `size_t` in a way that works both on 32-bit and 64-bit?
Something like this? [http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch10s07.html](http://www.cs.uwm.edu/classes/cs315/Bacon/Lecture/HTML/ch10s07.html)
This book might be getting hit by fake copies on Amazon.
one written by you hehe
That's even more confusing..Maybe because I'm new to this Can you tell me how function g gets its value assigned to the same place function f returned(2)??
how is it not related to c its related to compiling &amp;#x200B;
Nothing in your question indicates so. Please add more details so it is apparent what you are trying to do. Right now, your question is an error message without any details as to what produced it and how. That's just impossible to answer.
On Linux (glibc, musl, etc.) and BSD, the `0x` is printed automatically so your example ends up looking like this: 0x 0x7ffe3f22a8ec MSVC doesn't print the 0x, so there it looks like this (notice that it's capitalized): 0x00007FFE3F22A8EC Here's what it looks like on UNIX System V (Solaris, etc.): 0x 7ffe3f22a8ec All this variation is just for one architecture (x86-64). `%p` is irritatingly inconsistent everywhere. 
Number. As in "number of bytes allowed provided"
I do like reading these types of archeology, but I've never considered the RANDOM function of a shell in the least bit important -- if I use it, it's more of a o e-off gimmick. Is there anything important it's used for?
Let's start with a different question -- where is this coming from? If it's a class, your instructor should have provided notes and a source, right?
This was on my operating systems course past paper We have done call stack. but only the basics 
You've got the right idea. In practice, `PRIxPTR` is `"x"`, `"lx"`, or "llx"`. You won't get leading zeros by default, but you could always add your own field width specifier. The standard keeps it pretty simple (C99 7.8.1-1): &gt; Each of the following object-like macros expands to a character string &gt; literal containing a conversion specifier, possibly modified by a &gt; length modifier, suitable for use within the format argument of a &gt; formatted input/output function when converting the corresponding &gt; integer type. 
Use the `z` length modifier from C99: printf("%zu\n", sizeof(long));
yeah thats how I remember it!
&gt;o e-off gimmick. What does that mean?
"one-off gimmick". Sorry, writing on a phone. 
I found this really nice repository on GitHub that links to a lot of project-based tutorials. They have a bunch for a ton of other languages, but their C ones I have found to be really good for actually getting your hands wet with a real C project. https://github.com/tuvtran/project-based-learning/blob/master/README.md#cc I have worked through the writing your own text editor in C and writing your own shell. They're both pretty easy to follow through, and then once you've completed the tutorial, you've got a great base to add new features, which will help you learn even more! Happy coding!
Lol can't argue with that. I mean it's got: ASCII support, 1 instance per file so you don't have to worry about losing all your work at one time, optional word wrap support, you can browse for files, and you can use tabs for formatting. Did I mention you can find in both up and down directions* (* can only search 1 direction at a time, no wrapping back to the top/end of file).
`s` = string (or rather, memory buffer), not stream.
Mhm nice :) thank you for your input!
I am learning emacs now, so thank you! :)
code::blocks? :P haha
Cool and all but why doesn’t the shell just read 2 bytes off of /dev/urandom? Is it that much faster?
I would hazard a guess: `/dev/urandom` might not exist where the bash instance is running. It's also peculiarly a 15-bit number. Whilst the latter is trivially surmountable, the former could have a ton of edge cases.
It's likely legacy stuff - I know I've used $RANDOM in the past to just quickly grab some random number for throwaway purposes. One of the apt scripts that cron calls daily on Linux systems has a sleep derived from $RANDOM
It's likely legacy stuff - I know I've used $RANDOM in the past to just quickly grab some random number for throwaway purposes. One of the apt scripts that cron calls daily on Linux systems has a sleep derived from $RANDOM to stagger calls to mirrors. For this sort of thing it is "random enough". Funnily enough, there is this line inside the function that alludes to what /u/0zeronegative was suggesting before, precisely for shells that don't have $RANDOM: RANDOM=$(( $(dd if=/dev/urandom bs=2 count=1 2&gt; /dev/null | cksum | cut -d' ' -f1) % 32767 ))
It's likely legacy stuff - I know I've used $RANDOM in the past to just quickly grab some random number for throwaway purposes. One of the apt scripts that cron calls daily on Linux systems has a sleep derived from $RANDOM to stagger calls to mirrors. For this sort of thing it is "random enough". Funnily enough, there is this line inside the function that alludes to what /u/0zeronegative was suggesting before, precisely for shells that don't have $RANDOM: RANDOM=$(( $(dd if=/dev/urandom bs=2 count=1 2&gt; /dev/null | cksum | cut -d' ' -f1) % 32767 ))
Gonna add my 2 cents here. I think if you have a line longer than 80 or so chars you would be trying to squeeze as much into a single compact statement which is not that bad, but most of the time you can just decompose it into few separate statements before so they are easier to read, debug and understand. Remember that compilers are much smarter that you and it will most likely perform the same but without "those sick oneliners i came up with".
(a)/(a+d) to scale between 0 and 1 for attack a and defense d. Multiply this with the weapons base damage. Multiply this with something random between 0.5 and 2 for example. 
If "decompiling" means get back on source files from the binary one, you can use IDA (or radare2) plus Hex-Rays to get a C code representation of the dll but don't except an easily readable code. You need a fair share of target architeture knowledge to be able to reverse engineering it in a "good enough" way, though.
snowman [https://derevenets.com/](https://derevenets.com/)
code::blocks? pfffsh, that's nowhere near as good as your one :P
[challenge accepted](https://github.com/Skyb0rg007/doit4me)
narrrrr, notepad, a cmd window, and mingw jesus was a breatharian, he realised that less is more :D
OKay, whatever the Solaris thing does is most likely correct as that one is the standards compliant machine whereas everything else is just whatever you get is what you get. However I'll hack together a function to return a char* to me from some arbitrary value and always get a consistent looking result like : "0xffffffff7ffff2d0". I am still on the fence about the leading "0x" which is optional really. Maybe a switch for uppercase or lowercase so that I can get "0xFEEDBEEFBADCAFFE" type output. 
Also, `a` = allocate as in [`asnprintf(3)`](http://man7.org/linux/man-pages/man3/asprintf.3.html)
When in doubt check the standard. Always a good idea. Or just check all code on a Oracle Solaris boxen with c99 which sorts out junk right quick. All manner of goodness in there. I see "7.8.1 Macros for format specifiers" and that pretty much clears it up. I may hack together a function that returns a char* with a reasonable uppercase hex representation where I always get the leading zeros and always get a sane width on 32-bit machines and 64-bit machines regardless of endianess. Just for giggles : int eflag = 1; /* in mem 0x00000001 big endian */ eflag = (*(uint8_t*)&amp;eflag == 1) ? 1 : 0; printf("eflag = %i\n", eflag ); That does a fine job of telling me the endian nature of the blinken lights boxen. 
Actually that won't work. On a 32-bit arm7 boxen you get a double 0x0x output.
Right .. tried that and sometimes you get the leading 0x and sometimes you don't. 
Since this sounded like fun to write: An alternative for the paranoid who doesn't trust `PRIxPTR`. Byte order agnostic without needing to detect it. #include &lt;stdint.h&gt; // uintptr_t #include &lt;string.h&gt; // memcpy() #define PTR_PRINT_PREFIX (1 &lt;&lt; 0) // prefix with 0x #define PTR_PRINT_UPPER (1 &lt;&lt; 1) // upper case hex #define PTR_PRINT_NIL (1 &lt;&lt; 2) // use "(nil)" for NULL /* Write a human-readable hex representation of at least WIDTH hex * bytes to BUF. Returns BUF. */ char * ptr_print(char *buf, const void *ptr, int width, int flags) { static const char lower[16] = "0123456789abcdef"; static const char upper[16] = "0123456789ABCDEF"; const char *hex = flags &amp; PTR_PRINT_UPPER ? upper : lower; if (!ptr &amp;&amp; flags &amp; PTR_PRINT_NIL) { memcpy(buf, "(nil)", 6); return buf; } /* Extract bytes */ int len = 0; uintptr_t n = (uintptr_t)ptr; while (n) { buf[len++] = hex[n % 16]; n /= 16; buf[len++] = hex[n % 16]; n /= 16; } /* Prepend prefix */ while (len &lt; width * 2) { buf[len++] = '0'; buf[len++] = '0'; } if (PTR_PRINT_PREFIX &amp; flags) { buf[len++] = 'x'; buf[len++] = '0'; } buf[len] = 0; /* Reverse */ for (int i = 0; i &lt; len / 2; i++) { int tmp = buf[i]; buf[i] = buf[len - i - 1]; buf[len - i - 1] = tmp; } return buf; } 
If you know Vim binding you can use either spacEMacs or Doom Emacs... Two powerful Emacs frameworks! &amp;#x200B; [spacemacs.org](https://spacemacs.org) [https://github.com/hlissner/doom-emacs](https://github.com/hlissner/doom-emacs) &amp;#x200B; If you like Emacs bindings there is Prelude Emacs &amp;#x200B; [https://github.com/bbatsov/prelude](https://github.com/bbatsov/prelude) &amp;#x200B;
Vim has a better choice for C/C++/ObjC development: LSP servers Vim + Cquery/Ccls/ClangD &amp;#x200B;
Emacs has now packages to connect LSP server: Eglot, LSP-mode
well golly gee ... you beat me to it .. I was just ... char* ptr_printf ( uint8_t* v ) { /* consistent width upper case hex address from * an n-bit value in v such that we return a * string like 0xFEEDBEEFBADCAFFE ffffffff7ffff2d0 */ int eflag = 1; /* in mem 0x00000001 big endian */ eflag = (*(uint8_t*)&amp;eflag == 1) ? 1 : 0; fprintf ( stderr, "DEBUG : eflag = %i\n", eflag ); . . . wasn't sure what came next .. was interrupted by all manner of things. I am going to go with your solution there ( for fun .. yep .. I get that ) and see what I see on a mixture of systems. 
Oh, well it isn't for `%x`, annoying that it is for `%p`. And I forgot to say that it should be left-padded by zeroes, that's right. Then it would look correctly (at least on a true UNIX). Damn GNU
If you want to go the standards route, Solaris is definitely the best.
Hasn't to do with arm, more that the GNU guys have some weird ideas.
Why does consistency matter? This value should never be seen by end-users of whatever this is running on -- this is strictly for debugging/development.
.8 only specifies the maximum digits, but it's non-standard to specify precision and flags because %p is implementation-defined, e.g. the GNU guys decided to use `(nil)` to represent a NULL pointer...
ha .. yeah .. well that isn't the first time someone has uttered those words. Won't be the last either.
When multiple terminal windows are lined up across multiple monitors it is easy to glance around and see weird output when ALL the output from all systems and architectures are consistent in format. 
Just a pita to deal with Oracle these days. I wonder if the FreeBSD guys have a POSIX compliance sticker on the tin? No idea.
Nah, most BSDs aren't POSIX certified. Oracle did much shit in general, I wouldn't trust them at all.
this man walks the walk 
Their testing suite is the best I've seen 
Oracle is like an Ironman movie : looks great on the screen and fun to watch but doesn't ( and shouldn't ) exist in the real world. Feels like a modern ENRON situation where they keep selling cloud this and cloud that and no one ever sees anything real. 
That sounds like an easily gotten around problem -- you aren't comparing pointers across different machines, and certainly not different architectures. 
oh really ... okay. 
Interesting, so this would hook into your application and send out stack traces and stuff? I wonder if there's a way I could put this in a container of sorts like how gdb is able to intercept signals.
&gt; By default it returns non-deterministic, cryptographic-quality results seeded from system entropy (via the misnamed arc4random(3)), à la /dev/urandom. OpenBSD's libc arc4random (and rand/random) internally use the descriptor-less `getentropy(2)` system call, not /dev/urandom. Ted T'so attempted to "borrow" this from OpenBSD for Linux's semantically broken getrandom syscall, however they failed to get it right. OpenBSD's `getentropy` never blocks, and always provides high quality random numbers. Note: `arc4random` on OpenBSD actually uses the ChaCha20 stream cipher, the name is historic, first introduced in 1996.
I've only ever heard of this ans noticed that many C++ programmers tend to put the cost on "the west". I've not seen real C codebases where such an insanity has taken place.
Try vimtutor to get the basics down. I use vim with makefiles to manage my c projects
Manual merging is much easier with a merge tool like http://meldmerge.org/ or `ediff` mode in Emacs. There are tons of these tools out there, search for "three way merge tool" on your favorite search engine and you will find lots of options. I really love the one included in JetBrains products like CLion or IntelliJ, it is very nice.
Author of the article here. "à la" means "in the style or manner of", not "by means of". So I'm saying `arc4random(3)` is *in the style of* /dev/urandom — a securely-seeded CSPRNG.
Author of the article here. I use `od` [in my own scripts](https://github.com/skeeto/dotfiles/blob/master/bin/rand64): printf '0x%s\n' $(od -An -tx8 -N8 /dev/urandom) I wanted to know what was behind various implementations of `$RANDOM` so that I could understand its limitations and, in a pinch, I could use within reason.
There's not even a guarantee that a pointer can be represented as a single number. There are architectures with memory that's only word-addressable and pointers are structures with a word address and an offset. Not that you're likely to run into one of those, but just be aware there's a limit to how "standards compliant" any printout of a pointer is going to be.
Neat, so what does this have to do with C programming?
 end-&gt;next = node; You forgot to change what next is pointing to.
well, next is pointing to the successive nodes.
End is set to start and then never updated.
each node's next element needs to point to the succeeding node, but that doesn't happen while setting up the list in the code you provided. an easy way to compose linked lists is to first write a function that allocates storage for a node and initialises it struct integer_list *new(int n, struct integer_list *next) { struct integer_list *p; p = malloc(sizeof *p); if (p == NULL) { &lt;handle error&gt; } p-&gt;num = n; p-&gt;next = next; return p; } a list can then be created with the following composition start = new(1, new(2, new(3, new(4, NULL)))); and traversed ... for (node = start; node != NULL; node = node-&gt;next) printf("%d\n", node-&gt;num); bon appetit!
[Pointers on C](https://www.amazon.com/Pointers-C-Kenneth-Reek/dp/0673999866) is the best intermediate level C book I've read, but it's really hard to get for a reasonable price, and not exactly modern. But you really will learn more just diving in with a project. Debugging your early segfaults will give more understanding about pointers and memory than you can passively absorb from any book in my opinion. 
Do you know the glibc extension on printf family ? xprintf sometimes called. %p (or any format letter) can be overridden by user supplied formatter function. Not a recommendation, of course.
&gt; Can't google documentation to this directive, [Intel SDM](https://software.intel.com/en-us/articles/intel-sdm), Volume 1 section 6.3 "Calling Procedures Using CALL and RET", and Volume 2 section 3.2 "Instructions (A-L)", subsection "CALL—Call Procedure".
Call is used to call a function. If you google "x86 call" you will get some meaningful results. The actual calls are finalized at linking time, so the address for the next instruction is used as a placeholder on what to call. If you want to get more information, add the -r switch to your objdump call to see relocations. You can also read the .s file directly, which has a bit more information than disassembling the object file.
If you leave out the -c from the second command, you see what is called more clearly e8 ee ff ff ff call 80485ab &lt;FooDummy&gt; &amp;#x200B;
Some small critiques: - `printf(USAGE);` is generally a bad idea; `USAGE` could contain a % somewhere, which would be interpreted as a specifier for a printf argument. Use `printf("%s\n", USAGE);` or `puts(USAGE);` instead. - I dislike `!strcmp(a, b)`; I think writing `strcmp(a, b) == 0` is more clear.
thank you! I was trying to google something like "NASM assembly directive CALL documentation" but it was a dead end. 
Call is an instruction, not a directive and it's not nasm specific. That might have had sufficiently confused Google.
See, _even if_ this were a NASM-specific thing (which it isn't), Googling for something like that is silly. NASM has official documentation. It's a lot simpler and faster to just go to this official documentation and look at the table of contents. I repeat: search engines are at their most effective when you want to find something _not_ covered by documentation.
I'm going to fire that right back at you: if you think locating the NASM documentation and then finding and opening the correct chapter is faster than using Google, you don't know how to use Google.
It looks nice. Too much ornaments on the spruce though :) &amp;#x200B; Here some old style X11 snow, from your idea: [https://pastebin.com/VbFpRarU](https://pastebin.com/VbFpRarU)
Probably best to focus on being helpful. Everybody gets stuck sometimes.
Inspecting the .s (unassembled compiler output) file, particularly if compiling with -ggdb or similar compile flag can be *very* instructive. It's often very useful to start with -O0 as well, and then watch how the emitted code changes when you switch to a "production" optimization (some choice of -O2 or -O3 and various flags to go with) flag scheme.
Really. If you're doing this across systems for debugging, you either print the value the pointer is pointing to, or an I'd tag of a struct for that purpose. Don't get me wrong -- I've also had to resort to tracing pointers for some poor convoluted logic I had made, but I wouldn't try to simultaneously track those pointer values across machines (since they are worthless). Typically, I would run with different levels of logging output at different times. 
Thanks!
haha are you bamboozling me? :D
Thank you!
Utility doesn't always follow speed.
Appreciate the clarification, thanks.
Best of luck with the job hunt, Charles!
How much time did you spend writing this?
The preprocessor took about 10 days - I wrote that back in May. The compiler was about three weeks. The assembler/linker/driver were a total of a week, I'd guess. &amp;#x200B; &amp;#x200B;
Thanks! I've spent the last year looking for something "appropriate" with no luck -- most gigs seem to be in the enterprise C#/Java/web space, which doesn't interest me. I'm probably going to end up getting my CDL and driving a truck for a living ... no joke! &amp;#x200B;
Out of simple curiosity, what made you target "classic" C as the input language, rather than say, C89/ANSI C?
 I would think writing a C compiler demonstrates a high level of competence and you should be in high demand. All I hear is that there is a shortage for people in software. Yet you are struggling to find a job. As someone who also wants to get into the embedded scene, it's a bit discouraging.
The *short* answer is that ANSI adds very little in terms of functionality, while adding a lot of red tape to the front end. About the only thing possible in ANSI that is not in K&amp;R is struct assignment (which includes passing structs as arguments to and return values from functions).
Have you tried applying to semiconductor companies? Intel, AMD, Arm, Qualcomm, Google, Amazon, Apple, Nvidia, and lots of smaller companies would be looking for somebody like you.
I have a feeling it's not easy cause the older generation of developers isn't old enough to retire and jobs in embedded development isn't in increasing demands. I'm interested in getting into the scene also, but it seems to require quite a lot of experience and I don't know where to get it.
Asking for help with your cs50 assignment?
I'm not a cs50 student, like most doing the online course, I'm just using it as a learning tool. But, yes, it is help on the cs50 assignment, as I'm hard stuck.
I'm a bot, *bleep*, *bloop*. Someone has linked to this thread from another place on reddit: - [/r/compilers] [Introducing NCC, the "new" C Compiler](https://www.reddit.com/r/Compilers/comments/aa0gjk/introducing_ncc_the_new_c_compiler/) &amp;nbsp;*^(If you follow any of the above links, please respect the rules of reddit and don't vote in the other threads.) ^\([Info](/r/TotesMessenger) ^/ ^[Contact](/message/compose?to=/r/TotesMessenger))*
Don't be too discouraged. I'm at a disadvantage in a few ways: my degree is in mathematics (not engineering) from a crappy university, I'm older (41, specifically), I live in the sticks (South Carolina) and I'm not willing to permanently relocate, and I spent the last decade or so at a web startup that was a dead end. This is a very tough sell for "serious" programming jobs -- I imagine my resume (in its many revisions) hasn't even made it past preliminary HR screening in most cases. If you play your hand a bit better, I'm sure you'll have plenty of opportunities.
I'm 40 and have been in web development my entire professional career. I'm kinda burnt out and looking for a change. Plus I don't like the direction web development is moving in general. If embedded systems doesn't work out, maybe i'll get into independent game development or something...
``` FILE *inptr = fopen(infile,"r"); if(inptr == NULL) { fprintf(stderr, "Invalid infile\n"); return 2; } ``` it is a good practice to say why input file cannot be opened. `strerror(errno)` can help here. ``` int scanFile(FILE *inptr) { unsigned char *curImage = malloc(sizeof(unsigned char) * 512); ``` That `sizeof` is no longer needed in case of `char` and `unsigned char`. You also don't check if `malloc()` returned `NULL` (out of memory). ``` fread(curImage, sizeof(char), 512, inptr); ``` `fread` may read less than requested number of bytes. This is **very** important when writing read data back to file. ``` FILE *outptr = fopen(fileName,"a"); if(outptr == NULL) { fprintf(stderr,"Ran out of memory.\n"); return 3; } ``` `NULL` returned by `fopen` does not have to mean you ran out of memory. You should consult `errno` to see why this function could not open the file. ``` //Write what was read. fwrite(curImage, sizeof(unsigned char), 512, outptr); ``` You are not checking for write errors and writing entire buffer content, no matter how many bytes were read by `fread`. Expect junk in output file. ``` //Read next 512 Bytes. fread(curImage, sizeof(unsigned char), 512, inptr); ``` Try reading in a loop. ``` //Check if those 512 Bytes were the start of a new image. if(curImage[0] == 0xFF &amp;&amp; curImage[1] == 0xD8 &amp;&amp; curImage[2] == 0xFF &amp;&amp; curImage[3] &gt;= 0xE0 &amp;&amp; curImage[3] &lt;= 0xEF) { fclose(outptr); }else { fwrite(curImage, sizeof(unsigned char), 512, outptr); } ``` Are you sure the size of JPEG file is exactly a multiply of 512 bytes? If not, read data may not start at file boundary. 
I apologize, I'm reviewing this on a phone, so I'll be brief -- I'm not sure what you're trying to accomplish with the repetitive if-else statements in which you are reading subsequent 512-byte chunks of the file. It seems a bit heavy-handed, and at first glance, I bet we can pinpoint your issue down to that. Could you briefly explain your image processing algorithm requirements? Maybe we can help you reorganize your control flow a bit.
I'd work on modernizing your style. I work on toolchain at one of the tech giants and wouldn't want somebody trying to write 1976 style code in our code base.
Check out Pivotal and specifically their Greenplum product. I have a friend who has a fairly similar profile to you that enjoys coding C for them remotedly. I don't know the fineprint specifics though.
Have you looked into WebAssembly? C could be pretty useful in the near future for fast compact libraries.
Cool! Can it build any existing libc's and can it build any other C compilers?
jesus christ that's fast. You're talented
Point taken - the compiler needs to compile itself, though, which is why the style is the way it is. This is an academic exercise, not an attempt to replace GCC or LLVM.
The file I'm reading from is FAT, so the JPEG's are always at the start of 512 Byte blocks. The file in question is perfectly divisible by 512, so I don't think fread returning less than 512 is a huge issue here, as it should theoretically, always read 512, to the best of my knowledge. I'm trying to read a 512 Byte block, and if that block was the beginning of an image, write all of that data to the output file. Then, keep writing every 512 Byte block to the file, until a new JPEG is found, in which case, close the file. That's what I'm attempting at least. :(
Brace yourself : this is someone else's requirement that the output look consistent across a few risc boxen as well as ye old x86_crap boxen.
I was given a FAT file filled with random JPEG images. What I'm trying to do with my code, with varying degrees of success, is read 512 bytes of data, then, see if that block contained a JPEG. If it doesn't, just move on to the next block of data, until you eventually find a JPEG file. Once a JPEG is found, write every block to the file, until a new JPEG is found, in which case, close the file, and start making a new one. The program works in that it detects all 50 images in the file, and outputs 50 files. From what I can tell, it's outputting the first block of data correctly to each JPEG, then filling it with garbage. Also worth noting, all data not relevant to the JPEG are 0x00, so I don't need to handle anything too complex with padding.
Pretty much every company developing space hardware is looking for good systems programmers. &amp;#x200B; I say this as someone currently writing C / C++ / Java in a UK space startup. The latter took a weekend to learn (syntax-wise) and a few weeks more for tooling + common libraries + common pitfalls.
Got it... I think I'm wrapping my head around the assignment and coming up with a better way to organize your code... One last thing -- how are you determining that you are at the end of a jpeg? If I'm reading this correctly, you're checking the current chunk to see if it's a new jpeg, and if not, assume it's part of the last one, correct? Any other markers?
I'm basically doing just that, since I know each JPEG in this assignment is stored in the block directly after the last, I simply check if a block is a new JPEG. If it's not, I'm safe to write that block into the JPEG I'm currently editing. If it's a new JPEG, I know that the JPEG I was editing is done, and close it. 
I'm not making fun of you or trying to be rude, I'm just saying I remember these sets of problems and I'm having ptsd flashbacks. The later ones are worse. Sadly my cloud9 I used when I took the course ia closed.
Hm? You have a raw FAT partition dump? I do not know that filesystem, but I believe there are some structure data along with file data and files can be fragmented. Anyway, carving filesystem blocks into JPEG files directly does not seem to be the correct way. Even if FAT block size is 512 bytes, formatted files (like JPEG) are not filesystem block-based. You are not copying data from device to device, but from device (in this case - a dump) to file.
The best dialect of C in many ways was the one processed by early versions of gcc. Support for prototypes and // comments, and most importantly, the Spirit of C including "Don't prevent the programmer from doing what needs to be done". While there would be some advantage to having a recognized subset of the language which could be processed by a "single-shot" C-to-assembly compiler, and while it may not be practical for such compilers to support statement expressions or compound literals, such constructs should not pose any difficulty for compilers that build a syntax tree for a function and can analyze it before having to generate code. Some aspects of the C Standard may have been intended to facilitate 'simple' implementation, but end up making things more difficult for many implementations as well as less useful. As a simple example, tokenizing something like `1.23E-E` as `1.23E - 5` and then having a compiler peek at the next token following `1.23E` to see if it's a `+`, `-`, or a number, would eliminate any need for the preprocessor to know or care about exponential format, but instead the Standard requires that the preprocessor effectively ignore a `+` or `-` sign that immediately follows an `E` or `e` within a numeric constant--even a hexadecimal one. I'm not sure what ncc does, but I'd admire a compiler writer who acknowledges the Standard's rule as silly. 
I believe it, the courses learning curve seems to be particularly insane, if you take it online, without help. week1 1 was literally just making a pyramid with printf.. and this is week 3. It's daunting, to say the least.
The key thing here is simplifying your read/write algorithm so you're only reading from one place in your code. The purpose of this is to make your code more maintainable, and easier to debug. The thought behind this is as following: 1. While we can still successfully read 512 bytes... 2. If we detect a new image, close the previous one (if it exists) and open a new file. 3. Write the bytes that we've read to whatever file we currently have open. 4. When we can no longer successfully read 512 bytes, do error handling and file cleanup. That pretty much sums up the logic, correct? With that in mind, here's what I have.... Please note: this is quickly-thrown together half-C, half-pseudo-code. It doesn't contain (robust) error handling, and will likely not compile, but it's enough to point you in the right direction. Actual implementation (and error handling!) is left as an exercise for the reader. /* Helper function to clean up code a bit... * returns 1 if chunk is beginning of a new jpeg, and 0 otherwise * Note: I'm not crazy about this, but if it works, it works, and * you can replace it or add error handling later. */ int checkNewJpeg(char *chunk){ return (curImage[0] == 0xFF &amp;&amp; curImage[1] == 0xD8 &amp;&amp; curImage[2] == 0xFF &amp;&amp; curImage[3] &gt;= 0xE0 &amp;&amp; curImage[3] &lt;= 0xEF) } unsigned char *curImage = malloc(sizeof(unsigned char) * 512); FILE* outptr = NULL; int i = 0; // fread returns 0 if nothing is read, which breaks the while loop while(fread(curImage, sizeof(char), 512, inptr)){ // if we've detected a new jpeg... if(checkNewJpeg(curImage)){ // .. check to see if there's an open file, and if so, close it if(outptr != NULL){ fclose(outptr); } // open/create a new file. (I've omitted the filename string creation) outptr = fopen(filename, "a"); } // write buffer to the file fwrite(curImage, sizeof(unsigned char), 512, outptr); } free(curImage); // if we have an open file pointer, close it if(outptr != NULL){ fclose(outptr); } // error handling if(ferror(inptr)){ // print some debugging info fclose(inptr) return 1; } Does this make sense? Do you understand how the control flow logic can be vastly simplified in this regard?
Your input is something \_else\_ than concatenated JPEGs ? They would be random length, not nicely aligned to 512 bytes.
At that point, converting to anything -- for example, the value converted to a 32-bit int -- should suffice, since it isn't being used for anything important. 
The biggest issue with the first program for me and the friends I took it with was that I don't think they gave you the hex numbers, I had to go on their reddit and find somwone else's example with the magic numbers on them. Have you looked at r/cs50 ? They have a lot of asked and answered questions about the problems there, and it was the only way I got through the end of the C part. Also some future advice: stop at the javascript part. They don't explain anything they use at all and I had to blindly stumble through html and css mixed with js using flask and then their own provided functions that grab stock stats broke so I couldn't get most of the points. After that, they expect you to interface with Google API they don't explain at all and reinvent google maps or some stupid crap. Everyone just accepted a failing grade on these last two projects and moved on.
If you’re looking to turn this into something more interesting, my university is looking at energy efficient computing at the compiler level (like converting an int to an unsigned char if it never goes above 100 for example, or converting a range of negative numbers to positive ones to avoid using 2s complements). Could be something that people actually use. 
The Rationale makes clear that when the Standard uses the term "Undefined behavior", the intention is to allow compiler writers to process a construct in whatever way they see fit, with the intention that compiler writers would interpret it as an invitation to exercise judgment, not as an indication to throw judgment out the window. The following quote appears in the context of translation limits, but is just as applicable here: "While a deficient implementation could probably contrive a program that meets this requirement, yet still succeed in being useless, the C89 Committee felt that such ingenuity would probably require more work than making something useful." If an implementation uses different representations for different kinds of pointers, it would be useful for it to process various flag characters as indicating that different types of pointers are being passed. It would hardly be implausible that using incorrect flag characters on such an implementation might cause arguments to be fetched off the stack incorrectly, with unpredictable results. If an implementation for 8086 small model uses `L` as a flag chracter for printing "far"-qualified pointers, both the following would have defined behavior on such an implementation: void *p = 0; void far *pfar = 0; printf("%p %Lp %s", p, pfar, "Hey there!"); but neither of the following would [assume same declarations]: printf("%p %s", pfar, "Hey there!"); printf("%Lp %s", p, "Hey there!"); The use of flag characters other than those mandated by the Standard is classified as UB rather than IDB because there's no requirement that implementations define the behavior of any flags beyond those given by the Standard, and the Standard only requires that implementations document things that are required to exist. The notion that the Standard intends UB as an invitation to infer that a program will never receive certain combinations of inputs, and thus something that programmers must avoid at all costs, is unsupported by anything I've found in the published rationale documents. The Standard would allow for such treatment, just as it would allow for a conforming implementation that was incapable of meaningfully processing *any* useful programs, but in no way implies that such treatment should be viewed as sensible, nor that programmers should not expect quality implementations to behave sensibly when practical. 
You're right of course, it's unlikely that printf will dramatically fail, usually the implementations do interpret it as intended (if possible), however it will not result in consistent behavior across the systems as intended by OP.
 while (fgets(string, sizeof string, fp1)) { char *p = string; int i; while (*p) { for (i = 0; i &lt; 2; ++i) if (!strncmp(p, word[i], strlen(word[i]))) break; if (i &lt; 2) { fputs(replace[i], fp2); p += strlen(word[i]); } else { putc(*p, fp2); p++; Merry Christmas!
You're not alone. I know someone currently driving a truck for money, even though their education was in math/programming.
In the language the Standard was written to describe, any object value may be decomposed into a sequence of numbers in the range 0..UCHAR_MAX, and copying any sequence of numbers that has been yielded by such action into an object will cause it to yield the original value. Thus, one could write a universal "output pointer" function whose behavior would be defined as outputting a sequence of numbers on all conforming implementations where it wouldn't violate a translation limit: void outPointer(void *p) { unsigned char *pp = (unsigned char*)&amp;p; printf("(%d", *pp); for (int i=1; i&lt;sizeof p; i++) printf(".%d", pp[i]); printf(")"); } The Standard doesn't require that the byte sequence for pointers have any meaningful relationship to the addresses represented thereby, nor even that all pointer objects holding the same address contain the same sequence of bytes. Such code, however, would have defined behavior except when it would cause a translation limit to be exceeded (in which case all bets would be off). Note that judging from some documents written about "pointer provenance", there seems to be some question as to whether the Standard requires, or should require, that implementation allow for the possibility that any pointer value which has been decomposed into a sequence of 0..UCHAR_MAX values could be later assembled out of a sequence of numbers which happen to be match the originals, but are not visibly derived from them. While it might be useful to have a category of implementations which are suitable for systems programming and allow for such a possibilities, and another which is intended for high-level-only tasks which does not, the authors of the Standard seem disinclined to make such distinctions. 
I actually wrote the C preprocessor for a different project and is it strictly ANSI compliant, so it does in fact use the standard definitions of preprocessing tokens (including preprocessing numbers as you describe). I agree that it's dumb. That's far from the worst of what the ANSI committee came up with, though. Consider, e.g., type qualifiers. **const**, for example, has effectively no use except to express policy and hold the programmer to it (contrary to popular belief, it is of *no value* in optimization). The official rationale the committee gave for including it was basically "C++ has it" \[1\] (although ironically, C++ **const** has always had different semantics), and even dmr thought it was of dubious value though he gave up trying to get it tossed out \[2\]. That's not all -- the committee went beyond their charter in quite a few ways, and in *most* of those ways, they got it wrong, as committees often do. &amp;#x200B; \[1\] [http://gnuless.org/rat/c5.html#3-5-3](http://gnuless.org/rat/c5.html#3-5-3) \[2\] [http://gnuless.org/dmr-on-noalias.html](http://gnuless.org/dmr-on-noalias.html) &amp;#x200B;
This subreddit is about C only. Please post JS content to other subreddits, but not to this one.
I hear embedded has a big demand/shortage of people over in the EU
Right, but 100% of what you've posted here attached to your request for references shows horribly antiquated code style. As somebody who makes decisions on bringing in new employees to a toolchain team, I'm impressed by the body of work and the technical know-how but the thought of dealing with a guy stuck in the 70s sounds awful. I'd do something else in modern C++ clang-formatted and clang-tidied to LLVM's standards and put it up on your Github. Maybe rewrite the linker in C++17 or something that shows that you're not stubborn and stuck in your ways but rather you're super willing to adapt and follow the practices standard to the domain you're working in (e.g. pre-ANSI C when working on pre-ANSI C and modern and safe when doing modern things). 
So you want him to use C++ instead of C, which is a different language. Your advice sounds horrible except for the clang-format part.
GCC and LLVM require you to rewrite C++. I work in toolchain. He wrote a toolchain. He's not working in toolchain unless he writes C++. Not sure what you're complaining about. I'm sure you can find a way to work on GNU tools and only ever touch the C portions of the codebase but I'm not sure you could find anybody to hire you to do so. 
Where are you located?
He is demonstrating his skills in C writing a C compiler that can compile itself. Furthermore he wants a system/embedded job (probably also in C since that is what he is demonstrating). You are telling him to use a completly different language instead on the C programming reddit. Sounds like a problem. Furthermore C++ still does t get you far in embedded.
This is so god damn pedantic. The guy is smart and knowledgeable enough to write an entire C toolchain. He's smart enough to realize how to modify my suggestion to fit the career he wants more accurately. 
Well, C `const` did turn out to be mostly useless to the compiler (unlike C++ `const`, which is useful to it, but the type system isn’t nearly sane enough to express APIs with ut), but the aliasing restrictions (`restrict`, in the current incarnation) and the associated aggressive optimizations Ritchie decried are _extremely_ useful on a modern superscalar, easily getting you an order-of-magnitude speed increase in array-processing code. Not so if you’re on a microcontroller with something like two wait states for RAM access, of course. My point is that the systems Ritchie had in mind are more like the controller, not the superscalar.
My point is that the authors of the Standard often use "UB" to describe situations where some quality implementations should define useful behaviors, and programmers should be entitled to expect such behavior from them. The published rationale for the Standard makes abundantly clear that the authors never intended that all programs which invoke UB be regarded as "broken", but unfortunately the maintainers of gcc and clang don't care.
Thanks for pointing that out, changed with fixes applied. Also like the complex skeleton.
By the way, can you describe in a couple of words how you stayed sane writing the AMD64 assembler? I recently wrote one in Forth, and even with DSLing the hell out of things and no AVX the encodings made me cry.
Yep, although this of course in some cases impacts portability across compilers and architectures and limits optimization possibilities. Also Undefined Behavior is used quite often to also specify things which I doubt are intended to be specified by the implementation, those are usually -- arguably more appropriately -- cases of implementation defined behavior. Anyhow, there's no portable way of doing what OP wants using the p format specifier due to it being a) Implementation defined and b) flags for p undefined (although possibly actually imple defined)
Expressing policy is important in programming. Const lets you express policy and also have the compiler enforce it, which is a good thing. 
From what I understand, the original `noalias` proposals failed to make clear what assumptions a compiler could or could not make in response to it. A concept which is present in the description of `restrict` but I don't think was present in the original "noalias" proposals was the notion of a derived pointer. If a function receives a `char * restrict foo` and passes it to `strchr`, the return value from that should be usable in the same context as `foo`, but I don't think the original `noalias` proposals would have allowed that. With both qualifiers, however, I think a major problem is that there are several useful things for them to mean, but the over-emphasis on "keep the language simple" made it necessary to try to implement "compromise" semantics rather than allowing programmers to use directives that accurately describe the assumptions a compiler should be able to make. Perhaps the most fundamental problem with the relationship between DMR and the Committee is that it failed to define adequate means by which programs can distinguish between cases where code does things compilers should expect to understand, and cases where code does more "unusual" things. A good execution model should recognize the concepts of objects having a state in a logical-memory machine as well as one or more "cached" states, and that the states are synchronized at some times but may diverge in others. The Standard's model where bytes of storage have fictitious "Effective Types" has no relationship to reality and serves no purpose except to sow confusion and discord. 
BTW, in what sense is `const` "useless" to the compiler? It allows a compiler to place objects with that qualifier into a separate data section, which may in some cases allow objects to be placed in read-only storage which may be a region of RAM that can be shared among multiple processes or, for embedded systems, need not be RAM at all. How would a compiler know which objects it should place in read-only sections without `const`?
Wow, I'm always surprised how problematic job hunting seems to be in the US, even as a programmer or computer scientist. I feel like it's far easier here (in Germany), people are seriously begging us to work, depending on the field. I have no doubt you'd find something. Mostly companies doing all the IoT-Crap are searching for firmware and embedded developers like us, so if you haven't looked specifically there, it might be worth it.
The *only* difference between IDB and UB is that implementations are *required* to document a behavior for the former even implementations targeting platforms where guaranteeing any kind of predictable behavior would be most expensive but least useful. The distinction was only expected to be significant on implementations that couldn't process an action usefully at essentially zero cost. In cases where only obscure implementations wouldn't be able to process an action usefully at zero cost, letting the people who work with an implementation decide whether the costs of supporting the common behaviors would be justified by their usefulness made more sense than having the authors of the Standard--who had likely never even heard of the implementations in question--make such decisions. 
Honestly, at the end of the day, it's not going to make any difference. Just use the one you are more comfortable with and who cares about the opinion of other people. If you need it to work in a company, your choice would be forced anyway. Code::blocks is very simple to configure, the build system is very nice and the interface with the debugger is awesome. That said, at the moment, I am settled on VS Code, but most of the editors and IDE mentioned in the other answers are good. I personally don't like CLion and VIM, but I mean... Who am I to tell you what to use and what to discard? What matters is your proficiency in the language and your programming skills. 
Energy efficiency is good, but many platforms are more efficient performing 32-bit arithmetic than performing any other size, even though loading and storing smaller values may be slightly more efficient than loading/storing 32-bit ones. Achieving optimal performance would require a language with a better set of numeric types than C, however, including the concept of objects whose range could vary depending upon whether they get mapped to registers or stored in RAM.
Yeah, but the issues about portability and optimization stand. I'm not arguing against you that Undefined Behavior always some kind of hard no-no.
&gt; I don't like the direction web development is moving in general. Now I'm curious. Would you mind elaborating more on the specifics? Purely bystander interest on my part - I don't do webdev (thank god).
&gt; it also doesn't work currently What part doesn't work? And what part works? &amp;#x200B;
If your main in more than 100 lines you need multiple file and a make file 
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
Yes, policy is important, but it's out of place in C. C is about mechanism, not policy. *const* makes sense in C++, but it's a black sheep in C.
That is the one valid use of \*const\*, but for that purpose it should have been a storage class, not a type qualifier. The committee considered, and rejected, that position.
Hated. Every. Minute. Of. It. I cheated, too, by only implementing the instructions the compiler would emit or I write myself, so the instruction tables are incomplete. Didn't bother with the AVX instructions! Didn't need more than SSE2. &amp;#x200B;
The Great State of South Carolina! Ok, it's not that great, but my wife wants to live here.
Down boy! I appreciate your support here, but I think u/lanzaio is merely offering that rarest of things on reddit, *constructive criticism*. :)
How about trying kernel development? Remote jobs are super common and you seem to be a quick learner. 
Moving my post. Thanks for the link.
Man, these downvotes are vicious -- I see where you're coming from. The antiquated style of NCC really is an artifact of the project itself, not an indication that I'm hoarding butterfly collars in my closet. Rewriting the linker sounds about as exciting as gnawing off my left foot, but what if I were to, say, make a front-end for LLVM? Pascal or something, another obviously academic exercise. If it were properly executed, using appropriate modern C++ paradigms and such, would that go a long way towards closing the gap here? From the perspective of a hiring manager *in toolchain*, as you say.
Even when I have 110% motivation, I can't pump out that much code. I envy the coding wizards who can work non-stop like this.
Proprietary object format? Are you able to speak more on that? Either way this is an impressive project. Certainly inspiring to a guy working on a scripting language at the moment who hopes to get into JIT compiling. I'll probably be studying alot of your code in the future.
The merge should likely not be done pairwise -- do them all at the same time, since you know how many sets of data there are. This could also be done in-place, instead of making a bunch of temporary arrays. Also, it would likely help a little to name your variables things other than l/m/etc. 
What's with those long commented lines? That just make things even more confuse! 
Trucking would give you so much time to listen to audiobooks and podcasts. That's one thing I wish I was able to do while programming. 
What nonsense. Policy is a part of any programming. 
Got any interest in LLVM? DM me. 
Well, yes, policy is part of any programming, but my point is that *compiler-enforced* policy is not. Name another construct in C which is concerned *solely* with policy, and not mechanism -- there isn't one. That's why `const` is the black sheep. &amp;#x200B; &amp;#x200B;
Oh, it's nothing special, it's just not the same \[in the minor details\] as anyone else's object format. The header file pretty much says everything you need to know: [https://github.com/gnuless/ncc/blob/master/obj.h](https://github.com/gnuless/ncc/blob/master/obj.h) Good luck with your scripting language!
As u/gnuless already said, if the only usage of `const` were to mark the programmer’s promise not to change the object (like `auto` marks the programmer’s promise not to create pointers to it), it would have been a (useful) storage class, not a qualifier. Perhaps I should’ve said “to the optimizer”, not “to the compiler”, as the useless part is _pointers_ to const: unlike in C++, the optimizer _can’t_ infer that the contents of a `const` pointer won’t change, unless the pointer is marked non-aliased with `restrict`, in which case the optimizer can track any possible changes anyway. The point is that pointers to `const` are frequently used to express a part of a function’s contract... in a way that’s both limiting (we need moar parametric polymorphism) and completely useless optimization-wise. C in general could use more support for declaring sections—creative use the linker can do a lot of things (_e.g._ allocate tags for an open enumeration type defined across several files), but reading vendor libraries for microcontrollers (where _e.g._ your use of `const` is most useful) makes me think those vendors never heard of a linker script.