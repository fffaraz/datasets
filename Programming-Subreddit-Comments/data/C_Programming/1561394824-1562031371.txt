If ever there were a time in which I'd be happy to see code-formatting nannies running amok, that time would be now.
Yes? There's room to language lawyer a squadron of B-52's through "earlier successful call to the ftell function on a stream associated with the same file". As ftell() do in fact tell that the advanced pointer is now at 6, it's pretty clear that the next invocation of the program will satisfy the first half of that constraint. The standard does not require that the offset is gathered in the same execution, much less from the same stream.
I like this, though `ftell()` doesn't necessarily return 6 at all, even though the `printf()` output suggests it does. All bets are off with undefined behavior. Note too that `ftell()` returns a `long`, so the use of `%d` also yields undefined behavior.
Out of curiosity how/where would one compile Javascript?
You're right, I missed that part. Thank you for the correction.
I guess you found out about `strptime` and `strftime`.
I believe it can be compiled to WebAssembly. But even though there are no common use cases and few tools to do it, doesn't mean it can't be done.
Removed as off topic.
Well, if JIT compilation counts, any high-performance javascript engine is dynamically compiling it to native code. If Chrome/Firefox/Safari only used the interpreter, it'd be ridiculously slow.
No need to spend $17, you can get these for a dollar or two on aliexpress.
You know; it doesn’t take that long to craft a question that would get you an answer but not look like a homework question.
You know this isn't a copy of some previous post right?
Ha! And I was thinking that $17 is cheap
It isn't. But your post is not about C, so it's not on topic.
Most implementations for most target environments extend the semantics of language by defining the behavior of `fseek()` and `ftell()` with text files in more detail than required by the Standard. On platforms such as MS-DOS and Windows where newlines are represented with a two-byte sequence CR+LF, the typical behavior is for `ftell()` to report the physical byte offset, and for `fseek()` to use physical byte offsets, but for `fread()` to read as many bytes as would be required to yield the required amount of text. Implementations are inconsistent with regard to how they process individual CR or LF characters that are not within CR+LF pairs, and it's possible that `fread()` is skipping over many such characters.
OK Then is there a way to make it on topic or no matter what I do it’s not on topic?
Because the second edition was published in 1988. Even before the C89 standard came out. It is woefully out of date.
This is a very common gripe in the sub, and I'm not the first to say it, and certainly not the last. When you submit a post there are clear guidelines on how to post code. &gt; so just shut up and do the transcription if you actually care about the visually impaired Ok you're a shit cunt. I was heavily vision impaired for 10+ years; I'm lucky that I had surgery which fixed my eyesight, but I'm considerate of those people and therefore I post that link specifically with those people in mind (which the link explains), furthermore -- I actually do like pasting code in my IDE. Also, if someone (OP) litters on the street and runs, what purpose is it to "fix" the problem (transcribe the code), when onlookers don't give a shit (you) and think I'm virtue signaling and then give me shit about it. Seriously, if you actually clicked the link, it's not arrogant, it's not about complaining, it's gently trying to indicate to the OP that pasting image of code is rude and should consider the wider audience -- especially when they're asking for help. That is essentially the standard for this sub, always has been.
"The stack" is an implementation detail also. Some compilers don't use stacks for automatic variables (I've used one).
In the original language invented by Dennis Ritchie, both unused parameters and unused arguments were ignored. The behavior of: int test(x,y) int x,y; { ... } and int test(x) int x; { ... } would be identical if none of the code in the `...` made any use of `y`. Both functions could be invoked with `test(0);`, `test(0,0);`, or `test(0,1,2,3,4,5);`, and it wouldn't matter unless--as stated above--the function made use of `y`. Some platforms would have had difficulty processing that language, and thus the Standard only requires that implementations handle situations where either the caller supplies precisely the arguments specified in the function definition, or where a prototype with a `...` parameter is visible at the point of a function call.
You can make it on topic by establishing a strong connection to the C programming language. Projects in C or for programming in C, but the focus needs to be on the C part. I'm not sure how to make your post on topic. You can try again if you like. I'm trying to treat you as fair as any other user.
A) The rule is that pasted code must be legible. That picture is legible is not posted code and if it were it's still be legible. B) Mighty convenient of you to have _been_ visually impaired, I'm glad you made a recovery. C) Apples to oranges, may as well try and compare the picture to genocide for all the relevance the comparison has. Stop being an insufferable jackass, either put in the effort to do the transcriptions or don't. Posting masturbatory links to feel smug isn't helping anything but makes you look like a twat.
That doesn't really explain why they don't write void in the book, though. They use void everywhere in the book, where the function isn't supposed to take arguments, but they don't do it for main.
What you said sounds nice, but it's virtually meaningless. It leaves me wondering what you think the stack is.
Ok I well try again in a week or two then maybe I will have some example projects to show along with C only reference templates. Thanks.
You think I'm an insufferable jackass, I think you're being a shit cunt. Let's call it a wash.
Void is not valid, the arguments to main are... main(int argc, char *argv[])
Get it correct, you ___are___ an insufferable jackass, I think you're a twat.
That sounds cool! I'm looking forwards to it.
It’s syntactically and semantically okay to omit `void` from the definition of a function, it’s just really bad mojo because parameter lists `()` and `(void)` mean different things literally anywhere else. (There’s also disagreement with C++ in how `()` is treated, and `(void)` allows easier interoperation between the two languages when the need arises.) E.g., per C, the type of a function defined as `foo()` is `int(void)`, but the type of a function that’s only *declared* as `foo()` is `int()`, the first taking no arguments and the second taking ~any. Per C++, the type of a function defined or declared as `int foo()` is basically the same as `int foo(void)`, and the type `int()` is basically the same as `int(void)`. So using `void` consistently in your C code is clearer, and it makes it much easier to interoperate with C++ code when you need it to work in both environments. Wrt the K&amp;R C book, probably they just changed the minimum amount of code they could. There may also have been some fleshing-out required of `void`’s behavior and the semantic differences between `func()` and `func(void)`, since `void` wasn’t implemented universally or consistently pre-C89—the earliest C compilers didn’t even have `void *`, and used `char *` for that purpose. There are a few special cases carved out in the standards for the `main` function, so they may have been playing it somewhat safe.
What exactly do you mean by "dynamic array"? C already has dynamic arrays, hence calloc...
they don't write `main(void)` because `main` is not a function of `(void)` --- it's a function of `(int, char**)`; they're just ignoring its arguments.
Probably an oversight, but one which may have stemmed from uncertainty about what the Standard would allow or require when invoking various functions. While the Standard ended up not allowing such a thing, there are some platforms where it would have been useful to use a different calling convention based upon whether a function has a prototype or an old-style definition [with old-style definitions causing the implementation to e.g. pass a dummy argument indicating the total size of the other arguments]. If `main()` was treated like any other function, and an execution environment expected to call it using the old-style approach, that could necessitate using an old-style definition with that environment.
C89 seems like an odd choice. C99 compilers are pretty common on systems today and C99 has a lot of improvements over C89. C11 is less common, but `gcc` and `clang` are great choices. Unless a system specifically requires C89, most real world code I've seen is safe with C99 or C11 so if you want to learn proper modern C I would recommend compiling against one of those standards. Indentation is a mix of spaces and tabs (look at the line endings for macros in the GitHub file explorer). Pro tip: If you use spaces for indentation at the beginning of the line you should use spaces everywhere. You use unity for testing but don't include the license. This is a license violation. You should have a `deps` directory or something which includes all code from other people / repositories (with proper licenses included). You aren't necessarily freeing resources that have been allocated. On [linked_list.c line 25](https://github.com/mmhanson/Basecamp/blob/a75ddbee9289b02838ed3c180ed9c5df6e0d3770/linked_list/linked_list.c#L25) you correctly check `NULL` returns, but don't free resources if only one of them returned `NULL`. Since `NULL` can always safely be passed to `free` I would recommend doing something like: LinkedList *linked_list_construct(void *head_key) { LinkedListNode *new_node; LinkedList *new_llist; new_node = create_node(head_key); new_llist = malloc(sizeof(LinkedList)); if (new_node == NULL || new_llist == NULL) { /* Allocation failed. */ free(new_node); /* NOTE THE FREE */ free(new_llist); /*NOTE THE FREE */ return NULL; } new_llist-&gt;size = 1; new_llist-&gt;head = new_node; new_llist-&gt;tail = new_node; return new_llist; }
I'm pretty sure main has two correct prototypes, int main(void), and int main(int, char**).
It's actually got 3 arguments on most Unix like platforms: int main (int c, char **v, char **envp) { ...... }
Might be due to a case of mixing up "static" with "statically allocated." "Static" refers to variables with a lifetime that persists for the entire program, like string literals, which some people might sometimes also call "static strings." A string literal is still a `char` buffer, but its memory is created as part of process creation when the program is run, not as part of any function call. As such, there's not actually any runtime *allocation* happening. "Statically allocated" variables are variables that are *allocated*, but the allocation itself is of a static size. An `int` on x86-64 will consume 4 bytes. In the allocation for the local variables of a function which includes an `int`, the size of the local variables is hardcoded into the function, and the function will never allocate a different amount of memory for itself under any circumstances. This as opposed to dynamic allocation (`malloc`, usually), which can allocate memory of variable size.
That was standard practice at the time. main(void) wasn’t so much a thing until C89.
i believe this to be true in C89, as standardized a year after the revised edition of your book. i do not think it was reliably true at the time when the book was written. the version written in the book would compile no matter what signature the compiler expects for `main`, which is why K&amp;R does not write `main(void)`.
Looking at your linked list stuff: Hooray for implementing a linked list I guess, but you’re doing way too much of the wrong kind of work. Construction and destruction of an object are usually handled independently from allocation and deallocation of the object—e.g., a C++ ctor/dtor, which respectively construct and destruct, just init or deinit whatever chunk of memory (or compile-time value-ness) you point them at. Unless you have some compelling reason to `malloc` and indirect *everything*, you want to deescalate to `void T_init(T *inst)` and `T_deinit` functions, usually with `T_INIT(things) {things}` sorts of static initializer macros when possible. Then if the caller wants to `malloc` it, they can; if they want to declare one `auto` or `static` or a field in a `struct`, they can. Also, your constructor is documented as “return[ing] a valid LinkedList” but (a.) it’s returning a `LinkedList *`, which is a completely different thing, and (b.) `NULL` is a valid `LinkedList *`, and your function can return `NULL`. Failure modes are an extremely important aspect of documentation, as is precise description of argument and return values and possible side-effects. More generally, your ownership model is upside-down. You have the list owning nodes that own “keys” (keys are for maps, not lists), but this is the most annoying kind of API to deal with because it’s often so much easier+faster+smaller to run a few links through a single node, and linked list management isn’t so intolerably complicated that one ends up caring enough to deal with an external dependency for it. And then if I do call out to your library, your ownership model is imposed upon my code, and anybody’s code that uses my code, and anybody’s code that uses theirs. This model also makes it much more difficult to go multithreaded. Instead, the caller should manage the node, the node manages the lifetime of the links, the list manages the use and update of the links, and then you’re freed from any consideration of extraneous stuff like how node “equality” should work in a search etc. Each list has `Link`s that run through nodes at a particular (compile-time-contant) offset from the start of each. The list itself is a pair (`[2]` or `struct`) of pointers, each API function takes an offset for the link field when that’s needed, and that’s where the API boundary should be. If you’re aiming for GNU/-ish compilers you can usually minimize the caller work to like extern void LL_doThing(LinkedList *list, void *node, size_t offset); #define LL_doThing(list, node, link) \ LL_doThing((list), (node), offsetof(__typeof__(*(node)), link)) since `__typeof__` allows you to cheat properly. Iteration through a list you do with #define LL_forEach(list, nodePtr, field) \ for((nodePtr) = (list)-&gt;head; (nodePtr); (nodePtr) = (nodePtr)-&gt;field.next) If you need canned iteration, offer a `map` function like typedef void *LL_iterator_f(int *cancel, void *context, void *p, void *node); void *LL_map( const LinkedList *list, size_t link_offs, int *cancel, LL_iterator_f *func, const volatile void *context, const volatile void *p); which is clunkier but less fragile than `LL_forEach`.
why does `linkedlist.h` include `&lt;stdlib.h&gt;`? you don't use any of its typedefs in the header, so don't force other files that include `linkedlist.h` to transitively include `&lt;stdlib.h&gt;`. just put the `#include &lt;stdlib.h&gt;` in `linkedlist.c`, where you use it. i don't know what on earth you're trying to do in `dynamic_array`, but please don't do it. don't use the C preprocessor to implement generics --- if you want to write C++, do it in C++. in C, "dynamic arrays" are created by just putting an integer expression on the left-hand side of your binding, like: void zero_dynamic_array(int a[], int len) { int *top = a + len; for (int *i = a; i &lt; top; i++) { *i = 0; } } int any_function(); int do_stuff_with_array(int a[], int len); int main(void) { int len = any_function(); int a[len]; zero_dynamic_array(a, len); int res_a = do_stuff_with_array(a, len); } if you need to store a dynamic array on the heap, that is, if the allocation needs to outlive its caller, allocate an array with `calloc`. do not abuse the C preprocessor --- other people, smarter than you or i, have tried to write generic code using CPP macros, and it doesn't work, and it's disgusting. don't do it. C is a low-level language, and you have to monomorphize your generic algorithms by hand. suck it up, or go back to C++.
Do you have gdb or any other debugger? [https://www.gnu.org/software/gdb/](https://www.gnu.org/software/gdb/) Do you know how to use it? If not, here's a short overview. [https://beej.us/guide/bggdb/](https://beej.us/guide/bggdb/) (Never used. Just found it Google search and thought it was okay) What are your compilation flags? Can you add -g?
Piggybacking off of this, is there an easy set up guide to using VScode on Windows to write C programs? I tried following the guide on VScodes website, but I failed repeatedly. Which terminal do you use in VSCode? I've just resorted to using Linux Subsystem for Windows with Vim and tmux (which there are heaps of guides for, so was easy to get going once I learnt all the keyboard shortcuts)
&gt; Why aren't you using \_Generic, while trying to make generic code? I do not know about other things that you mentioned. But I think OP is using C89 and this is something that wasn't available in C89.
&gt; It leaves me wondering what you think the stack is. An implementation detail that's mostly irrelevant to C programming, but seems to be focused on by a lot of learning materials.
There's so much to unpack here I'm just gonna throw out the luggage instead.
Also true for Windows, for MSVC, Clang and MinGW GCC.
&gt;what is the difference? Object with a static storage duration live for the full lifetime of the program, so the static area of memory doesn't change. Objective allocated on the stack are local variable, and live only as long as their block (the pair of {} they were declared in) does. ``` #include &lt;stdio.h&gt; void foo() { int a = 1; static int b = 1; printf("a: %d, b: %d\n", a, b); a++; b++; } int main() { foo(); foo(); foo(); } ``` This should print: ``` a: 1, b: 1 a: 1, b: 2 a: 1, b: 3 ``` Because a is allocated and freed every time you call foo, but b lives from the start of the program to the end of the program. &gt; Most references I have come across say that stack and static memory are the same. Well, the one source of truth of the language, ie. The Standard, has this to say (this is not C11, so there is no mention of thread local): &gt; # 6.2.4 Storage durations of objects &gt; An object has a storage duration that determines its lifetime. There are three storage durations: static, automatic, and allocated. Allocated storage is described in 7.20.3. [Allocated storage is dynamic memory in standard legalese] &gt; The lifetime of an object is the portion of program execution during which storage is guaranteed to be reserved for it. An object exists, has a constant address, and retains its last-stored value throughout its lifetime. If an object is referred to outside of its lifetime, the behavior is undefined. The value of a pointer becomes indeterminate when the object it points to reaches the end of its lifetime. &gt; An object whose identifier is declared with external or internal linkage, or with the storage-class specifier static has static storage duration.Its lifetime is the entire execution of the program and its stored value is initialized only once, prior to program startup. &gt; An object whose identifier is declared with no linkage and without the storage-class specifier static has automatic storage duration. [That's on the stack] &gt; For such an object that does not have a variable length array type, its lifetime extends from entry into the block with which it is associated until execution of that block ends in anyway. (Entering an enclosed block or calling a function suspends, but does not end, execution of the current block.) If the block is entered recursively, a new instance of the object is created each time. The initial value of the object is indeterminate. If an initialization is specified for the object, it is performed each time the declaration is reached in the execution of the block; otherwise, the value becomes indeterminate each time the declaration is reached. &gt; For such an object that does have a variable length array type, its lifetime extends from the declaration of the object until execution of the program leaves the scope of the declaration. If the scope is entered recursively, a new instance of the object is created each time. The initial value of the object is indeterminate.
I would suggest to change the name of your variables to English when you post online since that makes it easier for people to understand your intend. I tested your code and it works so the issue is not on the part you showed, it might be somewhere else in the code you are not showing. Your logic works though so it might just be that you are overlooking something and maybe you using uninitialized memory or forgetting to clear and old variable that you reuse. &amp;#x200B; This is what I used, maybe it can be of help. [https://gist.github.com/wudang-monk/bc3133f3533c6a365ff56ff60ae733dc](https://gist.github.com/wudang-monk/bc3133f3533c6a365ff56ff60ae733dc)
What code?
I am not a professional programmer nor am I likely to need anything as fancy as an IDE.
Traditionally, on Unix systems, the heap can grow with the `sbrk(2)` syscall - and additional memory mapping depending on your `malloc(3)` implementation, while the stack is a static mapping - but the size is usually set at load time. (Some systems allows you to resize set the stack mapping at runtime, but you need to ask for that explicitly)
The main problem with c99 comes from wanting to use Microsoft stuff, either visual studio or cl directly, since it’s modern c support is so bad &gt;:( though it should support most of what I saw in the repo.
Honestly I disagree with your stance on generics. Obviously if you’re doing embedded then writing the data structures/algorithms by hand is probably the way to go, but if you’re just using c for fun, I see nothing wrong with creating a generic implementation of collections you’ll use with many types, especially more complicated ones like hash maps and dynamic queues.
Try doing 'fflush(stdin);' just after i++.
Yea, this is wrong.
Main is/and was an entry point. As Ken Thompson said, "It was a place to start." &amp;#x200B; That is it.
Don't use `scanf()` for user input.
There's an easier way: obradi(Elem **pp) { Elem *p; while ((p = *pp) != NULL) { if (p-&gt;broj == 2) { *pp = p-&gt;sled; free(p); } else pp = &amp;p-&gt;sled; } }
That looks like it must be already C#.. Do you want to convert that to C? More context might be helpful.. Why just that one line? Generally, you have to convert a whole project from scratch because the language and its libraries are different.
Nope, its not yet in C#... this one line of code is my only problem, the other parts of this code is easy.
This is a C subreddit, not C#.
SO SORRY! thank you for the clarification, I will now remove this post.
It doesn't look like C.. It looks like you"re calling a method (member function) of an object (Item), and C is not an object-oriented language.
Hi Mojo, I just looked at the array. I do like your naming, emulating the C++ naming. Personally, I think that using void\* is a solution that makes the resulting code basically useless for most users. I really need the compiler to warn me about type-safety and without this so many things go wrong that I just give up. I would encourage to look into ways in which type-safety can be maintained. Maybe how darray ([https://github.com/rustyrussell/ccan/tree/master/ccan/darray](https://github.com/rustyrussell/ccan/tree/master/ccan/darray)) will give you some inspiration? Good luck!
First think of how you would solve this yourself. Take a paper, draw the chessboard and add the input queens on it. What do you do to check if they attack each other or not?
&gt; I'm guessing we have to use a 2 dimensional matrix See if you can think of a way of solve this problem _without_ this 2-dimensional array. I can assure you that it using this 2d array would only make things harder. &gt; but then I don't know how to check if they can attack each other or not. Think about what it means for two queens to attack each other. You're give the positions of the queens on two dimensions... but perhaps thinking two _additional_ dimensions, each at 45 degrees to the first set, might be useful? Consider the following diagrams: a b c d +-+-+-+-+ 1| | | | | +-+-+-+-+ 2| |Q| | | +-+-+-+-+ 3| | | | | +-+-+-+-+ 4| | | | | +-+-+-+-+ a b c d / / / / +-+-+-+-+ e |/|/|/|/|/ +-+-+-+-+ f |/|Q|/|/|/ +-+-+-+-+ g |/|/|/|/|/ +-+-+-+-+ |/|/|/|/| +-+-+-+-+ d c b a \ \ \ \ e +-+-+-+-+ \|\|\|\|\| f +-+-+-+-+ \|\|Q|\|\| g +-+-+-+-+ \|\|\|\|\| h +-+-+-+-+ \|\|\|\|\| +-+-+-+-+ The queen in cell b2 in the first diagram is also in diagonal c in the second diagram and diagonal d in the third diagram. How could you leverage this to work out whether it's able to attack another queen?
Unfortunately this is considered Undefined Behavior so it's best to avoid it.
&gt; That doesn't really explain why they don't write void in the book, though. They use void everywhere in the book, where the function isn't supposed to take arguments, but they don't do it for main. A close reading of 4.2, A.8.6.3, A.10.1 provides no definitive why but suggests that it was a conscious and pragmatic choice arising from the fact that they didn't gain anything from the use in `main()` as they did in the other cases. Just to find a difference, folks had to come up with overly contrived cases involving argument-bearing recursive calls to `main()`.
6pm CEDT happens when this comment is 5 hours and 43 minutes old. You can find the live countdown here: https://countle.com/aMQOmCPgg --- I'm a bot, if you want to send feedback, please comment below or send a PM.
What operating system are you programming for?
C++ is off topic in this subreddit. Please post C++ content to /r/cplusplus or /r/cpp instead.
Android
When you say "an app", what do you mean? Do you mean a program on your computer, an app for your mobile phone, or something else? I'd suggest looking at some starter C tutorials and work from there. They'll teach you how to make a program (aka an app), and then you can work towards writing a program to do what you want. Just google for "hello world in C" for dozens of tutorials on the subject.
Do you have any prior knowledge of C? If not, Android might not be the best platform to start with.
Yeah I know a little bit, but why is android not so beginner friendly?
i mean an app for my mobile phone
Android is meant to be developed for with Java. If you want to write C programs for Android, that is possible but a bit tricky. You also need to provide multiple binaries for different CPU architectures or your application won't work on all Android devices. The key term to search for is “native code.” To get started, [use the Android NDK](https://developer.android.com/ndk/guides).
You don't develop Android apps with C. Even if you used C in an Android application, it would only be in places where you want the strengths of C, fast execution time, low level interface, etc. Most of your code would be in Java/Kotlin anyway. I recommend you learn Kotlin and don't bother with NDK this early in your journey.
&gt; is there an easy set up guide to using VScode on Windows to write C programs? Nope, there is not. Windows have like 3 different terminals (cmd, developer cmd and powershell) each one using it's own paths. The developer cmd have "cl", the compiler for MSVC++ (aka, windows C/C++ compiler), and may allow you to compile monolithic programs, if you want to do modular stuff it will end up pretty messy. Perhaps you could figure out something using CygWin with gcc.exe and make.exe, but you will need to add thos programs to the path of the cmd, by the time you have done this you could have installed like 10 VMs with debian or ubuntu. At last, but not least, also keep in mind, if you are goint to use 3rd party libraries, such as ZeroC Ice (Middleware for communications, very practical), vulkan/opengl (for game development) or Boost (MASSIVE generic library) paths in windows will not be configured to use them in every project.
Thank you
Thank you very much
!remindme 20 hours
I will be messaging you on [**2019-06-26 07:46:23 UTC**](http://www.wolframalpha.com/input/?i=2019-06-26 07:46:23 UTC To Local Time) to remind you of [**this link.**](https://www.reddit.com/r/C_Programming/comments/c578ls/how_does_the_for_loop_actually_work/es02804/) [**CLICK THIS LINK**](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[https://www.reddit.com/r/C_Programming/comments/c578ls/how_does_the_for_loop_actually_work/es02804/]%0A%0ARemindMe! 20 hours) to send a PM to also be reminded and to reduce spam. ^(Parent commenter can ) [^(delete this message to hide from others.)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete Comment&amp;message=Delete! ____id____) _____ |[^(FAQs)](http://np.reddit.com/r/RemindMeBot/comments/24duzp/remindmebot_info/)|[^(Custom)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=[LINK INSIDE SQUARE BRACKETS else default to FAQs]%0A%0ANOTE: Don't forget to add the time options after the command.%0A%0ARemindMe!)|[^(Your Reminders)](http://np.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List Of Reminders&amp;message=MyReminders!)|[^(Feedback)](http://np.reddit.com/message/compose/?to=RemindMeBotWrangler&amp;subject=Feedback)|[^(Code)](https://github.com/SIlver--/remindmebot-reddit)|[^(Browser Extensions)](https://np.reddit.com/r/RemindMeBot/comments/4kldad/remindmebot_extensions/) |-|-|-|-|-|-|
Definitions are also declarations; in `void f()` and `void g() {}` both functions have the same type (returns void, takes any number of arguments). It's not a constraint violation to pass arguments to g (as it would be if the type were void(void)) , just silent undefined behaviour.
I really don't think you can "call" a for loop like you do a function. A for loop is a compiler/language construct, I really don't think you can "link" and "call" it. I believe the closest you can do is have a label on the first instruction of the loop and make sure the "calling" code puts the loop conditions/variables in the right places. You said you know how a for loop works, but do you know what is the equivalent ASM for a for loop? It's just a bunch of jmp's and labels. I suggest you take a look at godbolt's Compiler Explorer (Google it). See what's generated from simple loops there.
Consider the following program: int main(void) { for (unsigned long long i=0; i&lt;100; ++i) printf("%ld\n", i); return (0); } It would be compiled to: .LC0: .string "%ld\n" main: push rbx xor ebx, ebx .L2: mov rsi, rbx mov edi, OFFSET FLAT:.LC0 xor eax, eax add rbx, 1 call printf cmp rbx, 100 jne .L2 xor eax, eax pop rbx ret Of course, the compiler optimized some stuff, but the core algorithm is easy to understand. The `for` loop itself would be: xor ebx, ebx loop: # Do stuff add rbx, 1 cmp rbx, 100 jne loop Basically, you zero `rbx`, and start the loop. In the loop, you increment it, check if it is zero, and if it is not, jump to the loop's start. Otherwise, carry on with the code. `jne` means `jump not equal`, which operates based on the comparison between rbx and 100 (`cmp rbx, 100`)
&gt;I really don't think you can "call" a for loop like you do a function. A for loop is a compiler/language construct, I really don't think you can "link" and "call" it. You can. A `for` loop is a section of code that calls itself until a comparison is evaluated as true. You can have an object file for the loop and link it.
This is way out of my wheelhouse but can you pass a couple of pointers to a few procedures?(actually I think 3 or 4 now that I'm really thinking about it). One for each part of the for expression? for(proc1; proc2; proc3){proc4;}. Sorry if this is completely unhelpful because I've done very little ASM work, but it's a cool question and I just wanted to participate. Anyway, how do you link a for loop anyway? Aren't functions and variables usually linked? I don't really understand what that means off the top of my head. Are you literally mean to replace the "for" keyword or just make a new one like myCoolFor(...){...} or can you just use some ghetto syntax that does something similar to for like myGhettoFor(pointer1, pointer2, pointer3, pointer4);??
Thanks!
I think OP knows how to write a loop in ASM, but isn't sure how to link it. This would look completly different if you did, for example, an infinite loop like for(;;true) {...}. I dunno, perhaps I misunderstood the question but I think you're answering the wrong question. That being said I think you're better equipped to give a good answer than I am, so maybe I'm just confused.
Sorry, I wanted to change variables' names but I was a bit tired when I posted this so I just used them as they are. Thanks for github link, will look into it.
 The `if (part1; part2; part3) body;` becomes: part1; start: if (!part2) goto end; body; part3; goto start; end:
!remindme 24 hours
This isn't quite right. A `call` is fundamentally different from a `jmp`. A for loop `jmp`s. It has to do with the stack, a call pushes the return address and function arguments on the stack, then jumps. A jump just changes the instruction pointer. If a for loop called itself, you would stack overflow on large iterations. I also think there's a misunderstanding of the problem. Op has to implement the loop mechanics in asm, and use those mechanics to iterate over a code block in c.
Ye my bad. I didn't take the time to read the question and just jumped to typing. I'll just piggyback for a moment - would you happen to know why I can't just `mov` to the IP? it would make things easier.
How do you link it? This is just a compiled for loop, right? My interpretation is that the OP is trying to replace the "for" keyword.
Insturction pointers are in the code section of the executable, which is read only on most architectures.
i probably wouldn't be complaining as much about an implementation that: - wasn't a data structure that already trivially exists in C - used `_Generic` instead of a bunch of ugly hand-rolled macros
What functions do you need to figure this out? You probably want something like bool CanQueensAttack( int q1Row, int q1Col, int q2Row, int q2Col) That will return true if q1/q2 can attach each other. Think about what functions you would need to call from that function. Break it down to 1 direction at a time. &amp;#x200B; Then you need to figure out how to iterate over the list of queen positions.
I would: 1. Get the number of times the user wants to loop 2. Check that is between 1 and 5 (inclusive) 3. Use that number as the number of times to loop
Should I use fgets instead
I prefer `getline()`, which makes memory management easier. You can validate user input when parsing and converting.
`for` is not a function, and a `for` loop cannot be called like a procedure. when the C compiler sees a `for` loop, it expands it inline, so that: for (int i = 0; i &lt; 10; i++) { /* body */ } expands into code that's conceptually similar to: int i = 0; mark start: if (i &lt; 10) { goto end; } /* body */ goto start; mark end: this happens inline, so any two lexically distinct `for` loops are likely to correspond to different sections of the binary, even if their clauses are all exactly identical.
Are you agreeing or disagreeing, or what? The 1974 C Reference Manual has an implementation of \`printf\` which would be useless if callers had to supply precisely the arguments functions are declared as accepting.
Exactly. And perhaps more importantly, a jmp is (usually) local, resolved at compile time, whereas a call can go far away and is only resolved at link time.
You might want to check /r/cpp, more people might be of help there for C++ related questions.
I'm a professional programmer, and I use Vim and Visual Studio Code.
The old DOS `graphics.h` interface is a dead end. I recommend you to avoid it. What error messages did you exactly get? it is possible that you forgot to link the graphics libary in.
Please stop the PVS Studio spam.
No, that's not correct. There are no function calls going on.
Okay, I'm sorry, I was not aware this topics is prohibited here.
Ah ok, so what's the best library to use for graphics in Linux? I want to make it fairly basic for him to draw shapes. He eventually wants to make a really simple game - like pong or something similar.
The topic itself is not prohibited. Advertisement however is. The purpose of this subreddit is not to advertise for your product. And PVS Studio articles are already posted so often that there is no place for this blatant advertisement. I'm a bit sick of it, personally.
SDL is probably what you're looking for.
The best library is SDL. For a beginner, I recommend the old SDL version 1. You can easily set up a frame buffer and then start to draw stuff into it. I think there are functions for drawing lines and circles, too.
Your best bet is [Raylib](https://www.raylib.com/), it is a C library for game development. Plenty of examples and good documentation. SDL2 is a good library if you are an experienced developer.
I agree with /u/FUZxxl that you should avoid *graphics.h*. I suggest that you use something more modern, like Raylib or SDL2. If you really want to use graphics.h, check the original article https://askubuntu.com/questions/525051/how-do-i-use-graphics-h-in-ubuntu especially the section about installing the prerequisites for Ubuntu 18.04 on which the latest Elementary OS is based.
Understandable.
Allegro isn’t bad for beginner game graphics.
Looking very briefly through the book, I didn't see any mention of the fact that the optimizers on gcc and clang cannot handle some of the "popular extensions" that are required for systems programming, including the meta-extension "If some parts of the Standard describe how an action will behave, but some other part of the Standard would say it's undefined, give priority to the former when practical." For example, while most hardware platforms have a natural global transitive non-overlapping ordering for all pointers, some do not. To avoid imposing an undue burden on platforms that do not have such an ordering, the Standard allows implementations to process relational operators between pointers to unrelated objects in any manner they see fit. Implementations which target platforms where addresses are naturally strongly ordered and are intended to be usable as "high-level assemblers"--*something the authors of the Standard have expressly said they did not wish to preclude*--will implement relational operators that test those orderings. With optimizations enabled, however, clang and gcc will not do so reliably. The language described in this book is a very useful one, but it's important to note that it relies upon extensions not mandated by the Standard, and which some optimizers do not support. Personally, I would like to see a free compiler which makes a bona fide effort to support those extensions when practical, while optimizing code that would not conflict with them, but at present programmers seem to be limited to using gcc/clang with optimizations disabled, using a commercial compiler, refraining from doing anything the authors of clang and gcc don't feel like supporting, or enabling clang/gcc optimizations while hoping for the best.
SFML is really easy to get started with
As said in my readme, I'm targeting C89 so `_Generic` isn't available or I would have used it. It seems like a perfectly valid way to program to me, [freeBSD's tree.h uses it](https://svnweb.freebsd.org/base/head/sys/sys/tree.h). I agree that C++ and C11 has better support for generics but I'm learning C89.
I'm targeting C89 so \_Generic isn't available. To make a generic array I could use macros or void pointers with constant casting, I chose macros for the easier use although they're harder to develop/debug. I don't see anything wrong with that choice myself. &amp;#x200B; I understand I could just use calloc to get larger arrays. My justification for making the header is so I could consolidate the logic of expanding and contracting the array into one, thoroughly tested set of functions. This would make it a lot easier to get this functionality into future projects and significantly (in my opinion) reduce the risk of bugs like out-of-bounds array indexing. &amp;#x200B; What do you find ugly about my formatting? I think it looks pretty great.
The third parameter for main is used (at least) in Unix-like systems to pass in environment variables, so it is not wrong.
Both `left` and `right` are uninitialized. Dereferencing them with [] is undefined behavior.
You haven't allocated memory for either of `left` or `right`. You're getting luck in that attempting to access `left[i]` is probably just overwriting the `right` pointer, but attempting to dereference it with `right[i]` is accessing unallocated memory. Try something like `char left[20], right[20];`, or (more properly), dynamically allocate the arrays with `malloc()` once you know how long they need to be.
I'm having a hard time following your advice, but this is what I gather. &gt;Construction and destruction of an object are usually handled independently from allocation and deallocation of the object—e.g., a C++ ctor/dtor, which respectively construct and destruct, just init or deinit whatever chunk of memory (or compile-time value-ness) you point them at. Unless you have some compelling reason to `malloc` and indirect *everything*, you want to deescalate to `void T_init(T *inst)` and `T_deinit` functions, usually with `T_INIT(things) {things}` sorts of static initializer macros when possible. Then if the caller wants to `malloc` it, they can; if they want to declare one `auto` or `static` or a field in a `struct`, they can. I think you're trying to say is the list shouldn't handle memory allocation, it should only initialize whatever chunk of memory the client gives it. This is something I was thinking about as well. You're suggesting that instead of a constructor that allocates and initializes on the heap, I make an initializer that takes a pointer to an already-allocated list and initializes the members? &gt;Also, your constructor is documented as “return\[ing\] a valid LinkedList” but (a.) it’s returning a LinkedList \*, which is a completely different thing, and (b.) NULL is a valid LinkedList \*, and your function can return NULL. Failure modes are an extremely important aspect of documentation, as is precise description of argument and return values and possible side-effects. That's a good observation, I think if I change the constructor to more of an initializer it would remove the possibility of a `malloc` error and remove this problem. &gt;More generally, your ownership model is upside-down. You have the list owning nodes that own “keys” (keys are for maps, not lists), but this is the most annoying kind of API to deal with because it’s often so much easier+faster+smaller to run a few links through a single node, and linked list management isn’t so intolerably complicated that one ends up caring enough to deal with an external dependency for it. And then if I do call out to your library, your ownership model is imposed upon my code, and anybody’s code that uses my code, and anybody’s code that uses theirs. This model also makes it much more difficult to go multithreaded. &gt; &gt;Instead, the caller should manage the node, the node manages the lifetime of the links, the list manages the use and update of the links, and then you’re freed from any consideration of extraneous stuff like how node “equality” should work in a search etc. Each list has Links that run through nodes at a particular (compile-time-contant) offset from the start of each. The list itself is a pair (\[2\]or struct) of pointers, each API function takes an offset for the link field when that’s needed, and that’s where the API boundary should be. If you’re aiming for GNU/-ish compilers you can usually minimize the caller work to like I don't quite understand what you're saying. It might be my C++/Java accent, but it seems more proper to me to have the list manage its own nodes. The reason I put all this into its own header was to get the node operations consolidated and thoroughly tested. This would make it a lot easier to put linked lists into my future projects and reduce the risk of bugs. It for sure doesn't fit all use cases (eg multithreading), but I wasn't trying to make a silver bullet. What are the reasons you'd suggest having the client manage the list nodes rather than the list itself?
&gt;C89 seems like an odd choice. C99 compilers are pretty common on systems today and C99 has a lot of improvements over C89. C11 is less common, but gcc and clang are great choices. Unless a system specifically requires C89, most real world code I've seen is safe with C99 or C11 so if you want to learn proper modern C I would recommend compiling against one of those standards. &amp;#x200B; You're totally correct. The only reason I want to nail C89 is because I want to go into embedded systems development. From what I understand a lot of the compilers for smaller systems are pretty quirky (read shitty) and C89 is the only reliable standard for any kind of portable C in that domain. I will get some more practice with modern C for sure, but I want to also get some practice with older C as well. &amp;#x200B; &gt;Indentation is a mix of spaces and tabs (look at the line endings for macros in the GitHub file explorer). Pro tip: If you use spaces for indentation at the beginning of the line you should use spaces everywhere. &amp;#x200B; I didn't realize that, thanks for letting me know :). &amp;#x200B; &gt;You use unity for testing but don't include the license. This is a license violation. You should have a deps directory or something which includes all code from other people / repositories (with proper licenses included). &amp;#x200B; Thanks for the heads up on that one, I didn't even know that was a thing. &amp;#x200B; &gt;You aren't necessarily freeing resources that have been allocated. On [linked\_list.c line 25](https://github.com/mmhanson/Basecamp/blob/a75ddbee9289b02838ed3c180ed9c5df6e0d3770/linked_list/linked_list.c#L25) you correctly check NULL returns, but don't free resources if only one of them returned NULL. Since NULL can always safely be passed to free I would recommend doing something like: ... &amp;#x200B; Once again, I didn't even realize that. I'll fix it. &amp;#x200B; Great advice. Thanks for taking a look at my code :).
Ya MSVC is just painful if you're writing C. It was much worse in the past, but still there are pain points. I generally try to use strict C99 and `#ifdef WIN32` platform specific workarounds if MSVC doesn't support a certain feature or function such as the underscored POSIX functions. Overall though with WSL, I prefer to do my personal C projects from the command line in WSL if I'm on windows.
Check out `valgrind` for checking memory leaks. I use the following alias in my `.bashrc` for debugging memory leaks: alias valgrind-mc='valgrind --leak-check=yes --leak-check=full --show-leak-kinds=all --track-origins=yes' Also if you are using GCC or clang try: -fsanitize=undefined -fsanitize=address as compiler flags. It will check for bad memory access / hanging resources / undefined behavior.
I would recommend using [SFML](https://www.sfml-dev.org/) instead. It's a bit more modern, very beginner friendly (tutorials, books, blogs, etc), and jives well with C++ idioms. I use SFML for hobby projects and I think it's really friendly, which would be perfect for an 11 year old learning C++.
&gt; I think you're trying to say is the list shouldn't handle memory allocation, it should only initialize whatever chunk of memory the client gives it. Yes. As in C++, construction and initialization should be kept separate. If you don’t, you’re forcing your caller(s) into a weird corner with Java and Python, where every object in memory is referenced indirectly. &gt; I think if I change the constructor to more of an initializer it would remove the possibility of a malloc error and remove this problem. Yes, although you might want to `assert` that instance arguments to initializer &amp;al. are nonnull. &gt; but it seems more proper to me to have the list manage its own nodes. This presumes that the list owns the nodes. It’s easier for the list to run through nodes, so the list is in charge of managing its own link fields but not the nodes they’re in. &gt; It for sure doesn't fit all use cases (eg multithreading), but I wasn't trying to make a silver bullet. In practice, linked lists are ridiculously easy to implement. There’s no reason to make a linked list library that doesn’t apply to a broad spectrum of use cases, because rolling-your-own is dirt cheap. E.g., here’s a link-last for doubly-linked list: node-&gt;next = NULL; *(!!(node-&gt;prev = list.tail) ? &amp;list.tail-&gt;next : &amp;list.head) = node; list.tail = node; So if you abstract wrong, or at the wrong level, you’re imposing a ton of overhead for the sake of DRYing a small handful of statements that would be just fine as macros or inline functions. &gt; What are the reasons you'd suggest having the client manage the list nodes rather than the list itself? OK, so the biggest one is just basic usability. If you’re using list-contains-nodes-contains-“keys”, then what happens to a list element (i.e., the node’s `void *key`) when you remove it from the list, or when you empty the list at deinitialization? Did you allocate `key`? Is anything else still referencing it? Is it your job to `free` it? Is it safe to `free` it? Does anything need to be done at all? (I note with some concern that you’re `free`ing `key`s yourself, when you had absolutely nothing to do with the allocation of `key`. This precludes perfectly reasonable things like adding the string literal `"foo"` to a list-of-strings—e.g., constructing a batch of command-line arguments—because `"foo"`’s array was probably not dynamically allocated. Ditto things like `MAP_FAILED`, ditto things like `int`s that are only being stored as `void *` because you forced them to be and it would be too insanely high-overhead to `malloc` for each `int`.) So if you go with that approach, you need to take a dtor function with each `key` in case the `key` needs cleaned up, and every unsupervised removal needs that dtor called whether or not it does anything. For supervised removals (i.e., where you’re returning the `void *` from the node you removed), should you call the dtor or just hope the caller will do what’s right? At best, you can have the compiler warn about unused return values from something like an unlink-first. The memory overhead on the `void *` approach is also higher. A bare-bones link field needs one or two pointers, depending on the type of list you’re implementing. If your list is allocating nodes, you’re running at ~100% overhead. `malloc` usually gives you a block preceded by a `sizeof(void *)`-byte attribute/link field, and the `void *` for the out-of-line key is also overhead vs. link-fields only, so you have at least `2*sizeof(void *)` bytes of management for your `2*sizeof(Node *)`-byte payloads. The dynamically allocated approach is also likely to scatter nodes willy-nilly around memory, which is the worst possible scenario for cache usage. And since the list is entirely in charge of node allocation, there’s no way to optimize those `malloc`s away without changing your code. If I want to make an arena from which list nodes should be allocated, I can’t do that without modifying your code or (heaven forfend) hooking `malloc` more generally. And since everything is just a `void *`, your list impl has no idea how to implement things like searches properly, not that you should be implementing searches in so abstract a fashion anyway. If the referenced `void *` is unique (e.g., `int`-as-`void *` or interned), then you can do a simple `a == b` comparison. But the type of thing behind that `void *` determines how it should be compared; e.g., if it’s pointing in-/to a string, `strcmp` should be used. And you’re doing bag semantics with `point_to_key`, which is a more generalized data structure that *can* but generally shouldn’t be implemented on top of a linked list. So that’s the basic stuff, at least. The list and links are the purest distillation of the data structure you’re trying to represent, so focus on them. Moreover, if you implement my suggested version of the library, you can implement *your* version of the library on top of it. That suggests that going with the list+link is a better decomposition. The only annoying thing about the link-management stuff is that the offset to the specific link needs to be handed around with the various functions so they can emulate a C++-style pointer-to-member. (In C++, you’d reference the link via `Link T::*` given node type `T`.)
C89 required main to be defined in one of the two forms (like the current standard). But K&amp;R don't mention this, even in the second edition. Instead in section 5.10 (or 5.11 in the first edition) it is just stated that main is called with two arguments. So apparently `main(int argc, char *argv[])` was meant to be the only actual prototype, and `main()` just omits the arguments.
This is not strictly a problem—not having allocated `left` or `right` is the big one—but calling `strlen` each time through a `for` loop will add an O(*n*) factor to your run time unless the compiler happens to understand `strlen` very well and optimization is enabled. This is a bad habit to start; cache the string length in a `size_t` variable first and refer to that, or find it manually as you walk through the string. Also, `int` is potentially too narrow and/or signed to represent a string length, which is of type `size_t` (from `&lt;stddef.h&gt;`).
In the second edition (4.2, page 73) K&amp;R actually recommend to use void for functions taking no arguments, and state that an empty argument list is allowed just for backward compatibility with old programs. They just don't do it for main(), probably because they treat `int main(int argc, char *argv[])` as the only prototype for main; `main()` simply ignores the arguments.
I'm guessing OP is confused and their task is really just writing a subroutine containing a loop in NASM and calling it from C. The answer depends on the C implementation, but it's usually as easy as putting a label at the start if the subroutine and using the `global` directive to make NASM include it in the ELF export table. You can then declare an `extern` function with the same name in the C file and call it. You will of course have to make sure that the NASM subroutine adheres to the calling conventions of the C implementation.
That makes sense. So in the book, the only formal prototype for main was 'int main(int, char**)'? But leaving it blank just indicates that you're ignoring them?
If you’re having problems linking to your assembly (as others are suggesting), then make sure the symbol for your asm function is declared with a [global directive](https://nasm.us/doc/nasmdoc6.html#section-6.6); in AT&amp;T syntax you use `.globl` instead. This allows other code to see the symbol in your file. In C, come up with the prototype and declare it in a header file, optionally with `extern` (which is optional for non-`static` functions). This tells the compiler that your function exists, and since there (hopefully) won’t be an implementation in any of your C files, the linker will always refer to an external definition of the symbol. Also make sure you follow your platform’s ABI—interacting with C means that certain registers must be preserved across your call, and arguments and return values have to be set up in the right places.
That's essentially what `jmp` does. But when reading assembly code you'll often want to get an overview of its branches first before examining it more closely. Having seperate mnemonics for branching makes this much easier. And you'd also need seperate mnemonics for conditional branching anyway.
This has nothing to do with C. And the the error is quite clear, the computer has no idea what `x` is, so you even have yourself an idea what `x` is supposed to be.
'using' is a c#/c++ directive. are you sure you're programming in C? &amp;#x200B; let's see the whole code and the error message.
* [*Modern C*](http://icube-icps.unistra.fr/img_auth.php/d/db/ModernC.pdf) (Gustedt) * *C: A Reference Manual* (Harbison, Steele)
https://en.wikipedia.org/wiki/C99
You can’t link object files all containing a main function.
Thanks for the reply. Does this mean I have to create a new project for each file? Because I save each file as .cpp and of course this will be turned into .obj by the compiler.
Yes basically. I have added it to my previous answer. Otherwise keep the other source files but remove main, and you can call the functions from them if you have them declared in a header file and the header is included in the other source files or the source file with main.
In my point of view raylib library is more close to graphics.h in its programming style than SDL or fsml.
Gustedt's book is awesome! but it deals mainly with C11.
I guess so. * According to A10.1 ("Function Definitions", page 225/226) in the reference section of K&amp;R 2nd edition, `main() {...}` is syntactically an "old-style" function definition, and defines a function with exactly zero parameters. * 5.10 (page 114) says that "when main is called, it is called with two arguments". * A7.3.2 ("Function Calls", page 201/202) states that "the effect of the call is undefined if the number of arguments disagrees with the number of parameters in the definition of the function", but it is not really clear if the applies only to new-style function definitions. I think `main() {...}` is used because this old-style form allows it to ignore the two arguments it is called with. `int main(void) {...}` would be the new-style form, and this would mean undefined behavior because the main is called with the wrong number of arguments, at least in the language discribed in the second edition. ANSI C fixed this issue by allowing main to be defined with either zero or two parameters as a special case, so since ANSI C we can use the new-style form `int main(void) {...}`.
SDL2 with SDL2_gfx for shapes or SFML.
The number of differences is rather small. You won't have problems adapting.
There can only be one main() in any given project/program. BTW what do you mean by "add an item to the project"?
+1 for https://www.raylib.com/
It's because you don't have parentheses around the body of your `if` statement. Thus if (!ptr_file) printf("Not work"); return 1; is interpreted as if (!ptr_file) { printf("Not work"); } return 1; which is not what you want.
oh right. Thanks for your help
I can highly recommend using [Simple2D](https://github.com/simple2d/simple2d). It works on top of SDL, and has a dead-simple, stripped down API that is very easy to use. A friend of mine used it whilst learning C and was able to pick it up with no problems. Check out the example [here](https://github.com/simple2d/simple2d/blob/master/test/triangle.c). It also provides a simple interface for handling sound, keyboard input and more.
Project &gt; Add &gt; New Item (or Existing Item) This makes a new window under the same project. I was hoping to keep all of my lab stuff for one chapter under one project, but it looks like I have to start a new project every time instead.
Here are a couple of links you might find useful. [Are You Ready For C99?](http://www.25hoursaday.com/C99.html) and [Incompatibilities Between ISO C and ISO C++](http://david.tribble.com/text/cdiffs.htm). The latter compares C99 and C++98, but it's still a useful synopsis of a lot of the new stuff in C99.
Haven't done it myself, but I am pretty sure that C++ and [Qt](https://doc.qt.io/qt-5/examples-android.html) would be a easier.
 if (everything is fine) /* Carry on */ else printf("Oh no!"); I love this
To start with it might be easier to teach him [ascii graphics using VT100 escape sequences](http://www.lihaoyi.com/post/BuildyourownCommandLinewithANSIescapecodes.html) to start with. Also, I stumbled across [svgalib](http://www.svgalib.org/). From a quick skim of the site it looks like it might be worth checking out.
What are you expecting to happen? It seems to work as it should for me (CLion + mingw64 gcc8).
But that's no what's happening for me. Instead there is no change to the file.
And you're sure you're in the right build folder, with the binary corresponding to source code here? If so, I'm not really sure. This program seems like it should be doing what you think it's doing.
Your code is unreadable and I had to spend 5min trying to format it properly. You are also violating the C standard several times. I cannot understand what your program is trying to do, or what the variables even are. Could you clarify?
This is the result of a slow evening: a serious attempt at a pointless library. Silliness aside, I’m all ears for comments &amp; tips!
There isn't a question here either...
Guys im sorry im frantically trying to get a project done for my first year comp class. Basically the question is everything runs fine and values get passed through the functions fine but when i reach the final function "PrintOutput(double Result, float vi, float t, float a)" it prints all the stored data values except 'D'. D is my distance displacement from the previous function. So basically im having trouble passing data from my AccelerationDisplacement function back to main. Thank you for not berating me.. yet.
What is \*best\* depends on what are you requirements, knowledge, etc etc. It is up to you do decide what is \*best\*. &amp;#x200B; In my opinion GLUT is still king of the hill when it comes to simplicity vs features. You may try OpenGLUT or FreeGLUT as drop-in replacement for somewhat newer, but standard GLUT does just fine for the most part. No idea if Elemntary OS package glut or other libraries, but you can easily find it and install from the source, shouldn't be difficult. Don't forget to install hardware drivers for your graphics card. There are tutorials and examples all over the place on how to use it. For example [here](http://www.lighthouse3d.com/tutorials/glut-tutorial/). It is used in some books, for example in red book and if I am not misstaken in OpenGL Superbible, and in lots of code you will find online. It is there for use with OpenGL, and I would definitely recommend it as an easier option to go learning with and making simple demos and games.
I reformated your code so that it fits in one function and doesn't hurt anyone's eyes: #include &lt;stdio.h&gt; #include &lt;math.h&gt; #include &lt;stdlib.h&gt; #define ACC_LOWER_LIMIT 0.01 #define ACC_UPPER_LIMIT 50 #define VEL_UPPER_LIMIT 100 #define VEL_LOWER_LIMIT 0 #define TIME_LOWER_LIMIT 0.01 #define TIME_UPPER_LIMIT 60 int main(void) { double vel; double acc; double dt; do { printf("Acceleration (m.s-2): "); scanf("%lf", &amp;acc); } while (acc&lt;ACC_LOWER_LIMIT || acc&gt;ACC_UPPER_LIMIT); do { printf("Please enter the initial velocity in m/s:"); scanf("%lf", &amp;vel); } while (vel&lt;VEL_LOWER_LIMIT || vel&gt;VEL_UPPER_LIMIT ); do { printf("Time (s): "); scanf("%lf", &amp;dt); } while (dt&lt;TIME_LOWER_LIMIT || dt&gt;TIME_UPPER_LIMIT); printf("Displacement: %lf m\nVelocity: %lf m.s-1\nAcceleration: %lf m.s-2\ndt: %lf s", dt*((0.5*acc*dt)+vel), vel, acc, dt); return (EXIT_SUCCESS); }
Small library: check Allows me to use my own allocation strategy: check Documentation: check Tests: check Usable license: check It may be a shitpost, but it's a very well put together shitpost. I'm unironically starring the github repo.
SDL is modelled after DirectX. It is almost a rewrite of DX :-). They do locking surfaces and all the jazz one would do in DX. In my opinition GLUT or GLFW are much simpler to get started with for a beginner. They take care of all the details that SDL exposes. Even SFML might be a better choice, but it requires a knowledge of C++ and object oriented programming, maybe not something you want to toss on a beginner while he/she is learning other stuff like graphics. GLUT is just fine, supersimplified, and lets you focus on graphics part.
&gt; parentheses Curly braces - {} not ()
Well if you are really obnoxious you can, but it wouldn't be the most memory or cpu efficient way :-). For exampleTCL implements it's for loop and all the other reserve words as function calls (everythign in TCL is a command). If I am not wrong Bash does it as well so. But normally, one wants for as a language construct rather than a function call for efficiency reasons. In a nutshell, a for loop is a conditional jump, where for loops expression will guide execution of statements and where to jump after the for loop. Depending on how you want your code to work, you might wish to do take some care of label naming and variable naming so they don't clas with previous variables. You may see a for-loop as a new "block" or "activation record" or whatever your parlance is, where those statements in for loop are local variables or not. It depends on what you want to do. Say you have something like for(int i=0; i&lt;10; i++){ .... }, then you might wish to create new unique temporary name for i so you overshadow previous declarations of i if any. Same for statements in { }. Or you don't, up to your design :-).
Used to call this 'being hoisted by the short curly's'. Stick them in all the time and problem goes away (although the formatting argument starts!).
Keyboard and mice stuff is kinda hard in terminal linux, see [this blog](http://blog.robertelder.org/detect-keyup-event-linux-terminal/). I don't know the best way without X11, but my honest advice would to either use X11 or a nice wrapper library like SDL. You don't wanna have to reinvent this wheel.
Thank you for the article!
Needs autotools though.
Sure, but then again, the interpreters (TCL, Bash, etc) "translate" the for loop syntax into function calls. One could of course create a function in assembly implementing a for loop and then call that function in C, but that would be no different than calling any function. It wouldn't be anything like the for loop construct. You probably wouldn't be able to pass a code block as an argument for example (perhaps a function pointer would work), and so on.
Hold up. Can you give us a proper run down on what's supposed to be happening? comments maybe or something like that.
Perhaps what you're looking for is something like a macro, where you write a "generic" for loop and the assembler expands the macro for you replacing some variables, just like a C preprocessor macro would do. I'm not sure about nasm but I believe most assemblers provide some macro-like facilities.
You've forgotten to include: `#include &lt;stdio.h&gt;` and `#include &lt;stdlib.h&gt;` &amp;#x200B; and you should execute the binary in the directory you want "test.txt" to be written to.
For what it’s worth, I love your response to the GutHub inquiry as to why C and not C++.
Sorry for the terrible output. This is my first class programming. Basically the question is everything runs fine and values get passed through the functions fine but when i reach the final function "PrintOutput(double Result, float vi, float t, float a)" it prints all the stored data types except my 'D'. D is my distance displacement from the previous function (AccelerationDisplacement). So basically im having trouble passing data from my AccelerationDisplacement function back to main so it will be transferred back to my final function.
thing is friendo, i have barely any idea what’s going on in that code. mind commenting it line by line so i can possibly figure out where the program did an oopsie?
Dude I thank you for that, but had this assignment did not require specific methods, I would have just used it all in main as well. The purpose of this project was to implement functions and to get data types to pass through each function and produce an output.
 float GetA(float A_LOWER, float A_UPPER){ float a; float t; printf("Please enter acceleration:"); scanf("%f", &amp;a); if ( a &lt; A_LOWER || a &gt; A_UPPER){ a = 0.0; printf("Calculation was not completed due to error."); exit(0); } return a; } double AccelerationDisplacement(float vi, float t, float a){ double D; D = vi*t+0.5*a*pow(t, 2); printf("*********************************\n"); printf("*The distance traveled in meters*\n*\t is: %7.2f\t\t\t\t*\n", D); return D; } // So my accelerationDisplacement will not return my 'D' data back to main. The reason // I need it too is because its supposed to be outputted in my final function below // It prints my other data but it does not print my 'D' void PrintOutput(float vi, float t, float a){ printf("*vi: %6.2f a: %6.2f t: %6.2f *\n", vi , a, t); printf("*********************************\n"); exit(0); }
Not listed I have three other functions for getting inputs from the user and it returns those values to main and does a calculation in my displacement function.. just after it does that calculation I can not get it to store in the variable type and return it to main to be used in my final function.
Thanks, I hate it
`calloc(sizeof(hashmap), 1000081);` oh. ok then.
Could also double-check folder permissions?
The time I save by always including them outweighs the time I lose arguing about formatting. If I'm forced to adhere to a style, I toss in an inline comment as a reminder... Hmmm and now that I think about it, I'm tempted to start writing my (stylistically correct!) statements like this: if (something) //{ something_else(); //} Anyone else enjoy malicious compliance?
well I'd already included stdio.h on a different line not shown but adding stdlib.h did not fix the problem
Wow! Thanks so much for the responses, this is all super helpful!
In general, and especially when starting out, don't get lazy with syntax. It's better to be explicit and a bit verbose than to end up with seemingly mysterious bugs like this.
&gt; CLI needs only libc, library is freestanding! I starred it too.
Good lord no
[The `Makefile`][makefile] is really nice, though. Isn't it possible to set `CC` in order to cross-compile? [makefile]: https://github.com/sjmulder/leftpad/blob/master/Makefile
What tecuniques do you HAVE to use? Because your program is raping C standards.
Can you share more of your code? How are you checking the value of g\_supply\_temp? Are you sure that is the only place it is called? Is it a volatile variable in the caller function? Also, is this repeatable? aka Do you always see -2.718750 when g\_suppy\_temp is -50.0? Have you used [gdb](https://sourceware.org/gdb/current/onlinedocs/gdb/) for debugging?
Can you or is it worth changing O/S for the experiment ? It's been a while since I tried something similar, but it was doable on FreeBSD. Each device was just that, a normally behaving separate kbd or whatever in /dev. Std ioctls. Hard part was finding how to prevent text console and/or Xorg from hogging the devices. *kbdcontrol* for the former and *Option "Ignore" "true"* for the latter.
Librightpad ?
There is still a lot of really critical software out there written for C89 only and knowing C99 and C11 changes is a really good idea. However for the most part, the really critical stuff is still in good ol C89. An example would be all of OpenSSL and pretty much all of Apache and others.
&gt; 1000081 https://www.numberempire.com/1000081
Thinking about it, a single **midpad()** would fill many use cases: /* midpad */ midpad("Introduction", ".", "p14", 70, buf, bufsz); /* leftpad */ midpad("", ".", "p14", 70, buf, bufsz); /* rightpad */ midpad("Introduction", ".", "", 70, buf, bufsz); /* fill */ midpad("", ".", "", 70, buf, bufsz); /* concat */ midpad("Introduction", "", "p14", 70, buf, bufsz);
Naming it `pad` would make it even more pleasant.
`strrev` is not a standard C or POSIX function. What do you think defines it?
`strrev` is not a standard C or POSIX function. What do you think defines it?
in rtems, i add a oled\_write\_temp declare in a .h file, then get the right value
In the world of Unicode, it's not even clear to me what the reverse of a string would mean. Reverse the bytes? Probably not. Reverse the codepoints? Reverse the glyphs? Reverse the graphemes? Outside of dinky little homework assignments, I don't think a strrev would have much use.
&gt; in rtems, i add a oled\_write\_temp declare in a .h file, then get the right value This isn't helpful at all. I have used rtems. But I need more context please. What .h file are you talking about? Where is the caller? How g\_supply\_temp is declared ? How do you check the value of that variable ? Do you have a debugger? Since you are using rtems, I am assuming you have a cross-compiled toolchain. Do you have a cross-compiled debugger ?
Well you have a valid point there for sure. It would be interesting to try to reverse a string of UTF-8 bytes. One would need to gather up a full code point ( one byte up to four bytes of UTF-8 ) and push that onto an empty stack as a single character. Then gather the next UTF-8 byte sequence and push them in same order onto the stack. Keep going and that would reverse the UTF-8 sequence. Not sure what value the whole exercise would be however.
&gt; The `Makefile` is really nice, though Thanks, I always try to to put some effort in that. Didn't know about COMPILE.c etc and assumed it would be GNU make only but I'm happy to find that BSD make supports it too. There doesn't seem to be a direct equivalent for the `.o.c` rule though (`$(LINK.c)`, which includes CFLAGS, will do for now) nor for `$(AR) $(ARFLAGS)`.
Stay within C89 and everything you do will work everywhere.
Can't you just edit single files in VS, without creating a project? Genuinely asking, because I never used this IDE.
It's always refreshing to see a short and sweet `Makefile`. The world needs more of that. I ended up creating a huge GNU Make framework to automate things only to realize I'd recreated much of `autoconf`.
You have couple of options: for ( i = 0; i &lt; num_tries; i++ ) { /* do something num_tries times */ } or for ( ; num_tries--; ) { /* do something num_tries times, modify num_tries */ } or while ( num_tries-- ) { /* do something num_tries times, modify num_tries */ } or do { /* do something num_tries times, execute at least once, even if num_tries is zero, modify num_tries */ } while ( --num_tries &gt; 0 ); Choose what you like/need.
What do you mean "no change to the file"? What are you expecting to happen? The code you've given should create test.txt if it does not already exist, and write "test" to it. If there is already data in the file it will be lost. Is this what you want to do?
&gt; It's always refreshing to see a short and sweet Makefile. The world needs more of that. I ended up creating a huge GNU Make framework to automate things only to realize I'd recreated much of autoconf. Ha, familiar. I learnt to distinguish between configuration (including discovery) and building (dependency resolution, recipies). I try to use make only for the latter. (Here I cheated with `#ifdef __OpenBSD__` rather than using configure but it'll do.) &gt; I'm not sure if those variables are standardized by POSIX. I always use GNU make so I'm biased in that regard. Running make -p has always helped me discover variables and their definitions. From my understanding POSIX make doesn't have `+=` which precludes one from playing nicely wrt. CFLAGS and such so I try to stick to a common subset of GNU make and BSD make.
The Makefile is not nice at all because `?=` and `+=` are not portable `make` features. At the same time, `-shared` is not a standard compiler option and might not work on some systems. That's why autotools exist.
Don't guess if something is in POSIX. [Look it up](http://pubs.opengroup.org/onlinepubs/9699919799/utilities/make.html) instead.
iam using the usart log
Searching for `COMPILE.c` gave me zero results, and searching for `.c` gave me [suffix rules]. Looks like it's not standardized. That shouldn't stop anyone though. [Rules of Makefiles]: &gt; 1. Use GNU make. &gt; Don’t hassle with writing portable makefiles, use a portable `make` instead! [suffix rules]: https://www.gnu.org/software/make/manual/html_node/Suffix-Rules.html [Rules of Makefiles]: http://make.mad-scientist.net/papers/rules-of-makefiles/
Anybody can make rules. GNU make is not make.
What systems are going to be affected by this?
I would also recommend SFML, as someone already did. If you get errors like this one undefined reference to ... It typically means you didn't _link_ the graphics library when compiling your code. Anyway, if you go with SFML, here is a basic run through https://www.sfml-dev.org/tutorials/2.5/start-linux.php In addition to including a library in your code #include &lt;SFML/Graphics.hpp&gt; You also have to link those libraries when compiling the code. Notice this line, g++ main.o -o sfml-app -lsfml-graphics -lsfml-window -lsfml-system the options starting with -l basically tells the compiler to link the required libraries.
Personally, there is nothing good old about C89 :) I wouldn't recommend sticking to it unless absolutely necessary. OpenSSL is probably written in C89 because it needs to run on platforms where C89 is the only option. And that can't be many these days.
&gt; Anybody can make rules. This argument also applies to the POSIX standard. People simply made it up. &gt; GNU make is not make. GNU make is the best make though. It's got features that elevate it far above all the others. The `eval` function, for example.
GNU make is not the only make and is not installed by default on many systems. Given that it's GPL licensed, it's also annoying to distribute in some situations.
here is c solution to reverse utf-8 in place. &amp;#x200B; [https://stackoverflow.com/questions/199260/how-do-i-reverse-a-utf-8-string-in-place](https://stackoverflow.com/questions/199260/how-do-i-reverse-a-utf-8-string-in-place)
&gt; It may be a shitpost, but it's a very well put together shitpost. I'm unironically starring the github repo. But it's using null terminated strings :'(
The ones whose `make` implementations don't support it. Just because an example isn't readily available doesn't mean they don't exist. Just because extensions can save you some typing doesn't mean you should use them and eschew portability. If portability isn't important to you, by all means feel free to use whatever you like, but in general it's better to write your software to be as portable as possible, and that includes your makefiles.
Yeah I had a quick play with SFML. Is there a way to speed up the compile process? You have to compile, then compile again to link the libraries. Can it be done in one go?
FUZxxl sent me a first edition K&amp;R in 2016. I still use it when I need a refresher or inspiration. Even if it is out of date, I would not consider it useless
Never said it was useless. I have both editions on my book shelf. In response to the OP, woefully out of date is accurate.
This sort of bullshit is one of the reasons why I avoid IDEs.
I need to check if a number is even or odd, but I should never ever have to reinvent the wheel so please make sure to include a libiseven and a libisodd thanks
libiseven is included with recent versions of glibc but you'll need to define _GNU_SOURCE. On BSD you must _not_ define _GNU_SOURCE for iseven to be visible, but you'll need to link -ln. NetBSD has its own version of libiseven included but it has a different signature so there you'll want to get libniseven from pkgsrc (not in Q1 release yet, use anoncvs). If you're pledged, you'll need to add "numeric" starting with OpenBSD 6.6 or your program will crash. But regardless, if you're wanting to use it with 128 bit integers just vendor libnisevenw because builtin support is spotty all round.
The easiest way to swap the order of delimeter-separated items in place is to reverse all of the characters in each item, and then reverse the characters of the string containing both. While some other algorithms may be somewhat more efficient, the reverse-based approach is simple, and it will work just fine whether one is reversing bytes, code points, or grapheme clusters since the order of bytes within each grapheme cluster will get reversed an even number of times.
I can, but is not worth. I have a small game library that can use multiple controls in Windows, I wanted to make for Linux as well. X event loop is just like win32 event loop, it clumps all devices together, and spits out keyboard event, mouse event and so on. Win32 has added raw\_input, where they pack raw scan code and device it originates from. I don't know of something similar in X11. There is /dev/input in Linux and is readable, but hard part is to figure out what to read and probably to play well together with other processes, presumably X as you say. This is what I see in my when I list input/by-id: ○ → ls /dev/input/by-id totalt 0 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_Abyssus-event-mouse -&gt; ../event5 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_Abyssus-mouse -&gt; ../mouse0 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_DeathStalker-event-if01 -&gt; ../event6 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_DeathStalker-event-kbd -&gt; ../event4 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_DeathStalker-if01-event-kbd -&gt; ../event7 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_DeathStalker-if02-event-mouse -&gt; ../event6 lrwxrwxrwx 1 root root 9 26 jun 16.35 usb-Razer\_Razer\_DeathStalker-if02-mouse -&gt; ../mouse1 &amp;#x200B; And by-path will sort them more like mouse or keyboard. But as said, hard part is to sort out what to read :-). Then there is translation, but I guess I could use xmodmap or something. No idea. It is just a thought I had. Will look a bit more at libusb next week, and see if I can use that one.
as a DON'T REPEAT YOURSELF extremist, I am ok with this.
Semantic differences between the language processed by modern compilers and that processed by classic-era compilers go far beyond the syntactic differences between C89 and C11. The authors of the Standard noted that one of the strengths of C was the ability of non-portable programs to do things the Standard could not fully describe by exploiting "popular extensions". Classic-era compilers generally extended the semantics of the language by applying the principle "If some parts of the Standard and an implementation's documentation together describe the behavior of some construct, and other parts categorize an overlapping category of actions as invoking Undefined Behavior, give priority to the former without a good, documented, reason for doing otherwise." By contrast, "modern" compilers give priority to the latter even in cases where the former would be useful and would generally cost nothing. As a simple example, consider something like: unsigned mul(unsigned short x, unsigned short y) { return x*y; } On a classic-era compiler, that would reliably yield arithmetically-correct results, without side-effects, for all combinations of `x` and `y` whose value didn't exceed `UINT_MAX`. According to the published Rationale for the Standard, one of the authors' considerations when writing the integer-promotion rules which didn't require such behavior for values exceeding `INT_MAX` is that they expected that the only implementations that wouldn't behave that way were the rare ones that ran on something other than quiet-wraparound two's-complement platforms. Because the Standard imposes no requirements on what happens if the product of the passed arguments exceeds `INT_MAX`, however, some "clever" compiler writers view that as an opportunity to eliminate any branches in the calling code that could only occur in cases where the product would exceed that value. Such compiler writers claim any code that would rely upon the previously-commonplace behavior was always "broken", even though K&amp;R described the behavior of overflow as "machine-dependent", most platforms document how they handle overflow, and the authors of the Standard have expressly recognized that programs need not be portable to be conforming. If you're used to classic-era compilers, be prepared to be astonished by the ways that "modern" compilers' optimizers will break code using constructs that used to be non-controversial.
There is X Input Extension, with which you can differentiate between devices. I've used that to read motion from a spesific mouse, turned into a knob box. XListInputDevices XOpenDevice XSelectExtensionEvent etc.
Although compilers' debug information will typically report places where values are temporarily held in registers, they don't necessarily do this for values that are duplicated in registers and are not expected to change. If you have the ability to step through machine code and have at least a vague understanding of how instructions move values between registers, looking at machine code and registers can sometimes clear up confusion over what's really going on. My favorite "war story" in that regard was a Macintosh Programmers' Workshop C compiler circa 1990. A compiler loaded the address of a global object into a register that was supposed to be preserved across a function call. When I asked the debugger for the address of the object, it reported the address of the global object. When code tried to write to the object, however, it wrote to the address in the register. Had I not been able to use MacsBug to find out that the register that was supposed to hold the address of the object, didn't, I have no idea how I would have solved the problem [which turned out to be a result of a compiler generating incorrect code to create stack frames which were between 32768 and 65532 bytes]. While your problem is more likely due to erroneous code than a compiler bug, the ability to examine machine code can also be helpful in many other cases where program bugs cause a "const" object to be overwritten. If you add something like: unsigned volatile forceWriteHere; ... forceWriteHere = valueThatShouldBeConst; and set a breakpoint on that write, it should usually be easy to figure out what register if any is being used to hold the value in question, and whether its content matches the backing storage.
Cheapest I’m seeing there is ~$10 ?
Compile with all warnings enabled and make sure you don't have some stupid bug the compiler can detect. Use the debugger to figure out what's happening. This is not enough information for anyone to know what's going on.
A lot of embedded platforms (microcontrollers) only have C89 compilers available, and even then they are often not standards compliant.
The language that became popular for embedded systems is fundamentally different from the language processed by gcc and clang with optimizations enabled. In particular, whenever the Standard describes some category of actions as invoking Undefined Behavior, the language that became popular for embedded systems work interprets that as meaning "...*except* in cases where the Standard and/or platform documentation describe a useful behavior", while the language favored by the authors of clang and gcc interpret it as "...*even* in cases where the Standard and/or platform documentation describe a useful behavior, or where failing to process the action predictably would be indefensibly stupid." [using a rather low bar for the latter qualification]. From what I've seen, clang and gcc can be configured to process the former language, but not as efficiently as other compilers.
That won't work for combining characters (U + combining-^ for instance) and you would really have to consider what to do when you hit a right-to-left or left-to-right codepoints.
Are you saying you see the problem only when you call the function with no declaration? That's expected. A function that receives a float must have a declaration (either in the form of a prototype like in a header file, or the function definition itself) before calling it, or the compiler will promote any float argument to a double when calling it.
Sometimes when you’re writing out an integer as ASCII you’ll reverse the characters afterwards (usually can be avoided, but probably about the same cost regardless), but those are just in the alphanumeric range so no multibyte worries.
`strrev(x) = /* RIGHT TO LEFT MARK -&gt; */ 0xE2808F + x`.
&gt; Just because an example isn't readily available doesn't mean they don't exist. They're apparently so obscure nobody knows what they are. Is it worth it? &gt; Just because extensions can save you some typing doesn't mean you should use them and eschew portability. If you're writing complex makefiles by hand, GNU make's extensions are great. POSIX make doesn't even have [pattern rules]. Leave portable makefike generation to autotools. &gt; it's better to write your software to be as portable as possible, and that includes your makefiles. GNU make itself is portable. People can just install it. Alternatively, the maintainers of the platform's make can add the GNU make features to their own version. The GNU make source code is actually really easy to understand. [pattern rules]: https://www.gnu.org/software/make/manual/html_node/Pattern-Rules.html
There are several kinds of device involved in this. If you want raw-ish input directly from the hardware device(s), you end up doing different things on different platforms; on Linux you can go through the /dev/input side of things. Also look into [libinput](https://www.freedesktop.org/wiki/Software/libinput/), which might be a good place to start. If you want raw-ish input from tty devices (e.g., a binding of a keyboard to a terminal screen + translation of keycodes) then that works a little differently, but you can do it fairly platform-independently by using a handle of `ioctl` and termios calls on the appropriate devices. [Here’s](http://man7.org/linux/man-pages/man0/termios.h.0p.html) a manpage for the termios stuff; [here’s](http://man7.org/linux/man-pages/man2/ioctl_tty.2.html) the Linux ioctls, which include some of the \*ix ones.
&gt; They're apparently so obscure nobody knows what they are. Is it worth it? To the people who need to retool your code it is. Especially since it would have been easier for you to develop it with portability in mind, instead of them having to retool it. &gt; GNU make itself is portable. People can just install it. I feel like that solution is counterintuitive to the UNIX philosophy of "do one thing well." It's kind of wasteful to have multiple `make` implementations installed. It's also kind of wasteful to force people to install a bloated implementation that does more than it needs to. &gt; ... maintainers of the platform's make can add the GNU make features to their own version. Not every group has those resources available. Maybe you don't have the resources to justify writing portable makefiles either, but I'm willing to wager that it's easier for you to prioritize portability than it is for them to change the scope of their `make` implementation. Regardless of all of that, I was never saying you had to prioritize portability, it's just a best practice that I think is worth everyone's consideration. Sorry we disagree, I'm going to go back to work now.
Main competitor: http://clucene.sourceforge.net/
Props for those graphics.
Yep .. it just isn't even worth doing.
Your code is beautiful. Well written. Perfectly indented and so very easy to read. Anyone could read this : https://github.com/mmhanson/Basecamp/blob/master/linked_list/linked_list.c Excellent comments. What to fix or change? Nothing !! You keep on plugging away with clean code like that and everyone everywhere would love to read it.
Thanks :)
Lol I’m glad someone understands
It is your responsibility to determine whether `scanf()` was able to read a number and react appropriately if it was not. You can start by reading [the documentation](https://en.cppreference.com/w/c/io/fscanf).
Not trying to be a jerk, but that's a really basic issue. Looks like you skipped some steps while started learning C. In resume, you have to specify your input in the scanf function, %d is for int, %f for float, %c for char... The list goes on. I strongly recommend you to read some book about C, like O'Reilly practical C. It will be boring sometimes, but you I'll learn some really important peculiarities. If you sent me a DM I can sent you a link for a PDF.
In this case I’d say you should try to read the users input into an in memory buffer (ideally as a string). Check whether the string represents a number, then use sscanf to parse the string out. If it isn’t a number, print an error message and return a false value. I’m a newb to C as well so there may be better ways but this is what pops to me right now.
I prefer `strtol` over `scanf` for integers. It's easy enough to use, doesn't require funky format strings, and gives reasonably sane errors for bad inputs. There may be even better ways out there, too, but this is the best I've actually used.
True. I’m so used to languages which use parseInt, that I always forget strtol strtod strtof exist. Thnx for the reminder. (ღ✪ｖ✪)
 while(1) { unsigned int input; printf("Number: "); if (scanf("%u", &amp;input) == 1) { printf("The number is: %u\n", input); } else printf("Bad input\n"); int c; while((c=getchar())!='\n'&amp;&amp;c!=EOF); // clear buffer }
RTFM as we used to say back in the day :p
BOFH tool of choice.
http://c-faq.com/stdio/scanfjam.html
why i dont declare, compiler outputs no error
I feel like the C subs *really* need an RTFM sticky, and people teaching C classes *really* need to focus on the idea that the world is out to get you, and you can trust very little outside your own compilation unit. (Sometimes not even that.)
Thank you! Will definitely check it.
Yeah, I have already been checking on ioctl, conio (the linux version), termios &amp; co, inclusive curses and ncurses. I am just not really into it yet, but I will definitely play more with it. Thank you for clarification and pointing out libinput. I guess you are right about terminal and mouse, that probably isn't the best way. I just thought it might have been easier to go below X, but maybe it isn't.
Yeah, you should be able to do it in one go, something like this g++ main.cpp -o myapp -lsfml-graphics -lsfml-window -lsfml-system
with letters it will use acsii values. You can add a check whether value lies between 48 and 57.
You don't get any error because the standard says it's OK to call a function that hasn't been declared (section [6.5.2.2 paragraph 6 of the C11 standard](http://port70.net/~nsz/c/c11/n1570.html#6.5.2.2p6) says that when you call a function with no prototype, "the integer promotions are performed on each argument, and arguments that have type float are promoted to double"). So if you call a function without declaring it, you're responsible for passing the correct arguments. That said, if you enable warnings your compiler should tell you that the function is being called without a declaration. The error result happens because the function receives a double while expecting a float. What exactly happens depends on your system, but since these types have different sizes, the data will most likely come out garbled. The solution is simple, though: always declare functions before calling them, and enable warnings on your compiler (e.g. use "-Wall" if you're using gcc).
C is simple, especially comapred the the monstrosities that is C++ and rust. as for buginess, there's literally no way to tell which language tends to create more bugs, but with C++ being many times larger in scope and standard library and being an absolute beginner, it's not unfeasible that the complexity would give you some bugs on it's own.
Well, bugs are created by developers, so it ultimately depends on you. &amp;#x200B; There is a higher complexity in C++, however it does provide you a much larger arsenal of compiler checks and tools for writing code in ways that minimizes bugs.
I think putting aside literal bugs in the language, I'd think OP is technically correct in saying you might experience more 'bugs' in C than C++ or Rust simply because it's just about the only language I know of without a garbage collector / has explicit memory allocation. Add buffer overflows, memory copying, etc and it can be daunting to a beginner, especially one that either hasn't programmed before or has only used HLL's and/or OOL's
Its pretty subjective, but I know what you mean. It's pretty much based on the developer and the task at hand. I would say you are more likely to create memory leaks vs other languages if you don't know what you are doing. Bugs on the other hand depend on expirance in general with any language.
Hm. It's true that C++ is monstrously complex at this point. A significant portion of that complexity comes from features that are specifically designed to allow to you avoid bugs that are common in C. Memory management bugs, resource management bugs, string handling bugs, code duplication bugs, type safety bugs, etc. So I don't think it's very clear-cut. It depends highly on what you are building. I would personally say that if I'm building something complex, I would probably use "tasteful" C++, avoiding the typical pitfalls of over-engineered C++.
It's trivially easy to write buggy, unsafe code in either language, and fairly challenging to write correct, safe code. Neither language tries very hard to save you from yourself (in large part because most valid C is also valid C++). &amp;#x200B; Modern C++ gives you better tools for protecting against resource/memory leaks, but given the immense complexity of the language and your level of proficiency in it, I'm not sure if that'll yield a net improvement in bugs.
Agree. But it's a but annoying to have to reset errno to distinguish overflow from correct conversion of a limit value.
Exactly. I just wanted to know if hope's not all lost. That there's nice and safe functions that I can use to avoid most of the big issues :)
I think your problem might be that by default if you run your program using CLion's tools, it will build and run your binary in the cmake-build-debug folder. As you can see in [this help page](https://www.jetbrains.com/help/clion/run-debug-configuration-application.html) there is a run configuration option named Working Directory, which is the starting point of relative paths for your executable, try changing this to your project directory.
On the other hand, you have people asking for help while entirely ignoring compiler error messages because "the compiler is out to get me" when they could've fixed the problem themselves easily.
C isn't really simple, I'd argue its one of the most complex languages out there. Yes, there aren't many elements to it and you could't learn all C syntax plus standard library in a week, but C forces you to think a lot about stuff other languages automate for you. Example: Allocate and free resources. In C you HAVE to do everything by hand, C++ gives you RAII, smart pointer and dynamic data structs (list, map, vector...) for free...
Rust is quite safe as far as memory bugs go, but if you're not coming in with a fair understanding of programmatic memory, you might have a bad time.
C and C++ are always going to be inherently more buggy than a language that has managed memory. I know people say that it's developers that create bugs but the tools we use matter a lot. If you need to write a robust project, I suggest you use something like Java or C#. I'm biased of course because I used java a lot recently but I'd rather have a minor slowdown in speed than have to deal with segmentation faults.
It’s absolutely okay run into issue that will make you scratch you head. But with some tools &amp; coffee you will get it done. ☕
I'd say that it's easier to write non-buggy code in C++ simply due to being able to use references instead of pointers
Resetting errno is just one line of trivial code. Why is it annoying? :D
Bit of a bikeshed thread, but I consider it humanly impossible to write any sizable C or C++ project without bugs and at least borderline humanly impossible to do so without any security-related bugs in particular. History shows 99+% of even security-minded developers fucking up time and time again, if only because the large projects in which they participate aren't the work of a single person. A lot of subtle security bugs have to do with the way the vast majority of C programmers casually throw types around without regard for the integer promotions, usual arithmetic conversions, integer overflow, and such. And that in turn has a lot to do with the way the C language is taught. I can't think of a single C book that teaches those subjects with the emphasis they deserve. Hell, you're lucky if you get a C book that teaches how to use `strncpy()` properly as opposed to stuffing two or three obscure sentences into an appendix or such.
If the array contains pointers and you know there will never be any gaps, or that the pointers will never be NULL to indicate some error, you could interpret a NULL pointer as being the end of an array, and simply iterate through each time you need a size. Just have to remember your convention and the discrepancy between the number of available pointers and the actual size of the allocated array.
I think if you have a chance to choose between modern C and modern C++, choose modern C++. (I will be using just C &amp; C++ for modern C &amp; C++) Why? C may seem to be simple at first but trust me that's just an illusion. It's not type safe and void * pointers are really scary. Even though you could do object oriented design with C, it isn't equipped with OOPs constructs. You would need to do manual memory management. (Which isn't the case with C++ smart pointers) . C also doesn't have any error handling mechanism but just return codes. (C++ has exception handling, not a lot of people use it though). Plus the biggest issue, you need to implement common algorithms like sort, min, max in C or use a external library (but C++ comes with a standard template library along with many libraries of C language). And trust me, you are never compromising performance by choosing C++ over C. If not faster, C++ is as fast as C.
strncpy (or strlcpy if you can use it) would be fine here, why the hesitation? If you absolutely detest memory copying, allocate one entry before getting input and read directly into it, then tokenize the line, using pointers to each word. struct entry { struct entry *next; char *a, *b; // each point into storage[] char storage[100]; // imput line, poked with many \0 }; Too much fuss, IMHO.
I solved using a normal array, was actually pretty simple!
You can use `strchr()` to find the separator (a colon). This function returns a pointer, which then can be used to calculate the length of the string preceeding the separator, which then can be used to make a copy of that string with `strncpy()` (note the 'n' in name). If you advance the pointer returned by `strchr()` you will get access to the string after the separator, to make a copy with `strdup()`. Don't forget to `free()` these copies when no longer needed. Use pointers instead of hard-sized arrays in your linked list.
btw be careful cause strncpy is not a better strcpy, if you don't copy the null character at the end of the string copied, the copy string won't be null terminated. Anyway, that's a detail but don't assume those are safe functions. Otherwise many bugs in C are because people forget to free a malloc or make a string null terminated, so they are rarely complex bugs and more so simple but fatal omissions. Learn to use some debugging and logging and it will greatly help. Most tools/debuggers just require basics to save you from most programming bugs.
Personally I'd say c++ can encourage bugs, its tendency to obsessive abstraction can make it difficult to see what an actual piece of code is doing, there's a current fashion to mark anything verbose as bad and I suppose this is the excuse used for not properly commenting code too... &amp;#x200B; However code that is explicit and well commented is very much easier to maintain in the longer run.
The difference between "modern" and "antiquated" is mostly sticking to modern style guides and conventions. What makes C code prone to bugs is that you often have to write your own implementations of common data structures and algorithms. Templates and Object Inheritance let you re-use the same tested code as much as possible. It's very likely you will spend time debugging your container and collection code that you wouldn't in c++ since most STL implementations have been vetted 100 times over at this point. Unless you absolutely need the minimum dependencies, it's perfectly fine to learn a subset of C++ and use that for your projects. I.E. write most of you application in the procedural style, but make use of pre-written classes wherever it makes your life easier. Like using the string and vector classes instead of implementing your own in C. You can always go back and refactor as you understand things better.
What are `Global` and `variable`? They aren't defined in C to my knowledge.
Breaking out of the loop once the product is bigger than x will save you a lot of time.
References provide virtually no safety guarantees above pointers.
ok so I did add break statements and it does work faster, however for this question we are asked to find how many hamming numbers exist between two numbers. One of the test cases is between 1 and 10000000. It still takes a really time here because it has to check for every number in that range. Any more suggestions?
Declaration is when you create this variable. Definitely is when you assign a value to that variable.
Instead of trying to enumerate all the possible (i, j, k) tuples that _might_ be Hamming numbers, how about turning the problem around? If you've got a number n, how could you find out if it had any factors _other_ than 2, 3 or 5? What would happen if you were to take n and divide out as many 2's, 3's and 5's as you can?
How 1 and 2 differ?
Marking a variable extern declares it without definition. Even if you have that int over there.
yeah okay that made my program ridiculously faster and it works perfectly. Thanks a lot. Just wish I had thought of that myself!
References are essentially a static assertion that the value you are being given a reference to exists, whereas a pointer can be null; I consider that a significant safety guarantee.
Right, but there's no actual guarantee of that at all since it's relatively common to just dereference a pointer and turn it into a reference on the assumption that it's not null.
I would not try teaching an 11-yo kid C. Try Java, Go, Python, anything but low-level systems programming.
Could you provide me an example of a large body of code where this is so common?
1. It's just global variable.. No extern
If it’s just global, then it’s both
The thing with C++ is that you have to take the time to learn some common design patterns and stick to them. Once you do you can take full advantage of the language and eliminate bugs that are caused by low level memory manipulation (that is pointers which imo are the most common root of problems). Instead modern C++ uses a lot of abstraction techniques to memory management and many other things. That is of course if you are willing to spend the time to learnt them ;)
How 3. is both?
Sry, it should be just declaration in my view.
Actually C++ has no garbage collector too but modern C++'s features like smart pointers are used to wrap raw memory operations and provide some safety compared to C. C++ is as low level as C but with more features and some abstraction mechanisms.
Yes, let me just spent 3 hours trawling through all of the codebases I've worked on previously /s I've seen it happen quite a lot. You may think that references are a satisfactory form of pointer safety, but they're not.
&gt;Modern C++ gives you better tools for protecting against resource/memory leaks, but it also introduces whole new classes of bugs Could you elaborate on the "bugs" that C++ introduces?
If you're less experienced with it.
the biggest problem with C is that development has been mostly abandoned by the mainstream. so there aren't a lot of tools to catch your bugs as you type them. despite this, it's still manages to have the fastest compilers, produces the fastest running code with the smallest file sizes.
Wouldn't abstractions raise C++ above C in terms of how high-level they are?
 void dictAdd(size_t len, char stuff[]) { char org[50]; //TODO: make variable size char trs[50]; struct dict *entry = (struct dict*) malloc(sizeof(struct dict)); //you need to parse the stuff and get it into org and trs here //Then do a string Copy strcpy(entry-&gt;org, org); strcpy(entry-&gt;trs, trs); if (head == NULL) { //if list is empty, make element head of list head = entry; last = entry; } else { last-&gt;next = entry; last = entry; } };
&gt; Yes, let me just spent 3 hours trawling through all of the codebases I've worked on previously /s Your claim, your burden of proof.
Wrong. The state of the universe is that "things do no by default prevent people writing buggy code". The original assertion that you made... &gt; I'd say that it's easier to write non-buggy code in C++ simply due to being able to use references instead of pointers ...implies that references lie with the set of "things that prevent buggy code". This is a positive statement, and so the burden of proof sits with you. My claim is basically "there is no teapot orbiting between Mars and Jupiter", and you've now yelled "BUT FIRST YOU MUST PROVE IT'S NOT THERE". That's a total defenestration of Occam's razor.
&gt; The original assertion that you made... That was not my assertion.
IIRC you don't have to use the abstractions, so it's capable of being as low-level as C. I haven't touched C++ for a few years now and it was a fairly introductory course at that, so I may be wrong.
That's not how burden of proof works.
Everything thing that is done in cpp can be done in C. Just takes more work and understanding.
either language (new keywords) or library based CSP style concurrency.
If this is truly a learning project then knock yourself out, use whatever language you please. But understand that it probably won't go all that well, since it takes a while to become competent with C or C++ and your first efforts will probably be junk. If it were a serious project, I'd say that you should question your need for "high performance" and what it really takes to get it. You should understand in what way your project needs to have high performance and whether your chosen language actually helps achieve that. Usually it's best to use the most high-level approach that you can get away with.
I do not know what module is in C++, but in C it is just a pair of header and source files. There's nothing wrong with them, other than the lack of namespaces may cause name clashes. Modules make development faster because you don't have to recompile the whole project when only one or few files have changed. They also make code reuse easier, when you don't want to or cannot use libraries.
In order to get many things done in C you'll have to use two things that cause many issues for beginners: pointers and malloc. (Most) functions that come with manipulating memory and buffers can be written safely, but they are easy to mess up if you're not careful. You can get runtime errors (ie segfaults), or some worse things under the hood like not checking your buffer bounds which can lead to buffer overflow attacks that attack your program's eexecution flow and stack.
This seems more like the `puts` function rather then `printf`.
This is more puts() than printf(). printf() does all sorts of stuff around formatting, as well as buffered output.
Well, that literally just invokes the `sys_write` Linux syscall. It's the _last_ thing that's done to output a chunk of memory to a file. `printf`, on the other hand, has to do formatting, which is a whole other story, and buffering besides simply outputting stuff.
Yeah that's accurate. Old C++98 was largely a superset on C.
the first thing any assembly programmer learns is how to output strings to a screen. congratulations, you can go on to lesson 2.
I have to add to it considerably. It seems that I can only output strings, so I still have to work on converting numbers, plus formatting, etc.
Agreed. But, I can print "hello, world" in ASM, and that's the starting point in C, as well.
I can print "hello, world." That's the starting point for every language.
Everyone's calling me out on that. We all start with "hello, world," though.
That's poor design. If you have a pointer that could be null, you shouldn't assign it to a reference. Users should always be able to assume a reference is non-null, or you've broken the contract.
Absolutely! Even more, all functions that write anything to a file in Linux end up using the `sys_write` function provides by the kernel, which is basically an `__asm__` block containing almost exactly the code featured on the page. So that might be a building block of something bigger.
&gt;We all start with "hello, world," though. Depends on the language ;) But yeah sure start out simply, however it's still misleading when you say you have printf in 5 lines of code, you would have been better saying you have a print function in 5 lines of code.
True, but I think the function will stay the same. What will change is the equivalent of what is inside the parenthesis.
I’m going to get downvoted to oblivion for saying this but the lack of the features you mention is exactly why C can be such a simple and elegant language to use. It all depends on how you choose to manage your data. If you’re the type of developer who needs to new / delete a ton of things and have nested data new’ed into a parent, etc etc etc... then yes C++ seems like the better choice. But if you’re the type of developer that knows what data you’re working with, can allocate a block of memory that your systems use and carve out what they need, and use pointers in a sane way then C is actually quite safe.
This is the first piece of a neural network. I need inputs and outputs!
Perhaps they refer to the &amp;&amp; references related bugs, and string\_view is possibly one of the worst ideas ever. However, I still find that modern C++ is less error prone than C++98, especially if the [guidelines](https://github.com/isocpp/CppCoreGuidelines) are followed.
I am an electronic engineering student in uni and I have been programming in C extensively for literally all kinds of microcontroller projects because this is the only language that is taught and able to run on these devices. I learned C++ myself this summer and I can say I was largely wasting my time before. As long as your compiler is up to date, using modern C++ is much more convenient and error-free. Those who deny it are simply because they have read a book written in like the last century.
More work + More Understanding = C++ code in C More work + Less Understanding = Buggy C++ code in C Which do you think is more likely and why did you think that was a helpful thing to say?
Wake until you find out you can write a reverse shell in 90 bytes.
You can use a cross compiler to compile an application for Linux from MacOS; however, you’ll probably find it much easier to just set up a virtual machine or a Docker container to do it via virtualization rather than cross-compilation. With virtualization, you can compile and test your application natively and you can leverage pre-built dependencies rather than having to figure out how to get them cross-compiled as well. If you’re doing a lot of Linux development under MacOS, I’d recommend getting something like [Parallels](https://www.parallels.com), which makes it easy to run/manage VMs. If you want to stick with free products, VirtualBox or QEMU would work. For automating Linux compilation as part of CI, I’d recommend Docker. Docker makes it easy to spin up a container, run your build, grab your artifacts, and then tear down the container—all from a script.
Well, yes and no. Logically and algorithmically maybe. C can't do reflection and compile time checking and compile time evaluation.
C++ also has a lot of data-structures, algorithms and quality of life improvements you have to implement in C yourself (And some stuff you can never have). I don't buy the "simple and elegant" mantra of C. As an electrical engineer I use the most usefull tools to solve problems in a practial way, yes. But C is a shitty hack of a PL that should'nt exist in a sane world. Was ist usefull? Sure! Should it still be used? Meh. Too much undefined behaviour. Too much cognitive overhead for a programmer. And no language feature to use for big projects...
They should remove Annex K and C11threads.
finally! I knew the problem was something like this but I couldn't find out any of the specifics. Thanks a lot.
I use C and C++ everyday in my job and freelance. Then I needed to learn C# to create a desktop app. I was surprised how clean C# is as a language. No headers, the compiler is smart enough to figure out things. When you read a code from another c# developer, high probability you will get it faster versus reading a c++ code from another developer. In my opinion, C# is my standard of how clean a compiled language should be.
C by itself is useless. What make it are OS APIs and device drivers. New APIs are added to Linux nearly monthly. It's already an evergrowing language.
Add constexpr and/or consteval. So i could just force the compiler to pre-calculate values at compile time. Right now you need to use either very long sophisticated macros or modify the source with another program.
I agree, but why exactly? Is it because it's pointless and because nobody cares about it, or it's dead weight, or what?
Annex K is a poorly designed interface nobody wanted. C11 threads should be removed and threading left to the POSIX committee.
That's still a form of tracking though. Just implicit vs explicit.
1. The statement 'int a;' put at file (it's not global, but file) scope is a declaration. Variable a has external linkage and static storage duration. 2. The statement 'int a;' in a function scope and/or block scope is a definition. Variable a has internal linkage and auto storage duration. It's value is indeterminate, cause no initialization is specified. 3. This is type definition. 4. Variable declaration with an explicit initializer at file scope is a definition. 5. Where is it? At file scope, it's invalid. At block scope this is a definition of variable 'a' and an assignment. 6. At file scope both statements are the same. 'extern' is applied to every symbol at file scope, ie. Symbols declared at file scope have external storage duration by default. At block scope this is invalid - the second statement redeclares symbol with different linkage.
Thanks for that tip on Annex K. Found this that dies a nice field experience - http://www.open-std.org/jtc1/sc22/wg14/www/docs/n1967.htm
I find that language attach a certain standard style; i.e Java/C# have different styles which are considered best for that language. C/C++ still probably has older styles associated with them still; you certainly can use more modern code style with those languages.
For your own mental health, use rust or if you really want to C++ but not C
By abstractions I mean features of the library. The library is not part of the language itself but it uses the built in stuff. For example smart pointers are not built in (like they would in Java for example) but they are implemented using raw (C) pointers. You can use raw pointers if you want and when a language lets you do manual memory management it should be considered low level.
any language by itself is useless.
That's absolutely not true. C is basically a crude language that someone came up just to program Unix. It's simplicity and speed what made it what it is today. You can write fully fledged applications in any higher level language, without needing APIs from OSes. Higher level of abstraction makes them useful by themselves.
to make the code more safe use strncpy instead of strcpy to prevent buffer overflows
Note that the default compiler on macOS is a modified version of clang. I think this is rather difficult to do. Perhaps check out the darling project for some ideas.
Your main problem is that you're writing C# code in a C project.
 [https://idownvotedbecau.se/imageofcode](https://idownvotedbecau.se/imageofcode) Also, this does not seem to be C code. Please post to r/cpp_questions or r/csharp
I haven't even installed anything c related, only c#, and when I go to make a project it just says that it is compatible with c# so idk what I'm doing wrong
 if(i=j){ Should be if(i==j){ You're re-assigning i on each iteration, which is going to mess up your loop.
I realized that this could be read as *modern* C++ introducing bug classes, but I'm talking about just going from C to C++ in general. Exceptions are one of the big things that comes mind. If you're trying to write code that needs to be reliable, safe, and deterministic, and anything can throw a big goto to god knows where, through any number of potentially exception-unsafe transitive callers (that can be *perfectly correct otherwise*), exceptions can make code exquisitely difficult to reason about. Add concurrency to the mix and your headaches multiply exponentially. [This post](http://250bpm.com/blog:4) echoes some headaches I've experienced personally. C++ lets you do many things you have to do explicitly in C implicitly, and this is mostly good, until it isn't and you find yourself dealing with some object lifetime headache, or some initialization/deinitialization order headache, or you can't guarantee that a destructor cleans up correctly. C++ has higher-level abstractions for many things, but that also makes the actual behavior/performance/thread-safety characteristics more opaque and potentially error prone. My point is that C++ still largely places the mental burden of writing safe, bug-free code on the programmer. I'm not against C++ and personally use it for most things over C unless I have a good reason not to, but if you're unskilled in the language and don't consistently adhere to best practices, you're probably going to write bad, misarchitected, buggy code.
Why are you posting it in /r/C_Programming then?
Just to add on to this a bit: While the original question asks why the `scanf` loop doesn't end, the `scanf` loop *does*, in fact, end. The issue dragon_wrangler is describing here is sending it into an infinite loop *after* it finishes the `scanf` loop. You can get an idea of where something like this is happening by setting a breakpoint, or even just simply putting a `printf` between the two loops (like `printf("Reached second loop\n");`).
Tried to find the c# subreddit but didn't think it was r/csharp and not r/c# and I am desperate for help so yeah
r/csharp would seem like the place
Thanks...That solved my problem..!!
Did you get a compiler about using = within conditional?
Whether or not one would regard C as simple would depend upon how one would view the behavior of a function like the following: struct foo {int x,y;}; int test(void *p) { return ((struct foo *)p)-&gt;y; } The Standard would allow implementations to process the function by taking the passed-in address, adding a number of bytes equal to the offset of `y` within `struct foo`, reading an `int` from the resulting address, and returning that value to the caller. If one views the behavior of the function in that fashion, then the language is simple. It would only require that implementations process the function in that way, however, if certain conditions are met, though there has never been any consensus on what exactly those conditions are. If those conditions aren't met, a conforming implementation would be allowed to process the function in any way whatsoever. If one views the behavior of the function in a way that only requires the implementation to behave as described in cases where the Standard actually requires it, then the language is sufficiently complex that nobody can know precisely what it really means (since there has never been a consensus).
__attribute__((cleanup)) (as added by gcc, clang, icc) would be useful. It makes resource cleanup much easier and less error-prone; it essentially adds RAII to C. And I would welcome a type qualifier to make pointers non-nullable. I don't want a complex type system, but this is something that has to be documented in basically every api.
If you have a lot of hard drive space you can also try [this](https://github.com/kholia/OSX-KVM/).
From what I've heard on /r/ProgrammingLanguages this language kinda sucks and the implementation moronically handled, and it's unrelated to this subreddit but I'm not a mod.
I would be *incredibly* impressed if this can actually translate arbitrary C++ is into anything approximating readable code. Like, to the point where I don't believe it.
It can't, a fault was found with `println(s.len)` recently, and people at /r/programming found a hack to get around the language's safeness very quickly.
I was under the impression that C++ is strictly a superset of C. What's an example of C code that wouldn't be valid C++?
Originally, variable-length arrays were not valid in C++ but I think they're supported now (?) void foo(int n){ int array[n]; // Compiler error } The definition and behaviour of `inline` is different. And some other fairly minor points that prevent it from being a strict superset. https://stackoverflow.com/questions/3777031/what-prevents-c-from-being-a-strict-superset-of-c
Sure, it's poor design. Terrible design. But the language itself does very little to discourage it. That is bad language design.
Okay, thank you. "int new = 1;" is my favorite example, lol.
He is just giving us info.
Very informative answer, thanks !
What I'd most like to see would be a recognized category of implementations that was designed to be capable of just about everything that could be done with a "high-level assembler" that slavishly decomposed every program into primitive operations whose precise behavior would be controlled by the target platform. While slavishly processing code in that fashion would impede optimization, such a problem could be alleviated by adding directives to invite compilers to make various optimizations without regard for whether they might affect program behavior. Such directives could allow optimizations to be performed more easily, effectively, and safely than is possible using current "UB-based" approaches which were never intended to be suitable for such purposes. &amp;#x200B; Implementations in this category would only be allowed to jump the rails in cases where the underlying platform fails to behave as specified; otherwise behavior would always be defined as being at worst an Unspecified choice from among defined possibilities. If something like a stray pointer write knocks an execution platform off the rails, such behavior would be a consequence of the platform, not the implementation.
This.
Some info seems to be missing. "Global variable" probably isn't part of the C code but an explanation. Maybe they are using different fonts in the original text?
C# is off topic in this subreddit. Please post C# questions to /r/learncsharp instead.
That's fine. But don't call it `printf` if it's missing the core feature of `printf` (formatted printing).
Exactly! And that's why your claim of having implemented `printf` is so preposterous.
&gt; And I would welcome a type qualifier to make pointers non-nullable. Already exists! Use `int[static 1]` to declare an argument of type “pointer to array of int with at least one element,” i.e. non-nullable pointer to `int`. this does not add code but can improve code generation (as the compiler can assume the pointer can be dereferenced) and may cause better warnings to be generated.
I've thought about this too, but it's very hard to get this right for all people who are interested in this. `volatile` pretty much does exactly this by the way.
Just like any language (well, most languages) if you choose to allow stuff like this in your code then yes things get complicated. If you don't then things are quite simple. This is highly dependent on things like team size, etc.. so don't try to take this argument as a "its best for all." As everything in software development... "it depends" and you can come up with use cases on either side to fit your argument. Having said that my C code is a lot like the old id software code (open source, look it up) and yes a lot of that code is fairly straightforward and maintainable. This works best in smaller teams that can control their codebases. As the team grows you need more automated tooling to ensure that functions like the above don't creep in.
&gt; C++ also has a lot of data-structures, algorithms and quality of life improvements you have to implement in C yourself (And some stuff you can never have). Depends on your use case and situations. The answer is never as black and white as people are saying. Due to the nature of hardware and how its changed over the years a sequential memory block of data housed in a simple array is quite fast (maybe not in a BigO notation sense but in a cache miss sense) and the more you `new` things the more you end up pointer chasing which causes more slowdowns. Again, all of this is highly dependent on the situation so i CAN NOT say that you're right or wrong.. but i can absolutely say that for the domain I use C for I wouldn't give it up for C++.
They’re assertion-ish in the type theory sense, but there’s not necessarily any means of determining whether a reference is null at compile-time, and if it does turn out to be null at run time then that’s UB. So it’s a safety guarantee except when it’s not.
Debuggers can see enum values. Most debuggers can't see preprocessor macros.
`int input[n];` defines a _new_ empty array. If you put this in a loop, a new array will be allocated on _each new iteration_ and destroyed at the end of this iteration. Furthermore, this syntax is not standard, and thus may not be supported by standard-complying compilers. You should allocate memory dynamically with `malloc` instead.
Yes, but at the moment using `[static 1]` for non-nullable pointers is unusual and will probably confuse many people, and it feels like abusing a language feature for something it wasn't meant for. I haven't seen it in any real-world API yet. Maybe it becomes more idiomatic if the standard library uses it; Jens Gustedt [mentioned this](https://gustedt.wordpress.com/2018/11/12/c2x/) as an "idea floating around" for C2x. Besides, it is just a halfway solution because you can't use it for return values.
`pow(2, i)` is (a.) way heavier than what you need, and (b.) prone to really bizarre errors if you overflow the eventual `int` type the `double` result needs to fit in. 2^(*i*) is usually best as `1 &lt;&lt; i`, although your use of signed `int`s makes it dangerous for `i` ≥ `CHAR_BIT*sizeof(int)`−1. The other `pow`s don’t need to be recomputed over and over again; build them up with normal integer multiplies as you run the loop. Here’s a good place to use multiple iterator variables, actually; unsigned long pow2i, pow3j, pow5k; for(i=0, pow2i=1; i &lt; whatever; i++, pow2i += pow2i) etc. Note that your pow3 and pow5 variables may overflow `int`, which is UB. I recommend using `unsigned` for those and implementing a checked multiply, something like unsigned long mul_checked_ul(unsigned long a, unsigned long b) { return a &lt;= ULONG_MAX / b ? a * b : 0; } If that returns `0` (which could be a normal case more generally, but not for exponentials) then there would’ve been an overflow. More generally, don’t use the `math.h` stuff for integers. It cacn cost a lot to hand off between the integer and fp parts of the CPU, and the `math.h` functions are built to handle a very different kind of number.
From [this post](https://www.reddit.com/r/rust/comments/6rxoty/tutorial_cross_compiling_from_linux_for_osx/) I learn that MacOS GUI apps need to be signed, and you can only do that on a Mac.
I mean, nothing stops you from adding an assertion or comment to indicate that a pointer cannot be null.
I would also add that you should validate your input. I can see very obvious division by zero and a segfault (assuming OP keeps using the VLA) bugs here, depending on what the user puts in.
The dialects that allow such things are much *simpler* than those that forbid them. Aside from the vaguely-named identifiers, I'm not sure what you see as being wrong with having a function accept a `void*`, convert it to a structure pointer, and access a member thereof. How would one use a library like `qsort` to sort an array of structures without having the comparator function perform such casts and dereferencing operations? If one wants to be pedantic, I think one could make a strong case that the Standard is written in such a way that the behavior of the vast majority of code isn't mandated by the Standard, but instead by the once-popular extension "don't be obtuse". For example, given: struct foo {int x[2]; } a,b; void test(void) { a=b; a.x[0]=1; b=a; } The second assignment uses an lvalue of type `int`. If that is not viewed as affecting the stored value of `a`, then a compiler would be entitled to omit the writeback to `b` since the stored value of `a` will not have been changed since it was loaded from `b`. If it is viewed as affecting the stored value of `a`, then code would invoke UB because `int` is not among the types that code would be allowed to used to access a `struct foo`. I don't know of any compilers that are sufficiently obtuse as to not recognize the effect of the second assignment on the stored value of `a`, but that doesn't mean the Standard actually requires such behavior. Any interpretation I can formulate that would define the behavior in the above case would also define it in cases that gcc and clang treat as UB.
Ok, so I concede that if you take a null pointer and pass in the dereference of it to a by-reference argument then you break the argument's contract and can cause undefined behavior. Having said that, for this to happen you need to take the very explicit step of dereferencing the pointer to pass it into the argument, so you can see what what you are doing is unsafe behavior and therefore you had better be sure of what you are doing.
I can't look at your indentation
Remove: gets(), strncat(), ato*(). My additions would likely change C into quite a different language: either a first-class string type or type-safe maps. There are other things I would have preferred to see added at the start, but for which it is now likely too late (for example, non-global locales and a richer error-reporting scheme than `errno`).
True, you're still using data not otherwise needed. I guess even with compiler extensions letting you query allocated memory, something, somewhere, is tracking that memory.
A bit obscure I guess, but printf("hi"+sizeof(' ')); ... works in C++ (where ' ' is a char) but not C (where ' ' is an int) unless `sizeof(char)==sizeof(int)`, which is allowed but unusual.
CFLAGS = "-Wall -Wextra -Wfatal-errors -Werror -Wformat=2 -Wno-format-extra-args -Wno-format-zero-length -Winit-self -Wimplicit-int -Wmissing-include-dirs -Wswitch-default -Wswitch-enum -Wunused-parameter -std=c99 -D_POSIX_SOURCE -D_POSIX_C_SOURCE=200112L -D_XOPEN_SOURCE=700 -pedantic-errors" that should do it for basics.
They're (mostly) zero cost abstractions. If you want to go conservative even x86 assembly is not low level anymore, let alone C.
Had it not been a poorly designed interface, would the fact that nobody wanted it be the only argument left against it? I'm asking because the idea of a standard threading library isn't inherently bad, it's actually really good. Sure, it's pretty much useless in C++, and orders of magnitude more so in C, but it's not inherently bad. So in other words, the question is: Was it ever feasible to come up with a good standard threading library design? Or it is and always has been doomed to fail?
do not fix c99... it works, don't fix it.... seriously..... A number of languages have been chasing the latest programming fads, to their detriment, especially Java... C works in so many platforms in many cases to a slight tweek to a simple makefile (usually just for paths) its almost like a cross platform asm, why destroy it by adding half arsed ideas that frankly just ain't needed....
&gt; Had it not been a poorly designed interface, would the fact that nobody wanted it be the only argument left against it? Annex K (poorly designed) is a separate thing from C11 threads (out of scope). The latter have an okay design but should be left out because threading is the job of the operating system and different operating systems have such different ideas of how concurrent processes are supposed to work that a single API won't cut it. The C standard was always meant to be OS agnostic and C11 threads impose a significant constraint on that. Note that we already have a standardised threading API. it's called pthreads. If the C committee should standardise any threading API, they should standardise a subset of pthreads. But they didn't. And that's the other key problem. So to summarise: we have a mostly useless threading API that significantly restricts implementation's ability to have their own idea of what concurrent programming is supposed to look like. That at the same time encroaches into POSIX' ability to define the standard threading API. IMHO the only thing we should keep of C11 threads are atomics (and their support functions) and thread-local storage.
Anything can be done in binary as well, but that's not exactly helpful when we're talking about developing software
It seems like a Go-like wrapper to Go.
https://www.reddit.com/r/programmingcirclejerk/comments/c4la0z/v_language_is_released_turns_out_v_stands_for
`strlcat` and `strlcpy` would be nice.
Ha feel the wrath of the downvote!
Following
What's the windows equivalent?
Ditto? I've got people who literally follow me to downvote. It's what happens when you anger Conservatives on this site. What you said was still just a snivelingly useless addition. It was neither helpful to the OP nor did it further the discussion. Don't bother saying something so meritless.
In the same sense that a tractor and an 18-wheeler are the same, except for all the stuff around the fuel.
Here is a simple example that you are likely to encounter in the real world: int *arr = malloc(N * sizeof(*arr)); Not only is this valid C, but it is also considered good style. In order to make this valid C++ however, you need to cast the result of malloc: int *arr = (int *)malloc(N * sizeof(*arr)); This is because C++ is more strongly typed than C. But in C, the cast is unnecessary and it can conceal some serious bugs if you accidentally cast to the wrong type. (when you cast like this in C, you are saying to the compiler "trust me, I know what I'm doing")
Please do not delete your post after receiving an answer.
I would not enable `-Werror` with all of these.
Thanks this pointed me the right ended up ditching scanf for getchar in a loop
Personally I'd say it depends on what you're trying to accomplish, as well as what C++ features you're using. If you're writing kernel-level code, C is going to be way less buggy than C++, and if you're using fun features like templates, those can cause lots of bugs that obviously aren't possible in C. That said, I find that proper use of OOP principles can make my code a lot less buggy when I'm writing stuff in userland (where most of my code runs), so for high-performance applications that are more than a few hundred loc, C++ tends to be less buggy for me than C. Remember that there's absolutely no reason why you should use every single feature that C++ provides; you should stick with the features that make programming \*easier for you\*. As long as you stick by that, your C++ code can be less buggy than "equivalent" C code.
I thought it was fully backwards compatible, that's C++'s whole shtick
``` static unsigned a[1] = {1 &lt;&lt; 31}; // change to 1u &lt;&lt; 31 to work with C++ int main() { } ``` This compiles with C but not C++
Implementations could process `volatile` in a way that would support the required semantics, but many implementations don't and there's no way that code for a freestanding implementation can say, e.g. #if (!(__STDC_VOLATILE_BLOCKS_COMPILER_REORDERING &gt; 1)) #error Sorry, this code requires stronger volatile semantics. #endif and then safely use constructs which build data in "ordinary" storage using ordinary writes, use a `volatile` write to give ownership to some other execution context, use `volatile` reads to find out if the other context is done with the storage, and then use ordinary reads to fetch the information stored by the other context. As for the difficulty of "getting it right", I think the biggest problem is the accumulation of decades worth of technical debt from the long-standing refusal to actually define ways of doing what needs to be done, and the consequent inconsistency of workarounds. The resolution for this would be to officially define a number of ways of accomplishing the things that need to be done, with some being preferred and others deprecated. Before a construct can be properly deprecated, however, it must be recognized, and there must be a proper replacement that's just as good. If the Standard would ever get around to adding compiler-memory-ordering intrinsics (which wouldn't be required to offer any guarantees beyond a ensuring that the last preceding operation upon any object--qualified or not--whose address is observable would not be reordered across the start of the first following operation upon such an object) then reliance upon `volatile` could be deprecated in favor of such a directive, but as it is the Standard offers no way to ensure that a compiler can't reorder things. Even if code does something like: volatile sig_actomic_t buffer_ready_for_transfer; void initiate_transfer(void) { buffer_ready_for_transfer = 1; } void (volatile *)initiate_transfer_proc(void) = initiate_transfer; memcpy(buffer, whatever, size); initate_transfer_proc(); a compiler wouldn't be required to perform the `memcpy` before setting the `initiate_transfer` flag, since the Standard would allow a compiler to rewrite the code as: void (*)temp_proc(void) = initiate_transfer_proc; if (temp_proc == initiate_transfer) { buffer_ready_for_transfer = 1; memcpy(buffer, whatever, size); } else { memcpy(buffer, whatever, size); temp_proc(); } I don't think any of today's compilers are "clever" enough to make such optimizations, but compiler writers have made clear that their failure to make an "optimization" today does not imply that future versions will continue to miss such opportunities.
&gt; if only because the large projects in which they participate aren't the work of a single person. This is a key point; I've found that even with only 2 people working on a project it's extremely important to design and document in a way that is conducive to such development. It's easy for 1 person to do something to the code that they've internalized and have the other person not realize the implications of it.
&gt; what, if anything would you like to see added to C? ? &amp;#x200B; &gt; what, if anything would you like to see taken away from C? long long &amp;#x200B; &gt; i hear a lot of activity in c++ that they are adding modules. are header files really that evil? what advantages do modules have over headers? Compile-time scalability: Each time a header is included, the compiler must preprocess and parse the text in that header and every header it includes, transitively. ... C++ is particularly bad, because the compilation model for templates forces a huge amount of code into headers. source: [https://clang.llvm.org/docs/Modules.html](https://clang.llvm.org/docs/Modules.html)
In the long-ago, before Internet fora, the common wisdom was "choice of language doesn't really matter." It is true, and it is not true. The top two pathologies that seem to scare people about C are signed integer overflow and buffer overruns/memory overwrites. They are not hard to defend against but it can get a bit tedious. In order to defend against them, you need to be able to think about how to constrain the code away from doing that. Just understand that you are learning and don't by shy about throwing code away. It may not seem like it but it's a pretty good way to be productive.
1. I'm not understud. int a ; main() {}
&gt; This is because C++ is more strongly typed than C. Slightly. And there's nothing to keep you from using the C++ toolchain for what would otherwise be C code. Indeed, it's a pretty decent cross check.
It's almost backwards compatible.
If you're worried a lot about safety, use something else. I think the unsafety of C is highly overstated.
It remains ( at least to me ) to be shown that this matters all that much.
thx!
There's a lot of YAGNI in C++. I use a lot of the C++ capabilities but it wouldn't be that onerous to do it old school. As to UB - I remember vividly when that just... didn't... happen any more. Let's call that 1995. I don't know if the mores of open source caused it to come back, or if I was just lucky to miss a lot of UB in the wild but quelle surprise - I start reading Reddit in 2015 and find out the old plague is still upon us. I'll make that statement modulo some of the optimization based UB invented by toolchain developers for... reasons.
i==j not i=j, i=j is assignment and will always be true and that's why loops forever.
One use case where C++ makes it tougher is for nonblocking async. That means callbacks, and C++ is callback-hostile. Callbacks are really outside any and seemingly all OO languages. The deal is - you can use model checking concepts with fully async stuff and having to support callbacks will enforce a certain discipline. If your design is essentially message-sequence-chart oriented you can get a long way generating test drivers from those. And yes - there have been keyword cadges put into C++ to make it work but they're not ... attractive.
also avoid using int j inside for loop, declare j before using it, some c compilers don't allow this.
Better macro support. Let us write multi line macros with something like `#macro macro(x, y, z)` ... \#endmacro and `#defifndef NAME VALUE` which would work like `#ifndef NAME` `##define NAME VALUE` `##endif`
If I were a co-worker, I'd need as pretty good explanation as to why you're going outside the type system. I may also suggest that you use the ability of C++ to have multiple signatures of a call to your advantage to keep what type checking you have available.
 #include &lt;stdio.h&gt; #include &lt;string.h&gt; #include &lt;malloc.h&gt; #define ERR(x) { printf("%s: %s\n", __func__, x); return -1; } struct node { char *string; struct node *next; }; int main(int argc, char *argv[]) { if (argc &lt; 2) ERR("Missing input"); struct node *a, *b; if ((a = b = (struct node *) malloc(sizeof(struct node))) == NULL) ERR("malloc() failure"); for (int i = 1; i != argc; i++) { if ((a-&gt;string = (char *) malloc(strlen(argv[i]) + 1)) == NULL) ERR("malloc() failure"); (void) snprintf(a-&gt;string, strlen(argv[i]) + 1, "%s", argv[i]); if ((a-&gt;next = (struct node *) malloc(sizeof(struct node))) == NULL) ERR("malloc() failure"); a = a-&gt;next; } a-&gt;next = NULL; while (b-&gt;next) { printf("%s\n", b-&gt;string); a = b; b = b-&gt;next; free(a-&gt;string); free(a); } free(a); return 0; } Here's a more dynamic way
How would you make use of `qsort()` or any other function which is designed to pass caller-supplied data to caller-supplied functions without having to know or care about the details of the types involved? In the language Dennis Ritchie invented, the programmer was responsible for ensuring that functions are passed pointers to objects with suitable layout, but functions could operate upon objects with suitable layout interchangeably. Some tasks don't require such abilities, but for other tasks they are indispensable, and they don't make the language more complicated. It's only the Standard's unworkable rules that complicate things.
&gt; I consider it humanly impossible to write any sizable C or C++ project without bugs and at least borderline humanly impossible to do so without any security-related bugs in particular. History shows 99+% of even security-minded developers ... I suppose that design is a dead art. :) You don't write a *program*, you implement *protocols* with a *program* and demonstrate that the programs are some subset of closed over the domain of those protocols. You use the term "message" liberally and make that whatever flavor is performant enough to work. That's the only way anybody really ever gets anything to scale. 40kloc is a middling to rather small system. It may well be a bit much for a first project. I cheated ( 95% of the code is generated in a scripting language, which is a whole valley of pain on its own ), but I put about 15kLOC of Cish looking C++ together last week. Granted, I'm still debugging it but still... it's the 5% that's being stubborn :) In reality, the problem being solved is still revealing itself... And I will say - when there was still money being made, security got pushed to the back burner. It's only when the money stopped - when Web2.0 made UGC the central artifact - that any attention at all was paid. The result is the world in which you now live, for better or for worse...
C doesn't require multiple files, it's just for organisation
qsort() gets a pass because it's legacy and because it's a public utility. I mean that as a team member, casting from void ( unless you have 1) a pretty good mechanism for showing it doesn't cause Bad things and 2) it really does produce a superior solution ) at the very least will cause confusion :) &gt; It's only the Standard's unworkable rules that complicate things. That's a pretty fair observation, although it's really a phenomenon based in the cultural decline in interest in simply producing things, simply. The Standard is giant collection of "something must be done; this is something; this must be done."
The design of qsort() isn't perfect, but the fundamental aspect of a callback using \`void\*\` to receive pointers to client-supplied data of a type the library function knows nothing about is hardly a "legacy" issue. As for the Standard, C89 was designed to specify a "core" language which implementations intended for various purposes were expected to extend as appropriate to achieve those purposes. If all compilers were processing some action usefully, and there was no reason to expect that compiler writers would even think of doing anything else unless they had a really good reason, nobody should have reason to care about whether the Standard actually mandated that the action be processed in commonplace fashion.
Compare: You: *there is someone in another part of the town that sells weed* The compiler: *cool story, bro* Versus:: You: *there is someone in another part of the town that sells weed*. *we’ll get some* The compiler: *okay, we’ll get in the car and go there. hey, linker, you’ll put the address of that guy in the gps* The linker: *never heard of him, you need to find something else to do*
Absolute fact.
And also a complete fucking hoax, and btw the syntax is garbage.
Deffo not Threads.h, it's great if any platforms would actually support it.
I'd like enums to be able to store unsigned 64 bit ints. I'd like some more attributes than C2x is currently talking about offering. I've got a bigger wish list somewhere that I halfheartedly thought about proposing, but I never really figured out how it all works and it's just a lot of work to get it to happen.
The problem with leaving threading to POSIX, is of course, the 800lb Gorilla in the room, Microsoft, which refuses to support POSIX (or recent C standards evidently but that's another issue.)
Yeah, that is a killer feature C++ has. Hell, I'd even be ok with the preprocessor's job being expanded to support it, idc. it'd be great if there was a way to get a function's name that calls another function at compile time based on which function it's calling.
`[[cleanup]]` is C2x's syntax for attributes, and feel free to propose it. I emailed Aaron Bellman about how to propose attributes to WG14 (the C standard body) and he offered me some great advice and helped me walk through the logic of my proposal. I mean he eventually convinced me it was a bad idea, but he was very helpful.
I want to add: - A user-defined namespace for compile-time symbols (for macros, types, enums... but not for functions which cause name-mangling issues). - Named and/or default arguments for functions (like C# or Python, but in easy-to-parse way). I want to remove: - So many undefined behaviors. If one is cannot be defined by standard, then at least a standard macro should say its behavior. - Cryptic and inconsistant naming of standard functions. (Yeah, I know it cannot be fixed.)
Why not?
I agree with this comment. C99 has all the features I need when writing C; if I want more features, I'll just use C++
...you don't intend to write a neural network in C, do you?
It's not an error
John Carmack did it! Actually C++ but still... He said he should have just done it in C lol https://www.reddit.com/r/MachineLearning/comments/82mqtw/d_john_carmacks_1week_experience_learning_neural/
Make a struct and store it separately in several uint32\_t.
If you need to check palindrome, you can save the number as an array of chars and compare bitwise
if it's just ones and zeros you can just store it binary, bit wise operations do take a bit of getting used to but it's not too complicated. Looks like it would be 24 bits so 3 chars or anything that's 32 bit will get the job done. A vector with an element for each decimal place/bit is probably the easiest way if you need to do lots of iterating through it. It's not the most efficient or elegant way but it's simple and fast to implement. So it really depends on the kind of operations you need to do and how important performance or efficient use of memory is to you.
You need to think what is the range of the number and what operations are you going to do with the number, and choose accordingly: 1) store lower and upper halves in separate long/unsigned long (or a list of segments, if two longs are not enough) 2) store the number as a product of two longs (or a list of multiplicands, if two longs are not enough) 3) store it as a string 4) store it as an array of bytes one decimal digit per byte 5) store it as an array of packed BCD (two decimal digits per byte) ... this is not an exhaustive list, other task-specific representations are possible.
This is probably a binary number with only 24 bits; it is not really that big and fits in a single 32-bit integer variable (`long`, usually even `int`). But you cannot specify binary literals in C code - C has only decimal, hexadecimal and octal literals. (C++ allows this tough, and your C compiler might support it in C too as an extension.) So to use this number in C, just convert it e.g. to hexadecimal: unsigned long foo = 0x989680; ...or to decimal: unsigned long foo = 10000000; If it is not a constant but a binary number that you get at runtime as a string, then you have to iterate over the individual digits and calculate the number via bitshift.
`extern int a=10;` at file scope is very nearly equivalent to `int a=10;` at file scope: int a = 10; int main(void) { return 0;} extern int a = 10; int main(void) { return 0;} In the above two cases, both declarations are *external definitions* (a needlessly confusing term, imo), and both identifiers have *external linkage* (external definitions can also have *internal linkage*, hence the needless confusion -- think `static int foo = 42;` at file scope). Compilers may warn about the second case, but it's actually fine. static int a; int a = 10; int main(void) { return 0;} static int a; extern int a = 10; int main(void) { return 0;} The above two cases, which you honestly shouldn't bother indulging too much -- because nobody sane programs like this -- show why `int a = 10;` and `extern int a = 10;` aren't exactly equivalent. In both cases, the declaration `static int a;` is a *tentative definition* of an identifier with internal linkage. In the first case, there's a linkage conflict yielding undefined behavior because `int a = 10;` defaults to external linkage. In the second case, there's no linkage conflict, because `extern` does more than most programmers think and has a linkage-inheritance property.
Don't go overboard with a rigorous hunt for something that satisfy a linter or style convention. Instead, go and hunt some bugs. After a while, you'll get a general idea what buggy code look like. Avoid writing code like that.
If you are working with someone else's code, use the indentation style that matches the existing code. If you wrote the code to start with, choose any one of the popular indentation styles and stick to it. Consistency is king. You should definitely compile your code with `-Wall`, which turns lots of useful warnings on. While debugging, you should also consider using `-fsanitize=address` to detect memory management bugs, and/or `-fsanitize=undefined` to detect undefined behavior, although the extra checks will slow down your program.
Indentation style doesn't really matter, just pick one and stick to it. Also, use `clang-format` instead of wasting time formatting your code manually. &gt; are there any warning flags I should have * use `-std=c17 -pedantic`, unless you really want to use language extensions * always use `-Wall` * you might want to you use `-Wextra`, but it sometimes reports false positives and stings that are a matter of taste; but you can disable some of the `-Wextra` warnings individually. And write unit tests, and try to get 100% (branch) test coverage. 100% test coverage doesn't automatically mean that your code and your tests are good, but trying to get there forces you to to write good modular code, think about edge cases, remove unnecessary/redundant code etc.
Writing software is a three stage process: design, code, test/debug. In my experience, code generally takes time proportional to how big the program you are coding. And generally speaking as well, the less time you spend in design, the more time you spend in test/debug, usually disproportionately. So, my advice, spend time up front in design: data structures, algorithms, code structures (APIs). Don’t skimp on documentation. Also spend time on a QA plan - test harnesses, test coverage, etc. Adage: if it ain’t tested, it’s broken. Code to your own level of familiarity. Using lambdas or closures or ... when you are uncomfortable with them leads to bugs.
can you give the link to the thread please? /u/BadBoy6767
Because it's very easy to trigger any of these with perfectly valid code.
Why not pthreads though?
then use a test case to verify that the code is valid and brings back the desired result (s) and skip the file from using -Werror, but keep it for everything else?
&gt; If the Standard would ever get around to adding compiler-memory-ordering intrinsics (which wouldn't be required to offer any guarantees beyond a ensuring that the last preceding operation upon any object--qualified or not--whose address is observable would not be reordered across the start of the first following operation upon such an object) then reliance upon volatile could be deprecated in favor of such a directive, but as it is the Standard offers no way to ensure that a compiler can't reorder things. Note that C11 atomic variables do support memory-ordering specifications. Volatile is not meant for memory-ordering (in the sense of “visibility from other threads”). Note also that your example is bogus since the standard requires that all operations that have been sequenced before an operation on a volatile variable must have been performed at the time the volatile operation is performed; the compiler is not allowed to move the `memcpy` operation before the operation on the volatile variable. No memory barrier must be errected though. Given that you declare `initiate_transfer_proc` as a volatile pointer, dereferencing this pointer must follow the same semantics and cannot be moved before the `memcpy`.
No. Disable `-Werror` and simply watch for warnings. Compiler warnings are hints that something could be wrong and many of the non-default warnings alert you for perfectly fine code. For example, I think this code would yield at least two warnings: extern size_t strlen(const char *s) { size_t i; for (i = 0; s[i] != '\0'; i++) ; return (i); } Especially for beginners, it is important to distinguish what is forbidden to do and what is merely bad style. Enabling all warnings does not allow beginners to understand how the C language actually works because the compiler shouts at them for things that are not actually wrong.
Consistency in style is key. Enforce-it using tools (clang-format for example). If you are working with other people, use an agreed-upon coding style. If you are planning on publishing your work for open-source, use a popular coding-style (linux kernel is usually well-received). If you are working alone and are not planning on fostering a community, use whatever you prefer. But whichever you choose, use it consistently. It is key to avoid dangerous mistakes and bugs. &gt; I get that the GNU coding style looks awful, and I shouldn't use it, but should I completely discard it like Linus Torvalds suggests? This is subjective. If you enforce the GNU coding style on your developper base (in a team or open-source project), you will against a majority of your dev sensibilities. I'd say throwing it is the proper way to go. If you like it however, there is nothing wrong with using it in your personal projects. &gt; I'f I always compile with gcc without optimizations (-o0), are there any warning flags I should have on to ensure my code can be the best it can be? You should at least compile in -O2. -O0 output is terrible and is not more correct. For the warnings, -Wall -Werror is the minimum. Some will use -pedantic but it can be an issue with libraries / frameworks for example (when they export makefiles to be used downstream). But the -Werror is necessary. Do not let any warning in your code, they can *all* be fixed. &gt; Any other tips would be greatly appreciated. All C codebase should be covered by static analysis and fuzzing. They will produce a lot of false positives, and those needs to be squashed from the beginning. Adding those two tools down the road will be a lot of effort unless it is done progressively, making it impossible to properly cover your project. I will also maybe go against the grain, from what I read in other comments, but: unit-tests are important, but do not attempt to have 100% coverage with unit-test. This is not a good practice. Cover the complex parts, where it is necessary (crypto, IO, lock torture). APIs and behavior otherwise will be covered by integration testing. Unit-tests are heavy technical debts. It will slow you down in introducing not only features but also just better structure / refactoring. It will actually hamper you from improving your code if there are too many. They should be used, but only in critical and complex parts.
 GiT GuD! &gt;to ensure my code can be the best it can be? Define "best". Use a consistent coding style enforced by an official, justified and referenceable document and if you can't find one that suits your purpose, write your own. ###
\#include "stdint.h" Look in the file and you'll fine dtandard size definitions for int datatypes. Use uint32_t This is an unsigned 32-bit variable, more than enough for what you need
Synonyms for "best" include optimal and ideal.
I'm not interested in enforcing GNU style, but what about GNU standards? GNU style is a part of the standards, but the style can be safely discarded. Check it out -&gt; https://www.gnu.org/prep/standards/html_node/index.html. Why -O2? It just seems to be a speed optimized version of -O0, and -O0 is supposed to always produce code that behaves exactly the way the C is written (best for debugging). See -&gt; https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html.
Sounds good to me! How are you guys so fast at replying? You seem to have replied as soon as I posted.
Slow down my program or the compiler?
Will do. Why do you think manually formatting code is a waste of time? I do it all the time and it saves me a lot of headache. Ti-BASIC was my go-to language before C so I had terrible formatting habits. My code worked but was never readable. Formatting changed the way I saw code, which changed the way I wrote it. All for the better.
&gt;optimal What are you optimizing for? I usually optimize between speed of development and readability as a function of days till deadline. &gt;ideal PlatonicCode™ .
I'll have to check out effective QA. Thanks!
With enough subscribers to the subreddit, the probability of another subscriber reacting immediatelish to your post is rather high.
Your program, the sanitizers include instrumentation for runtime analysis in the binary, the checks happen at runtime. You need to turn them off when you are compiling your final release.
It slows down the running of your program, because the compiler inserts extra code into your program to check if memory bugs or undefined behavior would occur. That's why people often compile two different versions of the program: A 'debug' version compiled with no optimization and sanity-checks enabled e.g. `-Wall -O0 -g -fsanitize=address -fsanitize=undefined`, and a 'release' version with full optimization and no checks, e.g. `-Wall -O3`.
Oh cool! I didn't know this.
True.
I'm optimizing for bestness.
There is a program called *indent* that can swap between different styles. Style can be important to define if you have many people working on it, but if its just one person then do it your way and be happy.
that number is only 24 bits. should be easy to store in an `int32_t`.
Theoretically hand-formatted code can be better than automatically formatted code, because you have more understanding of the code than a formatter, and can try to make certain things clearer through formatting. But these are rare cases, and usually there are better ways to improve hard-to-understand code than through formatting. E.g. if you have a complicated if-condition, try to split it into meaningful variables or functions. In general, the most important thing is that formatting/indentation is consistent, especially if several people work on the same code. clang-format works well, can be fine-tuned very well, and you can add the .clang-format configuration to the project so other people working on it can keep the formatting consistent.
c# is diverging from that 'clean' feel lately. several years ago, i used to recommend it to every beginner. now i always just recommend C.
&gt; long long &amp;#x200B; i too would like to see native support for the data types similar to modern languages... but typedef is your friend? but i guess that would require another include file to have with all your programs. &amp;#x200B; &gt; Compile-time scalability: Each time a header is included, the compiler must preprocess and parse the text in that header and every header it includes, transitively. ... C++ is particularly bad, because the compilation model for templates forces a huge amount of code into headers. &amp;#x200B; i suppose that's why c++ has precompiled headers in vs. but c has headers as well and still manages to be among the fastest compilers. faster certainly than every other language i've tried maybe with the exception of D.
lol wut?
&gt; Why -O2? One good reason: Some warnings won't fire unless you have optimizations enabled because otherwise the compiler isn't examining your code thoroughly enough.
that will work. my memory is a bit foggy, but if you are going to be compiling large open source projects, you may need to install batch-building tools manually like make, cmake and perhaps some other nonsense. those however can come later.
MSYS2 gives you two things: * A unix-like development environment -- you can use a package manager to install libraries, and invoke build systems that rely on Bash-like shell scripts and makefiles, etc. * Support for more POSIX functionality than a standalone mingw-w64 install has. If you are learning then I would personally recommend using MSYS2 + QtCreator. That gives you an IDE with good features. IMO it is useful to have things like tooltip completion, inline compiler diagnostics, and integrated debugging when learning the language. [How to install MSYS2+mingw-w64](https://stackoverflow.com/a/30071634/1505939) [How to install QtCreator to MSYS2](https://stackoverflow.com/a/38337759/1505939)
All versions of the Standard have required that `volatile` accesses be ordered *relative to other volatile accesses*, and the C11 Standard adds ordering guarantees *between atomic-qualified objects and other atomic-qualified objects*, but no version of the Standard yet includes any means of ensuring relative ordering between ordinary accesses to ordinary objects and anything else, nor any means by which programmers can demand that an implementation indicate whether actions are atomic with respect to the target platform's native semantics. If a platform with a 32-bit locked-load/conditional-store instruction will be running some code processed by the Acme Implementation, and some code processed by BobCo Implementation, and both implementations use that instruction, atomic objects performed by either implementation using that instruction will be atomic with respect to both implementations even if they're not lock-free (in many hardware LL/CS implementations, stores may fail even in the absence of contention, and a free-standing atomic library may need to rely upon system factors it has no control over to make live-lock impossible). Unfortunately, the Standard would require that the Acme and BobCo implementations also supply 64-bit atomic operations even if Acme's 64-bit atomic operations would be not atomic with respect to other operations made by BobCo, and BobCo's operations would not be atomic with respect to those made by Acme. I'm actually at a bit of a loss as to when the "phony" atomics would actually be useful. If an OS supports a 64-bit increment that's atomic with respect to other 64-bit increments using the OS, and compilers' implementations can chain to that, great. If the execution environment doesn't supply a real 64-bit atomic increment, however, what purpose would a phony one serve?
Neat. I didn't know this.
&gt; that ~~are~~may not be actually wrong.
Related to the address sanitizer, you can also use -D_FORTIFY_SOURCE=2 to get some extra stack protections at the cost of a very, very minor performance hit.
Do you have access to Clang? If so, I'd recommend using Clang's scan-build tool. It's a static code analysis tool that will find a lot of potential problems for you. You don't need to actually release with Clang to use it, just have it installed.
I do! I'll definitely check it out as well as all the other utilities others have mentioned. Thanks!
&gt; What's the best way to assure code quality? code reviews and static analysis (which are just code reviews done by a computer). &gt; writing it right the first time This is impossible. :) Even though the code I push may be correct the first time I push it, it doesn't meant it was the first time it was written...
Aah, you got me! Do you mind explaining what you mean when you said that C is poorly typed?
&gt;a 'release' version with full optimization and no checks Your release build should still use `-Wall`.
&gt; Is learning about things like monads and lamba's going to make me that much of a better programmer? Should I always use monads and lambda's when applicable, lest I be bad at what I do? &amp;#x200B; Those things aren't C. Some people coming from other languages have shoe-horned them into C, but they are not C. Computer languages all have a philosophy behind them which is reflected in their implementation. C is a language that doesn't obscure the reality of registers and comparisons and jumps and returns. It's just a couple of steps less complicated than assembly. Program C like a C programmer for best results. If you're writing Python, by all means go crazy with lambdas. Your peers will mock you if you don't. If for some reason I'll never understand, you're writing production code in Haskell, monad away. When in Rome, I guess.
&gt; I'f I always compile with gcc without optimizations (-o0), are there any warning flags I should have on to ensure my code can be the best it can be? For regular building, `-O3 -Wall -Wextra -Werror -Wshadow`. For lots of useful warnings in GCC: `-O3 -Wall -Wextra -Wshadow -Wsuggest-attribute=pure -Wsuggest-attribute=const -Wsuggest-attribute=noreturn -Wstrict-aliasing -Wfloat-equal -Wpointer-arith -Wwrite-strings -Wformat=2 -Wtrampolines -Wstrict-overflow -Wunreachable-code -Wredundant-decls -Wmissing-declarations -Wold-style-definition -Wnested-externs` For lots of useful warnings in Clang: `-O3 -Wall -Wextra -Wshadow -Wstrict-aliasing -Wfloat-equal -Wpointer-arith -Wwrite-strings -Wformat=2 -Wstrict-overflow -Wunreachable-code -Wredundant-decls -Wmissing-declarations -Wold-style-definition -Wnested-externs -Wcast-qual -Wconversion -Wmissing-format-attribute -Wmissing-noreturn` The following (GCC) can be informative, but will emit a lot of warnings: `-funsafe-loop-optimizations -Wunsafe-loop-optimizations`
I had no idea. I remember someone somewhere saying something about how C code, while it may still be good, isn't great unless you pay attention to using monads, and that all good C programmers use them. Someone somewhere also mentioned something about reducing LOC count by using lambda's.
Which warnings are dependent on optimization levels? I've never heard of that before
It probably should. I initially decided not to include it because I would compile a debug version first, and then compile the corresponding release version from exactly the same code only if no issues were revealed while testing the debug version.
Ok... and what is your question?
First things I notice: - `nc + '0'` won't work if `nc` is bigger than 9 - Rather than checking `strlen(compress) &gt; len` at the end, you can check `j &gt;= len` inside the loop and return early in that case - Your comment says `aabcccccaaa will become a2blc5a3` when it should probably say `a2b1c5a3` (numeral 1 not letter L)
its a competitive exam question found on a random website. It was a total of 10 questions and i am posting it one by one. Read carefully, you will surely understand.
Yeah but you can just right click in VSCode and click “format document” and as long as you have clang-format setup the way you like it, you’re done.
I'm not sure how to respond. But I'm pretty sure that neither of those words even appear in K&amp;R, ANSI, or any other version of the C spec
One of the best things you can do for code quality is to make sure your code is readable. Both the code behavior and the intent of the coder should be clear to anyone looking at your code, including future you. Code formatting is a huge are of debate, and that’s silly. Use a beautifier to keep your code in one of the standard formats. By all means, use lint or something similar to keep your code from surprising you. Use -Wall but getting a clean response from that isn’t a goal in itself. There too, you want to see if your code is going to surprise you.
This isn't really a question about C, it's whether or not you know enough math to see a good solution. Cheap and dirty way to solve this is recognize this is a [Diophantine equation](http://mathworld.wolfram.com/DiophantineEquation.html) and hope `x` is small. Start at zero and check if there's a solution in range, rinse &amp; repeat.
https://old.reddit.com/r/programming/comments/c3t1mp/v_lang_is_released/
Do you use any beautifiers yourself, besides clang-format?
Just random people on Reddit. Not in any spec.
This problem is solved by the extended Euclidean algorithm. It's not too hard to implement.
Thank you, i will try.
The problem is that while `-O0` or -`O1` is useful for debugging, the warnings emitted by `-O0 -Wall` are not always the same as the warnings emitted by `-O3 -Wall`.
I've decided to remove your posts as they are not specifically about programming in C. Please post general programming challenges elsewhere, e.g. to /r/learnprogramming or perhaps /r/computerscience.
implicit type conversion between the integers; typedefs and enums can happily be passed to an `int` with most compilers letting it slide by default. It leads to some really crappy code and eventually UB. You can catch most of it with manually settings warnings in gcc and clang, but you'll find most people don't.
Obviously for debugging you should stay in O0, but do not release in production without any optimization. O2 is close enough to the spec. All code we release at work is in O3 and we have never had any issue with it. What do you mean by GNU standard? GNU has extensions to the standard, but it is not a standard. You can entirely forego the coding style while still using GNU extensions. The coding style is awful, there is no reason to force it if you don't even like it. The extensions themselves are sometimes useful in the code, but I avoid the new functions / behaviour. Syntax extensions are usually available in other compilers, while the glibc might not be.
I've only ever tried passing vars to function of the same or of convertible types. Couldn't I theoretically do something like use typedef to create an alias for a float, and then pass it to a function requiring an int? If so, I'm very surprised and couldn't imagine how broken that would be. Imagine someone passing a float typedef alias into snscanf or something similar.
C has it's uses, C++ has it's uses. If I am writing a library, I would usually go for C instead of C++. If I write executable application I would usually go for C++. That is my personal preference, but I know it varies. I prefer to write libraries in C due to C's compiler independence (no name mangling and simpler ABI), and I prefer C++ for end applications because of STL and Boost (when I can afford to not care about size and speed of STL and Boost:-)). When it comes to speed of execution it does not depend just on compiler, it depends on how you write the software. C++ can be as fast or faster than C but can also be slower than C. It depends on you and what you do with it. I agree if one would go about OOP traditional way with classes and so on, then C++ is better choice. I don't understand why are people stubborn and implement manual handling of classes and virtual tables and do everything by hand. Compilers are there to write code for us. The slowness of C++ does not come from compiler generating virtual tables and objects but for using virtual tables and all the indirection and lots of fragmented small objects in first place :-). Why is using external library in C drawback but then you praise C++ for included library? STL is pretty bloated, and if your biggest problem is sorting algorithm, then type in your terminal "man qsort" and see what it does. But yes, generally STL and Boost are much richer in functionality than stdlib, but who cares. Usually you use external libraries anyway. I see it rather as a plus side than minus side. Anyway, for me, when it comes to C, the most missing feature is polymorphism, especially function and operator overloading. Yes operator overloading. As illustration, what makes string class in STL look nice is actually that you can use operators on it. C has bstrings and sds strings which are really great libraries both, but usage is much nicer when you can use '=' and '+' for strings. Same goes for computer graphics and usual suspects as linear algebra and other obvious mathematical stuff. Check OpenGL or Intel's IPP and see function names, and imagine how nicer it would look if we could drop all those type reminders/differentiators. Or Microsoft's ugly Hungarian scheme. In one word C is unfortunately much more verbose. But on plus side, the verbosity makes it easier to see what is going on and to can prevent some mistakes, so I guess it is a bit of individual preference.
Thanks for your feedback, I will correct the code.
&gt; C code, while it may still be good, isn't great unless you pay attention to using monads, and that all good C programmers use them. I've been programming C since I was 11 and this very sentence is literally the first time I've ever heard the word "monad" used while discussing C. The person who said that may use it for some legitimate, highly specific reason where it makes sense, but it's also likely that whoever said it likes buzzwords and shoehorning things where they don't belong.
Thanks for your feedback , I will modify the code.
Let's say I'm running Gentoo. What benefit would I get, besides a small speed boost, by adding -O3 to my cflags over -O0? Is the speed boost really that significant? Shouldn't I prefer execution correctness over what I'm guessing is slightly speedy execution?
Haha I understand.
&gt; I've only ever tried passing vars to function of the same or **of convertible types**. Couldn't I theoretically do something like use typedef to create an alias for a float, and then pass it to a function requiring an int? You can't typedef a float and then pass it to an int. But you CAN typedef a float and then pass it to a function exception a float and vice-versa.... even though it's **not** a float, it's a "different" type. And that's the main problem! What's the point in a new type if the type system will convert them for you? It's not a new type, it's just a midleading name for a new type. The second problem is that 50% of typedfs you'll encounter are just typedefs of an int, which means socket_t lol = ...; int wtf = lol + 5; is legit code. (The other 50% is someone who is too lazy to type `struct my_data` everywhere and wants to just type `mydata_t`). And should the definition of that type ever change, which is one of the point of typedefs, things go wrong, and sometimes you might not realise why, because some *more* implicit type conversion will try and be helpful.
There is no garbage collector in C++ either :-). Reference counting can be implemented in C as well (and is, for example glib is notable for using ref counting all over the place). However RAII is a bit easier implemented in C++ than in C. C requires more manual work, but if you don't know how to do it properly in C++, you can still shoot yourself in the foot, just as easy as if you use plain C.
* If the only reason you're using `len` is as a stop condition for iterating over the input string, you don't need it at all. * You don't need to call `strlen(compress)`. * Produces undesired output for runs of more than 9 characters. * Buffer overflow. If this were an interview, this question would be a great segue into asking you about the property of returning the uncompressed string if it's shorter. How do you anticipate that actually gets used? What are the pitfalls?
Wow, I get it now. Why do people even typedef ints anyways? Why not stick to ints? Shame on lazy programmers. Shouldn't someone just use a macro instead if it's absolutely unbearable to type 14 characters?
To add to this, most editors have some extensions that allow you to run code formatting at a specific section of code, this works quite well. If I were to start a new project today, I'd rather enforce full auto clang-format though. IMO sections of suboptimally formatted code is a fair price to pay for never having formatting arguments in reviews again.
Formatting has little to do with code quality - I can change formatting to my preference with one button click. I'd say "quality" code is about the quality of the program design, attention to corner cases, and last but certainly not least, the thoroughness of testing. What constitutes good design is a wide-ranging topic. Here's a few general principles I think are valuable. Don't optimize prematurely, focus instead on clarity, simplicity and correctness. Don't reinvent the wheel, and use other people's well-tested code where you can. Don't get clever \*unless\* it pays off in clarity, simplicity, or correctness. Your code should be modular, the modules should have clearly defined purpose and functionality exposed through a well-defined interface - code external to the module should have to know as little as possible about the module's guts, it should just expect it to honor the defined interface. If all your code has complex interdependencies with other unrelated code, that will sooner or later make it very hard to reason about or maintain. For similar reasons, global state should be avoided to the extent possible. Don't repeat yourself - if you find repetitive patterns in your code, that you will have to go and fix individually if you need to change something, that's a sign of poor design. And as they say, untested code is broken code, and I think having to design your code in a way that facilitates automatic testing, and writing automated tests \*as\* you write your modules, will naturally enforce certain good design principles; and having good tests I think the only \*real\* way to learn it is to keep working on projects of sufficient complexity, and learning what works, and what bites you in the ass.
Warnings can be different between different compilers and different versions. Using strict compiler flags may lead to some pain for people building from source. If you do not mind this, go ahead and include `-Wall`, `-Wextra`, and `-Werror`. But be aware.
&gt;You need to turn them off when you are compiling your final release. Or not, you probably don't need the speed obtained by removing those checks. Specially because the optimizer is smart enough to remove unecessary checks. Most software doesn't need that much of bit brushing, you should benchmark it first to see if it's necessary.
it's as fast as C.... but it has to use a C compiler to compile translated C code. lol.
What kind of framework do you use for C code testing and coverage report?
Indeed. And memory model exposed by C has long time ago started to be real machine memory model and is really not so level any more, at least not on x86/x64. Maybe some Atmel or similar small machine, C can be seen as low level abstraction, but on "fairly :-)" complicated machine like desktop CPU with virtual memory, MMUs , LRUs, paging, caches and so on, the "low level C" is actually not so quite low any more (if it ever was).
These flags are good for compiling your own source. They're not as suitable for sharing your project with others. Still, depending on your target (e.g., reasonably modern Unix platforms), it can be completely reasonable for source to pass -Wall -Wextra -Werror across a lot of compilers and platforms.
I made the mistake of assuming this function I wanted to use that returned a pointer would return null in case of something going wrong. I should have actually looked at the implementation of that function before just using it, but I just went by the name and figured it would be safe to use. Boy was I wrong. Code went out to production and started to get crashes. What did I do!? Looked at the stack traces and saw where it was failing and looked at the function I was using. Oh....they returned “1” for errors...a function that’s supposed to return a pointer...well of course my null check would fucking fail...
Not a list, but something you can start looking into. &gt; Look into the [GCC manual about Warnings](https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html) and search for "optimization". Some examples: &gt; &gt; &gt; This option is only active when -fdelete-null-pointer-checks is active, which is enabled by optimizations in most targets. &gt; &gt; or &gt; &gt; &gt; Because these warnings depend on optimization, the exact variables or elements for which there are warnings depends on the precise optimization options and version of GCC used. &gt; &gt; I've had also personal experience where a warning only showed up when I activated optimizations. (Can't remember what program or warning it was) (From https://www.reddit.com/r/C_Programming/comments/9m76z6/simple_program_works_on_some_compilers_not_mine/e7e6g0a/?context=3 )
That's almost like a php function that returns an integer, false on error and null if an exception happened while processing the request args.
And you never got segfault with C++? :D All those things you mention are equally possible to achieve with C++ and usually are achieved as well :-). C++ has new/delete, which has it's own pitfalls (for example using delete when you really mean delete \[\]) and some other more annoying stuff, when you for example do not want memory allocation AND initialization to happen at same time. In C you can forget to initiate your allocated memory, or forget to call free. In C++ you can as well forget to call delete if you don't use smart pointers, or you can forget to write a destructor and end with default one which might be inadequate. And then we have virtual destructors, and so on. Lots of places to make an error. So yeah, both C and C++ have pitfalls, it is just to take your time and learn how to use stuff properly. There is no shortcut to learning your tool!
@arthurno1 I completely agree with you, man. I just meant C++ has much more language features than C and is easy to use for object oriented design. Yep, I know C is preferred for libraries, as it can be utilized in other languages like the sqlite3 library. But C++ can be used for libraries too. The Realm database and library is made from C++ and can be used with several languages. I don't have any problem with C, just it really feels like a wrapper on assembly (just joking 😂). And I don't know about you, but things like casting types back and forth to void * scares me. It's so unsafe. But after all, C++ evolved from C. I am just excited for the development C++ is going through. C++ 20 comes with modules, caroutines enabled and standardized by default and the new spaceship operator [&lt;=&gt;] (Sounds cool, right?) and much more. We may even see meta classes as proposed by Herb Sutter.
That would require significant runtime support which just isn't available on many platforms. Also, this can be solved by a library, so no need to put it into the standard.
&gt; Modules make development faster because you don't have to recompile the whole project when only one or few files have changed. Have you heard of a tool called `make`? &gt; They also make code reuse easier, when you don't want to or cannot use libraries. How so?
&gt; Have you heard of a tool called make? Yes. That's exactly what I'm talking about here. Make uses modules to compile only changed files. &gt; How so? You just reuse the module :)
I would put C++ implementation of lambdas also in a category of overworked bad ideas of modern C++. I would prefer simpler implementation based on just opening inner block and having outer scope available inside lambda instead of that ugly function object constructor stuff passing in \[\] ( I guess they have implemented it as a function object). It would be mentally so simpler, and thus simpler to learn for new programmers and less error prone, if they just implemented it as really anonymous function instead. Maybe be a bit of regression from your comment, but I just dislike some ideas in newer standard versions.
/r/learnprogramming doesn't tolerate asking for or giving out complete solutions, so please don't send people like this over there.
Make doesn't need modules to do so. You can also reuse source files, no modules needed. Modules do not solve a problem that isn't solved by existing tooling.
&gt; Make doesn't need modules to do so. In fact, make was written specifically to have a tool that only recompiles changed source files. So? How does that make my answer invalid? &gt; I don't see why modules are so important for this purpose. I never said they are important :/ &gt; You can also reuse source files, no modules needed. Yes, you can just include a .c file, but that will cause problems if that file is included in more than one file. And when you include all source files in one file instead of using headers, make will have to recompile the whole project.
I think you should get some training in programming. Take course, or just get few books and actually make few tiny bit more complicated applications than toy programs. Say make a small videogame where you do everything from scratch (input, graphics, resources, datastructures) or maybe a compiler or text editor. These are usual projects where CS students learn to actually learn how to program. It will make you good I promise. You are not the first electrical engineer who think he can program but then appears to not really know what he is talking about. Unfortunately it is hard to understand our own ignorance, and I don't mean it in any bad meaning or as insult, just a general reflection and advice from someone who have worked on some hardware projects and had pull his hair out when electrical engineers where stubborn that they know what they doing and refused to learn. Some projects costed so much more money then what they should have because of lack of knowledge and will to listen. Precanned software or "the quality of life" as you call it is nice. These days you can get most of precanned stuff for both C or C++ and these days many times for few other suspects as well (Java, Python, Node). But observe someone have to write that precanned quality of life, and all those electrical stuff need some interfacing with external world, and those interfaces often live very low level lifes, as C or assembly libraries.
So many cookie eaters here. Seems like lots of Java programmers had to turn into C++ programmers for some reason. &amp;#x200B; Playing with bits and bytes, memory layout and memory at all has become big no-no nowadays. Usually because "one can get it wrong and make a mistake". One can get wrong anything, it is a matter of learning the tool one works with. It is always possible to make wrong assumption, or to forget something. It is good to have tools that minimizes those errors, however it is necessary to have both tools and knowledge to access those low level bits when needed. C, function pointers and some other stuff are such a tool. &amp;#x200B; There is however no shortcut to getting knowledge. It is just to sit and learn. The burden comes with the profession. It is as if a stone cutter refused to learn how to use a hammer because there is a pneumatic one. Sure. But sometime your pneumatic one will be just too much or inadequate. Good luck finishing your job than if you haven't learned how to use your tools.
if you think formattting is code quality, this is not a discussion you're ready to have. go read pmbok
Because then it creates a dependency on POSIX features being available, which doesn't hold up everywhere. Maybe I'm too paranoid or whatever, but I'm not tying any of my shit to anyone elses.
You can use either, but don't mix both together, and `cin` is far better and safer.
You said: &gt; So? How does that make my answer invalid? You said in a comment further above: &gt; Modules make development faster because you don't have to recompile the whole project when only one or few files have changed. Given that the same is achieved without modules with `make`, modules do not actually make development any faster in this regard. &gt; I never said they are important :/ Yes, you pretty much did (but not with this exact phrasing). If not that, what else did you mean? &gt; Yes, you can just include a .c file, but that will cause problems if that file is included in more than one file. “include” as in “include the file into your project,” i.e. copy the file into the project directory and add it to your build scripts, not as in “use a `#include` directive to paste its content into another file.”
C++ is off topic in this subreddit. Please post C++ questions to /r/cpp_questions instead.
What I mean is that instead of making their own threading API, the C committee should have gone and added a reasonable subset of the POSIX threading API to the C standard. This could have been done without introducing any more portability issues than C11 threads introduced and would have actually avoided any portability issues.
How is segfault more difficult than dealing with NullPointerExceptions in Java or arrays out of bounds etc? "Minor slowdown" is quite a generous epitet when describing Java :-).
c++ is off-topic for this subreddit That said, I almost never see `scanf` in the wild -- there are almost always better options than using the `scanf` -- especially as it relates to getting interactive user input. `scanf` is great for well-structured, well-formatted input. User input is not that. `scanf` is horribad for validating user input, and providing useful error handling so you can recover from unexpected input. When you see `scanf` used for user input, it's typically used in an intro-to-CS assignment in which the goal *isn't* responsible handling of user input, but rather the goal is simply to teach the budding new programmer how to use functions.
Sorry, my bad
&gt;the biggest problem with C is that development has been mostly abandoned by the mainstream. so there aren't a lot of tools to catch your bugs as you type them. Whauuuh :-). This was an interesting statement. [https://stackify.com/popular-programming-languages-2018/](https://stackify.com/popular-programming-languages-2018/) [https://www.techworm.net/2018/02/popular-programming-languages-2018-according-tiobe-pypl.html](https://www.techworm.net/2018/02/popular-programming-languages-2018-according-tiobe-pypl.html) [https://fossbytes.com/most-popular-programming-languages/](https://fossbytes.com/most-popular-programming-languages/) [https://www.inc.com/larry-kim/10-most-popular-programming-languages-today.html](https://www.inc.com/larry-kim/10-most-popular-programming-languages-today.html) [https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages](https://spectrum.ieee.org/at-work/innovation/the-2018-top-programming-languages) While not THE number 1, C is certainly not look like abandoned by mainstream. I would rather say that taking anything from 2nd to 5th place in popularity would actually make C quite a popular language :-): C is a compiled language. When it comes to finding errors and helping with bugs, C share same fate as all other compiled languages. If you don't have the source, you will have to work with disassembled code, otherwise I see no difference why C debuggers and analyzers would be worse than say those for Java or C#. I would even go so far as to say that C has better debuggers, profilers and analyzers than Java or Python or many other languages, but I don't really programm those any more so I can't really say. 20 years ago when I was programming Java on everyday basis, the debugger for rather a joke compared to gdb or visual studio. No idea how good is it nowadays. Actually catching errors while you type kind-of started with VisualStudio which from beginning was a C/C++ IDE + VisualBasic. These day my Emacs setup catches errors and suggests what I should type (it actually types it as well for the most).
&gt; Given that the same is achieved without modules with make, modules do not actually make development any faster in this regard. I suspect we are talking about two different things and we are circling in that misunderstanding. By module I mean a pair of files: .c and .h, where header file (.h) is included in other source files. This source file can then be compiled to an object, which then is linked along other object files into a binary. If you too are talking about compiling source files individually and then linking them, each source file that uses anything from other source file either has to include it or include its header file. In the first case, preprocessor will compose those source files before passing it to the compiler and individual compilation does nothing, apart from making the whole process longer. &gt; Yes, you pretty much did (but not with this exact phrasing). If not that, what else did you mean? Well, "important" implies they are essential. &gt; “include” as in “include the file into your project,” i.e. copy the file into the project directory and add it to your build scripts, not as in “use a #include directive to paste its content into another file.” I do not know what you mean by "adding it to your build scripts". With make and autotools, adding a source file to build is done by appending its path to `whatever_SOURCES`. That does not make it automatically recognisable by the rest of the source files. How do you use functions defined in `sourceB.c` inside a file `sourceA.c`? You have to tell it "there's this function, this is its prototype" and you do that by including a header file. Or you can include the whole `sourceB.c` in `sourceA.c`, but we already know how it ends.
Indeed. [https://www.youtube.com/watch?v=5tJPXYA0Nec](https://www.youtube.com/watch?v=5tJPXYA0Nec) Might be very important! :-) And yes I agree about learning. It is how I learned mostly, by making apps, testing, throwing away, rebuilding, playing around and being curious about how stuff works and coding as much as possible.
All of this depends on what you are doing. Read the details here for example: https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html and see whether there would be an issue. In the vast majority of cases, -O2 will be the best trade-off. In extremely optimized code, -O3 actually does a difference and is still preferable. All of these are still valid execution-wise. The -Ofast level will however leave the "strict standard compliance", allowing for example to use less precise computations in floating point for speed gains. This is not just some "slightly speedy execution". In -O2, GCC will identify hot code path and put the instructions in contiguous pages, allowing a far better cache locality when loading instructions. Most of the optimizations here are just basic and common-sense. There are benchmarks making some performance comparison: https://www.phoronix.com/scan.php?page=article&amp;item=gcc_47_optimizations&amp;num=2
Why do you think you wasted your time? All those principles and C you learned are valid and used in C++. With C++ you just get more tools you can use. Now lets pretend you are done with UNI, and work at some company that maybe asks you to implement a driver and library that ships with your device, either for configurability or for extension capability or both. Of course your API should work across different compilers and operating systems, and be callable from C, C++, Java, Python and Node. On top of it, it is closed source driver and you can't ship certain implementation code with your product. As electrical engineer, they probably won't task you with those choices, but let's pretend you are there, and tell me what implementation language would you choose, why and how would you go about implementing it?
The idea of not using what you don't want still works. The C++ standard is so big you'll probably not care about ALL the features of the language. And if you really dislike C++ features you can always go the C way on the code (with a few exceptions). But that should really be the last resort.
The ability to play with bits and bytes is a *fundamental* part of many embedded programming tasks. On many platforms, the closest thing to an "operating system" is a group of hardware circuits that will force the program counter to a certain address on startup, and which will--in response to a certain stimulus--save the current value of the program counter and load it with some other address. One could write a meaningful program for such a platform without playing with bits and bytes. One meaningful program in fact: `int main(void) {while(1);}` If all you need is something you can load in your system that won't release nasal demons, launch nuclear missiles, etc. that program will be perfect. Doing literally *anything* else meaningful, however, would require playing with bits and bytes. I find myself perplexed by why anyone who isn't interested in playing with bits and bytes would want to learn C. I'd be hard-pressed to think of any other purpose (other than maintenance of existing code) for which some other language wouldn't be better. C was my primary language for writing hosted applications in the 1990s, but the total length of all hosted C applications I've written in the last decade is probably less than 1000 lines (as compared with many tens of thousands of lines written in embedded C or other hosted languages).
I have used astyle and bcpp. I think I might have used jsbeautifier at some point.
There are still other features then low level machine programming. One is easier exporting. C++ mangles names, and ABI is harder to export than from C. Some C++ libraries still have C wrapper just for this purpose. Other one is that libraries are often not so very architecturally complex, so C interfaces is somewhat to prefer. But whit that said, there are big libraries written in c++, like say OpenCV or similar. But they come with source so one compiles everything for compiler and system in hand.
is that a binary number or a decimal number?
&gt; Why do people even typedef ints anyways? Why not stick to ints? Shame on lazy programmers. They're trying to create abstractions, e.g. socket_t, size_t, _bool etc. They could be single ints or structs, you're not meant to "know". But at the end of the day the compiler sees them all as a typedef for int and happily lets you use them all together, even though you shouldn't, and that's because the C spec was dreamed up in the 1970s and the committee has been too scared to change it since then.
https://en.wikipedia.org/wiki/Huffman_coding
Cool. What sorts of protections?
Certainly if one needs to write a library that can interface with code written in other languages, C is something of a lingua franca, but most of the situations I've seen where one would need to have code in one language call a library written in another involve playing with bits and bytes.
It enables buffer protections when using libc memory and buffer manipulation functions. There are two levels. Setting it to 1 is mostly compile time checks with essentially no performance or behavior impacts. 2 is safer but does much more checking at run time. Another useful hardening flag (that I forgot was separate from fortify earlier) is -fstack-protector. That enables stack canaries, another useful overflow defense.
Excellent. Thank you.
Thanks! I'll look more into it.
Specific to the Arianne, there's an entire disciple related to floating point in general, and extra work needed when reducing floating point vales to integers. Indeed - there should have been a traceable requirement associated with that ( it sounds like an interface requirement else they would have simply used 64 bit FP ) and an associated test to validate that it worked properly over well-chosen ranges. Nobody's perfect though.
qsort() is fine - it's tractable by very brief inspection.
I mean, when used for abstractions, it sort of implies that programmers shouldn't mix them together. Why would someone, say, increment a socket_t? That sounds like a terrible idea. Does the spec really have to restrict people from shooting themselves in the foot like that? I'm sure you know the old Unix saying about guns and feet.
Oh I see.
Try it and see what happens. If it errors, research the error. Actually spend time trying to figure out what went wrong. After all that, come back and ask specific questions, along with the code you've written. When posting your code to Reddit be sure to indent each line by at least 4 spaces so that it renders correctly.
It’s a paper exam for some dumb reason. I’d try but my laptop is currently broken so I don’t really have a way of finding out.
The code you've provided shouldn't compile. Please check it again.
On the exam paper it's written in pseudocode as while x%3=0 do I + 1
Try https://repl.it/languages/c
Thank you , managed to figure it out.
There has to be more to it than that, because that will just infinite loop. X never changes.
There was , x=x/k (X was 9 k was 3
There is already `stdbool.h` that defines `bool`, you should use that instead of rolling your own. Also it'd be good to handle what happens if they type letters instead of numbers at the exercise number input.
None of the ex functions make use of argc or argv, so I question whether they are implemented correctly/completely, or why you are passing those arguments into the various ex functions.
A couple of low-hanging fruit: * If zero isn't a valid exercise number, it's more idiomatic to make zero your "no exercise selected" number. That way your loop can be while (!res). * i has improper scope * initial assignment of exResult is pointless I don't know your build system, so I can't suggest a good way to avoid needing this in the first place. I could probably suggest something with Makefiles, but that's about it. However, I think you should earn yourself bonus points by simply taking the exercise number as the first command-line parameter.
Thanks I'll give that a shot when I have a chance :)
I am just passing it through so the function call is the same for when I eventually need to use them in an exercise :)
Thanks! I’ll take these suggestions into mind when refactoring the code :)
Programming is a craft. There are hundreds of small things which, together, make up a high-quality program. Attempting to boil code quality down into a small set of things is difficult. Honestly, indentation and code style may be the least important (but being consistent does help readability and maintainability). &amp;#x200B; Here's my take on things to focus on to write high quality software: \- Focus on creating code "units" with well-defined interfaces. A unit of code should have a single responsibility. Don't do too much in one place. \- Write tests for each of these "units" to exercise their interfaces. \- In the application's main or in higher level components, stitch these units up together. &amp;#x200B; For compiler warnings, enable them all and make sure the compile fails on warnings. Do not commit code that fails any warnings. &amp;#x200B; Good luck!
Thanks! Though I can't help but wonder why so many consider formatting/style to be so unimportant for quality. If I'm working with others, it's a million times better to get the style right than to use no style at all. C is half telling the computer what to do and half telling others and our future selves what the program does. That's why we don't use labels and goto's in C. For and while loops convey both the raw instructions as well as the intent.
I'm not saying it's not important, but not THE most important. But to be honest, having consistent formatting and style is pretty much assumed for any high-quality professional software.
Good reply! I've never really used C++ at all for more than a simple 'hello world', but I tried to stick to what I knew well in my answers / opinions which was C
As someone who has used Gentoo (I use Fedora on my notebook currently, but Gentoo is my distro of choice on my desktop) I can assure you you should **not** be using -O0 for all your packages. -O2 is fine and will not break or cause incorrect execution for any packages. Although -O3 *is* risky, in recent versions (since GCC 5 I'd say) it's **much** more stable than it used to be, and can be safely used without issue. I've ran both my desktop and my previous notebook Gentoo installation using full on *-O3 -march=native* optimization and didn't run into any problems caused by the flag. Since you are worried about ease of debugging, what I'd recommend you to do is have a [package.env setup](https://wiki.gentoo.org/wiki//etc/portage/package.env) with a "profile" that replaces your CFLAGS and CXXFLAGS with "-O0 -ggdb". Then, in case you need to debug a package, you apply that profile to the package and rebuild it.
So some of this has been alluded to briefly already, but a few really important things for C specifically: - A good C programmer *really* needs to understand the language as specified in the standards (at least have a flip through C89, C99, C11, and C17 or their late drafts and keep an eye on the ISO WG) and what’s happening in and beyond the compiler, as well as which component of the development/build/execution process you’re interacting with when and how. UB/ISB, type limits, aliasing rules, etc. have been mentioned, but basically treat everything outside your direct sight as an adversary. Developer tools are usually the Go Fish sort of adversaries rather than Monopoly, but they still want you to lose. - Along these lines, good code is defensive. You mentioned using OS/crt/libc/POSIX docs, and props&amp;c. for that. However, even the specs leave more- and less-defensive options for the caller. Oftentimes the specs will say (e.g.) “RETURNS −1 for error else FD as ≥0.” In those situations, properly anything &lt;0 should be counted as an error since you can’t/shouldn’t try to make use of an FD&lt;0 anyway; nitpickily, &lt;−1→abortable internal or system error vs. −1 for `errno`-denoted, but &lt;−1→`errno=EFUCKIT` works too. Handling all cases makes the code more robust, and it even helps the compiler optimize better. - More fundamentally, the ever-lurking, Lovecraft-opium-trip–type terror in C is that the compiler may fuck freely with what it’s fed, and it may introspect into or analyze that in any way you haven’t forbidden. It probably also knows the rules for C better than you, or it may have pieced them together in unexpected ways. Some compilers will rewrite your inline assembly code for you. Bad code loses its mind when command-line flags change; good code shouldn’t alter its behavior outside of what’s expected and documented. This makes the code considerably more portable and time-/tamper-proof, and it helps ensure developers are notified if operating outside of what you considered during D&amp;D. It also means they have less reason to blame you (whether or not that matters in practice) if something goes wrong outside those bounds. - In system headers, you have to be *extra* ***super*** careful about everything you do. - Some kinds of headers can be included in arbitrary languages like C++ (often), assembly, FORTRAN, OpenCL &amp; other C-like DSLs, Objective-C/++, UPC/++, or WinRC - The few remaining traditional/-mode-capable compilers follow different “rules”; e.g., token pasting with `/* */`, no operator `defined`, missing/overridable keywords (e.g., `signed`, `const`, `volatile`), missing concepts like prototypes, lax type-checking, etc. Prohibit or deal with this possibility. - Identifier discipline needs to be super-strict; do not use or emit a name in a namespace you don’t control unless specifically asked to by client code, and undefine most things before you declare/define them. Clearly doc as UB somebody using your prefix wrong. - Make reasonable attempts to prevent or detect+kvetch about misuse of the header as a whole and constructs within it, if it’s part of a public API. - Use `extern "C" {}` and `extern "C++" {}` judiciously in C++ embeddings. - Scoping can override your identifiers, so break or escape that, and clearly doc as UB if somebody misuses your prefix. - Test your headers rigorously with every supported language, every supp standard, every supp family of compiler, a handful of supp versions of each compiler, every supp host ABI×arch×platform; always with `-Wall -Wextra -Werror -pedantic` or eqv. - You may have to be careful with inclusions of your own. E.g., in a C header that needs to make some POSIX API call, don’t `#include&lt;unistd.h&gt;` unless you document your header as pulling that in. If you can thunk or alias through your own prefixed name for it, do that instead. It’s mostly fine in purest C, but in C++, C headers are rightly viewed as pissing all over the global namespace, which makes it easy for collision or conflict to arise in perfectly valid client C++ code. - Mutation of shared state should be avoided when possible &amp; reasonable, or else deal with it *very* carefully. There are potentially thread safety, reentrance, synchronization, async event, use-after-free, error handling, security, UB, and de-/initialization concerns. Corrolary: Avoid `errno` use if possible; it’s slow and (ironically) error-prone. If you have to use it, either save and restore it at enter/return, return with it unchanged when OK but defined if error, or force it to known ≥0 on return. - Robust code takes into account that signals/interrupts can arrive at any time unless masked. Your program can also shit itself suddenly and without warning, even if it’s not your/its fault (e.g., an `mmap`ed DLL is deleted, `SIGBUS`ing your process). This means that you should *very* carefully walk your program through state changes so that you can (usually, mostly) walk back or tear down cleanly at any point. It may also mean that you need to mask signals during some setup/teardown actions (you can’t mask faults, so ± there), and often you’ll need a few different cleanup/shutdown mechanisms for different components or situations. You can’t always clean up fully, so focus on the important stuff; e.g., restoring CTTY config &gt; deleting temp files unless private data’s involved. Don’t be a hero if you hit a proper fault—your attempt at recovery may do more harm than good. - Clocks and time are huge problem areas, and they’re only sorta-acceptably addressed by OSes &amp; libc these days. Non-monotonic clocks can wander all over the place. Non-realtime clocks may not have a constant/-ish rate (might be 0), and realtime ones may change rate to drift-compensate. Some delay functions will screw up `SIGALRM`, and hooking someting like `SIGPROF` will break profiling. If you’re decomposing a time into anything beyond hours, minutes, and seconds you’re probably doing it wrong. You probably can’t handle timezone deltas correctly on your own, so don’t. - Output from stuff like `sprintf` can be affected by l10n. Text content can be drastically affected by i18n, so use non-text-based comparisons whenever you can help it and force the locale to `C` if you can’t. Unicode is really tricky to get right; if you don’t need full i18n or special chars yet, passthrough or forbid chars outside the 7-bit range, and if you do need it try to use a preexisting &amp; relatively common library or API if you can. It’s easy to do some of the lower-level string ops yourself if you’re hand-optimizing, though that’s often undesirable practice decomposition-wise. - Don’t take unexpected scale lightly. We’ll probably see 128-bit `long`s at some point, eventually maybe freeform or &gt;64-bit pointers, ridiculously wide vectors, extended f.p. formats (e.g., 256-bit, or with fancy extry components), huge hardware thread counts, enormous memories on broader memory networks, and entirely other modes of computing we can’t even make use of yet. Take reasonable precautions to guard against them breaking your code, and stick within the bounds you maintain tests for. Counter to this advice, lots of old games went on to self-immolate pointlessly (usually div-by-zero) as CPU clock rates went from kHz to MHz to GHz. More generally: - Don’t impose your own design on everybody who uses your code, especially if you’re trying to implement something basic/pseudo-generic like buffers or linked lists. Try to let the caller manage anything that’s not specifically your concern (e.g., object placement and lifetime), and make sure you don’t do anything crazy in ctors and dtors. - Fudge factors and epsilons are often necessary, but they should be documented, bounded explicitly, and configured along with whatever they’d pull a default from. - Magic numbers should be referred to by name. `0`, `1`, `2`, and `10` usually aren’t magic unless derived arbitrarily or serendipitously. - Impose limits for good reasons only. Consicously select types according to what you need/want to support. - It’s all the rage to support Unicode identifiers. It’s based on positive intent and all that, but it’s otherwise all bad. ASCII is well-supported at every point in any remotely modern stack, miscellaneous 8-bit or control characters are usually handled properly when unsupported, and most importantly an ASCII string renders ~unambiguously, left to right then top to bottom, unless it’s in a bad font or you’re on a TTY. (Corrolaries: Don’t use bad fonts where characters are ambiguous, and be aware of potential effects on TTYs.) Unicode text does not have these nice properties. There are countless, detailed requirements (Arabic, I’m sorry, but ‮u crazy‬); code review or editing tend to go out the window for nonnative programmers; there are tricks to make one Unicode thing look like another or mask/relocate things unusually, making a Unicodebase insecure against malicious developers. (Really, any remotely ambiguous-looking Unicode text needs to be escaped, even in string literals. E.g., NBSP and SP may render ~identically but mean very different things.) Unicode also has a very different correctness model from ASCII, and whereas the “invalid ASCII” predicate is mostly `&gt;=0 &amp;&amp; &lt;= 127`, the “invalid Unicode” consideration is complex and stateful wrt encoding &amp; semantics. Glyph rendering is also complex &amp; stateful, and unimplementeds, bugs, and oopsies daisy are common. Fonts support is a wildcard outside ASCII/-like ranges. Even for ASCII, ambiguities may arise; monospaced l and 1 often look similar, as may 0Oo {( )} \`'"\^\* ,. ;:, so graphical clarity should be included as part of style. Generally, style should usually be a secondary consideration; syntax and semantics are what will bite the most people in their respective asses. However, good style usually reflects good disciprine, which usually leads to overall better code.
Thank you so much! Extremely helpful and useful!
Then it looks to me that you're on the right track. Good work.
Thanks for the insider info. Just a heads up, I don't actually use Gentoo, I just used it in the scenario. I think I might be a bit confused. I know that there are some architecture specific optimizations that can be done harmlessly, but to me it seems like anything past maybe -O1 will gladly alter the correctness of the binary output in the name of speed. Can't this break the executable? Can't this cause undefined behavior or flaws when fuzzing?
Thank you 😀
Mostly about programming style, adding onto what others have said: * Avoid magic numbers. I have no idea why you assign exResult to -100. If you need a number like that lying around in your code, put it in a #define and name it something useful. Also a note: if you're using -100 to determine if the exercise failed, a more conventional thing would be to pass a pointer to an int for the function to put the result in, and then have the function return a bool status to indicate success or failure. * isValidExercise is a misleading function name. "isBlah" names should be for functions that return bools/statuses. Your function returns an index, so it should be named something less misleading, like "getExerciseIndex". Then, it becomes clear why a negative return value is treated as an error. (This gets very important when you start working on large projects and helps keep code neat and extensible.) * Why is len(x) a macro function? Nothing wrong about it, but macro functions in general are just very annoying to debug. If you're doing this for performance, you should beware premature optimization. * On a different note, why is len a function at all? If you're going to hardcode the exercises, you may as well hardcode the length of exercises too, since that's the only place you use len. (And of course, put the length in a define too, with a useful name!) * Check return values! What if someone puts in a bogus number or nothing at all? Then in main, \`i\` could be left uninitialized. So check the return value of scanf. Be robust! This isn't bad at all for someone who is learning C. :D Keep it up!!
Thank you :) I will definitely avoid magic numbers when I refactor ! I put length as a macro so that I wouldn’t need to type out the whole expression which is essentially just a divide. You’re right! I should use define for the array length since it’s known :) I’ll make sure to handle input validation in a more robust manner, something to learn more about C for me :P I’ll also make sure to change that misleading function name, that was a bad oversight!
-O2 will most definitely not alter the correctness of the binary. -O3 has a **very very very thin** chance of *maybe* breaking the program, but nowadays it's very rare. It was the case back in the GCC 2/3 days. Optimization flags like -O2 or -O3 won't affect the correctness of the binary output or break specification. All they do is analyse the code to find **more effective ways** to do the **same operation**. If optimization flags broke code, nobody would use them. If you want to be sure that you have good code quality, what you should do is make sure to use sanitizers (especially the ones available in more recent verisons of GCC), make sure you're running a recent version of GCC (I'd say at least version 8, there have been HUGE improvements since then. I'm running 9.1.1 here, but that's because I'm on Fedora Rawhide), use -Wall and compile your code using both -O0 **and** -O2 flags. As /u/skeeto pointed out in their reply to a comment of yours, some warnings will appear when compiling with -O2 that wouldn't if you used -O0 because the compiler will analyse your code further when optimizing it. I've had program crashing bugs appear when compiling Release builds that never showed up in Debug builds. That wasn't because the binary generated by the compiler was incorrect, but due to my own coding mistakes. I'd also advise to read up on GCC release notes to keep up-to-date on new features that might help you write clean, correct code.
i meant development of C.
All of the below items are standard practice for software quality assurance in code for regulated industries (medical, aerospace, etc )... with related standards: ISO13485, IEC62304, ISO9001, etc. This goes well beyond code style that you like or don't like... it takes a comprehensive approach to quality assurance. How heavy a process that is... is hotly debated. Below are some thoughts... 1. Reasonably defined project and/or company-wide process (QMS) and standards 2. Consistent!!! Compile flags as mentioned \`-Wall -Werror -Wextra\` 1. You ship what you build and test with (e.g. if you do your for-credit verification testing with -O2 that's what you ship... none of this -DNDEBUG -DXXX stuff) 3. Static analysis tools 1. Commercial (Grammatech CodeSonar, Perforce, Veracode) 2. Free (OCLInt, cpplint, etc) 4. Dynamic Analysis tools (Valgrind, libasan, libubsan, etc) 1. [https://gcc.gnu.org/onlinedocs/gcc/Instrumentation-Options.html](https://gcc.gnu.org/onlinedocs/gcc/Instrumentation-Options.html) 2. Note that I've found that libasan and some of those -fsanitize flags conflict with certain libraries (CUDA for example) 5. Coverage Analysis (see instrumentation link above) &amp;#x200B; Just FYI: In the medical code I've been working on lately, we don't bother worrying about how people code stylistically (tabs, spaces, {} placement etc.)... We have a defined '.clang-format' file (similar to astyle rules) that's part of our code base and whatever code format the clang-format tool creates based on that file is "by definition" correct per our documented development plans and part of our CI/build process. Think of it as automated compliance.
Hmm, not sure how development of tools is somehow related to development of the language itself, but I am quite sure that none of those have stopped. We have got two updates of the language in last 10 years and I have seen quite some update to C tooling as well. For example mighty gdb is getting improvements in every version, VTune is getting improved constantly, VStudio and associated debugging tools are improved constantly as well. What makes you think that either the language itself is not developed or that there are not tools to help you find bugs and errors?
Good to know! Thanks!
compared to the modern languages, the development of C and tooling for C is pretty much at a standstill. there hasn't really been an update to C since 2007. &amp;#x200B; &gt; However even if language itself is not developed, how is that a reason not to use it? &amp;#x200B; i don't recall ever saying C shouldn't be used.
Well, that is content of your original post more or less. Or why would you write what you did? As I understand you are trying to detract from using C since you believe that development is stalled and there are no good tools to catch bugs as you type them. Neither of those is true by the way, but my question wasn't about your exact wording. It is about your logic. If a language itself is not actively developed why shouldn't it be used (or why do you point that out otherwise)? A programming language is not used because it is actively developed, but usually for other reasons, like features, portability, quality of tooling and implementation and so on. Exactly the thing that language is well-developed (old) can usually mean also that tooling has got good and well-developed as well. Generally young languages and libraries have some new features but worse tooling, while more stable older language might miss on some new syntactic sugar or concept, but have much better tooling and support since tools have been longer in production. C is a bit special language and not being updated too often is seen rather as a feature then as a downside by some people.
Thanks for your feedback , I will read the linked document.
Thanks for you feedback.
I like that len(arr) macro, but always struggled to find a good name for it. COUNT(arr), SIZE(arr) ... NELEM... hmmmph. &amp;#x200B; Exercise \*getExercise(int number);
In C and C++ a functions name is a pointer to the function, so no need to use &amp;funcName. I think the other comment cover all other issues. Great code though for someone getting started
&gt; Why would someone, say, increment a socket_t? That sounds like a terrible idea. Does the spec really have to restrict people from shooting themselves in the foot like that? I'm sure you know the old Unix saying about guns and feet. "Why would anyone ever make a mistake?" Great logic.
There are some problems with your algorith,. &amp;#x200B; Let's go step by step: &amp;#x200B; What do you expect &amp;#x200B; int tot\_size = sizeof(psh) + sizeof(snd\_tcph); to give you? &amp;#x200B; Please notice that snd\_tcph is a pointer, so its size is the size of the pointer and not the size of the undelrlying structure. &amp;#x200B; Also, you need to set checksum in the tcp header to 0 before computing it.
There are some problems with your algorith,. &amp;#x200B; Let's go step by step: &amp;#x200B; What do you expect &amp;#x200B; int tot\_size = sizeof(psh) + sizeof(snd\_tcph); to give you? &amp;#x200B; Please notice that snd\_tcph is a pointer, so its size is the size of the pointer and not the size of the undelrlying structure. &amp;#x200B; Also, you need to set checksum in the tcp header to 0 before computing it.
I think there are a few problems with your algorithm. Let's go step by step: &amp;#x200B; What do you expect tot\_size variable to store? The size of the tcp header and the size of the data on top of it? &amp;#x200B; Please notice that snd\_tcph is a pointer, so its size is the size of the pointer and not the size of the undelrlying structure. &amp;#x200B; Also, you need to set checksum in the tcp header to 0 before computing it.
&gt; 4. ... subtracting 0xffff whenever the sum =&gt; 0x10000 ... Note that most checksum implementations won't actually do it this way. Instead, they'll use a 32-bit accumulator instead, with this at the end: sum = (sum &gt;&gt; 16) + (sum &amp; 0xffff); sum = (sum &gt;&gt; 16) + (sum &amp; 0xffff);
tot\_size was meant for the size of the psuedo header, tcp header and the data. But since there is no data being sent, I have not included data. &amp;#x200B; I did fix it. It was due to not setting the tcp checksum to 0 and because of not converting it to network byte order. &amp;#x200B; Thanks a lot for your help. fixed: [https://github.com/venkat-abhi/Half-open-port-scanner/blob/dev/src/checksum.c](https://github.com/venkat-abhi/Half-open-port-scanner/blob/dev/src/checksum.c)
Yeah, I did see that in a couple of places but this was the first time I was dealing with bitwise calculations and wanted to keep it simple.
I would never declare two variables on the same line. I would declare them separately at the top of the function accompanied by a small comment describing their purpose. If a function argument is just an input, declare it as const.
Aren't you leaking each pseudogram? Is it even necessary, sum each block separately and add the results.
Yup will do that. Thanks
Ah the good ol' "you just using it wrong" excuse.
&gt; Well, that is content of your original post more or less. Or why would you write what you did? &amp;#x200B; the content of my original post just pointed out one problem i see with C. i did not try to detract from using C, in fact i concluded with: " despite this, it's still manages to have the fastest compilers, produces the fastest running code with the smallest file sizes. " &amp;#x200B; that hardly seems like a detraction to me. rather the opposite.
I would rename the len macro to len_unchecked. Therefore you don't have to comment on it and it's also clear when using the macro, which is imo even more important than for it to be clear when seeing the definition.
``... it boils down to using an indenting style consistently and writing it right the first time...'' Let's be honest, getting it right first time beats any other advice you can get. IMO consistent indentation is waaay overrated: it's the non-white-space bits that make the difference!
Thank you ! :) Yes, naming can be quite a tricky and subjective thing. I personally used Len because of my experience in other languages that use that name.
Thank you. It makes sense that the name is a pointer, but I never thought about it that way! :)
Sounds like a good idea thank you. It'd make the code easier to read and syntactically smoother. :)
Thanks :) yeah that is definitely a good idea since it is an unsafe function.
First of all, a segfault happens if you're lucky. You could just have silent buffer overflows without knowing for a long time. And the bugs that come out of it are very unpredictable. Also, a NPE or any exception in java is much more useful because you have more information about what happened. Java is indeed slower than C but not by that much recently. The JVM optimizes code quite well and in most cases you get near native speeds.
If you can find an implementation of C11 threads for your platforms, sure.
no it dosent work like that
I didn't read the code, but I've had to do this in the past. To test it, you should capture some packets on your box and then given a header recalculate the checksum with your own code. Do that until you get it right.
It looks like C11 threads.h is still not supported on windows (msys2). I get "no such file or directory" when trying to include it. But POSIX threads work well with msys2, just compile with `gcc -pthread ...`.
You'd have a better chance to write portable C code if you use POSIX threads *pthreads* which work even on Windows. C11 threads are not implemented by most vendors. I think only some versions of *glibc* on Windows have support for C11 threads.
I also use Windows, and it seems not to support C11 libraries. But Pelles C does work with its own libraries. So I let it compile the C11 sources into a DLL, and port it with its dependencies (e.g. pocrt.dll). What a mess :(
Damn. To be honest, a lot of my understanding of computers is heavily influenced by 1970's design and thinking. Learning about the Unix days was what got me over a very interesting barrier of entry. I felt like I knew all I could learn about computers back when I exclusively ran Windows, but I knew there was so much more out there. If you don't mind me asking, since it was created by Ken Thompson and Rib Pike themselves, what do you think of the Go programming language compared to C? Also, I notice that C is used practically everywhere and has been very popular since the 70's. I have a hard time believing that it could be replaced, unless the replacement language was hugely superior.
&gt; what do you think of the Go programming language compared to C? It uses a tuple to pass back the `(error, return value)`, but the compiler doesn't enforce that the error is checked. That's a deliberate decision by the compiler writers not to do that, and it's based on their 1970s mindset of repeatidly forcing people to write all the pointless boiler-plate rather than letting the machine do that work for us, and it leads to very subtle errors that could have been easily prevented had they cared to do so.
With just C and no libraries, it's not possible to do USB IO in C. You need at least platform-specific extensions to the libc to open device files and issue `ioctl` calls on them. Libusb is the low-level interface to use.
Had a good read of the docs, seems super simple to get something up and running
You should learn an entirely different language next. Perhaps try something like Scheme.
You'll have to be a bit more specific. By USB I/O, do you mean interacting with an USB device, or do you mean writing a custom USB stack? If it's just about talking to an USB device, then use libusb. You can use it with C99 just fine, and the API is fairly simple once you understand the underlying concepts of the different USB transfers and channels. Libusb exists for both Windows and Linux AFAIK. But even then, you'll have to to pick a specific USB version to target. USB1.1 and 2.0 are relatively easy and well supported, but if you want to do OTG or 3.0 it becomes a lot more advanced. If you want to write your own USB stack, well, surely you can do it in C99, but let's say that the language dialect would be the least of your worries.
Go, assembly language, or, like @FUZxxl said, something lispy.
After some quick googling Scheme seems very interesting. Could you provide any resources (links/books) on its fundamental concepts?
It's good to get diversity in the languages you choose to learn (and hopefully stay proficient with). C &amp; asm are great tools for low level programming so maybe now it would good to go the opposite level and try something very high level. Some high level languages I would suggest for this route are: + [Python3](https://www.python.org/) : Great for developing software systems and also makes a great glue language. Can interop with C and C++ so you could potentially incorporate some of what you know from C later down the line. Tons of jobs, libraries, blogs, etc for Python so you would have one hell of a good community. + Lisp : Completely changes the way you think about programming. Great for learning functional programming in an intuitive and fun way. Very simple language family to learn (at least the fundamentals) but **extreamely** powerful once you master it. Personally I write in the [R5RS](https://schemers.org/Documents/Standards/R5RS/HTML/) Scheme dialect of Lisp with [Chicken Scheme](https://www.call-cc.org/), but for a beginner [Racket](https://racket-lang.org/) is a fantastic choice. + ML : ML is another one of those language families that is easy to learn, but very powerful once mastered. ML has high level features like pattern matching and functional concepts, but still gives you a pretty good idea of how control flow is happening under the hood. For a beginner I would recommend [OCaml](http://ocaml.org/). Good luck and have fun!
Python, maybe? &amp;#x200B; That will lead you to /r/nim \- a personal favorite of mine. &amp;#x200B; I also suggest that you look into Lisp. Perhaps /r/Racket ?
You could simply increment and get a headstart on C++. If you've managed pretty adequately with everything C had to offer then dipping your toes into C++ shouldn't be too challenging. On the otherhand, going for a garbage collected language could be a good contrast. Something OO might be useful as that's one of the paradigms C++ is often used with. Consider Java or Kotlin.
Try a different paradigm. List based like Lisp. Functional like Haskell. OO like Java. Dynamic like Python. Or my new multi paradigm favorite, Perl 6.
&gt; Python3 I dabbled in Python for most of high school but frankly it didnt interest me. I can admit though I have tried experimenting with Markov chains and LSTM's (for text generation) and almost all the code i found was python. Lisp on the other hand seems very interesting. I think I will need a more in-depth look before I decide, but it seems the most fitting option of the bunch for me.
[Structure and Interpretation of Computer Programs](https://mitpress.mit.edu/books/structure-and-interpretation-computer-programs-second-edition) is the classic work. Otherwise [The Little Schemer](https://mitpress.mit.edu/books/little-schemer-fourth-edition).
Try “Structure and Interpretation of Computer Programs” as a general purpose textbook with a focus on Scheme. Specifically about Scheme, I have no idea. The purpose of learning a new programming language should be to broaden your horizon about what other ways of programming there are. The book above is a good general introduction into the concepts and ideas and it's rather famous.
Also if you want something interesting, also try J or Forth. Haskell is a good choice as well. For production use, I can also recommend Perl and learning some shell programming.
It’s fairly easy to just #ifdef and use windows threads on Windows and pthreads on posix. I think I could write an example project to show how it’s done.
[C11 threads were specified kinda wonkily](https://gustedt.wordpress.com/2012/10/14/c11-defects-c-threads-are-not-realizable-with-posix-threads/), so the `&lt;threads.h&gt;` API isn’t well-supported, so few applications make use of it. The C17 fixups should help *some* with this, but problems remain. Regardless of standardization, the `thrd_t` may bear little relation to the platform-specific `pthread_t`, PID/TID, or `HANDLE` used by everything other than C11 threads. This leaves you somewhat more crippled than you’d be just `#if`/`#else`ing Pthreads and Windows into your own shim API. E.g., the Pthread and OS identifiers are necessary for some signal handling stuff—e.g., sticking with C11 `thrd_t` means you can’t use platform `pthread_`\* or `sched_`\* APIs when they’re available, and you can’t make proper use of things like `siginfo_t::si_pid` when they refer to one of your `thrd_t`-threads since you don’t even know if `thd_t` thread partially/fully green (i.e., the OS can’t tell there’s more than one thread there). Worse, TLS for `_Thread_local` might not be compatible with normal platform TLS implementation(s), so compiler-specific `__thread_local`/eqv and `pthread_key_`\*/-like things would need to be handled separately/differently. C11 synchro primitives are also more annoying to use, since (modulo `ONCE_FLAG_INIT`) you have to run an initializer function before using them and a deinitializer to clean up, which precludes you from using `static`/global variables+initialization.
Try learning an object oriented language like Java or a functional programming language like Elixir.
In addition to some of the other mentions, I’d recommend playing with Erlang a bit—totally different paradigm on a few fronts and the execution model is quite different from those of other functional languages. It feels very much like an underspecified functional C89.
Assembly, with a recommendation for x86 and x64.
Ocaml is a good choice, it is easy and powerful.
This is a C subreddit.
Ahhhh I didn’t specify the C++ version, totally forgot that GCC defaults to 14? I used GCC 9.1.0
I’d strongly recommend sticking with a third-party library or at least going at it via some FD-/`HANDLE`-based API provided by the OS libs/kernel. On \*ix, `strace` and/or a debugger will help you figure out how USB command-line tools (e.g.., `lsusb`) are working under the hood on your OS, so you can work from there fairly easily. If you want to have at USB hardware directly (in which case the language standard is mostly irrelevant), you’re going to have to do a shit-ton of work and it’s probably going to have to be kernel/supervisor-mode software. Different USB hardware needs different drivers, and any attempt to fiddle with the hardware directly from user mode will potentially bring you into conflict with anything else on the same host that happens to use USB.
This is a C-only subreddit, but I think you're only changing the object the function-scoped class1 variable is pointing to. The class1 variable outside the function remains unaffected.
\&gt; qsort() is fine - it's tractable by very brief inspection. &amp;#x200B; Nowadays, if a function is going to take a callback, it should generally allow the client a means of passing data to the callback if desired. When qsort() was written, it was reasonable for clients to pass data to callback using objects of static duration, but today it's generally better to have pointers to such information passed as parameters.
Your `len` function is what most people call `countof`, FFR. `len` tends to be either a getter (e.g., `(thing)-&gt;length`) or something that takes &gt;O(1) time (e.g., `strlen`), but either way it’s usually something the compiler can’t see right away. Whereas your `len` macro looks and behaves analogously to `sizeof`, `offsetof`, `__typeof__`, `_Alignof`/`__alignof__`, referring only to static properties of the argument’s type.
C# is off topic in this subreddit. Please post C# questions to /r/learncsharp instead.
While Javascript is in many ways a crummy language, Javascript implementations are included in almost every web browser, and some of them offer astonishingly good performance. The browser dialect of Javscript have some limitations, but no other language comes closer to being able to "run anywhere".
My next step would be Python. Given its power, broad usefulness, and popularity, I think it's worth knowing for every working software engineer. If you know C, dipping your feet into C++ is a logical next step, but it's a vastly bigger language and how much of it is worth knowing probably depends on what kind of software you work on. Beyond that, I'd learn the languages that are actually used in the software domain you're interested in working on. I don't really understand the whole notion of learning a language for its own sake. For me at least, a programming language is a tool I have to grudgingly learn to make the software I want, so the choice of language follows the real-world problems I'm trying to solve.
Now that you know a programming language, it should be petty easy for you to pick up another one. So from here I would suggest picking a programming language suited for something you want to do / try. The first thing I wanted to try after learning C, was web application development. I personally learned JavaScript (nodeJS, and different libraries like React) after C. And then I learned python. By knowing these two, I was able to make very rich web apps, and I was able to work on both front and back end (which is something I’ve always loved). List of programming languages I recommend: - JavaScript (great for web development, both front and back) you can do just about anything with JavaScript these days. Even make desktop apps (with electron) - Python. (Specifically learn Python 2, then 3). I say this because as someone who works in the industry, most companies still use Python 2.7 because of stability. Python 3 will be fine for personal scripting. - C++/C# for game development (unreal or unity)
I can't believe nobody has suggested Perl yet. It's a good contrast as it's a high level scripting language that'll help break you into a bunch of new ideas.
&gt; A lot of subtle security bugs have to do with the way the vast majority of C programmers casually throw types around without regard for the integer promotions, usual arithmetic conversions, integer overflow, and such (the properties of which can differ across different systems, compounding the problem). System designs are much more uniform than in decades past. What has changed is that compiler designs used to extend the languages in ways that helped programs meet the primary requirements: 1. When given correct data, yield correct results. 2. Don't do anything harmful beyond yielding meaningless results, even when given malicious data. The amount of machine code necessary to meet the second requirement will usually be fairly minimal. Unfortunately, today's compilers are prone to aggressively "optimize out" the relatively small number of instructions that would be necessary to meet the above requirements unless forced to generate extra code to ensure that *specific* meaningless results are produced. For example, there are many situations where it may be faster to perform a calculation whose results may or may not matter, than to determine if the calculation is necessary. When using just about any kind of compiler, it has always been necessary to worry about whether overflows could occur in cases where the results of a calculation would matter. When using "traditional" compilers compilers, however, there was no need to worry about overflows in calculations whose results would end up being ignored or otherwise irrelevant. Transformations such as replacing `x+y&gt;z` with `y&gt;0` in cases where `x` and `z` are known to be equal might affect whether the expression yields 0 or 1 in case of overflow, but if the only situations where overflow could occur are those where it wouldn't matter whether the expression yields 0 or 1, that wouldn't be a problem. Unfortunately, when using "modern" compilers, there's no nice way to express the concept "if the arithmetical value of (x+y) is within the range of "int", yield "x+y &gt; z", and otherwise yield 0 or 1 in whatever fashion is convenient". I would think that for most kinds of tasks, such semantics would be useful more often than "if (x+y) is within the range of "int", yield "x+y &gt; z", and otherwise behave in totally arbitrary fashion", but "modern" C compilers prefer to use the simple "x+y &gt; z" to represent the latter meaning, rather than supporting the [once] "popular extension" of giving it the latter.
 struct psuedo\_header mispelled? Nice coding style by the way
It gets a bit complicated:)
I've done it in the past, but I think the gcc port for windows supports pthread. However you're not entirely wrong, since a programmer may want their code to be portable between microsoft visual studio's compiler and gcc. Also there is a pthread-win32 library which I *think* works in Visual Studio, so if that is the case I would rather make use of this library instead of mixing WinAPI and POSIX code.
My multithreaded renderer works natively on posix and windows, wasn’t painful at all to get it working.
I agree that it isn't painful. But for another developer to work on the code you wrote would require him to know WinAPI threads and POSIX, not to mention that it leads to a code bloat. I'm not saying your code is bloated, but if a naïve person attempts to do this it would probably lead to a code bloat.
OpenMP works on both platforms but you don't get the same kind of fine grained control as with threads.h. Most people don't really need ylvery fine grained control though. The plus point if the system doesn't support multithreading the code will still compile
I wish the authors of the Standard would recognize that many C programs are intended to run in real execution environments, rather than an "abstract machine", and focus on *general* means by which programmers can exploit features that various targets support, recognizing that some features may not be supportable in all environments, For example, instead of having a `_Thread_local` qualifier which is limited to one specific purpose, I would like to see a syntactic construct that would allow structures to be built from declarations in different compilation units (something that many linkers could support by creating for each "section" a structure that starts at address zero), and a syntax to export some or all members of a structure as identifiers in the enclosing scope, with or without indirection. A number of embedded operating systems support a single pointer-sized thread-local object, but that's all that a program would need if a structure could be built containing all of the thread-local objects needed throughout the program, an instance of that structure was created for each thread, and if references to thread-local variables could be written to access them through the one thread-local pointer. For a compiler to support `_Thread_local`, it would have to coordinate its efforts with the rest of the system, which would only impossible in scenarios where a freestanding compiler is used to process code for a system about which the compiler writer knows nothing except the CPU architecture. Not all linkers would be able to support such constructs, and use of such constructs might make programs unusable on platforms whose linkers can't support them. On the other hand, relatively few platforms would be incapable of using linkers that could support such constructs, if such linkers existed. Having the language recognize optional features requiring advanced linkers would help minimize the need to use 1970s linker technology.
Yeah great idea. Thanks
Yup, will fix it. Thank you
1. No, but some tools can detect going out of array cases. 2. It doesn't matter where you decide your function will accept the size. Some functions have it before the array pointer, some after.
Scheme, small talk or prolog These are separate paradigms that aren’t the traditional C language. They’ll change the way you think and solve problems
The best leverage I have gotten from a language is from Tcl. It's an "everything is a string" language. It can easily open C ( or any other language; C just has better binary data handling than most ) programs as pipes. The GIU kit for Tcl , Tk is used on lots of other languages and IMO, is the easiest way to get a cheap gui. It seems to be widely ignored. Where (IMO) both Python and Perl suffer from popularity, Tcl is steered by some extremely smart computer scientists. Tcl also suffers from Quoting Hell. It's a constant irritant. If you go with Tcl, find a copy of the Brent Welch book. I'd get it in a printed-on-paper format. ISBN-13: 9780130385604
I still use Octave but I've been eyeing NumPy and playing a bit with it. NumPy *seems* a good thing for exploiting video card supercomputing; I am not sure that Octave even does at all; MATLAB does but even as reasonable as the price is, I can't bring myself to.
You don't have to use much of C++ really. std::string, std::vector and std::map are worth using. And even templates have their charms, if they're a bit clunky. I just wish they'd hire somebody who can speak English to rewrite all the compiler errors/warnings in C++.
Just remember that this is just syntax sugar for documentation puposes. The array argument still decays to pointer. Some people (I believe Torvalds is one of them) strongly opposes doing this because it can trick the programmer into thinking s/he has an actual array and not just a pointer.
I use AStyle ( Artistic Style ) when I can, and it makes the entire code formatting issue completely moot.
This guy codes :)
Yeah; I dunno - I use monad-like[1] interfaces between chunks of C code. You can then hook those together with UDP or TCP sockets for final assembly. [1] I have absolutely no interest in teh tweed-jacket-with-leather-patches verbiage surrounding them as abstract objects, though. It should not matter but it sure seems to. And it's generally pretty performant; localhost is pretty quick. Hook together quick; optimize at leisure.
So if you make a multithreaded C application which uses strings to communicate between threads, you've already gathered what is IMO the best thing about monads. You're also into the "actor pattern" from Haskell. And if you use a socket interface, you can write scripts that do lots and lots of testing for you. If strings are a bottleneck, the optomize it. Sockets *may* be, but there's always a way. localhost is pretty quick. This isn't much help with games or device drivers, but I bet there's a way to shim something in to help with those. Or you could be a masochist. Your call :) ( and it may be that I am the masochist here, hard to say :)
I would emphasize C++. It takes the foundation of C, and tries to add zero-overhead abstractions on top. So, many design patterns that you may want to use in C are automatically included already in C++ via the language itself or libraries. And of course you can still use “a lot” of C in a C++ program, so it is an efficient use of your time in that sense, though it is by no means easy or obvious. You will of course have some understanding of “close to the metal” programming from C, and then become exposed to abstractions via C++, giving you a good foundation before you explore more abstract languages like for scripting. Starting with scripting first may not be so bad, but I think starting at C is important for one who really wants to understand how thing work rather than just “get things done”. So, in the end it depends on your personality. Do you want to get into the nitty gritty or do you want to be a bit more pragmatic. What do you want to do in the future, and how may that influence your future learning path. I started with Java and it always frustrated me because it was so abstract that certain rules and patterns confused me deep down. Learning C and then C++ has helped me finally grasp programming to the degree where I am now comfortable and confident writing code without having so much doubt about what was being done “for me”. But that’s just my personal experience, and I still have a lot to learn, especially the functional paradigm. But C++ has options for that, and OOP, so I think it was a gold decision to get a good understanding of that first, especially because it is useful in more traditional programming tasks like UI work.
Classic case of fixing something that isn't broken.
Based on usefulness on the job python and bash scripting. Test frameworks tend to make use of both use with Jenkins and git and you will be able work well in an environment with continuous integration.
Bash is good to know. It is very powerful and can be used to get the most out of your already existing c programs. If any of your programs can do stream manipulation, they could included into a bash pipeline like any other bash tool like cut, grep or sed.
try a functional language like F# or clojure. i'm particularly fond of F#
I'd dabble in C# and C++ and maybe a dynamic language like Python and just find what you like working with more.
6 or 5?
&gt;I just wish they'd hire somebody who can speak English to rewrite all the compiler errors/warnings in C++. Use a relatively recent version of clang to compile.
NumPy doesn't do GPU processing. For that, you'd need either CuPy, Tensorflow or PyTorch.
I say this every time. SICP isn;t that useful day to day but for the whole of your career it will change the way you think about and write code.
This is slightly less useful than: int main(int argc, char** argv) /* argc+1 elements, null terminated */ But much more error prone.
Ah - okay. Thanks
Heh - thanks for the suggestion. I may just - although I have to use GNU at work.
I started with C++, learned C deeply during my operating systems class, and now when I go back to C++ it feels very unwieldy by comparison, especially with the pressure to use the modern features. I don't even get to touch raw pointers anymore which is naturally disappointing. C-style arrays can be pried from my cold, dead hands, apparently there's even a wrapper for those too.
The Unix Shell (e.g. bash) is a functional language, using pipelined coprocessing I/O for process composition. As such it is the most popular, and a skill far beyond anything that comes up in job posting keywords, for some unfathomable reason.
Huh... ML isn't just for machine learning
CuPy uses the same API as NumPy, though, so it's a simple solution.
What do you want to do? Knowing what you want to do with the language would help to narrow the choices down. If you're just interested in learning a language for its own sake, you could try: * Taking a course online that covers multiple languages, such as Programming Languages on Coursera: [Part A](https://www.coursera.org/learn/programming-languages) (ML), [Part B](https://www.coursera.org/learn/programming-languages-part-b) (Racket), [Part C](https://www.coursera.org/learn/programming-languages-part-c) (Ruby). * Scheme (a lisp). I've heard great things about [SICP](https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html), but I haven't read it, regretfully. * Python, as others have suggested, is very different from C and also very practical. It's a top language right now in industry, especially in data science and machine learning. I think it's still a major contender in server development as well. * Java. Lots of people hate it for some reason. I love Java. It's got its quirks, sure, but I get the warm and fuzzies every time I use it, and it's one of the top languages in use today. Lots of practical stuff you can do with Java. Write servers. Program robots. Create a Minecraft plugin or write your own game. * Rust. A very exciting up and coming languages. Has a lot of really nice features like compile time memory/thread safety guarantees, traits and an official package manager. It's a tough nut to crack at first (I'm still working on it myself), but it's really nice. * JavaScript. Say what you will, but JavaScript is one of the top languages in use today. Learning JavaScript opens up frontend web development as an option for you. If you want to make a website that doesn't just sit there looking pretty, you'll have to go down this road eventually. Like Java, JavaScript has its quirks too, but is overall an enjoyable experience to use if you ignore some features (like double equals, unless you really know what you're doing). Paired with TypeScript, it's actually a joy. If you're feeling analysis paralysis, I'd go with Java if you want something practical, or Scheme if you want your mind blown.
I would recommend learning Bash, but it’s not really a functional language because it doesn’t have first class functions or lambdas.
C++
Once you have a solid grasp of functional programming I have to highly recommend Haskell as well. Very modern and clean but a bit strange at first glance, but a ton of fun to program in.
You should learn bash because of legacy code and your hatred for its syntax and limitations will be your catalyst to learn a proper scripting language like Python or Perl 😁
It's been ages since I last heard someone mentioning Perl 6.
I think getting a grasp of a few different paradigms would be useful. C# for object oriented Haskell for functional Python/Ruby for a scripting approach (I know it's not a paradigm, but I feel like being able to write small and useful scripts is a good skill). But there are tons of languages that would fit these bills, these are just my favorites. Look at software you like and see what they were written in. Check out what features you like and go from there. Every language you experiment with can teach you new ideas that will help you all around.
Mingw "gcc version 5.1.0 (tdm-1)" supports pthreads naturally. Google for "pthreads-win32" for the VC library.
Yeah, sure using the standard library is verbose, but one could use a typefdef to simplify the code. The crucial reason is that the standard library containers have type safety. Therefore, if type safety is important, you don’t mind the extra step. It depends on programming style. Type safety is an important limitation within the C++ standards because in OOP one would naturally use type safety for inheritance and polymorphism. The creator of C++ Mr. Stroustrup explains it well when he says that it lends itself to the OOP abstraction of framing a program, in particular, using “is a” relationships to deduce a structure for the program based on objective abstraction. The fact that there is no run-time abstraction is what makes it even more appealing. One can get compile time checks which can catch certain classes of bugs.
It's come a long way. Check out Damien Conway's take on the Perl Weekly Challenge and the various paradigms available in Perl 6 he uses to solve the challenges. Good stuff. http://blogs.perl.org/users/damian_conway/2019/05/why-i-love-perl-6.html
You are nominally correct, but there is a technical work-around: https://stackoverflow.com/a/14027415/949568
I understand the OOP principles pretty well and I don't think I abuse them (I had two years of lower division CS, I took a one year track in Java and retook it later in C++) and I've written some sizable projects in modern C++ (a raytracer, ~4k LoC), I just don't find it as enjoyable to write compared to C. I suppose if I tried to rewrite it in C I might change my mind since templates in particular made some parts much easier than they would have been in C.
It's come a long way. I came across Damien Conway's take on the Perl Weekly Challenge and his use of the various paradigms available to Perl 6 to solve the challenges. Got me excited again. Check it out. http://blogs.perl.org/users/damian_conway/2019/05/why-i-love-perl-6.html
Just to note though, GCC does support them, just use 0b or 0B prefix. &amp;#x200B; Example: `0b100110001001011010000000`
Either is probably fine. Five has a larger install base obviously but six is clearly the way forward.
Scheme is really just another lisp, but a good one. Guile is a wonderful implementation of it. And it can be used (as it was designed to be) as an extension of C. So it is worth learning for two reasons: get a new paradigm and extend your usefulness as a C programmer!
That's genius and gets the job done.
Mandate that 128-bit integers work on all compilers — yes, even on 32-bit targets! — and call the type `(u)int128_t` instead of `__(u)int128_t`
Is there a preference between char* argv[] and char** argv? I've always used char* argv[]. I guess that signals to the programmer that the pointer is to an array.
Thanks mate
I second the Rust suggestion. I personally think that Rust is one of the more interesting languages in recent years. It has this extremely rare quality of "having a cake and eating it too" in how it approaches memory management.
This is a matter of taste, and our taste may differ. Context: when I started using unix in the early 90s, I was advised against stuff like ``alias rm bin/rm -i`` by grey-bearded hackers, because *you need to understand that rm does not forgive you, and should learn to live with that, or it will bite you when you less expect it* So, you are right that ``char *argv[]`` is more specific, but the chars are themselves arrays, so the notation should be `char argv[][]``, which is not legal. And you need to learn both representation. My opinion is that, if you have to handle something, don’t hide it. And, in C, in function arguments declarations, `char **foo`` and `char *foo[]``are absolutely identical, and only the first can be uniformly applied. So, it means that when you see ``x *y``, ``y`` may or may not be an array. *so you have to cognitively handle it*. Hence, this is what I do, and never use arrays when we really are talking about pointers. Not sure if my point is understandable, but I tried :-)
As everyone said, [SICP](https://mitpress.mit.edu/books/structure-and-interpretation-computer-programs-second-edition), even if you never ever program in scheme after that.
I would say, do a functional language. Haskell, for instance. Just to open your mind a bit. Or [Scheme](https://mitpress.mit.edu/books/structure-and-interpretation-computer-programs-second-edition).
That's fascinating language wise, thanks for sharing.
Who said otherwise?
Sadness
I just want to interact with the USB unit directly, but seems that is not possible without a library. Sadness
&gt; Sadness
Beware that in the future you will likely run into more and more systems that forbid mapping pages with both `PROT_EXEC` and `PROT_WRITE` at the same time. That would manifest as a `MAP_FAILED`. The work around is to either map `PROT_WRITE` first, write in the instructions, then `mprotect()` to switch over to `PROT_EXEC`. Or you could map the same physical memory twice: `PROT_WRITE` in one place and `PROT_EXEC` in the other. If the purpose here is performance, you should probably avoid doing the map/unmap every time you call a native function. That's a lot of overhead: two page table modifications, a page fault (those pages start off mapped to the read-only zero page), and the kernel zeroing the new pages before they're actually available.
Do you mean Unix Like Platforms just to clarify as this is always confusing? As you cant get UNIX itself?
I mean, what did you expect? The C standard explicitly does not concern itself with hardware access and platform specific code. You can go and replicate the functionality of the libusb in your own code, but even then you do not write pure C code because you need to use platform-specific functions to access the necessary device files.
&gt; *the properties of which can differ across different systems* &gt; System designs are much more uniform than in decades past. What has changed is that compiler designs **Integer overflow:** A `size_t` will overflow on many 32-bit machines before it overflows on a 64-bit machine. Same with a `long`. This is a difference that exists even before aggressive optimizing compilers enter the picture to thwart wrap-around expectation on signed integers and optimize away some erroneous overflow tests that are actually fairly rare in practice, much rarer than instances of `size_t` unsigned overflow during size calculations for `malloc()` and friends. **Usual arithmetic conversions:** `puts((long)-1 &lt; (unsigned)10 ? "yes": "no");` will print "no" on typical 32-bit machines, "yes" on typical 64-bit machines.
I like functional programming languages, they are very elegant and help to avoid common imperative bugs but I can't decide to learn Haskell (or any other) in depth because I believe that they are not used in the industry except from special occasions (e.g. compilers). Am I wrong?
They are actually used a fair bit in industry. Not as much as behemoths like Java or C++ obviously, but a non-trivial amount of real world software is written in functional languages. I think more importantly though it's the knowledge you gain from learning a functional language that is most useful, not the language itself. I **fucking love** Scheme, but outside of personal projects and university research I've never used Scheme to build a software system (I'm primarily a C/C++ developer). And that's okay, because the most important things that Scheme has taught me are applicable across languages and projects. I would say its say how leaning an instrument is useful: most people are not going to be professional musicians, but the knowledge you gain from picking up the instrument may help in other areas of your personal and professional life and allow you to view the world in a new way.
&gt; int main(int argc, char* argv[argc+1]) Does this even satisfy 5.1.2.2.1's "or equivalent" in relation to `int main(int argc, char *argv[])` when one allows `argc == INT_MAX` but the other does not?
It's almost like i mentioned a specific platform i wanted to use, but what do i know. Apperently i wanted to do it in pure C code and not use the windows API.
The vast majority of so-called "aliasing violations" do not involve aliasing in the code as written, but instead involve compiler writers either misconstruing N1570 6.5p7 as defining the term "aliasing", rather than merely saying that the purpose of the rules is to specify when compilers must make allowances for it, or else interpreting the phrase "when things may alias" as including "when compilers may rewrite code to *introduce* aliasing without having to make allowances for the consequences thereof." While the Standard doesn't explicitly specify that the combined acts of using a pointer or lvalue of one type to derive a pointer or lvalue of another, and accessing the object identified by the latter pointer or lvalue while is freshly derived, should be recognized as an access made via the original pointer or lvalue. Without such allowance, something like `myStruct.arrayMember[i] = 1;` would only be allowed if `arrayMember` is an array of character type. Note that in terms of actual aliasing, there's a huge difference between: myStruct = someOtherStruct; int *p = myStruct.arrayMember; *p = 'H'; someOtherStruct = myStruct; and int *p = myStruct.arrayMember; myStruct = someOtherStruct; *p = 'H'; someOtherStruct = myStruct; In the former case, `p` is used at a time when its formation represents the last usage of the object from which it was derived and it is thus "fresh". All other uses of `myStruct` occur either before the formation of `p` or after its last use, and thus do not alias it. In the latter case, the assignment to `myStruct` occurs between the formation of `p` and its last use, and consequently the lvalue `myStruct` in that assignment aliases `p`, and the Standard would not require accommodation for that. If compilers writers had recognized that simple principle (which many people at the time would have thought was so obvious it didn't need to be stated) reliance upon the "character type" exception could have been deprecated ages ago, greatly improving the performance of code that uses character types to represent characters, while simultaneously eliminating most of the need for the `-fno-strict-aliasing` flag.
If you're not afraid facing challenges, then mind taking a look at Rust. It has a C/C++ like syntax but many more advantages that reduce runtime errors to a minimum. Also it is (like C) a systems programming language which also can be used for higher level purposes and also web development. I had a dedicated course at uni just to learn Rust and I'm glad that I attended it. It's absolutely worth learning it or at least taking a look at it. It might seem hard at first, and it is.., but as soon as you get into the features like lifetimes, borrowing, etc. you'll love it.
What exactly is “pure C code” for you? If you mean “the language and functions specified in ISO/IEC 9899:1999,” then it's not possible on any platform that I know. Please keep in mind that C was always and still is a language that is based on rich libraries provided by the platform. For standardisation, they removed all the pieces from the C standard library that are for hardware access or OS-specific things. That doesn't mean that such functionality doesn't exist, it's just not part of the C standard because it's out of scope.
What does “directly” mean for you? Note that even if you directly talk to the operating system (whatever that means), there are still multiple layers between the operating system and the USB device. The most direct way to talk to a USB device would be to cut off the cable and to manually induce current into the signal lines to send signals. But that's of course rather pointless.
That's probably the most beautiful thing I have ever read.
And again, do not remove your post after receiving answers! By removing your post, you deny future people the answers you got. That's just a shit thing to do. Don't be that person. Don't delete your posts!
If you consider `eval` to be the function call operator, then it does have first class functions as functions are just text.
What answer, I apperently give a coherent question
I think Perl 5 and Perl 6 are very different languages, both in the way they were designed and in the way they are meant to be used. The syntax is very similar, but that shouldn't fool you.
I see at least 5 comments that can be understood as giving an answer. If someone else has the same question, that person is now going to make a new thread to get the same answers, wasting everybody's time. Don't waste time like that. Do not delete your posts.
&gt; I dabbled in Python for most of high school but frankly it didnt interest me. It doesn't have to interest you. I'm not interested in Python either but boy is it nice to whip up quick little tools to automate testing some new feature I'm developing, or to provide some little convenience utility. If you plan on being a professional you _need_ to have a good scripting language in the toolbox.
For numerical stuff my goto language is julia
Nobody, I was just surprised
I didn't say they were perfectly uniform--I said systems were more uniform than the past. How many systems could not sensibly accommodate a C compiler where \`char\` is 8 bits, \`short\` 16, \`int\` 16 or 32, \`long\` and \`size\_t\` 32, and \`long long\` 64, and pointers either 16, 32, or 64? Some kinds of programs would require implementations where \`long\` and \`size\_t\` are 64 bits, but many others could run more efficiently using smaller types. Provided that code uses fixed-type sizes on ABI boundaries, the only situation where interoperation between 32-bit and 64-bit code should pose any kind of a problem would be with \`va\_args\`. One could write a 32-bit implementation that used a 64-bit ABI for \`va\_args\` functions, but it wouldn't be binary-compatible with other 32-bit code. &amp;#x200B; Also, looking at the published Rationale, do you think the authors of the Standard could have imagined that something purported to be a quality general-purpose compiler targeting silent-wraparound two's-complement hardware would use the Standard as an excuse not to process: &amp;#x200B; unsigned mulMod65536(unsigned short x, unsigned short y) { return (x\*y) &amp; 0xFFFF; } &amp;#x200B; in predictable fashion for all values of \`x\` and \`y\`?
I think the bigger problem is that there isn’t really possibility for nesting variable scoping since functions-as-text is really limiting. `local` is the only way to limit scoping of a variable which limits what kinds of higher-order functions you can create.
I treat Python like duct tape. I don't care if you're a systems programmer, an engineer, a carpenter .. sooner or later you're going to reach for a good roll of tape.
&gt; Note that in terms of actual aliasing, there's a huge difference between: &gt; &gt; myStruct = someOtherStruct; &gt; int *p = myStruct.arrayMember; &gt; *p = 'H'; &gt; someOtherStruct = myStruct; &gt; &gt; and &gt; &gt; int *p = myStruct.arrayMember; &gt; myStruct = someOtherStruct; &gt; *p = 'H'; &gt; someOtherStruct = myStruct; Assuming `arrayMember[0]` is an `int`, neither of these snippets violates 6.5#6-7. The relevant lvalue expressions are `*p` and `myStruct`. `*p` has "a type compatible with the effective type of the object", and `myStruct` has "an aggregate or union type that includes one of the aforementioned types among its members (including, recursively, a member of a subaggregate or contained union)." I'm not sure what you're trying to say, unless it's only that there have been compiler writers who'd fail to recognize both of those snippets as valid C.
#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; int main(){ char first[50], last[50], id[50], pw[50]; start: printf("Hello! This program helps you create an account!\n\n"); printf("Enter your first name: "); gets(first); printf("Enter your last name: "); gets(last); strcpy(id, first); strcat(id, last); password: printf("Enter your password: "); gets(pw); if (strstr(pw, first) || strstr(first,pw)){ nada: printf("\n\nCan't have similarity between password, last, and first\n"); goto password; } else if (strstr(pw, last) || strstr(last,pw)){ goto nada; } else if (strstr(pw, id) || strstr(id,pw)){ goto nada; } else{system("pause");system("cls");printf("Here is your account details.\n\nUsername: %s\nPassword: Your password has %d characters\n", id, strlen(pw)); system("pause");system("cls"); } int a; printf("Do you want to:\n(1)Return Home\n(2)Exit Program\nEnter Number Choice: "); scanf("%i", &amp;a); if (a==1){goto start; } else{return 0; } }
Seconded, I also don't understand the question completely. OP, are you saying that you want to bit-bang the USB protocol all by yourself on just the D- and D+ differential wires? Because that [is possible](https://www.obdev.at/products/vusb/index.html), kind of, but it's not for the faint of heart. USB is not a simple protocol! It's very unlike oldfashioned RS232. I suggest [this site](http://www.usbmadesimple.co.uk) as a decent hand-holding through all the language of the USB spec.
Put 4 spaces in front of lines to get the formatting to work: #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; ...
Can you please upload your code onto pastebin or format it by putting 4 spaces before each line.
&gt; #include &lt;stdio.h&gt; &gt; &gt; #include &lt;stdlib.h&gt; &gt; &gt; #include &lt;string.h&gt; &gt; &gt; int main(){ &gt; &gt; char first[50], last[50], id[50], pw[50]; &gt; &gt; start: &gt; &gt; printf("Hello! This program helps you create an account!\n\n"); &gt; &gt; printf("Enter your first name: "); &gt; &gt; gets(first); &gt; &gt; printf("Enter your last name: "); &gt; &gt; gets(last); &gt; &gt; strcpy(id, first); &gt; &gt; strcat(id, last); &gt; &gt; password: &gt; &gt; printf("Enter your password: "); &gt; &gt; gets(pw); &gt; &gt; if (strstr(pw, first) || strstr(first,pw)){ &gt; &gt; nada: &gt; &gt; printf("\n\nCan't have similarity between password, last, and first\n"); &gt; &gt; goto password; &gt; &gt; } &gt; &gt; else if (strstr(pw, last) || strstr(last,pw)){ &gt; &gt; goto nada; &gt; &gt; } &gt; &gt; else if (strstr(pw, id) || strstr(id,pw)){ &gt; &gt; goto nada; &gt; &gt; } &gt; &gt; else{system("pause");system("cls");printf("Here is your account details.\n\nUsername: %s\nPassword: Your password has %d characters\n", id, strlen(pw)); &gt; &gt; system("pause");system("cls"); &gt; &gt; } &gt; &gt; int a; &gt; &gt; printf("Do you want to:\n(1)Return Home\n(2)Exit Program\nEnter Number Choice: "); &gt; &gt; scanf("%i", &amp;a); &gt; &gt; if (a==1){goto start; &gt; &gt; } &gt; &gt; else{return 0; &gt; }
That's a different thing, but it is a valid point of criticism.
&gt;Also, looking at the published Rationale, do you think the authors of the Standard could have imagined that something purported to be a quality general-purpose compiler targeting silent-wraparound two's-complement hardware would use the Standard as an excuse not to process ... in predictable fashion for all values of `x` and `y`? No, which is why I support the proposals seeking to limit "run-away" interpretations of the standard.
Just a general comment not related to goto: gets() is an antiquated function vulnerable to certain attacks. It's best to get in the habit of using getline() instead of gets() if you need to read user input.
Please format your code, now it looks very ugly.
Really? I have to be the guy to say it? Fine then. "goto" is fine when you're stuck and there's no other way out. But most of the time, a simple while() or do/while()loop does the trick for what you're trying to accomplish. Also, it's considered much better form. I'm not the guy who says "goto should be banned." But, I assure you that if you ever work in the field, or take a class on C, somebody is going to come down hard on you for using four gotos in 20-some lines of code.
Thank you for actually giving useful advice.
The standard allows an lvalue of aggregate type to be used to access an `int` within it. The Standard does not allow an aggregate to be accessed using an arbitrary lvalue of member type. C would be rather anemic if aggregates couldn't be accessed using member lvalues, but nearly all code that needs to access aggregates using lvalues of member types uses pointers or lvalues which are visibly freshly derived from the aggregates in question, and the accesses should thus be recognized as being accesses to those aggregates. Very little code would require aliasing between struct members and seemingly-unrelated pointers of member type, and requiring that compilers accommodate that would needlessly impair optimization. Consider the function: struct foo { int size; int *dat; }; void clear_foo(struct foo *p) { for (int i=0; i &lt; p-&gt;size; i++) p-&gt;dat[i] = 0; } Should a general-purpose implementation be expected to allow for the possibility that it's invoked by a function like the following: int test(void) { struct foo s; s.size = 1000; s.dat = &amp;s.size; clear_foo(&amp;s); return s.size; } or would it be reasonable for a general-purpose implementation to assume that `p-&gt;size` won't change during the execution of `clear_foo`? When invoked by `test` above, `p-&gt;dat` is not formed from `p-&gt;size` (nor any part of any `struct foo`) between the time that `p-&gt;size` is accessed via other means unrelated to `p-&gt;dat` and the time that `p-&gt;dat` is dereferenced to access the same storage. I don't think the authors of the Standard wanted to require compilers to make provisions for such aliasing. Fundamentally, the notion of *storage* having "effective types" is worse than useless. It might be useful to attach the notion of an "effective target type set" to pointers, so that given `float*p;`, the effective target type set of `(uint32_t*)p` would be the union of {`uint32_t`} and the effective type set of `p`. If `p` is an argument of a function, a compiler would be allowed to process it in either of two ways at its leisure: (1) assume the effective type set is simply {`float`}, but only apply the rules to objects that are referenced within the function, or (2) assume the effective type set is that of the actual passed-in argument, and apply the rules to objects which are accessed in either the function or its caller. Such rules would be resolvable when generating code for any function based solely upon a program's static structure, without having to consider anything outside the function except when performing in-lining (which would naturally require simultaneous consideration of the functions involved). By contrast, consider how the Standard's Effective Type rules interact with something like: void test(float *fp, int *ip, int mode) { *fp = 1.0f; *ip = 2; if (mode) *fp = 1.0f; } If the Standard didn't attach Effective Type to storage, a compiler would be entitled to assume that `fp` and `ip` won't alias, which would allow it to move the write to `*ip` before the first write to `*fp` or after the last write, and in either case eliminate the conditional test and the second write to `*fp`. If one recognizes that an access to a pointer derived from a `float*` should be presumed to interact with accesses to any `float` the original could have identified, however, that would make clear that something like: float *fp1 = malloc(4); *fp1 = 1.0f; ... int *ip = (int*)fp1; // Reuse the storage as "int". *ip = 2; ... float *fp2 = (float*)ip; *fp2 = 1.0f; a compiler should recognize that any operation on `*fp1` which predate the creation of `ip` should precede any operation performed using `ip` or a derivative, and any operation on `*ip` should precede any operation performed using `fp2` or a derivative. Note that the above code should *not* be considered equivalent to: float *fp1 = malloc(4); *fp1 = 1.0f; ... int *ip = (int*)fp1; // Reuse the storage as "int". *ip = 2; ... *fp1 = 1.0f; because the last floating-point assignment uses a pointer which is not derived from `ip`.
To specifically answer your question, the call to scanf("%i") is your problem. You read an integer, but don't read the new line character that the user enters after the int. When you go back to start, this newline will be read from stdin at the first gets, so it treats it as the user entering no information and just hitting enter. Some more general advice: using gets() is a bad idea as it is a security vulnerability, use fgets or something like fscanf or getline instead. fscanf is better than using scanf as well.
&gt; The standard allows an lvalue of aggregate type to be used to access an int within it. The Standard does not allow an aggregate to be accessed using an arbitrary lvalue of member type. What exactly are you replying to? If you think something in one of those snippets violates 6.5#6-7, it would be far less tedious to point out the offending line and what it violates.
The Standard has accumulated decades of technical debt based upon the fact that there were many corner cases where compiler writers of the day would have no reason to care about what the Standard said, and there was thus no reason to expend the effort necessary to consider all of them in detail. Suppose that a program needs behavior equivalent to the following: void copy(void *restrict dest, void const *restrict src, size_t n) { if (!n) return; if (dest == src) memmove(dest, dest, n); // Note that no data storage is ever accessed via src else memcpy(dest, src, n); } There are two ways this need could sensibly be accommodated: by the programmer writing the function as above, or by the programmer writing `memcpy(dest, src, n);` and an implementation processing `memcpy` as described above in all cases where the Standard would define the behavior of the above. For code which is only going to run on platforms where `memcpy` could offer the above guarantees at zero cost, it would make sense to have `memcpy` offer such guarantees, and have the code exploit them. If on some platform it would be expensive to have `memcpy` offer such guarantees, requiring that programmers handle those corner cases themselves, while letting the compiler generates cheaper `memcpy` code that doesn't handle then, *might* make more sense. Expending ink requiring that all compilers offer such guarantees would risk making the language less useful for the only people who would have reason to care about whether such guarantees are required.
Like for error handling
The bug you're asking about is probably due to `scanf` leaving a newline in the standard input buffer. Also, as others have pointed out, there are a few more problems you should fix while you're here: * Remove the `goto`s. They're a pain in the ass to read. Loops that break or terminate when certain conditions are met are generally preferable to `goto`. For example: something like `while (invalid) { // check and maybe get new input }` would make the password validity checks more readable. * Do not use `gets`, `strcpy`, or `strcat`. They are vulnerable to buffer overflows and there are better options (such as `fgets`, `strncpy`, and `strncat`). * `scanf("%i", &amp;a)` will fail to read a valid integer if the user enters something like "goodbye". As a result, `a` may not contain a useful value. Your program should check for error conditions like this whenever it takes input from the user, because it is unreasonable to expect the user to always type the correct thing. There's probably something I've missed, and there are probably even better alternatives to some of my suggestions, but hopefully this will point you in the right direction.
Something that might help you in programming C is to learn about [structured programming](https://medium.com/@floriopotter7/structured-programming-assignment-help-abcec6f2509c). Structured programming, among other things, avoids using \`goto\` because use/over use of \`goto\` can result in [spaghetti code](https://en.wikipedia.org/wiki/Spaghetti_code). One idiomatic way of doing input validation in C is to use a do/while loop. &amp;#x200B; int pw\_invalid = 0; do { pw\_invalid = 0; printf(" Enter your password: "); gets(pw); &amp;#x200B; if(strstr(pw, first) || strstr(first, pw)) { printf("\\n\\nCan't have similarity between password and first name); pw\_invalid = 1; } &amp;#x200B; if(strstr(pw, last) || strstr(last, pw) { printf("\\n\\nCan't have similarity between password and last name); pw\_invalid = 1; } &amp;#x200B; } while(pw\_invalid);
**Spaghetti code** Spaghetti code is a pejorative phrase for unstructured and difficult-to-maintain source code. Spaghetti code can be caused by several factors, such as volatile project requirements, lack of programming style rules, and insufficient ability or experience. *** ^[ [^PM](https://www.reddit.com/message/compose?to=kittens_from_space) ^| [^Exclude ^me](https://reddit.com/message/compose?to=WikiTextBot&amp;message=Excludeme&amp;subject=Excludeme) ^| [^Exclude ^from ^subreddit](https://np.reddit.com/r/C_Programming/about/banned) ^| [^FAQ ^/ ^Information](https://np.reddit.com/r/WikiTextBot/wiki/index) ^| [^Source](https://github.com/kittenswolf/WikiTextBot) ^] ^Downvote ^to ^remove ^| ^v0.28
I've already written sinple neural networks in C. Now I'm writing one in ASM.
"cold dead hands" yeah those arrays work great for the writer but when somebody else comes along can they have to trace where every malloc got freed.
As far as I can tell, The string called format is the template of the value "a decimal either + or - and an another decimal", just like we use in the function scanf(). sscanf gets a string and matches the template with the string, %*[+-]j is the part where it discards the sign and two arguments are the pointers for the values read by %d tokens.
The way it's put there is not very beginner friendly, all this function does is a `scanf` over a pre submitted string. We can re-arrange it as a bit of code that will take the input in the standard input ```c scanf(format, &amp;real, &amp;imaginary); // do some calculation on the real &amp; imag. part of the parsed number ``` One other complex thing here is the format : instead of writing a literal as it is usually done in beginner courses, the format is stored in a variable and contains an odd identifier (`%*[+-]`) which means "any characters in the square bracket" (it's very regex-like thought I'm unsure about its complete abilities). The whole format stands for "an integer followed by either + or - then j and finally another integer". I hope I answered your question clearly !
&gt; No, which is why I support the proposals seeking to limit "run-away" interpretations of the standard. All of the proposals I've seen miss a fundamental point: what Ritchie designed was not so much a language, but rather a meta-language. If a program represents a mapping between inputs and outputs, and a language represents a mapping between source texts and programs, then C (as Ritchie invented it) is a mapping between execution environments and languages. If some action is supposed to "Do X in the underlying execution environment, with whatever consequences result", and some platforms specify the consequences of that action but others don't, there's no way the behavior can be described within a language that isn't tied to a particular environment. If one were to define a C implementation as being an entity that works with, but is separate from, an execution environment, then many of these issues would go away. Indeed, it would be practical to define a dialect of C which was completely devoid of any form of Undefined Behavior within the language, save for one proviso: if anything causes the execution environment to behave in a way contrary to what the implementation requires, the implementation would be entitled to request any arbitrary sequence of operations from the execution environment, with any consequences that could occur as a result of executing some sequence of operations. That isn't to say that everything in such a dialect would be fully and completely specified. To the contrary, many actions could have behavior specified as choosing in Unspecified fashion from among various courses of action whose consequences would be identical for "normal" cases, but might differ in various corner cases. If some allowable way of processing a construct knocks the environment off the rails, and as a consequence it ceases to meet its requirements, the implementation would then be allowed to jump the rails as well, but the implementation would only be allowed to jump the rails as a consequence of the environment having done so. Although requiring that implementations strictly follow the low-level recipe implied by a program would impede optimization, the remedy for that is not to declare many things to be UB, but instead to recognize situations in which implementations would be allowed, or could be invited, to replace various constructs with generally-equivalent constructs without regard for whether those replacements might alter program behavior. It would be the programmer's responsibility to ensure that all valid replacements would meet requirements. For example, the behavior of `restrict` could be specified by saying that if any region of storage is accessed both via `restrict` qualified pointer (or another derived from it) and via other means, the compiler may behave as though the accesses were performed in arbitrary sequence during the lifetime of the pointer. Allowing such reordering would be sufficient to enable most of the useful optimizations which `restrict` could facilitate, but avoid the need for programmers to write special code to avoid corner cases in which conflicts might occur, but where it wouldn't matter who "won". Specifying optimizations in terms of allowable transformations would make it easier for compilers to know when optimizations could be applied, reduce the amount of corner-case code programmers would have to write (and in many cases optimizers wouldn't be able to eliminate), and reduce the frequency of compiler bugs that occur when a compiler fails to recognize the possibility of a construct having defined behavior. Wins all around, but I've never seen any proposals take such an approach.
Just a warning: you say "checks its format" but that is something it *doesn't* do. I could pass in "xyz" in complexNumber and you will essentially get a random result back. sscanf returns the number of successful conversions it has done (which would be 2 for a valid complex number) but that return value isn't checked here. Just FYI.
I was referring to arrays allocated on the stack actually, when I do need a dynamic array I still use a C-style array but I use a smart pointer to point to it.
`sscanf` works exactly like `scanf`, except instead of `stdin` it will reads from a string.
Yeah, I expresed myself bad there, but I get the point. Thanks!
Mozes da stavis lepo scanf bez problema, to format nzm sto su koristili kad niko normalan nece to staviti. U potpisu scanf funkcije imas da je ono sto uvek pises pod navodnicima niz const char valjda, pa je zbog toga moguce ovako pisati. Srecno prekosutra
Thank you very much. Explanation was thorough enough. I understood that he will take two integers between + or - but wasn't quite sure about **%\*** thing. Now I understand.
Given `struct s {unsigned arr[5];} foo = {0};`, would the statement `foo.arr[0]++;` access the stored value of object `foo`? What is the Effective Type of `foo`, and what is the type of lvalue `foo,arr[0]`? So far as I can tell, since `foo` has declared type `struct s`, the Effective Type of `foo` is `struct s`. The lvalue expression `foo.arr[0]` is equivalent to `*(foo.arr+0)`. That expression in turn computes a value of type `unsigned*` and dereferences it. Applying the `*` operator to a value of type `unsigned*` yields an lvalue of type `unsigned`. Consequently, the type of the lvalue used in the statement `foo.arr[0]++;` to access the stored value of a `struct s` is `unsigned`. I think that's all pretty clear from the Standard. Am I missing anything? The Standard exhaustively lists all of the types of lvalue types that implementations must allow to access objects with some particular Effective Type. They are: &gt;-- a type compatible with the effective type of the object, -- a qualified version of a type compatible with the effective type of the object, -- a type that is the signed or unsigned type corresponding to the effective type of the object, -- a type that is the signed or unsigned type corresponding to a qualified version of the effective type of the object, -- an aggregate or union type that includes one of the aforementioned types among its members (including, recursively, a member of a subaggregate or contained union), or -- a character type. So far as I can tell, `unsigned` is not compatible with `struct s`, nor a qualified, signed, or unsigned version of `struct s`. Further, it is not any kind of aggregate, nor a character type. Consequently, it does not meet the requirements for Nf1570 6.5p7. I am by no means suggesting that the authors of the Standard didn't intend that `foo.arr[0]++;` should behave in sensible fashion. I don't think that they wrote N1570 6.5p7 in such a way as to *require* implementations to support such a construct, however, Since the authors of the Standard have acknowledged in the Rationale that an implementation could conform to the Standard while being of such poor quality as to be useless, and since any compiler that wasn't trying to be useless would support `foo.arr[0]++;` whether required by the Standard or not, there was no reason for the authors of the Standard worry about whether the actually mandated sensible behavior in such cases. The only "problem" caused by the failure of the Standard to mandate behavior of code that should obviously be expected to work sensibly is that acknowledging such failure would expose the absurdity of the notion that failure to mandate the behavior of a construct implies any sort of judgment as to whether implementations should *generally* be expected to support it,
- Haskell to broaden your horizon . - Python. Can't hate a language which is so damn useful. - C++. Why not?
I’ve never actually seen this done in practice (been programming in C since like 1994), and it’s one of those corners of C that probably shouldn’t be exercised much. Aside from other problems, this is the one feature in the language that still requires old-style function definitions, which again, are almost nonexistent now. If you go in (buffer, size) order, which is considerably more common than (size, buffer), you have to do char *foo(char buffer[*], size_t size); for the prototype because `size` isn’t declared first, and char *foo(buffer, size) size_t size; char buffer[size]; {…} for the definition. If you use something other than `size_t` for the size—e.g., `int`—you’re getting into UB territory if the `int` is &lt;0. (And the compiler may be able to *assume* that the `int` is &lt;0 even if it’s not, which means explicit bounds checks inside or around the function call might be deleted.) Neither the prototype nor definition form are remotely compatible with C++ either, so if you need cross-language compatibility you need to maintain two completely different prototypes and definitions. And the `[size]` version doesn’t give you any extra assertions, it’s just a boy-I-hope-there’s-an-array-there sorta thing. You can (ab)use the `static` keyword to type-assert nonnullness of the array (`[static]`, `[static *]`, `[static size]`), but again, it’s just a boy-I-hope-it’s-nonnull and it may impede you from actually *checking* that it’s nonnull.
Pa da. Bilo mi je cudno malo jer i sa scanf moze da se postigne isto to. Hvala za prekosutra :D Inace, nisam bas ocekivao da cu naleteti ovde na nekoga sa faksa, krajnje prijatno iznenadjenje.
You're more helpful than you realize : )
If you want to have at something specific like a Raspberry Pi, that’s not a terrible idea as long as you work from within supervisor mode—one thing to aim for, preexisting source code to imitate, etc.
Make something text-based, like a small adventure game or a menu-based combat game (like something where two Pokémon fight each other). If you’re doing it in straight C then you won’t have enough to put graphics in there, especially since it’s an introductory course.
Tic tac toe
You could make a breakout clone with SDL. get started here: [http://lazyfoo.net/tutorials/SDL/index.php](http://lazyfoo.net/tutorials/SDL/index.php) First couple of lessons should get you started and you should have everything you need by lesson 30. You could use 2 dimesional arrays for levels, and load them from files.(IDK what you mean by archives but that could pass as them i guess.) also you could use different types of blocks(like some of them breaks in more than one hit), and keep color/texture variation, hit count etc in a struct for each block. so you would have 2 dim. arrays of structs.
Consider a single level of space invaders; you could even do it in ASCII (using ncurses). You could use ECS and bang this out in less than a week.
Anything with graphics (even console graphics) is probably out of the question if you're a beginner, so think much simpler. Things like tic tac toe (multiplayer, no AI), guessing games, trivia games, or anything else that can be done with a simple text interface is probably a good place to look.
Chess, crosswords, sudoku, noughts and crosses (with some rudimentary AI)
Hunt the Wumpus.
Tic Tac Toe
&gt;we have to make a game in C, including the use of arrays and structs and archives. These requirements clearly suggest a simple text game. I'm not sure what you mean by "archives" though.
I'm a big fan of Go and its builtin cross compiler, but for other languages it can be a hassle to setup cross compilation toolchains. Here's a tool for wrangling many different platform builds, using Vagrant boxes to do the heavy lifting. Tested for basic C, C++, D, Haskell, and Rust projects. Includes demos for building amd64, i386, ppc64el, cloudabi, and wasm binaries. GNU/Linux host recommended.
This code doesn’t check anything. First off, if `complexNumber` (which should be declared as `const char *`, because you’re not modifying it) happens not to be of exactly the right format, you won’t initialize `real` or `imaginary` correctly, so anything using `real` or `imaginary` after that is potentially UB. You need to check the return value from `sscanf` to make sure you actually got 2 things out of it. `sscanf` also doesn’t check for anything *after* its format, so there could be a ton of garbage after an otherwise-valid complex number. The `scanf` family of functions is not all that useful in the real world; just walk through the string, check its format, and extract the values yourself. `strtod` or `strtol` would be better for this, and those *do* allow you to check format properly. Also, whenever possible, the format should be directly passed to `printf`/`scanf` functions (both for your own sake and so the compiler can error-check more easily), and there’s absolutely no reason you should be creating a format string whose contents are statically known on the stack. There’s even a major overflow vulnerability here, because you’re accepting `int`s and then squaring them and adding them together. You should *probably* be accepting `double`s in the first place, but if you stick with `int`s, you need to do (double)x*x + (double)y*y to ensure you don’t induce UB by overflowing a signed calculation. And usually you don’t want input and calculation to be all in the same place; input to a data structure (e.g., `_Complex` or a `struct`) in one place, and use a separate `magnitude` function for the `sqrt` part of things.
The key question is whether access to a member object constitutes access to a containing object, and I've never seen this question answered in a convincing manner using only the letter of the standard (ignoring the obvious intent). But if we go this route, then both of the two snippets you gave involve the same problem with `*p = 'H';`, and I still fail to see the "huge difference" between them. Did you mistype one of the snippets?
Hangman
You are probably right. I haven't looked at six much at all. I honestly still feel like I learn something new about five every week.
I did think of those, but as it will be a work to finish the subject I think it may be way too simple, as in lazy, no?
Not for your first semester
A common coding question in interviews is connect 4 although tic-tak-toe would be easier.
&gt; The key question is whether access to a member object constitutes access to a containing object, and I've never seen this question answered in a convincing manner using only the letter of the standard (ignoring the obvious intent). For all purposes other than 6.5p7, I'd say that an access to a member object is clearly and incontrovertibly an access to the containing object. I see no basis to interpret those words specially for N1570 6.5p7. A very simple one-word change, however, could have made the predecessor to N1570 6.5p7 almost perfect except for the unnecessary `character` type exemption that needlessly restricts optimization and should be deprecated. Simply replace the word "accessed" with "aliased", noting that aliasing involves conflicting accesses by *seemingly-unrelated objects*. &gt; But if we go this route, then both of the two snippets you gave involve the same problem with *p = 'H';, and I still fail to see the "huge difference" between them. Did you mistype one of the snippets? Imagine a compiler which uses common sub-expression elimination and loop hoisting, but otherwise processes code in linear fashion and doesn't otherwise make any effort to keep track of where pointers have come from. Such a description would fit a reasonably advanced compiler in the 1980s or 1990s--the sort of thing toward which the Standard was geared. Given the code: struct s { int x; } myStruct; int test(void) { int *p; ... myStruct.x = 1; *p = 2; return myStruct.x; } the only ways such a compiler could avoid caching `myStruct.x` in cases where `*p` might identify the same storage would be to either keep track of what was in the `...` or refrain from caching it regardless of whether anything in the `...` would cause problems. My first example fits that pattern. The second example, however, fits a different pattern: struct s { int x; } myStruct; int test(void) { int *p; ... myStruct.x = 1; p = &amp;myStruct.x; ... Here, all a compiler would have to do to avoid problems if code in the `...` happens to write to `*p` and then read `myStruct.x` would be to flush any cached value for `myStruct.x` when its address is taken. Although such flushing wouldn't be necessary if nothing ends up writing `*p`, the cost of pessimistically flushing would generally be fairly low. Further, the amount of additional compiler sophistication needed to avoid most of the unnecessary flushes would be relatively low, as would be the cost of performing flushes except when they could be *proven* to be unnecessary. The approach I'd like to see the Standard adopt to recognize such distinction would be to define the concept of an "lref", along with verbs "to reference" and "to write-reference". The behavior of `someStruct.member` in cases where `someStruct` is an lvalue would be defined as either referencing or write-referencing `someStruct`, based upon whether the resulting reference would ever be used to modify the object. Adding that concept would make clear that an operation `int *p=&amp;myStruct.x;` is an action involving `myStruct` with sequencing implications on `myStruct` and the resulting pointer.
A simple number guessing game would be a good candidate for an introductory project. Your program thinks of a number between 1 and N, player tries to guess, program answers with "too high," or "too low." Player gets 3-5 tries to guess before game over.
Another problem is that %d in scanf causes UB if the input contains a value out of range for int
&gt; unless by "actual aliasing" you're not referring to the standard, which is how I interpreted it after you'd talked about compiler writers misconstruing the standard. The Standard never defines aliasing. The Standard has been construed as defining 'aliasing' as describing all uses of objects contrary to N1570 6.5p7 or its predecessors, but its actual purpose is the opposite: to describe situations which compilers must accommodate *despite* aliasing between the objects involved. As used elsewhere in computer sciences, two references to an object alias if they are used directly or indirectly to access the same resource in conflicting fashion during each others' active lifetimes, in a context where neither is a recognizable derivative of the other. Things like loop hoisting can make a precise definition of "aliasing" difficult, which may be part of the reason the Standard didn't bother defining the term. Such issues could be dealt with by recognizing that a compiler processing code for a function or bona fide loop need not recognize, within that function or loop, relationships between references created outside it.
Check this book out: https://archive.org/details/bitsavers_decBooks10Mar75_26006648 Other than that I guess tic tac toe, battleships, hangman etc. If you're feeling brave you could try snake.
You could make something like the game of fifteen, Conway's game of life, or snake if you're feeling ambitious. You can use ANSI escape sequences to clear the terminal screen, allowing you to utilize simple textual graphics.
or better, make a minesweeper clone
&gt;The Standard never defines aliasing. No, but there's a non-normative statement of intent in a footnote. &gt;The Standard has been construed as defining 'aliasing' as describing all uses of objects contrary to N1570 6.5p7 or its predecessors I've never seen this. I've only ever seen "aliasing rules" (or "strict-aliasing rules," for that matter) used to refer to the list in 6.5#7 and "aliasing violation" used to refer to the undefined behavior resulting from the violation of the "shall" attached to that list.
&gt; &gt; The Standard never defines aliasing. &gt; No, but there's a non-normative statement of intent in a footnote. Nothing in the footnote indicates any intention to define aliasing. Instead it assumes that the reader will know what the term means. &gt; &gt; The Standard has been construed as defining 'aliasing' as describing all uses of objects contrary to N1570 6.5p7 or its predecessors &gt; I've never seen this. I've only ever seen "aliasing rules" (or "strict-aliasing rules," for that matter) used to refer to the list in 6.5#7 and "aliasing violation" used to refer to the undefined behavior resulting from the violation of the "shall" attached to that list. The use of the term "aliasing violation" would suggest a situation in which references alias, but which does not meet the requirements of 6.5p7. That would necessitate a definition of "aliasing" which is applicable to all the situations the compiler refuses to handle, whether or not those situations would involve aliasing by any other definition.
Conway's Game of Life is technically a game, and is probably the most interesting simple project you could do.
&gt; The use of the term "aliasing violation" would suggest a situation in which references alias, but which does not meet the requirements of 6.5p7. That would necessitate a definition of "aliasing" This is getting kind of ridiculous now -- a far cry from a post simply talking about common causes of security flaws. But I see no difficulty at all with taking 6.5#7 and its non-normative footnote ("The intent of this list is to specify those circumstances in which an object may or may not be aliased") to be self-contained and to regard an aliasing violation as simply the violation of the 6.5#7 "shall." That's certainly what C programmers mean in practice.
A few step by step examples here: https://gtk.dashgl.com/
There is a package called bsd-games that contains a bunch of games that run in the terminal that are written in C. You might come up with some ideas looking through the list.
Chess is harder than you would think.
You've never really answered my earlier question, except by suggesting that a write to a struct member kinda sorta doesn't really affect the stored value of the parent. I'm not sure where you get that notion from. Should a compiler given: struct foo {int arr[10]; } x, y; void test(void) { x = y; x.arr[0] = 1; y = x; } be expected to process `test` in a way that writes to y.arr[0]? Why? If, as you're claiming, the write to `x.arr[0]` doesn't affect the stored value of `x`, that would make the write to `y` redundant, allowing a quality compiler to eliminate it. And I see no way the write to `*(x.arr+0)` could affect the stored value of `x` without invoking UB. My conclusion is that the Standard is worded in a way that fails to define the behavior, because the authors of the Standard thought that people writing quality implementations would seek to usefully fill in any gaps, without the Standard having to order them to.
I think they mean files.
Probably too big for a beginner
Sudoku
Most card games can work well here, even in text mode. If you don't want to make extensive "AI", games like Solitaire or 21 don't need it at all (the latter is actually very simple and a good choice for the first project).
That's actually a very good idea, didn't think of it!! Thanks!
Yeah sorry hahah mixed uos the words, but that makes sense
Assuming int on your platform is 4 bytes... If there is 1 byte remaining, then why are you memcpy-ing 4 bytes into a uin16_t that cannot handle that much data: if (length % 2 != 0) { accumalator = accumalator+ length/2; /* Point accumalator to the end block */ uint16_t end_block = 0; memcpy(&amp;end_block, accumalator, sizeof(length)); sum += ntohs(end_block); if (sum &gt;= 0x10000) { sum -= 0xffff; } }
I've seen this before and it's a really great video
hangman sounds perfect the files would be list of words you'd get word from random. see how many words a player gets before dying. upon dying, highscore can be created, containing name and score, read and saved to file highscore itself uses array and struct(name,score)
Don't forget about godbolt which is amazing for this. For example [this](https://godbolt.org/z/jBg9PR) shows how you go from #include &lt;array&gt; std::array&lt;int64_t, 4&gt; a; void func(void){ a[0] = 1; a[1] = 3; a[2] = 5; a[3] = 4; } to this func(): mov QWORD PTR a[rip], 1 mov QWORD PTR a[rip+8], 3 mov QWORD PTR a[rip+16], 5 mov QWORD PTR a[rip+24], 4 ret a: .zero 32
That is not C my dude...
Ah jeeze, sorry, force of habit. Fixed, thanks!
Could somebody explain why C needs type pointers if machine code does not?
The machine code uses relative addressing buried inside the operand to do the same job as a pointer+offset. You usually won't see a direct equivalent usage in the machine code because the compiler knows where all the variables are in memory and will re-base the pointer usage off an address already loaded into a register, usually the stack pointer if you're accessing a local variable. You can see pointer usage if the write to the pointer is opaque and the compiler can't optimize it, eg: extern int* p; void foo() { *p = 42; } turns in to: foo(): push rbp mov rbp, rsp mov rax, QWORD PTR p[rip] &lt;- load pointer into register mov DWORD PTR [rax], 42 &lt;- deref and write 42 nop pop rbp ret
For those that would like to go further I can only recommend Reverse Engineering for Beginners ([RE4B](https://beginners.re/)). It's a free book that teaches how to read assembly fluently. It's method is really simple and effective so you don't really need the book (but it really helps understanding why some compilers do things such way): * choose a very simple piece of C code * compile it * show how things fit into place * compile it with more compilers, with or without optimization etc * point out any difference in the code generated * start again with a more complex piece of C code It certainly did the trick for me. (Also if anyone wants to go deeper in reverse engineering note that RE4B doesn't delve into protection mechanism, file format RE, protocol RE or dynamic analysis so you may want to read `Reversing: secrets of reverse engineering`.)
but why does C have to have the type of data in its pointers in order to access the data? or is that just to check type specific operations at write time?
C needs to know how to read the data, specifically it needs the size. The type information is used to determine how many bytes to read. The machine code bakes that information into the encoded instruction, in the example I posted the "DWORD PTR" bit indicates the instruction is set to read 4 bytes into the register. Change the pointer to a `char*` and that would change to "BYTE" and be encoded differently.
&gt; I've always used char* argv[]. I guess that signals to the programmer that the pointer is to an array. `char* argv[]` means "array of pointers", not "pointer to array".
Yes it does make sense in light of the other requirements. Write an array of structs from memory to file and read it back later. It would demonstrate clearly that you understand the concepts.
This would be a perfect one week project!
An array is a pointer. That's what I'm saying.
I once heard Ben Eater described as the Bob Ross of electronics. And I think they were right about that.
[Relevant](https://queue.acm.org/detail.cfm?id=3212479).
You catch segfualt at runtime, just like you do with Java's NullPointers and ArrayIndexOutOfBounds. You can also use safe programming libraries to eliminate the risk. Also most compilers have switches to prevent stack and buffer overflows. If I am not misstaken they are not part of the language, but most compilers include those as compile time switches. I was programming lots in Java, and once knew it inside out, almost every API. Yes, in casual code you can get almost same speeds. But when it comes to certain tasks where performance matter you will never come remotely close to C/C++ speed. Unless you go jini path which is then implementing it in C/C++ anyway and just wrapping it with Java for convenience, which defeats the purpose.
Of course. But you will have to be somewhat experienced to know what to go for and what to leave out for "later" when it comes to learning the language itself.
At least with [Zenobia](https://en.wikipedia.org/wiki/MS_Zenobia) we got a great diving site thanks to software error! Being there last summer, can just recommend, great dive. Unfortunately I wish I could make a joke with Boeing too, but the tragedy is real :-(.
Ok. My bad then for having prejudice to your post. I am truly sorry. However, C still has very good to help you catch errors and help you find bugs, and even profile your code to get it running faster and more memory efficient. Just as a correction to your written work :-).
I have been programming in C since 1991, and I rarely need to use pointers... But, C is able to do some real witchcraft acrobatics with pointers, should the need arise.. C can do it.. Git 'er done.. Ok, now can someone please give the real answer?
One conclusion is that culturally, we're simply no longer interested in that sort of thing.
Thank you
Hmmm ... what is "that sort of thing"? C programming?
By the way, seems that development is quite going on: [http://www.open-std.org/JTC1/SC22/WG14/www/docs/n2370.htm](http://www.open-std.org/JTC1/SC22/WG14/www/docs/n2370.htm)
you i love you
Beep boop, I'm a bot. This user above is a spammy bot that makes posts completely at random. Some stats: This bot has made (spammed) 996 replies in the last 2 hours. --- ^^^Pardon&amp;#32;me,&amp;#32;r&amp;#47;C_Programming,&amp;#32;I&amp;#32;am&amp;#32;a&amp;#32;bot&amp;#32;trying&amp;#32;to&amp;#32;make&amp;#32;sure&amp;#32;you&amp;#32;know&amp;#32;that&amp;#32;nkid&amp;#50;99&amp;#32;is&amp;#32;a&amp;#32;bot&amp;#32;|&amp;#32;[Learn&amp;#32;why&amp;#32;or&amp;#32;give&amp;#32;feedback](https://reddit.com/c4g9i5)&amp;#32;|&amp;#32;Reply&amp;#32;with&amp;#32;'delete&amp;#32;this'&amp;#32;to&amp;#32;remove&amp;#32;if&amp;#32;&lt;&amp;#32;3&amp;#32;votes!
delete this
Technology in general. By that I mainly mean the sort of technology needed to produce actual goods, events and services. When you say "tech company" now, you mean an organization that burns through investor money and maybe has some sort of unexpected payoff. Meanwhile, in the "real economy" there's less and less interest in it. Perhaps rightfully. Specific to C, the set of compromises it represents have either been forgotten or made a scapegoat. Maybe that will matter. Maybe it won't. It remains to be seen. In the end, this represents the resistance to giving tech an actual seat at the table and is more about making tech "go away". You'll all be working for truck drivers...
&gt; You've never really answered my earlier question, except by suggesting that a write to a struct member kinda sorta doesn't really affect the stored value of the parent. I did answer it: "The key question is whether access to a member object constitutes access to a containing object, and I've never seen this question answered in a convincing manner using only the letter of the standard (ignoring the obvious intent)." If it wasn't clear from that, I consider it a formalist defect in the standard that the intent with regard to aliasing -- that access to a member object under the provisions of 6.5#7 should not violate those provisions with regard to access to the containing object -- can't be derived convincingly from the mere letter of the standard. You seem to want to argue with someone in tedious detail that the standard isn't a model of formalist perfection, and if you can't find someone who holds the opinion that it is a model of formalist perfection, you seem to want to force that opinion on someone nevertheless. For the record, I think the standard is very far from a model of formalist perfection, and I often enjoy quoting the standard as a *reductio ad absurdum* against those who treat it as a model of formalist perfection. &gt;My conclusion is that the Standard is worded in a way that fails to define the behavior, because the authors of the Standard thought that people writing quality implementations would seek to usefully fill in any gaps, without the Standard having to order them to. Yet you seem to object not only when people don't usefully fill in any gaps (authors of optimizing compilers -- and I fully agree that they've gone too far) but also when people do.
Hmma, nah, both yes and no. For the no part, there are companies that are producing values and new technologies. But for the yes, sure, there are companies that are generally living they luxury life on investor's money. I think that was always a trait of the human beings, it is just that it has become easier to create a joke company doing tech nobody really understands or need. About C, you might have right that some reasoning behind C has got forgotten. It may be it is no longer relevant. I don't know either. One of things that C made as it is, is the philosophy that declarations should look same as initialization, which was meant as a general rule to make C syntax look simple and easy to learn. No idea if this has become irrelevant, but when I look at some modern C++ syntax, Rust and some other languages, notably Haskell, I actually appreciate the thought. In the very, very end, I believe that tech is still in the driving seat in means of saying what will fly and what will not fly. But it is for sure that we are not flying the most efficiently. Looking at hardware improvement last two decades, and software layers that are pretty much eating all those improvements leaves one to wonder how fast we could really fly. Who remembers Eight Megabyte And Constantly Swapping text editor in world of 100 megabytes downloads for a simple chat or note taking app based on things like Electron? We have lowered our acceptance on efficiency quite in order to make programming available to everyone. Good or bad, it remains to be seen indeed.
We have C11, and it seems that development on C is quite going on: [http://www.open-std.org/JTC1/SC22/WG14/www/docs/n2370.htm](http://www.open-std.org/JTC1/SC22/WG14/www/docs/n2370.htm). (my comment below went wrong).
Either Haskell, Prolog or Lisp would be a good conceptual contrast to C and assembly. If you prefer something more used than Prolog or Lisp, then maybe Scala, Clojure or Erlang. Prolog is really cool, but not many people seem to use it nowadays. It is all about functional programming seems like.
Yeah. Python is new TCL. TCL was better than Python (in my opinion), but it is not always that better technology wins.
&gt; I did answer it: "The key question is whether access to a member object constitutes access to a containing object, and I've never seen this question answered in a convincing manner using only the letter of the standard (ignoring the obvious intent)." If it wasn't clear from that, I consider it a formalist defect in the standard that the intent with regard to aliasing -- that access to a member object under the provisions of 6.5#7 should not violate those provisions with regard to access to the containing object -- can't be derived convincingly from the mere letter of the standard. The way terms "stored", "value" and "object" are used elsewhere in the Standard would imply that the stored value of an aggregate comprises the stored value of its constituent parts. I guess the question is whether one wants to adopt a definition of "stored value" which differs from any usage anywhere else in the Standard, or one wants to believe that the Standard's normal way of saying "Action X should be processed in fashion Y except when it would be impractical to do that consistently, in which case an implementation may behave in whatever fashion is necessary" is "The behavior of action X is undefined". I think the latter belief is more reasonable, given that there are other places in the Standard where the authors use the latter phrasing but the Rationale indicates that they expected the former meaning. In the discussion of integer promotion rules, for example, they discuss the circumstances under which commonplace implementations will process signed and unsigned arithmetic identically or differently, and indicate that commonplace implementations will process them identically even in circumstances where the Standard would not require it, Returning to questions of security, a C implementation could facilitate the writing of secure programs by constraining behaviors in circumstances beyond those required by the Standard. The notion that "optimization" should be focused on exploiting UB in a way that increases the cost of writing safe code, rather than on directives which would allow optimizations to be performed more easily, safely, and effectively, not only makes the language needlessly dangerous, but it also distracts from optimizations that could blow away the other languages that have been taking over. Consider, for example, the optimization benefits of a statement that would set an lvalue or sequence thereof to "unspecified" value. If a compiler can determine that the value of an object will get overwritten with "unspecified", it can prune out any operations involved in the calculation of that value. While such pruning could also be facilitated by e.g. forcibly zeroing the array, zeroing the array would typically add needless cost compared with simply letting the compiler know the array contents weren't needed. Overflow handling could also be enormously enhanced if C were to include an optional feature with a "latching" overflow flag that would indicate whether any overflow had occurred *which might have resulted in mathematically-incorrect computations*. The level of optimization allowed by semantics could be much better than the performance offered by languages using trap-on-overflow semantics, while at the same time avoiding the security pitfalls that could be posed by unchecked overflows.
I told you less than a week ago to cut out the PVS Studio spam and you keep doing it.
&gt; but things like casting types back and forth to void * scares me Just a question, how often is that necessary though? If you have a function that expects a certain pointer type, you'd pass that type in and not a void pointer. The only time I find myself casting voids is usually in the return calls from memory allocation routines - malloc/calloc/etc. In my regular code either I'm passing a pointer to a particular structure type or an array of items, in both cases I don't use void pointers.
Well, whenever you need to use type erasure, you use void * in C. Whereas C++ has templates, dynamic polymorphism, static polymorphism (CRTP), and these are much more type safe than void * pointers. I asked a question about libraries on reddit two days ago. And most people suggested to use C for libraries because of its stable ABI, which in turns helps to use the library in other languages. Whereas people suggested modern c++ is much more useful in implementing applications. With the advent of C++ 20 , it becomes more powerful.
Because you never print it. Sit down with your code and a notebook and “play computer”: read through the program line by line and take a note of what happens at each step and what value each variable has every time it changes.
On line 23, fgetc() pulls the first char to see if it's eof. If that test is passed, you start printing the remaining chars, effectively throwing the first char into the garbage. &amp;#x200B; One trick you can use is to find the full file length, and stop reading once you get to the end. The way you find the file length in C is comical and I'll leave it as an exercise.
This is all true I'm just curious how frequently you really need to type erasure.
You call `fgetc` in line 23 and store the resulting character in in `ch`. You then call `fgetc` again and overwrite `ch` before doing anything with it in line 27. The character is lost before you do anything with it.
&gt; An array is a pointer. Inside function parameter lists, array parameters are silently understood as pointer parameters. But aside from that very special rule, arrays and pointers are different types. For example, given `char a[5]`, `sizeof(a)` is 5, but `sizeof(&amp;a[0])` is 4 or 8, depending on the platform. Also, you can increment pointers, but you cannot increment arrays. There is an implicit conversion from arrays to pointers, but that doesn't mean they're the same thing!
&gt;The way you find the file length in C is comical and I'll leave it as an exercise. A proper Unix program cannot and should not find the length of a file.
Probably very frequently, it really depends on your project.
And while you are doing that, think about what happens if the last character in the file is not a newline.
What about "ls"?
Could've used this back when I was taking classes that made you hand-write assembly code on exams.
What about when your reading from stdin rather than an actual file on a filesystem?
The answer has already been given by others, I just wanted to suggest putting `a.out` in your .gitignore file, you don't really want binaries under version control.
The ls and stat programs should display information from the stat system call (or equivalent), which may include the length of the file. The wc program does actually use the file length--if it's only counting bytes--but correctly only uses that as a short-cut, with a fallback to simply reading the file.
Named pipe. Character device. File that's still being appended to.
fseek() returns -1 and errno is set to reflect the specific error.
So your suggested method won't work then.
Yup, those are good examples too.
look at the include flag for gcc (-I) &amp;#x200B; instead of "../includes/some\_header.h" you would use the -I for the folders that include the headers you are using, and simply #include the head name if you're not using make and makefiles yet to build your project, I would highly recommend starting that earlier rather than later.
Thanks, guys, working code available at the same link.
https://stackoverflow.com/questions/597318/what-are-the-benefits-of-a-relative-path-such-as-include-header-h-for-a-hea
CUDA is definitely better than OpenCL but it's exclusive to Nvidia GPUs. OpenCL is for Intel HD graphics.
In addition to what thegreatunclean said (which is right), the compiler uses the type information associated with a pointer in all kinds of alias analysis, which tells it what pointers can or can’t possibly point to the same object, and it uses the size associated with the pointed-to type for assistance in address calculation in operations like `a[i]`. In theory, alignment information for the pointer can inform the compiler as to what accesses might be UB too, although that’s more of a danger than a benefit. More generally, pointers are a really abstract version of addresses, and the two things may operate independently depending on how much optimization’s enabled and what the target ISA/ABI are.
Thank you for this brilliant explanation. I am using a makefile, (Multiple, actually.) They're just taking me some time to learn, on top of learning C. I'll update things now :)
&gt;errno is set to reflect the specific error It's set to `ESPIPE`.
Just to latch onto the top comment. It's generally good to make an `include` directory in your project, so that if you have `include/foo.h` and `include/bar.h` you can do `-Iinclude` and then in C source just have: #include &lt;foo.h&gt; #include &lt;bar.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; ...etc
CUDA. As much as I hate proprietary shit.
This is better, but it only works because the last line of your file ends in \\n. What if it didn't? Can you rewrite your file-reading loop (currently `while (1) { ... }`) so that it contains only one call to `fgetc`?
&gt;CUDA is definitely better than OpenCL but it's exclusive to Nvidia GPUs. OpenCL is for ~~Intel HD graphics~~ Any graphics or accelerator device.
Nice, thank you!
If you want to learn a bit about machine language, machines, and programming, I highly recommend "Code: The Hidden Language of Computer Hardware and Software" by Charles Petzold.
You can list/display that information and make rough estimates based on it, but actually using that to bound your file accesses is a bad idea because the metadata *before* accessing the file may be completely different by the time you make your own accesses. This sort of thing makes for an easy security hole (e.g., timing attack). It also only applies to files, not devices, pipes, FIFOs, or sockets, so `stat`-based file length is not all that useful in practice.
"You can create anything that makes you happy." - Bob Ross
Years ago I saw this video, went down the rabbit hole of the rest of Ben Eaters videos, and related channels, and ended up trying to build a CPU from discreet transistors. Take that as a warning or as encouragement.
There are good examples to for makefiles when your project is going to get complex/large. Like building the list of files by using a shell command like *find* to return a list of all the .c files in a directory. also considering building objects into their own folder so that "make clean" just deletes the objects directory (or everything in the objects directory)
Boy, you wasted no time formatting that, huh. Look at your `add` function. You’re taking a `struct *` argument, then immediately overwriting that value with a `malloc`d one that will disappear as soon as the function returns. This is analogous to void foo(int x) { x = 4; … } Nobody outside `foo` will ever see the `4` you assigned to `x`. Also, `int` is not at all a safe type for sizes or counts.
Modern OpenCL implementations should be fine on NVidia devices; it used to be that CUDA worked much better and NVidia didn’t care much about how the OpenCL side of things did, but now most stuff is well supported. In terms of ease of use, CUDA’s slightly easier to work with most of the time because the GPU code is better integrated with the CPU code until you look too closely at it. OpenCL C often ends up being embedded as huge inline strings, although you can &amp; should do otherwise.
It sounds like you're answering from a C++ perspective when you mention generic programming and class hierarchies. Do you do much generic style programming in straight C where type erasure comes into play?
There are two kinds of headers, and you’ll want to handle them differently. One type of header is for distribution and installation, and is needed to use your API. That type of header should be in its own `include` directory, preferably with subdirectory structure mirroring the installed structure. This `include` directory should be included via environment (e.g., `CPATH`) or `-I` option, and you should use the angle-bracket `#include &lt;&gt;` directives. The other type of header is for internal use within your project only. You *can* put these in their own include directory, but they shouldn’t be mixed with the installable includes and it’s usually easier to have them in the same place as the source files that use them. Use the quoted `#include ""` directives for these. Only in the toyest, crappiest crap toy code should you ever use `..`, `.`, or path-initial `/` components in your `#include` paths. These are not at all portable, and it usually indicates you’ve got things configured or arranged wrong.
I'm sorry for the slightly unrelated comment, but would you happen to know why the command in my makefile doesn't work properly? I'm attempting to compile with -I for my two separate headers, and -L for my .a file from my main library. The main doesn't actually error at all, but my functions compiled with this say : undefined reference to function_name_here For all of my libraries functions. gcc -L ./libft/libft.a -I ./libft/includes -I ./includes/ $(SRCS) The file structure is as such: https://i.imgur.com/OMXtvjb.png The header files being inside of ./includes, and ./libft/includes. I don't know if this is more helpful to you, but here is a repository with the issue: https://github.com/cameronstaljaard/push_swap
I wasn't suggesting using that data. Only that they are real programs considered part of a standard distribution, and they know how long a file is. So this silly idea that a linux program should never know a file's length is just talk.
The “should” doesn’t enter into it, it’s more of a “can’t” except in passing.
Thank you for the brilliant explanation. That actually is very helpful to me, and in response to similar comments, I did try to start using -I with gcc, but unfortunately, not to great success. If you're able, would you mind running make on a repository, and seeing why it may not be working? It was functioning perfectly, until it was modified to accommodate -I. https://github.com/cameronstaljaard/push_swap
`-L` is to specify a library directory, `-l` is to link in a library. You want: `-L ./libft -lft` You might as well go full proper Makefile. Make `CFLAGS`, `CPPFLAGS`, `LDFLAGS` variables. Have a rule to link your .o files into an executable. Have a generic rule to build .o files from .c files.
Just suppose you need to implement a management system in c. Maybe something like a salary management system. First you define a employee struct and it's associated functions. Then you make different employees based on designation. You need to call a function (let's say calculateSalary()). And when you do it on different types of employees you want different type of function to be called, you use void * pointers for type Erasure and function pointers. This is a pretty common scenario. Btw I know C, but haven't used it in any medium or big project.
Even with that change to the -L, it doesn't appear to be working. gcc -L ./libft -lft -I ./libft/includes -I ./includes $(SRCS) For the same reasons, it seems. "Undefined reference", for any and all functions called from my library. I do actually have basically those variables in my makefile, but I thought they'd be kind of useless to send in a reply, as you'd not be able to see what's inside of them without the inclusion of the entire makefile.
&gt; You seem to want to argue with someone in tedious detail that the standard isn't a model of formalist perfection, and if you can't find someone who holds the opinion that it is a model of formalist perfection, you seem to want to force that opinion on someone nevertheless. For the record, I think the standard is very far from a model of formalist perfection, and I often enjoy quoting the standard as a reductio ad absurdum against those who treat it as a model of formalist perfection. Sorry if I've been misinterpreting your intentions as trying to justify the Standard using what seems to me an absurd argument. Perhaps it would be better to impartially consider a couple of issues of terminology. If one were to strike 6.5p6 and 6.5p7, one could then say that every region of storage with size and alignment to hold an object of type T, is associated with an object of that type, whose stored value is represented by the sequence of byte values within that region. So far as I can tell, such definitions would be consistent with everything else in the Standard. Do you know of anything other than 6.5p6 and 6.5p7 that would contradict them? If one omits 6.5p6 (the "Effective type" rules, which use the term "object" in a way inconsistent with its usage elsewhere), then so far as I can tell, 6.5p7 could work just fine with the aforementioned definitions, with only a couple of tweaks. Replace the term "object" with "region of storage used as an object of a particular type during a particular execution of a function or bona fide loop", and replace the phrase "has one of the following types" with "is, within that context, freshly visibly derived(*) from an lvalue of one of the following types". Include a footnote "(*) the range of circumstances in which an lvalue is recognized as visibly derived from another is a Quality of Implementation issue, but a quality implementation intended for any particular purpose should seek to recognize such derivation in any cases necessary to serve that purpose". While I recognize that there would be some severe political problems with such tweaks, I'm curious if you would see any technical problems or cases where such rules would be inadequate.
That 404s, and I’m only seeing “Electron” and “Alone-in-the-Dark” repos. Offhand, maybe make sure you’re using absolute paths for `-I`?
Whoops, it's private sorry. That's my own stupidity there, I'll fix that now.
It's now public.
You don't need a rule to make .o files from .c files, there's an implicit one already. http://www.chiark.greenend.org.uk/doc/make-doc/make.html/Implicit-Rules.html#Catalogue-of-Rules
CUDA reigns king.
Couple of points of feedback: 1. Why wouldn't you use `fgets`? If you care about line numbers, that means presumably you only care about reading in whole lines. Using getc seems like it'd make your life harder. 2. As a matter of good coding practice, try not to get into the habit of writing `while(1)` loops. That's an open invitation for bugs to creep up. You should make your while loop contain a clear condition upon which it will terminate. I would write this as `while((ch = fgetc(fp)) != EOF)` 3. You should increment l_no closer to where we care about its value. When I'm incrementing a variable inside a loop like this, I often like to use the increment operator as I read the value of the variable if I don't need to reference it again. If I do, again, put it where you care about its value most. Since you're only using l_no once per loop, I believe you should do this: `printf("%d \t", l_no++);`
What does that even mean? It can't, except when it does?
https://stackoverflow.com/questions/11893996/why-does-the-order-of-l-option-in-gcc-matter
It means you can read the length of a file, but it’s meaningless wrt what you’ll actually see when you read the file. LIS it’s useful for a buffer-sizing estimate or something like that, but not for anything authoritative.
Thank you, I had no idea this was my issue.
Adding a "static" qualifier to the dimension will, at least in theory, allow a compiler to optimize by reading array elements out of order without regard for whether code actually ends up reading them. For example, given int any_nonzeros(int array[static 10000]) { for (int i=0; i&lt;10000; i++) if (array[i] != 0) return 1; return 0; } in the absence of the `static 10000`, this code would have defined behavior if passed a pointer to a single non-zero item, and thus a compiler would have to generate code that could handle such an argument. Adding the `static 10000` means that the compiler may assume that at least 10000 items may be read using that pointer, it it would thus be entitled to e.g. process the values in groups of eight without regard for whether that might cause it to read seven items past the first non-zero value.
It might be esier to put internal headers with the source but it's still bad practice. put them in /include/Private
This is a C sub, not C\#.
AMD
my bad, lol
I don't know anyone who rents AMD compute servers.
What I mean is that there used to be a large number of small to medium companies that had a tech component. Those are gone. If someone has a tech component, they resell stuff. That in itself is not all bad but it means that technology is no longer a strategic means of production - it's just a necessary evil. What I mean is that the firms that still exist by and large are composed of acquired companies. No longer are they doing much more than milking the brand. The "capitalist" class is simply no longer interested in that sort of thing.
Plenty of people who have amd hardware in their computers tho, so if you’re writing software for personal use, like a rendering engine, you should absolutely use OpenCL.
Your specific problem is almost certainly because you’re not using absolute paths in your `-I` options; you’re also invoking `make` on a directory that has no Makefile, which doesn’t help. Anyway, there’s some unusual stuff here, among which `srcs` and `includes` stand out as likely to bite somebody in the ass if they use this. (Usually it’s just src and include.) `CFLAGS` is the usual variable for options to `CC` in compile and later phases. `CPPFLAGS` is where the `-I` and most `-W` options should go, since includes are resolved by the preprocessor and warning/error settings matter to it too. You don’t need those flags anywhere else. Usually `-I` options are all one word, so `-Iinclude`(`s`). You also have two completely separate copies of the same `FLAGS` variable, which is not a good idea. Also, do not include `-c` or anything else that sets mode, input, output, etc. in those variables—`FLAGS` variables should be about tuning the compiler, and the specific make rule will decide whether to put `-c` or `-o` or whatever in there. I also note that you’re not using `CC` to link, you’re not giving it an output filename, and your a.out and binaries are checked into the repo, all no-nos. You should have a .gitignore like *.a *.o /checker7 along with *anything* else that can be rebuilt automatically. In your `clean` rules, don’t use `/bin/rm -f`, use the `$(RM)` variable that exists specifically for this purpose. Also, your phony rules need to be dependencies of `.PHONY` so `make` doesn’t get confused and think there should be files created. You don’t need the `-l` flag for a library that (a.) you know is of the static variety, and (b.) you have the full path for since you built it. You can just include the .a on the command-line. `-L` and `-l` are mostly for pre-installed libraries or where you could use either .so or .a. Anyway: # Suggested top-level Makefile libft.a is its own mostly-self-contained thing, and checker7 references libft.a but has no other final artifacts. So: # Need these to construct absolute paths; could do $(abspath) also but IIRC # that’s on the GNU side of things. top_srcdir := $(shell pwd) srcdir := $(top_srcdir) # End-of-table for convenience. __EOT__ := CC := gcc # The -W flags are optional; the -I flags are not. ifndef CPPFLAGS CPPFLAGS := endif CPPFLAGS += -Wall -Wextra -Werror override CPPFLAGS += \ -I$(top_srcdir)/includes \ -I$(top_srcdir)/libft/includes \ $(__EOT__) export CC CPPFLAGS CFLAGS top_srcdir TARGETS := checker7 checker7_SOURCES := \ srcs/checker.c \ srcs/ft_print_stack.c \ srcs/stacks.c \ $(__EOT__) all: $(TARGETS) checker7: $(checker7_SOURCES:%.c=%.o) libft/libft.a ⇥$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^ %.o: %.c ⇥$(CC) $(CPPFLAGS) $(CFLAGS) -c -o $@ $^ # Forward “libft”-related things into that Makefile. libft: libft/all libft/%: ⇥$(MAKE) -C libft $(@:libft/%=%) # Usually clean/allclean, clean/maintainer-clean, tidy/clean, etc. are used # for the different kinds of cleanup; “fclean” is not at all clear. clean: ⇥$(MAKE) -C libft clean || true ⇥$(RM) $(TARGETS) # Note that the RM variable exists specifically for this purpose. Use of # `/bin/rm` is unnecessary and not at all portable. tidy: ⇥$(MAKE) -C libft tidy || true ⇥$(RM) $(checker7_SOURCES:%.c=%.o) .PHONY: \ all clean tidy \ libft libft/all libft/clean libft/tidy # libft/Makefile srcdir := $(shell pwd) ifndef top_srcdir top_srcdir := $(srcdir)/.. endif __EOT__ := # These sorts of things would be handled better via `include`, but w/e. CC := gcc override CPPFLAGS += -I$(srcdir)/includes AR := ar RANLIB := $(shell which ranlib 2&gt;/dev/null || echo true) TARGETS := libft.a libft_SRCS = \ LIST SOURCE FILES HERE ALPHABETICALLY, ONE PER LINE \ $(__EOT__) all: $(TARGETS) libft.a: $(libft_SRCS:%.c=%.o) ⇥$(AR) rc $@ $^ ⇥$(RANLIB) $@ %.o: %.c ⇥$(CC) $(CPPFLAGS) $(CFLAGS) -c -o $@ $&lt; # Theoretically, headers should be part of the dependency list for these; # Automake would do all this for you. clean: tidy ⇥$(RM) $(TARGETS) tidy: ⇥$(RM) $(libft_SRCS:%.c=%.o) .PHONY: all clean tidy So that should be a good start, at least, although I won’t pretend there’re no bugs. This sort of mess is why people use even bigger messes like Autotools; they’re kinda awful, but they don’t require you to juggle quite so many weird effects of make and its subprocesses.
C# is off topic in this subreddit. Please post C# questions to /r/learncsharp.
Do not post pictures of text. Please replace the pictures of sample output with text.
my first programming was on an atari 400 using atari basic. ((i wrote my first(and only) assembler on that atari)) working on that atari writing basic(with line numbers), i can remember writing my first FOR LOOP that printed zero thru something like six hundred or nine hundred in one second! i felt like james bond working on that beautiful machine. atari basic: https://www.youtube.com/watch?v=_vK84lvwQwo
Note that using `$(shell` restricts the project to being built on a system with compatible shell.
IMO it's clearer to explicitly state your rule so the reader can see it (and see where the flags are coming from).
Why is it bad practice? They have to distribute with the source and they shouldn’t install, which is exactly the same sitch as for the source files. If you put them in the common include directory, even under their own, you’re more likely to accidentally refer to them (i.e., it should be an error but isn’t) and you’re more likely to get them installed by accident as well.
You're not protecting the write to `bag_errno` which constitutes a data race. The fix isn't to put a mutex around this because then it just becomes a race condition (threads clobber each other's error flags), but eliminate the global variable altogether. Instead return the error directly. (Besides this, you're not creating the global variable properly. By putting it in the header, every translation unit that includes the header gets its own copy of the variable.) Here's a simple demonstration of the data race. #include &lt;stdint.h&gt; #include "bagarray.h" void * insert(void *bag) { for (int i = 0; i &lt; 1000000; i++) { bag_array_insert(bag, (void *)(uintptr_t)i); } return 0; } int main(void) { bag_array_t *bag = bag_array_create(); pthread_t thr[100]; for (int i = 0; i &lt; 100; i++) { pthread_create(thr + i, 0, insert, bag); } for (int i = 0; i &lt; 100; i++) { pthread_join(thr[i], 0); } bag_array_destroy(bag); } Compile with thread sanitzer and run: $ gcc -Os -ggdb3 -pthread -fsanitize=undefined -fsanitize=thread \ main.c bagarray.c $ ./a.out Output from thread sanitizer: ================== WARNING: ThreadSanitizer: data race (pid=21785) Write of size 4 at 0x000000604040 by thread T2: #0 bag_array_insert /tmp/bagarray.c:111 (a.out+0x401734) #1 insert /tmp/bmain.c:8 (a.out+0x400f81) Previous write of size 4 at 0x000000604040 by thread T1: #0 bag_array_insert /tmp/bagarray.c:111 (a.out+0x401734) #1 insert /tmp/bmain.c:8 (a.out+0x400f81) Location is global 'bag_errno' of size 4 at 0x000000604040 (a.out+0x000000604040) Thread T2 (tid=21788, running) created by main thread at: #0 pthread_create ../../../../gcc-9.1.0/libsanitizer/tsan/tsan_interceptors.cc:964 (libtsan.so.0+0x2c6db) #1 main /tmp/bmain.c:19 (a.out+0x400de2) Thread T1 (tid=21787, running) created by main thread at: #0 pthread_create ../../../../gcc-9.1.0/libsanitizer/tsan/tsan_interceptors.cc:964 (libtsan.so.0+0x2c6db) #1 main /tmp/bmain.c:19 (a.out+0x400de2) SUMMARY: ThreadSanitizer: data race /tmp/bagarray.c:111 in bag_array_insert ==================
I disagree with your opinion that it somehow becomes easier to end up distributing the private headers. It's seriously not difficult to manage this way, I find it easier.
Beej's Guides =================----------------- Find it useful? Make payments with PayPal - it's fast, free and secure! Make a donation! News! Hey! Check out my tech blog!—and tell all your friends! Over the years I've accumulated much information on many different things, and I enjoy taking a crack at explaining them to other people. If you're lucky, that's you! The time I spend working on these projects ebbs and flows, but check back some time in the future, and you might find some entirely new stuff. book cover image Buy a (real) Book! Beej's Guide to Network Programming (online and for download) This is a beginner's guide to socket programming with Internet sockets. It is meant to be a springboard that will launch you into the exciting world of TCP/IP programming. This document has earned a fair amount of praise. Beej's Guide to Unix Interprocess Communication Now that you have extra-process communication going on with sockets, why not try your hand at some interprocess communication, eh? Shared memory, semaphores, signals, and memory mapped files await you! Beej's Guide to C [rough draft] This is a bit of a practice book for later when I write a real book. Since I know C more like the back of my hand than any other language, it's a good place to start (because I don't need many references to write it!) Keep in mind that this is completely incomplete right now, and I haven't even read most of what I've written. Some of it is Just Plain Wrong. So if you see errors of any kind, and there are a lot of them, feel free to drop me a line so I can fix them. Likewise, if you think the structure could be rearranged, or something was left out (since the book is in progress, lots of stuff is still left out), or if there was something you'd like to see, etc., etc., just let me know and I promise to at least think about it. :) Beej's Guide to the GNU Debugger (GDB) This is a quick introduction to using GDB, GNU's famous debugger, from the command line. Beej's Guide to Photography I take photos for fun and maybe even profit (from time to time). To do this, I use a number of little tricks and techniques that tend to make the photos turn on better. And I've decided to share a number of those with you fine folks! Also be sure to check out the "Toys" section while you're in there! Beej's Guide to Killing Dragons If you play Moria, this will help you slay Ancient Multi-Hued Dragons from the safety of your own home. Beej's Bit Bucket This is my programming-and-tech-related blog where I scribble down various pieces of information, as well as short programming tutorials. It's basically my outlet when I don't feel like writing an entire book. Contact: beej@beej.us Visit: Beej's Home Page.
Hi, thanks for the insightful analysis! So do you suggest passing the error code through either by return statement or passing the values as pointers? Is that what we call reentrant? Sorry for the newb question but I just want to get somethings straight before trying to explain it to someone else what I did.
Beej guide to C: https://beej.us/guide/bgc/html/single/bgc.html Full index of beej's guides: https://beej.us/guide/
Which is fine, but it’s not bad practice, you just do it differently.
Thanks so much for this!
Use a return statement. Right now you're just returning -1 on error, but you could return the error value itself if it's truly necessary to distinguish between different error conditions. &gt; Is that what we call reentrant? You've got the right idea. That `bag_errno` variable makes your library [non-reentrant (not *re-enterable*)](https://deadbeef.me/2017/09/reentrant-threadsafe) in addition to thread-unsafe.
That's really interesting and something I wasn't aware of before. So does that mean that the argument char a[] and a local char b[5] are actually two different things under the hood? char a[] is equivalent to char *a, but b actually isn't a pointer at all? Thanks for clearing that up for me. And it looks like there are practical differences (potential gotchas) as well, like the sizeof that you point out. I'll have to keep that in mind moving forward.
Thanks, but before making these changes I'd like to know if using signals and function calls like raise() would also make it reentrant, I mean I could use a message queue for these signals and get more information using siginfo_t listed in the sigaction() man page. What do you think?
Signals are exactly where reentrancy is critical even in a single-threaded program. When the signal arrives, the currently running function is paused and control is passed to the signal handler. If that function is called from the signal handler, it's being *re-entered*. With signals, do you mean having the signal handler register in your data structure that a signal occurred and letting a thread pull it out later to handle as a kind of "event"? The problem with this is that you can't use thread synchronization from the signal handler, so you can't interact with your bag array. For example, imagine a thread has locked a mutex and gets interrupted by a signal. The signal handler than tries to lock the mutex and blocks, waiting on itself. That's a deadlock. Oh, and I forgot to mention it in my original comment: You don't need the `volatile` qualifier. That's for special circumstances that don't apply here, not for thread synchronization.
I am not sure I agree with you on that one. As I said, both yes and no. We are getting new inventions, new applications, new algorithms and so on, so not everyone is just milking the brand. But certainly there are such. And everything in-between.
The internal business slogan over at the NVidia offices : .... what is best in life? To crush your enemies, see them driven before you... ...and to hear the lamentation of their women.
Beej’s C book has been in “rough draft” forever, lol.